{
    "paper_title": "MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching",
    "authors": [
        "Fabian David Schmidt",
        "Florian Schneider",
        "Chris Biemann",
        "Goran Glavaš"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages -- over 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 5 8 2 1 . 2 0 5 2 : r MVL-SIB: Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching Fabian David Schmidt1*, Florian Schneider2*, Chris Biemann2, Goran Glavaš1 1Center for Artificial Intelligence and Data Science, University of Würzburg, Germany 2Language Technology Group, University of Hamburg, Germany fabian.schmidt@uni-wuerzburg.de, florian.schneider-1@uni-hamburg.de Dataset: MVL-SIB"
        },
        {
            "title": "Abstract",
            "content": "Existing multilingual vision-language (VL) benchmarks often only cover handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for lowresource languages. To address this limitation, we introduce MVL-SIB, massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languagesover 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark range of of open-weight LVLMs together with GPT-4o(- mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like NKoo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing topic with more than one image, suggesting that these models are not yet fully effective at handling multiimage tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as comprehensive probe of multilingual VL understanding in LVLMs."
        },
        {
            "title": "Introduction",
            "content": "Large Vision-Language Models (LVLMs) extend Large Language Models (LLMs) to take images as inputs, leveraging their advanced language capabilities for vision-language (VL) tasks like image captioning and visual question answering (VQA). However, LVLMs are typically trained mainly on English data, leading to significant limitations despite the base LLMs multilingual abilities. They *Equal contribution. 1 Images-To-Sentences (I2S) Which sentence best matches the topic of the images? The images and the sentences each belong to one of the following topics: \"entertainment\", \"geography\", \"health\", \"politics\", \"science and technology\", \"sports\", or \"travel\". Choose one sentence from A, B, C, or D. Output only single letter! # Images # Sentences A. ```Maroochydore führte am Ende die Rangfolge an, mit sechs Punkten Vorsprung vor Noosa als Zweitem.``` B. ```Es wurden keine schwere Verletzungen gemeldet, jedoch mussten mindestens fünf der zur Zeit der Explosion Anwesenden aufgrund von Schocksymptomen behandelt werden.``` C. ```Finnland ist ein großartiges Reiseziel für Bootstouren. Das Land der tausend Seen hat auch Tausende von Inseln in den Seen und in den Küstenarchipelen.``` D. ```Es ist auch nicht erforderlich, dass Sie eine lokale Nummer von der Gemeinde erhalten, in der Sie leben. Sie können eine Internetverbindung über Satellit in der Wildnis on Chicken in Alaska erhalten und eine Nummer auswählen, die vorgibt, dass Sie im sonnigen Arizona sind.``` Your answer letter: Figure 1: Cross-modal topic matching Images-ToSentence for German with k=5 reference images. may fail to follow instructions or struggle to interpret text within images in Non-English languages (Schneider and Sitaram, 2024; Tang et al., 2024). Although many multilingual VL benchmarks exist, they typically cover at most 10 languages (Bugliarello et al., 2022; Liu et al., 2021; Tang et al., 2024, inter alia). Only concurrent work has scaled VL evaluation to 100 languages using machine translation (MT) with human post-editing (Vayani et al., 2024). Nevertheless, benchmarks constructed using semi-manual MT cannot support truly low-resource languages adequately, as current MT models lack the necessary quality for these languages. Moreover, existing benchmarks primarily assess lower-level VL semantics through concrete text-image relationships, such as those found in VQA. This underscores the need for VL benchmarks that cover truly low-resource languages and evaluate more abstract VL interactions. To address these challenges, we introduce the massively multilingual vision-language SIB (MVLSIB) dataset, which extends the topic labels of the multi-way parallel sentences from SIB-200 (Adelani et al., 2024) by associating each topic with hand-selected images. MVL-SIB evaluates cross-modal image-text topic matching in 205 languages: LVLMs must select one of 4 candidate Figure 2: Images-To-Sentences @ k=3. The English prompt describes the cross-modal topic matching task, lists all topics, and provides both k=3 reference images and 4 sentences in the corresponding language {eng_Latn, . . . , nqo_Nkoo}. LVLMs must select the sentence of 4 options that topically fits k=3 reference images. The sentences spanning 205 languages and 7 topics are drawn from SIB-200 (Adelani et al., 2024), while images for the topics were hand-selected (cf. Appendix A.1). An example prompt is shown in Appendix A.7.2; further details are in 4. Plot. The x-axis orders the languages of the candidate sentences {eng_Latn, . . . , nqo_Nkoo}, respectively, by descending performance (y-axis). The top x-axis indicates the running index of each language Li (i {1, . . . , 205}). sentences that best matches the topic of the reference images (images-to-sentence, cf. Figure 1) or, conversely, choose one of 4 candidate images corresponding to the topic of the reference sentences (sentences-to-image). Figure 2 displays the images-to-sentence performance for across all languages in MVL-SIB, sorted in descending order. Notably, GPT-4o-mini performs robustly on the top 125 languages. However, beyond that range its performance declines sharply, falling to chance levels in the lowest-resource languages, such as NKoo. Bridging these gaps is crucial for developing genuinely inclusive VL technology. Contributions. 1) MVL-SIB supports parallel VL evaluation in 205 languages on professionally translated texts, 105 more languages than any other VL benchmark. The tasks, images-to-sentence and sentences-to-images with prefix and postfix images in context, respectively, allows for fine-grained analysis of VL interactions. We also define corresponding text-only tasks by replacing the images with the topic label to compare the VL support (by language) of LVLMs against the text-only support of their underlying LLMs. Both tasks allow to vary the number of included images to analyze the shift from single to multi-image support in LVLMs. 2) We thoroughly evaluate LVLMs on cross-modal image-text topic matching, finding that task performance is closely associated with both model size and the size of pre-training corpora of the respective languages. We further find that only GPT-4o-mini seizes on multiple references in both cross-modal tasks. Open-weights LVLMs, moreover, favor one of the two tasks, highlighting the asymmetry in their VL support. 3) We analyze the relationship between stand-alone text and visionlanguage support in LVLMs by also benchmarking LVLMs on text-only topic matching. The performance gap between matching sentences to reference images or the topic tends to be larger the better the LVLM supports the underlying language. Conversely, the spread in performance between picking the fitting image or topic for reference sentences increases the worse the vision-language support of the LVLM for the evaluated language is. 4) We correlate MVL-SIB with established multilingual VL benchmarks on the languages shared between the respective pairs of datasets, showing that the MVL-SIB tasks align well with all VL tasks except OCR. We further show that images-to-sentence and sentence-to-image probe distinct aspects of VL interaction, as certain benchmarks correlate more strongly with one task than with the other. This analysis shows that MVL-SIB constitutes reliable and comprehensive VL benchmark for the lowerresource languages that are not covered by other datasets."
        },
        {
            "title": "2 Related Work",
            "content": "Multilingual Vision-Language Models. Researchers have extended more English-centric LVLMs like BLIP-2 or LLaVA by continuing to train on multilingual data. Googles PaLI models were the earliest closed-weight models trained on multilingual captions and VQA data; their openweight PaliGemma followed comparable strategy. Meanwhile, modern LLMs (e.g., Qwen 2.5, Llama 3, Gemma 2, Aya) have improved in multi2 lingual tasks but frequently fail to respond consistently in non-English languages, particularly in lowresource settings (Schneider and Sitaram, 2024). However, foundational models tend to focus on higher-resource languages and do not fully account for broader linguistic contexts in vision-language tasks. mBLIP is the first open multilingual LVLM trained on image captions and small set of translated instruct data in 98 languages (Geigle et al., 2024a). Pangea incorporates multicultural dimensions by blending machine-translated data, existing multilingual resources, and synthetic data, on 39 languages (Yue et al., 2024). Most recently, Geigle et al. (2025) studied the composition of training data for multilingual adaptation of LVLMs, observing that only 25-50% of the data needs to be in English. The authors apply their findings when training Centurio, which achieves state-of-the-art performance on 14 multilingual VL benchmarks. Multilingual Vision-Language Benchmarks. Existing datasets span VQA, natural language inference (NLI), image captioning, outlier detection, and culturally grounded QA: VQA. xGQA extends the questions of the GQA dataset into 8 languages (5 scripts), but the answers in English (Pfeiffer et al., 2022). MaXM offers short-form QA fully in 7 languages, pairing culturally aligned images with same-language QA pairs (Changpinyo et al., 2023). MTVQA focuses on text-heavy VQA in 9 languages (Tang et al., 2024). Culturally-grounded VQA. CVQA collects culturally diverse images and queries in 31 languages with multiple country-specific variants (Romero et al., 2024). The concurrent ALM-Bench covers 100 languages via MT with GPT-4o followed by human post editing with both generic and cultural multiple-choice and true or false questions, as well as free-form VQA (Vayani et al., 2024). Visual Reasoning & NLI. XVNLI evaluates crosslingual visual NLI on 5 languages (Bugliarello et al., 2022). Binary reasoning tasks include MaRVL (Liu et al., 2021) and M5B-VGR (Schneider and Sitaram, 2024), each using linguistically specific images and textual statements. M5BVLOD presents an outlier detection challenge, where statement holds true for all but one image (Schneider and Sitaram, 2024). Multiple-Choice QA. Babel-ImageNet (Geigle et al., 2024c) translates ImageNet labels into nearly 300 languages for multiple-choice object classification. M3Exam and xMMMU also feature multiplechoice VQA in 9 and 7 languages, respectively. MVL-SIB fills the gaps in existing multilingual VL benchmarks. It provides test data professionally translated to 205 languages, covering over 105 more languages than other benchmarks for which MT cannot synthesize reliable data for. Other VL datasets that eschew MT, such as culturallygrounded VQA benchmarks, typically construct more language-specific non-parallel data, that does not support comparative evaluation across languages. The cross-modal topic matching tasks can be also framed text-only (cf. 3.2), replacing the images that represent topics with the explicit topic labels. Thereby, MVL-SIB enables to ablate the vision-language support from the textual support for language. Finally, the benchmark allows to vary the number of images provided LVLMs to analyze the support for multi-image reasoning."
        },
        {
            "title": "3.1 Dataset",
            "content": "For MVL-SIB, we extend the following Floresbased datasets to create massively multilingual, multi-way parallel VL benchmark for identifying topical associations between images and sentences. Flores. Flores is machine translation benchmark containing 3,001 sentences from English Wikipedia paragraphs (Team et al., 2022), professionally translated into over 200 languages.1 SIB-200. Adelani et al. (2024) grouped the coarse topical annotations for sentences in the DEV and DEVTEST subsets of Flores into 7 higher-level topics.2 The resulting SIB-200 dataset is benchmark for topical classification with 1,004 parallel examples for 205 language variants. MVL-SIB. For each topic, we first manually collect 10 permissively licensed images that distinctly represent the topic (e.g., sports) with minimal overlap or ambiguity (cf. Appendix A.1). We verify that all LVLMs in our study correctly classify the topics for the images when prompted (cf. Appendix A.7.1). We next create 3 different MVLSIB instances from each of the 1,004 SIB sentences, totaling 3,012 MVL-SIB instances. For each MVLSIB instance, we couple the respective SIB sentence with (1) random selection of 5 positive im1Flores splits 3,001 sentences into DEV (997), DEVTEST (1,012), and TEST (992) sets. The TEST set was not released. 2The topics are entertainment, geography, health, politics, science/technology, sports, and travel. 3 ages (same topic) and 4 additional sentences from the same category as the original sentence, as well as (2) 3 negative images and sentences randomly sampled from different topics compared to the starting SIB sentence. The set of sampled sentences and images by instance is maintained across languages."
        },
        {
            "title": "3.2 Cross-modal & Text-only Topic Matching",
            "content": "We formulate both cross-modal and text-only topic matching tasks based on MVL-SIB. In every task, we present the model with the list of topics that images and sentences may be associated with.2 Otherwise, it would be unclear along which dimension the model should match images and sentences. The portion of the prompt that introduces the task is provided in English, while the sentences to be topically aligned with images are presented in one of the 205 languages included in MVL-SIB. LLMs reliably perform tasks described in English, even when task-related information is conveyed in other languages (Muennighoff et al., 2022; Romanou et al., 2024). This ensures fair comparison across all 205 languages, where MT would not accurately preserve the meaning of the prompts. We detail prompts for each task in Appendix A.2. Cross-modal Topic Matching. Using our textimage samples (cf. 3.1), we define two crossmodal topic matching tasks: Images-To-Sentence (I2S) and Sentences-To-Image (S2I). In I2S, the model must select, from 4 candidates, the sentence that matches the topic of reference images. Conversely, in S2I, the model chooses, from 4 options, the image that shares the topic with reference sentences. In both tasks, we present the model with {1, 3, 5} references, respectively. These tasks evaluate the models ability to align high-level visual and textual cues on topics. Text-only Topic Matching. We construct two tasks by replacing the images in I2S and S2I, that represent the topics, with their corresponding labels (e.g., sports). The resulting unimodal tasks, Topic-ToSentence (T2S) and Sentences-To-Topic (S2T), mirror the cross-modal tasks, I2S and S2I, respectively. For T2S, we evaluate only k=1, since repeating the topic label adds no information. These baseline tasks allow us to delineate between language support and vision-language understanding in LVLMs. MVL-SIB offers 4 crucial advantages over prior benchmarks. 1) It supports evaluation in 205 languages, covering over 100 more languages than existing benchmarks for which MT models fail to synthesize reliable evaluation data. 2) MVLSIB supports ablating language understanding and multimodal reasoning of LVLMs by comparatively evaluating the mirroring text-only and cross-modal topic matching tasks (cf. 3.2). 3) MVL-SIB enables intricate analysis of singleand multi-image VL interactions in LVLMs by allowing topics to be represented by varying numbers of images in cross-modal tasks. 4) MVL-SIB comprises higherlevel VL reasoning tasks, pairing varied images and diverse texts to test nuanced VL understanding."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Models. We test state-of-the-art LVLMs Qwen2VL (Wang et al., 2024), InternVL 2.5 (Chen et al., 2024), Centurio-Qwen (Geigle et al., 2025), and GPT-4o(-mini) across available sizes.3 Smaller LVLMs are evaluated on all languages, while larger ones (26B+) are tested on subsets (cf. 5.3). For cross-modal topic matching, we also evaluate on mSigLIP-base (Zhai et al., 2023). Trained explicitly for semantic similarity on multilingual imagecaption pairs, the ViT represents strong baseline.4 Its prediction denotes the choice that has the highest average cosine similarity to the references. Image preprocessing. We downsample the images to 640480 pixels, as the tasks rely on higher-level visual cues for topically associating images and texts rather than finer image details. This can significantly reduce the number of visual tokens input to LVLMs, enabling more efficient inference. Hyperparameters. We decode text greedily with temperature set to 0.0 to ensure reproducibility. Metric. We compute the share of prompts for which responses begin with the right letter. If the label is \"A\", response such as \"A.\" is also correct."
        },
        {
            "title": "5 Results and Discussion",
            "content": "We categorize the languages in MVL-SIB based on their resourceness. To do so, we reorganize the language groups from Joshi et al. (2020) into four tiers. We first rank the tiers w.r.t. Wikipedia size and then merge (i) the two highest-resource tiers and (ii) the two lowest-resource tiers.5 This both better reflects current corpus availability for LLM 3We provide details on the LVLMs in Appendix A.3. 4The model is available on Huggingface at: google/siglipbase-patch16-256-multilingual. 5Sorting by Wikipedia size (in number of pages) swaps tiers 1 and 2; we then merge Tier 0 with the new Tier 1, as well as tiers 4 and 5."
        },
        {
            "title": "Resourceness",
            "content": "High"
        },
        {
            "title": "Mid",
            "content": "Low"
        },
        {
            "title": "ENGLISH",
            "content": "TIER 4 (26) TIER 3 (32) TIER 2 (96) TIER 1 (51) References 3 5 1 3 5 3 5 1 3 5 3 5 Images-To-Sentence: Select 1 of 4 sentences topically matching reference images mSigLIP-base Qwen2-VL 2B Qwen2-VL 7B InternVL 2.5 4B InternVL 2.5 8B Centurio Qwen GPT-4o-mini 57.7 36.3 65.8 52.5 67.7 54.8 68.3 64.6 34.8 63.1 49.2 67.9 60.0 78. 66.4 34.9 58.9 48.1 68.7 62.4 77.4 53.3 35.5 57.7 50.3 64.6 54.2 71.6 58.6 35.3 56.5 46.6 64.9 59.2 79.0 59.7 34.1 51.7 47.7 65.7 60.6 78.1 51.4 34.5 55.4 48.6 61.2 53.4 72.0 56.2 34.3 54.5 45.3 60.8 58.1 78. 57.1 33.2 49.6 46.1 61.6 58.9 77.7 Sentences-To-Image: Select 1 of 4 images topically matching reference sentences mSigLIP-base Qwen2-VL 2B Qwen2-VL 7B InternVL 2.5 4B InternVL 2.5 8B Centurio Qwen GPT-4o-mini 56.3 41.9 71.7 47.7 66.2 35.3 77.5 66.0 43.1 70.4 44.5 69.0 36.1 86.4 69.6 43.4 68.6 43.0 68.7 35.6 89. 51.8 41.6 65.5 38.0 57.5 31.1 77.2 61.6 42.5 65.5 40.3 62.5 32.9 86.5 64.0 42.7 63.5 40.4 61.6 33.3 88.6 49.1 40.8 64.4 36.7 52.9 31.0 77.1 58.3 42.4 65.3 39.6 58.5 32.8 86.1 60.2 42.4 64.1 40.3 58.1 33.1 88. 38.9 31.0 44.3 38.7 51.0 46.6 63.5 36.0 33.7 50.3 30.7 43.4 28.7 68.4 41.2 30.6 44.4 37.1 51.4 48.9 68.0 40.4 35.6 52.5 34.4 49.8 29.7 79.8 41.7 30.0 40.5 37.3 51.8 49.2 66.4 41.2 35.5 52.9 35.7 49.7 29.8 82. 36.1 29.5 39.6 35.4 46.1 43.0 56.9 32.9 31.0 43.5 28.8 39.7 28.1 61.7 37.6 29.0 39.7 34.5 46.0 44.2 60.3 36.3 32.7 45.9 32.1 45.9 28.7 74.0 38.0 28.6 36.5 34.5 46.3 44.7 58.7 36.9 32.8 46.6 33.7 46.2 28.7 77. Table 1: Cross-modal Topic Matching: LVLMs must select the candidate sentence (image) from 4 choices that topically align with reference images (sentences). Prompts provided in A.2. Languages are tiered by Wikipedia sizes (cf. 5). Number of languages in parentheses. Metric: share of responses starting with correct option letter. Details in 4. In each column, the best model is emphasized in bold, the second-best model is underlined."
        },
        {
            "title": "Resourceness",
            "content": "High"
        },
        {
            "title": "Mid",
            "content": "Low"
        },
        {
            "title": "Task",
            "content": "k References Qwen2-VL 2B Qwen2-VL 7B InternVL 2.5 4B InternVL 2.5 8B Centurio Qwen GPT-4o-mini"
        },
        {
            "title": "ENGLISH",
            "content": "TIER 4 (26) TIER 3 (32) TIER 2 (96) TIER 1 (51) Topic-To Sentence Sentences To-Topic Topic-To Sentence Sentences To-Topic Topic-To Sentence Sentences To-Topic Topic-To Sentence Sentences To-Topic Topic-To Sentence Sentences To-Topic 1 56.7 85.7 81.4 87.0 85.4 88.5 1 5 86.1 96.5 98.2 89.1 95.3 97.5 90.6 98.0 99.1 91.7 98.1 99.0 89.9 96.7 97.7 92.4 98.1 99.1 1 49.1 81.8 72.7 83.0 83.6 89.3 1 5 78.4 92.0 95.2 84.1 93.3 96.1 85.2 95.8 97.9 86.3 96.3 98.3 88.0 95.8 97.7 91.6 98.4 99.3 1 45.2 80.5 68.2 79.1 82.6 89.3 1 5 73.7 89.6 93.6 84.1 93.5 96.6 83.7 95.5 97.7 82.4 94.1 96.7 87.7 96.0 97.9 91.6 98.4 99.3 1 36.4 63.7 50.2 65.4 70.4 80.9 1 5 53.7 69.3 76.4 67.8 81.3 86.7 67.8 83.4 87.9 66.8 81.8 86.9 73.1 86.8 90.7 82.1 93.3 95.7 1 33.0 55.3 44.6 58.1 64.0 73.9 1 5 46.4 60.7 68.4 59.1 73.7 79.8 60.7 77.4 83.1 58.3 74.6 80.9 66.1 81.2 85.7 74.3 88.2 92.1 Table 2: Text-only Topic Matching: LVLMs must select the candidate sentence (topic) of 4 choices that aligns topically with the reference topics (k reference sentences). See Table 1 for further details. pre-training (Xue et al., 2021; Kudugunta et al., 2023), and aligns with downstream performance (cf. Appendix A.3.1). We isolate English from Tier 4, since it is the pivotal language in NLP. The full per-language results by task and model are provided in Appendix A.7."
        },
        {
            "title": "5.1 Cross-modal Topic Matching",
            "content": "Images-To-Sentence (I2S). The upper segment of Table 1 displays the results for I2S, in which the LVLMs must pick the candidate sentence that topically matches the reference images. English. The performance on I2S with English candidate sentences scales well with model size. The small Qwen2-VL 2B performs only slightly better than chance (25% vs. ca. 35%). The comparably sized Qwen2-VL 7B, InternVL 2.5 8B, and Centurio-Qwen 8B peak around 62% to 68% at various k. These models nevertheless non-negligibly trail GPT-4o-mini (78.1%). Among LVLMs, only InternVL 2.5 8B, Centurio-Qwen, and GPT-4omini benefit from multiple reference images. When the number of references increases from 3 to 5, GPT-4o-mini declines slightly in performance, while InternVL and Centurio-Qwen continue to improve marginally (ca. +1%) and more notably (ca. +3-6%), respectively. All other LVLMs deteriorate materially with more reference images (ca. -3-4%). mSigLIP indeed is strong baseline, trailing only GPT-4o-mini and InternVL 2.5 8B at k=5. The ViT yields large gains with 4 more images (+9.7%). Tiers. The performance gap of other languages to English correlates well with their resource levels by language tier. When presented with candidate sentences in non-English high-resource languages (cf. Tier 4), GPT-4o-mini even performs better slightly better. For very low-resource languages in Tier 1, such as NKoo or Tamazight, all models fail 5 to perform better than chance (cf. Appendix A.7.2). Among LVLMs, only GPT-4o-mini remains overall robust for topically matching sentences of lowresource languages to images, whereas other models drop severely in performance (ca. 15-20%). While mSigLIP still performs well, it declines more notably than LVLMs on lower-resource languages. Sentences-To-Image (S2I). The lower part of Table 1 presents the results for S2I. Here, the models select the candidate image among four options that topically fits the reference sentences. English. Performance again correlates well with model capacity. However, in S2I, only GPT4o-mini significantly seizes on additional references (+13%) to excel with 89%, while models like Qwen2-VL 7B and InternVL 2.5 8B exhibit peak performance at k=1 and k=3, respectively, that taper slightly with more sentence references. Centurio-Qwen performs only slightly better than random (25% vs. ca. 35%). In S2I, mSigLIP is again very strong, second only to GPT-4o-mini at k=5. The encoder once more seizes sizable gains from additional references (+13.3%). Tiers. In non-English evaluations, the overall trend remains similar, though absolute performance is lower. The gap between highand low-resource language tiers is evident, as all models yield higher scores across Tiers 4 and 3. GPT-4o-mini maintains robust performance even in the most challenging Tier 1 when provided multiple references (ca.75%). In sum, both the training protocol and the model size collectively determine whether models favor I2S or S2I. For instance, InternVL 2.5 8B outperforms Qwen2-VL 7B on I2S across the board, while trailing on S2I. Moreover, only GPT-4omini consistently seizes on additional references and remains largely robust to the lowest-resource languages on both tasks. This likely stems from insufficient training to enable open-weight LVLMs to perform higher-level VL reasoning with diverse texts and multiple images successfully. For instance, Centurio Qwen was mostly trained on data that prefixes images to text, resulting in low performance when images are postfixed to the context."
        },
        {
            "title": "5.2 Text-only Topic Matching",
            "content": "Table 2 lists the results for text-only topic matching, in which the images are exchanged with their topic label. These tasks denominate upper-bounds for their cross-modal counterparts to enable ablations of language support and VL support in LVLMs. Topic-to-Sentence (T2S). In this task, LVLMs choose the sentence that best suits the reference topic. Barring Qwen2-VL 2B, all models perform well on the task. Notably, 5 images should capture the underlying topic well for all models (cf. Appendix A.7.1). Despite that, the gap between textonly and vision-language tasks (cf. Table 1) is sizable across all open-weights models for English (ca. 20%+). In English, GPT-4o-mini and InternVL 2.5 8B achieve the highest accuracies, indicating strong topic comprehension. For non-English languages, while the overall scores are reduced, highresource languages benefit from richer training signals compared to their low-resource counterparts models like Qwen2-VL 2B and Centurio-Qwen 8B show more pronounced drop in the latter, underscoring the impact of language resources. Sentences-to-Topic (S2T). In S2T, where models choose the topic that best aligns with reference sentences, performance scales both with model size and the number of references. The gains from additional context (35 k) are particularly notable for larger LVLMs. In English, GPT-4o-mini improves markedly from 92.4% at k=1 to 98.1% at k=3. In non-English languages, similar patterns emerge: high-resource languages consistently yield higher accuracies than low-resource ones, with GPT-4omini and InternVL 2.5 8B exhibiting the most stable improvements across varying k. This reinforces the role of model capacity and training data diversity in effective cross-lingual topic matching. Comparing the results of cross-modal and textonly topic matching sheds further light on the VL interactions for lower-resource languages in LVLMs. The performance gap between matching sentences to reference images and matching them to textual topics tends to narrow regardless of the number of references, likely reflecting their limited textual support in LVLMs. In contrast, the discrepancy between selecting an image versus topic for reference sentences becomes much more pronounced, especially at k=5. These findings suggest that VL support degrades more sharply than textual support for lower-resource languages in LVLMs."
        },
        {
            "title": "5.3 Further Analyses",
            "content": "Task Correlation. To compare MVL-SIB with other tasks, we access the results for Qwen2-VL and InternVL 2.5 models on several established multilingual VL benchmarks from Geigle et al. 6 Figure 3: I2S with k=3. Figure 4: S2I with k=3. Correlations Between MVL-SIB & Multilingual VL Benchmarks. Pearson correlation coefficients obtained by regressing MVL-SIB performance against performance on multilingual VL tasks on languages common to both datasets, respectively. An asterisk (*) indicates whether the coefficient is statistically significant at 0.05. (2025).6 Next, we align the results across languages for the MVL-SIB tasks and the other benchmarks. Finally, we plot the linear regressions of performance on I2S and S2I with k=3 against the performance on the VL benchmarks, respectively, pooled over models, in Figures 3 and 4. MVL-SIB positively correlates with all tasks in both the I2S and S2I evaluations. However, both the magnitude and statistical significance of these linear relationships vary across VL benchmarks. Both I2S and S2I exhibit the strongest connections with XVNLI (visual inference), BIN-MC (multiple-choice image classification), MaRVL, and M5-VGR (both visually-grounded boolean reasoning). Since these tasks restrict valid answers to small set of fixed options (i.e., choice letters or yes/no/maybe), LVLMs must engage in higher-level vision-language disambiguation rather than relying solely on lower-level visual cues to solve these tasks. In addition, xMMMU, M3Exam, and M5-VLOD are significantly related only to I2S, whereas xGQA is significantly aligned solely with S2I. The former tasks are structurally similar to I2S, typically presenting one or more images that LVLMs are given as visual context to answer multiple-choice questions. We hypothesize that only S2I is significantly correlated with xGQA, since S2I is more analogous to object-centric benchmarks. In S2I, LVLMs likely leverage targeted semantic cues (e.g., keywords or phrases) from the 6We omit Centurio-Qwen, since it degrades on the S2I task. We further provide details on all the multilingual visionlanguage benchmarks we correlate MVL-SIB with in Appendix A.4 7Note that we include constant in our regression model reference sentences to better disambiguate candidate images by topic. This behavior aligns with lower-level VL tasks such as xGQA (cross-lingual short-form QA) or BIN-MC (multiple-choice object classification), where texts and images are more deliberately connected. In contrast, MTVQA, SMPQA-name, and SMPQA-ground show only weak or statistically insignificant correlations with the MVL-SIB tasks. Since these tasks require LVLMs to comprehend text embedded in images, low-level, fine-grained VL task, they differ substantially from the higher-level VL reasoning evaluated by MVL-SIB. Overall, the regression analysis indicates that I2S and S2I capture distinct yet complementary aspects of VL understanding, collectively exhibiting strong relationship with broad set of VL tasks. This aligns with our main results (cf. Table 1), which show that different LVLMs may favor one MVL-SIB task over the other. This renders MVLSIB as suitable benchmark for evaluating universal VL understanding (across 205 languages). It also enables to ablate performance across the key axes of analysis, the task formulation (I2S vs. S2I), the language vs. vision-language support for 205 languages, and the number of images in context.8 Larger LVLMs. To evaluate larger LVLMs on MVL-SIB, we construct language-tier subsets that reliably estimate performance while mitigating excessive computational overhead (cf. 5). For both I2S and S2I, we identify the three languages in each tier that best replicate the average performance of 8In S2I, candidates could comprise more than single to bridge task-specific scales of results. image. 7 Figure 5: Larger LVLMs on Subsampled Tiers. We extract 3 languages per tier that mimic avg. performance full language groups (cf. 5.3) and evaluate LVLMs across all model sizes on {I2S,S2I,T2S,S2T } @ k=3 (cf. 3.2). the tier. First, we compute the average performance per language tier for both I2S and S2I with k=3, pooling results across models (cf. 5). Then, for each tier, we select the three languages whose performance deviates least from the tier mean. The languages chosen for each tier by task are detailed in Appendix A.6. Finally, we test GPT-4o, InternVL 2.5 {26,32,72}B, and Qwen 2.5 VL {32,70}B on these subsets to assess how well larger models perform across language tiers. Figure 5 displays the results for both I2S and S2I with k=3.9 We observe that larger models catch up to GPT-4o on I2S and even outperform it on S2I for higher-resource languages. Since openweight LVLMs have more limited language support for low-resource languages, GPT-4 and GPT-4omini outperform all other models on I2S for languages in tier 1 and 2. Increasing model capacity yields the largest gains on S2I, for which the models exceed GPT-4o up to the lowest-resource Tier 1. Moreover, unlike smaller models, all larger LVLMs (+26B) more effectively leverage multiple image references to improve performance (cf. Appendix A.6). They reap the largest benefit when the number of references increases from 1 to 3 (ca. +3%), with further improvements at = 5 (ca. +1.5%). These results suggest that larger LVLMs have fundamentally better VL support irrespective of the evaluated language (cf. 5.2). The results on textonly topic matching further underscore this notion (cf. lower segment of Figure 5). Larger LVLMs near perfectly match both the reference topic to the correct sentence (ca. 90%) and references sentences to the right topic (ca. 98%). Notably, as model size increases, the performance gap between 9Qwen2-VL 72B frequently responded with the first letter of the correct topic. If that letter uniquely identifies the correct choice, the answer is considered correct. cross-modal and text-only matching narrows. Collectively, these results further indicate that VL support relative to text-only support improves with increasing model capacity in LVLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "We present the massively multilingual visionlanguage benchmark MVL-SIB for cross-modal (and text-only topic matching) in 205 languages that offers key advantages over prior multilingual VL benchmarks. Notably, it covers over 100 additional languages without relying on machine translation. MVL-SIB allows for clear separation between textual language support and visionlanguage support in LVLMs by comparing performance on mirrored cross-modal and text-only tasks. Moreover, it allows us to study how LVLMs handle single-image versus multi-image formulations of cross-modal topic matching by varying the number of images provided. In our comparative evaluation of state-of-the-art LVLMs on MVL-SIB, we find that model performance is strongly correlated with both model size and the volume of available pre-training data for each language. However, all LVLMs experience dramatic performance drop on the lowest-resource languages. Our analysis further reveals that vision-language support deteriorates disproportionately relative to language support, highlighting the need to incorporate lowresource languages into VL training. Moreover, providing multiple images does not benefit openweight LVLMs in cross-modal topic matching, suggesting that LVLMs are not yet fully effective in multi-image tasks. Lastly, we validate that MVLSIB correlates well with existing multilingual VL benchmarks, underscoring its reliability as source of evaluation data for 205 languages."
        },
        {
            "title": "References",
            "content": "Our work faces three primary limitations. First, although vast number of LVLMs exist, we selected representative subset based on key criteria. Specifically, the LVLMs in our study (Qwen2VL and InternVL, with the exception of Centurio) span range of parameter counts typical of LLMs. Additionally, we include GPT-4o-mini in the full evaluation and GPT-4o on the subsampled language tiers. Evaluating MVL-SIB across all four tasks I2S, S2I, T2S, and S2T (cf. 3.2) at various {1, 3, 5} over 205 languages (i.e., evaluations per model and task, or 2050, in sum per model) becomes computationally intractable. This accumulates to 3 205 = 615 evaluations per model (205 for T2S as only k=1 reference topic exists) or 3 205 + 205 = 2050 evaluations in total. We therefore both provide subsets of the language tiers to evaluate on and demonstrate that evaluation only requires 1K instances to reliably estimate task performance. Second, while we strove to choose diverse set of images to capture the full semantic range of each topic, further diversification is possible by sourcing additional images. However, due to the limited availability of openly licensed images, some topics (e.g., politics and entertainment) are represented predominantly by images that embody the topic in more Western-centric cultural context. Hand-selecting images by topic for each language or, more broadly, cultural groups would not scale to 205 languages and would hinder the comparability of results. Our results furthermore confirm that models just as well perform on broad range of languages spanning diverse cultural backgrounds as on English (cf. Figure 2). At the same time, LVLMs perform best on Western-centric images, mitigating any variation that would originate from using more culture-specific images. Finally, for the topic geography, we manually selected images that are representative within the context of SIB, as the broader definition of geography is too diffuse to capture visually."
        },
        {
            "title": "Acknowledgements",
            "content": "We used AI assistance (chatGPT o3-mini) to polish the writing and the tables of the manuscript as well as to refine the code for our visualizations. David Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, and En-Shiun Lee. 2024. SIB-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 226245, St. Julians, Malta. Association for Computational Linguistics. Željko Agic and Natalie Schluter. 2018. Baselines and Test Data for Cross-Lingual Inference. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. large annotated corpus for learning natural language inferIn Proceedings of the 2015 Conference on ence. Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 632642. The Association for Computational Linguistics. Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, and Ivan Vulic. 2022. IGLUE: benchmark for transfer learning across modalities, tasks, and languages. ArXiv, abs/2201.11732. Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, and Radu Soricut. 2023. MaXM: Towards multilingual visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 26672682, Singapore. Association for Computational Linguistics. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. 2024. Internvl: Scaling up vision foundation models and aligning for In Proceedings of generic visual-linguistic tasks. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198. J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li FeiFei. 2009. ImageNet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255. William Gaviria Rojas, Sudnya Diamos, Keertan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2022. The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World. Advances in Neural Information Processing Systems, 35:1297912990. Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavaš. 2024a. mBLIP: Efficient bootstrapping of In Proceedings of the multilingual vision-LLMs. 3rd Workshop on Advances in Language and Vision Research (ALVR), pages 725, Bangkok, Thailand. Association for Computational Linguistics. 9 Gregor Geigle, Florian Schneider, Carolin Holtermann, Chris Biemann, Radu Timofte, Anne Lauscher, and Goran Glavaš. 2025. Centurio: On drivers of multilingual ability of large vision-language model. Gregor Geigle, Radu Timofte, and Goran Glavaš. 2024b. African or European swallow? benchmarking large vision-language models for fine-grained object classification. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 26532669, Miami, Florida, USA. Association for Computational Linguistics. Gregor Geigle, Radu Timofte, and Goran Glavaš. 2024c. Babel-ImageNet: Massively multilingual evaluation of vision-and-language representations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50645084, Bangkok, Thailand. Association for Computational Linguistics. Drew A. Hudson and Christopher D. Manning. 2019. GQA: New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 67006709. Computer Vision Foundation / IEEE. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online. Association for Computational Linguistics. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Int. J. Crowdsourced Dense Image Annotations. Comput. Vision, 123(1):3273. Place: USA Publisher: Kluwer Academic Publishers. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. Madlad-400: multilingual and document-level large audited dataset. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021. Visually grounded reasoning across lanIn Proceedings of the 2021 guages and cultures. Conference on Empirical Methods in Natural Language Processing, pages 1046710485, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanMartin O. Steitz, Stefan Roth, Ivan Vulic, and Iryna Gurevych. 2022. xGQA: Cross-lingual visual question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2497 2511, Dublin, Ireland. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, and Antoine Bosselut. 2024. Include: Evaluating multilingual language understanding with regional knowledge. David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando DHaro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago 10 Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula Mónica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Teresa Clifford, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, and Alham Fikri Aji. 2024. Cvqa: Culturally-diverse multilingual visual question answering benchmark. Florian Schneider and Sunayana Sitaram. 2024. M5 diverse benchmark to assess the performance of large multimodal models across multilingual and multicultural vision-language tasks. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 43094345, Miami, Florida, USA. Association for Computational Linguistics. Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, and Han-Jia Ye. 2024. Parrot: Multilingual visual instruction tuning. Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, and Can Huang. 2024. Mtvqa: Benchmarking multilingual text-centric visual question answering. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling humancentered machine translation. Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: Massively Multilingual Multimodal Evaluation Dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 715729, Abu Dhabi, United Arab Emirates. Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Mihaylov, Chao Qin, Abdelrahman Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Amirudin, Muhammad Ridzuan, Daniya Kareem, Ketan More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan ObandoCeron, Olympiah Otieno, Fabian Farestam, Muztoba Rabbani, Sanoojan Baliah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Xavier, Amit Bhatkal, Hawau Toyin, Aman Chadha, Hisham Cholakkal, Rao Muhammad Anwer, Michael Felsberg, Jorma Laaksonen, Thamar Solorio, Monojit Choudhury, Ivan Laptev, Mubarak Shah, Salman Khan, and Fahad Khan. 2024. All languages matter: Evaluating lmms on culturally diverse 100 languages. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2vl: Enhancing vision-language models perception of the world at any resolution. Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2019. Visual Entailment: Novel Task for Fine-grained Image Understanding. arXiv preprint arXiv:1901.06706. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: massively multilingual pre-trained text-to-text transformer. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: Massive Multi-discipline Multimodal 11 Understanding and Reasoning Benchmark for Expert AGI. CoRR, abs/2311.16502. ArXiv: 2311.16502. Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. 2024. Pangea: fully open multilingual multimodal llm for 39 languages. arXiv preprint arXiv:2410.16153. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. 2023. M3Exam: Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. Advances in Neural Information Processing Systems, 36:5484 5505."
        },
        {
            "title": "A Appendix",
            "content": "12 A."
        },
        {
            "title": "Politics",
            "content": "Science & Technology"
        },
        {
            "title": "Travel",
            "content": "13 A.2 Prompts Images-To-Sentences (I2S) Which sentence best matches the topic of the images? The images and the sentences each belong to one of the following topics: \"entertainment\", \"geography\", \"health\", \"politics\", \"science and technology\", \"sports\", or \"travel\". Choose one sentence from A, B, C, or D. Output only single letter! # Images # Sentences A. ```Maroochydore führte am Ende die Rangfolge an, mit sechs Punkten Vorsprung vor Noosa als Zweitem.``` B. ```Es wurden keine schwere Verletzungen gemeldet, jedoch mussten mindestens fünf der zur Zeit der Explosion Anwesenden aufgrund von Schocksymptomen behandelt werden.``` C. ```Finnland ist ein großartiges Reiseziel für Bootstouren. Das Land der tausend Seen hat auch Tausende von Inseln in den Seen und in den Küstenarchipelen.``` D. ```Es ist auch nicht erforderlich, dass Sie eine lokale Nummer von der Gemeinde erhalten, in der Sie leben. Sie können eine Internetverbindung über Satellit in der Wildnis on Chicken in Alaska erhalten und eine Nummer auswählen, die vorgibt, dass Sie im sonnigen Arizona sind.``` Your answer letter: 14 Sentences-To-Images (S2I) Which image best matches the topic of the sentences? The sentences and the images each belong to one of the following topics: \"entertainment\", \"geography\", \"health\", \"politics\", \"science and technology\", \"sports\", or \"travel\". Choose one image from A, B, C, or D. Output only single letter! # Sentences ```Maroochydore führte am Ende die Rangfolge an, mit sechs Punkten Vorsprung vor Noosa als Zweitem.``` ```Die Schlagmänner der mittleren Reihe, Sachin Tendulkar und Rahul Dravid, zeigten gute Leistungen und erzielten eine Partnerschaft mit 100 Runs.``` ```Da pro Tag nur achtzehn Medaillen zur Verfügung stehen, hat es ein Anzahl an Ländern nicht auf das Podium geschafft.``` ```Wintersportarten sind in den nördlichen Regionen am beliebtesten und Italiener nehmen an internationalen Wettkämpfen und olympischen Spielen teil.``` ```Nach dem Rennen bleibt Keselowski mit 2.250 Punkten Spitzenreiter in der Fahrerwertung. # Images B. A. C. D. Your answer letter: Topic-To-Sentence (T2S) Which sentence best matches the topic \"sports\"? The sentences each belong to one of the following topics: \"entertainment\", \"geography\", \"health\", \"politics\", \"science and technology\", \"sports\", or \"travel\". Choose one sentence from A, B, C, or D. Output only single letter! # Sentences A. ```Maroochydore führte am Ende die Rangfolge an, mit sechs Punkten Vorsprung vor Noosa als Zweitem.``` B. ```Es wurden keine schwere Verletzungen gemeldet, jedoch mussten mindestens fünf der zur Zeit der Explosion Anwesenden aufgrund von Schocksymptomen behandelt werden.``` C. ```Finnland ist ein großartiges Reiseziel für Bootstouren. Das Land der tausend Seen hat auch Tausende von Inseln in den Seen und in den Küstenarchipelen.``` D. ```Es ist auch nicht erforderlich, dass Sie eine lokale Nummer von der Gemeinde erhalten, in der Sie leben. Sie können eine Internetverbindung über Satellit in der Wildnis on Chicken in Alaska erhalten und eine Nummer auswählen, die vorgibt, dass Sie im sonnigen Arizona sind.``` Your answer letter: 16 Sentences-To-Topics (S2T) Which topic best matches the sentences? The sentences belong to one of the following topics: \"entertainment\", \"geography\", \"health\", \"politics\", \"science and technology\", \"sports\", or \"travel\". Choose one topic from A, B, C, or D. Output only single letter! # Sentences ```Maroochydore führte am Ende die Rangfolge an, mit sechs Punkten Vorsprung vor Noosa als Zweitem.``` ```Die Schlagmänner der mittleren Reihe, Sachin Tendulkar und Rahul Dravid, zeigten gute Leistungen und erzielten eine Partnerschaft mit 100 Runs. ```Da pro Tag nur achtzehn Medaillen zur Verfügung stehen, hat es ein Anzahl an Ländern nicht auf das Podium geschafft.``` ```Wintersportarten sind in den nördlichen Regionen am beliebtesten und Italiener nehmen an internationalen Wettkämpfen und olympischen Spielen teil.``` ```Nach dem Rennen bleibt Keselowski mit 2.250 Punkten Spitzenreiter in der Fahrerwertung.``` # Topics A. sports B. health C. travel D. science and technology Your answer letter: 17 A.3 Further Details Models. We test state-of-the-art instruction finetuned LVLMs across various sizes. Smaller models are evaluated on all languages, while larger LVLMs (26B+) are tested on subsets of the MVL-SIB languages (cf. 5.3). GPT-4o. We evaluate MVL-SIB on GPT-4o-mini2024-07-18 and GPT-4o-2024-08-06. We set the image detail in the API to low, since our tasks require high-level reasoning that does not depend on finer image details.10 Prior works show that GPT-4o is the best-performing multilingual LVLM (Schneider and Sitaram, 2024; Vayani et al., 2024). Qwen2-VL. Qwen2-VL ties 675M parameter vision-transformer (ViT) into Qwen2 LLMs (Wang et al., 2024). An MLP compresses adjacent 22 visual tokens embedded by the ViT into one token representation, which is then input to the LLM. InternVL 2.5. Depending on the model size, InternVL uses Qwen2.5 or InternLM as its LLM backbone (Chen et al., 2024). The model embeds images either by 6B or by distilled 300M ViT pretrained with CLIP (Radford et al., 2021). The resulting image patch encodings are downsampled by factor 4 and fed through an MLP to the LLM. Centurio. Centurio is the latest massively multilingual LVLM trained on 100 languages (Geigle et al., 2025), outperforming alternatives like Parrot (Sun et al., 2024) or Pangea (Yue et al., 2024). It employs Qwen2.5 as its LLM (Yang et al., 2024) and SigLIP SO400/384 as its ViT (Zhai et al., 2023). The model mixes resolutions by stacking the encodings of the full image and those of 22 tiles along the features. The combined embedding is then projected via an MLP to the LLMs input space. Besides architectures, the LVLMs crucially differ in dataset mixtures on which they were trained. Centurio translates image-caption, VQA, OCR, and few multi-image datasets to 100 languages with NLLB (Team et al., 2022) to mix 50:50 with the original English data. Qwen2-VL and InternVL, however, were trained on much larger, more diverse datasets that comprise sizable multi-image comparison and video understanding datasets. Moreover, assuming that the LLMs of Qwen2-VL, InternVL, and GPT-4o were pretrained on Flores, their performance would be overly optimistic. 10We use GPT-4o-mini because evaluating GPT-4o would be too expensive. 18 A.3.1 Performance by Model over Languages grouped by Language Tier Figure 6: Images-To-Sentences @ k=3. The English prompt describes the cross-modal topic matching task, lists all topics, and provides both k=3 reference images and 4 sentences in the corresponding language {eng_Latn, . . . , nqo_Nkoo}. LVLMs must select the sentence of 4 options that topically fits k=3 reference images. The sentences spanning 205 languages and 7 topics are drawn from SIB-200 (Adelani et al., 2024), while images for the topics were hand-selected (cf. Appendix A.1). An example prompt is shown in Appendix A.7.2; further details are in 4. Plot. The x-axis orders the languages of the candidate sentences {eng_Latn, . . . , nqo_Nkoo}, respectively, by descending performance (y-axis). The top x-axis indicates the running index of each language Li (i {1, . . . , 205}). Tiers. The languages are grouped by tiers derived from Joshi et al. (2020) (cf. 5). A.3.2 Calibration Analysis of Cross-Modal Topic Matching Figure 7: Calibration Analysis of Cross-Modal Topic Matching for InternVL 2.5 8B. Analysis: To assess the reliability of cross-modal topic matching with fewer samples than our full dataset (1004 samples per language), we randomly select 500 trajectories. We then compute performance metrics on cumulative subsets, incrementing by 50 examples at each step. The difference in performance between the full dataset and each subset is calculated to quantify the deviation at each sample size. Plot: The plot displays the average absolute spread in performance (averaged over all languages) along with the standard deviation for InternVL 2.5 8B. We restrict ourselves to single open-weight LVLM, since the analysis yields identical results across all combinations of LVLMs and language tiers. Insights: Our analysis shows that performance stabilizes rapidly, with deviations of only about 1% observed at 1,004 instances same size as the subset from which the dataset was created. This indicates that reliable evaluation of cross-modal topic matching can be achieved with far fewer than 3,012 samples. 19 A.4 Overview of Multilingual Vision-Language Benchmarks xGQA. The xGQA dataset (Pfeiffer et al., 2022) is cross-lingual visual question-answering resource. It extends the well-known English-only GQA dataset (Hudson and Manning, 2019) by providing manual translations of the questions in the balanced test-dev set. The dataset contains 9666 questions available in eight languages across five scripts, while the answers remain in English. In addition, it features 300 unique images from Visual Genome (Krishna et al., 2017). MaXM. MaXM, introduced by Changpinyo et al. (2023), is VQA dataset covering seven languages written in five scripts. In this dataset, both the questions and their corresponding answers are presented in the same language. The images are drawn from subset of the XM3600 (Thapliyal et al., 2022) dataset and are selected to correspond to regions where the question-answer pairs language is spoken, ensuring both linguistic and cultural diversity. XVNLI. The XVNLI dataset (Bugliarello et al., 2022) introduces the task of Cross-lingual Visual Natural Language Inference, where model must determine if textual hypothesis entails, contradicts, or is neutral with respect to visual premise. This dataset spans five languages across three scripts and includes 357 unique images from Visual Genome. It is built upon combination of the text-only SNLI (Bowman et al., 2015) dataset and its cross-lingual (Agic and Schluter, 2018) and cross-modal (Xie et al., 2019) counterparts. MaRVL. The MaRVL dataset (Liu et al., 2021) is designed to benchmark models on Multicultural Reasoning over Vision and Language. Each sample consists of two images, textual statement, and binary (true/false) answer grounded in the images. Covering five languages across three scripts, MaRVL includes 4914 culturally diverse images that align with the respective languages. The images in each sample are selected to reflect the culture of the annotator who composed the textual statement in their native language. XM3600. The XM3600 dataset (Thapliyal et al., 2022) is an extensive multilingual image captioning resource encompassing 36 languages. It contains 261375 captions across 13 scripts, with 100 unique images per language. The images are chosen to reflect the cultural background of the language, ensuring both cultural and linguistic diversity. All captions were manually produced by professional native speakers rather than being automatically generated. Due to the datasets large size, we evaluate XM3600 using randomly selected subset of 500 images per language. Babel-ImageNet (multiple-choice) (BIN-MC). Babel-ImageNet (Geigle et al., 2024c) translates ImageNets (Deng et al., 2009) labels into nearly 300 languages, allowing us to assess whether models can recognize and correctly link diverse ImageNet objects to their labels in the target language. Given the computational cost, we focus on languages that appear in only one or two other datasets, in addition to English and select few high-resource languages, and we use 10 images per class instead of 50. We follow Geigle et al. (2024b) and frame the task as multiple-choice problem by mining hard negative options from the complete label pool. This approach avoids the ambiguity inherent in traditional open-ended VQA formats. Negatives are selected based on the English labels, filtering out candidates not translated by Babel-ImageNet into the target language, and ultimately choosing the three most similar negative labels available. SMPQA. Geigle et al. (2025) introduce SMPQA (Synthetic Multilingual Plot QA) as test dataset for evaluating multilingual OCR capabilities for bar plots and pie charts, covering 11 languages and various scripts and resource levels. M5B-VGR. The M5B-VGR dataset, presented by (Schneider and Sitaram, 2024), is visually grounded reasoning benchmark akin to MaRVL. Each sample comprises two images, textual statement, and binary (true/false) answer based It spans 12 languages across 7 on the images. scripts and features culturally diverse photos from regions where the respective languages are spoken. The images are sampled from the Dollar Street (Gaviria Rojas et al., 2022) dataset, with 120 samples provided per language. M5B-VLOD. The M5B-VLOD (Visio-Linguistic also introduced Outlier Detection) dataset, by (Schneider and Sitaram, 2024), consists of samples containing five images paired with textual statement that is true for all but one image. The task is to identify the outlier image that does not match the statement. This dataset covers the same 12 languages as M5B-VGR, with images sampled in similar manner from the same source, and provides"
        },
        {
            "title": "Captioning",
            "content": "Multiple-Choice Visual Question Answering Text-Heavy Multiple-Choice Visual Question Answering Text-Heavy Visual Question Answering Text-Heavy Visually Grounded Reasoning Visio-Linguistic Outlier Detection"
        },
        {
            "title": "Dataset",
            "content": "XM"
        },
        {
            "title": "Textual Input",
            "content": "Single-Image Prompt (English)"
        },
        {
            "title": "Target Output",
            "content": "Caption (Target Language)"
        },
        {
            "title": "CIDEr",
            "content": "#Lang. 36 BabelImageNet-MC Single-Image Question (Target Language)"
        },
        {
            "title": "Relaxed Accuracy",
            "content": "20 M3Exam MMMU xMMMU MTVQA SMPQA - Name Single or Multi-Image Question (Target Language) Question (English) Question (Target Language)"
        },
        {
            "title": "Relaxed Accuracy",
            "content": "Single-Image Question (Target Language) Word or Phrase (Target Language)"
        },
        {
            "title": "Exact Accuracy",
            "content": "SMPQA - Ground Single-Image Question (Target Language) yes / no"
        },
        {
            "title": "Exact Accuracy",
            "content": "M5B-VLOD Multi-Image Hypothesis (Target Language)"
        },
        {
            "title": "MaXM\nxGQA",
            "content": "M5B-VGR MaRVL Single-Image Hypothesis (Target Language) yes / no / maybe"
        },
        {
            "title": "Exact Accuracy",
            "content": "Single-Image Question (Target Language) Word or Phrase (Target Language) Word or Phrase (English)"
        },
        {
            "title": "Exact Accuracy",
            "content": "Multi-Image Hypothesis (Target Language) yes / no"
        },
        {
            "title": "Exact Accuracy",
            "content": "7 1 7 9 11 11 12 5 6 12 6 Table 3: Summary of multilingual vision-language benchmarks we correlate MVL-SIB against. Relaxed denotes responses that start with the correct option letter (cf. 4). multiple-choice questions in the target language, accompanied by up to 8 images that may appear in both the question and the answer options, with the number of choices ranging from 4 to 8 per sample. xMMMU. xMMMU, introduced by (Yue et al., 2024), comprises college-level multiple-choice VQA samples in seven languages. It was automatically translated using GPT4o from randomly selected subset of 300 questions from the MMMU (Yue et al., 2023) validation split. A.5 Prompts We list the prompts for each dataset in our test suite used for all models in Figure 8. 120 samples per language. MTVQA. The MTVQA dataset, introduced by (Tang et al., 2024), features text-heavy visual question answering tasks. It includes expert human annotations in 9 diverse languages, comprising 6778 question-answer pairs across 2116 images. The images predominantly contain text in the corresponding language, with questions and answers closely tied to that text. These images are sourced from various publicly available datasets. CVQA. The CVQA dataset, introduced by (Romero et al., 2024), is multilingual and culturally nuanced VQA benchmark that includes broad array of languages, many of which are underrepresented in NLP. It consists of 10000 questions spanning 30 countries and 31 languages, forming 39 distinct country-language pairs (for instance, Spanish appears in 7 different splits corresponding to 7 Spanish-speaking countries). The images were manually collected by human annotators to accurately depict the culture associated with each country-language pair. Each sample includes one image and question in the respective language. Although the test set is not publicly available, the authors permit up to 5 daily leaderboard submissions for evaluation. M3Exam. The M3Exam dataset, presented by (Zhang et al., 2023), contains real-world exam questions in 9 languages, available as either textonly or multimodal samples. For our evaluation, we only include samples that require at least one image. Due to the limited number of samples for Swhalili and Javanese, we focus on the remaining 7 languages. The selected samples consist of"
        },
        {
            "title": "SMPQA",
            "content": "<IMG>{QUESTION}nAnswer the question using single word or phrase."
        },
        {
            "title": "CVQA",
            "content": "<IMG>{QUESTION}nThere are several options:nA. {OPTION A}nB. {OPTION B}nC. {OPTION C}nD. {OPTION D}nAnswer with the options letter from the given choices directly. xMMMU {QUESTION}nThere are several options:nA. {OPTION A}nB. {OPTION B}nC. {OPTION C}nD. {OPTION D}nAnswer with the options letter from the given choices directly."
        },
        {
            "title": "MTVQA",
            "content": "<IMG>{QUESTION}nAnswer the question using single word or phrase.nAnswer in {LANGUAGE}. M3Exam {QUESTION}nOptions:nA. {OPTION A}nB. {OPTION B}nC. {OPTION C}nD. {OPTION D}n Answer with the options letter from the given choices directly. BIN-MC <IMG>Which of these choices (in English) is shown in the image?n Choices:nA. {CHOICE A}nB. {CHOICE B}nC. {CHOICE C}nD. {CHOICE D}n Answer with the letter from the given choices directly. xGQA <IMG>{QUESTION}?nAnswer the question using single word or phrase.nAnswer in English."
        },
        {
            "title": "MaXM",
            "content": "<IMG>{QUESTION}?nAnswer the question using single word or phrase.nAnswer in {LANGUAGE}."
        },
        {
            "title": "MaRVL",
            "content": "<IMG>Given the two images <IMG><IMG>, is it correct to say {HYPOTHESIS}? Answer yes or no."
        },
        {
            "title": "XVNLI",
            "content": "<IMG>Is it guaranteed true that {HYPOTHESIS}? Yes, no, or maybe? Answer in English. M5-VGR Given the two images <IMG><IMG>, is it correct to say {HYPOTHESIS}? Answer yes or no. M5-VLOD Based on the 5 images <IMG><IMG><IMG><IMG><IMG> ordered from top-left to bottom-right, which image does not match the hypothesis {HYPOTHESIS}? Choose one from [A, B, C, D, E] and only output single letter: XM Briefly describe the image in {LANGUAGE} in one sentence. Figure 8: Prompts used for the different datasets of our test suite. For M3Exam and xMMMU, the questions contain images at individual positions, and also the options can consist of images. In total, sample of M3Exam can contain up to 8 images and 8 options, and sample of xMMMU can contain up to 4 images and 4 options. 22 A.6 Full Results For Subsets by Task, Model, and Language Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o acm_Arab aka_Latn apc_Arab arb_Arab azj_Latn bul_Cyrl ces_Latn eng_Latn fin_Latn hat_Latn hau_Latn min_Arab umb_Latn 3 2 3 4 2 3 4 4 4 1 1 2 1 34.9 28.7 34.9 35.2 34.6 35.0 36.2 36.3 33.0 32.1 29.1 27.0 29.1 55.4 38.0 54.8 57.1 53.8 56.6 58.2 65.8 57.1 50.4 39.2 30.4 34.8 80.8 57.2 80.0 81.4 80.5 79.3 79.5 79.8 79.1 77.8 56.3 58.4 52.5 50.0 34.6 49.1 51.7 49.5 50.3 52.0 52.5 49.8 43.0 32.4 27.8 31. 61.0 46.3 61.3 62.2 58.6 65.3 65.6 67.7 64.1 54.7 41.8 33.6 42.5 74.1 51.3 74.2 75.8 70.9 77.1 78.5 N/A 76.5 66.1 45.2 36.3 45.2 80.2 58.3 78.8 80.8 78.3 79.8 79.5 79.7 79.2 74.1 58.0 53.4 50.8 82.2 57.0 82.1 82.8 80.0 81.6 81.5 81.7 81.1 74.2 57.5 47.8 52.0 54.4 42.4 54.4 55.2 51.8 53.9 53.2 54.8 54.1 54.9 43.9 37.1 35.4 72.0 61.4 71.5 73.2 72.8 71.5 72.7 68.3 70.2 69.3 70.2 44.8 47. N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Table 4: Subsets of language tiers for I2S @ k=1. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o acm_Arab aka_Latn apc_Arab arb_Arab azj_Latn bul_Cyrl ces_Latn eng_Latn fin_Latn hat_Latn hau_Latn min_Arab umb_Latn 3 2 3 4 2 3 4 4 4 1 1 2 35.2 27.9 34.7 35.2 33.1 34.7 35.6 34.8 33.5 30.7 28.7 27.4 28.9 55.1 38.3 54.3 56.4 53.4 54.6 56.8 63.1 56.4 52.7 39.0 32.6 34.8 82.4 63.2 82.2 81.8 82.3 82.7 82.2 82.6 81.9 78.9 61.6 61.0 58.7 45.6 34.2 45.1 46.9 46.2 45.9 48.9 49.2 45.8 42.1 30.2 28.9 30.5 60.6 46.3 60.5 60.9 60.9 64.4 65.6 67.9 65.6 58.1 42.2 32.1 42.2 74.0 52.9 73.9 76.0 71.4 78.8 78.4 80.1 77.2 67.9 46.8 36.4 45. 85.5 61.1 84.1 85.0 82.7 84.8 83.6 84.9 83.2 77.6 59.8 55.9 52.6 86.7 60.6 85.7 87.1 83.5 84.5 85.3 85.5 84.4 79.3 59.8 50.2 53.5 59.7 42.6 59.4 58.7 57.9 58.8 57.3 60.0 59.7 58.0 44.5 35.9 35.8 79.3 63.3 78.5 79.7 78.6 79.5 78.6 78.1 78.3 77.1 75.6 46.3 46.8 86.0 79.3 85.8 86.7 86.5 86.4 86.3 87.1 86.5 85.1 85.1 76.1 61.1 Table 5: Subsets of language tiers for I2S @ k=3. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2. Centurio Qwen GPT-4o acm_Arab aka_Latn apc_Arab arb_Arab azj_Latn bul_Cyrl ces_Latn eng_Latn fin_Latn hat_Latn hau_Latn min_Arab umb_Latn 3 2 3 4 2 3 4 4 4 1 1 2 1 33.7 27.0 32.7 33.3 32.2 33.2 34.9 34.9 32.6 31.1 28.4 27.4 28.0 48.7 34.0 48.9 50.6 47.9 50.1 52.2 58.9 51.1 49.5 35.9 30.1 31. 83.2 64.8 83.9 83.2 82.2 83.1 82.2 83.9 82.2 80.0 63.3 62.0 59.1 47.6 34.5 46.2 49.3 45.5 48.5 48.6 48.1 46.8 41.3 31.4 28.5 31.6 60.6 47.0 61.2 62.3 63.0 66.0 66.0 68.7 66.6 59.4 42.8 32.3 42.7 73.9 52.6 73.6 75.6 72.8 77.6 78.8 N/A 76.3 68.7 46.2 36.3 46.6 85.8 60.6 84.7 85.9 83.6 85.2 84.7 84.5 83.7 77.0 60.5 55.7 52.3 87.1 60.3 87.3 87.7 85.3 86.5 86.4 87.0 86.3 80.4 60.4 49.8 53. 61.1 43.2 60.3 60.2 59.4 60.2 58.9 62.4 60.7 58.5 44.7 36.4 35.7 77.3 60.7 77.4 78.2 78.1 78.3 79.3 77.4 77.0 75.6 73.9 43.9 45.4 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Table 6: Subsets of language tiers for I2S @ k=5. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o acm_Arab aka_Latn apc_Arab arb_Arab azj_Latn bul_Cyrl ces_Latn eng_Latn fin_Latn hat_Latn hau_Latn min_Arab umb_Latn 3 2 3 4 2 3 4 4 4 1 1 2 1 46.6 32.3 46.0 48.1 42.6 47.1 50.8 56.7 42.3 40.2 32.6 28.3 30.3 81.1 53.2 80.9 82.7 80.2 83.3 82.8 85.7 82.0 70.8 52.1 42.3 47.1 91.1 68.1 91.3 91.4 90.2 91.2 90.6 91.8 91.4 87.3 66.5 68.0 61.4 71.5 44.4 70.0 73.1 65.5 73.4 75.3 81.4 70.5 57.9 40.0 32.8 39. 80.5 59.6 80.4 81.3 76.9 83.5 85.3 87.0 82.3 74.1 53.8 41.5 52.4 84.9 64.4 84.6 86.5 82.0 88.2 89.8 91.3 87.1 79.2 57.5 41.4 57.2 91.3 68.6 90.9 91.7 89.1 90.6 90.8 91.8 90.2 84.9 70.7 63.7 N/A 89.6 66.5 89.1 90.2 87.3 89.2 90.0 91.5 88.1 83.7 66.2 56.4 59.6 83.2 63.5 82.7 84.9 82.0 83.9 84.6 85.4 81.8 82.0 66.5 49.8 53.4 89.9 79.9 89.4 89.9 89.1 89.3 90.1 88.5 89.2 87.3 87.3 62.8 62. 92.3 87.7 92.5 92.8 91.9 92.5 92.2 92.0 92.3 91.6 91.9 86.4 71.7 Table 7: Subsets of language tiers for T2S @ k=1. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o aeb_Arab arb_Arab ben_Beng eng_Latn fao_Latn kac_Latn kas_Deva lit_Latn lua_Latn mal_Mlym srp_Cyrl tur_Latn wol_Latn 3 4 3 4 2 1 2 3 1 2 4 4 43.1 44.3 39.8 41.9 33.6 28.8 33.3 38.1 31.0 32.1 42.7 42.7 31.5 62.9 66.5 64.4 71.7 52.2 35.4 47.1 63.6 43.8 64.3 66.2 66.6 42.1 80.1 80.2 81.2 N/A 75.1 51.5 72.4 78.9 57.6 80.1 79.5 78.5 56.8 36.8 37.7 41.2 47.7 30.9 26.4 30.2 34.7 29.3 34.1 37.0 38.7 28.4 53.2 55.2 50.9 66.2 43.9 37.8 42.4 50.2 41.7 47.0 53.0 56.4 40.0 68.7 71.9 69.0 N/A 56.8 44.7 53.0 68.8 48.8 62.3 71.3 74.4 49. 79.3 81.4 79.1 81.7 73.6 51.6 69.0 78.4 57.4 77.0 80.1 80.5 57.0 77.6 79.9 77.2 81.3 69.0 46.9 59.3 75.1 55.9 73.2 78.2 77.6 54.0 29.8 30.8 33.0 35.3 29.2 26.7 27.7 30.1 27.7 31.0 31.7 31.0 27.2 76.3 77.7 76.4 77.5 75.8 46.6 66.5 77.4 54.9 76.2 77.9 77.4 56.5 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Table 8: Subsets of language tiers for S2I @ k=1. 23 Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o aeb_Arab arb_Arab ben_Beng eng_Latn fao_Latn kac_Latn kas_Deva lit_Latn lua_Latn mal_Mlym srp_Cyrl tur_Latn wol_Latn 3 4 3 4 2 1 2 3 1 2 4 4 1 44.1 44.8 40.4 43.1 35.0 31.6 35.1 42.1 33.7 33.7 44.1 44.6 34. 65.3 67.6 67.0 70.4 55.2 41.0 50.1 64.7 46.2 65.3 65.1 66.7 45.8 89.6 89.1 88.9 88.5 87.5 69.7 87.0 89.1 74.6 88.0 88.7 88.3 75.2 38.5 39.2 43.4 44.5 34.9 29.2 31.9 39.2 33.0 38.1 39.7 41.3 32.9 58.6 59.8 58.1 69.0 50.6 46.3 48.9 58.5 50.8 50.8 59.4 61.0 49.7 76.0 78.9 74.5 84.0 68.9 57.5 60.6 77.4 61.5 69.7 78.3 81.7 62.3 92.0 92.4 90.8 92.1 89.5 72.3 86.5 91.0 78.5 90.7 91.8 91.8 79. 89.0 90.1 89.2 90.0 85.9 68.0 78.0 89.4 77.1 88.2 89.8 88.8 77.0 31.7 32.8 33.9 36.1 30.5 27.4 28.9 32.2 27.8 31.0 32.6 34.3 28.1 85.4 86.8 85.7 86.4 86.0 64.2 80.4 86.2 70.4 85.1 87.1 87.0 72.7 88.3 89.1 87.9 89.1 89.1 70.5 86.1 89.0 79.3 87.8 89.5 88.5 83.1 Table 9: Subsets of language tiers for S2I @ k=3. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o aeb_Arab arb_Arab ben_Beng eng_Latn fao_Latn kac_Latn kas_Deva lit_Latn lua_Latn mal_Mlym srp_Cyrl tur_Latn wol_Latn 3 4 3 4 2 1 2 3 1 2 4 4 1 44.0 44.3 41.1 43.4 35.3 31.8 35.3 42.1 34.6 31.6 44.5 45.6 34.4 65.4 65.7 63.2 68.6 56.6 42.3 48.8 63.5 48.4 62.7 63.5 64.8 46.3 91.6 90.8 89.3 89.6 90.4 74.8 89.0 91.1 80.2 88.9 90.7 90.3 82. 39.0 40.0 45.9 43.0 36.0 31.6 34.1 39.5 34.6 39.0 41.2 40.9 34.8 57.6 59.9 59.4 68.7 50.8 46.2 50.0 57.7 49.7 51.8 58.4 59.9 50.9 77.0 79.4 76.4 N/A 71.8 60.6 64.7 78.1 63.1 69.3 79.4 81.4 64.3 93.6 94.0 93.0 93.7 92.8 79.2 91.3 93.8 85.1 93.2 93.6 93.3 86.1 91.8 91.9 91.5 91.7 89.5 73.9 82.9 92.1 82.8 91.7 91.4 91.3 82.2 32.8 33.5 34.8 35.6 30.7 27.0 29.4 32.8 28.1 31.5 33.3 34.8 27. 88.9 88.7 88.4 89.1 87.8 68.4 83.2 88.2 75.9 87.1 89.1 88.5 77.9 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Table 10: Subsets of language tiers for S2I @ k=5. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o aeb_Arab arb_Arab ben_Beng eng_Latn fao_Latn kac_Latn kas_Deva lit_Latn lua_Latn mal_Mlym srp_Cyrl tur_Latn wol_Latn 3 4 3 4 2 1 2 3 1 2 4 4 1 77.9 81.5 68.8 86.1 52.1 41.3 50.1 67.2 49.4 49.0 77.3 76.4 50.4 85.2 86.5 85.2 89.1 72.1 51.5 72.4 81.7 58.5 83.0 86.1 85.0 59.1 91.0 91.7 91.2 91.0 86.3 58.8 83.5 90.1 65.2 89.7 91.0 91.3 66.8 87.5 88.9 83.7 90.6 71.0 55.2 71.7 76.6 61.3 76.9 84.8 85.1 63.3 83.5 86.1 79.5 91.7 72.8 54.6 65.1 77.7 61.0 69.1 85.6 87.5 64. 83.9 88.0 81.6 92.0 76.9 55.7 63.4 84.6 59.0 74.3 87.0 89.4 62.0 91.5 92.8 91.2 92.4 86.5 57.4 80.4 89.6 64.8 88.7 91.5 91.7 66.6 91.9 93.4 90.8 93.5 83.9 58.0 74.2 89.6 68.5 87.2 92.3 91.6 67.6 88.8 89.8 86.4 89.9 80.3 54.7 75.0 84.2 63.3 80.6 88.2 88.2 65.5 91.5 92.7 91.1 92.4 90.0 58.2 82.9 91.1 65.8 90.5 91.7 91.5 69.5 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Table 11: Subsets of language tiers for S2T @ k=1. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o aeb_Arab arb_Arab ben_Beng eng_Latn fao_Latn kac_Latn kas_Deva lit_Latn lua_Latn mal_Mlym srp_Cyrl tur_Latn wol_Latn 3 4 3 4 2 1 2 3 1 2 4 4 1 94.1 95.7 86.9 96.5 69.4 57.6 68.6 87.1 66.7 61.1 93.2 92.5 66. 93.9 94.4 93.9 95.3 86.6 68.0 88.4 92.4 76.9 93.3 94.4 94.9 77.1 98.7 98.7 98.7 98.3 97.6 80.1 96.5 98.6 86.2 98.7 98.7 98.8 88.9 97.2 97.3 95.3 98.0 88.7 75.0 86.9 92.2 82.1 91.2 96.2 96.6 84.5 95.3 96.7 93.0 98.1 87.1 77.7 81.2 94.9 83.0 85.5 96.5 97.2 84.9 96.3 98.0 94.7 98.7 92.6 75.5 82.3 97.6 82.1 90.7 97.6 98.6 86.0 98.8 98.7 98.6 98.7 97.2 79.4 95.4 98.2 86.5 98.3 98.7 98.6 88. 99.0 99.1 98.4 98.9 97.2 79.9 91.4 98.3 88.3 97.7 98.9 99.0 89.6 96.6 95.7 96.5 96.7 94.1 75.3 90.5 94.9 81.4 94.0 96.5 96.0 85.0 98.4 98.6 98.5 98.1 98.2 78.6 95.9 98.6 87.4 98.3 98.3 98.6 89.3 98.4 98.4 98.3 98.0 98.3 79.1 96.7 98.2 92.2 98.1 98.4 98.1 95.1 Table 12: Subsets of language tiers for S2T @ k=3. Lang Tier Qwen2-VL 2B Qwen2-VL 7B Qwen2-VL 72B InternVL 2.5 4B InternVL 2.5 8B InternVL 2.5 26B InternVL 2.5 38B InternVL 2.5 78B Centurio Qwen 8B GPT-4o-mini GPT-4o Qwen2-VL InternVL 2.5 Centurio Qwen GPT-4o aeb_Arab arb_Arab ben_Beng eng_Latn fao_Latn kac_Latn kas_Deva lit_Latn lua_Latn mal_Mlym srp_Cyrl tur_Latn wol_Latn 3 4 3 4 2 1 2 3 1 2 4 4 1 97.2 97.6 92.6 98.2 79.9 68.0 78.4 93.8 75.9 67.6 97.2 95.6 77.5 97.4 97.4 96.9 97.5 91.8 75.0 92.4 96.0 83.3 96.7 97.4 97.2 84.3 99.7 99.6 99.5 99.2 99.4 87.4 99.0 99.4 91.9 99.6 99.4 99.4 94. 98.7 98.6 97.6 99.1 92.7 81.5 92.3 95.9 88.1 95.5 98.0 98.4 90.4 97.7 98.5 96.8 99.0 92.9 86.0 87.9 98.2 90.1 90.2 99.0 98.8 92.7 98.8 99.3 97.9 99.5 96.3 83.9 87.5 99.3 89.1 95.7 99.4 99.5 92.9 99.7 99.7 99.6 99.4 99.2 87.1 99.0 99.4 92.4 99.5 99.6 99.4 95.2 99.7 99.7 99.5 99.6 99.5 87.3 96.6 99.7 93.5 99.1 99.6 99.7 95.7 98.3 97.6 97.7 97.7 96.9 80.9 93.8 97.2 85.2 97.4 98.2 97.6 91. 99.5 99.3 99.2 99.1 99.0 86.3 98.7 99.4 93.1 99.2 99.4 99.4 94.9 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Table 13: Subsets of language tiers for S2T @ k=5. 24 A.7 Full Performance by Task, Model, and Language A.7. Images-To-Topics"
        },
        {
            "title": "Model",
            "content": "Entertainment Geography Health Politics Science & Tech. Sports Travel QwenVL-2.5-2B QwenVL-2.5-7B InternVL-2.5-4B InternVL-2.5-8B Centurio-Qwen 99.4 100.0 99.5 100.0 99.6 83.1 92.9 84.5 90.0 90.6 100.0 100.0 100.0 100.0 100.0 99.9 100.0 100.0 100.0 100. 100.0 99.8 99.7 97.5 99.2 100.0 100.0 100.0 100.0 100.0 95.7 100.0 100.0 100.0 100.0 Table 14: Image-To-Topics. For reference image, the model must pick the correct topic out of 4 choices. A.7.2 Images-To-Sentences Lang. Avg eng_Latn ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn bul_Cyrl cat_Latn ceb_Latn ces_Latn cjk_Latn ckb_Arab crh_Latn cym_Latn dan_Latn deu_Latn dik_Latn dyu_Latn dzo_Tibt ell_Grek epo_Latn est_Latn eus_Latn ewe_Latn fao_Latn fij_Latn fin_Latn fon_Latn fra_Latn fur_Latn fuv_Latn gaz_Latn gla_Latn gle_Latn glg_Latn grn_Latn guj_Gujr hat_Latn hau_Latn heb_Hebr hin_Deva hne_Deva hrv_Latn hun_Latn hye_Armn ibo_Latn ilo_Latn ind_Latn isl_Latn ita_Latn jav_Latn jpn_Jpan kab_Latn kac_Latn kam_Latn kan_Knda kas_Arab kas_Deva kat_Geor kaz_Cyrl kbp_Latn kea_Latn khk_Cyrl khm_Khmr kik_Latn kin_Latn kir_Cyrl kmb_Latn kmr_Latn knc_Arab knc_Latn kon_Latn kor_Hang QwenVL-2 2B 5 3 1 QwenVL-2 7B 5 3 1 InternVL-2.5 4B 5 3 1 InternVL-2.5 8B 5 3 1 Centurio-Qwen 5 3 31.8 36.3 27.7 32.4 34.9 35.2 34.6 35.4 35.2 28.7 33.4 24.9 34.9 35.2 28.7 34.7 33.8 35.2 30.7 35.9 34.4 29.0 31.9 34.6 32.5 28.0 35.5 34.1 28.7 32.8 33.8 28.0 33.3 25.3 35.9 33.2 35.0 36.6 33.4 36.2 29.0 29.1 33.6 30.5 36.7 36.5 29.3 29.0 23.9 34.0 34.1 34.4 33.5 26.5 29.9 28.5 33.0 27.4 35.9 34.2 29.2 29.0 28.8 29.0 35.0 33.4 30.7 32.1 29.1 35.4 34.2 34.8 35.5 35.1 27.8 27.8 33.5 37.8 28.7 35.4 34.9 37.1 26.0 28.6 28.3 28.8 33.5 30.4 29.7 33.5 28.5 34.5 31.0 27.5 29.8 28.2 32.9 28.1 31.0 25.9 29.3 30.4 36.6 31.4 34.8 27.9 33.1 35.2 35.0 33.5 35.0 34.1 27.9 33.5 25.3 34.7 35.2 28.7 34.6 32.4 33.9 30.2 34.9 34.5 28.0 31.3 33.1 33.8 27.7 36.3 35.6 28.9 33.6 33.5 27.2 31.9 24.5 35.4 32.6 34.7 36.0 33.3 35.6 29.2 29.4 31.3 30.1 35.9 36.7 28.4 28.1 23.8 35.0 33.8 34.0 34.2 27.0 29.6 27.4 33.5 26.1 35.5 32.4 28.4 27.7 28.6 29.4 35.3 32.9 31.6 30.7 28.7 34.9 34.2 35.4 35.8 33.7 28.2 27.1 33.5 37.0 28.0 35.1 33.7 37.6 25.9 26.8 27.7 28.6 33.1 29.6 30.3 34.3 27.5 33.2 31.2 28.3 29.4 27.4 33.2 28.7 30.4 24.9 28.5 30.2 35.4 30.7 34.9 27.4 32.0 33.7 32.8 32.2 34.7 33.1 27.0 32.4 24.9 32.7 33.3 28.9 33.1 31.6 33.2 30.0 34.1 33.0 28.2 31.0 32.2 32.2 26.9 35.1 33.7 28.6 32.2 32.5 27.3 32.1 24.4 34.1 32.5 33.2 34.6 32.7 34.9 29.0 29.2 30.7 30.2 35.0 35.4 27.7 28.0 24.4 34.6 33.2 32.9 33.4 26.3 29.7 27.5 32.6 26.2 34.1 32.2 28.2 27.4 28.6 29.1 34.0 32.4 30.2 31.1 28.4 34.3 33.0 34.4 34.8 32.4 27.3 26.8 33.0 35.8 27.8 33.9 33.8 36.5 25.4 27.2 28.0 29.1 32.0 28.9 30.1 32.9 27.1 32.6 30.6 27.6 28.8 28.2 31.7 27.9 29.2 25.4 28.8 29.5 33.6 46.6 65.8 33.4 45.5 55.4 56.4 53.6 56.8 55.9 38.0 55.4 30.1 54.8 57.1 39.2 56.4 52.2 56.1 46.1 58.7 48.1 33.8 44.3 53.8 49.3 34.7 51.9 52.8 39.5 48.8 45.3 31.6 50.6 25.6 58.6 44.5 56.6 59.4 55.5 58.2 36.9 41.7 50.6 53.3 59.8 61.0 35.7 36.3 24.4 52.5 58.0 55.7 50.3 33.8 49.3 37.4 57.1 31.9 62.7 50.7 38.0 34.2 42.5 46.6 60.2 44.1 46.1 50.4 39.2 57.0 50.7 45.6 59.4 56.2 48.5 37.2 50.7 60.2 50.2 62.1 54.1 56.7 28.1 33.7 36.0 44.5 41.4 37.5 47.5 49.9 34.2 51.6 46.9 44.6 38.3 37.6 47.6 35.1 42.8 28.0 37.5 41.8 56.6 46.4 63.1 34.3 45.7 55.1 55.2 52.0 57.4 55.4 38.3 54.7 31.5 54.3 56.4 39.5 56.4 50.7 55.5 45.5 58.0 46.5 33.5 44.2 53.4 48.8 34.3 54.3 51.9 40.2 46.5 44.1 33.8 51.9 26.2 58.0 44.5 54.6 58.5 56.1 56.8 36.8 39.4 50.1 53.1 58.4 59.2 35.9 37.3 25.5 50.7 57.9 56.6 50.4 34.1 48.5 37.6 56.4 31.5 61.3 51.4 38.3 35.3 43.8 46.9 58.8 44.4 44.8 52.7 39.0 56.8 48.4 44.7 59.2 54.4 47.7 36.9 52.7 59.6 49.5 60.3 56.0 55.0 29.1 34.1 36.9 42.9 41.0 37.3 47.0 48.3 34.4 52.5 45.5 44.1 38.6 37.1 46.6 35.1 42.9 29.1 37.8 43.1 54.5 42.4 58.9 31.7 42.0 48.7 50.0 46.7 52.8 49.2 34.0 51.0 29.5 48.9 50.6 37.5 50.7 45.7 49.6 41.0 54.2 41.8 31.1 40.1 47.9 42.6 32.4 49.1 47.2 36.6 41.6 39.7 31.2 48.0 25.0 53.8 40.7 50.1 54.0 51.2 52.2 34.5 36.1 45.3 48.6 53.2 53.8 33.5 34.0 24.3 46.2 53.3 51.3 45.8 30.6 44.7 35.0 51.1 28.8 55.3 47.8 34.5 32.4 40.7 43.3 54.2 41.2 39.4 49.5 35.9 51.8 43.6 40.1 54.7 49.2 43.2 34.2 48.1 54.4 45.7 55.7 51.1 50.0 27.4 31.1 33.7 38.4 37.0 34.4 41.0 43.8 32.4 48.4 41.0 40.6 35.7 35.3 41.1 32.0 38.1 27.8 34.2 39.3 48. 40.9 52.5 29.5 39.9 50.0 50.8 48.2 48.4 49.8 34.6 43.9 33.8 49.1 51.7 35.4 51.3 45.8 50.9 44.6 46.8 43.4 30.5 33.3 49.5 41.5 32.3 41.2 45.9 35.4 47.0 42.3 27.7 41.8 25.5 51.7 37.1 50.3 51.1 47.4 52.0 32.7 35.7 43.1 40.9 53.3 51.7 33.1 33.0 24.5 45.5 50.3 48.5 44.0 30.3 40.3 32.0 49.8 29.8 52.2 43.2 33.9 31.9 36.5 37.5 50.7 37.6 44.8 43.0 32.4 47.9 47.0 40.7 51.8 48.0 33.0 36.0 43.0 50.6 40.4 51.8 43.2 51.7 27.4 30.1 33.7 42.8 35.4 31.9 39.8 45.1 31.8 46.5 37.5 40.5 34.8 31.9 41.4 33.0 38.9 25.6 34.0 36.4 52.5 39.0 49.2 28.9 38.7 45.6 45.8 44.2 45.2 46.2 34.2 42.3 33.5 45.1 46.9 34.0 46.6 43.1 46.5 42.6 43.6 40.9 30.0 31.8 46.2 40.1 31.2 39.2 43.6 34.7 44.3 38.6 27.5 39.0 25.1 49.3 36.2 45.9 46.0 44.2 48.9 32.4 35.2 40.9 39.5 48.1 46.6 32.4 31.4 25.1 43.9 46.7 44.4 41.3 30.6 37.2 31.3 45.8 29.9 48.1 40.7 32.8 31.1 35.5 37.7 47.8 37.6 42.9 42.1 30.2 44.9 44.8 38.8 48.6 45.2 33.2 35.0 42.1 47.1 38.6 47.9 41.1 47.3 27.1 29.6 32.5 40.5 35.4 29.3 38.9 41.5 30.5 44.0 35.7 39.6 34.3 31.8 40.1 31.7 39.0 26.4 33.0 35.9 47.3 39.3 48.1 29.4 38.4 47.6 48.3 45.1 46.4 48.0 34.5 42.9 32.4 46.2 49.3 35.2 49.2 43.5 47.9 40.4 45.1 39.9 29.7 32.7 45.5 39.3 31.5 39.4 43.8 34.9 43.3 38.2 28.6 39.4 24.7 50.3 36.8 48.5 48.3 45.0 48.6 33.2 35.4 40.9 39.9 49.3 47.8 33.5 32.4 24.8 41.8 47.4 43.9 42.6 30.0 38.8 32.4 46.8 30.0 49.4 41.6 31.7 31.6 35.0 38.2 47.3 38.1 40.9 41.3 31.4 46.8 43.7 37.0 49.7 46.4 33.0 35.0 41.9 48.3 38.5 49.6 41.4 47.7 26.8 29.8 33.5 38.6 34.3 31.3 37.5 42.1 31.5 44.5 36.3 37.5 34.3 32.0 40.2 31.5 38.0 25.6 33.0 36.5 48.0 53.1 67.7 34.6 55.7 61.0 61.8 59.2 61.0 60.4 46.3 59.1 29.7 61.3 62.2 43.5 62.0 56.8 61.4 56.4 61.9 63.0 40.6 49.8 58.6 54.9 41.9 56.4 57.7 46.4 62.2 61.2 33.5 54.2 42.7 65.9 51.5 65.3 64.9 60.4 65.6 43.3 39.9 58.7 57.7 64.7 67.2 45.5 44.1 39.5 62.7 61.7 61.8 60.1 41.3 54.4 43.1 64.1 39.9 67.0 56.8 43.1 38.7 46.4 49.5 64.0 52.5 60.6 54.7 41.8 58.5 64.9 61.7 65.9 64.6 35.0 46.1 57.9 64.1 53.4 67.6 55.6 65.8 33.5 43.5 44.6 56.4 51.0 51.1 37.4 56.0 42.5 60.3 45.7 47.9 47.4 44.6 51.9 42.2 51.1 31.4 44.4 49.2 66.9 53.3 67.9 34.1 57.1 60.6 60.8 58.2 64.3 58.7 46.3 61.0 28.3 60.5 60.9 44.7 60.7 56.5 58.8 52.8 63.8 59.5 41.3 49.2 60.9 55.8 42.0 61.5 57.0 47.7 58.1 56.2 32.0 57.3 41.7 67.1 54.0 64.4 66.4 62.4 65.6 45.6 39.2 60.8 59.7 66.4 68.2 45.8 45.6 37.5 62.0 63.8 63.0 61.1 42.1 56.8 43.9 65.6 40.8 67.7 60.0 44.3 39.3 47.4 51.4 64.0 53.7 56.0 58.1 42.2 57.2 60.3 57.5 67.9 65.4 33.1 44.6 61.1 65.9 55.0 67.1 59.2 66.6 33.3 43.5 45.2 51.5 50.4 48.3 34.6 56.0 41.3 62.1 44.9 44.2 46.7 44.5 51.4 44.3 51.1 29.7 45.0 50.6 65.9 53.7 68.7 34.0 58.0 60.6 61.0 59.3 64.7 60.8 47.0 62.8 27.4 61.2 62.3 46.3 61.5 57.5 61.6 51.6 64.2 57.4 41.9 51.3 63.0 57.0 43.6 61.1 58.5 47.3 55.7 56.5 32.5 57.8 40.7 67.6 54.2 66.0 66.4 62.7 66.0 46.6 39.0 61.8 61.7 66.9 67.7 46.6 47.6 36.1 63.3 64.8 64.2 61.8 42.1 58.8 45.4 66.6 40.9 68.3 60.1 45.0 41.1 49.3 52.8 64.9 55.1 54.6 59.4 42.8 59.1 60.7 56.8 67.9 66.7 33.3 43.7 61.7 65.8 56.5 67.7 59.8 67.3 34.6 43.7 46.2 48.6 50.1 48.7 33.3 57.7 41.8 63.1 46.5 41.5 48.2 44.7 52.1 44.7 52.4 30.4 46.7 50.9 68. 47.8 54.8 38.9 46.1 54.4 54.1 55.3 55.4 53.9 42.4 51.1 44.3 54.4 55.2 46.3 54.7 54.0 55.1 51.5 54.9 53.8 35.4 52.9 51.8 49.4 40.5 51.6 51.9 39.9 51.7 52.3 35.5 53.6 38.0 52.9 45.9 53.9 55.1 53.6 53.2 38.2 40.3 53.4 50.9 55.2 54.9 37.9 41.3 38.4 50.5 52.6 53.4 52.2 34.8 51.8 39.0 54.1 34.9 54.2 52.6 39.0 39.3 47.5 49.1 54.8 45.9 48.4 54.9 43.9 54.5 52.2 53.1 54.4 56.6 42.3 46.6 49.9 54.6 50.9 53.3 52.4 53.6 32.8 35.8 38.5 45.9 47.7 46.3 49.3 53.9 34.6 54.0 42.4 48.9 41.9 36.3 51.4 36.7 43.0 31.6 37.1 43.0 54.3 50.5 60.0 37.5 51.7 59.7 59.1 59.4 60.8 59.2 42.6 56.6 46.8 59.4 58.7 46.2 59.1 57.4 60.0 53.1 61.6 58.7 34.5 54.2 57.9 51.2 40.2 56.8 55.7 40.3 55.7 56.3 36.4 57.3 37.6 58.0 48.6 58.8 59.0 57.7 57.3 38.1 40.7 57.5 52.8 61.1 60.6 38.1 42.7 37.5 53.0 58.7 57.8 57.2 34.3 53.7 40.6 59.7 34.3 59.5 55.5 39.2 39.5 49.3 50.6 61.3 48.6 50.7 58.0 44.5 60.3 57.8 56.1 59.0 61.8 43.7 47.1 53.6 60.7 54.3 59.8 57.7 60.2 31.5 34.5 40.2 49.3 48.9 47.0 52.6 57.9 34.0 57.6 43.4 49.3 42.8 36.0 55.2 36.1 44.1 28.5 38.5 43.4 59.5 51.1 62.4 38.3 51.5 61.1 60.9 60.3 61.8 60.5 43.2 58.3 47.1 60.3 60.2 46.9 59.7 58.8 61.3 51.8 61.7 58.7 35.7 55.5 59.4 53.1 41.4 57.6 56.2 40.9 54.3 57.2 35.4 59.2 36.5 59.8 48.5 60.2 60.8 59.4 58.9 38.0 41.5 59.1 52.5 63.6 63.0 38.3 42.6 36.4 54.0 58.1 57.7 58.6 34.7 54.4 40.2 60.7 36.1 61.2 56.1 39.7 39.2 49.0 50.8 62.6 48.8 50.4 58.5 44.7 60.3 57.6 57.4 60.3 62.2 43.4 47.1 54.0 61.3 56.5 61.8 58.2 60.4 30.6 35.2 40.2 46.4 50.0 48.3 53.2 58.2 34.8 57.4 42.5 48.4 41.7 36.4 55.4 37.4 44.5 29.0 37.7 44.3 60.5 1 64.2 68.3 48.1 62.4 72.0 72.6 71.8 71.0 72.6 61.4 72.6 57.2 71.6 73.2 67.8 72.7 70.3 72.9 70.8 69.3 70.7 44.0 63.1 72.8 71.0 43.4 67.3 73.0 56.0 72.4 70.5 47.1 66.4 32.5 71.4 59.5 71.5 71.4 71.4 72.7 43.3 64.7 69.4 71.6 70.8 70.7 44.1 45.6 30.9 73.3 70.0 70.0 69.2 40.7 68.9 60.4 70.2 36.9 71.7 66.6 50.5 65.4 67.4 70.1 70.1 64.1 70.9 69.3 70.2 71.7 72.3 71.1 72.7 71.4 72.2 69.8 69.1 71.9 71.1 71.0 70.2 72.6 35.0 41.1 51.4 71.6 65.3 60.8 71.8 70.8 37.9 68.3 70.5 69.0 52.4 70.7 71.4 42.5 65.3 32.4 46.0 57.1 70.8 4o-mini 69.2 78.1 49.4 67.0 79.3 79.5 78.6 78.5 78.6 63.3 78.5 58.1 78.5 79.7 72.9 80.2 76.5 79.0 75.7 77.5 77.6 47.3 68.7 78.6 76.2 45.2 72.3 78.6 58.0 78.9 76.1 47.9 73.3 32.7 79.2 62.0 79.5 78.6 77.2 78.6 46.2 66.4 74.5 77.8 78.7 79.0 45.8 48.8 31.5 78.7 77.1 78.3 76.7 42.3 74.7 62.8 78.3 38.0 79.8 73.7 54.0 68.2 71.4 73.9 78.9 68.6 77.5 77.1 75.6 80.4 79.7 76.1 80.5 76.9 77.2 74.3 75.2 79.5 77.6 80.3 77.6 79.8 34.0 43.4 53.8 77.0 67.9 64.6 79.5 79.2 39.7 73.7 76.3 72.5 53.3 75.3 77.8 44.9 68.1 31.8 47.6 59.2 78.8 5 67.7 77.4 46.6 65.3 77.3 78.0 76.3 77.7 76.8 60.7 77.4 54.4 77.3 78.2 71.6 78.4 74.9 77.9 74.1 76.3 76.5 46.1 66.6 78.1 75.0 43.9 70.3 78.2 55.4 77.1 75.2 45.1 71.2 30.5 78.2 60.3 78.3 78.4 76.1 79.3 45.3 64.3 72.9 77.0 77.5 78.5 44.3 46.4 29.5 77.8 77.0 77.1 75.1 41.1 72.4 61.2 77.0 37.6 78.7 71.9 52.2 65.8 69.3 72.2 77.8 66.6 76.4 75.6 73.9 78.7 78.1 74.5 78.7 76.2 76.3 72.6 73.4 79.5 76.4 79.7 75.7 79.2 34.2 41.6 52.0 76.0 63.8 62.2 77.7 78.3 38.8 72.1 75.5 70.3 51.6 73.9 76.8 42.5 66.9 30.4 46.4 57.0 77.7 Lang. lao_Laoo lij_Latn lim_Latn lin_Latn lit_Latn lmo_Latn ltg_Latn ltz_Latn lua_Latn lug_Latn luo_Latn lus_Latn lvs_Latn mag_Deva mai_Deva mal_Mlym mar_Deva min_Arab min_Latn mkd_Cyrl mlt_Latn mni_Beng mos_Latn mri_Latn mya_Mymr nld_Latn nno_Latn nob_Latn npi_Deva nqo_Nkoo nso_Latn nus_Latn nya_Latn oci_Latn ory_Orya pag_Latn pan_Guru pap_Latn pbt_Arab pes_Arab plt_Latn pol_Latn por_Latn prs_Arab quy_Latn ron_Latn run_Latn rus_Cyrl sag_Latn san_Deva sat_Olck scn_Latn shn_Mymr sin_Sinh slk_Latn slv_Latn smo_Latn sna_Latn snd_Arab som_Latn sot_Latn spa_Latn srd_Latn srp_Cyrl ssw_Latn sun_Latn swe_Latn swh_Latn szl_Latn tam_Taml taq_Latn taq_Tfng tat_Cyrl tel_Telu tgk_Cyrl tgl_Latn tha_Thai tir_Ethi tpi_Latn tsn_Latn tso_Latn tuk_Latn tum_Latn tur_Latn twi_Latn tzm_Tfng uig_Arab ukr_Cyrl umb_Latn urd_Arab uzn_Latn vec_Latn vie_Latn war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn QwenVL-2 2B 5 3 1 QwenVL-2 7B 5 3 InternVL-2.5 4B 5 3 1 InternVL-2.5 8B 5 3 1 Centurio-Qwen 5 3 1 26.6 33.2 33.1 30.5 34.9 33.9 32.5 34.5 30.2 28.2 29.6 32.7 33.5 34.2 34.8 30.8 32.0 27.0 34.4 35.2 33.5 29.7 29.5 28.2 26.9 37.5 36.0 36.9 34.6 24.3 29.2 28.0 30.3 34.3 26.7 35.2 30.5 34.4 32.7 35.5 29.4 37.1 35.2 33.8 30.8 35.8 28.1 35.5 29.4 31.2 24.7 33.4 27.2 24.3 35.3 34.8 28.1 28.3 31.2 28.3 28.5 35.4 33.8 35.9 28.6 35.3 36.4 30.1 32.9 27.0 29.3 24.2 32.1 29.4 31.8 34.0 35.6 24.6 33.8 28.7 28.0 32.0 28.4 36.5 29.1 24.4 28.2 34.2 29.1 33.6 32.1 34.5 35.2 34.2 30.3 29.7 27.9 27.6 37.0 37.0 37.0 36.6 28.1 27.0 31.8 31.9 30.4 34.4 32.7 31.5 34.3 30.3 27.3 28.3 31.3 33.5 34.3 35.2 30.8 32.2 27.4 33.9 34.6 32.7 27.7 28.3 27.0 26.1 36.9 34.7 35.8 35.5 24.2 28.6 26.7 28.8 34.1 25.9 34.6 31.6 34.1 32.7 35.0 29.7 36.4 35.2 34.3 30.5 35.0 27.3 35.8 28.7 29.6 24.5 32.1 25.7 24.7 35.0 34.5 28.4 28.3 30.5 28.2 27.4 35.1 32.1 34.7 27.9 34.6 35.5 28.9 31.9 26.7 29.0 23.6 33.5 31.1 30.8 33.9 36.6 24.4 32.3 28.3 28.2 31.9 28.5 36.1 28.1 24.3 28.0 34.8 28.9 33.8 32.1 32.7 35.2 34.5 29.8 29.1 27.4 26.6 38.0 36.4 37.5 35.7 27.9 26.0 31.5 31.7 30.9 33.5 31.3 30.4 33.3 29.5 27.2 28.0 30.8 32.9 32.1 34.0 29.9 31.3 27.4 33.4 33.4 32.9 27.7 28.0 26.6 25.5 35.4 33.7 34.7 33.8 24.8 28.4 26.4 28.5 32.4 26.3 33.4 30.4 33.1 30.1 33.7 29.8 35.0 33.9 32.5 30.0 33.3 27.4 34.1 27.6 29.2 24.9 31.5 25.1 25.0 34.0 32.9 28.7 27.8 29.5 27.7 27.2 33.9 30.9 32.8 27.8 34.6 35.0 29.0 31.8 26.5 28.7 24.4 32.2 29.2 29.4 33.0 35.0 25.1 31.8 27.8 27.6 31.2 27.7 35.0 27.9 24.1 26.9 33.5 28.0 33.3 31.0 32.3 33.6 33.6 28.9 29.2 27.5 27.0 37.1 36.1 36.8 34.6 27. 36.8 49.8 50.3 40.7 56.9 52.2 45.7 49.7 40.2 36.3 36.9 42.5 57.5 46.3 45.2 46.2 48.2 30.4 50.5 56.1 53.0 32.2 34.1 39.2 41.6 60.6 58.3 59.3 49.3 25.0 38.7 29.8 40.3 56.6 41.5 50.5 45.5 52.2 48.8 55.4 40.2 60.1 61.8 53.9 38.7 59.9 36.4 59.2 37.7 39.7 24.8 51.5 31.1 40.0 57.4 57.1 38.5 37.5 44.2 41.1 38.8 61.4 50.5 54.5 38.1 54.0 59.2 50.8 48.5 44.9 37.0 26.4 48.2 45.9 46.6 56.5 55.7 27.9 52.2 37.2 37.1 47.9 38.0 59.2 38.5 25.9 42.2 55.6 34.8 50.1 50.1 54.0 60.0 54.3 39.6 40.9 37.4 36.8 60.1 61.3 59.9 59.1 38.0 36.4 50.1 50.7 42.2 56.6 52.7 47.4 51.5 39.9 36.2 37.5 42.8 57.0 45.2 44.3 44.3 46.4 32.6 51.3 55.6 53.5 33.4 34.9 39.3 41.1 59.9 58.0 58.5 48.2 25.4 39.2 30.9 40.6 56.1 39.5 53.3 43.7 52.5 47.3 52.6 41.4 58.7 60.6 51.4 38.9 58.9 37.1 57.1 37.6 40.1 25.1 52.9 30.5 38.9 55.9 57.0 38.9 37.5 44.4 41.1 38.4 60.3 50.4 53.7 38.3 54.0 59.5 52.1 48.7 43.6 38.0 26.8 47.5 44.9 45.7 56.6 53.4 28.5 53.0 37.1 37.1 47.8 37.8 58.5 38.5 26.7 41.6 53.6 34.8 48.2 50.4 54.3 59.4 55.0 40.3 40.9 39.8 37.5 57.5 58.2 57.1 58.7 38.1 34.0 46.7 47.0 39.5 52.3 48.1 42.6 46.8 36.7 33.4 34.5 38.5 51.9 40.5 39.8 38.3 42.3 30.1 46.4 51.3 48.4 31.3 31.4 36.4 37.3 54.8 52.7 53.8 42.8 24.9 35.7 28.8 37.5 51.2 33.6 48.9 38.3 47.6 42.6 47.2 38.1 53.1 54.6 46.1 35.5 54.2 34.1 51.9 34.9 37.4 24.2 47.5 27.8 35.1 51.5 52.9 35.3 34.5 38.8 38.6 35.9 55.4 46.4 49.4 35.0 50.0 55.3 47.9 44.9 38.5 34.7 25.9 42.6 38.7 41.5 53.1 48.2 27.6 48.5 34.2 34.0 43.6 34.7 53.5 34.4 25.0 37.0 49.0 31.7 44.0 45.6 49.9 54.1 50.5 36.2 38.6 36.2 33.4 53.9 54.8 53.7 53.6 36.5 39.6 43.1 42.0 38.0 49.9 43.2 42.3 41.6 36.0 32.5 32.3 36.3 49.6 41.6 39.3 42.1 42.7 27.8 41.5 48.0 42.9 30.2 31.3 33.9 37.1 50.3 50.0 51.4 42.9 25.1 33.5 30.3 34.8 47.9 43.8 45.1 44.5 46.5 37.9 48.5 34.2 50.7 52.5 46.8 34.2 50.7 31.1 51.1 32.8 36.9 24.7 43.2 32.3 37.6 51.3 51.0 33.3 32.5 41.1 35.7 34.5 49.8 41.0 48.7 33.8 45.0 53.1 40.0 40.0 41.8 32.5 26.5 42.5 43.2 38.7 49.0 48.4 30.6 43.4 32.9 32.2 40.3 34.5 52.8 34.8 25.8 34.8 49.0 31.1 44.4 46.6 43.8 49.8 46.3 35.1 37.2 30.2 34.8 53.4 55.0 52.8 48.0 32.8 38.0 41.3 38.8 36.7 45.1 41.6 39.9 39.8 35.3 31.0 31.4 36.2 47.2 39.5 37.4 40.4 40.6 28.9 39.0 45.0 41.6 30.2 30.9 33.0 34.1 47.8 46.7 47.9 40.6 25.3 31.4 31.0 34.8 44.2 40.8 42.6 42.4 42.9 36.9 45.8 33.9 47.0 47.7 44.3 33.3 47.2 31.0 46.3 32.1 36.0 24.9 42.1 30.2 36.4 48.3 47.3 32.4 33.1 38.8 34.2 33.0 46.1 39.4 46.7 33.3 43.6 48.3 39.6 37.9 39.2 33.2 26.2 40.0 40.1 37.9 45.0 46.5 30.0 41.9 31.5 31.8 38.6 34.0 47.5 33.9 26.0 34.4 45.8 30.5 42.3 43.1 41.1 47.5 43.8 33.7 36.1 30.8 33.9 49.7 51.6 48.6 45.3 33.3 37.5 42.7 39.7 36.8 46.6 41.4 41.0 40.7 35.7 32.3 31.3 35.5 47.4 38.2 37.1 37.7 39.9 28.5 39.4 46.0 42.5 29.4 31.0 33.5 32.5 49.1 46.4 48.5 40.9 24.5 32.7 30.3 35.2 46.4 38.1 43.1 39.1 44.7 37.5 48.3 34.7 49.0 49.1 45.2 34.0 49.3 31.8 47.3 32.9 34.7 24.1 42.9 28.3 34.9 49.6 48.1 32.6 32.5 39.2 34.2 33.0 47.8 39.7 47.2 33.3 44.0 49.5 39.5 40.2 36.9 33.3 25.6 41.6 39.0 39.7 46.3 48.3 29.7 43.3 32.3 31.8 40.2 34.5 48.7 34.0 25.3 33.8 46.7 31.6 42.9 44.7 42.3 48.7 45.1 35.0 36.3 31.5 33.8 50.4 51.5 50.4 46.2 32. 46.5 56.4 56.6 49.7 61.2 57.0 58.7 57.2 48.3 43.2 43.2 54.0 64.6 62.4 63.5 55.7 57.9 33.6 55.0 61.1 58.9 37.8 41.9 46.8 35.2 65.5 62.5 64.3 63.2 26.5 45.2 37.2 47.2 60.5 57.5 58.6 59.5 56.8 49.7 63.1 46.0 65.9 66.2 62.5 47.0 67.5 43.5 68.6 44.4 55.6 26.3 56.2 42.3 31.7 64.9 63.6 44.6 44.6 50.1 42.0 46.7 67.0 58.1 60.6 45.0 57.3 66.0 54.0 57.1 54.7 44.5 27.6 53.6 59.0 47.2 65.0 60.9 27.3 60.1 44.3 44.2 55.1 45.5 66.2 48.5 25.9 55.3 65.3 42.5 59.3 56.2 58.4 65.9 61.3 48.3 49.5 32.8 44.9 66.7 66.7 66.5 62.9 44.9 43.7 58.6 58.0 50.9 61.5 60.8 61.0 60.0 50.5 43.7 45.1 54.1 64.7 58.1 58.3 51.1 53.5 32.1 58.8 59.8 60.1 35.1 43.6 47.6 33.4 66.3 65.3 66.6 58.4 25.9 46.8 38.3 48.6 63.4 52.2 62.0 56.5 60.5 48.5 63.5 48.0 67.5 66.5 62.1 48.4 68.1 43.8 66.0 46.3 52.9 25.1 59.1 39.1 29.6 65.0 64.4 45.8 44.8 48.1 43.6 47.1 65.8 60.4 61.2 44.3 59.9 67.5 55.6 59.4 50.8 46.4 25.5 55.0 54.8 47.6 65.2 59.0 26.7 62.5 43.4 44.8 56.3 47.3 67.3 49.5 25.9 53.5 64.9 42.2 58.2 57.4 61.4 63.8 63.3 49.9 49.4 32.4 43.8 68.2 68.6 68.1 64.2 44.2 41.1 59.1 60.1 52.1 62.8 62.0 62.8 61.5 50.3 44.5 44.9 55.6 65.0 57.0 57.0 48.5 54.4 32.3 59.0 61.7 63.0 34.2 44.0 49.2 31.1 67.2 65.1 67.0 57.8 25.3 48.1 39.2 49.1 64.7 50.3 62.5 54.1 61.2 49.2 64.3 47.9 67.6 66.5 62.3 49.1 69.3 43.6 67.5 45.8 51.8 24.6 60.0 36.9 28.7 66.7 65.9 46.5 44.7 48.8 44.2 47.4 66.3 62.3 63.3 44.5 60.5 68.0 56.2 60.2 49.1 46.0 25.8 55.9 53.2 48.7 65.9 60.0 26.3 63.0 45.6 45.1 57.8 47.7 68.8 48.6 24.8 53.4 66.4 42.7 58.0 59.6 62.1 66.4 64.1 50.6 48.9 31.1 45.2 68.5 68.4 68.2 64.8 43.8 46.8 51.1 51.5 48.8 52.2 51.7 50.3 53.1 39.8 37.2 38.1 42.5 51.8 53.1 54.3 45.5 51.5 37.1 52.7 53.7 51.1 37.6 36.6 47.7 47.0 55.2 55.6 55.7 53.7 26.2 40.6 35.1 40.6 52.5 44.5 51.8 49.6 51.0 48.4 55.7 40.5 54.3 54.5 54.4 43.3 54.9 37.6 54.1 43.1 51.8 26.3 51.8 31.7 44.0 52.0 53.5 47.9 39.2 52.1 45.5 41.6 54.9 52.4 54.7 47.4 53.4 54.9 51.1 49.2 49.1 37.9 27.6 49.2 47.0 47.6 53.7 55.2 42.0 53.1 42.7 46.4 46.4 39.8 55.9 43.4 27.9 46.0 53.1 35.4 52.6 51.6 52.4 56.6 51.7 44.0 46.9 41.2 42.9 56.1 55.6 55.4 54.7 47.0 46.2 55.8 55.9 52.0 57.5 56.9 52.9 56.9 40.8 37.8 41.0 43.7 55.3 57.5 57.2 47.9 55.2 35.9 56.7 58.2 54.8 36.7 36.6 49.2 46.8 62.0 60.8 61.6 57.2 24.3 40.9 35.5 41.0 57.1 48.9 54.9 50.7 55.1 51.3 59.3 41.5 59.6 60.3 59.1 44.1 59.7 36.6 58.4 44.9 54.2 25.2 55.5 29.5 44.8 57.3 57.4 48.6 38.5 52.9 46.2 41.6 59.4 55.3 57.4 48.3 58.2 61.5 52.8 53.4 51.2 38.9 25.1 52.0 49.2 49.7 57.8 59.6 43.5 56.1 43.8 47.4 50.7 41.4 62.2 43.7 25.7 45.4 57.9 35.8 57.6 56.8 57.4 60.9 55.1 45.3 49.7 41.9 44.7 60.8 61.6 60.5 59.7 50.0 47.0 55.2 57.9 52.8 57.9 57.3 52.6 58.8 40.8 36.8 40.3 43.5 56.1 58.1 57.3 48.4 55.5 36.4 57.4 60.5 54.6 38.4 37.4 49.4 45.6 63.5 62.4 63.9 58.3 25.6 41.4 35.9 41.0 57.5 48.5 56.1 49.0 55.9 50.4 61.2 42.2 60.9 61.3 60.1 44.2 61.2 36.0 60.4 43.5 54.7 25.8 57.0 30.3 43.7 59.1 58.4 49.4 38.6 52.8 48.1 42.4 61.9 56.2 60.1 48.7 58.9 62.4 52.1 55.1 49.8 39.6 26.0 53.2 48.6 50.3 59.4 61.0 43.2 56.5 42.8 47.6 51.6 41.9 63.3 43.9 26.0 46.1 58.5 35.7 56.8 56.5 59.6 61.5 55.4 45.9 49.8 42.5 44.9 61.8 64.2 62.1 61.1 50. 1 53.4 66.3 66.8 65.1 72.0 68.2 64.8 71.7 51.9 59.8 47.4 56.9 71.8 70.2 71.7 70.3 72.0 44.8 66.7 72.0 71.3 50.3 42.3 65.6 69.9 70.4 70.5 70.1 71.0 24.7 62.9 36.3 68.1 72.0 65.4 65.3 70.2 67.2 69.9 72.7 69.0 72.3 71.2 71.6 50.7 73.1 66.7 71.3 45.8 68.0 24.9 68.9 34.3 68.3 72.4 71.0 67.9 68.5 71.6 69.0 65.5 70.3 66.9 73.4 65.5 69.6 72.8 71.5 67.5 70.8 45.1 26.6 70.7 71.2 71.9 72.6 74.6 46.4 67.8 63.5 61.0 70.6 63.2 72.5 60.4 25.9 68.9 72.5 47.5 72.6 71.4 68.8 73.5 70.6 53.9 69.7 67.1 64.4 72.4 72.8 72.5 71.9 68.6 4o-mini 3 54.8 71.1 71.9 68.1 77.2 73.5 68.6 77.7 53.1 63.2 49.8 61.1 78.5 76.3 78.4 77.1 78.4 46.3 71.9 79.0 77.6 51.4 43.8 68.7 72.8 79.0 78.4 77.9 78.1 24.1 67.5 37.5 72.7 76.9 67.2 71.0 74.8 73.9 75.2 79.6 74.6 79.2 79.0 77.8 54.3 79.1 72.0 80.9 47.3 70.1 26.0 75.1 35.1 72.4 79.3 78.6 71.2 74.9 77.8 74.2 70.1 79.3 72.0 80.1 68.8 77.6 79.3 77.1 71.5 76.9 47.4 25.5 77.8 77.4 77.8 79.2 78.8 46.0 74.0 66.2 64.8 76.3 66.0 80.3 63.9 26.6 73.0 79.9 46.7 78.6 78.0 74.8 79.3 75.8 57.6 73.6 72.8 66.9 79.8 79.7 80.2 80.0 74.4 5 50.6 69.7 69.6 66.8 77.6 70.9 67.0 77.0 53.0 60.8 48.2 60.2 78.0 75.0 77.1 74.8 77.5 43.9 70.7 78.1 76.3 49.9 42.1 67.9 71.0 77.7 77.2 77.4 77.5 24.0 65.9 36.1 71.9 77.0 63.9 70.0 73.2 72.0 73.7 79.0 72.0 77.8 78.5 76.5 53.0 78.4 69.8 79.3 44.9 69.5 24.5 73.4 32.2 70.4 78.4 77.7 71.1 72.8 76.3 73.1 68.4 78.3 70.1 78.4 67.7 75.5 78.8 76.5 69.2 74.6 45.0 26.5 77.4 75.4 76.5 77.5 78.3 42.1 70.5 63.8 62.9 75.0 63.8 79.2 61.5 25.4 70.6 79.1 45.3 77.7 77.6 73.1 79.1 74.8 55.8 73.0 71.0 65.7 78.6 79.5 78.7 77.3 73. 25 A.7.3 Topic-To-Sentence Lang. Avg eng_Latn ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn bul_Cyrl cat_Latn ceb_Latn ces_Latn cjk_Latn ckb_Arab crh_Latn cym_Latn dan_Latn deu_Latn dik_Latn dyu_Latn dzo_Tibt ell_Grek epo_Latn est_Latn eus_Latn ewe_Latn fao_Latn fij_Latn fin_Latn fon_Latn fra_Latn fur_Latn fuv_Latn gaz_Latn gla_Latn gle_Latn glg_Latn grn_Latn guj_Gujr hat_Latn hau_Latn heb_Hebr hin_Deva hne_Deva hrv_Latn hun_Latn hye_Armn ibo_Latn ilo_Latn ind_Latn isl_Latn ita_Latn jav_Latn jpn_Jpan kab_Latn kac_Latn kam_Latn kan_Knda kas_Arab kas_Deva kat_Geor kaz_Cyrl kbp_Latn kea_Latn khk_Cyrl khm_Khmr kik_Latn kin_Latn kir_Cyrl kmb_Latn kmr_Latn knc_Arab knc_Latn kon_Latn kor_Hang QwenVL-2 2B QwenVL-2 7B InternVL-2.5 4B InternVL-2.5 8B Centurio-Qwen 4o-mini 38.6 56.7 29.9 39.6 46.6 47.5 45.7 49.9 46.3 32.3 41.2 26.0 46.0 48.1 31.2 48.0 43.3 46.6 37.0 50.1 41.5 32.2 38.7 42.6 38.1 30.9 46.2 44.2 31.1 38.9 39.1 28.8 43.3 26.3 50.8 39.9 47.1 51.9 41.8 50.8 31.0 30.3 41.1 33.6 52.6 52.9 32.0 33.4 24.9 40.4 46.2 42.3 40.2 29.3 36.0 31.6 42.3 29.6 53.3 43.4 33.0 30.3 32.3 32.1 51.2 40.2 34.0 40.2 32.6 46.0 39.9 41.1 51.5 44.3 28.0 29.6 39.8 53.4 34.1 53.6 46.0 52.0 27.4 30.8 31.6 30.3 36.1 34.6 32.8 39.2 30.4 45.9 33.3 29.9 32.4 31.2 37.9 29.3 34.7 28.0 33.1 33.6 48. 66.5 85.7 46.4 68.7 81.1 81.6 79.6 82.5 81.1 53.2 77.6 38.1 80.9 82.7 56.1 82.4 77.0 81.2 69.6 82.1 74.5 46.3 69.5 80.2 74.3 46.6 75.3 80.4 52.9 77.2 69.8 44.3 73.6 28.4 83.6 64.8 83.3 83.8 79.7 82.8 50.6 61.3 74.8 72.8 83.6 85.2 48.6 51.3 24.8 77.7 83.1 79.0 71.2 45.1 74.0 51.8 82.0 42.6 85.2 76.4 52.0 45.8 59.9 66.5 83.7 64.1 72.8 70.8 52.1 82.7 78.9 70.6 84.0 80.9 69.7 49.4 73.7 84.2 74.1 84.5 77.2 81.9 35.7 47.0 51.6 69.3 62.5 55.2 69.0 76.3 45.3 76.3 69.1 63.6 52.5 49.9 72.1 46.6 60.1 34.2 52.0 59.5 82.6 54.5 81.4 34.9 52.0 71.5 71.3 68.4 70.6 70.5 44.4 57.9 37.5 70.0 73.1 44.9 72.7 64.1 71.5 57.8 71.6 59.4 37.0 46.0 65.5 53.1 39.4 56.5 63.7 42.3 64.5 56.4 33.0 57.2 26.9 73.5 50.4 73.4 75.2 63.6 75.3 40.6 43.1 59.4 49.1 74.8 77.4 41.0 40.4 24.2 60.3 69.3 66.3 55.6 37.2 54.3 38.8 70.5 36.5 77.7 61.4 42.6 38.9 44.7 47.6 75.8 51.7 56.5 57.9 40.0 69.1 65.2 55.6 73.2 66.8 40.5 41.1 56.6 75.0 55.2 75.5 59.9 76.0 29.7 37.7 40.4 52.7 46.3 43.5 51.9 58.9 38.1 63.8 44.6 52.7 42.6 39.8 53.6 38.6 50.5 29.0 42.8 46.0 76.0 68.0 87.0 42.1 71.8 80.5 80.6 78.2 82.3 80.2 59.6 76.1 31.2 80.4 81.3 56.6 81.2 75.9 80.7 72.0 82.2 78.2 52.3 68.0 76.9 69.6 53.3 74.8 77.7 59.3 76.9 76.8 41.2 72.8 52.5 83.6 68.8 83.5 84.0 79.9 85.3 55.2 49.2 75.7 72.8 84.5 85.5 59.3 58.6 46.9 79.2 81.1 77.5 74.5 53.5 73.5 57.5 82.3 50.9 86.4 77.5 56.7 48.4 59.3 62.2 85.0 69.6 74.3 74.1 53.8 76.9 80.3 76.5 84.0 81.3 38.4 57.0 76.0 84.7 70.6 86.6 74.4 86.8 40.3 56.3 56.2 68.1 67.9 63.3 44.9 71.7 54.2 78.2 57.6 57.9 60.0 54.4 67.7 53.3 65.1 37.3 59.8 63.5 86.0 72.4 85.4 54.1 72.9 83.2 84.0 82.1 83.4 83.6 63.5 80.0 68.5 82.7 84.9 69.8 84.3 81.4 83.1 79.2 83.5 81.7 54.3 77.8 82.0 74.7 58.8 79.0 83.9 59.9 81.9 80.3 52.8 78.6 48.9 82.9 69.3 83.9 84.2 82.1 84.6 57.0 58.2 80.0 75.9 83.3 84.7 55.2 59.0 46.8 81.6 83.2 80.2 77.5 50.5 76.0 56.7 81.8 49.2 85.9 79.9 56.9 53.1 71.6 73.8 85.2 75.4 79.0 82.0 66.5 82.7 84.1 80.3 83.1 81.2 64.2 71.1 76.3 84.4 77.2 86.0 80.9 84.7 41.3 52.6 57.9 77.5 71.9 67.4 76.8 81.1 51.4 81.1 61.4 73.0 64.9 54.8 77.9 53.0 65.9 36.9 60.5 65.4 83.3 81.5 88.5 66.3 80.6 89.9 90.0 88.9 89.3 89.3 79.9 89.4 81.6 89.4 89.9 85.1 90.0 88.2 89.3 88.2 89.4 88.6 62.0 83.2 89.1 88.2 58.5 85.4 89.4 72.6 88.6 88.0 64.8 84.9 49.9 88.8 77.2 89.3 89.9 88.6 90.1 60.8 82.3 86.4 88.5 88.5 89.7 61.9 63.0 45.1 90.1 89.2 89.3 88.4 57.2 87.4 77.6 89.2 55.2 89.9 86.6 68.5 81.7 85.8 87.7 88.8 81.3 88.7 87.3 87.3 88.8 89.1 88.3 89.7 89.9 88.4 85.8 87.8 89.3 88.7 89.3 87.5 90.0 46.3 57.3 69.7 87.9 84.9 80.8 88.9 89.0 57.1 86.2 88.5 87.4 70.8 86.8 88.0 59.2 83.4 46.1 64.2 73.9 88.9 QwenVL-2 2B QwenVL-2 7B InternVL-2.5 4B InternVL-2.5 8B Centurio-Qwen 4o-mini 28.2 42.2 42.5 34.6 44.6 44.1 38.2 44.8 34.3 30.5 31.7 38.0 42.6 40.6 42.5 32.5 37.1 28.3 43.4 45.3 41.5 29.3 32.1 30.2 27.8 54.1 50.5 52.3 42.4 25.6 31.9 28.4 31.8 47.9 27.5 42.8 34.0 43.3 37.7 44.5 33.6 51.1 53.9 42.7 34.9 50.0 30.3 50.0 32.2 36.1 24.9 43.5 27.3 26.2 48.5 47.6 31.8 30.4 35.4 33.6 31.7 52.7 42.3 46.2 30.7 45.2 50.8 33.5 42.0 28.3 31.9 25.8 38.1 31.5 33.2 43.2 48.0 25.2 41.7 31.3 30.4 38.0 30.5 49.4 32.6 25.2 29.2 46.6 30.3 39.7 37.9 45.7 53.9 42.8 34.8 32.1 28.7 29.0 53.8 51.6 53.8 51.5 29.4 53.1 75.8 77.8 59.4 81.3 77.2 71.0 75.6 56.0 48.6 50.0 62.6 81.4 71.7 70.5 70.9 72.8 42.3 73.1 82.8 76.4 46.1 48.0 53.4 54.7 84.0 83.3 83.8 73.8 24.8 51.8 38.7 56.0 81.5 62.4 75.3 68.4 75.1 70.1 82.0 53.8 83.4 85.5 80.0 57.4 83.9 48.3 84.4 54.0 59.2 24.6 76.4 41.2 54.2 83.5 82.2 53.1 51.7 64.7 55.2 52.7 84.8 74.3 82.9 48.6 77.0 83.9 68.2 76.1 65.9 52.7 29.4 73.5 68.7 68.3 80.4 84.0 32.8 75.4 48.1 49.4 70.3 51.7 82.1 54.2 27.3 61.3 83.7 47.1 76.7 72.2 79.9 84.8 78.4 57.8 55.1 56.6 48.0 84.2 84.0 84.2 82.5 50.0 47.6 60.3 61.9 46.5 63.7 61.3 54.6 59.7 45.2 39.9 39.9 49.2 66.1 56.5 53.8 52.9 56.4 32.8 56.5 69.3 57.5 36.2 40.5 42.1 44.2 76.9 72.8 75.2 59.0 25.3 41.0 34.8 43.8 70.3 54.2 59.5 56.3 62.7 48.6 67.5 42.0 73.9 77.5 65.7 42.9 74.3 37.7 76.3 41.5 50.1 24.6 59.6 36.7 43.9 73.7 70.9 40.7 41.1 50.3 43.3 42.0 76.6 56.9 71.5 39.7 63.7 76.1 48.6 57.1 50.4 41.8 27.3 53.9 54.6 50.6 69.1 74.2 33.1 57.5 39.6 38.3 52.3 40.5 74.3 44.5 25.7 42.7 71.3 39.3 58.6 58.2 65.3 74.7 61.5 45.4 44.6 37.2 41.4 77.4 78.5 77.6 72.7 39.9 57.8 76.7 75.7 62.5 77.3 77.4 72.6 75.7 61.9 53.4 55.3 69.2 78.0 77.0 77.7 67.5 73.2 41.5 73.4 79.5 74.2 48.3 56.7 60.4 41.0 85.3 83.7 84.0 76.3 26.4 56.9 46.1 61.3 81.4 71.5 77.5 73.8 76.8 65.8 82.5 57.6 85.1 86.4 80.2 61.8 85.3 53.5 85.4 59.2 69.8 26.1 76.2 54.0 35.6 83.6 82.0 58.9 56.5 65.8 54.9 58.3 85.9 76.9 80.9 53.0 75.8 84.8 68.3 76.6 67.4 57.7 29.1 69.4 72.0 61.8 83.8 79.4 30.3 79.2 54.5 55.2 71.1 57.6 84.1 62.0 27.8 69.2 83.3 52.4 76.5 69.6 80.0 84.0 79.2 63.0 61.7 36.2 54.7 85.1 85.8 86.2 82.8 55.5 74.6 79.2 80.0 75.6 80.3 79.9 75.4 82.8 62.1 53.6 56.7 68.3 82.2 80.7 80.7 77.7 79.9 49.8 77.5 82.3 82.2 57.1 53.1 69.3 67.2 85.1 83.5 84.2 80.4 27.0 62.6 45.2 62.4 83.8 75.9 78.6 79.1 78.8 72.8 82.7 59.9 84.7 85.4 82.0 67.4 84.2 54.2 85.3 65.9 78.6 26.3 80.2 44.5 65.7 83.7 82.0 71.5 57.6 79.9 64.7 61.4 85.3 80.9 83.8 69.1 79.9 84.0 75.6 78.7 77.2 58.2 28.6 75.0 78.8 74.4 82.1 84.8 62.0 82.6 63.9 66.5 70.9 59.0 84.1 63.9 29.5 67.8 84.8 53.4 80.5 80.7 81.6 85.7 82.3 67.5 73.0 64.9 60.1 85.9 84.6 85.7 83.4 70. 80.1 86.4 85.1 81.2 89.6 85.9 83.4 89.0 69.1 76.5 63.1 74.7 89.5 87.9 89.3 88.0 88.6 62.8 84.4 89.2 89.5 68.2 58.9 83.1 86.5 89.0 88.4 88.7 88.3 26.6 80.3 51.5 85.2 89.0 86.4 84.3 88.4 85.4 86.6 90.0 87.0 89.3 89.1 88.1 68.8 89.5 84.1 89.2 63.1 84.8 27.6 87.7 52.2 86.6 89.8 89.0 86.1 85.4 88.7 85.0 82.7 89.2 84.7 89.3 82.6 88.1 89.6 88.7 87.4 87.5 66.0 31.7 88.8 87.7 88.0 89.4 90.3 69.6 86.4 79.7 77.4 87.6 81.8 89.4 78.7 29.3 87.1 89.7 62.9 88.9 88.7 87.7 89.7 88.4 70.2 86.1 87.5 80.7 89.5 88.9 89.8 89.3 85.8 Lang. lao_Laoo lij_Latn lim_Latn lin_Latn lit_Latn lmo_Latn ltg_Latn ltz_Latn lua_Latn lug_Latn luo_Latn lus_Latn lvs_Latn mag_Deva mai_Deva mal_Mlym mar_Deva min_Arab min_Latn mkd_Cyrl mlt_Latn mni_Beng mos_Latn mri_Latn mya_Mymr nld_Latn nno_Latn nob_Latn npi_Deva nqo_Nkoo nso_Latn nus_Latn nya_Latn oci_Latn ory_Orya pag_Latn pan_Guru pap_Latn pbt_Arab pes_Arab plt_Latn pol_Latn por_Latn prs_Arab quy_Latn ron_Latn run_Latn rus_Cyrl sag_Latn san_Deva sat_Olck scn_Latn shn_Mymr sin_Sinh slk_Latn slv_Latn smo_Latn sna_Latn snd_Arab som_Latn sot_Latn spa_Latn srd_Latn srp_Cyrl ssw_Latn sun_Latn swe_Latn swh_Latn szl_Latn tam_Taml taq_Latn taq_Tfng tat_Cyrl tel_Telu tgk_Cyrl tgl_Latn tha_Thai tir_Ethi tpi_Latn tsn_Latn tso_Latn tuk_Latn tum_Latn tur_Latn twi_Latn tzm_Tfng uig_Arab ukr_Cyrl umb_Latn urd_Arab uzn_Latn vec_Latn vie_Latn war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn 26 A.7.4 Sentences-To-Images Lang. Avg eng_Latn ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn bul_Cyrl cat_Latn ceb_Latn ces_Latn cjk_Latn ckb_Arab crh_Latn cym_Latn dan_Latn deu_Latn dik_Latn dyu_Latn dzo_Tibt ell_Grek epo_Latn est_Latn eus_Latn ewe_Latn fao_Latn fij_Latn fin_Latn fon_Latn fra_Latn fur_Latn fuv_Latn gaz_Latn gla_Latn gle_Latn glg_Latn grn_Latn guj_Gujr hat_Latn hau_Latn heb_Hebr hin_Deva hne_Deva hrv_Latn hun_Latn hye_Armn ibo_Latn ilo_Latn ind_Latn isl_Latn ita_Latn jav_Latn jpn_Jpan kab_Latn kac_Latn kam_Latn kan_Knda kas_Arab kas_Deva kat_Geor kaz_Cyrl kbp_Latn kea_Latn khk_Cyrl khm_Khmr kik_Latn kin_Latn kir_Cyrl kmb_Latn kmr_Latn knc_Arab knc_Latn kon_Latn kor_Hang QwenVL-2 2B 5 3 QwenVL-2 7B 5 3 1 InternVL-2.5 4B 5 3 1 InternVL-2.5 8B 5 3 1 Centurio-Qwen 5 3 1 35.1 41.9 29.2 36.5 44.1 44.6 43.1 40.3 43.2 30.2 36.8 24.8 43.1 44.3 29.6 43.9 42.8 43.3 35.2 41.8 40.5 28.4 37.6 39.7 34.6 27.7 39.9 40.6 31.5 39.8 39.8 28.4 38.2 23.6 42.8 35.8 43.2 42.3 38.6 42.7 30.4 30.7 37.8 31.9 41.9 42.0 29.6 29.7 23.6 38.1 39.6 36.8 34.5 28.3 33.6 29.9 37.6 27.0 43.2 39.1 29.6 27.7 29.6 30.1 42.3 36.4 34.5 37.2 29.2 41.6 41.9 38.9 42.8 38.1 30.2 29.2 36.5 43.8 30.1 43.1 38.8 43.4 26.9 28.8 29.2 28.3 35.9 33.3 33.4 36.9 26.5 38.2 31.9 27.6 30.4 28.5 35.8 28.4 31.5 26.5 29.9 32.0 40.5 36.8 43.1 30.4 38.0 45.3 45.6 44.1 41.6 44.6 31.7 39.7 25.0 44.6 44.8 31.5 44.5 43.7 44.9 36.9 43.0 42.4 30.8 40.0 42.3 39.2 30.3 41.6 42.8 33.3 40.4 41.8 29.5 41.3 24.7 44.7 38.1 44.3 43.9 42.7 43.2 33.4 31.9 40.2 34.6 43.1 42.7 32.9 32.3 23.9 40.2 41.2 38.5 37.4 29.0 35.0 32.8 39.7 28.7 41.7 41.3 32.2 30.0 31.5 30.8 42.3 38.6 35.8 38.5 31.5 43.6 41.9 41.3 43.2 40.0 31.6 30.7 39.0 43.2 32.2 43.8 41.4 44.4 27.6 31.6 31.0 29.7 36.2 35.1 35.2 39.9 28.0 41.1 33.8 29.2 32.8 29.9 39.3 31.0 35.2 27.2 31.6 34.9 43. 36.8 43.4 30.6 38.0 43.9 44.4 44.0 42.7 44.4 32.4 39.6 25.5 45.0 44.3 31.0 43.8 43.4 44.0 36.5 42.5 43.3 31.1 38.7 42.6 38.6 29.7 41.3 42.2 33.2 41.1 42.7 28.3 41.4 24.7 44.5 38.5 45.8 43.3 42.0 42.7 32.9 31.6 41.3 33.8 43.2 43.0 33.1 33.1 24.6 40.9 41.2 38.6 36.7 30.4 35.3 32.5 40.9 29.2 42.6 42.0 32.7 29.7 30.3 31.3 42.9 39.5 34.3 39.2 30.8 43.4 42.5 41.4 44.2 41.8 31.3 30.0 38.1 43.5 32.4 43.8 40.7 43.9 27.7 31.8 31.5 28.9 37.4 35.3 34.8 40.5 29.4 40.5 34.5 27.9 32.5 30.2 39.5 30.9 34.6 26.9 32.3 34.2 43.3 52.8 71.7 36.7 52.5 63.8 64.7 62.9 64.4 63.5 43.1 60.1 28.7 64.2 66.5 43.3 66.4 60.9 64.0 59.6 63.7 61.1 35.2 53.4 64.8 57.4 34.2 59.5 63.2 41.5 64.4 58.9 34.0 58.2 23.9 66.5 51.4 66.0 65.7 62.4 65.5 38.6 45.5 58.9 55.4 66.2 68.4 39.3 37.6 21.3 63.1 64.7 62.0 53.8 35.5 52.2 39.7 64.2 35.0 67.8 59.9 39.2 33.8 44.0 47.1 64.1 51.5 63.7 54.6 38.0 65.6 65.4 58.0 66.9 64.7 55.9 37.7 57.6 68.4 53.9 67.2 60.1 68.3 27.5 35.4 38.7 58.7 49.4 47.1 58.5 60.5 36.2 59.2 50.5 51.7 42.1 37.7 54.0 36.2 45.3 26.7 41.0 45.3 68.1 54.5 70.4 38.8 55.8 66.6 66.1 65.3 64.2 65.0 45.2 62.3 30.4 64.8 67.6 48.0 66.9 63.7 66.1 63.9 64.6 64.1 38.4 58.1 64.4 60.1 37.3 62.9 63.3 45.5 67.0 61.4 35.6 62.8 24.0 64.9 53.1 66.7 64.4 63.5 64.8 40.9 47.5 62.6 58.1 66.5 67.0 43.3 39.6 20.9 63.4 66.0 64.0 57.2 36.9 55.2 44.0 64.9 35.2 67.6 61.5 41.3 36.0 46.2 49.0 65.7 55.5 63.8 57.8 39.7 65.9 66.0 62.0 65.7 64.9 60.5 41.0 59.2 67.6 56.4 66.8 62.3 68.6 28.4 41.0 41.7 61.2 53.4 50.1 63.7 63.0 38.0 61.4 52.3 55.4 45.8 39.8 56.9 38.8 47.9 28.6 43.0 49.5 68.4 54.4 68.6 41.8 56.9 65.2 65.0 65.4 63.2 63.8 46.6 64.5 31.8 63.9 65.7 51.6 65.4 63.3 65.0 59.9 63.6 61.7 39.6 60.2 64.8 61.2 37.6 62.5 63.4 46.5 63.2 61.3 37.5 63.3 24.4 63.3 53.8 64.9 62.1 65.5 63.0 42.4 48.0 62.3 59.3 64.7 64.5 45.6 42.3 21.8 61.7 64.2 63.2 57.5 38.0 56.6 46.3 64.3 35.1 64.5 61.5 42.3 37.3 47.6 50.3 64.4 58.7 60.5 58.5 40.6 63.3 63.0 60.9 63.6 64.0 56.8 42.9 59.9 65.9 57.6 63.7 62.4 66.0 30.1 42.3 43.1 58.1 54.3 48.8 61.6 64.8 39.2 61.0 54.1 52.2 48.1 39.6 59.5 40.6 48.1 29.1 43.8 51.9 65.4 32.2 47.7 26.3 31.2 37.9 37.6 36.8 36.2 36.0 28.2 33.4 26.0 36.5 37.7 28.5 37.9 35.9 37.5 36.6 37.9 35.1 26.3 31.1 35.3 30.6 25.8 33.6 33.1 28.4 41.2 34.9 25.3 32.8 22.8 37.2 31.1 37.8 38.7 35.1 37.3 27.8 28.7 33.6 29.2 38.3 41.0 28.2 27.3 21.8 36.1 36.4 33.3 30.7 25.8 30.9 26.3 34.7 25.2 41.1 33.9 27.8 25.6 28.3 29.1 39.8 31.4 36.3 32.2 26.0 37.1 37.8 34.0 37.5 35.2 29.4 27.7 33.7 39.0 30.4 39.4 33.8 40.2 23.3 26.4 27.1 33.3 30.9 30.2 30.7 32.2 25.2 34.6 28.2 31.8 28.5 26.2 30.4 25.6 30.2 23.7 27.2 29.1 39.9 35.4 44.5 28.2 36.9 39.0 38.7 38.5 40.3 38.7 32.3 36.8 30.1 37.5 39.2 31.5 39.1 37.4 39.2 41.9 40.8 38.4 29.7 32.6 39.1 36.4 29.4 38.5 38.1 31.8 43.4 38.6 26.7 36.8 23.4 39.4 35.4 40.2 40.9 39.5 39.7 30.1 31.7 38.3 34.0 40.6 41.7 31.9 30.5 23.1 40.3 39.8 39.0 37.2 27.7 34.9 30.1 39.8 27.7 41.9 37.4 30.7 28.9 31.6 31.3 40.8 36.6 41.1 37.7 28.4 39.8 41.4 37.3 39.9 38.0 30.9 30.3 39.2 41.2 34.1 41.6 38.1 42.5 25.5 29.2 30.1 38.1 33.5 31.9 35.7 38.0 29.0 39.5 33.2 35.9 31.6 28.7 36.5 29.7 35.4 25.1 30.9 33.2 40. 36.6 43.0 30.9 38.2 39.9 39.9 39.0 40.2 39.5 33.6 38.8 32.4 38.9 40.0 33.2 40.1 38.0 40.2 43.8 40.4 41.8 31.5 35.7 41.3 38.2 31.2 39.4 40.1 34.2 45.9 41.2 29.7 36.3 24.9 39.2 36.7 42.2 40.3 39.1 41.1 31.5 34.4 38.5 35.2 40.5 41.5 32.9 32.2 23.3 41.1 40.3 39.7 37.0 30.4 36.0 32.2 39.6 30.1 42.8 38.7 31.9 30.7 32.8 32.6 40.8 38.9 41.4 38.7 29.6 39.5 43.8 39.8 39.6 38.3 33.2 32.5 39.9 39.8 35.4 41.5 37.5 41.7 26.3 31.6 31.0 39.6 35.7 34.1 37.2 40.1 30.9 38.9 35.8 37.4 33.6 31.4 38.5 31.6 36.4 26.8 32.3 35.6 40.7 45.8 66.2 29.7 45.5 54.1 54.7 53.2 54.3 53.3 40.8 47.9 26.1 53.2 55.2 37.5 54.8 49.9 53.9 45.9 57.7 54.3 36.8 43.8 49.6 42.9 34.1 49.7 50.2 41.6 50.9 53.5 29.2 49.3 33.3 57.5 44.5 56.2 57.9 54.3 60.4 36.4 32.9 47.6 46.3 58.1 60.8 38.4 37.4 30.3 51.9 54.0 49.4 47.0 35.3 43.9 37.3 55.7 35.2 62.6 50.8 37.5 31.9 37.7 39.1 60.1 44.6 51.7 46.3 35.6 50.4 56.6 52.8 58.0 51.9 28.3 39.0 50.2 59.7 41.3 60.4 48.7 59.0 29.4 37.8 38.2 44.5 42.9 42.4 31.1 42.6 36.4 50.5 36.0 41.2 42.0 37.3 40.9 36.1 39.4 27.8 39.8 41.5 60.5 51.8 69.0 31.8 54.1 59.6 60.4 58.6 60.8 60.3 48.9 55.8 27.8 59.8 59.8 44.4 59.6 56.1 60.8 51.4 63.8 62.3 43.1 49.3 55.7 48.8 40.8 58.2 54.4 48.2 58.1 60.1 30.9 54.9 35.6 63.4 52.5 62.4 63.2 59.6 65.4 43.5 36.7 55.2 53.2 63.1 64.6 48.0 45.1 31.6 55.5 60.9 59.0 54.8 41.1 50.6 46.0 61.2 40.7 65.7 58.8 45.6 39.3 42.5 44.6 64.8 55.3 58.1 56.4 43.3 56.8 61.6 60.7 62.5 57.4 28.2 45.9 59.4 62.7 48.3 64.7 58.0 65.4 32.9 46.3 45.2 48.1 48.2 48.9 32.5 49.2 41.8 59.2 40.5 46.6 49.9 43.6 47.5 43.2 47.8 28.5 47.1 50.7 65.1 51.7 68.7 30.7 54.6 59.7 59.9 57.6 60.6 59.8 48.7 55.9 28.4 59.0 59.9 44.4 59.8 55.6 59.8 52.4 63.6 62.3 43.5 48.9 54.8 48.2 40.3 58.3 54.4 47.9 59.4 60.2 30.0 55.8 36.4 63.3 53.4 61.2 62.6 59.9 64.6 43.2 36.7 53.1 53.7 62.3 63.1 47.0 44.7 32.3 53.9 59.5 58.4 53.5 41.0 50.8 47.3 60.6 40.3 65.7 58.8 46.0 41.1 43.3 44.5 63.9 55.8 58.5 57.0 43.5 56.1 62.1 61.8 63.3 56.5 26.8 46.0 59.7 62.2 48.0 62.7 57.5 63.3 32.5 46.2 45.2 49.4 47.6 50.0 33.3 47.7 40.5 59.2 39.3 46.8 48.7 44.0 45.9 42.7 48.3 28.4 47.5 51.0 62.9 29.2 35.3 26.8 28.5 30.5 30.1 29.8 31.0 30.2 28.4 30.4 28.6 30.0 30.8 27.5 30.9 29.2 30.0 32.3 30.2 31.6 26.7 29.9 31.0 29.2 26.1 29.4 31.0 27.6 33.0 30.1 26.2 28.9 25.7 30.7 28.2 32.3 31.6 31.0 31.1 27.4 27.5 29.3 28.5 30.7 32.3 26.7 26.9 26.0 32.9 31.2 31.0 29.4 26.6 29.2 26.7 30.4 26.2 32.8 28.8 26.8 26.1 28.1 29.5 30.8 28.2 32.5 30.8 27.1 30.9 32.3 30.3 30.6 30.7 27.5 27.9 29.4 32.3 30.2 31.6 30.7 30.0 26.3 26.7 26.9 30.8 27.8 27.7 30.2 30.6 26.8 30.2 28.0 29.4 28.3 26.9 29.5 26.6 27.2 25.1 27.1 27.3 30.5 30.3 36.1 26.5 29.4 33.2 32.6 31.7 32.4 33.5 27.9 32.3 27.7 33.3 32.8 28.2 32.7 32.3 33.3 33.2 32.7 33.5 27.1 30.7 32.5 29.5 26.8 31.6 32.2 28.0 33.9 32.5 26.4 31.7 25.5 33.2 29.2 33.8 34.1 33.7 32.8 27.6 26.7 32.0 30.5 32.9 32.9 27.8 27.7 26.1 33.0 33.4 31.8 31.9 26.4 30.5 27.2 32.3 26.3 34.0 31.1 27.1 26.6 29.1 29.8 33.9 29.6 32.2 32.7 27.8 33.5 33.8 32.2 33.2 32.8 28.4 28.5 31.3 33.2 29.7 32.9 32.8 31.4 25.9 27.4 27.8 31.7 27.7 28.9 30.4 31.9 26.6 31.1 27.4 30.5 28.1 27.9 30.0 27.0 26.7 25.4 27.3 28.6 31. 30.5 35.6 26.4 29.4 33.5 33.5 32.8 32.0 34.2 28.3 32.1 27.7 33.6 33.5 28.1 33.5 33.1 34.3 33.9 34.4 32.8 26.9 30.9 32.5 29.9 27.0 32.3 31.6 27.5 34.8 32.1 27.0 32.7 25.9 33.7 29.8 34.1 33.6 34.1 34.2 27.3 26.9 33.4 30.5 32.8 33.4 27.1 27.1 25.7 33.6 34.3 32.3 31.7 26.5 30.7 27.6 33.3 26.5 34.4 31.4 27.7 26.8 29.2 29.4 34.0 29.5 30.8 32.5 27.7 34.0 32.9 33.0 33.9 33.5 28.5 28.2 31.5 34.1 30.1 34.0 33.4 32.9 25.7 27.0 27.3 29.9 28.9 29.4 29.7 30.7 26.9 30.5 26.9 30.4 28.1 27.0 30.3 26.5 27.1 25.7 27.6 28.3 32.9 1 69.2 77.5 53.7 66.5 77.5 77.2 76.3 76.6 77.8 66.4 76.8 70.1 76.9 77.7 73.4 77.9 76.0 77.6 75.8 76.5 75.4 46.7 71.0 77.5 76.0 46.3 72.6 77.1 58.7 76.4 76.2 51.8 71.7 39.2 76.9 63.6 77.1 77.8 77.0 77.9 48.7 70.4 74.9 76.3 77.1 77.0 49.0 49.4 37.4 76.7 76.8 77.8 76.7 44.9 75.9 63.8 78.0 43.8 77.1 73.8 52.3 67.0 72.6 73.7 77.6 68.8 76.5 75.8 75.2 77.0 78.1 76.1 76.9 77.2 76.6 72.4 75.3 77.9 77.4 77.5 75.5 78.0 35.9 46.6 53.2 76.0 70.9 66.5 76.7 77.7 45.0 73.3 76.3 75.7 55.9 73.6 77.0 45.0 68.8 36.1 50.1 58.7 77.3 4o-mini 3 80.2 86.4 70.0 80.9 86.6 86.6 85.4 86.3 85.7 79.5 87.4 80.6 86.1 86.8 84.8 86.4 85.9 86.6 85.8 86.3 86.3 63.0 83.8 86.4 86.1 62.4 84.2 85.8 75.3 85.7 86.5 68.0 84.1 41.7 87.1 79.2 86.8 87.0 86.3 86.6 64.4 83.2 86.1 86.7 86.2 86.9 66.8 65.9 38.1 86.5 85.9 85.9 85.8 61.0 86.0 78.1 86.5 57.6 86.6 85.6 70.8 82.3 83.5 84.1 86.4 81.9 86.2 85.2 84.1 85.6 86.2 85.7 87.5 86.7 85.6 83.8 85.1 86.4 86.2 86.9 85.9 86.2 46.0 64.2 70.0 84.9 83.3 80.4 85.8 85.6 57.8 84.5 85.6 84.7 71.6 84.4 86.1 62.8 82.6 44.5 67.1 75.3 85.9 83.0 89.1 73.6 84.9 88.8 88.6 88.9 88.6 88.0 82.9 89.3 82.9 89.0 88.7 87.4 88.7 88.4 88.8 88.1 88.8 88.2 67.8 86.6 88.8 88.7 63.3 87.1 88.8 80.2 88.4 88.5 71.7 87.5 41.5 89.2 82.8 88.4 89.3 87.9 88.6 69.9 85.3 88.5 88.8 88.4 88.8 72.6 71.5 36.4 88.5 89.3 87.8 87.6 65.0 87.8 82.3 88.4 61.9 88.8 88.0 75.8 85.5 86.2 87.5 88.8 84.9 88.7 87.9 86.7 88.2 88.5 87.7 89.3 88.5 88.5 87.2 88.3 89.0 88.2 88.9 88.0 88.8 49.1 68.4 73.9 87.5 86.3 83.2 88.4 88.2 62.8 86.5 87.2 86.5 74.8 87.2 88.2 66.4 85.4 47.2 72.8 80.3 87.9 Lang. lao_Laoo lij_Latn lim_Latn lin_Latn lit_Latn lmo_Latn ltg_Latn ltz_Latn lua_Latn lug_Latn luo_Latn lus_Latn lvs_Latn mag_Deva mai_Deva mal_Mlym mar_Deva min_Arab min_Latn mkd_Cyrl mlt_Latn mni_Beng mos_Latn mri_Latn mya_Mymr nld_Latn nno_Latn nob_Latn npi_Deva nqo_Nkoo nso_Latn nus_Latn nya_Latn oci_Latn ory_Orya pag_Latn pan_Guru pap_Latn pbt_Arab pes_Arab plt_Latn pol_Latn por_Latn prs_Arab quy_Latn ron_Latn run_Latn rus_Cyrl sag_Latn san_Deva sat_Olck scn_Latn shn_Mymr sin_Sinh slk_Latn slv_Latn smo_Latn sna_Latn snd_Arab som_Latn sot_Latn spa_Latn srd_Latn srp_Cyrl ssw_Latn sun_Latn swe_Latn swh_Latn szl_Latn tam_Taml taq_Latn taq_Tfng tat_Cyrl tel_Telu tgk_Cyrl tgl_Latn tha_Thai tir_Ethi tpi_Latn tsn_Latn tso_Latn tuk_Latn tum_Latn tur_Latn twi_Latn tzm_Tfng uig_Arab ukr_Cyrl umb_Latn urd_Arab uzn_Latn vec_Latn vie_Latn war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn InternVL-2.5-4B 5 3 1 InternVL-2.5-4B 5 3 1 QwenVL-2.5-2B 5 3 1 QwenVL-2.5-2B 5 3 Centurio-Qwen 5 3 1 28.1 39.5 39.1 31.8 38.1 39.8 37.0 39.9 31.0 29.4 31.1 33.2 40.1 39.9 39.4 32.1 36.6 28.1 38.1 42.4 37.8 28.4 28.2 28.5 27.6 42.1 41.1 41.4 38.8 23.9 29.4 27.2 30.1 42.7 25.7 38.4 31.7 38.6 36.2 42.7 30.4 44.0 42.1 42.1 31.8 42.5 28.4 43.2 31.1 32.6 24.2 39.4 26.8 24.4 43.3 41.3 30.4 29.6 32.4 28.2 28.8 42.6 40.1 42.7 28.5 38.8 41.3 30.5 40.4 27.8 30.4 24.0 36.3 31.7 32.8 38.3 44.7 24.5 37.9 28.7 28.5 34.7 29.3 42.7 30.7 24.2 29.9 42.2 29.4 40.3 34.7 41.1 42.5 40.0 31.5 31.1 27.5 27.1 45.0 44.9 45.2 41.7 28.8 28.7 42.7 42.3 34.7 42.1 41.4 38.1 41.7 33.7 30.6 32.8 36.0 41.9 41.7 41.7 33.7 37.9 28.6 40.8 43.0 39.0 30.0 31.6 30.5 27.7 43.1 42.4 43.2 41.8 24.8 31.6 28.5 32.7 42.7 26.0 40.7 33.7 40.5 38.8 43.3 32.5 44.4 42.6 42.7 35.2 44.9 30.4 43.4 33.5 33.8 23.9 42.3 27.9 26.0 43.0 43.7 32.3 31.0 33.5 30.2 31.4 43.0 41.5 44.1 31.2 40.7 43.0 33.1 41.2 28.7 32.5 24.2 39.3 32.2 35.2 41.9 44.6 24.0 40.0 32.4 30.0 37.0 32.1 44.6 32.1 24.4 30.9 43.1 31.9 42.5 36.5 42.9 42.7 42.1 34.8 31.9 28.5 28.6 45.0 45.5 45.5 43.4 30.3 28.0 43.3 41.4 34.8 42.1 41.4 38.0 41.5 34.6 30.5 33.2 36.4 42.0 42.7 40.9 31.6 39.0 27.7 40.9 44.1 38.2 30.0 32.0 29.8 27.0 43.7 42.7 43.2 40.6 24.7 31.5 29.2 33.3 42.8 26.0 40.8 32.6 40.1 37.9 42.8 32.3 44.3 42.9 42.6 35.1 45.4 30.0 43.8 34.3 33.4 24.4 43.1 25.9 25.0 43.1 43.9 32.2 30.6 33.0 30.5 31.1 43.7 41.8 44.5 30.6 41.1 42.7 33.6 40.3 27.6 32.6 24.4 38.4 31.0 34.6 42.6 44.4 24.1 39.5 31.9 30.1 38.2 31.3 45.6 33.2 24.1 30.8 43.7 32.4 42.7 35.9 43.2 44.2 42.3 34.4 32.1 28.1 29.1 45.2 43.7 44.0 43.9 31.0 40.8 61.6 60.5 46.1 63.6 61.1 55.5 60.3 43.8 37.1 38.1 48.0 66.8 59.0 58.5 64.3 61.5 32.2 58.2 64.9 57.4 41.2 37.1 40.0 52.1 67.0 64.6 66.5 60.3 23.0 40.7 31.7 43.0 63.0 59.8 58.9 60.6 58.2 54.2 66.8 40.8 66.8 68.1 65.3 43.3 67.8 36.6 68.7 39.4 54.5 22.2 61.3 39.4 46.2 65.6 63.8 41.9 38.4 48.2 37.6 39.8 67.7 60.5 66.2 37.0 60.3 66.3 48.6 60.5 57.8 41.2 24.9 55.5 59.1 50.8 65.3 69.9 27.0 60.8 37.5 38.0 53.7 39.7 66.6 43.6 23.6 50.5 66.4 35.7 62.0 59.2 63.4 68.2 63.3 42.1 41.4 39.8 36.6 67.3 68.4 67.1 67.1 37.6 42.6 61.9 62.6 49.2 64.7 62.9 58.0 61.3 46.2 39.0 40.8 52.4 64.4 63.7 62.4 65.3 62.0 34.2 60.9 65.1 60.8 47.4 39.6 42.1 56.1 65.8 64.3 66.3 64.6 23.3 41.7 33.8 44.9 63.7 60.5 62.2 63.0 60.4 56.4 66.6 42.1 66.4 68.2 65.5 47.2 66.3 36.9 69.0 43.1 57.6 22.8 63.9 43.1 47.3 63.6 64.3 45.3 39.6 52.9 38.3 41.7 66.6 61.6 65.1 40.2 61.9 66.1 52.6 63.0 61.4 44.5 25.4 58.8 60.0 52.4 67.5 70.5 27.8 63.7 40.2 39.7 57.8 43.3 66.7 45.8 22.7 52.8 66.3 39.4 64.0 63.0 63.5 67.6 64.2 45.8 44.6 43.0 36.7 66.6 67.6 67.6 66.5 38. 40.3 63.1 62.8 51.0 63.5 63.2 58.7 60.3 48.4 40.0 42.6 53.2 64.4 62.2 61.2 62.7 59.3 36.4 61.1 64.2 62.4 47.0 42.6 43.1 56.1 64.4 62.8 65.0 61.0 24.7 42.7 35.4 46.6 63.3 61.4 61.9 60.2 59.3 56.1 64.7 43.2 63.2 65.8 63.9 48.0 63.3 37.6 64.6 45.7 57.3 23.5 64.3 44.5 45.7 62.1 62.8 48.0 40.5 54.8 38.4 42.5 63.6 62.1 63.5 40.0 62.2 63.6 54.7 62.1 58.9 44.7 27.0 60.2 56.1 55.6 67.4 69.6 28.2 64.3 41.1 40.8 61.0 45.3 64.8 47.3 23.3 55.1 65.0 41.0 63.8 63.4 62.7 64.6 64.3 46.3 45.0 45.5 37.9 65.0 65.1 65.7 64.9 38.8 30.2 34.8 33.5 28.8 34.7 34.2 31.3 33.5 29.3 26.2 26.4 29.9 35.4 33.5 33.7 34.1 35.1 24.3 32.9 36.0 32.8 29.3 26.5 27.1 30.7 38.8 37.2 38.3 34.7 22.8 27.6 25.4 27.5 37.7 34.8 34.4 35.6 33.9 30.6 37.7 26.9 38.0 40.3 37.8 28.3 38.9 26.0 40.6 27.6 33.6 22.8 33.5 26.6 31.5 37.5 36.3 26.7 26.5 30.9 27.3 26.9 41.4 33.2 37.0 26.3 34.7 38.5 28.9 33.5 33.9 27.2 23.4 31.0 35.9 29.4 38.5 41.4 25.1 35.4 26.8 26.3 30.4 26.8 38.7 28.1 22.7 28.7 38.5 26.1 35.9 32.8 35.6 39.9 35.6 28.4 28.6 25.0 26.0 39.1 38.4 38.4 36.7 25.7 33.4 37.6 38.6 34.0 39.2 38.4 35.8 37.5 33.0 29.4 30.7 34.6 39.7 37.6 36.9 38.1 38.2 27.0 37.4 38.7 36.2 31.0 29.9 29.4 33.6 41.8 40.7 40.6 39.3 24.4 29.4 27.9 31.1 40.1 39.6 38.9 42.2 39.2 33.7 39.8 30.9 40.1 42.1 38.3 32.4 41.0 29.2 42.3 31.2 36.8 23.9 38.1 28.3 35.4 39.8 39.9 30.8 30.2 35.2 29.5 30.5 41.9 38.7 39.7 29.8 38.8 41.4 34.0 35.6 37.9 31.7 24.9 35.8 39.6 33.6 41.6 42.9 27.9 40.9 29.2 29.3 35.5 31.2 41.3 32.4 24.3 31.7 41.4 29.3 37.4 38.0 39.5 40.3 40.4 32.9 32.5 29.0 29.3 40.5 39.8 39.5 39.8 28.9 34.3 38.5 38.7 34.3 39.5 39.6 37.2 38.3 34.6 31.5 32.2 35.6 40.0 40.9 39.5 39.0 40.4 28.9 37.8 39.7 37.8 32.6 31.2 31.6 33.9 41.2 40.5 40.7 40.9 24.6 31.7 28.7 32.5 40.3 40.0 39.8 42.7 38.7 37.6 41.8 32.2 39.6 42.3 40.8 34.4 41.0 30.2 42.2 33.0 39.2 24.5 39.6 29.1 36.2 40.2 40.1 32.8 31.9 36.8 31.4 32.0 42.1 39.3 41.2 31.2 37.9 41.4 35.1 37.8 40.2 33.5 25.9 37.8 39.7 35.8 41.3 43.1 30.0 41.9 31.1 30.7 37.5 32.9 40.9 33.2 24.3 34.1 43.4 31.3 40.2 39.5 39.2 40.2 39.6 34.8 33.6 31.5 31.3 39.8 39.1 39.9 38.6 30.6 39.4 49.6 51.0 41.8 50.2 50.4 46.3 49.7 41.7 36.9 37.7 44.8 51.6 54.9 54.5 47.0 49.2 27.8 48.9 52.6 50.0 35.0 35.9 36.3 30.8 59.4 56.1 57.8 50.8 25.3 38.7 33.2 41.2 55.7 47.0 52.0 51.3 51.2 42.1 55.5 38.1 60.6 61.8 54.7 41.2 58.8 36.0 61.7 38.5 46.6 24.1 49.8 40.5 28.5 56.2 53.6 38.2 38.2 40.0 35.4 38.4 62.0 50.5 53.0 37.1 49.0 59.6 42.6 50.9 42.4 38.9 27.4 42.5 49.0 38.0 58.7 53.5 25.6 54.2 38.5 38.6 42.9 38.9 56.4 40.9 25.0 43.6 56.3 36.2 52.3 45.3 54.4 59.5 54.7 40.0 42.0 28.0 36.6 60.1 61.1 60.0 56.6 37.4 44.1 56.9 59.0 51.1 58.5 58.6 56.0 56.9 50.8 44.5 46.2 56.7 59.1 60.5 61.6 50.8 55.8 30.1 55.5 58.5 56.9 37.5 42.9 42.1 33.8 63.4 62.3 63.1 58.3 25.7 47.2 38.4 48.5 62.0 52.8 62.3 58.9 58.7 44.0 60.7 45.9 65.1 66.7 59.4 49.3 62.1 41.5 66.2 48.1 53.3 24.6 58.1 48.6 29.7 62.0 60.4 46.3 46.0 46.1 40.2 45.4 65.8 57.9 59.4 44.4 56.2 63.4 49.0 57.7 47.8 47.4 28.9 48.9 56.2 41.1 63.1 56.1 25.9 63.5 44.8 44.9 51.5 45.5 61.0 49.1 26.4 47.5 62.0 43.8 57.4 52.1 60.8 64.1 62.2 49.7 49.5 29.3 42.0 65.6 66.1 65.6 61.3 43. 44.6 57.3 58.4 50.5 57.7 58.0 55.5 55.9 49.7 43.6 47.7 56.7 59.2 62.5 62.6 51.8 57.5 29.5 55.6 57.2 56.7 40.6 43.0 42.0 34.2 62.3 61.4 62.2 59.1 26.0 46.2 38.6 49.1 60.8 55.0 61.7 59.5 58.4 42.8 59.3 46.4 64.6 65.7 59.4 50.2 61.5 41.8 64.7 47.2 54.9 24.7 58.1 49.5 30.7 62.1 60.6 47.3 45.9 44.5 40.8 46.0 64.4 58.3 58.4 43.8 56.6 62.2 48.6 57.3 48.2 47.9 29.7 46.8 56.8 41.7 62.2 57.9 26.5 64.6 45.6 45.0 50.9 44.9 59.9 48.6 27.1 48.3 60.9 44.8 57.7 51.3 61.1 63.5 60.6 50.9 48.9 29.8 40.7 64.1 65.1 64.3 60.3 44.0 29.9 29.7 29.4 29.0 30.1 29.4 28.3 29.9 27.7 26.7 26.9 27.1 30.9 30.4 30.3 31.0 31.3 26.2 29.2 31.4 30.6 27.1 27.5 28.2 27.9 31.9 31.1 30.5 31.1 24.8 26.8 26.9 27.3 30.9 31.6 29.1 31.0 28.8 28.3 31.4 28.1 30.7 32.4 31.7 27.4 31.5 26.3 31.9 27.9 29.6 24.3 29.2 25.9 29.5 30.6 29.8 28.6 27.9 29.8 27.2 27.0 31.7 30.5 31.7 28.2 29.6 31.7 29.5 29.0 31.5 27.2 24.8 29.1 32.3 28.4 31.2 30.5 27.8 31.1 27.4 27.9 28.4 27.1 31.0 27.7 25.1 28.2 32.3 27.5 31.7 30.1 29.5 31.7 31.7 27.2 28.2 26.5 27.7 30.0 30.3 29.8 32.0 28.9 28.9 31.3 31.0 30.3 32.2 31.3 30.0 31.3 27.8 27.3 27.7 28.5 33.0 33.0 32.8 31.0 32.9 26.0 31.0 32.9 32.1 28.0 27.2 29.2 27.9 33.1 33.1 33.3 32.5 25.4 27.6 25.8 27.9 32.4 29.7 31.0 31.4 31.6 29.2 33.9 28.4 33.6 33.5 33.2 28.3 33.0 27.8 33.3 27.8 30.4 25.3 31.5 25.8 28.5 32.5 31.8 29.5 27.5 31.6 26.6 27.5 34.8 32.2 32.6 28.2 31.6 33.4 29.4 30.9 32.9 27.0 25.2 30.0 31.8 29.0 33.7 31.2 26.8 32.2 28.0 29.2 29.5 27.6 34.3 28.0 25.1 27.5 33.8 27.2 33.1 31.7 32.0 34.1 33.0 28.1 28.6 28.1 27.2 31.4 32.2 32.5 33.3 27.8 29.2 31.2 31.0 30.0 32.8 31.8 30.1 31.3 28.1 27.1 26.9 29.2 33.5 33.3 33.3 31.5 32.3 26.0 32.0 33.5 32.9 28.0 27.1 28.3 28.0 32.5 32.8 33.2 33.2 25.1 27.0 26.5 28.1 33.0 29.8 31.0 30.5 30.7 29.5 34.0 27.4 34.2 34.2 33.4 28.2 34.2 27.2 33.3 27.4 30.5 25.3 32.0 26.0 28.6 33.3 32.6 29.1 27.3 31.4 27.3 27.0 34.2 33.2 33.3 28.0 32.5 33.7 30.3 31.3 31.2 26.7 25.1 29.5 31.2 28.6 34.2 31.8 27.4 32.9 27.6 28.4 30.2 27.4 34.8 27.5 25.4 28.2 33.0 26.9 32.6 32.9 32.0 35.5 33.4 27.9 28.3 28.2 27.4 31.3 32.5 31.9 33.4 27.6 1 67.7 73.9 73.3 67.2 77.4 74.2 71.8 76.3 54.9 60.1 49.0 61.0 77.0 76.5 76.7 76.2 76.5 49.5 71.9 77.0 76.1 54.9 47.6 70.0 74.6 77.6 76.5 76.9 77.1 25.6 64.6 41.2 72.4 76.9 75.5 71.1 76.1 73.6 76.0 77.3 72.5 76.1 77.1 75.9 55.9 77.1 70.6 76.8 51.6 72.5 25.0 75.1 44.3 74.9 77.3 77.2 72.5 73.2 75.8 71.8 68.5 77.0 72.3 77.9 67.5 75.1 77.8 76.3 73.9 75.9 51.8 28.1 76.2 75.9 76.0 76.3 76.6 57.2 75.5 64.7 61.7 75.2 65.1 77.3 66.3 26.9 75.6 77.8 47.8 77.2 76.1 75.5 77.1 76.0 56.5 72.5 72.7 68.7 78.7 78.0 77.4 77.2 74. 4o-mini 3 77.8 85.0 84.7 81.6 86.2 84.8 84.0 86.5 70.4 76.8 66.9 76.5 85.4 86.2 86.2 85.1 86.0 65.3 83.5 86.4 85.5 72.0 64.0 82.8 84.1 85.5 86.3 86.4 85.9 27.6 79.1 53.3 83.2 86.4 85.0 84.1 85.7 83.6 84.8 86.7 84.5 86.5 86.6 86.4 73.1 85.9 83.2 86.2 67.3 82.5 25.9 85.7 54.8 85.0 86.8 86.5 84.2 84.4 86.1 83.2 81.4 87.1 84.8 87.1 81.0 85.6 86.2 86.9 85.0 85.1 68.6 32.5 85.4 85.6 85.8 86.1 86.1 69.3 86.2 78.4 76.9 85.7 80.8 87.1 79.8 28.7 85.3 86.3 65.6 86.5 85.5 86.0 86.5 86.2 72.6 84.3 84.8 80.8 87.1 86.7 86.5 85.7 84.1 5 79.5 86.9 87.3 85.5 88.2 87.9 87.0 88.3 75.9 80.2 73.6 80.0 87.6 88.3 87.9 87.1 88.2 68.8 86.5 88.2 88.0 77.7 69.0 86.0 87.5 87.8 88.9 89.2 88.2 28.8 83.6 57.8 86.5 88.2 86.6 87.3 87.7 86.8 87.9 88.8 87.3 88.6 88.9 88.3 78.5 88.3 87.3 88.6 72.9 86.1 25.7 87.8 56.6 87.3 88.4 88.4 87.3 86.9 88.2 86.8 85.6 89.2 87.4 89.1 84.7 88.1 88.4 89.7 87.4 87.6 72.6 33.0 88.4 87.9 88.0 88.3 88.0 71.4 88.5 83.2 81.9 88.9 85.1 88.7 84.1 28.9 87.5 88.2 71.4 88.3 88.1 88.4 88.8 87.7 77.8 86.2 86.9 83.7 89.0 88.4 88.8 89.0 87.7 27 A.7.5 Sentences-To-Topics Lang. Avg eng_Latn ace_Arab ace_Latn acm_Arab acq_Arab aeb_Arab afr_Latn ajp_Arab aka_Latn als_Latn amh_Ethi apc_Arab arb_Arab arb_Latn ars_Arab ary_Arab arz_Arab asm_Beng ast_Latn awa_Deva ayr_Latn azb_Arab azj_Latn bak_Cyrl bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn bul_Cyrl cat_Latn ceb_Latn ces_Latn cjk_Latn ckb_Arab crh_Latn cym_Latn dan_Latn deu_Latn dik_Latn dyu_Latn dzo_Tibt ell_Grek epo_Latn est_Latn eus_Latn ewe_Latn fao_Latn fij_Latn fin_Latn fon_Latn fra_Latn fur_Latn fuv_Latn gaz_Latn gla_Latn gle_Latn glg_Latn grn_Latn guj_Gujr hat_Latn hau_Latn heb_Hebr hin_Deva hne_Deva hrv_Latn hun_Latn hye_Armn ibo_Latn ilo_Latn ind_Latn isl_Latn ita_Latn jav_Latn jpn_Jpan kab_Latn kac_Latn kam_Latn kan_Knda kas_Arab kas_Deva kat_Geor kaz_Cyrl kbp_Latn kea_Latn khk_Cyrl khm_Khmr kik_Latn kin_Latn kir_Cyrl kmb_Latn kmr_Latn knc_Arab knc_Latn kon_Latn kor_Hang InternVL-2.5-4B 5 3 1 InternVL-2.5-4B 5 3 1 QwenVL-2.5-2B 5 3 1 QwenVL-2.5-2B 5 3 1 Centurio-Qwen 5 3 58.2 86.1 36.9 59.8 80.8 81.3 77.9 77.0 79.2 49.1 61.8 24.9 78.3 81.5 39.2 81.3 76.6 79.2 54.8 79.1 68.6 41.9 59.5 69.2 58.5 40.7 68.3 73.3 46.5 68.8 65.6 36.6 65.1 26.9 79.5 58.8 79.5 82.0 65.8 82.9 44.4 42.7 64.2 48.7 80.7 84.5 45.2 43.4 24.2 72.9 70.4 63.6 57.9 40.2 52.1 46.7 64.6 39.0 85.8 70.7 44.7 36.9 43.8 43.9 83.0 61.4 52.0 58.0 44.0 76.4 72.2 64.5 79.8 69.8 41.7 40.6 63.0 84.2 46.3 84.4 66.8 83.8 31.8 41.3 44.5 39.9 50.3 50.1 54.1 61.1 37.4 69.9 43.9 41.9 47.0 42.2 57.0 41.8 49.1 30.9 46.1 49.7 81.8 73.2 96.5 48.1 79.6 95.2 95.3 94.1 92.9 94.4 66.8 84.4 29.6 95.0 95.7 51.7 95.7 92.6 95.3 77.5 95.0 88.1 57.5 81.3 85.1 79.2 52.7 87.8 91.8 63.3 86.9 85.9 45.8 87.3 30.1 94.2 77.5 94.3 95.6 85.2 95.1 59.5 57.4 82.2 63.2 94.4 95.9 61.8 59.0 27.1 89.5 90.4 84.0 80.5 52.9 69.4 59.2 82.8 51.2 95.8 90.8 60.3 48.6 55.4 54.5 96.0 80.5 68.8 79.9 57.2 93.1 88.9 85.5 94.5 87.2 53.5 54.7 83.1 95.6 59.7 96.1 88.0 95.9 37.2 57.6 57.4 48.0 67.9 68.6 72.4 80.4 52.2 88.7 61.6 57.7 63.7 56.1 76.9 53.5 67.6 37.6 61.8 66.2 93.8 79.5 98.2 54.6 87.2 97.4 97.4 97.2 96.1 96.8 76.6 91.7 33.6 97.8 97.6 60.4 97.5 96.9 97.4 84.5 97.2 93.1 68.8 89.4 92.2 86.9 63.1 93.3 95.5 73.0 92.6 91.8 51.2 93.7 31.8 97.1 85.4 97.4 97.8 91.3 97.6 69.7 61.5 90.2 73.3 97.2 97.9 72.1 68.6 26.6 92.7 96.0 91.4 87.5 63.9 79.9 68.5 91.3 62.3 97.9 95.5 71.2 58.5 65.7 64.6 97.6 88.8 74.2 87.9 67.9 96.1 93.5 91.3 97.2 93.4 59.4 65.9 90.7 97.7 69.6 97.8 94.3 97.8 43.9 68.0 66.8 51.5 78.7 78.4 80.3 87.8 62.5 93.5 69.5 63.1 73.3 67.0 85.2 62.8 79.0 42.9 72.8 74.9 96.7 70.3 89.1 55.2 71.8 85.7 86.2 85.2 82.9 85.4 57.5 79.9 37.0 86.7 86.5 58.5 86.2 82.7 85.5 80.4 83.9 83.7 49.3 76.4 82.1 78.9 47.6 78.7 84.5 54.9 85.2 83.5 49.8 77.4 31.7 85.0 69.2 85.8 85.1 81.5 84.4 53.0 67.4 78.8 74.2 84.0 85.8 54.2 54.0 24.9 83.6 83.5 79.9 71.8 47.7 72.1 53.6 83.5 47.9 85.9 79.6 53.5 45.4 60.3 66.4 84.4 70.0 82.2 74.3 50.4 84.5 85.5 83.7 85.1 83.6 78.3 53.7 77.1 86.1 72.1 86.9 79.1 86.2 35.6 51.5 52.2 78.7 74.7 72.4 80.6 81.8 51.7 79.0 71.4 73.4 56.7 51.8 75.5 48.7 63.8 36.9 55.1 59.8 86.8 82.8 95.3 70.0 86.6 94.2 94.0 93.9 93.6 94.1 73.9 91.6 48.0 94.9 94.4 76.3 94.2 93.2 94.4 92.0 94.1 94.0 66.6 91.1 92.9 91.5 65.5 92.5 93.7 74.2 93.9 93.6 64.6 90.1 35.3 93.9 85.4 94.2 94.1 92.5 92.7 69.8 84.2 91.6 86.6 93.3 94.4 70.8 72.5 22.7 91.9 93.3 91.4 85.8 62.7 86.6 71.0 92.4 63.1 94.8 91.7 70.8 61.6 76.6 83.3 93.8 87.0 93.6 90.2 67.3 93.7 95.0 94.7 93.9 92.9 88.9 69.6 89.3 94.4 85.0 94.7 91.8 95.4 41.3 68.0 68.1 90.8 89.9 88.4 91.2 92.7 69.2 92.1 84.4 85.1 73.2 68.9 89.2 66.0 80.8 49.2 71.6 79.3 94.6 87.7 97.5 77.9 92.6 97.4 96.9 97.4 96.4 97.3 80.9 96.2 54.0 97.7 97.4 83.9 97.2 97.2 97.2 95.2 96.6 96.9 74.9 95.4 96.7 96.5 73.6 96.1 96.7 80.9 96.9 96.9 72.4 94.5 41.0 96.1 91.4 96.8 96.8 96.2 96.1 78.0 90.3 96.3 93.0 95.9 96.7 77.2 79.8 24.1 95.6 96.8 95.7 90.6 71.3 91.8 77.6 96.0 71.0 96.7 96.1 77.7 70.3 85.0 90.0 96.6 92.3 96.3 94.4 74.9 96.4 97.7 97.3 96.7 96.4 93.6 77.5 93.9 97.1 90.7 96.9 95.7 97.3 45.0 75.0 75.8 94.9 94.5 92.4 95.5 97.3 77.5 96.3 91.1 90.6 79.4 77.4 95.0 72.7 86.5 57.5 79.2 86.3 96. 70.8 90.6 52.5 74.6 88.0 88.6 87.5 84.3 87.6 61.5 74.8 49.5 87.9 88.9 59.8 89.0 84.7 88.2 79.6 86.8 82.7 51.1 72.3 77.6 73.4 51.4 79.8 82.4 59.4 83.7 81.9 47.9 76.9 33.4 86.2 71.9 85.8 87.7 80.0 87.3 55.1 61.5 77.4 64.7 85.7 86.8 58.4 57.3 28.7 81.2 83.1 76.5 71.7 50.9 71.0 57.0 80.0 52.3 88.8 81.0 57.5 50.4 56.6 59.7 88.2 74.7 80.7 73.9 52.0 86.0 85.4 82.0 86.1 81.3 59.1 56.7 78.5 88.1 67.6 88.0 78.8 87.6 37.5 55.2 53.6 74.3 72.9 71.7 69.0 74.8 52.2 81.7 61.6 70.0 60.9 55.1 72.1 52.0 66.8 36.2 58.8 62.8 87.5 85.4 98.0 69.0 91.4 97.1 97.2 97.2 96.0 96.9 79.9 91.3 64.3 96.7 97.3 79.2 97.4 96.1 97.4 92.6 96.9 94.9 71.0 90.2 94.2 91.1 71.1 94.6 96.4 79.5 95.3 95.4 62.5 92.8 39.5 96.6 89.2 96.7 96.9 93.1 97.3 73.3 80.1 93.7 82.2 96.8 97.3 77.7 76.8 27.8 95.0 96.6 92.7 88.1 67.6 88.7 77.7 95.1 69.5 97.3 95.1 78.5 70.3 75.0 80.2 97.2 91.7 94.4 91.4 71.9 95.5 96.4 95.7 96.8 94.8 74.4 73.1 93.8 96.5 87.3 97.2 93.6 97.0 46.9 75.0 71.8 90.6 89.9 86.9 88.3 92.6 70.3 95.2 79.5 83.0 79.1 74.0 90.1 70.5 86.2 46.3 79.8 83.6 96.8 89.5 99.1 73.6 96.2 98.6 98.7 98.7 97.9 98.3 86.8 95.3 71.0 98.1 98.6 85.4 98.6 98.0 98.9 96.3 98.7 98.1 79.0 94.1 96.6 95.3 78.7 97.6 97.9 85.3 97.6 98.1 67.3 96.5 44.1 98.3 94.9 98.3 98.5 96.7 98.7 81.0 86.2 97.0 88.2 98.6 99.0 83.3 84.6 27.0 97.8 98.2 96.6 92.6 76.6 92.7 83.0 98.5 77.4 98.7 97.8 84.5 78.9 80.9 85.0 98.7 95.5 96.9 96.2 77.9 98.3 98.2 98.2 98.4 97.7 80.5 80.0 96.7 98.6 91.8 98.7 96.8 98.7 53.4 81.5 79.1 94.4 94.1 92.3 93.2 96.2 77.5 97.8 84.8 87.3 85.3 81.4 95.4 77.8 90.9 51.7 86.4 89.8 98.9 69.6 91.7 42.3 74.4 84.9 85.5 83.5 85.8 83.2 61.4 77.2 28.2 84.8 86.1 56.6 85.8 80.1 83.7 71.9 87.9 82.3 50.8 71.2 79.0 72.2 52.9 79.5 83.4 60.0 79.5 80.5 39.6 77.0 48.6 87.7 71.7 87.7 88.7 83.3 89.5 54.0 47.1 78.0 74.2 88.3 88.5 59.9 58.2 41.2 82.8 86.0 78.5 73.8 51.5 72.8 56.6 85.2 49.3 89.4 83.2 57.0 45.2 57.5 62.0 89.3 73.7 77.4 75.0 53.4 78.8 83.9 80.3 87.5 83.4 38.7 54.8 79.2 88.4 69.7 89.6 78.3 89.4 40.8 54.6 55.6 70.4 70.4 65.1 45.8 73.3 55.7 83.0 55.0 60.1 58.9 53.8 68.2 52.2 63.8 33.0 59.2 62.7 88.3 83.8 98.1 50.9 92.6 95.9 96.5 95.3 96.6 95.7 82.2 94.5 33.8 96.0 96.7 76.1 96.3 93.8 95.7 85.2 97.9 94.6 70.0 86.5 94.3 89.7 72.5 95.9 94.9 81.3 93.0 94.6 44.5 92.0 53.1 97.8 89.7 97.4 97.6 96.1 97.9 74.7 57.2 93.4 91.5 97.3 97.6 79.8 78.1 41.7 95.6 96.9 94.8 92.7 71.8 87.1 73.6 96.7 70.2 97.9 97.2 78.3 63.7 76.0 81.0 97.9 91.2 92.4 92.8 72.1 93.3 95.6 94.7 97.7 95.7 42.0 74.4 94.8 97.6 83.4 97.8 93.4 97.7 49.3 77.7 75.9 83.5 83.2 81.2 52.9 90.7 74.7 96.1 70.7 76.9 78.9 73.6 85.2 72.4 84.8 40.7 79.4 83.2 97.7 88.4 99.0 54.3 96.9 98.5 98.5 97.7 98.9 98.2 89.3 98.6 35.5 98.1 98.5 84.0 98.4 97.1 98.2 91.0 99.3 97.9 78.2 91.6 97.9 95.9 82.1 98.6 97.6 89.2 96.8 97.9 45.6 97.1 56.9 99.2 95.4 99.2 99.2 98.9 99.2 84.0 61.4 97.4 97.0 98.8 98.8 86.7 87.3 43.3 98.7 99.2 98.1 97.3 80.9 92.9 81.8 99.2 80.2 98.9 98.8 86.4 72.9 85.1 88.7 99.0 96.7 96.9 97.9 81.8 97.2 98.4 98.5 99.3 98.5 42.6 82.5 98.6 99.0 89.5 99.0 97.8 98.9 53.8 86.0 83.7 89.1 89.9 87.9 56.6 96.7 82.7 99.1 77.5 83.1 84.6 83.0 91.7 79.8 91.4 41.4 87.3 91.9 99. 75.5 89.9 58.1 76.3 90.6 89.9 88.8 88.4 90.1 65.6 83.4 68.0 89.5 89.8 69.0 90.0 87.4 89.6 84.6 88.9 86.1 54.1 80.1 84.7 78.2 57.7 82.7 89.1 63.6 86.4 85.5 53.1 80.4 50.0 88.9 71.5 89.3 89.5 86.2 89.4 59.2 61.7 83.2 77.4 89.0 90.0 58.0 60.3 44.5 87.1 88.2 84.2 79.4 50.3 80.3 61.1 85.7 51.5 90.9 84.1 58.2 55.5 73.7 76.9 89.3 77.3 83.7 83.0 66.3 87.7 87.6 84.7 88.3 85.3 69.3 71.7 79.4 89.7 81.0 90.3 82.9 89.8 43.5 54.7 59.8 80.5 74.7 75.0 81.7 84.7 53.8 84.0 63.1 77.1 66.1 57.3 80.2 54.5 66.3 35.9 60.7 65.6 88.7 88.0 96.7 75.5 91.5 96.2 95.9 96.6 96.1 96.9 81.3 95.1 83.0 97.1 95.7 87.0 96.0 96.1 96.8 95.6 97.0 95.9 73.5 93.2 95.3 93.2 77.2 95.0 97.2 79.7 96.5 96.4 68.3 93.9 62.5 96.6 89.0 96.2 96.6 95.7 95.6 76.3 77.4 94.9 93.1 96.2 96.1 78.9 80.4 55.7 96.8 96.3 95.2 92.6 69.8 94.1 80.5 94.8 70.8 96.6 95.6 79.3 74.7 88.7 91.7 96.4 92.2 95.8 95.4 81.2 96.2 96.6 95.7 96.8 95.9 82.9 86.2 93.5 96.3 94.0 96.7 95.3 96.7 52.8 75.3 77.8 93.1 90.2 90.5 94.7 95.6 71.2 96.4 78.0 90.4 80.2 74.3 93.3 72.9 85.8 48.7 79.9 82.3 95.8 91.5 97.7 82.1 95.7 98.0 97.8 98.3 97.8 98.2 85.5 97.5 88.5 98.5 97.6 92.6 97.7 98.5 98.2 97.7 98.8 97.8 81.1 96.6 97.9 96.4 83.2 98.1 98.6 83.0 97.7 98.3 76.6 97.6 67.6 98.4 93.7 98.2 98.4 98.0 97.7 82.5 82.1 97.7 96.7 97.8 97.7 85.9 87.5 59.5 98.3 98.0 97.5 96.0 77.3 96.9 86.3 97.6 77.7 98.4 98.3 84.6 81.5 94.2 96.0 98.1 95.5 98.1 98.1 86.7 98.2 98.2 98.0 98.5 97.6 87.4 91.4 96.3 97.5 96.9 98.2 97.7 97.8 57.7 80.9 82.7 96.5 95.1 93.8 97.8 97.5 76.7 98.7 83.1 94.9 85.0 79.2 96.9 77.1 90.1 57.0 86.1 87.8 97.7 1 82.9 92.4 66.8 80.5 92.2 92.2 91.5 91.6 92.0 80.2 91.4 85.2 92.0 92.7 87.7 92.3 90.6 91.9 91.2 90.5 91.7 57.6 85.8 91.6 90.6 55.4 86.7 91.6 71.0 91.1 90.7 63.4 87.7 56.8 91.6 77.4 92.1 92.0 90.9 91.8 59.4 85.0 88.9 90.5 92.1 91.6 58.5 60.0 54.4 92.3 91.5 91.5 90.3 53.7 90.0 78.5 91.5 54.1 92.1 87.7 65.2 82.2 87.6 89.9 91.4 82.6 90.9 88.7 87.8 91.5 91.8 90.6 92.0 91.5 91.7 87.3 88.9 92.0 91.1 91.6 89.9 92.1 42.8 58.2 65.7 89.9 86.7 82.9 91.7 91.8 53.7 87.5 90.5 90.3 67.8 88.0 91.5 55.1 82.5 41.6 61.7 70.1 91.1 4o-mini 93.5 98.1 85.4 94.9 98.5 98.7 98.4 98.3 98.0 94.5 98.3 96.8 98.4 98.6 98.2 98.5 98.0 98.6 98.3 98.2 98.4 78.9 97.2 98.8 98.6 78.8 98.0 98.6 90.6 98.5 98.5 85.5 97.8 64.4 98.5 93.8 98.7 98.4 98.4 98.4 80.8 97.4 98.1 98.2 98.3 98.4 81.3 83.1 57.8 98.6 98.6 98.4 98.3 75.4 98.2 92.2 98.4 73.8 98.2 97.7 87.4 96.9 97.4 98.1 98.3 95.8 98.4 98.2 97.5 98.4 98.4 98.5 98.7 98.6 98.4 97.0 97.5 98.2 98.1 98.4 98.1 98.2 58.2 78.6 86.7 98.5 97.1 95.9 98.5 98.5 74.6 97.4 98.4 97.9 87.3 98.1 98.6 77.2 96.4 59.0 83.2 91.0 98.3 5 95.8 99.1 91.6 98.2 99.4 99.4 99.5 99.3 99.5 98.2 99.3 98.4 99.5 99.3 99.1 99.2 99.5 99.4 99.2 99.5 99.1 87.3 99.0 99.5 99.5 87.4 99.3 99.4 96.5 99.2 99.2 92.1 99.6 66.4 99.4 97.9 99.4 99.1 99.2 99.2 88.8 98.9 99.4 99.4 99.2 99.3 88.4 89.9 57.6 99.4 99.3 99.4 99.5 82.6 99.0 96.6 99.2 82.2 99.0 99.1 93.3 99.3 99.0 99.0 99.1 98.2 99.4 99.3 99.0 99.2 99.4 99.2 99.5 99.3 99.5 98.6 99.1 99.2 99.2 99.3 99.4 99.1 65.3 86.3 92.6 99.3 99.4 98.7 99.3 99.5 82.4 99.1 99.5 99.5 92.8 99.3 99.5 84.8 98.9 68.3 91.2 95.8 99.2 Lang. lao_Laoo lij_Latn lim_Latn lin_Latn lit_Latn lmo_Latn ltg_Latn ltz_Latn lua_Latn lug_Latn luo_Latn lus_Latn lvs_Latn mag_Deva mai_Deva mal_Mlym mar_Deva min_Arab min_Latn mkd_Cyrl mlt_Latn mni_Beng mos_Latn mri_Latn mya_Mymr nld_Latn nno_Latn nob_Latn npi_Deva nqo_Nkoo nso_Latn nus_Latn nya_Latn oci_Latn ory_Orya pag_Latn pan_Guru pap_Latn pbt_Arab pes_Arab plt_Latn pol_Latn por_Latn prs_Arab quy_Latn ron_Latn run_Latn rus_Cyrl sag_Latn san_Deva sat_Olck scn_Latn shn_Mymr sin_Sinh slk_Latn slv_Latn smo_Latn sna_Latn snd_Arab som_Latn sot_Latn spa_Latn srd_Latn srp_Cyrl ssw_Latn sun_Latn swe_Latn swh_Latn szl_Latn tam_Taml taq_Latn taq_Tfng tat_Cyrl tel_Telu tgk_Cyrl tgl_Latn tha_Thai tir_Ethi tpi_Latn tsn_Latn tso_Latn tuk_Latn tum_Latn tur_Latn twi_Latn tzm_Tfng uig_Arab ukr_Cyrl umb_Latn urd_Arab uzn_Latn vec_Latn vie_Latn war_Latn wol_Latn xho_Latn ydd_Hebr yor_Latn yue_Hant zho_Hans zho_Hant zsm_Latn zul_Latn InternVL-2.5-4B 5 3 1 InternVL-2.5-4B 5 3 QwenVL-2.5-2B 5 3 1 QwenVL-2.5-2B 5 3 1 Centurio-Qwen 5 3 1 38.6 72.5 70.2 50.9 67.2 72.1 60.4 71.0 49.4 41.9 45.0 53.5 67.8 67.2 66.4 49.0 60.6 34.5 65.7 76.2 63.0 36.7 43.4 40.6 36.4 83.0 78.5 80.1 66.1 25.2 45.8 35.8 46.5 79.4 32.0 64.2 47.3 70.7 57.2 77.8 47.0 83.8 84.2 76.3 48.7 79.8 39.9 84.8 45.1 53.5 25.5 69.0 40.5 28.3 79.9 75.9 46.6 44.5 45.2 42.9 45.5 85.4 70.5 77.3 41.1 68.0 80.0 46.1 72.6 35.0 47.1 25.9 58.4 43.3 49.3 65.3 81.9 24.2 67.6 43.0 42.6 54.7 44.1 76.4 48.7 25.9 40.3 80.3 41.5 65.9 55.7 75.3 83.0 68.7 50.4 45.8 32.7 38.7 85.1 86.5 86.3 79.8 40.3 52.4 89.3 89.4 67.4 87.1 90.6 80.0 89.3 66.7 54.6 60.8 74.1 86.4 86.6 86.2 61.1 80.2 42.8 86.8 92.7 83.9 51.8 57.4 51.7 44.1 95.8 93.2 94.2 87.0 28.0 60.7 46.5 61.9 94.0 38.2 85.0 60.9 89.4 78.0 94.5 61.7 95.4 96.1 92.4 65.5 95.3 53.0 96.1 60.4 72.1 26.1 88.9 55.1 33.5 94.1 93.0 61.3 59.0 62.9 54.2 59.3 96.5 89.9 93.2 53.1 87.8 94.4 60.0 90.0 45.1 62.5 32.4 76.3 54.4 68.5 84.0 94.7 27.7 83.9 58.7 54.7 72.9 58.9 92.5 65.5 30.1 51.9 94.5 55.0 82.2 72.7 92.6 95.9 87.1 66.5 60.6 44.4 51.4 96.6 97.1 96.8 95.3 52.8 59.0 94.3 94.8 77.3 93.8 95.2 89.7 94.6 75.9 63.5 69.6 82.5 94.2 92.9 92.0 67.6 86.7 48.7 92.3 96.6 90.2 58.8 69.1 63.1 46.7 97.9 96.0 96.6 91.5 28.7 69.9 54.0 72.5 97.4 40.5 90.8 64.7 94.4 85.7 97.3 72.3 97.4 98.0 96.1 76.2 97.8 63.5 98.3 70.5 79.2 28.1 94.0 60.8 35.0 97.4 97.2 71.3 69.6 71.4 66.3 69.5 98.3 95.2 97.2 62.6 93.5 97.2 70.5 94.7 50.1 72.9 34.5 84.9 59.1 76.4 90.6 96.9 30.5 90.3 69.1 65.4 82.7 69.1 95.6 75.6 31.5 55.8 97.3 65.7 89.0 81.0 96.4 97.9 93.1 77.5 70.3 51.8 61.3 98.0 98.5 98.1 97.4 63. 61.2 80.4 78.5 61.4 81.7 81.2 76.4 79.3 58.5 49.3 52.9 65.8 83.8 84.5 83.3 83.0 82.0 47.9 76.2 85.2 78.3 62.6 54.2 50.7 70.9 85.0 83.2 83.8 83.5 30.2 50.7 44.0 56.2 83.0 78.5 77.8 80.5 78.6 76.2 86.0 55.0 86.0 86.9 85.5 58.6 86.6 49.3 86.2 56.1 75.0 29.3 79.3 52.6 64.1 84.0 83.1 55.7 51.0 71.1 50.3 52.6 86.0 79.5 86.1 47.2 79.6 84.2 68.3 80.0 77.2 58.2 28.6 79.4 80.2 72.0 82.8 87.6 34.8 78.4 49.0 49.1 73.2 55.1 85.0 57.8 24.8 70.7 85.9 47.4 83.3 75.7 81.8 86.3 80.5 59.1 54.0 59.9 48.2 87.0 87.9 87.7 85.1 49.1 75.9 91.7 92.0 78.3 92.4 93.0 90.6 91.2 76.9 67.1 69.0 82.8 93.0 94.7 94.5 93.3 92.9 62.6 89.3 93.9 91.7 79.7 69.8 67.2 85.2 93.9 92.4 93.2 93.6 33.3 69.8 55.1 75.2 93.8 90.9 92.3 92.7 92.1 89.6 95.3 71.8 94.6 94.9 94.8 76.1 94.0 65.9 94.6 72.4 87.0 29.8 92.6 70.5 78.4 93.2 93.0 73.0 69.0 87.2 66.5 70.1 94.3 91.4 94.4 67.2 91.3 93.8 84.4 91.8 88.6 72.3 34.0 90.7 92.3 88.1 94.3 95.8 43.0 90.8 67.9 67.6 88.0 73.8 94.9 73.1 27.2 88.4 94.2 66.7 93.4 90.1 92.9 95.3 92.9 77.1 71.2 77.4 62.6 95.1 95.1 94.9 94.9 66.8 82.9 96.2 95.9 84.1 96.0 96.4 95.2 95.4 83.3 74.8 77.7 88.4 96.4 97.3 97.6 96.7 96.2 69.7 93.6 96.8 96.0 85.8 77.4 74.9 91.0 96.3 95.3 96.0 97.3 37.9 77.9 62.9 82.8 96.9 95.4 95.8 96.6 95.9 94.3 97.6 78.9 96.6 97.3 97.3 83.6 96.9 73.6 96.4 79.4 91.8 33.0 96.8 79.2 85.1 95.9 96.2 82.0 77.1 94.2 74.1 77.9 96.5 95.5 97.4 74.1 95.3 96.2 90.4 95.4 93.1 80.1 37.0 95.9 95.8 94.0 97.2 98.1 50.0 94.7 75.4 76.2 94.6 81.0 97.2 79.7 28.8 93.4 97.1 74.6 97.5 94.4 96.3 97.6 96.7 84.3 79.1 84.9 70.1 97.3 97.4 97.4 97.4 74.9 67.8 82.6 82.1 65.4 76.6 83.7 73.6 79.5 61.3 52.0 57.9 69.7 80.5 83.0 82.8 76.9 77.9 46.0 77.8 83.8 76.5 57.5 56.0 53.9 59.1 87.5 84.5 85.8 80.8 27.1 55.6 47.9 57.0 86.8 79.4 80.5 80.8 81.9 70.1 85.8 58.4 87.9 88.2 83.6 60.7 86.0 51.6 87.7 57.5 75.1 26.0 82.1 53.7 61.8 86.6 83.7 58.3 54.7 70.5 54.0 56.0 89.2 81.3 84.8 52.8 80.6 86.2 62.6 81.8 72.8 59.6 33.7 73.6 79.4 67.9 83.3 88.0 44.9 79.8 53.9 53.3 70.1 57.0 85.1 61.1 30.4 61.6 86.4 53.4 81.5 73.5 85.9 88.2 81.6 63.3 57.2 53.6 53.9 88.9 88.7 88.9 85.0 52.6 82.4 95.3 95.8 84.0 92.2 96.0 90.8 94.8 82.1 70.8 76.5 87.4 94.0 95.4 95.7 91.2 92.6 59.7 92.3 95.8 92.4 72.5 75.7 73.4 72.7 97.2 96.8 96.9 95.0 30.2 76.3 63.3 77.2 97.2 93.0 94.7 95.0 94.5 87.7 97.0 78.1 96.9 97.4 95.6 80.5 97.1 69.3 97.7 79.4 90.1 27.0 96.1 73.2 75.5 97.1 96.6 77.7 73.2 87.3 72.6 76.5 97.5 94.7 96.2 72.4 94.3 96.7 82.5 95.4 88.5 77.2 42.0 90.2 94.1 86.6 95.2 97.3 59.2 95.0 73.9 72.7 88.9 76.6 96.6 79.2 34.4 81.4 97.4 73.5 94.3 89.7 96.5 97.1 94.9 84.5 77.4 73.4 70.1 97.3 97.3 97.3 96.2 72.1 87.9 98.1 97.8 89.6 95.9 98.6 95.9 97.5 88.1 78.3 84.5 93.1 97.1 98.1 98.2 95.5 96.2 64.3 96.3 98.2 96.2 79.9 82.5 79.4 76.9 98.7 98.7 98.3 97.9 32.6 83.6 69.6 84.9 98.3 97.1 98.1 97.6 97.6 92.2 98.5 83.8 98.3 98.6 97.6 87.6 98.6 77.9 98.8 85.5 94.0 27.9 98.3 81.8 80.5 98.5 98.5 84.5 80.7 92.2 81.0 83.7 98.8 97.3 98.0 80.4 97.2 99.0 89.3 97.4 92.1 84.8 47.7 94.4 96.5 92.3 97.7 98.4 66.3 98.1 80.3 79.8 94.8 83.3 98.4 84.9 37.2 88.2 98.4 79.9 97.1 94.2 98.5 99.0 97.9 90.4 84.0 82.1 78.6 98.8 98.8 99.0 97.9 78. 55.3 83.0 80.8 62.7 77.7 83.3 74.0 81.6 61.0 52.7 56.2 70.6 80.7 81.7 82.3 69.1 76.7 36.2 76.6 84.7 76.4 46.5 55.1 52.4 40.6 88.3 86.6 87.9 78.4 27.5 55.0 47.0 60.6 87.6 72.5 81.5 76.5 82.2 66.9 84.7 59.1 89.0 89.6 83.6 59.4 88.5 51.6 90.2 57.2 74.5 31.9 80.9 51.9 33.8 87.2 85.9 57.7 54.1 69.5 51.5 57.2 90.1 81.9 85.6 53.3 80.1 87.9 66.8 81.9 67.8 58.6 28.3 71.4 74.5 62.8 86.8 83.5 26.5 81.7 55.2 53.8 71.2 58.6 87.5 62.0 24.9 72.6 87.7 54.3 79.7 70.2 87.1 87.8 83.5 64.0 60.5 33.9 53.6 90.0 90.9 90.7 86.7 54.0 72.5 95.5 95.2 83.8 94.9 95.8 93.5 96.2 83.0 73.4 78.0 89.5 95.2 94.9 95.1 85.5 92.2 39.2 92.9 96.5 92.4 57.1 76.4 68.3 51.7 97.4 96.9 97.2 92.7 30.1 77.5 61.9 83.1 97.6 85.8 96.4 90.7 96.1 82.8 95.7 78.3 97.7 97.6 95.3 79.2 98.1 71.0 98.2 77.3 88.3 27.4 95.1 75.8 35.8 97.5 97.1 77.2 76.3 82.9 68.4 79.2 97.8 96.3 96.5 74.6 94.7 97.3 86.2 95.8 81.9 76.7 36.2 88.5 91.5 78.2 96.6 95.2 31.2 95.4 76.8 74.5 87.8 78.8 97.2 81.2 29.8 88.6 96.7 75.1 92.3 89.4 96.9 97.2 96.4 84.9 80.6 39.9 72.2 98.0 97.7 98.1 97.4 76.2 80.7 98.7 98.4 91.7 98.2 98.7 97.9 98.8 90.1 82.3 87.4 95.5 98.1 98.4 98.6 90.2 95.9 38.4 96.8 98.9 96.9 63.8 85.8 73.5 56.8 98.8 98.9 98.9 96.6 31.7 86.2 69.2 90.3 99.1 91.3 99.0 95.3 98.9 88.6 98.1 86.6 99.1 99.3 97.7 86.5 99.0 80.5 99.4 85.5 92.4 29.3 98.5 84.0 38.3 99.1 99.2 86.9 85.1 88.5 78.4 87.3 99.1 98.8 99.0 83.4 98.3 98.7 94.0 98.5 86.7 85.7 42.6 93.5 96.3 84.1 98.7 97.8 31.7 99.1 85.0 83.3 93.9 86.9 98.8 88.9 32.4 94.5 98.6 83.2 96.4 95.6 98.6 99.1 98.8 92.7 88.3 41.5 81.1 98.5 98.3 98.9 98.8 84.2 77.5 83.8 82.7 75.4 84.2 84.3 79.5 86.3 63.3 54.7 57.9 70.0 87.2 85.4 85.4 80.6 84.0 49.6 80.4 87.8 85.9 60.5 55.4 72.2 71.2 89.4 88.0 88.9 84.3 24.9 61.3 48.7 64.1 89.0 81.4 81.4 83.1 81.7 77.0 86.9 60.6 90.3 89.9 86.8 63.6 88.2 55.2 90.2 65.3 81.5 29.1 85.6 50.6 68.2 88.4 86.2 72.1 60.6 81.7 65.4 62.6 89.5 84.8 88.2 68.8 82.6 89.5 75.7 84.4 80.9 60.6 27.8 78.9 84.1 75.2 86.0 90.9 64.0 86.1 63.1 68.9 73.4 62.7 88.2 64.1 27.9 73.1 89.6 55.4 85.6 82.4 87.7 90.0 86.5 65.5 72.7 66.1 61.4 89.8 90.3 90.4 87.8 71.1 91.4 95.7 96.1 89.8 94.9 96.0 94.3 96.7 81.4 73.3 76.4 88.0 95.3 96.6 96.5 94.0 95.1 64.3 93.8 96.2 95.8 79.2 76.5 87.6 84.7 96.3 97.0 96.7 96.3 30.3 79.1 61.4 79.1 96.9 94.1 94.6 95.0 94.8 91.6 96.2 78.4 96.4 96.4 94.9 82.5 96.3 73.8 97.1 83.2 94.0 28.7 95.8 72.1 82.7 95.5 95.7 88.2 76.5 93.7 82.0 79.5 96.5 95.7 96.5 85.7 95.2 96.7 91.5 95.5 92.1 79.4 35.6 93.5 95.5 90.7 96.3 97.3 81.1 96.6 80.7 84.8 89.9 78.8 96.0 79.8 33.7 87.0 96.0 75.1 95.1 94.4 96.4 96.5 96.0 85.0 87.6 84.8 78.2 96.1 96.3 96.0 96.5 86.0 94.8 98.0 98.2 93.3 97.2 98.0 97.4 98.5 85.2 79.1 82.8 92.8 97.8 99.0 98.7 97.4 97.7 69.9 97.0 98.2 98.4 87.1 83.4 93.0 89.7 97.9 98.7 98.4 98.1 34.1 83.9 66.7 83.8 98.6 97.2 97.5 97.3 97.7 95.4 98.3 84.8 97.8 98.0 97.1 88.6 98.2 79.4 98.4 88.4 96.7 31.6 98.5 82.7 87.3 97.4 98.1 92.5 81.4 96.4 87.6 84.2 97.7 98.5 98.2 91.3 98.0 98.3 95.6 98.5 95.3 85.0 39.7 96.5 97.7 95.1 98.3 98.6 86.4 99.1 85.8 89.7 94.1 83.9 97.6 84.8 35.7 91.7 98.2 81.0 96.9 97.3 98.7 98.3 98.5 91.1 92.3 90.1 82.4 97.1 98.0 97.5 98.0 91. 1 83.8 88.4 87.5 80.3 91.1 89.1 86.3 90.6 65.8 72.4 60.5 73.9 91.3 91.5 91.7 90.5 90.8 62.9 85.7 91.7 91.4 68.6 57.2 84.1 90.0 91.6 91.3 91.5 91.2 27.3 79.5 49.4 85.5 91.3 88.6 85.1 91.1 87.7 90.3 91.9 88.7 91.4 92.2 91.6 67.7 91.6 85.7 91.8 61.1 86.5 25.5 89.2 52.7 89.2 91.1 91.1 86.7 88.2 89.7 84.7 83.4 92.0 86.7 91.7 80.9 90.3 91.6 90.8 89.2 89.7 64.1 31.9 90.5 90.5 90.2 91.1 92.0 73.0 88.6 78.0 78.4 89.1 80.7 91.5 80.8 29.0 89.5 91.5 58.6 91.8 91.2 89.8 92.6 89.9 69.5 87.5 87.4 83.0 91.7 91.9 92.2 91.8 88.6 4o-mini 3 94.4 97.3 97.5 95.7 98.6 98.0 96.8 98.6 87.4 91.3 82.2 90.9 98.4 98.4 98.4 98.3 98.6 82.2 97.2 98.5 98.4 88.9 80.0 96.6 97.9 98.4 98.5 98.5 98.3 33.6 94.9 66.2 96.9 98.4 97.6 96.9 98.4 97.4 98.0 98.6 98.2 98.3 98.3 98.1 87.4 98.3 97.2 98.5 83.6 97.5 29.1 98.0 76.8 97.8 98.3 98.4 97.1 97.5 98.4 97.0 96.2 98.5 97.2 98.3 96.1 98.6 98.2 98.5 98.1 98.4 83.6 42.3 98.5 98.0 98.1 98.2 98.1 90.1 98.1 94.2 92.8 98.3 95.0 98.6 94.0 35.2 98.6 98.2 81.6 98.3 98.7 97.9 98.5 98.2 89.3 98.0 97.9 95.6 98.5 98.3 98.3 98.0 97.6 5 97.3 99.0 99.1 98.5 99.4 99.2 99.1 99.3 93.1 96.3 90.2 95.8 99.3 99.5 99.4 99.2 99.5 89.9 98.9 99.4 99.3 95.0 89.3 98.7 99.2 99.3 99.3 99.3 99.2 38.2 98.2 75.5 99.1 99.3 99.1 99.1 99.5 99.3 99.1 99.4 99.2 99.2 99.3 99.1 92.9 99.4 99.4 99.4 90.6 99.2 29.9 99.5 85.3 99.3 99.2 99.3 99.3 99.2 99.3 99.2 98.9 99.2 99.0 99.4 98.7 99.3 99.1 99.2 99.3 99.4 91.0 49.5 99.6 99.4 99.5 99.2 99.2 94.5 99.4 97.7 97.6 99.5 98.5 99.4 97.7 39.4 99.5 99.4 89.7 99.3 99.5 99.2 99.3 99.0 94.9 99.2 99.1 97.9 99.3 99.1 99.2 99.3 99."
        }
    ],
    "affiliations": [
        "Center for Artificial Intelligence and Data Science, University of Würzburg, Germany",
        "Language Technology Group, University of Hamburg, Germany"
    ]
}