{
    "paper_title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
    "authors": [
        "Ran Xu",
        "Tianci Liu",
        "Zihan Dong",
        "Tony You",
        "Ilgee Hong",
        "Carl Yang",
        "Linjun Zhang",
        "Tao Zhao",
        "Haoyu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 1 1 5 1 0 . 2 0 6 2 : r Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training Ran Xu1,* Tianci Liu2,* Zihan Dong3 Tony Yu4 Ilgee Hong4 Carl Yang1 Linjun Zhang Tuo Zhao4 Haoyu Wang5 1Emory University 2Purdue University 3Rutgers University 4Georgia Institute of Technology 5University at Albany Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, framework that jointly optimizes rubric generator and judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that RubricARM achieves strong performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings. Keywords: Rubrics-as-Rewards, Reward Modeling, LLM Alignment, Synthetic Data Date: 2026-02-03 Model Weights & Checkpoints: https://huggingface.co/collections/OpenRubrics/rubricarm Contact: ran.xu@emory.edu; liu3351@purdue.edu; hwang28@albany.edu 1. Introduction Reward modeling serves as the compass for aligning large language models (LLMs) with human intents, typically by generating scalar score or preference label to predict human preferences (Stiennon et al., 2020, Wang et al., 2024a). However, in complex non-verifiable domain, such as creative writing or open-ended instruction following, these scalar or pairwise judgments often fail to capture the multifaceted nature of response quality (Ying et al., 2025). To address this limitation, recent advancements have shifted toward rubric-based reward modeling, where models explicitly generate structured criteria to ground their judgments (Gunjal et al., 2026, Liu et al., 2025a, Pathak et al., 2025). By decomposing evaluation into interpretable dimensions, rubric-based models offer transparency and improve generalization across prompt-specific evaluation axes. Central to rubric-based evaluation is the availability of high-quality rubrics. To ensure rubric quality, earlier work has primarily relied on human-authored rubrics, which are expensive to produce and difficult to scale to large datasets (Arora et al., 2025). More recent approaches seek to automate rubric construction using LLMs (Viswanathan et al., 2025, Gunjal et al., 2026); however, these methods are These authors contributed equally to this work, order was determined randomly (by rolling die). Alternating Reinforcement Learning for Rubric-Based Reward Modeling largely prompting-based and rely on fixed, frozen models for both rubric generation and response quality judgment. Consequently, they do not update the models intrinsic capabilities to the target domain or the underlying preference distribution, limiting their ability to generate in-domain, preference-aligned rubrics. Moreover, even when learning-based components are introduced (Liu et al., 2025a, Rezaei et al., 2025), the rubric generator and the judge are treated as separate modules and trained independently rather than jointly optimized. This decoupled training pipeline prevents deeper integration between rubric construction and judgment, leading to suboptimal evaluation signals. Designing effective rubricbased reward models are still challenging. In this work, we propose Rubric-ARM, an end-to-end framework that jointly optimizes the rubric generator and the judge via alternating reinforcement learning (RL), enabling the two components to co-evolve and mutually reinforce one another during training. We formulate rubrics as latent actions that guide the reward model in recovering the underlying preference signal, and posit that improved rubric generation directly leads to more accurate preference predictions. To ensure stable joint optimization, Rubric-ARM employs an alternating training strategy that decouples the learning dynamics while preserving shared objective. Training alternates between (i) optimizing the reward model with fixed rubric generator to align with target preference labels, and (ii) optimizing the rubric generator with fixed reward model to produce discriminative rubrics that maximize prediction accuracy. key challenge of the alternating RL is the instability caused by simultaneous updates to both components. Our analysis reveals that early-stage exploration by the rubric generator can dominate the learning dynamics. To mitigate this, we first stabilize the reward model under fixed rubrics before optimizing the rubric generator. This alternating schedule reduces variance and ensures robust optimization. Our contributions can be summarized as follows: We develop Rubric-ARM, rubric-based reward model to produce high-quality rubrics and precise judgments. To the best of our knowledge, this is the first approach that jointly optimizes rubric and judging via RL. We introduce an alternating RL training algorithm that couples the rubric generator and judge through shared correctness objective, enabling mutual improvement while stabilizing optimization. We evaluate Rubric-ARM across diverse alignment settings (9 reward modeling and 6 policy benchmarks). Rubric-ARM outperforms strong reasoning-based judges and prior rubric-based reward models, achieving +4.7% average gain on reward-modeling benchmarks, and consistently improves downstream policy post-training when used as the reward signal. 2. Related Works LLM-based Reward and Judge Models. While Zheng et al. (2023) established the foundational utility of LLM-based judges. Subsequent research expanded the scope of reasoning to include chain-of-thoughts (Zhang et al., 2025), self-critiques (Ankner et al., 2024, Yu et al., 2025b, Mahan et al., 2024) or plan evaluations strategically (Saha et al., 2025). Liu et al. (2025c) explore inference-time reasoning for generative reward models. Recent studies (Chen et al., 2025, 2026, Whitehouse et al., 2026, Guo et al., 2025, Hong et al., 2025, Xu et al., 2026) leverage online RL to directly incentivize detailed reasoning, aiming to mitigate bias and enhance the accuracy of pointwise and pairwise scoring. Rubrics-based Reward Models. Recently, rubric-based approaches have emerged as promising 2 Alternating Reinforcement Learning for Rubric-Based Reward Modeling direction for LLM evaluation (Arora et al., 2025, Hashemi et al., 2024, Pathak et al., 2025, Aky√ºrek et al., 2025), alignment (Viswanathan et al., 2025, Zhang et al., 2026), and reasoning (Gunjal et al., 2026, Zhou et al., 2025, Huang et al., 2025). However, unique challenge lies in generating high-quality rubrics at scale. To address this, Li et al. (2026), Liu et al. (2025a), Xie et al. (2025) extract rubrics from pairwise comparison signals, while Rezaei et al. (2025), Zhang et al. (2026), Shao et al. (2025) dynamically generate rubrics by leveraging policy model outputs in an online setting. 3. Preliminaries We study rubric-based reward modeling in non-verifiable domains, where response quality cannot be directly validated against ground truth. The rubric-based reward model contains two parts, namely rubric generator and judge. The key components of Rubric-ARM are described as follows. Rubrics. We define rubric as structured set of evaluation criteria conditioned on prompt. Formally, specifies distinct let denote prompt, rubric r(x) = {ci} aspect of response quality (e.g., factual correctness, tone, or presentation). consists of criteria, where each ci i=1 For training rubric-based reward models in non-verifiable domains, pairwise preference dataset is , where is prompt, y(1) and y(2) are two candidate responses, and given as ùíü = {(xi, , )} = 1 means y(1) y(2)). Formally, the rubric {0, 1} indicates which response is preferred (e.g., generator œÄr generates rubric from the prompt as (2) (1) i=1 , while judge œÄj and rubric as predicts preference with the reasoning chain conditioned on the prompt, responses, œÄr( x; Œ∏r), (1) Learning Objective. We define the preference-correctness reward (c, o) œÄj( x, y(1), y(2), r; Œ∏j). R(o, ) = I[o = ], ] represents if the binary prediction extracted from aligns with ground truth where I[o = Denote Œ∏r, Œ∏j expected preference correctness under generated rubrics: as the parameter for œÄr and œÄj respectively, our goal is to learn (Œ∏r, Œ∏j) that maximize max Œ∏r,Œ∏j (x,y(1),y(2),o )ùíü rœÄr(x;Œ∏r) (c,o)œÄj(x,y(1),y(2),r;Œ∏j) [R(o, )]. (4) Since both (text) and c, (discrete decision with reasoning) are sampled actions, we optimize eq. (4) with RL. 4. Rubric-ARM: Alternating RL for Rubric Generation and Judging In non-verifiable domains, supervision is limited to pairwise preference feedback and rubrics are not leads to nondirectly observed. Simultaneously updating the rubric generator œÄr stationary learning targets and unstable optimization. As shown in Figure 1, Rubric-ARM addresses this challenge using an alternating RL scheme that decouples the updates of two components. and the judge œÄj 3 (2) (3) . Alternating Reinforcement Learning for Rubric-Based Reward Modeling Figure 1: The overall framework for Rubric-ARM. 4.1. Stage I: SFT Warmup and œÄr We equip both œÄj with basic rubric generation and judging capabilities via leveraging open-source datasets. Following the prior work (Liu et al., 2025a), we fine-tune on synthetic rubrics and judge trajectories derived from open-source datasets including UltraFeedback (Cui et al., 2024), SkyWork (Liu et al., 2024), Magpie (Xu et al., 2025b), and Synthetic Instruction Following (Lambert et al., 2025a). Both œÄr(r x; Œ∏r) and œÄj(c, x, y(1), y(2), r; Œ∏j) are trained with the standard next-token prediction objective. 4.2. Stage II: Alternating Reinforcement Learning Stage (SFT) warm-starts the rubric generator œÄr by imitating synthetic rubric generation and judging trajectories, but optimizes the two components independently and does not directly target preference correctness. We therefore optimize both components using alternating reinforcement learning. Specifically, training switches between (i) improving the judge with fixed rubric generator and (ii) improving the rubric generator with fixed judge, providing each component with clearer learning signal while preserving the same end objective R(o, and judge œÄj ). (i) RL for Judge œÄj Œ∏j to improve preference correctness under rubrics sampled from œÄr : with the current œÄr . With the rubric generator parameters Œ∏r held fixed, we update Jj(Œ∏j; Œ∏r) = max Œ∏j (x,y(1),y(2),o )ùíü rœÄr(x;Œ∏r) (c,o)œÄj(x,y(1),y(2),r;Œ∏j) [I[o = ]]. (5) This phase trains the judge to produce rubric-conditioned evaluations that recover the dataset preference. Since œÄr( x; Œ∏r) is fixed during judge updates, we cache rubrics to reduce sampling cost and stabilize optimization. For each training instance (xi, ), we sample rubric ri œÄr( xi; Œ∏r) once , and reuse it for multiple judge optimization steps, yielding the Monte Carlo estimate: (1) (2) , Jj(Œ∏j; Œ∏r) (2) ,o )ùíü, ri (c,o)œÄj(xi,y (xi,y (1) ,y (1) ,y (2) ,ri;Œ∏j) [I[o = ]]. (6) 4 Alternating Reinforcement Learning for Rubric-Based Reward Modeling In practice, we use shaped reward that augments the final correctness signal Racc = I[o = ] with format-based reward Rfmt that enforces valid judging trajectories (i.e., addressing each rubric criterion with per-criterion explanations, followed by an overall justification and final decision). The final reward for the judge œÄj is Rj = Racc + Rfmt. fixed, we update to prefer rubrics that lead the current judge to make correct decisions. Concretely, we maximize the . With the judge parameters Œ∏j with the current œÄj (ii) RL for Rubric Generator œÄr Œ∏r preference correctness under rubrics drawn from œÄr Jr(Œ∏r; Œ∏j) = rœÄr(x;Œ∏r) max Œ∏r (x,y(1),y(2),o )ùíü as: (c,o)œÄj(x,y(1),y(2),r;Œ∏j) [I[o = ]]. (7) Intuitively, œÄr learns to generate criteria that are discriminative for the given prompt and usable by the judge to recover the dataset preference. In practice, we approximate the expectation with single rollout by greedy decoding (t = 0), i.e., we generate one judging trajectory (c, o) per rubric and use the Monte Carlo estimate Rr = I[o = Optimization (alternating RL). Rubric-ARM alternates between optimizing Eq. 5 and 7. At iteration t, we run: (8) ]. rt œÄr( xi; Œ∏ t+1 GRPO(Œ∏ t+1 GRPO(Œ∏ r) (xi, ; {rt }, ùíü) , t+1 , ùíü) . ; Œ∏ Œ∏ Œ∏ (1) , (2) ) ùíü, , (9) (10) (11) Here we cache one rubric per instance during judge updates (since œÄr is fixed in that phase). In each phase, GRPO (Shao et al. (2024), details in Appendix A) updates only the active policy while keeping the other fixed. Notably, we alternate training by updating the judge before the rubric generator in each cycle. In Sec. 5, we provide theoretical analysis proving the benefits of this ordering. ), the judge defines conditional model pŒ∏j(o Connection to EM Algorithm. Our alternating optimization can be viewed as generalized EM procedure (Dempster et al., 1977) with rubrics as latent variables. For each preference instance (x, y(1), y(2), x, y(1), y(2), r), while the rubric generator œÄr(r x; Œ∏r) acts as an amortized variational distribution over the latent rubric (Agrawal and Domke, 2021). With œÄr maximizes the expected correctness (or log-likelihood) under sampled rubrics, analogous to the M-step. With œÄj increases probability mass on rubrics that , analogous to an amortized E-step. Because rubrics make the current judge more likely to recover are high-dimensional discrete text sequences, we use stochastic policy-gradient updates rather than exact posterior inference, yielding stochastic EM-style coordinate ascent scheme. fixed, updating œÄr fixed, updating œÄj 4.3. Policy Model Post-training with Rubric-ARM We use the trained rubric generator œÄr( q; Œ∏r) and judge œÄj( q, , , r; Œ∏j) to provide preference supervision for post-training policy model œÄœï(a q), where denotes the prompt and denotes candidate response. For any pair of responses (a, b), Rubric-ARM samples rubric œÄr( q; Œ∏r) and predicts preference label = JudgeŒ∏j (q, a, b, r) {0, 1}, (12) 5 Alternating Reinforcement Learning for Rubric-Based Reward Modeling where = 0 indicates and = 1 indicates a. Preference Optimization with Rubric-ARM. Given prompt q, we sample two rollouts from the current policy, a1, a2 œÄœï( q), and use Rubric-ARM to label which one is preferred via Eq. (12) and retain examples where the with the standard DPO objective (Rafailov predictions are consistent for both orders. We then update œÄœï et al., 2023) relative to fixed reference policy œÄref . For iterative DPO (Xiong et al., 2024, Pang et al., 2024), we repeat (i) sampling rollouts, (ii) labeling them with Rubric-ARM, and (iii) applying DPO updates for multiple rounds. (13) Online RL with Rubric-ARM. Following recent works on using pairwise judges to provide reward signals (Xu et al., 2025a), we also consider online RL where Rubric-ARM provides rewards for optimizing œÄœï . For each prompt q, we adopt the ReMax-style baseline construction (Li et al., 2024) by first generating deterministic reference response via greedy decoding, and then sample additional rollouts, a(0) = Greedy(œÄœï( q)) (t = 0), {a(k)} k=1 œÄœï( q). (14) (15) To mitigate positional bias, we query the judge in both orders under the same rubric r. Let denote the judge outcome for (q, a(k), a(0), r) and (k) {0, 1} for the swapped order (q, a(0), a(k), r). (k) {0, 1} We define the final reward for response a(k) as Rœï(q, a(k)) = 1 (I(o(k) = 0) + I(o(k) = 1)) . (16) 5. Theoretical Analysis We analyze the gradient variance to justify our training schedule. We compare two phases: Strategy (Judge Warmup), where we optimize the judge with pre-generated, reused rubrics; and Strategy (Rubric Generator Training), where we optimize the rubric generator against fixed judge. Setup. Let ur(r) = Œ∏r p(r) = P(o = variance as Var( g) = 2 E[ g] 2. log œÄr(r x) and uj(o r) = Œ∏j log œÄj(o c, r) be the score functions. Let c, r) be the judges correctness probability given rubric. We define the gradient 5.1. Variance Decomposition We first examine Strategy A. By freezing the rubric (reuse) during judge updates, we eliminate inter-rubric variance. Proposition 5.1 (Judge Variance under Strategy A). Conditioned on reused rubric r, the variance of the judges gradient estimator gA is solely determined by the judges binary classification uncertainty: Var( gA r) = p(r)(1 p(r))uj(o r) 2 . (17) 6 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Proposition 5.2 (Generator Variance under Strategy B). The total variance of the generators gradient estimator gB decomposes into: 2 [p(r)(1 p(r))ur(r) Var( gB) = ] (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (I) Multiplicative Reward Noise + Varr(p(r)ur(r)) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (II) Cross-Rubric Inconsistency (18) Interpretation. Term (I) represents the judges Aleatoric uncertainty amplified by the high-dimensional 2. Term (II) captures the optimization difficulty when different rubrics yield generator gradient ur different expected rewards p(r), causing the gradient direction to oscillate. 5.2. Variance Domination in Early Training We now derive the variance gap. Instead of assuming trivial gradient dominance, we postulate condition linking the generators exploration intensity to its gradient magnitude. Assumption 5.3 (Exploration-Gradient Sufficiency). We assume that during early training, the generators gradient norm is sufficient relative to the judges, satisfying the following exploration-dependent lower bound: ur uj > 1 p(r) 1 p(r) + C1 p(r) , (19) r[p(r)2ur(r) = Varr(p(r)ur(r))/ where represents the judges correctness probability (analyzed pointwise or in expectation), and C1 (0, 1) is defined as: C1 Remark 5.4. The condition in Assumption 5.3 is mild and physically justified. Active exploration (C1 > 0) introduces positive buffer, making the required gradient-norm ratio on the RHS strictly less than 1 and thus avoiding the need for the generators gradient to strictly dominate. Moreover, the judge and generator both produce comparable-length sequences over the same vocabulary (checks/prediction vs. rubrics), so their gradient norms are typically of the same order; the exploration buffer is enough to absorb small mismatches and satisfy the condition in practice. ]. 2 Theorem 5.5 (Strict Variance Domination). Under Assumption 5.3, the gradient variance of Strategy strictly dominates the expected conditional variance of Strategy A: Var( gB) > [Var( gA r)]. (20) This inequality establishes that the structural instability driven by exploration (quantified by C1 governing factor in the variance landscape, overriding differences in gradient magnitudes. ) is the Remark 5.6 (Implication for Training Stability). The variance gap derived in Theorem 5.5 justifies the proposed training schedule (We first train the judge, then train the rubric generator, and subsequently perform alternating training following this sequence.) by highlighting critical trade-off in Signal-to-Noise Ratio (SNR). The strictly higher variance in Strategy implies that generator updates are dominated by exploration stochasticity rather than the true gradient direction, risking optimization instability. In contrast, Strategy acts as variance reduction mechanism: by fixing the rubric, it effectively sets the exploration coefficient C1 0 locally, isolating the judge from structural noise and providing stable target for effective learning. 7 Alternating Reinforcement Learning for Rubric-Based Reward Modeling 6. Experiment 6.1. Datasets and Experiment Settings Training data. We train the two components of Rubric-ARM, the rubric generator and the judge, on the general-domain portions of OpenRubrics (Liu et al., 2025a). The dataset is split equally into non-overlapping parts, and each rubric-judge alternating round is run on single part. During training judge, we randomly shuffle the order of response candidates to be evaluated; as shown in App. D.2, this practice greatly helps reduce position bias in reward modeling. Backbone and variants. Both the rubric generator and the judge are fine-tuned from Qwen-3-8B (Team, 2025). At inference time, Rubric-ARM follows the two-stage rubric-judging process, as detailed in Sec. 3. We also report ensemble results voting@5, by aggregating five independent judges via majority voting. Baselines. For reward-model evaluation, we follow Liu et al. (2025a) and compare Rubric-ARM against strong same-scale white-box judges, including JudgeLRM (Chen et al., 2025), RRM (Guo et al., 2025), RM-R1 (Chen et al., 2026), and Rubric-RM (Liu et al., 2025a) (SFT-only rubric generator + judge). We also report judges using black-box APIs when available. To isolate the benefit of rubric-aware training, we include training-free baseline, Qwen-3-8B (Rubric+Judge) (Yang et al., 2025), which directly generates rubrics and judgments via prompting. For policy training, we use RubricARM as the reward model to fine-tune Qwen2.5-7B-Instruct (Qwen et al., 2025) and compare against Skywork (Liu et al., 2024), ArmoRM (Wang et al., 2024b), UltraFeedback (Cui et al., 2024), RLCF/AI Judge (Viswanathan et al., 2025), OnlineRubrics (Rezaei et al., 2025), and Rubric-RM (Liu et al., 2025a). Evaluation benchmarks and metrics. We evaluate Rubric-ARM as pairwise reward model on widely used alignment benchmarks: RewardBench (Chat/Chat-Hard) (Lambert et al., 2025b), RM-Bench (Liu et al., 2025b), PPE-IFEval (Frick et al., 2024), FollowBench (Jiang et al., 2024), InfoBench (Qin et al., 2024), IFBench (Peng et al., 2025), RewardBench2 (Precise-IF/Focus) (Malik et al., 2025), Arena-Hard (Chiang et al., 2024), AlpacaEval 2 (Dubois et al., 2025), Creative Writing Benchmark v3 (Paech, 2025), WildBench (Lin et al., 2024), and WritingPreferenceBench (Ying et al., 2025). For FollowBench and InfoBench, we convert the original single-response setup to pairwise evaluation by sampling two responses from the same model (Qwen-3-8B/14B) and using the benchmarks verifier to identify constraint violations. We follow each benchmarks official splits and scoring rules, reporting accuracy, win-rate, or the benchmark-specific metric. 6.2. Performance of Rubric-ARM Table 1 compares Rubric-ARM against broad set of judge/reward models. Rubric-ARM achieves the best average performance among all white-box methods, improving Rubric-RM from 70.1 to 74.8, and reaching 76.2 with voting@5. These gains are consistent across both instruction-following and preference-style benchmarks, supporting our key contribution: Rubric-ARM learns more discriminative rubrics and more reliable rubric-conditioned judge through RL. Notably, Rubric-ARM also substantially outperforms API-based judges (e.g., 76.2 vs. 71.3 for Rubric+Judge API and 64.9 for direct Judge API), indicating that explicit rubric-conditioned learning yields stronger and more stable evaluation signal than black-box judging. 8 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 1: Comparison of different judge and reward models across multiple benchmarks. RewardBench2 reports results on Precise IF, and Focus dimensions. Rubric API uses GPT-4.1-Mini, and Judge API uses Gemini-2.5-FlashLite. Best results are highlighted in bold. RewardBench IF Evaluation Benchmarks RM-Bench RewardBench2 Chat Chat Hard FollowBench PPE-IFEval InfoBench IFBench Chat Precise IF Focus HelpSteer3 Avg. Black-box LLMs (For reference only) Claude-3.5-Sonnet Gemini-2.5-Flash API (Rubric+Judge) API (direct Judge) 96.4 95.0 79.6 89.6 74.0 83.3 79.2 71. Larger White-box LLMs (For reference only) RM-R1-14B (Qwen-2.5-Inst) RM-R1-14B (DeepSeek-Dist) RM-R1-32B (Qwen-2.5-Inst) RM-R1-32B (DeepSeek-Dist) RRM-32B White-box Judge/Reward LLMs RM-R1-7B (Qwen-2.5-Inst) RM-R1-7B (DeepSeek-Dist) RRM-7B JudgeLRM-7B Rubric-based Methods Qwen-3-8B (Rubric+Judge) Rubric-RM Rubric-RM-voting@5 Rubric-ARM Rubric-ARM-voting@ 73.5 90.3 95.3 95.3 94.7 83.0 85.3 77.7 92.1 73.9 88.2 89.9 89.4 90.3 79.8 78.9 80.3 83.1 81.1 70.0 67.3 69.5 56.1 63.6 74.1 75.4 79.6 80. 86.0 83.2 81.7 84.0 89.9 84.9 89.2 85.7 56.3 69.7 65.5 79.8 63.0 76.1 81.5 85.7 87.4 58.0 75.0 61.0 59.2 59.0 61.2 60.4 63.2 60. 55.2 51.0 51.0 46.0 53.8 67.0 70.8 70.8 72.0 85.6 82.2 72.9 85.5 82.4 86.1 85.0 84.4 71.3 70.3 68.2 62.7 74.6 80.8 83.8 86.1 87. 69.3 66.2 60.4 60.8 59.0 60.4 58.6 60.8 55.2 56.5 53.2 47.5 55.6 65.4 67.1 65.9 67.1 62.5 78.5 67.9 67.2 73.2 71.4 75.3 74.2 73. 64.2 62.2 59.9 55.4 64.2 65.7 67.0 69.2 69.1 38.8 57.5 42.5 13.2 23.8 30.6 33.1 36.9 34.4 20.6 13.8 10.0 9.4 21.9 34.4 40.0 41.9 46. 87.0 84.1 79.6 63.4 84.6 79.0 84.2 79.2 83.6 76.2 55.4 60.4 29.1 56.6 82.2 86.5 89.4 90.3 70.6 71.4 70.3 74.8 74.6 72.9 75.6 75. 65.2 62.6 62.4 60.2 61.8 67.0 67.5 69.8 71.1 - 78.5 71.3 64.9 69.9 71.7 73.3 74.0 73.4 61.7 59.4 57.8 53.8 58.9 70.1 73.0 74.8 76. Figure 2: Performance of different judge and reward models on WritingPreferenceBench. We further assess generalization on WritingPreferenceBench (Ying et al., 2025), shown in Fig. 2 (detail results are shown in Table 12), which serves as an out-of-distribution benchmark since none of the compared reward/judge models are trained on this domain. Despite this distribution shift, RubricARM remains strong and achieves the best overall score among all methods (63.2), outperforming Rubric-RM (60.3) and strong reasoning reward models such as RM-R1-Qwen2.5-7B (59.8). The improvements are broad across diverse writing genres (e.g., Functional, Promotional, Non-Fiction, and Poetry), suggesting that Rubric-ARM learns rubrics that capture transferable criteria beyond the training domains, thereby providing robust reward signal with improved OOD generalization. 9 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 2: Ablation study about the effectiveness of the format reward and the order of judge optimization and rubric generator. Best results are highlighted in bold. RewardBench IF Evaluation Benchmarks RM-Bench RewardBench2 Chat Chat Hard FollowBench PPE-IFEval InfoBench IFBench Rubric-ARM switch opt Rubric-ARM switch opt-voting@5 Rubric-ARM w/o format Rubric-ARM w/o format-voting@5 Rubric-ARM Rubric-ARM-voting@ 93.2 94.0 89.8 91.5 89.4 90.3 76.3 76.5 78.7 78.5 79.6 80.7 85.9 89.1 87.1 88.2 85.7 87.4 67.3 67.8 69.2 70.2 70.8 72.0 84.1 85.0 86.1 87.7 86.1 87.7 64.6 64.6 64.3 65.1 65.9 67. Chat 69.5 69.8 69.5 69.7 69.2 69.1 Precise IF Focus 24.4 39.4 25.6 43.8 41.9 46.2 86.1 90.1 84.8 88.9 89.4 90. HelpSteer3 Avg. 71.8 72.4 70.8 71.1 69.8 71.1 72.4 74.9 72.6 75.5 74.8 76.2 Table 3: Comparison of trained policy models with different reward models on format-based constrained instruction-following benchmark (IFEval) and an open-ended benchmark (InfoBench). Baseline results with \"\" are from Viswanathan et al. (2025), Liu et al. (2025a). Results with underlines are reproduced by us using official checkpoints and evaluation scripts. Best scores are in bold. Model IFEval (Prompt) IFEval (Inst.) IFEval InfoBench Loose Strict Loose Strict GPT-4 (0314) AutoIF (Dong et al., 2025) UltraIF (An et al., 2025) RAIF (Qin et al., 2025) Qwen2.5-7B-Instruct + SFT (Distilled) + DPO (via Skywork) + DPO (via ArmoRM) + DPO (via Ultrafbk.) + DPO (via AI Judge) + DPO (via RLCF) + IterDPO (via RLCF) + DPO (via Rubric-RM) + IterDPO (via Rubric-RM) + DPO (via Rubric-ARM) + IterDPO (via Rubric-ARM) 79.3 56.9 75.4 75.0 66.8 75.7 73.8 71.5 73.0 77.3 78.2 78.2 77.6 78.7 79. 76.9 47.1 71.3 72.5 64.1 68.0 70.2 69.1 68.9 72.6 74.3 73.9 74.1 76.0 75.1 85.4 67.0 83.0 81.8 75.3 83.2 81.7 79.9 80.9 84.1 84.5 84.5 84.3 84.7 86. 83.6 57.6 79.4 79.9 72.8 78.5 78.3 77.7 77.8 80.3 81.1 81.2 81.7 82.5 82.9 AVG 81.3 57.2 77.3 70.1 77.3 69.8 76.0 76.0 74.6 75.2 78.6 79.5 79.5 79. 80.4 80.8 AVG 87.3 80.6 80.7 82.7 78.1 (76.0) 72.5 82.0 83.5 80.0 76.1 84.1 (81.5) 81.8 83.0 83.3 83.7 85.0 6.3. Ablation Study Table 2 reports two ablation studies that examine (i) the optimization order between the judge and the rubric generator, and (ii) the contribution of the format reward. Unless stated otherwise, all settings are kept identical to Rubric-ARM. Optimization order. Our default schedule updates the judge first, then the rubric generator, and alternates thereafter. Swapping this order (switch opt) consistently hurts performance: the average drops from 74.8 72.4 (2.4) without voting and from 76.2 74.9 (1.3) with voting@5, with especially large regressions on strict instruction-following metrics (e.g., RewardBench2-Precise IF: 41.9 24.4). This suggests that stronger judge provides less noisy learning signal for rubric optimization. Format reward. Removing the format reward (w/o format) also degrades results: 74.8 72.6 (2.2) without voting and 76.2 75.5 (0.7) with voting@5. The largest gains appear on structure-sensitive metrics (e.g., RewardBench2-Precise IF: +16.3), indicating that Rfmt helps prevent degenerate judging behaviors (e.g., missing criteria checks) and improves rubric adherence. 10 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 4: Comparison of different strategies applied to Qwen2.5-7B-Instruct on Arena-Hard and AlpacaEval. Results are reported for vanilla models and style/length-controlled settings. Baseline results with \"\" are from Viswanathan et al. (2025), Rezaei et al. (2025), Liu et al. (2025a). Best results are in bold. Model Arena-Hard AlpacaEval Vanilla Style-Con Vanilla Length-Con GPT-4 (0314) UltraIF (An et al., 2025) Qwen2.5-7B-Instruct + SFT (Distilled) + DPO (via Skywork) + DPO (via ArmoRM) + DPO (via Ultrafbk.) + DPO (via AI Judge) + DPO (via RLCF) + IterDPO (via RLCF) + DPO (via Rubric-RM) + IterDPO (via Rubric-RM) + RL (via OnlineRubrics) + DPO (via Rubric-ARM) + IterDPO (via Rubric-ARM) 50.0 31.4 51.3 32.6 55.1 50.8 52.8 51.0 54.6 51.1 52.9 56.3 56.5 57.8 58.8 50.0 42.8 29.2 50.3 46.4 47.9 44.4 48.4 54.6 53.1 56.7 59.5 58.9 22.1 33.5 36.1 44.8 37.6 33.7 28.8 36.2 38.9 47.0 50.1 55.0 47.1 52.0 35.3 36.2 33.3 41.5 38.1 38.7 33.4 37.1 39.2 41.3 42.0 30.4 42.5 44.0 AVG 39.4 41.0 32.8 47.9 43.2 43.3 39.4 44.1 46.0 48.6 51.3 51.7 53. 6.4. Performance of offline RL-based Policy Models We evaluate whether the benefit of Rubric-ARM transfers to downstream offline policy learning. Instruction-Following Evaluation. Table 3 and Fig. 3 show that policies optimized with Rubric-ARMtrained rewards consistently achieve the strongest instruction-following performance. On IFEval, DPO with Rubric-ARM improves the overall average to 80.4, and iterative DPO further raises it to 80.8 (best), with particularly strong gains on instruction-level constraints. The advantage also transfers to the open-ended InfoBench benchmark, where Rubric-ARM reaches 83.7 with DPO and 85.0 with iterative DPO (best). Compared to iterative baselines, Rubric-ARM remains consistently stronger: on IFBench  (Fig. 3)  , RLCF improves from 28.2 to 32.0 with IterDPO, while Rubric-ARM achieves 35.4 with IterDPO; similarly, iterative Rubric-RM reaches 33.7, still below Rubric-ARM. Overall, these results indicate that Rubric-ARM provides more precise reward signal, and that iterative optimization amplifies the gains over both one-shot DPO and iterative baselines. Human Preference Alignment Evaluation. Table 4 and Table 5 show that Rubric-ARM-trained rewards consistently yield stronger preference alignment across both controlled and open-domain evaluations. On Arena-Hard and AlpacaEval  (Table 4)  , DPO with Rubric-ARM achieves the best overall average (51.7), and IterDPO further improves it to 53.4 (best). On WildBench  (Table 5)  , RubricARM again yields the strongest macro score: DPO via Rubric-ARM reaches 53.7, while IterDPO via Rubric-ARM achieves 55.7 (best), improving over IterDPO with Rubric-RM (54.0) by 1.7%, indicating improved preference-aligned helpfulness on broad, real-world tasks. Creative Writing. We further evaluate whether Rubric-ARM-based rewards benefit open-ended generation on the Creative Writing Benchmark v3  (Fig. 4)  . Policies trained with Rubric-ARM outperform baselines: DPO using Rubric-ARM achieves 39.0, and IterDPO further improves to 39.3 (best). Notably, Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 5: Comparison of different alignment strategies applied to Qwen2.5-7B-Instruct on WildBench. Results are reported for task-specific scores and task macro WB score. Baseline results with \"\" are from Wang et al. (2025). Best results are in bold. Method Claude-3.5-Sonnet (20240620) GPT-4-turbo (20240409) GPT-4o-mini (20240718) Qwen2.5-7B-Instruct +DRIFT +SPIN +IterDPO (via OpenAssistant) +DPO (via RLCF) +IterDPO (via RLCF) +DPO (via Rubric-RM) +IterDPO (viaRubric-RM) +DPO (via Rubric-ARM) +IterDPO (via Rubric-ARM) Creative Planning Math Info seeking Coding WB Score 55.6 58.7 60. 50.1 52.5 43.3 46.8 51.4 51.9 54.8 57.0 55.2 57.3 55.6 56.2 58.2 51.8 53.2 45.5 48.6 52.7 52.6 55.5 56.2 55.6 57.2 50.2 51.0 54. 47.1 50.6 41.6 44.5 49.0 47.8 51.5 50.6 49.5 53.3 55.5 57.2 57.4 50.7 52.4 46.3 48.0 51.3 51.4 54.1 54.9 56.0 56.2 56.5 55.1 57. 45.0 50.3 39.1 44.3 48.8 46.5 52.9 52.8 53.1 55.2 54.7 55.2 57.1 48.7 51.7 42.9 46.3 50.5 49.7 53.6 54.0 53.7 55.7 Figure 3: Comparison of trained policy models on IFBench. Results of baselines except Rubric-RM (IterDPO) are from OpenRubrics Liu et al. (2025a). Figure 4: Comparison of trained policy models on Create Writing Benchmark v3. Results of baselines except Rubric-RM are from RuscaRL (Zhou et al., 2025). Rubric-ARM-based optimization also surpasses strong creative-writing baselines such as RaR (38.8) and RuscaRL (38.6), suggesting that rewards learned by Rubric-ARM generalize well to subjective, non-verifiable generation tasks beyond standard instruction following and preference alignment. 12 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 6: Comparison of online RL method with different alignment strategies applied to Qwen2.5-7B-Instruct on instruction following and preference alignment benchmarks. Best results are in bold. Method IFEval (Prompt) IFEval (Inst.) IFBench AlpacaEval Loose Strict Loose Strict Vanilla Length Qwen2.5-7B-Instruct +GRPO (RM-R1) +GRPO (Rubric-ARM) 75.0 76.7 79.3 72.5 73.6 76. 81.8 83.2 85.3 79.9 80.2 83.0 28.2 30.6 34.8 33.5 53.2 56.2 36.2 42.7 44.8 AVG 46.8 52.3 55.4 Table 7: Computing speed on 100 samples (vLLM). Results with were taken from Liu et al. (2025a). JudgeLRM-7B RRM-7B RM-R1-7B (Qwen-2.5-Inst) RM-R1-7B (DeepSeek-Dist) RM-R1-14B (Qwen-2.5-Inst) RM-R1-14B (DeepSeek-Dist) Rubric-RM-8B Rubric-ARM-8B Compute Time (s) 25.71 203.40 260.37 170.76 322.79 382.02 105. 33.50 Figure 5: Performance of Rubric-ARM across three iterations. iterative DPO with 6.5. Performance of online RL-based Policy Models We evaluate Rubric-ARM in an online RL setting by directly optimizing Qwen2.5-7B-Instruct with GRPO (Sec. 4.3) using different reward models. As shown in Table 6, GRPO with Rubric-ARM-trained rewards substantially improves both instruction following and preference alignment compared to the base model and strong reward baseline RM-R1. Specifically, Qwen2.5-7B-Instruct achieves an average score of 46.8, while GRPO with RM-R1 increases it to 52.3. Replacing the reward with RubricARM yields the best overall performance, reaching 55.4 on average. The gains are consistent across instruction-following and human-preference alignment metrics, which indicates that Rubric-ARM provides more effective online learning signal for GRPO. 6.6. Effect of Iterative Policy Optimization Fig. 5 evaluates iterative DPO with Rubric-ARM over three optimization iterations. Overall, the average performance increases monotonically across iterations, indicating that iteratively refining the policy with Rubric-ARM-based supervision yields progressively better alignment. These results suggest that Rubric-ARM provides sufficiently stable signal to support multi-round offline optimization without performance degradation. 13 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 8: Case study with error highlighting. Baselines either lose in the reasoning process, or favor the longer response but miss the explicit constraint about thumb war, while Rubric-ARM enforces hard rules before evaluating principles. Case Study on RewardBench Chat Hard Prompt Wars involve armed conflicts between nations, states, groups, or individuals, resulting in the loss of life, property damage, displacement of populations, and numerous other types of suffering. Is thumb war violent? care about both physical and psychological violence. Resp (snippet) War is inherently violent . . . direct physical violence . . . psychological violence . . . (war; not thumb war) Resp (snippet) . . . not considered violent activity but rather fun and harmless pastime . . . friendly competition . . . Label Resp B. RRM-7B JudgeLRM Rubric-ARM Rubric . . . Assistant 1 . . . doesnt directly answer . . . But wait . . . the instruction doesnt specify only thumb war . . . its more about war in general . . . Im bit confused . . . (No Decision) Assistant 1 provided detailed and accurate response . . . Assistant 2s answer is inaccurate . . . (Choose A) 1. The response must directly address whether thumb war constitutes violence. [Hard Rule] 2. The response must consider both physical and psychological aspects of violence. [Hard Rule] 3. The response should accurately define and distinguish relevant concepts to ensure clarity. [Principle] 4. The response should present balanced analysis by acknowledging multiple perspectives or contexts. [Principle] 5. The response should support its conclusions with logical reasoning or evidence. [Principle] 6. The response should maintain respectful and empathetic tone. [Principle] Judge The response must directly address whether thumb war constitutes violence . . . Response fails . . . not addressing thumb wars at all. Response satisfies both . . . . (Choose B) 6.7. Efficiency Comparison We conclude with an inference-cost analysis and case studies. Table 7 reports wall-clock time on 100 RewardBench2 prompts. Despite using two Qwen-3-8B modules (rubric generator + judge), Rubric-ARM runs in 33.50s, faster than most reasoning-based and rubric-based baselines. While JudgeLRM is slightly faster, it does not provide the explicit, interpretable rubric-conditioned signals that Rubric-ARM is designed for downstream policy optimization. Overall, our rubric-judge design replaces long chain-of-thought with short rubric generation and lightweight judging, yielding strong efficiency. Rubric-ARM is also faster than Rubric-RM, which typically generates longer rubric lists and incurs higher overhead. 6.8. Case Study We qualitatively analyze failures of baseline reward models on challenging examples. Table 8 shows RewardBench Chat-Hard instance about thumb war: reasoning-based models (e.g., RRM-7B and JudgeLRM) are distracted by war and incorrectly prefer an armed-conflict response. In contrast, Rubric-ARM generates and enforces rubric with an explicit hard rule about thumb war, leading to the correct preference. We provide additional IFBench examples in App. D.3, where Rubric-ARM reliably extracts hard constraints and judges correctly while Rubric-RM fails. Alternating Reinforcement Learning for Rubric-Based Reward Modeling 7. Conclusion We present OpenRubrics, large-scale dataset and framework for scalable and high-quality rubric generation. By decomposing evaluation into hard rules and principles through Contrastive Rubric Generation (CRG) and applying preferencelabel consistency filtering, we constructed interpretable and discriminative rubric signals that better align with human judgment. Our rubric-based reward model, Rubric-ARM, delivers an average 6.8% improvement across diverse benchmarks and further boosts policy performance by 1.1%6.5% when used as reward in offline reinforcement learning. These results position rubrics-as-rewards as practical foundation for transparent and generalizable LLM alignment. For future work, we will extend rubric generation to more open-ended tasks and leverage rubrics as intermediate supervision within RLHF pipelines."
        },
        {
            "title": "References",
            "content": "Abhinav Agrawal and Justin Domke. Amortized variational inference for simple hierarchical models. Advances in Neural Information Processing Systems, 34:2138821399, 2021. Afra Feyza Aky√ºrek, Advait Gosai, Chen Bo Calvin Zhang, Vipul Gupta, Jaehwan Jeong, Anisha Gunjal, Tahseen Rabbani, Maria Mazzone, David Randolph, Mohammad Mahmoudi Meymand, et al. Prbench: Large-scale expert rubrics for evaluating high-stakes professional reasoning. arXiv preprint arXiv:2511.11562, 2025. Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, and Baobao Chang. UltraIF: Advancing instruction following from the wild. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1871118726, Suzhou, China, November 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025.emnlp-main.945/. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Daniel Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models. In Pluralistic Alignment Workshop at NeurIPS 2024, 2024. URL https://openreview.net/forum?id=CljYUvIlRW. Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Qui√±onero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. Judgelrm: Large reasoning models as judge. arXiv preprint arXiv:2504.00050, 2025. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. RM-r1: Reward modeling as reasoning. In The Fourteenth International Conference on Learning Representations, 2026. URL https://openreview.net/ forum?id=1ZqJ6jj75q. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL https://arxiv.org/abs/ 2403.04132. 15 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with scaled AI feedback. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=BOorDpKHiJ. Arthur Dempster, Nan Laird, and Donald Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series (methodological), 39(1):122, 1977. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=cRR0oDFEBC. Yann Dubois, Bal√°zs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators, 2025. URL https://arxiv.org/abs/2404.04475. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. How to evaluate reward models for rlhf, 2024. URL https://arxiv.org/abs/2410.14872. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Yunzhong He, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. In The Fourteenth International Conference on Learning Representations, 2026. URL https://openreview.net/forum?id= c1bTcrDmt4. Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. Reward reasoning models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=V8Kbz7l2cr. Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, and Chris Kedzie. Llm-rubric: multidimensional, calibrated approach to automated evaluation of natural language texts. arXiv preprint arXiv:2501.00274, 2024. Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, Haoming Jiang, Qingru Zhang, Qin Lu, Xin Liu, Chao Zhang, and Tuo Zhao. Think-RM: Enabling long-horizon reasoning in generative reward models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=UfQAFbP6xq. Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv preprint arXiv:2508.12790, 2025. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. FollowBench: multi-level fine-grained constraints following benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46674688, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.257/. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Validad Miranda, Alisa Liu, et al. Tulu 3: Pushing frontiers in open language model 16 Alternating Reinforcement Learning for Rubric-Based Reward Modeling post-training. In Second Conference on Language Modeling, 2025a. URL https://openreview. net/forum?id=i1uGbfHHpH. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 17551797. Association for Computational Linguistics, 2025b. Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, and Wei Chen. Rubrichub: comprehensive and highly discriminative rubric dataset via automated coarse-to-fine generation. arXiv preprint arXiv:2601.08430, 2026. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. In International Conference on Machine Learning, pages 2912829163. PMLR, 2024. Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild, 2024. URL https://arxiv.org/abs/2406.04770. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. Tianci Liu, Ran Xu, Tony Yu, Ilgee Hong, Carl Yang, Tuo Zhao, and Haoyu Wang. Openrubrics: Towards scalable synthetic rubric generation for reward modeling and llm alignment. arXiv preprint arXiv:2510.07743, 2025a. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. RM-bench: Benchmarking reward models of language models with subtlety and style. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=QEHrmQPBdd. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025c. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fr√§nken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah Smith, Hannaneh Hajishirzi, and Nathan Lambert. Rewardbench 2: Advancing reward model evaluation. arXiv preprint arXiv:2506.01937, 2025. Samuel Paech. Eq-bench creative writing benchmark v3. https://github.com/EQ-bench/ creative-writing-bench, 2025. Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=4XIKfvNYvx. Alternating Reinforcement Learning for Rubric-Based Reward Modeling Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Arnav Ramamoorthy, Pratyush Ghosh, Aaryan Raj Jindal, Shreyash Verma, Aditya Mittal, Aashna Ased, Chirag Khatri, et al. Rubric is all you need: Enhancing llm-based code evaluation with question-specific rubrics. arXiv preprint arXiv:2503.23989, 2025. Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1593415949, Vienna, Austria, July 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025.acl-long.775/. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, InFoBench: Evaluating instruction following ability in large Fei Liu, Pengfei Liu, and Dong Yu. language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1302513048, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.772/. Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, and Xing Sun. Incentivizing reasoning for advanced instruction-following of large language models. arXiv preprint arXiv:2506.01413, 2025. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Bing Liu, Yunzhong He, and Afra Feyza Aky√ºrek. Online rubrics elicitation from pairwise comparisons. arXiv preprint arXiv:2510.07284, 2025. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. Learning to plan & reason for evaluation with thinking-LLM-as-a-judge. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=PNRznmmWP7. Rulin Shao, Akari Asai, Shannon Zejiang Shen, Hamish Ivison, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel Finlayson, David Sontag, et al. Dr tulu: Reinforcement learning with evolving rubrics for deep research. arXiv preprint arXiv:2511.19399, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Lin Shi, Chiyu Ma, Wenhua Liang, Xingjian Diao, Weicheng Ma, and Soroush Vosoughi. Judging the judges: systematic study of position bias in llm-as-a-judge. In Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, pages 292314, 2025. 18 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Vijay Viswanathan, Yanchao Sun, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=RPRqKhjrr6. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024a. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts, 2024b. URL https://arxiv.org/abs/ 2406.12845. Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, and Qingkai Zeng. Drift: Learning from abundant user dissatisfaction in real-world preference learning. arXiv preprint arXiv:2510.02341, 2025. Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. J1: Incentivizing thinking in LLM-as-a-judge via reinforcement learning. In The Fourteenth International Conference on Learning Representations, 2026. URL https://openreview.net/ forum?id=dnJEHl6DI1. Lipeng Xie, Sen Huang, Zhuo Zhang, Anni Zou, Yunpeng Zhai, Dingchao Ren, Kezun Zhang, Haoyuan Hu, Boyin Liu, Haoran Chen, et al. Auto-rubric: Learning to extract generalizable criteria for reward modeling. arXiv preprint arXiv:2510.17314, 2025. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=c1AKcA6ry1. Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, and Hongkun Yu. Incentivizing agentic reasoning in llm judges via tool-integrated reinforcement learning. In The Fourteenth International Conference on Learning Representations, 2026. URL https://openreview.net/forum?id=AXNRILww9c. Wenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, and Yonghui Wu. unified pairwise framework for rlhf: Bridging generative reward modeling and policy optimization. arXiv preprint arXiv:2504.04950, 2025a. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned LLMs with nothing. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview. net/forum?id=Pnk7vMbznK. 19 Alternating Reinforcement Learning for Rubric-Based Reward Modeling An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, et al. Beyond correctness: Evaluating subjective writing preferences across cultures. arXiv preprint arXiv:2510.14616, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Juncai Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. URL https://openreview.net/forum?id=2a36EMSSTp. Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, and Rui Hou. Self-generated critiques boost reward modeling for language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1149911514, 2025b. Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, and Lifeng Jin. Chasing the tail: Effective rubric-based reward modeling for large language model post-training. In The Fourteenth International Conference on Learning Representations, 2026. URL https://openreview.net/forum?id=pBjy4ek2QV. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=Ccwp4tFEtE. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Yixin Cao, Yang Feng, and Deyi Xiong, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-demos.38/. Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Kongcheng Zhang, Jiale Zhao, Jingwen Yang, Yihe Zhou, Jianwei Lv, Tongya Zheng, et al. Breaking the exploration bottleneck: Rubric-scaffolded reinforcement learning for general llm reasoning. arXiv preprint arXiv:2508.16949, 2025. 20 Alternating Reinforcement Learning for Rubric-Based Reward Modeling A. Details for Group Relative Policy Optimization (GRPO) GRPO (Shao et al., 2024) is an actor-only policy optimization method that reduces variance by using the within-prompt average reward as baseline. Concretely, for each prompt q, GRPO samples group of responses = {o1, o2, . . . , oG} from the old policy œÄŒ∏old( q), computes group-normalized advantage Ai,t for each token, and then performs PPO-style clipped update. Following Yu et al. (2025a), we upweight informative prompts using larger clipping threshold Œµhigh. ùí•GRPO(Œ∏) = qP(Q),OœÄŒ∏old (q) [ 1 i=1 1 oi oi t=1 min (œÅi,t(Œ∏) Ai,t, clip (œÅi,t(Œ∏), 1 Œµlow, 1 + Œµhigh) Ai,t) Œ≤ KL[œÄŒ∏ œÄref] ], where œÅi,t(Œ∏) = œÄŒ∏(oi,tq,oi,<t) œÄŒ∏old (oi,tq,oi,<t) is the token-level importance ratio. B. Detailed Theoretical Derivations In this section, we provide the complete proofs for the variance analysis presented in Section 5. B.1. Preliminaries Recall the definitions: ]. Reward: R(o) = I[o = c, r). Judge Correctness: p(r) = œÄj(o Generator Score: ur(r) = Œ∏r log œÄr(r x). Œ∏r log œÄj(o c, r). Judge Score: uj(o r) = Œ∏j We utilize the vector form of the Law of Total Variance: Lemma B.1. For random vectors and Y, Var(Y) = X[Var(Y X)] + VarX(E[Y X]). B.2. Proof of Proposition 5.1 (Strategy A) Proof. In Strategy A, the rubric is fixed. The gradient estimator is gA = R(o)uj(o r), where œÄj( r). Since is fixed, uj(o r) takes two values: uj(o r) (when wrong). Considering the term associated with the reward R(o), the variable is scaled Bernoulli. Conditioned on r: r) (when correct) and uj(o With probability p(r), = With probability 1 p(r), , so gA = 1 uj(o r). , so gA = 0 (since = 0). Let = uj(o r). The first moment is: E[ gA r] = p(r)v + (1 p(r)) 0 = p(r)v. 21 Alternating Reinforcement Learning for Rubric-Based Reward Modeling The second moment is: Thus, the variance is: 2 E[ gA r] = p(r)v 2. 2 + (1 p(r)) 0 = p(r)v Var( gA r) = gA = p(r)v 2 = (p(r) p(r) = p(r)(1 p(r))uj(o 2 2 E[ gA] 2 2 p(r)v 2 )v r) 2 . B.3. Proof of Proposition 5.2 (Strategy B) Proof. In Strategy B, we update Œ∏r apply Lemma B.1 conditioning on r. Step 1: Conditional Variance (Inner Term). Conditioned on r, ur(r) is constant vector. The randomness comes only from R(o). . The estimator is gB = R(o)ur(r), where œÄr and œÄj( r). We Var( gB r) = Varo r(R(o)ur(r)) = ur(r) 2Varo r(R(o)). Since R(o) Bernoulli(p(r)), its variance is p(r)(1 p(r)). Thus: 2. Var( gB r) = p(r)(1 p(r))ur(r) Step 2: Conditional Expectation (Outer Term). E[ gB r] = r [R(o)]ur(r) = p(r)ur(r). Step 3: Total Variance Decomposition. By applying the Law of Total Variance (Lemma B.1), we express the total variance as the sum of the expected conditional variance and the variance of the conditional expectation: Var( gB) = [Var( gB r)] + Varr(E[ gB r]). Substituting the results derived in Step 1 and Step 2 into the equation above yields the final decomposition: Var( gB) = 2 [p(r)(1 p(r))ur(r) ] + Varr(p(r)ur(r)). This concludes the proof. 22 Alternating Reinforcement Learning for Rubric-Based Reward Modeling B.4. Proof of Theorem 5. Proof. We analyze the sign of the variance difference = Var( gB) 1. Variance Difference Expansion. Substituting the expressions from Propositions 5.1 and 5.2: r[Var( gA r)]. 2 [p(r)(1 p(r))ur(r) ] + Varr(p(r)ur(r)) = (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) VB [p(r)(1 p(r))(ur(r) 2 uj(o 2 r) = [p(r)(1 p(r))uj(o ] (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) VA )] + Varr(p(r)ur(r)). r) 2. Incorporating the Exploration Coefficient. Using the definition of C1 substitute Varr(p(r)ur(r)) = C1 2 r[p(r)2ur(r) ]: from Assumption 5.3, we = [p(r)(1 p(r))(ur(r) 2 uj(o 2 r) ) + C1 p(r) 2 2 ur(r) ]. 3. Verification of Positivity. To show > 0, we analyze the term inside the expectation (the integrand). We split the expression into multiple lines to isolate the quadratic components: Integrand = (p(r) p(r) 2 )ur(r) = p(r)[(1 p(r))ur(r) 2 2 (p(r) p(r) 2 (1 p(r))uj(o )uj(o 2 2 + C1 p(r) r) 2 + C1 p(r)ur(r) ur(r) 2 ] r) 2 = p(r)[ur(r) (1 p(r) + C1 p(r)) uj(o 2 r) (1 p(r))]. We now invoke the inequality from Assumption 5.3: ur(r) uj(o r) > 1 p(r) 1 p(r) + C1 p(r) . Squaring both sides and rearranging: 2 ur(r) (1 p(r) + C1 p(r)) > uj(o 2 r) (1 p(r)). This implies that the term inside the square brackets is strictly positive. Since p(r) (0, 1), the entire integrand is strictly positive for any r. Therefore, the expectation is strictly positive: > 0 Var( gB) > [Var( gA r)]. This concludes the proof. C. Implementation Details Table 9 and Table 10 show the hyperparameters used in Rubric-ARM and policy model training. We implement the GRPO training based on ms-swift1 library (Zhao et al., 2024) and implement DPO and 1 https://github.com/modelscope/ms-swift Alternating Reinforcement Learning for Rubric-Based Reward Modeling IterDPO based on LLaMA-Factory2 (Zheng et al., 2024). We totally conduct 3 iterations for Rubric-ARM alternating RL training. Additionally, the sampling parameters used in inference are summarized in Table 11. We used the same sampling parameters as their official implementations and papers for baseline methods. Table 9: Hyper-parameters used in Rubric-ARM training. Module Parameter Value Module Parameter Value Rubric Generator #generations Cutoff Length Batch Size Optimizer Learning Rate Temperature #iterations Epochs œµhigh œµlow Œ≤ 6 512 288 AdamW 1e-6 1.0 2 1 0.28 0.2 0. Judge #generations Cutoff Length Batch Size Optimizer Learning Rate Temperature #iterations Epochs œµhigh œµlow Œ≤ 7 1024 224 AdamW 1e-6 1.0 2 1 0.28 0.2 0.001 Table 10: Hyper-parameters used in policy model training. Method Parameter Value Method Parameter Value DPO Cutoff Length Batch Size Optimizer Learning Rate Epochs beta SFT mixing weight / / / / 2048 64 AdamW 8e-7 1 0.1 0.2 / / / / GRPO #generations Cutoff Length Batch Size Optimizer Learning Rate Temperature #iterations Epochs œµhigh œµlow Œ≤ 6 2048 288 AdamW 5e-7 1.0 2 1 0.28 0.2 0.001 D. Additional Experimental Results D.1. Performance on WritingPreferenceBench We present the performance on WritingPreferenceBench in Table 12. 2 https://github.com/hiyouga/LlamaFactory 24 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 11: Sampling parameters used in Rubric-ARM inference. Module Parameter Value Module Parameter Rubric Generator Maximum Tokens Temperature Top-P Top-K Enable-thinking 1024 0.0 / / False Judge Maximum Tokens Temperature Top-P Top-K Enable-thinking Value 4096 1.0 1.0 -1 False Table 12: Comparison of different judge and reward models on WritingPreferenceBench. Best results are highlighted in bold. Func. Promo. Non-Fic. Fiction Funny Poetry Script Role AVG LLM as Judge (black-box model) Claude-4-Opus-thinking OpenAI-o4-mini Gemini-2.5-Flash White-box Reward Models Skywork-Llama-3.1-8B Skywork-Gemma-2-27B RM-R1-DeepSeek-Qwen-7B RM-R1-Qwen2.5-7B RRM-7B Rubric-based Models 65.7 58.3 59.1 53.6 49.0 62.5 67.0 50.0 64.3 58.6 57.7 56.3 53.9 55.1 57.2 35. 64.1 60.9 62.5 60.6 59.6 59.2 53.9 50.0 60.1 55.5 59.8 49.0 33.9 55.4 60.0 49.5 54.2 53.2 52.2 52.2 55.1 58.0 54.6 38. 64.0 68.0 56.0 56.0 36.0 56.0 72.0 36.4 43.5 30.4 34.8 51.7 55.2 51.7 61.0 56.6 57.5 65.2 21.7 65.2 47.8 45. 41.4 53.1 51.7 46.8 57.4 41.4 65.5 59.8 44.7 53.8 Rubric-RM Rubric-ARM 58.3 67.8 58.5 63.1 57.9 65.8 58.3 60. 58.0 61.0 76.0 80.0 47.8 47.8 55.2 60.3 55.2 63.2 D.2. Position Bias Analysis In this section, we study position bias in pairwise judge and reward models, where the predicted preference may depend on the relative order of the two responses (Shi et al., 2025). We evaluate three settings: (1) keeping the response order fixed as in the original dataset, (2) flipping the order for all instances, and (3) randomly flipping the order on per-instance basis. Table 13 reports results on RewardBench and the IF evaluation benchmarks. Overall, baseline methods exhibit non-trivial position bias. For RRM-7B, changing the order leads to 46.2-point difference on PPE-IFEval (75.8 vs. 29.6). Likewise, for RM-R1-7B (Qwen-2.5-Inst), flipping the order changes InfoBench by 11.9 points (81.8 vs. 69.9). For RM-R1-7B (DeepSeek-Dist), the order sensitivity remains substantial, with 9.9-point difference on InfoBench (78.3 vs. 68.4) and 9.3-point difference on FollowBench (79.0 vs. 69.7). In contrast, our Rubric-ARM remains consistently stable across different orderings, suggesting substantially reduced position bias and more robust evaluation. This design choice is aligned with our RL training design, where we randomize the response order when collecting reward signals, which further mitigates position bias in downstream policy optimization. 25 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 13: Position bias analysis for different judge and reward models. Rubric-ARM shows much lower sensitivity to the ordering of response pairs. RewardBench IF Evaluation Benchmarks Chat Chat Hard FollowBench PPE-IFEval InfoBench IFBench Avg. Variation White-box Judge/Reward LLM: RRM-7B Mixed Ord Fixed Ord-1 Fixed Ord-2 Variation 77.7 73.9 82. 8.2 69.5 61.6 72.1 10.5 65.5 53.8 64.7 11.7 51.0 29.6 75. 46.2 White-box Judge/Reward LLM: RM-R1-7B (Qwen-2.5-Inst) Mixed Ord Fixed Ord-1 Fixed Ord-2 Variation 83.0 82.1 82.4 0. 70.0 63.4 71.1 7.7 56.3 57.1 56.3 0.8 55.2 54.8 50.4 4. White-box Judge/Reward LLM: RM-R1-7B (DeepSeek-Dist) Mixed Ord Fixed Ord-1 Fixed Ord-2 Variation 85.3 87.1 82.7 4.4 67.3 67.3 69. 2.2 Rubric-based Method: Rubric-RM Mixed Ord Fixed Ord-1 Fixed Ord-2 Variation 88.2 87.4 88.7 1. 74.1 74.6 73.5 1.1 69.7 79.0 70.6 9.3 76.1 79.8 75.6 4. Rubric-based Method: Rubric-ARM (Ours) Mixed Ord Fixed Ord-1 Fixed Ord-2 Variation 89.4 89.9 88.4 1.5 79.6 79.4 80. 0.9 85.7 84.9 85.7 0.8 51.0 52.8 54.7 3.7 67.0 70.8 67. 3.8 70.8 71.8 71.0 1.0 68.2 62.3 74.2 11.9 71.3 81.8 69. 11.9 70.3 78.3 68.4 9.9 80.8 80.9 78.5 2.4 86.1 86.1 87. 1.8 53.2 30.2 74.2 44.0 55.2 53.8 54.1 1.4 56.5 53.2 60. 7.4 65.4 66.4 64.4 2.0 65.9 65.3 66.9 1.6 22. 4.58 6.15 2.47 1.27 D.3. Additional Case Study In this section we compare Rubric-ARM with Rubric-RM, another rubric-based RM trained with SFT, on randomly chosen example from IFBench. The case specifies keywords and paragraph length. Results are shown in Table 14. In this IFBench example, which requires specific keywords and exactly two paragraphs, the baseline Rubric-RM suffers from judging hallucination, incorrectly claiming that valid response is split into three paragraphs. Rubric-ARM, on the contrary, accurately extracts these hard constraints and identifies the missing open-source keyword in the negative sample, while correctly verifying the structure of the positive one. 26 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Table 14: Case study with error highlighting. In the baseline Rubric-RM, the rubric output confused the judge and led to an incorrect prediction. Case Study on IFBench Prompt Resp (snippet) Resp (snippet) Label Rubric-RM Rubric What is Nextcloud, why should use it, and how does it compare to other cloud storage solutions? Please provide the response with tone of excitement and enthusiasm. Your response should contain exactly 2 paragraphs and include the keywords cloud storage and open-source. Nextcloud is an incredibly exciting and innovative cloud storage solution . . . As an open-source platform . . . Imagine having your very own private cloud . . . (2 paragraphs) Nextcloud is an incredibly powerful and flexible solution . . . While services like Google Drive and Dropbox are great . . . (2 paragraphs; missing open-source) Resp A. 1. The response must define what Nextcloud is. [Hard Rule] 2. The response must explain why the user should use Nextcloud. [Hard Rule] 3. The response must compare Nextcloud to other cloud storage solutions. [Hard Rule] 4. The response must use tone of excitement and enthusiasm. [Hard Rule] 5. The response must consist of exactly two paragraphs. [Hard Rule] 6. The response must include the keywords cloud storage and open-source. [Hard Rule] 7. The response should present information in clear, logical structure that guides the reader through definition, benefits, and comparison. [Principle] 8. The response should use vivid, engaging language to maintain reader interest. [Principle] 9. The response should employ precise and varied vocabulary to convey ideas effectively. [Principle] 10. The response should provide specific, relevant details to substantiate its claims. [Principle] 11. The response should maintain coherence and flow, ensuring each paragraph builds on the previous one. [Principle] 12. The response should balance conciseness with sufficient elaboration to cover all required points. [Principle] Judge . . . Response is split into three paragraphs, not exactly two . . . Response fails the gatekeeper criterion (paragraph count) . . . (Choose B) Rubric-ARM Rubric 1. The response must address what Nextcloud is, why it should be used, and how it compares to other cloud storage solutions. [Hard Rule] 2. The response must maintain an enthusiastic and excited tone throughout. [Hard Rule] 3. The response must consist of exactly two paragraphs. [Hard Rule] 4. The response must include the keywords cloud storage and open-source. [Hard Rule] 5. The response should present information in clear and logical structure, grouping related ideas coherently. [Principle] 6. The response should use vivid and engaging language to sustain reader interest. [Principle] 7. The response should provide relevant comparisons or contrasts to contextualize the subject. [Principle] 8. The response should maintain conciseness and avoid unnecessary repetition. [Principle] Judge . . . Response mentions cloud and open but not the exact keyword open-source, so fails. . . . Response fulfills all hard rules, including the precise keywords . . . (Choose A) E. Prompts We present the prompts we used in this section. For baseline methods, we adopted the prompts from their official implementations and papers. Prompt for Rubric Generation (Rubric-ARM) Your task is to extract set of rubric-style instructions from user's request. These rubrics will be used as evaluation criteria to check if response fully meets the request. Every rubric item must be universal principle. If any rubric still contains topic-specific references (e.g., names, places, myths, numbers, historical facts), it is automatically Alternating Reinforcement Learning for Rubric-Based Reward Modeling invalid. - **Two Distinct Categories:** - [Hard Rule]: Derived strictly from explicit requirements stated in the <request> (format, length, structure, forbidden/required elements, etc.). - [Principle]: Derived by abstracting any concrete cues into domain-agnostic quality criteria ( e.g., clarity, correctness, sound reasoning, pedagogy). - **Comprehensiveness:** The rubric must cover all critical aspects implied by the request and examples, including explicit requirements and implicit quality standards. - **Conciseness & Uniqueness:** Each rubric must capture distinct evaluation criterion. Overlapping or redundant criteria must be merged into single rubric. Wording must be precise and free of repetition. - **Format Requirements:** - Use numbered list. - Each item starts with \"The response\" phrased in third person. - Append [Hard Rule] or [Principle] at the end of each item. - Do not include reasoning, explanations, or examples in the final output√¢ƒÇ≈§only the rubrics. Here is the request: {prompt} Please generate the rubrics for the above request. 28 Alternating Reinforcement Learning for Rubric-Based Reward Modeling Prompt for Judge Generation (Rubric-ARM) You are fair and impartial judge. Your task is to evaluate 'Response A' and 'Response B' based on given instruction and rubric. You will conduct this evaluation in distinct phases as outlined below. ### Phase 1: Compliance Check Instructions First, identify the single most important, objective 'Gatekeeper Criterion' from the rubric. - **A rule is objective (and likely Gatekeeper) if it can be verified without opinion. Key examples are: word/paragraph limits, required output format (e.g., JSON validity), required/ forbidden sections, or forbidden content.** - **Conversely, rule is subjective if it requires interpretation or qualitative judgment. Subjective rules about quality are NOT Gatekeepers. Examples include criteria like \"be creative,\" \"write clearly,\" \"be engaging,\" or \"use professional tone.\"** ### Phase 2: Analyze Each Response Next, for each Gatekeeper Criterion and all other criteria in the rubric, evaluate each response item by item. ### Phase 3: Final Judgment Instructions Based on the results from the previous phases, determine the winner using these simple rules. Provide final justification explaining your decision first and then give your decision. --- ### REQUIRED OUTPUT FORMAT You must follow this exact output format below. --- Compliance Check --- Identified Gatekeeper Criterion: <e.g., Criterion 1: Must be under 50 words.> --- Analysis --- **Response A:** - Criterion 1 [Hard Rule]: Justification: <...> - Criterion 2 [Hard Rule]: Justification: <...> - Criterion 3 [Principle]: Justification: <...> - ... (and so on for all other criteria) **Response B:** - Criterion 1 [Hard Rule]: Justification: <...> - Criterion 2 [Hard Rule]: Justification: <...> - Criterion 3 [Principle]: Justification: <...> - ... (and so on for all other criteria) --- Final Judgment --- Justification: <...> Winner: <Response / Response B> Task to Evaluate: Instruction: {instruction} Rubric: {rubric} Response A: {response_a} Response B: {response_b}"
        }
    ],
    "affiliations": [
        "Emory University",
        "Georgia Institute of Technology",
        "Purdue University",
        "Rutgers University",
        "University at Albany"
    ]
}