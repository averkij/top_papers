{
    "paper_title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
    "authors": [
        "Chengshuai Zhao",
        "Zhen Tan",
        "Pingchuan Ma",
        "Dawei Li",
        "Bohan Jiang",
        "Yancheng Wang",
        "Yingzhen Yang",
        "Huan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 2 1 9 1 1 0 . 8 0 5 2 : r Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Chengshuai Zhao1, Zhen Tan1, Pingchuan Ma1, Dawei Li1, Bohan Jiang1, Yancheng Wang1, Yingzhen Yang1 and Huan Liu1 1Arizona State University, USA Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via data distribution lens and investigate if CoT reasoning reflects structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataA lchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is brittle mirage that vanishes when it is pushed beyond training distributions. This work offers deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning. Our code is available at GitHub: https://github.com/ChengshuaiZhao0/DataAlchemy. 1. Introduction Recent years have witnessed Large Language Models (LLMs) dominant role in various domains (Li et al., 2025b; Ting et al., 2025; Zhao et al., 2025, 2023) through versatile prompting techniques (Kojima et al., 2022; Wei et al., 2022; Yao et al., 2023). Among these, Chainof-Thought (CoT) prompting (Wei et al., 2022) has emerged as prominent method for eliciting structured reasoning from LLMs (a.k.a., CoT reasoning). By appending simple cue such as Lets think step by step, LLMs decompose complex problems into intermediate steps, producing outputs that resemble human-like reasoning. It has been shown to be effective in tasks requiring logical inference(Xu et al., 2024), mathematical problem solving (Imani et al., 2023), and commonsense reasoning (Wei et al., 2022). The empirical successes of CoT reasoning lead to the perception that LLMs engage in deliberate inferential processes (Ling et al., 2023; Yu et al., 2023; Zhang et al., 2024a,c). Figure 1 The data perspective lens. CoT reasonings effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. Guided by this lens, we dissect CoT reasoning via three dimensions: task, length, and format. Corresponding author(s): {czhao93, ztan36, pingchua, daweili5, bjiang14, yancheng.wang, yingzhen.yang, huanliu}@asu.edu Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens However, closer examination reveals inconsistencies that challenge this optimistic view. Consider this straightforward question: The day the US was established is in leap year or normal year? When prompted with the CoT prefix, the modern LLM Gemini responded: The United States was established in 1776. 1776 is divisible by 4, but its not century year, so its leap year. Therefore, the day the US was established was in normal year. This response exemplifies concerning pattern: the model correctly recites the leap year rule and articulates intermediate reasoning steps, yet produces logically inconsistent conclusion (i.e., asserting 1776 is both leap year and normal year). Such inconsistencies suggest that there is distinction between human-like inference and CoT reasoning. An expanding body of analyses reveals that LLMs tend to rely on surface-level semantics and clues rather than logical procedures (Bentham et al., 2024; Chen et al., 2025b; Lanham et al., 2023). LLMs construct superficial chains of logic based on learned token associations, often failing on tasks that deviate from commonsense heuristics or familiar templates (Tang et al., 2023). In the reasoning process, performance degrades sharply when irrelevant clauses are introduced, which indicates that models cannot grasp the underlying logic (Mirzadeh et al., 2024). This fragility becomes even more apparent when models are tested on more complex tasks, where they frequently produce incoherent solutions and fail to follow consistent reasoning paths (Shojaee et al., 2025). Collectively, these pioneering works deepen the skepticism surrounding the true nature of CoT reasoning. In light of this line of research, we question the CoT reasoning by proposing an alternative lens through data distribution and further investigating why and when it fails. We hypothesize that CoT reasoning reflects structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. As such, its effectiveness is inherently limited by the nature and extent of the distribution discrepancy between training data and the test queries. Guided by this data distribution lens, we dissect CoT reasoning via three dimensions: (i) taskTo what extent CoT reasoning can handle tasks that involve transformations or previously unseen task structures. (2) lengthhow CoT reasoning generalizes to chains with length different from that of training data; and (3) formathow sensitive CoT reasoning is to surface-level query form variations. To evaluate each aspect, we introduce Data lc my, controlled and isolated experiment that allows us to train LLMs from scratch and systematically probe them under various distribution shifts. Our findings reveal that CoT reasoning works effectively when applied to in-distribution or near in-distribution data but becomes fragile and prone to failure even under moderate distribution shifts. In some cases, LLMs generate fluent yet logically inconsistent reasoning steps. The results suggest that what appears to be structured reasoning can be mirage, emerging from memorized or interpolated patterns in the training data rather than logical inference. These insights carry important implications for both practitioners and researchers. For practitioners, our results highlight the risk of relying on CoT as plug-and-play solution for reasoning tasks and caution against equating CoT-style output with human thinking. For researchers, the results underscore the ongoing challenge of achieving reasoning that is both faithful and generalizable, motivating the need to develop models that can move beyond surface-level pattern recognition to exhibit deeper inferential competence. Our contributions are summarized as follows: Novel perspective. We propose data distribution lens for CoT reasoning, illuminating that its effectiveness stems from structured inductive biases learned from in-distribution training data. This framework provides principled lens for understanding why and when CoT reasoning succeeds or fails. Controlled environment. We introduce Data lc my, an isolated experimental framework that enables training LLMs from scratch and systematically probing CoT reasoning. This controlled setting allows us to isolate and analyze the effects of distribution shifts on CoT reasoning without 2 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens interference from complex patterns learned during large-scale pre-training. Empirical validation. We conduct systematic empirical validation across three critical dimensions task, length, and format. Our experiments demonstrate that CoT reasoning exhibits sharp performance degradation under distribution shifts, revealing that seemingly coherent reasoning masks shallow pattern replication. Real-world implication. This work reframes the understanding of contemporary LLMs reasoning capabilities and emphasizes the risk of over-reliance on COT reasoning as universal problemsolving paradigm. It underscores the necessity for proper evaluation methods and the development of LLMs that possess authentic and generalizable reasoning capabilities. 2. Related Work 2.1. LLM Prompting and CoT Chain-of-Thought (CoT) prompting revolutionized how we elicit reasoning from Large Language Models by decomposing complex problems into intermediate steps (Wei et al., 2022). By augmenting few-shot exemplars with reasoning chains, CoT showed substantial performance gains on various tasks (Imani et al., 2023; Wei et al., 2022; Xu et al., 2024). Building on this, several variants emerged. Zero-shot CoT triggers reasoning without exemplars using instructional prompts (Kojima et al., 2022), and self-consistency enhances performance via majority voting over sampled chains (Wang et al., 2023). To reduce manual effort, Auto-CoT generates CoT exemplars using the models themselves (Zhang et al., 2023). Beyond linear chains, Tree-of-Thought (ToT) frames CoT as tree search over partial reasoning paths (Yao et al., 2023), enabling lookahead and backtracking. SymbCoT combines symbolic reasoning with CoT by converting problems into formal representations (Xu et al., 2024). Recent work increasingly integrates CoT into the LLM inference process, generating long-form CoTs (Guo et al., 2025; Jaech et al., 2024; Team et al., 2025; Team, 2024). This enables flexible strategies like mistake correction, step decomposition, reflection, and alternative reasoning paths (Chen et al., 2025a; Yeo et al., 2025). The success of prompting techniques and long-form CoTs has led many to view them as evidence of emergent, human-like reasoning in LLMs. In this work, we challenge that viewpoint by adopting data-centric perspective and demonstrating that CoT behavior arises largely from pattern matching over training distributions. 2.2. Discussion on Illusion of LLM Reasoning While Chain-of-Thought prompting has led to impressive gains on complex reasoning tasks, growing body of work has started questioning the nature of these gains. One major line of research highlights the fragility of CoT reasoning. Minor and semantically irrelevant perturbations such as distractor phrases or altered symbolic forms can cause significant performance drops in state-of-the-art models (Mirzadeh et al., 2024; Tang et al., 2023). Models often incorporate such irrelevant details into their reasoning, revealing lack of sensitivity to salient information. Other studies show that models prioritize the surface form of reasoning over logical soundness; in some cases, longer but flawed reasoning paths yield better final answers than shorter, correct ones (Bentham et al., 2024). Similarly, performance does not scale with problem complexity as expectedmodels may overthink easy problems and give up on harder ones (Shojaee et al., 2025). Another critical concern is the faithfulness of the reasoning process. Intervention-based studies reveal that final answers often remain unchanged even when intermediate steps are falsified or omitted (Lanham et al., 2023), phenomenon dubbed the illusion of transparency (Bentham et al., 2024; Chen et al., 2025b). Together, these findings suggest that LLMs are not principled reasoners but rather sophisticated simulators of reasoning-like text. However, systematic understanding of why and when CoT reasoning fails is still mystery. Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Figure 2 Framework of DataA lchemy. It creates an isolated and controlled environment to train LLMs from scratch and probe the task, length, and format generalization. 2.3. OOD Generalization of LLMs Out-of-distribution (OOD) generalization, where test inputs differ from training data, remains key challenge in machine learning, particularly for large language models (LLMs)(Budnikov et al., 2025; Yang et al., 2024, 2023; Zhang et al., 2024b). Recent studies show that LLMs prompted to learn novel functions often revert to similar functions encountered during pretraining (Garg et al., 2022; Wang et al., 2024). Likewise, LLM generalization frequently depends on mapping new problems onto familiar compositional structures (Song et al., 2025). CoT prompting improves OOD generalization (Wei et al., 2022), with early work demonstrating length generalization for multi-step problems beyond training distributions (Shen et al., 2025; Yao et al., 2025). However, this ability is not inherent to CoT and heavily depends on model architecture and training setups. For instance, strong generalization in arithmetic tasks was achieved only when algorithmic structures were encoded into positional encodings (Cho et al., 2024). Similarly, finer-grained CoT demonstrations during training boost OOD performance, highlighting the importance of data granularity (Wang et al., 2025a). Theoretical and empirical evidence shows that CoT generalizes well only when test inputs share latent structures with training data; otherwise, performance declines sharply (Li et al., 2025a; Wang et al., 2025b). Despite its promise, CoT still struggles with genuinely novel tasks or formats. In the light of these brilliant findings, we propose rethinking CoT reasoning through data distribution lens: decomposing CoT into task, length, and format generalization, and systematically investigating each in controlled setting. 3. The Data Distribution Lens We propose fundamental reframing to understand what CoT actually represents. We hypothesize that the underlying mechanism is better understood through the lens of data distribution: rather than executing explicit reasoning procedures, CoT operates as pattern-matching process that interpolates and extrapolates from the statistical regularities present in its training distribution. Specifically, we posit that CoTs success stems not from models inherent reasoning capacity, but from its ability to generalize conditionally to out-of-distribution (OOD) test cases that are structurally similar to in-distribution exemplars. To formalize this view, we model CoT prompting as conditional generation process constrained by the distributional properties of the training data. Let Dtrain denote the training distribution over input-output pairs (𝑥, 𝑦), where 𝑥 represents reasoning problem and 𝑦 denotes the solution sequence (including intermediate reasoning steps). The model learns an approximation 𝑓𝜃(𝑥) 𝑦 by minimizing empirical risk over samples drawn from Dtrain. 4 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Let the expected training risk be defined as: 𝑅train( 𝑓𝜃) = 𝔼(𝑥,𝑦)Dtrain [ℓ( 𝑓𝜃(𝑥), 𝑦)], (1) where ℓ is task-specific loss function (e.g., cross-entropy, token-level accuracy). At inference time, given test input 𝑎test sampled from potentially different distribution Dtest, the model generates response 𝑦test conditioned on patterns learned from Dtrain. The corresponding expected test risk is: 𝑅test( 𝑓𝜃) = 𝔼(𝑥,𝑦)Dtest [ℓ( 𝑓𝜃(𝑥), 𝑦)]. (2) The degree to which the model generalizes from Dtrain to Dtest is governed by the distributional discrepancy between the two, which we quantify using divergence measures: Definition 3.1 (Distributional Discrepancy). Given training distribution Dtrain and test distribution Dtest, the distributional discrepancy is defined as: Δ(Dtrain, Dtest) = (Dtrain Dtest) (3) where ( ) is divergence measure (e.g., KL divergence, Wasserstein distance) that quantifies the statistical distance between the two distributions. Theorem 3.1 (CoT Generalization Bound). Let 𝑓𝜃 denote model trained on Dtrain with expected training risk 𝑅train( 𝑓𝜃). For test distribution Dtest, the expected test risk 𝑅test( 𝑓𝜃) is bounded by: 𝑅test( 𝑓𝜃) 𝑅train( 𝑓𝜃) + Λ Δ(Dtrain, Dtest) + (cid:32) (cid:33) log(1/𝛿) 𝑛 (4) where Λ > 0 is Lipschitz constant that depends on the model architecture and task complexity, 𝑛 is the training sample size, and the bound holds with probability 1 𝛿, where 𝛿 is the failure propability. The proof is provided in Appendix A.1 Building on this data distribution perspective, we identify three critical dimensions along which distributional shifts can occur, each revealing different aspects of CoTs pattern-matching nature: ➊ Task generalization examines how well CoT transfers across different types of reasoning tasks. Novel tasks may have unique elements and underlying logical structure, which introduces distributional shifts that challenge the models ability to apply learned reasoning patterns. ➋ Length generalization investigates CoTs robustness to reasoning chains of varying lengths. Since training data typically contains reasoning sequences within certain length range, test cases requiring substantially longer or shorter reasoning chains represent form of distributional shift along the sequence length dimension. This length discrepancy could result from the reasoning step or the text-dependent solution space. ➌ Format generalization explores how sensitive CoT is to variations in prompt formulation and structure. Due to various reasons (e.g., sophistical training data or diverse background of users), it is challenging for LLM practitioners to design golden prompt to elicit knowledge suitable for the current case. Their detailed definition and implementation are given in subsequent sections. Each dimension provides unique lens for understanding the boundaries of CoTs effectiveness and the mechanisms underlying its apparent reasoning capabilities. By systematically varying these dimensions in controlled experimental settings, we can empirically validate our hypothesis that CoT performance degrades predictably as distributional discrepancy increases, thereby revealing its fundamental nature as pattern-matching rather than reasoning system. 5 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens 4. DataAlchemy: An Isolated and Controlled Environment To systematically investigate the influence of distributional shifts on CoT reasoning capabilities, we introduce DataA lchemy, synthetic dataset framework designed for controlled experimentation. This environment enables us to train language models from scratch under precisely defined conditions, allowing for rigorous analysis of CoT behavior across different OOD scenarios. The overview is shown in Figure 2. 4.1. Basic Atoms and Elements Let = {A, B, C, . . . , Z} denote the alphabet of 26 basic atoms. An element is defined as an ordered sequence of atoms: = (𝑎0, 𝑎1, . . . , 𝑎𝑙1) where (5) This design provides versatile manipulation for the size of the dataset (i.e., = A𝑙) by varying element length 𝑙 to train language models with various capacities. Meanwhile, it also allows us to systematically probe text length generalization capabilities. 𝑎𝑖 A, 𝑙 ℤ+ 4.2. Transformations transformation is an operation that operates on elements 𝐹 : ˆe. In this work, we consider two fundamental transformations: the ROT Transformation and the Cyclic Position Shift. To formally define the transformations, we introduce bijective mapping 𝜙 : ℤ26, where ℤ26 = {0, 1, . . . , 25}, such that 𝜙(𝑐) maps character to its zero-based alphabetical index. Definition 4.1 (ROT Transformation). Given an element = (𝑎0, . . . , 𝑎𝑙1) and rotation parameter 𝑛 ℤ, the ROT Transformation 𝑓rot produces an element ˆe = (ˆ𝑎0, . . . , ˆ𝑎𝑙1). Each atom ˆ𝑎𝑖 is: ˆ𝑎𝑖 = 𝜙1((𝜙(𝑎𝑖) + 𝑛) (mod 26)) (6) This operation cyclically shifts each atom 𝑛 positions forward in alphabetical order. For example, if = (A, P, P, L, E) and 𝑛 = 13, then 𝑓rot(e, 13) = (N, C, C, Y, R). Definition 4.2 (Cyclic Position Shift). Given an element = (𝑎0, . . . , 𝑎𝑙1) and shift parameter 𝑛 ℤ, the Cyclic Position Shift 𝑓pos produces an element ˆe = (ˆ𝑎0, . . . , ˆ𝑎𝑙1). Each atom ˆ𝑎𝑖 is defined by cyclic shift of indices: ˆ𝑎𝑖 = 𝑎(𝑖𝑛) (mod 𝑙) (7) This transformation cyclically shifts the positions of the atoms within the sequence by 𝑛 positions to the right. For instance, if = (A, P, P, L, E) and 𝑛 = 1, then 𝑓pos(e, 1) = (E, A, P, P, L). Definition 4.3 (Generalized Compositional Transformation). To model multi-step reasoning, we define compositional transformation as the successive application of sequence of operations. Let 𝑆 = ( 𝑓1, 𝑓2, . . . , 𝑓𝑘) be sequence of operations, where each 𝑓𝑖 is one of the fundamental transformations = { 𝑓rot, 𝑓pos} with its respective parameters. The compositional transformation 𝑓S for the sequence 𝑆 is the function composition: The resulting element ˆe is obtained by applying the operations sequentially to an initial element e: 𝑓S = 𝑓𝑘 𝑓𝑘 𝑓1 ˆe = 𝑓𝑘 ( 𝑓𝑘1(. . . ( 𝑓1(e)) . . .)) (8) (9) Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens This design enables the construction of arbitrarily complex transformation chains by varying the type, parameters, order, and length of operations within the sequence. At the sample time, we can naturally acquire the COT reasoning step by decomposing the intermediate process: 𝑓S(e) : (cid:32) (cid:32) (cid:123)(cid:122) (cid:125) (cid:124) Query 4.3. Environment Setting (cid:124) 𝑓1 e(1) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 𝑓2 e(2) (cid:123)(cid:122) COT reasoning steps 𝑓𝑘1 e(𝑘1) 𝑓𝑘 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) ˆe (cid:124)(cid:123)(cid:122)(cid:125) Answer (10) Through systematic manipulation of elements and transformations, DataA lchemyoffers flexible and controllable framework for training LLMs from scratch, facilitating rigorous investigation of diverse OOD scenarios. Without specification, we employ decoder-only language model GPT2 (Radford et al., 2019) with configuration of 4 layers, 32 hidden dimensions, and 4 attention heads. We utilize Byte-Pair Encoding (BPE) tokenizer. Both LLMs and the tokenizer follow the general modern LLM pipeline. During the inference time, we set the temperature to 1e-5. For rigor, we also study LLMs with various parameters, architectures, and temperatures in Section 8. Details of the implementation are provided in the Appendix B. We consider that each element consists of 4 basic atoms, which produces 456,976 samples for each dataset with varied transformations and token amounts. We initialize the two transformations 𝑓1 = 𝑓rot(𝑒, 13) and 𝑓2 = 𝑓pos(𝑒, 1). We consider the exact match rate, Levenshtein distance (i.e., edit distance) (Yujian and Bo, 2007), and BLEU score (Papineni et al., 2002) as metrics and evaluate the produced reasoning step, answer, and full chain. Examples of the datasets and evaluations are shown in Appendix 5. Task Generalization Task generalization represents fundamental challenge for CoT reasoning, as it directly tests models ability to apply learned concepts and reasoning patterns to unseen scenarios. In our controlled experiments, both transformation and elements could be novel. Following this, we decompose task generalization into two primary dimensions: element generalization and transformation generalization. Task Generalization Complexity. Guided by the data distribution lens, we first introduce measure for generalization difficulty: Proposition 5.1 (Task Generalization Complexity). For reasoning chain 𝑓𝑆 operating on elements = (𝑎0, . . . , 𝑎𝑙1), define: TGC(𝐶) =𝛼 𝑚 𝑖= 𝕀 (cid:2)𝑎𝑖 𝑖 train (cid:3) + 𝛽 𝑛 𝑗=1 𝕀 (cid:2) 𝑓 𝑗 Ftrain (cid:3) + 𝛾𝕀 [( 𝑓1, 𝑓2, . . . , 𝑓𝑘) Ptrain ] + 𝐶𝑇 (11) as measurement of task discrepancy Δ𝑡𝑎𝑠𝑘, where 𝛼, 𝛽, 𝛾 are weighting parameters for different novelty types and 𝐶𝑇 is task specific constant. 𝑖 , Ftrain , and Ptrain denote the bit-wise element set, relation set and the order of relation set used during training. train We establish critical threshold beyond which CoT reasoning fails exponentially: Theorem 5.1 (Task Generalization Failure Threshold). There exists threshold 𝜏 such that when TGC(𝐶) > 𝜏, the probability of correct CoT reasoning drops exponentially: 𝑃(correct𝐶) 𝑒𝛿(TGC(𝐶) 𝜏) The proof is provided in Appendix A.2. (12) 7 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens 5.1. Transformation Generalization Transformation generalization evaluates the ability of CoT reasoning to effectively transfer when models encounter novel transformations during testing, which is an especially prevalent scenario in real-world applications. Experimental Setup. To systematically evaluate the impact of transformations, we conduct experiments by varying transformations between training and testing sets while keeping other factors constant (e.g., elements, length, and format). Guided by the intuition formalized in Proposition 5.1, we define four incremental levels of distribution shift in transformations as shown in Figure 2: (i) In-Distribution (ID): The transformations in the test set are identical to those observed during training, e.g., 𝑓1 𝑓1 𝑓1 𝑓1. (ii) Composition (CMP): Test samples comprise novel compositions of previously encountered transformations, though each individual transformation remains familiar, e.g., 𝑓1 𝑓1, 𝑓1 𝑓2, 𝑓2 𝑓1 𝑓2 𝑓2. (iii) Partial Out-of-Distribution (POOD): Test data include compositions involving at least one novel transformation not seen during training, e.g., 𝑓1 𝑓1 𝑓1 𝑓2. (iv) Out-of-Distribution (OOD): The test set contains entirely novel transformation types that are unseen in training, e.g., 𝑓1 𝑓1 𝑓2 𝑓2. Figure 3 Performance of CoT reasoning on transformation generalization. Efficacy of CoT reasoning declines as the degree of distributional discrepancy increases. Table 1 Full chain evaluation under different scenarios for transformation generalization. Transformation (Train Test) Scenario Exact Match Edit Distance BLEU Score 𝑓1 𝑓1 𝑓1 𝑓1 { 𝑓2 𝑓2, 𝑓1 𝑓2, 𝑓2 𝑓1} 𝑓1 𝑓1 𝑓1 𝑓2 𝑓1 𝑓1 𝑓2 𝑓2 𝑓1 𝑓 ID CMP POOD OOD 100.00% 0.01% 0.00% 0.00% 0 0.1326 0.1671 0.2997 1 0.6867 0.4538 0.2947 Findings. Figure 3 illustrates the performance of the full chain under different distribution discrepancies computed by task generalize complexities (normalized between 0 and 1) in Definition 5.1. We can observe that, in general, the effectiveness of CoT reasoning decreases when distribution discrepancy increases. For the instance shown in Table 1, from in-distribution to composition, POOD, and OOD, the exact match decreases from 1 to 0.01, 0, and 0, and the edit distance increases from 0 to 0.13, 0.17 when tested on data with transformation 𝑓1 𝑓1. Apart from ID, LLMs cannot produce correct full chain in most cases, while they can produce correct CoT reasoning when exposed to some composition and POOD conditions by accident. As shown in Table 2, from 𝑓1 𝑓2 to 𝑓2 𝑓2, the LLMs can correctly answer 0.1% of questions. close examination reveals that it is coincidence, e.g., the query element is , N, , N, which happened to produce the same result for the two operations detailed in the Appendix D.1. When further analysis is performed by breaking the full chain into reasoning steps and answers, we observe strong consistency between the reasoning steps and answers. For example, under the composition generalization setting, the reasoning steps are entirely correct on test data distribution 𝑓1 𝑓1 and 𝑓2 𝑓2, but with wrong answers. Probe these insistent cases in Appendix D.1, Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens we can find that when novel transformation (say 𝑓1 𝑓1) is present, LLMs try to generalize the reasoning paths based on the most similar ones (i.e., 𝑓1 𝑓2) seen during training, which leads to correct reasoning paths, yet incorrect answer, which echo the example in the introduction. Similarly, generalization from 𝑓1 𝑓2 to 𝑓2 𝑓1 or vice versa allows LLMs to produce correct answers that are attributed to the commutative property between the two orthogonal transformations with unfaithful reasoning paths. Collectively, the above results indicate that the CoT reasoning fails to generalize to novel transformations, not even to novel composition transforms. Rather than demonstrating true understanding of text, CoT reasoning under task transformations appears to reflect replication of patterns learned during training. Table 2 Evaluation on different components in CoT reasoning on transformation generalization. CoT reasoning shows inconsistency with the reasoning steps and answers. Transformation (Train Test) Exact Match Edit Distance Reason Answer Full Chain Reason Answer Full Chain { 𝑓1 𝑓1, 𝑓1 𝑓2, 𝑓2 𝑓1} 𝑓2 𝑓2 { 𝑓1 𝑓2, 𝑓2 𝑓1, 𝑓2 𝑓2} 𝑓1 𝑓1 𝑓1 𝑓2 𝑓2 𝑓1 𝑓2 𝑓1 𝑓1 𝑓2 100.00% 0.01% 100.00% 0.01% 0.00% 100.00% 0.00% 100.00% 0.01% 0.01% 0.00% 0.00% 0.000 0.000 0.373 0.373 0.481 0.481 0.000 0.000 0.133 0.133 0.167 0.167 Experiment settings. To further probe when CoT reasoning can generalize to unseen transformations, we conduct supervised fine-tuning (SFT) on small portion 𝜆 of unseen data. In this way, we can decrease the distribution discrepancy between the training and test sets, which might help LLMs to generalize to test queries. Findings. As shown in Figure 4, we can find that generally very small portion (𝜆 = 1.5𝑒4) of data can make the model quickly generalize to unseen transformations. The less discrepancy between the training and testing data, the quicker the model can generalize. This indicates that similar pattern appears in the training data, helping LLMs to generalize to the test dataset. 5.2. Element Generalization Element generalization is another critical factor to consider when LLMs try to generalize to new tasks. Figure 4 Performance on unseen transformation using SFT in various levels of distribution shift. Introducing small amount of unseen data helps CoT reasoning to generalize across different scenarios. Experiment settings. Similar to transformation generalization, we fix other factors and consider three progressive distribution shifts for elements: ID, CMP, and OOD, as shown in Figure 2. It is noted that in composition, we test if CoT reasoning can be generalized to novel combinations when seeing all the basic atoms in the elements, e.g., (A, B, C, D) (B, C, D, A). Based on the atom order in combination (can be measured by edit distance 𝑛), the CMP can be further developed. While for OOD, atoms that constitute the elements are totally unseen during the training. Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Findings. Similar to transformation generalization, the performances degrade sharply when facing the distribution shift consistently across all transformations, as shown in Figure 5. From ID to CMP and OOD, the exact match decreases from 1.0 to 0 and 0, for all cases. Most strikingly, the BLEU score is 0 when transferred to 𝑓1 and 𝑓2 transformations. failure case in Appendix D.1 shows that the models cannot respond to any words when novel elements are present. We further explore when CoT reasoning can generalize to novel elements by conducting SFT. The results are summarized in Figure 6. We evaluate the performance under three exact matches for the full chain under three scenarios, CMP based on the edit distance n. The result is similar to SFT on transformation. The performance increases rapidly when presented with similar (a small 𝑛) examples in the training data. Interestingly, the exact match rate for CoT reasoning aligns with the lower bound of performance when 𝑛 = 3, which might suggest the generalization of CoT reasoning on novel elements is very limited, even SFT on the downstream task. When we further analyze the exact match of reasoning, answer, and token during the training for 𝑛 = 3, as summarized in Figure 6b. We find that there is mismatch of accuracy between the answer and the reasoning step during the training process, which somehow might provide an explanation regarding why CoT reasoning is inconsistent in some cases. Figure 5 Element generalization results on various scenarios and relations. (a) Performance on unseen element via SFT in various CMP scenarios. (b) Evaluation of CoT reasoning in SFT. Figure 6 SFT performances for element generalization. SFT helps to generalize to novel elements. 6. Length Generalization Length generalization examines how CoT reasoning degrades when models encounter test cases that differ in length from their training distribution. The difference in length could be introduced from the text space or the reasoning space of the problem. Therefore, we decompose length generalization into two complementary aspects: text length generalization and reasoning step generalization. Guided by 10 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens instinct, we first propose to measure the length discrepancy. Length Extrapolation Bound. We establish power-law relationship for length extrapolation: Proposition 6.1 (Length Extrapolation Gaussian Degradation). For model trained on chain-of-thought sequences of fixed length 𝐿train, the generalization error at test length 𝐿 follows Gaussian distribution: (𝐿) = E0 + (1 E0) 1 exp (cid:32) (cid:32) (cid:33)(cid:33) (𝐿 𝐿train )2 2𝜎2 (13) where E0 is the in-distribution error at 𝐿 = 𝐿train, 𝜎 is the length generalization width parameter, and 𝐿 is the test sequence length The proof is provided in Appendix A.3. Table 3 Evaluation for text length generalization. Length 2 3 4 5 6 Exact Match (%) Edit Distance BLEU Score Full Chain Reason Answer Full Chain Reason Answer Full Chain Reason Answer 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 100.00% 100.00% 100.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.3772 0.2221 0.0000 0.1818 0.3294 0.4969 0.3203 0.0000 0.2667 0.4816 0.5000 0.2540 0.0000 0.2000 0. 0.4214 0.5471 1.0000 0.6220 0.4763 0.1186 0.1519 1.0000 0.1958 0.1174 0.0000 0.0000 1.0000 0.2688 0.2077 6.1. Text Length Generalization Text length generalization evaluates how CoT performance varies when the input text length (i.e., the element length 𝑙) differs from training examples. Considering the way LLMs process long text, this aspect is crucial because real-world problems often involve varying degrees of complexity that manifest as differences in problem statement length, context size, or information density. Experiment settings. We pre-train LLMs on the dataset with text length merely on 𝑙 = 4 while fixing other factors and evaluate the performance on variety of lengths. We consider three different padding strategies during the pre-training: (i) None: LLMs do not use any padding. (ii) Padding: We pad LLM to the max length of the context window. (iii) Group: We group the text and truncate it into segments with maximum length. Findings. As illustrated in the Table 3, the CoT reasoning failed to directly generate two test cases even though those lengths present mild distribution shift. Further, the performance declines as the length discrepancy increases shown in Figure 7. For instance, from data with 𝑙 = 4 to those with 𝑙 = 3 or 𝑙 = 5, the BLEU score decreases from 1 to 0.55 and 0.62. Examples in Appendix D.1 indicate that LLMs attempt to Figure 7 Performance of text length generalization across various padding strategies. Group strategies contribute to length generalization. 11 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens produce CoT reasoning with the same length as the training data by adding or removing tokens in the reasoning chains. The efficacy of CoT reasoning length generalization deteriorates as the discrepancy increases. Moreover, we consider using different padding strategy to decrease the divergence between the training data and test cases. We found that padding to the max length doesnt contribute to length generalization. However, the performance increases when we replace the padding with text by using the group strategy, which indicates its effectiveness. 6.2. Reasoning Step Generalization The reasoning step generalization investigates whether models can extrapolate to reasoning chains requiring different steps 𝑘 from those observed during training. which is popular setting in multi-step reasoning tasks. Experiment settings. Similar to text length generalization, we first pre-train the LLM with reasoning step 𝑘 = 2, and evaluate on data with reasoning step 𝑘 = 1 or 𝑘 = 3. (a) Reasoning step. From k=2 to k=1 (b) Reasoning step. From k=2 to k=3 Figure 8 Test performance for reasoning-step generalization across varying training data compositions. Performance varies systematically with changes in the distribution of training data components. Findings. As showcased in Figure 8, CoT reasoning cannot generalize across data requiring different reasoning steps, indicating the failure of generalization. Then, we try to decrease the distribution discrepancy introduced by gradually increasing the ratio of unseen data while keeping the dataset size the same when pre-training the model. And then, we evaluate the performance on two datasets. As we can observe, the performance on the target dataset increases along with the ratio. At the same time, the LLMs can not generalize to the original training dataset because of the small amount of training data. The trend is similar when testing different-step generalization, which follows the intuition and validates our hypothesis directly. 7. Format Generalization Format generalization assesses the robustness of CoT reasoning to surface-level variations in test queries. This dimension is especially crucial for determining whether models have internalized flexible, transferable reasoning strategies or remain reliant on the specific templates and phrasings encountered during training. 12 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Format Alignment Score. We introduce metric for measuring prompt similarity: Definition 7.1 (Format Alignment Score). For training prompt distribution 𝑃𝑡𝑟𝑎𝑖𝑛 and test prompt 𝑝𝑡𝑒𝑠𝑡: PAS( 𝑝𝑡𝑒𝑠𝑡) = max 𝑝 𝑃𝑡𝑟𝑎𝑖𝑛 cos(𝜙( 𝑝), 𝜙( 𝑝𝑡𝑒𝑠𝑡)) (14) where 𝜙 is prompt embedding function. (a) Format generalization. Performance under various perturbation methods. (b) Format generalization. Performance vs. various applied perturbation areas. Figure 9 Performance of format generalization. Testing performance varies with different noise levels and areas where the noise is applied. Experiment settings. To systematically probe this, we introduce four distinct perturbation modes to simulate scenario in real-world: (i) insertion, where noise token is inserted before each original token; (ii) deletion: it deletes the original token; (iii) modification: it replaces the original token with noise token; and (iv) hybrid mode: it combines multiple perturbations. Each mode is applied for tokens with probabilities 𝑝, enabling us to quantify the models resilience to increasing degrees of prompt distribution shift. Findings. As shown in Figure 9a, we found that generally CoT reasoning can be easily affected by the format changes. No matter insertion, deletion, modifications, or hybrid mode, it creates format discrepancy that affects the correctness. Among them, the deletion slightly affects the performance. While the insertions are relatively highly influential on the results. We further divide the query into several sections: elements, transformations, and prompt tokens. As shown in Figure 9b, we found that the elements and transformation play an important role in the format, whereas the changes to other tokens rarely affect the results. 8. Temperature and Model Size Temperature and model size generalization explores how variations in sampling temperature and model capacity can influence the stability and robustness of CoT reasoning. For the sake of rigorous evaluation, we further investigate whether different choices of temperatures and model sizes may significantly affect our results. Experiment settings. We explore the impact of different temperatures on the validity of the presented results. We adopt the same setting in the transformation generalization. 13 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Findings. As illustrated in Figure 10a, LLMs tend to generate consistent and reliable CoT reasoning across broad range of temperature settings (e.g., from 1e-5 up to 1), provided the values remain within suitable range. This stability is maintained even when the models are evaluated under variety of distribution shifts. (a) Influences of various temperatures. (b) Influences of various sizes. Figure 10 Temperature and model size. The findings hold under different temperatures and model sizes. Experiment settings. We further examine the influence of model size by employing the same experimental configuration as used in the novel relation SFT study. In particular, we first pretrain models of different sizes using the transformation 𝑓1 𝑓1, and subsequently perform SFT on 𝑓2 𝑓2 while varying the SFT ratios. Finding. Fig. 10b shows the accuracy of models with different sizes using different SFT ratios, which closely matches the result of our default model size across all evaluated settings and configurations. 9. Discussion and Implication Our investigation, conducted through the controlled environment of Data lc my, reveals that the apparent reasoning prowess of Chain-of-Thought (CoT) is largely brittle mirage. The findings across task, length, and format generalization experiments converge on conclusion: CoT is not mechanism for genuine logical inference but rather sophisticated form of structured pattern matching, fundamentally bounded by the data distribution seen during training. When pushed even slightly beyond this distribution, its performance degrades significantly, exposing the superficial nature of the reasoning it produces. While our experiments utilized models trained from scratch in controlled environment, the principles uncovered are extensible to large-scale pre-trained models. We summarize the implications for practitioners as follows. Guard Against Over-reliance and False Confidence. CoT should not be treated as plug-andplay module for robust reasoning, especially in high-stakes domains like medicine, finance, or legal analysis. The ability of LLMs to produce fluent nonsenseplausible but logically flawed reasoning chainscan be more deceptive and damaging than an outright incorrect answer, as it projects false aura of dependability. Sufficient auditing from domain experts is indispensable. 14 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Prioritize Out-of-Distribution (OOD) Testing. Standard validation practices, where the test set closely mirrors the training set, are insufficient to gauge the true robustness of CoT-enabled system. Practitioners must implement rigorous adversarial and OOD testing that systematically probes for vulnerabilities across task, length, and format variations. Recognize Fine-Tuning as Patch, Not Panacea. Our results show that Supervised FineTuning (SFT) can quickly patch models performance on new, specific data distribution. However, this should not be mistaken for achieving true generalization. It simply expands the models indistribution bubble slightly. Relying on SFT to fix every OOD failure is an unsustainable and reactive strategy that fails to address the core issue: the models lack of abstract reasoning capability. 10. Conclusion In this paper, we critically examine the COT reasoning of LLMs through the lens of data distribution, revealing that the perceived structured reasoning capability largely arises from inductive biases shaped by in-distribution training data. We propose controlled environment, Data lc my, allowing systematic probing of CoT reasoning along three crucial dimensions: task structure, reasoning length, and query format. Empirical findings consistently demonstrate that CoT reasoning effectively reproduces reasoning patterns closely aligned with training distributions but suffers significant degradation when faced with distributional deviations. Such observations reveal the inherent brittleness and superficiality of current CoT reasoning capabilities. We provide insights that emphasize real-world implications for both practitioners and researchers."
        },
        {
            "title": "References",
            "content": "O. Bentham, N. Stringham, and A. Marasovic. Chain-of-thought unfaithfulness as disguised accuracy. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview. net/forum?id=ydcrP55u2e. Reproducibility Certification. M. Budnikov, A. Bykova, and I. P. Yamshchikov. Generalization potential of large language models. Neural Computing and Applications, 37(4):19731997, 2025. Q. Chen, L. Qin, J. Liu, D. Peng, J. Guan, P. Wang, M. Hu, Y. Zhou, T. Gao, and W. Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025a. Y. Chen, J. Benton, A. Radhakrishnan, J. Uesato, C. Denison, J. Schulman, A. Somani, P. Hase, M. Wagner, F. Roger, et al. Reasoning models dont always say what they think. arXiv preprint arXiv:2505.05410, 2025b. H. Cho, J. Cha, P. Awasthi, S. Bhojanapalli, A. Gupta, and C. Yun. Position coupling: Improving length generalization of arithmetic transformers using task structure. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=5cIRdGM1uG. S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? case study of simple function classes. Advances in neural information processing systems, 35:3058330598, 2022. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 15 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens S. Imani, L. Du, and H. Shrivastava. Mathprompter: Mathematical reasoning using large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 3742, 2023. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. T. Lanham, A. Chen, A. Radhakrishnan, B. Steiner, C. Denison, D. Hernandez, D. Li, E. Durmus, E. Hubinger, J. Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023. H. Li, S. Lu, P.-Y. Chen, X. Cui, and M. Wang. Training nonlinear transformers for chain-of-thought inference: theoretical generalization analysis. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview.net/forum?id=n7n8McETXw. Y. Li, Z. Lai, W. Bao, Z. Tan, A. Dao, K. Sui, J. Shen, D. Liu, H. Liu, and Y. Kong. Visual large language models for generalized and specialized applications. arXiv preprint arXiv:2501.02765, 2025b. Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memisevic, and H. Su. Deductive verification of chain-ofthought reasoning. Advances in Neural Information Processing Systems, 36:3640736433, 2023. I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, and M. Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Z. Shen, H. Yan, L. Zhang, Z. Hu, Y. Du, and Y. He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. P. Shojaee, I. Mirzadeh, K. Alizadeh, M. Horton, S. Bengio, and M. Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity. arXiv preprint arXiv:2506.06941, 2025. J. Song, Z. Xu, and Y. Zhong. Out-of-distribution generalization via composition: lens through induction heads in transformers. Proceedings of the National Academy of Sciences, 122(6):e2417182122, 2025. X. Tang, Z. Zheng, J. Li, F. Meng, S.-C. Zhu, Y. Liang, and M. Zhang. Large language models are in-context semantic reasoners rather than symbolic reasoners. arXiv preprint arXiv:2305.14825, 2023. K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Q. Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face, 2024. 16 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens L. P.-Y. Ting, C. Zhao, Y.-H. Zeng, Y. J. Lim, and K.-T. Chuang. Beyond rag: Reinforced reasoning augmented generation for clinical notes. arXiv preprint arXiv:2506.05386, 2025. Q. Wang, Y. Wang, Y. Wang, and X. Ying. Can in-context learning really generalize to out-of-distribution tasks? arXiv preprint arXiv:2410.09695, 2024. X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= 1PL1NIMMrw. Y. Wang, F.-C. Chang, and P.-Y. Wu. Chain-of-thought prompting for out-of-distribution samples: latent-variable study. arXiv e-prints, pages arXiv2504, 2025a. Y. Wang, F.-C. Chang, and P.-Y. Wu. theoretical framework for ood robustness in transformers using gevrey classes. arXiv preprint arXiv:2504.12991, 2025b. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. J. Xu, H. Fei, L. Pan, Q. Liu, M.-L. Lee, and W. Hsu. Faithful logical reasoning via symbolic chain-ofthought. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1332613365, 2024. J. Yang, K. Zhou, Y. Li, and Z. Liu. Generalized out-of-distribution detection: survey. International Journal of Computer Vision, 132(12):56355662, 2024. L. Yang, Y. Song, X. Ren, C. Lyu, Y. Wang, J. Zhuo, L. Liu, J. Wang, J. Foster, and Y. Zhang. Out-ofdistribution generalization in natural language processing: Past, present, and future. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 45334559, 2023. S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36: 1180911822, 2023. X. Yao, R. Ren, Y. Liao, and Y. Liu. Unveiling the mechanisms of explicit cot training: How chain-ofthought enhances reasoning generalization. arXiv e-prints, pages arXiv2502, 2025. E. Yeo, Y. Tong, M. Niu, G. Neubig, and X. Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Z. Yu, L. He, Z. Wu, X. Dai, and J. Chen. Towards better chain-of-thought prompting strategies: survey. arXiv preprint arXiv:2310.04959, 2023. L. Yujian and L. Bo. normalized levenshtein distance metric. IEEE transactions on pattern analysis and machine intelligence, 29(6):10911095, 2007. X. Zhang, C. Du, T. Pang, Q. Liu, W. Gao, and M. Lin. Chain of preference optimization: Improving chain-of-thought reasoning in llms. Advances in Neural Information Processing Systems, 37:333356, 2024a. Y. Zhang, H. Wang, S. Feng, Z. Tan, X. Han, T. He, and Y. Tsvetkov. Can llm graph reasoning generalize beyond pattern memorization? In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 22892305, 2024b. 17 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens Z. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=5NTt8GFjUHkr. Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research, 2024, 2024c. C. Zhao, Z. Tan, C.-W. Wong, X. Zhao, T. Chen, and H. Liu. Scale: Towards collaborative content analysis in social science with large language model agents and human intervention. arXiv preprint arXiv:2502.10937, 2025. W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. A. Proof of Theorems A.1. Proof of CoT Generalization Bound Proof. Let 𝑓𝜃 be model trained on samples from the distribution Dtrain using loss function ℓ( 𝑓𝜃(𝑥), 𝑦) that is Λ-Lipschitz and bounded. The expected test risk is given by We can decompose the test risk as 𝑅test( 𝑓𝜃) = 𝔼(𝑥,𝑦)Dtest [ℓ( 𝑓𝜃(𝑥), 𝑦)] . 𝑅test( 𝑓𝜃) = 𝑅train( 𝑓𝜃) + (𝑅test( 𝑓𝜃) 𝑅train( 𝑓𝜃)) . (15) (16) To bound the discrepancy between 𝑅test and 𝑅train, we invoke standard result from statistical learning theory. Given that ℓ is Λ-Lipschitz and the discrepancy measure Δ(Dtrain, Dtest) is an integral probability metric (e.g., Wasserstein-1 distance), we have 𝑅test( 𝑓𝜃) 𝑅train( 𝑓𝜃) Λ Δ(Dtrain, Dtest). Therefore, the test risk satisfies 𝑅test( 𝑓𝜃) 𝑅train( 𝑓𝜃) + Λ Δ(Dtrain, Dtest). (17) (18) We next account for the generalization gap between the empirical training risk ˆ𝑅train( 𝑓𝜃) and the expected training risk 𝑅train( 𝑓𝜃). By applying concentration inequality (e.g., Hoeffdings inequality), with probability at least 1 𝛿, we have 𝑅train( 𝑓𝜃) ˆ𝑅train( 𝑓𝜃) + (cid:32) log(1/𝛿) 𝑛 (cid:33) , where 𝑛 is the number of training samples. Combining the above, we obtain that with high probability, 𝑅test( 𝑓𝜃) ˆ𝑅train( 𝑓𝜃) + Λ Δ(Dtrain, Dtest) + (cid:32) log(1/𝛿) 𝑛 (cid:33) . This concludes the proof. (19) (20) 18 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens A.2. Proof of Task Generalization Failure Threshold We establish the exponential decay bound through probabilistic analysis of reasoning failure modes in the presence of task generalization complexity. Let Ω denote the sample space of all possible reasoning configurations, and let 𝐶 Ω represent specific configuration. We define the following events: 𝐴𝑖 as the event that element 𝑎𝑖 is novel, i.e., 𝑎𝑖 𝑖 train; 𝐹 𝑗 as the event that transformation 𝑓 𝑗 is novel, i.e., 𝑓 𝑗 Ftrain; and as the event that the transformation sequence ( 𝑓1, 𝑓2, . . . , 𝑓𝑘) is novel, i.e., ( 𝑓1, 𝑓2, . . . , 𝑓𝑘) Ptrain. Here we make the assumption that the reasoning failures induced by novel arguments, functions, and patterns contribute independently to the overall failure probability and hence we model the success probability as product of component-wise success rates: 𝑃(correct𝐶) = 𝑃0 𝑚 (cid:214) 𝑖=1 𝜌𝕀[ 𝐴𝑖 ] 𝑎 𝑛 (cid:214) 𝑗=1 𝜌𝕀[𝐹 𝑗 ] 𝑓 𝜌𝕀[ ] 𝑝 𝜌𝐶𝑇 𝑐 where 𝑃0 (0, 1] represents the baseline success probability when all components are within the training distribution, and 𝜌𝑎, 𝜌 𝑓 , 𝜌𝑝, 𝜌𝑐 (0, 1) are the degradation factors associated with novel arguments, functions, patterns, and task-specific complexity, respectively. ln 𝑃(correct 𝐶) = ln 𝑃0 + 𝑚 𝑖=1 𝕀[ 𝐴𝑖] ln 𝜌𝑎 + 𝑛 𝑗=1 𝕀[𝐹 𝑗] ln 𝜌 𝑓 + 𝕀[Q] ln 𝜌𝑝 + 𝐶𝑇 ln 𝜌𝑐 (21) For notational convenience, we define the positive constants: 𝜉𝑎 := ln 𝜌𝑎 > 0, 𝜉 𝑓 := ln 𝜌 𝑓 > 0, 𝜉𝑝 := ln 𝜌𝑝 > 0, 𝜉𝑐 := ln 𝜌𝑐 > 0 hence we have: ln 𝑃(correct𝐶) = ln 𝑃0 𝜉𝑎 𝑚 𝑖=1 𝕀[ 𝐴𝑖] 𝜉 𝑓 𝑛 𝑗=1 𝕀[𝐹 𝑗] 𝜉𝑝𝕀[Q] 𝜉𝑐𝐶𝑇 (22) Lemma: Relationship to TGC. The expression in equation above can be bounded in terms of TGC(𝐶) as follows: ln 𝑃(correct𝐶) ln 𝑃0 𝛿 TGC(𝐶) where 𝛿 = min( 𝜉𝑎 𝛼 , 𝜉 𝑓 𝛽 , 𝜉𝑝 𝛾 , 𝜉𝑐) > 0. Proof of Lemma: From the definition of TGC(𝐶) in Eq. (11), we have: TGC(𝐶) = 𝛼 𝑚 𝑖= 𝕀[ 𝐴𝑖] + 𝛽 𝑛 𝑗=1 𝕀[𝐹 𝑗] + 𝛾𝕀[Q] + 𝐶𝑇 By the definition of 𝛿, each term in Eq. (22) satisfies: 𝜉𝑎 𝜉 𝑓 𝑚 𝑖=1 𝑛 𝑗=1 𝕀[ 𝐴𝑖] 𝛿𝛼 𝕀[𝐹 𝑗] 𝛿𝛽 𝑚 𝑖=1 𝑛 𝑗=1 𝕀[ 𝐴𝑖] 𝕀[𝐹 𝑗] (23) (24) (25) (26) Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens 𝜉𝑝𝕀[Q] 𝛿𝛾𝕀[Q] 𝜉𝑐𝐶𝑇 𝛿𝐶𝑇 Summing these inequalities establishes Eq. (23). We now define the threshold 𝜏 := ln 𝑃0 𝛿 . From Eq. (23), when TGC(𝐶) > 𝜏, we have: ln 𝑃(correct 𝐶) ln 𝑃0 𝛿 TGC(𝐶) = 𝛿(𝜏 TGC(𝐶)) = 𝛿(TGC(𝐶) 𝜏) Exponentiating both sides yields the desired bound: 𝑃(correct 𝐶) 𝑒𝛿(TGC(𝐶) 𝜏) A.3. Proof of Length Extrapolation Bound (27) (28) (29) (30) (31) Proof. Consider transformer model 𝑓𝜃 processing sequences of length 𝐿. The model implicitly learns position-dependent representations through positional encodings PE(𝑖) ℝ𝑑 for position 𝑖 {1, . . . , 𝐿} and attention patterns 𝐴𝑖 𝑗 = softmax (cid:18) 𝑄𝑖 𝐾𝑇 𝑗 𝑑 (cid:19) . During training on fixed length 𝐿train, the model learns specific distribution: 𝑝train(h) = 𝑝(h 𝐿 = 𝐿train) (32) where = {ℎ1, . . . , ℎ𝐿} represents hidden states. For sequences of length 𝐿 𝐿train, we encounter distribution shift in two forms: (1) positional encoding mismatch, where the model has never seen positions 𝑖 > 𝐿train if 𝐿 > 𝐿train, and (2) attention pattern disruption, where the learned attention patterns are calibrated for length 𝐿train. The KL divergence between training and test distributions can be bounded: 𝐷𝐾𝐿( 𝑝test 𝑝train) 𝐿 𝐿train2 (33) This quadratic relationship arises from linear accumulation of positional encoding errors and quadratic growth in attention pattern misalignment due to pairwise interactions. Let (𝐿) be the prediction error at length 𝐿. We decompose it as: (𝐿) = Einherent(𝐿) + Eshift(𝐿) (34) where Einherent(𝐿) = E0 is the inherent model error (constant) and Eshift(𝐿) is the error due to distribution shift. The distribution shift error follows from the Central Limit Theorem. As the error accumulates over sequence positions, the total shift error converges to: Eshift(𝐿) = (1 E0) (cid:18) 1 exp (cid:18) (𝐿 𝐿train)2 2𝜎 (cid:19)(cid:19) (35) This form ensures that Eshift(𝐿train) = 0 (no shift at training length) and lim 𝐿𝐿train Eshift(𝐿) = 1 E0 (maximum error bounded by 1). 20 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens The width parameter 𝜎 depends on: 𝜎 = 𝜎0 𝑑 𝐿train (36) where 𝜎0 is model-specific constant, 𝑑 is the model dimension, and the 𝑑/𝐿train factor captures the concentration of measure in high dimensions. Therefore, the total error follows: (𝐿) = E0 + (1 E0) (cid:18) 1 exp (cid:18) (𝐿 𝐿train)2 2𝜎2 (cid:19)(cid:19) (37) This Gaussian form naturally emerges from the accumulation of position-dependent errors and matches the experimental observation of near-zero error at 𝐿 = 𝐿train with symmetric increase in both directions. B. Experiment Details We fine-tune GPT-2style decoder-only Transformer with vocabulary size of 10,000. The model supports maximum context length of 256 tokens. The hidden dimension is 32, the number of Transformer layers is 4, and the number of attention heads is 4. Each block includes GELU-activated feed-forward sublayer with width 4 𝑑model. The model is trained using the AdamW optimiser in mixed precision (FP16). The default learning rate is 3 103, and the schedule follows cosine decay with 10% warm-up ratio. Training is conducted for 10 epochs, using batch size of 1024. weight decay of 0.01 is applied, and gradient norms are clipped at 1.0. C. Illustration of Datasets Below are the examples of transformation 𝑓1 and 𝑓2: Transformation[F1]: Q [F1] <answer> D Transformation[F2]: P [F2] <answer> A aside from single transformation, we can composite transformations arbitrarily: Transformation[F1F2]: A [F1] [F2] <think> V [F2] <answer> N Transformation[F2F2]: S [F2] [F2] <think> N [F2] <answer> O we use exact match, edit distance and BELU score to measure the discrepancy between generated tokens and the labels. For more than one transformation examples, we can further measure the discrepancy for reasoning and answering seperately. 21 Is Chain-of-Thought Reasoning of LLMs Mirage? Data Distribution Lens D. Additional Experimental Results D.1. Additional Qualitative Analysis D.1.1. Orthognal Transformation Caused Coincidence The following case shows that even if the transformation is different, the model that trained on transformation 𝑓2 𝑓1 can still provide correct answer through incorrect reasoning: B [ F1 ] [ F2 ] <answer > Prompt : Generated : A [ F1 ] <answer> N Expected : N D.1.2. Correct reasoning but failed in final answer The following case shows that the model pretrained on the union of three transformation 𝑓1 𝑓2, 𝑓2 𝑓1, 𝑓2 𝑓2 and test on 𝑓1 𝑓1 Prompt : Generated : Expected : D [R1] [R1] <t k > N [R1] <answer> N N [R1] <answer> D D.1.3. Failure to generalize to novel element The following case shows that the model trained on element set 𝑎𝑖 [ 𝐴, 𝑀] can not generalize to unseen elements such as or Prompt : Generated : Expected : N [ F1 ] [ F1 ] <t k > S [ F1 ] <answer> F B [ F1 ] <answer> O D.1.4. LLM reproduces CoT reasoning at seen lengths The following case shows that model trained under 𝑓1 𝑓1 tried to reproduce the length in training data by adding tokens in the reason chain even prompted with seen transformation 𝑓1 Prompt : Generated : D [ 1 ] <answer > A O [ 1 ] <answer> D Expected : O"
        }
    ],
    "affiliations": [
        "Arizona State University, USA"
    ]
}