{
    "paper_title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models",
    "authors": [
        "Xiaomei Zhang",
        "Zhaoxi Zhang",
        "Leo Yu Zhang",
        "Yanjun Zhang",
        "Guanhong Tao",
        "Shirui Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off."
        },
        {
            "title": "Start",
            "content": "Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Zhaoxi Zhang University of Technology Sydney zhaoxi.zhang-1@student.uts.edu.au Xiaomei Zhang Griffith University xiaomei.zhang@griffithuni.edu.au Leo Yu Zhang Griffith University leo.zhang@griffith.edu.au 6 2 0 2 7 ] . [ 1 2 4 0 2 1 . 1 0 6 2 : r Yanjun Zhang Griffith University yanjun.zhang@griffith.edu.au Guanhong Tao University of Utah guanhong.tao@utah.edu Shirui Pan Griffith University s.pan@griffith.edu.au Abstract Visual token compression is widely adopted to improve the inference efficiency of Large VisionLanguage Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose Compression-Aware Attack (CAA) to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA (T-CAA), where neither the target model nor the compression configuration is accessible. Experimental results show that compression-induced security risks persist even under these practical settings. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across multiple models, datasets, and compression methods consistently demonstrate that visual token compression significantly undermines model robustness, exposing previously overlooked trade-off between efficiency and security. ACM Reference Format: Xiaomei Zhang, Zhaoxi Zhang, Leo Yu Zhang, Yanjun Zhang, Guanhong Tao, and Shirui Pan. 2018. Less Is More Until It Breaks: Security Pitfalls of Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX Vision Token Compression in Large Vision-Language Models. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 19 pages. https: //doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nRecent years have witnessed rapid progress in Large Vision-Language\nModels (LVLMs), such as LLaVA [31] and the Qwen-VL [4] series,\nwhich integrate pretrained vision encoders with Large Language\nModels (LLMs) to powerful multimodal perception and reasoning.\nThese models achieve state-of-the-art performance on a wide range\nof tasks, including image captioning and visual question answer-\ning, and are increasingly deployed in real-world applications such\nas autonomous driving [6, 11], embodied agents [34], and multi-\nmodal assistants [24, 45]. To align with the next-token prediction\nparadigm of LLMs, visual inputs are encoded into sequences of\nvisual tokens and concatenated with text tokens for joint model-\ning [4, 10, 12, 31, 62]. While this design effectively leverages the\nreasoning capabilities of LLMs, it also inherits the quadratic com-\nputational and memory cost of self-attention with respect to the\ninput token length [23, 51]. This challenge is particularly acute for\nLVLMs, where high-resolution images and long videos can generate\ntens of thousands of visual tokens. For instance, Qwen-2.5VL [4]\nsupports up to 16k tokens for multi-image or video inputs, while\nLongVA [57] encodes a 128-frame video into over 18k visual tokens.\nSuch token explosion severely hinders the deployment of LVLMs in\nlatency-sensitive and resource-constrained environments [11, 53].\nTo address this challenge, visual token compression has emerged\nas a promising inference-time solution. This paradigm is motivated\nby two key observations: (i) visual data exhibits substantial spatial\nredundancy, where neighboring regions often convey highly sim-\nilar information [51]; and (ii) vision-language tasks often require\nonly a small subset of task-relevant visual evidence for a given text\nprompt [9, 29, 49, 54, 58, 60, 64]. Accordingly, token compression\nmethods rank visual tokens by estimated importance and retain\nonly the most informative ones, discarding redundant tokens with-\nout retraining the model [9, 29, 49, 51, 54, 58, 60, 64]. By adjusting\nthe token retention rate (i.e., the percentage of tokens preserved\nduring inference), these methods offer a flexible trade-off between\nefficiency and accuracy, making them well suited for real-time infer-\nence, edge deployment, and dynamic workload adaptation [25, 63].\nDespite its effectiveness in improving efficiency, the impact of vi-\nsual token compression on the robustness of LVLMs remains largely\nunexplored. In this work, we uncover a concerning phenomenon:",
            "content": "Conference acronym XX, June 0305, 2018, xx et al. task-critical evidence, thereby inducing failures under compressed inference. Building on this insight, CAA targets the compression mechanism through three tightly coupled components. First, CAA employs selective perturbation strategy that constrains adversarial perturbation to regions originally assigned low importance, reshaping compression behavior while minimally affecting uncompressed inference. Second, hierarchical ranking objective explicitly enforces adversarial reordering of token importance, enabling fine-grained manipulation of compression decisions. Finally, semantic erasure module corrupts the semantic content of tokens that survive adversarial compression, further amplifying performance degradation under compressed inference. We further extend CAA to realistic black-box settings, termed T-CAA, where neither the target model nor the compression configuration is known. Adversarial examples are generated on surrogate model and then transferred to the target model. In this setting, T-CAA optimizes adversarial perturbations to induce consistent manipulation of token importance across range of plausible compression layers. To address model uncertainty, we introduce an uninformative image border as the perturbation region, mitigating reliance on model-specific least-important regions. Using two jointly optimized universal templates that elevate border-token importance while mildly suppressing the original image content, TCAA reliably forces the retention of uninformative tokens, demonstrating that compression-induced risks persist under practical black-box constraints. We conduct extensive experiments across multiple state-of-theart LVLMs, diverse benchmarks, and representative token compression paradigms. Our results show that existing vision token compression methods suffer severe robustness degradation under CAA, revealing an inherent efficiency-security trade-off. We further evaluate potential defense strategies and find that they provide limited protection. Our main contributions are summarized as follows: We show that visual token compression fundamentally degrades the robustness of LVLMs: models that are robust under full-token inference become highly vulnerable once compression is enabled. We identify token importance ranking instability as key mechanism underlying this vulnerability. We propose Compression-Aware Attack (CAA), new adversarial framework that manipulates token importance ranking to induce failures only under compressed inference. We further extend CAA to realistic black-box settings, demonstrating that compression-induced vulnerabilities persist even when the target model and compression configuration are unknown. Through extensive evaluations across models, benchmarks, and compression methods, we demonstrate severe and consistent compression-specific robustness degradation. CAA induces an average Compression Sensitivity Gap (CSG) of 47.61%, compared to only 2.36% induced by baseline attacks. These results expose previously overlooked efficiencysecurity trade-off in visual token compression and highlight the need for more robust compression designs1. 1Our code: https://anonymous.4open.science/r/Compression_Aware_Attack-368D Figure 1: Compression-induced safety risk in autonomous driving. For clean inputs, both the compressed and uncompressed models attend to critical cues and make safe decisions. Under adversarial input (bottom), the uncompressed model remains correct, whereas compression discards critical visual cues and produces an unsafe Yes response, leading to severe accident. visual token compression fundamentally alters the robustness behavior of LVLMs. While models are generally robust to small random perturbations under full-token inference, they become highly vulnerable once compression is activated, with vulnerability increasing as the token retention rate decreases. This effect poses severe risks in practical systems, such as dynamic systems, where the visual token retention rate is often automatically adjusted according to system load, or computational budgets [16, 25, 63]. Under high-load and low-retention settings, compressed inference can exhibit severe performance failures that do not occur under full-token inference. This discrepancy complicates debugging and robustness evaluation, as adversarial inputs may appear benign during standard (uncompressed) offline testing yet reliably induce failures when the system operates under resource constraints. Such compression-specific failures introduce new and stealthy attack surface in the LVLM inference pipeline. To study compression-specific vulnerabilities, adversarial attacks must decouple failures caused by compressed inference from model robustness. However, existing adversarial attacks [5, 35, 38] are designed for fixed, standard inference settings and do not account for the presence of token compression. Consequently, they fail to capture failures that arise exclusively under compressed inference. Moreover, the non-differentiable token selection decisions introduced by compression further limit the applicability of classical end-to-end gradient-based attacks [5, 35]. To bridge this critical gap, we propose Compression-Aware Attack (CAA), new adversarial framework designed to expose vulnerabilities introduced by visual token compression. Our key insight is that compression relies on importance-based token selection, and that the relative ordering of token importance is inherently fragile. Small, imperceptible perturbations can alter this ordering, leading compression to preserve less relevant tokens while discarding Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018,"
        },
        {
            "title": "2 Background\n2.1 Large Vision Language Models\nLarge Vision-Language Models (LVLMs), such as LLaVA [31] and\nQwen-VL2 [4], integrate visual perception and language reasoning\nwithin a unified framework [10], enabling multimodal understand-\ning and generation across a broad spectrum of tasks.\nInput Processing. An LVLM ğ‘“ takes an image ğ¼ and a text prompt\nğ‘‡ as input, and generates a textual output sequence ğ‘Œ = ğ‘“ (ğ¼,ğ‘‡ ).\nThe vision encoder ğ‘“vis partitions the image ğ¼ into visual patches\nğ‘›ğ‘‰\nğ‘—=1 and encodes them into visual features, which are\nğ‘ƒ = {ğ‘ƒ ğ‘— }\nfurther mapped by a projection module ğ‘“proj into vision tokens\nğ‘›ğ‘‰\nğ‘—=1 = ğ‘“proj (ğ‘“vis (ğ¼ )), where each token ğ‘£ ğ‘— âˆˆ Rğ‘‘ corresponds\nğ‘‰ = {ğ‘£ ğ‘— }\nto a patch ğ‘ƒ ğ‘— , and ğ‘‘ is the hidden dimension of the language model\nğ‘›ğ‘‡\nğ‘“llm. The text prompt3 is tokenized into ğ‘‡ = {ğ‘¡ğ‘– }\nğ‘–=1. The vision\ntokens and text tokens are concatenated to form the initial input\nsequence ğ‘‹ (0) = [ğ‘‰ ,ğ‘‡ ], with total length ğ‘› = ğ‘›ğ‘‰ + ğ‘›ğ‘‡ .\nAttention Computation. The language model processes the input\nthrough ğ¿ transformer layers. At layer ğ‘™, given hidden states ğ‘‹ (ğ‘™ ) =\n{ğ‘¥ (ğ‘™ )\nâˆˆ Rğ‘‘ , the query and key vectors are computed\nğ‘–\nas",
            "content": "ğ‘–=1 with ğ‘¥ (ğ‘™ ) }ğ‘› ğ‘– = ğ‘¥ (ğ‘™ ) ğ‘ (ğ‘™ ) where ğ‘Š (ğ‘™ ) Rğ‘‘ ğ‘‘ğ‘˜ are learnable projection matrices and ğ‘‘ğ‘˜ is the per-head attention dimension. The attention weight from token ğ‘– to token ğ‘– is ğ‘– = ğ‘¥ (ğ‘™ ) ğ‘˜ (ğ‘™ ) ğ‘„ ,ğ‘Š (ğ‘™ ) ğ‘– ğ‘Š (ğ‘™ ) ğ‘„ , ğ‘– ğ‘Š (ğ‘™ ) ğ¾ , (1) ğ¾ ğ‘– ğ‘– . (cid:33) (2) (cid:32) ğ‘ (ğ‘™ ) ğ‘– = (cid:205)ğ‘› ğ‘–,ğ‘– ğ‘¥ (ğ‘™ ) ğ‘–=1 ğ‘ (ğ‘™ ) ğ‘ (ğ‘™ ) ğ‘–,ğ‘– = softmaxğ‘– (ğ‘˜ (ğ‘™ ) ğ‘– ) ğ‘‘ğ‘˜ ğ‘– ğ‘Š (ğ‘™ ) The attention output is computed as ğ‘œ (ğ‘™ ) ğ‘‰ , where ğ‘Š (ğ‘™ ) ğ‘‰ Rğ‘‘ ğ‘‘ğ‘˜ . The multi-head attention applies this operation in parallel across multiple heads, followed by linear projection and feed-forward network to produce the next-layer hidden states ğ‘‹ (ğ‘™+1) . After ğ¿ layers, the final hidden states ğ‘‹ (ğ¿) are used for autoregressive decoding to generate the output sequence ğ‘Œ until an end-of-sequence (EOS) token or maximum length is reached. Inference Complexity. Self-attention dominates the computational cost of transformer-based LVLMs, with quadratic complexity of (ğ‘›2) in the input length ğ‘›. In multimodal inference, this cost is largely driven by visual tokens since ğ‘›ğ‘‰ > ğ‘›ğ‘‡ . Empirically, the number of vision tokens ğ‘›ğ‘‰ often accounts for substantial portion of the input sequence (e.g., around 80% in common multimodal reasoning tasks [43]), making them the primary bottleneck for inference efficiency."
        },
        {
            "title": "2.2 Vision Token Compression\nVisual token compression has emerged as a promising approach\nfor alleviating the substantial computational burden caused by\nthe large number of visual tokens in LVLMs without requiring re-\ntraining. Motivated by the substantial spatial redundancy in visual\ndata, text-agnostic compression methods prune or merge visual\ntokens based on attention scores from the vision encoder or em-\nbedding similarity [39, 42, 51, 65]. However, these methods do not",
            "content": "2Throughout this work, we use Qwen and Qwen-VL interchangeably to refer to the Qwen2.5-VL model. 3For simplicity, we use ğ‘‡ to denote both the textual input and the corresponding text tokens. Figure 2: LVLM inference with vision token compression. Multimodal input (ğ¼,ğ‘‡ ) enters the LLM. At compression layer ( ğ‘™ ) , the module evaluates token importance (ğ¹ğ‘  ) and retains tokens based on retention rate ğ‘Ÿ ( ğ‘™ ) to produce the compressed sequence ğ‘‰ ( ğ‘™ ) , which forms the input for the subsequent layer ğ‘‰ ( ğ‘™+1) . The bottom row shows the visual token count. account for the task-specific visual requirements induced by language prompts. more effective approach is text-guided compression, which leverages textual context to perform more aggressive yet accuracy-preserving token reduction [9, 29, 49, 54, 58, 60, 64]. Accordingly, we focus on this compression in this work. More related work is provided in Appendix C. compression mechanism is characterized by three components: (i) where to compress, specified by set of target LLM layers ğ¿ = { ğ‘™1, ğ‘™2, }; (ii) how to score visual tokens, given by an importance function ğ¹ğ‘  ; and (iii) how many tokens to retain, specified by retention schedule ğ‘… = ğ‘Ÿ ( ğ‘™ ) (0, 1] ğ‘™ ğ¿, where ğ‘Ÿ ( ğ‘™ ) denotes the fraction of visual tokens retained at layer ğ‘™ relative to the original vision token count ğ‘›ğ‘‰ . Given input (ğ¼,ğ‘‡ ) and compression mechanism = { ğ¿, ğ‘…, ğ¹ğ‘  }, as illustrated in Fig. 2, the LVLM inference with compression is ğ‘Œ = ğ‘“ (ğ¼,ğ‘‡ ; C). At compression layer ğ‘™ ğ¿, the input sequence is [ğ‘‰ ( ğ‘™ ),ğ‘‡ ], ğ‘› ( ğ‘™ ) where ğ‘‰ ( ğ‘™ ) = {ğ‘£ ( ğ‘™ ) ğ‘—=1 denotes the visual tokens at that layer. Given ğ‘‰ ğ‘— } an importance function ğ¹ğ‘  and retention rate ğ‘Ÿ ( ğ‘™ ) , the compression operator ( ğ‘™ ) retains the top ğ‘Ÿ ( ğ‘™ )ğ‘›ğ‘‰ visual tokens according to their importance scores: ( ğ‘™ ) : (ğ‘‰ ( ğ‘™ ), ğ¹ğ‘ , ğ‘Ÿ ( ğ‘™ ) ) ğ‘‰ ( ğ‘™ ) . (3) Text tokens are left unchanged. Attention is the most widely adopted approach for estimating visual token importance in text-guided compression, as it directly reflects each visual tokens contribution to the models response under given text query [9, 29, 46, 58, 60]. Token compression of this form is theoretically guaranteed to preserve attention distributions and downstream model outputs [46], and have been adopted in practical systems [14, 15]. We focus on attention-based importance estimation in this work. Let Iref {1, , ğ‘›ğ‘‡ } denote the indices of reference text tokens, the importance score of ğ‘£ ( ğ‘™ ) ğ‘— is: ğ‘— = ğ¹ğ‘  (ğ‘£ ( ğ‘™ ) ğ‘  ( ğ‘™ ) ğ‘— ) = 1 Iref ğ‘– Iref ğ‘ ( ğ‘™ ) ğ‘› ( ğ‘™ ) ğ‘‰ +ğ‘–,ğ‘— , (4) Conference acronym XX, June 0305, 2018, xx et al. (a) LLaVA (b) Qwen-VL (a) LLaVA ğœ– = 32/255 (b) Qwen-VL ğœ– = 32/255 Figure 3: Robustness gap under different token retention rates on LLaVA and Qwen-VL. The solid curves (compressed, retention rate < 1) consistently lie above the dashed baseline (uncompressed), demonstrating that the introduction of compression incurs additional robustness degradation. Figure 4: Effect of token importance ranking on robustness under compressed inference. The shaded region highlights the significant performance recovery achieved by restoring correct rankings. is the attention weight from the ğ‘–-th text token (at where ğ‘ ( ğ‘™ ) ğ‘› ( ğ‘™ ) ğ‘‰ +ğ‘–,ğ‘— position (ğ‘› ( ğ‘™ ) ğ‘‰ + ğ‘–) in the current layers sequence) to the ğ‘—-th visual token. The compressed visual sequence is obtained by selecting the top-ranked tokens and restoring their original order: (ğ‘‰ ( ğ‘™ ), ğ‘  ( ğ‘™ ) ) ğ‘‰ ( ğ‘™ ) = Sortindex (cid:16)TopK (5) (cid:17) . ğ‘Ÿ ( ğ‘™ ) ğ‘›ğ‘‰ ğ‘‰ ( ğ‘™ ) is used to form the input to the next layer ğ‘‰ ( ğ‘™+1) . If multiple layers apply compression, each operation uses the already-compressed tokens from the previous layer."
        },
        {
            "title": "Induced by Compression",
            "content": "While vision token compression is widely used to accelerate LVLM inference, its impact on robustness remains underexplored. We first present empirical evidence that compression degrades robustness, and then analyze the compression pipeline to identify token ranking as the primary source of this degradation."
        },
        {
            "title": "Robustness",
            "content": "To empirically assess how compression affects model robustness, we conduct controlled experiments on the POPE dataset [27]. Following common practice, we adopt single-layer compression setup with compression applied at the second LLM layer [9]. We generate perturbed inputs by injecting Gaussian noise into the image, bounded by an â„“ norm ğœ– 16/255, 32/255, yielding samples (ğ¼ + ğ›¿ rand,ğ‘‡ ). We measure the robustness gap, defined as the performance difference between clean and perturbed inputs, under wide range of token retention rates from 1.0 (no compression) down to 0.1. The results on LLaVA and Qwen-VL are shown in Fig. 3. The dashed lines represent the robustness gap of the no-compression baseline (retention rate = 1), while the solid curves show the gaps under different token retention rates. Across models and perturbation magnitudes, token compression consistently enlarges the robustness gap compared to the uncompressed baseline, with the gap increasing as the retention rate decreases. These results indicate that visual token compression amplifies sensitivity to input perturbations and introduces additional robustness degradation."
        },
        {
            "title": "Under Compression",
            "content": "To understand why compression increases model vulnerability, we analyze its core mechanism: discrete token ranking and selection. The Critical Role of Token Importance Ranking in Robustness Degradation. Under compressed inference with perturbed inputs (ğ¼ +ğ›¿ rand,ğ‘‡ ), robustness degradation arises from two factors: (i) perturbations may destabilize importance ranking, leading to suboptimal token selection; and (ii) perturbations may corrupt the semantic content of the retained tokens. To disentangle these effects, Fig. 4 and Fig.13 in Appendix D.1 (for ğœ– =64/255) compare the model performance under three inference configurations: (a) Clean input + clean ranking (blue solid line), serving as the performance upper bound; (b) Perturbed input + perturbed ranking (gray dashed line), where the input is perturbed and compression is performed using rankings derived from the perturbed input; and (c) Perturbed input + clean ranking (orange solid line), controlled setting in which the input is perturbed but the compression module is forced to use fixed ranking derived from the clean input. Fig. 4 shows that restoring the clean ranking (from (b) to (c)) substantially recovers performance, bringing it close to the clean baseline in (a), as highlighted by the shaded region. These results indicate that robustness degradation under compression is primarily driven by instability in token importance ranking: identical perturbations largely preserve token semantics while substantially altering their relative ordering. Importance Ranking Is Unstable Under Input Perturbations. We next quantitatively compare the token rankings induced by the clean input (ğ¼,ğ‘‡ ) and the perturbed input (ğ¼ + ğ›¿ rand,ğ‘‡ ) through 3 metrics: (i) Rank correlation, measured by Kendalls ğœ and Spearmans ğœŒ 4, to characterize the overall agreement between the two rankings; (ii) Top-100 preservation rate, defined as the fraction of tokens that appear in the Top-100 under the clean input and remain in the Top-100 after perturbation; and (iii) Bottom-100 infiltration rate, defined as the fraction of tokens that belong to the Bottom-100 under the clean input but enter the Top-100 after perturbation. The results are shown in Fig. 5 for LLaVA and Fig. 14 for QwenVL in Appendix D.2. Increasing perturbation magnitude destabilizes 4They are classical rank correlation metrics that quantify the agreement between two orderings by comparing pairwise consistency and monotonic relationships [26], respectively. Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018, (a) LLaVA (b) LLaVA Figure 5: Ranking stability under random â„“-bounded noise with different budgets. Global stability is measured using Kendalls ğœ and Spearmans ğœŒ. Local stability is evaluated via Top-100 preservation and Bottom-100 infiltration rate. (a) LLaVA (b) Qwen-VL Figure 6: Impact of token selection strategy under compression. Retaining the Top-ğ‘˜ most important tokens preserves performance, while retaining the Bottom-ğ‘˜ least important tokens leads to drastic performance collapse. token-importance rankings, as reflected by declining Kendalls ğœ and Spearmans ğœŒ, reduced Top-100 preservation, and increased Bottom-100 infiltration, ultimately degrading model predictions. Extreme Misranking Leads to Complete Model Collapse. To further illustrate the critical role of correct token selection, we consider an extreme scenario: instead of retaining the Top-ğ‘˜ tokens based on importance scores, we intentionally keep the Bottom-ğ‘˜ tokens. As shown in Fig. 6, this reversed selection leads to complete performance collapse, demonstrating the models strong dependence on selecting the correct subset of visual tokens. This impact is amplified in multi-layer compression, such early misselection is irreversible and propagates across layers."
        },
        {
            "title": "4 The Proposed Compression-Aware Attack\n4.1 Motivation\nSection 3 shows that visual token compression introduces a com-\npression specific robustness degradation driven by instability in im-\nportance ranking. As compression is increasingly adopted to reduce\ninference cost in practical deployments, understanding the security\nimplications of this mechanism becomes essential. For example, in\ndynamic systems, compression is enabled and the retention rate is\nadjusted to satisfy latency or resource constraints [16, 25, 63]. As\na consequence, the same input may be processed under different\ncompression states at different times, and failures that occur only in\ncompressed settings are unlikely to surface during standard security\ninspection, which is typically conducted under resource-sufficient,",
            "content": "non-compressed inference. These state-dependent failures remain hidden, rendering reliable reproduction and attribution difficult. To reveal and measure such compression-induced vulnerabilities, it is essential to disentangle them from the models inherent adversarial weaknesses. This necessitates attacks that preserve fullsequence behavior while inducing failure only when compression is enabled. Traditional adversarial attacks fail this requirement, as they are designed to induce failures in the full-sequence (noncompressed) model and are unaware of the compression mechanism, thereby conflating compression-induced failures with inherent model vulnerabilities [5, 35, 38]. Moreover, compression introduces discrete, non-differentiable operations, which render gradient-based optimization ineffective. These challenges call for new class of attacks that are explicitly aware of the compression."
        },
        {
            "title": "4.2 Threat Model",
            "content": "Attacker Goal. The attacker aims to craft visual adversarial input that induce failures through the vision token compression mechanism, causing the model to break down under compressed inference while remaining unaffected in the non-compressed setting. Formally, the compression-aware attack objective is: max ğ›¿ s.t. (ğ‘“ (ğ¼ + ğ›¿,ğ‘‡ ; C), ğ‘“ (ğ¼,ğ‘‡ )) ğœ† (ğ‘“ (ğ¼ + ğ›¿,ğ‘‡ ), ğ‘“ (ğ¼,ğ‘‡ )) ğ›¿ ğœ–. (6) where ğ‘“ (ğ¼ + ğ›¿,ğ‘‡ ; C) and ğ‘“ (ğ¼ + ğ›¿,ğ‘‡ ) are the outputs on the adversarial input with and without compression, respectively, and (, ) measures the divergence between two outputs. Attacker Capabilities. We consider two attack settings: White-box Setting. The attacker has full access to the target model parameters, gradients, and the details of the compression mechanism. This setting enables direct gradient-based optimization and represents worst-case scenario for assessing the security limits of compressed inference. Black-box Setting. The attacker lacks access to target model parameters, gradients, and compression configurations (e.g., layer placement, retention rates). Furthermore, modern LVLM systems typically deploy access control mechanisms to mitigate malicious behavior, rendering query-intensive probing attacks infeasible [13, 19, 36]."
        },
        {
            "title": "4.3 White-box Attack\nSection 3 shows that ranking instability drives compression-induced\nrobustness degradation. Thus, directly manipulating importance\nscores offers a principled strategy for compression-aware attacks.\nAttack Analysis. In the white-box setting, the attacker has full\nknowledge of the target model and the compression algorithm, in-\ncluding the first compression layer Ëœğ‘™1 and its retention rate ğ‘Ÿ ( Ëœğ‘™1 ) . The\nattack therefore targets this stage to disrupt importance ranking,\ncausing task-irrelevant tokens to be retained. This strategy applies\nto both single- and multi-layer compression, as token selection at\nËœğ‘™1 is irreversible and errors propagate to all subsequent stages.\nOverview. Guided by this analysis, our attack perturbs selected\nimage regions to manipulate token-importance rankings, preserv-\ning uncompressed behavior while degrading compressed inference.",
            "content": "Conference acronym XX, June 0305, 2018, xx et al. Figure 7: CAA selectively perturbs low-importance regions to preserve correct behavior under non-compressed inference. hierarchical ranking objective manipulates token importance ordering, promoting low-importance tokens over the most important ones and reversing their relative order, while semantic erasure objective removes informative cues from them. As result, the adversarial image induces failures only under compressed inference. The attack comprises three components  (Fig. 7)  : (i) Selective perturbation, which restricts modifications to low-importance regions to preserve uncompressed inference; (ii) Hierarchical ranking objective, which effectively promotes low-importance tokens into the retained set; and (iii) Semantic erasure, which corrupts retained tokens to further impair compressed inference. Selective Perturbation. To satisfy the compression-aware objective in Eq. 6, the attack must maximize the discrepancy between compressed and uncompressed inference. This requires avoiding perturbations to high-importance visual tokens, which encode the core semantic evidence for correct predictions. Consequently, we restrict perturbations exclusively to the least important regions. Given the visual tokens ğ‘‰ = {ğ‘£ ğ‘— } ğ‘›ğ‘‰ ğ‘—=1 from image ğ¼ , we compute their importance scores ğ‘  ( ğ‘™ ) (Eq. 4) at layer ğ‘™ and sort them ğ‘— in descending order. Let Î©most denote the indices of the highestscoring tokens and Î©least denote those of the lowest-scoring tokens. Perturbations are applied only to patches corresponding to Î©least: Ë†ğ‘ƒ ğ‘— = (cid:40)ğ‘ƒ ğ‘— + ğ›¿ ğ‘—, ğ‘ƒ ğ‘—, ğ‘— Î©least, ğ›¿ ğ‘— ğœ–, ğ‘— Î©least, and Ë†ğ¼ = [ Ë†ğ‘ƒ ğ‘— ] ğ‘›ğ‘‰ ğ‘—=1. (7) ğ‘›ğ‘‰ ğ‘›ğ‘‰ where [ Ë†ğ‘ƒ ğ‘— ] ğ‘—=1 satisğ‘—=1 reconstructs the full image, and ğ›¿ = [ğ›¿ ğ‘— ] fies ğ›¿ ğœ–. Under uncompressed inference, ranking perturbations may temporarily divert attention to less informative tokens, but core semantic content remains intact, allowing later layers to re-attend to informative tokens and maintain stable predictions. In contrast, under compressed inference, the same ranking shift causes irreversible token removal, forcing reliance on perturbed low-importance tokens and leading to severe performance degradation. Hierarchical Ranking Objective. Under perturbation ğ›¿, the perturbed image Ë†ğ¼ yields set of visual tokens Ë†ğ‘‰ at the attack layer, each associated with perturbed importance score Ë†ğ‘  ( ğ‘™ ) (Eq. 4). To ğ‘— manipulate relative importance ordering, we adopt Bayesian Personalized Ranking (BPR) [40], extending it to the token set setting to enforce least-important tokens to outrank most-important ones. However, this inter-group constraint leaves the internal ordering of Î©least unconstrained, allowing the model to retain the most informative tokens within Î©least. To prevent this, we introduce intra-group least least, , Î© (ğ‘›ğ‘” ) reversal. We partition Î©least into ğ‘›ğ‘” subgroups Î© (1) according to their clean importance scores, where smaller indices correspond to higher clean informativeness. The resulting hierarchical ranking objective comprises two components: (i) Inter-group manipulation: all tokens in Î©least are ranked above those in Î©most; (ii) Intra-group reversal: the ordering within Î©least is inverted, such that tokens with lower clean importance receive higher perturbed scores. These conditions are encoded via two sets of pairwise preferences: LM = {(â„“, ğ‘š) â„“ Î© (ğ‘”) (ğ‘”) = {(â„“ğ‘, â„“ğ‘) â„“ğ‘ Î© (ğ‘) (ğ‘,ğ‘) LL least, ğ‘š Î©most}, least, â„“ğ‘ Î© (ğ‘ ) least, ğ‘ < ğ‘}, The hierarchical ranking loss is: Lrank (ğ›¿) = ğ›¼ ğ‘›ğ‘” ğ‘”=1 log ğœ (cid:0)Ë†ğ‘  ( ğ‘™ ) â„“ Ë†ğ‘  ( ğ‘™ ) ğ‘š (cid:1) (â„“,ğ‘š) (ğ‘”) LM ğ›½ ğ‘<ğ‘ (â„“ğ‘ ,â„“ğ‘ ) (ğ‘,ğ‘) LL log ğœ (cid:0)Ë†ğ‘  ( ğ‘™ ) â„“ğ‘ Ë†ğ‘  ( ğ‘™ ) â„“ğ‘ (cid:1), (8) (9) where ğ›¼ and ğ›½ balance the interand intra-group terms. These constraints enforce the ordering Î© (ğ‘›ğ‘” ) > Î©most, least causing compression to retain minimally informative tokens. > > Î© (1) least To stabilize and accelerate optimization, we exploit the fact that importance scores are proportional to the dot product of vision keys and text queries (Section 2.2). Consequently, we explicitly encourage the alignment of perturbed vision keys with reference text queries and define the query-guided alignment loss as: Lkey (ğ›¿) = 1 Î©least Iref Ë†ğ‘ ( ğ‘™ ) ğ‘– Ë†ğ‘˜ ( ğ‘™ ) â„“ , â„“ Î©least ğ‘– Iref (10) â„“ ğ‘Š ( ğ‘™ ) ğ‘– ğ‘Š ( ğ‘™ ) ğ‘– = Ë†ğ‘¡ ( ğ‘™ ) ğ‘„ and Ë†ğ‘˜ ( ğ‘™ ) â„“ = Ë†ğ‘£ ( ğ‘™ ) where Ë†ğ‘ ( ğ‘™ ) ğ¾ are computed from the perturbed input ( Ë†ğ¼,ğ‘‡ ), and Iref denotes the reference text indices. Semantic Erasure. While the hierarchical objective controls token selection, retained tokens may still preserve residual semantics that support inference. To eliminate such information, we introduce semantic erasure objective that corrupts the representations of the Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018, Figure 8: T-CAA employs border perturbation to avoid reliance on model-specific least-informative regions. Two self-universal templates are optimized: raise template that amplifies border importance and down template that suppresses the original image. As result, the attack transfers across models, preserving uncompressed utility while failing under compression in the target model. retained tokens. Let ğ‘£ ( ğ‘™ ) denote the visual token represenâ„“ tations at layer ğ‘™ under the clean image ğ¼ and the perturbed image Ë†ğ¼ , respectively. We encourage maximal deviation between them: and Ë†ğ‘£ ( ğ‘™ ) â„“ Lerase (ğ›¿) = 1 Î©least â„“ Î©least (cid:13) Ë†ğ‘£ ( ğ‘™ ) â„“ ğ‘£ ( ğ‘™ ) (cid:13) (cid:13) â„“ 2 (cid:13) (cid:13) (cid:13) . (11) This objective complements the ranking manipulation by ensuring that the tokens selected by compression are both incorrectly ranked and semantically uninformative. Total Objective. The total objective is formulated as: min ğ›¿ Lrank (ğ›¿) + ğœ†ğ‘’ Lerase (ğ›¿) + ğœ†ğ‘˜ Lkey (ğ›¿) s.t. ğ›¿ ğœ–. (12) Here, ğ›¿ denotes the effective perturbation applied to the image, obtained by aggregating per-patch perturbations {ğ›¿ ğ‘— }. We solve this optimization problem using projected gradient descent (PGD) [35], and summarize the attack procedure in Algorithm 1 in Appendix F."
        },
        {
            "title": "4.4 Transfer Attack Under Black-box Setting\nAttack Analysis. Compared to the white-box setting, the black-\nbox attacker follows the same high-level attack strategy: targeting\nthe layer where token compression is applied and optimizing per-\nturbations to induce incorrect token selection under compression.\nHowever, due to limited access to the deployed system, the attacker\nfaces two fundamental gaps:",
            "content": "Unknown Compression Configuration: The attacker has no knowledge of the compression mechanisms internal configuration, including the placement of compression layers and their retention rates, which prevents precise identification of the first compression stage or tailor attacks to specific retention settings. Limited Target Model Access: The attacker has no access to the target models parameters, internal states, or gradients, and is restricted to interacting with the model through query-limited API, making gradient-based or query-intensive attacks infeasible. 4.4.1 Handling Unknown Compression Configurations. Although the attacker has no knowledge of the exact compression configuration, it can estimate range of compression layers ğ¿ğ‘ over which the attack is expected to be effective, and jointly manipulate tokenimportance rankings across all layers within this range. This enforces consistent adversarial token orderings throughout the candidate layers and ensures attack effectiveness despite configuration uncertainty. Consequently, if the target model applies token compression at least once within the estimated range, the attack induces erroneous token selection at that layer. To estimate the desired range, the attacker first infers the target models average retention rate ğ‘Ÿ defined as the average fraction of visual tokens retained per layer across the model, from public documentation [15, 18, 37] or limited API latency measurements by exploring differences in inference time between compressed and non-compressed settings. Given this estimate, the attacker enumerates plausible compression configurations consistent with common design practices [9, 49, 51, 58] (e.g., multi-stage compression with progressively decreasing retention rate), and derives the corresponding range of candidate compression layers over which the attack is applied (An example is provided in the Appendix G.2). 4.4.2 Handling Limited Target Model Access. To address the lack of access to target model, we adopt transfer-based attacks. However, model mismatches introduce two key challenges: (i) adversarial perturbations tend to decay during transfer [22, 41], weakening their ability to elevate least-important tokens over salient ones in the target model, resulting in ineffective attacks; (ii) different LVLMs often identify different image regions as the least important for the same input [21, 48], as illustrated in Fig. 18 in Appendix G.1, making region selection non-transferable. To address these challenges, we introduce common least informative region by augmenting the input image with an uninformative border, inspired by [20, 33]. Based on this design, we optimize two self-universal perturbation templates with complementary roles to construct transferable adversarial examples. (i) raise template ğ›¿ğ‘… is applied to the border region to consistently amplify the importance of border tokens, forming shared lowsalience region across models. (ii) down template ğ›¿ğ· is applied Conference acronym XX, June 0305, 2018, xx et al. to the original image content to mildly suppress its attention and reduce the dominance of inherently salient tokens. Both templates are jointly optimized across the candidate compression-layer range, ensuring robustness to unknown compression configurations. An overview of Transferable Compression-Aware Attack (T-CAA) is illustrated in Fig. 8. Self-Universal Perturbation Templates. Prior work has shown that self-universal perturbations can mitigate the effectiveness decay commonly observed in transfer attacks [47]. Motivated by this, we jointly optimize two self-universal perturbation templates, ğ›¿ğ‘… and ğ›¿ğ· , over the candidate compression-layer range to enable consistent attention manipulation across models. Before optimization, for each candidate compression layer ğ‘™ of the surrogate model ğ‘“sur, we extract the visual token sequence ğ‘‰ ( ğ‘™ ) = {ğ‘£ ( ğ‘™ ) ğ‘›ğ‘‰ ğ‘—=1 from the clean image ğ¼ and sort the tokens by değ‘— } scending importance. We then select set of shared lowest-ranked tokens Î©least and partition it into two subsets Î©high , such least that Î©least = Î©high least ranked tokens Î©most and partition it into Î©high . Similarly, we define the set of highestleast Î©low and Î©low least most and Î©low most. The raise template ğ›¿ğ‘… is applied to regions corresponding to tokens in Î©low to increase their importance above that of unperleast turbed highest-ranked tokens, thereby altering the token ranking used by compression. Formally, the loss at layer ğ‘™ is defined as Lraise (ğ›¿ğ‘…, ğ‘™) = ğ›¼raise log ğœ (cid:16)Ë†ğ‘  ( ğ‘™ ) â„“ Ë†ğ‘  ( ğ‘™ ) ğ‘š (cid:17) â„“ Î©low least ğ‘šÎ©high most ğ›¾raise 1 Î©low least Iref Ë†ğ‘ ( ğ‘™ ) ğ‘– Ë†ğ‘˜ ( ğ‘™ ) â„“ , â„“ Î©low least ğ‘– Iref (13) denote the importance score and key vector where Ë†ğ‘  ( ğ‘™ ) and Ë†ğ‘˜ ( ğ‘™ ) â„“ of token Ë†ğ‘£â„“ at layer ğ‘™, and { Ë†ğ‘ ( ğ‘™ ) denote the query vectors of ğ‘– }ğ‘– Iref the reference text tokens used for compression, all computed from the perturbed input ( Ë†ğ¼,ğ‘‡ ). For the down template ğ›¿ğ· , we apply ğ›¿ğ· uniformly to tokens in Î©low most to suppress their importance, ensuring . Formally, the loss at layer ğ‘™ is their scores fall below those of Î©high least defined as Ldown (ğ›¿ğ·, ğ‘™) = ğ›¼down log ğœ (cid:16)Ë†ğ‘  ( ğ‘™ ) ğ‘š Ë†ğ‘  ( ğ‘™ ) â„“ (cid:17) ğ‘šÎ©low most â„“ Î©high least + ğ›¾down 1 Î©low most Iref Ë†ğ‘ ( ğ‘™ ) ğ‘– Ë†ğ‘˜ ( ğ‘™ ) ğ‘š . ğ‘šÎ©low most ğ‘– Iref (14) Final Perturbation and Transfer Objective. We jointly optimize ğ›¿ğ‘… and ğ›¿ğ· across all candidate layers ğ¿ğ‘ on the surrogate model via gradient-based updates. The transfer objective is min ğ›¿ğ‘…, ğ›¿ğ· Ltransfer = 1 ğ¿ğ‘ s.t. ğ›¿ğ‘… ğœ–ğ‘…, Lraise (ğ›¿ğ‘…, ğ‘™) + Ldown (ğ›¿ğ·, ğ‘™), ğ‘™ ğ¿ğ‘ ğ›¿ğ· ğœ–ğ·, (15) Border-Based Perturbation. After optimization, we construct the final adversarial image by applying ğ›¿ğ‘… to all border patches and ğ›¿ğ· to all patches belonging to the original image: Ë†ğ‘ƒ ğ‘— = (cid:40)ğ‘ƒ ğ‘— + ğ›¿ğ‘…, ğ‘ƒ ğ‘— + ğ›¿ğ·, ğ‘— B, otherwise, Ë†ğ¼border = { Ë†ğ‘ƒ ğ‘— } ğ‘›ğ‘‰ + ğ‘—=1 . (16) is the set of border-aligned patches; is the number of border tokens; and Ë†ğ¼border the border-augmented image. We adopt asymmetric perturbation budgets with ğœ–ğ· < ğœ–ğ‘…, weakly modifying the original image to preserve uncompressed performance while strongly perturbing the border to ensure its retention under compression. As the border consists of the down template that lack task-relevant semantics. It establishes consistent low-importance prior across models, enabling transferable compression-induced degradation"
        },
        {
            "title": "5.2 Evaluation Metrics\nTo evaluate compression-aware behavior, we define three met-\nrics based on model performance M (input, state), where input âˆˆ\n{cl, adv} (clean or adversarial) and state âˆˆ {nc, c} (non-compressed\nor compressed). These metrics measure an attackâ€™s ability to pre-\nserve accuracy without compression while selectively degrading\nperformance under compression:",
            "content": "Uncompressed Performance Retention (UPR) (): We define UPR = (adv, nc) . UPR quantifies the extent to which (cl, nc) adversarial examples preserve model performance under uncompressed inference. Compressed Attack Effectiveness (CAE) (): We define CAE = 1 (adv, c) , which quantifies the relative perforM (cl, c) mance degradation induced by the attack under compression. Compression Sensitivity Gap (CSG) (): We define CSG = (cl, nc) (adv, c) (adv, nc) to captures the performance gap inM (cl, c) duced by compression. Larger values indicate benign behavior without compression and harmful effects under compression, reflecting state-dependent behavior. Vision Token Compression Methods. We evaluate our attacks on three representative compression methods [52, 55]. FastV [9] applies single-layer vision token compression at early LLM layers, with token importance guided by the final text token. PDrop [49] employs progressive multi-layer compression, gradually pruning visual tokens across depth based on importance signals from the final text token. SparseVLM [58] performs multi-layer vision token compression guided by text tokens relevant to visual features, merging low-information visual tokens. Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018, Table 1: Comparison between our Compression-Aware Attack (CAA) and two baselines. CAA causes sharp performance drop only in the compressed model while leaving the uncompressed model largely unaffected, resulting in stable CSG that quantifies the additional risk introduced by the compression mechanism. Dataset Model M(cl,nc) M(cl,c) Vanilla Attack Random Attack M(adv,nc) M(adv,c) UPR CAE CSG M(adv,nc) M(adv,c) UPR CAE CSG M(adv,nc) M(adv,c) UPR Compression-Aware Attack CAE CSG POPE TextVQA MME POPE TextVQA MME LLaVA LLaVA-NEXT Qwen-VL LLaVA LLaVA-NEXT Qwen-VL LLaVA LLaVA-NEXT Qwen-VL LLaVA LLaVA-NEXT Qwen-VL LLaVA LLaVA-NEXT Qwen-VL LLaVA LLaVA-NEXT Qwen-VL 0.9775 0.9003 0.9550 0.8817 0.6318 0.9135 0.8714 0.8617 0.9085 0.9775 0.9003 0.9550 0.8817 0.6318 0.9135 0.8714 0.8617 0. 0.8842 0.8199 0.8199 0.7428 0.5820 0.6662 0.8360 0.8264 0.7451 0.6785 0.6913 0.7781 0.6154 0.5042 0.6714 0.7556 0.8135 0. 0.1210 0.2866 0.1465 0.1306 0.1541 0.3261 0.4713 0.4459 0.2051 0.1210 0.2866 0.1720 0.1306 0.1541 0.3299 0.4713 0.4459 0. 0.1238 0.3183 0.1534 0.1481 0.2439 0.3570 0.5409 0.5175 0.2258 0.1238 0.3183 0.1801 0.1481 0.2439 0.3249 0.5409 0.5175 0. 0.8559 0.7203 0.7980 0.7359 0.7418 0.5383 0.4438 0.4759 0.5613 0.8122 0.7421 0.6071 0.6253 0.6363 0.5532 0.4183 0.4519 0. FastV -0.0203 -0.1559 -0.0782 -0.1404 -0.1345 -0.3379 -0.4324 -0.4003 -0.3149 0.9550 0.8650 0.9260 0.7473 0.5695 0. 0.8489 0.8424 0.8954 SparseVLM -0.0640 0.0604 -0.2395 -0.2266 -0.1198 -0.1219 -0.0408 -0.0307 -0.1952 0.9550 0.8650 0. 0.7473 0.5695 0.8556 0.8489 0.8424 0.8424 0.1274 0.2293 0.1656 0.1962 0.1503 0.3076 0.4650 0.4331 0.3269 0.1274 0.1783 0. 0.2306 0.1834 0.3000 0.4395 0.4459 0.3205 0.7910 0.7492 0.7524 0.6331 0.4977 0.6196 0.7685 0.8006 0.7340 0.6624 0.6629 0. 0.5206 0.4539 0.6299 0.7358 0.7878 0.7009 0.9770 0.9608 0.9696 0.8476 0.9014 0.9366 0.9742 0.9776 0.9856 0.9770 0.9608 0. 0.8476 0.9014 0.9342 0.9742 0.9776 0.9272 0.1054 0.0862 0.0823 0.1477 0.1448 0.0699 0.0807 0.0312 0.0149 0.0237 0.0411 0. 0.1540 0.0998 0.0618 0.0262 0.0316 0.0795 0.0824 0.0470 0.0520 -0.0047 0.0462 0.0066 0.0549 0.0088 0.0005 0.0007 0.0019 0. 0.0016 0.0012 -0.0016 0.0004 0.0092 0.0067 0.9196 0.9045 0.8746 0.7354 0.6000 0.8585 0.8071 0.7500 0.8489 0.9003 0.8981 0. 0.7217 0.6338 0.8331 0.7500 0.7833 0.8706 0.3633 0.3631 0.0611 0.2158 0.3382 0.1916 0.4823 0.4000 0.2572 0.3215 0.3248 0. 0.1548 0.2726 0.3376 0.4157 0.4667 0.3495 0.9408 1.0047 0.9158 0.8341 0.9497 0.9398 0.9262 0.9262 0.9344 0.9210 0.9210 0. 0.8185 1.0032 0.9120 0.8607 0.9090 0.9583 0.5891 0.5571 0.9255 0.7095 0.4189 0.7124 0.4231 0.5160 0.6548 0.5262 0.5302 0. 0.7485 0.4593 0.4972 0.4498 0.4263 0.5410 0.5299 0.5618 0.8413 0.5435 0.3686 0.6522 0.3493 0.3863 0.5892 0.4472 0.5277 0. 0.5670 0.4625 0.4092 0.3105 0.3353 0.4993 Table 2: CAA performance under different token retention rates on POPE. Model LLaVA Qwen-VL Ratio M(cl, nc) M(cl,c) M(adv,nc) M(adv,c) UPR 0.9210 0.9312 0.9440 0.9408 0.9868 0.9518 0.9404 0.9164 0.8842 0.7395 0.9003 0.9102 0.9228 0.9196 0.9646 0.6049 0.5618 0.4759 0.3633 0.1593 0.5 0.4 0.3 0.2 0.1 0. 0.5 0.4 0.3 0.2 0.1 0.9550 0.9325 0.9165 0.9003 0.8199 0.7846 0.8167 0.8578 0.8746 0.8746 0.9108 0.3794 0.2941 0.2219 0.0611 0.1083 0.8552 0.8982 0.9158 0.9158 0. CAE 0.3645 0.4026 0.4807 0.5891 0.7846 0.5931 0.6791 0.7535 0.9255 0.8620 CSG 0.2855 0.3337 0.4247 0.5299 0.7714 0.4483 0.5773 0.6693 0.8413 0.8157 Table 3: Ablation on Selective Perturbation. \"CAA w/ Full\" perturbs the entire image. \"Most-only\" targets highimportance regions to simultaneously corrupt semantics and ensure retention during compression. Model Most-Only CAE UPR CSG UPR CAA-Full CAE CSG UPR CAA CAE CSG POPE LLaVA Qwen-VL 0.8818 0. 0.1893 0.7427 0.0712 0.4363 0.8235 0.7272 0.6146 0.7137 0.4381 0.4410 0.9408 0. 0.5891 0.9255 LLaVA Qwen-VL 0.7116 0.6895 0.2971 0.5650 0.0087 0.2545 0.6769 0. 0.6060 0.6288 0.2828 0.3982 0.8341 0.9398 0.7095 0.7124 TextVQA 0.5299 0. 0.5435 0."
        },
        {
            "title": "5.3 Compression Aware-Attack Evaluation\n5.3.1 Experimental Setup. We evaluate CAA against two baseline\nattacks. (i) Vanilla Attack: Existing adversarial attacks rely on\ngradient-based optimization and designed for non-compressed mod-\nels [5, 38], therefore they cannot be directly applied to compressed\ninference which involves discrete operations (e.g., ranking and\npruning). We thus evaluate vanilla perturbations crafted on the\nnon-compressed model under compression to test whether they\ncan trigger compression-specific failures. Implementation details of",
            "content": "vanilla adversarial attacks are provided in Appendix F.1. (ii) Random Attack: This baseline applies random noise to the entire input, without targeting model predictions or the compression mechanism. The uncontrolled perturbation cannot serve as reliable attack for quantifying compression-induced robustness degradation. Following prior work [49, 58, 60, 65], we focus on compression settings with initial retention rates in the range of 0.1 to 0.5 to balance efficiency and performance. For multi-layer compression, we target the first compression layer and its retention rate. Unless otherwise specified, all white-box experiments are conducted on FastV [9] with compression applied at the second layer and retention rate of 0.2, using perturbation budget of 32/255. We also evaluate different perturbation budgets in Appendix F.5. 5.3.2 Attack Performance. Results for the commonly used retention rate of 0.2 are reported in Table 1 and Table 9 (Appendix F.2), while additional results for other retention rates are provided in Table 2 and Table 10 in Appendix F.2. Under CAA, compressed models suffer substantial performance degradation (average drop = 56.60%), while uncompressed models largely preserve accuracy (UPR = 91.99%), yielding high average CSG of 47.61%. In contrast, vanilla adversarial attacks exhibit highly unstable and often negative CSG values, indicating that they primarily disrupt uncompressed inference, and their adversarial effects are frequently attenuated by compression. Random attacks yield consistently positive but marginal CSG (2.36%), reflecting their inability to reliably exploiting this vulnerability. To understand why CAA induces compression-specific failures, we analyze how perturbations alter token-importance rankings across layers. Figure 9 tracks the evolution of Top-100 preservation and Bottom-100 infiltration rates, defined in Section 3.2. At the attack layer (layer 2, retention rate 0.2), CAA disrupts the ranking by promoting originally irrelevant Bottom-100 tokens into the Top100 set. Under uncompressed inference, this distortion is transient, as subsequent layers reallocate attention to correct the misaligned Conference acronym XX, June 0305, 2018, xx et al. (a) LLaVA (b) Qwen-VL (a) Rate Mismatch (b) Layer Mismatch Figure 9: Layer-wise Top-100 preservation and Bottom-100 infiltration rates under adversarial attack (attack at Layer 2). Figure 11: (a) Mismatch between attack and test retention rates. (b) Mismatch between the attack layer and the compression layer. Evaluated on LLaVA with the POPE dataset. (a) LLaVA (b) Qwen-VL Figure 10: Comparison between CAA, which explicitly optimizes relative token ordering via BPR, and variant that maximizes only absolute importance scores. focus. Under compression, however, token selection becomes irreversible: noisy tokens are locked into the Top-100 set, permanently discarding essential visual evidence and causing inevitable performance collapse. Table 4: Ablation of CAA Components. Three variants are formed by progressively removing hierarchical ranking, semantic erasure, and query-guided term. Attack CAA w/o Hierarchy CAA w/o Erasure CAA w/o Query CAA POPE CAE 0.5532 0.5883 0.4896 0.5891 UPR 0.9433 0.9357 0.9597 0.9408 CSG 0.4965 0.5240 0.4493 0.5299 UPR 0.7738 0.7728 0.8240 0.8341 TextVQA CAE 0.5485 0.6236 0.5866 0. CSG 0.3223 0.3964 0.4105 0.5435 MME CAE 0.3282 0.3713 0.4390 0.4231 UPR 0.9705 0.9445 0.8937 0.9262 CSG 0.2987 0.3157 0.3327 0.3493 5.3.3 The Impact of Selective Perturbation. To assess the necessity of selective perturbation, we compare CAA with two alternative attack variants. (i) CAA-Full, which perturbs the entire image, and (ii) Most-Only, which perturbs only high-importance tokens to corrupt semantics while preserving their ranks for retention under compression. As shown in Table 3, CAA-Full substantially degrades non-compressed inference (Avg. UPR 0.9152 0.7691), resulting in low CSG (0.3475). Meanwhile, the Most-Only attack attains high CAE (up to 0.7427) but exhibits low average CSG (0.1919), as conflicting objectives between semantic corruption and rank preservation limit its effectiveness. Impact of the BPR Ranking Loss. Comparing the BPR-based 5.3.4 objective with variant that directly optimizes absolute importance scores (CAA w/o BPR) highlights the critical role of BPR. Fig. 10 shows that omitting BPR collapses the average CAE from 0.6691 to 0.1715 and reduces CSG by 0.4482. This confirms that increasing absolute scores is insufficient for Top-ğ‘˜ inclusion, as compression relies on relative ranking. BPR addresses this by enforcing pairwise constraints for stable rank manipulation. Impact of Key Components. We ablate three key components 5.3.5 of CAA, with results summarized in Table 4 for LLaVA and Table 11 for Qwen-VL (Appendix F.3). Hierarchical Ranking Constraint (CAA w/o Hierarchy). Removing the hierarchical ranking constraint substantially weakens the attack, reducing CAE from 0.6691 to 0.5714 and CSG to 0.4954. Without explicit intra-group ranking optimization, informative tokens can still remain highly ranked, enabling correct reasoning. Semantic Erasure (CAA w/o Erasure). Omitting semantic erasure also degrades attack effectiveness, lowering average CAE (0.6691 0.5807) and CSG (0.5842 0.4887). We adopt semantic erasure instead of fixed meaningless targets, as the latter conflicts with the query-guided alignment required for effective ranking manipulation. Query-Guided Optimization (CAA w/o Query). Removing query guidance leads to clear drop in CSG (0.5842 0.4864), as token importance is determined by keyquery dot products, making ranking manipulation less stable and effective without explicit guidance. 5.3.6 Effectiveness to Configuration Mismatch. We study CAA under mismatched attack-time and test-time compression seetings. Retention Rate Mismatch. We generate adversarial examples with attack retention rates in {0.5, 0.3, 0.2} and evaluate them under test retention rates in {0.7, 0.5, 0.3, 0.2}. As shown in Fig. 11a and Fig. 15 (Appendix F.4), CAA sustains strong attack impact under moderate mismatch: when the test retention rate is higher, retained clean tokens dilute the attack effect, whereas when it is lower or equal, retained tokens are dominated by the perturbed set. Layer Mismatch. We vary the attack layer around representative compression layers (Layers 2 and Layer 8) and report results in Fig. 11b and Fig. 16 (Appendix F.4). Attacks are remain effective under moderate layer mismatch. For example, on LLaVA-POPE with compression at Layer 2, attacking the same layer achieves CSG of 0.6986, while attacking Layer 8 still yields 0.4106. Our analysis suggests an intuitive strategy when compression configurations are unknown: perturb larger set of low-importance tokens and target later layers, thereby maximizing perturbed token overlap and preserving promoted tokens across layers. Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018,"
        },
        {
            "title": "5.4 Transfer Attack Evaluation\n5.4.1 Experimental Setup. In the black-box setting, the attacker\ndoes not know the exact compression layers or retention rates, and\ntherefore enforces the desired token ranking over a plausible range\nof layers. The attack remains effective as long as this range includes\nthe earliest compression layer within the attack-effective regime,\nsince ranking errors introduced there propagate through subse-\nquent compression stages. As shown in Section 5.3, CAA remains\neffective across a broad range of retention rates (ğ‘Ÿ âˆˆ [0.1, 0.4]),\noffering a practical margin against configuration uncertainty. To\nderive such a candidate range, the attacker first obtains a coarse\nestimate of the target modelâ€™s average retention rate Â¯ğ‘Ÿ (i.e., the av-\nerage fraction of visual tokens retained per layer), for example\nfrom public documentation or a small number of API-side latency\nprobes. Given Â¯ğ‘Ÿ , the attacker enumerates compression configu-\nrations consistent with common deployment practices reported\nin prior work [9, 42, 49, 58, 60]. Specifically, following the litera-\nture, we summarize two practical constraints: (i) Rate constraints:\nthe retention rate at the first compression layer is drawn from\n{0.1, 0.2, . . . , 0.9}, and at subsequent stages decreases progressively\nas ğ‘Ÿ ( Ëœğ‘™ğ‘– ) = max(cid:16) 1\n2ğ‘Ÿ ( Ëœğ‘™ğ‘– âˆ’1 ), 0.1(cid:17); (ii) Layer constraints: successive\ncompression layers, indexed by Ëœğ‘™ğ‘– and Ëœğ‘™ğ‘–+1, satisfy Ëœğ‘™ğ‘–+1 âˆ’ Ëœğ‘™ğ‘– â‰¥ 4\nand compression typically starts after the first layer. Under these\nconstraints, the attacker enumerates all feasible compression config-\nurations ( Ëœğ¿, ğ‘…) via depth-first search, where Ëœğ¿ denotes compression\nlayers and ğ‘… their retention rates. An example for Â¯ğ‘Ÿ â‰¤ 0.2 is provided\nin Appendix G.2.",
            "content": "Following prior implementations [49, 51, 58, 64], we focus on commonly used regime with average retention rate ğ‘Ÿ [0.1, 0.4], where compression is applied at most three times. Enumerating all configurations within this regime, we select layers 1-11 as the candidate attack range ğ¿ğ‘ . This choice covers over 80% of feasible configurations, measured by the fraction for which the earliest compression layer with retention rate in the attack-effective regime (ğ‘Ÿ [0.1, 0.4]) falls within ğ¿ğ‘ . Perturbation templates that increase token importance are assigned larger budget (255/255), as they are applied to the border region, whereas down templates use smaller budget (16/255). Covering approximately 30% of the visual patches after border augmentation, the border is sufficient to dominate the compressed representation, given that the maximum attack-effective retention rate is 0.4. Visually, this 30% border mimics natural margins or background regions, ensuring the adversarial examples remain plausible and unobtrusive in realistic settings. 5.4.2 Attack Performance for T-CAA. Transferability Across Models and Compression Configurations. We focus on the commonly used average retention-rate regime ğ‘Ÿ [0.1, 0.4], and evaluate representative single-layer and multi-layer compression configurations that realize similar average retention rates5. To hedge against uncertainty in the target compression configuration, adversarial examples are generated once on surrogate model using fixed retention rate ğ‘Ÿ = 0.4 across layers 1-11, and reused across 5We omit multi-layer settings with first compression layer retention rate of 0.1, as under the setup in Section 5.4.1 this degenerates into single-layer compression with retention rate 0.1. Table 5: T-CAA performance across different models and compression configurations. Rate means retention rate. Layer Configuration Rate 0.3 0.2 0.1 2 8 10 2,8 2, 8,16 0.2 0.1 0.1 0.5,0.25 0.4,0.2 0.3,0.15 0.2,0.1 0.5,0.25 0.4,0.2 0.3,0.15 0.2,0.1 0.3,0.15 0.2,0. 0.7,0.35,0.175 0.5,0.25,0.125 0.4,0.2,0.1 0.3,0.15,0.1 0.2,0.1,0.1 0.3,0.15,0.1 0.2,0.1,0.1 2,8,16 8,16,24 Avg. Rate 0.344 0.250 0. 0.400 0.325 0.381 0.344 0.288 0.231 0.175 0.359 0.300 0.241 0.181 0.400 0.350 0.369 0.281 0.238 0.206 0. 0.388 0.350 LLaVA->LLaVA-NEXT LLaVA->Qwen-VL Qwen-VL->LLaVA CAE 0.2795 0.5461 0.6212 CSG 0.1694 0.4361 0.5111 CSG 0.2095 0.3943 0.5195 CAE 0.3491 0.5338 0.6591 CAE 0.4061 0.5791 0. CSG 0.2207 0.3937 0.4833 0.4237 0.5295 0.4014 0.2641 0.3845 0.4688 0.4999 0.2501 0.4233 0.4662 0.5077 0.3455 0. 0.2477 0.2664 0.4308 0.4688 0.5249 0.3235 0.3333 0.3136 0.4194 0.2914 0.1540 0.2745 0.3587 0.3899 0.1400 0.3133 0.3561 0. 0.2355 0.2853 0.1377 0.1564 0.3207 0.3587 0.4149 0.2134 0.2233 0.3498 0.3303 0.4014 0.3064 0.4239 0.4573 0. 0.3286 0.4468 0.5050 0.5966 0.3313 0.3715 0.2896 0.3136 0.3551 0.4573 0.5787 0.2973 0.3028 0.2102 0.1908 0. 0.1668 0.2843 0.3177 0.4465 0.1891 0.3073 0.3654 0.4570 0.1917 0.2319 0.1500 0.1741 0.2155 0.3177 0.4391 0.1577 0.1632 0.4853 0. 0.3868 0.4183 0.4183 0.4504 0.6230 0.4091 0.4437 0.4391 0.5109 0.3818 0.3817 0.3934 0.4115 0.4265 0.4843 0.5787 0.3950 0. 0.2999 0.3662 0.2014 0.2329 0.2330 0.2650 0.4376 0.2238 0.2583 0.2537 0.3255 0.1965 0.1963 0.2080 0.2261 0.2411 0.2989 0. 0.2096 0.2497 Table 6: Comparison of single- (\"Final\") vs. multi-token (\"Multi\") reference strategies at similar avg. retention rates. Configuration Rate Layer Avg. Rate 0.2 0.250 Model UPR LLaVA->Qwen-VL 0.8388 Qwen-VL->LLaVA 0.8145 Final->Multi CAE 0.5273 0.4604 CSG 0.3661 0. Final->Final UPR 0.5338 0.5338 0.8604 0.5412 0.8146 2,8 0.3,0.15 0.231 2,8, 0.4,0.2,0.1 0.238 0.8388 LLaVA->Qwen-VL Qwen-VL->LLaVA 0.8145 LLaVA->Qwen-VL 0.8388 Qwen-VL->LLaVA 0.8145 0.5388 0.4778 0.3456 0. 0.3777 0.2923 0.1845 0.2401 0.8604 0.8146 0.8604 0.8146 0.4573 0.4504 0.3551 0. CSG 0.3943 0.3558 0.3177 0.2650 0.2155 0.2411 all compression settings. Consequently, UPR is fixed per transfer pair: 0.8899 (LLaVA LLaVA-Next), 0.8604 (LLaVA Qwen), and 0.8146 (Qwen LLaVA). As shown in Table 5 and Table 13 for POPE (Appendix G.3), T-CAA demonstrates robust transferability, achieving an average CSG of 29.97%. These results confirm that T-CAA reliably exposes compression vulnerabilities in black-box settings, without requiring precise knowledge of either the target model or its compression configuration. Effect of Text-Reference Strategies on Transfer Attacks. We examine whether different strategies for computing visual-token importance influence transfer attack performance. Specifically, we compare (i) using the final text token as the reference [9, 49] and (ii) jointly using multiple text tokens [58]. Results in Table 6 for POPE show negligible differences between the two strategies. This is attributed to the border-based design of T-CAA, where visually uninformative border tokens are inherently ranked as low-importance regardless of the text prompt. As result, both strategies yield comparable performance. 5.4.3 Comparison with Traditional Black-box Adversarial Attacks. Although query limits and variable compression states typically invalidate traditional black-box attacks, we evaluate T-CAA against representative baselines under relaxed threat model. This setting allows repeated queries for evaluation, condition that is rarely met in real-world deployments. We consider two label-only attacks: HSJA [8], which estimates gradients via binary feedback, Conference acronym XX, June 0305, 2018, xx et al. Table 7: Comparison of T-CAA with two representative traditional black-box adversarial attacks. Target Model UPR HSJA CAE CSG UPR RayS CAE CSG UPR T-CAA CAE CSG LLaVA Qwen-VL 0.9778 0.9915 0.1192 0.0610 0.0970 0.0525 LLaVA Qwen-VL 0.7146 0. 0.2745 0.2986 -0.0109 -0.1251 POPE 0.9871 0.9271 0.1071 0.1610 0.0942 0.0881 0.8146 0. 0.5791 0.5338 0.3937 0.3943 TextVQA 0.6689 0.9794 0.3286 0.0236 -0.0024 0.0030 0.9256 0. 0.5024 0.4305 0.4280 0.3103 Table 8: Adaptive Defense Performance. We compare the undefended baseline (\"None\") against random and importancebased region removal defenses guided by reference LVLMs. Mask type + Ref Model M(cl,nc) M(cl,c) M(adv,nc) M(adv,c) None (Baseline) Most - LLaVA Least - LLaVA Least - Qwen Random 0.9769 0.8496 0.9538 0.9362 0. 0.8408 0.6554 0.8362 0.8064 0.8192 0.9538 0.9346 0.9115 0.9231 0.9269 0.3365 0.6685 0.3423 0.3984 0.4035 CSG 0.5761 0.1616 0.5259 0.4711 0.4689 (a) LLaVA-Rotation (b) LLaVA-JPEG (c) LLaVA-Blur (d) Qwen-Rotation (e) Qwen-JPEG (f) Qwen-Blur Figure 12: CLIP Embedding similarity between the original image and its transformed variants (rotation, JPEG compression, and Gaussian blur) for clean and CAA adversarial samples. and RayS [7], which employs discrete ray search for boundary detection. Both are tested in the untargeted setting (Section 5.3.1) with query budget of 600 and ğœ– = 32/255. As shown in Table 7, T-CAA substantially outperforms both baselines. All results are obtained under single-layer compression applied at Layer 2 with retention rate of 0.2. HSJA and RayS struggle to probe consistent decision boundary, as perturbations alter the set of retained tokens, effectively changing the compressed model state with each query. Even when successful, they indiscriminately degrade both compressed and uncompressed inference. In contrast, T-CAA explicitly targets the compression mechanism, inducing compression-specific failures and offering deeper insights for safer compression design. 5.4.4 Ablation Study for T-CAA. We also perform ablation studies on T-CAA to assess the effects of the perturbation region, optimization strategy, and down-template (Appendix G.4)."
        },
        {
            "title": "7 Conclusion\nIn this work, we present the first systematic study of robustness\nrisks introduced by visual token compression in LVLMs. We show\nthat compression fundamentally reshapes the failure landscape\nby introducing stealthy vulnerabilities that manifest only under\ncompressed inference and evade standard robustness evaluation.\nTo characterize this risk, we propose Compression-Aware Attack\n(CAA), which exploits the fragility of token ranking to induce\ncompression-specific failures while largely preserving uncompressed\ninference behavior. Our results demonstrate that these vulner-\nabilities persist in realistic black-box settings and that existing\ndefenses provide limited protection, exposing an overlooked effi-\nciencyâ€“security trade-off in efficient LVLMs.",
            "content": "Our findings underscore the need for security-aware compression mechanisms that balance efficiency and robustness. Moreover, the compression-related failure modes identified in this work may be further explored to construct stealthy backdoors, posing potential risks to the supply chain of efficient LVLM deployment. Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018, Acknowledgments This paper was edited for grammar using ChatGPT. References [1] Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. 2025. Divprune: Diversity-based visual token pruning for large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference. 93929401. [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision. 24252433. [3] Eugene Bagdasaryan, Rishi Jha, Vitaly Shmatikov, and Tingwei Zhang. 2024. Adversarial illusions in {Multi-Modal} embeddings. In 33rd USENIX Security Symposium (USENIX Security 24). 30093025. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025). [5] Luke Bailey, Euan Ong, Stuart Russell, and Scott Emmons. 2024. Image Hijacks: Adversarial Images can Control Generative Models at Runtime. In International Conference on Machine Learning. PMLR, 24432455. [6] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can Cui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang, James Rehg, et al. 2024. Maplm: real-world largescale vision-language benchmark for map and traffic scene understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2181921830. [7] Jinghui Chen and Quanquan Gu. 2020. Rays: ray searching method for hardlabel adversarial attack. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 17391747. [8] Jianbo Chen, Michael Jordan, and Martin Wainwright. 2020. Hopskipjumpattack: query-efficient decision-based attack. In 2020 ieee symposium on security and privacy (sp). IEEE, 12771294. [9] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. 2024. An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models. In ECCV. [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. 2024. How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with OpenSource Suites. arXiv preprint arXiv:2404.16821 (2024). [11] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. 2024. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. 958979. [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems 36 (2023), 4925049267. [13] Deepseek. 2025. DeepSeek Terms of Use. https://cdn.deepseek.com/policies/enUS/deepseek-terms-of-use.html (Last Update: January 20, 2025). [14] DeepSeek. 2025. DeepSeek-V3.2 Release. https://api-docs.deepseek.com/news/ news251201 (Effective December 21, 2025). [15] DeepSeek. 2025. Introducing DeepSeek-V3.2-Exp. https://api-docs.deepseek. com/news/news250929 (Effective December 21, 2025). [16] Qi Fan, An Zou, and Yehan Ma. 2025. TimeBill: Time-Budgeted Inference for Large Language Models. arXiv preprint arXiv:2512.21859 (2025). [17] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014). [18] Google. 2025. Gemini 3 Flash: frontier intelligence built for speed. https: //blog.google/products/gemini/gemini-3-flash/ (Effective December 21, 2025). [19] Google. 2025. Gemini API Additional Terms of Service. https://ai.google.dev/ gemini-api/terms (Effective April 3rd, 2025). [20] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. 2024. Agent Smith: single image can jailbreak one million multimodal LLM agents exponentially fast. In Proceedings of the 41st International Conference on Machine Learning. 1664716672. [21] Yiqiang Guo, Lei Zhong, Bin Chen, Jia-Li Yin, Xiaolei Liu, and Shouling Ji. 2025. Focus on Generalization: Improving Adversarial Transferability via Bi-Level Bias Mitigation. In Proceedings of the 33rd ACM International Conference on Multimedia (Dublin, Ireland) (MM 25). Association for Computing Machinery, New York, NY, USA, 86548662. doi:10.1145/3746027.3755611 [22] Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, and Jing Zhang. 2025. Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models. arXiv preprint arXiv:2512.21815 (2025). [23] Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. 2025. ZipVL: Accelerating Vision-Language Models through Dynamic Token Sparsity. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2047720486. [24] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [25] Shibo Jie, Yehui Tang, Jianyuan Guo, Zhi-Hong Deng, Kai Han, and Yunhe Wang. 2024. Token compensator: Altering inference cost of vision transformer without re-tuning. In European conference on computer vision. Springer, 7694. [26] Maurice Kendall. 1938. new measure of rank correlation. Biometrika 30, 1-2 (1938), 8193. [27] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating Object Hallucination in Large Vision-Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 292305. [28] Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, and Zhiqiang Shen. [n. d.]. Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1. In ICML 2025 Workshop on Reliable and Responsible Foundation Models. [29] Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. 2025. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. In AAAI, Vol. 39. 53345342. [30] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. Llavanext: Improved reasoning, ocr, and world knowledge. [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems 36 (2023), 3489234916. [32] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2024. Mmbench: Is your multi-modal model an all-around player?. In European conference on computer vision. Springer, 216233. [33] Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, and Min Lin. 2024. Test-time backdoor attacks on multimodal large language models. arXiv preprint arXiv:2402.08577 (2024). [34] Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. 2024. survey on vision-language-action models for embodied ai. arXiv preprint arXiv:2405.14093 (2024). [35] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In International Conference on Learning Representations. [36] Openai. 2024. Terms of Use. https://openai.com/policies/row-terms-of-use/ (Published: December 11, 2024). [37] Openai. 2025. GPT-5 mini. https://platform.openai.com/docs/models/gpt-5-mini (Effective December 21, 2025). [38] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. 2024. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI conference on artificial intelligence, Vol. 38. 2152721536. [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 87488763. [40] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. 452461. [41] Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristobal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, et al. 2024. Failures to find transferable image jailbreaks between vision-language models. In International Conference on Learning Representations. [42] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. 2025. Llavaprumerge: Adaptive token reduction for efficient large multimodal models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 22857 22867. [43] Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, and Huan Wang. 2025. When tokens talk too much: survey of multimodal long-context token compression across images, videos, and audios. arXiv preprint arXiv:2507.20198 (2025). [44] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 83178326. [45] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024). [46] Georgios Tzachristas, Lei Deng, Ioannis Tzachristas, Gong Zhang, and Renhai Chen. 2025. Mathematical Theory of Top-ğ‘˜ Sparse Attention via Total Variation Distance. arXiv preprint arXiv:2512.07647 (2025). [47] Zhipeng Wei, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang. 2023. Enhancing the self-universality for transferable targeted attacks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1228112290. Conference acronym XX, June 0305, 2018, xx et al. [48] Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael Lyu, and Yu-Wing Tai. 2020. Boosting the transferability of adversarial samples via attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 11611170. [49] Long Xing, Qidong Huang, Xiao wen Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, and Dahua Lin. 2025. PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction. In CVPR. [50] Binyan Xu, Xilin Dai, Di Tang, and Kehuan Zhang. 2025. One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP. In Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security. 30873101. [51] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. 2025. VisionZip: Longer is Better but Not Necessary in Vision Language Models. In CVPR. [52] Linli Yao, Yang Shi, Long Xing, Sida Li, Yuanxin Liu, Yuhao Dong, Yifan Zhang, Lei Li, Qingxiu Dong, Xiaoyi Dong, Qidong Huang, Haotian Wang, Feng Wu, Yuanxing Zhang, Pengfei Wan, Zhouchen Lin, and Xu Sun. 2025. Towards Efficient Multimodal Large Language Models: Survey on Token Compression. Authorea Preprints (2025). [53] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Chi Chen, Haoyu Li, Weilin Zhao, et al. 2025. Efficient GPT-4V level multimodal large language model for deployment on edge devices. Nature Communications 16, 1 (2025), 5509. [54] Weihao Ye, Qiong Wu, Wenhao Lin, and Yiyi Zhou. 2025. Fit and prune: Fast and training-free visual token pruning for multi-modal large language models. In AAAI, Vol. 39. 2212822136. [55] Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Niculae Sebe, and Heng Tao Shen. 2025. Survey on Efficient Vision-Language-Action Models. ArXiv abs/2510.24795 (2025). https://api. semanticscholar.org/CorpusID: [56] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, et al. 2025. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2307823097. [57] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. 2024. Long Context Transfer from Language to Vision. Trans. Mach. Learn. Res. 2025 (2024). https://api.semanticscholar.org/CorpusID:270703489 [58] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. 2025. SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference. In ICML. [59] Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang, Dimitris N. Metaxas, and Licheng Yu. 2025. Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction. In cvpr. [60] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Zhikai Li, Yibing Song, Kai Wang, Zhangyang Wang, and Yang You. 2025. Stitch in Time Saves Nine: Small VLM is Precise Guidance for Accelerating Large VLMs. In CVPR. [61] Wei Zhao, Zhe Li, Yige Li, and Jun Sun. 2026. Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security. In Network and Distributed System Security Symposium. [62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2024. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. In ICLR. [63] Jianian Zhu, Hang Wu, Haojie Wang, Yinghui Li, Biao Hou, Ruixuan Li, and Jidong Zhai. 2025. Fastcache: Optimizing multimodal llm serving through lightweight kv-cache compression framework. arXiv preprint arXiv:2503.08461 (2025). [64] Jiedong Zhuang, Lu Lu, Ming Dai, Rui Hu, Jian Chen, Qiang Liu, and Haoji Hu. 2025. ST3: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming. In AAAI. https://api.semanticscholar.org/CorpusID: 277755083 [65] Xin Zou, Di Lu, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Xu Zheng, Linfeng Zhang, and Xuming Hu. 2025. Dont Just Chase Highlighted Tokens in MLLMs: Revisiting Visual Holistic Context Retention. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Ethical Considerations This work examines the security and robustness implications of visual token compression in Large Vision-Language Models. Although we introduce adversarial techniques, our goal is strictly defensive: to identify previously overlooked risks and inform the design of more secure and reliable LVLM systems. The vulnerabilities discussed in this paper stem from compression mechanisms that are already widely deployed in practice. By showing that visual token compression can introduce stealthy and hard-to-diagnose failure modes, we highlight compression as security-sensitive component rather than purely efficiency-oriented optimization. Understanding these risks is essential for deploying LVLMs in safety-critical and security-sensitive settings. To mitigate potential misuse, we present our methods at an abstract level appropriate for academic analysis. Our evaluation further indicates that existing defenses are insufficient, underscoring the need for future research on robustness-aware compression mechanisms and principled mitigation strategies. Overall, we aim to encourage responsible AI development by promoting the consideration of security and robustness alongside efficiency when designing and deploying compressed LVLM systems. Code Open Source In the spirit of open science, we will release the code for this work to facilitate reproducibility and further research. The codebase includes implementations of Compression-Aware Attack (CAA) and its black-box extension T-CAA, evaluation scripts for multiple LVLMs and visual token compression methods, and tools for reproducing the main experimental results reported in this paper. We will also provide example configurations and documentation to support evaluation under differen settings. The code will be made publicly available upon publication at: https://anonymous.4open. science/r/Compression_Aware_Attack-368D Related Work C.1 Vision Token Compression Vision token compression has emerged as practical inference-time optimization for large visionlanguage models (LVLMs), aiming to reduce computational cost by selectively retaining subset of visual tokens while discarding redundant ones. From the perspective of prompt dependency, existing approaches can be broadly categorized into text-agnostic and text-guided methods. Text-agnostic approaches exploit spatial redundancy in visual information, with compression typically performed within the vision encoder. Representative methods such as VisionZip [51] and LLaVA-PruneMerge [42] select tokens based on attention scores between the [CLS] token and visual tokens; ZipVL [23] further supports layeror task-adaptive sparsity, while HoloV [65] adopts holistic strategy to ensure global visual coverage. Text-guided methods leverage the semantic information in the language prompt to assess the task relevance of visual tokens, enabling more aggressive compression while preserving performance. Representative approaches including FastV [9], PDrop [49], and SparseVLM [58] prune tokens based on attention between text and visual tokens, adopting early one-shot compression, progressive depth-wise pruning, and multi-text-reference guidance, respectively. Other methods such as ğ‘†ğ‘‡ 3 [64] perform cross-layer progressive pruning, while G-Search [59] and FitPrune [54] determine layer-wise Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018, retention via search or optimization. From the perspective of importance scoring, attention-based methods dominate current practice [14, 15]. Attention scores naturally arise during inference, directly reflect each visual tokens contribution to the models response under given query [46, 56], and all approaches discussed above fall into this category. Beyond this method, some approaches adopt embedding-similarity strategies, merging visually similar tokens based on feature distance or formulating token selection as diversity-maximization problem [1]. However, these methods typically require explicit similarity computation, introducing additional overhead. Accordingly, this work focuses on attention-based, text-guided token compression. C.2 Adversarial Attack Adversarial examples are inputs perturbed with imperceptible noise to induce incorrect model predictions. Depending on the attackers knowledge and access to the target model, adversarial attacks are commonly categorized into white-box attacks, query-based blackbox attacks, and transfer attacks. White-box attacks assume full access to the model and exploit gradient information to optimize adversarial perturbations. Classic methods such as FGSM [17] and PGD [35] achieve high attack success under constrained distortion and are widely used as benchmarks for evaluating model robustness, representing an upper bound on adversarial vulnerability. Query-based attacks craft adversarial examples solely through interaction with the model. Representative query-based methods include HSJA [8], which estimates gradient directions at the decision boundary by using binary feedback, and RayS [7], which efficiently searches for the nearest boundary along discrete rays. While effective, such attacks typically require large number of queries, often exceeding tens of thousands [50], making them impractical under realistic access restrictions and security auditing [18]. Transfer attacks assume no access to the target model but rely on surrogate model with similar architecture or training distribution. Adversarial examples are generated on the surrogate and then transferred to the target model. Typical strategies to improve transferability include applying input transformations to increase perturbation diversity and ensembling multiple surrogate models to reduce overfitting [28]. Such attacks are commonly used to evaluate robustness under the most restrictive attacker assumptions. However, existing adversarial attacks are tailored for standard, uncompressed models and remain agnostic to the compression mechanism. Moreover, the compression process often introduces non-differentiable decision boundaries, making traditional gradientbased attacks difficult to apply directly to the compressed state. Simultaneously, since real-world LVLM deployments typically impose strict query limits, query-intensive black-box attacks face significant practical challenges in this scenario. Consequently, evaluating the security vulnerabilities specifically introduced by compression requires novel attack. (a) LLaVA ğœ– = 64/255 (b) Qwen-VL ğœ– = 64/255 Figure 13: Effect of token importance ranking on robustness under compressed inference. The shaded region highlights the significant performance recovery achieved by restoring correct rankings, suggesting that ranking instability plays an important role in robustness degradation under compression. Compression and Robustness D.1 The Impact of Token Ranking on"
        },
        {
            "title": "Robustness",
            "content": "We provide additional results under larger perturbation budget (ğœ– = 64/255) to examine whether the observations in Section 3 remain consistent. Fig. 13 reports model performance under the same three inference configurations as in Fig. 4. Restoring the clean token-importance ranking (configuration (c)) leads to substantial performance recovery compared to using the perturbed ranking (configuration (b)), and the performance under configuration (c) remains close to the clean baseline (configuration (a)). This indicates that robustness degradation under compression is dominated by instability in token importance ranking. D.2 Ranking Instability Token importance ranking is highly sensitive to input perturbations. Fig. 14 illustrates the ranking instability of Qwen-VL under random noise by comparing the token importance ordering induced by perturbed inputs with that of clean inputs. As the noise magnitude increases, the ranking becomes progressively more disordered: tokens that originally ranked among the bottom 100 increasingly enter the top-100 set. This misranking causes uninformative tokens to be retained while task-critical ones are discarded, ultimately degrading model performance. Victim Models and Datasets Following prior work [49, 58, 61], we evaluate our attacks on three state-of-the-art large visionlanguage models: LLaVA [31], representative LVLM architecture; LLaVA-Next [30], which extends LLaVA with higher-resolution inputs and flexible aspect ratios; and Qwen2.5-VL [4], highly competitive open-source LVLM. We evaluate our method on three widely used benchmarks. POPE [27] evaluates object hallucination via binary visual question answering. MME [32] assesses multimodal perception and reasoning with multi-domain multiple-choice questions. TextVQA [44] measures models ability to recognize and reason about embedded text in images. For TextVQA, we adopt the standard VQA accuracy metric [2], which evaluates model predictions against human annotations. An answer receives full credit if it matches responses Conference acronym XX, June 0305, 2018, xx et al. Algorithm 1: Compression-Aware Attack (CAA) Input : Model ğ‘“ ; image ğ¼ ; prompt ğ‘‡ ; target layer ğ‘™; step size ğœ‚; iterations ğ‘ ; â„“ constrain ğœ–; weights {ğ›¼, ğ›½, ğœ†ğ‘’, ğœ†ğ‘˜ }; hierarchy depth ğ‘›ğ‘” Output : Adversarial image Ë†ğ¼ (a) Qwen-VL (b) Qwen-VL Figure 14: Ranking stability under random â„“-bounded noise with different budgets. Global stability is measured using Kendalls ğœ and Spearmans ğœŒ. Local stability is evaluated via Top-100 Preservation (how many top-important tokens remain stable) and Bottom-100 Infiltration (how much lowimportance tokens enters the top ranks). Larger perturbations lead to progressively more severe ranking inconsistency. from at least three annotators, formally defined as VQA_ACC = min(cid:16) # matching human answers , 1(cid:17) . 3 Some benchmark samples can be correctly answered without visual input (e.g., in TextVQA). We therefore retain only samples for which the models prediction differs with and without the image, i.e., ğ‘“ (ğ¼,ğ‘‡ ) ğ‘“ (,ğ‘‡ ), ensuring that the evaluation set requires genuine multimodal reasoning. White-box Attack CAA injects adversarial perturbations into unimportant regions of the image to manipulate the compression mechanism into retaining uninformative tokens, thereby causing the model to fail under compressed inference. The overall procedure is summarized in Algorithm 1. F.1 Vanilla Adversarial attacks To account for lexical variability in LVLM outputs (e.g., yes, Yeah), we suppress predefined set of correct answer variants Ycor. Since this set cannot cover all semantically equivalent responses, we additionally promote set of incorrect anchor answers Ywro to explicitly steer the output toward incorrect predictions. Let ğ‘ (ğ¼ +ğ›¿,ğ‘‡ ) denote the output distribution over the first generated token. The attack loss is defined over the full image as follows: Lvanilla (ğ›¿) = 1 Yğ‘¤ğ‘Ÿğ‘œ + Yğ‘ğ‘œğ‘Ÿ log ğ‘ğ‘¦ğ‘¤ (ğ¼ + ğ›¿,ğ‘‡ ) ğ‘¦ğ‘ Yğ‘¤ğ‘Ÿğ‘œ log ğ‘ğ‘¦ğ‘ (ğ¼ + ğ›¿,ğ‘‡ ), ğ‘¦ğ‘” Yğ‘ğ‘œğ‘Ÿ (17) where ğ›¿ ğœ–, with ğœ– set to 32/255 by default. F.2 Main Results for CAA. Table 9 reports the performance of CAA under the PDrop compression method, while Table 10 further evaluates CAA across different retention rates on TextVQA. As shown, CAA consistently achieves high CSG values under all tested settings, indicating that 1 Forward (ğ¼,ğ‘‡ ) to layer ğ‘™ to get {ğ‘£ ( ğ‘™ ) ğ‘— } 2 Compute clean importance {ğ‘  ( ğ‘™ ) ğ‘›ğ‘‰ ğ‘—=1; ğ‘— } (Eq. 4) and split indices into Î©most, Î©least and partition Î©least into ğ‘›ğ‘” groups and form (ğ‘”) (Eq. 8); LM , (ğ‘,ğ‘) LL 3 Initialize ğ›¿ ğ‘— ğºğ‘ğ‘¢ğ‘ ğ‘ ğ‘–ğ‘ğ‘›(0, 1) for ğ‘— Î©least and ğ›¿ ğ‘— = 0 otherwise; 4 for ğ‘› = 1 to ğ‘ do Ë†ğ¼ ğ¼ + ğ›¿; 5 Forward ( Ë†ğ¼,ğ‘‡ ) to layer ğ‘™ and compute {Ë†ğ‘  ( ğ‘™ ) and { Ë†ğ‘˜ ( ğ‘™ ) Compute { Ë†ğ‘ ( ğ‘™ ) Ltotal Lrank + ğœ†ğ‘’ Lerase + ğœ†ğ‘˜ Lkey via Eqs. 9,11,10; ğ›¿ clip(ğ›¿ ğœ‚ sign(ğ›¿ Ltotal), ğœ–, ğœ–(cid:1), with ğ›¿ ğ‘— = 0 for ğ‘— Î©least; ğ‘— } via Eq. 4; ğ‘— } ğ‘— Î©least ğ‘– }ğ‘– Iref via Eq. 1; 9 8 10 end 11 return Ë†ğ¼ ; it preserves performance in the non-compressed state while inducing substantial degradation under compression. These results confirm that CAA effectively exposes vulnerabilities introduced by the compression mechanism and remains effective across varying compression strengths. F.3 Ablation Study of Key Components in CAA We conduct the same ablation studies on Qwen-VL to verify the generality of our findings. As shown in Table 11, removing any key component of CAA consistently degrades attack effectiveness under compressed inference. In particular, eliminating the hierarchical ranking constraint or the query-guided optimization leads to substantial drops in CSG, while removing semantic erasure further weakens the attack by allowing retained tokens to preserve usable visual content. These results closely mirror those observed on LLaVA, confirming that the effectiveness of CAA relies on all three components and generalizes across different LVLM architectures. F.4 Effectiveness to Configuration Mismatch Retention Rate Mismatch. Fig. 15 shows the attack performance when the number of least-important tokens targeted by the attack does not match the number of tokens retained by the compression mechanism. The results indicate that CAA remains highly effective when the mismatch between the two is not significant. Layer Mismatch. Fig. 16 reports the attack performance when the attack layer does not align with the compression layer. The results show that CAA is robust to certain degree of layer mismatch, and that attacks applied at later layers can still effectively influence compression applied at earlier layers. Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018, Table 9: Comparison of CAA with two baseline attacks under the PDrop compression method. Dataset Model M(cl,nc) M(cl,c) POPE TextVQA MME LLaVA LLaVA-NEXT Qwen-VL LLaVA LLaVA-NEXT Qwen-VL LLaVA LLaVA-NEXT Qwen-VL 0.9775 0.9003 0.9550 0.8817 0.6318 0.9135 0.8714 0.8617 0.9085 0.8392 0.7878 0.7781 0.7222 0.5563 0.6363 0.7910 0.8296 0. Vanilla Attack Random Attack M(adv,nc) M(adv,c) UPR 0.1238 0.3183 0.1534 0.1529 0.2229 0.3057 0.1210 0.2866 0.1465 0.1306 0.1541 0. 0.4713 0.4459 0.2115 0.1656 0.1503 0.3331 0.4395 0.4268 0.3718 0.1481 0.2439 0.3611 0.5409 0.5175 0.2328 CAE 0.8178 0.7171 0. 0.7707 0.7298 0.4765 0.4444 0.4855 0.5449 CSG M(adv,nc) M(adv,c) UPR 0.9770 -0.0584 0.9608 0.0354 0.8936 -0.2395 0.9550 0.8650 0.8534 0.7621 0.7331 0.6116 -0.0812 -0.0263 -0. -0.0148 0.0030 -0.2223 0.7473 0.5695 0.8534 0.8489 0.8424 0.8954 0.6099 0.5008 0.5939 0.7690 0.8099 0.8036 0.8476 0.9014 0. 0.9742 0.9776 0.9856 CSG M(adv,nc) M(adv,c) UPR 0.9579 0.0689 1.0400 0.0302 0.9579 0.1076 Compression-Aware Attack CAE 0.7571 0.7413 0.5359 0.9363 0.9363 0.7618 0.2038 0.2038 0.3611 0.0031 0.0012 0. 0.0020 0.0013 0.0020 0.7296 0.5633 0.7618 0.8328 0.8360 0.7314 0.1637 0.3270 0.3311 0.3826 0.5481 0.4042 0.8275 0.8916 0. 0.9579 0.9702 0.8051 0.7733 0.4122 0.4796 0.5163 0.3393 0.5053 CAE 0.0919 0.0694 0.2140 0.1555 0.0998 0.0666 0.0278 0.0237 0. CSG 0.7150 0.7813 0.3336 0.6008 0.3038 0.3136 0.4720 0.3095 0.3103 Table 10: CAA performance under different token retention rates on TextVQA. Model LLaVA Qwen-VL Ratio M(cl, nc) M(cl,c) M(adv,nc) M(adv,c) UPR 0.7799 0.8182 0.8171 0.8341 0.7970 0.8444 0.8197 0.7997 0.7428 0.6402 0.6876 0.7214 0.7204 0.7354 0.7027 0.4308 0.4323 0.4035 0.2158 0. 0.5 0.4 0.3 0.2 0.1 0.8817 0.5 0.4 0.3 0.2 0.1 0.9135 0.8495 0.8034 0.7585 0.6662 0.6135 0.7058 0.7389 0.7592 0.8585 0. 0.3058 0.2816 0.2322 0.1916 0.1879 0.7726 0.8089 0.8311 0.9398 0.8904 CAE 0.4898 0.4726 0.4954 0.7095 0.8396 0.6400 0.6495 0.6939 0.7124 0.6937 CSG 0.2697 0.2908 0.3125 0.5435 0.6366 0.4127 0.4584 0.5250 0.6522 0. (a) LLaVA-TextVQA (b) LLaVA-MME Table 11: Ablation of CAA Components on Qwen-VL. Three variants are formed by progressively removing hierarchical ranking, semantic erasure, and query-guided alignment. (c) Qwen-POPE (d) Qwen-TextVQA (e) Qwen-MME Figure 16: Attack effectiveness under mismatches between the attack layer and the compression layer. Attack CAA w/o Hierarchy CAA w/o Erasure CAA w/o Query CAA POPE CAE 0.8078 0.7734 0.7086 0.9255 UPR 0.9654 0.9638 0.9823 0.9158 CSG 0.7732 0.7372 0.6909 0.8413 UPR 0.9356 0.9571 0.9707 0. TextVQA CAE 0.6219 0.6187 0.5587 0.7124 CSG 0.5575 0.5758 0.5294 0.6522 MME CAE 0.5688 0.5087 0.6405 0.6548 UPR 0.9552 0.8746 0.8648 0.9344 CSG 0.5240 0.3833 0.5053 0.5892 (a) LLaVA-TextVQA (b) LLaVA-MME Figure 17: The impact of perturbation budgets on attack performance (CSG) across different datasets. (a) LLaVA (b) Qwen-VL (c) Qwen-POPE (d) Qwen-TextVQA (e) Qwen-MME Figure 15: Impact of mismatch between attack and test retention rates on attack effectiveness. F.5 Impact of the Perturbation Budget We evaluate CAA under different perturbation budgets, with ğœ– {16/255, 32/255, 64/255}, and report CSG results in Fig. 17. since perturbations are restricted to small portion of the image, the resulting visual distortion is largely imperceptible. As ğœ– increases, attack effectiveness improves consistently across models and datasets. However, at larger budgets, perturbations begin to degrade uncompressed inference and introduce visible artifacts, leading to reduced overall CSG. This indicates clear trade-off between attack strength and perceptual stealth, motivating the use of moderate perturbation budgets in practice. Transfer Attack G.1 Least Important Region Mismacth. Fig. 18 shows that different models exhibit substantial disagreement in identifying the least-important regions for the same input. As result, directly applying white-box attacks that perturb regions deemed least important by surrogate model is suboptimal when transferred to target model. Conference acronym XX, June 0305, 2018, xx et al. (a) LLaVA (b) Qwen-VL Figure 18: Visualization of the most important (red) and least important (blue) regions identified by LLaVA and QwenVL for the same text-visual prompt, illustrating modeldependent differences in region importance. Table 12: Enumerated compression configurations (ğ‘Ÿ 0.2) with their average retention rates and corresponding (layer, rate) information. Avg. Rate Configuration (Layer, Rate) Avg. Rate Configuration (Layer, Rate) 0.156 0.169 0.172 0.175 0.178 0.181 0.184 0. [(2, 0.1)] [(2, 0.2), (6, 0.1)] [(2, 0.2), (7, 0.1)] [(2, 0.2), (8, 0.1)] [(2, 0.2), (9, 0.1)] [(2, 0.2), (10, 0.1)] [(3, 0.1)] [(2, 0.2), (11, 0.1)] 0.188 0.191 0.194 0.197 0.197 0.200 0.200 [(2, 0.2), (12, 0.1)] [(2, 0.2), (13, 0.1)] [(2, 0.2), (14, 0.1)] [(2, 0.2), (15, 0.1)] [(3, 0.2), (7, 0.1)] [(2, 0.2), (16, 0.1)] [(3, 0.2), (8, 0.1)] Table 13: T-CAA performance across different models and compression configurations. Rate denotes the compressionlayer retention rate, and Avg. Rate denotes the average perlayer visual-token retention rate. Configuration Rate Layer 0.3 0.2 0.1 6 2,6 0.3 0.2 0.1 0.5,0.25 0.4,0.2 0.3,0.15 0.2,0.1 Avg. Rate 0.388 0.300 0. 0.300 0.200 0.100 0.328 0.275 0.222 0.169 LLaVA->LLaVA-NEXT LLaVA->Qwen-VL Qwen-VL->LLaVA CAE 0.3603 0.4275 0.4917 CSG 0.2005 0.2463 0.5611 CAE 0.3858 0.4316 0.7464 CSG 0.3516 0.4778 0. CSG 0.2502 0.3174 0.3816 CAE 0.4912 0.6174 0.7887 0.3796 0.4308 0.5631 0.2748 0.3817 0.4733 0.5040 0.2695 0.3207 0.4530 0.1647 0.2716 0.3632 0. 0.3013 0.4821 0.5159 0.4447 0.4838 0.6004 0.6312 0.1617 0.3426 0.3763 0.3051 0.3442 0.4609 0.4916 0.3817 0.4095 0.6542 0.3668 0.3977 0.4160 0. 0.1964 0.2242 0.4688 0.1814 0.2123 0.2306 0.3540 Table 14: Ablation study of T-CAA components. Least-Pert perturbs least-important regions, Full-Opt optimizes the full border instead of universal templates, and T-CAA w/o Down removes the suppression template. Configuration Rate Layer Avg. Rate Attack UPR CAE CSG 2 0. 0.250 2,8 0.3,0.15 0.231 2,8,16 0.4,0.2,0. 0.238 Least-Pert Full-Opt T-CAA w/o Down T-CAA Least-Pert Full-Opt T-CAA w/o Down T-CAA Least-Pert Full-Opt T-CAA w/o Down T-CAA 0.9034 0.6981 0.9671 0.8604 0.9034 0.6981 0.9671 0. 0.9034 0.6981 0.9671 0.8604 0.3392 0.4476 0.2542 0.5338 0.3098 0.5032 0.1125 0.4573 0.2069 0.5092 0.0341 0.3551 0.2425 0.1457 0.2213 0.3943 0.2131 0.2013 0.0796 0. 0.1103 0.2073 0.0013 0.2155 Therefore, when ğ‘Ÿ 0.2, the selected attack layer range corresponds to layers 23, where the retention rate first falls within our attack-effective regime. This example illustrates how candidate compression layer ranges can be systematically derived from an estimated ğ‘Ÿ . G.3 Main Results for Transfer Attack In the transfer setting, T-CAA jointly learns two such templates: one applied to an uninformative image border to amplify the importance of border tokens, and another applied to the original image content to mildly suppress its token importance. Together, these perturbations manipulate the relative ordering of vision-token importance, steering compression to retain uninformative tokens. Table 13 reports the cross-model and cross-configuration performance of T-CAA on the POPE dataset under additional compression settings. The results show that T-CAA preserves model behavior under non-compressed inference, achieving an average Uncompressed Performance Retention (UPR) of 0.8550. At the same time, it induces severe performance degradation once compression is enabled, with an average Compressed Attack Effectiveness (CAE) of 0.4445, demonstrating the strong transferability and effectiveness of the proposed attack across models and compression configurations. G.2 Example Enumeration of Compression G.4 Ablation Study of Key Components in"
        },
        {
            "title": "Configurations",
            "content": "T-CAA We provide an illustrative example of compression configuration enumeration under the constraints described in Sec. 5.4.1. Given an estimated average retention rate ğ‘Ÿ , the attacker enumerates compression configurations consistent with common deployment practices used in prior work [9, 42, 49, 58, 60]. Specifically, we follow the same rate and interval constraints as summarize in Sec. 5.4.1 and enumerate all feasible configurations ( ğ¿, ğ‘…) via depth-first search, where ğ¿ denotes the compression layers and ğ‘… denotes corresponding retention rates. Under these constraints, average retention rates ğ‘Ÿ 0.4 yield several thousand feasible configurations. For clarity and space considerations, we report configurations with ğ‘Ÿ 0.2, shown in Table 12. Our attack-effective regime is ğ‘Ÿ [0.1, 0.4]. We conduct ablation studies to analyze the three key design components of T-CAA in the black-box setting. Specifically, we evaluate: (1) the choice of attack region (border-based vs. surrogate-selected least regions), (2) the use of universal perturbation templates versus direct full-border optimization, and (3) the necessity of the down template applied to image content. Impact of Attack Region Selection on Transferability. To evaluate the role of the artificially introduced border, we compare T-CAA with variant that perturbs regions identified as least important by the surrogate model (denoted as the Least-Pert strategy). As shown in Table 14, the least-region strategy performs substantially worse than border-based T-CAA, with CSG dropping from 0.3091 to Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Conference acronym XX, June 0305, 2018, 0.1886. This performance gap reveals key limitation of surrogatebased region selection in black-box settings. Due to model-specific differences in importance estimation, regions deemed unimportant by the surrogate may still encode task-relevant information for the target model, which can inadvertently degrade non-compressed inference and weaken attack effectiveness under compression. Effect of Full-Border Optimization. Next, we evaluate the importance of universal templates using variant that directly optimizes the entire border (T-CAAFull-Opt). As shown in Table 14 T-CAAFull-Opt achieves substantially lower average CSG than TCAA, indicating much weaker transferability. Directly optimizing border pixels yields surrogate-specific perturbations that fail to induce consistent token-ranking shifts on target models. In contrast, universal templates capture more model-agnostic patterns, enabling more reliable transfer. Necessity of the Down Template. Finally, we assess the role of the down template by removing it from T-CAA (T-CAAw/o Down). As shown in Table 14, removing the down template substantially reduces attack effectiveness on compressed models (average CAE = 0.1336), indicating that boosting border-token importance alone is insufficient for reliable transfer. Although border tokens may outrank image content on the surrogate model, cross-model variations in attention often cause target models to still favor original content. By mildly suppressing content tokens while amplifying border tokens, the down template increases the likelihood that border tokens dominate the ranking across models, leading to more robust transfer attacks."
        }
    ],
    "affiliations": [
        "Griffith University",
        "University of Technology Sydney",
        "University of Utah"
    ]
}