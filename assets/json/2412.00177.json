{
    "paper_title": "LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting",
    "authors": [
        "Xiaoyan Xing",
        "Konrad Groh",
        "Sezer Karaoglu",
        "Theo Gevers",
        "Anand Bhattad"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target's lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning. Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 2 7 7 1 0 0 . 2 1 4 2 : r LUMINET: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting Xiaoyan Xing1 Konrad Groh2 Sezer Karaoglu1 Theo Gevers1 Anand Bhattad3 1UvA-Bosch Delta Lab 2BCAI-Bosch 3Toyota Technological Institute at Chicago https://luminet-relight.github.io Figure 1. LUMINET transfers complex lighting conditions from target image (a) to source image (b), synthesizing relit version of the source image (c) while preserving its geometry and albedo. In the top row, observe how LUMINET transforms the scene from nighttime to daytime by transferring strong directional light from the target images window to the source image. Key details in the relit image include pronounced gloss on the table, shadows cast onto the carpet (center left), cast shadows from the TV stand (left corner), and, most importantly, reflections of the table on the TV screen (d, e). These changes demonstrate plausible control over both direct and indirect lighting effects, such as reflections, specular highlights and shadow placement. In the bottom row, LUMINET knows about luminaires. In the relit image, two bedside lamps illuminate the scene, transforming it from dimly lit room into well-lit environment. This suggests that LUMINET recognizes the spatial arrangement of objects and infers where light sources should be switched on. Note how LUMINET introduces specular highlights on the left painting (see crop) and gloss in the far-right corner of the bedroom, where previously invisible bedside lamp is now turned on. These results show LUMINETs ability to handle complex lighting phenomenaincluding direct illumination, specular highlights, cast shadows, inter-reflections and other indirect effectswhile maintaining scene geometry, and albedo."
        },
        {
            "title": "Abstract",
            "content": "We introduce LUMINET, novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given source image and target lighting image, LUMINET synthesizes relit version of the source scene that captures the targets lighting. Our approach makes two key contributions: data curation strategy from the StyleGAN-based relighting model for our training, and modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through learned adaptor (MLP) that injects the targets latent extrinsic properties via cross-attention and fine-tuning. Unlike traditional ControlNet, which generates images with conditional maps from single scene, LUMINET processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input. 1 1. Introduction Transferring lighting conditions between indoor scenes has applications in cinematography, architectural visualization, and mixed reality. While recent advances in neural rendering have shown promising results for single image relighting, transferring lighting between different images remains challenging due to the complex interplay of scene geometry, materials, and illumination. The key challenge stems from the difficulty in decomposing and transferring lighting effects between scenes with different spatial layouts and surface properties. Moreover, light in scenes cannot just appear but must come from luminaires, meaning that transferring lighting pattern from scene to scene requires detailed understanding of light sources in the scene. Furthermore, indoor scenes have complex light transport phenomena including interreflections, shadows, and spatially-varying material interactions that are highly scene-specific [67]. Traditional inverse rendering approaches attempting to recover scene components explicitly often struggle with model limitations and error propagation [32]. Other approaches either require extensive multi-view capture setups, are limited to specific object categories [23, 61] or portraits [26, 46], or cannot transfer complex lighting effects between different scenes [62, 69]. Recent studies have shown promising directions. Bhattad et al. [7] showed that StyleGANs latent space [24] contains disentangled lighting representations and uses them to manipulate the lighting of generated images, but their approach does not transfer well to real images [5]. Zhang et al. demonstrated that latent intrinsic decomposition can capture emergent properties of albedo and illumination, and can be used for relighting [69]. While these representations are robust, our experiments demonstrate they do not generalize to complex, arbitrary scenes. Meanwhile, diffusion models [20, 48] with ControlNet [64] have shown exceptional conditional image-generation capabilities. DiffusionLight [44] recovers environment maps by inpainting chrome balls, while IC-Light [65] relights portrait images. However, these methods cannot relight complex indoor scenes. We present LUMINET, novel approach that synthesizes the strengths of these different generators while addressing their individual limitations. Our key insight is that by carefully modifying the ControlNet architecture to operate on latent representations of scene intrinsics and extrinsics [69], we can achieve robust lighting transfer between arbitrary indoor scenes. First, we develop training pipeline that integrates variational StyleGAN architecture with real indoor scene data to alleviate mode collapse issues common in indoor scene generation. This approach also addresses the lack of training data for real indoor scenes lit under different lighting conditions. Second, we train Latent ControlNet that learns to decompose and transfer lighting features by operating in learned latent spaces and using lighting featureaware fine-tuning, without requiring explicit 3D reconstruction or material modeling. Third, we introduce lightingaware adaptor network that maps low-dimensional latent lighting extrinsic vector to high-dimensional code. This code is integrated into pretrained diffusion model by finetuning its cross-attention layers, helping the model to preserve target lighting characteristics effectively. Our method successfully relights challenging cases where the target (Fig. 1a) and the source images (Fig. 1b) differ significantly in spatial arrangements and material properties, exploiting learned priors from powerful image generators. Results (Fig. 1c) demonstrate that our relighting method can create complex lighting phenomena in physically plausible ways, including specular highlights, soft shadows, and indirect illumination effects like interreflections (as shown in Fig. 1d; see the TV in the top row). Extensive experiments show that LUMINET outperforms previous methods, requiring only single image as input. On the challenging MIT Multi-Illumination dataset [41], LUMINET surpasses previous SOTA by over 20% on quantitative metrics. In summary, our main contributions are: Novel Framework: LUMINET combines latent intrinsic control with diffusion models for high-quality indoor scene relighting without 3D or multi-view inputs. Training Data: variational StyleGAN approach maps real images to latent space of StyleGAN, enabling diverse data generation for our training. Generalizable Relighting: Despite training only on samescene pairs, LUMINET successfully transfers lighting between scenes in the wild with different layouts . Plausible Lighting Effects: LUMINET can relight diverse indoor scenes with complex lighting effects, including specular highlights, cast shadows, and inter-reflections. Extensive evaluationsquantitative and qualitative, as well as user studiesvalidate LUMINETs effectiveness. 2. Related work Transferring lighting conditions across scenes requires fundamental understanding of each scenes intrinsic properties and lighting. We categorize prior works based on their inputs for relighting and discuss intrinsic images, which serve as foundation for this process. 2.1. Intrinsic-image-based Relighting This section categorizes methods that require explicit inIndoor scene relighting trinsic properties for relighting. presents unique challenges due to complex light transport phenomena, multiple light sources, and intricate material interactions. Traditional approaches requiring 3D scene reconstruction [30, 32, 33, 63, 73] and inverse graphics-based intrinsic image decomposition methods to achieve highquality results but are computationally intensive and require 2 detailed geometry. large portion of these methods are multi-view based, as multi-view images provide richer information about the scene. For instance, Duchˆene et al. [15] achieve time-lapse relighting in outdoor scenes, Philip et al. [43] extend controllability by using geometry-aware network. Paired with neural radiance fields [25, 40], these relighting approaches have been applied to objects [4, 53, 68], outdoor scenes [17, 19, 34, 50], and human portraits [8, 22]. Another subset of methods operates with single image. Li et al. [32] model both the scenes intrinsic properties and the invisible light sources, using ray-tracing renderer to achieve relighting results. Leveraging the rich priors learned by diffusion models [48], RGB-X [62] demonstrates relighting results by fixing the intrinsic channel while altering lighting based on text prompt and irradiance fields. LightIt [27] achieves consistent and controllable lighting changes in image generation by conditioning on shading and normal maps in diffusion models. Other recent image-based methods have explored various representations with diffusion models including shading maps [38] and spherical gaussians [28]. Despite the clear physics indications provided by explicit intrinsic images, these methods are limited by the performance of intrinsic prediction models and the challenges of generating complex lighting effects in real-world scenarios. In contrast, our approach generates relit images based on latent intrinsic representations estimated from the image. 2.2. Image-based Relighting Image-based relighting has seen significant progress, particularly in specialized domains. Portrait relighting has been extensively studied [26, 39, 42, 46, 51, 52, 72], with methods typically leveraging face-specific priors and lightstage training data. For outdoor scenes, self-supervised approaches have shown success by decomposing images and modeling parametric illumination, benefiting from the relatively simple lighting conditions dominated by sky and sunlight [35, 60]. Early learning-based methods showed promise in generalizing across diverse scenes. Hu et al. [21] introduced self-attention autoencoder to separate scene representation from lighting estimation, while Yang et al. [58] enhanced this approach through depth-guided relighting. However, these methods are constrained by model capacity and training data diversity, limiting their effectiveness on complex real-world scenes. StyLitGAN [6] explores latent lighting representation to relight images generated by StyleGAN [24], yet it only works for the GANs synthetic images. Recent diffusion-based approaches have made progress in addressing generalization challenges. DilightNet [61], IllumiNeRF [71], and Neural Gaffer [23] focus on object relighting using 3D rendering data and NeRF representations, ZeroComp [70] and FlashTex [12] train light-aware ControlNet and facilitate effective relighting of objects. PoirierGinter et al. [45] achieve multi-view relighting effects using direct lighting data [41] and Gaussian splatting [25]. Retinex-diffusion [56] proposes training-free lighting conditioned scheme in diffusion model using retinex theory [29], yet it only works with predefined light direction and pixel-based diffusion models. IC-Light [65] demonstrates strong performance in controlling foreground lighting effects through large-scale dataset, but struggles with scenelevel relighting as it assumes consistent light transfer between foreground and background. In contrast, our approach tackles the challenging problem of cross-scene relighting in real-world environments without requiring 3D information or geometric supervision. We demonstrate superior performance compared to stateof-the-art methods including IC-Light [65] on real-world indoor scenes. 2.3. Intrinsic Image Decomposition The concept of intrinsic decomposition can be traced back to Barrow and Tenenbaum [2]. Early approaches, such as SIRFS [1], use shading information to recover shape, illumination, and reflectance, highlighting the importance of modeling these factors in intrinsic image analysis. Comprehensive reviews of intrinsic images methods up to 2022 can be found in [16, 18]. Relying on synthetic training data, recent strategies improve the intrinsic estimation via different focuses such as: ordinal Shading [9, 10, 13], surface normal [3]. Das et al. [11] estimate albedo using edge color priors, while Xing et al. [55] investigate intrinsic images using point cloud representations. More recently, conditional generative models [14, 28, 38, 54, 62] have been employed to derive intrinsic properties using diffusion priors. Despite the clear physical meaning of intrinsic images, transferring complex lighting conditions from one scene to another remains challenging, as lighting conditions typically align closely with the original scene structure. Alternatively, Zhang et al. [69] proposes learning latent intrinsic properties for relighting. Building on this concept, we project intrinsic image properties into latent space and use them to control lighting conditions. 3. Overview Given real-world scene So with lighting condition Lo, we learn lighting transformation model fθ to replace Lo with the lighting condition Lt from target scene St. The lighting transformation can be expressed as: fθ(SLo , SLt ) SLt . (1) Lighting transfer models demand comprehensive understanding of the scene. Most previous approaches tackle 3 Figure 2. LUMINETs Architecture and Training Pipeline. Left: Inference pipeline of LUMINET, which takes two inputs: source image and target lighting condition image. The model (fθ) transfers lighting characteristics while preserving the source scenes structure and materials (a). Right: Our training requires latent intrinsic representations from source and target images from pretrained model [69]. The latent intrinsic model decomposes an image into lighting-invariant intrinsic feature maps and low-dimensional extrinsic lighting vector. We then train conditional latent diffusion model along with lightweight MLP adaptor network that transforms low-dimensional latent lighting extrinsics to match latent diffusions text embedding dimensions. We use empty prompts for our text conditioning. The training uses paired scenes (same geometry, material, and layout) under different lighting conditions with latent diffusion loss (: VAE encoder and decoder are omitted in the diagram.) to ensure accurate lighting transfer. As we demonstrate in our results, LUMINET shows strong generalization ability in lighting transfer between scenes with completely different layouts and material properties even though they are trained with image-relight pairs from the same scene. this problem using two-step process: inverse rendering (or intrinsic decomposition) followed by re-rendering (often utilizing off-the-shelf ray tracers). In contrast, we propose reframing this problem as conditional image generation task, where the generated image is conditioned on the intrinsic properties of the real-world scene So and the target lighting Lt. Previous methods for conditional generative relighting predominantly rely on image-space representations, such as environment maps for lighting. These approaches focus on tasks like object-centric harmonization [23, 61, 65] or portrait relighting [46, 65]. However, environment maps have inherent limitations when applied to scene-level relighting, as they cannot accurately represent light sources within scene. Additionally, image-space representations alone are inadequate for cross-scene lighting transfer because conventional lighting representations (e.g., irradiance [62] or shading [27, 38]) are fundamentally tied to scene geometry. We propose novel approach that represents both the scenes intrinsic properties and target lighting using latent features, enabling control over the generation process. By leveraging generative model, we perform scene relighting in an end-to-end manner  (Fig. 2)  . Despite the effectiveness of the latent features, learning to generate relit scene comes with significant challenges: 1) the intrinsic components of the relit scene should closely resemble those of the original scene, ensuring minimal deviation; and 2) the lighting transfer must appear realistic, as light in scene cannot simply appear arbitrarilyit must originate from plausible luminaires. These highlight the importance of deep understanding of lighting variations and scene intrinsic properties, as well as the need to effectively incorporate the rich priors in generative models. In following sections, we emphasize the necessity of careful dataset construction (Sec. 4) and introduce systematic approach to manage lighting transfer effectively (Sec. 5). Finally, we validate our proposed method through comprehensive quantitative and qualitative experiments (Sec. 6). 4. Data Preparation Acquiring paired images of real-world scenes under different lighting conditions is extremely challenging, requiring carefully controlled environments and extensive setup. To address this data limitation, we develop two-stage data preparation strategy: (1) variational-synthetic scene generation approach that captures essential lighting patterns, and (2) curated collection of in-the-wild images that ensures diverse and balanced training data. This combination enables our model to learn robust lighting transfer while maintaining photorealistic quality. 4.1. Variational Relit Scene Generation StyLitGAN [7] generates plausible relit images by interpolating StyleGANs latent space. It maps random Gaussian noise to latent style code w, then adds predefined lighting direction to generate relighting images. However, StyleGAN [24] can suffer from mode collapse when searching its high-dimensional latent space, producing partially identical images from different latent vecFigure 3. Training Framework of Variational StyLitGAN. (a) Traditional StyleGAN suffers from mode collapse when sampling latent from Gaussian distribution, producing similar outputs every 10-20 iterations despite different latent codes. (b) Our variational approach learns to map real images to StyleGANs latent space through an encoder (qe), while using frozen pretrained generator (pg) from StyLitGAN [7]. The colored bars represent StyLitGANs disentangled lighting codes, which we leverage to generate diverse pool of scenes under different lighting conditions. While the learned mapping is approximate, it provides sufficient diversity for training LUMINET by exploiting the natural variation in real images. (c) We apply CLIP similarity filtering to ensure high-quality generated samples. tors (Fig. 3(a)). potential solution is to map real images using GAN inversion-based approach. However, the best-performing GAN inversion method [5] relies on an optimization-based technique, which is too slow for efficient data curation. To address this, we propose variationalStyLitGAN (Fig. 3(b)), which maps real-world images to StyleGANs latent space using ConvNext-based [36] variational encoder qe(zx). The encoder maps input image to variational latent code z, which is then mapped to style code w+ by the pretrained mapper. The frozen StyLitGAN generator pd(xw+) reconstructs the scene image ˆx. We optimize the network using: (cid:124) = MSE(x, ˆx) + LPIPS(x, ˆx) (cid:125) (cid:123)(cid:122) Lrec + DKL(qϕ(zx) (0, I)) (cid:123)(cid:122) (cid:125) LKL (cid:124) (2) 5. LUMINET where Lrec (LPIPS) [66] regularizes the latent distribution. combines MSE and for accurate reconstruction, perceptual loss and LKL For dataset generation, we encode LSUN-bedroom images to obtain z, map to w+, and add lighting direction to generate seven lighting variations per scene. We further curate 1K high-quality unique images using CLIP [47] similarity to keywords photo-realistic, good lighting, and illumination (Fig. 3(c)). While StyLitGAN provides good lighting control for generated images, the gap between generated and real images makes it challenging to train solely on synthetic data. Therefore, we use this pipeline primarily for data generation, leveraging its diverse lighting variations to train LUMINET for cross-scene light transfer. 4.2. In-the-Wild Training Data To complement our generated samples, we leverage several real-world datasets: Multi-Illumination Images in the Wild (MIIW) [41] provides controlled lighting variations across over 1,000 indoor scenes, each captured under 25 distinct conditions, offering high-quality specular effects and direct lighting. BigTime [31] contributes diverse lighting effects including hard shadows through time-lapse captures of 460 scenes under 20-50 lighting conditions. We additionally sample 1,000 images per training from LSUN Bedroom [59] to enhance training distribution diversity. Unlike prior works focused on object-level or portrait relighting [23, 46, 65], our approach targets scene-level relighting, thus avoiding object-centric datasets. Summing it up, we train LUMINET on 2,500 unique scenes with their relit pairs and 1,000 scenes from LSUN for which we do not have relighting pairs. Our goal is to learn generative model that can transfer lighting between indoor scenes while preserving scene structure. The key challenge lies in conceptualizing lighting and its complex interactions within scenes. Our solution leverages latent intrinsic representations during training, grounded in photometric stereo theory which separates images into illumination-invariant (intrinsic) and illuminationdependent (extrinsic) components. 5.1. Latent Intrinsic Extraction Traditional intrinsic decomposition in pixel space (e.g., albedo, roughness, surface normals) faces two key challenges: (1) perfect decomposition from monocular images is nearly impossible, and (2) obtaining all necessary components is computationally expensive. Instead, we process intrinsic information entirely in latent space. Building on Zhang et al. [69], given an image pair (SLo , SLt ) of scene So under different lighting conditions Lo and Lt, we use pre-trained latent-intrinsic encoder fλ to extract latent intrinsic features Ao RHW 128 and lighting codes {ILo, ILt} R16. Figure 4. Our LUMINET architecture transfers complex lighting conditions between indoor scenes using latent intrinsic representations while preserving scene layout, geometry, and albedo. Each scene shows an original image (left) paired with its relighted version (right) matching the target lighting shown at the top. Our method preserves scene structure and materials while accurately transferring lighting characteristics. Left panel demonstrates our method can adjust luminaires to match lighting conditions: it knows that to get more light in the right place in the room, it must switch on bedside lights (first row and second row) or table lamps (third row and fourth row), showing our models ability to handle direct illumination. Zoomed-in crops highlight the changes in images caused by relighting. In the first row, observe the added gloss on the wall behind the lamp in the top crop, as well as the effects on the side of the bed in the bottom crop, influenced by the invisible luminaire. In the second row, note the gloss removal on the side wall, as shown in the bottom crop. In the third row, you can see the reflection of the lamp on the large stationary glass window on the left, highlighted in the top crop. Finally, in the bottom row, observe the strong gloss added to the chair and the faint inter-reflection on the TV screen. Right panel shows natural lighting scenarios where bedside lamps are off. Top rows crop shows suppressed specular reflections on the glass table and realistic lamp pole shadows added after relighting. Second row shows strong specular highlights on the wall clock and strong cast shadows from the AC unit. Third row captures soft ambient lighting with intricate specular details on window frames and appropriate surface sheen on furniture. Fourth row demonstrates the removal of bright light from the lamps and all indirect effects, including the recovery of sharp edges at the intersection of the ceiling and side walls. 5.2. Latent Intrinsic Control Our illumination control scheme consists of two key components. First, unlike traditional ControlNet [64] that operates on images, we implement control directly in latent space through our Latent Intrinsic ControlNet. We expand the target latent illumination ILt to match spatial dimensions of Ao, then concatenate them to form {Ao, IL This concatenated feature is processed through convolution layers to obtain RH/2W/2512. } RHW 144. Second, we enhance lighting control through crossattention in the diffusion model. learned MLP (3072 4096 4096 4096 3072) transforms the lighting code (with necessary rescaling) into IEt R31024, matching the text embedding dimensions. We exclude text prompts to focus purely on image-based lighting transfer. 5.3. Training Objective During training, we focus on same-scene lighting transfer through latent diffusion process. The process begins by 6 encoding target lighting scene SLt to latent ϵ(SLt), then progressively adds noise to obtain ϵ(SLt)t. The model predicts noise using multiple conditions: time step t, latent features {Ao, IL }, lighting embedding IEt and original scene SLo. The objective function is: LLumi = ϵ θ(ϵ(SLt)t, t, {Ao, IL }, IEt, ϵ(SLo))2 2 (3) We train only the latent intrinsic control network and cross-attention layers while keeping other diffusion model parameters frozen. 6. Experiment We first introduce the implementation details of LUMINET, then we evaluate light transfer ability on both the controlled lighting dataset and real world image. Finally, an ablation study is conducted. 6.1. Implementation Details Training. We use Stable Diffusion 2.1 [48] as our base model to balance performance and training costs. To better preserve the details of the input images, we jointly estimate the de-noised image and noise map at each denoising step (known as the v-prediction). Our method also applies to other objective functions, such as ϵ (only predicts the noise map). All training and testing are conducted on an 8-GPU NVIDIA A6000 Ada NVLINK 48GB node. For the SD2.1 base model, we train on images with resolution of 512 512. An AdamW [37] optimizer with learning rate of 4 105 and decay rate of 0.9 is used. Training requires approximately 120 hours on single GPU. At inference time, LUMINET outputs relighted image (resolution: 512 512) in 5 seconds with 50 DDIM steps. Nearest Neighbor based Selection. Despite LUMINETs generalizable ability in light transfer, the generative model is still affected by initial seeds [57], which can produce suboptimal relighting results, particularly when precise control over local lighting effects is required, such as turning lamps on and off. We propose nearest neighbor searching scheme based on the latent lighting code of images generated with random seeds and the target lighting image. Notably, the nearest neighbor search approach is only used for precise control of local lighting effects and is not applied for coarse lighting effects, such as direct relighting on the MIIW dataset. Flow-Based Clean Up. While our method performs well for conditioned relighting effects, U-Net-based diffusion model may still produce sub-optimal artifacts in complex indoor scenes. We employ rectified-flow inversion [49] with η = 0.99 to remove artifacts and achieve higher resolution. Table 1. Quantitative Evaluation. We evaluate quantitatively using the multi-illumination dataset [41] where ground truth relights are available. Our method outperforms across all metrics by significant margin. Notably, our quantitative evaluation does not involve any post-processing, such as nearest-neighbor search on latent extrinsic or flow-based cleanup, as these approaches are computationally expensive for large image pools. Methods Labels Raw Output Input Img SA-AE [21] SA-AE [21] S3Net [58] S3Net [58] Latent-Intrinsic [69] (σ = 0) Latent-Intrinsic [69] RGB-X [62] Ours - Light - Depth - - - - - RMSE 0.384 0.288 0.443 0.512 0.499 0.326 0.297 0.256 0. SSIM RMSE 0.312 0.438 0.232 0.484 0.317 0.300 0.418 0.331 0.414 0.336 0.242 0.232 0.222 0.473 0.253 0.476 0.144 0.647 Color Correction SSIM 0.492 0.559 0.431 0.374 0.377 0.541 0.571 0.470 0.673 Importantly, we do not introduce any prompts related to the lighting conditions of the image, to prevent any lightingrelated changes by the rectified-flow model. Similar to the nearest neighbor searching scheme, the flow-based visual enhancer is not used for the MIIW relighting results. 6.2. Quantitative Evaluation We compare our method against recent advancements using deep networks (SA-AE [21], S3Net, [69]) and diffusion models (RGB-X [62]) on the test set from the MIIW dataset, which was not included in our training set. Following the experimental setup of Zhang et al. [69], we randomly select an image and its 12 reference lighting conditions from the entire test set. To minimize bias from random selection, we repeat the experiment multiple times with different seed for each run and report the average results. As shown in Tab. 1, we conduct two types of experiments: the first is based on the raw output, directly compared with the ground truth image; the second applies color correction, where global color shift is adjusted using single color vector (R, G, B) to account for potential color shifts (white balance) under varying lighting, details in Zhang et al. [69]. In both setups, our method achieves state-of-the-art performance on RMSE and SSIM, surpassing competing methods by large margin (over 20%). Fig. 5 illustrates visual results from the MIIW dataset, comparing our method with Zhang et al. [69] and RGBX [62]. Our method effectively transfers lighting effects (e.g., highlights, soft shadows) from the reference image to the input while preserving most of the geometry and intrinsic properties. The state-of-the-art deep network [69] struggles to generate specific lighting effects, such as highlights. Notably, although RGB-X achieves the second-best results in Tab. 1, it is unable to transfer lighting across different scenes, as it requires all intrinsic channels to originate from the same scene. Additionally, the alternative text-promptbased relighting method (using the albedo channel along 7 Target Light Original Image Latent Intrinsic RGB-X Ours Ground Truth Figure 5. Image relighting comparison on MIIW [41] dataset. Our method outperforms the current state-of-the-art, Latent Intrinsic [69], achieving superior relighting from distinct directions. Latent Intrinsic fails to capture fine geometric details and color. RGB-X [62] is unable to generate relighting results using image prompting. We were unable to evaluate the text-prompt version, as it does not allow precise specification of lighting direction. Importantly, for our evaluation on the MIIW dataset, we did not use nearest-neighbor search or flow-based enhancement. We used random seed and present results directly from LUMINET without any post-processing. Figure 6. In-the-wild image relighting visual comparison. We evaluate LUMINET on diverse indoor scenes under various target lighting conditions, more in the supplemental. Both RGB-X [62] and IC-Light-v2 [65] require text prompts to achieve relighting, where we use descriptions derived from the target lighting image (including actions like turning lights on/off, lamp placement, and scene type) as text prompts. In contrast, Latent Intrinsic [69] and our method rely solely on image input. When we pass the estimated irradiance from the target light image to RGB-Xs intrinsic channels (RGB-X image prompt), it fails to produce meaningful image. with descriptive text prompt for lighting conditions) is unsuitable for quantitative evaluation due to the difficulty of specifying fine-grained lighting directions in text. We include text-prompting version of RGB-X [62] in room-level relighting, as describing general lighting conditions is more feasible at that level. 6.3. Geometry Consistency and User Study In open-world relighting scenarios, we evaluate our method based on surface normal consistency and conduct user study to assess perceptual image relighting quality, as no ground truth is available. We compare LUMINET with ICLight [65], RGB-X [62], and Latent-Intrinsic [69]. The visual examples are in Fig. 6. For RGB-X [62], we use only the text-prompting relighting for the user study, as irradiance-based relighting is not effective in this setting (see Sec. 6.2 for details). For IC-Light [65], we use the latest FLUX version, specifically IC-Light-v2 (with foreground conditioning), as it offers the best performance. The result for the IC-Light-v1 (with both foreground and background conditioning) can be found in supplemental. To evaluate the geometry consistency, we use RGBX [62] to generate the surface normal for both the original and relight images, and use the surface normal for the origi8 Figure 7. Ablation Study. Left: target light. Second column: source image. Vanilla ControlNet (i.e., without latent intrinsic; third column) fails to perform relighting, changing the average color of the target light while losing all the details from the source image. Without our variational StyleGAN data for training (fourth column), LUMINET does not recognize light sources, such as switching lamps on and off. Without the adaptor network and cross-attention fine-tuning via the light embedding (fifth column), LUMINET cannot generate secondorder lighting effects, such as the gloss on the table (top row). Without flow inversion (sixth column), while relighting is reasonable, artifacts emerge from latent decoding. Combining all components eliminates these artifacts, resulting in plausible relights with secondorder lighting effects (last column). Table 2. Geometry consistency and Perceptual Image Generation Quality. We perform quantitative evaluation of surface normal consistency and conduct user study inspired by [27]. Our method is compared against RGB-X [62], IC-Light-v2 [65], and Latent-Intrinsic [69]. Participants in the study were presented with four images generated by the aforementioned methods, with images generated by our approach, all conditioned on the same target lighting (image or text prompt). We evaluated perceptual quality in terms of image quality (I-PQ), lighting quality (L-PQ), and alignment with the lighting prompt (P-PQ). Our method outperforms all others across all metrics, demonstrating its strong and robust relighting capabilities in the open-world. Method Surface Normal Perceptual Relighting Quality RGB-X [62] IC-Light-v2 [65] Latent-Intrinsic [69] Ours Median-AE 3.14 3.42 3.61 2.74 I-PQ L-PQ 2.21 3.06 2.24 1.71 2.88 2.57 2.52 1. P-PQ 2.70 2.74 2.40 1.40 nal image as the ground-truth. Following the common evaluation protocol for the surface normal evaluation, we measure angular error (AE) for the pixels with ground truth, and report the median value in Tab. 2. Thanks to the carefully designed latent intrinsic condition, our method successfully preserves the geometry details with the median-AE lower than 3 degree. While RGB-X [62], IC-Light-v2 [65] and Latent-Intrinsic [69] all report error larger than 3 degree. We conduct user study with 31 participants to access the perceptual image generation quality inspired by [27]. The metrics are: 1) relit image quality (I-PQ), which evaluates the intrinsic preservation of the relit image; 2) lighting quality (L-PQ), which evaluates the realistic of the lighting; and 3) alignment with the lighting prompt (P-PQ). The question in the study included the original image, the target light image, and four randomly shuffled relit images (produced by the four aforementioned methods, respectively). For each metric, users are asked to rank the four relit images on scale from one to four (where lower score is better). We can not compare with [27] as it targets on outdoor direct relighting, and their model is not publicly available. As reported in Tab. 2 we dominant the leader board by notable margin, which again proves the efficient of our method. 6.4. Ablation Study Fig. 7 shows the visual ablation study. With the same dataset and training setting, ControlNet [64] (Fig. 7 - 2nd column) fails to achieve meaningful relighting, instead it produces an averaged color across the generated image. With the latent intrinsic condition (Fig. 7 - 5th to 7th column), the model can learn lighting transfer and generate effects such as turning lamp on or off. However, when relying solely on the latent intrinsic condition (Fig. 7 - 5th column), the model fails to capture second-order lighting effects, such as reflections on table. This shows the importance of fine-tuning the cross-attention layers in the model. The Flow-based inversion (Fig. 7 - 7th column), helps us clean up noisy artifacts from our LUMINET. To investigate the necessity of Variational StyLitGAN for dataset generation, we removed the data generated by it. As shown in Fig. 7 - 4th column, although general illumination effects can be learned from other datasets, the specific effects caused by light sources (e.g., lamps) in the scene are not captured due to the absence of paired relit images. This highlights the importance of the proposed Variational StyLitGAN for generating such data. 9 Figure 8. Nearest Neighbor Search. Diffusion models are sensitive to seed choice [57]. We observed that the choice of random seeds significantly impacts relighting quality. Here, we present sampled relights generated from 30 random seeds, sorted by their match to the target lighting image. Sorting is based on nearest-neighbor matching of the latent extrinsic (a low-dimensional lighting vector) to the target. 7. Discussion Our work demonstrates that complex indoor scene relighting can be achieved through purely image-based approach using latent representations. Through careful design of latent intrinsic control and diffusion-based generation, LUMINET successfully handles challenging lighting phenomena that previous methods struggled with - from thin cast shadows and specular highlights to complex indirect illumination effects. By leveraging the complementary strengths of latent intrinsic representations and pretrained diffusion models, we achieve photorealistic lighting transfer between diverse indoor scenes without requiring geometric reconstruction or multi-view inputs. While our results show significant progress in image-based relighting, several exciting directions remain for future exploration. These include extending the framework to dynamic scenes, ensuring 3D consistency across multiple viewpoints, and optimizing for real-time applications. Additionally, reducing artifacts without relying on external enhancement methods like RF-Inversion remains an important area for improvement. The success of our latent-space approach suggests broader paradigm shift in how we might tackle complex image manipulation tasks, moving away from explicit physical modeling while maintaining physical plausibility. While LUMINET is trained exclusively on paired images from the same scene, it demonstrates strong generalization to cross-scene lighting transfer in the wild. This ability to transfer lighting between completely different scenes - despite never seeing such examples during training - suggests that our latent intrinsic control mechanism effectively learns to disentangle lighting from scene content. The model successfully preserves the complex structures and materials of the scene while transferring sophisticated lighting effects including specular highlights, cast shadows, and indirect illumination between scenes with vastly different spatial arFigure 9. Failure case. Our method fails to recognize the lamp when the lamp in the original image is either too small or positioned with its back to the camera. Moreover, our method fails to transfer the dramatic lighting color (chromaticity), such as the lighting of Karaoke room. rangements and material properties. This generalization capability emerges from our careful architecture design combining latent intrinsic representations with diffusion models, allowing LUMINET to learn robust lighting transfer principles that extend beyond its training distribution. Limitation. We observed that our method struggles to recognize lamps when they are too small or when the ambiance or vibe of the target light changes dramatically  (Fig. 9)  . We believe this limitation can be alleviated with more diverse data. Another limitation is the inability to control the intensity of the light. We generate plausible relighting results that align with the target lighting, though some may exhibit inaccuracies in lighting intensity or color (chromaticity). Quantifying these discrepancies, however, is challenging in the absence of ground truth data. Our evaluation on the MIT Multi-Illumination dataset shows promising, state-of-theart performance. However, the dataset is largely composed of scenes captured from close camera perspectives and lacks scenarios involving dynamic, natural lighting changes commonly encountered in everyday life, such as lamps turning on and off. 10 Figure 10. Additional Relit Images (switching on ceiling lamps). The target lighting is shown in the top-left image, where ceiling lamp is switched on. Ceiling lamps are very rare in our training data; however, we find that LUMINET is still able to understand them and synthesize plausible relit images, as shown in the third column. In the first row, notice the suppression of gloss near the window at the top (see crop) and the added gloss due to inter-reflection on the TV screen. Also, note how the shaft lighting effect from the source image is suppressed. In the second row, observe how three ceiling lamps significantly brighten the room, with strong gloss visible on both the wooden floor and the dining table. In the third row, notice the sheen on the sofa and the edge of the coffee table, which become clearly visible after relighting. In the fourth row, see how the reflection of the lamp appears on the painting on the side wall. Also, note the shadow cast by the chair on the side wall below the painting. Finally, in the last row, observe how soft shadows along the edges of the ceiling and side wall are suppressed, while soft-light gloss becomes visible. Further, note the reflection on mirror-like object in the bottom crop. 11 Figure 11. Additional Relit Images. The target light is shown in the top-left image, where all lamps are switched off, and the only illumination comes from diffused natural light entering through window on the right. The second column displays the source images to be relit to match the target light, while the third column presents the relit images. The final column highlights cropped regions before and after relighting, emphasizing the second-order lighting effects captured by LUMINET. In the top row (first relit image), note the tables reflection in the TV and the strong gloss on the table from the directional window light. In the fourth row, observe how the sky changes to reflect the ambiance of the target light. In the last row, notice specular highlights on the table because of the direction light from the window. Also, notice the shadow cast by the cabinet in the bottom crop. 12 Figure 12. Additional Relit Images. The target lighting is shown in the top-left image, where all lamps are switched on. The second column displays the source images to be relit to match the target lighting, where all lamps are switched off, and the third column presents the relit images. The final column highlights cropped regions before and after relighting. In the top row (first relit image), note the overall change in the rooms color and the colored gloss added to the side of the bedsheet. In the second row, notice that the strong gloss on the carpet is removed. In the third row, switching on the side lamps removes the lamp shadow; also, observe the effect of the lamp on the ceiling and the gloss added to the edge of the table, as shown in the crop. In the fourth row, notice that the left side of the bed is now well-lit due to the lamp. Finally, in the last row, observe the gloss added to the wallpaper because of switching on the lamp"
        },
        {
            "title": "Acknowledgment",
            "content": "Ocal, and Alexander We thank Pingping Song, Melis Timans for their insightful discussions. We are also grateful to David Forsyth for his suggestion to emphasize that light in scenes cannot simply appear, along with other valuable comments on our manuscript. Additionally, we thank Xiao Zhang for providing the code and model for Latent Intrinsic. Finally, we thank all our participants for their time in finishing our user study. This project is financially supported by Bosch (Bosch Center for Artificial Intelligence), the University of Amsterdam and the allowance of Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron and Jitendra Malik. Shape, illumination, IEEE TPAMI, 37(8):1670 and reflectance from shading. 1687, 2015. 3 [2] H.G. Barrow and J.M. Tenenbaum. Recovering intrinsic scene characteristics from images. In Computer Vision Systems, 1978. 3 [3] Anil Baslamisli, Partha Das, Hoang-An Le, Sezer Karaoglu, and Theo Gevers. Shadingnet: Image intrinsics by fine-grained shading decomposition. IJCV, 129(8):2445 2473, 2021. 3 [4] Anand Bhattad and David Forsyth. Cut-and-paste object insertion by enabling deep image prior for reshading. In 2022 International Conference on 3D Vision (3DV), pages 332 341. IEEE, 2022. 3 [5] Anand Bhattad, Viraj Shah, Derek Hoiem, and DA Forsyth. Make it so: Steering stylegan for any image inversion and editing. arXiv preprint arXiv:2304.14403, 2023. 2, [6] Anand Bhattad, Daniel McKee, Derek Hoiem, and David Forsyth. Stylegan knows normal, depth, albedo, and more. Advances in Neural Information Processing Systems, 36, 2024. 3 [7] Anand Bhattad, James Soole, and David A. Forsyth. Stylitgan: Image-based relighting via latent control. In CVPR, 2024. 2, 4, 5 [8] Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, and Lin Gao. Real-time 3d-aware portrait video relighting. In CVPR, pages 62216231, 2024. 3 [9] Chris Careaga and Yagız Aksoy. Intrinsic image decomposition via ordinal shading. ACM Transactions on Graphics, 2023. 3 [10] Chris Careaga and Yagız Aksoy. Colorful diffuse intrinsic image decomposition in the wild. ACM ToG, 2024. 3 [11] Partha Das, Sezer Karaoglu, and Theo Gevers. Pie-net: Photometric invariant edge guided network for intrinsic image decomposition. In CVPR, 2022. 3 [12] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Jun-Yan Zhu, Tinghui Zhou, and Maneesh Ramanan, Agrawala. Flashtex: Fast relightable mesh texturing with lightcontrolnet. In ECCV, 2024. 3 [13] Sebastian Dille, Chris Careaga, and Yagız Aksoy. Intrinsic single-image hdr reconstruction. In ECCV, 2024. 3 [14] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? arXiv preprint do they know things? arXiv:2311.17137, 2023. 3 lets find out! [15] Sylvain Duchˆene, Clement Riant, Gaurav Chaurasia, Jorge Lopez-Moreno, Pierre-Yves Laffont, Stefan Popov, Adrien Bousseau, and George Drettakis. Multi-view intrinsic images of outdoors scenes with an application to relighting. ACM Transactions on Graphics, page 16, 2015. 3 [16] David Forsyth and Jason Rock. Intrinsic image decomposition using paradigms. IEEE TPAMI, 44(11):76247637, 2021. [17] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXiv preprint arXiv:2311.16043, 2023. 3 [18] Elena Garces, Carlos Rodriguez-Pardo, Dan Casas, and Jorge Lopez-Moreno. survey on intrinsic images: Delving deep into lambert and beyond. IJCV, 130(3):836868, 2022. 3 [19] James Gardner, Evgenii Kashin, Bernhard Egger, and William Alfred Peter Smith. The skys the limit: Relightable outdoor scenes via sky-pixel constrained illumination prior and outside-in visibility. In ECCV, pages 126143. Springer, 2024. 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2 [21] Zhongyun Hu, Xin Huang, Yaning Li, and Qing Wang. SaIn ECCV, pages 535549. ae for any-to-any relighting. Springer, 2020. 3, 7 [22] Kaiwen Jiang, Shu-Yu Chen, Hongbo Fu, and Lin Gao. Nerffacelighting: Implicit and disentangled face lighting representation leveraging generative prior in neural radiance fields. ACM ToG, 42, 2023. 3 [23] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. In NeurIPS, 2024. 2, 3, 4, 5 [24] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, 2019. 2, 3, 4 [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM ToG, 42(4), 2023. 3 [26] Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, and Sanghyun Woo. Switchlight: Co-design of physicsdriven architecture and pre-training framework for human portrait relighting. In CVPR, pages 2509625106, 2024. 2, [27] Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, and Yannick Hold-Geoffroy. Lightit: Illumination modeling and control for diffusion models. arXiv preprint arXiv:2403.10615, 2024. 3, 4, 9 [28] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for single-view material estimation. In CVPR, 2024. 3 14 [29] Edwin Land. The retinex theory of color vision. Scientific american, 1977. 3 [30] Junxuan Li, Hongdong Li, and Yasuyuki Matsushita. Lighting, reflectance and geometry estimation from 360 panoramic stereo, 2021. [31] Zhengqi Li and Noah Snavely. Learning intrinsic image decomposition from watching the world. In CVPR, 2018. 5 [32] Zhengqin Li, Jia Shi, Sai Bi, Rui Zhu, Kalyan Sunkavalli, Miloˇs Haˇsan, Zexiang Xu, Ravi Ramamoorthi, and Manmohan Chandraker. Physically-based editing of indoor scene In ECCV, pages 555572. lighting from single image. Springer, 2022. 2, 3 [33] Zhen Li, Lingli Wang, Mofang Cheng, Cihui Pan, and Jiaqi Yang. Multi-view inverse rendering for large-scale realworld indoor scenes, 2023. 2 [34] Zhi-Hao Lin, Bohan Liu, Yi-Ting Chen, Kuan-Sheng Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, and Shenlong Wang. Urbanir: Large-scale urban scene inverse rendering from single video. In 3DV, 2025. 3 [35] Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei Efros, and Noah Snavely. Learning to factorize and relight city. In ECCV, pages 544561. Springer, 2020. 3 [36] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, pages 1197611986, 2022. 5 [37] Loshchilov. Decoupled weight decay regularization. In ICLR, 2017. 7 [38] Jundan Luo, Duygu Ceylan, Jae Shin Yoon, Nanxuan Zhao, Julien Philip, Anna Fruhstuck, Wenbin Li, Christian Richardt, and Tuanfeng Wang. Intrinsicdiffusion: joint intrinsic layers from latent diffusion models. In SIGGRAPH, pages 111, 2024. 3, 4 [39] Yiqun Mei, He Zhang, Xuaner Zhang, Jianming Zhang, Zhixin Shu, Yilin Wang, Zijun Wei, Shi Yan, HyunJoon Jung, and Vishal M. Patel. Lightpainter: Interactive portrait relighting with freehand scribble. In CVPR, 2023. 3 [40] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. arXiv preprint arXiv:2003.08934, 2020. 3 [41] Lukas Murmann, Michael Gharbi, Miika Aittala, and Fredo Durand. multi-illumination dataset of indoor object appearance. In ICCV, 2019. 2, 3, 5, 7, 8 [42] Thomas Nestmeyer, Jean-Francois Lalonde, Iain Matthews, Epic Games, Andreas Lehrmann, and AI Borealis. Learning physics-guided face relighting under directional light. 2020. [43] Julien Philip, Michael Gharbi, Tinghui Zhou, Alexei Efros, and George Drettakis. Multi-view relighting using geometry-aware network. ACM Trans. Graph., 38(4):781, 2019. 3 [44] Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, and Supasorn Suwajanakorn. Diffusionlight: Light probes for free by painting chrome ball. In CVPR, 2024. 2 [45] Yohan Poirier-Ginter, Alban Gauthier, Julien Philip, JeanFrancois Lalonde, and George Drettakis. Diffusion Approach to Radiance Field Relighting using MultiIllumination Synthesis. Computer Graphics Forum, 2024. 3 [46] Puntawat Ponglertnapakorn, Nontawat Tritrong, and Supasorn Suwajanakorn. Difareli: Diffusion face relighting. In ICCV, 2023. 2, 3, 4, 5 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, [49] Rout, Chen, Ruiz, Caramanis, Shakkottai, and Chu. Semantic image inversion and editing using rectified stochastic differential equations. 2024. 7 [50] Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, and Christian Theobalt. Nerf for outdoor scene relighting. In ECCV, 2022. 3 [51] Soumyadip Sengupta, Brian Curless, Ira KemelmacherShlizerman, and Steven Seitz. light stage on every desk. In ICCV, pages 24202429, 2021. 3 [52] Tiancheng Sun, Jonathan Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. Single image portrait relighting. ACM Transactions on Graphics (Proceedings SIGGRAPH), 2019. 3 [53] Marco Toschi, Riccardo De Matteo, Riccardo Spezialetti, Daniele De Gregorio, Luigi Di Stefano, and Samuele Salti. Relight my nerf: dataset for novel view synthesis and relighting of real world objects. In CVPR, pages 2076220772, 2023. [54] Chen Xi, Peng Sida, Yang Dongchen, Liu Yuan, Pan Bowen, Lv Chengfei, and Zhou. Xiaowei. Intrinsicanything: Learning diffusion priors for inverse rendering under unknown illumination. In ECCV, 2024. 3 [55] Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, and Theo GevIntrinsic appearance decomposition using point cloud ers. representation. In ICCVW, 2023. 3 [56] Xiaoyan Xing, Vincent Tao Hu, Jan Hendrik Metzen, Konrad Groh, Sezer Karaoglu, and Theo Gevers. Retinex-diffusion: On controlling illumination conditions in diffusion models via retinex theory. arXiv preprint arXiv:2407.20785, 2024. 3 [57] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Good seed makes good crop: Discovering secret seeds in text-toimage diffusion models. arXiv preprint arXiv:2405.14828, 2024. 7, 10 [58] Hao-Hsiang Yang, Wei-Ting Chen, and Sy-Yen Kuo. S3net: single stream structure for depth guided image relighting. In CVPR, pages 276283, 2021. 3, [59] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of large-scale image dataset 15 using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 5 [60] Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian Theobalt, and William AP Smith. SelfIn ECCV, pages 84 supervised outdoor scene relighting. 101. Springer, 2020. 3 [61] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH 2024 Conference Proceedings, 2024. 2, 3, 4 [62] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. Rgb-x: Image decomposition and synthesis using material-and lighting-aware diffusion models. In SIGGRAPH, pages 111, 2024. 2, 3, 4, 7, 8, 9 [63] Edward Zhang, Michael F. Cohen, and Brian Curless. Emptying, refurnishing, and relighting indoor spaces. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2016), 35(6), 2016. [64] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 6, 9 [65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Ic-light: More relighting! GitHub Repository, 2024. https:// github.com/lllyasviel/IC-Light. 2, 3, 4, 5, 8, 9 [66] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 5 [67] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul Debevec, et al. Neural light transport for relighting and view synthesis. ACM TOG, 40(1):117, 2021. 2 [68] Xiuming Zhang, Pratul Srinivasan, Boyang Deng, Paul Debevec, William Freeman, and Jonathan Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM TOG, 40(6):118, 2021. 3 [69] Xiao Zhang, William Gao, Seemandhar Jain, Michael Maire, David Forsyth, and Anand Bhattad. Latent intrinsics emerge from training to relight. In NeurIPS, 2024. 2, 3, 4, 5, 7, 8, 9 [70] Zitian Zhang, Frederic Fortier-Chouinard, Mathieu Garon, Anand Bhattad, and Jean-Francois Lalonde. Zerocomp: Zero-shot object compositing from image intrinsics via diffusion. arXiv preprint arXiv:2410.08168, 2024. 3 [71] Xiaoming Zhao, Pratul P. Srinivasan, Dor Verbin, Keunhong Park, Ricardo Martin Brualla, and Philipp Henzler. IllumiNeRF: 3D Relighting Without Inverse Rendering. In NeruIPS, 2024. 3 [72] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David Jacobs. Deep single-image portrait relighting. In Proceedings of the IEEE International Conference on Computer Vision, pages 71947202, 2019. 3 [73] Jingsen Zhu, Yuchi Huo, Qi Ye, Fujun Luan, Jifan Li, Dianbing Xi, Lisha Wang, Rui Tang, Wei Hua, Hujun Bao, and Rui Wang. I2-sdf: Intrinsic indoor scene reconstruction and editing via raytracing in neural sdfs, 2023."
        }
    ],
    "affiliations": [
        "BCAI-Bosch",
        "Toyota Technological Institute at Chicago",
        "UvA-Bosch Delta Lab"
    ]
}