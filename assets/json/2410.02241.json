{
    "paper_title": "MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction",
    "authors": [
        "Zhaojian Yu",
        "Yinghao Wu",
        "Genesis Wang",
        "Heming Weng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Stock market prediction has remained an extremely challenging problem for many decades owing to its inherent high volatility and low information noisy ratio. Existing solutions based on machine learning or deep learning demonstrate superior performance by employing a single model trained on the entire stock dataset to generate predictions across all types of stocks. However, due to the significant variations in stock styles and market trends, a single end-to-end model struggles to fully capture the differences in these stylized stock features, leading to relatively inaccurate predictions for all types of stocks. In this paper, we present MIGA, a novel Mixture of Expert with Group Aggregation framework designed to generate specialized predictions for stocks with different styles by dynamically switching between distinct style experts. To promote collaboration among different experts in MIGA, we propose a novel inner group attention architecture, enabling experts within the same group to share information and thereby enhancing the overall performance of all experts. As a result, MIGA significantly outperforms other end-to-end models on three Chinese Stock Index benchmarks including CSI300, CSI500, and CSI1000. Notably, MIGA-Conv reaches 24 % excess annual return on CSI300 benchmark, surpassing the previous state-of-the-art model by 8% absolute. Furthermore, we conduct a comprehensive analysis of mixture of experts for stock market prediction, providing valuable insights for future research."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 1 4 2 2 0 . 0 1 4 2 : r MIGA: MIXTURE-OF-EXPERTS WITH GROUP AG-"
        },
        {
            "title": "GREGATION FOR STOCK MARKET PREDICTION",
            "content": "Zhaojian Yu1,2, Yinghao Wu1, Genesis Wang2,, Heming Weng2 1Tsinghua University 2XM Capital yzj23@mails.tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Stock market prediction has remained an extremely challenging problem for many decades owing to its inherent high volatility and low information noisy ratio. Existing solutions based on machine learning or deep learning demonstrate superior performance by employing single model trained on the entire stock dataset to generate predictions across all types of stocks. However, due to the significant variations in stock styles and market trends, single end-to-end model struggles to fully capture the differences in these stylized stock features, leading to relatively inaccurate predictions for all types of stocks. In this paper, we present MIGA, novel Mixture of Expert with Group Aggregation framework designed to generate specialized predictions for stocks with different styles by dynamically switching between distinct style experts. To promote collaboration among different experts in MIGA, we propose novel inner group attention architecture, enabling experts within the same group to share information and thereby enhancing the overall performance of all experts. As result, MIGA significantly outperforms other end-to-end models on three Chinese Stock Index benchmarks including CSI300, CSI500, and CSI1000. Notably, MIGA-Conv reaches 24 % excess annual return on CSI300 benchmark, surpassing the previous state-of-the-art model by 8% absolute. Furthermore, we conduct comprehensive analysis of mixture of experts for stock market prediction, providing valuable insights for future research."
        },
        {
            "title": "INTRODUCTION",
            "content": "The stock market, complex dynamic system, has long attracted investors worldwide who seek to generate high returns through risk-taking and achieve their investment objectives. However, due to the inherent volatility and noisy characteristics of financial markets (Fama, 1970; Malkiel, 2003), forecasting stock prices accurately and making profitable investment decisions are extremely challenging tasks. These challenges are considered to stem from the interplay of multiple stock factors, including market participants behavior (Shiller, 2003), macroeconomic variables (Chen, 1988), and the difficult-to-quantify flow of information (Grossman & Stiglitz, 1980). Over past decades, machine learning (ML) and deep learning (DL) methods have shown remarkable capabilities in stock market prediction by learning from dataset comprised of various stock factors in an end-to-end manner. Most existing ML or DL methods adopt an advanced network architectures derives from some basic networks like LSTM (Hochreiter, 1997) and Transformer (Vaswani et al., 2017). For instance, FactorVAE (Duan et al., 2022) introduces novel approach to predicting excess returns and benchmark returns separately using Variational Auto-Encoders (Kingma, 2013). MASTER (Li et al., 2024) proposes transformer-based model that addresses complex stock correlations through alternating intra-stock and inter-stock information aggregation. These methods adopt stock market prediction as supervised learning task, where the historical stock factors is always seemed as features and the return rate is applied as the target for single end-to-end model. Therefore, the inherent temporal characteristics of stock market enable end-to-end models to effectively predict future stock returns. However, in the context of financial markets, stocks of different styles often tend to exhibit significant variations in their features of stock factors. For instance, large-cap stocks are stable and offer 1 Figure 1: Comparison between MIGA and single end-to-end model. MIGA generates specialized predictions for stocks with different styles by dynamically switching between distinct style experts. predictable growth, appealing to risk-averse investors seeking modest returns. But mall-cap stocks are more volatile and have higher growth potential, making them suitable for investors with higher risk tolerance and longer investment horizon. These salient stylized features are often input into the training of single end-to-end model without differentiation, enabling single end-to-end model struggles to capture the differences in these stylized stock features. In this paper, we introduce MIGA, novel Mixture of Expert with Group Aggregation framework that could generate specialized predictions of various styles of stocks by dynamically switching between specialized experts. As depicted in Figure 1, the proposed MIGA architecture diverges from traditional end-to-end models in its two-stage design. The first stage employs an expert router, built upon the end-to-end model, which encodes raw stock data into vector representations and subsequently converts these representations into expert allocation weights via linear transformation. The second stage introduces novel expert gathering aggregation structure, wherein experts are grouped and an internal group attention mechanism is incorporated within each expert group to facilitate knowledge sharing and collaboration among experts, thereby enhancing the overall predictive performance of the model. To validate MIGA, we conduct comprehensive evaluation using 3 distinct types of feature encoders (convolution-based, recurrence-based, and attention-based) as our expert router, and linear layer as our experts, consistent with the design principles established in prior research (Dai et al., 2024; Zoph et al., 2022).We denote these three MIGA models as MIGA-Conv, MIGA-Rec, and MIGA-Attn. Our experimental evaluation involves assessing the performance of these MIGA models on three prominent Chinese stock indices (CSI300, CSI500, and CSI1000) with long-only and long-short stock portfolios. The results demonstrate that our MIGA models exhibit superior performance, outperforming all other end-to-end models. Notably, on the CSI300 benchmark, MIGA-Conv achieves remarkable 24% excess annual return with long-only portfolio. Furthermore, we provide an in-depth analysis of the Mixture of Expert for stock prediction, providing valuable insights for future work."
        },
        {
            "title": "2 MIGA: MOE WITH GROUP AGGREGATION",
            "content": "A Mixture of Experts (MoE) architecture comprises two main components: the router and the experts. The router assigns weights for each data point, while each expert generates its own prediction. The final output of the MoE is the weighted aggregation of the predictions from all experts. In this section, we present the methodology by which MIGA generates predictions and undergoes training process. 2 Figure 2: Overview of Mixture of Expert with Group Aggregation. nner group attention refers to the self-attention mechanism within each expert group, which facilitates the aggregation of information among experts within the same group. 2.1 PROBLEM FORMULATION Following previous work (Duan et al., 2022; Li et al., 2024), we formulate stock market prediction as supervised learning task by generating trading strategies based on daily cross-sectional stock prices. Consider stock pool of size , where for each stock on trading day t, there is corresponding RN D, where is the number of features and is and feature vector xi average price pi the range of days for stocks. For each feature vector xi t, we utilize the stocks future return rate as training label to supervise model learning which can be calculated as follows: yi = t+2 pi pi pi t+1 t+1 (1) Unless otherwise stated, we use xi,t, yi,t to denote the sequential feature vectors (xi future return yi of stock on time in the following of this work for simplicity. tT , ..., xi t) and"
        },
        {
            "title": "2.2.1 ROUTER",
            "content": "As depicted in Figure 2, we initiate the processing pipeline by leveraging trainable router to perform cross-sectional encoding of the stock set N = (x1,t, x2,t, ..., xN ,t), thereby capturing the complex inter-relationships between the constituent stocks and generate routing weights for them. Subsequently, we employ top-k selection strategy to identify the most salient experts, characterized by the highest routing weights, and then apply softmax normalization to the selected subset, thereby obtaining the final, normalized weights that capture the relative importance of each experts contribution. This process can be expressed as: Hi,t GE = Router(xi,t) (2) 3 Hi,t GE = {hi,t j,k}, = 1, 2, ..., G, = 1, 2, ..., (cid:40) {wi,t j,k}GE = softmax({hi,t 0, j,k}), hi,t j,k TopK(Hi,t otherwise GE ) (3) (4) where G, indicates the group pool and the expert pool respectively, hi,t j,k denotes the latent state representation of the routers output for the i-th stock of day at the k-th of the j-th group and wi,t j,k represents the routing weight for the i-th stock of day assigned by the router to each expert at the k-th of the j-th group, which governs the degree of contribution from each expert to the final output. 2.2.2 EXPERT For expert models, each expert takes the same hidden representation of stocks generated by router as input and generates their own predictions oi,t j,k. In MIGA, we set all experts as linear layers to prevent the final model from becoming overly complex. The generation process can be expressed as: j,k = Expertj,k(hi,t oi,t j,k) As illustrated in Figure 1, these predictions are then aggregated through an inner-group attention mechanism, which facilitates interaction among the experts within the group and enriches the output representation of each individual expert through information sharing. (5)"
        },
        {
            "title": "2.2.3 GROUP AGGREGATION",
            "content": "For group aggregation, we begin by concatenating the output oi,t experts to form unified input representation. Subsequently, we generate the query Qi,t and value i,t matrices necessary for the inner-group attention mechanism as follows: j,k representations of all constituent , key i,t , Oi,t = Concat({oi,t j,kk = 1, 2, ..., E}) (6) Qi,t = WqOi,t = WkOi,t The outputs of individual experts within the group are then cross-interacted via the attention mechanism, yielding mixed output Oi,t j,kk = 1, 2, ..., E} representation that aggregates the knowledge and insights of all experts in the group. = WvOi,t = {oi,t , i,t , i,t (7) Oi,t = MultiHeadSelfAttention(Qi,t , i,t , i,t ) (8) After obtaining the weight wi,t j,k from each expert within the expert group for future returns, we compute the weighted composite prediction of all experts to derive MIGAs prediction of future profitability for stock at time t. And for each prediction ˆyi j,k emitted by the router and the aggregated predictions oi,t , we have: ˆyi = (cid:88) (cid:88) j=0 k=0 j,k oi,t wi,t j,k (9)"
        },
        {
            "title": "2.3 TRAINING",
            "content": "Expert Loss For the training process, we depart from the conventional approach of employing mean squared error (MSE) (Li et al., 2024) as the loss function for model training. Instead, we incorporate the information coefficient (IC) into the loss calculation framework to ensure the correlation between labels and predictions. Consider the actual stock cross-sectional returns ti = 1, 2, ..., } at time and the predicted stock cross-sectional returns ti = 1, 2, ..., }, we calculated the LExpert as follows: label = {yi pred = {ˆyi 4 LExpert = 1 (cid:88) tT (cid:113) cov(Y pred, label) pred)var(Y label) var(Y (10) Load Balance Consideration Automatically learned routing strategies are susceptible to load imbalance, which can lead to the phenomenon of routing collapse (Shazeer et al., 2017a). Specifically, this occurs when the model consistently favors subset of experts, thereby depriving other experts of adequate training opportunities and hindering their ability to contribute to the overall performance of the model. In many existing MoE architectures designed for training large language models (Zoph et al., 2022; Dai et al., 2024), token load balance auxiliary loss (Shazeer et al., 2017a) is employed to ensure that all tokens are distributed as evenly as possible among experts. However, this approach is not directly applicable to stock market prediction tasks, as there is no analogous concept of tokens in stock market. Hence, we reduce this imbalance by minimizing the distance between the router output = {hi,t hi j,k} and its mean value. The loss function can be written as: LRouter = (cid:88) (cid:88) tT i=1 (hi mean(hi t)) (11) Integrating all the aforementioned considerations, we arrive at the final loss function utilized for training MIGA: LMIGA = αLRouter + βLExpert (12) By minimizing LMIGA, we can optimize the correlation (LExpert) between predicted returns and actual returns, concurrently mitigating load imbalance by reducing the routing loss (LRouter)."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "To instantiate MIGA, we leverage PyTorch to implement the entire MoE architecture and utilize NVIDIA A100-80GB GPU for training. For each model in our experiments, we set the maximum number of training epochs to 60 and implement an early stopping strategy to prevent overfitting and ensure model generalizability. We set the initial learning rate at 5e-4 and the hyper-parameters α, β in LMIGA at 2e-3, 1 respectively. Following previous work (Li et al., 2024), we set the history stock sequence length as 5, which is week of trading days. Furthermore, for the selection of the number of groups and experts, we conduct comprehensive analysis in Section 3.5."
        },
        {
            "title": "3.2 EVALUATION SETUP",
            "content": "Datasets Our dataset comprises 626 daily features which covers all stocks in the Chinese stock market, ranging from January 1, 2014, to July 25, 2024. We partitioned the dataset temporally, using data from January 1, 2014, to July 25, 2022, as the training set, data from July 25, 2022, to July 25, 2023, as the validation set, and data from July 25, 2023, to July 25, 2024, as the test set. At the end of each training epoch, the model was evaluated on the validation set, and the optimal model configuration was retained as the final model. Baselines To evaluate the performance of MIGA more comprehensively, we replaced the stock representation encoder in different routers and trained three different MIGA models. Specifically, for MIGA-Conv, we adopted the convolutional operation from the Temporal Convolutional Network (TCN) (Hewage et al., 2020). For MIGA-Rec, we utilized an LSTM (Greff et al., 2016) as the encoder in our router. For MIGA-Attn, we employed the Transformer (Ding et al., 2020) encoder. Subsequently, We compare the performance of MIGA models (MIGA-Conv, MIGA-Rec, MIGAAttn) with several stock price forecasting baselines : TCN (Hewage et al., 2020), LSTM (Greff et al., 2016), Transformer (Vaswani et al., 2017) and three previous state-of-the-art (SoTA) models: ModernTCN (Donghao & Xue, 2024), Mamba (Gu & Dao, 2023) and MASTER (Li et al., 2024). Benchamrks We evaluate all MIGA models on three Chinese Stock Index benchmarks including CSI300, CSI500 and CSI1000 stock sets. The CSI300 index is blue-chip index comprising 300 5 Figure 3: The comparison of the training loss and validation set IC between MIGA and single end-to-end models during the first 8 epochs, where the optimal performance of most models emerges. large-cap stocks with superior liquidity. In contrast, the CSI500 index is mid-cap index that includes 500 medium-sized companies with strong liquidity, characterized by their growth potential and dynamic business models, making the CSI500 valuable indicator of the mid-cap segment of Chinas A-share market. The CSI1000 index, on the other hand, consists of 1,000 small and medium-cap stocks. Collectively, these three indices provide comprehensive representation of the overall performance of Chinas A-share market."
        },
        {
            "title": "3.3 METRICS",
            "content": "To provide comprehensive assessment of the models performance, we employ combination of ranking metrics and portfolio-based metrics. Specifically, we consider four ranking metrics: Information Coefficient (IC), Rank Information Coefficient (RankIC), Information Ratio-based IC (ICIR), and Information Ratio-based RankIC (RankICIR). These metrics are defined as follows: IC and RankIC represent the Pearson correlation coefficient and Spearman correlation coefficient, averaged at daily frequency. ICIR and RankICIR are normalized variants of IC and RankIC, obtained by dividing by the standard deviation. ICIR = ean(IC) Std(IC) , RankICIR = ean(RankIC) Std(RankIC) (13) These metrics are widely used in the literature (e.g., Xu et al. (2021); Yang et al. (2020)) to evaluate the performance of forecasting models from both value and rank perspectives. For portfolio-based metrics, we sort all stocks daily based on their predicted return rate, selecting the top 5% for long-only strategy. Simultaneously, we identify the bottom 5% for short selling, combining both positions into long-short portfolio. Following previous work (Li et al., 2024), we report the excess annualized return (AR) and information ratio (IR) metrics. The AR metric measures the annual expected excess return generated by the investment, while the IR metric evaluates the risk-adjusted performance of the investment."
        },
        {
            "title": "3.4 RESULT",
            "content": "Table 1, 2 present the results of MIGA on 3 Chinese Stock Index (CSI300, CSI500, CSI1000) benchmarks, highlighting the following salient observations: MIGA achieves SoTA performance by surpassing the previous SoTA models. MIGA models achieve the best results on 15/16 of the ranking metrics and 14/16 portfolio-based metrics. Notably, 6 Table 1: Overall performance comparison on ranking metrics. For each model, we conducted three experiments, using the mean as the final result and the standard deviation (in brackets) to indicate the confidence interval. The best results are in bold and the second-best results are underlined. IC Rank ICIR Stockset RankIC Model ICIR Type CSI300 CSI CSI1000 Average Baselines SoTA models MIGA Baselines SoTA models MIGA Baselines SoTA models MIGA Baselines SoTA models MIGA TCN LSTM Transformer 0.041 (0.001) 0.013 (0.002) 0.039 (0.001) 0.041 (0.008) Mamba MASTER 0.040 (0.008) ModernTCN 0.041 (0.005) MIGA-Conv MIGA-Rec MIGA-Attn TCN LSTM Transformer 0.052 (0.002) 0.034 (0.007) 0.048 (0.002) 0.035 (0.004) 0.024 (0.009) 0.031 (0.003) 0.037 (0.003) Mamba MASTER 0.034 (0.003) ModernTCN 0.040 (0.004) MIGA-Conv MIGA-Rec MIGA-Attn TCN LSTM Transformer 0.042 (0.003) 0.042 (0.002) 0.035 (0.001) 0.040 (0.004) 0.032 (0.006) 0.034 (0.002) 0.042 (0.001) Mamba MASTER 0.040 (0.001) ModernTCN 0.047 (0.001) MIGA-Conv MIGA-Rec MIGA-Attn TCN LSTM Transformer 0.043 (0.002) 0.048 (0.001) 0.039 (0.001) 0.039 (0.009) 0.023 (0.017) 0.035 (0.006) 0.040 (0.012) Mamba MASTER 0.038 (0.012) ModernTCN 0.043 (0.010) MIGA-Conv MIGA-Rec MIGA-Attn 0.046 (0.007) 0.041 (0.010) 0.041 (0.004) 0.272 (0.034) 0.127 (0.015) 0.222 (0.017) 0.239 (0.018) 0.248 (0.003) 0.240 (0.013) 0.265 (0.007) 0.230 (0.011) 0.273 (0.004) 0.253 (0.018) 0.233 (0.061) 0.236 (0.015) 0.281(0.030) 0.269 (0.018) 0.299 (0.022) 0.279 (0.027) 0.326 (0.018) 0.270 (0.017) 0.310 (0.025) 0.302 (0.031) 0.250 (0.031) 0.311 (0.018) 0.311 (0.007) 0.358 (0.015) 0.299 (0.016) 0.383 (0.017) 0.296 (0.013) 0.278 (0.077) 0.221 (0.107) 0.236 (0.063) 0.277 (0.066) 0.276 (0.028) 0.299 (0.050) 0.281 (0.050) 0.313 (0.046) 0.280 (0.034) 0.063 (0.003) 0.026 (0.005) 0.066 (0.015) 0.060 (0.016) 0.065 (0.013) 0.067 (0.011) 0.079 (0.004) 0.046 (0.011) 0.074 (0.004) 0.058 (0.004) 0.039 (0.014) 0.058 (0.006) 0.056 (0.009) 0.057 (0.006) 0.062 (0.010) 0.069 (0.001) 0.049 (0.010) 0.059 (0.002) 0.067 (0.001) 0.057 (0.004) 0.069 (0.003) 0.059 (0.008) 0.068 (0.003) 0.071 (0.004) 0.072 (0.003) 0.069 (0.001) 0.067 (0.001) 0.063 (0.008) 0.041 (0.023) 0.064 (0.024) 0.058 (0.033) 0.063 (0.022) 0.067 (0.025) 0.073 (0.008) 0.055 (0.022) 0.067 (0.007) 0.382 (0.043) 0.249 (0.031) 0.375 (0.016) 0.333 (0.029) 0.363 (0.004) 0.363 (0.007) 0.365 (0.014) 0.297 (0.012) 0.394 (0.010) 0.388 (0.041) 0.370 (0.085) 0.433 (0.038) 0.392 (0.036) 0.413 (0.020) 0.424 (0.011) 0.404 (0.012) 0.363 (0.067) 0.433 (0.012) 0.471 (0.011) 0.502 (0.010) 0.495 (0.005) 0.397 (0.137) 0.483 (0.165) 0.477 (0.008) 0.452 (0.004) 0.496 (0.044) 0.493 (0.007) 0.414 (0.095) 0.374 (0.126) 0.434 (0.059) 0.374 (0.202) 0.420 (0.189) 0.421 (0.026) 0.407 (0.030) 0.385 (0.123) 0.440 (0.029) MIGA-Conv achieves 24% improvement in long-only portfolio metrics compared to ModernTCN, the SoTA end-to-end model, with an annual return (AR) of 0.24 versus 0.18 and Information Ratio (IR) of 1.80 versus 1.37 on CSI300 benchmark as well as IC of 0.046 versus 0.043 and Rank IC of 0.073 versus 0.067 on of the average on 3 benchmarks. Although the ICIR and RankICIR are slightly lower, the final portfolio return of MIGA is higher than ModernTCN. The achievements in both types of metrics imply that MIGA is of good predicting ability on the 3 stock set without trade-off. MIGA can significantly enhance the performance of single end-to-end model. Compared with the baseline end-to-end models, MIGA significantly outperforms them on all benchmarks. For example, MIGA-Conv and TCN are both convolution-based models, with MIGA-Conv demonstrating superior performance across all CSI benchmarks. For instance, MIGA-Conv substantially outperforms TCN under long-only portfolio (0.24 vs 0.13 on AR, 1.80 vs 0.97 on IR). This trend is also observed in the comparison between MIGA-Rec and LSTM, as well as MIGA-Attn and Transformer, highlighting the architectural advantages of MIGA. Furthermore, compared to single end-to-end models, MIGA 7 Table 2: Comparison of portfolio result on three stocksets. For each model, we conducted three experiments, using the mean as the final result and the standard deviation (in brackets) to indicate the confidence interval. The best results are in bold and the second-best results are underlined. Stockset Type Model Long-only AR IR Long-short AR IR CSI CSI500 Baselines SoTA models MIGA Baselines SoTA models MIGA Baselines CSI1000 SoTA models MIGA Baselines SoTA models MIGA Average TCN LSTM Transformer Mamba MASTER ModernTCN MIGA-Conv MIGA-Rec MIGA-Attn TCN LSTM Transformer Mamba MASTER ModernTCN MIGA-Conv MIGA-Rec MIGA-Attn TCN LSTM Transformer Mamba MASTER ModernTCN MIGA-Conv MIGA-Rec MIGA-Attn TCN LSTM Transformer Mamba MASTER ModernTCN MIGA-Conv MIGA-Rec MIGA-Attn 0.13 (0.07) -0.03 (0.05) 0.17 (0.08) 0.97 (0.50) -0.18 (0.38) 1.24 (0.51) 0.15 (0.10) 0.16 (0.04) 0.18 (0.08) 0.24 (0.04) 0.07 (0.02) 0.20 (0.01) 0.13 (0.05) 0.08 (0.06) 0.14 (0.04) 0.17 (0.05) 0.18 (0.04) 0.14 (0.04) 0.18 (0.03) 0.11 (0.06) 0.11 (0.02) 0.11 (0.02) 0.13 (0.04) 0.17 (0.02) 0.10 (0.06) 0.13 (0.03) 0.14 (0.04) 0.15 (0.02) 0.13 (0.04) 0.09 (0.02) 0.12 (0.14) 0.06 (0.15) 0.16 (0.14) 0.14 (0.21) 0.16 (0.11) 0.15 (0.16) 0.19 (0.09) 0.10 (0.12) 0.13 (0.05) 1.06 (0.73) 1.22 (0.33) 1.37 (0.52) 1.80 (0.29) 0.50 (0.02) 1.47 (0.03) 0.73 (0.26) 0.45 (0.33) 0.78 (0.19) 0.94 (0.29) 0.99 (0.23) 0.76 (0.26) 1.07 (0.18) 0.64 (0.36) 0.60 (0.13) 0.51 (0.07) 0.59 (0.17) 0.77 (0.10) 0.45 (0.26) 0.60 (0.13) 0.69 (0.21) 0.69 (0.09) 0.61 (0.20) 0.42 (0.08) 0.74 (0.83) 0.29 (0.88) 0.93 (0.80) 0.82 (1.28) 0.94 (0.69) 0.94 (0.99) 1.19 (0.56) 0.58 (0.58) 0.83 (0.24) 0.60 (0.13) 0.11 (0.07) 0.66 (0.22) 0.66 (0.09) 0.57 (0.09) 0.75 (0.08) 0.82 (0.10) 0.55 (0.15) 0.80 (0.06) 1.00 (0.07) 0.67 (0.26) 0.91 (0.08) 1.09 (0.26) 1.03 (0.16) 0.99 (0.13) 1.17 (0.03) 1.17 (0.11) 0.89 (0.20) 1.49 (0.10) 1.18 (0.12) 1.45 (0.11) 1.55 (0.25) 1.47 (0.17) 1.67 (0.17) 1.61 (0.12) 1.74 (0.10) 1.41 (0.07) 1.03 (0.30) 0.65 (0.45) 1.01 (0.41) 1.10 (0.60) 1.02 (0.42) 1.14 (0.38) 1.20 (0.25) 1.15 (0.36) 1.03 (0.33) 3.33 (0.73) 0.86 (0.58) 3.64 (0.65) 3.71 (0.91) 3.49 (0.40) 4.10 (0.33) 3.94 (0.50) 3.39 (0.71) 4.37 (0.29) 6.80 (0.34) 5.21 (1.39) 5.95 (0.77) 6.92 (2.16) 6.67 (1.01) 6.51 (0.88) 7.06 (0.03) 7.79 (0.51) 6.51 (0.88) 8.39 (0.47) 8.40 (0.32) 8.43 (0.82) 9.08 (1.35) 8.40 (0.72) 9.89 (0.72) 8.76 (0.67) 10.20 (0.48) 8.16 (0.44) 6.17 (1.54) 4.82 (2.29) 6.01 (2.24) 6.57 (4.42) 6.19 (2.13) 6.83 (1.93) 6.59 (1.20) 7.13 (1.70) 6.35 (1.61) demonstrates more balanced performance across different benchmarks by aggregating different experts. This approach allows MIGA to effectively adapt to the varying characteristics of the CSI300, CSI500, and CSI1000 indices by switching the expert with different styles, showcasing its effectiveness and versatility in diverse market styles. MIGA exhibits superior stock market prediction capabilities on unseen dataset. As illustrated in Figure 3, we compare the training loss and validation set information correlation (IC) between the Multi-Input Gradient-Aware (MIGA) model and single end-to-end models during the first 8 epochs, period during which the optimal performance of most models typically emerges. Despite the comparable training loss observed between the two approaches, MIGA demonstrates notably higher information correlation on the validation set. This superior performance on the validation set suggests that MIGA has more robust capability to predict future profits, which is particularly significant in financial forecasting. Figure 4: Comparison about different number of experts. The optimal setting of 8 out of 63 experts (7 groups of 9 experts each) achieve the best result on 8/12 of the metrics. Table 3: Impact of inner group attention. We compared the long-only portfolio results of 3 types of MIGA models with inner group attention or not. The results show that removing inner group attention led to decline in model performance."
        },
        {
            "title": "Model",
            "content": "CSI300 IR AR CSI500 IR AR CSI1000 IR AR MIGA-Conv w/o inner group attention MIGA-Rec w/o inner group attention MIGA-Attn w/o inner group attention 0.21 0.10 0.06 0.05 0.19 0.22 1.56 0. 0.46 0.33 1.44 1.59 0.16 0.18 0.10 0.05 0.08 0.11 0.94 0. 0.58 0.23 0.46 0.65 0.14 0.12 0.17 0.08 0.12 0.06 0.64 0. 0.85 0.35 0.53 0."
        },
        {
            "title": "3.5 ABLATION STUDY",
            "content": "Scaling up the number of experts To investigate the effect of expert aggregation size on model performance, we conducted an experiment using the MIGA architecture, with results illustrated in Figure 4. The findings indicate that increasing the number of experts leads to improved ICIR and RankICIR scores, suggesting that this approach can substantially enhance the stability of the models prediction. To further explore the upper limit of the models potential, we selected 8 out of 63 routed experts (organized into 7 groups of 9 experts each) for in-depth analysis in this paper. Impact of inner group attention To evaluate the efficacy of inner group attention, we compare MIGA with mixture of isolated experts that does not incorporate inner group attention. As shown in Table 3, MIGA outperforms the mixture of isolated experts and demonstrates more balanced performance across the CSI300, CSI500, and CSI1000 benchmarks. This improvement can be attributed to the enhanced overall level of experts achieved through inner group attention, highlighting the effectiveness of inner group attention in facilitating more efficient and effective knowledge sharing among experts in Mixture of Experts (MoE) systems."
        },
        {
            "title": "3.6 ANALYSIS ON EXPERT SPECIALIZATION",
            "content": "To evaluate the expert specialization in MIGA, we conduct individual predictions on the test set using each expert in MIGA-Conv and compare their excess annual returns across various stock benchmarks and portfolios. As shown in Figure 5 and 6 (in Appendix A), the majority of experts demonstrate accurate predictions, with only 7 out of 63 experts failing to generate excess returns. This suggests that each expert has acquired sufficient predictive capabilities during MIGAs training. Furthermore, different experts exhibit varying levels of proficiency across different areas. For instance, in MIGAConv (Figure 5), Expert 3 achieves an impressive excess return of nearly 30% on CSI300 but fails to replicate this performance on CSI500 and CSI1000. In contrast, Expert 39 delivers relatively high excess return on CSI500, but only mediocre results on CSI300 and CSI1000. This highlights the specialization of MIGAs experts for different types of stocks. Moreover, similar specialization 9 was observed for the rise and fall of stocks. For example, also in MIGA-Conv (Figure 6), Expert 37 generates significantly higher excess returns for long positions compared to short positions, whereas Expert 4 excels in short positions, outperforming long positions."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Stock market prediction Recent advancements in stock market prediction have been propelled by end-to-end methods such as Temporal Convolutional Network (TCN) (Hewage et al., 2020), LSTM (Greff et al., 2016) and Transformer Vaswani et al. (2017). Although these end-to-end models were not originally designed for stock market forecasting, subsequent research (Ding et al., 2020; Nelson et al., 2017; Sun et al., 2023) has validated their effectiveness in this domain, which motivated us to propose MIGA and investigate its model structure. Mixture of Experts The mixture-of-experts (MoE) approach has been widely adopted and extensively researched since its introduction over two decades ago (Jacobs et al., 1991). Many MoE models have achieved remarkable success in various challenging fields, including computer vision (Riquelme et al., 2021; Wang et al., 2021) and natural language processing (Shazeer et al., 2017b; Du et al., 2022). The effectiveness of this approach is further demonstrated by the success of MoE-based large language models, such as DeepSeekMoe (Dai et al., 2024) and Mixtral (Jiang et al., 2024). However, despite the broad applicability of MoE methods, notable gap exists in the development of MoE frameworks specifically tailored to quantitative investment in stochastic financial markets. which inspire us to further explore it and propose MIGA."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper presents MIGA, series of Mixture of Experts with Group Aggregation models that combines different expert for stock market prediction. Our approach demonstrates the potential of integrating Mixture of Experts framework in quantitative investment, enabling end-to-end models to tackle stochastic stock market. MIGA achieves state-of-the-art performance on 3 Chinese Stock Index benchmarks, substantially outperforming existing end-to-end forecasting approaches. Furthermore, we conduct systematic analysis of the benefits aspire for this work to provide valuable insights for future research, contributing to the development of more advanced real-world market solution."
        },
        {
            "title": "REFERENCES",
            "content": "Ping Chen. Empirical and theoretical evidence of economic chaos. System Dynamics Review, 4(1-2): 81108, 1988. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Qianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Jian Guo. Hierarchical multi-scale gaussian transformer for stock movement prediction. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 46404646, 2020. Luo Donghao and Wang Xue. ModernTCN: modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=vpJMJerXHU. Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 55475569. PMLR, 2022. Yitong Duan, Lei Wang, Qizhong Zhang, and Jian Li. Factorvae: probabilistic dynamic factor model based on variational autoencoder for predicting cross-sectional stock returns. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 44684476, 2022. Eugene Fama. Efficient capital markets. Journal of finance, 25(2):383417, 1970. Klaus Greff, Rupesh Srivastava, Jan Koutník, Bas Steunebrink, and Jürgen Schmidhuber. Lstm: search space odyssey. IEEE transactions on neural networks and learning systems, 28(10): 22222232, 2016. Sanford Grossman and Joseph Stiglitz. On the impossibility of informationally efficient markets. The American economic review, 70(3):393408, 1980. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani, Francesco Palmieri, and Yonghuai Liu. Temporal convolutional neural (tcn) network for an effective weather forecasting using time-series data from the local weather station. Soft Computing, 24:1645316482, 2020. Hochreiter. Long short-term memory. Neural Computation MIT-Press, 1997. Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3(1):7987, 1991. doi: 10.1162/neco.1991.3.1.79. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, and Sen Huang. Master: Marketguided stock transformer for stock price forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 162170, 2024. Burton Malkiel. Passive investment strategies and efficient markets. European financial management, 9(1):110, 2003. David MQ Nelson, Adriano CM Pereira, and Renato De Oliveira. Stock markets price movement prediction with lstm neural networks. In 2017 International joint conference on neural networks (IJCNN), pp. 14191426. Ieee, 2017. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017a. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017b. Robert Shiller. From efficient markets theory to behavioral finance. Journal of economic perspectives, 17(1):83104, 2003. Shuo Sun, Xinrun Wang, Wanqi Xue, Xiaoxuan Lou, and Bo An. Mastering stock markets with efficient mixture of diversified trading experts. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 21092119, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella Yu. Long-tailed recognition by routing diverse distribution-aware experts. In International Conference on Learning Representations, 2021. Wentao Xu, Weiqing Liu, Lewen Wang, Yingce Xia, Jiang Bian, Jian Yin, and Tie-Yan Liu. Hist: graph-based framework for stock trend forecasting via mining concept-oriented shared information. arXiv preprint arXiv:2110.13716, 2021. Xiao Yang, Weiqing Liu, Dong Zhou, Jiang Bian, and Tie-Yan Liu. Qlib: An ai-oriented quantitative investment platform. arXiv preprint arXiv:2009.11189, 2020. Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022."
        },
        {
            "title": "A EXPERT VISUALIZATION",
            "content": "Figure 5: Stock specialization (top) and portfolio specialization (bottom) of MIGA-Conv. 13 Figure 6: Stock specialization (top) and portfolio specialization (bottom) of MIGA-Rec."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "XM Capital"
    ]
}