{
    "paper_title": "ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer",
    "authors": [
        "Zhen Han",
        "Zeyinzi Jiang",
        "Yulin Pan",
        "Jingfeng Zhang",
        "Chaojie Mao",
        "Chenwei Xie",
        "Yu Liu",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as a powerful generative technology and have been found to be applicable in various scenarios. Most existing foundational diffusion models are primarily designed for text-guided visual generation and do not support multi-modal conditions, which are essential for many visual editing tasks. This limitation prevents these foundational diffusion models from serving as a unified model in the field of visual generation, like GPT-4 in the natural language processing field. In this work, we propose ACE, an All-round Creator and Editor, which achieves comparable performance compared to those expert models in a wide range of visual generation tasks. To achieve this goal, we first introduce a unified condition format termed Long-context Condition Unit (LCU), and propose a novel Transformer-based diffusion model that uses LCU as input, aiming for joint training across various generation and editing tasks. Furthermore, we propose an efficient data collection approach to address the issue of the absence of available training data. It involves acquiring pairwise images with synthesis-based or clustering-based pipelines and supplying these pairs with accurate textual instructions by leveraging a fine-tuned multi-modal large language model. To comprehensively evaluate the performance of our model, we establish a benchmark of manually annotated pairs data across a variety of visual generation tasks. The extensive experimental results demonstrate the superiority of our model in visual generation fields. Thanks to the all-in-one capabilities of our model, we can easily build a multi-modal chat system that responds to any interactive request for image creation using a single model to serve as the backend, avoiding the cumbersome pipeline typically employed in visual agents. Code and models will be available on the project page: https://ali-vilab.github.io/ace-page/."
        },
        {
            "title": "Start",
            "content": ": ALL-ROUND CREATOR AND EDITOR FOLLOW-"
        },
        {
            "title": "ING INSTRUCTIONS VIA DIFFUSION TRANSFORMER",
            "content": "Zhen Han Zeyinzi Jiang Yulin Pan Chenwei Xie Yu Liu Jingfeng Zhang Chaojie Mao Jingren Zhou"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models have emerged as powerful generative technology and have been found to be applicable in various scenarios. Most existing foundational diffusion models are primarily designed for text-guided visual generation and do not support multi-modal conditions, which are essential for many visual editing tasks. This limitation prevents these foundational diffusion models from serving as unified model in the field of visual generation, like GPT-4 in the natural language processing field. In this work, we propose ACE, an All-round Creator and Editor, which achieves comparable performance compared to those expert models in wide range of visual generation tasks. To achieve this goal, we first introduce unified condition format termed Long-context Condition Unit (LCU), and propose novel Transformer-based diffusion model that uses LCU as input, aiming for joint training across various generation and editing tasks. Furthermore, we propose an efficient data collection approach to address the issue of the absence of available training data. It involves acquiring pairwise images with synthesis-based or clustering-based pipelines and supplying these pairs with accurate textual instructions by leveraging fine-tuned multi-modal large language model. To comprehensively evaluate the performance of our model, we establish benchmark of manually annotated pairs data across variety of visual generation tasks. The extensive experimental results demonstrate the superiority of our model in visual generation fields. Thanks to the all-in-one capabilities of our model, we can easily build multi-modal chat system that responds to any interactive request for image creation using single model to serve as the backend, avoiding the cumbersome pipeline typically employed in visual agents. Code and models will be available on the project page: https://ali-vilab.github.io/ace-page/. 4 2 0 2 5 ] . [ 2 6 8 0 0 0 . 0 1 4 2 : r Equal Contribution. Order is determined by random dice rolling. Project leader and corresponding author."
        },
        {
            "title": "Table of Contents for ACE",
            "content": "1 Introduction 2 All-Round Creator and Editor"
        },
        {
            "title": "2.1 Problem Definition .\n2.1.1 Tasks .\n.\n2.1.2",
            "content": ". . . Input Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.2 Architecture .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Datasets"
        },
        {
            "title": "3.1 Pair Data Collection .",
            "content": "3.2 Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiments"
        },
        {
            "title": "4.1 Benchmarks and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "4.2 Qualitative Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Quantitative Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion Related Work Datasets Detail B.1 Text-guided Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Low-level Visual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Controllable Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Semantic Editing . . . . B.4.1 Facial Editing . B.4.2 Style Editing . . B.4.3 General Editing . . B.5 Element Editing . . . B.5.1 Text Editing . . B.5.2 Object Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Repainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6.1 Unconditional Inpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6.2 Text-guided Inpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6.3 Outpainting . . . . . . B.7 Layer Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.8 Reference Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.8.1 Multi-reference Generation . . . . . . . . . . . . . . . . . . . . . . . . . . B.8.2 Reference-guided Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . B.9 Multi-turn and Long-context Generation . . . . . . . . . . . . . . . . . . . . . . . Benchmark Details Implementation Details More Experiments Application F.1 Workflow Distillation . F.2 Chat Bot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Visualization Discussion 2 4 5 5 6 6 7 7 9 9 11 11 12 19 19 19 21 21 22 23 23 24 25 27 27 27 28 28 29 30 30 30 31 32 35 35 35"
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Multi-turn image editing results of ACE. ACE supports wide range of image generation and editing tasks through natural language instructions, allowing complex and precise editing requests to be easily accomplished through multi-turn interactions."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, foundational generative models have made groundbreaking progress in natural language processing (NLP) (Anil et al., 2023; Anthropic, 2023a;b; Ouyang et al., 2022). Conversational language models like ChatGPT (Brown et al., 2020; OpenAI, 2023b) offer unified framework for addressing various NLP tasks through prompt-guided approach. By employing unified inputoutput structure, these models can achieve dynamic multi-turn interactions with users. Furthermore, by harnessing the knowledge of historical dialogues (Anthropic, 2024; OpenAI, 2024), they possess the capacity to comprehend intricate queries with greater nuance and depth. However, such unified architecture has not been fully explored in visual generation field. Existing foundational models of visual generation typically create images or videos from pure text, which is not compatible with most visual generation tasks, such as controllable image generation (Zhang et al., 2023b; Jiang et al., 2024) or image editing (Brooks et al., 2023). Thereby, specific visual generation tasks still require tailored tuning based on these foundational models, which is inflexible and inefficient. For this reason, the visual generative model has not yet become powerful and unified productivity tool in various application scenarios like large language models (LLMs) (Abdin et al., 2024; Dubey et al., 2024; Bai et al., 2023; Yang et al., 2024)."
        },
        {
            "title": "Technical Report",
            "content": "One major challenge of building an all-in-one visual generation model lies in the diversity of multimodal input formats and the variety of supported generation tasks. To address this, we design unified framework using Diffusion Transformer generation model that accommodates wide range of inputs and tasks, empowering it to serve as an All-round Creator and Editor, which we refer to as ACE. First, we analyze the condition inputs of most visual generation tasks, and define Condition Unit (CU), which establishes unified input paradigm consisting of core elements such as image, mask, and textual instruction. Second, for those CUs containing multiple images, we introduce Image Indicator Embedding to ensure the order of the images mentioned in instruction matches image sequence within the CUs. Besides, we imply 3d position embedding instead of 2d spatial-level position embedding on the image sequence, allowing for better exploring the relationships among conditional images. Third, we concatenate the current CU with historical information from previous generation rounds to construct the Long-context Condition Unit (LCU). By leveraging this chain of generation information, we expect the model to better understand the users request and create the desired image. As depicted in Fig. 1, ACE supports range of generating and editing capabilities, allowing it to accomplish complex and precise generation tasks through multi-turn instructions. To address the issue of the absence of available training data for various visual generation tasks, we establish meticulous data collection and processing workflow to collect high-quality structured CU data at scale of 0.7 billion. For visual conditions, we collect image pairs by synthesizing images from source images or by pairing images from large-scale databases. The former utilizes powerful open-source models to edit images to meet specific requirements, such as changing styles (Han et al., 2024) or adding objects (Pan et al., 2024), while the latter involves clustering and grouping images from extensive databases to provide sufficient real data, thereby minimizing the risk of overfitting to the synthesized data distribution. For textual instructions, we first manually construct instructions for diverse tasks by building templates or requesting LLMs, then optimize the instruction construction process by training an end-to-end instruction-labeling multi-modal large language model (MLLM) (Chen et al., 2024), thereby enriching the diversity of the text instructions. Our ACE provides more comprehensive coverage of tasks on single model compared to previous approaches. Therefore, to thoroughly evaluate the performance of our generation model, we construct an evaluation benchmark that encompasses the main tasks. This benchmark incorporates inputs sourced from both the real world and model-generated data, supporting global and local editing tasks. It is larger in scale and broader in scope compared to previous benchmarks (Sheynin et al., 2024; Zhang et al., 2023a). We conduct user study to subjectively assess the quality of images generated by our method and the adherence to instructions, revealing that our approach generally aligns more closely with human perception across the majority of tasks. We summarize our main contributions as follows: We propose ACE, unified foundational model framework that supports wide range of visual generation tasks. To our knowledge, this is the most comprehensive diffusion generation model to date in terms of task coverage. By defining the CU for unifying multi-modal inputs across different tasks and incorporating longcontext CU, we introduce historical contextual information into visual generation tasks, paving the way for ChatGPT-like dialog systems in visual generation. We design specific data construction pipelines for various tasks to enhance the quality and efficiency of data collection, and we ensure the richness of multi-modal data through MLLM finetuning for automated instruction labeling. We establish more comprehensive evaluation benchmark compared to previous ones, covering the most known visual generation tasks. Evaluation results indicate that ACE demonstrates notable competitiveness in specialized models while also exhibiting strong generalization capabilities across broader range of open tasks."
        },
        {
            "title": "2 ALL-ROUND CREATOR AND EDITOR",
            "content": "ACE is an image creation and editing model based on the Diffusion Transformer that follows textual instructions. It establishes unified framework that covers wide range of tasks through the definition of standard input paradigm and strategy for aligning multi-modal information. With this"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: The overview of all generation and editing task types supported by ACE. These tasks are categorized into 8 basic types, multi-turn and long-context generation based on different input conditions (in green) and are formulated using the proposed input paradigm as 3 formats (in blue). exquisite design, the model is capable of handling various single tasks, multi-turn tasks, and longcontext tasks with historical information. 2.1 PROBLEM DEFINITION 2.1.1 TASKS When it comes to generation and editing, the input condition information varies significantly depending on the specific task types. This encompasses diverse range of forms, including textual instructions, conditioning images in controllable generation, masks used in region editing, and images in guided generation, among others. We analyze and categorize these conditions from textual and visual modalities respectively: (i) Textual modality: we refer to all types of textual conditions as instructions and categorize them into Generating-based Instructions and Editing-based Instructions, depending on whether they describe the content of the generated image directly or the difference from the input visual cues; (ii) Visual modality: we categorize all generation tasks into 8 basic types, as shown in Fig. 2. Text-guided Generation. It only uses generating-based text prompt as condition to create images, and none of the visual cues are adopted. Low-level Visual Analysis. It extracts low-level visual features from input images, such as edge maps or segmentation maps. One source image and editing-based instruction are required in the task to accomplish creation. Controllable Generation. It is the inverse task of Low-level Visual Analysis, which creates vivid images based on given conditions, e.g., edge map, contour image, doodle image, scribble image, depth map, segmentation map, low-resolution image, etc. Semantic Editing. It aims to modify some semantic attributes of an input image by providing editing instructions, such as altering the style of an image or modifying the facial attributes of character. Element Editing. It focuses on adding, deleting, or replacing specific subject in the image while keeping other elements unchanged. Repainting. It erases and repaints partial image content of input image indicated by given mask and instruction. Layer Editing. It decomposes an input image into different layers, each of which contains subject or background, or reversely fuses different layers. Reference Generation. It generates an image based on one or more reference images, analyzing the common elements among them and presenting these elements in the generated image."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: The illustration of ACE framework. Condition Tokenizing module tokenizes each input CU, concatenating them to obtain the visual token sequence and the text token sequence. The Image Indicator Embedding module employs pre-defined textual tokens to indicate the image order in textual instructions and distinguish various input images. The Long-context Attention Block ensures effective communication and integration of long-context sequences. By leveraging the generation tasks of these fundamental units, we can combine them to create multiturn scenarios. Furthermore, utilizing the historical information from every round makes it possible to tackle long-context visual generation tasks. 2.1.2 INPUT PARADIGM significant obstacle to implementing different types of generation and editing task requests within one framework lies in the diverse input condition formats of tasks. To address this issue, we design unified input paradigm defined as Conditional Unit (CU) that fits as many tasks as possible. The CUs composed of textual instruction that describes the generation requirements, along with (if there visual information , where consists of set of images that can be defined as = 1, 2, . . . , are no source image) or = (if there are source images) and corresponding masks { . When there is no specific mask, is set to blank image. The overall = } formulation of the CU is as follows: 1, 2, . . . , { } CU = T, { } , = [I 1; 1], [I 2; 2], . . . , [I ; ] , } { where channel-wise connection operation is performed between corresponding and , represents the total number of visual information inputs for this task. Furthermore, to better address the demands of complex long-context generation and editing, historical information can be optionally integrated into CU, which is formulated as: LCUi = Ti m, Ti , m+1, . . . , Ti} Vi { m, Vi m+1, . . . , Vi}} {{ where denotes the maximum number of rounds of historical knowledge introduced in the current request. LCUi is Long-context Condition Unit used to generate desired content for the i-th request. (1) (2) 2.2 ARCHITECTURE In this section, we introduce unified visual generation framework that can perform all visual generation tasks within single model, and incorporate long-context conditions to enhance comprehension. As illustrated in Fig. 3a, the overall framework is built based on Diffusion Transformer model (Vaswani et al., 2017; Peebles & Xie, 2023), and integrated with three novel components to achieve unified generation: Condition Tokenizing, Image Indicator Embedding, and Long-context Attention Block. We will provide detailed description of them below."
        },
        {
            "title": "Technical Report",
            "content": "Condition Tokenizing. Considering an LCU that comprises CUs, the model involves three entry points for each CU: language model (T5) (Raffel et al., 2020) to encode textual instructions, Variational Autoencoder (VAE) (Kingma & Welling, 2014) to compress reference image to latent representation, and down-sampling module to resize mask to the shape of corresponding latent image. The latent image and its mask (an all-one mask if no mask is provided) are concatenated along the channel dimension. These image-mask pairs are then patchified into 1-dimensional visual token sequences um,n,p, where m, are indexes for CUs and visual information Vs in each CU, while denotes the spatial index in patchified latent images. Similarly, textual instructions are encoded into 1-dimensional token sequences ym. After processing within each CU, we separately concatenate all visual token sequences and all textual token sequences to form long-context sequence. Image Indicator Embedding. As illustrated as Fig. 3b, to indicate the image order in textual instructions and distinguish various input images, we encode some pre-defined textual tokens into T5 embeddings as Image Indicator Embeddings (I-Emb). } { These indicator embeddings are added to the corresponding image embedding sequence and text embedding sequence, which is formulated as: imageN { image1 { , image } , ..., } ym,n = ym + I-Embm,n, (3) um,n,p = um,n,p + I-Embm,n. In this way, image indicator tokens in textual instructions and the corresponding images are implicitly associated. (4) Long-context Attention Block. Given the long-context visual sequence, we first modulate it with the time step embedding (T -Emb), then incorporate 3D Rotational Positional Encodings (RoPE) (Su et al., 2023) to differentiate between different spatialand frame-level image embeddings. During the Long Context Self-Attention, all image embeddings of each CU at each spatial location, are equivalently and comprehensively interact with each other by µ = Attn(u, u). Next, unlike the cross-attention layer of the conventional Diffusion Transformer model, where each visual token attends to all of the textual tokens, we implement cross-attention operation with each condition unit. That means image tokens in m-th CU will only attend to the textual tokens from the same CU. This can be formulated as: This ensures that, within the cross-attention layer, the text embeddings and image embeddings align on frame-by-frame basis. ˆum,n = Attn(µm,n, ym,n). (5)"
        },
        {
            "title": "3 DATASETS",
            "content": "3.1 PAIR DATA COLLECTION critical challenge of training foundational visual generation model lies in how to acquire pairwise images for various tasks. In this section, we introduce two ways to efficiently build highquality datasets for most of the generation and editing tasks: (i) Synthesizing from source image: thanks to the rapid development in the field of visual generation, there have been many of powerful open-source models designed to solve one specific problem. Leveraging these powerful single-point technologies, we could synthesis plenty of image pairs for lots of generation and editing tasks, such as controllable generation, style editing, object editing, and so on. (ii) Pairing from massive databases: though the synthesis-based method is efficient and straightforward in acquiring pairwise data. However, It still possesses two drawbacks. First, some editing problems have not been fully explored, and there are no powerful open-source models available for these tasks. Second, using only synthetic data can easily cause over-fitting and reduce the quality of generated images. Therefore, it is essential to provide sufficient real data to address the aforementioned drawbacks. We propose hierarchically aggregating pipeline for pairing content-related images from massive databases to build pairs of data for training, as illustrated in Fig. 4. We first extract semantic features using SigLIP (Zhai et al., 2023) from large-scale datasets (e.g., LAION-5B (Schuhmann et al., 2022), OpenImages (OpenImage, 2023), and our private datasets). Then leveraging K-means clustering technology, coarse-grained clustering is implemented to divide all images into tens of thousands of clusters. Within each cluster, we implement two-turn union-find algorithm to achieve fine-grained"
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: The pipeline of dataset construction and instructions labeling. In data construction, two methods are utilized: synthesizing using open-source expert models and mining from largescale data. For instruction labeling, we combined templating with MLLM labeling, further training the Instruction Captioner to achieve large-scale instruction labeling. image aggregation. The first turn is based on the SigLIP feature and the second turn uses similarity score tailored for specific tasks. For instance, we calculate the face similarity score for the facial editing task and the object consistency score for the general editing task. Finally, we collect all possible pairs from each disjoint set and implement cleaning strategies to filter high-quality pairs. Benefiting from these two automatic pipelines, we construct large-scale training dataset that consists of nearly 0.7 billion image pairs, covering 8 basic types of tasks, multi-turn and long-context generation. We depict its distribution in Fig. 6 and provide detailed description of the specific data construction methods for each task, please refer to appendix B. 3.2 INSTRUCTIONS In addition to collecting image pairs, it is essential to label clear natural language instructions that indicate how to transform one image into another. Compared to the caption generation commonly used in text-to-image task, instruction labeling is generally more challenging, as it requires analyzing not only the semantics of individual images, but also the discrepancies across multiple images. We employ both Template-based and MLLM-based methods to tackle this challenge. Template-based method constructs instruction templates for specific vision tasks by leveraging human knowledge priors. However, the instructions generated by this method lack diversity, which can lead to significant overfitting problems. MLLM-based method generates unique instructions for each given editing pair, leveraging off-the-shelf MLLMs. Nonetheless, current MLLMs exhibit limitations in producing precise instructions for editing tasks involving non-natural images, such as depth-controlled image generation and image segmentation. Thus, we combine these two methods and design an effective strategy to mitigate the aforementioned drawbacks. For tasks that contain non-natural images, we utilize template-based method to generate instruction templates. These templates are then combined with the generated captions to produce the final instructions. To address the issue of insufficient diversity, we employ LLMs to reformulate instructions multiple times, and tune prompts to ensure that each rewritten version is distinct from all preceding instructions. For tasks that contain natural images, we employ an MLLM to predict the differences and commonalities between the images in the input pair. Then an LLM is used to generate instructions focusing on semantic distinctions according to the analysis of the differences and commonalities. Further, the collected instructions generated by these two methods undergo human annotation and correction. The revised instructions are used for fine-tuning an open-source MLLM, enabling it to predict instructions for any given image pair. Specifically, we collect dataset of approximately 800,000 curated instructions and train an Instruction Captioner by fine-tuning the InternVL2-26B (Chen et al., 2024)."
        },
        {
            "title": "Technical Report",
            "content": "Table 1: Results on the MagicBrush benchmark. LC denotes long-context generation with history. Settings Methods L1 CLIP-I DINO CLIP-T Global Description-guided t - n r - u SD-SDEdit (Meng et al., 2021) Null Text Inversion (Mokady et al., 2022) GLIDE (Nichol et al., 2022) Blended Diffusion (Avrahami et al., 2022) ACE (Ours) 0.1014 0.0749 3.4973 3.5631 0. 0.0278 0.0197 115.8347 119.2813 0.0160 HIVE (Zhang et al., 2024) InstructPix2Pix (Brooks et al., 2023) MagicBrush (Zhang et al., 2023a) UltraEdit (Zhao et al., 2024) ACE (Ours) SD-SDEdit (Meng et al., 2021) Null Text Inversion (Mokady et al., 2022) GLIDE (Nichol et al., 2022) Blended Diffusion (Avrahami et al., 2022) ACE (Ours) ACE (Ours w/ LC) HIVE (Zhang et al., 2024) InstructPix2Pix (Brooks et al., 2023) MagicBrush (Zhang et al., 2023a) UltraEdit (Zhao et al., 2024) ACE (Ours) ACE (Ours w/ LC) Instruction-guided 0.1092 0.1122 0.0625 0.0575 0. 0.0380 0.0371 0.0203 0.0172 0.0165 Global Description-guided 0.1616 0.1057 11.7487 14.5439 0.0778 0.0768 0.0602 0.0335 1079.5997 1510.2271 0.0290 0.0285 Instruction-guided 0.1521 0.1584 0.0964 0.0745 0.0773 0. 0.0557 0.0598 0.0353 0.0236 0.0293 0.0284 0.8526 0.8827 0.9487 0.9291 0.9436 0.8519 0.8524 0.9332 0.9307 0.9453 0.7933 0.8468 0.9094 0.8782 0.9124 0.9136 0.8004 0.7924 0.8924 0.9045 0.9128 0.9140 0.7726 0.8206 0.9206 0.8644 0. 0.7500 0.7428 0.8987 0.8982 0.9215 0.6212 0.7529 0.8494 0.7690 0.8611 0.8635 0.6463 0.6177 0.8273 0.8505 0.8661 0.8668 0.2777 0.2737 0.2249 0.2622 0.2833 - 0.2764 0.2781 - 0.2841 0.2694 0.2710 0.2252 0.2619 0.2843 0. 0.2673 0.2726 0.2754 - 0.2855 0.2809 Once trained, the Instruction Captioner is able to take any two images as input and generates the instruction for transforming the source image to the target image. It can also be further extended to the processing of cluster data, by entering set of images, obtaining the similarity description among images within the cluster, and the differences between each pair within the cluster. The above process is illustrated in Fig. 4."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 BENCHMARKS AND METRICS Existing Benchmarks. We first evaluate on the commonly used benchmark MagicBrush (Zhang et al., 2023a). It contains an overall 1,053 edit turns and 535 edit sessions for single-turn and multiturn image editing respectively. It compares the output images with groundtruth images and the provided target text descriptions. Following the setting proposed in the MagicBrush benchmark, we calculate the L1 distance, L2 distance, CLIP (Radford et al., 2021) similarity, DINO (Liu et al., 2023a) similarity between the generated image and groundtruth image, and CLIP similarity between the generated image and textual prompt. We also evaluate the Emu Edit benchmark (Sheynin et al., 2024), please see appendix for details. ACE Benchmark. To thoroughly evaluate the performance of various visual generation tasks, we build benchmark dataset that covers all types of tasks the aforementioned. ACE benchmark consists of both real and generated images. The real images are primarily sourced from the MSCOCO (Lin et al., 2014) dataset and the generated images are created by Midjourney (Midjourney, 2023), using prompts obtained from JourneyDB (Sun et al., 2023a). For each task type, we manually craft instructions and masks to closely resemble actual user input patterns, reaching total of 12,000 entries. The detailed statistics of ACE benchmark can be found in Fig. 24. We evaluate image quality and prompt following scores through user study. The image quality score assesses the aesthetic quality of the generated images, while the prompt following score measures how well the images align with the provided textual instructions."
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Comparison and visualization of ACE performance with expert models in different tasks. ACE demonstrates adaptability to multi-task and achieves superior performance."
        },
        {
            "title": "Technical Report",
            "content": "Table 2: User study results on ACE benchmark. For each method in every supported task, we evaluate both prompt following and image quality, reporting the two scores in single cell, separated by /. - means this task does not exist or is not supported by the current method. Txt2img 2 Controllable Semantic Element Repainting n t e i e e e S r G T t m . d . m i I i u Global Editing SD1.5 (AI, 2022a) SDXL (StabilityAI, 2022) CtrlNet (Zhang et al., 2023b) StyleBooth (Han et al., 2024) IP-Adapter (Ye et al., 2023) InstantID (Wang et al., 2024b) FaceChain (Liu et al., 2023b) SDEdit (Meng et al., 2021) IP2P (Brooks et al., 2023) MB (Zhang et al., 2023a) SEED-X (Ge et al., 2024b) CosXL (StabilityAI, 2024) UltraEdit (Zhao et al., 2024) ACE (Ours) - - - - - - - - - - - - - - - - - - - - 2.5/2.0 3.8/2.4 1.9/2.0 2.9/1.9 - - - - 1.7/2.5 - - - - - - 2.0/2.2 2.5/2.7 2.0/3.0 3.3/2.2 4.1/2.8 - - - - - - - - - - - - - - 3.3/2.6 - - - 1.4/1.9 1.3/1.8 1.1/1.6 1.2/1.4 1.3/2.1 1.1/1.7 1.5/2.1 1.1/2.2 1.1/1.7 1.5/2.1 1.1/2.0 1.9/2.0 1.7/2.0 1.5/2.3 1.4/1.4 2.3/2.4 2.4/2.5 2.2/2.4 1.1/2.6 1.3/2.6 2.0/2.4 1.5/2.4 1.3/1.8 1.3/1.7 1.3/1.9 1.1/1.3 2.4/2.3 1.4/2.0 2.2/2.3 1.5/2.4 2.2/2.5 3.1/2.2 2.1/2.4 1.6/2.1 1.7/2.0 1.7/2.2 1.5/1.5 2.0/2.7 2.2/2.5 2.1/2.7 1.3/2.6 2.1/2.6 1.9/2.6 2.5/2.4 4.1/2.9 4.1/2.8 2.6/2.9 3.7/2.1 2.9/3.1 3.2/3.0 3.2/2.9 1.4/2.7 1.0/2.9 2.8/2.5 1.1/3.1 1.7/2.2 1.2/1.8 1.3/2.3 1.1/1.3 2.3/2.5 2.1/2.4 2.6/2.5 1.7/2.6 1.1/2.7 2.7/2.3 1.5/2.6 3.7/2.5 4.6/2.7 4.5/2.8 4.8/2.9 4.1/2.3 2.8/2.8 2.4/2.6 2.1/2.5 2.8/2.7 4.4/2.9 2.6/2.4 3.9/2.5 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Local Editing LaMa (Suvorov et al., 2022) SDInpaint (AI, 2022b) CtrlNet (Zhang et al., 2023b) AnyText (Tuo et al., 2023) UDiffText (Zhao & Lian, 2024) UltraEdit (Zhao et al., 2024) ACE (Ours) - - - - - - - - - - - - - - - - - 1.4/1.9 1.2/1.8 1.2/2.0 4.8/2.6 4.3/2.5 4.8/2.6 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 4.5/2.8 1.6/2.3 3.0/2.4 3.6/2.8 2.6/2.6 1.6/2.7 2.2/2.5 3.6/2.6 2.9/2.7 1.9/2.5 2.6/2.2 3.0/2.1 3.2/2.1 - - - - - 3.5/2.7 3.6/2.7 1.1/2.8 1.2/2.9 2.9/2.5 1.4/2.5 1.1/1.7 1.1/2.1 4.5/2.9 4.5/2.9 3.7/2.5 4.3/2.5 4.4/2.7 4.6/2. - - - - - - - - - 4.2 QUALITATIVE EVALUATION In our qualitative evaluation, we present comparison of our method with SOTA approaches across various tasks, including ControlNet (Zhang et al., 2023b), InstructPix2Pix (Brooks et al., 2023), MagicBrush (Zhang et al., 2023a), CosXL (StabilityAI, 2024), SEED-X Edit (Ge et al., 2024a), UltraEdit (Zhao et al., 2024), StyleBooth (Han et al., 2024), SDEdit (Meng et al., 2021), LoRA (Hu et al., 2022), SD-Inpaint (AI, 2022b), LaMa (Suvorov et al., 2022), IP-Adapter (Ye et al., 2023), InstantID (Wang et al., 2024b), FaceChain (Liu et al., 2023b), AnyText (Tuo et al., 2023), UDiffText (Zhao & Lian, 2024). In Fig. 5, we present qualitative comparisons between our single ACE model and 16 other methods across 12 subtasks. Overall, our method not only addresses diverse range of tasks but also performs superior compared to task-specific methods. Additionally, we also show some extra tasks that the comparison methods do not perform well in the last three lines. Please see appendix G, for more examples of qualitative evaluation. 4.3 QUANTITATIVE EVALUATION Evaluation on Existing Benchmarks. We first compare our method with baselines on the MagicBrush benchmark. Results are present on Tab. 1. For single-turn image editing, ACE significantly outperforms other methods under an instruction-guided setting while demonstrating comparable performance under description-guided setting. For each setting of multi-turn image editing, we first employ the same inference way as MagicBrush, performing independent and continuous edits on single image. The results show that our approach has significant advantages. Furthermore, we construct long sequence using the historical information from each editing round, achieving certain improvement in performance compared to not using it. This also demonstrates the effectiveness of LCU and architecture design. Evaluation on ACE Benchmark. We conduct comprehensive human evaluation using our benchmark to assess the performance of generated images, employing image scoring as the evaluation metric. Specifically, we score each image considering two aspects: prompt following and image quality. The prompt following metric measures the image compliance with text instructions or text descriptions, and is categorized into five levels. The image quality metric encompasses various as-"
        },
        {
            "title": "Technical Report",
            "content": "pects such as generated color, details, layout, and visual appeal, and is scored on scale from 1 to 5. Considering the broad capabilities of our method, we compare it with several common approaches and some experts designed for specific tasks. We engaged 5 professional designers as evaluators to carry out these assessments. For each task, the data is evenly distributed among the evaluators in an anonymous manner, and scores are aggregated for analysis. As shown in Tab. 2, we compare our approach across multiple global editing tasks and local editing tasks. The prompt following score and image quality score are presented together, separated by / pattern. The bold numbers represent the best, and the underlined numbers indicate the second best. Our method achieves the highest prompt following scores in 7 of 12 global editing tasks and 8 of 10 local editing tasks, which demonstrates that ACE fully understands the intention of the instruction and is able to correctly generate an image that meets the instruction. Furthermore, ACE achieves the best image quality scores in 5 of 10 global editing tasks and 7 of 10 local editing tasks. These results indicate that ACE excels at generating high aesthetic images across various image editing tasks. Nonetheless, our method performs unsatisfactorily in certain tasks, such as general editing and style editing. One possible reason is that images generated by methods using larger models, such as those producing 1024-resolution images based on the SDXL model, are more preferred by evaluators compared to those produced by our model, which has size of 0.6B parameters and an output resolution of around 512."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose ACE, versatile foundational generative model that excels at creating images, and following instructions across wide range of generative tasks. Users can specify their generation intentions through customized text prompts and image inputs. Furthermore, we advance the exploration of capabilities within interactive dialogue scenarios, marking significant step forward in the processing of long contextual historical information in the field of visual generation. Our work aims to provide comprehensive generative model for the public and professional designers, serving as productivity enhancement tool to foster innovation and creativity. Acknowledgments. We sincerely appreciate the contributions of many colleagues for their insightful discussions, valuable suggestions, and constructive feedback, including: Haiming Zhao, Yuntao Hong, You Wu, Jixuan Chen, Yuwei Wang, and Sheng Yao for their data contributions, and Lianghua Huang, Kai Zhu, and Yutong Feng for their discussions, suggestions, and the sharing of resources."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, et al. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. arXiv preprint arXiv:2404.14219, 2024. Runway AI. Stable Diffusion v1.5 Model Card, https://huggingface.co/runwayml/ stable-diffusion-v1-5, 2022a. Runway AI. Stable Diffusion Inpainting Model Card, https://huggingface.co/ runwayml/stable-diffusion-inpainting, 2022b. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, et al. PaLM 2 Technical Report. arXiv preprint arXiv:2305.10403, 2023. Anthropic."
        },
        {
            "title": "Introducing",
            "content": "Claude, https://www.anthropic.com/index/ introducing-claude., 2023a. Anthropic. Claude 2. Technical report, https://www-files.anthropic.com/ production/images/Model-Card-Claude-2.pdf, 2023b. Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku, https://www-cdn. anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf, 2024. Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended Diffusion for Text-driven Editing of Natural Images. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1820818218, 2022. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, et al. Qwen Technical Report. arXiv preprint arXiv:2309.16609, 2023. Rumeysa Bodur, Erhan Gundogdu, Binod Bhattarai, Tae-Kyun Kim, Michael Donoser, and Loris In IEEE Conf. iEdit: Localised Text-guided Image Editing with Weak Supervision. Bazzani. Comput. Vis. Pattern Recog., pp. 74267435, 2024. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning To Follow Image Editing Instructions. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1839218402, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, et al. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020. John Canny. Computational Approach to Edge Detection. IEEE Trans. Pattern Anal. Mach. Intell., pp. 679698, 1986. Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields. IEEE Trans. Pattern Anal. Mach. Intell., 43(1):172186, 2021. ISSN 0162-8828, 2160-9292, 1939-3539. Caroline Chan, Fredo Durand, and Phillip Isola. Learning To Generate Line Drawings That Convey Geometry and Semantics. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 79157925, 2022. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. arXiv preprint arXiv:2310.00426, 2023a. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. AnyDoor: Zero-shot Object-level Image Customization. arXiv preprint arXiv:2307.09481, 2023b."
        },
        {
            "title": "Technical Report",
            "content": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 2418524198, 2024. Alibaba Cloud. Tongyi Wanxiang, https://tongyi.aliyun.com/wanxiang, 2023. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 46904699, 2019a. Jiankang Deng, Jia Guo, Debing Zhang, Yafeng Deng, Xiangju Lu, and Song Shi. Lightweight Face Recognition Challenge. In Int. Conf. Comput. Vis., pp. 00, 2019b. Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, and Haoshuang Wang. PP-OCR: Practical Ultra Lightweight OCR System. arXiv preprint arXiv:2009.09941, 2020. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. In Int. Conf. Mach. Learn., 2024. FLUX. FLUX, https://blackforestlabs.ai/, 2024. Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. SEED-Data-Edit Technical Report: Hybrid Dataset for Instructional Image Editing. arXiv preprint arXiv:2405.04007, 2024a. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation. arXiv preprint arXiv:2404.14396, 2024b. Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, Dong Chen, and Baining Guo. InstructDiffusion: Generalist In IEEE Conf. Comput. Vis. Pattern Recog., pp. 12709 Modeling Interface for Vision Tasks. 12720, 2024. Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, and Jingfeng Zhang. StyleBooth: Image Style Editing with Multimodal Instruction. arXiv preprint arXiv:2404.12154, 2024. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In Int. Conf. Learn. Represent., 2022. Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun Zhou, Yuhao Cheng, Shutao Liao, Long Chen, Yiqiang Yan, Shengcai Liao, and Xiaodan Liang. ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving. arXiv preprint arXiv:2404.16771, 2024a. Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and Controllable Image Synthesis with Composable Conditions. In Int. Conf. Mach. Learn., 2023. Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, and Ying Shan. SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 83628371, 2024b. Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose. arXiv preprint arXiv:2303.07399, 2023."
        },
        {
            "title": "Technical Report",
            "content": "Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, and Jingfeng Zhang. SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 89959004, 2024. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Int. Conf. Learn. Represent., 2014. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment Anything. In Int. Conf. Comput. Vis., pp. 40154026, 2023. KOLORS. KOLORS, https://github.com/Kwai-Kolors/Kolors, 2024. Pengzhi Li, QInxuan Huang, Yikang Ding, and Zhiheng Li. LayerDiffusion: Layered Controlled Image Editing with Diffusion Models. arXiv preprint arXiv:2305.18676, 2023. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-DiT: Powerful MultiResolution Diffusion Transformer with Fine-Grained Chinese Understanding. arXiv preprint arXiv:2405.08748, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common objects in context. In Eur. Conf. Comput. Vis., pp. 740755, 2014. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. arXiv preprint arXiv:2303.05499, 2023a. Yang Liu, Cheng Yu, Lei Shang, Yongyi He, Ziheng Wu, Xingjun Wang, Chao Xu, Haoyu Xie, Weida Wang, Yuze Zhao, Lin Zhu, Chen Cheng, Weitao Chen, Yuan Yao, Wenmeng Zhou, Jiaqi Xu, Qiang Wang, Yingda Chen, Xuansong Xie, and Baigui Sun. FaceChain: Playground for Human-centric Artificial Intelligence Generated Content. arXiv preprint arXiv:2308.14256, 2023b. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In Int. Conf. Learn. Represent., 2018. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In Int. Conf. Learn. Represent., 2021. Midjourney. Midjourney, https://www.midjourney.com, 2023. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text Inversion for Editing Real Images using Guided Diffusion Models. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 60386047, 2022. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-toImage Diffusion Models. arXiv preprint arXiv:2302.08453, 2023. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. arXiv preprint arXiv:2112.10741, 2022. OpenAI. DALLE 2, https://openai.com/dall-e-2, 2022. OpenAI. DALLE 3, https://openai.com/dall-e-3, 2023a. OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023b. OpenAI. Hello GPT-4o, https://openai.com/index/hello-gpt-4o/, 2024."
        },
        {
            "title": "Technical Report",
            "content": "OpenImage. OpenImage, https://storage.googleapis.com/openimages/web/ index.html, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, et al. Training language models to follow instructions with human feedback. In Adv. Neural Inform. Process. Syst., pp. 2773027744, 2022. Yulin Pan, Chaojie Mao, Zeyinzi Jiang, Zhen Han, and Jingfeng Zhang. Refine: Taming Customized Image Inpainting with Text-Subject Guidance. arXiv:2403.19534, 2024. Locate, Assign, arXiv preprint William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In Int. Conf. Comput. Vis., pp. 41954305, 2023. Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, and Ran Xu. UniControl: Unified Diffusion Model for Controllable Visual Generation In the Wild. arXiv preprint arXiv:2305.11147, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. arXiv preprint arXiv:2103.00020, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with Unified Text-toText Transformer. J. Mach. Learn. Res., pp. 167, 2020. Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer. IEEE Trans. Pattern Anal. Mach. Intell., pp. 16231637, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. HighIn IEEE Conf. Comput. Vis. Pattern resolution image synthesis with latent diffusion models. Recog., pp. 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 2250022510, 2023. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In Adv. Neural Inform. Process. Syst., 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W. Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R. Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Adv. Neural Inform. Process. Syst., 2022. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu Edit: Precise Image Editing via Recognition and Generation Tasks. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 88718879, 2024. Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, and Song Bai. DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 88398849, 2024. Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring Style Similarity in Diffusion Models. arXiv preprint arXiv:2404.01292, 2024."
        },
        {
            "title": "Technical Report",
            "content": "StabilityAI. Stable Diffusion XL Model Card, https://huggingface.co/stabilityai/ stable-diffusion-xl-base-1.0, 2022. StabilityAI. 2024. CosXL Model Card, https://huggingface.co/stabilityai/cosxl, Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864, 2023. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. JourneyDB: Benchmark for Generative Image Understanding. In Adv. Neural Inform. Process. Syst., 2023a. Ya Sheng Sun, Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Hu, Lili Qiu, and Hideki Koike. ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation. In Adv. Neural Inform. Process. Syst., 2023b. Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. In IEEE Winter Conf. Resolution-Robust Large Mask Inpainting With Fourier Convolutions. Appl. Comput. Vis., pp. 21492159, 2022. Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. AnyText: Multilingual Visual Text Generation and Editing. In Int. Conf. Learn. Represent., 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Adv. Neural Inform. Process. Syst., 2017. Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation. arXiv preprint arXiv:2404.02733, 2024a. Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. InstantID: Zero-shot IdentityPreserving Generation in Seconds. arXiv preprint arXiv:2401.07519, 2024b. Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-ESRGAN: Training Real-World In Int. Conf. Comput. Vis., pp. 19051914, Blind Super-Resolution with Pure Synthetic Data. 2021. Zhizhong Wang, Lei Zhao, and Wei Xing. StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models. In Int. Conf. Comput. Vis., pp. 76777689, 2023. Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. SmartBrush: Text and Shape Guided Object Inpainting With Diffusion Model. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 2242822437, 2023. Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, and Vikas Chandra. EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1611116121, 2023. Jiacong Xu, Zixiang Xiong, and Shankar P. Bhattacharyya. PIDNet: Real-time Semantic Segmentation Network Inspired by PID Controllers. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1952919539, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, et al. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671, 2024. Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, and Kai Chen. GlyphControl: Glyph Conditional Control for Visual Text Generation. In Adv. Neural Inform. Process. Syst., 2023."
        },
        {
            "title": "Technical Report",
            "content": "Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models. arXiv preprint arXiv:2308.06721, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre-Training. In Int. Conf. Comput. Vis., pp. 1197511986, 2023. Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation. arXiv preprint arXiv:2112.15283, 2021. Hua Zhang, Si Liu, Changqing Zhang, Wenqi Ren, Rui Wang, and Xiaochun Cao. SketchNet: Sketch Classification with Web Images. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1105 1113, 2016a. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. MagicBrush: Manually Annotated Dataset for Instruction-Guided Image Editing. In Adv. Neural Inform. Process. Syst., 2023a. Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks. IEEE Sign. Process. Letters, pp. 14991503, 2016b. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. In Int. Conf. Comput. Vis., pp. 38363847, 2023b. Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. HIVE: Harnessing Human Feedback for Instructional Visual Editing. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 9026 9036, 2024. Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, Yandong Guo, and Lei Zhang. Recognize Anything: Strong Image Tagging Model. arXiv preprint arXiv:2306.03514, 2023c. Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. UltraEdit: Instruction-based Fine-Grained Image Editing at Scale. arXiv preprint arXiv:2407.05282v1, 2024. Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee K. Wong. Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models. In Adv. Neural Inform. Process. Syst., 2023. Yiming Zhao and Zhouhui Lian. UDiffText: Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models. In Eur. Conf. Comput. Vis., 2024."
        },
        {
            "title": "A RELATED WORK",
            "content": "Visual generation, which takes multi-modal conditions (e.g., textual instruction and reference image) as input to generate creative image, has emerged as popular research trend in recent years. As the basic task, text-guided image generation has undergone significant development, marked by remarkable advancements in recent years. Many approaches (Nichol et al., 2022; Saharia et al., 2022; OpenAI, 2022; Rombach et al., 2022; StabilityAI, 2022; OpenAI, 2023a; Midjourney, 2023; Cloud, 2023; Zhang et al., 2021; Chen et al., 2023a; Esser et al., 2024; KOLORS, 2024; Li et al., 2024; FLUX, 2024) have been proposed and achieved impressive results in terms of both image quality and semantic fidelity. By incorporating low-level visual features as input, Huang et al. (2023) and Zhang et al. (2023b) pave the way for the initial forms of multi-modal controllable generation. Recently, some approaches (Mou et al., 2023; Zhao et al., 2023; Qin et al., 2023) have tried to use multiple visual features as conditions, facilitating the multi-modal controllable generation. By integrating fine-tuning technologies such as Ruiz et al. (2023); Hu et al. (2022), these approaches have further enabled the customization of diverse controllable generation applications. Another popular trend is image editing technology (Ye et al., 2023; Han et al., 2024; Wang et al., 2024b; Huang et al., 2024a; Wang et al., 2024a; Liu et al., 2023b; Tuo et al., 2023; Chen et al., 2023b; Pan et al., 2024; Wang et al., 2023; Xie et al., 2023; Sun et al., 2023b; Huang et al., 2024b; Bodur et al., 2024; Shi et al., 2024; Li et al., 2023; Meng et al., 2021), which focus on editing input images according to text prompts and preserving some identity such as person, scene, subject, or style. While the above models excel at generating image in one specific task or scenario, they have difficulty in extending to unseen tasks. To address the aforementioned challenges, some methods have been introduced to edit input images by following natural language instructions (Brooks et al., 2023; StabilityAI, 2024; Geng et al., 2024; Sheynin et al., 2024; Zhao et al., 2024; Ge et al., 2024b) which is more flexible to implement various tasks within single model. However, key bottleneck for these methods lies in the construction of high-quality instruction-paired datasets with annotated edits, which cause limited generalizability and suboptimal performance. In this paper, we focus on establishing unified definition for multi-modal generation problems. Based on this definition, we aim to construct higher-quality, annotated data and instruction sets further to develop unified foundational model for multimodal generation."
        },
        {
            "title": "B DATASETS DETAIL",
            "content": "We use an internal dataset of 0.7 billion data pairs to train foundational model for generation and editing. The supported tasks include 8 basic types consisting of 37 subtasks, as well as multiturn and long-context generation task. These tasks use textual instructions along with zero or more reference images for generating or editing image. The data distribution is depicted in Fig. 6a, and the absolute data scale is illustrated in Fig. 6b. In this section, we provide detailed introduction to the data construction methods for various tasks. B.1 TEXT-GUIDED GENERATION We collect approximately 117 million images and use MLLM model to supplement captions for images, creating pair data for text-to-image tasks. Additionally, this portion of the data serves as an intermediary bridge in various generation and editing tasks, allowing the combination of different task instructions to obtain pairs from original images to target images. B.2 LOW-LEVEL VISUAL ANALYSIS Low-level Visual Analysis tasks involve analyzing and extracting various low-level visual features from given image, like an edge map or segmentation map. These low-level visual features are typically employed as control signals in the controllable generation. We select 10 commonly used low-level features in the controllable generation, including segmentation map, depth map, human pose, mosaic image, blurry image, gray image, edge map, doodle image, contour image, and scribble image. The visual features extracted at global and local levels are illustrated in Fig. 7 and Fig. 8, respectively."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Statistics on the data scale for various tasks. We collect 0.7 billion data pairs, which cover 8 basic types including 37 subtasks, multi-turn and long-context generation datasets. Image Segmentation involves extracting image spatial region information for different targets within an image. This is achieved by selecting and modifying specific areas for operations and editing in downstream tasks. We employ the Efficient SAM (Xiong et al., 2023) tool for marking different target areas within an image. Depth Estimation indicates the relative distance information of different targets within an image. We use the Midas (Ranftl et al., 2022) algorithm to extract depth information. Human-pose Estimation is employed for modeling the human body to obtain structured information about body posture. We make use of the RTMPose (Jiang et al., 2023) algorithm to extract information from images containing human figures, and posture information visualization is done using OpenPoses 17-point (Cao et al., 2021) modeling method. Image Mosaic pixelates specific areas or the entire image to protect sensitive information. Image Degradation is used to degrade the quality of an image to simulate the phenomenon of image distortion found in the real world. Following the practice of super-resolution algorithms (Wang et al., 2021), we add random noise to the input images. Image Grayscale is typically done to facilitate the editing of an images original colors downstream. We do this conversion directly using OpenCVs Grayscale function. Edge Detection detects the edge information from the original image. We utilize the edge detection method named Canny (Canny, 1986) implemented by OpenCV. Doodle Extraction is usually used to simulate relatively rough hand-drawn sketches by extracting the outline of objects and ignoring their details. We use the PIDNet (Xu et al., 2023) and SketchNet (Zhang et al., 2016a) to extract this information. Contour Extraction is about delineating the outline of targets within an image, which simulates the drawing process of the image and is often used for secondary processing of images. We use the contour module from the informative drawing (Chan et al., 2022) for this information extraction. Scribble Extraction involves retrieving the original line art information to capture the sketch-like form of the image. We utilize the anime-style module from informative drawings (Chan et al., 2022) to extract the relevant information."
        },
        {
            "title": "Technical Report",
            "content": "Figure 7: The visualization of low-level visual analysis preprocessing. Figure 8: The visualization of regional low-level visual analysis preprocessing. B.3 CONTROLLABLE GENERATION In the realm of vision-based generative foundation models, the ability to generate corresponding content using any provided prompts is commonly present. To further control aspects such as spatial layout, structure, or color in the generated images, additional conditional information is often incorporated as inputs to the model. We integrate various controllable condition-to-image tasks within unified framework to accommodate different visual conditions. The control conditions include the visual features mentioned in the low-level visual analysis section. For training data, we employ pairs constituted by the aforementioned control conditions in Fig. 7 and regional control conditions in Fig. 8 obtained through low-level visual analysis, using the conditional part as inputs to the model to achieve pixel-precise image generation. For text guidance, we construct the instructions based on image captions with our proposed Instruction Captioner. B.4 SEMANTIC EDITING Semantic Editing aims to modify specific semantic attributes of an input image by providing detailed instructions. It involves facial editing, which aims to modify partial attributes of characters while preserving the overall identity, and style transforming, which aims to transform the image style to specific artist theme guided by instruction while keeping content unchanged. Additionally, any other semantic editing requests that do not fall into these two categories are classified as general editing, e.g., changing the background scene of an image, adjusting subjects posture, and modifying the camera view. We discuss the specifics according to the particular tasks."
        },
        {
            "title": "Technical Report",
            "content": "Figure 9: Illustration of facial editing data processing workflow. Figure 10: The dataset visualization of facial editing. B.4.1 FACIAL EDITING Facial Editing encompasses both the transformation and preservation of facial attributes. Specifically, the facial attributes preservation task focuses on editing other elements of the image while maintaining the consistency of complex identity details in facial representations. The facial attributes transformation task is primarily concerned with altering specific attributes of the face without affecting other aspects of the image. Facial Attribute Preservation. The facial attribute preservation dataset is divided into two main parts: aligned and misaligned facial data as shown in Fig. 10a. There are two novel processing workflows as shown in Fig. 9. (i) Aligned facial data. We generate pixel-aligned face data using generative models such as InstantID (Wang et al., 2024b) and combine it with GPT models to produce diverse prompts. Subsequently, we train multiple lightweight binary classification models to clean the generated data based on image quality, PPI score, aesthetic scores, and other metrics. Additionally, we extract facial features using ArcFace (Deng et al., 2019a) for similarity calculations, selecting high-matching data pairs with similarity score exceeding 0.65. Once our model demonstrates the ability to maintain facial integrity, we initiate self-iterative training process to generate higher quality data, as illustrated in Fig. 9a. (ii) Misaligned facial data. We first employ face detection algorithm (Zhang et al., 2016b) to filter images containing only one face. Subsequently, we utilized facial features to perform K-means clustering, resulting in 10,000 clusters. Within each"
        },
        {
            "title": "Technical Report",
            "content": "Figure 11: The dataset visualization of style editing. cluster, we conducted second clustering using the union-find algorithm. Faces with similarity score greater than 0.8 and less than 0.9 were linked to avoid grouping perfectly identical images. Finally, manual annotation and deduplication were performed on the remaining clusters, yielding the final unaligned facial dataset as shown in Fig. 9b. Based on the general instruction construction process in Sec. 3.2, we design the instructions for facial editing to emphasize that the individuals in the image pairs being annotated are the same person. The instructions must reflect this and focus on the differences in personal details between the two images. Facial Attribute Transformation. We add four fine-grained facial attribute transformation tasks: smiling, beard, makeup, and hair dyeing. We obtained the relevant data in bulk by calling the Aliyun API and trained binary classifiers for each category to filter out data with indistinct changes. As result, we acquired total of 1.4 million high-quality pairs of data as shown in Fig. 10b. Equally, we strive to guide the generated captions to closely reflect the facial attributes, thereby enhancing the models understanding of the similarities and differences in tasks related to facial attributes. B.4.2 STYLE EDITING Following the similar image pair construction strategy from StyleBooth (Han et al., 2024), we prepare larger training data that encompasses over 80 styles and 63000 image pairs. Besides, additional real-world and synthesized style images are collected as style editing target images, and their corresponding original images are generated by transforming these collected images to different graphic styles such as cinematic, photography, etc. In this way, we obtain around 70000 input and output image pairs of about 400 high-quality styles. We show samples of the final style editing data in Fig. 11. We conduct different filter strategies to leverage the data quality: (i) Like StyleBooth, we use CLIP score as the metric to filter out the image pairs which have too minor or too great differences. (ii) To further filter out the faultful synthesized target images that are not particularly aligned with the provided prompt keywords in terms of style, we use CSR (Somepalli et al., 2024) representations and implement style clustering within every style subgroup. Setting threshold of 0.65, cosine similarities are calculated for union-find clustering. The largest cluster contains images in similar visual style while other clusters are filtered out. B.4.3 GENERAL EDITING The objective of general editing is to curate an image that seamlessly harmonizes with both textual and visual prompts for variety of purposes. It involves two tasks, i.e., caption-guided image generation and instruction-guided image adaption. The former task receives one reference image and one caption as prompts to generate the image, and the latter task intends to adapt the source image by following the given instructions. The training data for these two tasks can be unified into the same format, which consists of content-related image pair (Isource, Itarget), and text prompt indicates how to generate target image. An essential goal of building such training dataset is to acquire content-related image pairs, one of which serves as the source image and another serves as target image. The overall dataset construction pipeline is depicted in Fig. 12. It includes two branches, i.e., clustering-based method, and synthesis-based method."
        },
        {
            "title": "Technical Report",
            "content": "Figure 12: The dataset construction pipeline for general editing task. Figure 13: General editing sample pairs generated by our dataset construction pipeline. Image pairs in the first row are generated by cluster-based method, and those in the second row are generated from synthesis-based method. Cluster-based method. We employ embedding-based clustering on the database to group contentsimilar images. Union-find technology is employed inside each cluster to achieve more fine-grained image pair aggregation. We then collect all possible pairs from each disjoint set. Additionally, binary classifier evaluates the content relevance of pairs, and those with low relevance are discarded. Synthesis-based method. We use IP-Adapter technology to synthesize images according to the reference images and text prompts, thus the content-related image pairs can be obtained. To ensure visual content is similar but not the same, we set the image control strength λ to 0.6, and binary classifier is utilized to filter out content-unrelated pairs. We depict some generated samples in Fig. 13. For the text prompt of each image pair, we use the MLLM to generate both caption that describes the visual content of the target image, and an instruction that indicates how to adapt the source image to the target image, as described in Sec. 3.2. B.5 ELEMENT EDITING Element editing focuses on the selective manipulation of specific subjects within an image. This process allows for the addition, deletion, or replacement of particular subject while ensuring that the other elements within the image remain unchanged. By doing so, the integrity of the overall com-"
        },
        {
            "title": "Technical Report",
            "content": "Figure 14: The pipeline of building training data of text editing. Figure 15: The dataset visualization of multi-lingual text editing. Figure 16: Template-based instructions and MLLM-based instructions on text editing. position is preserved, allowing users to make precise edits and achieve desired alterations without disrupting the context of the original scene. We focus on two common elements: text and objects. B.5.1 TEXT EDITING Text editing is an important task of element editing. Despite the progress gained in image generation, the capability of text rendering is still far from satisfying. Therefore, text editing is necessary technology to revise the incorrect or deformed text rendered in image. Text editing involves text removing task, which is to erase text from image while preserving all other visual cues, and text rendering task, which is to render specific text at any location of an image. The goals of these two tasks are exactly the opposite, hence their training data can be shared to each other. For instance, for any image pair , suppose the text removing represents the generation direction from Ia to Ib, on the contrary, the generation direction from Ib to Ia stands for text rendering. Therefore, the objective of constructing the dataset thus becomes how to obtain large number of image pairs, where one image contains the specified text and the other does not while keeping the non-text content unchanged. Ia, Ib} { We propose two-branch data collection method to address this issue. The overall pipeline shown in Fig. 14 includes two paths: (i) Text remove path. For images containing text data, which typi-"
        },
        {
            "title": "Technical Report",
            "content": "Figure 17: Illustration of data construction pipeline for object editing in element editing. Figure 18: The dataset visualization of object editing in element editing. cally from publicly available text datasets such as AnyWord3M (Tuo et al., 2023) and LaionGlyph10M (Yang et al., 2023), we first mask out all text regions. Then, we redraw the masked areas leveraging image inpainting method. To ensure the regenerated image does not contain any textual information, we employ OCR detection leveraging the open-sourcing OCR model (e.g., PPOCR) (Du et al., 2020) and filter out all images that contain any texts. Finally, we adopt an image quality score predictor which is trained with small amounts of manually annotated data to score all text-removed images and pick high-quality samples according to the score. (ii) Text render path. For any image dataset, We first employ OCR detection to ensure input images contain no text. Then random characters are rendered in random locations of these images by utilizing existing text editing methods (e.g., AnyText) (Tuo et al., 2023). We render text using Chinese or English characters to support multi-lingual text rendering capability. We depict some cases in Fig. 15. Finally, we implement OCR detection on the edited image to ensure all characters are rendered correctly. When training, image pairs collected from both two paths are merged to form the total dataset. We adopt template-based and MLLM-based methods to construct instructions that describe how to render or remove text from the input image. For MLLM-based method, besides the content of characters, we add extra color and position controls by specifying the text color and render position in the textual instruction. Given text image, we utilize pre-trained MLLM to describe the color, content, and position information of text in this image, thus text editing instruction can be easily inferred based on these descriptions. Some cases of template-based and MLLM-based instructions are illustrated in Fig. 16. B.5.2 OBJECT EDITING Object-based image editing is one of the most commonly used techniques for creatively manipulating images. Its primary goal is to either remove or add objects in an image based on text instructions provided by the user, while ensuring harmonious composition. To obtain training data for this task, we need to construct pair of data to indicate the presence relationship of objects. Specifically, we"
        },
        {
            "title": "Technical Report",
            "content": "Figure 19: The dataset visualization of Repainting. focus on images that either contain specific object or do not, ensuring that all other parts of the images remain as unchanged as possible, except for the area where the object is located. We can see the entire dataset process from Fig. 17. We first utilize RAM (Zhang et al., 2023c) for open-label tagging, obtaining semantic labels for different subjects in the image, and then applying Grounded-SAM (Liu et al., 2023a; Kirillov et al., 2023) to segment the input semantics. Next, we perform preliminary screening of objects based on filtering criteria including the area of the masks and bounding boxes, as well as their effective ratios, removing any unreasonable subjects. We then use the LaMa (Suvorov et al., 2022) method, combining the original image with the subject mask area for inpainting. This operation effectively removes local objects without significantly affecting other areas. Finally, we employ pre-trained binary classification model to determine whether the inpainted image meets expectations, filtering out artifacts introduced by the inpainting algorithm. In terms of instruction formulation, we employ template format that incorporates the object name { Through the data construction pipeline, we can obtain the original image, the image with the object removed, the object mask, and the corresponding text instructions. This way, we can implement forward pipeline for object removal and reverse pipeline for object addition, while ensuring the integrity of the image and the accuracy of the text instructions, as in Fig. 18. tag, while also utilizing common instruction based on image pairs. } B.6 REPAINTING The repainting task can be defined as the process of reconstructing missing image information within specified masked regions. Depending on the location of the masked area and input conditions, this task can be categorized into three distinct types: unconditional inpainting, text-guided inpainting, and outpainting. Some examples of training data are shown in Fig. 19. B.6.1 UNCONDITIONAL INPAINTING Unconditional image inpainting typically utilizes methods such as low-level textual information and Fourier Convolutions, combined with contextual information from the known areas of the image, to reconstruct the missing portions. This process usually requires an input consisting of an image to be inpainted and mask indicating the regions that need to be filled, leading to an output image where the missing areas are completed. The task demands that the original information is preserved and that there is high-quality seamless integration between the original and the filled-in areas. By employing LaMas (Suvorov et al., 2022) mask generation strategy, we randomly apply bbox or irregular-shaped masks to the images and vary the degree of this operation to enable the model to handle different types of missing regions as effectively as possible. B.6.2 TEXT-GUIDED INPAINTING Text-guided inpainting primarily aims to fill and restore missing parts of an image by utilizing text descriptions to guide the process. Unlike traditional unconditional inpainting, this method integrates textual information to guide the model, resulting in images that better meet the users specific"
        },
        {
            "title": "Technical Report",
            "content": "Figure 20: Illustration of inference pipeline layer decouple and layer fusion in layer editing. Figure 21: Sample data for training layer decouple and layer fusion in layer editing. requirements. In constructing this dataset, we not only employ random masks paired with corresponding textual descriptions of the original images but also refine the process to focus on local regions. First, we obtain multiple object masks from the image, and then extract detailed textual descriptions for each object. Finally, we create triplets consisting of the original image, the local object mask, and the local object caption. This approach enables the generation of richer and more controllable details within local areas. B.6.3 OUTPAINTING The outpainting task involves intelligently generating and completing the edge regions of an existing image so that the extended new image appears natural and continuous visually. The major challenge of this task is producing images that are rich in detail, diverse in content, and exhibit certain level of associative ability. In terms of data processing, we employed commonly used techniques, applying random masks to the areas and directions that need to be expanded, in order to adapt to different scenarios of image completion. B.7 LAYER EDITING Hierarchical layer editing operations on images involve two aspects: (i) Layer decouple: enables the separation of the main subject within single image, resulting in complete subject and reconstructed background. The subject must be restored to its complete form, mitigating any gaps caused by occlusion or other reasons present in the original image. Meanwhile, the background is filled in for the blank areas left after the subjects separation, achieving fully deconstructed fore/background. (ii) Layer fusion: allows for the incorporation of distinct independent subjects into target image, facilitating high-quality image integration. The inference pipeline can be seen in Fig. 20. For the data construction, we follow the data workflow from the object editing task, focusing on slightly larger subjects and data containing multiple subjects within single image. This approach"
        },
        {
            "title": "Technical Report",
            "content": "Figure 22: The dataset visualization of multi-reference. creates compositions that allow for lossless splitting of single original image into multiple subimages, and conversely establishes correspondence for combining several images into one. Specifically, as shown in Fig. 21, in layer decouple stage, we follow the instructions to transition from the original image to either singular subject or singular background. During training, the non-subject areas of the subject image and the incomplete portions of the background are filled with white color. Additionally, to simulate the scenario of subjects obscured in the image, we perform random masking on the extracted subject images. The output targets are the complete subject or background. In layer fusion stage, we employ multi-reference image strategy, taking single or multiple subjects along with the background as inputs to guide the generation of the target image. Similarly, different subjects are supplemented with white color and placed on randomly sized white canvas, with the training goal being to generate harmonious and complete composite image. B.8 REFERENCE GENERATION Ordinary image generation and editing tasks require no more than one input image. Under certain circumstances, image generation needs multiple image inputs, such as multiple conditions in con-"
        },
        {
            "title": "Technical Report",
            "content": "Figure 23: Sample data for training multi-turn and long-context generation task. trollable generation, and group of character design images for ID preservation. The same is true for editing tasks, one or more additional exemplar images are necessary to specify the expected visual elements in the editing area. For example, reference image can be interpreted as the target image style appearance, face identity, etc. Therefore, we prepare training data for multi-reference generation and reference-guided editing. Examples of training data are shown in Fig. 22. B.8.1 MULTI-REFERENCE GENERATION Multi-condition generation. In controllable generation, overlaying different types of conditions is usually necessary to control the different visual aspects of generated images. Similar to the process in appendix B.3, canny edge maps, depth maps, color maps, grayscale images, contours, scribbles, doodles, and pose keypoints are included for multi-condition generation. To make it possible to composite objects in different conditions, we use object segmentation to assign each area with different condition. Series Generation. It has been widely studied how to generate images about one consistent visual element, like the portrait of specific figure, pictures with the same styles, etc. Usually, tuning themed tuner (e.g., LoRA) (Hu et al., 2022) with few images is the primary option. However, we are aiming to teach our model to understand and follow the rules lying behind image series. We collect image groups through image clustering. During the training phase, we randomly sample one image in the cluster as target and 3 to 8 images as input images. B.8.2 REFERENCE-GUIDED EDITING Style and face are two typical editing tasks benefiting from additional reference image inputs, providing supplementary visual information of the target images. Style reference editing. To construct the training data, we extend the data of style editing (appendix B.4.2) by assigning an additional style reference image for each edit-target image pair. Reference images are randomly selected from other styled images within the same style category. Face reference editing. We use image pairs of misaligned facial data (appendix B.4.1) for face reference editing. We pick one of the two images as reference image while another as target image. Therefore, the target and reference image are the same person but slightly different. The edit image is derived from target image by erasing the face area to avoid any spoilers. B.9 MULTI-TURN AND LONG-CONTEXT GENERATION Multi-turn editing refers to the process of obtaining the final image from an input image through multiple independent instruction-based editing, which poses significant challenges in both the models"
        },
        {
            "title": "Technical Report",
            "content": "Figure 24: The overview of benchmark distribution. Global and Local refer to editing or generating based on the entire image, and editing or generating based on specific local areas of the image, respectively. Table 3: The comparison between ACE benchmark and existing benchmarks. Benchmark Real Image? Generated Image? Multi-turn? Regional? Tasks Data Scale MagicBrush Emu Edit ACE Y Y Y - 8 31 1588 3589 precise understanding of instructions and the control over image quality in every round. Further, the long-context generation process aims to leverage the contextual information provided in each round of interactions to construct long sequence, thereby generating images that align with the intended directives. The generated images reference multiple images and their corresponding instruction information from previous interactions, capturing the users genuine intent within the interaction framework, and allowing for more precise image editing. The data construction consists of two parts: (i) Homogenous content-based condition unit: this involves employing pair data collection strategy to obtain various clusters from large-scale database, as shown in Sec. 3.1, where each cluster contains images paired with their respective captions and instruction generated in pairs. During training, we select one image from any chosen cluster as the starting point and build multiple rounds data chain using its caption or instruction, predicting the final image as the endpoint of the chain. (ii) Task-based condition unit: we treat all the previously mentioned single-image tasks as individual turns within the task and randomly sample them to form complete multiple precursor unit that guides the final image generation."
        },
        {
            "title": "C BENCHMARK DETAILS",
            "content": "Previous methods have proposed benchmarks to evaluate model performance for image editing, with notable examples including MagicBrush (Zhang et al., 2023a) and Emu Edit (Sheynin et al., 2024). MagicBrush has 1,588 samples, which includes 1,053 single-turn and 535 multi-turn instances, and primarily comes from MS-COCO (Lin et al., 2014). Emu Edit first defines 8 different categories of potential image editing operations and constructs the instructions by human annotators. The main issues with the above methods are insufficient coverage of tasks and generally poor data quality."
        },
        {
            "title": "Technical Report",
            "content": "Table 4: The multi-stage training details for ACE. Stage Model Capacity Train Data Scale Visual Sequence Length Max Image Number Training Steps Batch Size Instruction Align Instruction Align Aesthetic Improvement Aesthetic Improvement 0.6B 0.6B 0.6B 0.6B 0.7 Billion 0.7 Billion 50 million 50 million 1024 1024 1024 1 9 9 9 900K 100K 500K 100K 800 400 400 960 Figure 25: The effectiveness of Image Indicator Embeddings. Model follows the image indicators in instructions to distinguish the source and reference images. ACE builds benchmark comprising 12,000 samples, covering more than 31 tasks while accommodating 5900 real images and 6100 generated images. In addition, ACE benchmark supports both regional editing and multi-turn editing tasks. The specific statistics are shown in the Fig. 24, and the comparison with other benchmarks is presented in the Tab. 3."
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "We employ the T5 language model as the text encoder and DiT-XL/2 (Peebles & Xie, 2023) as the base network architecture. The model capacity is nearly 0.6B and the parameters are partly initialized by PixArt-α (Chen et al., 2023a). The maximum length of the text token sequence is set to 120. We freeze VAE and T5 modules, utilizing AdamW (Loshchilov & Hutter, 2018) optimizer to train the DiT module with weight decay of 5e-4 and learning rate of 2e-5. All experiments are conducted in A800. multi-stage training strategy is employed to progressively enhance the aesthetic quality and increase the generalizability of model. The training details are presented in Tab. 4. First, we train the instruction-following capability on single-image tasks using 0.7 billion data points, with the number of single image tokens limited to 1024. Next, we expand the tasks to include multiple-image scenarios. After learning the instruction alignment, we utilize high-aesthetic data to enhance the models aesthetics and extend the max image token number to 4096 for generating higher-resolution image."
        },
        {
            "title": "E MORE EXPERIMENTS",
            "content": "Design of Image Indicator Embeddings. We test the effectiveness of Image Indicator Embeddings by adjusting the order of input images. As we can see in Fig. 25, the model always understands which image is the source image and which is the reference following the image indicators in the"
        },
        {
            "title": "Technical Report",
            "content": "Table 5: Results on Emu Edit benchmark. ACE shows comparable performance to its baselines. Method CLIPdir CLIPout L1 CLIPimg DINO InstructPix2Pix (Brooks et al., 2023) MagicBrush (Zhang et al., 2023a) Emu Edit (Sheynin et al., 2024) UltraEdit (Zhao et al., 2024) CosXL (StabilityAI, 2024) ACE (Ours) 0.0739 0.0831 0.1073 0.0888 0.0901 0.0855 0.2681 0.2701 0.2791 0.2783 0. 0.2746 0.1240 0.0995 0.0893 0.0532 0.0940 0.0761 0.8508 0.8664 0.8743 0.8814 0.8686 0.8952 0.7647 0.7927 0.8398 0.8524 0. 0.8620 Table 6: Quantitative results for Facial Editing tasks on ACE benchmark. requires an additional landmark as condition, while other methods do not. indicates that InstanID Method Face Similarity Effective Score InstantID Wang et al. (2024b) CosXL StabilityAI (2024) UltraEdit Zhao et al. (2024) IP-Adapter Ye et al. (2023) FaceChain Liu et al. (2023b) ACE (Ours) 84.08 66.49 62.91 66.51 65.46 70.07 0.96 0.37 0.16 0.31 0.42 0. Table 7: Quantitative results for Local Text Render tasks on ACE benchmark. Method Edit Distance Sentence Accuracy UDiffText (Zhao & Lian, 2024) AnyText (Tuo et al., 2023) ACE (Ours) 0.6827 0.6035 0. 0.4110 0.3313 0.5767 instruction. This means textual instructions and images are implicitly associated via the design of Image Indicator Embeddings. Emu Edit Benchmark. We also conduct comparison on Emu Edit benchmark (Sheynin et al., 2024). It includes 3,589 examples of 8 tasks: background alteration, comprehensive image changes, style alteration, object removal, object addition, localized modifications, color/texture alterations, and text editing. This benchmark measures the similarity between output and input images and the provided captions. We calculate the L1 distance, CLIP similarity, and DINO similarity between the generated image and input image, together with the CLIP text-image direction similarity measuring agreement between the change in captions and the change in images, and CLIP similarity between the generated image and output caption. We use the code adapted from the MagicBrush evaluation code and models of CLIP ViT-B/32 and DINO ViT-S/16. As shown in Tab. 5, ACE achieves comparable performance to its baselines. Facial Editing. When evaluating the face identity preservation ability, we designed Face Similarity (FS) metric to measure the consistency of faces between generated images and original images. We first detect the face region using MTCNN (Zhang et al., 2016b), then extract face embeddings with the ArcFace (Deng et al., 2019a) algorithm. The cosine similarity between normalized embeddings is calculated as the face similarity score. The images generated by MagicBrush and InstructPix2Pix exhibit excessive similarity to the original images, thus metrics for these two methods are not computed. We observed non-linear growth in the Face Similarity score. To analyze this, we extracted facial features from over 5 million data points in MS1M V3 (Deng et al., 2019b) and grouped them into clusters based on their face id. We then calculated the pairwise similarity within each cluster, resulting in mean of mean = 0.5258 and standard deviation of std = 0.1765. Due to the large standard deviation, we further analyzed the percentage of samples with scores above 0.3493(mean std) that met the instructions to evaluate the Effective Score(ES) of facial ID persistence."
        },
        {
            "title": "Technical Report",
            "content": "Figure 26: The pipeline of workflow distillation. Figure 27: The results visualization of workflow distillation. Our model significantly outperforms other methods in the absence of facial landmark information, with improvements of 3.56% and 25% in the FS and ES metrics as shown in Tab. 6. Although our model (0.6B) demonstrates inferior performance on metrics compared to InstantID (2.6B), it is important to highlight that InstantID utilizes an additional facial landmark as conditioning factor. Moreover, as indicated by the results of prompt-following and image quality assessments in Tab. 2, our model shows highly competitive performance overall. Local Text Render. To adequately evaluate the performance of text editing, we provide the quantitative analysis of our method with two SOTA text render methods, i.e., UDiffText, and Anytext, on the local text render task of ACE benchmark. Each generated text line is cropped according to the specific position and fed into an OCR model to obtain predicted results. As described in Anytext, we calculate the Sentence Accuracy and the Normalized Edit Distance for each method. The former metric evaluates the sentence-level accuracy and the latter metric evaluates the char-level precision. From Tab. 7 we can observe that our ACE outperforms the other two methods, achieving performance gains of 14% and 16% in terms of Normalized Edit Distance and Sentence Accuracy, respectively. This demonstrates our superior text rendering capability."
        },
        {
            "title": "Technical Report",
            "content": "Figure 28: The multi-turn conversation pipeline of our chat bot application. Figure 29: The user interface of the chat bot application built with Gradio."
        },
        {
            "title": "F APPLICATION",
            "content": "F.1 WORKFLOW DISTILLATION There are many excellent workflows assembling LoRAs, ControlNets, and T2I models on opensource platforms, which enable users to achieve certain results. To show the capability and compatibility of ACE, we collect several outstanding workflows to obtain their result images for distillation. We train ACE with the inputs and corresponding outputs of these workflows, as well as fixed special trigger instruction, as illustrated at Fig. 26. Our model acquires similar abilities of these workflows, as shown in Fig. 27, which demonstrates the great potential of ACE. F.2 CHAT BOT Leveraging our diffusion model, we build chat bot application to achieve chat-based image generation and editing. Rather than cumbersome visual agent pipeline, our chat bot supports all image creation requests with only one model serving as the backend, hence achieving significant efficiency improvement compared with visual agents. We depict multi-turn conversation sample in 28 and"
        },
        {
            "title": "Technical Report",
            "content": "illustrate the user interface in Fig. 29. We could command the model to create any desired image by chatting with it using natural language. The overall system can be formulated as Aj = ChatBot(H<j, Qj), where Aj denotes the j-th round output of chat bot, Qj represents the j-th round user request, and H<j = represents the history of dialogue before j-th 1) round. By introducing the history dialogue information into the current conversation, our model excels at understanding complex user requests, therefore achieving better prompt following ability. (Q1, A1), (Q2, A2), ..., (Qj 1, Aj { } (6)"
        },
        {
            "title": "G MORE VISUALIZATION",
            "content": "In Fig. 30, and Fig. 31, we present the visualization results of ACE in low-level visual analysis. The Fig. 32, Fig. 33, and Fig. 34 are the visualization of controllable generation. The visualization results of repainting are depicted in Fig. 35. Semantic editing tasks such as general editing, facial editing, and style editing are illustrated in Fig. 36, Fig. 37, Fig. 38. The visualization results of elements editing including text editing, and object editing are shown in Fig. 39, and Fig. 40. In Fig. 41, we present the visualization results of layer decouple and layer fusion. The visualization of reference generation can be found at Fig. 42 and that of multi-turn and long-context generation are present in Fig. 43. ACE demonstrates proficient instruction following, high-quality generation, and versatility across different tasks."
        },
        {
            "title": "H DISCUSSION",
            "content": "Societal Impacts. From positive perspective, the intelligent generation and editing of images can provide artists and designers with innovative tools to inspire new concepts, enhance creativity and artistic expression in images, lower the barriers to artistic creation, and reduce the labor-intensive manual processes involved. Additionally, the method can serve various industries. In the field of education and training, they can be used to create supplementary teaching materials, such as illustrations for picture books, enhancing students learning experiences and improving communication and understanding in lessons. In business environments, companies can utilize the method to generate marketing materials and product designs, thereby increasing production efficiency and creative output. The positive impacts of these technologies offer new possibilities for creativity, educational quality, and business efficiency, making it worthwhile for us to actively explore and apply them. New technologies not only bring new opportunities but also come with challenges. Firstly, issues related to copyright and authorship are prominent, potentially infringing upon the rights of original works and leading to legal disputes. Secondly, false information generated by models may exacerbate the spread of rumors, undermining public trust in information. Lastly, the inherent biases and stereotypes present in generated content can challenge societal values and moral standards. Therefore, while we acknowledge the conveniences and innovations offered by these technologies, it is imperative to carefully consider and effectively manage these negative impacts to ensure the sustainability of technological development and uphold social responsibility. Limitations. First, our approach offers unified framework for existing editing tasks. For specific tasks such as text-to-image generation, the aesthetic quality of our generated results lags behind that of state-ofthe-art generative models like Midjourney and FLUX. These models have achieved breakthroughs by focusing on single task of generating images from text prompts. In contrast, our model supports broader range of input types and handles wider variety of tasks, such as performing diverse edits under open-ended instructions. Additionally, training on higher-quality data and using larger-scale model could help bridge this gap. Second, the model for instruction editing needs to accurately capture the users actual intent. In our framework, we utilize fixed encoder-decoder language model to encode text instructions. However, as user instructions become more complex and diverse, the difficulty of interpreting these instructions also increases. Furthermore, the current model is unable to handle multiple intents or tasks from single instruction simultaneously and requires intent decomposition."
        },
        {
            "title": "Technical Report",
            "content": "Third, we support the input of multiple images and multiple instructions to construct long contextual information for generation. On one hand, it is inevitable that, due to limited hardware resources, training and inference with multiple images become increasingly challenging as the number of tokens increases. On the other hand, excessively long contextual inputs pose significant challenge for the model, as the forgetting of historical information during the process may lead to biases in the final generated results. Future Work. We try to illustrate some existing constraints of the model in limitations, which can serve as directions for our future work. Firstly, the phenomenon of scaling laws has been demonstrated in the NLP field, indicating that further exploration of scaling laws in complex generation tasks is warranted. We will focus on two main approaches: on one hand, we will work on expanding high-quality data, which includes improving data quality, incorporating more complex tasks, and enhancing the precision of instructional data; on the other hand, we will directly increase the model architectures scale to enhance its general generative capabilities. Secondly, we aim to introduce LLMs or MLLMs to accurately capture the intentions of users at the instruction level, leveraging their robust general understanding of language and images. This involves two specific objectives: firstly, to enhance the models ability to generalize from input text instructions to match single-task or multi-task contexts; and secondly, to improve the understanding of input images that are to be edited, thereby assisting subsequent instruction operations and ensuring the accuracy and diversity of the generated content. Finally, it is essential to explore the long-sequence modeling of multi-modal data comprising multiple rounds of image and text interactions. We will engage in continuous contemplation regarding how to ensure that the historical context of image-text pairs is truly beneficial, similar to the functionality of chatGPT-like language models."
        },
        {
            "title": "Technical Report",
            "content": "Figure 30: The ACEs generated visualization of image segmentation, depth estimation, human-pose estimation, image mosaic, and image grayscale in low-level visual analysis."
        },
        {
            "title": "Technical Report",
            "content": "Figure 31: The ACEs generated visualization of image degradation, edge extraction, contour extraction, and scribble extraction in low-level visual analysis."
        },
        {
            "title": "Technical Report",
            "content": "Figure 32: The ACEs generated visualization of segmentation-based, depth-based, pose-based, mosaic-based, and grayscale-based generation in controllable generation."
        },
        {
            "title": "Technical Report",
            "content": "Figure 33: The ACEs generated visualization of degradation-based, edge-based, doodle-based, contour-based, and scribble-based generation in controllable generation."
        },
        {
            "title": "Technical Report",
            "content": "Figure 34: The ACEs generated visualization in scribble-based controllable generation. Figure 35: The ACEs generated visualization of repainting. Figure 36: The ACEs generated visualization of general editing in semantic editing."
        },
        {
            "title": "Technical Report",
            "content": "Figure 37: The ACEs generated visualization of facial editing in semantic editing."
        },
        {
            "title": "Technical Report",
            "content": "Figure 38: The ACEs generated visualization of style editing in semantic editing."
        },
        {
            "title": "Technical Report",
            "content": "Figure 39: The ACEs generated visualization of text editing in element editing."
        },
        {
            "title": "Technical Report",
            "content": "Figure 40: The ACEs generated visualization of object editing in element editing. Figure 41: The ACEs generated visualization of layer decouple and layer fusion in layer editing."
        },
        {
            "title": "Technical Report",
            "content": "Figure 42: The ACEs generated visualization of multi-reference generation and reference-guided editing."
        },
        {
            "title": "Technical Report",
            "content": "Figure 43: The ACEs generated visualization of multi-turn and long-context generation."
        }
    ],
    "affiliations": []
}