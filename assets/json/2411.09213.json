{
    "paper_title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering",
    "authors": [
        "Nghia Trung Ngo",
        "Chien Van Nguyen",
        "Franck Dernoncourt",
        "Thien Huu Nguyen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain."
        },
        {
            "title": "Start",
            "content": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering Nghia Trung Ngo1, Chien Van Nguyen1, Franck Dernoncourt2, Thien Huu Nguyen 1 1Department of Computer Science,University of Oregon, OR, USA 2Adobe Research, USA {nghian,chienn,thien@cs}@uoregon.edu, franck.dernoncourt@adobe.com 4 2 0 2 4 1 ] . [ 1 3 1 2 9 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) has emerged as promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of reliable medical system. This paper addresses this gap by providing comprehensive evaluation framework for medical question-answering (QA) systems in RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both stateof-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in solving complex medical problems, achieving state-of-the-art performance across various benchmarks. However, ensuring the reliability and truthworthiness of an artificial intelligent (AI) medical system remains critical challenge, especially in healthcare applications. Retrieval-augmented generation (RAG) has emerged as promising approach to reduce LLMs hallucination problem by integrating external knowledge sources. While RAG has potential to improve the factual accuracy of LLMs response, incorporating an information retriever also presents new complexities that warrant careful evaluation. Consider the example in Fig. 1. The retrieved documents can contain not only useful knowledge that helps determine the true answer, but also noise information, or more serious, factual errors that can misleads the LLMs. To consciously apply RAG for medical QA, we must consider Figure 1: Blue texts are useful information that should be extract to help determine the answer. Red texts are factual errors that potentially mislead the LLMs. these practical scenarios and evaluate LLMs ability to interact with retrieved documents reliably. Recent efforts have been made to evaluate AI systems with LLMs in the medical domain (Nori et al. 2023; He et al. 2023; Xiong et al. 2024). For example, MedEval (He et al. 2023) presents large-scale, expert-annotated benchmark that cover various medical tasks and domains. (Xiong et al. 2024) evaluates RAG extensively based on their MIRAGE benchmark which cover 5 medical QA datasets. However, they only focus on the effect of RAG modules on target accuracy, missing other important aspects of AI medical system. Several recent works have explore RAG evaluation more comprehensively in general domain (Es et al. 2023; Chen et al. 2024b), RAGAS (Es et al. 2023) assess 3 qualities of RAGs outputs for QA tasks including: Faithfulness - degree to which responses align with the provided context, Answer Relevance - the extent to which generated responses address the actual question posed, and Context Precision-Recall - the quality of retrieved context. We follow the work from (Chen et al. 2024b) which establishes Retrieval-Augmented Generation Benchmark (RGB) to measure 4 abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. In particular, using questions from 4 medical QA datasets from MIRAGE as basis, we create Medical Retrieval-Augmented Generation Benchmark (MedRGB) to evaluate RAG system in the following 4 test scenarios: Standard-RAG: evaluates LLMs performance when presented with multiple retrieved signal documents to create context to answer to question. Sufficiency: evaluates LLMs reliability when there are noise documents within the retrieved context. By adding Insufficient Information as an additional response option, LLMs should only answer when they are confident to have enough information to determine the correct answer. This requires LLMs to not only be aware of its own internal knowledge, but also be able to filter out noisy information from external documents. Integration: evaluates LLMs ability to answer multiple supporting questions and integrate the extracted information to help address the main question. Robustness: evaluates LLMs resiliency to factual errors in the retrieved context. trustworthy AI medical system should be able detect factually incorrect documents and provide the corrected information. In total, MedRGB consists of 3480 instances for 4 test scenarios, which is over 5 times that of RGB. Using MedRGB, we evaluation 7 LLMs, including both state-ofthe art commercial LLMs and open-source models. In summary, our contributions are three-fold: We establish MedRGB with four test scenarios to evaluate LLMs for medical QA tasks in RAG settings. To best of our knowledge, it is the first benchmark comprehensively assess medical RAG systems in these practical setting. Using MedRGB, we extensively evaluate 7 LLMs, including both state-of-the art commercial LLMs and open-source models, across multiple RAG conditions. Experiment results demonstrate their limitation in addressing the more complex scenarios. We analyzed the errors of the LLMs and their reasoning process to provide insights and suggest future directions for developing more reliable and trustworthy medical RAG systems."
        },
        {
            "title": "Related Work",
            "content": "Medical Retrieval-augmented Generation The application of LLMs in medical domain demands high level of accuracy and reliablity, which most of the current LLMs still struggle with (Zhou et al. 2023). Retrievalaugmented Generation (RAG) (Lewis et al. 2020) addresses this problem by helping LLMs integrating external knowledge sources in their generation process. Recent works has achieved success in leverage RAG for knowledge-intensive tasks (Cui et al. 2023; Peng et al. 2023; Ram et al. 2023). Specifically for medical domain, (Hiesinger et al. 2023; Wang et al. 2024; Xiong et al. 2024) explore RAG for healthcare and clinical tasks. Medical Benchmarks Previous medical benchmarks usually focus solely on target performance of medical problems, which consist of only QA pairs (Jin et al. 2020a, 2019; Krithara et al. 2023). Some recent benchmark also included evidence for LLMs to reasoning on (Chen et al. 2024a). Most current systematic evaluations of LLMs in medical domain do not involve RAG (He et al. 2023; Nori et al. 2023). (Xiong et al. 2024) attempts to provide systematic evaluations of RAG systems in medicine. We build on their work to further evaluate important criteria of medical RAG system for variety of practical settings. Medical Retrieval-Augmented Generation Benchmark MedRGB creation process is demonstrated in Fig. 2, which involves three main steps: retrieval topic generation, document retrieval, and benchmark creation."
        },
        {
            "title": "Medical QA Dataset",
            "content": "The basis of MedRGB are the multiple-choice questions from 4 medical QA dataset from MIRAGE, including 2 from medical examination (MMLU and MedQA) and 2 from biomedical research (PubMedQA, BioASQ). MMLU (MMLU-Med in (Xiong et al. 2024)) subset of six tasks from 57 tasks of MMLU (Hendrycks et al. 2021), including anatomy, clinical knowledge, professional medicine, human genetics, college medicine, and college biologys. There are the 1089 questions with 4 answer options. MedQA (MedQA-US in (Xiong et al. 2024)) the English portion of MedQA (Jin et al. 2020a), which is multi-choice QA dataset collected from professional medical board exams. These are 1273 real-world questions with 4 answer options. PubMedQA (PubMedQA* in (Xiong et al. 2024)) consist of 500 expert-annotated test samples of PubMedQA (Jin et al. 2019). Each question is constructed from PubMed abstracts with yes/no/maybe answer options. BioASQ (BioASQ-Y/N in (Xiong et al. 2024)) test set of the recent annual competition for biomedical QA from 2019 to 2023 (Krithara et al. 2023). In total, there are 618 yes/no question, constructed based on biomedical literature for machine reading comprehension track."
        },
        {
            "title": "Topic Generation",
            "content": "In conventional RAG setup for QA tasks, dense retriever encodes the input question into high-dimensional vector to find semantically similar passages from an external corpus. The top-k documents (based on imilarity metric like cosine similarity) are then combined to create the context for LLMs to answer the original question. This approach often results in redundancy in the retrieved set due to the lack of diversity when using the similarity metric. To address this limitation, we first ask LLMs to generate diverse set of retrieval (sub-)topics using the prompt from Fig. 3. Each topic is then following the above process to create smaller set of relevant documents. These sets are then aggregated to create the final retrieval set for the original QA pair. Figure 2: The overall construction process of MedRGB. The green OpenAI symbol implies that the block involves data generation using the GPT-4o model. You are medical expert. Generate ranked search topics to help answer medical question. Follow these guidelines: 1. Rank topics by importance to the question. 2. Ensure relevance to the question and answer options. 3. The topics should be differentiable and efficient for information retrieval. Figure 3: Retrieval topic generation prompt (shorten version). You are medical expert answering multiple-choice question using provided documents. Follow these instructions: 1. Analyze the provide documents and question. 2. Think step-by-step and determine the correct answer. Figure 4: Standard-RAG test inference prompt (shorten version). You are medical expert answering multiple-choice question using provided documents. Some documents may be irrelevant. Follow these instructions: 1. Identify relevant documents. 2. Think step-by-step to determine the correct answer. If all documents are irrelevant: 1. Answer based on your knowledge if certain. 2. Return insufficient information if unsure. You are medical expert answering multiple-choice main question and related sub-questions using provided documents. Some documents may be irrelevant. Follow these instructions: 1. Analyze all documents. 2. Answer each sub-question using the most relevant document. Each subanswer should be concise string from the corresponding document. 3. Think step-by-step, use sub-question information to determine the correct answer to the main question. Figure 7: Integration test inference prompt (shorten version). You are medical expert tasked with creating deliberately incorrect answerdocument pair for given medical question. Follow these instructions: 1. Analyze the provided question, original answer, and document. 2. Generate deliberately incorrect new answer. 3. Minimally edit the original document to create persuasive but factually incorrect new document supporting the incorrect answer. Figure 8: Robustness test data generation prompt (shorten version). You are medical expert answering multiple-choice main question and subquestions using provided documents. Some documents may contain factual errors. Follow these instructions: 1. For each sub-question, identify the corresponding relevant document. 2. Determine if the document contains any factual error. 2.1. Extract the sub-answer from the factually correct document. 2.2. Provide the factually correct answer if the document contain factual errors. Figure 5: Sufficiency test inference prompt (shorten version). 3. Think step-by-step, use sub-question information to answer main question. You are medical research expert. For each provided document related to main medical question, create sub Q-A pair following these guidelines: 1. Explore different aspects related to the main question. 2. Sub-question must be specific to document. 3. Sub-answer must be short string extracted from the corresponding document. Figure 6: Integration test data generation prompt (shorten version). Figure 9: Robustness test inference prompt (shorten version). You are an expert in medical question answering evaluation. Given Question, model Prediction, and Ground Truth answer, judge whether the Prediction semantically matches the Ground Truth answer. Follow the instructions: 1. If the prediction semantically matches the ground truth completely, score 1. 2. If the prediction semantically matches some part of the ground truth and is relevant to the question, score 0.5. 3. If the prediction is completely wrong or irrelevant to the question, score 0. Figure 10: GPT-based Scoring Prompt (shorten version). Document Retrieval We incorporate two retrieval processes in MedRGB to simulate the real-world difference between the use case of an expert and non-expert person. Offline Retrieval Process In the expert use case, the retrieval corpus should contain highly specialized information that should not be publicly available. We use MedCorp (Xiong et al. 2024) as our offline corpus which contains medical documents from 4 different sources, including Pubmed, StatPearls, Textbooks and Wikipedia. The retrieval topics from previous step are encoded by MedCPT (Jin et al. 2023), biomedical-domain dense retriever, to query the corpus for relavent documents. We choose MedCPT instead of general-domain retriever like Contriever (Izacard et al. 2021) or lexical retriever like BM25 (Robertson and Zaragoza 2009) due to its consistent performance retrieving from medical domain, as demonstrateed in (Xiong et al. 2024). Top-3 documents for each retrieval topics are collected to create the retrieval set. Online Retrieval Process In non-expert use case, user simply asks question to general LLMs, which then retrieves document through online search engine to help answer users query. For each of the original medical question, the generated sub-topics are used to query the internet through Google Custom Search API1, which return topscored links. The content from each retrieved link is then extracted and summarized (by the LLMs) to create signal document of the corresponding topic. Benchmark Creation This section describes the construction process of each of the four test scenarios. Aside from the standard-RAG test, each of the other three settings is evaluated across multiple degrees of noise, specified by the variable - the percentage of signal documents in the retrieved context. Standard-RAG Test In this setting, the retrieved context consists of predefined number of signal documents, which are sampled from the signal set. The LLMs are instructed, using the prompt in Fig. 4, to first generate their step-by-step reasoning, before outputting their answer option. Sufficiency Test For each question, we sample both signal documents and irrelevant noise documents to create retrieval set with in [0, 20, 40, 60, 80, 100]. Given the mixed context, LLMs are prompted, according to Fig. 5, to answer the question with an additional Insufficient Information option. The LLMs are instructed to detect noise documents first, before proceeding with their step-by-step reasoning based only on the relevant documents. Integration Test complex medical question may requires LLMs to address multiple sub-problems first before attempting to solve the main question. we measure this ability of LLMs by generating sub-question based on each signal document, according to the data generation prompt in Fig. 6. The LLMs are instructed, according to the prompt in 1https://developers.google.com/custom-search/v1/overview Figure 11: Sufficiency test main question accuracy. Figure 12: Sufficiency test percentage of information insufficiency (histogram) and noise detection rate (line graph). Fig. 7, to find the specific relevant document for each subquestion in the noisy context, then extract the sub-answer from the corresponding document. This test measures LLMs ability to answers an increasing number of sub-questions, and ability to integrate these information in their reasoning to infer the answer to the main question. Robustness Test The robustness test aim to measure LLMs resilient to factual errors, especially those that are adversarially designed to cause misinformation. Based on the sub-questions from the Integration test, we alter both the sub-answer and the corresponding document to create counterfactual example, using the prompt in the Fig. 8. The adversarial answer should semantically contradict the original answer, and the relevant document should be minimumly edited in convincing way. In this test, all documents in retrieved context are relevant, and is the percentage of factually correct documents. The LLMs are instructed, according to the prompt in Fig. 8, to detect documents with factually incorrect information before answering the subquestions and the main question."
        },
        {
            "title": "Experiments",
            "content": "This section assesses various LLMs in the four scenarios of MedRGB, analyzing their reasoning process and discussing key findings from the experimental results. BioASQ PubmedQA MedQA MMLU LLMs GPT-3.5 GPT-4o-mini GPT-4o PMC-LLAMA-13b MEDITRON-70b GEMMA-2-27b Llama-3-70b No Retrieval 77.7 82.9 87.9 64.2 68.8 80.3 82.9 Offline Retrieval Online Retrieval 20 doc 5 doc 87.9 81.2 90.0 85.3 87.4 86.1 64.1 64.6 79.2 74.0 89.2 83.3 89.3 84.6 20 doc 87.2 90.5 90.8 64.6 74.8 88.7 89.3 5 doc 87.2 89.0 87.4 63.9 79.8 88.7 89.3 No Retrieval 49.8 47.0 52.6 55.4 53.0 41.0 59. Offline Retrieval Online Retrieval 20 doc 5 doc 60.6 59.6 61.2 60.8 54.4 59.2 54.6 54.0 46.8 53.4 49.4 52.0 59.2 77.6 20 doc 71.0 71.8 71.2 54.0 47.8 59.0 70.8 5 doc 58.4 60.6 53.2 54.8 58.8 52.6 59.4 No Retrieval 68.3 79.2 89.5 44.5 51.7 71.2 82.9 Offline Retrieval Online Retrieval 20 doc 5 doc 68.4 63.0 80.6 77.1 86.9 83.7 43.7 38.9 62.9 56.0 76.9 69.8 78.3 73.6 20 doc 67.3 79.5 86.9 38.8 57.4 71.7 79. 5 doc 68.0 79.0 84.6 43.4 61.8 75.9 76.1 No Retrieval 76.3 88.3 93.4 49.7 65.3 83.5 85.2 Offline Retrieval Online Retrieval 20 doc 5 doc 74.8 70.3 87.1 84.6 89.1 88.3 48.2 43.7 69.3 65.1 83.6 77.9 83.8 77.6 20 doc 73.0 87.3 90.1 44.0 66.3 82.5 83.4 5 doc 75.7 86.0 89.5 48.4 67.6 82.2 81.8 Table 1: Standard-RAG test accuracy. Evaluation Setting We evaluate wide range of state-of-the-art LLMs using MedRGB. For closed commercial LLMs, we assess both GPT-3.52 and GPT-4o3 from OpenAI. Additionally, we evaluate the recent GPT-4o-mini4, which achieves performance almost comparable to its full-sized counterpart5. In the open-source category, we examine both general LLMs and domain-specific fine-tuned models. The former includes the instruction-tuned Gemma-2-27b6 from Microsoft and Llama-3-70b7 from Meta. For domain-specific models, we include the medical-domain pretrained MEDITRON-70b (Chen et al. 2023) and PMC-Llama-13b (Wu et al. 2023), which are tailored for healthcare applications. Standard-RAG Evaluation Table 1 shows the accuracy of all considered LLMs for 3 settings: the baseline No Retrieval and standard-RAG setting with 5 and 20 signal documents retrieved as context. Results GPT-4o emerges as the the top performer across most settings, demonstrating the positive effects of scaling both parameters and training data. Surprisingly, GPT4o-mini, despite having only 8 billion reported parameters, achieved comparable results to its larger counterpart. Among open-source models, Gemma-2-27b and Llama-370b show strong performance, highlighting the effectiveness of general domain instruction-tuned models in zero-shot settings. In contrast, domain-specific fine-tuned models like PMC-Llama-13 and MEDITRON-70b both yield mixed results. The effectiveness of RAG varies across different factors. Large models with strong internal knowledge, such as GPT4o and Llama-3-70b, benefit less from RAG compared to smaller models like GPT-4o-mini and Gemma-2-27b. While adding more retrieved documents generally improves performance, models with shorter context lengths (e.g., PMCLlama-13b, MEDITRON-70b) struggle to fully utilize this additional information, resulting in only marginal improvements. Analysis Interestingly, the impact of document quantity differs between the two retrieval sources. The search-based 2https://platform.openai.com/docs/models/gpt-3-5-turbo 3https://platform.openai.com/docs/models/gpt-4o 4https://platform.openai.com/docs/models/gpt-4o-mini 5https://openai.com/index/gpt-4o-mini-advancing-costefficient-intelligence/ 6https://huggingface.co/google/gemma-2-27b-it-pytorch 7https://huggingface.co/meta-llama/Meta-Llama-3-70B-"
        },
        {
            "title": "Instruct",
            "content": "online retrieval often performs best with fewer documents, while the offline retrieval using MedCorp typically improves with more documents. This discrepancy likely stems from the nature of each search algorithm and retrieval source. Google Search tends to provide high-quality top results but introduces more noise as the number of results increases. In contrast, retrieving from MedCorp with MedCPT offers more consistently relevant, high-quality documents, potentially more valuable in larger quantities. We also observe slight adverse effect when applying RAG on GPT-4o and Llama-3-70b for MMLU and MedQA. This could be attributed to their strong internal knowledge and potential data leakage issues with more popular datasets. The following evaluations focus on more specific abilities of LLMs in RAG setting. Due to constraint on computation and cost, we will focus on GPT-3.5 and GPT-4o-mini (which is 100x time cheaper than GPT-4o while achieving relatively comparable results.) for commercial LLMs, and the best open-source model LLama-3-70b."
        },
        {
            "title": "Sufficiency Evaluation",
            "content": "We assess the models ability to handle varying levels of noise in retrieved documents and their capacity to recognize when they lack sufficient information to answer question reliably. The results are shown in Fig. 11 and 12. Result We observe that at = 0, the models mostly returns insufficient information. However, there is significant increase in accuracy across all datasets as the signal percentage increased from 0 to 20. This indicates even small amount of signal greatly enhances the models confidence to answer. However, the improvement diminishes as more signal documents are added. Even when the retrieved context contains all signal documents (p = 100), performance is dramatically reduced compared to the standard RAG setting. This suggests that in the standard-RAG test, models may attempt to answer questions even when they are not fully confident or lack sufficient context, which can be an undesirable characteristic for medical application. Llama-3-70b consistently outperforms other models in noise detection across all datasets and settings. Adding More retrieved documents generally leads to improved performance with fewer insufficient information responses due to the increased context. However, this also resulted in decreased noise detection accuracy, as models occasionally misinterpreted noise documents as part of the signal. This highlights the delicate balance between providing enough context for accurate answers and maintaining the ability to discern relevant information from noise. tionally, we measure sub-question accuracy in Fig. 14 with two metrics. One of them is the strict exact-match score for extractive QA task, and the other is more lenient GPTbased score using the prompt from Fig. 10. The intuition for this metric is that, since these are sub-questions, their exact accuracy is not as important. Sub-answers that are relatively accurate and help infer the main answer should also be rewarded. We limit the number of retrieved documents to 10, as this setting required much more context length than previous ones due to the inclusion of the sub-tasks. starts from 20 so that there is at least one signal document to ask subquestion from. Result Compared to the sufficiency evaluation, the introduction of sub-questions significantly improves the main accuracy at [20, 40]. This suggests that sub-questions can be useful, especially when there are more noise documents than signal documents. However, at = 100 in which retrieval context is similar to that of standard-RAG setting, we observe models perform slight worse, indicating the inability to integrate information from sub-task effectively. Regarding the sub-task performance, the exact-match accuracy is relatively low, ranging from 20 to 30 percents. In contrast, the GPT-baseds consistently stay above 80% for all settings. This suggests that aiming to optimize the sub-task performance may not be useful to the main task. Analysis The evaluation demonstrates that the integration of sub-questions can be beneficial, particularly when noise is present. Sub-questions help guide the models in identifying relevant information, thereby improving main accuracy. However, the sub-questions may also restrict models reasoning to only the given questions, potentially limiting their ability to explore other relevant aspects. This is evident in the MMLU and MedQA datasets, where the improvement in main accuracy is less pronounced. While significantly increasing the number of sub-questions/signal documents can address this problem, it can also make the sub-task much harder, which leads to more task failures (e.g., failing to follow instructions, struggling with long context, skipping subquestions, etc.). Robustness Evaluation We assess the models ability to detect and handle misinformation in retrieved documents, as well as their performance in answering questions in the presence of factually incorrect information. Similar to Integration test, Fig 15 provides the main question accuracy, whereas Fig. 16 presents sub-task scores together with models factual error detection rate. Results In general, the presence of misinformation reduces the overall performance of the model on the main task, as we observe an increase in accuracy as the number of factually correct documents gradually increases. However, compared to the performance in the Sufficiency test, we see that the models perform better in the presence of misinformation than with noisy irrelevant documents. This indicates that models are able to leverage information from the adversarial documents to improve their accuracy, which can be Figure 13: Integration test main question accuracy. Figure 14: Integration test sub-question GPT-based score (filled histogram) and exact-match accuracy (shaded histogram). Analysis Fig. 12 shows all models have high noise detection accuracy at low values of p, but this accuracy decreases as the signal percentage rises. In particular, models tend to ignore the documents completely at = 0, relying solely on their internal knowledge. While this makes detecting noise documents much easier, it can lead to insufficient information responses even when the model has the internal knowledge to answer. Surprisingly, when the context consists only of signal documents (p = 100), models struggle to identify them as relevant. We hypothesize that without specific description of relevant documents, the models internal relevant criteria become much stricter when only signal documents are present. In contrast, when there is mix of noise and signal in the retrieved context, models can more effectively infer the criteria for distinguishing between the contents of the noise and signal documents. Finally, we observe some cases where slight performance improvement at [60, 80] compared to the noiseless setting at = 100. This suggests that introducing small amount of noise might be beneficial, similar to the concept of dropout training, where noise helps models generalize better by preventing overfitting to specific patterns. This approach could enhance the models ability to discern relevant information by providing more varied context for evaluation. Integration Evaluation Fig. 13 presents the main question accuracy after models integrate information from answering sub-questions. Addihigh standard. Our results demonstrate that while models can achieve high performance at standard-RAG setting, they still fall short in others. Future works should aim to improve LLMss ability to address these complex but practical scenarios, instead of just focusing on optimizing the target accuracy. Specialized Components for RAG Systems The experiments clearly show that simply relying on LLMs is insufficient for complete reliable medical system. Even the most advanced models struggled with complex integration tasks and were susceptible to noise and misinformation. This highlights the importance of developing specialized modules that can complement LLMs strengths while mitigating their weaknesses. Limitation Compared to prior work, we aims to comprehensively evaluate LLMs with RAG for medical applications in four different practical settings. Consequently, this has led to significant financial and computational demands, forcing us to limit and simplify some aspects of our experiments to ensure manageability. This section outlines what we have and has not been able to address, and suggests promising future directions that can be followed from our findings. Model Architecture: We focus on limited set of model architectures. Future work could explore more efficient architectures (e.g., adapter-based architectures, quantized model, etc.) for medical RAG applications. Additionally, Investigating advanced RAG architectures, such as those proposed in recent literature, could yield further insights. Task Scope: While we only address question answering task, future studies could extend to other medical NLP tasks. Interaction Complexity: Our evaluation used singleturn prompts. Multi-turn interactions could provide more realistic assessment of RAG systems in clinical settings."
        },
        {
            "title": "Conclusion",
            "content": "This paper extends the evaluation of large language models (LLMs) in retrieval-augmented generation (RAG) settings for medical question answering (QA) tasks to crucial aspects of reliable medical AI systems, including sufficiency, integration, and robustness. We create Medical RetrievalAugmented Generation Benchmark (MedRGB) that provides retrieval topics, signal documents, sub-QA pairs, and adversarial documents, for four medical QA datasets. Using MedRGB, we assess wide range of LLMs, including both closed commercial LLMs and open-source models and their reasoning processes in each of the test scenarios. Our experimental results reveal current RAG systems limitations in handling these practical but complex situations. The finding from our analysis provides practical guidelines and future directions for developing more reliable and trustworthy medical RAG systems. Figure 15: Robustness test main question accuracy. Figure 16: Robustness test sub-question gpt-based score (filled histogram), exact-match accuracy (shaded histogram), and factual error detection rate(line graph). problematic for reliable systems. We observe models struggle to identify all instances of misinformation when is low, in which retrieved context is predominantly adversarial documents. Interestingly, GPT-3.5 has the highest factual errors detection rate while being the worst-performing model. Analysis We observe models often fail to detect fake information, resulting in high false positive rate, mostly accepting the misinformation as truth. Accordingly, the subtask score is low for smaller value of as models proceed to answer to sub-questions as if the information in the corresponding documents are factually correct. The fake information sometimes is used in models reasoning, leading to incorrect answers to the main question. This highlights critical area for improvement, as the ability to discern factual inaccuracies is essential for medical RAG systems. Interestingly, GPT-based scores for sub-tasks remain low for lower p, compared to Integration test where they are consistently above 80. This may potentially be an indicator of the presence of misinformation in the retrieved context. Discussion Our comprehensive evaluation of retrieval-augmented generation (RAG) systems for medical question answering highlights several critical insights into the capabilities and limitations of these models. In this section, we discuss these findings and reveals several key insights and potential directions for future research. Evaluation Criteria in Medical Domain For medical applications, all evaluation criteria, including performance, sufficiency, integration, and robustness, must be met to References Chen, H.; Fang, Z.; Singla, Y.; and Dredze, M. 2024a. Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions. arXiv:2402.18060. Chen, J.; Lin, H.; Han, X.; and Sun, L. 2024b. Benchmarking Large Language Models in Retrieval-Augmented Generation. In Wooldridge, M. J.; Dy, J. G.; and Natarajan, S., eds., Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, 1775417762. AAAI Press. Chen, Z.; Hernandez-Cano, A.; Romanou, A.; Bonnet, A.; Matoba, K.; Salvi, F.; Pagliardini, M.; Fan, S.; Kopf, A.; Mohtashami, A.; Sallinen, A.; Sakhaeirad, A.; Swamy, V.; Krawczuk, I.; Bayazit, D.; Marmet, A.; Montariol, S.; Hartley, M.; Jaggi, M.; and Bosselut, A. 2023. MEDITRON70B: Scaling Medical Pretraining for Large Language Models. CoRR, abs/2311.16079. Cui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023. ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. CoRR, abs/2306.16092. Es, S.; James, J.; Espinosa-Anke, L.; and Schockaert, S. 2023. RAGAS: Automated Evaluation of Retrieval Augmented Generation. arXiv:2309.15217. He, Z.; Wang, Y.; Yan, A.; Liu, Y.; Chang, E. Y.; Gentili, A.; McAuley, J. J.; and Hsu, C. 2023. MedEval: MultiLevel, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, 8725 8744. Association for Computational Linguistics. Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Hiesinger, W.; Zakka, C.; Chaurasia, A.; Shad, R.; Dalal, A.; Kim, J.; Moor, M.; Alexander, K.; Ashley, E.; Boyd, J.; Boyd, K.; Hirsch, K.; Langlotz, C.; and Nelson, J. 2023. Almanac: Retrieval-Augmented Language Models for Clinical Medicine. Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski, P.; Joulin, A.; and Grave, E. 2021. Towards Unsupervised Dense Information Retrieval with Contrastive Learning. CoRR, abs/2112.09118. Jin, D.; Pan, E.; Oufattole, N.; Weng, W.; Fang, H.; and Szolovits, P. 2020a. What Disease does this Patient Have? Large-scale Open Domain Question Answering Dataset from Medical Exams. CoRR, abs/2009.13081. Jin, D.; Pan, E.; Oufattole, N.; Weng, W.; Fang, H.; and Szolovits, P. 2020b. What Disease does this Patient Have? Large-scale Open Domain Question Answering Dataset from Medical Exams. CoRR, abs/2009.13081. Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W. W.; and Lu, X. 2019. PubMedQA: Dataset for Biomedical Research Question Answering. In Inui, K.; Jiang, J.; Ng, V.; and Wan, X., eds., Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, 25672577. Association for Computational Linguistics. Jin, Q.; Kim, W.; Chen, Q.; Comeau, D. C.; Yeganova, L.; Wilbur, W. J.; and Lu, Z. 2023. MedCPT: Contrastive Pretrained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval. Bioinform., 39(10). Krithara, A.; Nentidis, A.; Bougiatiotis, K.; and Paliouras, G. 2023. BioASQ-QA: manually curated corpus for Biomedical Question Answering. Scientific Data, 10: 170. Lewis, P. S. H.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Kuttler, H.; Lewis, M.; Yih, W.; Rocktaschel, T.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Lu, Z. 2011. PubMed and beyond: survey of web tools for searching biomedical literature. Database J. Biol. Databases Curation, 2011. Nori, H.; King, N.; McKinney, S. M.; Carignan, D.; and Horvitz, E. 2023. Capabilities of GPT-4 on Medical Challenge Problems. CoRR, abs/2303.13375. Peng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y.; Hu, Y.; Huang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. CoRR, abs/2302.12813. Ram, O.; Levine, Y.; Dalmedigos, I.; Muhlgay, D.; Shashua, In-Context A.; Leyton-Brown, K.; and Shoham, Y. 2023. Trans. Assoc. Retrieval-Augmented Language Models. Comput. Linguistics, 11: 13161331. Robertson, S. E.; and Zaragoza, H. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr., 3(4): 333389. Wang, J.; Yang, Z.; Yao, Z.; and Yu, H. 2024. JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability. arXiv:2402.17887. Wu, C.; Lin, W.; Zhang, X.; Zhang, Y.; Wang, Y.; and Xie, W. 2023. PMC-LLaMA: Towards Building Open-source Language Models for Medicine. arXiv:2304.14454. Xiong, G.; Jin, Q.; Lu, Z.; and Zhang, A. 2024. Benchmarking Retrieval-Augmented Generation for Medicine. CoRR, abs/2402.13178. Zhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li, Y.; Chen, S. S.; Zhou, P.; Liu, J.; Hua, Y.; Mao, C.; You, C.; Wu, X.; Zheng, Y.; Clifton, L.; Li, Z.; Luo, J.; and Clifton, D. A. 2023. Survey of Large Language Models in Medicine: Progress, Application, and Challenge. arXiv:2311.05112."
        },
        {
            "title": "Corpus\nPubMed\nStatPearls\nTextbooks\nWikipedia",
            "content": "Number of Docs Number of Snippets Average Length 23.9 301.2 125.8 29.9 M"
        },
        {
            "title": "23.9 M\n9.3 k\n18\n6.5 M",
            "content": "296 119"
        },
        {
            "title": "Domain\nBiomedical\nClinics\nMedicine\nGeneral",
            "content": "LLMs GPT-3.5-turbo GPT-4o-mini GPT-4o PMC-Llama-13b MEDITRON-70b Gemma-2-27b Llama-3-70b Availability Knowledge Cutoff Number of Parameters Context Length Domain General General General Medical Medical General General 20 billions* 8 billions* 200 billions* 13 billions 70 billions 27 billions 70 billions Sep, 2021 Oct, 2023 Oct, 2023 Sep, 2023 Aug, 2023* June, 2024* Dec, 2023 16384 128000 128000 2048 4096 4096 8192 Closed Closed Closed Open Open Open Open Table 2: MedCorp coporas statistics (adapted from (Xiong et al. 2024)). Table 3: Statistics of the LLMs used in our experiments. Numbers with * are reported but not confirmed."
        },
        {
            "title": "Experiment Details",
            "content": "Offline Retrieval In our Offline Retrieval process, dense retriever model (MedCPT) is used to query the offline corpus (MedCorp) for relevant documents. MedCPT (Jin et al. 2023) contrastively pretrained biomedical embedding model, consisting of Query Encoder8 and Article Encoder9. MedCorp (Xiong et al. 2024) composes MedCorp which is the combination of four individual copora: Wikipedia large-scale open-source encyclopedia. The process data are downloaded from Huggingface10. Textbooks (Jin et al. 2020b) 18 popular reference textbooks for United States Medical Licensing Examination (USLME). StatPearls (Xiong et al. 2024) 9,330 publicly available StatPearl articles from NCBI Bookshelf11. Pubmed subset of Pubmed (Lu 2011) with 23.9 million articles with valid titles and abstracts. Each corpus is chunked into short snippets for retrieval, with statistics shown in Fig 2 Online Retrieval Our Online Retrieval process follows similar process in ResearchGPT repository12. First, we perform web searching using each retrieval topic from previous step as query to the Google Custom Search JSON API13. The search engine will return (unordered) list of links of web pages with high relevancy score (the page score is determined by number of factors such as quality of pages contents, popularity of the pages, how many other sites link to the page). The returned links are then further assessed by GPT-4o, using the prompt from Fig. 21, to be ranked and sorted in order of relevance to the given main question. Finally, the content from each chosen links are scraped, extracted and summarized by GPT-4o, according to the prompt from Fig. 22, into signal document containing only the most important information to the main question. Retrieval Context Composition: for each question from the medical QA datasets, we first determine number of signal and noise documents needed, based on the total number of documents and the value of p. Then, the signal documents are randomly sampled from set of the signal set of that question. Similarly, the noise documents are randomly sampled 8https://huggingface.co/ncbi/MedCPT-Query-Encoder 9https://huggingface.co/ncbi/MedCPT-Article-Encoder 10https://huggingface.co/datasets/wikipedia 11https://www.ncbi.nlm.nih.gov/books/NBK430685/ 12https://github.com/pbj224/ResearchGPT 13https://developers.google.com/custom-search/v1/overview from set of all signal documents from other questions. The retrieval context are then composed by gathering the ids and contents of the signal and noise documents, in randomized order. LLMs Details Table 3 presents the details of the LLMs evaluated in the paper. For closed commercial LLMs (GPT-3.5-turbo, GPT-4o, GPT-4o-mini), we query responses from the models using OpenAI Chat Completions API14, with temperatures set to 0 for deterministic outputs. Open source models (PMC-Llama-13b, MEDITRON-70b, Gemma-2-27b, and Llama-3-70b) are run using 2 NVIDIA A100 80GB GPUs. PyTorch 2.1.215 and HuggingfaceTransformer 4.42.3 16 are used to implement the models. Source code with specification of all dependencies, including external libraries: Our data and source code will be released upon acceptance of the paper. Experimental Results We provides the full tables of our experimental results for the Sufficieny test, the Integration test, and the Robustness test in Tables 4, 5, and 6, respectively. Step-by-step Reasoning Examples In Fig. 17, 18, and 19, we presents examples of models step-by-step reasoning process in the Sufficiency test, the Integration test, and the Robustness test, respectively. Prompt Templates We provide all of the prompt templates in full version used in our experiments. Fig. 20 presents the prompt use to generate retrieval topic. Fig. 21 and 22 presents the prompts used in the Online Retrieval process. Fig. 25 are the full prompt used in the inference process of Sufficiency test. Fig. 26 and 27 are the full prompts used in the benchmark creation and inference processes of Integration test. Fig. 28 and 29 are the full prompts used in the benchmark creation and inference processes of Robustness test. 14https://platform.openai.com/docs/guides/text-generation 15https://pytorch.org/get-started/pytorch-2.0/ 16https://github.com/huggingface/transformers 5 doc Main Acc GPT-3.5 GPT-4o-mini Llama-3-70b Noise Acc GPT-3.5 GPT-4o-mini Llama-3-70b Num Insuf (%) GPT-3.5 GPT-4o-mini Llama-3-70b 20 doc Main Acc GPT-3.5 GPT-4o-mini Llama-3-70b Noise Acc GPT-3.5 GPT-4o-mini Llama-3-70b Num Insuf GPT-3.5 GPT-4o-mini Llama-3-70b"
        },
        {
            "title": "MMLU",
            "content": "0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 64.1 10.2 80.0 9.4 75.6 6.0 66.8 77.0 74.3 63.0 60.6 60.2 63.0 57.6 58.2 59.6 51.8 52.0 56.8 51.2 49. 43.8 54.6 56.0 48.6 68.1 63.2 51.9 72.4 66.3 53.6 72.6 67.6 55.2 74.0 69.1 55.3 73.3 70. 40.9 43.5 40.5 57.5 66.5 65.5 61.6 72.5 73.1 64.8 75.9 74.8 61.5 60.8 54.1 50.6 35.2 34. 70.2 70.9 67.5 75.4 76.5 74.3 77.2 80.6 78.3 76.9 81.6 80.1 7.8 0.8 0.2 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 48.8 78.4 46.6 94.5 56.0 97. 93.0 91.2 93.9 82.5 82.5 90.8 68.5 73.1 81.0 52.9 62.9 64.7 74.6 93.8 96.7 96.5 80.0 93. 90.7 68.9 89.8 76.7 58.1 85.2 63.3 49.2 75.1 46.4 50.1 62.0 72.5 99.1 96.7 94.9 84.0 94. 91.4 70.4 88.1 80.0 59.7 81.8 58.3 61.7 67.9 78.0 77.1 75.0 99.2 98.0 99.5 99.2 99.0 99. 91.5 85.8 93.9 83.7 80.5 89.8 71.2 72.8 79.6 65.1 50.9 71.2 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 82.2 90.0 93.2 24.4 31.7 26. 6.7 10.1 4.6 40.2 52.4 52.7 11.9 20.6 15.5 5.1 13.3 8.6 2.6 2.8 14.0 5.6 14.2 36. 5.3 6.8 11.3 16.5 25.9 34.8 83.8 97.2 99.2 5.7 8.9 13.9 7.8 14.2 21.0 1.8 1.8 4. 2.8 3.6 3.4 2.4 1.8 3.1 2.7 1.2 2.3 4.3 7.7 7.6 3.3 7.1 6.3 1.9 5.1 5. 3.8 2.2 8.2 2.2 1.4 6.4 5.2 6.2 9.9 2.3 1.1 1."
        },
        {
            "title": "MMLU",
            "content": "0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 66.8 20.6 84.3 16.8 78.3 7.6 67.9 83.6 83.8 70.4 69.0 40.2 68.0 69.0 42.2 64.8 66.4 51.2 62.8 64.8 53. 48.2 73.4 74.2 55.1 74.0 72.6 55.8 72.4 70.3 56.1 74.6 65.9 57.1 76.1 72.7 59.1 76.8 71. 32.1 73.7 55.6 66.1 79.6 78.2 67.1 78.7 80.1 67.2 81.6 80.0 76.9 75.6 73.0 58.6 54.2 55. 76.4 84.5 65.2 79.6 85.8 66.7 79.9 85.9 73.5 81.9 85.3 68.5 11.2 2.0 3.4 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 20.6 62.6 13.4 81.3 39.3 95. 36.1 41.4 33.5 59.0 32.2 61.2 44.4 36.9 46.0 48.5 38.1 53.4 61.8 88.2 82.3 68.0 22.4 77. 55.2 17.0 65.5 43.4 17.4 56.9 33.3 15.9 54.8 18.7 12.5 40.3 69.6 87.6 91.1 73.1 32.0 84. 61.1 24.1 70.6 45.9 21.0 63.9 74.2 51.1 77.4 74.2 47.5 77.7 57.6 39.0 57.9 51.5 40.3 55. 45.5 42.2 61.6 39.5 45.5 48.1 74.4 24.1 91.2 34.3 15.2 58.7 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 66.3 79.1 85.3 17.3 3.0 3. 53.3 15.9 35.5 74.2 82.8 80.6 1.6 0.5 0.2 1.7 0.4 0.3 1.0 0.3 0.2 2.3 0.9 0. 0.9 0.5 0.3 0.4 0.6 0.2 0.2 0.2 0.2 0.0 0.2 0.0 2.9 1.4 2.4 1.9 1.5 1. 1.7 1.0 1.5 1.6 1.2 2.0 1.3 1.5 1.3 1.9 1.5 1.5 2.6 2.8 3.7 2.1 1.3 1. 1.6 0.6 0.8 0.6 0.2 0.0 4.2 2.1 2.9 1.3 1.6 1.3 Table 4: Sufficiency test full results table, including main question accuracy, noise detection accuracy, and number of insufficient information response (in percentage of dataset). 5 doc Main Acc GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (exact) GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (gpt) GPT-3.5 GPT-4o-mini Llama-3-70b 10 doc Main Acc GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (exact) GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (gpt) GPT-3.5 GPT-4o-mini Llama-3-70b"
        },
        {
            "title": "MMLU",
            "content": "0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 66.9 82.5 75.7 67.8 81.3 74.7 58.6 55.0 57.6 52.4 52.0 53.0 45.2 40.6 35.8 72.2 78.2 72. 82.9 85.6 84.8 66.3 73.0 59.4 60.6 57.2 61.2 63.4 60.2 63.2 57.3 72.2 66.5 55.9 72.7 68. 55.7 72.9 68.1 56.3 73.1 68.7 56.4 72.6 70.1 66.0 80.5 71.9 66.8 81.7 74.0 68.5 81.7 75. 79.0 83.5 82.7 78.2 82.4 79.9 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 30.1 25.7 28.8 29.9 24.0 27.7 32.9 29.2 33.6 28.2 21.8 26. 29.6 25.2 27.3 30.6 26.3 29.6 33.0 29.6 35.2 30.8 25.4 31.1 31.7 27.9 33.1 31.0 26.3 30. 31.4 27.6 31.3 31.7 28.2 32.1 33.2 28.9 32.6 28.2 21.7 23.6 29.0 23.3 26.3 29.8 24.0 27. 29.1 25.0 28.8 28.6 23.8 27.3 26.9 21.0 24.9 28.4 25.6 29.4 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 79.4 80.3 81.0 80.0 79.9 80. 82.5 82.6 83.2 80.3 82.4 80.7 80.9 81.3 80.2 81.6 82.1 82.9 82.6 82.8 83.5 82.0 81.3 82. 82.4 81.9 82.9 80.2 81.3 81.3 81.1 81.9 82.0 81.6 82.4 82.4 81.3 82.1 82.9 81.8 82.2 82. 78.6 79.0 80.0 79.4 79.9 80.8 79.8 80.1 81.1 79.8 81.6 80.4 80.9 81.7 81.0 80.9 80.4 80."
        },
        {
            "title": "MMLU",
            "content": "0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 68.3 84.1 80.8 68.9 82.4 78.2 80.3 83.8 83.0 83.8 89.6 89.6 65.0 62.6 65.0 67.6 65.6 68. 55.7 73.1 71.2 62.8 58.6 63.6 69.0 66.2 69.4 55.0 73.7 70.7 58.3 74.2 72.4 58.6 75.2 74. 59.6 74.0 74.0 67.2 82.4 75.8 67.9 82.3 77.1 67.6 82.2 78.5 83.2 89.0 89.2 82.7 86.1 84. 73.5 79.5 74.0 56.6 51.6 54.2 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 30.7 27.2 30.8 30.0 26.8 29.8 28.0 21.2 26.0 29.0 25.2 27. 32.8 28.3 34.0 27.9 26.1 28.7 27.9 26.8 31.0 30.8 25.6 30.6 33.1 30.5 36.3 32.8 30.9 37. 33.4 32.9 38.9 30.8 25.7 30.7 31.5 27.4 32.1 31.5 28.7 32.4 32.1 30.1 33.4 32.8 30.3 33. 28.2 23.1 25.9 29.4 24.0 27.5 29.3 25.9 29.3 27.2 25.9 29.8 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 79.0 80.4 81.3 77.7 80.2 80. 80.6 81.6 82.5 79.2 81.6 79.8 81.1 81.7 81.9 78.2 80.7 80.6 81.1 82.4 83.2 81.8 81.5 82. 81.2 82.3 82.8 81.0 81.8 82.1 80.5 82.0 82.6 80.4 82.1 82.8 81.4 82.2 83.1 78.5 79.6 79. 78.4 79.7 79.9 77.8 79.8 80.4 75.9 80.2 79.7 76.2 80.7 80.1 80.3 81.0 79.8 82.2 82.6 82. Table 5: Integration test full results table, including main question accuracy and sub question accuracy (exact-match and GPT-based). 5 doc Main Acc GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (exact) GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (gpt) GPT-3.5 GPT-4o-mini Llama-3-70b Fact Detect GPT-3.5 GPT-4o-mini Llama-3-70b 10 doc Main Acc GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (exact) GPT-3.5 GPT-4o-mini Llama-3-70b Sub Acc (gpt) GPT-3.5 GPT-4o-mini Llama-3-70b Fact Detect GPT-3.5 GPT-4o-mini Llama-3-70b BioASQ PubmedQA MedQA MMLU 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 65.8 63.3 81.4 70.6 76.0 68.3 65.8 80.9 73.9 48.2 48.4 49.4 53.3 71.1 66.4 53.3 71.9 69. 55.2 72.6 70.2 56.7 71.4 71.9 60.1 80.4 69.9 61.8 80.5 72.9 62.4 80.9 71.9 64.5 80.6 75. 50.4 71.4 67.3 51.7 70.8 67.1 54.6 50.2 51.4 56.6 53.6 57.0 64.4 59.4 62.8 72.3 78.5 75. 77.0 84.3 81.4 79.8 85.3 84.0 41.6 40.8 42.2 76.2 81.1 80.6 45.2 45.4 44.8 67.8 76.1 70. 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 34.4 0.7 27.0 0.9 33.7 0.8 36.3 27.0 35.7 46.1 35.0 45.9 38.4 31.9 37.8 27.7 21.0 27.8 17.9 13.2 18. 15.8 12.6 15.6 24.1 19.7 23.8 14.0 11.0 13.9 21.5 17.3 20.9 14.1 10.7 14.0 28.5 22.5 28. 23.4 17.1 20.9 35.5 27.9 35.1 30.8 25.4 30.1 27.5 21.5 26.9 0.2 0.3 0.2 9.2 7.1 9. 0.3 0.8 0.7 8.7 6.9 8.7 0.3 1.1 0.9 7.7 5.9 8.1 8.2 6.2 8.2 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 80.0 4.5 82.4 9.1 83.4 6. 81.3 83.4 83.8 65.2 66.9 67.6 82.1 84.6 85.2 33.1 35.0 34.6 50.5 52.0 51.5 18.8 23.1 21. 34.5 38.6 37.6 50.1 54.5 53.2 18.5 23.1 21.8 33.3 37.9 37.2 49.7 53.3 52.6 17.9 19.9 19. 20.4 24.9 22.9 33.8 38.6 36.3 50.3 53.8 52.0 64.0 67.2 66.2 79.7 82.0 82.0 66.0 69.6 68. 64.7 67.8 67.5 2.0 6.9 4.6 2.5 8.0 6.0 1.8 3.0 2.6 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 92.0 28.8 95.0 13.6 99.3 8.3 45.2 33.1 27. 55.1 50.0 44.6 67.7 66.7 63.5 88.1 96.8 99.5 76.0 81.4 80.1 78.7 80.6 81.1 94.4 98.2 99. 49.6 48.0 45.2 64.4 64.6 63.3 33.4 29.5 27.8 77.5 79.9 82.0 93.2 94.7 99.5 63.8 65.6 64. 37.0 33.9 32.3 50.6 49.5 49.0 16.2 14.4 13.9 36.3 35.2 32.4 51.1 49.8 49.7 64.5 66.0 65. 79.0 79.4 82.3 17.9 14.3 13.2 15.3 10.0 8.2 BioASQ PubmedQA MedQA MMLU 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 66.2 68.8 81.8 75.1 79.9 73.1 66.8 82.3 78.0 57.4 55.6 57.2 61.0 58.2 63.8 62.2 61.4 67. 66.0 68.2 69.6 52.0 71.2 69.3 52.4 72.1 70.0 54.5 72.2 71.8 56.1 73.5 72.7 57.0 71.8 73. 60.7 73.0 72.9 59.5 79.7 73.1 63.5 79.9 74.3 62.2 80.0 75.9 63.0 81.9 77.4 43.8 44.6 47. 47.8 48.6 50.8 84.8 89.6 89.2 72.3 81.7 79.3 77.5 82.5 82.0 83.2 85.6 85.6 82.2 89.3 88. 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 34.4 1.9 28.0 2.4 33.6 2.7 43.7 32.6 44.4 34.9 25.8 35.2 38.5 33.1 38.4 18.0 13.5 18.9 26.6 19.7 27. 15.8 13.2 15.9 23.6 19.9 23.2 14.7 11.7 14.5 21.4 17.0 20.9 15.6 12.5 15.6 9.9 7.7 10. 23.0 17.5 22.4 28.9 21.4 27.4 35.4 26.8 34.1 30.2 26.0 30.5 27.2 22.4 26.2 0.4 1.2 0. 8.1 6.9 8.5 0.4 1.3 0.9 7.6 6.2 7.7 9.1 7.0 8.9 1.2 1.1 1.4 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 80.2 8.3 82.0 14.0 82.8 11. 22.7 28.9 26.0 36.4 42.5 40.4 52.0 55.9 54.6 78.7 81.0 81.1 65.1 67.8 67.3 80.3 83.5 83. 64.7 67.6 67.4 81.8 83.9 84.4 33.5 37.0 36.2 49.6 52.2 51.2 18.8 24.1 21.7 34.2 39.2 37. 50.3 54.2 53.0 18.6 23.8 21.9 33.5 38.5 37.4 49.2 53.0 52.3 19.3 22.7 21.0 65.3 68.6 68. 64.0 67.2 66.8 2.6 8.2 5.6 3.0 8.7 6.5 3.9 5.9 4.9 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% 91.1 28.2 89.8 14.3 99.4 7.8 52.6 48.8 44. 63.9 63.6 61.3 90.2 94.6 99.6 42.2 35.3 26.0 73.2 73.9 76.9 75.7 72.6 77.4 94.5 94.8 99. 61.4 60.1 60.5 45.4 44.7 42.8 32.7 31.4 25.4 92.1 88.1 99.4 74.4 72.2 80.9 16.6 15.2 13. 34.9 36.1 31.6 47.8 47.4 48.9 61.4 61.5 64.9 75.8 70.0 82.0 18.4 13.7 12.8 34.9 33.8 31. 47.4 46.2 47.8 61.0 61.4 64.4 17.6 12.4 7.9 Table 6: Robustness test full results table, including main question accuracy, sub question accuracy (exact-match and GPT-based), and factual error detection rate. Figure 17: Example of models step-by-step reasoning process in Sufficiency test. With all-noise context, the model is refuse to answer the question, despite having the internal knowledge to do it, as shown in the without-retrieval example. In case of all-signal context, the model has difficulty in accepting all documents are relevant, as the signal criteria become stricter. Figure 18: Example of models step-by-step reasoning process in Integration test. We see that the model is able to integrate information from multiple sub-questions to arrive at the correct answer. In the other hand, not having enough sub-questions may restricts the model reasoning scope, leading to incorrect answer. Figure 19: Example of models step-by-step reasoning process in Robustness test. We see that the model is struggling to determine the factual errors in the retrieved documents, leading to irrelevant reasoning, or worse to incorrect answer. You are an expert in medical research and information retrieval. Your task is to generate search queries to answer multi-choice medical question. Follow these guidelines: 1. **Rank by Importance**: The queries must be ranked by importance with respect to the question, with the first one being the most important. 2. **Relevance to Question**: Each query may ask for information regarding the answer options but must always be relevant to the question. 3. **Differentiable and Efficient**: The queries must be differentiable and efficient, ensuring that the aggregate retrieved information from all queries provides as much as possible the needed information to arrive at the correct answer. **Example** INPUT: { \"question\": \"Which of the following is the most common cause of acute pancreatitis?\", \"options\": { \"A\": \"Gallstones\", \"B\": \"Alcohol abuse\", \"C\": \"Hypertriglyceridemia\", \"D\": \"Medications\" }, \"number_of_searches\": 3 } OUTPUT: { \"number_of_searches\": 3, \"search_queries\": { \"1\": \"most common cause of acute pancreatitis\", \"2\": \"acute pancreatitis gallstones alcohol abuse\", \"3\": \"acute pancreatitis hypertriglyceridemia medications\" }, \"search_query_goals\": { \"1\": \"What is the most common cause of acute pancreatitis?\", \"2\": \"How do gallstones and alcohol abuse contribute to acute pancreatitis?\", \"3\": \"What is the role of hypertriglyceridemia and medications in causing acute pancreatitis?\" } } search_query_goal should always be phrased as question. The output must follow the JSON format as in the above example. Figure 20: Retrieval topic generation prompt (full version). You are helpful, pattern-following assistant. Output in the format of python list the order in which of following links are most likely to best answer the query: { query } Answer with list of ints only. Example output formatting (order is random. Use as formatting example only. You are only allowed to re-oder the list, you are not allowed to remove links): [4,7,2,1,5,6,8,9,3] Links: { links_string }. Figure 21: Page link ordering prompt searchGPT). (adapted from ReYou are helpful, pattern-following assistant. You are given some text retrieved from website and research query and you generate very detailed and comprehensive summary of only the parts of the text relevant and useful to the answering the research query. Include as much detail as is physically possible. You only answer using the following JSON format and strictly follow JSON formatting conventions: { \"is_relevant\" : boolean, #true if the provided text provides information relevant to answering the users query, false if the text irrelevant to the query or just discusses access denial to webpage \"summary\": \"string\" #very detailed (but not wordy) summary of the key information in the text if is_relevant is true (squeeze as much info into as few characters as you can) } Only output one JSON summary. Figure 22: Page summary prompt (adapted from ResearchGPT). You are helpful medical expert, and your task is to answer multi-choice medical question. Please first think step-by-step and then choose the answer from the provided options. Organize your output in json formatted as Dict{\"step_by_step_thinking\": Str(explanation), \"answer_choice\": Str{A/B/C/...}}. Your responses will be used for research purposes only, so please have definite answer. Here is the question: { question } Here are the potential choices: { options } Please think step-by-step and generate your output in json: Figure 23: No Retrieval Inference Prompt (Adapted from (Xiong et al. 2024)). You are helpful medical expert, and your task is to answer multi-choice medical question using the relevant documents. Please first think step-by-step and then choose the answer from the provided options. Organize your output in json formatted as Dict{\"step_by_step_thinking\": Str(explanation), \"answer_choice\": Str{A/B/C/...}}. Your responses will be used for research purposes only, so please have definite answer. Here are the relevant documents: { context } Here is the question: { question } Here are the potential choices: { options } Please think step-by-step and generate your output in json: Figure 24: Standard-RAG Inference Prompt (Adapted from (Xiong et al. 2024)). You are knowledgeable medical expert. Your task is to answer multiple-choice medical question using the provided external documents. Note that some of these documents may be noisy or irrelevant to the question. Instructions: 1. Carefully analyze all provided documents. 2. Identify which documents are relevant and which are irrelevant to the question. 3. Think through the problem step-by-step, using only the relevant documents to determine the correct answer. 4. Organize your output in JSON format with the following structure: { } \"relevant_doc_id\": {list of relevant document IDs}, \"irrelevant_doc_id\": {list of irrelevant document IDs}, \"step_by_step_thinking\": \"Detailed explanation of your reasoning process\", \"answer_choice\": \"Selected option (A/B/C/D/etc.)\" **Example** INPUT: Here are the retrieved documents: --- Start of DOC_1 --- ID: doc_1_id Content: Common symptoms of type 2 diabetes include increased thirst, frequent urination, and unexplained weight loss. --- END of DOC_1 --- --- Start of DOC_2 --- ID: doc_2_id Content: Joint pain is common symptom of rheumatoid arthritis but is not typically associated with diabetes. --- END of DOC_2 --- --- Start of DOC_3 --- ID: doc_3_id Content: Diabetic neuropathy can cause nerve pain, but this is different from joint pain. --- END of DOC_3 --- --- Start of DOC_4 --- ID: doc_4_id Content: The Earth's atmosphere is composed primarily of nitrogen and oxygen. --- END of DOC_4 --- Here is the question: Which of the following is NOT typical symptom of type 2 diabetes? Here are the potential choices: A. Increased thirst B. Frequent urination C. Unexplained weight loss D. Joint pain OUTPUT: { \"relevant_doc_id\": [\"doc_1_id\", \"doc_2_id\", \"doc_3_id\"], \"irrelevant_doc_id\": [\"doc_4_id\"], \"step_by_step_thinking\": \"DOC_4 is irrelevant as it discusses atmospheric composition. DOC_1 lists increased thirst, frequent urination, and unexplained weight loss as common symptoms of type 2 diabetes. DOC_2 explicitly states that joint pain is not typically associated with diabetes. DOC_3 mentions nerve pain in diabetes but distinguishes it from joint pain. Based on this information, joint pain is the only option that is NOT typical symptom of type 2 diabetes.\", \"answer_choice\": \"D\" } Your responses will be used for research purposes only, so please provide definite answer and format the output as JSON object as instructed. If all of the retrieved documents are irrelevant, you have two options: 1. Answer based on your own knowledge if you are absolutely certain. 2. Refuse to answer by setting 'answer_choice' to 'insufficient information'. Here are the relevant documents: { context } Here is the question: { question } Here are the potential choices: { options } Please think step-by-step and generate your output in json: Figure 25: Sufficiency test inference prompt (full version). You are an expert in medical research and information retrieval. Given an initial medical question MAIN_QUESTION and set of relevant documents DOC_1, ..., your task is to ask sub questions to each document to help find the answer to the MAIN_QUESTION. Follow these guidelines: For the i_th document DOC_i from the list of the provided DOCUMENTS, generate sub Q-A pair (SUB_QUESTION_i, SUB_ANSWER_i) that satisfies the following criteria: - Each Q-A pair should explore different but related aspect to the MAIN_QUESTION - QUESTION_i must be about information that is only contained in DOCUMENT_i. - SUB_ANSWER_i must be very short string of at most 4 to 5 words extracted directly from DOCUMENT_i. Provide definite, factual answer to the new question you generate. Do not express uncertainty. **Example** INPUT: { \"MAIN_QUESTION\": \"What are the main treatments for type 2 diabetes?\", \"DOC_1\": \"Metformin is often the first medication prescribed for type 2 diabetes. It works by improving the way your body handles insulin to control blood sugar levels. Metformin also has the advantages of being inexpensive and not causing weight gain. In some cases, other medications like sulfonylureas or insulin may be added if metformin alone is not sufficient.\", \"DOC_2\": \"Lifestyle changes are crucial in managing type 2 diabetes. Regular exercise can help lower blood sugar levels and improve insulin sensitivity. Aim for at least 150 minutes of moderate-intensity aerobic activity or 75 minutes of vigorous aerobic activity week, spread over at least three days. Additionally, balanced diet rich in whole grains, fruits, and vegetables is recommended.\" } OUTPUT: { \"DOC_1\": { \"SUB_QUESTION_1\": \"What is the primary medication prescribed for type 2 diabetes?\", \"SUB_ANSWER_1\": \"Metformin\" }, \"DOC_2\": { \"SUB_QUESTION_2\": \"What type of lifestyle change is crucial for managing type 2 diabetes?\", \"SUB_ANSWER_2\": \"Regular exercise\" } } The output must be formatted as JSON object, with every document (DOC_i) having its own sub-object containing the generated sub-question and very concise sub-answer of 4 to 5 words maximum, directly extracted from the corresponding document. Figure 26: Integration test benchmark creation prompt (full version). You are knowledgeable medical expert. Your task is to answer multiple-choice medical question together with series of related sub-questions, using the provided external documents. The sub-questions are designed to support answering the main question. Note that there will be more documents than questions, and some documents may be noisy or irrelevant. Instructions: 1. Carefully analyze all provided documents. 2. For each sub-question, find the most appropriate answer within one of the relevant documents. 3. Extract the answer as concise, relevant string from the document. 4. Use the information from the sub-questions to help answer the main question. 5. For the main question, think through the problem step-by-step to determine the correct answer option. 6. Organize your output in JSON format with the following structure: { \"sub_1\": { \"question\": \"Question string\", \"answer\": \"Extracted answer string\", \"doc_id\": \"ID of the document containing the answer\"}, # ... and so on for each sub-question \"main\": { \"question\": \"Question string\", \"step_by_step_thinking\": \"Detailed explanation of your reasoning process, including how the sub-questions support the answer\", \"answer_choice\": \"Selected option (A/B/C/D/etc.)\"} } **Example** INPUT: Here are the retrieved documents: --- Start of DOC_1 --- ID: doc_1_id Content: Type 2 diabetes is characterized by insulin resistance. Common symptoms include increased thirst, frequent urination, and unexplained weight loss. --- END of DOC_1 --- --- Start of DOC_2 --- ID: doc_2_id Content: Treatment for type 2 diabetes typically involves lifestyle changes, monitoring blood sugar levels, and sometimes medication or insulin therapy. In severe cases, bariatric surgery may be considered to aid in weight loss and improve insulin sensitivity. --- END of DOC_2 --- --- Start of DOC_3 --- ID: doc_3_id Content: The Earth's atmosphere is composed primarily of nitrogen and oxygen. --- END of DOC_3 --- --- Start of DOC_4 --- ID: doc_4_id Content: Lifestyle changes for managing type 2 diabetes include regular exercise, maintaining healthy diet, and weight management. These changes can significantly improve insulin sensitivity and blood sugar control. --- END of DOC_4 --- Here is the main question: Which of the following is considered first-line treatment approach for type 2 diabetes? Here are the potential choices: A. Bariatric surgery B. Insulin therapy C. Lifestyle changes D. Kidney transplant Here are the sub-questions: 1. What are the typical treatment approaches for type 2 diabetes? 2. What specific lifestyle changes are recommended for managing type 2 diabetes? OUTPUT: { \"sub_1\": { \"question\": \"What are the typical treatment approaches for type 2 diabetes?\", \"answer\": \"lifestyle changes, monitoring blood sugar levels, and sometimes medication or insulin therapy\", \"doc_id\": \"doc_2_id\"}, \"sub_2\": { \"question\": \"What specific lifestyle changes are recommended for managing type 2 diabetes?\", \"answer\": \"regular exercise, maintaining healthy diet, and weight management\", \"doc_id\": \"doc_4_id\"}, \"main\": { \"question\": \"Which of the following is considered first-line treatment approach for type 2 diabetes?\", \"step_by_step_thinking\": \"1. From sub-question 1, we learn that typical treatments include lifestyle changes, monitoring blood sugar levels, and sometimes medication or insulin therapy. 2. Sub-question 2 provides specific examples of lifestyle changes, which are regular exercise, maintaining healthy diet, and weight management. 3. Comparing this information with the given options: A) Bariatric surgery - only for severe cases, not first-line. B) Insulin therapy - mentioned as 'sometimes' used, not necessarily first-line. C) Lifestyle changes - mentioned first in the list of treatments and elaborated upon, suggesting it's primary approach. D) Kidney transplant - not mentioned at all in the context of diabetes treatment. 5. Therefore, based on the information provided, lifestyle changes are considered first-line treatment approach for type 2 diabetes.\", \"answer_choice\": \"C\"} } Your responses will be used for research purposes only, so please provide definite answers and format the output as JSON object as instructed. Here are the relevant documents: { context } Here is the main question: { question } Here are the potential choices: { options} Here are the sub-questions: { sub_questions } Please generate your output as JSON object as instructed: Figure 27: Integration test inference prompt (full version). You are knowledgeable medical expert. Given medical question and an answer extracted from relevant document, your task is to generate deliberately incorrect answer-document pair for the question. The pair must satisfy the following requirements: 1. The `NEW_ANSWER` MUST be factual error, strongly contrasting with the `ORIGINAL_ANSWER` to the `QUESTION`. It should be completely made up and factually incorrect. 2. The `NEW_DOC` (400 words maximum) should be modified version of the `ORIGINAL_DOC`, with the `ORIGINAL_ANSWER` replaced by the `NEW_ANSWER`. You MUST use completely made-up, factually incorrect information to support the fake answer. The document should appear coherent and persuasive despite Some contents from `ORIGINAL_DOC` can be ignored to ensure that `NEW_DOC` is being entirely false. within 400 words maximum. Instructions: 1. Carefully analyze the provided question, original answer, and original document. 2. Generate new answer that is deliberately incorrect and strongly contrasts with the original answer. 3. Create new document that supports the incorrect answer using entirely fabricated information. Ensure it appears coherent and persuasive despite being false. 4. Organize your output in JSON format with the following structure: { \"QUESTION\": \"Question string\", \"NEW_ANSWER\": \"New, factually incorrect answer string\", \"NEW_DOC\": \"New document text with fabricated information\" } **Example 1** INPUT: { \"QUESTION\": \"Which of the following is the most common cause of infective endocarditis in intravenous drug users?\", \"ORIGINAL_ANSWER\": \"Staphylococcus aureus\", \"ORIGINAL_DOC\": \"Staphylococcus aureus is the leading cause of infective endocarditis in intravenous drug users. This gram-positive bacterium has high affinity for damaged heart valves, particularly the tricuspid valve. S. aureus enters the bloodstream through contaminated injection equipment and colonizes the endocardium, leading to vegetation formation and valvular destruction. The clinical presentation is often acute, with fever, sepsis, and pulmonary complications.\" } OUTPUT: { \"QUESTION\": \"Which of the following is the most common cause of infective endocarditis in intravenous drug users?\", \"NEW_ANSWER\": \"Probiotic Lactobacillus acidophilus\", \"NEW_DOC\": \"Probiotic Lactobacillus acidophilus is the leading cause of infective endocarditis in intravenous drug users. This beneficial bacterium, commonly found in yogurt, has an unexpected affinity for damaged heart valves, particularly the mitral valve. L. acidophilus enters the bloodstream through probiotic-enriched nutritional supplements often used by drug users to boost their immune system. It then colonizes the endocardium, leading to the formation of protective biofilms and valve regeneration. The clinical presentation is often asymptomatic, with occasional reports of increased energy and improved digestion.\" } **Example 2** INPUT: { \"QUESTION\": \"What percentage of adults worldwide are estimated to have hypertension?\", \"ORIGINAL_ANSWER\": \"31.1%\", \"ORIGINAL_DOC\": \"Hypertension is major global health concern. According to the World Health Organization, an estimated 31.1% of adults worldwide had hypertension in 2021. This condition is defined as having systolic blood pressure >= 140 mmHg and/or diastolic blood pressure >= 90 mmHg. The prevalence of hypertension varies across regions, with higher rates in lowand middle-income countries. Factors contributing to hypertension include age, genetics, diet high in sodium, lack of physical activity, and obesity. \" } OUTPUT: { \"QUESTION\": \"What percentage of adults worldwide are estimated to have hypertension?\", \"NEW_ANSWER\": \"3.7%\", \"NEW_DOC\": \"Hypertension is rare global health phenomenon. According to the World Relaxation Organization, an estimated 3.7% of adults worldwide had hypertension in 2021. This condition is defined as having systolic blood pressure >= 200 mmHg and/or diastolic blood pressure >= 150 mmHg. The prevalence of hypertension varies across regions, with higher rates in high-income countries due to advanced blood pressure detection technology. Factors protecting against hypertension include consuming large quantities of salt, maintaining sedentary lifestyle, and having high body mass index.\" } Your responses will be used for research purposes only. Remember, the goal is to create entirely false but seemingly plausible medical information. Please provide definite answers and format the output as JSON object as instructed. Figure 28: Robustness test benchmark creation prompt (full version). You are knowledgeable medical expert. Your task is to answer multiple-choice medical question together with series of related sub-questions, using the provided external documents. The sub-questions are designed to support answering the main question. Note that each document corresponds to one sub-question, but some documents may contain factual errors. Instructions: 1. Carefully analyze all provided documents. 2. For each sub-question, identify the corresponding relevant document. 3. Determine if the document contains any factual errors. 4. If the document is factually correct, answer the sub-question by extracting concise, relevant string from the document (4 to 5 words maximum). 5. If the document is factually incorrect, answer the sub-question by providing the correct fact (4 to 5 words maximum), without referring to the incorrect information. 6. For the main question, think through the problem step-by-step and use the information from the sub-questions to determine the correct answer option. 7. Organize your output in JSON format with the following structure: { } \"sub_1\": { \"question\": \"Question string\", \"relevant_doc_id\": \"ID of the document relevant to the sub-question\", \"factual_correctness\": \"true/false\", \"answer\": \"Answer to the sub-question as instructed\", }, # ... and so on for each sub question \"main\": { \"question\": \"Question string\", \"step_by_step_thinking\": \"Detailed explanation of your reasoning process, including how the sub-questions support the answer\", \"answer_choice\": \"Selected option (A/B/C/D/etc.)\"} **Example** INPUT: Here are the retrieved documents: --- START of DOC_1 --- ID: doc_1_id Content: Type 2 diabetes is characterized by excessive insulin production. Common symptoms include increased hunger, frequent urination, and unexplained weight gain. --- END of DOC_1 --- --- START of DOC_2 --- ID: doc_2_id Content: Treatment for type 2 diabetes typically involves lifestyle changes, monitoring blood sugar levels, and sometimes medication or insulin therapy. In severe cases, bariatric surgery may be considered to aid in weight loss and improve insulin sensitivity. --- END of DOC_2 --- --- START of DOC_3 --- ID: doc_3_id Content: Lifestyle changes for managing type 2 diabetes include avoiding all carbohydrates, engaging in high-intensity exercise daily, and maintaining very low-calorie diet. These changes can cure diabetes within few months. --- END of DOC_3 --- Here is the main question: Which of the following is considered first-line treatment approach for type 2 diabetes? Here are the potential choices: A. Bariatric surgery B. Insulin therapy C. Lifestyle changes D. Kidney transplant Here are the sub-questions: 1. What characterizes type 2 diabetes? 2. What are typical treatment approaches? 3. What lifestyle changes are recommended? OUTPUT: { \"sub_1\": { \"question\": \"What characterizes type 2 diabetes?\", \"relevant_doc_id\": \"doc_1_id\", \"factual_correctness\": \"false\", \"answer\": \"Insulin resistance, impaired glucose\"}, \"sub_2\": { \"question\": \"What are typical treatment approaches?\", \"relevant_doc_id\": \"doc_2_id\", \"factual_correctness\": \"true\", \"answer\": \"Lifestyle changes, monitoring, medication\"}, \"sub_3\": { \"question\": \"What lifestyle changes are recommended?\", \"relevant_doc_id\": \"doc_3_id\", \"factual_correctness\": \"false\", \"answer\": \"Balanced diet, regular exercise\"}, \"main\": { \"question\": \"Which of the following is considered first-line treatment approach for type 2 diabetes?\", \"step_by_step_thinking\": \"1. From sub-question 2, we learn that typical treatments include lifestyle changes, monitoring, and medication. 2. Sub-question 3 confirms that lifestyle changes, specifically balanced diet and regular exercise, are recommended for managing diabetes. 3. Comparing this information with the given options: A) Bariatric surgery - not mentioned as first-line treatment. B) Insulin therapy - not mentioned as first-line treatment. C) Lifestyle changes - mentioned first in the list of treatments, suggesting it's primary approach. D) Kidney transplant - not mentioned at all. 4. Therefore, based on the correct information provided, lifestyle changes are considered first-line treatment approach for type 2 diabetes.\", \"answer_choice\": \"C\"} } Your responses will be used for research purposes only, so please provide definite answers and format the output as JSON object as instructed. Here are the relevant documents: { context } Here is the main question: { question } Here are the potential choices: { options} Here are the sub-questions: { sub_questions } Please generate your output as JSON object as instructed: Figure 29: Robustness test inference prompt (full version)."
        }
    ],
    "affiliations": [
        "Adobe Research, USA",
        "Department of Computer Science, University of Oregon, OR, USA"
    ]
}