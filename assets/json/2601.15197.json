{
    "paper_title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "authors": [
        "Shijie Lian",
        "Bin Yu",
        "Xiaopeng Lin",
        "Laurence T. Yang",
        "Zhaolong Shen",
        "Changti Wu",
        "Yuzhuo Miao",
        "Cong Huang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $π(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 2 7 9 1 5 1 . 1 0 6 2 : r 2026-01-23 Work in progress. BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries Shijie Lian1,2,* Bin Yu2,4,* Xiaopeng Lin2,5,* Laurence T. Yang6,1, Zhaolong Shen2,7 Changti Wu2,8 Yuzhuo Miao1,2 Cong Huang2,3 Kai Chen2,3,9, 1HUST 2ZGCA 3ZGCI 4HIT 5HKUST(GZ) 6ZZU 7BUAA 8ECNU 9DeepCybo https://github.com/ZGC-EmbodyAI/BayesianVLA"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify critical pathology in current training paradigms where goal-driven data collection creates dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct dual-branch architecture to estimate both vision-only prior p(a v) and language-conditioned posterior π(a v, ℓ). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language-Action (VLA) models (Kim et al., 2024; Liu et al., 2025; Bjorck et al., 2025b; Black et al., 2025) have emerged as promising paradigm for general-purpose robot manipulation, leveraging the vast knowledge of pre-trained Vision-Language Models (VLMs) to ground natural language instructions into physical actions. By training on large-scale datasets of human demonstrations, these models aim to learn policy π(a v, ℓ) that can execute diverse tasks specified by language ℓ given visual observations v. While demonstrating strong performance in in-distribution settings, current VLA models still face challenges in generalizing to novel instructions or complex multi-task scenarios, particularly in out-ofdistribution (OOD) environments. This limitation is especially pronounced during post-training, where fine-tuning on narrow, task-specific datasets can lead to catastrophic forgetting of the VLMs general capabilities and impair its ability to generalize to new tasks. We hypothesize that this fragility is exacerbated by prevalent bias in current robotic datasets. Most robotic datasets are collected in goal-driven manner, where human operator performs specific task repeatedly in fixed scene. In such datasets, the mapping from visual scene to language instruction ℓ is nearly injective: seeing cabinet in the *Equal contribution Corresponding author 1 scene almost invariably implies the task open the cabinet, while seeing bottle implies pick up the bottle. This deterministic coupling results in sharp conditional distribution p(ℓ v). From Bayesian perspective, the optimal policy can be decomposed as: π(a v, ℓ) = p(ℓ a, v) p(a v) p(ℓ v) . (1) Here, p(a v) represents vision-only prior (i.e., what actions are likely in this scene?), and p(ℓ a, v) is the likelihood (i.e., how well does action explain instruction ℓ?). When p(ℓ v) is sharp, the model can predict ℓ solely from without attending to a. Consequently, the likelihood term p(ℓ a, v) collapses to p(ℓ v), and the posterior policy degenerates to the prior: π(a v, ℓ) p(a v). (2) In other words, the model effectively ignores the language instruction, learning vision shortcut that fails whenever the task is ambiguous or the environment changes. To address this, we propose BayesianVLA, novel framework that explicitly enforces instruction following via Bayesian decomposition. Our key insight is to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions, which is equivalent to maximizing the log-likelihood ratio (LLR): log p(ℓ a, v) log p(ℓ v). This objective penalizes the vision shortcut by requiring the action to provide additional information about ℓ that cannot be inferred from alone. We instantiate this framework by introducing Latent Action Queriesa set of learnable tokens injected into the VLM. These queries serve dual purpose: they act as bottleneck to extract actionrelevant features for downstream Diffusion Transformer (DiT) policy, and they enable dual-branch training strategy. In the Priori Branch, queries attend only to vision to learn p(a v); in the Posteriori Branch, they attend to both vision and language to learn π(a v, ℓ). By optimizing the LLR between these branches, BayesianVLA learns to ground language robustly without requiring new data. Our contributions are three-fold: 1. We identify and empirically validate the vision shortcut pathology in current VLA training, showing that standard models often ignore language in favor of dataset-specific visual correlations. 2. We propose BayesianVLA, method that leverages Latent Action Queries and dual-branch Bayesian objective to recover the true language-conditioned policy from biased data. 3. We demonstrate that BayesianVLA achieves state-of-the-art performance on SimplerEnv and RoboCasa, with remarkable 11.3% improvement in OOD generalization on SimplerEnv, proving its effectiveness in breaking the vision shortcut."
        },
        {
            "title": "2 Motivation: The Illusion of Instruction Following",
            "content": "Before detailing our method, we present empirical evidence to substantiate our hypothesis: that standard VLA models trained on goal-driven datasets often learn vision-only policy p(a v) rather than true language-conditioned policy π(a v, ℓ). Specifically, we employ the Qwen3VL-4B-GR00T model from starVLA (starVLA, 2025) as our representative VLA architecture. We conduct three pilot experiments to reveal this illusion of instruction following. In all three experiments, we train the model by feeding only the visual observation (masking the language instruction ℓ), effectively testing the vision-only prior p(a v)."
        },
        {
            "title": "2.1 Experiment 1: The Vision Shortcut in ID Testing",
            "content": "We first train standard VLA model on subset of the Humanoid robot tabletop manipulation data from PhysicalAI-Robotics-GR00T-X-Embodiment-Sim (Bjorck et al., 2025b) and evaluate on 24 tasks from the RoboCasa benchmark (Nasiriany et al., 2024). Averaged across all 24 tasks, the vision-only model achieves success rate of 44.6%, which is close to the language-conditioned baseline of 47.8%. This small gap reveals that the model can succeed without relying on language instructions, as the training and evaluation scenes and tasks are highly similar, enabling the model to learn near-deterministic mapping from vision to action. Figure 1 provides relevant example. 2 Figure 1: Examples of the vision shortcut in RoboCasa (Nasiriany et al., 2024). Training data exhibits visual diversity but limited task diversity. As result, the model learns to execute tasks directly based on specific visual cues rather than relying on language instructions. 2.2 Experiment 2: Failure in Ambiguous Scenarios To further investigate this behavior, we train standard VLA model on the LIBERO benchmark (Liu et al., 2023), which contains four subsets: Spatial, Object, Long, and Goal. We train on all four training sets and evaluate on all four test sets. The vision-only model achieves success rates comparable to the full VLA model on three subsets (Spatial: 90.2%, Object: 99.6%, Long: 86.0% in Vision-Only, Spatial: 98.8%, Object: 99.0%, Long: 94.4% in Baseline), where each visual scene corresponds to single task. However, on the LIBERO Goal subset, the vision-only success rate plummets to 9.8% (98.0% in Baseline). The key difference is that LIBERO Goal presents inherent ambiguity: multiple valid tasks are associated with the same object configuration during training. For instance, scene with multiple bowls, stove, and drawer could correspond to either put bowl in drawer or put bowl on stove. This confirms that while the model can exploit vision-action correlations in unambiguous datasets, when multiple tasks share the same visual context, due to lack of language to resolve ambiguity, the model is dominated by the prior p(a v) learned from dataset statistics. Figure 2(a) illustrates examples where the same visual scene in LIBERO Goal corresponds to multiple distinct tasks. Figure 2: (a) In LIBERO Goal (Liu et al., 2023), the same scene corresponds to multiple tasks, revealing the ambiguity that vision-only models fail to resolve. (b) Action loss curves on BridgeDataV2 (Walke et al., 2023) show that the vision-only model achieves comparable training loss to the full vision-language model, indicating the presence of visual shortcuts even in diverse, in-the-wild datasets."
        },
        {
            "title": "2.3 Experiment 3: Catastrophic Failure in OOD Generalization",
            "content": "Finally, we test the generalization capability by training on the high-quality BridgeDataV2 dataset (Walke et al., 2023) (diverse, in-the-wild scenes) and evaluating on SimplerEnv (Li et al., 2024c) (simulation, OOD). During training on BridgeDataV2, the vision-only model achieves an action loss of 0.13, comparable to the full language-conditioned models loss of 0.08 (as shown in Figure 2(b)). This indicates that even in diverse, in-the-wild scenarios, the model can still identify visual shortcuts (e.g., specific lighting or background features mapping to specific actions) to minimize the training objective without truly grounding the language instructions. However, this reliance on visual shortcuts proves catastrophic for generalization. When evaluated on 3 Figure 3: The framework of BayesianVLA. The framework employs dual-branch architecture with shared VLM weights. The Priori Branch (left) processes [v, Q, ℓ] with causal masking to learn the vision-only prior p(a v). The Posteriori Branch (right) processes [v, ℓ, Q] to learn the full policy π(a v, ℓ). Latent Action Queries serve as bottleneck interface, and the LLR objective (in Eq. 7) encourages the model to maximize the information between actions and instructions. At inference, only the Posteriori Branch is used, incurring no additional computational overhead. SimplerEnv, which presents visually distinct simulation environments, the vision-only baseline achieves near 0% success. This confirms that the low training loss on Bridge was achieved by overfitting to domain-specific visual patterns rather than learning generalizable manipulation skills. Consequently, when these specific visual cues are absent in the OOD environment, the policy fails completely."
        },
        {
            "title": "2.4 Theoretical Insight: Information Collapse",
            "content": "We formalize the vision shortcut as collapse of the conditional mutual information (CMI) between instructions and actions. Ideally, robust VLA policy should maintain high I(ℓ; v), meaning the action choice significantly reduces uncertainty about the instruction. However, the CMI is bounded by the conditional entropy of the language: I(ℓ; v) = H(ℓ v) H(ℓ a, v) H(ℓ v). (3) In goal-driven datasets, the deterministic mapping ℓ implies H(ℓ v) 0. Consequently, I(ℓ; v) is forced to vanish, theoretically preventing the model from learning any dependency between and ℓ beyond what is already captured by v. To break this deadlock, we cannot rely on standard likelihood Instead, we must explicitly intervene to maximize the information gain provided by maximization. the action. This motivates our use of the Log-Likelihood Ratio (LLR), which effectively estimates the Pointwise Mutual Information (PMI), rewarding the policy only when it captures the specific semantics of ℓ that are not predictable from v."
        },
        {
            "title": "3 Method: BayesianVLA",
            "content": "In this section, we introduce BayesianVLA, framework designed to mitigate the vision shortcut in VLA models. We present the overall framework of BayesianVLA in Figure 3. We first formalize the problem through Bayesian lens (Section 3.1), deriving an objective that maximizes the mutual information between actions and instructions. We then present our architecture, which uses Latent Action Queries to instantiate this decomposition (Section 3.2), and detail our dual-branch training strategy (Section 3.3)."
        },
        {
            "title": "3.1 Objective Formulation",
            "content": "As established in Section 2.4, standard VLA training on goal-driven datasets leads to information collapse where π(a v, ℓ) p(a v). To counteract this, we propose to regularize the policy by maximizing the conditional Pointwise Mutual Information (PMI) between the action and the instruction. This 4 objective can be formulated as maximizing the Log-Likelihood Ratio (LLR) between the posterior policy and the vision-only prior: LLLR = log π(a v, ℓ) p(a v) = log p(ℓ a, v) log p(ℓ v). (4) The detailed derivation is provided in Appendix A. This formulation requires us to simultaneously model the posterior π(a v, ℓ) and the prior p(a v). In the following sections, we describe how BayesianVLA efficiently instantiates these two distributions using shared architecture with Latent Action Queries. 3.2 Latent Action Queries To efficiently instantiate the proposed Bayesian decomposition within unified VLM architecture, we introduce Latent Action Queries. We extend the VLM vocabulary with = 64 learnable tokens, denoted as = {<action_0>, . . . , <action_K>}. These queries function as dedicated bottleneck interface between the VLM (e.g., Qwen3-VL (Bai et al., 2025)) and the continuous action head (a Diffusion Transformer (Peebles and Xie, 2023)). Unlike recent VLA architectures such as π0 (Black et al., 2024) and GR00T (Bjorck et al., 2025b; GEAR-Team et al., 2025), which typically feed the hidden states of all input tokens to the action expert, we append to the input sequence and exclusively use their corresponding hidden states HQ RKD to condition the action head. This design choice is critical: by leveraging the causal masking inherent in decoder-only VLMs, we can precisely control the information encoded in HQ simply by changing the position of in the input sequence. This flexibility enables the strict separation of vision-only and vision-language contexts required for our dual-branch strategy."
        },
        {
            "title": "3.3 Dual-Branch Training Framework",
            "content": "We propose training paradigm with two parallel branches sharing the same VLM weights but different input structures. 1. Priori Branch (Vision-Only). To estimate the prior p(a v), we construct the input sequence as: Inputprior = [v, Q, ℓ]. Due to the causal attention mask of the decoder-only VLM, the tokens in can attend to the visual observation but cannot attend to the language instruction ℓ (which appears later). Thus, the hidden states Hprior encode purely visual information. We use these features to predict the action a, optimizing flow-matching loss Lprior to learn the datasets inherent action bias. (5) 2. Posteriori Branch (Vision + Language). To estimate the true policy π(a v, ℓ), we arrange the input as: Inputpost = [v, ℓ, Q]. Here, appears after ℓ, allowing it to attend to both vision and language. The resulting hidden states Hpost encode the full context. We optimize main flow-matching loss Lmain to learn the expert action. (6) 3. Maximizing the Likelihood Ratio. In addition to action prediction, we explicitly optimize the LLR objective. We treat the VLMs language modeling loss as proxy for log p(ℓ . . . ). Specifically, in the Priori Branch, the language tokens ℓ attend to [v, Q]. Since encodes the action information (via the prior), the probability of generating ℓ in this branch approximates p(ℓ v, aprior). In the Posteriori Branch, we can compute baseline p(ℓ v) by detaching gradients or using separate pass. However, more direct and numerically stable approach is to maximize the difference in log-probabilities of the language tokens between the two branches. We define the LLR loss as: LLLR = log p(ℓ v, Hprior ) sg (log p(ℓ v)) , (7) where sg() denotes the stop-gradient operator. We maximize this term (minimize LLLR) to force the action representations HQ to carry information that explains ℓ. The stop-gradient operation is employed to prevent the model from trivially maximizing the ratio by degrading the baseline p(ℓ v) (i.e., damaging the VLMs general language capabilities) rather than improving the numerator. 5 3.4 Total Training Objective We train the action decoder using the Rectified Flow Matching objective (Liu et al., 2022; Bjorck et al., 2025b). Specifically, we apply this objective to both the Priori Branch (conditioned on Hprior ) and the Posteriori Branch (conditioned on Hpost }, the flow-matching loss is defined as: ). Given condition {Hpost , Hprior LFM(ψ; C) = Et,a0,a (cid:2)vψ(at, t, C) (a1 a0)2(cid:3) , (8) where vψ is the Diffusion Transformer (DiT) predicting the velocity field, a1 is the ground truth action trajectory, a0 (0, I) is sampled from standard Gaussian, and at = (1 t)a0 + ta1 represents the interpolated state at timestep [0, 1]. The final training loss combines the action prediction losses from both branches with the LLR maximization term: Ltotal = (1 λ)LFM(ψ; Hpost ) + λLFM(ψ; Hprior ) βLLLR, (9) where λ balances the contribution of the prior and posterior action losses, and β controls the strength of the LLR regularization. During inference, we exclusively execute the Posteriori Branch to obtain Hpost and generate actions via the DiT. This ensures that our method incurs no additional computational overhead compared to standard VLA baselines at test time."
        },
        {
            "title": "4 Experiment",
            "content": "To comprehensively evaluate the effectiveness of BayesianVLA, we conducted extensive experiments on two simulation benchmarks: SimplerEnv, and RoboCasa. Our training pipeline is built upon the StarVLA framework (starVLA, 2025), distributed across 16 NVIDIA H100 GPUs, and strictly follows its default training protocols to ensure fair comparison. In our experiments, BayesianVLA is instantiated on the QwenGR00T architecture from StarVLA. Broader simulation benchmarks and real-world robot experiments are currently underway. We employ the AdamW optimizer (Loshchilov and Hutter, 2017) initialized with learning rate of 1e-5 and cosine annealing schedule. System-level optimizations include DeepSpeed ZeRO-2 (Rasley et al., 2020), gradient clipping at norm of 1.0, and no gradient accumulation. Table 1: Results of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation environment. We highlight the best results in bold and the second-best results with underline. Method RT-1-X (ONeill et al., 2024) Octo-Small (Team et al., 2024) OpenVLA (Kim et al., 2024) OpenVLA-OFT (Kim et al., 2025) RoboVLM (Li et al., 2024b) CogACT (Li et al., 2024a) SpatialVLA (Qu et al., 2025) TraceVLA (Zheng et al., 2025b) VideoVLA (Shen et al., 2025) π0 (Black et al., 2024) π0.5 (Black et al., 2025) Isaac-GR00T-N1.6-Bridge (GEAR-Team et al., 2025) QwenGR00T (Baseline) + Qwen3-VL-4B (starVLA, 2025) BayesianVLA + Qwen3-VL-4B Put Spoon on Towel Put Carrot on Plate Stack Green Block on Yellow Block Put Eggplant in Yellow Basket Average 0.0 41.7 4.2 12.5 50.0 71.7 20.8 12.5 75. 29.1 49.3 64.5 87.5 89.6 4.2 8.2 0.0 4.2 37.5 50.8 20.8 16.6 20.8 0.0 64.7 65.5 50.0 63.8 0.0 0.0 0.0 4.2 0.0 15.0 25.0 16.6 45. 16.6 44.7 5.5 29.2 33.3 0.0 56.7 12.5 72.5 83.3 67.5 70.8 65.0 70.8 62.5 69.7 93.0 64.2 79.2 1.1 26.7 4.2 23.4 42.7 51.3 34.4 27.7 53. 27.1 57.1 57.1 55.2 66."
        },
        {
            "title": "4.1 Experiments on SimplerEnv",
            "content": "We utilize two large-scale subsets from the Open X-Embodiment (OXE) dataset: BridgeDataV2 (Walke et al., 2023) and Fractal (Brohan et al., 2022). The model is fine-tuned for 40k steps on cluster of 16 GPUs (batch size 16 per device). This benchmark includes four manipulation tasks: Put spoon on towel, Put carrot on plate, Stack green cube on yellow cube, and Put eggplant in yellow basket. 6 Table 2: Results of evaluating the VLA models with the GR1 robot in the RoboCasa Tabletop simulation environment. The results for the first five baseline methods are sourced from the official starVLA experiments (starVLA, 2025). We highlight the best results in bold and the second-best results with underline. Task Isaac-GR00T N1.6 QwenGR00T +Qwen3VL QwenPI +Qwen3VL QwenOFT +Qwen3VL QwenFAST +Qwen3VL VisionOnly QwenGR00T BayesianVLA +Qwen3VL PnP Bottle To Cabinet Close PnP Can To Drawer Close PnP Cup To Drawer Close PnP Milk To Microwave Close PnP Potato To Microwave Close PnP Wine To Cabinet Close PnP Novel From Cuttingboard To Basket PnP Novel From Cuttingboard To Cardboardbox PnP Novel From Cuttingboard To Pan PnP Novel From Cuttingboard To Pot PnP Novel From Cuttingboard To Tieredbasket PnP Novel From Placemat To Basket PnP Novel From Placemat To Bowl PnP Novel From Placemat To Plate PnP Novel From Placemat To Tieredshelf PnP Novel From Plate To Bowl PnP Novel From Plate To Cardboardbox PnP Novel From Plate To Pan PnP Novel From Plate To Plate PnP Novel From Tray To Cardboardbox PnP Novel From Tray To Plate PnP Novel From Tray To Pot PnP Novel From Tray To Tieredbasket PnP Novel From Tray To Tieredshelf Average 51.5 13.0 8.5 14.0 41.5 16.5 58.0 46.5 68.5 65.0 46.5 58.5 57.5 63.0 28.5 57.0 43.5 51.0 78.7 51.5 71.0 64.5 57.0 31.5 47.6 46.0 80.0 54.0 48.0 28.0 46.0 48.0 40.0 68.0 52.0 56.0 42.0 44.0 48.0 18.0 60.0 50.0 54.0 70.0 38.0 56.0 50.0 36.0 16.0 47.8 26.0 62.0 42.0 50.0 42.0 32.0 40.0 46.0 60.0 40.0 44.0 44.0 52.0 50.0 28.0 52.0 40.0 36.0 48.0 34.0 64.0 44.0 50.0 28. 43.9 30.0 76.0 44.0 44.0 32.0 36.0 50.0 40.0 70.0 54.0 38.0 32.0 58.0 52.0 24.0 60.0 50.0 66.0 68.0 44.0 56.0 62.0 54.0 30.0 48.8 38.0 44.0 56.0 44.0 14.0 14.0 54.0 42.0 58.0 58.0 40.0 36.0 38.0 42.0 18.0 52.0 30.0 48.0 50.0 28.0 34.0 46.0 36.0 16.0 39.0 70.0 78.0 42.0 50.0 44.0 40.0 58.0 26.0 72.0 50.0 20.0 48.0 32.0 34.0 16.0 26.0 38.0 44.0 60.0 50.0 64.0 52.0 42.0 16. 44.7 60.0 72.0 46.0 54.0 32.0 56.0 62.0 44.0 68.0 40.0 36.0 46.0 42.0 70.0 24.0 34.0 48.0 52.0 70.0 60.0 66.0 62.0 44.0 22.0 50.4 For each task, we evaluate the VLA policies using the official evaluation scripts provided by the SimplerEnv repository (Li et al., 2024c). To mitigate the effects of randomness, we run 480 independent trials and report the average performance (Avg@480). The results are summarized in Table 1. BayesianVLA consistently outperforms comparison baselines, achieving state-of-the-art average success rate of 66.5%. Notably, compared to the direct baseline QwenGR00T (55.2%) built on the same StarVLA framework, our method delivers an absolute improvement of 11.3%, validating that the performance gain stems from our proposed Bayesian decomposition rather than the base architecture. Significant improvements are observed in tasks requiring precise object identification and manipulation, such as Put Carrot on Plate (+13.6%) and Put Eggplant in Yellow Basket (+15.0%). Furthermore, BayesianVLA surpasses other recent strong competitors, including the flow-matching-based π0.5 (57.1%) and the dual-system Isaac-GR00T-N1.6 (57.1%). These results confirm that by explicitly optimizing the mutual information between language and action, BayesianVLA effectively mitigates the vision shortcut. Fundamentally, this validates that our approach prevents the policy from collapsing into spurious vision-only prior p(av) caused by dataset determinism, and instead compels the model to learn the true causal dependency of actions on language instructions."
        },
        {
            "title": "4.2 Experiments on RoboCasa",
            "content": "We evaluate our method on the RoboCasa GR1 Tabletop Manipulation Benchmark (Nasiriany et al., 2024), which consists of 24 diverse manipulation tasks. These tasks feature complex interactions with articulated objects and varied geometries, exemplified by specific tasks like PnPBottleToCabinetClose 7 Figure 4: Qualitative comparison of general multimodal reasoning. We present case where the model is asked to solve mathematical problem. The standard VLA baseline (QwenGR00T) suffers from catastrophic forgetting; while the text before the comma implies differentiating all terms together, the subsequent output degenerates into repetitive and meaningless gibberish (bottom right). In contrast, BayesianVLA (top right) retains the VLMs original reasoning and language generation capabilities (left), successfully solving the problem. and PnPCanToDrawerClose, as well as scenarios involving appliances like microwaves and toasters. For training, we utilize the Humanoid Robot Tabletop Manipulation subset from the PhysicalAIRobotics-GR00T-X-Embodiment-Sim (Bjorck et al., 2025b) dataset. Apart from the dataset, all experimental configurations are identical to those detailed in Section 4.1. To guarantee statistical significance, we evaluate each task using 50 independent trials and report the average success rate (Avg@50). The quantitative results on RoboCasa are presented in Table 2. Consistent with the empirical evidence presented in our motivation (Section 2), the VisionOnly baseline achieves surprisingly high success rate of 44.7%, lagging only slightly behind the standard QwenGR00T baseline (47.8%). This observation reconfirms the prevalence of the vision shortcut in this benchmark, suggesting that significant portion of tasks can be solved by relying solely on visual cues. However, BayesianVLA breaks this performance ceiling, achieving state-of-the-art average success rate of 50.4% and surpassing all competing baselines, including QwenOFT (48.8%) and Isaac-GR00T (47.6%). Crucially, our method demonstrates substantial gains in tasks where the vision-only policy falters. For instance, in the task PnP Novel From Placemat To Plate, BayesianVLA achieves 70.0% success, significantly outperforming both the VisionOnly baseline (34.0%) and the standard QwenGR00T (48.0%). These results indicate that maximizing the LLR objective successfully forces the policy to extract and utilize task-specifying information from language, rather than settling for local optima based on visual shortcuts."
        },
        {
            "title": "4.3 Preservation of General Capabilities",
            "content": "A prevalent view is that fine-tuning VLMs for robotic action generation (VLA training) diminishes the models foundational reasoning and multimodal understanding, potentially leading to loss of general conversational abilities (Zhou et al., 2025; Xu et al., 2025; Hancock et al., 2025; Yu et al., 2026). ChatVLA (Zhou et al., 2025) attributes this to two factors: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. We observe similar degradation in our baseline: as shown in Figure 4, the standard QwenGR00T model loses its ability to converse coherently even when prompted with pure text inputs. In contrast, BayesianVLA remarkably preserves these capabilities when queried with language instructions. It is crucial to note nuanced distinction: while BayesianVLA preserves normal text-only conversational ability, its general vision-language conversation (image + text inputs) can still degrade after VLA training. We hypothesize this is mainly because, in our training setup, the vision tower (and the multimodal fusion/projection layers) must be adapted for control, which can shift visual representations away from the pre-trained visionlanguage alignment manifold. Such specialization is expected for an 8 Figure 5: Additional qualitative comparison. Demonstrating the preservation of general VLM capabilities on another example. embodied agent optimized for action. Importantly, BayesianVLA helps prevent the text-only language behavior of the backbone from collapsing. In the baseline, the vision shortcut can render instruction tokens effectively redundant for control, weakening the training signal that encourages meaningful language processing and increasing drift in shared parameters, which manifests as failures even on pure text queries (Fig. 4 and Fig. 5). Conversely, our methods LLR objective enforces strong dependency on language. This acts as regularizer that maintains the functional utility of instruction tokens, thereby preserving the backbones text-only conversational ability even as the visual modality is specialized for control. This preservation is of significant practical value: it ensures that the VLM backbone does not degenerate into shallow feature mapper. By retaining its linguistic core, the agent preserves the potential for higher-level reasoning and generalization to novel instructions, which are the primary motivations for employing Foundation Models in robotics."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "We conduct ablation studies on SimplerEnv to validate the contributions of individual components in BayesianVLA. All experiments utilize the Qwen3-VL-4B backbone, and results are presented in Table 3. Effectiveness of Bayesian Decomposition. Comparing the full BayesianVLA (63.5%) with the + Action Query ablation (57.5%), we observe significant performance boost (+6.0%). This indicates that while the architectural changes provide some benefit, the core improvement stems from our dual-branch Bayesian learning objective. By explicitly modeling and maximizing the pointwise mutual information (PMI) between instructions and actions, the model effectively overcomes the vision shortcut, validating the central hypothesis of this work. Potential of Latent Action Queries. Even without the dual-branch definition, introducing Latent Action Queries (QwenGR00T + Action Query) improves upon the QwenGR00T baseline (55.2% 57.5%). This suggests that Latent Action Queries function as promising architectural inductive bias. Unlike standard approaches that feed full sequences of vision and language token embeddings into the action decoder, our query-based mechanism forces the VLM to compress and summarize task-relevant information into compact set of latent tokens. From computational perspective, this design is highly efficient. It decouples the complexity of the Diffusion Transformer (DiT) from the length of the VLM input context. Specifically, the complexity of condition processing in the DiT is reduced from O(N 2) (scaling with the massive number of vision-language tokens ) to O(K2) (scaling with the small, constant number of query tokens K), thereby streamlining the action generation process."
        },
        {
            "title": "5 Discussion",
            "content": "Based on our analysis of the vision shortcut and the Bayesian decomposition framework, we discuss several potential insights that may guide future research and community practices. Rethinking Data Collection Strategies. Our experiments suggest that the deterministic mapping from visual scenes to language instructions (H(ℓ v) 0) in goal-driven datasets is significant factor con9 Table 3: Ablation study on SimplerEnv. All experiments are based on the Qwen3-VL-4B backbone. We compare the baseline QwenGR00T, the addition of Latent Action Queries, and the full BayesianVLA framework to validate the contributions of each component. Method Put Spoon on Towel Put Carrot on Plate Stack Green Block on Yellow Block Put Eggplant in Yellow Basket Average QwenGR00T (starVLA, 2025) QwenGR00T + Action Query BayesianVLA 87.5 74.6 81.9 50.0 58.3 66. 29.2 29.2 34.7 54.2 67.9 70.8 55.2 57.5 63.5 tributing to the vision shortcut. To mitigate this, we hypothesize that shift in data collection strategies could be beneficial. Prioritizing data collection in ambiguous scenarioswhere the task cannot be inferred solely from the initial observationmight naturally increase the conditional entropy of language. By enriching datasets with scenes that support multiple valid tasks, models may be forced to rely more heavily on instructions for disambiguation. Leveraging Human Data for Robustness. Recently, there has been growing interest in training robot models on large-scale human video data, such as HRDT (Bi et al., 2025), In-N-On (Cai et al., 2025), METIS (Fu et al., 2025), and PhysBrain (Lin et al., 2025). Unlike curated robot datasets, human activities are inherently multimodal and context-dependent; the same environment often hosts wide variety of behaviors, potentially leading to less sharp p(ℓ v). We conjecture that injecting action knowledge from such rich human distributions might help mitigate the information collapse observed in robot-only datasets. World Models as an Alternative Bayesian Formulation. Beyond the VLM framework focused on in this work, recent studies have also explored adapting World Models for VLA control, as seen in F1VLA (Lv et al., 2025), Mantis (Yang et al., 2025), and InternVLA-A1 (contributors, 2026). From theoretical perspective, these approaches can be viewed as an alternative instantiation of the Bayesian rule, specifically performing inverse dynamics on imagined futures. If we consider as sequence of past frames vt, and treat the future state vt+1 as latent variable generated by the model (conditioned on ℓ), the action inference can be expressed as: p(a vt, vt+1, ℓ) = p(vt+1 vt, a, ℓ) p(a vt, ℓ) p(vt+1 vt, ℓ) . (10) Here, the numerator p(vt+1 vt, a, ℓ) represents world model (forward dynamics) predicting the future state. The term p(a vt, ℓ) serves as an action prior, and the denominator p(vt+1 vt, ℓ) represents the future prediction marginalized over actions. In this formulation, the policy execution involves first imagining desired future vt+1 consistent with ℓ, and then inferring the optimal action via the equation above. Since world models are typically trained on vast amounts of video data, the predictive distribution (the numerator) is often rich and highly sensitive to the action a. We hypothesize that this sensitivity prevents the collapse of the numerator to the denominator. This suggests that world model-based architectures could offer another robust technical path toward solving the vision shortcut, which we plan to explore in future work."
        },
        {
            "title": "6 Related Work",
            "content": "We build our work upon the following rigorous foundations: Vision-Language-Action Dataset and Benchmark. The advancement of generalist robot policies relies heavily on large-scale datasets and rigorous benchmarks. LIBERO (Liu et al., 2023) pioneered the systematic study of knowledge transfer in lifelong robot learning. To scale up real-world data, BridgeData V2 (Walke et al., 2023) provided diverse interaction trajectories on low-cost hardware. This effort was expanded by Open X-Embodiment (OXE) (ONeill et al., 2024), which aggregated data across 22 robot embodiments, and Droid (Khazatsky et al., 2024), which further increased diversity with distributed data collection. For scalable evaluation, RoboCasa (Nasiriany et al., 2024) introduced large-scale simulation framework with realistic kitchen environments, while SimplerEnv (Li et al., 2024c) provided 10 simulated evaluation proxy to correlate with real-world performance, addressing the reproducibility crisis in physical evaluation. More recently, RoboTwin 2.0 (Chen et al., 2025) offered unified benchmark for bimanual manipulation with automated data generation, and AgiBot-World (Bu et al., 2025) scaled training data to over 1 million trajectories with human-in-the-loop verification. Vision-Language-Action Models. To bridge the gap between semantic understanding and physical control, Vision-Language-Action (VLA) models have emerged as dominant paradigm. Early works like Octo (Team et al., 2024) established the transformer-based policy as versatile initialization, utilizing diffusion heads to handle multimodal action distributions. Building on pre-trained VLMs, OpenVLA (Kim et al., 2024) and its optimized variant OpenVLA-OFT (Kim et al., 2025) fine-tune large language models for robotic control, demonstrating strong generalization. To further enhance action modeling, specialized architectures have been proposed. CogACT (Li et al., 2024a) and RDT-1B (Liu et al., 2025) employ dedicated diffusion transformers to capture complex action dynamics. Recent works explore advanced architectures and generation algorithms to better balance reasoning and control. The GR00T series (N1, N1.5, N1.6) (Bjorck et al., 2025b,a; GEAR-Team et al., 2025) utilizes dual-system design, architecturally coupling VLM for perception with diffusion head for action generation. In parallel, the π0 series (π0, π0.5, FAST) (Black et al., 2024, 2025; Pertsch et al., 2025) leverages Flow Matching as superior generation objective, adopting two-stage training paradigm that pre-trains the VLM backbone before fine-tuning. Specifically, FAST introduces discrete motion tokens to efficiently encode actions for the VLM, while π0.5 incorporates subtask decomposition to enhance long-horizon planning. Other approaches like X-VLA (Zheng et al., 2025a) introduce embodiment-specific soft prompts to facilitate cross-embodiment generalization. By learning separate sets of embeddings for each data source, X-VLA effectively leverages heterogeneous robot data with minimal additional parameters. SpatialVLA (Qu et al., 2025) argues that spatial understanding is central to manipulation, introducing Ego3D Position Encoding and Adaptive Action Grids to inject 3D information and learn transferable spatial action knowledge. Finally, VideoVLA (Shen et al., 2025) explores transforming video generation models into robot manipulators. By jointly predicting action sequences and future visual outcomes, it leverages the \"visual imagination\" of generative models to enhance generalization across novel tasks and objects. Despite these architectural advancements, standard VLA training often suffers from the vision shortcut, where models ignore language instructions in goal-driven datasets. Unlike previous methods that focus on scaling or architectural tuning, we address this fundamental issue through structural Bayesian decomposition, explicitly maximizing the mutual information between language and actions to ensure robust instruction following."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we identified critical pathology in standard VLA training: the vision shortcut, where models learn to ignore language instructions in goal-driven datasets, leading to collapse of the mutual information between instructions and actions. This phenomenon results in catastrophic failure when the model faces ambiguous scenarios or out-of-distribution environments where visual shortcuts no longer hold. To address this, we introduced BayesianVLA, novel framework that formalizes VLA learning through Bayesian lens. By decomposing the policy into vision-only prior and language-conditioned posterior, we derived training objective that explicitly maximizes the Log-Likelihood Ratio (LLR), effectively optimizing the Pointwise Mutual Information (PMI) between language and action. We instantiated this decomposition using unified architecture with Latent Action Queries and dual-branch training strategy, which incurs no additional computational overhead during inference. Our extensive experiments on SimplerEnv and RoboCasa demonstrate that BayesianVLA effectively mitigates the vision shortcut problem. It significantly outperforms baselines under various conditions; in out-of-distribution simulation environments, where standard methods fail completely, BayesianVLA achieves robust generalization. These results highlight the importance of introducing structural inductive biases in learning robust, generalizable robot policies. We hope our work inspires further research into the informationtheoretic foundations of embodied AI, pushing towards robots that truly understand and reliably execute human intent."
        },
        {
            "title": "Limitation and Future Work",
            "content": "While BayesianVLA offers significant improvements in robustness, the dual-branch architecture introduces limitation regarding computational overhead during training. Since the model must compute both the Priori and Posteriori branches, the computational cost per iteration theoretically increases. However, we note that the visual input prefix is identical for both branches, and the number of visual tokens vastly outnumbers that of the language and latent action query tokens. By employing prefix prefill strategy to compute and reuse the visual representations (e.g., vision encoder outputs) for both branches, the actual increase in training time is marginal. Thus, the additional computational overhead remains within completely acceptable range. For future work, we intend to provide more comprehensive empirical evaluation in subsequent versions of this manuscript. We plan to extend our experimental validation to include the RoboTwin and LIBERO benchmarks, alongside real-world robot experiments. We also aim to scale our framework to larger foundation models, such as conducting experiments with BayesianVLA on Qwen3VL-8B. Additionally, we plan to include more extensive ablation studies to further dissect the contribution of each component and hyperparameter within our Bayesian decomposition framework, thereby rendering the analysis more complete."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025. Qwen3-VL technical report. arXiv preprint arXiv:2511.21631. Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, and Jun Zhu. 2025. H-RDT: Human manipulation enhanced bimanual robotic manipulation. arXiv preprint arXiv:2507.23523. Johan Bjorck, Valts Blukis, Fernando Casta neda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Jim Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Xiaowei Jiang, Kaushil Kundalia, Jan Kautz, Zhiqi Li, Kevin Lin, Zongyu Lin, Loic Magne, and 19 others. 2025a. GR00T N1.5: An improved open foundation model for generalist humanoid robots. https://research.nvidia.com/labs/gear/gr00t-n1_5/. Accessed: 2026-01-19. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, and 1 others. 2025b. GR00T N1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734. Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, and 16 others. 2025. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, and 1 others. 2024. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, and 32 others. 2022. Rt-1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, and 1 others. 2025. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669. Xiongyi Cai, Ri-Zhao Qiu, Geng Chen, Lai Wei, Isabella Liu, Tianshu Huang, Xuxin Cheng, and Xiaolong In-N-On: Scaling egocentric manipulation with in-the-wild and on-task data. arXiv preprint Wang. 2025. arXiv:2511.15704. Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, and 1 others. 2025. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088. InternVLA-A1 contributors. 2026. InternVLA-A1: Unifying understanding, generation and action for robotic manipulation. arXiv preprint arXiv:2601.02456. Yankai Fu, Ning Chen, Junkai Zhao, Shaozhe Shan, Guocai Yao, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. 2025. METIS: Multi-source egocentric training for integrated dexterous vision-language-action model. arXiv preprint arXiv:2511.17366. GEAR-Team, Allison Azzolini, Johan Bjorck, Valts Blukis, Fernando Castañeda, Rahul Chand, and 1 others. 2025. Gr00t n1.6: An improved open foundation model for generalist humanoid robots. https://research. nvidia.com/labs/gear/gr00t-n1_6/. Asher Hancock, Xindi Wu, Lihan Zha, Olga Russakovsky, and Anirudha Majumdar. 2025. Actions as language: Fine-tuning vlms into vlas without catastrophic forgetting. arXiv preprint arXiv:2509.22195. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, and 1 others. 2024. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945. Moo Jin Kim, Chelsea Finn, and Percy Liang. 2025. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. 2024. OpenVLA: An open-source vision-languageaction model. In Annual Conference on Robot Learning (CoRL). Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, and 1 others. 2024a. CogACT: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. 2024b. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. 2024c. SimplerEnv: Evaluating real-world robot manipulation policies in simulation. In Annual Conference on Robot Learning (CoRL). Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, and 1 others. 2025. PhysBrain: Human egocentric data as bridge from vision language models to physical intelligence. arXiv preprint arXiv:2512.16793. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. 2023. LIBERO: Benchmarking knowledge transfer for lifelong robot learning. Advances in neural information processing systems (NeurIPS), 36:4477644791. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. 2025. RDT-1B: diffusion foundation model for bimanual manipulation. In International Conference on Learning Representations (ICLR). Xingchao Liu, Chengyue Gong, and 1 others. 2022. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations (ICLR). Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR). Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, and Jiangmiao Pang. 2025. F1: vision-language-action model bridging understanding and generation to actions. arXiv preprint arXiv:2509.06951. Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. 2024. RoboCasa: Large-scale simulation of everyday tasks for generalist robots. In Robotics: Science and Systems. 13 Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, and 1 others. 2024. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205. In Proceedings of the Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. 2025. FAST: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and 1 others. 2025. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506. Yichao Shen, Fangyun Wei, Zhiying Du, Yaobo Liang, Yan Lu, Jiaolong Yang, Nanning Zheng, and Baining Guo. 2025. VideoVLA: Video generators can be generalizable robot manipulators. In Advances in neural information processing systems (NeurIPS). starVLA. 2025. Starvla: lego-like codebase for vision-language-action model developing. GitHub repository. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, and 1 others. 2024. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, and 1 others. 2023. Bridgedata v2: dataset for robot learning at scale. In Annual Conference on Robot Learning (CoRL), pages 17231736. PMLR. Kechun Xu, Zhenjie Zhu, Anzhe Chen, Shuqi Zhao, Qing Huang, Yifei Yang, Haojian Lu, Rong Xiong, Masayoshi Tomizuka, and Yue Wang. 2025. Seeing to act, prompting to specify: bayesian factorization of vision language action policy. arXiv preprint arXiv:2512.11218. Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, and Zhijie Deng. 2025. Mantis: versatile vision-language-action model with disentangled visual foresight. arXiv preprint arXiv:2511.16175. Bin Yu, Shijie Lian, Xiaopeng Lin, Yuliang Wei, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Xinming Wang, Bailing Wang, Cong Huang, and Kai Chen. 2026. TwinBrainVLA: Unleashing the potential of generalist vlms for embodied tasks via asymmetric mixture-of-transformers. arXiv preprint arXiv:2601.14133. Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, and 1 others. 2025a. X-VLA: Soft-prompted transformer as scalable cross-embodiment vision-language-action model. arXiv preprint arXiv:2510.10274. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. 2025b. TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345. Zhongyi Zhou, Yichen Zhu, Junjie Wen, Chaomin Shen, and Yi Xu. 2025. Vision-language-action model with open-world embodied reasoning from pretrained knowledge. arXiv preprint arXiv:2505.21906."
        },
        {
            "title": "A Derivation of the LLR Objective",
            "content": "In this section, we provide the derivation for the Log-Likelihood Ratio (LLR) objective used in BayesVLA. Our goal is to maximize the conditional Pointwise Mutual Information (PMI) between the action and the language instruction ℓ, given the visual observation v. The PMI is defined as: PMI(a, ℓ v) = log π(a, ℓ v) p(a v)p(ℓ v) Using the chain rule of probability π(a, ℓ v) = π(a v, ℓ)p(ℓ v), we can rewrite the PMI as: PMI(a, ℓ v) = log π(a v, ℓ)p(ℓ v) p(a v)p(ℓ v) = log π(a v, ℓ) p(a v) (11) (12) This corresponds to the first form of our LLR objective: the log-ratio between the posterior policy and the vision-only prior. Alternatively, using the chain rule π(a, ℓ v) = p(ℓ a, v)p(a v), we can rewrite the PMI as: PMI(a, ℓ v) = log p(ℓ a, v)p(a v) p(a v)p(ℓ v) = log p(ℓ a, v) p(ℓ v) = log p(ℓ a, v) log p(ℓ v) (13) This corresponds to the second form of our LLR objective, which we optimize in practice. It represents the difference between the log-likelihood of the instruction given the action and vision, and the loglikelihood of the instruction given vision alone. Maximizing this quantity encourages the model to select actions that make the instruction ℓ more probable than it would be based on vision alone."
        }
    ],
    "affiliations": [
        "BUAA",
        "DeepCybo",
        "ECNU",
        "HIT",
        "HKUST(GZ)",
        "HUST",
        "ZGCA",
        "ZGCI",
        "ZZU"
    ]
}