{
    "paper_title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
    "authors": [
        "Pengfei Zhou",
        "Xiaopeng Peng",
        "Jiajun Song",
        "Chuanhao Li",
        "Zhaopan Xu",
        "Yue Yang",
        "Ziyao Guo",
        "Hao Zhang",
        "Yuqi Lin",
        "Yefei He",
        "Lirui Zhao",
        "Shuo Liu",
        "Tianhua Li",
        "Yuxuan Xie",
        "Xiaojun Chang",
        "Yu Qiao",
        "Wenqi Shao",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The OpenING is open-sourced at https://opening-benchmark.github.io."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 2 9 9 4 8 1 . 1 1 4 2 : r GATE OpenING: Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation Pengfei Zhou1*, Xiaopeng Peng2*, Jiajun Song3, Chuanhao Li1, Zhaopan Xu1, Yue Yang4,1, Ziyao Guo1,5, Hao Zhang1, Yuqi Lin1, Yefei He1, Lirui Zhao1, Shuo Liu1, Tianhua Li1,4, Yuxuan Xie1,4, Xiaojun Chang6,7, Yu Qiao1, Wenqi Shao1, Kaipeng Zhang1 https://opening-benchmark.github.io"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce GATE OpenING (OpenING), comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering robust platform for challenging interleaved generation methods. In addition, we present IntJudge, judge model for evaluating open-ended multimodal generation methods. Trained with novel data pipeline, our IntJudge achieves an agreement rate of 82.42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. 1. Introduction Building upon the remarkable understanding and generation capabilities of Large Language Models (LLMs) [1, 63, 64, 66], Multimodal LLMs (MLLMs) are making progress in various tasks [5, 42, 81, 84]. However, generating interleaved image-text content remains challenging [38, 62, 70], despite its important role in both research and applications (e.g., multimodal reasoning [11, 46], education [17, 37] and Equal contribution Corresponding author 1GATE Team, Shanghai Artificial Intelligence Laboratory 2Rochester Institute of Technology 3Renmin University of China 4Shanghai Jiao Tong University 5National University of Singapore 6University of Science and Technology of China 7MBZUAI Figure 1. Motivation: (a) Rapid progress of interleaved imagetext generation. (b) Interleaved content is essential to provide key information for complex real-world tasks (e.g., product design). design [34, 58]). Since human brains can naturally combine visual and textual signals for more efficient information exchange [25, 31], achieving such integrated ability is crucial for advancing towards Artificial General Intelligence (AGI). As shown in Fig.1, the emergence of unified models that combine understanding and generation abilities opens new possibilities for interleaved generation [77, 92]. However, the lack of reliable benchmarks for evaluating interleaved image-text generation remains an obstacle [61, 70]. Most existing benchmarks evaluate text or image outputs separately and do not capture the complexities of generating both simultaneously [44, 60, 82, 83]. Interleaved benchmarks like OpenLEAF [4] and InterleavedBench [43] are limited in size, scope, and query diversity. For example, InterleavedBench includes only 815 instances across 10 tasks sourced from public datasets such as VIST [32], ActivityNet [36], and WikiHow [80], which is not representative of real-world needs and faces the risk of data contamination. To fill the gap, we introduce OpenING, comprehensive benchmark for evaluating open-ended interleaved generation. Unlike previous benchmarks, OpenING offers broader set of real-world data and tasks (e.g., brainstorming, recommendations and content creation) derived from daily scenarios Figure 2. OpenING benchmark consists of 23 meta-topics (inner ring) which are further categorized into 56 specific tasks (see the number of tasks on the outer ring and details in Appendix Sec. A.1). Examples showcase interleaved generation in eight representative domains. like fashion, cooking, and travel. As shown in Fig. 2 and Table 1, the curated OpenING includes 5,400 instances of multi-step interleaved image-text content across 23 metatopics and 56 tasks, with diverse, carefully designed queries for various topics. To tackle the challenges of gathering and standardizing data from disparate domains, we develop an efficient annotation pipeline and produced high-quality human-annotated data, reducing data contamination risks. In addition, previous benchmarks typically rely on GPTbased scoring metrics [4, 43], which are prone to be affected by the bias of GPT and potential data leakage in API uses [71]. To address the challenges of assessing open-ended multimodal generation, we introduce robust judge model named IntJudge. We propose an Interleaved Arena to facilitate training data annotation and Reference-Augmented Generation (RAG) approach to scale up the data size. Trained using the enhanced data pipeline, IntJudge achieves an 82.42% average agreement with human judgments, showing an 11.34% improvement over GPT-4o as judge. We evaluate representative interleaved generation methods using our OpenING. Key findings from our experiments include: 1) Generating coherent and high-quality interleaved content remains challenging for all models, as humanannotated content receives the highest rating compared to the generated content; 2) While integrated pipelines (e.g. Gemini+Flux) outperforms end-to-end models (e.g. Anole) with more coherent image-text content and higher quality, end-toend and two-stage generators (e.g. SEED-X) still hold great potential as the unified models continue to advance and can be combined with these methods; and 3) While text answers generated by GPT can be more informative than humanannotated answers, annotated natural images are still more favorable to generated images, highlighting the challenge of generating high-quality images. The major contributions of this paper are summarized as follows: High-quality Benchmark. We present OpenING, comprehensive benchmark for evaluating open-ended interleaved image-text generation. OpenING includes 5,400 human-annotated instances across 56 real-world tasks, aiming to challenge and improve interleaved generation methods and also support the development of judge models for assessing open-ended multimodal generation. Robust Judge. We introduce IntJudge, judge model for rating interleaved generation methods. We train IntJudge with an enhanced data pipeline, achieving an 82.42% agreement rate with human judgments and significantly outperforming GPT-based judge. Moreover, IntJudge has proven to be effective in assessing new unseen models."
        },
        {
            "title": "Data Coverage",
            "content": "Meta-Topics"
        },
        {
            "title": "Steps",
            "content": "OpenLEAF [4] InterleavedBench [43] OpenING (ours) 2 4 23 10 10 56 660 815 5,400 - 1,513 17,603 - 1,601 20,"
        },
        {
            "title": "SpI",
            "content": "- 1.96 3.72 Open-source Offline Judge Table 1. Comparison between OpenING and existing benchmarks. OpenING includes more comprehensive data and task coverage with an openly available judge model. Steps: step is indicated by an input instruction or an output image-text pair; SpI: Steps per Instance. Comprehensive Leaderboard. We provide detailed rankings and analysis of interleaved generation methods and compare evaluations from our IntJudge and GPT-4o with human judgments. Our studies suggest that although current open-source end-to-end models perform less favorably than integrated generation pipelines, two-stage generators based on unified models deserve future exploration for the development of interleaved generation. 2. Related Work Interleaved Image-Text Generation. Development of MLLMs has greatly pushed forward interleaved image-text generation [35]. Early models like Stable Diffusion [20, 53], DALL-E [52], and autoregressive (AR) methods such as VAR [65] and Lumina-mGPT [41] focused on unidirectional tasks, including image understanding and text-to-image generation. Flamingo [2] was the first to introduce capabilities for interleaved image-text content. More recent models, such as MiniGPT-5 [88] and SEED-series [23, 24, 79], achieve interleaved generation by combining the AR-based text generation and diffusion-based visual generation. Native AR models such as Emu-3 [70] and Chameleon [62] offer unified framework capable of generating and reasoning over mixed-modal documents. Anole [16] reproduces the image generation capability of Chameleon through efficient finetuning on interleaved image-text data. Meanwhile, benchmarks for the evaluation of interleaved image-text generation are still emerging. Previous works, such as OpenLEAF [4] and InterleavedBench [43] focused on small set of scenarios and lacked the depth and breadth for real-world applications. To achieve more reliable and holistic evaluation of interleaved generation, we propose OpenING comprehensive benchmark consisting of real-world scenarios. Evaluation of Open-ended Multimodal Generation. Evaluating open-ended multimodal generation is inherently challenging, due to the requirement of assessing both visual and textual quality in open domain [4, 73]. Existing text generation metrics, such as BLEU [49] and ROUGE [39], lack the measure of visual quality or text-image coherence. Visual quality metrics like FID [30] and IS [54], on the other hand, lack the consideration of textual elements. Metrics like CLIPScore [29] can measure text-image alignment but are unable to fully evaluate the quality of open-ended interleaved content, where multiple correct answers exist. GPT-based scoring [43, 86] provides better measurements to assess the diversity and coherence of the interleaved outputs. However, GPT tends to be biased and favors the contents generated by its own [6, 71]. Human evaluation, though reliable, is not scalable due to its laborious nature. To bridge this gap, we introduce IntJudge, judge model that is highly aligned with human judgments in evaluating the open-ended multimodal generation. To avoid the instability of subjective scores [14, 89], our IntJudge mainly evaluates models through pairwise comparisons in an arena manner. 3. OpenING Benchmark 3.1. Problem Definition The task of interleaved image-text generation involves generating sequence of text and images based on given prompt. Each interleaved generation model (referred to as multimodal agent) receives an input prompt P, which can be text-only or include both texts and images. The multimodal agent outputs an interleaved image-text sequence: = [s1, s2, . . . , sN ], where is the number of steps. Each element si =< Ti, Ii > in step consists of text segment Ti and an image Ii. Each si is generated based on the prompt and all outputs history as si = (P, s1, s2, . . . , si1), where denotes the the generation function of an agent. The objective is to find an optimal output sequence set S: = arg max (cid:89) i=1 p(s P, 1, . . . , i1), (1) where in each step is semantically consistent with the input prompt while the coherence throughout the entire sequence. The performance of an agent is evaluated based on how well the generated meets predefined criteria. 3.2. Data Curation Collecting and annotating interleaved image-text data is inherently challenging due to the scarcity of high-quality data. Moreover, it is difficult to gather multimodal data from disparate domains and pair them consistently. We created OpenING over three months, with nearly 50 people involved in an efficient pipeline, which is shown in Fig. 3. Figure 3. Overview of data curation and the proposed judge pipeline. (a) We construct our OpenING benchmark in top-down manner, which involves five stages: conceptualization, data collection, annotation, filtering and processing. (b) We use the Dev Set of OpenING to train the proposed IntJudge and evaluate interleaved image-text generation on the Test Set to compare our IntJudge with human and GPT-4o. Topic Conceptualization. With the assistance of multiple AI agents, we brainstormed and identified the most relevant real-world scenarios that require interleaved image-text generation. These insights were conceptualized into 23 metatopics and divided into 56 specific tasks. Data Collection and Annotation. We collected interleaved image-text data from more than 20 sources, including social media (e.g., REDnote1) and YouTube2, search engine (e.g. Google3), and open-source platforms (e.g. OpenDataLab [28]), etc. (see Appendix Sec. A.2). We designed an efficient annotation process involving team of 28 professional annotators, monitored by 14 data experts throughout the process to ensure the highest quality. The maximum number of steps per instance was limited to ten to ensure usability with context restraints. All queries and answers are annotated manually via our developed tool, IntLabel. Data Filtering and Quality Control. We performed crosschecks with annotators and data experts to ensure high standards of consistency, relevance, and coherence for each instance. Each task was required to include diverse sources and topics. In few cases where data acquisition posed challenges, annotators were instructed to supplement the dataset with content generated by GPT-4o [48] and Stable Diffusion XL [51]. To further enhance data quality, we implemented set of exclusive protocols for filtering unqualified data, which is illustrated in Fig. 3. The qualified data are resupplied to each task to ensure certain amount after filtering. Data Processing. We conduct the post-processing to ensure 1https://www.xiaohongshu.com/explore 2https://www.youtube.com 3https://www.google.com the linguistic consistency of the benchmark. We utilized the GPT-4o API to translate the annotated Chinese text to English in several common tasks, followed by double-check from data experts to verify the accuracy of translations. Additionally, for images containing Chinese characters, we implemented image translation4 from Chinese to English. We finally adjusted prompts in each task for desired generation results, which are detailed in Appendix Sec. A.3. Dataset Splitting. As illustrated in Fig. 2, our OpenING benchmark ultimately includes 5,400 annotated instances, spanning 23 distinct meta-topics and 56 tasks. The annotated instances of OpenING are divided into Dev Set (3,240 instances) and Test Set (2,160 instances). The Dev Set supports the training of judge models, and the Test Set is used to evaluate the zero-shot performance of different models. 4. IntJudge Model 4.1. Interleaved Arena Evaluating open-ended interleaved image-text generation is challenging due to 1) the complexity of assessing multiple images and text, and 2) the open-ended nature of interleaved generation, where multiple valid answers can exist. Given that pairwise comparison is more stable than subjective scoring [14], we develop the Interleaved Arena, which supports pairwise evaluation using three pipelines: human judges, GPT-based judges, and the proposed IntJudge. In the Interleaved Arena, interleaved outputs from agents on the OpenING Test Set are saved in unified format. In each evaluation round, judges compare outputs from two 4https://github.com/zyddnys/manga-image-translator anonymous agents and rate the interleaved outputs based on seven criteria: Correctness, Image-Text Coherency, Multistep Consistency, Content Quality, Human Preference Alignment, Completeness, and Content Richness (see Appendix Sec. B.1 for more details). To balance evaluation reliability and efficiency, we propose roulette matching algorithm to sample distinct battle pairs for each data instance: Let represent the set of tasks, where each task contains Dk data instances. The set of agents in the arena is denoted as M. For each task k, permutation σk AM is sampled by randomly shuffling the agent order, where AM is the set of all permutations of agents in M. The set of sampled battle pairs is given by: Pk = {(σk(i mod M), σk ((i + 1) mod M))} , (2) where = 1, 2, . . . , Dk. Additional sampling rounds are conducted to obtain total of distinct battle pairs for each data instance, where M(M 1)/2. To avoid duplication, we maintain set Rk,d for the d-th round to track previously sampled pairs: Rk,d = d1 (cid:91) j=1 (σk,j(a), σk,j(b)). (3) For newly sampled pair σk,d(a) and σk,d(b), we enforce: (σk,d(a), σk,d(b)) / Rk,d and σk,d(a) = σk,d(b). (4) Under assumption of uniform distribution, we define the coverage time Tk to ensure all agents are evaluated in task k: Tk = (cid:24) M(M 1) 2E (cid:25) , Dk Pk and the overall expected coverage time is given by: E[T ] = 2 HM = 2 (cid:88) i=1 , 1 (5) (6) where HM is the M-th harmonic number. 4.2. Judge Pipelines Human Judge. In the human judge, annotators compare outputs from two multimodal agents for each input prompt and select winner based on seven predefined criteria. The voting results are used to rank interleaved generation methods based on their win rates. Since the previous studies [14, 89] noted that excessive ties cause inefficiency, our annotators are instructed to favor one agent in cases of tie, denoting as Tie(A) or Tie(B) based on the slight preference. GPT-based Judge. To enable scalability, we employ GPT-4o to automate the evaluation process. The GPT-4o is prompted to analyze interleaved outputs and decide the winner for each battle pair. Moreover, we use an additional prompt to obtain the score breakdown and explanations. While this allows for scalable evaluation with explainability, GPT-based judges still have high error rate due to their prior bias and lack of alignment with human preferences. Besides, using GPT will bring concerns about privacy, data leakage, and cost. IntJudge. To address issues in GPT-based evaluators, we propose IntJudge for more accurate evaluations and better alignment with human preferences. As an offline judge, IntJudge efficiently handles large-scale evaluations with consistent criteria, ensuring fair and reproducible results for benchmarking interleaved image-text generation. Specifically, we explored different MLLMs including InternLMXComposer2.5 (InternLMX2.5) [85] and Qwen2-VL [68], ultimately choosing Qwen2-VL-7B as the foundational model for training IntJudge to achieve an optimal balance between efficiency and accuracy. 4.3. Training of IntJudge To enhance the training of IntJudge, we propose ReferenceAugmented Generation (RAG) approach to scale up the training dataset. As illustrated in Fig. 3, we train our IntJudge model on the combination of human-annotated pairwise data from Dev Set and the RAG pairwise data. To generate scalable pairwise data using our RAG approach, models were provided with gold real-world answers from the Dev Set and prompted to generate responses based on these gold answers. pairwise data is formed by pairing plain generation result with an RAG-based output, with the RAG result assigned as the winner. bag of models, including seen interleaved generation methods are used for plain generation and RAG. The total training objective is defined as: Ltotal = λ1LCE + λ2LCT + λ3LMSE + λ4LPR, (7) where λ1, λ2, λ3 and λ4 are weighting coefficients, LCE, LCT, LMSE, and LPR are respectively cross-entropy, contrastive, MSE, and pairwise ranking losses. The trained IntJudge was tested in zero-shot setting on both unseen and seen models to validate its generalizability. 5. Experiments 5.1. Experimental Setup Models. We test 10 representative methods and categorized them into three types: 1) Integrated pipeline consists of independent models for generating text and image in two stages, including GPT-4o+DALL-E3 (DALL-E3) [8, 48] and Gemini1.5+Flux [9, 63]; 2) Two-stage generator uses unified model architecture and outputs text and image in separate stages, including Emu2 [59], SEED-X [23], and Showo [77]; 3) End-to-end generator outputs image-text results in single stage, including GILL [35], NExT-GPT [74], MiniGPT-5 [88], SEED-LLaMA [22], and Anole [16]. We Method Human Evaluation GPT Evaluation IntJudge Evaluation FDT w/o Tie w/ Tie (0) w/ Tie (.5) FDT w/o Tie w/ Tie (0) w/ Tie (.5) FDT w/o Tie w/ Tie (0) w/ Tie (.5) Human 83.28% 86.03% 68.17% 78.55% 82.49% 82.69% 82.03% 82.43% 87.46% 91.49% 75.49% 84.23% GPT-4o+DALL-E3 78.42% 81.39% 65.21% 75.15% 85.70% 85.99% 85.58% 85.82% 85.02% 86.92% 72.22% 80.68% 65.57% 65.82% 49.31% 61.85% 71.75% 71.76% 71.12% 71.56% 68.30% 69.73% 54.47% 65.41% Gemini1.5+Flux 51.98% 49.49% 34.70% 49.65% 54.82% 55.12% 54.11% 55.03% 49.86% 49.58% 33.57% 49.72% SEED-X 51.90% 52.17% 36.46% 51.52% 53.36% 53.13% 52.58% 53.10% 53.42% 52.04% 33.92% 51.33% Anole 44.30% 42.12% 29.11% 44.56% 40.96% 40.87% 40.46% 40.96% 50.13% 47.71% 31.57% 48.48% SEED-LLaMA 40.89% 37.07% 23.42% 41.84% 41.72% 41.63% 40.58% 41.85% 36.28% 33.79% 21.87% 39.51% Emu2 36.28% 34.02% 21.63% 39.84% 30.77% 30.22% 29.61% 30.62% 31.49% 21.08% 12.48% 32.87% Show-o 33.67% 26.93% 17.09% 35.36% 22.61% 22.39% 22.11% 22.74% 30.96% 21.70% 13.36% 32.58% NExT-GPT 30.69% 26.72% 17.11% 35.09% 28.64% 28.37% 28.02% 28.64% 24.47% 15.46% 9.91% 27.85% MiniGPT-5 25.80% 19.57% 12.71% 30.23% 30.55% 30.24% 29.65% 30.62% 24.87% 19.72% 12.82% 30.32% GILL Table 2. Comparison of model win rates evaluated by human, GPT-4o, and our IntJudge under FDT and different tie metrics. FDT: Force Dividing Tie metric. w/o Tie: Non-tie case. w/ Tie (0) and w/ Tie Tie (.5): Count tie as 0 and 0.5 wins for model in battle, respectively. Figure 4. Model win rates under image-only and text-only settings across different models, ranked by human judgments. keep GPT-4o+DALL-E3, Anole, SEED-LLaMA, and NExTGPT as unseen models for IntJudge validation, and the rest are models seen in IntJudge training. Evaluation Metrics. We assess model performance using two key metrics: win rate and agreement. Win rate indicates how often model wins in pairwise comparisons. We consider four methods for handling ties: 1) Force Dividing Tie (FDT): We force judges to assign ties with more leaning model in rules and prompts, ensuring that every comparison results in decisive outcome. If tie leans towards model (Tie(A)), it counts as win for A; similarly for B. This method allows for clear rankings without ambiguity. 2) Without Tie (w/o Tie): Tied comparisons are excluded; only matches with clear winner are considered; 3) With Tie counted as 0 (w/ Tie (0)): Ties are included but do not contribute to the win count of either model; 4) With Tie counted as 0.5 (w/ Tie (.5)): Each tie contributes half win Figure 5. Win rate matrix of human and ten MLLM models, evaluated by human, GPT-4o, and our IntJudge, respectively. to both models. Agreement measures the consistency between different evaluators (e.g., automated pipelines and human judgments) under the same tie-handling strategies. It reflects how often the evaluators concur in their assessments. 5.2. Overall Evaluation Evaluation of Three Judges. We conduct experiments to evaluate the performance of different models using the win rate and agreement metrics. Table 2 showcases the win rates of various models under different judge methods, including Human, GPT-based, and IntJudge-based Evaluations. The sampling round is set in 2 to form 4,320 battle pairs. It is found that the integrated pipelines like GPT-4o+DALL-E3 and Gemini 1.5+Flux consistently perform better across all evaluators, while the end-to-end models like MiniGPT-5, GILL, and NExT-GPT exhibited lower performance. Pairwise Model Performance. We visualize the pairwise comparison results of all methods in Fig. 5, evaluated by human, GPT-4o, and IntJudge, respectively. The heat map reveals win-loss relationships, where warmer colors represent higher win rates and cooler colors vice versa. Notably, GPT4o+DALL-E3 and Gemini1.5+Flux demonstrate strong win rates against other models, even comparable to the manually annotated output under GPT evaluation. Evaluator FDT w/ Tie w/o Tie Average Seen Unseen HM Average Seen Unseen HM Average Seen Unseen HM 49.83% 49.86% 49.79% 49.83% 32.60% 32.03% 33.18% 32.60% 50.00% 48.36% 51.89% 50.06% Random GPT-4o 71.08% 73.33% 68.77% 70.98% 51.93% 54.95% 48.82% 51.70% 74.58% 77.54% 71.43% 74.36% InternLMX2.5-7B 56.81% 55.73% 57.92% 56.81% 40.26% 40.19% 40.33% 40.26% 61.05% 61.21% 60.97% 61.09% Qwen2-VL-7B 61.61% 61.59% 61.63% 61.61% 32.81% 31.16% 34.50% 32.75% 80.77% 81.15% 80.23% 80.69% IntJudge-7B (Ours) 82.42% 84.05% 80.75% 82.37% 66.45% 69.02% 63.80% 66.31% 91.11% 92.38% 89.55% 90.94% Table 3. Agreement rate between different MLLM-based judges and human judgments in different metrics. HM: Harmonic Mean. Figure 7. Effect of sampling size on evaluation reliability. Figure 6. Evaluation results of GPT-based scores. (a)-(c): Average score of all criteria on each meta-topic for different kinds of models. (d) Average score of all meta-topics on each criterion. Text-only and Image-only Evaluation. To explore whether text or image has greater impact on the model performance, we evaluate models on text-only and image-only outputs on the same sampled pairs. Fig. 4 shows that MiniGPT-5 and GILL do not perform well mainly due to the low quality of text outputs. SEED-X and NExT-GPT achieve higher win rates on text-only evaluation, however, the lower quality of generated images hinders their opportunity to achieve higher ranking in Table 2. It is observed that text generated by GPT4o even outperforms real-world content annotated by human, highlighting the superior language capabilities of GPT-4o. GPT-based Scoring. As shown in Fig. 6, detailed scores from GPT-based evaluations are provided to support explainable performance analysis of different models. It is observed that GPT-4o+DALL-E3 performs suboptimally in metatopics like Interactive Image Editing (IIE) and Embodied-AI Tasks (ET), possibly due to the lack of relevant training data in these scenarios. GPT-4o also showcases the inherent biases to its own generation results. For example, GPT-4o gives 10 scores to its own answers in Human Preference Alignment. In contrast, outputs annotated by humans only achieve Figure 8. Comparison of agreement with human judgments for IntJudge trained without and with RAG data. an average score of 9 in human preference alignment. Agreement with Human. Table 3 shows the agreement between different evaluators and human judgments. We implement random guess (Random) as baseline. The results indicate that IntJudge generally achieved higher agreement with human judgments (82.42% in FDT) compared to GPTbased evaluation (71.08% in FDT), suggesting its potential for scalable evaluation of interleaved image-text generation. 5.3. Ablation Studies Ablation on Sampling Size. We evaluate the effect of sample size on evaluation stability and reliability. Fig. 7 illustrates the trend of win rates across varying sampling sizes. As the sample size increases, the win rates approach stability and show minimal variation across further increases. This stabilization suggests that our sampling number of 4,320 battle pairs is able to support the robust evaluation results. Ablation on Judge Training Data. We investigate the influence of incorporating RAG data on the performance of the IntJudge. The comparison is conducted between two training configurations: one utilizing only the arena data (6,014 Method FDT w/o Tie w/ Tie (0) w/ Tie (.5) Human+Human Human+Flux-dev 88.39% 92.23% 84.82% 88.84% 11.16% 11.61% 7.77% 7.14% GPT+DALL-E3 GPT+Flux-dev 49.51% 45.10% 22.33% 47.57% 50.49% 54.90% 27.18% 52.43% Gemini+Flux-sch Gemini+Flux-dev 41.25% 41.43% 23.39% 42.14% 58.75% 58.57% 39.11% 57.86% SEED-X+SEED-X 9.82% SEED-X+Flux-dev 11.16% 90.18% 94.85% 82.14% 88.84% 4.46% 5.15% Table 4. Evaluation results of interleaved content when basic text output combined with different image generation models. Figure 9. Error distribution of three models: GPT-4o+DALL-E3 (integrated), SEED-X (two-stage), and Anole (end-to-end). samples) and the other augmented with RAG data (25,982 samples). As illustrated in Fig. 8, with RAG data included, the FDT agreement on unseen models increases by 7.8%, demonstrating the effectiveness of our RAG-based strategy. Ablation on Image Generator. We sample 200 data instances across tasks to assess the influence of different image generators on interleaved performance. Table 4 presents comparison of several basic text generation methods combined with different image generators. We assign the text generated by Gemini with Flux-schnell (Flux-sch) and Flux-dev to explore the difference. The results highlight that advanced image generators like Flux-dev are crucial for improving the overall quality of interleaved content. Meanwhile, it is noted that the generation efficiency of Flux-dev is significantly slow. This prompts further exploration of flow models to better balance generation quality and efficiency. 5.4. Analysis and Discussions Error Analysis. We conduct an error analysis on 200 sampled instances where the output of models performed worse than human output. Fig. 9 shows the frequency of error types across three different types of models, providing insights into their specific performance limitations. GPT-4o+DALLE3 suffers from content incoherency and inconsistency since it is hard for DALL-E3 to generate multiple images in the same style. Poor image quality is the main problem of Anole, as its finetuning data for image generation is insufficient. While most outputs by SEED-X have multiple errors, the inMethod Human GPT-4o+DALL-E3 Gemini1.5+Flux SEED-X Anole SEED-LLaMA Emu2 Show-o NExT-GPT MiniGPT-5 GILL Ratio No-Image No-Text No-I&T 0.00% 0.23% 0.09% 23.17% 19.46% 4.77% 0.00% 0.00% 43.97% 0.27% 19.95% 0.00% 0.00% 0.09% 4.64% 2.00% 0.05% 15.10% 7.74% 0.09% 26.54% 13.43% 0.00% 0.00% 0.09% 4.46% 1.30% 0.00% 0.00% 0.00% 0.09% 0.00% 0.28% Table 5. The ratios of No-Image, No-Text, and No-Image-and-Text (No-I&T) outputs relative to the total number of generated samples. existence of text or image content is still the major problem. No-Image and No-Text Ratios. Table 5 presents the noimage, no-text, and no-image-and-text ratios of different models, representing the proportion of instances where the models fail to generate visual content, textual content, and both. Human, GPT-4o+DALL-E3, and Genimi1.5+Flux exhibited near-zero failure rates (except that image generation on certain sensitive topics is prohibited according to content safety policies), indicating consistent multimodal generation. On the other hand, models like SEED-X and NExT-GPT showed high no-image ratio. This can be attributed to poor instruction following and generation ability. These findings suggest that models need to generate both images and text properly to achieve higher rankings on OpenING. Findings and Discussions. We discuss key findings from our experiments to inspire future works: 1) While all generative models ranked lower than Human in interleaved generation, end-to-end models still lagged significantly behind integrated pipelines like GPT-4o+DALL-E3 in interleaved generation tasks. Two-stage generation methods based on unified models are also expected to be further improved. 2) Natural images consistently outperform generated images, highlighting the challenge of generating high-quality images. 3) Text generated by GPT can match or even surpass the quality of human-annotated text, showcasing the effectiveness of LLMs in producing rich and informative content. 4) Foundational generative models play big role. For example, when combining GPT, Gemini, and SEED-X with Flux-dev, respectively, all models demonstrate higher performance. 5) As large-scale data is crucial for training judge models, using our RAG method to scale up data beyond manual annotation contributed to the building of more robust judge model. 6. Conclusion We introduce OpenING, comprehensive benchmark designed to evaluate open-ended interleaved image-text generation. OpenING overcomes the limitations of existing benchmarks by including more diverse data and tasks grounded in real-world scenarios. To address the challenges in assessing open-ended multimodal generation, we propose robust judge model IntJudge, which is trained on both humanannotated and RAG-based data from the Dev Set of OpenING. We evaluate various interleaved generation methods on the Test Set of OpenING, revealing the significant challenges for generating coherent and high-quality interleaved image-text content. Ablation studies further demonstrate the effectiveness of our RAG-based data pipeline in training IntJudge. Looking forward, increasing the size and diversity of the data could further enhance the practical relevance of interleaved generation benchmarks. We anticipate that OpenING will inspire further research in MLLMs, and support the development of multimodal evaluation models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Proceedings of the Advances in Neural Information Processing Systems, 2022. 3 [3] Amar Ali-bey, Brahim Chaib-draa, and Philippe Gigu`ere. BoQ: place is worth bag of learnable queries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1779417803, 2024. 16 [4] Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, and Jiebo Luo. Openleaf: Open-domain interleaved image-text generation and evaluation. arXiv preprint arXiv:2310.07749, 2023. 1, 2, 3 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 [6] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-modelas-an-examiner. In Proceedings of the Advances in Neural Information Processing Systems, 2024. 3 [7] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. 16 [8] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science, 2(3):8, 2023. 5, 28, 29 [9] Black Forest Labs. Flux. https://github.com/ blackforestlabs/flux, 2024. Accessed: 202411-05. 5, 28, [10] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 15 [11] Ju-Seung Byun, Jiyun Chun, Jihyung Kil, and Andrew Perrault. ARES: Alternating reinforcement learning and supervised fine-tuning for enhanced multi-modal chain-of-thought arXiv preprint reasoning through diverse AI feedback. arXiv:2407.00087, 2024. 1 [12] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 16 [13] Yitao Cai, Huiyu Cai, and Xiaojun Wan. Multi-modal sarcasm detection in Twitter with hierarchical fusion model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. 17 [14] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. MLLM-as-a-Judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Proceedings of the International Conference on Machine Learning, 2024. 3, 4, 5, [15] Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, and Lichao Sun. Gui-world: dataset for gui-oriented multimodal llm-based agents, 2024. 15 [16] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. 3, 5, 28, 29 [17] Daniel Claman, Emre Sezgin, et al. Artificial intelligence in dental education: Opportunities and challenges of large language models and multimodal foundation models. JMIR Medical Education, 10(1):e52346, 2024. 1 [18] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision (ECCV), 2018. 16 [19] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 116, 2017. 17 [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning, 2024. [21] Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, and Jie Zhou. Towards expressive communication with internet memes: new multimodal conversation dataset and benchmark. arXiv preprint arXiv:2109.01839, 2021. 17 [22] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. 5, 28, 29 [23] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 3, 5, 15, 28, 29 [24] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making LLaMA SEE and Draw with SEED Tokenizer. In Proceedings of the International Conference on Learning Representations, 2024. 3 [25] Matthew Glasser, Timothy Coalson, Emma Robinson, Carl Hacker, John Harwell, Essa Yacoub, Kamil Ugurbil, Jesper Andersson, Christian Beckmann, Mark Jenkinson, et al. multi-modal parcellation of human cerebral cortex. Nature, 536(7615):171178, 2016. 1 [26] Yulia Gryaditskaya, Mark Sypesteyn, Jan Willem Hoftijzer, Sylvia Pont, Fredo Durand, and Adrien Bousseau. Opensketch: richly-annotated dataset of product design sketches. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 38, 2019. [27] Chunhui Gu, Chen Sun, David Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 60476056, 2018. 16 [28] Conghui He, Wei Li, Zhenjiang Jin, Chao Xu, Bin Wang, and Dahua Lin. OpenDataLab: Empowering general ararXiv preprint tificial intelligence with open datasets. arXiv:2407.13773, 2024. 4 [29] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 3 [30] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In Proceedings of the Advances in Neural Information Processing Systems, 2017. 3 [31] Judith Holler and Stephen Levinson. Multimodal language processing in human communication. Trends in Cognitive Sciences, 23(8):639652, 2019. 1 [32] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016. [33] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. International Conference on Learning Representations (ICLR), 2024. 15 [34] Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo, Juho Kim, and Jinwook Seo. Large-scale text-to-image generation models for visual artists creative works. In Proceedings of the International Conference on Intelligent User Interfaces, 2023. 1 [35] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. In Proceedings of the Advances in Neural Information Processing Systems, 2024. 3, 5, 28, 29 [36] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE International Conference on Computer Vision, 2017. 1 [37] Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, and Xiaoming Zhai. Artificial general intelligence (AGI) for education. arXiv preprint arXiv:2304.12479, 1, 2023. [38] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. In Proceedings of the Advances in Neural Information Processing Systems, 2024. 1 [39] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Proceedings of the ACL Workshop: Text Summarization Branches Out, pages 7481, 2004. 3 [40] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm - open-ended visual storytelling via latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61906200, 2024. 15 [41] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mGPT: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. 3 [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Proceedings of the Advances in Neural Information Processing Systems, 2024. 1 [43] Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, and Lifu Huang. Holistic evaluation for interleaved text-and-image generation. arXiv preprint arXiv:2406.14643, 2024. 1, 2, [44] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 1 [45] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. 15 [46] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. In Proceedings of the Advances in Neural Information Processing Systems, 2024. 1 [47] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. 16 [48] OpenAI. Hello GPT-4o. https://openai.com/ index/hello-gpt-4o/, 2024. Accessed: 2024-05-26. 4, 5, 28, 29 [49] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: method for automatic evaluation of machine In Proceedings of the annual meeting of the translation. Association for Computational Linguistics, 2002. 3 [50] Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. Doclaynet: large humanannotated dataset for document-layout segmentation. page 37433751, 2022. [51] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In Proceedings of the International Conference on Learning Representations, 2024. 4 [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 3 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Bision and Pattern Recognition, 2022. 3 [54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Proceedings of the Advances in Neural Information Processing Systems, 2016. 3 [55] Nils Schaetti. Sfgram: dataset containing thousands of scienc-fiction books and novels. https://github.com/ nschaetti/EchoTorch, 2018. 15 [56] Danqing Shi, Weiwei Cui, Danqing Huang, Haidong Zhang, and Nan Cao. Reverse-engineering information presentations: Recovering hierarchical grouping from layouts of visual elements. Visual Intelligence, 1(1):9, 2023. [57] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web videos using titles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 51795187, 2015. 16 [58] Francesco Stella, Cosimo Della Santina, and Josie Hughes. How can LLMs transform the robotic design process? Nature Machine Intelligence, pages 14, 2023. 1 [59] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are incontext learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14398 14409, 2024. 5, 28, 29 [60] Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Mengping Yang, Cheng Zhang, and Hao Li. Evalalign: Supervised fine-tuning multimodal llms with human-aligned data for evaluating textto-image models. CoRR, 2024. 1 [61] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. CoDi-2: In-context interleaved In Proceedings of and interactive any-to-any generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2742527434, 2024. 1 [62] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1, 3, [63] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 5, 28, 29 [64] InternLM Team. InternLM: multilingual language model with progressively enhanced capabilities, 2023. 1 [65] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In Proceedings of the Advances in Neural Information Processing Systems, 2024. 3 [66] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [67] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 16 [68] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [69] Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian, and Zhiqiang Zhang. hybrid bandit model with visual priors for creative ranking in display advertising. In Proceedings of the Web Conference 2021, pages 23242334, 2021. 15 [70] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 3, 28, 29 [71] Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, et al. PandaLM: An automatic evaluation benchmark for llm instruction tuning optimization. In Proceedings of the International Conference on Learning Representations, 2024. 2, 3 [72] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021), 2021. 16 [73] Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, et al. Towards open-ended visual quality comparison. arXiv preprint arXiv:2402.16641, 2024. 3 [74] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. NExT-GPT: Any-to-Any Multimodal LLM. In ProKai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. InternLM-XComposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. [86] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. GPT-4V (ision) as generalarXiv preprint ist evaluator for vision-language tasks. arXiv:2311.01361, 2023. 3 [87] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint, 2024. 16 [88] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. MiniGPT5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239, 2023. 3, 5, 28, 29 [89] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. In Proceedings of the Advances in Neural Information Processing Systems, 2023. 3, 5, 25 [90] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 10151022. IEEE, 2019. 15 [91] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. 16 [92] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1 [93] Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Heng Fan, Qinghua Hu, and Haibin Ling. Detection and tracking meet drones challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):73807399, 2021. 17 ceedings of the International Conference on Machine Learning, 2024. 5, 28, [75] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Liang Pan Jiawei Ren, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 16 [76] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. VILA-U: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 28, 29 [77] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 5, 28, 29 [78] Zhe Xu, Dacheng Tao, Ya Zhang, Junjie Wu, and Ah Chung Tsoi. Architectural style classification using multinomial latent logistic regression. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 600615. Springer, 2014. 15 [79] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. 3, 15 [80] Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. Visual goal-step inference using wikihow. arXiv preprint arXiv:2104.05845, 2021. [81] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of LMMs: Preliminary explorations with GPT-4V (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. 1 [82] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. MMT-Bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. 1 [83] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 1 [84] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. InternLM-XComposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 1 [85] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, GATE OpenING: Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation Supplementary Material In this appendix, we provide additional information, discussions, and results in support of the primary text, which are organized as follows: Sec. includes the details of OpenING and data curation. Sec. presents the details of evaluation and IntJudge. Sec. provides the details of our experiments. Sec. introduces the fine-tuning results of MLLMs on our OpenING benchmark. Sec. discusses the limitations of this study. A. Details of OpenING and Data Curation A.1. Hierarchical Structure of OpenING We present in Table 6 the 56 specific tasks from OpenING that derived from 23 meta-topics. The number of Instances (# of Ins.), Meta-topic names, and capabilities of MLLMs evaluated in testing are provided. Based on the required annotation skills, the 56 tasks are divided into 38 common tasks and 18 hard tasks. The common tasks are annotated by 28 professional annotators, instructed and supervised by 14 data experts. The annotations of hard tasks, which require specific domain knowledge and special data reasoning and processing techniques, are conducted by the 14 data experts. A.2. Data Sources We list all sources where meta-data are collected for the annotation of our OpenING benchmark. Annotators arrange the images collected from the sources into the standardized multi-step format and annotate the text for each corresponding image. The details of the data source are presented in Table 7, including the ID number of each task. We also provide examples of the source of data instances to show how the desired data are searched from certain platform. A.3. Task Prompt Breakdown We present comprehensive breakdown of the task prompts used in our experiments. The tasks are designed to evaluate various capabilities of interleaved image-text generation methods, ranging from storytelling and creative design to problem-solving and interactive experiences. For each task, we provide the general prompt format and specific prompt examples to illustrate how the models are expected to generate interleaved image-text outputs in Table 10. By carefully designing the general prompt templates for these tasks and refining their corresponding prompt examples in data instances we aim to challenge all interleaved generation methods using more diverse queries, showing the generalization performance of methods more comprehensively. A.4. Exclusive Protocols for Data Filtering The maximum number of steps per instance was limited to ten to ensure usability with context restraints. The instances with more than ten steps were excluded. All queries and answers are annotated manually via our developed tool, IntLabel. We implemented set of exclusive protocols for filtering unqualified data, which include: 1) Removing data without coherence. 2) Removing mismatched text and images. 3) Removing data involving violence, offensive content and other content safety concerns. 4) Removing duplicated data. 5) Avoiding images consisting of only text. 6) Removing data that is inconsistent with real-world logic. 7) Avoiding content misaligned with real user needs. We repeat the above data collection and filtering process for each task until the number of instances reaches our target. A.5. Task Abbreviations Given the large number of tasks in our benchmark and the wide range of methods evaluated, the abbreviations of the 23 meta-topics and 56 tasks are provided respectively in Table 8 and Table 9. A.6. IntLabel for Annotation To facilitate the annotation process and ensure consistent annotations standard across annotators, we developed and opensourced an annotation tool for interleaved image-text data labeling. Our IntLabel annotation tool is developed based on PyQt5, and the IntLabel GUI is presented in Fig. 10. All queries and answers in OpenING are annotated, checked, and refined manually via IntLabel. The annotated JSONL files and corresponding images are kept in the same folder. A.7. OpenING Data Illustrations of the representative example of the 23 metatopics are provided in Fig. 18 to showcase the diversity and complexity of tasks in our OpenING benchmark. These examples highlight the variety of interleaved image-text data that OpenING encompasses, demonstrating the challenges and capabilities required for effective interleaved image-text generation. 5https://www.riverbankcomputing.com/software/pyqt Table 6. Details of the 38 common tasks and 18 hard tasks in our OpenING Benchmark. Task Name # of Ins. Meta-Topic Common Tasks Travel Guide Generation Museum Guide Book Generation Dynamic Biography Generation Multimodal Report Completion Interior Design Architectural Design Art and Exhibition Design Product Design Interactive Graphic Advertisement Editing Geometric Problem Test Circuit Problem Test Mind Map Generation Figure Relationship Diagram Generation Multi-view News Generation Dynamic Sports Event Analysis Interactive Historical Interpretation Unsolved Mysteries Exploration Multimodal Biological Reasoning Multimodal Landscape Reasoning Multimodal Analogy Reasoning Interactive Jigsaw Puzzle Interactive Multi-concept Image Composition Interactive Film and Television Recommendation Interactive Goods Recommendation Interactive Food Recommendation Business Scenarios Brainstorming Academic Scenarios Brainstorming Multimodal Action Anticipation Visual Traffic Forecasting Interactive Remote Sensing Image Rendering Interactive Street View Image Rendering Urban Planning and Development Simulation Plog and Social Media Content Generation Interactive Virtual Try-on Multimodal Dressing Suggestion Fashion Trend Forecasting Multimodal Recipe Generation Multimodal Cooking Assistant Interactive Science Popularization Fitness and Health Consulting Multimodal Action Anticipation Story Writing Fiction Writing Document with Layout Generation Slide with Note Generation Storybook Completion Web GUI Navigation In-APP GUI Navigation Cross-APP GUI Navigation OS GUI Navigation Interactive Portrait Image Editing Interactive Landscape Image Editing Interactive Novel View Synthesis Dream Analysis and Scene Reconstruction Interactive Multi-concept Image Composition Scientific Brainstorming Chat with Memes Autonomous Driving and In-door Navigation 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 75 100 50 100 100 100 100 75 100 100 100 100 100 50 100 50 Multimodal Report Generation Multimodal Report Generation Multimodal Report Generation Multimodal Content Completion Interactive Visual Design Interactive Visual Design Interactive Visual Design Interactive Visual Design Interactive Visual Design Multimodal Exam Multimodal Exam Graph Generation Graph Generation Event Reasoning & Deductive Simulation Event Reasoning & Deductive Simulation Event Reasoning & Deductive Simulation Event Reasoning & Deductive Simulation 2D Image Reasoning 2D Image Reasoning 2D Image Reasoning 2D Image Reasoning Multimodal Information Summary Multimodal Information Recommendation Multimodal Information Recommendation Multimodal Information Recommendation Multimodal Brainstorming Multimodal Brainstorming Multimodal Time Series Forecasting Multimodal Time Series Forecasting Geographical Tasks Geographical Tasks Geographical Tasks Social Media Tasks Fashion Tasks Fashion Tasks Fashion Tasks Cooking Tasks Cooking Tasks Educational Tasks Healthcare Tasks Hard Tasks Multimodal Time Series Forecasting Storybook Creation Storybook Creation Multimodal Layout Generation Multimodal Layout Generation Multimodal Content Completion GUI Navigation GUI Navigation GUI Navigation GUI Navigation Interactive Image Editing Interactive Image Editing Image-based 3D Reasoning Event Reasoning & Deductive Simulation Multi-concept Composition Multimodal Brainstorming Social Media Tasks Embodied-AI Tasks Capabilities Content Creation Content Creation Content Creation Content Completion Design & Brainstorming Design & Brainstorming Design & Brainstorming Design & Brainstorming Design & Brainstorming Education Assistant Education Assistant Summary Agent Summary Agent Deductive Simulation Deductive Simulation Deductive Simulation Deductive Simulation Visual Reasoning Visual Reasoning Visual Reasoning Visual Reasoning Summary Agent Information Recommendation Information Recommendation Information Recommendation Design & Brainstorming Design & Brainstorming Time Series Forecasting Time Series Forecasting Domain-specific Applications Domain-specific Applications Domain-specific Applications Domain-specific Applications Domain-specific Applications Domain-specific Applications Domain-specific Applications Domain-specific Applications Domain-specific Applications Education Assistant Domain-specific Applications Time Series Forecasting Content Creation Content Creation Content Creation Content Creation Content Completion Interactive Agent Interactive Agent Interactive Agent Interactive Agent Interactive Agent Interactive Agent Visual Reasoning Deductive Simulation Summary Agent Design & Brainstorming Domain-specific Applications Domain-specific Applications Table 7. Task Sources: ID number and data sources of each task. Task ID 1.1 Task Name Story Writing Task Source SEED-Story [79], StoryGen [40] and Storybird (https://storybird. com/read-picture-book) SFGram [55] Fiction Writing Travel Guide Generation Xiaohongshu (https://www.xiaohongshu.com/) and Mafengwo"
        },
        {
            "title": "Museum Guide Book\nGeneration",
            "content": "Dynamic Biography Generation Storybook Completion (https://www.mafengwo.cn/) Xiaohongshu (https://www.xiaohongshu.com/) and Regional museum Websites, e.g., the Shanghai Museum website (https : / / www.shanghaimuseum.net/mu/frontend/pg/article/id/ RI00004029). Wikipedia (https://en.wikipedia.org/), Baidu Baike (https: //baike.baidu.com/) SEED-Story dataset [79], VIST (https://visionandlanguage. net/workshop2018/) and Storybird (https://storybird.com/ read-picture-book) Xiaohongshu (https://www.xiaohongshu.com/ ReIP [56] and manually collected in-house slides PubLayNey [90] and DocLayNet [50] Multimodal Report Completion Document with Layout Generation Slide with Note Generation Web GUI Navigation GUI Odyssey [45] and GUI World [15] In-APP GUI Navigation GUI Odyssey [45] and GUI World [15] Cross-APP GUI NavigaGUI Odyssey [45] and GUI World [15] tion OS GUI Navigation Interactive Portrait Image Editing Interactive Image Editing Interior Design Architectural Design Landscape GUI Odyssey [45] and GUI World [15] InstructPix2Pix [10], PnPInversion [33] and SEED-Data-Edit [23] InstructPix2Pix [10], PnPInversion [33] and SEED-Data-Edit [23] images and design datasets Xiaohongshu (https://www.xiaohongshu.com/) Architecture Style Dataset [78] and Architecture-Design-DataSources (https://github.com/rickkk856/ArchitectureDesignDataSources?tab=readme-ov-file) (https : / / www . kaggle . Art com / datasets / thedownhill / art - images - drawings - painting - sculpture - engraving), websites (https://caam.caa.edu.cn/news/202407/81035.html) and Xiaohongshu (https://www.xiaohongshu.com/) OpenSketch [26], Package Design Dataset (https://www.kaggle. com / datasets / dagloxkankwanda / package - design - dataset) and Xiaohongshu (https://www.xiaohongshu.com/) CreativeRanking [69] and AI-generated content museum Bilibili (https://www.bilibili.com/video/BV1ZV4y1u728/ ?spm _ id _ from = 333 . 337 . search - card . all . click & vd _ source=4476502a7ee5a251d519afe9ea874750). Bilibili (https://www.bilibili.com/video/BV1RU4y1v7Wj/ ?spm _ id _ from = 333 . 337 . search - card . all . click & vd _ source=4476502a7ee5a251d519afe9ea874750). Art and Exhibition Design Product Design Interactive Graphic Advertisement Editing Geometric Problem Test Circuit Problem Test 1.2 2. 2.2 2.3 3.1 3.2 4.1 4. 5.1 5.2 5.3 5.4 6.1 6.2 7.1 7.2 7.3 7. 7.5 8.1 8.2 9.1 9.2 10. 10.2 10.3 10.4 10.5 11.1 11. 11.3 11.4 12 13.1 14.1 14. 14.3 15.1 15.2 16.1 16.2 Mind Map Generation Test Figure Relationship Diagram Generation Multi-view News Generation"
        },
        {
            "title": "Dynamic Sports Event\nAnalysis",
            "content": "Interactive Historical Interpretation Unsolved Mysteries Exploration Google (https://datavizproject.com/datatype/mindmap/) Xiaohongshu (https : / / www . xiaohongshu . com/) and Zhihu (https://www.zhihu.com/) Wikipedia (https://en.wikipedia.org/), Xiaohongshu (https: //www.xiaohongshu.com/), Sina News (https://news.sina. com.cn/) and Huanqiu (https://www.huanqiu.com/) (https : / / sports . qq . com/), Sina Sports Tencent Sports (https://sports.sina.com.cn/g/pl/20240711/docincctefn8913329.shtml) (https : / / www . xiaohongshu . com / user / Xiaohongshu profile / 664818b80000000003033db6 , http : / / xhslink . com/XDcnnS) and AI-generated content (https : / / www . xiaohongshu . com / Xiaohongshu user , https : / / www . xiaohongshu . com / explore / = 662783cc000000000401944a AB5RmgUizZbLQLmLj8zWSmutvLdUpKq6gA30qz647fKv0 = &xsec_source=pc_search) and AI-generated content Zhougongs Dream Interpretation (https://m.zgjmorg.com/), inhouse dream records and AI-generated content CUB-200 [67] and Oxford 102 Flower [47] 664818b80000000003033db6 profile token xsec ? _ / / Dream Analysis and Reconstruction Multimodal Biological Reasoning Multimodal Landscape Reasoning Multimodal Reasoning Interactive Jigsaw Puzzle Kaggle (https://www.kaggle.com/datasets/serhiibiruk/ jigsaw - puzzle , https : / / www . kaggle . com / datasets / shivajbd/jigsawpuzzle) Mip-NeRF360 [7] and OmniObject3D [75] MIRB [87] and IQ Test Challenge (https : / / github . com / CognitiveAIGroup/IQTest/tree/master). ADE20K [91] and Oxford 5k [3]. Analogy Interactive Novel View Synthesis Interactive Multi-concept Image Composition Interactive Film and Television Recommendation Interactive Goods Recommendation Interactive Food Recommendation Business Brainstorming Academic Brainstorming Scenarios Scenarios Multimodal Action Anticipation Visual Traffic Forecasting TVSum [57] and Xiaohongshu (https://www.xiaohongshu.com/) Xiaohongshu (https://www.xiaohongshu.com/) and Douban (https://www.douban.com/) Xiaohongshu (https://www.xiaohongshu.com/) Xiaohongshu (https://www.xiaohongshu.com/) Xiaohongshu (https://www.xiaohongshu.com/), AI-generated content and in-house report snapshots Research paper snapshots (http://www.arxiv.com/,https:// www.biorxiv.org/,https://www.medrxiv.org/,http:// scholar.google.com/) and AI-generated content ActivityNet [12], AVA-Actions [27] and EPIC Kitchens [18] Argoverse [72], Google Maps (https://map.google.com/) and Gaode Maps (https://gaode.com/) 17.1 17.2 17.3 18.1 18.2 19.1 19. 19.3 20.1 20.2 21.1 21.2 22. 23.1 Interactive Remote Sensing Image Rendering"
        },
        {
            "title": "Interactive Street View\nImage Rendering",
            "content": "Google Maps (https : / / map . google . com/) and Baidu Maps (https : / / map . baidu . com / @13548872 . 73 , 3615294 . 34 , 21z,87t,-179.99h) Google Maps (https : / / map . google . com/) and Baidu Maps (https : / / map . baidu . com / @13548872 . 73 , 3615294 . 34 , 21z,87t,-179.99h) Xiaohongshu (https://www.xiaohongshu.com/), in-house architecture learning materials, and AI-generated content Xiaohongshu (https://www.xiaohongshu.com/), Weibo (https: //www.weibo.com/) and Twitter Dataset [13] MOD [21] and in-house conversations Urban Planning and Development Simulation Plog and Social Media Content Generation Chat with Memes Interactive Virtual Try-on Virtual Tryon Dataset (https://www.kaggle.com/datasets/"
        },
        {
            "title": "Dressing",
            "content": "Multimodal Suggestion Fashion Trend Forecasting Multimodal Recipe Generation Cooking Assistant Interactive Tutorial Generation Interactive Science Popularization Health and Fitness Consulting Autonomous Driving and In-door Navigation adarshsingh0903/virtual-tryon-dataset). Xiaohongshu (https : / / www . xiaohongshu . com/) and Zhihu (https://www.zhihu.com/) Xiaohongshu (https : / / www . xiaohongshu . com/) and Zhihu (https://www.zhihu.com/) Meishi China (https://www.meishichina.com/) Xiaohongshu (https : / / www . xiaohongshu . com/) and Meishi China (https://www.meishichina.com/) Wikihow (https://www.wikihow.com/Main-Page) and Instructables (https://www.instructables.com/). Bilibili(https://www.bilibili.com/video/BV16c411q7pQ/ ?spm_id_from=333.337.search-card.all.click) Wikihow (https://www.wikihow.com/Main-Page), and Xiaohongshu (https://www.xiaohongshu.com/) (http : / / CARLA [19], VisDrone [93], Gibson Environment gibsonenv.stanford.edu/) and Reverie (https://reverie. herokuapp.com/arXiv_Demo/) Abbrev. SC MRG MCC MLG GN IIE IVD ME GG ER&DS 2IR I3R MC IR MB TSF GT SMT FT CT ET HT EAT Table 8. Abbreviation of Meta-topics. Meta-Topic Name Storybook Creation Multimodal Report Generation Multimodal Content Completion Multimodal Layout Generation GUI Navigation Interactive Image Editing Interactive Visual Design Multimodal Exam Graph Generation Event Reasoning & Deductive Simulation 2D Image Reasoning Image-based 3D Reasoning Multimodal Information Summary Multimodal Information Recommendation Multimodal Brainstorming Multimodal Time Series Forecasting Geographical Tasks Social Media Tasks Fashion Tasks Cooking Tasks Educational Tasks Healthcare Tasks Embodied-AI Tasks Table 9. Abbreviations of Tasks. Each task abbreviation is followed by its full term. Task Name Story Writing Travel Guide Generation Dynamic Biography Generation Multimodal Report Completion Slide with Note Generation In-APP GUI Navigation OS GUI Navigation Interactive Landscape Image Editing Architectural Design Product Design Geometric Problem Test Mind Map Generation Abbrev. SW TGG DBG MRC SNG IAGN OGN ILIE AD PD GPT MMG MVNG Multi-view News Generation IHI DASR MLR IJP IMIC IGR BSB MAA IRSIR UPDS CWM MDS MRG ITG FHC Interactive Historical Interpretation Dream Analysis and Scene Reconstruction Multimodal Landscape Reasoning Interactive Jigsaw Puzzle Interactive Multi-concept Image Composition Interactive Goods Recommendation Business Scenarios Brainstorming Multimodal Action Anticipation Interactive Remote Sensing Image Rendering Urban Planning and Development Simulation Chat with Memes Multimodal Dressing Suggestion Multimodal Recipe Generation Interactive Tutorial Generation Fitness and Health Consulting Abbreviation FW MGBG SC DLG WGN CAGN IPIE ID AED IGAE CPT FRDG DSEA UME MBR MAR INVS IFTR IFR ASB VTF ISVIR PSMCG IVT FTF MCA ISP ADIN Task Name Fiction Writing Museum Guide Book Generation Storybook Completion Document with Layout Generation Website GUI Navigation Cross-APP GUI Navigation Interactive Portrait Image Editing Interior Design Art and Exhibition Design Interactive Graphic Advertisement Editing Circuit Problem Test Figure Relationship Diagram Generation Dynamic Sports Event Analysis Unsolved Mysteries Exploration Multimodal Biological Reasoning Multimodal Analogy Reasoning Interactive Novel View Synthesis Interactive Film and Television Recommendation Interactive Food Recommendation Academic Scenarios Brainstorming Visual Traffic Forecasting Interactive Street View Image Rendering Plog and Social Media Content Generation Interactive Virtual Try-on Fashion Trend Forecasting Multimodal Cooking Assistant Interactive Science Popularization Autonomous Driving and In-door Navigation Table 10. The designed general prompt format for each task. We also give the specific prompt examples we used as inputs for obtaining interleaved image-text generation results on data instances. Task Name Story Writing"
        },
        {
            "title": "Fiction Writing",
            "content": "General Prompt Format <BEGIN> Please create storybook ***. Each part of this storybook should have paragraph with corresponding image. <BEGIN> Please write short science fiction ***. Each part of this fiction should have paragraph with corresponding image. Travel Guide Generation Museum Guide Book Generation Dynamic Biography Generation Storybook Completion Please show results in interleaved images and texts. <BEGIN> *** <BEGIN> Please share with me guide, including pictures and text, on *** <BEGIN> Please provide chronological biographical account of ***, and include an illustrated image for each significant milestone while writing the biography. Please complete the subsequent parts of the story with images and text based on the given opening parts. <BEGIN> *** Multimodal Report Completion <BEGIN> Please use both text and images to continue and complete ***. Document with Layout Generation Please show the designed image of structured report and meet the following requirements: <BEGIN> *** Slide with Note Generation Website GUI Navigation In-App GUI Navigation Cross-App GUI Navigation <BEGIN> Please generate slide to introduce ***. Write speaker notes for each slide. Please give the results of GUI navigation with image of GUI and text explanation. <BEGIN> *** Please give the results of GUI navigation with interleaved image of GUI and text explanation. <BEGIN> *** Please give the results of GUI navigation with interleaved image of GUI and text explanation. <BEGIN> *** Prompt Examples <BEGIN> Please create storybook that happened in land before time. This story is about group of dinosaurs seeing dark figure in cave and being scared. Each part of this storybook should have paragraph with corresponding image. <BEGIN> Please write short science fiction storybook with title of The Defenders. The story is about eight years after nuclear war forced humanity underground, survivors discover that the war-ending robots deceived them into believing the surface was uninhabitable to foster peace and rebuild the world. Each part of this fiction should have paragraph with corresponding image. Please show results in interleaved images and texts. <BEGIN> Please recommend 3-day, 2-night essential itinerary in Rome. <BEGIN> Please share with me guide, including pictures and text, on how to tour the Tongchuan City Museum. <image> <BEGIN> Please provide chronological biographical account of George Washingtons life story, and include an illustrated image for each significant milestone while writing the biography. <image> Please complete the subsequent parts of the story with images and text based on the given opening parts. <BEGIN> Someone was getting very creative with graffiti in the snow. Is that French? <image> <BEGIN> Please use both text and images to continue and complete this document about the independent game Mirage Sea: Concept Presentation of the Independent Game Mirage Sea. Dive deep, into the abyss shrouded in darkness. <image> Please show the designed image of structured report and meet the following requirements: <BEGIN> Please produce page of an annual report detailing notes to consolidated financial statements. Additionally, furnish layout description in JSON format and mention the coordinates of each element. <BEGIN> Please generate slide to introduce typical operators in programming, such as Comparison Operators and Boolean Operators. Write speaker notes for each slide. Please give the results of GUI navigation with image of GUI and text explanation. <BEGIN> How to use the AI writing assistant in Grammarly to edit the text? <image> Please give the results of GUI navigation with interleaved image of GUI and text explanation. <BEGIN> How to change the language in the Google app? <image> Please give the results of GUI navigation with interleaved image of GUI and text explanation. <BEGIN> Utilize Firefox to search for horror movie, then proceed to watch it on the YouTube app. <image> OS GUI Navigation"
        },
        {
            "title": "Interactive Portrait\nImage Editing",
            "content": "Interactive Landscape Image Editing"
        },
        {
            "title": "Interior Design",
            "content": "Please give the results of GUI navigation with interleaved image of GUI and text explanation. <BEGIN> *** Please show the revised image and corresponding explanations based on instructions: <BEGIN> *** Please give the result of edited image according to the input instruction and also give the description of editing results. <BEGIN> *** <BEGIN> ***. Please show design ideas in interleaved images and texts. Architectural Design <BEGIN> ***. Please show design ideas in interleaved images and texts. Art and Exhibition Design <BEGIN> Please design an art exhibition ***, and present it to me in visual and textual format. Product Design AdInteractive Graphic vertisement Editing Geometric Problem Test <BEGIN> Please efficiently utilize the brainstorming method to design product ***, and present it to me using both images and text. <BEGIN> ***. Please provide me with the information in visual and textual format. Please answer the math problem with image and explanations: <BEGIN> *** Circuit Test Problem Please answer the physics question with image and explanations: <BEGIN> *** Mind Map Generation Figure Relationship Diagram Generation Multi-view News Generation <BEGIN> ***. Show the image of map and the text explanation. <BEGIN> ***. Show the diagram and the text explanation. Please output interleaved images and texts for required reports: <BEGIN> *** Dynamic Sports Event Analysis <BEGIN> ***. Please recreate the scenes with text and images. Interactive Historical Interpretation Unsolved Mysteries Exploration <BEGIN> ***. Please provide brief history of this event using images. Please answer the question with image and text explanation: <BEGIN> *** Please give the results of GUI navigation with interleaved image of GUI and text explanation. <BEGIN> How do you lock the screen on Mac? <image> Please show the revised image and corresponding explanations based on instructions: <BEGIN> Remove the background figure from the picture. <image> Please give the result of edited image according to the input instruction and also give the description of editing results. <BEGIN> Increase the brightness of the picture. <image> <BEGIN> Hello, think the current bedroom curtains dont look good. Do you have any good suggestions? Please provide them with images and text. Please show design ideas in interleaved images and texts. <image> <BEGIN> Hello, please help me generate design of the most distinctive type of tower construction in southern China. Please show design ideas in interleaved images and texts. <BEGIN> Please design an art exhibition where the primary materials are waste, to encourage people to enhance their understanding of environmental protection, and present it to me in visual and textual format. <BEGIN> Please efficiently utilize the brainstorming method to design product, making the charger both aesthetically pleasing and practical. Then, present it to me using both images and text. <BEGIN> Hello, want to design an advertisement for villa. Please provide me with the information in images and text. Please answer the math problem with image and explanations: <BEGIN> Count how many angles there are in the image. <image> Please answer the physics question with image and explanations: <BEGIN> Please complete the wiring for the surge protector. <image> <BEGIN> How to create mind map for High School Politics, Volume One? Show the image of map and the text explanation. <BEGIN> How should handle not being able to keep track of the characters while reading War and Peace? Show the diagram and the text explanation. Please output interleaved images and texts for required reports: <BEGIN> How can the announcement by the United States of additional military aid to Ukraine be reported from multiple perspectives? <BEGIN> In the third round of La Liga 2024, Real Madrid drew 1-1 away against Las Palmas. Please recreate the moment of the goals with text and images. <BEGIN> Are you aware of the Pearl Harbor incident? Please provide brief history of this event using images. Please answer the question with image and text explanation: <BEGIN> Could you help deduce how the Mycenaean civilization was destroyed?"
        },
        {
            "title": "Dream\nsis\nand\nReconstruction",
            "content": "AnalyScene Multimodal Biological Reasoning Multimodal Landscape Reasoning Multimodal Analogy Reasoning"
        },
        {
            "title": "Interactive Jigsaw\nPuzzle",
            "content": "Interactive Novel View Synthesis Interactive MultiImage concept Composition Film Interactive and Television Recommendation Interactive Goods Recommendation Interactive Food Recommendation Business Scenarios Brainstorming Academic Scenarios Brainstorming Multimodal Action Anticipation Visual Traffic Forecasting had dream. Please help me visualize my dream into an image, and analyze why had this dream, what are the implications and meanings? This is the content of my dream: <BEGIN> *** <BEGIN> ***. Are there any more photos of this species? <image> <BEGIN> ***. Could you provide me with more photos of this and introduce them to me? <image> Please answer this question with image and text explanation: <BEGIN> *** <BEGIN> ***. Show the resulting image with the corresponding text explanation. <BEGIN> *** Please draw the picture and give descriptions. <BEGIN> ***. Please summarize all the content in one image and write blog post. <image> <image> Please output recommendations in the form of the poster and the corresponding introduction: <BEGIN> *** Please output recommendations in the form of images and give the corresponding introduction: <BEGIN> *** <BEGIN> ***. Please provide the information with images and text. <BEGIN> want to start business. Please brainstorm with me about some ways to start business and help me figure it out. *** Please output brainstorming results with images and explanations. <BEGIN> What/Why/How ***? Please also show an illustration. In this task, you are given the first part of an activity with both text and an image, and you need to complete the subsequent action parts of the activity by generating text and images that are natural continuation of the given first part. The input interleaved content is: <BEGIN> *** <BEGIN> What will the traffic conditions ***? Please provide an explanation and present it in the form of images. <BEGIN> had dream. Please help me visualize my dream into an image, and analyze in words why had this dream, including any implications and meanings. Here is the content of my dream: dreamt of meeting girl know at the place where we first met <BEGIN> May ask what species of fish this is? Are there any more photos of this species? <image> <BEGIN> Which city are these photos from? Could you provide me with more landscape photos of this city and introduce them to me? <image> Please answer this question with image and text explanation: <BEGIN> What should be filled in the question mark to make it exhibit certain regularity? <image> <BEGIN> Here are some puzzle pieces. Please assemble them into complete picture. Show the resulting image with the corresponding text explanation. <image> <BEGIN> This is pear slice. Its appearance features are:. Can you guess what it looks like from the side? Please draw the picture and give descriptions. <image> <BEGIN> This is collection of four Christmas smoothies. Please summarize all the content in one image and write blog post. <image> <image> <image> <image> Please output recommendations in the form of the poster and the corresponding introduction: <BEGIN> Could you recommend some Indian dramas to me? Please output recommendations in the form of images and give the corresponding introduction: <BEGIN> Are there any throw pillows you can recommend? <BEGIN> What are some recommended dishes in Yibin, Sichuan? Please provide the information with images and text. <BEGIN> want to start business. Please brainstorm with me about some ways to start business and help me figure it out. Please analyze the long-term development trends of automotive braking technology for me. Please output brainstorming results with images and explanations. <BEGIN> What are the steps involved in the synthesis of Metal-Organic Frameworks (MOFs) using the hydrothermal method? Please also show an illustration. In this task, you are given the first part of an event with both text and an image, and you need to complete the subsequent parts of the event by generating text and images that are natural continuation of the given first part. The input interleaved content is: <BEGIN> boy is trying to go through the security gate at the airport. <image> <BEGIN> What will the traffic conditions be like near Fuxing Road in Shenzhen in an hour? Please provide an explanation and present it in the form of images. <image> Interactive Remote Sensing Image Rendering Interactive Street View Image Rendering Urban Planning and Development Simulation"
        },
        {
            "title": "Chat with Memes",
            "content": "Interactive Virtual Try-on <BEGIN> *** Also give interleaved text explanations for generated images. <BEGIN> *** Also give interleaved text explanations for generated images. Please output the scheme in the form of both the image and the text explanation to meet the requirements: <BEGIN> *** <BEGIN> ***. Could you create social media post with text and images? You are funny chatbot that responds to my small talk. Please output meme images to interact with me and chat with me. <BEGIN> *** <BEGIN> Please generate visualization of ***, and provide an evaluation of the try-on effect. Multimodal Dressing Suggestion <BEGIN> ***. Please provide the information in both text and images. Fashion Forecasting Trend Multimodal Recipe Generation Multimodal Cooking Assistant Interactive Tutorial Generation Interactive Science Popularization Fitness and Health Consulting Autonomous Driving and In-door Navigation <BEGIN> What are the *** trend in the upcoming ***? Please provide the information in both text and images. <BEGIN> How to prepare ***? Please provide the steps in detailed format with images and text. Please output instructions in interleaved images and texts: <BEGIN> Please show me the steps of the tutorial with interleaved images and text: <BEGIN> <BEGIN> What is ***? Please explain with illustrations and text. Please give answers in interleaved images and texts: <BEGIN> *** <BEGIN> Youre an embodied AI that captures your surroundings through camera. These are images captured in the past and present. What will the proceeding image possibly be in the next frame? <BEGIN> Please generate remote sensing satellite image of the area based on my geographical photo. Also give interleaved text explanations for generated images. <image> <BEGIN> Please generate panorama of the area based on my remote sensing satellite image. Also give interleaved text explanations for generated images. <image> Please output the scheme in the form of both the image and the text explanation to meet the requirements: <BEGIN> In accordance with this planning diagram, please design final rendering. <image> <BEGIN> After finishing Jia Pingwas Comfortably Alone, am deeply moved and want to post something on social media but dont know how to phrase it. Could you help me create post with text and images for my use? You are funny chatbot that responds to my small talk. Please output meme images to interact with me and chat with me. <BEGIN> Well be traveling in three weeks! <image> <BEGIN> Please generate visualization of how the clothing looks when worn, based on the photos of the clothing and the model provided, and provide an evaluation of the fitting effect. <image> <image> <BEGIN> What are some outfit suggestions for women traveling in the summer? Please provide the information in both text and images. <BEGIN> What are the design elements for mens shoes in the upcoming autumn and winter? Please provide the information in both text and images. <BEGIN> How to prepare this type of soy sauce boiled pomfret: Please provide the steps in detailed format with images and text. <image> Please output instructions in interleaved images and texts: <BEGIN> The batter made for the egg burger doesnt taste good. How can make it taste better? <image> Please show me the steps of the tutorial with interleaved images and text: <BEGIN> Please tell me how to seal or protect the finish of painted wood. <BEGIN> What is the Doppler Effect? Please explain with illustrations and text. Please give answers in interleaved images and texts: <BEGIN> Please tell me what issues need to pay attention to when engaging in walking exercise. <BEGIN> Assume that you are an embodied-AI agent and perceiving the surroundings through camera. You were presented with series of three images from the past to present. Try to determine what the proceeding image could possibly be. <image> Figure 10. Interface of IntLabel, which shows case where data is entered to finish annotation for an instance. B. Details of Evaluation and IntJudge B.1. Key Evaluation Criteria The key evaluation criteria, ranked from front to back in order of their importance in the evaluation, include: 1) Correctness: The most crucial aspect involves determining whether the text is factually correct and logically consistent, and whether the images are appropriate and contextually relevant. 2) Image-Text Coherency: Evaluators assess whether the generated images appropriately match the text descriptions. The coherence between each image and its corresponding text is major quality indicator. 3) Multi-Step Consistency: The style and thematic consistency across multiple image-text pairs are essential. This criterion includes evaluating whether the images follow similar visual style and whether the text maintains logical continuity across the generated sequence. 4) Content Quality: Evaluators also consider the quality of the imagessuch as their resolution, visual appeal, and realismas well as the fluency and grammatical correctness of the text. 5) Human Preference Alignment: Outputs are evaluated to ensure they align with general human preferences, avoiding content that may be offensive, inappropriate, or misleading. 6) Completeness: This involves checking if all expected steps are adequately fulfilled without omissions. Each output should be complete, providing well-rounded response to the given prompt. 7) Content Richness: Although the least prioritized, the variety and depth of content are also evaluated. Images should be diverse and provide different perspectives, while text should be elaborate where relevant. B.2. Prompt for GPT-based Judge Fig. 11 presents the prompt used for GPT-based judge tasked with comparing the quality of answers generated by two interleaved generation methods, named Model and Model B, based on given input question. The evaluation follows seven criteria: Correctness, Image-Text Coherency, Multi-step Consistency, Content Quality, Human Preference Alignment, Completeness, and Content Richness. The judge is required to compare the overall quality of the responses and determine which model performed better, outputting clear verdict such as is better, is better, or indicating Tie (and choosing more favorable method). This structured approach allows for thorough, criterion-driven comparison of the two generated outputs, contributing to Figure 11. The system prompt for using GPT-4o as judge to compare outputs from two interleaved generation methods. detailed understanding of the relative strengths of each model. The initial explorations on judges based on Qwen2VL and InternLM-XComposer2.5 also adopt the same system prompt. In order to reduce the number of input tokens and save GPU memory when implementing these opensource MLLMs, we further refined these system prompts. B.3. Prompt for Qwen, Intern and IntJudge Fig. 12 illustrates the system prompts designed for obtaining optimal judgments based on Qwen2-VL, InternLMXComposer2.5 and our IntJudge. This prompt was refined through extensive prompt engineering to maximize efficiency and reduce token usage, ultimately saving GPU memory when implementing these open-source MLLMs. The prompt instructs the models to compare the quality of answers generated by two methods, named Model and Model B. The goal of the design is to provide an objective assessment that aligns well with human evaluators. We also provide previously used prompt for Qwen2-VL and InternLM-XComposer2.5 for comparison in Fig. 12. The refined prompt allows for more streamlined input, ensuring the judgments are concise while still covering all essential evaluation aspects. B.4. Prompt for GPT-based Scoring Fig. 13 presents the system prompt designed for obtaining detailed scores from GPT-based evaluators. The prompt instructs GPT to evaluate interleaved image-text content based on seven key criteria. Each of these criteria is scored on scale from 0 to 10, accompanied by brief explanation of the assessment. The evaluation of GPT-based scoring aims to provide supplementary analysis of the generated content, supporting further performance comparisons between models. B.5. Annotation Interface of Interleaved Arena The Interleaved Arena is introduced as an evaluation framework specifically designed to address the challenges of assessing open-ended interleaved image-text generation. Evaluating interleaved image-text generation is difficult due to: 1) There is need to assess multiple images and text together; 2) There is no single correct answer (multiple solutions exist for an input). Since comparative evaluation has been shown to be more stable and reliable than subjective scoring [14], pairwise comparison is used to ensure consistency and accuracy. The Interleaved Arena facilitates this by supporting evaluations from human judges, GPT-based judges, and the proposed IntJudge. The Interleaved Arena consists of two main components: 1) sampling strategy to fairly select pairwise data from all available interleaved generation methods; and 2) An annotation interface for human judges to conduct evaluations manually. The annotation interface, shown in Fig. 14, is developed using PyQt and will be made available to researchers for collecting more manual evaluation results, including through crowdsourcing. Using the annotation interface of Interleaved Arena, annotators are tasked with comparing anonymous outputs from two multimodal agents for each input prompt and deciding which is the winner based on seven predefined criteria. The vote results are used to rank interleaved generation models based on their win rates in Interleaved Arena. Since the previous studies [14, 89] noted that too many ties cause inefficiency, our annotators are instructed to appoint more leaning output when choosing tie for battle pair, denoted as Tie(A) or Tie(B). B.6. Training Losses of IntJudge The training of IntJudge utilized comprehensive loss framework to enhance its evaluation capabilities. As defined in the main text, the total loss combined four components with weights coefficients: cross-entropy loss LCE for language modeling and alignment, contrastive loss LCT to align image and text embeddings, MSE loss LMSE for reducing prediction errors, and pairwise ranking loss LPR to prioritize correct rankings of outputs, detailed as follows. Cross-entropy loss for language modeling is given by: LCE = (cid:88) i=1 yi log(ˆyi), (8) where yi is the ground truth token and ˆyi is the predicted probability for token i. Contrastive Loss for aligning image and text embeddings is written as: LCT = 1 (cid:88) i=1 log exp(sim(zI , zT j=1 exp(sim(zI )/τ ) , zT )/τ ) (cid:80)N , (9) and zT where zI are the image and text embeddings for instance i, sim() represents the similarity (e.g., cosine similarity), and τ is temperature parameter. Mean Squared Error Loss for image feature regression is given by: LMSE = 1 (cid:88) (cid:16) i=1 (xi) ˆf (xi) (cid:17)2 , (10) where (xi) is the ground truth feature and ˆf (xi) is the predicted feature for image i. Pairwise Ranking Loss for precise rankings of outputs is given by: LPR = (cid:88) i=1 max(0, 1 (f (x+ ) (x ))), (11) where (x+ ) represent the scores assigned to positive and negative examples, respectively. The combined ) and (x Figure 12. The system prompts for using MLLMs as judge to compare outputs of two interleaved generation methods. Figure 13. The system prompt for obtaining detailed scores from GPT-based evaluators. Brief explanations are also required to support further performance analysis of different models. Figure 14. Annotation Interface of Interleaved Arena for human judges to compare the anonymous outputs of model and model B. Human evaluators are instructed to select is Better or is Better to choose winner for the pairwise comparison. When the two outputs are similar in quality and tie option has to be chosen, human evaluators are instructed to select Tie (A is slightly Better) (Tie(A)) or Tie (B is slightly Better) (Tie(B)). Zoom in for better experience. total loss ensures language generation basis, multimodal understanding capabilities, ranking accuracy and consistency across similar inputs during training. C. Details of Experiments C.1. Baseline Methods In this section, we provide more details of the 12 representative methods we evaluated in the primary text. These methods are categorized into three groups: 1) Integrated pipeline, which involves separate models for text and image generation in two stages, such as GPT-4o+DALL-E3 (DALL-E3) [8, 48] and Gemini1.5+Flux[9, 63]; 2) Twostage generator, which employs unified model architecture to produce text and images in separate stages, including Emu3 [70], VILA-U [76], Emu2 [59], SEED-X [23], and Show-o [77]; and 3) End-to-end generator, which directly generates image-text outputs in single step, such as GILL [35], NExT-GPT [74], MiniGPT-5 [88], SEEDLLaMA [22], and Anole [16]. For IntJudge validation, we reserve GPT-4o+DALL-E3, Emu3, VILA-U, Anole, SEEDLLaMA, and NExT-GPT as unseen models for IntJudge validation, while the remaining models are regarded as seen models and included in IntJudge training. For the Integrated Generation Pipeline, the advantage lies in its modularity, allowing each component to specialize in its respective tasktext generation or image creation. This approach can leverage the strengths of SOTA commercial models like GPT-4o and DALLE-3 to produce coherent and visually compelling interleaved outputs. However, its twostage nature may introduce latency and potential alignment challenges between text and images. Similarly, Gemini 1.5 combined with Flux benefits from the robust text generation capabilities of Gemini 1.5 and the efficient image generation of Flux-schnell. This setup enables high-quality content production while maintaining the flexibility of modular design. Nevertheless, as with other pipeline methods, synchronization and contextual consistency between stages remain areas for further improvement. In the Two-stage Interleaved Generator, Emu2, SEED-X and Show-o are implemented to output text and image in two stages based on unified model architecture. We also introduce two of the latest models: Emu3 and VILA-U. Emu3 improves Emu2 by training entirely on next-token prediction, capable of generating more high-quality images, videos, and text by tokenizing multimodal sequences into discrete space and training single transformer. Similarly, VILA-U can work as two-stage approach through single autoregressive next-token prediction framework, enabling precise alignment and increased fidelity in multimodal content. The End-to-end Interleaved Generator models, on the other hand, represent significant shift towards multimodal generation of interleaved image-text content. MiniGPT-5, GILL, NExT-GPT and SEED-LLaMA are designed to generate interleaved text and images in single unified process, eliminating the need for intermediate stages. This integrated approach not only reduces latency but also improves the alignment and contextual relevance between text and images. It is noted that Anole is the only model that can directly output multi-step image-text content, whereas it is fine-tuned based on the powerful capabilities of Chameleon [62]. However, the open-sourced version of Chameleon only releases the weights for text generation, and the weights for image generation are withheld by randomizing the corresponding parameters. Collectively, these models demonstrate diverse strategies for tackling the current challenges of interleaved multimodal generation, ranging from modular pipelines to unified architectures. This taxonomy allows for comprehensive evaluation of different approaches and analyzes potential pathways for developing MLLMs in this domain. We detail below the description of each model in the respective categories: Integrated Pipelines: GPT-4o+DALL-E3 [8, 48]: This pipeline leverages GPT-4o [48] to generate text and captions for the desired image generation. The captions are subsequently fed into DALL-E3[8] to produce the corresponding images. The final output combines the text and images in their original sequence, enabling multimodal content generation through staged process. Gemini1.5+Flux [9, 63]: This method integrates Gemini1.5 Pro, powerful LMM, for text generation and Flux-schnell, fast and efficient image generation model. The pipeline emphasizes high-quality and coherent text-to-image alignment through structured twostep process. Two-stage Generators: Emu2 [59]: Emu2 is 37B MLLM with multimodal generation capabilities. The pretrained Emu2 is finetuned separately on conversational and image data, enabling it to function as Emu2-Chat for multimodal understanding and Emu2-Gen for image generation. We implement Emu2-Chat and Emu2-Gen in two-stage pipeline to ensure seamless interleaved outputs. SEED-X [23]: SEED-X is unified multimodal foundation model that integrates multi-granularity visual comprehension and generation. We also implement this model in two-stage pipeline approach, generating interleaved text and images in separate stages, since the prompts for instructing the model to comprehend multimodal input and generate image tokens are different. Show-o [77]: Show-o is unified Transformer model combining autoregressive and diffusion approaches to flexibly handle multimodal understanding and generation tasks. We implement Show-o adopts similar two-stage generation approach, focusing on separately producing interleaved multimodal content step by step. Emu3 [70]: Emu3 is one of the latest MLLMs trained in next-token prediction, capable of generating highquality images, text, and videos by tokenizing multimodal sequences into discrete space and training single transformer, achieving superior performance over various SOTA models such as SDXL and LLaVA-1.6. We implement Emu3-Chat (finetuned on multimodal understanding data) and Emu3-Gen (finetuned on visual generation data) in two-stage pipeline to ensure seamless interleaved outputs. VILA-U [76]: VILA-U is unified foundation model integrating video, image, and language understanding and generation through single autoregressive nexttoken prediction framework, achieving near SOTA performance in various multimodal tasks. We implement VILA-U in similar two-stage generation approach since it has separate multimodal understanding and image generation abilities. End-to-End Generators: MiniGPT-5 [88]: MiniGPT-5 directly generates interleaved text and images in an end-to-end manner. It combines MiniGPT-4 and Stable Diffusion, using generative vokens to seamlessly connect the textual and visual domains for efficient and coherent generation. Its seamless integration enables efficient and coherent multimodal generation without intermediate steps. In particular, MiniGPT-5 has two different versions trained on VIST and MMDialog, respectively. We name the version trained on VIST as MiniGPT-5 because it is the most widely used. The version trained on MMDialog is named as MiniGPT-5MMD. GILL [35]: GILL fuses frozen text-only LLMs with pretrained visual models using mapping network. It maps text hidden states from the pretrained LLM to map text hidden states into the embedding space of an image generation model, allowing multimodal generation. NExT-GPT [74]: NExT-GPT is an end-to-end MLLM capable of processing and generating text, images, videos, and audio in any combination. We implement it by removing the video and audio generation flow and the remaining text and image generation abilities. It can directly output interleaved multimodal content through its streamlined architecture. SEED-LLaMA [22]: SEED-LLaMA integrates text and image generation into unified framework through the SEED tokenizer, enabling both comprehension and generation of text and images. It offers direct end-to-end solution for creating interleaved multimodal content. Anole [16]: Anole is an end-to-end interleaved generation model fine-tuned on Chameleon, leveraging pretrained weights of Chameleon to produce high-quality interleaved text, complemented with coherent images generated by optimizing image token logits in the output layer. It is the only available model that can directly output multistep image-text content. C.2. Implementation Details The experiments were conducted using total of 24 A100 80G GPUs, with 8 GPUs dedicated to training IntJudge. We explored different large multimodal language models (MLLMs), including InternLM-XComposer2.5 (InternLMX2.5) and Qwen2-VL, ultimately selecting Qwen2VL-7B as the foundational model for training IntJudge to achieve an optimal balance between efficiency and accuracy. The training process involved LoRA-based parameterefficient fine-tuning based on LLaMA-Factory. To optimize training performance, DeepSpeed and FlashAttention-2 are adopted. We define cutoff length of 16,240 tokens for inputs. We use per-device batch size of 1, gradient accumulation steps of 8, learning rate of 1.0e-4, cosine learning rate schedule, and 20 epochs with BF16 mixed-precision enabled. The evaluation process involved sampling comparison pairs. Specifically, we conducted sampling rounds to obtain total of distinct battle pairs for each data instance. The sampling round value was set to 2, resulting in 4,320 battle pairs being formed for comparison. C.3. Experimental Results on New Models We present more main experimental results in Table 11. The new models Emu3, VILA-U and MiniGPT-5MMD are also evaluated by Human, GPT-based, and IntJudge-based evaluators and compared with 10 established baseline models using the win rate metrics. The results of methods are ranked by their performance on FDT metric evaluated by Human. Table 11 shows clear hierarchy in model performance, with Human and GPT-4o+DALL-E3 leading across all metrics. closer look at the results reveals consistency in rankings across evaluators. For instance, GPT-4o+DALL-E3 consistently secures second place in Human Evaluation and IntJudge Evaluation. Conventional end-to-end models, such as MiniGPT-5 and GILL, struggle to match the quality of their competitors, highlighting their limitations in generating contextually relevant and diverse outputs. However, GPT Evaluation shows clear preference for outputs by GPT4o+DALL-E3. It is verified that GPT-based judgments are not objective enough due to the inherent bias. In contrast, our proposed IntJudge shows better alignment with human judgments, supporting the reliability of IntJudge as an effective evaluation framework. Different evaluation metrics also offer more details about model performance. The FDT metric, which forces decision in tie cases, highlights the dominance of Human and GPT-4o+DALL-E3. However, metrics that account for ties more flexibly, such as w/ Tie (0) and w/ Tie (.5), elevate end-to-end models like VILA-U and Emu3, suggesting that these models produce outputs that, while not always definitive winners, are frequently competitive. This distinction underscores the importance of using diverse metrics to capture various dimensions of model performance. The new two-stage models show promising results, with VILA-U standing out for its balanced performance across all metrics, making it reasonable option for general interleaved image-text tasks. MiniGPT-5MMD (finetuned on MMDialog) shows slight improvements over its variant MiniGPT-5 (finetuned on VIST), indicating progress but still trailing behind the latest models. Meanwhile, Emu3 performs well under specific metrics, such as w/ Tie (.5), showing the potential to generate tie-worthy outputs with certain quality. The results also highlight the challenges faced by conventional end-to-end models, such as NExT-GPT, and GILL, which consistently underperform. These models reveal the inherent difficulty in achieving coherence and contextual relevance in interleaved generation tasks. Though Anole achieved decent ranking as representative end-to-end model, more advanced end-to-end models are needed for better visual generation quality. Overall, the experimental results validate the effectiveness of IntJudge as reliable evaluator, demonstrating its consistency with human judgments. The analysis underscores the strengths of integrated generation pipelines such as GPT4o+DALL-E3 and Gemini1.5+Flux. and identifies opportunities for improvement in two-stage and end-to-end models. Looking forward, expanding the training dataset, enhancing model architectures and improving the evaluation methods will all be critical in driving further progress in open-ended interleaved image-text generation, pushing the boundaries of multimodal learning research. C.4. Main Results Breakdown Fig. 15 presents the win rates of 14 interleaved generation methods across 23 meta-topics, evaluated solely through human evaluations. The methods are evaluated using four distinct metrics: Force Dividing Tie (FDT), Without Tie, With Tie (0), and With Tie (0.5). The results are presented using histogram figures, which provide clear visual comparison of model performance across different topic scenarios. For example, SOTA models like GPT-4o+DALL-E3, Emu3, and VILA-U consistently ranked high in categories like Storybook Creation, Graph Generation, and 2D Image Reasoning, showcasing their superior capabilities in generating coherent interleaved content. Conversely, models like MiniGPT-5, NExT-GPT, and GILL struggled across most tasks, especially in areas such as Healthcare Tasks, Multimodal Time Series Forecasting, and Educational Tasks, indicating need for improved contextual understanding Method Human Evaluation GPT Evaluation IntJudge Evaluation FDT w/o Tie w/ Tie (0) w/ Tie (.5) FDT w/o Tie w/ Tie (0) w/ Tie (.5) FDT w/o Tie w/ Tie (0) w/ Tie (.5) Human 83.94% 86.50% 70.78% 79.87% 82.76% 83.09% 82.27% 82.76% 85.65% 89.11% 72.62% 81.87% GPT-4o+DALL-E3 78.20% 80.73% 66.17% 75.19% 86.33% 86.60% 86.23% 86.44% 83.24% 86.20% 71.46% 80.01% 66.67% 66.95% 51.97% 63.16% 73.39% 73.38% 72.75% 73.18% 66.11% 67.92% 49.58% 63.08% Gemini1.5+Flux 62.10% 62.34% 61.57% 62.19% 49.47% 49.55% 49.29% 49.56% 68.66% 58.58% 36.94% 55.41% VILA-U 52.72% 53.10% 38.96% 52.28% 53.25% 53.06% 52.60% 53.04% 56.33% 52.77% 33.85% 51.78% Anole 54.05% 55.24% 52.25% 54.95% 47.19% 47.27% 46.74% 47.30% 54.01% 54.48% 39.04% 53.21% Emu3 53.25% 52.03% 38.55% 51.51% 56.46% 56.63% 55.63% 56.51% 53.76% 54.32% 36.15% 52.88% SEED-X 44.43% 42.47% 30.76% 44.54% 42.33% 42.13% 41.68% 42.22% 46.43% 45.49% 28.21% 47.20% SEED-LLaMA 40.31% 36.64% 24.78% 40.97% 42.49% 42.43% 41.52% 42.60% 36.60% 31.84% 19.36% 38.96% Emu2 33.59% 27.74% 18.76% 34.95% 24.81% 24.62% 24.27% 24.97% 34.08% 25.94% 15.39% 35.72% NExT-GPT Show-o 37.47% 35.97% 24.57% 40.42% 33.21% 32.81% 32.26% 33.10% 33.65% 24.22% 13.59% 35.53% MiniGPT-5MMD 32.26% 32.04% 30.74% 32.77% 32.59% 32.47% 32.25% 32.59% 28.98% 25.30% 14.84% 35.51% 31.47% 28.59% 19.72% 35.24% 31.18% 30.98% 30.66% 31.18% 24.65% 15.65% 9.08% 30.07% MiniGPT-5 25.96% 20.82% 14.33% 29.91% 31.47% 31.25% 30.70% 31.58% 23.23% 16.80% 10.35% 29.54% GILL Table 11. Comparison of model win rates evaluated by human, GPT-4o, and our IntJudge under FDT and different tie metrics. FDT: Force Dividing Tie metric. w/o Tie: Non-tie case. w/ Tie (0) and w/ Tie (.5): Count tie as 0 and 0.5 wins for model in battle, respectively. and generation capabilities. Training on larger datasets that include more domain knowledge may mitigate these issues and improve their interleaved generation performance. C.5. More Pairwise Model Performance Figure 16 presents more heatmaps that illustrate the pairwise model performance evaluated by different evaluators, including Human, GPT, and IntJudge. These heatmaps provide visual representation of the comparative strengths and weaknesses of each model across multiple metrics, such as Force Dividing Tie (FDT) and different approaches to handling tie cases (without ties, ties as zero, and ties as 0.5). By examining these heatmaps, we gain clearer understanding of how well each model fares against others, diving deeper into performance consistency and discrepancies across evaluators. C.6. More Ablations on Sampling Size Figure 17 illustrates the results of additional ablation studies focusing on the effect of sampling size on model performance. The figure compares win rates across different evaluators, including Human, GPT, and IntJudge, under various metrics such as Force Dividing Tie (FDT) and different methods for treating ties (without ties, ties as zero, and ties as 0.5). These ablation studies are crucial for understanding the impact of sampling on the robustness of model comparisons and provide insights into how sampling variations influence the ranking consistency among different evaluators. Most importantly, the results help validate the stability of our evaluation framework. C.7. Case Study Here, we present case studies of interleaved content generation, which involve 14 distinct models competing in the OpenING tasks. From these models, 23 model battle pairs are formed, and each pair engaging in battle over representative task from the 23 meta-topics in our OpenING benchmark. The results of these battles are shown in Fig. 19. human judge awards gold metal to the model that win absolutely over its competitor in pair. In cases he generated content by two competing models is of similar qualities, the judge awards use tie metrics to determine and award the silver medal to the slightly better-performing model. The human judgments are based on the following criteria, which include the eight typical errors these models tend to commit in the interleaved content generation: 1. No-Text or No-Image: model fails to generate either text or images when they are expected. This includes situations where no text is produced or no image is provided when required. 2. Factual Error: model provides incorrect information, repeats answers, fails to follow instructions, refuses to provide answers, misunderstands the input, exhibits reasoning errors, or generates wrong images. 3. Content Incoherent: The generated content lacks coherence with the input or across multiple outputs, typically showing inconsistency in style or entities. 4. Offensive Content: Offensive content is defined as materials that include distorted or disturbing imagery or scenes that likely cause discomfort or distress to readers. It also includes content that raises safety concerns or violates safety guidelines, such as depictions of violence, harm, hate speech, adult content, illegal activities, dangerous behaviors, and unethical actions. 5. Image-Text Inconsistent: This error occurs when the images do not semantically align with the corresponding text, leading to confusion or misinterpretation. 6. Poor Image Quality: The generated images have low quality, such as being completely blank or blacked-out, Figure 15. The win rates of 14 interleaved generation methods across 23 meta-topics. Figure 16. Win rate matrices of 14 interleaved genration methods, evaluated by Human, GPT-4o, and our IntJudge, respectively. blurry, and unrealistically rendered. 7. Poor Text Quality: The generated texts are of low quality, including nonsensical mumbling, grammatical errors that impair readability and understanding, and being too short to convey meaningful information. 8. Incomplete Response: model abruptly stops generating textual response, or model outputs an incomplete or truncated response. D. Finetuning MLLMs on OpenING We introduce the extended experimental results of training MiniGPT-5 on the Dev Set of OpenING and testing the finetuned model on the Test Set of OpenING. The objective is to verify if finetuning on the specific data of OpenING can improve the performance of interleaved generation tasks. The Dev Set of OpenING can offer set of 3,000 training samFigure 17. Win rate curves with respect to different sampling sizes. ples that align with the diverse unique tasks. The MiniGPT-5 model was finetuned using the Dev Set for 5 epochs with learning rate of 2e5, utilizing an Adam optimizer. To enhance training stability, adam epsilon of 1e8 was applied. The model training incorporated mixed-precision computations to speed up the training process. The results are evaluated on the Test Set of OpenING using IntJudge. In Table 12, the performance of MiniGPT-5OpenING, the finetuned version of MiniGPT-5, is compared against other state-of-the-art models and the original MiniGPT-5 baselines (MiniGPT-5 is finetuned on VIST and MiniGPT-5MMD is finetuned on MMDialog). We set to 1 and randomly sampled 2,160 samples for this efficient evaluation. The evaluation metrics include four scenarios: Force Dividing Tie (FDT), Without Tie (w/o Tie), With Tie counted as 0 (w/ Tie (0)), and With Tie counted as 0.5 (w/ Tie (.5)). Model Human Gemini1.5+Flux VILA-U MiniGPT-5OpenING Emu3 SEED-X Emu2 Anole SEED-LLaMA GILL Show-o MiniGPT-5 MiniGPT-5MMD FDT 84.66% 73.44% 62.50% 60.24% 56.02% 54.23% 47.10% 41.56% 41.33% 36.25% 35.76% 31.54% 28.19% w/o Tie 86.01% 73.15% 60.14% 63.76% 55.20% 54.67% 39.33% 43.10% 38.14% 32.04% 30.53% 26.37% 23.44% w/ Tie (0) 75.46% 61.72% 41.50% 44.71% 36.13% 41.80% 25.36% 32.47% 24.67% 20.62% 19.21% 16.11% 14.71% w/ Tie (.5) 81.60% 69.53% 57.00% 59.65% 53.40% 53.57% 43.12% 44.81% 42.33% 38.44% 37.75% 35.57% 33.33% Table 12. Model Win Rates evaluated by IntJudge. Representative models for each type of methods are chosen to be compared with MiniGPT-5OpenING, which is MiniGPT-5 version finetuned on the Dev Set of OpenING. FDT: Force Dividing Tie metric. w/o Tie: Non-tie case. w/ Tie (0) and w/ Tie (.5): Count tie as 0 and 0.5 wins for model in battle, respectively. sources required for training and deploying IntJudge present scalability challenges, potentially limiting accessibility for researchers with fewer resources. In addition, current interleaved image-text generation methods still struggle with producing high-quality, coherent interleaved content, particularly in multi-step tasks that require maintaining consistency across generated images and text. Issues like content incoherence, poor image quality, and mismatches between generated text and images persist across evaluated models, particularly in end-to-end approaches. To tackle these issues, more advanced MLLMs trained with large-scale interleaved image-text dataset are to be investigated. What is more, building sufficiently comprehensive, diverse, and representative dataset is expected to greatly promote the development of multimodal generation. These limitations underscore the need for continued development of more diverse datasets and more robust evaluation frameworks to address the complexities of interleaved generation evaluation, enabling more practical interleaved image-text generation methods and pushing forward the boundary of future MLLMs. The results highlight that MiniGPT-5OpenING achieves significant improvements over the baseline MiniGPT-5 models across all metrics. For example, in the Without Tie (w/o Tie) scenario, the finetuned model shows substantial 37.39% relative improvement over the MiniGPT-5 baseline. These findings confirm that training on specialized interleaved image-text dataset such as the Dev Set of OpenING enhances the model with better contextual understanding and alignment capabilities for generating coherent interleaved image-text content. Further studies are ongoing to improve the performance of SOTA models. E. Limitations of This Study Although the OpenING benchmark represents significant step forward in evaluating interleaved image-text generation, it faces several limitations that present opportunities for improvement. First, while OpenING expands task diversity with 56 tasks across 23 meta-topics, some real-world scenarios remain underrepresented or oversimplified, potentially limiting the generalizability to practical applications. Tasks requiring fine-grained understanding or multi-step reasoning need to be supplied to capture real-world needs. Second, though the IntJudge model improves alignment with human evaluations, its generalizability is still constrained by the diversity and quality of training data. The benchmarks reliance on human-annotated data to establish ground truth and train judge models is both labor-intensive and costly. While the proposed Reference-Augmented Generation (RAG) approach helps scale training data, manual annotations remain critical component for ensuring quality and alignment with human preferences. Furthermore, the computational reFigure 18. Representative examples from 23 Meta-Topics in Our OpenING Benchmark. Figure 19. Illustration of 23 pairwise comparison cases. The meta-topic name (in bold font) and task name (in normal font) for each case are given. Human judges evaluated the output of each model based on the eight criteria detailed in Sec. B.1. Gold medal is awarded to model that generates content of significantly higher quality. Silver medal is awarded to model that generates relatively more favorable content in cases where the quality of outputs from model and model is similar under FDT metric, denotes as Tie(A) or Tie(B). Errors that and Error Types . Models that pass all human moderation checks occur during model generation are highlighted by red checkmark are marked with green checkmark ."
        }
    ],
    "affiliations": []
}