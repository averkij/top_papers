{
    "paper_title": "DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition",
    "authors": [
        "Alexander Polok",
        "Santosh Kesiraju",
        "Karel Beneš",
        "Bolaji Yusuf",
        "Lukáš Burget",
        "Jan Černocký"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a simple yet effective regularization for the internal language model induced by the decoder in encoder-decoder ASR models, thereby improving robustness and generalization in both in- and out-of-domain settings. The proposed method, Decoder-Centric Regularization in Encoder-Decoder (DeCRED), adds auxiliary classifiers to the decoder, enabling next token prediction via intermediate logits. Empirically, DeCRED reduces the mean internal LM BPE perplexity by 36.6% relative to 11 test sets. Furthermore, this translates into actual WER improvements over the baseline in 5 of 7 in-domain and 3 of 4 out-of-domain test sets, reducing macro WER from 6.4% to 6.3% and 18.2% to 16.2%, respectively. On TEDLIUM3, DeCRED achieves 7.0% WER, surpassing the baseline and encoder-centric InterCTC regularization by 0.6% and 0.5%, respectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium, showing competitive WERs despite training on much less data with fewer parameters."
        },
        {
            "title": "Start",
            "content": "DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition Alexander Polok, Santosh Kesiraju, Karel Beneˇs, Bolaji Yusuf, Lukaˇs Burget, Jan ˇCernocky Brno University of Technology, Brno, Czech Republic 5 2 0 2 2 1 ] . e [ 1 8 3 9 8 0 . 8 0 5 2 : r AbstractThis paper presents simple yet effective regularization for the internal language model induced by the decoder in encoder-decoder ASR models, thereby improving robustness and generalization in both inand out-of-domain settings. The proposed method, Decoder-Centric Regularization in EncoderDecoder (DeCRED), adds auxiliary classifiers to the decoder, enabling next token prediction via intermediate logits. Empirically, DeCRED reduces the mean internal LM BPE perplexity by 36.6 % relative to 11 test sets. Furthermore, this translates into actual WER improvements over the baseline in 5 of 7 in-domain and 3 of 4 out-of-domain test sets, reducing macro WER from 6.4 % to 6.3 % and 18.2 % to 16.2 %, respectively. On TEDLIUM3, DeCRED achieves 7.0 % WER, surpassing the baseline and encoder-centric InterCTC regularization by 0.6 % and 0.5 %, respectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium, showing competitive WERs despite training on much less data with fewer parameters. Index Termsspeech recognition, intermediate regularization, out-of-domain generalization I. INTRODUCTION One of the key challenges in automatic speech recognition (ASR) is to enhance the ability of models to generalize to new or unseen domains. Regularization techniques have long been central to improving robustness. Methods such as SpecAug [1], label smoothing [2], [3], multi-task training [4], and architecture-specific regularization [5] are now standard practices in ASR pipelines and are integrated into opensource toolkits [6], [7]. Another prominent strategy is largescale multi-domain training [8], as demonstrated in works like SpeechStew [9], Canary [10], Whisper [11], OWSM [12], and OWLS [13] which aggregate datasets from diverse domains (e.g., conversational, broadcast, telephone, and read speech) to train models capable of handling broad variability in data. Although Whisper achieves impressive recognition accuracy, its training data has prompted the scientific community to develop open-source equivalents like OWSM [12], trained on publicly available datasets. While scaling up training data has clear benefits, it is computationally expensive and inaccessible to many researchers. Moreover, it is hard to evaluate the out-of-domain generalization of these models since all the standard datasets were already seen during the training. Together, these prompt the question: Are there any other simpler yet effective techniques to improve robustness? the lack of transparency about In this paper, we propose straightforward approach to improve out-of-domain generalization by regularizing the auto-regressive Internal Language Model (ILM) [14], [15] in encoder-decoder-based ASR architectures. The use of auxiliary LAttn. LAttn. (D2) softmax softmax LCTC CTC (D)th Decoder layer C/A (D-2)th Decoder layer Speech Encoder (E-Branchformer) FBANKS (x1:T ) First Decoder layer Fig. 1. Architecture of the proposed DeCRED. In addition to the standard encoder-decoder framework for ASR (LAttn ), with the auxiliary CTC objective (LCTC), DeCRED uses possibly multiple auxiliary classifiers (LAttn ) attached to the decoder. In the illustration, we show one auxiliary classifier attached to the (D-2)-th decoder block. The embedding and positional encoding layers are not depicted for brevity. classifier(s) or intermediate regularizers has been explored in ASR, though most prior work applies these techniques to the encoder module. For example, [5] employs intermediate CTC objectives in the ASR encoder, while [16] extends this idea by feeding intermediate classifier outputs to subsequent layers, conditioning the predictions of the final layer on these intermediate outputs. Similarly, [17] applies such schemes in self-supervised training of speech encoders. More recently, [18] regularizes both the encoder and the decoder by passing the intermediate encoder output directly to the decoder layers. In this work, we present Decoder-Centric Regularisation in Encoder-Decoder (DeCRED), which integrates auxiliary classifiers into intermediate decoder layers and trains them with the same ASR prediction objective as the final layer. This enforces additional supervision on the decoder with negligible computational overhead (only the extra cost of single linear layer and the loss function evaluation) during training. Furthermore, it incurs no additional cost during inference as only the final decoder layer is used for inference (although we note that further improvements in ASR can be obtained by fusing the probabilities from various decoder layers). Thus, our approach differs from and complements prior approaches in the following ways: We exclusively regularize the decoder by introducing auxiliary classifiers into intermediate decoder layers. We analyze the impact of the proposed regularization scheme, showing that it improves the decoders capacity as an ILM, and that this improvement extends to domains which were not seen during training. Moreover, the ILM gains are not simply theoretical, but also accompanied by both in-domain and out-of-domain ASR improvements. We evaluate DeCRED in relatively large-scale setup using the E-Branchformer architecture [19], training on mix of datasets while reserving some for out-of-domain evaluation. Our experiments show that DeCRED improves outof-domain performance on four unseen datasets (AMI [20], FLEURS [21], Gigaspeech [22] and Earnings-22 [23]), reducing the macro-average WER from 18.2 % to 16.2 %, while also lowering in-domain macro-average WER from 6.4 % to 6.3 % compared to the non-regularized baseline. Furthermore, DeCRED complements large-scale training strategies, performing on par with OWSM v3.1 and achieving WERs near Whispermedium, despite relying on fraction of their training data and model size. II. DECODER-CENTRIC REGULARIZATION Our approach extends the training objective of the hybrid CTC-attention-based training scheme [4] by adding auxiliary cross-entropy loss functions. The objective function is defined as: = α LCTC + (1 α) LDeCRED, (1) where LCTC represents the standard CTC loss [24], α is hyper-parameter, and LDeCRED = (cid:88) d=1 βdLAttn , (2) where represents the number of layers in the decoder, LAttn is the cross-entropy loss given classifier layer (linear projection, followed by softmax function) attached to the d-th layer of the decoder, and βd is the weighting factor of d-th layer. We impose constraints (cid:80)D d=1 βd = 1 and βd 0. In practise [β1 . . . βD] is sparse vector. This definition allows us to explicitly regularize the ILM and force earlier layers of the decoder to learn discriminative features suitable for the task. Figure 1 illustrates the proposed architecture, where an auxiliary classifier is attached to the output of (D-2)-th decoder block. The decoding follows typical auto-regressive scheme observed in encoder-decoder ASR systems, where the posterior probability of an output token is obtained by conditioning on previously decoded tokens (partial hypothesis) and the input features. Formally, let x1:T be sequence of input speech (filterbank) features and let y1:N be sequence of output tokens. Following the joint CTC/attention decoding [4], the log posterior probability of output token yn is evaluated as log p(yn y1:n1, x1:T ) λ log pCTC(yn y1:n1, x1:T ) + (1 λ) log pDeCRED(yn y1:n1, x1:T ), (3) where λ is hyper-parameter. Now, let hd,n R1dmodel denote the hidden representation corresponding to the n-th output token obtained from the d-th layer of the decoder, and Wd RdmodelV represent linear projection from hidden dimension dmodel to vocabulary size . We obtain the following decoding methods by varying the definition of pDeCRED: 1) Vanilla joint CTC/attention decoding relying on representations only from the last layer D, using auxiliary classifier(s) only as regularization during training: pDeCRED(yn y1:n1, x1:T ) = softmax(hD,nWD) (4) 2) Sum of logits weighted by per-layer learnable vector vd R1V , where is elementwise product: pDeCRED() = softmax (cid:33) vd (hd,nWd) (5) (cid:32) (cid:88) d="
        },
        {
            "title": "These schemes can be easily integrated into any decoding",
            "content": "search algorithm, such as greedy or beam search. III. EXPERIMENTAL SETUP A. Model architecture Our baseline Encoder-Decoder (ED) model consists of 16 E-Branchformer [19] encoder layers with relative positional embeddings [25], Macaron-like feedforward modules [26], dmodel = 512, dff = 4dmodel, four attention heads, and dropout probability of 0.1. In line with the E-Branchformer architecture, we incorporate merge block followed by depth-wise convolution with kernel size of 31. The encoder is followed by an 8-layer Transformer decoder with sinusoidal positional embeddings, maintaining the same number of attention heads, dmodel, dff, and dropout ratio. The ED model has 172M parameters and processes 80dimensional filter-bank features as input. These first pass through two 2D convolutional layers with 512 output channels, kernel size of 3 3, and stride of 2 2, reducing sequence length. linear projection then matches dmodel. We use subword tokenizer with vocabulary of size = 5000 based on the Unigram algorithm from [27]. Unless stated otherwise, the DeCRED model extends the baseline ED model by adding single auxiliary classifier with βD2 = 0.4, introducing just dmodel additional parameters. By default joint greedy decoding with (λ = 0.3) is utilized. B. Datasets Following the data selection strategy of SpeechStew [9], we construct our training set using mixture of diverse, multi-domain speech datasets: Fisher (SWITCHBOARD) [28], WSJ [29], Common Voice en 13 [30], LibriSpeech [31], VoxPopuli [32], and TED-LIUM 3 [33], amounting to roughly 6,000 hours of transcribed audio. As reported in [12], training on such heterogeneous datasets risks overfitting to domainspecific annotation styles, which may hinder general-purpose performance. To mitigate this, we standardize all transcripts TABLE WERS (WITH CONFIDENCE INTERVALS) OF ED AND DECRED MODELS REPORTED ACROSS MULTIPLE IN-DOMAIN TEST SETS USING NORMALIZED TRANSCRIPTS, WITH WHISPER-MEDIUM AND OWSM V3.1 INCLUDED AS REFERENCES. Dataset Model ED(4) DeCRED(4) DeCRED(5) Whisper medium OWSM v3.1 CVSB eval2000 LS clean LS other TEDLIUM3 VoxPopuli WSJ Macro Avg. 11.9 12.2 11.7 12.0 12.3 11.8 12.2 12.5 11.8 12.4 12.6 12.1 12.9 13.2 12.5 9.2 9.6 8.8 9.4 9.7 8.9 9.1 9.5 8.7 14.7 15.2 14.2 11.2 14.6 9.2 2.5 2.7 2.3 2.4 2.5 2.2 2.3 2.5 2.2 3.0 3.4 2.7 2.4 2.6 2.2 5.7 6.1 5.4 5.5 5.7 5.3 5.5 5.8 5.3 5.9 6.2 5.6 5.0 5.3 4.8 6.6 7.1 6.1 6.3 6.8 5.9 5.7 6.1 5.3 4.2 4.6 3.8 5.0 5.4 4. 7.5 8.2 6.9 7.3 8.0 6.8 7.3 7.9 6.8 8.0 8.8 7.4 8.5 9.0 8.0 1.8 2.2 1.5 1.5 1.9 1.2 1.5 1.8 1.2 3.2 3.8 2.6 3.5 4.0 2.9 6.4 6.3 6.2 7.3 6.9 using the Whisper normalizer,1, ensuring consistency across datasets and reducing annotation-style biases. To evaluate the generalization ability of our models, we test them on four datasets not seen during training: AMI [20], FLEURS [21], GigaSpeech [22], and Earnings-22 [23]. C. Training setup All experiments are conducted using the open-source transformers library and trained on Nvidia A100 GPUs with the AdamW optimizer [34]. Training runs for 100 epochs with early stopping (patience of 10), learning rate of 2 103, weight decay of 1 106, linear decay scheduler, 40k warm-up steps, and label smoothing [2], [6] with weight of 0.1. To accelerate training, samples longer than 20 seconds are discarded. We apply speed perturbation with randomly selected factors 0.9, 1.0, 1.1 and delay SpecAug [1] until after 5k update steps. For all experiments, the best-performing checkpoint is selected based on the development WER. Furthermore, we introduce mechanism to mask special tokens and unfinished words (e.g., transcript [hesitation] to reto rerenew is transformed into [MASK] to [MASK] to [MASK] renew) during error backpropagation by not reflecting [MASK] token in the loss function. This strategy prevents penalization for unclear inputs, which are particularly common in the Fisher dataset. IV. RESULTS We evaluate our models using WER with confidence intervals2 computed via bootstrapping (α = 0.05, = 1000) [35]. Significance tests are performed with pair-wise bootstrapping. A. In domain performance Table presents the WER results of the baseline ED(4) and the proposed DeCRED(4) models across in-domain datasets. Superscripts in model names indicate the decoding strategy usedspecifically, Equation (4) corresponds to decoding from the last layer of the model. Notably, DeCRED(4) outperforms ED(4) on 5 out of 7 datasets. ED(4) achieves lower WER on CV-13, with pvalue of 0.26 when compared to DeCRED(4). Similarly, on 1https://github.com/openai/whisper/blob/main/whisper/normalizers/english. py 2Confidence intervals are displayed as subscripts and superscripts in corresponding tables. TABLE II COMPARISON OF ED AND DECRED MODELS ON OUT-OF-DOMAIN TEST SETS. Earnings22 Macro Avg."
        },
        {
            "title": "Dataset",
            "content": "Model ED(4) DeCRED(4) DeCRED(5) OWSM v3.1 Whisper medium"
        },
        {
            "title": "AMI\nihm",
            "content": "Gigaspeech 23.4 20.1 20.7 21.6 16.9 17.3 21.5 16.7 17.0 19.8 19.2 20.4 16.0 13.8 15.4 19.6 21.4 23.5 19.7 16.6 19.0 19.8 18.3 16.4 18.3 19.0 17.6 17.9 14.0 14.5 13.5 12.6 11.7 12.2 11.2 5.9 24.8 26.5 6.2 22.1 22.7 6.2 21.9 22.3 6.7 23.3 26.9 4.2 16.6 17.6 6.4 6.9 6.7 7.1 6.7 7.3 7.2 7.8 4.5 4.9 18.2 16.2 15. 15.9 11.7 Switchboard (eval2000), it outperforms DeCRED(4) with pvalue of 0.19. On all other test sets, DeCRED(4) demonstrates improvements over the baseline, with the following p-values: LS clean (0.24), LS other (0.13), TED-LIUM3 (0.20), VoxPopuli (0.19), and WSJ (0.10). for"
        },
        {
            "title": "To provide better context",
            "content": "Furthermore, inspired by Platt scaling [36], [37], we freeze the model parameters and train only the mixing weights for the decoding method described in Equation (5). With this setup, DeCRED(5) outperforms ED(4) on 6 out of 7 datasets (except CV-13), with p-value of 0.4 for Switchboard eval2000 and p-values below 0.2 for the remaining datasets. the reader, we include reference comparison with large-scale multilingual models: Whisper-medium3 [11] (764M parameters) and OWSM v3.1 [38] (1.02B parameters). This is intended as point of reference rather than direct comparison, given the substantial differences in model scale and design. For consistency, we apply the same text normalization pipeline to the outputs of Whisper and OWSM in our normalized setup. We also trained smaller variants of ED-small and DeCRED-small with 39M parameters, achieving macro-average WERs of 8.4 % and 8.1 %, respectively. B. Out of domain performance"
        },
        {
            "title": "Table II underscores",
            "content": "the significant WER reductions achieved by DeCRED(4) and DeCRED(5) on out-of-domain datasets, highlighting major outcome of our work. Despite not being exposed to these datasets during training, DeCRED delivers macro WER reduction of 2.0 and 2.3 percentage points, respectively. 3We use the multilingual version as it performs better across our datasets. TABLE III ZERO-ATTENTION ILM BPE-LEVEL PERPLEXITY ESTIMATION OF ED AND DECRED MODELS ON INAND OUT-OF-DOMAIN TEST SETS."
        },
        {
            "title": "Dataset",
            "content": "Model ED(4) 455.8 DeCRED(4) 215.7 CV-13 LS clean LS other SB eval2000 TEDLIUM3 VoxPopuli WSJ FLEURS AMI-ihm Gigaspeech Earnings-22 459.8 209.0 473.3 197.5 474.0 271.6 297.6 140. 286.2 141.0 676.8 723.2 306.7 161.1 537.8 310.4 297.7 134.1 592.1 266. TABLE IV MACRO AVERAGE WERS ON IN-DOMAIN DATASETS FOR DIFFERENT DECODING STRATEGIES."
        },
        {
            "title": "Decoding\nstrategy",
            "content": "greedy beam λ = 0 λ = 0.3 width 10, λ = 0.3 DeCRED(4) Early exiting DeCRED(5) 6.7 6.7 6.5 6.3 6.6 6. 5.8 6.2 5.8 exiting [41] is special case of DeCRED(5), where vD2[:] is set to one, and vD[:] are zeros. This allows decoding from fixed intermediate layer to reduce computation cost. The improvements in WER of DeCRED(5) are most notable under greedy decoding, which is illustrated in Figure 2. The the fastest model, relative slowdown is measured against ED-small(4), using fixed number of decoding steps and batch sizes constrained by the GPU memory of the A100 GPU. As shown in the figure, DeCRED improves WER over ED with minimal overhead. For DeCRED(5), the additional cost (cid:17) comes only from computing softmax . In particular, DeCRED-small () with greedy decoding performs similarly to ED (), while being more efficient and requiring fewer resources. d=1 vd (hdWd) (cid:16)(cid:80)D B. Effect of text normalization For consistency, we report most of the results in this work using normalized transcripts. However, to allow fair and direct comparison with previous and future works, we additionally trained and evaluated DeCRED(4) on original (unnormalized) transcripts. Table presents the resulting WERs, comparing DeCRED with Whisper medium and OWSM v3.1. DeCRED achieves performance on par with OWSM v3.1, with comparable macro-average WER (9.4 vs. 9.3), despite operating at significantly smaller scale172M parameters vs. 1.5B, 6K hours of English vs. 180K hours of multilingual data, and 2.2K vs. 24.6K A100 GPU hours. These results underscore the effectiveness of the proposed decoder-side regularization method, accompanied by an efficient training pipeline. We publicly release our recipes and framework to support reproducibility and adoption4. For broader comparison with more state-of-the-art models, we refer the reader to Open Automatic Speech Recognition Leaderboard [42]. C. Comparison with Encoder-Centric Regularization To contextualize DeCRED among related encoder-decoder regularization approaches, we conduct additional experiments 4https://github.com/BUTSpeechFIT/DeCRED Fig. 2. The impact of model size and decoding approach on the average time needed to transcribe an utterance and macro average WER While ED(4) performs better on FLEURS (p-values: 0.13 for DeCRED(4) and 0.24 for DeCRED(5)). DeCRED(4) and DeCRED(5) achieve statistically significantly lower WER on AMI (p = 0.004, < 0.001), Gigaspeech (p < 0.001, < 0.001), and Earnings-22 (p = 0.04, 0.13) respectively. Notably, OWSM v3.1 was trained on FLEURS, AMI, and Gigaspeech, while Whisper was trained on web-scale data that may include these datasets. Despite this, DeCRED performs comparably to OWSM v3.1 and remains competitive with Whisper, even when using greedy decoding with λ = 0. C. Analysis of internal language model In Table III, we present comparison of the subword-level perplexity estimates of the Zero-Attention Internal Language Model (ILM) [14] for the ED and DeCRED models. We note that these estimates should be interpreted with caution, as the ILM estimate is approximate and is not guaranteed to be valid when the end-to-end model does not strictly satisfy the conditions outlined in Proposition 1 of [39, Appendix A], [40]. With this caveat, the table shows consistent reductions in ILM perplexity for DeCRED compared to ED across all evaluated datasets. These reductions suggest an improvement in the regularization or internal language modeling capacity of DeCRED, which aligns with the WER trends observed in Table and Table II. V. FURTHER ANALYSIS A. Trade-off between performance and decoding time Table IV shows macro-average WERs on in-domain datasets for various decoding strategies applied to DeCRED. Early TABLE WERS (TOGETHER WITH CONFIDENCE INTERVALS) OF DECRED(4), WHISPER MEDIUM, AND OWSM V3.1 MODELS EVALUATED ON ORIGINAL TRANSCRIPTS ACROSS MULTIPLE IN-DOMAIN TEST SETS. Dataset CV-13 Model Whisper medium 13.2 13.9 12.6 DeCRED(4) 15.0 15.9 14.4 14.3 14.6 OWSM v3.1 14.0 SB eval LS clean LS other TEDLIUM3 VoxPopuli WSJ Macro Avg. 28.9 30.6 27.5 22.7 23.8 21.7 22.3 25.8 20.3 3.9 4.2 3.6 3.8 4.1 3.5 2.6 2.8 2.4 7.0 7.5 6.7 7.3 7.7 6.9 5.3 5.5 5.1 6.0 6.4 5.6 5.6 6.0 5.1 6.1 6.5 5.7 10.5 11.2 10.0 8.4 9.2 7.8 9.6 10.2 9.1 9.4 10.7 8.0 3.0 3.3 2.6 4.7 5.3 4. 11.3 9.4 9.3 TABLE VI WER COMPARISON OF OUR ED IMPLEMENTATION VS. ESPNETS ED, AND DECRED VS. INTERCTC ON THE TEDLIUM3 TEST SET. gains. Adding multiple auxiliary classifiers did not lead to any significant improvements."
        },
        {
            "title": "Model",
            "content": "Size [M] greedy beam width 40 VI. CONCLUSION AND LIMITATIONS ESPnet ED(4) Our ED(4) InterCTC(4) DeCRED(4) L/2 35.01 35.04 35.20 35.20 8.7 7.6 7.5 7.0 8. 7.2 7.1 6.8 TABLE VII EFFECT OF THE AUXILIARY CLASSIFIERS POSITION (d) AND WEIGHT (βd) ON WER FOR THE TEDLIUM3 TEST SET. Weight βd 0.3 0.4 0."
        },
        {
            "title": "Position d",
            "content": "2 7.5 7.1 3 7.0 7.1 6.7 4 7.0 6.8 7. 5 7.0 6.9 6.9 on the TEDLIUM3 dataset using small models (37M parameters) for both ED and DeCRED. Table VI compares DeCRED with the ED baseline and InterCTC [5], related method that applies intermediate supervision to the encoder. Both InterCTC and DeCRED introduce single auxiliary classifier with exactly the same parameter overhead (dmodel ), but only differ in the point of application: InterCTC regularizes the encoder, while DeCRED targets the decoder module. Although both approaches outperform the baseline ED, DeCRED yields the lowest WER, particularly under greedy decoding. For completeness, we also report results for the ESPnet ED baseline5, whose architecture, training configuration, and decoding setup we closely replicated to ensure comparable evaluation. Finally, Table VII examines the effect of varying the auxiliary classifiers position (d) and loss weight (βd). Consistent improvements are observed when placing the classifier around decoder layers 3 or 4 (the latter corresponding to D2), aligning with InterCTCs findings on optimal supervision depth. The best performance is achieved with β3 = 0.5 (6.7%) and β4 = 0.4 (6.8%), indicating that even single wellplaced decoder-side classifier is sufficient to obtain strong 5https://github.com/espnet/espnet/tree/master/egs2/tedlium3/asr1 We introduced the DeCRED regularization scheme, which effectively integrates an auxiliary classifier within the decoder of an encoder-decoder architecture. Alongside this, we proposed novel decoding method that leverages these classifiers. Without any additional computational overhead, DeCRED achieved lower WERs than the baseline model on 5 out of 7 in-domain datasets. More importantly, on 3 out of 4 outof-domain datasets, DeCRED obtained statistically significant WER reductions compared to the baseline. On average, DeCRED reduced the WER by 2.0 absolute percentage points on out-of-domain datasets, and the proposed domain adaptation scheme further improved WER by 0.3 absolute points in this setting. Despite its relatively small size, DeCRED achieved WERs comparable to much larger models such as OWSM v3.1 and Whisper medium. Despite these promising results, we identify few limitations in our work. First, due to computational budget constraints, we were only able to scale our experiments to 6k hours of training data and to model with 172M parameters. Secondly, our models were trained exclusively on English data, which complicates direct comparisons with multilingual models, as these models must allocate part of their capacity to handle multiple languages. It is also worth noting that some of the improvements from DeCRED diminish when beam-search decoding with wide beam is employed, as this comes at computational cost during inference."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "The work was supported by Ministry of Education, Youth and Sports of the Czech Republic (MoE) through the OP JAK project Linguistics, Artificial Intelligence and Language and Speech Technologies: from Research to Applications (ID:CZ.02.01.01/00/23 020/0008518). Computing on IT4I supercomputer was supported by MoE through the e-INFRA CZ (ID:90254)."
        },
        {
            "title": "REFERENCES",
            "content": "[1] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, SpecAugment: simple data augmentation method for automatic speech recognition, in Interspeech. ISCA, Sep. 2019. [Online]. Available: https://www.isca-archive.org/interspeech 2019/park19e interspeech.html [2] G. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and G. E. Hinton, Regularizing neural networks by penalizing confident output distributions, in 5th International Conference on Learning Representations, [Online]. Available: https://openreview.net/forum?id=HyhbYrGYe OpenReview.net, April 2017. ICLR. end-to-end speech recognition systems, in Interspeech. [3] S. Kim, M. Seltzer, J. Li, and R. Zhao, Improved training for online ISCA, 2018. [4] T. Hori, S. Watanabe, and J. Hershey, Joint CTC/attention decoding for end-to-end speech recognition, in Proceedings of the 55th the Association for Computational Linguistics Annual Meeting of (Volume 1: Long Papers). Vancouver, Canada: Association for Computational Linguistics, [Online]. Available: https: Jul. 2017. //aclanthology.org/P17-1048 [5] J. Lee and S. Watanabe, Intermediate loss regularization for CTC-based speech recognition, in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021. [6] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, ESPnet: End-to-end speech processing toolkit, in Interspeech. ISCA, 2018. [7] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. D. Mori, and Y. Bengio, SpeechBrain: generalpurpose speech toolkit, 2021, arXiv:2106.04624. [8] A. Narayanan, A. Misra, K. C. Sim, G. Pundak, A. Tripathi, M. Elfeky, P. Haghani, T. Strohman, and M. Bacchiani, Toward domain-invariant speech recognition via large scale training, in IEEE Spoken Language Technology Workshop (SLT). IEEE, Dec. 2018. [Online]. Available: https://ieeexplore.ieee.org/document/ [9] W. Chan, D. Park, C. Lee, Y. Zhang, Q. Le, and M. Norouzi, SpeechStew: Simply mix all available speech recognition data to train one large neural network, Apr. 2021. [Online]. Available: https://arxiv.org/abs/2104.02133v3 [10] K. C. Puvvada, P. Zelasko, H. Huang, O. Hrinchuk, N. R. Koluguri, K. Dhawan, S. Majumdar, E. Rastorgueva, Z. Chen, V. Lavrukhin, J. Balam, and B. Ginsburg, Less is more: Accurate speech recognition & translation without web-scale data, in Interspeech 2024, 2024, pp. 39643968. [11] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in Proceedings of the 40th International Conference on Machine Learning. iSSN: 2640-3498. [Online]. Available: https://proceedings.mlr.press/v202/radford23a.html PMLR, Jul. 2023, [12] Y. Peng, J. Tian, B. Yan, D. Berrebbi, X. Chang, X. Li, J. Shi, S. Arora, W. Chen, R. Sharma, W. Zhang, Y. Sudo, M. Shakeel, J.-W. Jung, S. Maiti, and S. Watanabe, Reproducing Whisper-style training using an open-source toolkit and publicly available data, in IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023. [13] W. Chen, J. Tian, Y. Peng, B. Yan, C.-H. H. Yang, and S. Watanabe, OWLS: Scaling laws for multilingual speech recognition and translation models, 2025. [Online]. Available: https://arxiv.org/abs/2502.10373 [14] M. Zeineldeen, A. Glushko, W. Michel, A. Zeyer, R. Schluter, and H. Ney, Investigating methods to improve language model integration for attention-based encoder-decoder ASR models, in Interspeech. ISCA, 2021. [15] Z. Zhao and P. Bell, Regarding the Existence of the Internal Language Model in CTC-Based E2E ASR, in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025, pp. 15. [16] J. Nozaki and T. Komatsu, Relaxing the conditional independence assumption of CTC-based ASR by conditioning on intermediate predictions, in Interspeech. ISCA, 2021. [17] C. Wang, Y. Wu, S. Chen, S. Liu, J. Li, Y. Qian, and Z. Yang, Improving self-supervised learning for speech recognition with intermediate layer supervision, in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. [18] J. Zhang, Y. Peng, H. Xu, Y. He, E. S. Chng, and H. Huang, Intermediate-layer output regularization for attention-based speech recognition with shared decoder, 2022, arXiv:2207.04177. [19] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe, E-Branchformer: Branchformer with enhanced merging for speech recognition, in IEEE Spoken Language Technology Workshop (SLT), Jan. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/ 10022656 [20] J. Carletta, Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus, Language Resources and Evaluation, vol. 41, no. 2, 2007. [21] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, FLEURS: Few-Shot Learning Evaluation of Universal Representations of Speech, in 2022 IEEE Spoken Language Technology Workshop (SLT), Jan. 2023. [Online]. Available: https://ieeexplore.ieee.org/document/ [22] G. Chen, S. Chai, G.-B. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang, M. Jin, S. Khudanpur, S. Watanabe, S. Zhao, W. Zou, X. Li, X. Yao, Y. Wang, Z. You, and Z. Yan, Gigaspeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio, in Interspeech. ISCA, 2021. [23] M. D. Rio, P. Ha, Q. McNamara, C. Miller, and S. Chandra, Earnings-22: practical benchmark for accents in the wild, 2022, arXiv:2203.15591. [Online]. Available: https://arxiv.org/abs/2203.15591 [24] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks, in Proceedings of the 23rd international conference on Machine learning, ser. ICML 06. New York, NY, USA: Association for Computing Machinery, Jun. 2006. [Online]. Available: https://dl.acm.org/doi/10.1145/1143844.1143891 [25] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov, Transformer-XL: Attentive language models beyond fixed-length context, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Italy: Association for Computational Linguistics, [Online]. Available: https: //aclanthology.org/P19-1285 Jul. 2019. Florence, [26] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, Conformer: Convolutionspeech recognition, in Interspeech. augmented Transformer ISCA, Oct. 2020. [Online]. Available: https://www.isca-archive.org/ interspeech 2020/gulati20 interspeech.html for [27] T. Kudo and J. Richardson, SentencePiece: simple and language independent text subword tokenizer and detokenizer processing, in Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Brussels, Belgium: Association for Computational Linguistics, Nov. 2018. [Online]. Available: https://aclanthology.org/D18-2012 for neural [28] J. Godfrey, E. Holliman, and J. McDaniel, SWITCHBOARD: telephone speech corpus for research and development, in Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1, 1992. [29] D. B. Paul and J. M. Baker, The design for the Wall Street Journalbased CSR corpus, in Speech and Natural Language: Proceedings of Workshop Held at Harriman, New York, February 1992. [Online]. Available: https://aclanthology.org/H92- [30] R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Henretty, R. Morais, L. Saunders, F. Tyers, and G. Weber, Common Voice: massively-multilingual speech corpus, in Proceedings of the Twelfth Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association, May 2020. [Online]. Available: https://aclanthology.org/2020.lrec-1.520 [31] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, Librispeech: An ASR corpus based on public domain audio books, in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2015, iSSN: 2379-190X. [Online]. Available: https: //ieeexplore.ieee.org/document/7178964 [32] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, VoxPopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Aug. 2021. [Online]. Available: https://aclanthology.org/2021.acl-long.80 [33] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Est`eve, TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation, in Speech and Computer. Cham: Springer International Publishing, 2018. [34] I. Decoupled weight Loshchilov Conference regularization, Representations. 2019. https://openreview.net/forum?id=Bkg6RiCqY7 F. Hutter, International OpenReview.net, decay Learning [Online]. Available: and in on [35] L. Ferrer and P. Riera, Confidence intervals for evaluation in machine learning. [Online]. Available: https://github.com/luferrer/ ConfidenceIntervals [36] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, On calibration the 34th ICML - [Online]. Available: of modern neural networks, in Proceedings of Volume 70, ser. https://proceedings.mlr.press/v70/guo17a/guo17a.pdf JMLR.org, 2017. ICML17. [37] M.-H. Lee and J.-H. Chang, Deep neural network calibration for E2E speech recognition system, in Interspeech. ISCA, 2021. [38] Y. Peng, J. Tian, W. Chen, S. Arora, B. Yan, Y. Sudo, M. Shakeel, K. Choi, J. Shi, X. Chang, J. weon Jung, and S. Watanabe, OWSM v3.1: Better and faster open Whisper-style speech models based on EBranchformer, in Interspeech. ISCA, 2024, pp. 352356. [39] E. Variani, D. Rybach, C. Allauzen, and M. Riley, Hybrid autoregressive transducer (HAT), in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 61396143. [40] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen, R. Zhao, J. Li, and Y. Gong, Internal language model estimation for domain-adaptive end-to-end speech recognition, in IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243250. [41] S. Scardapane, M. Scarpiniti, E. Baccarelli, and A. Uncini, Why should we add early exits to neural networks? Cognitive Computation, vol. 12, no. 5, Sep. 2020. [Online]. Available: https://doi.org/10.1007/ s12559-020-09734-4 [42] V. Srivastav, S. Majumdar, N. Koluguri, A. Moumen, S. Gandhi et al., Open automatic speech recognition leaderboard, https://huggingface. co/spaces/hf-audio/open asr leaderboard, 2023."
        }
    ],
    "affiliations": [
        "Brno University of Technology, Brno, Czech Republic"
    ]
}