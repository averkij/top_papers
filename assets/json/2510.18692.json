{
    "paper_title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
    "authors": [
        "Weinan Jia",
        "Yuning Lu",
        "Mengqi Huang",
        "Hualiang Wang",
        "Binyuan Huang",
        "Nan Chen",
        "Mu Liu",
        "Jidong Jiang",
        "Zhendong Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 9 6 8 1 . 0 1 5 2 : r MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation Weinan Jia1,, Yuning Lu2,, Mengqi Huang1, Hualiang Wang3,, Binyuan Huang4,, Nan Chen1, Mu Liu2, Jidong Jiang2, Zhendong Mao1, 1University of Science and Technology of China, 2FanqieAI, ByteDance China, 3Hong Kong University of Science and Technology, 4Wuhan University Work done at FanqieAI, ByteDance China, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by small subset of querykey pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracyefficiency trade-offs are constrained by block size. This paper introduces Mixtureof-Groups Attention (MoGA), an efficient sparse attention that uses lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach. Date: October 22, 2025 Project Page: MoGA"
        },
        {
            "title": "Introduction",
            "content": "A growing body of research indicates that scaling laws are primary driver of progress toward artificial general intelligence [4, 24, 33]. As model parameters and data scale to billions, Transformer-based foundation models [35] often exhibit emergent capabilities [24, 30, 39]. In video generation, given the inherently temporal nature, progress requires not only scaling parameters and data but, more critically, scaling the effective context length. This need is especially salient for long-form video generation (e.g., movies), where persistent memory is essential for maintaining consistency of environments and characters [46]. The main challenge of vanilla attention [35] for long sequences is its computational cost, which grows quadratically with the context length. To mitigate the challenge, prior work [17, 34, 37, 42, 56] adopts multi-stage pipeline that first generates key frames and then synthesizes intermediate frames. However, this design yields disjoint objectives that are not directly optimized for the end task, leading to error accumulation across stages. It also introduces hand-crafted inductive biases, hindering scalability. For end-to-end long video generation, one line of work compresses historical content to accommodate longer contexts (e.g., via recurrent layers [7] or FramePack [50]), which inevitably results in information loss. 1 Figure 1 Illustration of our motivation. (a) Full attention suffers from dense computing when dealing with long sequences. (b) Block sparse attention [27] may fail when block-level similarity is confused, resulting in unreliable attention. (c) Mixture-of-Groups attention uses lightweight token router (i.e., single linear layer ) that assigns tokens to specialized groups, enabling groupwise attention and efficient long-context modeling. complementary direction exploits sparse attention [49] by restricting computation to selected subset of salient querykey pairs. Existing selection strategies generally fall into two categories: (i) static selection, i.e., prior-driven heuristics that emphasize local spatiotemporal neighborhoods, which is efficient but limited in capturing dynamic long-range dependencies [12, 26, 32, 41]; and (ii) coarse-to-fine dynamic selection, which first estimates block-level importance scores, routes query tokens to the top-k blocks, and then applies fine-grained attention within the selected blocks [5, 27, 40, 43, 47]. As shown in Fig.1(b), the latter introduces an efficiency-performance trade-off: using larger blocks with small top-k reduces the computational cost of the coarse stage but reduces selection performance. In this work, we reveal that such coarse-grained estimation is unnecessary and each token should be precisely allocated. To achieve this, we propose Mixture-of-Groups Attention (MoGA), simple and efficient dynamic token routing solution for end-to-end long video generation. lightweight router (i.e., single linear layer ) is employed to assign tokens to specific groups, as illustrated in Fig. 1(c), inspired by the Mixture-of-Experts (MoE) [21]. Full attention is then performed within each group, where the groupwise attention integrates seamlessly with modern attention kernels, e.g., FlashAttention [8]. Intuitively, the linear routers weights can be viewed as implicit cluster centers, enabling direct assignment of tokens to learnable anchors, without global similarity estimation. Furthermore, to balance long-range coherence and local fidelity, we couple MoGA with the spatiotemporal window attention [12], which can be considered as groupwise attention with static, predefined groups. In addition, extended context alone is insufficient because single global prompt cannot reliably control scene transitions or orchestrate events at precise time points in long videos. We therefore introduce shot-level textual conditioning via cross-modal attention, where each shot is guided by concise description [14, 38]. To support this, we build data pipeline that produces minute-level video samples with dense, multi-shot captions and reliable shot segmentation. Our contributions: We propose MoGA, an effective sparse attention mechanism that replaces block-level scoring with precise group assignment via lightweight token router, enabling effective modeling of long contexts. Building on MoGA, we introduce video generation model capable of producing minute-level, multi-shot, 480p videos at 24 fps with context length of about 580k tokens. Fig. 7 illustrates one-minute video generated by our model. Extensive evaluations show consistent improvements over state-of-the-art (SoTA) sparse attention baselines and multi-shot video generation model."
        },
        {
            "title": "2.1 Long Video Generation",
            "content": "Previous work on long video generation beyond typical duration limits has converged on three main paradigms. Multistage methods decompose long video generation into multiple steps [17, 34, 37, 42, 44, 56]. For example, Captain Cinema [42] adopts hierarchical planning with top-down keyframe generation and bottom2 up synthesis for narrative coherence. Multistage approaches introduce hand-crafted inductive biases and pose challenges for end-to-end optimization. Autoregressive approaches generate videos through sequential segment synthesis [1, 6, 14, 16, 19, 45]. Diffusion Forcing [6] adapts denoising schedules for variable sequence lengths. CasusVid [45] distills bidirectional models into an efficient autoregressive model. StreamingT2V [16] combines shortand long-term memory for streaming video extension. FAR [14] introduces hierarchical causal representations for multiscale dependencies. MAGI-1 [1] demonstrates the scaling capability of this paradigm. Context compression methods address computational constraints by compressing historical content [7, 23, 50]. TTT [7] compresses long context via bidirectional recurrent layer. FramePack [50] employs importance-based frame compression to maintain fixed computational budget. However, these methods either produce videos of limited duration [1, 6, 19] or fail to generate multi-shot videos in real-world scenes [7, 14, 16, 23, 45, 50]. closely related line of work is LCT [15], which models interleaved multi-shot prompts and videos within local context window using full attention. While pioneering end-to-end multi-shot long video generation, LCT remains constrained by the quadratic cost of full attention."
        },
        {
            "title": "2.2 Sparse Attention for Video Generation",
            "content": "Attentionbased foundation models unify many domains and consistently exhibit common sparsity structure [9, 27, 47]. In video generation, given the inherent sparsity, natural approach to efficient generation is to select important query-key pairs. Prior work broadly falls into two categories: static priors [26, 41, 51] and coarse-to-fine dynamic routing [40, 43, 52]. Among static approaches, STA [51] employs 3D sliding windows with hardware-aware implementation. SVG [41] uses online pattern selection to classify attention heads as spatial or temporal sparse attention. Radial Attention [26] introduces static attention mask to perform spatiotemporal attention with O(n log n) complexity. However, these methods have difficulty modeling evolving long-range dependencies, which are crucial for maintaining cross-shot consistency. Another line of work adopts dynamic token routing for sparse attention. VSA [52] first obtains compressed representations of contiguous spatiotemporal blocks, and then selects the top-k blocks for fine-grained attention. Similarly, VMoBA [40] introduces an improved MoBA [27] tailored to video generation. In such methods, the block size presents trade-off between expressiveness and efficiency. Smaller blocks yield more accurate coarse-grained attention estimates but reduce efficiency. In addition, SVG2 [43] is training-free dynamic sparse attention method that performs online k-means clustering over tokens during inference and selects the top-k clusters based on their centroids. It shares similar motivation with MoGA, i.e., tokens can be grouped into semantically coherent clusters. However, online clustering in SVG2 introduces additional k-means computations during the forward pass and is not straightforward to differentiate through. In contrast, MoGA employs trainable cluster centroids to enable simple and efficient routing with minimal computational overhead, making it suitable for end-to-end training."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Vanilla self-attention [35] plays crucial role in video generation with Diffusion Transformers (DiTs) [29]. Consider an input sequence RN d, where = is the total number of tokens across the latent spatial dimensions (h w) and the latent temporal dimension (t), and denotes the models hidden dimension. For simplicity, we consider single query case, where is token from the input sequence and is its corresponding query. Vanilla self-attention (SA) is computed as: SA(q, K, ) = softmax( qK ) , (1) where and denote the keys and values. While self-attention excels at capturing long-range dependencies via global information aggregation, it incurs quadratic computational complexity of O(N 2). The computational burden becomes particularly prohibitive in long-video generation. For example, generating 1-minute video at 480p with approximately 1,600 tokens per frame across 961 frames (16 fps) yields total token count of about 384k1. Performing full attention on such long sequence is intractable. 1Following Wan [36], the VAE downsampling factors for (t, h, w) are (4, 8, 8) and patchify sizes are (1, 2, 2). 3 Figure 2 Left: Our model adopts DiT architecture with interleaved Visual Attention and Cross-Modal Attention blocks. Visual Attention exclusively processes visual content, whereas Cross-Modal Attention enables shot-level text conditioning, instantiated via either cross-attention [36] or multi-modal attention [10, 25]. Top-right (a): Visual Attention combining MoGA with Spatial-Temporal Group Attention for global-local consistency. Bottom-right (b): MoGA, where router groups tokens and performs intra-group attention, enabling long-range global interactions. In this section, we introduce MoGA for efficient long video generation. The overall architecture is shown in Fig. 2. We first present the preliminaries, then detail MoGA, and finally describe the pipeline for constructing multi-shot long-video training data. Beyond computational cost, full attention is not ideally aligned with the structure of videos. In videos, softmax attention is inherently sparse [41] because nearby tokens exhibit strong local spatiotemporal correlation, while only few globally shared, dynamic semantics persist across frames. Most querykey pairs contribute little, whereas small subset dominates [13]. For long videos, attention should leverage this sparsity by prioritizing important querykey interactions to reduce redundancy."
        },
        {
            "title": "3.2 Mixture-of-Groups Attention (MoGA)",
            "content": "MoGA addresses the above challenge via efficient token routing, where lightweight, trainable router assigns correlated tokens to groups and performs self-attention within each group. Specifically, the router is linear projection followed by softmax gating, similar to MoE [11]. Given token Rd and predetermined number of groups , the router computes routing scores RM as: = Router(x). The group assignment probabilities are computed as: and the token is assigned to the group with the highest probability: p(i x) = softmax(r)i, g(x) = arg max i[M ] p(i x). (2) (3) (4) Following group assignment, we apply self-attention independently within each group. The MoGA output is: MoGA(x) = p(g(x) x) SA(q, Kg(x), Vg(x)), (5) 4 Figure 3 Visualization of dynamic router grouping. Algorithm 1 MoGA Pseudocode with FlashAttention Require: Q, K, are the query, key and value of tokens 1: = router(X) 2: ˆQ, ˆK, ˆV , cu_seqlen, max_seqlen, permute_index = permute(Q, K, , g) 3: ˆO = flash_attn( ˆQ, ˆK, ˆV , cu_seqlen, max_seqlen) 4: = repermute( ˆO, permute_index) MoGA routing results MoGA recovers the original token positions where Kg(x) and Vg(x) are the keys and values of the group g(x), and is the query feature of x. This grouped attention mechanism reduces computational complexity from O(N 2) to theoretical minimum of O(N 2/M ) under uniform group assignment. As illustrated in Fig. 3, we extract the grouping assignments from an intermediate-layer router during the video generation process and visualize one representative group. After end-to-end training, the router assigns the mans head, hands, and portions of his clothing to the same group, indicating its ability to capture semantically coherent structures that span shot boundaries. MoGA builds on groupwise attention and remains compatible with high-performance kernels such as FlashAttention [8] (see Alg. 1). Beyond sparse attention, second pillar of long-context modeling is sequence parallelism [22], with which MoGA is also compatible. Before the sequence gather and head scatter step in each attention layer, MoGA computes routing scores over tokens (with whole heads) and then aggregates the routing results across all tokens. Group Balancing Loss. potential issue with token assignment is that the router may collapse by routing most tokens to only few groups, which would degenerate MoGA into full attention To encourage adaptive token allocation across groups, we introduce an auxiliary group balancing loss, inspired by the load balancing loss [11] used in MoE. The loss is defined as: Lgb = α (cid:88) i=1 FiPi, where α is loss weight and Fi is the fraction of tokens allocated to group i, Fi ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) 1(g(x) = i), where 1 is the indicator function, and Pi is the mean routing probability allocated for group i, Pi ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) g(x)=i p(g(x) x). (6) (7) (8) Minimizing Lgb encourages uniform token assignment across groups, as this objective attains its minimum under uniform distribution [11]. Spatial-Temporal Group Attention. Although MoGA captures long-range coherence, it lacks local continuity. We complement it with local spatiotemporal group attention (STGA) [12, 53], which restricts self-attention 5 Figure 4 Multi-shot long video data pipeline. to local windows in latent video space, as shown in Fig. 2(a). This captures short-range dependencies with bounded compute. We first partition the latent video into fixed spatial windows and then group frames along the temporal axis. Frames from different shots are assigned to distinct temporal groups. We empirically find that completely removing inter-shot interactions causes flicker in the first frame after shot cut. To mitigate this, when computing group attention, we augment the keys and values with two latent frames from adjacent shots (without augmenting the queries). This preserves continuity at shot boundaries with negligible additional compute. To enable intra-frame information exchange, we also perform per-frame attention by grouping tokens within each latent frame. Each token therefore receives outputs from multiple groups (one dynamic and two static), and we take their mean as the final output."
        },
        {
            "title": "3.3 Data Pipeline",
            "content": "We construct pipeline that converts raw long videos into one-minute, multi-shot clips with dense annotations for long video generation. The pipeline has two stages: video-level phase and shot-level phase  (Fig. 4)  . Video-Level. We first analyze raw videos using visual quality assessment (VQA) models (e.g., aesthetics [31], clarity, exposure) and simple operators (e.g., black-border detection) to obtain metadata and quality scores. We then filter raw videos with source-specific, calibrated thresholds to remove low-quality content. Because long video samples require temporal coherence, we relax clip-level filtering [25, 54] while applying stricter filtering at the source (raw-video) level. Next, we segment each video into single-shot clips using AutoShot [55] and PySceneDetect [3]. AutoShot shows higher sensitivity to fades and gradual transitions. Combining predictions from both tools allows us to label whether boundary is clean or affected by transition overlap. This stage yields pool of single-shot clips. Shot-Level. We process single-shot clips using VQA and optical character recognition (OCR) models and discard low-quality clips. Based on OCR results, we compute maximum-area crop that excludes watermarks and subtitles while preserving the original aspect ratio. Clips with insufficient retained area are discarded. Next, we generate captions for cropped clips using multimodal large language model [2]. Finally, we merge temporally adjacent single-shot clips into multi-shot training samples (up to 65 seconds) and trim few frames from clips affected by transition overlap to ensure clean boundaries."
        },
        {
            "title": "4 Experiments",
            "content": "Training Settings. We fine-tune MoGA on existing DiT-based short video generation models with the rectified flow objective [10]. For fair comparison with baselines, we train MoGA on the open-source Wan2.1 models (1.3B and 14B) [36]. The resulting model stably generates 477 frames at 16 fps (30 seconds) and 480p resolution, with context length of 187k. We use constant learning rate of 1e-5. The loss weight α is set to 0.1. We set the number of groups to = 5 and partition the spatial grid into 2 2 groups. We adopt multistage training strategy: 3k steps on 10-second clips followed by 1k steps on 30-second clips. Because MoGA is general sparse attention, we also apply it to video generation model built on MMDiT [10, 12, 25]. Unlike Wan, this model replaces cross-attention with MMDiT to perform cross-modal attention. It partitions space into 4 4 groups and sets the routers group number to = 20, enabling much longer context length. The MMDiT-based model generates 1,441 frames at 24 fps (60 seconds) at 480p, with context length of 578k. 6 Method Base Model Subject Consistency Background Consistency Motion Smoothness Aesthetic Quality Image Quality Sparsity Wan (Original) Wan2.1-14B DiTFastAttn (Training-based) Wan2.1-14B Wan2.1-14B SVG (Training-free) Wan2.1-14B VMoBA (Training-free) MoGA (Ours) Wan2.1-14B 0.9611 0.9456 0.9002 0. 0.9699 0.9560 0.9394 0.8926 0.8876 0.9542 0.9936 0.9924 0.9870 0. 0.9927 0.5807 0.5269 0.5370 0.5369 0.5810 0.6680 0.6466 0.6357 0. 0.6994 0% 50.00% 50.00% 31.00% 71.25% Table 1 Quantitative comparison for 5-second single-shot video generation. Method Base Model Subject Consistency Background Consistency Motion Smoothness Aesthetic Quality Image Quality Cross-Shot DINO Cross-Shot CLIP IC-Lora+Wan Wan2.1-1.3B Wan2.1-1.3B EchoShot MoGA (Ours) Wan2.1-1.3B 0.9476 0.9544 0. 0.9538 0.9518 0.9597 0.9901 0.9939 0.9919 0.5237 0.5718 0.6684 0. 0.4669 0.5961 0.5890 0.6729 0.6623 0.7169 0.8469 0. Table 2 Quantitative comparison for 10-second multi-shot video generation. Method Base Model Subject Consistency Background Consistency Motion Smoothness Aesthetic Quality Image Quality IC-Lora+Wan Wan2.1-14B MoGA (Ours) Wan2.1-14B MoGA (Ours) MMDiT 0.8946 0.9572 0. 0.9169 0.9475 0.9301 0.9872 0.9893 0.9895 0.5759 0.5789 0. 0.6835 0.6993 0.6996 Table 3 Quantitative comparison for 30-second multi-shot long video generation. Baselines. To evaluate our method, we compare with multiple baselines. For multi-shot long video generation, we include the keyframe-based pipeline IC-LoRA+Wan [18, 36] and EchoShot [38], which natively supports multi-shot generation. For sparse video generation, we compare against sparse attention methods, including the training-based DiTFastAttn [48] and the training-free methods SVG [41] and VMoBA [40]. Evaluation Metrics. Following prior work, we evaluate all methods using the metrics introduced by VBench [20]. Specifically, subject consistency and background consistency measure how well the main subjects and backgrounds of sampled frames are preserved throughout the video. Motion smoothness measures motion fluidity, penalizing jitter and abrupt transitions. We also report aesthetic quality and image quality to quantify the visual appeal and technical fidelity of each frame. To compute cross-shot consistency, we first sample fixed number of frames from different shots. We then compute feature similarities across shots using CLIP [30] and DINOv2 [28], referred to as Cross-Shot CLIP and Cross-Shot DINO. For single-shot 5-second video generation, we constructed diverse test set comprising 300 prompts. For multi-shot 10-second video generation, we use the 100 multi-shot prompt sets from [38]. For long video generation, we evaluate on test set of 11 scripts comprising 105 prompts. Each script contains 810 shots to produce 30-second video."
        },
        {
            "title": "4.1 Quantitative Results",
            "content": "First, we compare MoGA with prior sparse attention methods for single-shot, short video generation, following their evaluation settings to ensure fairness. As shown in Tab. 1, despite higher sparsity, MoGA achieves consistent improvements over existing sparse baselines across metrics. It is worth noting that although our method is highly sparse, it can still match or surpass the original Wan (full attention) on multiple metrics. Next, we compare MoGA with other multi-shot video generation methods. Tab. 2 reports quantitative comparisons among MoGA, IC-LoRA+Wan and EchoShot. Despite relying on sparse attention, our method outperforms the full attention baseline (EchoShot) on most metrics, indicating that preserving interactions among salient tokens not only reduces FLOPs but also suppresses noise from irrelevant content. This leads to stronger character identity consistency and improved temporal scene coherence. Finally, we benchmark long video generation against the baseline. Because few open-source methods can produce 30-second, multi-shot videos, we compare MoGA (with two backbones) to IC-LoRA+Wan. As 7 Figure 5 Qualitative results of MoGA and other methods. We present eight representative shots, demonstrating long-content coherence, character consistency, and visual quality. Figure 6 Computational efficiency. The x-axis denotes the generated video duration (s). As the number of groups (M ) increases, MoGAs FLOPs decrease substantially. shown in Tab. 3, MoGA substantially outperforms IC-LoRA+Wan under the same backbone, highlighting the benefits of end-to-end modeling over multistage pipelines. Notably, even under aggressive sparsity, MoGA with MMDiT maintains high visual fidelity, indicating scalable path to longer context lengths."
        },
        {
            "title": "4.2 Qualitative Results",
            "content": "Visual Comparison. In this subsection, we present qualitative results on 30-second videos across representative baselines. Because EchoShot cannot natively produce 30-second outputs, we concatenate video clips generated by EchoShot to form the full sequence. As shown in Fig. 5, the IC-LoRA+Wan pipeline is constrained by its per-iteration image cap (typically three frames), which limits its ability to cover larger number of shots. Consequently, it often exhibits subject drift and background inconsistency as the sequence progresses. 8 Figure 7 One-minute video generated by MoGA. EchoShot scales to more shots but still shows notable cross-shot inconsistencies on long temporal ranges. In contrast, MoGA maintains stable, coherent content over extended durations. For example, even without repeated or explicit specification across shots, the womans hat remains consistently preserved. Since STGA lacks explicit cross-shot information exchange, this consistency can be attributed to MoGA, which effectively selects and maintains shot-spanning identity and context. One-Minute Video of 1,441 Frames. In Fig. 7, we present the generated results of MoGA on an ultra-long video of over one minute, using the MMDiT-based MoGA model (M = 20). MoGA maintains strong long-range contextual consistency. The 1st and 22nd shots remain highly coherent, and fine details such as the womans hairpin and earrings are preserved across shots. Moreover, even with multiple faces appearing across different shots, the model avoids identity confusion. Emergence of Background Consistency. As shown in Fig. 8, we demonstrate MoGAs ability to maintain background consistency. After training on long, multi-shot videos, MoGA exhibits emergent, implicit control over consistency in both the environment and the characters. Even without explicit specification of details (e.g., the cabinet shape and the position of intravenous drip bottle), different shots automatically maintain coherent, temporally consistent depictions. Multi-Style Video Generation. Fig. 9 illustrates MoGAs multi-style generation capability. MoGA not only performs strongly in realistic spaces but also excels in stylized domains such as animation. It can produce high-quality, long-form 2D videos while maintaining temporal coherence, identity consistency, and scene continuity across diverse styles. 9 Figure 8 Emergence of background consistency. Figure 9 Animation style generation for MoGA."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Computational Efficiency. Fig. 6 plots the relationship between the number of groups (M ) and FLOPs for the Wan2.1-1.3B model. Our experiments show that even with relatively small group count (M =5) for 30-second videos, MoGA achieves substantial computational savings compared to full attention (2.26 PFLOPs vs. 6.94 PFLOPs). It also delivers 1.7 speedup in both training and inference. Notably, unlike alternative sparse attention such as VMoBA, which incur additional memory overhead due to their block-based mechanisms, our approach maintains memory efficiency without additional memory consumption. Routing Group Number . We conduct an ablation study on the number of groups under fixed computational budget (Tab. 4). Cross-shot DINO and CLIP scores exhibit rise-then-fall trend as the number of groups increases. This suggests that moderate level of grouped sparsity strikes balance between global consistency and efficiency, yielding near-optimal consistency while maintaining computational efficiency. Controllability of Subject Consistency. Fig. 10 visualizes comparison between MoGA and full attention. Both models are trained on 10-second data with Wan2.1-14B. The left panel illustrates MoGAs ability to maintain subject identity across multiple scenes, while the right panel demonstrates its robustness to appearance changes (e.g., clothing) when preserving identity consistency. Despite 71.25% sparsity, MoGA achieves narrative coherence and content editability on par with full attention, and in some cases delivers superior performance. 10 Group Numbers Cross-Shot DINO Cross-Shot CLIP Sparsity PFLOPs 1 2 4 8 16 0.8206 0.8589 0.8672 0.8606 0.8569 0.5919 0.6761 0.6853 0.6910 0. 0% 41.25% 66.25% 78.75% 81.25% 0.88 0.59 0.42 0.36 0.35 Table 4 Results of consistency for MoGA with Wan2.1-1.3B on 10-second videos. Figure 10 Visual comparison of MoGA vs. full attention for multi-shot generation with single subject. The left column shows the subject wearing the same outfit across different shots, while the right column shows the subject changing outfits at shot transitions according to the text instructions. Figure 11 Visual ablation of MoGA and STGA. Effectiveness of MoGA and STGA. As shown in Fig. 11, MoGA and STGA play complementary roles in enabling context-consistent long video generation. Using MoGA alone lacks local information exchange and fails to produce meaningful visual content. Conversely, using only STGA limits long-range shot interactions, leading to poor cross-shot consistency and weakened narrative coherence. When combined, the model achieves strong cross-shot consistency. These results indicate that MoGA effectively routes and preserves shot-spanning identity and context at relatively low computational cost."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces MoGA, sparse attention mechanism that replaces coarse block-level scoring with precise, learned group assignments via lightweight token router. By routing tokens into coherent groups, MoGA improves attention efficiency and fidelity for very long contexts. Building on MoGA, we propose the video generation model that produces minute-level, multi-shot videos at 480p resolution and 24 fps. Diverse experiments in video generation further demonstrate the effectiveness of our approach. Acknowledgments. We thank the ByteDance Seedance team and Wenfeng Lin for their support."
        },
        {
            "title": "References",
            "content": "[1] Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, and Yuqiao Li. Magi-1: Autoregressive video generation at scale. arXiv:2505.13211, 2025. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv:2502.13923, 2025. [3] Breakthrough and Contributors. Pyscenedetect: Video scene cut detection tool, 2014. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. [5] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv:2508.21058, 2025. [6] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In NeurIPS, 2024. [7] Karan Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In CVPR, 2025. [8] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv:2307.08691, 2023. [9] DeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context efficiency with deepseek sparse attention, 2025. [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [11] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. JMLR, 2022. [12] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv:2506.09113, 2025. [13] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv:2310.01801, 2023. [14] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv:2503.19325, 2025. [15] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv:2503.10589, 2025. [16] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In CVPR, 2025. [17] Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, and Xihui Liu. Filmaster: Bridging cinematic principles and generative ai for automated film generation. arXiv:2506.18899, 2025. [18] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv:2410.23775, 2024. [19] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. In NeurIPS, 2025. 12 [20] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. [21] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural Computation, 1991. [22] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv:2309.14509, 2023. [23] Jiaxiu Jiang, Wenbo Li, Jingjing Ren, Yuping Qiu, Yong Guo, Xiaogang Xu, Han Wu, and Wangmeng Zuo. Lovic: Efficient long video generation with context compression. arXiv:2507.12952, 2025. [24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv:2001.08361, 2020. [25] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv:2412.03603, 2024. [26] Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, et al. Radial attention: O(n log n) sparse attention with energy decay for long video generation. arXiv:2506.19852, 2025. [27] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv:2502.13189, 2025. [28] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv:2304.07193, 2023. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, 2023. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [31] Christoph Schuhmann, Romain Vencu, Romain Beaumont, Radu Kaczmarczyk, Charlie Mullis, Ashish Katta, Jenia Jitsev Coombes, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. [32] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv:2504.08685, 2025. [33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. [34] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di ZHANG, et al. Videotetris: Towards compositional text-to-video generation. In NeurIPS, 2024. [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [36] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv:2503.20314, 2025. [37] Bo Wang, Haoyang Huang, Zhiying Lu, Fengyuan Liu, Guoqing Ma, Jianlong Yuan, Yuan Zhang, Nan Duan, and Daxin Jiang. Storyanchors: Generating consistent multi-scene story frames for long-form narratives. arXiv:2505.08350, 2025. [38] Jiahao Wang, Hualian Sheng, Sijia Cai, Weizhan Zhang, Caixia Yan, Yachuang Feng, Bing Deng, and Jieping Ye. Echoshot: Multi-shot portrait video generation. arXiv:2506.15838, 2025. [39] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv:2206.07682, 2022. 13 [40] Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, and Yunhai Tong. Vmoba: Mixture-of-block attention for video diffusion models. arXiv:2506.23858, 2025. [41] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv:2502.01776, 2025. [42] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. arXiv:2507.18634, 2025. [43] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv:2505.18875, 2025. [44] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv:2303.12346, 2023. [45] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. [46] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv:2506.03141, 2025. [47] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv:2502.11089, 2025. [48] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. In NeurIPS, 2024. [49] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. In NeurIPS, 2020. [50] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv:2504.12626, 2025. [51] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv:2502.04507, 2025. [52] Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Faster video diffusion with trainable sparse attention. arXiv:2505.13389, 2025. [53] Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, and Bingyue Peng. Waver: Wave your way to lifelike video generation. arXiv:2508.15761, 2025. [54] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv:2412.20404, 2024. [55] Wentao Zhu, Yufang Huang, Xiufeng Xie, Wenxian Liu, Jincan Deng, Debing Zhang, Zhangyang Wang, and Ji Liu. Autoshot: short video dataset and state-of-the-art shot boundary detection. In CVPR, 2023. [56] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream vlog. In CVPR, 2024."
        },
        {
            "title": "A Details of the Computational Complexity",
            "content": "As shown in Tab. 5, it reports the computational cost under varying number of groups (M) and video duration. As the generation video duration increases, the computational complexity of STGA exhibits approximately linear growth and the computational complexity of MoGA is approximately 1/M of that of Full Attention."
        },
        {
            "title": "Video Duration",
            "content": "5s 10s 15s 20s 30s"
        },
        {
            "title": "Frames\nSequence Length",
            "content": "Full Attention MoGA (M = 5) MoGA (M = 10) MoGA (M = 20) 77 31200 0.28 0.093 0.065 0.051 157 62400 237 93600 317 477 187200 0.88 0.34 0.25 0.21 1.85 0.67 0.48 0.39 3.19 1.09 0.78 0.61 6.94 2.26 1.56 1."
        },
        {
            "title": "PFLOPs",
            "content": "Table 5 Compute (PFLOPs) versus group number and video duration on Wan2.1-1.3B."
        },
        {
            "title": "B Analysis of Group Balancing Loss",
            "content": "As shown in Fig. 12, we validate the effectiveness of the group balancing loss, which measures the balance of the routers token-to-group assignments. higher value indicates that tokens concentrate in few groups, whereas lower value indicates more balanced grouping. When we include this loss during training, the metric rapidly converges to around 1, reflecting globally balanced assignments. In contrast, without it, the metric increases as the router funnels tokens into few groups to gain short-term advantages in the diffusion MSE loss. Because our goal is to separate weakly related tokens and maintain balanced grouping, the additional group balance loss is necessary to enforce the desired assignments. Figure 12 Group balancing loss curves of MoGA."
        }
    ],
    "affiliations": [
        "FanqieAI, ByteDance China",
        "Hong Kong University of Science and Technology",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}