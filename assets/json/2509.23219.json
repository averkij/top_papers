{
    "paper_title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning",
    "authors": [
        "Xin Li",
        "Mengbing Liu",
        "Yiyang Zhu",
        "Wenhe Zhang",
        "Li Wei",
        "Jiancheng An",
        "Chau Yuen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at general mathematical reasoning but fail catastrophically on specialized technical mathematics. In wireless communications, where problems require precise manipulation of information-theoretic bounds, optimization constraints, and signal processing formulations, even state-of-the-art models struggle to achieve competent performance. We present WirelessMathLM, demonstrating that compact models (0.5B-7B parameters) can match or exceed much larger models through domain-specific reinforcement learning with verifiable rewards. Our key insight is that wireless mathematics problems possess a unique property--verifiable correctness--that enables effective reinforcement learning without human feedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027 problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with binary verification rewards, we train models directly from base checkpoints without supervised warm-start. Our 7B model achieves 39.5% accuracy on WirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times fewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B +81%), with positive transfer to general mathematics benchmarks--our models gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and AIME without any training on these tasks."
        },
        {
            "title": "Start",
            "content": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning Xin Li, Mengbing Liu, Yiyang Zhu, Wenhe Zhang Li Wei, Jiancheng An, Chau Yuen"
        },
        {
            "title": "Nanyang Technological University",
            "content": "Abstract Large language models (LLMs) excel at general mathematical reasoning but fail catastrophically on specialized technical mathematics. In wireless communications, where problems require precise manipulation of information-theoretic bounds, optimization constraints, and signal processing formulations, even state-of-the-art models struggle to achieve competent performance. We present WirelessMathLM, demonstrating that compact models (0.5B7B parameters) can match or exceed much larger models through domain-specific reinforcement learning with verifiable rewards. Our key insight is that wireless mathematics problems possess unique propertyverifiable correctnessthat enables effective reinforcement learning without human feedback. We construct WirelessMathBench-XL, comprehensive benchmark of 4,027 problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with binary verification rewards, we train models directly from base checkpoints without supervised warm-start. Our 7B model achieves 39.5% accuracy on WirelessMathBench-XL, approaching GPT-4o (40.4%) while using 100 fewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training nearly doubles performance across all model scales (0.5B: +11%, 3B: +103%, 7B: +81%), with positive transfer to general mathematics benchmarksour models gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and AIME without any training on these tasks. Project Homepage: https://lixin.ai/WirelessMathLM Date: September 30, 2025 5 2 0 2 7 2 ] . [ 1 9 1 2 3 2 . 9 0 5 2 : r Figure 1 WirelessMathLM achieves competitive performance through domain-specific GRPO training. (a) Our 7B model (39.5%) approaches GPT-4o (40.4%) on WirelessMathBench-XL while using far fewer parameters than top performers DeepSeek-R1 and GPT-5 (>57%). (b) GRPO training from base models yields dramatic gains: doubling performance for 3B (+103%) and near-doubling for 7B (+81%), showing that verifiable rewards enable efficient domain specialization. 1 3 4"
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 WirelessMathBench-XL: Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Collection Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 2.2 Mathematical Content Extraction and Problem Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Quality Assurance Framework Dataset Statistics and Analysis 2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Teaching Mathematical Reasoning with GRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Direct GRPO for Mathematical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Verification-Based Reward System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiments 4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Main Results on WirelessMathBench-XL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Generalization to General Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3 4 4 5 5 6 6 6 7 7 7 7 9 9 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Dataset Construction Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Paper Collection Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 16 Quality Assessment Rubric For Human . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Large Language Model-Assisted Quality Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Quality Assessment Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Real LLM Annotation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 18 18 18 Prompt Construction for Dataset Generation and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 System Model Extraction Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 D.1 20 D.2 Question Generation Prompt D.3 Quality Assessment Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 23 Standardized Evaluation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Representative System Model Extractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Example 1: Digital Twin-Assisted SIM-Based Air-Ground Communication . . . . . . . . . . . . . . . . . . . . . . . . . 24 E.1 26 Example 2: Multi-UAV Patrol Inspection with Mobile Edge Computing . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 E.3 28 Example 3: RIS-Aided Unsourced Random Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Model Extraction Quality Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 30 Human Expert Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 Score 5 - Excellent Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Score 4 - Good Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Score 3 - Acceptable Quality 35 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Score 2 - Poor Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Score 1 - Very Poor Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 F.1 F.2 F.3 F.4 F.5 G.1 High-Quality Solution Examples Representative Solution Examples from WirelessMathLM-7B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 G.1.1 Multiple Choice Question: Matrix All-Pass Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 Fill-in-the-Blank (100%): Cell-Free Massive MIMO Beamforming . . . . . . . . . . . . . . . . . . . . . . . . . 42 G.1.2 43 Fill-in-the-Blank (50%): Gaussian Function Components G.1.3 Error Analysis Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 G.2.1 Mathematical Equivalence Error 45 G.2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 G.2.3 MCQ Selection Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Conceptual Misunderstanding Error G."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) demonstrate remarkable general reasoning capabilities [1, 8, 10, 14], yet they fail catastrophically when confronted with specialized technical mathematics [9, 16, 24, 26]. This limitation is particularly acute in wireless communications, where problems demand rigorous handling of convex optimization constraints, information-theoretic bounds, and complex-valued matrix algebra [24]. Consider determining optimal beamforming for multi-user wideband MIMO systems under power and interference constraintsa routine task in 5G/6G design that requires coordinating multiple mathematical frameworks. The core challenge lies in fundamental tension: achieving expert-level performance in specialized domains typically requires either massive scale or extensive domain-specific supervision, yet wireless systems demand computational efficiency and lack large-scale annotated datasets. While recent work has successfully adapted LLMs to specialized fields like medicine [31] and biology [4], these approaches rely on either abundant training data or expensive human feedbackresources that remain scarce in wireless communications. The recent WirelessMathBench [24] highlighted this gap with only 587 problems from 40 papers, far below the scale needed for robust model training. We present WirelessMathLM, which resolves this tension through key insight: technical mathematics possesses an inherent structureverifiable correctnessthat can substitute for both scale and supervision. Unlike open-ended tasks where quality assessment requires human judgment, wireless problems have deterministic correct answers that can be automatically verified. We exploit this property through Group Relative Policy Optimization (GRPO) [30], training compact models (0.5B7B parameters) directly from base checkpoints using only binary verification signals. As Figure 1 demonstrates, 7B model trained with GRPO achieves 39.5% accuracy on wireless mathematics, approaching GPT-4o (40.4%) while using 100 fewer parameters than DeepSeek-R1 (671B, 57.4%). The improvements are consistent across scalesour 3B model doubles its accuracy (+103%), demonstrating that verification-based learning provides strong gradients even from sparse initial success. Most surprisingly, specialized training enhances rather than degrades general capabilities: our models gain an average of 8.4 points on standard mathematics benchmarks (MATH [17], Minerva-Math [22], OlympiadBench [16]) without any explicit training on these tasks. To enable this approach, we construct WirelessMathBench-XL, creating 4,027 problems from 970 papers. Our three-tier problem designmultiple-choice for concept recognition, progressive fill-in-the-blank with 25%-75% masking for structured reasoning, and full equation completion for comprehensive masteryprovides both training signal and fine-grained evaluation. Each problem includes complete variable definitions and context, enabling automated verification of student responses while the dataset construction itself employs rigorous dual-layer quality assurance combining automated screening with expert validation. Our contributions are threefold: We demonstrate that verification alone enables efficient domain specialization. GRPO training from base models, without supervised warm-start or human feedback, consistently improves performance across all model scales (0.5B: +11%, 3B: +103%, 7B: +81%). This challenges the assumption that reinforcement learning requires extensive pre-training. We show that specialized training develops transferable mathematical reasoning. The consistent gains on general benchmarks contradict conventional wisdom about catastrophic forgetting, suggesting that learning domain-specific mathematics strengthens fundamental capabilities. We provide infrastructure for reproducible research. WirelessMathBench-XL, our trained models, and the GRPO training framework are publicly released to accelerate development of efficient, specialized AI for technical domains."
        },
        {
            "title": "2 WirelessMathBench-XL: Dataset Construction",
            "content": "Creating high-quality benchmark for wireless communication mathematics requires addressing three key challenges: (1) extracting structured mathematical content from dense technical papers, (2) ensuring problem 3 Figure 2 Overview of the WirelessMathBench-XL construction pipeline. Starting from 47,000 arXiv papers, GPT-4o filtering identifies 970 papers with substantial mathematical content. DeepSeek-R1 extracts 10-25 formulas per paper, generating multiple-choice questions, progressive fill-in-the-blank (25%-75% masking), and full equation completion problems. Quality assurance employs dual-layer screening: automated GPT-4o evaluation followed by expert validation, with 78% of questions meeting the quality threshold (score 3/5). correctness and solvability, and (3) maintaining consistency across diverse mathematical formulations. We present systematic pipeline that construct WirelessMathBench-XL from 970 papers, yielding 4,027 problems. Figure 2 illustrates our three-stage pipeline for constructing WirelessMathBench-XL from raw arXiv papers to validated mathematical questions."
        },
        {
            "title": "2.1 Data Collection Pipeline",
            "content": "We developed an automated pipeline that comprehensively collects and processes wireless communication papers from arXiv. Our approach prioritizes broad coverage with sophisticated filtering rather than narrow targeting. Paper Collection and Filtering. We query 24 arXiv categories spanning core wireless domains (cs.NI, eess.SP, cs.IT), AI/ML (cs.LG, stat.ML), and interdisciplinary areas. Our crawler initially retrieves 47,000 papers from 2005-2025 using broad keyword queries across communication, signal processing, and networking terms. Each paper receives an automated relevance score based on keyword presence and category alignment. We then apply GPT-4-based filtering to identify 3,186 papers containing substantial mathematical content, from which we select the top 1,000 based on mathematical rigor, citation impact, and topical diversity. Full implementation details are provided in Appendix A."
        },
        {
            "title": "2.2 Mathematical Content Extraction and Problem Generation",
            "content": "Structured Model Extraction. We employ DeepSeek-R1 [14] to extract mathematical models from each papers LaTeX source. Our extraction preserves complete context including system equations, variable definitions with units and domain restrictions, underlying assumptions, and boundary conditions. Each paper yields structured summary with properly formatted mathematical notations (e.g., for vectors, CN for complex matrices). Appendix presents three representative examples of extracted system models demonstrating the comprehensiveness of our approach across different wireless domains: SIM-based airground communication, UAV-MEC systems, and RIS-aided random access. Automated Problem Generation. From extracted models, we generate three types of exam-style questions using carefully designed prompt templates (see Appendix for complete specifications): Multiple Choice Questions (MCQ): Equations are presented with masked right-hand sides, accompanied by 4 four carefully designed options. Distractors reflect common errors such as matrix dimension mismatches or incorrect operator sequences. Progressive Fill-in-the-Blank (Fill-in): Four difficulty levels with 25%, 50%, and 75% of equation components masked, testing incremental understanding. Full Equation Completion (FEC): Complete 100% masking requiring full equation recall"
        },
        {
            "title": "2.3 Quality Assurance Framework",
            "content": "Automated Evaluation. Each generated question undergoes systematic evaluation by GPT-4o across four critical dimensions: mathematical correctness, variable completeness, answer verifiability, and pedagogical value. The evaluation employs comprehensive 5-point quality rubric, which categorizes problems as invalid (score 1), poor (score 2), acceptable (score 3), good (score 4), or excellent (score 5). This automated screening utilizes specialized prompt templates described in Appendix to ensure consistent evaluation criteria across all question types. Expert Validation. Questions passing automated evaluation proceed to human expert review conducted by team of six domain specialists comprising four PhD students and two postdoctoral researchers with expertise spanning optimization theory, information theory, signal processing, and network analysis. Each question undergoes independent evaluation by at least two experts who assess mathematical rigor, notational consistency, problem clarity, and relevance to wireless communications. Questions must achieve minimum consensus score of 3/5 to qualify for dataset inclusion. The final acceptance rate of 78% reflects our stringent quality standards. Detailed scoring criteria and representative examples across all quality levels are provided in Appendices and F."
        },
        {
            "title": "2.4 Dataset Statistics and Analysis",
            "content": "The WirelessMathBench-XL dataset comprises 4,027 problems derived from 970 papers, providing comprehensive coverage across wireless communications mathematics. Technical Coverage. Figure 3 shows the distribution of mathematical techniques across source papers. Deep learning dominates (259 papers, 14.0%), followed by convex optimization (206, 11.2%) and MIMO/Massive MIMO (192, 10.4%). The dataset balances established techniquesbeamforming (185), channel coding (115), federated learning (110)with emerging paradigms including RIS/IRS (156), semantic communications (75), and NOMA (54). This distribution ensures representation of both foundational mathematics and frontier research areas. Temporal Distribution. The dataset spans three technological generations: 3G/4G (2005-2018: 28 papers, 2.9%), 5G deployment (2019-2023: 317 papers, 32.7%), and 5G-Advanced/6G research (2024-2025: 625 papers, 64.4%). This temporal weighting toward recent work captures state-of-the-art techniques while maintaining theoretical foundations. Question Format. All problems follow standardized structure with complete variable definitions including type specifications (scalar/vector/matrix), domain constraints (e.g., HRIS CM ), and physical units. Mathematical notation remains uniform: boldface for vectors (v), bold capitals for matrices (H), and standard operators (diag, tr, ). Fill-in-the-blank questions implement progressive difficulty through systematic masking (25%, 50%, 75%, 100%). Quality Distribution. Expert evaluation reveals that 35.53% of questions achieve acceptable quality (score 3), 30.89% are rated good (score 4), and 11.08% reach excellence (score 5). Questions scoring below threshold (scores 1-2: 22.50%) undergo revision or exclusion. Dataset Split. The 4,027 problems partition into training (3,227, 80%) and test (800, 20%) sets with balanced representation. Training set: Fill-in-75% (900), FEC (751), Fill-in-50% (680), MCQ (551), Fill-in-25% (345). Test set maintains proportional distribution: 218, 191, 160, 133, and 98 problems respectively. 5 Figure 3 Distribution of the top 20 key techniques across the 970 source papers in WirelessMathBench-XL. Deep learning leads with 259 papers (14.0%), followed by convex optimization (206, 11.2%) and MIMO/Massive MIMO (192, 10.4%). The distribution spans from foundational techniques (beamforming, channel coding) to emerging paradigms (RIS/IRS, semantic communications, NOMA)"
        },
        {
            "title": "3 Teaching Mathematical Reasoning with GRPO",
            "content": "Teaching language models mathematical reasoning in specialized domains leverages unique property: unlike general dialogue, wireless mathematics problems have verifiable correctness criteria. We employ GRPO [30] to directly train models from their base state, using automated verification as reward signals without expensive human feedback or supervised warm-start."
        },
        {
            "title": "3.1 Direct GRPO for Mathematical Reasoning",
            "content": "Given base language model πθ and wireless mathematics problem x, we aim to learn policy that generates correct solutions = (s1, ..., sn, a) where si denotes reasoning steps and is the final answer. Following Shao et al. [30], we optimize directly using the GRPO objective: JGRPO(θ) = xP (X) i=1πθold {yi}G (x) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 min (cid:18) πθ(yix) πθold(yix) Ai, clip (cid:18) πθ(yix) πθold(yix) (cid:19) (cid:19)(cid:35) , 1 ϵ, 1 + ϵ Ai (1) where = 8 responses are sampled per problem, ϵ = 0.2 for clipping, and the group-wise advantage is computed as: Ai = ri mean({rj}G std({rj}G j=1) j=1) (2) This formulation provides learning signal even when success rates are low, as the model learns from relative comparisons within each problem group rather than absolute rewards."
        },
        {
            "title": "3.2 Verification-Based Reward System",
            "content": "Our reward system leverages the structured nature of wireless mathematics through multi-level verification: 6 r(x, y) = α rformat(y) + (1 α) raccuracy(x, y) (3) where α = 0.1 balances format compliance with correctness. Format Reward (rformat): Ensures outputs follow expected structure with proper LaTeX formatting and boxed{} final answers: rformat(y) = L[regex match(y, \".*boxed{.*}.*\")] (4) Accuracy Reward (raccuracy): Verifies correctness through hierarchical evaluation system: (1) Direct matching: For multiple-choice questions, extract and compare letter answers. (2) Symbolic verification: For fill-in-theblank problems, normalize expressions (removing spaces, mathbf, boldsymbol) and check equivalence."
        },
        {
            "title": "3.3 Implementation Details",
            "content": "We train WirelessMathLM models directly from Qwen2.5 base checkpoints (0.5B, 3B, 7B) [29] using GRPO without supervised warm-start. Training employs AdamW optimizer with learning rate 106, cosine annealing, and KL penalty β = 0.01. We train for 40 epochs (240 steps) with evaluation every 5 steps on the held-out test set. Generation uses temperature = 0.6 for validation and = 1.0 for training rollouts. Training utilizes 4 NVIDIA A6000 GPUs with training time scaling by model size: 0.5B (14 hours), 3B (40 hours), and 7B (61 hours). The reward function implements hierarchical verification combining format checking with answer validation as described in Section 3.2."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Baselines. We benchmark WirelessMathLM against comprehensive baselines spanning proprietary and opensource models. Proprietary models include GPT-5 [27], GPT-4o [18], Claude-4.0-Sonnet [2], Gemini-2.5-Flash, and Gemini-2.5-Pro [11], representing state-of-the-art commercial systems. For open-source comparisons, we evaluate against general-purpose models including DeepSeek-R1 (671B) [14], DeepSeek-V3.1 (671B) [7], Llama-3.3-70B-Instruct [13], and Qwen2.5-72B-Instruct [35], as well as math-specialized models such as Qwen2.5-Math-72B-Instruct [36] and DeepSeekMath-7B-RL [30]. To isolate the impact of GRPO training, we include ablations using the corresponding Qwen2.5 base models (0.5B, 3B, 7B) without reinforcement learning. Standardized Evaluation Protocol. To ensure fair comparison, all models receive identical prompts constructed from standardized templates (see Appendix for complete specifications). Each prompt includes comprehensive variable definitions, equation context, and explicit formatting instructions. For MCQs, models must select from four options and provide their answer in boxed{} format. Fill-in-the-blank problems demand all masked positions be correctly filledpartial solutions receive no credit. For complex expressions where simple matching fails, GPT-4.1-mini performs semantic equivalence checking under the same all-or-nothing criterion."
        },
        {
            "title": "4.2 Main Results on WirelessMathBench-XL",
            "content": "Table 1 presents comprehensive evaluation results on the WirelessMathBench-XL test set. GRPO enables competitive performance with dramatic parameter reduction. Our 7B WirelessMathLM trained with GRPO achieves 39.5% overall accuracy, approaching the performance of GPT-4o (40.4%) while using orders of magnitude fewer parameters. This result is particularly striking when compared against opensource math-specialized models: our approach outperforms both Qwen2.5-Math-7B-Instruct (21.6%) and DeepSeekMath-7B-RL (21.5%) by nearly 2, despite these models being explicitly trained for mathematical reasoning. The performance gain stems from our domain-specific training strategywhile general math models struggle with the specialized notation and problem structures in wireless communications, our targeted approach with verifiable rewards enables efficient learning of domain-specific patterns. 7 Table 1 Performance on WirelessMathBench-XL test set (800 problems). MCQ: Multiple Choice Questions, Fill-in: Fill-in-the-blank, FEC: Full Equation Completion. Best result per category in bold. Size MCQ (%) Fill-in (%) FEC (%) Overall (%)"
        },
        {
            "title": "Model",
            "content": "Proprietary Models GPT-5 GPT-5-mini GPT-5-nano GPT-4o o4-mini Claude-4.0-Sonnet Gemini-2.5-Flash Gemini-2.5-Pro Grok-4-Fast Open-Source General Models DeepSeek-R1 DeepSeek-V3.1 Llama-3.3-70B-Instruct Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct Gemma 3 27B Gemma 3 12B 671B 671B 70B 72B 7B 27B 12B Open-Source Math-Specialized Models 72B Qwen2.5-Math-72B-Instruct 7B Qwen2.5-Math-7B-Instruct 7B DeepSeekMath-7B-RL - - - - - - - - - 63.91 67.67 57.14 54.14 67.67 60.15 63.16 66.17 70. 65.41 66.17 54.14 51.88 39.1 42.11 36.84 63.20 53.99 37.82 43.62 49.56 56.30 56.09 50.42 56.33 60.50 58.85 38.03 35.50 21.85 30.04 21.43 41.36 40.31 30.37 24.61 40.31 42.93 43.46 36.65 40.33 43.98 45.03 28.27 32.46 26.18 27.75 21.99 60.15 42.11 43. 40.55 14.71 13.66 33.51 24.61 25.65 WirelessMathLM (Ours) Qwen2.5-7B-Base + GRPO Qwen2.5-3B-Base + GRPO Qwen2.5-0.5B-Base + GRPO 7B 7B 3B 3B 0.5B 0.5B 44.36 53.38 26.32 48.87 27.07 30.08 14.29 36.97 7.14 17.02 5.25 6.09 25.13 36.13 15.71 28.80 24.08 26. 57.87 53.00 39.25 40.37 50.38 53.75 54.25 49.75 54.89 57.37 56.87 38.37 37.50 25.75 31.50 24.12 42.13 21.62 21.50 21.88 39.50 12.37 25.12 13.38 14.87 GRPO training yields consistent improvements across all model scales. The impact of GRPO training is substantial and scale-dependent. The 7B model nearly doubles its performance, improving from 21.9% to 39.5% (+81% relative), reaching within 0.9 percentage points of GPT-4o (40.4%). The 3B model demonstrates the most dramatic gains, more than doubling its accuracy from 12.4% to 25.1% (+103% relative). Even at minimal scale, the 0.5B model improves from 13.4% to 14.9% (+11% relative), suggesting that our dataset enables effective learning regardless of model capacity. Performance patterns reveal task-specific strengths. Analyzing performance across question types reveals interesting patterns. All models perform best on multiple-choice questions (MCQ), where our 7B model achieves 53.4% accuracywithin striking distance of proprietary models like GPT-4o (54.1%) and approaching DeepSeek-R1 (65.4%). Performance on fill-in-the-blank questions shows the largest improvement from GRPO training (14.3% 37.0% for 7B), suggesting that the reinforcement learning particularly helps with partial equation completion. Full equation completion (FEC) remains challenging across all models, though our 7B models 36.1% accuracy is competitive with GPT-5-mini (40.3%) and exceeds many larger open models. Comparison with state-of-the-art reveals efficiency-performance trade-offs. While DeepSeek-R1 (671B) achieves the highest open-source performance at 57.4%, it requires 100 more parameters than our 7B model. The performance gap of 17.9 percentage points represents favorable trade-off for deployment scenariosour model achieves 69% of DeepSeek-R1s performance with just 1% of its parameters. Among proprietary models, only GPT-5 (57.9%) significantly outperforms our approach, while models like Claude-4.0-Sonnet (53.8%) 8 and Gemini-2.5-Flash (54.3%) show more modest advantages despite their substantially larger scale and computational requirements."
        },
        {
            "title": "4.3 Generalization to General Mathematics",
            "content": "Surprisingly, training on wireless-specific mathematics enhances general mathematical reasoning  (Table 2)  . Domain-specific training strengthens fundamental mathematical capabilities. Our GRPO-trained models show substantial improvements on general mathematics benchmarks without any explicit training on these tasks. The 7B model improves from 52.0% to 67.0% on MATH 500 [17] (+28.8% relative), while the 3B model gains even more dramatically (41.6% 58.2%, +39.9% relative). These improvements extend across diverse mathematical domains: Minerva-Math [21] sees modest but consistent gains (7B: 12.1% 14.3%), OlympiadBench [16] improves substantially (7B: 25.3% 30.2%), and AMC [23] performance increases significantly (7B: 27.7% 41.0%). Even on the challenging AIME24 [23], the 7B model doubles its performance (6.7% 13.3%). Table 2 Transfer learning effects on general mathematical reasoning benchmarks."
        },
        {
            "title": "Model",
            "content": "MATH 500 Minerva-Math OlympiadBench"
        },
        {
            "title": "AMC",
            "content": "AIME"
        },
        {
            "title": "Average",
            "content": "Qwen2.5-7B-Base 52.00 + GRPO 67.00 (GRPO vs Base) +15.00 Qwen2.5-3B-Base 41.60 + GRPO 58.20 (GRPO vs Base) +16.60 12.13 14.34 +2.21 5.88 9.93 +4. 25.33 30.22 +4.89 14.67 22.96 +8.29 24.77 6.67 27.71 40.96 33.17 13.33 +13.25 +6.66 +8.40 18.07 21.69 +3.62 0.00 0.00 0.00 16.04 22.56 +6."
        },
        {
            "title": "4.4 Qualitative Analysis",
            "content": "To understand the reasoning capabilities developed through GRPO training, we conducted comprehensive analysis of 800 solutions generated by WirelessMathLM-7B on WirelessMathBench-XL test problems spanning all quality levels (see Appendix for representative examples). Mathematical Reasoning Structure and Coherence. Our analysis reveals that WirelessMathLM-7B produces systematically structured solutions consistently. Across all evaluated problems, 99.1% of responses demonstrate clear step-by-step reasoning using logical connectives such as therefore, thus, and hence. The model exhibits great problem decomposition strategies. In complex scenarios involving multiple mathematical frameworkssuch as MIMO beamforming under power constraintssolutions systematically establish physical principles before proceeding to mathematical derivations. For instance, when solving channel capacity problems, the model correctly identifies Shannons theorem applicability, establishes signal-to-noise ratio calculations, and methodically applies logarithmic transformations while maintaining dimensional consistency. Domain-Specific Knowledge Integration. Analysis of correct solutions demonstrates strong competency in applying wireless-specific mathematical frameworks. Among correct responses, 87% properly identify the underlying problem type and select appropriate methodologies. This suggests successful integration of procedural knowledge (solution techniques) with conceptual understanding (physical principles). Consider the models approach to Cell-Free Massive MIMO conjugate beamforming problem (Question ID 18369). The solution correctly identifies that conjugate beamforming requires complex conjugation of estimated channel coefficients, explains the physical rationale (cancel out phase shifts introduced by the channel), and derives the complete transmitted signal expression: sm = (cid:112) Pm (cid:88) k=1 ηmk ˆg mkuk (5) The response demonstrates understanding of power scaling, summation over users, and proper complex conjugationall domain-specific requirements absent in general mathematical training. 9 Solution Quality Indicators and Mathematical Sophistication. Several qualitative indicators demonstrate that domain-specific GRPO training has developed genuine mathematical reasoning rather than pattern matching: (1)Constraint Awareness: The model consistently recognizes and applies physical constraints without explicit prompting. Solutions automatically incorporate non-negativity constraints for power allocations, maintain causality in signal processing derivations, and respect dimensionality requirements in matrix operations. (2)Method Justification: Correct solutions routinely include explicit rationales for chosen approaches. For example, in matrix all-pass filter factorization problem (Question ID 11325), the model explains: matrix all-pass filter is filter whose frequency response has magnitude of 1 for all frequencies... before deriving the G(z) = N(z)D1(z) factorization and verifying the all-pass property through G(z)G1(z) = Im. (3)Physical Intuition Integration: Solutions frequently connect mathematical expressions to underlying physical phenomena. When deriving XOR operations for backscattered data processing, the model explains the commutative and associative properties of XOR before applying them to wireless tag data recovery."
        },
        {
            "title": "5 Related Work",
            "content": "Mathematical Reasoning in LLMs. Chain-of-thought prompting [34] demonstrated that eliciting step-by-step reasoning significantly improves mathematical problem-solving in large language models. This was extended through process supervision [25], where models receive feedback on intermediate steps rather than just final answers, and tool-augmented approaches like ToRA [12] that integrate external computation for complex calculations. While these advances have been evaluated on benchmarks ranging from elementary word problems (GSM8K [6]) to competition mathematics (MATH [17]) and formal theorem proving (MiniF2F [37]), such benchmarks do not capture the symbolic manipulation and domain knowledge required in technical fields. Domain Adaptation. Continued pre-training on domain-specific corpora [15] and instruction tuning [5] have proven effective for adapting language models to specialized fields. Scientific models like Galactica [32] attempted broad scientific reasoning, while BioBERT [20] and MedPaLM [31] achieved strong performance in biomedicine. Despite the mathematical intensity of wireless communications and its importance in 5G/6G systems, no prior work has developed specialized models for this domain. Reinforcement Learning from Verifiable Rewards. While RLHF [28] successfully aligns language models with human preferences, it requires expensive annotation that limits scalability. Recent alternatives include Constitutional AI [3] using principle-based self-critique, RLAIF [19] leveraging model-generated feedback, and GRPO [30] using outcome-based rewards for mathematics."
        },
        {
            "title": "6 Conclusion",
            "content": "We demonstrated that verification-based reinforcement learning enables efficient domain specialization without massive scale or extensive supervision. Our key findingthat direct GRPO training from base models yields dramatic improvements (up to 103% for our 3B model) while enhancing rather than degrading general mathematical capabilitieschallenges fundamental assumptions about both reinforcement learning prerequisites and catastrophic forgetting in domain adaptation. The success of WirelessMathLM, achieving near-GPT-4o performance with only 7B parameters, suggests that technical domains possessing verifiable correctness criteria constitute distinct class of problems where compact, specialized models can match or exceed much larger general-purpose systems. This principle extends beyond wireless communications to any field with formal verificationcircuit design, control theory, cryptographywhere our approach of exploiting domain structure through binary verification rewards could replace expensive annotation or massive scale. By releasing WirelessMathBench-XL, our trained models, and the training codes, we provide concrete tools for the research community to explore this efficiency-through-verification paradigm, potentially transforming how specialized AI systems are developed for technical domains where correctness is paramount and computational resources are constrained."
        },
        {
            "title": "Ethics Statement",
            "content": "We adhere to the ICLR Code of Ethics. This work focuses on advancing the mathematical reasoning capabilities of language models in the specialized domain of wireless communications. The WirelessMathBench-XL dataset was constructed from publicly accessible academic papers on arXiv, respecting the norms of scientific dissemination. Our data collection process did not involve human subjects or personally identifiable information. The expert validation phase was conducted by graduate students and postdoctoral researchers as part of their standard research activities. While any powerful AI technology carries potential for misuse, our work is foundational and does not present immediate dual-use concerns. We acknowledge that our dataset, being derived from existing literature, may reflect the inherent biases present in the field. We encourage responsible use of our models and dataset, and we are committed to addressing any ethical concerns that may arise."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To facilitate reproducibility of our work, we provide comprehensive details of our experimental methodology and make key resources publicly available. The complete WirelessMathBench-XL dataset containing 4,027 problems and evaluation results from all tested models is currently accessible at https://lixin.ai/ WirelessMathLM. Our dataset construction pipeline is thoroughly documented in Section 2, with detailed prompt templates, quality rubrics, and extraction procedures provided in Appendices through E. The GRPO training methodology is fully specified in Section 3, including the complete mathematical formulation of our reward system (Equations 1-4), hyperparameter settings, and implementation details. Our experimental protocol described in Section 4 provides exact evaluation procedures, model configurations, and standardized prompt templates used for all baseline comparisons. The appendices contain extensive documentation including representative problem examples across all quality levels (Appendix F), detailed solution analyses from our models (Appendix G), and comprehensive error taxonomies that enable understanding of model behavior. Upon paper acceptance, we will release the complete codebase including the GRPO training framework, all model checkpoints (0.5B, 3B, and 7B parameters), and evaluation scripts to ensure full reproducibility of our results. All experiments were conducted on NVIDIA A6000 GPUs with computational requirements documented in Section 3, enabling researchers to estimate resources needed for replication."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. Claude 4 sonnet. https://www.anthropic.com/claude/sonnet, 2025. [3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback. ArXiv, abs/2212.08073, 2022. URL https://api.semanticscholar.org/CorpusID:254823489. [4] Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. Meditron-70b: Scaling medical pretraining for large language models. ArXiv, abs/2311.16079, 2023. URL https://api.semanticscholar.org/CorpusID:265456229. [5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416. [6] Karl Cobbe, Vineet Kosaraju, Mo Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL https://api.semanticscholar.org/CorpusID:239998651. [7] DeepSeek-AI. Deepseek-v3.1 model introduction. https://www.deepseek.com/, 2025. Accessed: 2025-09-21. [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [9] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. Advances in neural information processing systems, 36, 2024. [10] Google DeepMind. Introducing gemini 2.0: our new ai model for the agentic era, 2024. URL https://blog.google/ technology/google-deepmind/google-gemini-ai-update-december-2024/. Accessed: 2024-12-11. [11] Google DeepMind. Gemini 2.5 models. https://deepmind.google/technologies/gemini/, 2025. [12] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. ArXiv, abs/2309.17452, 2023. URL https://api.semanticscholar.org/CorpusID:263310365. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, and et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Dont stop pretraining: Adapt language models to domains and tasks. ArXiv, abs/2004.10964, 2020. URL https://api.semanticscholar.org/CorpusID:216080466. [16] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.211/. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID: 261493811. [20] Jinhyuk Lee, WonJin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36:1234 1240, 2019. URL https://api.semanticscholar.org/CorpusID:59291975. [21] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. 12 [22] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7. [23] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. [24] Xin Li, Mengbing Liu, Li Wei, Jiancheng An, Merouane Debbah, and Chau Yuen. WirelessMathBench: Mathematical In Findings of the Association for Computational Modeling Benchmark for LLMs in Wireless Communications. Linguistics: ACL 2025, 2025. [25] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=v8L0pN6EOi. [26] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [27] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025. [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [29] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. Nature, 620:172 180, 2022. URL https://api.semanticscholar.org/CorpusID:255124952. [32] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S. Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: large language model for science. ArXiv, abs/2211.09085, 2022. URL https://api.semanticscholar.org/CorpusID:253553203. [33] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. Mineru: An open-source solution for precise document content extraction, 2024. URL https://arxiv.org/abs/2409.18839. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [35] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [36] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [37] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: cross-system benchmark for formal olympiad-level mathematics. ArXiv, abs/2109.00110, 2021. URL https://api.semanticscholar.org/CorpusID:237372712."
        },
        {
            "title": "Use of Large Language Models",
            "content": "In accordance with the ICLR 2026 policy on Large Language Model (LLM) usage, we disclose that LLMs were utilized as tools in various stages of this research project. The final responsibility for all content, including its accuracy and originality, rests with the human authors. Writing and Editing: LLMs were used to assist with improving the grammar, clarity, and style of the manuscript. The authors reviewed and edited all LLM-generated text to ensure it accurately reflects our research and findings. Literature Discovery: LLMs were employed to help summarize related work and accelerate the literature discovery process, assisting in identifying relevant prior research in mathematical reasoning and domain adaptation. Dataset Curation Pipeline: As detailed in Section C, LLMs were integral to the construction of the WirelessMathBench-XL dataset. Specifically: Paper Filtering: GPT-4o was used to perform an initial filtering of 47,000 papers to identify those with substantial mathematical content relevant to wireless communications. Content Extraction: DeepSeek-R1 was used to extract structured mathematical models from the LaTeX source of selected papers. Automated Quality Assessment: GPT-4o were used as part of multi-tier quality assurance framework to perform initial automated evaluations of generated questions. Evaluation: For our evaluation metric, GPT-4.1-mini was used to perform semantic equivalence checking on complex mathematical expressions where simple string matching was insufficient. In all instances, LLM outputs were critically reviewed, validated, and verified by the authors. We take full responsibility for the claims, results, and conclusions presented in this paper."
        },
        {
            "title": "A Dataset Construction Details",
            "content": "A.1 Detailed Paper Collection Methodology Multi-Category Coverage. We query across 24 arXiv categories to capture interdisciplinary research: Core categories: cs.NI (Networking), eess.SP (Signal Processing), cs.IT (Information Theory) AI/ML categories: cs.LG, stat.ML, cs.AI for learning-based approaches Systems categories: cs.SY, cs.DC, cs.MA for distributed and multi-agent systems Physics categories: physics.optics, quant-ph for emerging physical layer techniques Mathematical categories: math.OC, math.IT for optimization and theory Query Construction Strategy. We implement four complementary query strategies: queries = [ {name: basic_communication_terms, keywords: [communication, network, wireless, radio, signal, antenna, frequency, spectrum, transmission]}, {name: system_algorithm_terms, keywords: [system, algorithm, optimization, performance, model, framework, architecture]}, {name: application_computing_terms, keywords: [computing, sensing, iot, edge, cloud, distributed, energy, security]}, {name: data_intelligence_terms, keywords: [learning, intelligence, neural, prediction, detection, processing, estimation]} ] Relevance Scoring and Annotation. For each paper, we calculate: Relevance Score (0-1): Weighted sum of keyword presence in title (0.6 weight) and abstract (0.3 weight), plus category bonuses (eess.SP: 0.4, cs.NI: 0.35, cs.IT: 0.3) Technology Focus: Detected across 8 categories (wireless basic, advanced wireless, next gen, emerging - tech, signal processing, network protocol, ai ml, iot apps) Quality Tier: Based on relevance score (high: 0.7, medium: 0.4-0.7, low: 0.1-0.4) Research Type: Classified as survey, algorithmic, analytical, experimental, or theoretical PDF Processing. Papers undergo full-text processing using: MinerU [33] for PDF-to-markdown conversion preserving LaTeX equations Batch processing of 3-5 PDFs concurrently ( 40 seconds per paper) Rate-limited arXiv bulk access API with 3-second delays"
        },
        {
            "title": "B Quality Assessment Rubric For Human",
            "content": "Table 3 Detailed expert question quality assessment rubric"
        },
        {
            "title": "Score",
            "content": "1 - Invalid 2 - Poor"
        },
        {
            "title": "Criteria",
            "content": "Problem statement or solution is clearly wrong or contradictory; Not related to wireless communications domain; Cannot be used as valid question Statement correct but problem too trivial (answerable instantly); Problem too vague or nearly impossible to answer correctly; Very little learning or evaluation value 3 - Acceptable Statement and solution reasonable with no major errors; Difficulty and relevance are average; Can be kept but adds limited value (baseline quality) 4 - Good 5 - Excellent Clear and well-structured problem; Relevant to domain and moderately challenging; Provides meaningful assessment of understanding; Worth keeping and recommending Highly relevant to the domain; Strong depth, creativity, or insight required; Excellent for differentiating levels of understanding; Strongly recommended for inclusion 17 Large Language Model-Assisted Quality Assessment This section presents our comprehensive approach to leveraging large language models (LLMs) for scalable quality assessment of mathematical questions in wireless communications. Our methodology addresses the fundamental challenge of maintaining expert-level evaluation standards while achieving the scale necessary for large dataset curation. Role in the Overall Annotation Pipeline: This LLM-assisted quality assessment serves as the first filtering stage in our comprehensive annotation pipeline for our method. The complete pipeline consists of two sequential stages: (1) LLM-based filtering using our enhanced prompt system to automatically identify and remove low-quality questions, reducing the workload for human annotators; (2) Expert human annotation where domain experts review the filtered questions and provide detailed quality assessments; C.1 Quality Assessment Framework Our quality assessment framework employs systematic approach to evaluate technical questions across six dimensions: 1. Question Clarity (1-5): Measures the clarity and unambiguousness of the question statement 2. Background Relevance (1-5): Evaluates the completeness and relevance of provided context 3. Answer Accuracy (1-5): Assesses the correctness and formatting of the provided answer 4. Technical Appropriateness (1-5): Determines if the difficulty level matches the target audience 5. Mathematical Rigor (1-5): Evaluates mathematical notation and conventions 6. Wireless Communication Relevance (1-5): Measures domain relevance to wireless communications C.2 Real LLM Annotation Examples To demonstrate the practical effectiveness of our LLM-assisted quality assessment system, we present three representative examples from our evaluation dataset, showcasing different quality levels and the corresponding LLM assessments. Quality Level High Quality Medium Quality Low Quality Table 4 LLM Annotation Examples Across Quality Levels Question Content LLM Assessment Background: Federated fine-tuning system with low-rank adaptation matrices Ak Rdr, Bk Rrd Question: Which term completes: + [M ASK]? Options: A) AkBk B) BkAk Human: 4/ Background: H-NOMA system with variable definitions partially provided Question: Fill in [MASK] for the equation Human: 2/5 Background: Transformer model context with incomplete variable definitions Question: What replaces the full key matrix? Human: 1/5 LLM Score: 4/5 Strengths: Clear structure, complete context, accurate answer, rigorous notation Weaknesses: Could benefit from brief explanation of low-rank adaptation significance Agreement: LLM Score: 3/5 Strengths: Wireless relevance, accurate answer Weaknesses: Ambiguous [MASK] usage, lacks clarity in instructions Technical Issues: Missing variable definitions Disagreement: LLM too optimistic LLM Score: 3/5 Strengths: Clear structure, accurate answer Weaknesses: Limited wireless relevance, focuses more on tensor parallelism Technical Issues: Full key matrix not defined Bias: LLM shows optimistic scoring pattern"
        },
        {
            "title": "D Prompt Construction for Dataset Generation and Evaluation",
            "content": "We employ specialized prompt templates for dataset construction, quality assessment, and standardized evaluation to ensure consistency and fairness across all stages of our methodology. D.1 System Model Extraction Prompt The following prompt template guides the extraction of mathematical models from research papers:"
        },
        {
            "title": "System Model Extraction Template",
            "content": "Task: Act as an expert in wireless communications and mathematical modeling. Extract and summarize the mathematical system modeling from the paper. <<STRUCTURE REQUIREMENTS>> 1. **Model Extraction**: a) Identify ALL system equations with context b) For each equation: i) List ALL variables with units/dimensions ii) Specify underlying assumptions iii) Note domain restrictions 2. **Summary Organization**: paragraph{Background} (2-3 sentences contextualizing the model) paragraph{Key Assumptions} (bullet points with $bullet$) paragraph{Parameter Definitions} (table-like structure) paragraph{Core Equations} (numbered with original labels) 3. **Equation Formatting**: - Vectors: boldsymbol{v} - Matrices: mathbf{M} - Operators: mathrm{diag}, mathrm{tr} - Complex numbers: for imaginary unit <<CONTENT GUIDELINES>> - Variable Explanations: - For each symbol: $theta$ (Type: Phase shift; Domain: [0,2 $pi$); Unit: rad) - Matrix dimensions: $mathbf{H} in mathbb{C}^{Ntimes M}$ - Distinguish similar symbols: $h_{ij}$ vs $h_{i}^{(j)}$ - Model Validation: - Verify dimensional consistency - Check boundary conditions - Confirm parameter unit homogeneity 19 D.2 Question Generation Prompt The following template generates exam-style questions from extracted models:"
        },
        {
            "title": "Question Generation Template",
            "content": "Task: Generate exam-style questions from research paper summaries. <<STRUCTURE REQUIREMENTS>> 1. **Per Equation Processing**: a) Identify ALL system model equations b) For EACH equation: i) Mask the RHS with [MASK] ii) Generate 1 MCQ with 4 plausible options iii) Create 4 progressive fill-in-the-blank subquestions: - 25%, 50%, 75%, and 100% key symbols masked 2. **Question Components**: - For MCQs: * Background: MUST include detailed variable definitions Format: \"where $boldsymbol{x}$ is the transmitted signal vector, $mathbf{H} in mathbb{C}^{N times M}$ represents the channel matrix...\" * Equation: Masked equation in display math mode * Question: Explicitly ask to replace [MASK] * Options: 4 LaTeX-formatted choices (A)-(D) * Answer: Detailed derivation walkthrough <<ENHANCED BACKGROUND REQUIREMENTS>> - Variable Definition Format: - Start with system context: \"In this [type of system]...\" - List EVERY symbol that appears in the equation - Include matrix/vector dimensions - Specify units where applicable: \"(in watts)\", \"(in Hz)\" - Explain subscripts and superscripts - Distractor Design: 1) Matrix dimension mismatches 2) Incorrect operator sequences 3) Missing diag() operators 4) Channel matrix transposition errors 5) Incorrect matrix multiplication order - Masking Strategy: - 25%: Single critical variable - 50%: Two interdependent terms - 75%: Multiple components - 100%: Full equation recall 20 D.3 Quality Assessment Framework To ensure consistent quality evaluation across the dataset, we employ comprehensive assessment framework with few-shot learning enhancement. This framework guides both automated LLM evaluation and human expert review. Quality Assessment Prompt with Few-Shot Learning You are an expert evaluator specializing in wireless communication and mathematics education. Your task is to assess the quality of technical questions designed for advanced undergraduate and graduate students in wireless communications. ## EVALUATION METHODOLOGY Follow this systematic approach: ### STEP 1: Initial Question Analysis - Read the question, background, equation, and answer carefully - Identify the technical domain and complexity level - Check for obvious errors or inconsistencies ### STEP 2: Multi-Dimensional Quality Assessment Evaluate each dimension on 1-5 scale: 1. Question Clarity (1-5): Crystal clear vs confusing/incomprehensible 2. Background Relevance (1-5): Comprehensive context vs inadequate background 3. Answer Accuracy (1-5): Completely correct vs incorrect/flawed 4. Technical Appropriateness (1-5): Perfect difficulty vs inappropriate level 5. Mathematical Rigor (1-5): Excellent notation vs poor rigor 6. Wireless Relevance (1-5): Highly relevant vs not relevant ## HUMAN EXPERT EXAMPLES Learn from these actual expert evaluations: Example 1 - Score: 1 (Very Poor) Question: \"Which expression correctly calculates the sensitivity metric?\" Human Feedback: \"The definition of TN is not given\" Missing variable definitions make question unsolvable Example 2 - Score: 3 (Acceptable) Question: \"Which performance metric should replace [MASK]?\" Human Feedback: \"Some variables in choices are not given\" Minor gaps but workable with assumptions Example 3 - Score: 5 (Excellent) Question: Complete differential privacy equation with full context Human Feedback: \"Well-structured with complete information\" Ready for immediate use ## CRITICAL EVALUATION GUIDELINES Be especially strict about: - Missing Variable Definitions: Any undefined variablesScore$leq$ 2 - Incomplete Context: Key background missing Score $leq$ 2 - Vague Problem Statements: Ambiguous questions Score $leq$ 3 - Technical Accuracy: Mathematical/technical errors Score $leq$ 2 ## OUTPUT FORMAT Provide assessment in JSON: { \"overall_score\": [1-5 integer], \"dimension_scores\": { \"question_clarity\": [1-5], 21 \"background_relevance\": [1-5], \"answer_accuracy\": [1-5], \"technical_appropriateness\": [1-5], \"mathematical_rigor\": [1-5], \"wireless_relevance\": [1-5] }, \"binary_flags\": { \"is_correct\": [true/false], \"is_wireless_related\": [true/false] }, \"quality_analysis\": { \"strengths\": [\"Key strengths\"], \"weaknesses\": [\"Areas for improvement\"], \"specific_improvements\": [\"Detailed suggestions\"] } } Question Type: {question_type} Question Text: {question_text} Background: {background} Equation: {equation} Options: {options} Correct Answer: {correct_answer} 22 D.4 Standardized Evaluation Prompts To ensure reproducible evaluation, all models receive identical prompts constructed from the following templates: MCQ Evaluation Template:"
        },
        {
            "title": "MCQ Evaluation Template",
            "content": "**Background** [Complete variable definitions and system context] **Question** [Question text] **Equation** [Equation with [MASK] placeholder] **Options** A: [Option A] B: [Option B] C: [Option C] D: [Option D] --- Please analyze this problem step by step. Show your reasoning and calculations. Your final answer should be given at the end in the format: boxed{X} where is the letter of the correct option. Fill-in-the-Blank Evaluation Template: Fill-in-the-Blank Evaluation Template **Background** [Complete variable definitions and system context] **Question** [Question text] **Equation** [Equation with [MASK] placeholder(s)] --- Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). For single mask: Your final answer should be given at the end in the format: boxed{your_answer} For multiple masks: Your final answers should be given at the end in the format: boxed{answer1}, boxed{answer2}, ... (for the blanks in order)"
        },
        {
            "title": "E Representative System Model Extractions",
            "content": "This section presents three representative examples of system models extracted by DeepSeek-R1 from research papers in our corpus. These examples demonstrate the diversity and complexity of mathematical formulations captured in WirelessMathBench-XL. E.1 Example 1: Digital Twin-Assisted SIM-Based Air-Ground Communication This model integrates multi-layer stacked intelligent metasurface (SIM) beamforming with eVTOL trajectory optimization, representing the convergence of aerial communications and reconfigurable surface technologies. SIM-Based Air-Ground Communication System (Type: Duration of time slot; Domain: R+; Unit: s) (Type: ATCo station position; Domain: R31; Unit: m) The air-ground channel between the SIM and each eVTOL follows Rician fading model. Background This paper proposes Digital Twin (DT)-assisted framework for joint optimization of Stacked Intelligent Metasurface (SIM)-based air-ground communication and electric Vertical Take-off and Landing (eVTOL) flight control within prescribed air corridors. The system model integrates multi-layer SIM beamforming structure at the Air Traffic Control (ATCo) station with composite potential field method for eVTOL trajectory planning, aiming to maximize the sum transmission rate while ensuring safe navigation. Key Assumptions Each meta-atom on the SIM imposes an ideal, configurable phase shift without amplitude attenuation. The transmission matrices Wl between metasurface layers are modeled based on Rayleigh-Sommerfeld diffraction theory, assuming perfect knowledge of the SIMs physical structure. eVTOLs fly within predefined, non-overlapping air corridor Rcor. The signals for different eVTOLs are independent and identically distributed (i.i.d.) with zero mean and unit variance. The additive receiver noise is independent, circularly symmetric complex Gaussian (AWGN). Parameter Definitions = [xAT , yAT , 0]T (Type: Number of eVTOLs / ATCo antennas; Domain: Z+; Unit: None) (Type: Number of metasurface layers in SIM; Domain: Z+; Unit: None) (Type: Number of meta-atoms per metasurface layer; Domain: Z+; Unit: None) (Type: Number of discrete time slots; Domain: Z+; Unit: None) δ qm[n] = [xeV OL Vmax PAT pm[n] θl k[n] Ψl[n] = diag(ejθl Wl w1 λ dx, dy hH m[n] ρ0 αh κh sm[n] E{sm[n]2} = 1, i.i.d.) Core Equations 1) SIM Beamforming Matrix. The end-to-end beamforming matrix G[n] of the L-layer SIM is given by the product of the transmission and phase shift matrices across all layers. [n], zeV OL (Type: Maximum eVTOL velocity; Domain: R+; Unit: m/s) (Type: Total available transmission power at ATCo; Domain: R+; Unit: W) (Type: Transmission power allocated to eVTOL at time n; Domain: R+; Unit: W) (Type: Phase shift of meta-atom on layer at time n; Domain: [0, 2π); Unit: rad) (Type: Reference path loss at 1m; Domain: R+; Unit: None (often in dB)) (Type: Path loss exponent; Domain: R2; Unit: None) (Type: Rician factor; Domain: R+; Unit: dB) σ2 (Type: Size of meta-atom along and axes; Domain: R+; Unit: m) (Type: Channel vector from last SIM layer to eVTOL at time n; Domain: C1K ; Unit: None) (Type: Transmission vector from ATCo antenna to first metasurface layer; Domain: CK1; Unit: None) (Type: Transmission data symbol for eVTOL at time n; Domain: C; Unit: None; Assumption: E{sm[n]} = 0, (Type: Transmission matrix between layers 1 and l; Domain: CKK ; Unit: None) (Type: Receiver noise power at eVTOL m; Domain: R+; Unit: W) (Type: Phase shift matrix for layer at time n; Domain: CKK ; Unit: None) (Type: 3D position of eVTOL at time n; Domain: Rcor R3; Unit: m) (Type: Carrier wavelength; Domain: R+; Unit: m) [n], yeV OL 1[n], ..., ejθl [n]) [n]]T 2) Inter-layer Transmission Matrix Entry. The (k, k)-th entry of the transmission matrix Wl is derived from Rayleigh-Sommerfeld diffraction theory. G[n] = ΨL[n]WLΨL1[n] Ψ2[n]W2Ψ1[n] CKK wl k,k = dxdy cos χl dl k,k k,k (cid:32) 1 2πdl k,k (cid:33) 1 λ j2πdl k,k /λ k,k is the distance between meta-atoms, and χl where dl the layer. 3) Air-Ground Channel Model. The channel from the SIM to eVTOL is modeled as Rician fading channel. The k-th entry is: k,k is the angle between the propagation direction and the normal to hm,k[n] = (cid:115) (cid:115) ρ0 (dm[n])αh κh κh + 1 hm[n] 24 where dm[n] = qm[n] is the distance from the ATCo station to the eVTOL, and hm[n] = 1 is assumed for the LoS component. 4) Received Signal. The composite signal received by eVTOL at time slot is: ym[n] = hH m[n]G[n] (cid:88) m=1 w1 (cid:112)pm [n]sm [n] + τ where τ CN (0, σ2 5) Signal-to-Interference-plus-Noise Ratio (SINR). The SINR for eVTOL at time is: m) is the complex AWGN. SINRm[n] = hH m[n]G[n]w m2pm[n] (cid:80)M m=1,m=m hH [n]G[n]w1 2pm [n] + σ2 6) Achievable Data Rate. The achievable data rate for eVTOL at time is given by the Shannon capacity formula: Rm[n] = log (1 + SINRm[n]) 7) Joint Optimization Problem (P1). The overall problem is formulated to maximize the sum rate over all eVTOLs and time slots by jointly optimizing power allocation P, phase shifts Ψ, and trajectories Q. (P1) : max P,Ψ,Q g(P, Ψ, Q) = (cid:88) (cid:88) n=1 m=1 Rm[n] s.t. C1 : (cid:88) m=1 pm[n] PAT , k[n] [0, 2π), n, k, C2 : pm[n] 0, N, C3 : θl C4 : qm[n] qm[n 1] Vmaxδ, n, C5 : qm[n] Rcor, n, C6 : qm[0] = fm[0], qm[N ] = fm[N ], 8) Composite Potential Field (CPF) Force. The flight control acceleration for eVTOL is derived from the negative gradient of the combined potential fields. ai[n] = (cid:0)F com [n] + sep [n] + tar [n](cid:1) The individual fields (target tar, separation sep, communication com) are functions of the eVTOLs state and hyperparameters {ktar, ksep, kcom} which are optimized via DQN framework. 25 E.2 Example 2: Multi-UAV Patrol Inspection with Mobile Edge Computing This system model captures the complexity of joint communication, computation, and trajectory optimization in UAV-enabled MEC networks. UAV-MEC System Model Background This paper considers multi-UAV patrol inspection system where UAVs traverse predetermined cruise points to collect data and offload it to Ground Base Stations (GBSs) equipped with Mobile Edge Computing (MEC) servers for processing. The system model jointly optimizes cruise point assignment, communication scheduling, computational allocation, and UAV trajectory to minimize total energy consumption and balance task completion times among UAVs. Key Assumptions GBSs are deployed with sufficient density to ensure continuous cellular coverage. TDMA scheme is used for UAV-GBS communication. The communication rate model incorporates logistic function of the elevation angle, based on empirical measurements. The information causality constraint must be satisfied (processed data received data). UAV dynamics follow rotary-wing energy consumption model. The CPU cycles required per bit (CU ) are known and depend on the task type. UAVs fly at constant altitude HU . Parameter Definitions = {u1, ..., uN } Set of UAVs = {g1, ..., gM } Set of GBSs = {s1, ..., sK } Set of cruise points wsk R21 Coordinates of cruise point sk (m) wgm R21 Coordinates of GBS gm (m) HU , HG Altitude of UAV and GBS, respectively (m) η(t) R21 UAVs horizontal position at time (m) v(t) UAVs velocity vector at time (m/s); v(t) Vmax Qsk Data volume collected at cruise point sk (bits) Rgm (t) Real-time communication rate to GBS gm (bps) τgm (t) {0, 1} Binary scheduling indicator for GBS gm fU (t), fgm (t) CPU frequency of UAV and GBS gm, respectively (cycles/s) CU CPU cycles required per bit (cycles/bit) (t) UAV transmission power (W) Ti Task completion time for i-th UAV (s) ϑU UAVs effective capacitance coefficient (F) Core Equations 1) Distance and Elevation Angle. The distance between the UAV and GBS gm at time is: The corresponding elevation angle is: dgm (t) = (cid:113) (HU HG)2 + η(t) wgm 2 θgm (t) 180 π arctan (cid:18) HU HG (cid:19) η(t) wgm Assumptions: LOS propagation is dominant. UAV and GBS altitudes are constant. Domain: θgm (t) (0, 90], dgm (t) > 0. 2) Communication Rate Model. The real-time communication rate is given by: Rgm (t) = (cid:18) χ3 + χ4 1 + e(χ1+χ2θgm (t)) (cid:19) log (cid:18) 1 + (cid:19) ˆγP (t) (dgm (t))α Variables/Constants: χ1, χ2, χ3, χ4 are environment-dependent parameters (χ1 < 0, χ2 > 0, χ4 > 0, χ3 + χ4 = 1). is the bandwidth (Hz). ˆγ = β0/(σ2Λ) is the normalized SNR, where β0 is the reference channel gain (dB), σ2 is the noise power (W), and Λ is the SNR gap. α is the path-loss exponent. Assumptions: The model accounts for the practical dependence of antenna gain on the elevation angle. Domain: Rgm (t) 0. 26 3) Information Causal Constraint. The data processed by GBS cannot exceed the data received from the UAV: (cid:90) TP 0 fgm (t) CU dt (cid:90) TP 0 τgm (t)Rgm (t)dt, TP [0, Ti] Assumptions: No data buffering at the GBS beyond what is received. Domain: TP 0. 4) Energy Consumption Models. The total energy for the i-th UAV, Ei, is the sum of computation energy (Ec), transmission energy (Et), and flight energy (Ef ). (cid:88) Ki (cid:88) (cid:90) Tsk k=1 τgm (t)P (t)dt Et = Ec = Ef = m=1 (cid:90) Ti (cid:90) Ti 0 ϑU 3 (t)dt P0 (cid:32) 1 + 3v(t)2 2 tip (cid:33) (cid:32)(cid:115) + Pi 1 + v(t)4 4v4 0 v(t)2 2v2 0 (cid:33)1/2 + 1 2 d0ρsˆav(t)3 dt Ei = Ec + Et + Ef Variables/Constants: P0, Pi are blade profile and induced power in hover (W). Utip is rotor tip speed (m/s). v0 is mean rotor induced velocity in hover (m/s). d0 is fuselage drag ratio. ρ is air density (kg/m³). is rotor solidity. ˆa is rotor disc area (m²). Assumptions: Rotary-wing UAV dynamics. DVFS is used for computation. Domain: Et, Ec, Ef , Ei 0. 5) Original Optimization Problem (P0). The joint optimization problem is formulated as: (P0) : min {π(k)},{η(t)}, {τgm (t)},tsπ(k) ,Ti,Ki (cid:88) i=1 (Ei + ϕTi + λ(Ti Tavg)) s.t. τgm (t) {0, 1}, (cid:88) m= τgm (t) 1 (9a) Information causal constraint (4) Data processing demand: UAV + GBSs must process all collected data Qsπ(k) Trajectory constraints: Start at sI , visit all points in π, end at sF Velocity constraint: v(t) Vmax Variables/Constants: ϕ, λ are compensation factors to balance the dimensions of energy and time in the objective. Assumptions: The problem is decomposed into two tractable subproblems: Task Assignment and Path Planning. Domain: The problem is non-convex and requires decomposition for solution. 27 E.3 Example 3: RIS-Aided Unsourced Random Access RIS-Aided URA System The paper proposes RIS-aided unsourced random access (URA) system where massive number of users Background communicate with base station (BS) via reconfigurable intelligent surface (RIS). The direct user-BS links are assumed completely blocked, making the RIS essential for connectivity. The system employs slotted transmission structure with joint pilot detection, channel estimation, and RIS phase shift optimization to enable reliable communication. Key Assumptions Quasi-static block fading channels (constant over frame) Perfect knowledge of RIS-BS channel (stationary elements) Passive RIS with unit-modulus phase shifts: [wt]i = 1 Blocked direct user-BS links (no direct path) Saleh-Valenzuela channel model for RIS-BS and user-RIS links UPA antenna arrays at both BS and RIS (RIS phase shift vector at time t; Type: Control; Domain: [wt]i = 1) Parameter Definitions CM (RIS-BS channel matrix; Type: Geometric; Unit: dimensionless) hi C1N (User-RIS channel vector for user i; Type: Geometric; Unit: dimensionless) wt CN 1 xi,t (Transmitted symbol from user at time t; Type: Information; Unit: dimensionless) zt CM 1 (Noise vector; Type: AWGN; Distribution: CN (0, σ2 (Number of BS antennas; Type: Integer; Unit: dimensionless) (Number of RIS elements; Type: Integer; Unit: dimensionless) Ka (Number of active users; Type: Integer; Unit: dimensionless) (Total channel uses; Type: Integer; Unit: dimensionless) LG (Number of paths in RIS-BS channel; Type: Integer; Unit: dimensionless) LR,i (Number of paths in user-RIS channel; Type: Integer; Unit: dimensionless) IM )) Core Equations 1) Received Signal Model (Eq. 4): yt = Ka (cid:88) i=1 Gdiag(hi)wtxi,t + zt, = 1, . . . , Variables: yt CM 1 (received signal), CM , hi C1N , wt CN 1, xi,t C, zt CM 1 Assumptions: Blocked direct links, passive RIS, quasi-static channels Domain: [wt]i = 1, {1, . . . , n} 2) Pilot Phase Received Signal (Eq. 5): Yp = (cid:112)Pp Gdiag(hi)Wps diag(pi) + Zp (cid:88) iSs Variables: Yp CM np , Wps CN np , pi C1np , Zp CM np Assumptions: Fixed RIS configuration during pilot phase Domain: [Wps ]i,j = 1 3) Data Phase Received Signal (Eq. 6): Yc,f = (cid:112) Pc (cid:88) iSs Gdiag(hi)Wcs diag(bi)vi,f + Zc,f Variables: Yc,f CM ns , Wcs CN ns , bi C1ns , vi,f {1} Assumptions: Two RIS configurations C0 (constant) and C1 (varying) vi,f {1} Domain: [Wcs ]i,j = 1, 28 4) Channel Model - RIS-BS (Eq. 1): = LG (cid:88) l=1 µlaM (ϕr,l, ψr,l)T aN (ϕt,l, ψt,l) Variables: µl CN (0, L0dαP Assumptions: Saleh-Valenzuela model, UPA arrays Domain: ϕr,l, ψr,l [0, 2π), ϕt,l, ψt,l [0, 2π) ), aM (), aN () (steering vectors) 5) Channel Model - User-RIS (Eq. 3): hi = LR,i (cid:88) fi= µfi aN (ϕi,fi , ψi,fi ) Variables: µfi CN (0, L0dαP Assumptions: Same path loss model as RIS-BS channel Domain: ϕi,fi , ψi,fi [0, 2π) fi ) 6) Steering Vector Model (Eq. 2): aN (ϕ, ψ) = 1 ej2π ϕn1 ej2π ψn2 Variables: ϕ = sin(ϕ) cos(ψ), ψ = sin(ψ) n1 = λ [0, . . . , N1 1], n2 = λ [0, . . . , N2 1] Assumptions: UPA structure with antenna spacing = λ/2 Domain: ϕ, ψ [0, 2π) Model Validation Dimensional consistency: All matrix multiplications are dimensionally consistent (e.g., CM multiplied by diag(hi) CN yields CM matrix) Boundary conditions: Unit-modulus constraint [wt]i = 1 enforced for passive RIS Parameter homogeneity: All channel gains µl, µfi have consistent units (dimensionless with path loss scaling) Physical constraints: Angle parameters restricted to [0, 2π), array steering vectors properly normalized E.4 Model Extraction Quality Assessment These extracted models demonstrate several quality indicators that validate our automated extraction pipeline: Completeness: Each model includes comprehensive variable definitions with proper units and domains, ensuring self-contained mathematical descriptions suitable for question generation. Mathematical Rigor: The extractions preserve complex mathematical relationships including multi-layer matrix products, integral constraints, and summation indices, maintaining the precision required for technical education. Domain Coverage: The three examples span classical communication theory (Shannon capacity), modern optimization frameworks (joint resource allocation), and emerging technologies (RIS, SIM), reflecting the breadth of WirelessMathBench-XL. Hierarchical Structure: Models successfully capture equation dependencies, from basic distance calculations to complex optimization objectives, enabling progressive question difficulty design."
        },
        {
            "title": "F Human Expert Evaluation Examples",
            "content": "This section presents representative examples from our expert evaluation process, demonstrating the application of our quality rubric across different score levels. Each example includes the complete question as presented to evaluators, with expert annotations highlighting strengths and weaknesses. F.1 Score 5 - Excellent Quality Questions scoring 5 demonstrate comprehensive variable definitions, clear mathematical structure, and strong pedagogical value. These questions are ready for immediate use in educational or evaluation contexts. Question ID: 14024 Paper: 2508.03740v1 Answer: LV = sg[F] 2 + α sg[C]2 2 + βDKL (pcpu)"
        },
        {
            "title": "Background",
            "content": "In the vector quantization training process, composite loss function ensures proper codebook learning and feature quantization. The loss consists of three components: codebook loss, commitment loss, and usage regularization, where RM is the semantic feature matrix, RN is the codebook matrix, sg[] denotes the stop-gradient operator, DKL() is the Kullback-Leibler divergence, pc is the codeword usage distribution, pu is the uniform distribution, and α, β R+ are hyperparameters that weight the different loss components."
        },
        {
            "title": "Question",
            "content": "Write the complete vector quantization loss function with all three components."
        },
        {
            "title": "Equation",
            "content": "Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answer should be given at the end in the format: your answer [M ASK] 30 Question ID: 14134 Paper: 2206.08306v1 Answer: (cid:0)m dv dt + 1 2 ρairAf CDv2 + mgr0 cos(α) + mg sin(α)(cid:1) v/ηt + Paccessories ηe"
        },
        {
            "title": "Background",
            "content": "mf is the fuel The instantaneous fuel rate model calculates the mass flow rate of fuel consumed. Here, rate (in kg/s), is the vehicle mass (in kg), dv dt is the acceleration (in m/s²), ρair is the air density (in kg/m³), Af is the frontal area (in m²), CD is the drag coefficient (dimensionless), is the speed (in m/s), is gravitational acceleration (in m/s²), r0 is the rolling resistance coefficient (dimensionless), α is the road grade (in radians), ηt is the transmission efficiency (dimensionless), Paccessories is the power consumed by vehicle accessories (in W), and ηe is the engine efficiency (dimensionless)."
        },
        {
            "title": "Question",
            "content": "Write the complete equation for the instantaneous fuel rate."
        },
        {
            "title": "Equation",
            "content": "mf = [M ASK] Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answer should be given at the end in the format: your answer 31 Question ID: 4149 Paper: 2505.19983v"
        },
        {
            "title": "Background",
            "content": "Answer: PxWxx + PzWzz + Wnn In wireless semantic communication system with interference, the received real-valued signal after equalization combines the desired signal, an interference signal, and noise. The system model is derived from the complex baseband representation, where R2k is the equalized received real signal vector, R2k is the real-valued semantic feature vector to be transmitted, R2k is the real-valued interference vector, (0, σ2 2 I2k) is the real-valued additive white Gaussian noise vector, Px R+ is the desired signal transmit power (in linear scale), Pz R+ is the interference signal transmit power (in linear scale), Wx R2k2k is the channel transformation matrix for the desired signal, Wz R2k2k is the channel transformation matrix for the interference signal, and Wn R2k2k is the channel transformation matrix for the noise."
        },
        {
            "title": "Question",
            "content": "Write the complete received signal equation including all three components."
        },
        {
            "title": "Equation",
            "content": "y = [M ASK] Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answer should be given at the end in the format: your answer 32 F.2 Score 4 - Good Quality Questions scoring 4 contain solid technical content with minor areas for improvement, typically in completeness of context or clarity of problem statement. Question ID: Paper: 2208.11967v"
        },
        {
            "title": "Background",
            "content": "Answer: arctan (cid:19) (cid:18) hi In laser-powered UAV-assisted wireless network, the probability of having Line-of-Sight (LOS) link is crucial for signal propagation. This model characterizes the LOS probability between an aerial or terrestrial node and user, where Pi(r) is the probability of an LOS link for node type (where {Lu, Lb} representing LOS UAV and LOS TBS links, respectively), is the horizontal distance between the transmitter and receiver (in meters), hi is the altitude or height of the node type (in meters), and a, b, are environment-dependent parameters (dimensionless) that model the blockage characteristics in urban, suburban, or dense urban environments."
        },
        {
            "title": "Question",
            "content": "What trigonometric function of the elevation angle is the argument of the exponential?"
        },
        {
            "title": "Equation",
            "content": "Pi(r) = exp (b[M ASK]) + Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answer should be given at the end in the format: your answer 33 Question ID: 4439 Paper: 2506.01400v"
        },
        {
            "title": "Background",
            "content": "Answer: (cid:20) B(1 + νk,i) µ ln 2 N0,k + Ik,i λk,i (cid:21)+ The optimal power allocation for communication UEs in multi-user MIMO system is derived using the Karush-Kuhn-Tucker (KKT) conditions to solve the constrained optimization problem. This solution follows water-filling structure. Here, PC,k,i is the optimal power allocated to the i-th sub-channel of communication UE (in W), []+ = max(0, ) ensures non-negative power, is the bandwidth (in Hz), νk,i is the Lagrange multiplier associated with the minimum capacity constraint for the i-th sub-channel of UE (dimensionless), µ is the Lagrange multiplier associated with the total power constraint (in W1), N0,k is the noise power at UE (in W), Ik,i is the interference power (in W), and λk,i is the channel gain eigenvalue (dimensionless)."
        },
        {
            "title": "Question",
            "content": "Write the complete optimal power allocation formula for communication user equipment (UE)."
        },
        {
            "title": "Equation",
            "content": "PC,k,i = [M ASK] Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answer should be given at the end in the format: your answer 34 F.3 Score 3 - Acceptable Quality Questions scoring 3 meet minimum requirements but have noticeable gaps in clarity or completeness that limit their educational value. Question ID: Paper: 2208.07045v"
        },
        {
            "title": "Background",
            "content": "Answer: In an interference-coupled multi-cell RAN slicing system, the Signal-to-Interference-plus-Noise Ratio (SINR) is calculated at specific user location. The SINR determines the quality of the wireless link for user served by particular channel in slice, where γs,q(l, s,q) is the SINR at location for channel in slice (dimensionless), SI s,q(l) is the received signal power at location from the base station transmitting on channel in slice (in watts), Ns,q is the set of all slice-channel pairs that can potentially interfere with (s, q) (dimensionless), s,q is binary vector indicating which interfering transmitters in Ns,q are active (dimensionless), IN (s,q),(s,q)(l) is the interference power at location from an interfering transmitter on channel in slice (in watts), and N0 is the noise power (in watts)."
        },
        {
            "title": "Question",
            "content": "Which expression correctly represents the SINR calculation that should replace [MASK]?"
        },
        {
            "title": "Options",
            "content": "A: (cid:88) (s,q)Ns,q (s,q) γs,q(l, s,q) = [M ASK]"
        },
        {
            "title": "P SI",
            "content": "s,q(l) s,q(s, q)P IN (s,q),(s,q)(l) + N0 B: (cid:88) (s,q)Ns,q C: (cid:88) s,q(l) SI s,q(s, q)P IN (s,q),(s,q)(l) + N0 SI s,q(l) IN (s,q),(s,q)(l) + N0 (s,q)Ns,q(s,q) (cid:88) (s,q)Ns,q(s,q) s,q(s, q)P IN (s,q),(s,q)(l)"
        },
        {
            "title": "P SI",
            "content": "s,q(l) + N0 D: Your final answer should be given at the end in the format: 35 Question ID: 4275 Paper: 2504.18155v"
        },
        {
            "title": "Background",
            "content": "Answer: In the hierarchical cell-free massive MIMO uplink training phase, edge access points (eAPs) receive pilot sequences from multiple users. The received pilot signal matrix at eAP combines contributions from all users through their respective channels, where Ψl CNaτp represents the received pilot signal matrix at eAP l, pu is the user transmit power constraint (in watts), = {1, . . . , K} is the set of user indices, hkl CNa1 is the channel vector from user to eAP l, ik Cτp1 is the pilot sequence of user (dimensionless), Zl CNaτp is the additive noise matrix with entries CN (0, σ2 ), Na is the number of antennas per eAP, and τp is the pilot sequence length (in symbols)."
        },
        {
            "title": "Question",
            "content": "Which expression correctly represents the received pilot signal matrix at eAP l?"
        },
        {
            "title": "Options",
            "content": "Ψl = [M ASK] A: pu pu pu pu (cid:80) (cid:80) (cid:80) (cid:80) kK hkliT kK hkliH kK hT kK hH + Zl + Zl klik + Zl klik + Zl B: C: D: Your final answer should be given at the end in the format: 36 F.4 Score 2 - Poor Quality Questions scoring 2 have significant deficiencies that impair their usefulness, though they may contain salvageable elements. Question ID: 13936 Paper: 2502.11053v"
        },
        {
            "title": "Background",
            "content": "Answer: ϕ In the belief propagation decoding of LDPC codes, messages are passed between nodes on the Tanner graph. For the check node update, L(rji) is the log-likelihood ratio (LLR) message sent from check node to bit node i. L(qij) is the LLR message received from connected bit node i. The set BNji contains all bit nodes connected to check node except bit node i."
        },
        {
            "title": "Question",
            "content": "Which function is applied to the absolute value of each incoming LLR before summation in the stable SPA update?"
        },
        {
            "title": "Equation",
            "content": "L(rji) = (cid:89) iBNji sign (L(qij)) ϕ (cid:88) [M ASK] iBNji Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answer should be given at the end in the format: your answer 37 Question ID: Paper: 2505.18534v"
        },
        {
            "title": "Background",
            "content": "Answer: I(t) , cos , sin In DSP-free coherent optical interconnect system using offset-QAM modulation, the received in-phase and quadrature signals are processed before carrier phase recovery. The system aims to compensate for phase error between the received signal and the local oscillator. Here, (t) represents the received in-phase signal after mixing and before phase correction (in volts or amperes), Q(t) is the corresponding quadrature signal (in volts or amperes), I(t) {AOM A/2} is the original modulated in-phase data signal (in volts or amperes), Q(t) {AOM A/2} is the original modulated quadrature data signal (in volts or amperes), A0 R+ is the constant DC offset introduced by the offset-QAM modulation format (in volts or amperes), and ϕ (π, π] is the phase error between the transmitter and local oscillator paths (in radians)."
        },
        {
            "title": "Question",
            "content": "Complete the three missing components: the data signal and the two trigonometric functions."
        },
        {
            "title": "Equation",
            "content": "I (t) = ([M ASK] + A0)[M ASK](ϕ) + (Q(t) + A0)[M ASK](ϕ) Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answers should be given at the end in the format: answer1 , answer2 , ... (for the 3 blanks in order) 38 F.5 Score 1 - Very Poor Quality Questions scoring 1 have fundamental errors or omissions that render them unusable without complete revision. Question ID: Paper: 2412.01187v"
        },
        {
            "title": "Background",
            "content": "Answer: log In point-to-point, interference-free multi-terminal wireless system with NU single-antenna users communicating over parallel links, the instantaneous achievable rate is modeled for each link. The rate for terminal is function of the channel state and the allocated power, where ri(pi(h), hi) represents the instantaneous achievable rate on link (in bps/Hz), pi(h) is the power allocated to terminal for given channel realization (in watts), hi is the fading channel coefficient for terminal (dimensionless), and σ2 is the noise variance on link (in watts). The system assumes AWGN channels and perfect Channel State Information (CSI)."
        },
        {
            "title": "Question",
            "content": "What is the outer function that transforms the SNR into rate?"
        },
        {
            "title": "Equation",
            "content": "ri(pi(h), hi) [M ASK] (cid:18) 1 + (cid:19) pi(h) h2 σ2 Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answer should be given at the end in the format: your answer 39 Question ID: 4264 Paper: 2504.21128v"
        },
        {
            "title": "Background",
            "content": "Answer: vec(H)2 = (cid:88) k=1 tHMS H:,k2 The HMA system must obey global power conservation constraint for the passive metasurface. This constraint links the HMS configuration to the communication channel. vec(H) CN K1 is the vectorized channel matrix, tHMS CN 1 is the vector of complex transmission coefficients for the HMS unit-cells, H:,k CN 1 is the k-th column of the channel matrix (representing the channel from user to all unit-cells), and denotes the Hadamard (element-wise) product."
        },
        {
            "title": "Question",
            "content": "Write the complete global power conservation constraint equation."
        },
        {
            "title": "Equation",
            "content": "[M ASK] = [M ASK] Please solve this problem step by step. Fill in the [MASK] placeholder(s) with the correct mathematical expression(s). Your final answers should be given at the end in the format: answer1 , answer2 , ... (for the 2 blanks in order) 40 Representative Solution Examples from WirelessMathLM-7B This appendix presents detailed examples of solutions generated by WirelessMathLM-7B, organized by question type and quality level. These examples illustrate the models reasoning patterns, mathematical sophistication, and common error modes identified in our qualitative analysis. G.1 High-Quality Solution Examples G.1.1 Multiple Choice Question: Matrix All-Pass Filter Question ID: 11325 Correct Answer: Model Response: WirelessMathLM-7B Solution To determine the correct factorization of the matrix all-pass filter G(z), we need to understand the properties and definition of matrix all-pass filter. matrix all-pass filter is filter whose frequency response has magnitude of 1 for all frequencies, but its phase response can vary. Mathematically, matrix all-pass filter can be represented as: where N(z) is the numerator matrix polynomial and D(z) is the denominator matrix polynomial. The matrix all-pass filter is defined such that: G(z) = N(z)D1(z) G(z)G1(z) = Im Given that D0 = Im, the filter is causal and proper. This means that the denominator polynomial D(z) is proper polynomial (its degree is less than or equal to 1). To verify that the given factorization is correct, we can check the inverse of G(z): G1(z) = (N(z)D1(z))1 = D(z)N1(z) This is because the inverse of product of matrices is the product of their inverses in reverse order. Now, we can check the product G(z)G1(z): G(z)G1(z) = (N(z)D1(z))(D(z)N1(z)) = N(z)D1(z)D(z)N1(z) = N(z)N1(z) = Im This confirms that G(z) is indeed an all-pass filter. Therefore, the correct factorization of the matrix all-pass filter G(z) is: Thus, the correct answer is: G(z) = N(z)D1(z) Analysis: This solution demonstrates sophisticated understanding of matrix theory and filter design. The model correctly identifies the all-pass property, provides mathematical verification through matrix inverse operations, and connects the factorization to causality constraints. The reasoning is systematic, mathematically rigorous, and includes proper justification for each step. 41 G.1.2 Fill-in-the-Blank (100%): Cell-Free Massive MIMO Beamforming Question ID: 18369 Correct Answer: (cid:112) Pm (cid:88) k=1 ηmk ˆg mkuk WirelessMathLM-7B Solution To derive the conjugate beamforming transmitted signal in Cell-Free Massive MIMO (CFmMIMO) system, we need to consider the linear combination of the users data symbols, precoded using the locally estimated channel state information. The signal transmitted from the m-th AP is given by: sm = (cid:88) kK ηmk ˆg mkuk where: - sm is the transmitted symbol from the m-th AP (dimensionless), - Pm R+ is the maximum transmit power of the m-th AP (in watts, W), - ηmk R+ is the power control coefficient for user at AP (dimensionless), - ˆgmk is the estimated channel coefficient from AP to user (dimensionless), - uk is the information symbol intended for user (dimensionless; E[uk2] = 1). The term ˆg mk represents the complex conjugate of the estimated channel coefficient from AP to user k. This is because conjugate beamforming is used to cancel out the phase shifts introduced by the channel. Therefore, the complete equation for the conjugate beamforming transmitted signal is: sm = (cid:88) kK ηmk ˆg mkuk So, the final answer is: (cid:88) kK ηmk ˆg mkuk Analysis: This solution exhibits deep understanding of MIMO systems, correctly identifying conjugate beamforming principles and providing clear physical intuition. The model properly handles complex notation, explains the rationale for complex conjugation, and maintains dimensional consistency throughout the derivation. 42 G.1.3 Fill-in-the-Blank (50%): Gaussian Function Components Question ID: 5582 Correct Answer: (λ λp)2 and λ2 WirelessMathLM-7B Solution (Truncated) To solve the problem, we need to understand the form of the Gaussian function and how it is modified to include the secondary skewed Gaussian component. The given Gaussian function is: g(λ, λp, λ) = exp (cid:20) (cid:21) [MASK] [MASK] The standard form of Gaussian function centered at λp with width parameter λ is: g(λ, λp, λ) = exp (cid:20) (cid:21) (λ λp)2 2(λ)2 However, the problem involves secondary skewed Gaussian component, which is typically represented by function that is asymmetric around the peak wavelength λp... Given the form of the Gaussian function, the [MASK] placeholder in the exponent should be filled with (λ λp)2. Therefore, the final answer is: (λ λp)2 , (λ)2 Analysis: The model correctly identifies standard Gaussian form and provides appropriate mathematical expressions. The reasoning demonstrates understanding of probability density functions and their parameters in optical communication contexts. G.2 Error Analysis Examples G.2.1 Mathematical Equivalence Error Question ID: 2406 Type: Mathematical Equivalence Failure Correct Answer: 1 Model Response: Error Example: Mathematical Equivalence To determine the exponent of the sinusoidal term that defines the patterns shape, we need to analyze the given equation for the normalized power radiation pattern of single IRS element: (Π) (cid:26) (sin θ cos φ)[M ASK], 0, θ [0, π], φ [ π 2 , π 2 ], otherwise. The equation suggests that the power radiation pattern is function of the angles θ (elevation angle) and φ (azimuth angle). The term sin θ cos φ is product of two sinusoidal functions, and the exponent of this product is what we need to identify. Given that the problem involves sinusoidal term, we can infer that the exponent is likely constant that determines the shape of the pattern. Since the problem does not provide any additional information about the specific shape or the value of the exponent, we can assume that the simplest form of the pattern is first-order sinusoidal function. Therefore, the exponent of the sinusoidal term is G, where is the peak power gain of single IRS element. This means that the power radiation pattern is proportional to the square of the sine of the elevation angle and the cosine of the azimuth angle, raised to the power of G. Thus, the exponent of the sinusoidal term is . Error Analysis: The model provides reasonable physical interpretation but fails to derive the precise mathematical relationship 2 1. This represents common error type where domain knowledge is correctly applied but mathematical transformation is incomplete. The model recognizes that relates to the exponent but doesnt perform the necessary algebraic manipulation. G.2.2 Conceptual Misunderstanding Error Question ID: 16144 Type: Conceptual Misunderstanding Correct Answer: (cid:88) Ki(cid:88) (cid:90) Tsk m=1 k=1 0 τgm(t)P (t)dt Error Example: Conceptual Misunderstanding To determine the total transmission energy Et for UAV, we need to consider the energy consumed by each transmission to each GBS. The energy consumed by transmission is given by the product of the transmission power and the time spent transmitting. Therefore, the total transmission energy can be calculated by summing up the energy consumed for all transmissions to all GBSs. The energy consumed by transmission from the i-th UAV to the m-th GBS is (t) Tsk τgm (t), where (t) is the transmission power, Tsk is the time spent at the k-th cruise point, and τgm(t) is the binary scheduling variable... Thus, the total transmission energy Et is given by: Et = (cid:88) Ki(cid:88) (cid:88) i= k=1 m=1 (t) Tsk τgm (t) The final answer is: (cid:88) Ki(cid:88) (cid:88) i=1 k=1 m=1 (t) Tsk τgm (t) Error Analysis: This error demonstrates correct energy calculation principles but incorrect mathematical formulation. The model understands that energy equals power times time but fails to recognize the need for temporal integration rather than discrete summation. The error reflects misunderstanding of continuous vs. discrete system modeling rather than fundamental energy concepts. 45 G.2.3 MCQ Selection Error Question ID: 16315 Type: Multiple Choice Selection Correct Answer: Model Response: Error Example: MCQ Selection (Truncated) To determine the correct term that completes the equation for the phase compensation factor βℓ, we need to understand the physical significance of the phase compensation term. The phase compensation term βℓ accounts for the path difference that is corrected for in the virtual array projection... The total phase difference ϕ is the sum of these two phase differences: ϕ = ϕz + ϕhorizontal = 2πdz ℓ λ + 2π(cid:112)(dx ℓ )2 + (dy λ ℓ )2 To correct for this phase difference, we need to add the phase compensation term βℓ: βℓ = 2dz ℓ + (dx ℓ )2 + (dy 2Z ℓ )2 Therefore, the correct term that completes the equation is: Error Analysis: This example shows mathematically sound reasoning leading to an incorrect final selection. The model provides correct physical interpretation and mathematical derivations but selects the wrong multiplechoice option. This suggests challenges in mapping derived expressions to provided answer choices rather than fundamental understanding failures."
        }
    ],
    "affiliations": []
}