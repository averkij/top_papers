{
    "paper_title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning",
    "authors": [
        "Yao Zhang",
        "Yu Wu",
        "Haowei Zhang",
        "Weiguo Li",
        "Haokun Chen",
        "Jingpei Wu",
        "Guohao Li",
        "Zhen Han",
        "Volker Tresp"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 4 9 4 1 . 0 1 5 2 : r GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning Yao Zhang 1,6 Yu Wu 2 Haowei Zhang 3 Weiguo Li 4 Haokun Chen1 Jingpei Wu1 Guohao Li Zhen Han 1 Volker Tresp1,6 1 LMU Munich 2 Technical University of Munich 3 Fudan University 4 University Heidelberg 5 University of Oxford 6 Munich Center for Machine Learning"
        },
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors throughout the reasoning process. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing precise, execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the bestperforming PRM trained with auto-labeled supervision. Nevertheless, it achieves up to 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering scalable and verifiable path toward high-quality process-level reasoning. Our code is publicly released at github.com/GroundedPRM."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [1, 9, 31] have demonstrated impressive capabilities in planning [13, 44], decision-making [19], and complex task execution [38, 45]. However, they remain prone to hallucinations and reasoning errors, particularly in multi-step tasks such as mathematical problem solving. Existing methods like Chain-of-Thought prompting [36, 40] and Test-Time Scaling [21, 27] improve final accuracy, yet LLMs often produce solutions that appear coherent while containing errors in reasoning or calculation. These issues are further exacerbated by outcome-level supervision Corresponding authors: yzhang@dbs.ifi.lmu.de, hanzhen02111@163.com Equal contribution Work done during an internship at Technical University of Munich Preprint. and coarse decoding strategies, e.g., majority voting, which overlook step-level correctness and provide little guidance during intermediate reasoning. To mitigate these shortcomings, Process Reward Models (PRMs) have emerged as promising direction [20]. PRMs assign step-level scores to reasoning trajectories, enabling fine-grained supervision that supports better control and interpretability in multi-step reasoning. However, developing effective PRMs remains challenging due to the lack of reliable and faithful reward signals for training. Human annotation [20], while accurate, is costly and unscalable. LLM-as-a-judge [48] is more efficient but susceptible to hallucination, often rewarding fluent yet incorrect reasoning and thus compromising factual fidelity. Monte Carlo (MC) estimation [22, 35] provides another alternative by inferring step quality from final rollout outcomes, but it introduces noisy reward due to credit misattribution: correct steps may be penalized if the rollout fails, while flawed steps may be rewarded if the final answer happens to be correct [46]. Moreover, MC estimation typically evaluates only final outcomes, ignoring explicit assessment of intermediate step correctness, which misaligns the supervision signal with the objective of step-wise reasoning accuracy. Several recent works have attempted to refine MC-based supervision, but core limitations persist. OmegaPRM [22] uses binary search strategy to locate the first incorrect step, but still relies on rollout success to infer correctness, leaving credit assignment coarse. Qwen2.5-Math-PRM [46] filters samples based on agreement between MC estimation and LLM judgments, but this strategy inherits hallucination bias and scores each step solely based on rollout outcomes, without assessing whether it contributes to or hinders correct reasoning. BiRM [7] augments PRM with value head to predict future success probability, but both reward and value signals are derived from noisy rollouts and lack external validation. These approaches offer partial improvements, yet remain constrained by outcome-based heuristics, hallucination-prone feedback, or weak step-level credit modeling. To address these challenges, we propose GroundedPRM, tree-guided and fidelity-aware framework for automatic process supervision. GroundedPRM is designed to resolve three core limitations in existing PRMs: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. First, to reduce reward noise and improve credit attribution, GroundedPRM leverages Monte Carlo Tree Search (MCTS) to construct structured reasoning paths and assess each step based on its contribution within the trajectory. Second, to ensure factual grounding, each intermediate step is verified using an external math tool, producing correctness signals based on executable logic rather than LLM-generated feedback, thereby eliminating hallucinated supervision. Third, to combine steplevel validation with global outcome assessment, we design hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, all rewards are formatted into binary decisions paired with rationale-enhanced justifications, enabling interpretable supervision signals that are compatible with LLM-based generation and downstream reasoning workflows. We evaluate GroundedPRM on ProcessBench and observe substantial gains in both data efficiency and overall performance. It is trained on only 40K automatically labeled samples, just 10% of the data used by the best-performing PRM trained with auto-labeled supervision, yet achieves up to 26% relative improvement in average performance. Furthermore, when deployed in reward-guided greedy search, where candidate steps are selected based on predicted reward, GroundedPRM surpasses even PRMs trained with human-labeled supervision, establishing new state-of-the-art results across multiple mathematical reasoning benchmarks. These findings highlight the effectiveness, scalability, and practical value of our structured and fidelity-aware supervision framework for both training and inference. The key contributions of this work are: 1. We propose GroundedPRM, tree-guided and fidelity-aware process reward modeling framework that leverages MCTS to construct structured reasoning paths and support steplevel credit assignment. 2. We introduce fidelity-aware verification mechanism that validates each reasoning step using an external math tool, ensuring correctness grounded in executable logic and eliminating hallucinated supervision. 3. We design hybrid reward aggregation mechanism that integrates tool-based step validation with feedback derived from MCTS-guided reasoning paths. 2 4. We format rewards into rationale-enhanced, generative structure to improve interpretability and enable seamless integration into inference-time decoding and downstream reasoning workflows. 5. We demonstrate strong data efficiency and inference performance by evaluating GroundedPRM on ProcessBench and reward-guided greedy search."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Mathematical Reasoning with LLMs Large Language Models (LLMs) have shown remarkable progress in solving math problems via Chainof-Thought (CoT) reasoning, where step-by-step solutions often improve final answer accuracy [36]. Building on this, recent efforts have focused on enhancing reasoning capabilities through pretraining on math-related corpora [15, 25, 42], instruction tuning with annotated derivations [19, 40, 41, 43], and prompting strategies tailored for math tasks [4, 14, 17]. Despite these improvements, LLMs remain vulnerable to intermediate reasoning errors, even when final answers are correct [47]. This discrepancy undermines the reliability of generated solutions, motivating the use of external verification or inference-time selection strategies [9, 26, 32]. Such approaches typically operate at the output level, offering limited supervision for correcting internal steps. Unlike prior methods that intervene at the output level, our approach supervises the reasoning process itself via step-level reward modeling, enabling finer-grained error identification and ensuring more faithful alignment with step-level reasoning and factual correctness. 2.2 Process Reward Models for Step-Level Supervision To enhance reasoning fidelity and identify intermediate errors, PRMs have emerged as promising alternative to outcome-level supervision [20, 33]. PRMs evaluate the correctness of individual reasoning steps and have been shown to improve alignment and generalization across math tasks [35, 46]. key challenge lies in generating reliable step-level annotations. Early methods rely on expert-labeled datasets such as PRM800K [20], which are expensive to scale. Recent work has explored automatic synthesis through MC estimation [22, 35], often leveraging rollout outcomes to infer step validity. However, MC-based supervision introduces noise due to credit misattribution and dependency on the quality of the completion model [46, 47]. To mitigate this, several methods combine MC with LLM-as-a-judge consensus filtering [46] or adopt preference-based learning frameworks [6]. In contrast, our method GroundedPRM constructs PRM supervision from the ground up by integrating tree-structured search via MCTS [5], step-level verification with external math engines, and fused valuecorrectness reward modeling. This pipeline produces reward signals that are verifiable, structurally grounded, and directly aligned with step-level reasoning objectives, effectively addressing the fidelity and alignment issues that prior methods leave unresolved."
        },
        {
            "title": "3 Methodology",
            "content": "GroundedPRM is designed to address three core limitations of existing process reward modeling methods: noisy rewards, low factual fidelity caused by hallucinated self-assessment, and misalignment with step-level reasoning objectives. These challenges call for framework that can assign finegrained credit, validate the factual correctness of individual steps, and integrate local and global signals into reliable and interpretable supervision objective. To this end, GroundedPRM introduces tree-guided and fidelity-aware reward modeling framework composed of four core components. First, it employs Monte Carlo Tree Search (MCTS) to construct structured reasoning paths and assess each step based on its contribution within the search trajectory, enabling more stable and attribution-aware supervision than flat sampling-based methods. Second, it verifies each intermediate step using an external tool, producing binary correctness labels grounded in executable logic and thereby mitigating hallucinated feedback from the model. Third, it unifies verified step-level signals and final-answer correctness into joint supervision objective, maintaining fine-grained credit assignment while offering stable and reasoning-grounded supervision. Finally, the reward supervision is formatted into rationale-enhanced generative structure, pairing each step with both binary score and an explanation to support interpretability and compatibility 3 Figure 1: Overview of the GroundedPRM framework. GroundedPRM constructs reasoning paths via MCTS, where each node corresponds to an LLM-generated step. During simulation, intermediate steps are verified using an external tool, and final answers are checked against ground truth. Step-level and outcome-level correctness signals are aggregated into rollout reward, which is backpropagated along the tree to update node statistics; the next node is then selected by UCT, continuing the MCTS search until convergence or budget exhaustion. The framework enables verifiable, interpretable, and structure-aware process supervision for multi-step reasoning. The generative rationale provides interpretable feedback for each step. with instruction-tuned LLMs. An overview of this framework is illustrated in Fig. 1. We provide the full algorithmic pseudocode in Appendix A. 3.1 Tree-Guided Reasoning Path Construction To enable stable and attribution-aware process supervision, GroundedPRM employs MCTS to construct structured reasoning paths for each input problem . Each node in the search tree is associated with partial reasoning state = {s1, . . . , si}, representing the sequence of previously generated reasoning steps. In addition to the state, each node stores auxiliary information including tool queries q, verification outcomes v, and value estimates Q. reasoning step is represented as an action a, defined as natural language expression generated by the LLM that extends the current reasoning state, transitioning it from state to new state s. The value function Q(s, a) estimates the expected return of applying action in state s, and is updated through feedback from simulated rollouts. The search process consists of four stages: Selection. Starting from the root node, the algorithm recursively selects child nodes according to tree policy until reaching node that is not fully expanded. To balance exploration and exploitation, we use the Upper Confidence Bound for Trees (UCT) [16], which balances estimated value with an exploration bonus that decreases as node is visited more often, thereby encouraging the search toward promising yet under-explored nodes. The UCT score for each candidate action at state is computed as: UCT(s, a) = Q(s, a) + (cid:115) log (s) (s, a) , (1) where (s) and (s, a) are the visit counts of the parent and child nodes, respectively; and is hyperparameter controlling the exploration strength. If the selected node is not terminal, it is expanded by sampling new actions from Expansion. LLM, each producing distinct child state s. We set = 3 in our experiments. This constrains the branching factor while maintaining reasoning diversity. Simulation. From the newly expanded node, we simulate complete reasoning trajectory by sequentially sampling steps si+1, . . . , sT until the model produces final answer. We sample from 4 the current state using the LLM in left-to-right fashion to complete the solution. For each step sj where {i + 1, ..., 1}, we obtain binary correctness label vj {1, 1} using the tool-based verification procedure described in 3.2. Additionally, the final answer is compared against the ground-truth solution to determine the overall outcome {1, 1}. We adopt signed labels {-1,+1} instead of {0,1} so that incorrect steps propagate negative feedback, thereby decreasing node values during MCTS rather than being treated as neutral. These per-step and final correctness signals are subsequently aggregated into single rollout reward ui, as defined in 3.3. Backpropagation. The reward computed for the simulated trajectory is propagated backward along the path traversed during selection. For each visited state-action pair (sk, ak) at depth dk from the terminal node, we update its value as: Q(sk, ak) Q(sk, ak) + γdk (ui + vi), (2) where {0, ..., 1}, γ (0, 1) is decay factor controlling temporal discount, and dk denotes the number of steps from node i. This update scheme assigns stronger credit to steps closer to the final outcome, aligning attribution with their causal impact in the reasoning process. By iteratively executing the four MCTS stages, GroundedPRM constructs structured and diverse distribution over reasoning paths. This search process prioritizes trajectories with high step-level validity and globally correct outcomes, yielding supervision signals that are both structure-aware and attribution-sensitive. The resulting credit assignments are more stable and fine-grained than those produced by flat Monte Carlo rollouts, directly addressing reward noise and misattribution. Multiple rollouts are performed per input to balance path diversity with search efficiency. 3.2 Fidelity-Aware Step Verification with External Tool To ensure reward fidelity and eliminate hallucinated supervision, GroundedPRM integrates step-level verification into each reasoning step via external tools. During simulation (see 3.1), the LLM generates sequence of reasoning steps {si+1, . . . , sT }, where each sj (i + 1 1) denotes an intermediate reasoning step expressed in natural language during rollout. For each step sj, we construct corresponding structured math query and submit it to an external math tool, such as Wolfram Alpha (WA) [37]. The tools response is parsed to determine whether the computation or transformation expressed in sj is factually correct. We represent this outcome as binary verification label vj {1, 1}, where vj = 1 indicates successful verification and vj = 1 denotes failure. The resulting sequence {vi+1, . . . , vT 1} provides fine-grained step-level correctness evaluation for the entire reasoning trace. These step-level signals are used during rollout to compute the aggregated reward (see 3.3). Unlike LLM-based self-evaluation, which often overestimates fluent but invalid reasoning, this fidelity-aware mechanism grounds supervision in objective, tool-based verification. While WA is used in our experiments due to its strong mathematical solving capabilities, such as equation solving and equivalence checking, our verification module is tool-agnostic. It supports integration with alternatives like SymPy [23] or domain-specific solvers. This modular design ensures that GroundedPRM generalizes across reasoning domains while maintaining high verification precision. 3.3 Hybrid Reward Aggregation To construct reward signals that are both verifiable and forward-looking, GroundedPRM introduces hybrid aggregation mechanism that combines step-level verification with trajectory-level outcome assessment. This design balances two supervision objectives: (1) factual fidelity of intermediate reasoning steps, and (2) global correctness of the final answer. Given simulated reasoning trace of length , we collect step-level correctness signals {vi+1, . . . , vT 1}, where each vi {1, 1} is obtained via external tool verification (see 3.2). In addition, we evaluate the final answer against ground truth to obtain binary outcome signal {1, 1}. These signals are aggregated into single scalar reward: dj vj + β F, (3) ui = 1 1 1 (cid:88) j=i+1 5 where β 0 is weighting coefficient that adjusts the contribution of final answer correctness relative to step-level reliability. The resulting reward is used during backpropagation in MCTS (see 3.1) to update value estimates and guide exploration. We further define the MCTS value estimate at each stateaction pair (si, ai) as: Q(si, ai) = ui + vi. By fusing local and global correctness signals, this hybrid reward formulation offers more stable and interpretable supervision than prior MC-based methods that rely solely on rollout success. Moreover, this mechanism directly addresses the three core limitations of existing PRMs: it reduces reward noise via structure-aware simulation, avoids unverifiable supervision through external tool-based validation, and aligns the reward objective with both step-wise precision and task-level success. 3.4 Generative Process Reward Model GroundedPRM adopts generative reward modeling paradigm, enabling seamless integration with instruction-tuned LLMs and providing supervision for open-ended reasoning workflows. Each training instance is structured as rationale-enhanced sequence that pairs intermediate reasoning steps with corresponding verification outcomes and justifications. Formally, each instance includes: (1) the original problem ; (2) the full reasoning trajectory {s1, . . . , sT }; (3) binary labels indicating the sign of the aggregated reward, combining tool-verified step fidelity and rollout outcome signals; and (4) natural-language explanations derived from external tool feedback, retained after consistency filtering to align with the verified binary labels. Unlike conventional discriminative reward models that treat reward prediction as binary classification task, we train GroundedPRM autoregressively to generate both correctness labels and rationales conditioned on the problem and its intermediate reasoning trace. This generative formulation improves interpretability and enables seamless integration into LLM-based reasoning pipelines. 3.5 Data Construction for GroundedPRM Training To train GroundedPRM, we apply the full supervision framework described above to the MATH dataset [11], constructing reward-labeled dataset with tool-based step-level verification and hybrid scoring. For each problem, the policy model generates intermediate reasoning steps, which are verified using external tools (see 3.2). Each step is labeled based on tool-verified correctness, and the full trajectory is scored using the hybrid reward mechanism introduced in 3.3. To ensure coverage and diversity, we adopt multi-round MCTS rollout strategy that explores both optimal and suboptimal paths. Post-processing includes filtering incomplete, inconsistent, or tool-unverifiable traces, and formatting the final data into rationale-enhanced generative structure (see 3.4). Each instance includes the problem, full reasoning trace, correctness labels, and explanations. The resulting dataset contains approximately 40K verified samples, covering broad spectrum of problem types and reasoning strategies with high tool-verified fidelity. Exact generation and verification prompts are given in Appendix B."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup Benchmarks. We evaluate GroundedPRM from two perspectives: its ability to accurately identify erroneous steps within multi-step reasoning processes, and its effectiveness in directly enhancing downstream task performance. ProcessBench [47]. This benchmark evaluates the ability of reward models to supervise step-level reasoning in mathematical problems. Each instance includes an LLM-generated solution with the first incorrect step annotated by human experts. Models are evaluated based on their ability to accurately identify the first faulty step or confirm that all steps are valid, following standard PRM evaluation protocols. Reward-Guided Greedy Search. To further assess the utility of GroundedPRM in guiding multi-step reasoning, we perform inference-time decoding using reward-guided greedy strategy. At each generation step, we sample = 8 candidate actions from Qwen2.5-7B-Instruct [24] using temperature of 1, and select the candidate with the highest 6 Table 1: F1 scores on ProcessBench for models trained with auto-labeled data. Models marked with share the same base model: Qwen2.5-Math-7B-Instruct. GroundedPRM achieves the highest average F1, surpassing the strongest existing model, Math-Shepherd-PRM-7B, by 26% relative improvement while using only 10% of the training data. All baseline results are directly cited from [46]. Oly. denotes OlympiadBench. Full results are provided in Appendix D. Model #Sample GSM8K MATH Oly. Omni-MATH Avg. RLHFlow-DeepSeek-8B RLHFlow-Mistral-8B Qwen2.5-Math-7B-Math-Shepherd EurusPRM-Stage1 EurusPRM-Stage2 Math-Shepherd-PRM-7B GroundedPRM 253K 273K 445K 453K 230K 445K 40K 38.8 50.4 62.5 44.3 47.3 47.9 43.4 33.8 33.4 31.6 35.6 35.7 29.5 47. 16.9 13.8 13.7 21.7 21.2 24.8 33.8 16.9 15.8 7.7 23.1 20.9 23.8 34.4 26.6 28.4 28.9 31.2 31.3 31.5 39.7 predicted reward assigned by the PRM. This process is repeated iteratively until complete solution is generated. We evaluate this procedure on six mathematical benchmarks: AMC23 [3], AIME24 [2], MATH [11], College MATH [30], OlympiadBench [10], and Minerva MATH [18]. We also report the result of pass@n, i.e., the proportion of test samples where any of the samplings lead to the correct final answers. Baselines. For both ProcessBench and reward-guided greedy search experiments, we compare GroundedPRM against the following representative baselines. These baselines span diverse set of supervision strategies, including models trained with human-labeled rewards, automated annotations, and hybrid approaches, as well as range of training data scales. Math-Shepherd [35]: Utilizes MC estimation to perform automated step-level annotation with hard labels. RLHFlow-PRM series [8]: Includes DeepSeek and Mistral variants, both of which use MC estimation for data generation, but adopt the Direct Preference Optimization (DPO) training paradigm. Math-PSA-7B [34]: Trained on mixed annotated data, namely PRM800K [20], MathShepherd [35], and generated data following [22]. EurusPRM-series [28]: EurusPRM-Stage1 and EurusPRM-Stage2 construct weakly supervised labels from final outcomes using noise-aware heuristics. Qwen2.5-Math-7B series [47, 46]: Qwen2.5-Math-7B-Math-Shepherd and Qwen2.5-Math7B-PRM800K are trained with Math-Shepherd [35] and PRM800K [20] using Qwen2.5Math-7B-Instruct [40], respectively. Llemma-PRM800K-7B [29]: Utilizes MC estimation to perform automated step-level annotation with hard labels. ReasonEval-7B [39]: Prompt-based model for evaluating step validity and redundancy. Implementation Details. All reward models are fine-tuned on step-labeled reasoning trajectories using LoRA [12] for parameter-efficient adaptation. We use Qwen2.5-7B-Instruct [24] as the base model. Complete training hyperparameters are listed in Appendix C. 4.2 Results on ProcessBench GroundedPRM Achieves Strong Supervision Performance with High Data Efficiency. As shown in Tab. 1, GroundedPRM achieves the highest average F1 score among all PRMs trained with automatically labeled data, outperforming the second-best model, Math-Shepherd-PRM-7B, by relative improvement of 26% while using only 10% training samples. GroundedPRM also ranks first on MATH, OlympiadBench, and Omni-MATH, indicating strong capability in evaluating complex mathematical reasoning steps. These results reinforce our central hypothesis: verifiable, structureguided supervision is substantially more effective than scale alone. GroundedPRMs fidelity-aware 7 Table 2: F1 scores of GroundedPRM and Qwen2.5-Math-7B-PRM800K under matched training sizes. Both methods are trained using Qwen2.5-7B-Instruct but differ in supervision sources. Despite relying solely on automatically labeled data, GroundedPRM consistently outperforms Qwen2.5-Math7B-PRM800K across all data scales. Oly. denotes OlympiadBench. #Sample"
        },
        {
            "title": "Model",
            "content": "GSM8K MATH Oly. Omni-MATH Avg. 10K 20K 30K 40K Qwen2.5-Math-7B-PRM800K GroundedPRM Qwen2.5-Math-7B-PRM800K GroundedPRM Qwen2.5-Math-7B-PRM800K GroundedPRM Qwen2.5-Math-7B-PRM800K GroundedPRM 30.3 39.0 37.4 39.9 37.5 42. 43.1 43.4 31.6 41.9 32.9 44.0 40.0 47.4 46.0 47.0 21.9 29. 29.9 30.1 28.4 30.7 32.9 33.8 19.8 29.8 30.6 31.4 34.8 31. 34.0 34.4 25.9 35.0 32.7 36.4 35.2 38.0 39.0 39.7 Table 3: F1 scores on ProcessBench under different supervision and inference configurations within the GroundedPRM framework. Step-Only and Outcome-Only variants remove one supervision source during training, while the Inference w/o Rationale variant skips rationale generation and outputs correctness labels directly. All variants share the same model architecture; the full version combines step-level verification, outcome consistency, and rationale generation. Method GSM8K MATH OlympiadBench Omni-MATH Avg. Step-Only Outcome-Only Inference w/o Rationale GroundedPRM 40.1 1.4 34.1 43.4 42.3 3.3 34.7 47.0 28.3 1.0 22.7 33. 29.2 1.0 23.7 34.4 35.0 1.7 28.8 39.7 rewards, grounded in tool-based validation and MCTS-based credit assignment, enable efficient learning under low-resource constraints. Generative Supervision Enhances Interpretability and Robust Generalization. Unlike prior PRMs that produce only binary decisions, GroundedPRM adopts generative format that outputs both step-level reward and an accompanying rationale. This design improves alignment with instruction-tuned LLMs, encourages interpretable supervision, and enables the model to better distinguish between fluent but incorrect reasoning and truly valid logic. Empirically, GroundedPRM achieves notable improvements on challenging benchmarks like OlympiadBench and MATH, where fine-grained error localization is essential. These results suggest that explanation-based rewards foster more robust and generalizable reasoning behavior. 4.3 Analysis and Discussions GroundedPRM Provides Superior Data Efficiency through Structured and Fidelity-Aware Supervision. To compare the effectiveness of our automatically labeled supervision against humanlabeled reward models under identical data budgets, we conduct controlled comparison with the Qwen2.5-PRM series using the same model architecture, i.e., Qwen2.5-7B-Instruct, and matched training sizes. For each training size, we randomly sample subset of examples to ensure fair comparison. This setup isolates the effect of supervision quality by ensuring that both methods are evaluated under the same data scale. As shown in Tab. 2, GroundedPRM consistently achieves higher F1 scores across all training sizes, despite relying entirely on automatically constructed labels. Dual-Signal Supervision Enhances Data Fidelity and Credit Attribution. To assess the contribution of our dual-signal supervision, we compare GroundedPRM against two ablations: Outcome-Only Supervision, which assigns labels based solely on final-answer correctness from MCTS rollouts, and 8 Table 4: Accuracy of reward-guided greedy search using different PRMs to supervise the Qwen2.57B-Instruct policy model. GroundedPRM outperforms all PRMs trained with human, mixed, or automated labels, achieving the highest average accuracy. Oly. denotes OlympiadBench. Model #Sample AMC23 AIME24 MATH College Oly. Minerva Avg. pass@1 pass@8(Upper Bound) - - 50.0 82.5 10.0 20.0 73.4 90.4 48.5 61.0 30.0 48. 29.8 49.6 40.3 58.6 Reward-Guided Greedy Search (prm@8) Trained on Human Annotated Data (PRM800K) Qwen2.5-Math-7B-PRM800K Llemma-PRM800K-7B ReasonEval-7B 264K 350K 350K 60.0 42.5 52. 10.0 6.7 6.7 75.6 72.2 76.0 36.5 47.5 33.8 23.5 27.6 33.8 29.0 29.5 30.0 39.1 37.7 41. Trained on Mix of Human and Automated Annotation Data Math-PSA-7B 860K 47.5 13.3 69.8 46. 27.6 33.5 39.6 Trained on Automated Annotation Data Math-Shepherd-PRM-7B RLHFlow-DeepSeek-8B RLHFlow-Mistral-8B EurusPRM-Stage1 EurusPRM-Stage2 GroundedPRM 445K 253K 273K 453K 230K 40K 45.0 50.0 37.5 47.5 45.0 57. 10.0 6.7 13.3 10.0 13.3 10.0 74.8 74.2 74.8 73.0 73.6 74.8 48.5 48.0 50.5 49.0 51.0 49.0 28.0 30.9 29.8 30.1 31.6 31.3 29.0 27.5 30.0 31.0 32.5 32.5 39.2 39.5 39.3 40.1 41.1 42. Step-Only Supervision, which uses external tool verification without considering global trajectory outcomes. As shown in Tab. 3, Outcome-Only Supervision severely underperforms due to credit misattribution. Correct steps may be penalized if downstream steps fail, while flawed steps may be rewarded if the final answer happens to be correct. Step-Only Supervision achieves higher recall but suffers from precision loss, as external math tools can detect surface-level arithmetic errors but often fail to capture deeper logical flaws, resulting in false positives. detailed example of this failure mode is provided in Appendix E.2. In contrast, GroundedPRM fuses step-level correctness signals with trajectory-level feedback, enabling accurate credit assignment that is grounded in both local fidelity and global reasoning success. This hybrid design achieves the highest average F1, demonstrating the effectiveness of our supervision framework in producing reliable and structurally aligned reward signals. Rationale Generation Enhances Consistency and Long-Horizon Reasoning. To assess the impact of rationale generation, we compare the full GroundedPRM with an Inference w/o Rationale variant that directly predicts correctness labels without generating explanations. As shown in Tab. 3, removing rationales leads to consistent drop in F1 across all datasets, with larger gaps on more challenging benchmarks such as MATH and OlympiadBench. Generating intermediate justifications helps maintain step-level consistency, stabilize reward attribution, and localize reasoning errors in complex, long-horizon problems. Qualitative examples in Appendix E.1 further illustrate how rationale generation improves interpretability and factual grounding without compromising predictive accuracy. 4.4 Results on Reward-Guided Greedy Search As shown in Tab. 4, GroundedPRM, trained on only 40K automatically labeled examples, achieves the highest average accuracy across all PRMs, surpassing those trained on automated, mixed, or even large-scale human annotations. Within the automated annotation group, GroundedPRM achieves new state-of-the-art results on AMC23 and performs on par or better than all counterparts on MATH and Minerva. These results validate the effectiveness of the design: tool-grounded verification improves label fidelity, tree-guided path construction yields stable and attribution-aware credit assignment, and rationale-enhanced supervision delivers precise and verifiable step-level evaluation. By evaluating each candidate step with grounded feedback, GroundedPRM reliably guides the policy toward accurate multi-step reasoning without requiring external demonstrations or value-based lookahead."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced GroundedPRM, tree-guided and fidelity-aware framework for process supervision. By combining structured path exploration via MCTS, tool-based step-level verification, hybrid reward aggregation, and rationale-enhanced supervision formatting, GroundedPRM addresses three core limitations of prior PRMs: low factual fidelity, noisy reward signals, and misalignment with steplevel reasoning objectives. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision. These results underscore the effectiveness of structured, verifiable reward modeling in enhancing the reasoning capabilities of LLMs."
        },
        {
            "title": "6 Future Work",
            "content": "While GroundedPRM establishes strong foundation for fidelity-aware and tree-guided process reward modeling, several natural extensions remain. Scaling the underlying LLM may further improve the quality and diversity of generated reasoning paths. Expanding the set of external verifiers beyond the mathematical tool used in this work could enhance flexibility and extend applicability across different reasoning domains. GroundedPRM is inherently tool-agnostic: tool broadly refers to any fidelity verifier that provides execution-grounded feedback for intermediate reasoning steps, including model-based self-checkers, retrieval-augmented verifiers, and rule-based evaluators. Additionally, integrating human preference signals may further align supervision with interpretable and human-consistent reasoning. further direction is to integrate GroundedPRM into reinforcement learning pipelines, where it serves as verifiable reward function guiding policy optimization in long-horizon tasks. Such integration would enable process-level supervision under on-policy updates and reveal how structured rewards interact with exploration, search, and credit assignment. Although our experiments focus on the mathematical domain due to its established PRM benchmarks and baselines, the framework naturally generalizes to any domain where step-level fidelity can be defined and verified, offering unified and scalable paradigm for grounded process supervision."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] AI-MO. Amc 2023, 2024b. aimo-validation-aime, 2024. [3] AI-MO. Amc 2023, 2024b. aimo-validation-amc. https://huggingface.co/ datasets/AI-MO/aimo-validation-amc, 2024. Accessed: 2025-07-30. [4] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [5] Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143, 2012. [6] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Step-level value preference optimization for mathematical reasoning. arXiv preprint arXiv:2406.10858, 2024. [7] Wenxiang Chen, Wei He, Zhiheng Xi, Honglin Guo, Boyang Hong, Jiazheng Zhang, Rui Zheng, Nijun Li, Tao Gui, Yun Li, et al. Better process supervision with bi-directional rewarding signals. arXiv preprint arXiv:2503.04618, 2025. 10 [8] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [13] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey, 2024. [14] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023. [15] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [16] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer, 2006. [17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [18] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. [19] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37:100428100534, 2024. [20] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [21] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. [22] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. [23] Aaron Meurer, Christopher Smith, Mateusz Paprocki, Ondˇrej ˇCertík, Sergey Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason Moore, Sartaj Singh, et al. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, 2017. 11 [24] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [25] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [26] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [27] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute In The Thirteenth optimally can be more effective than scaling parameters for reasoning. International Conference on Learning Representations, 2025. [28] Lin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, and Ning Wu. Freeprm: Training process reward models without ground truth process labels. arXiv preprint arXiv:2506.03570, 2025. [29] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. Advances in Neural Information Processing Systems, 37:5111851168, 2024. [30] Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884, 2024. [31] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [32] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face, 2024. [33] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [34] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024. [35] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [37] Wolfram Alpha LLC. Wolframalpha. https://www.wolframalpha.com/. [38] Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024. [39] Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2772327730, 2025. 12 [40] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [41] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [42] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. [43] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [44] Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, and Volker Tresp. Swarmagentic: Towards fully automated agentic system generation via swarm intelligence. arXiv preprint arXiv:2506.15672, 2025. [45] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: versatile and autonomous multi-agent system for web task execution with strategic exploration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 23378 23386, 2025. [46] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [47] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. [48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. [49] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work"
        },
        {
            "title": "2.2 Process Reward Models for Step-Level Supervision . . . . . . . . . . . . . . . . .",
            "content": "3 Methodology"
        },
        {
            "title": "3.4 Generative Process Reward Model",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.5 Data Construction for GroundedPRM Training . . . . . . . . . . . . . . . . . . .",
            "content": "4 Experiment 4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Results on ProcessBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Analysis and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Results on Reward-Guided Greedy Search . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion 6 Future Work Algorithm Overview and Pseudocode Prompt Template for Data Annotation Training Hyperparameters Supplementary Evaluation Results Case Studies E.1 Fidelityand Error-TypeAware Reasoning Supervision . . . . . . . . . . . . . . . E.2 Dual-Signal Supervision Improves Data Fidelity and Credit Attribution . . . . . . 3 3 3 3 4 5 6 6 6 6 8 9 10 10 15 18 18 18 19"
        },
        {
            "title": "A Algorithm Overview and Pseudocode",
            "content": "To facilitate reproducibility and provide an intuitive understanding of our reward modeling pipeline, we present the high-level pseudocode of GroundedPRMs data generation algorithm. As described in Algo. 1, GroundedPRM integrates MCTS-based reasoning path construction, tool-based step-level verification, and hybrid reward aggregation into unified supervision framework. Algorithm 1 Algorithm of GroundedPRM Require: Initial state s0, Node n, Node Value Q, Execution Round r, Max Rounds R, Max Children K, Tool Result v, Rollout Reward u, Node List , Node Value List , Visit Count nselect Selection(n) {Select node with highest UCT score} if nselect is terminal or has no children then break end if nselect while is fully expanded do end while Generate(nselect) for aj from a1 to aK do si1 State(nselect) ni1 nselect si,j Transition(si1, aj) ni,j Children(ni1, si,j) Q(si) vi,j {Verify step with tool} 1: Initialize: s0 sinitial, n0, Q0 0 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: Transition(s, a) Children(n, s) + + n, end if nsim, State(nsim) while = nterminal do end for if vi,j = max(v) then nsim ni,j 1i end for j=i+1 dj vj + β for = 1 to 0 do 32: 33: 34: Q(si, ai) ui + vi {Aggregate reward} 35: 36: 37: 38: 39: end for Qk Qk + γdk Q(si, ai) Nk Nk + end for end while FinalAnswerCorrectness(n) {Compare to ground truth} Length(L), ni nsim for all vj do ui 1 (cid:80)T"
        },
        {
            "title": "B Prompt Template for Data Annotation",
            "content": "To construct the step-level supervision for GroundedPRM, we adopt two structured prompt templates. The prompt in Fig. 2 is used to autoregressively generate intermediate reasoning steps during MCTS rollouts, producing structured trajectories consisting of step objectives and corresponding actions. The prompt in Fig. 3 is applied to verify each generated step using external tools, outputting binary 15 correctness labels along with rationale-enhanced explanations, which together form the fidelity-aware supervision signals used to train GroundedPRM. Figure 2: Prompt used to generate the next reasoning step during MCTS rollouts. The output consists of structured step objective and logically grounded action aligned with the current goal. These step-level generations are used to construct diverse reasoning trajectories for reward modeling in GroundedPRM. 16 Figure 3: Prompt used for tool-based step-level verification. The assistant analyzes the reasoning step for logical consistency, evaluates the relevance of Wolfram Alpha responses, and outputs binary correctness label along with structured rationale, forming the fidelity-aware supervision signal for GroundedPRM."
        },
        {
            "title": "C Training Hyperparameters",
            "content": "Tab. 5 lists the hyperparameters used to train GroundedPRM. We fine-tuned the model in sequenceto-sequence manner using the LLaMA-Factory [49] Trainer implementation on 4A100 80GB GPUs. Table 5: Training configuration for Qwen2.5-7B-Instruct. Parameter Value Model Torch data type Attention implementation Lora rank Lora alpha Per-device train batch size Gradient accumulation steps Learning rate Number of training epochs LR scheduler type Max gradient norm Warmup ratio Seed Optimizer Gradient checkpointing Qwen2.5-7B-Instruct bfloat16 flash attention 2 128 256 4 8 3.0 105 6 cosine 1.0 0.1 42 Adam True"
        },
        {
            "title": "D Supplementary Evaluation Results",
            "content": "We assess the step-level supervision quality of GroundedPRM on ProcessBench, comparing it to several strong PRM baselines trained with automated labels. As shown in Tab. 1 of the main paper, GroundedPRM achieves the highest average F1 score across all benchmarks, with notable gains on MATH, OlympiadBench, and Omni-MATH. These results highlight the effectiveness of our fidelity-aware, structure-guided reward modeling framework in generating accurate and reliable supervision, even under limited data budgets. Full results are provided in Tab. 6. Table 6: F1 scores on ProcessBench across four math benchmarks. GroundedPRM achieves the highest average F1 score among all PRMs trained with automatically labeled data, outperforming all prior methods by significant margin, particularly on MATH, OlympiadBench, and Omni-MATH. Avg. F1 Scoring Approach OlympiadBench Omni-MATH GSM8K MATH error correct F1 error correct F1 error correct F1 error correct F1 RLHFlow-PRM-Deepseek-8B RLHFlow-PRM-Mistral-8B Qwen2.5-Math-7B-Math-Shepherd EurusPRM-Stage1 EurusPRM-Stage2 Math-Shepherd-PRM-7B GroundedPRM 24.2 33.8 46.4 46.9 51.2 32.4 31.9 98.4 99.0 95.9 42.0 44.0 91.7 67.9 38.8 50.4 62.5 44.3 47.3 47.9 43.4 21.4 21.7 18.9 33.3 36.4 18.0 36. 80.0 72.2 96.6 38.2 35.0 82.0 67.5 33.8 33.4 31.6 35.6 35.7 29.5 47.0 10.1 8.2 7.4 23.9 25.7 15.0 23.4 51.0 43.1 93.8 19.8 18.0 71.1 60.5 16.9 13.8 13.7 21.7 21.2 24.8 33.8 10.9 9.6 4.0 21.9 23.1 14.2 23. 51.9 45.2 95.0 24.5 19.1 73.0 61.4 16.9 15.8 7.7 23.1 20.9 23.8 34.4 26.6 28.4 28.9 31.2 31.3 31.5 39."
        },
        {
            "title": "E Case Studies",
            "content": "We conduct qualitative case studies to demonstrate how GroundedPRM performs fine-grained, interpretable supervision across diverse reasoning scenarios. E.1 illustrates its ability to detect arithmetic, algebraic, and constraint-based inconsistencies with high fidelity and structural awareness. E.2 examines an ablation case that contrasts Step-Only and Dual-Signal supervision, revealing how dual-signal fusion enhances data fidelity and accurate credit attribution. 18 E.1 Fidelityand Error-TypeAware Reasoning Supervision We present three qualitative cases to illustrate how GroundedPRM delivers fidelity-aware, errortypeaware, and first-wrong-step localization in process supervision. Across all cases, generalpurpose LLM-as-judge fails to catch basic inconsistencies, whereas GroundedPRM recomputes the relevant quantities, checks constraints, and explains why step is wrong. Case 1: Basic arithmetic aggregation  (Fig. 4)  . student sums nine quiz scores. The LLM solution totals them to 570, and the LLM-as-judge accepts this step. In contrast, GroundedPRM reproduces the additions step-by-step (50 130 210 270 310 400 500 570 630), recovers the correct total 630, and labels the presented step as incorrect. This shows fidelity-aware arithmetic checking and precise localization of the first wrong step. Case 2: Sum-of-pairs with spurious halving  (Fig. 5)  . The problem gives three pairwise sums of products. The LLM correctly aggregates them to 210 but then unjustifiably divides by 2 to claim 105; the LLM-as-judge still marks the step as correct. GroundedPRM re-evaluates the algebra and confirms 210, explicitly naming the first error as spurious halving and explaining why this single slip corrupts downstream reasoning. This evidences error-type awareness rather than mere outcome comparison. < 4 9 < Case 3: Ratio bounded by inequalities and number-theoretic constraints (Figs. 6,7). Given ba = 15 and 5 7 with gcd(a, b) = 1, the LLM rewrites = a+15 and proposes candidates; it eventually claims (a, b) = (26, 41), which violates the upper bound since 26 7 0.5714. Fig. 6 shows the LLM-as-judge validating this wrong candidate by failing to enforce the bound. Fig. 7 shows GroundedPRM re-checking the inequality numerically, verifying = 15 and coprimeness, and recovering valid pair (19, 34) with 19 7 ). This two-part case highlights constraint checking beyond arithmetic (inequality bounds + number theory). 34 0.5588 ( 5 41 0.6341 > 4 9 , Takeaways. Across the three cases, GroundedPRM (i) recomputes key quantities instead of trusting fluent text, (ii) localizes the first wrong step and names the error type (e.g., spurious halving), and (iii) verifies multi-constraint consistency (inequalities, differences, coprimeness). These behaviors support our central claim: fidelity-aware, structure-guided supervision yields more reliable step-level judgments than generic LLM-as-judge baselines, and produces actionable rationales aligned with process-level reasoning. E.2 Dual-Signal Supervision Improves Data Fidelity and Credit Attribution We illustrate the impact of dual-signal supervision through math factorization example where Step-Only verifier fails to detect missing factor. By combining step-level verification with trajectorylevel feedback, GroundedPRM identifies the omission and corrects the reasoning, demonstrating its advantage in aligning local fidelity with global consistency. Case 4: Prime factorization with an omitted factor (Figs. 8,9). An LLM computes the GCF of 180 and 168 but omits the factor 3 in 168 (using 23 7) and concludes 252; Step-Only supervisor validates this step. In contrast, GroundedPRM re-derives 168 = 23 3 7, checks common primes with 180 = 22 32 5, and recovers the correct GCF 22 3 = 12, labeling the presented step as incorrect. This highlights how dual-signal supervision prevents surface-level false positives and yields faithful credit attribution. 19 Question: Sally received the following scores on her math quizzes: 50, 80, 80, 60, 40, 90, 100, 70, and 60. Find her mean score. Reasoning Step: To calculate the mean score of Sallys math quizzes, first, need to add up all of her quiz scores. The question shows the score of each quiz is: 50, 80, 80, 60, 40, 90, 100, 70, and 60. Adding these together, we get: Total score = 570. GPT-4o as Judge: **Incorrect judgement.** The total sum of the scores is accurate: 50 + 80 + 80 + 60 + 40 + 90 + 100 + 70 + 60 = 570. This step is: + GroundedPRM as Judge: **Correct judgement.** The objective is to find Sallys mean score from her math quizzes. The given scores are 50, 80, 80, 60, 40, 90, 100, 70, and 60. First, need to verify if the LLMs reasoning correctly adds up all these scores. The LLM states that the total score is 570. Let me add them one by one to check. Starting with 50, adding 80 gives 130. Then, adding another 80 brings it to 210. Adding 60 makes it 270. Next, adding 40 results in 310. Adding 90 increases it to 400. Then, adding 100 brings it to 500. Adding 70 makes it 570, and finally, adding 60 gives 630. The correct total should be 630, not 570. So the LLMs answer is incorrect here. According to the conclusion of the judgement, the label is: Figure 4: Comparison between GPT-4o and GroundedPRM as judge models. GPT-4o incorrectly validates the miscalculated reasoning step (570 instead of 630), while GroundedPRM correctly re-evaluates the arithmetic, detects the error, and provides rationale-supported supervision label, demonstrating fidelity-aware verification. 20 Question: Alice thinks of four positive integers satisfying {ab + cd, ac + bd, ad + bc} = {40, 70, 100}. What are all the possible tuples (a, b, c, d) that Alice could be thinking of? Reasoning Step: Given the equations ab + cd = 40, ac + bd = 70, and ad + bc = 100, we need to find the positive integers a, b, c, such that d. First, lets add all three equations: (ab + cd) + (ac + bd) + (ad + bc) = 40 + 70 + 100 2(ab + ac + ad + bc + bd + cd) = 210 ab + ac + ad + bc + bd + cd = 105 GPT-4o as Judge: **Incorrect judgement.** The current step appears to be correct. Lets break it down: 1. You are given three equations involving products of four integers a, b, c, and d: ab + cd = 40, ac + bd = 70, ad + bc = 100 2. To simplify the problem, the step suggests adding all three equations together: (ab + cd) + (ac + bd) + (ad + bc) = 40 + 70 + 100 3. Simplifying the left-hand side: ab + cd + ac + bd + ad + bc. The expression ab + ac + ad + bc + bd + cd indeed correctly accounts for all pairwise products. 4. The right-hand side simplifies to:40 + 70 + 100 = 210 5. Dividing both sides by 2 to account for each pair being counted twice, you get: ab + ac + ad + bc + bd + cd = 105 Conclusively, the current step has correctly simplified the sum of the given equations to derive the result ab + ac + ad + bc + bd + cd = 105. This expression will be useful in further exploring potential solutions for the tuple (a, b, c, d). This step is: + GroundedPRM as Judge: **Correct judgement.** Alright, Im trying to verify the reasoning step provided. Let me go through it step by step. First, the problem gives us three equations: ab + cd = 40, ac + bd = 70, ad + bc = 100. We need to find positive integers a, b, c, with d. The reasoning starts by adding all three equations: (ab + cd) + (ac + bd) + (ad + bc) = 40 + 70 + 100, Which simplifies to: ab + cd + ac + bd + ad + bc = 210. Then, the reasoning factors this sum into: (ab + ac + ad + bc + bd + cd) = 105. need to check if this step is logically sound. Lets see: Adding the three equations: ab + cd = 40, ac + bd = 70, ad + bc = 100. Adding them together gives: ab + cd + ac + bd + ad + bc = 40 + 70 + 100 = 210. So, the sum of all these terms is indeed 210. The reasoning then states that this sum equals ab + ac + ad + bc + bd + cd = 105. Wait, that seems off because 210 divided by 2 is 105, but the sum is 210, not 105. Theres mistake here. So, the correct equation after adding should be: ab + ac + ad + bc + bd + cd = 210. Not 105. Therefore, the reasoning incorrectly halved the total sum, leading to an incorrect conclusion. This error affects the rest of the reasoning, as subsequent steps would be based on an incorrect total sum. Hence, the logical check fails because the step introduces an error in the equation. In summary, the step adds the equations correctly but incorrectly divides the total sum by 2, leading to an incorrect value for the sum of the terms. This makes the reasoning flawed. The logical check reveals an error in the reasoning step. Adding the three equations correctly gives total sum of 210, but the reasoning incorrectly calculates this as 105 by dividing by 2. This introduces an inconsistency in the problem-solving process. According to the judgment conclusion, the label of this step is: Figure 5: Comparison between GPT-4o and GroundedPRM as judge models. After correctly aggregating the three sums to 210, the LLM performs an unjustified halving to 105. GPT-4o incorrectly validates this step. GroundedPRM correctly re-evaluates the arithmetic to 210, verifies each sub-sum, localizes the first wrong step (the halving), and outputs rationale-supported negative label, showing fidelity-aware, error-typeaware verification. 21 Question: The positive integers and have no common divisor larger than 1 . If the difference between and is 15 and 7 , what is the value of 9 < < 4 ? Reasoning Step: 7 9 < < 4 9 < 9 < a+15 < 7 0.5714 7 implies: is less than 4 is greater than To find the value of under the given conditions, lets reason through the problem step-by-step: First, understanding the conditions: gcd(a, b) = 1 means that and are coprime, i.e., they have no common divisors other than 1. = 15 indicates the difference between and a. 5 < 4 gives us bounds on the ratio . Second, interpreting the inequality: The inequality 5 9 0.5556 5 Third, simplifying the problem: Since = 15, we can write = + 15. Substituting this into the inequality gives: 5 Fourth, solving for and b: We want to find values of and that satisfy the conditions while being integers. Lets test possible values of and keeping in mind = + 15. Fifth, finding the solution: We know from the bounds of the inequality that must be close to (but smaller) since both fractions are close to each other. However, and also differ by 15, so the fraction should give value slightly less than but close to 4 7 . Testing different values for and b, while ensuring they are coprime and = 15: - For = 1, = 16, 1 - For = 2, = 17, 2 - Continuing this process... - For = 15, = 30, 15 - For = 21, = 36, 21 Notice that when = 26 and = 41, we have: - 26 41 < 4 41 which satisfies the inequality 5 Sixth, conclusion: Given the constraints and the testing process, the values = 26 and = 41 satisfy all the conditions. Therefore, the value of 16 is too small. 17 is still too small. 2 is too large. 12 is too large. 7 and gcd(26, 41) = 1. 30 = 1 36 = 7 9 < 26 is 26 41 . GPT-4o as Judge: **Incorrect judgement.** The current step contains an error in calculations and reasoning, which should be addressed for correctness. Lets analyze the problem: 1. Given Conditions: - Positive integers and with gcd(a, b) = 1. - = 15. < 4 - 5 7 . 2. Express in terms of a: 9 < - Since = + 15, substitute to get a+15 . 3. Refine the inequality: - Plug = + 15 into 5 4. Estimation by calculation: a+15 > 5 5. Determine suitable integers: - Solve 9 and verifying bounds. Issues in the current step: since = 15. 9 < < 4 7 , giving 5 9 < a+15 < 4 7 . a+15 < 7 separately to identify candidate values for - The process should ensure integer solutions for and while maintaining coprimeness and - The calculation steps provided for = 21, = 36 yielding 21 36 = 7 12 do not match = 15 - The test case = 26, = 41 accurately satisfies the division 41 within bounds and has = 15, as well as ensuring gcd(26, 41) = 1. This is the correct solution for confirmed calculations. The correct logic requires testing every integer value while ensuring all conditions (gcd, ba = 15) are met, leading to the right solution. Thus, = 26 and = 41, is the correct match. This step is: + Figure 6: Comparison between GPT-4o and GroundedPRM as judge models. Part 1 of 2 (see Fig. 7 for part 2). Given = 15 and 5 a+15 and then proposes candidates. GPT-4o incorrectly validates the step that accepts (a, b) = (26, 41) even though 26 7 , the LLM rewrites the bound as 9 < < 4 41 0.6341 > 4 7 , failing the bound check. 22 GroundedPRM as Judge: **Correct judgement.** Okay, need to verify the LLMs reasoning step based on the given problem. Let me go through each part carefully. First, the objective is to find integers and such that their greatest common divisor is 1, their difference is 15, and the fraction a/b lies between 5/9 and 4/7. The LLM suggests testing values where is close to but smaller, with = + 15. Looking at the examples provided: - a=1, b=16: 1/16 0.0625, which is way below 5/9 ( 0.5556). So this doesnt fit. - a=2, b=17: 2/17 0.1176, still too low. - a=15, b=30: 15/30 = 0.5, which is exactly 1/2, but the upper bound is 4/7 0.5714. So its within the range, but let me check if and are coprime. gcd(15,30)=15, not 1. So this doesnt work. - a=21, b=36: 21/36 = 7/12 0.5833, which is above 4/7. So its outside the upper bound. - Finally, a=26, b=41: 26/41 0.6341, which is above 4/7. Wait, thats over. But the LLM says its within the range. That seems incorrect because 4/7 0.5714, and 26/41 0.6341, which is higher than 4/7. So maybe theres mistake here. Wait, perhaps miscalculated 26/41. Let me compute it again. 26/41 is indeed approximately 0.6341, which is greater than 4/7 ( 0.5714). So the LLMs conclusion that 26/41 is within the range is incorrect. Therefore, the reasoning might have an error. But wait, maybe made mistake in the comparison. Let me check 26/41 against 5/9 and 4/7 again. 5/9 0.5556, and 4/7 0.5714. 26/41 0.6341, which is above 4/7. So the fraction is actually outside the upper bound. Therefore, the LLMs answer might be wrong because it claims 26/41 is within the range when its not. However, perhaps should check if theres another pair that fits better. Maybe a=31, b=46: 31/46 0.6739, still above 4/7. a=36, b=51: 36/51 = 12/17 0.7059, which is even higher. Maybe a=25, b=40: 25/40 = 5/8 = 0.625, still above 4/7. Hmm, perhaps the correct and are lower. Wait, let me try a=16, b=31: 16/31 0.5161, which is above 5/9 but below 4/7. But = 15, so 31 16 = 15. Now, gcd(16, 31) = 1, since 16 is 24 and 31 is prime. So this could be valid solution. Let me check: 16/31 0.5161, which is between 5/9 0.5556? Wait, no, 0.5161 is less than 5/9. So that doesnt work. Alternatively, a=19, b=34: 19/34 0.5588, which is above 5/9 but below 4/7. Check gcd(19, 34) = 1. So 19/34 0.5588, which is between 5/9 and 4/7. So this might be the correct pair. So the LLMs answer with a=26 and b=41 is incorrect because 26/41 > 4/7. The correct pair should be = 19 and = 34, which satisfies all conditions. Therefore, the LLMs reasoning has logical error in verifying the upper bound of the fraction. It incorrectly concluded that 26/41 is within the required range when its actually outside. Hence, the result is false. The LLMs reasoning incorrectly concludes that 26 41 0.6341, which exceeds the upper bound of 4 26 tions is = 19 and = 34, yielding 19 the conclusion of the judgement, the label is: 41 is within the specified range. Upon verification, 7 0.5714. correct pair satisfying all condi2 , which is within the range and coprime. According to 34 = Figure 7: Comparison between GPT-4o and GroundedPRM as judge models. Part 2 of 2 (complements Fig. 6). For the same case, GroundedPRM correctly re-evaluates the inequality, verifies ba = 15 and coprimeness, rejects (26, 41), and identifies the valid pair (19, 34) with 19 34 0.5588; it outputs rationale-supported negative label, demonstrating fidelity-aware, error-typeaware verification. 23 Question: Find the greatest common factor of 180 and 168. Reasoning Step: To find the greatest common factor (GCF) of 180 and 168, we can use prime factorization. First, lets express each number as product of its prime factors: First, 180 = 22 32 5. Second, 168 = 23 7. Step-Only: **Incorrect judgement.** Okay, so need to verify the LLMs answer about finding the GCF of 180 and 168 using prime factorization. Let me start by understanding what the problem is asking. The objective is to find the greatest common factor of these two numbers. The LLM provided the prime factorizations: 180 is broken down into 22 32 5 and 168 into 23 7. should check if these factorizations are correct because thats the foundation for finding the GCF. Starting with 180: Lets do the prime factorization step by step. 180 divided by 2 is 90, which divided by 2 again is 45. Then 45 divided by 3 is 15, and 15 divided by 3 is 5, which is prime number. So yes, that gives us 22 32 5, which matches the LLMs answer. Now for 168: Lets factorize it. 168 divided by 2 is 84, divided by 2 again is 42, and once more by 2 is 21. 21 divided by 3 is 7, which is prime. So thats 23 3 7. Wait, the LLM wrote 23 7, but think they missed the 3. Hmm, that might be an issue. But wait, in their answer, they only mentioned 23 7, so maybe they considered only the highest powers present in both? Or perhaps they made mistake. But wait, the GCF is found by taking the lowest powers of the common prime factors. So even if 168 has 3, since 180 doesnt have 7, the common prime factors are just 2 and 3. So the LLM correctly identified the common factors as 22 and 31, leading to GCF of 22 3 = 4 3 = 12. However, their factorization of 168 didnt include the 3, which might be mistake, but since they still arrived at the correct GCF, maybe its acceptable. Wait, no, actually, the prime factorization must be accurate. If 168 is indeed 23 3 7, then the GCF should consider the common primes. So 180 has 22, 32, and 5. 168 has 23, 3, and 7. The common primes are 2 and 3. For 2, the lowest power is 22, and for 3, its 31. So the GCF should be 22 3 = 4 3 = 12. So even though the LLMs factorization of 168 was incomplete, their conclusion was correct because they only took the common factors. So perhaps its minor error in the factorization step but the GCF is still right. Alternatively, maybe the LLM intended to show only the common factors. Let me double-check. If 168 is 23 7, then the common factors would be 22, but then the GCF would be 4, which is incorrect because 3 is also common factor. So the LLM must have included the 3 in their factorization, even if they didnt mention it in the answer. So perhaps there was typo, but the actual factorization includes the 3, leading to the correct GCF. In any case, the key point is that the LLMs answer correctly identifies the GCF as 12, even if there was slight error in the intermediate steps. Therefore, the logical check passes because the reasoning aligns with the known conditions, and the calculation is correct despite possible minor miscalculation in the factorization of 168. The LLM correctly used prime factorization to find the GCF of 180 and 168. The prime factors of 180 are 22 32 5, and those of 168 are 23 3 7. The common factors are 22 and 3, leading to GCF of 12. Despite possible error in the factorization of 168, the final result is correct.According to the judgement conclusion, this step is: + Figure 8: Comparison between Step-Only and GroundedPRM. Step-Only incorrectly validates step that is arithmetically valid but logically inconsistent, while GroundedPRM correctly detects the missing factor in 168s prime factorization through dual-signal verification, ensuring high-fidelity and logically grounded supervision. Part 1 of 2 (see Fig. 9 for part 2). 24 GroundedPRM **Correct judgement.** Alright, Im trying to verify the LLMs answer regarding the GCF of 180 and 168 using prime factorization. Let me go through the process step by step. First, the objective is clear: find the GCF of 180 and 168. The LLM provided the prime factorizations as 180 = 2² 3² 5 and 168 = 2³ 7. need to check if these factorizations are correct. Starting with 180: dividing by 2 gives 90, then another 2 gives 45. So thats 2². Then 45 divides by 3 to get 15, then again by 3 to get 5. So thats 3² and 5¹. So 180s prime factors are indeed 2² 3² 5. Now for 168: dividing by 2 gives 84, then 42, then 21. 21 divides by 3 to get 7. So thats 2³ 3¹ 7¹. Wait, the LLM only wrote 2³ 7. That seems missing the 3. Hmm, so the LLM might have missed including the prime factor 3 in 168s factorization. But wait, maybe made mistake. Let me double-check. 168 divided by 2 is 84, divided by 2 again is 42, divided by 2 again is 21. 21 is 37. So yes, its 2³ 3 7. So the LLMs factorization is incomplete because they only included 2³ and 7, omitting the 3. However, when finding the GCF, we take the minimum exponents of the common prime factors. Both numbers have 2² and 3², but wait, 168 doesnt have 3². It only has 3¹. So actually, the common prime factors are 2 and 3, with exponents 2 and 1 respectively. Therefore, the GCF should be 2² 3 = 4 3 = 12. But the LLM didnt include the 3 in their factorization of 168, which could lead to an incorrect GCF. However, in this case, since 168 only has one 3, the GCF would still be 3¹, but the LLMs omission might have confused the process. Wait, but the LLMs answer is just providing the factorizations, not calculating the GCF yet. So perhaps the issue is more about the accuracy of the factorizations before proceeding. Since the LLM missed the 3 in 168s factorization, their information is incomplete, which could affect the subsequent GCF calculation. So, the logical check shows that the LLMs reasoning is flawed because it omitted prime factor in 168. The factorization should include 3, which is crucial for determining the correct GCF. Therefore, the result should be false because the provided factorizations are incorrect.According to the judgement conclusion, this step is: Figure 9: Comparison between Step-Only and GroundedPRM. Step-Only incorrectly validates step that is arithmetically valid but logically inconsistent, while GroundedPRM correctly detects the missing factor in 168s prime factorization through dual-signal verification, ensuring high-fidelity and logically grounded supervision. Part 2 of 2 (complements Fig. 8)."
        }
    ],
    "affiliations": [
        "Fudan University",
        "LMU Munich",
        "Munich Center for Machine Learning",
        "Technical University of Munich",
        "University Heidelberg",
        "University of Oxford"
    ]
}