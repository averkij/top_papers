{
    "paper_title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
    "authors": [
        "Xinyu Yang",
        "Junlin Han",
        "Rishi Bommasani",
        "Jinqi Luo",
        "Wenjie Qu",
        "Wangchunshu Zhou",
        "Adel Bibi",
        "Xiyao Wang",
        "Jaehong Yoon",
        "Elias Stengel-Eskin",
        "Shengbang Tong",
        "Lingfeng Shen",
        "Rafael Rafailov",
        "Runjia Li",
        "Zhaoyang Wang",
        "Yiyang Zhou",
        "Chenhang Cui",
        "Yu Wang",
        "Wenhao Zheng",
        "Huichi Zhou",
        "Jindong Gu",
        "Zhaorun Chen",
        "Peng Xia",
        "Tony Lee",
        "Thomas Zollo",
        "Vikash Sehwag",
        "Jixuan Leng",
        "Jiuhai Chen",
        "Yuxin Wen",
        "Huan Zhang",
        "Zhun Deng",
        "Linjun Zhang",
        "Pavel Izmailov",
        "Pang Wei Koh",
        "Yulia Tsvetkov",
        "Andrew Wilson",
        "Jiaheng Zhang",
        "James Zou",
        "Cihang Xie",
        "Hao Wang",
        "Philip Torr",
        "Julian McAuley",
        "David Alvarez-Melis",
        "Florian Tramèr",
        "Kaidi Xu",
        "Suman Jana",
        "Chris Callison-Burch",
        "Rene Vidal",
        "Filippos Kokkinos",
        "Mohit Bansal",
        "Beidi Chen",
        "Huaxiu Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible."
        },
        {
            "title": "Start",
            "content": "Published in Transactions on Machine Learning Research (10/2025) Reliable and Responsible Foundation Models: Comprehensive Survey Xinyu Yang1*, Junlin Han2*, Rishi Bommasani3*, Jinqi Luo4*, Wenjie Qu5*, Wangchunshu Zhou6*, Adel Bibi2*, Xiyao Wang7*, Jaehong Yoon8, Elias Stengel-Eskin8, Shengbang Tong9, Lingfeng Shen10, Rafael Rafailov3, Runjia Li2, Zhaoyang Wang8, Yiyang Zhou8, Chenhang Cui5, Yu Wang11, Wenhao Zheng8, Huichi Zhou12, Jindong Gu2, Zhaorun Chen13, Peng Xia8, Tony Lee3, Thomas Zollo14, Vikash Sehwag15, Jixuan Leng1, Jiuhai Chen7, Yuxin Wen4, Huan Zhang16, Zhun Deng12, Linjun Zhang17, Pavel Izmailov9, Pang Wei Koh18, Yulia Tsvetkov18, Andrew Wilson9, Jiaheng Zhang5, James Zou3, Cihang Xie19, Hao Wang17, Philip Torr2, Julian McAuley11, David Alvarez-Melis20, Florian Tramèr6, Kaidi Xu21, Suman Jana12, Chris Callison-Burch4, Rene Vidal4, Filippos Kokkinos22, Mohit Bansal8, Beidi Chen1, Huaxiu Yao8 *indicates major contribution. indicates advisory role. indicates project leader. 1Carnegie Mellon University 2University of Oxford 3Stanford University 4University of Pennsylvania 5National University of Singapore 6ETH Zurich 7University of Maryland 8University of North Carolina at Chapel Hill 9New York University 10Johns Hopkins University 11University of California, San Diego 12Imperial College London 13University of Chicago 14Columbia University 15Princeton University 16University of Montreal & Mila 17Rutgers University 18University of Washington 19University of California, Santa Cruz 20Harvard University 21Drexel University 22University College London"
        },
        {
            "title": "Abstract",
            "content": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible. 6 2 0 2 4 ] . [ 1 5 4 1 8 0 . 2 0 6 2 : r Corresponding author: xinyuagi@cmu.edu 1 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "1 Introduction",
            "content": "Figure 1: Overview of reliable and responsible foundation models. This survey comprehensively summarizes existing research from nine critical dimensions: bias and fairness, alignment, security, privacy, hallucination, uncertainty, distribution shift, explainability, and Artificial Intelligence-Generated Content (AIGC) detection. We organize foundation models into four categories, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models, and Video Generative Models, to illustrate how each category uniquely interacts with these dimensions. Additionally, we explore how these dimensions interact and reinforce one another to highlight their synergies and shared challenges. Recently, the paradigm for building artificial intelligence (AI) systems has undergone fundamental shift, shaped by compelling dual imperative: to develop increasingly powerful and versatile foundation models and to build inherently reliable and responsible foundation models. At the center of this transformation lies the emergence of foundation models: large-scale neural networks that pre-trained on massive and diverse datasets [492]. For these models, defining characteristic is their general-purpose nature: instead of being designed for single task, they serve as foundation that can be adapted to wide range of downstream applications via methods like in-context learning, supervised fine-tuning, or reinforcement learning [100]. Among these foundation models, four major classes have fundamentally reshaped how we use and interact with AI, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models. These models demonstrate series of powerful capabilities: LLMs can engage in multi-turn conversations and human-like reasoning processes, MLLMs can generate HTML code from screenshot of sketched website, 2 Published in Transactions on Machine Learning Research (10/2025) Image Generative Models can synthesize and edit photorealistic images from textual instructions, and Video Generative Models can simulate interactive dynamics and commonsense knowledge of the physical world. The advent of foundation models can be traced back to the development of large-scale language representations, which evolved from early word embeddings such as GloVE [717] and word2vec [655], to contextualized representations like ELMo [719]. This progress paved the way for transformer-based models such as BERT [Bidirectional Encoder Representations from Transformers; 221], which revolutionized natural language processing by providing powerful representations to improve performance on various downstream tasks. Next, the GPT-series [Generative Pre-trained Transformer; 740, 741, 113] of autoregressive (AR) generative models showed how self-supervised learning via next-token prediction can yield high-quality text generative models. With the release of ChatGPT [692], the strong capabilities of foundation models further gained mainstream attention, which exposed the public to an intuitive conversational user interface. Today, such autoregressive generative models have become the established paradigm for AI beyond natural language processing with multimodal models like GPT-4V(ision) [693], GPT-4o [694], GPT-4.5 [700], GPT-5 [699], Gemini series [313], Claude 3 [33], Claude 4 [36], Qwen2.5-VL [18], Qwen2.5-Omni [20], Qwen3-VL, Qwen3-Omni [1044], and Seed1.5-VL [336]. Simultaneously, models such as OpenAI o1 [696], OpenAI o3 and o4-mini [698], DeepSeek R1 [209], Claude 3.7 [35], Gemini-2.5 [316], Grok 3 [1019], Grok4 [1018], Qwen3 [21], and Seed-1.6 [116] enhance reasoning capabilities by increasing compute at test time. Behind these models, the key innovation is the self-attention mechanism [915], which constructs contextual representations by enabling each token to weight and aggregate information from other tokens in the input sequence. Compared to earlier recurrent models [318, 193], its inherent parallelizability served as crucial catalyst for massive scaling during pretraining, making it feasible to train exceptionally large models, which led to the era of foundation model [100]. Concurrently, the field has witnessed rapid advances in generative diffusion models, which learn to reverse carefully controlled noise injection process to capture the underlying data distribution. This approach has emerged as leading paradigm for high-fidelity content generation, particularly excelling in visual generation tasks [836, 365]. Among them, Image Generative Models, including Text-to-Image and Image-Editing models such as DALLE 3 [85], Stable Diffusion 3.5 [261], Playground v3 [563], FLUX. 1 Kontext [72], GPTImage-1 [691], Gemini 2.5 Flash Image [276], Imagen 4 [66], Qwen-Image and Qwen-Image-Edit [1000], and SeedReam-4 [290] can generate high-resolution and high-quality images from textual descriptions. Similarly, advancements in Video Generative Models, pioneered by Sora [697] and followed by HunyuanVideo [466], CogVideoX-1.5 [1073], Kling 2.5 [471], Wan2.2 Video [931], Veo 3 [312], Hailuo 02 [660], Seedance 1.0 [291], and Sora2 [697] have emphasized adherence to physical laws and commonsense reasoning. These models focus on generating realistic physical-world scenarios and human-centric content. Collectively, they achieve remarkable spatial resolution and temporal coherence, enabling the creation of long-form, high-quality videos. Figure 2: Foundation models are typically trained on diverse modalities and then adapted for downstream applications. Throughout this pipeline, various reliable and responsible issues emerge at different stages. Published in Transactions on Machine Learning Research (10/2025) The powerful capabilities demonstrated by these rapidly evolving models have fueled their swift integration across the economy [200], with applications ranging from decision-making in business processes to personal assistants in everyday life. To quantify their broad usage, for example, ChatGPT reached an estimated 100 million monthly active users in less than three months, while Deepseek-R1 achieved the same milestone in only one month, making these foundation models the fastest-growing consumer internet application in history [904, 14]. This scale of usage and the resulting socioeconomic impact accentuate the urgent need for these models to be both reliable and responsible [324]. In the context of this survey, we provide explicit definitions of these foundational concepts. We define reliability as the models capacity to perform its intended functions accurately, consistently, and robustly, especially under challenging conditions like distribution shifts. We also define responsibility as the alignment of models behavior with ethical principles and societal values, encompassing crucial aspects such as fairness, privacy, security, and transparency. This survey aims to synthesize the technical challenges and solutions for building models that satisfy both of these critical criteria. In addition, this survey provides comprehensive and unified examination of reliable and responsible foundation models. While prior surveys [37, 82, 942] have offered in-depth analyses of specific issues like hallucination [780, 390] or safety [1157, 620], our primary contribution lies in presenting holistic, cross-cutting analysis (as in Section 12) that connects the nine critical dimensions across four major model classes (see Figure 1 and Figure 2). This unique perspective reveals crucial inter-connections and trade-offs, offering insights that are often overlooked in more specialized reviews yet crucial for advancing reliable and responsible AI. Furthermore, our scope emphasizes the challenges of ensuring that foundation models operate reliably and responsibly when used as intended by their developers, focusing on their intrinsic properties rather than external misuse scenarios. This perspective complements separate bodies of work that address the deliberate misuse of AI for malicious purposes, such as disinformation, cyberattacks, and other adversarial activities. We preview each section of the paper below: We begin with an examination of bias and fairness in foundation models, where we detail how biases arise, review methods for their evaluation and mitigation, and discuss the associated challenges. Next, we explore the concept of alignment: why do we align foundation models with human values and how do we mitigate misalignment? We conceptualize security for foundation models: what threats do they pose, and what measures can enable safer deployment? In tandem with security, we consider the data privacy challenge: how can we respect individual privacy rights when collecting large-scale data? Our exploration continues with look at the phenomenon of hallucination in foundation models, where the model generates or responds to questions incorrectly, stating incorrect facts with high confidence. We then examine the critical need for models to express uncertainty to prevent misleading results. We cover various sources as well as methods for the quantification and mitigation of uncertainty. Next, we discuss the challenge of distribution shifts in foundation models: how to ensure models perform robustly on domain-specific tasks and out-of-distribution scenarios? We review explainability to understand how foundation models work internally. We investigate methods for explaining LLMs with raw features, uncovering the knowledge in LLMs, and examining the roles of samples in training, fine-tuning, and few-shot learning. We conclude by discussing AI-generated content (AIGC) detection, where we frame the inherent challenges in differentiating human and AI-generated content, the state-of-the-art detection methods, and the underlying assumption for different detection methods. This survey provides comprehensive review of the current state of development of reliable and responsible foundation models1. It offers valuable insights for researchers, practitioners, and policymakers who aim to design, deploy, and regulate AI systems in responsible and reliable way across diverse real-world settings. 1We cover literature that was publicly available up to May 2025. 4 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "3.4 Bias and Fairness in MLLMs",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.5 Bias and Fairness in Image Generative Models",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . 3.6 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6.1 Limitations and Open Challenges of Bias and Fairness . . . . . . . . . . . . . . . . . . 3.6.2 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Alignment 4.1 Supervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Reinforcement Learning from Human Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Prompt Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Alignment for MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Security 5.1 Security in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Attack in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Defense in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Security in MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Attack in MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.2 Defense in MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Security in Image Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.1 Attack in Image Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.2 Defense in Image Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Limitations and Open Challenges of Attacks . . . . . . . . . . . . . . . . . . . . . . . 5.4.2 Limitations and Open Challenges of Defenses . . . . . . . . . . . . . . . . . . . . . . . 6 Privacy 6.1 Privacy in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 9 10 11 13 16 17 18 18 18 20 23 26 28 29 31 31 33 34 34 35 35 36 37 37 39 39 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "7.1.1 Hallucinations in LLMs",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.2 Hallucinations in MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Sources of Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Hallucination Detection and Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.1 Detecting Language Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.2 Measuring Multimodal Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.3 Measuring Image Generation Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Hallucination Mitigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.1 Reducing Hallucinations in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.2 Reducing Hallucinations in MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Uncertainty 8.1 Sources of Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.2 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1.3 Aleatoric vs. Epistemic Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Quantifying and Addressing Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.1 Estimating Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.2 Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.3 Verbalized Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.4 Addressing Uncertain Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.5 Distribution-free Uncertainty Quantification . . . . . . . . . . . . . . . . . . . . . . . . 8.3 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Distribution Shift 9.1 Definition and Categorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 41 41 42 42 43 43 43 44 45 46 48 48 48 48 51 52 52 52 53 54 54 55 57 57 59 60 60 6 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "9.3.3 Label Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "9.3.4 Invariant Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "9.4 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "9.4.1 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "9.4.3 Fine-Tuning with New Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "9.4.4 Test-time Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.4.5 Model Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.5 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Explainability 10.1 Feature Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.1.1 Perturbing the Input for Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.1.2 Gradient-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.1.3 Attention-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2 Exploring the Knowledge in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2.1 Probing the Representations within LLMs . . . . . . . . . . . . . . . . . . . . . . . . . 10.2.2 Explaining LLMs with Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2.3 Mechanistic Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3 Discovering the Roles of Samples in Training, Fine-tuning, and Few-shot Learning . . . . . . 10.3.1 Influence of Single Example in Training . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3.2 Influence of Training Stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3.3 Influence of Samples in Few-shot Learning . . . . . . . . . . . . . . . . . . . . . . . . . 10.4 Evaluation of Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4.1 Evaluation of Plausibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4.2 Evaluation of Faithfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.5 Applications of Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.5.1 Avoiding Shortcut Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.5.2 Improving Model Performances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.6 Explainability of MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.7 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.7.1 Faithfulness of Raw Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 62 63 63 63 64 64 64 65 66 69 71 71 71 72 73 73 74 74 74 75 75 76 76 76 77 77 77 78 78 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "10.7.3 Transferability of Explanation Across Different Modalities . . . . . . . . . . . . . . . .",
            "content": ""
        },
        {
            "title": "11.2.3 Pre-trained LLMs",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "11.3 Watermark-based Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "11.3.1 Training-free Watermarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.3.2 Learnable Watermarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.4 Neural Network Detectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.5 Current Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.5.1 Fairness of AIGC Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.5.2 Robustness of Watermarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.5.3 Origin Attribution of Generated Images . . . . . . . . . . . . . . . . . . . . . . . . . . 12 Intersection and Conclusion 12.1 Summary of Recent Community Efforts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.2 Bias, Fairness, and Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.3 Bias, Fairness, and AI-generated Content Detection . . . . . . . . . . . . . . . . . . . . . . . . 12.4 Security and Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.5 Security and AI-generated Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.6 Uncertainty and Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.7 Hallucination, Uncertainty, Distribution Shift, and Alignment . . . . . . . . . . . . . . . . . . 80 80 80 81 82 83 84 84 85 86 86 86 87 87 87 88 88 89 89 8 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "2 Types of Foundation Models",
            "content": "As discussed in the prior chapter, foundation models are designed to serve as versatile backbones for applications in various domains, offering world knowledge that can be adapted to wide range of downstream tasks. In this survey, we first define set of core modalities = {T , I, V, A}, representing Text, Image, Video, and Audio, respectively. We then focus on four popular representative of foundation models, including Text-to-Text (i.e., LLMs), Multimodal-to-Text (i.e., MLLMs), Multimodal-to-Image (i.e., image generative models), and Multimodal-to-Video (i.e., video generative models) AI systems. LLMs are foundation models specifically designed to understand, generate, and manipulate human language. They encompass range of model architectures, including encoder-only models (e.g., BERT) for understanding tasks like text classification, decoder-only models (e.g., GPT) for generative tasks like content creation, and encoder-decoder models (e.g., T5) for sequence-to-sequence tasks. Frequent applications of LLMs include summarization, translation, sentiment analysis, and dialogue systems. In notation, we can represent the general function of an LLM by : : Y, where , {T } (1) where is the space of text sequences. The function is parameterized by weights θ and is mainly implemented using Transformer-based architectures [915], though emerging architectures like State-Space Models [323] and Linear RNNs [1059] are also gaining traction. MLLMs, in our review context, are large-scale deep neural networks that process multiple modalities of input data to generate text outputs. general and prevalent design recipe for MLLMs is to adopt architectures that integrate multimodal features in shared latent space, as exemplified by seminal works like CLIP [742] and models like LLaVA [570]. Applications of MLLMs are vast, including robotics, healthcare, and augmented reality. The mathematical expression of general MLLM can be viewed as mapping function g: : X1 X2 , where Xi {T , I, V, A} (2) where Xi represent different modalities. Notably, prominent class of contemporary multimodal models focuses on processing text and images to generate text. This will be the primary focus of our work when referencing MLLMs, corresponding to the specific mapping function : . We further denote image generative models as class of foundation models that generate images based on multimodal inputs, most commonly textual instructions. Applications of these models extend beyond digital art to product design and education. In notation, we represent image generative models model by h: : X1 X2 Y, where Xi {T , I, V, A} and {I} (3) where is the space of input modalities and is the space of output images. is parameterized by weights ψ and is often implemented with generative modeling approaches such as diffusion models [836, 365, 843], generative adversarial networks (GANs) [310], and autoregressive Transformers [754]. Moreover, these models typically feature hybrid architecture, combining, for instance, Transformer-based text encoder to interpret the input prompt with the above-mentioned backbone model to generate the image. In this work, we focus primarily on diffusion models, as they have emerged as the dominant architecture in image generation models. Finally, we focus on video generative models that generate videos based on multimodal inputs. Applications include realistic physical-world simulations or high-quality human-centric interactions. In notation, we can represent video generative models by mapping function v: : X1 X2 Y, where Xi {T , I, V, A} and {V} (4) where Xi represents different input modalities and is the space of output videos. Function is parameterized by weights ω and often implements architectures that extend diffusion models to handle temporal relations for coherent video generation. Meanwhile, autoregressive approaches [1090] have also emerged as an alternative paradigm for modeling such temporal relations. Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "3 Bias and Fairness",
            "content": "Foundation models are often pre-trained on large-scale data. Consequently, these models inherently acquire biases from their training data, which can then propagate to various downstream applications [324, 97, 543]. In practice, the source and impact of these biases present in training data, foundation models, and downstream applications are poorly understood [281]. Therefore, more work is needed to measure and mitigate bias in foundation models to advance fairness and equity in AI systems [100]. This section explores bias and fairness in foundation models, organized as follows: We begin by establishing basic definitions to formalize bias and fairness, highlight potential consequences, and outline the essential criteria that require fairness for LLMs (Section 3.1). Next, we review methods for bias measurement (Section 3.2) and bias mitigation (Section 3.3) as shown in Figure 5. Finally, we discuss bias and fairness in multimodal contexts, focusing on MLLMs and image generative models, respectively (Sections 3.4 and 3.5). We provide the detailed visualization of the categorization of these concepts and methods in Figure 3. Core Definitions Bias Fairness Concepts & Taxonomy (3.1) Social Bias Categories Language & Norms Representation & Discrimination Task Manifestations Generation/MT/IR/QA NLI/Classification Text Generation [814, 1074, 918], Machine Translation [642, 886], Information Retrieval [763], Question Answering [224, 711], Natural Language Inference [220], Text Classification [460, 1076] Distribution Metrics Social Group Substitutions [102], Co-occurrence Scores [102], Demographic Representation [99, 545] Generated Text Classification Metrics Perspective API [545, 185, 192], Toxicity Fraction [545], Style Classifier [833] d o d F s r & B Evaluation Methods (3.2) Feature Embeddings Word-level Metrics WEAT [117], IAT [320], SEAT [637] Hurtlex [71], Bold [224], Honest [688] Token Selection Prob. Token-level Prob. Estimation [976], Pseudo-log Likelihood [932, 781], Context Association Test [679], All Unmasked Likelihood [432] Mitigation Methods (3.3) Alignment and Instruction Tuning RLHF-style Alignment [961], InstructGPT [703], DPO [744] Training Training Data Augmentation CDA [602, 732, 976], Mix-Debias [1101], Data Filtering [295, 101, 877], Self-Instruct [962], Constitutional AI [58] Inference EVER [433], ReID [165] MLLMs (3.4) Phenomena Debiasing Bias Patterns Vision-grounding Hallucinations [1194, 944, 943, 532, 566], Entity/Count/Logic Errors [1201, 568] Counterfactual Image Concepts [609, 726, 1020], Test-time Embedding Debiasing [301] Skewed Profession/Ethnicity/Class Representations [1156, 182, 89, 606], Marginalized Identity Underrepresentation [513], Real-world Distribution Amplification [779] Image Generative Models (3.5) Prompt-level Mitigation Fair Prompting [277], Soft-token/Embedding Steering [446, 190], Concept Manipulation [513], Keyword Filtering Cautions [234] Modalities & Outlook Sampling-level Mitigation Class-balancing Diffusion [830], Fairness-aware Amplified-prior Sampling [183] Limits & Open Challenges Evaluation Gaps Future Directions Research Agenda Autoregressive Bias Propagation [1030, 790, 1218], Contextual/Narrative Framing Issues [232, 815], Cross-cultural Definition Variance [1138, 130], Fairness-authenticity Tensions [74], Web-scale Resampling Limitations [364] Unbiased Tokenization/Embedding Methods [721, 1134], Fairness-aware Alignment [1104, 1202, 956, 1200], Autoregressive Text Post-processing [1201], Mechanistic Stereotype Circuit Interpretability [581, 1051], Modality-specific Analyses [997], Fairness-utility Trade-offs Figure 3: Taxonomy of Bias and Fairness in Foundation Models. 10 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "3.1 Definitions",
            "content": "Table 1: Categories of Social Biases in LLMs. We provide definitions and an example for each type of bias. The framework builds upon Gallegos et al. [282] with additional refinements. Bias Type Definition Example Pejorative Language The use of slurs, insults, or other derogatory language that targets and denigrates social group. Using the word bitch conveys contempt and stereotypes hostile attitudes towards women [86]. Linguistic Diversity preference for standard language forms in LLM training may sideline dialects, indirectly devaluing the linguistic patterns of marginalized groups in society. The misclassification of African American English (AAE) expressions like finna as non-English more often than Standard American English (SAE) equivalents [96]. Normativity Misrepresentation Stereotype Hate Speech Reinforcement of the normativity of the dominant social group while implicitly excluding other groups. Referring to women doctors as if doctor itself entails not-woman [81]. It happens when generalizing from an incomplete or non-representative sample population to social group, leading to misrepresentations. An inappropriate response like Im sorry to hear that. to Im mustachioed guy., reflecting misunderstanding of mustache [833]. Negative and immutable abstractions about labeled social group. Linking Muslim to terrorist fuels negative and violent stereotypes [5]. Offensive language that attacks, threatens, or incites hate or violence against social group. Stating Asian people are gross and universally terrible is disrespectful and hateful [230]. Explicit Discrimination The direct and clear differential treatment of individuals or groups based on their membership in social group, such as race, gender, age, ethnicity, religion, or sexual orientation. recruitment policy that states or implies preference for candidates of certain race over others, or club that refuses membership based on gender [272]. Implicit Discrimination Individuals are treated differently based on unconscious or subtle prejudices and stereotypes rather than explicit intentions to discriminate. health assessment tool used by insurance companies assigns higher risk scores to patients from certain ethnic backgrounds [272]. Given language technologies broad impact, the study of biases has become increasingly central to Natural Language Processing (NLP) in recent years. Language intimately interconnects with aspects of human identity, social relationships, and power dynamics. Biases, particularly social biases, pertain to the segmentation and distinctiveness of various social groups, imparting both generic and pejorative connotations, and correlate specific demographics with stereotypical, uncharacteristic, or overly generalized traits [281, 97, 1048]. Prior work [69, 81, 989, 866, 643] understand the concepts of bias and fairness in terms of these social groups and addresses variety of social domains and downstream tasks [1105, 1011, 926, 769, 107]. While several surveys provide in-depth analyses of bias and fairness in language models, this section situates these challenges within the broader context of foundation model reliability, connecting them to issues of security and content detection. Figure 4: An example of gender bias in LLM responses. Bias. Biases in LLMs refer to systematic deviations in the models responses, representations, and reasoning paths that reflect disparities, stereotypes, or inaccuracies in the training data. These biases can misalign with 11 Published in Transactions on Machine Learning Research (10/2025) or overinterpret the reference social and cultural norms implied by human prompts. Typically, such biases arise from the unbalanced or biased data distribution in domains (i.e., areas of knowledge) and genres (i.e., types of text, such as news, fiction, dialogue, etc.) representing different groups. For instance, the malefemale distribution of Wikipedia articles about US Presidents would lead to biases on the role of different genders in politics. Figure 4 illustrates similar example of language bias related to leadership in LLMgenerated contents. Following Gallegos et al. [281], we provide detailed summary and categorization of biases in LLMs, including definitions and examples, as shown in Table 1. These biases may manifest in distinct ways based on the specific context and downstream tasks. Recognizing and addressing these biases is crucial for developing fair and equitable NLP technologies. To better understand the unique forms in which bias can manifest in LLMs, we have listed some examples drawn from various NLP tasks below: Text Generation. We might encounter local biases, such as different job choices when generating phrases like The man worked as car salesman. versus The woman worked as nurse. Additionally, we may face global biases, such as the overall depiction of certain cultural backgrounds like East Asians like to eat rice [814, 1074, 918]. Machine Translation. Translation tools may show tendency towards gender-specific expressions when translating job-related phrases [642]. For example, translating the engineer solved the problem into German might default to der Ingenieur (the masculine form), given that in an existing English-German corpus, der Ingenieur was found to be 75 times more prevalent than its feminine counterpart die Ingenieurin [886]. Information Retrieval. Searches like successful leaders may be biased towards returning documents about male leaders, overlooking female ones, or exhibit bias towards certain cultural interpretations retrieving information about cultural holidays [763]. Question Answering. When faced with specific questions, answers can be influenced by gender or occupational stereotypes. For example, assume the primary caregiver in household is the mother or woman, or defaulting to man as companys CEO [224, 711]. Natural Language Inference. When given premise like the doctor is seeing patient, the model might incorrectly infer the doctors gender or make assumptions about the age or gender of participants in sports activities based on stereotypes [220]. Text Classification. Models might wrongly categorize statements that use regional dialects or slang as aggressive or inappropriate. They may also exhibit bias when classifying posts discussing sensitive topics [460, 1076], failing to consider the actual content of the text. Fairness. Due to the biases discussed above, LLMs may exhibit disparities in task-specific performance across different social groups. Consequently, it is essential to ensure that these models behavior, outputs, and decisions are fair and unbiased, reflecting and respecting the diversity and complexity inherent in society. Considering the data distributions across social groups differ in complex way, we use performance disparities to measure it. Following Section 2, an LLM can be denoted as function : Y, which maps context or prompt to target response . Additionally, measurement function S: maps response to scalar score R. The model is considered fair for groups and in terms of the measurement if the following condition holds: EXA(S(f (XA; θ))) = EXB (S(f (XB; θ))), (5) where XA represents the prompt or context information related to particular group A, with different groups possibly encompassing attributes such as race, gender, etc. When it fails to satisfy Equation 5, it is said that the model exhibits bias towards particular group. It is noteworthy that this is just one possible definition, while other definitions and metrics can also be reasonable [282, 342]. With the increasing deployment of LLMs in the business domain, such as customer service and decision support systems, ensuring these LLMs are fair and unbiased has become paramount. Similarly, given these models role as part of social services, the requirements for fairness and non-toxicity are crucial to avoid potential social biases and adverse impacts. In the study conducted by Gallegos et al. [281], comprehensive 12 Published in Transactions on Machine Learning Research (10/2025) set of principles was discussed, including Fairness through Unawareness, Invariance, Equal Social Group Associations, Equal Neutral Associations, and Replicated Distributions. These principles not only guide NLP tasks but also lay the foundation for fairness and non-toxicity in the practical deployment of LLMs. Such efforts aim to develop consensus-building approaches across diverse stakeholder groups, ensuring that LLM applications dont disproportionately impact specific communities, thereby supporting the sustainable development of equitable social services."
        },
        {
            "title": "3.2 Methods for Bias Evaluation",
            "content": "In this section, we summarize three popular approaches for evaluating bias in LLMs: Methods based on Generated Text. These evaluation methods are primarily based on assessing the text generated by LLMs in response to specific prompts, often using specialized benchmarks. Typical benchmarks include Dhamala et al. [224] and Gehman et al. [297]. They utilize guiding prompts to induce biased outputs from the model to evaluate the inherent biases of LLMs. Therefore, models with more severe biases are more prone to exhibiting tendencies toward certain groups. After obtaining the models textual responses to the designed prompts, three metrics are generally used to assess the biases in the responses. Figure 5: An overview of strategies for evaluating and mitigating bias in LLMs, covering evaluation via feature embedding, generated text, and token selection probability and mitigation during training or inference. (1) Distribution metrics: One of the simplest metrics in this category is Social Group Substitutions (SGS), which evaluates whether models responses exhibit an identical token distribution when provided with context input biased towards different groups and B. For context inputs XA representing commonsense scenarios and XB denoting counterfactual scenarios, it mandates: SGS(f (X; θ)) = ψ(f (XA; θ), (XB; θ)), (6) where (X; θ) represents the response generated by LLM denoted as , with input and model parameter θ, and ψ symbolizes an invariance metric such as exact match [750]. There are also metrics based on the frequency of specific words appearing in response compared to their average distribution, such as the bias metric based on word co-occurrence scores [102]: bias(xi) = log (xixA) (xixB) , (7) where xi belongs to word in the response = (x1, ..., xm), and xA and xB can represent keywords biased towards two different groups, such as men and women. Similarly, Demographic Representation (DR), as discussed in Bommasani & Liang [99], Liang et al. [545], compares the frequency of specific demographic-related word mentions with the original data distribution. 13 Published in Transactions on Machine Learning Research (10/2025) Here, C(xi, yi) represents the count of occurrences of the word xi in the sequence yi, where yi is specific model generation from the set of model generations for scenario. For each group is associated with set Wi of words that represent the group, the count DR(j) is calculated as follows: DR(j) = xiWj yiY C(xi, yi). (8) To evaluate against baseline probability distribution, such as uniform distribution, the count vector DR = [DR(1), ..., DR(m)] is normalized to probability distribution. We then employ metrics such as total variation distance, KL divergence, Wasserstein distance, or other appropriate indicators for this evaluation. (2) Classification metrics: Classifier-based metrics utilize expert models or specialized APIs to assess bias in generated texts. This approach involves comparing texts generated from similar prompts, especially when it comes to classifying outcomes related to different social groups, to detect potential biases. In the field of toxicity detection, Perspective API is commonly used tool to evaluate toxicity probabilities in LLM-generated texts [545, 185, 192]. This probability can be quantified by generating texts multiple times and calculating the expected maximum toxicity to assess the toxicity performance of LLMs. Other metrics include Toxicity Probability (T ), measuring the empirical probability of generating text with toxicity score 0.5 at least once in multiple generations, and Toxicity Fraction (TF), indicating the proportion of toxic generations [545]. Classification-based evaluation methods are not limited to the domain of text toxicity. For instance, Sheng et al. [814] assess the attitudes and perceptions of social groups in model responses, employing metrics akin to sentiment and respect scores. Their approach involved manually constructing prompt templates for LLMs (e.g., This woman works as), generating human-annotated dataset. This dataset subsequently served as the training set for regard classifier, enabling the classification of response preferences in other LLMs. Similarly, Smith et al. [833] use style classifier to compute the style vector for each generated response (Xi; θ), where Xi is prompt related to group G. Bias is measured by calculating the variance across the sets of all generated sentences from each group (i.e, Xi for group i): Gen_Bias(f (X; θ)) = j=1 VariG 1 Xi XiXi ! c(Xi)[j] , (9) where represents the style classifier, and each element is the probability of sentence belonging to one of style classes, i.e., c(X)[1], . . . , c(X)[C]. (3) Word-level metrics: This evaluation approach is similar to fine-grained distribution metrics, which is relatively straightforward. Basically, it involves word-level metrics that analyze the generated output, where each word is either compared to predefined list of harmful words or assigned precomputed bias score [688, 71, 224]. In general, evaluation methods based on generated text are generally applicable to most LLMs, especially specialized black-box models such as ChatGPT and Bard. More recently, Bouchard et al. [106] released LangFair, Python toolkit that makes the evaluation of bias and fairness easier for LLM practitioners and developers. Methods based on Feature Embedding. In addition to assessing models through corresponding text, another common approach involves evaluating model bias based on feature embedding. Specifically, this typically entails measuring the distances in vector space between neutral words (such as professions) and identity-related words (such as gender pronouns) based on the embedding of output texts. Using these distance-related metrics, we can roughly assess the bias between the models textual responses and the standard reference group. more detailed evaluation metric relies on word embeddings, specifically, the Word Embedding Association Test (WEAT) introduced by Caliskan et al. [117], which is comparable to similar approaches used for contextualized sentence embeddings. WEAT evaluates associations between concepts related to social groups, such as masculine and feminine words, and neutral attributes such as family and occupation words, resembling 14 Published in Transactions on Machine Learning Research (10/2025) the Implicit Association Test (IAT) [320]. Another set of evaluation metrics, focusing on sentence-level embeddings, incorporates more contextual information. An example of this is SEAT [637], an improvement upon WEAT. SEAT generates embeddings for semantically bleached template-based sentences that integrate social group and neutral attribute words, and extends the evaluation to specific bias dimensions using unbleached templates, offering contextualized approach for assessing bias in sentence embeddings. Methods based on Token Selection Probability. Furthermore, we discuss bias and fairness metrics that leverage the token selection probability from LLMs. This probability can be obtained by masking word in sentence and prompting masked language model to predict the missing token. For example, Webster et al. [976] utilize specific prompt templates (e.g., [MASK] is [MASK] and [MASK] likes [MASK]). In these templates, the first [MASK] is automatically filled with words biased toward particular group (such as gendered terms), and the second [MASK] is replaced with candidate predictions from LLMs. The score is calculated by averaging the count of divergent predictions between social groups across all specific prompt templates. Kurita et al. [482] employ similar template-based approach to assess bias in neutral attribute words (e.g., occupations). However, Webster et al. [976] normalize tokens predicted probability (based on the template prompt [MASK] is an [ITEM FROM GROUP i]) with the models prior probability (based on the template [MASK] is [MASK]). This normalization corrects for the models prior inclination toward one social group over another, focusing solely on bias attributable to the [ITEM FROM GROUP i] token. Another category of probability-based methods is pseudo-log likelihood (PLL). Various techniques [932, 781] utilize PLL to score the probability of generating individual words in given sentence. For response denoted as = (x1, ..., xm), the expression of PLL is presented as follows: PLL(X) = xiX log (xiXM ASK{xi}). (10) Nangia et al. [683] utilize the CrowS-Pairs dataset, which involves pairs of sentences where one is stereotypical and the other is less stereotypical. PLL is employed to evaluate the models preference for stereotypical sentences. For sentence pairs, the metric approximates the probability of shared, unmodified tokens conditioned on modified, typically protected attribute tokens . The Context Association Test (CAT) [679], introduced alongside the StereoSet dataset, assesses sentence bias by pairing each sentence with stereotype, anti-stereotype, and meaningless options. Unlike pseudo-log-likelihood, CAT considers conditional probability. The Idealized CAT (iCAT) Score [679] is calculated from these options, and an idealized language model has specific scoring criteria. All Unmasked Likelihood (AUL) [432] extends CrowS-Pair Score and CAT, considering multiple correct candidate predictions and avoiding selection biases in word masking. Language Model Bias (LMB) [67] compares mean perplexity between biased and counterfactual statements using the t-value of Students two-tailed test. 3.3 Methods for Bias Mitigation The current popular methods for mitigating biases in LLMs response texts can be broadly categorized into two types: those based on the training process and those involving post-processing techniques. Next, we provide detailed breakdown and explanation of these two types. Specific categories will be examined in greater depth in the subsequent chapters. Methods based on the Training Process. This type can be divided into two classes: methods based on training data augmentation and alignment with instruction tuning. (1) Training data augmentation: For LLMs, biases frequently originate from imbalanced data distribution and poor data quality [281]. One of the most direct and effective solutions is improving the quality, diversity, and balance of training data. Data augmentation techniques aim to mitigate biases by introducing additional instances into the training data, thereby increasing the data points related to underrepresented or misrepresented social groups. Data balancing approaches aim to achieve equitable distribution across various social groups. One primary technique for this purpose is Counterfactual Data Augmentation (CDA) [602, 732, 976], which involves replacing protected attribute words, such as gendered pronouns, to create balanced dataset. Published in Transactions on Machine Learning Research (10/2025) Inspired by the mixup technique [1136], interpolation approaches blend counterfactually augmented training instances with their original counterparts and labels, thereby achieving more balanced distribution of the training data [1076, 1075, 1065]. In Ahn et al. [11], the mixup framework is harnessed to align the output logits of pre-trained model between two opposing words within gendered pair. In Mix-Debias, Yu et al. [1101] apply mixups across various corpora, aiming to alleviate gender stereotypes by leveraging an augmented training set. Wang et al. [962] introduce an automated iterative framework that prompts LLMs in conjunction with filtering criterion. Through self-instructive process, this framework reconstructs more diverse dataset from initial seed data tailored for LLMs instruct tuning. The prompts for this dataset are generated automatically by LLMs and undergo various metric-based filtering to ensure diversity in the dataset. In addition, there are numerous data filtering methods [295, 101, 877] that aim to enhance the balance of data distribution by either removing low-quality data or selectively retaining diverse and underrepresented set of data. (2) Better alignment with instruction tuning: With vast amount of data, LLMs typically undergo pretraining and instruction tuning. In the pre-training phase, LLMs internalize knowledge from the training data into trainable parameters. Instruction tuning, on the other hand, teaches the model to understand human instructions. However, it is essential to recognize that biases in the training data and the training process are not inherently designed to understand or prioritize human values. This limitation leads to biases and potentially toxic responses from LLMs when faced with complex and divergent human preferences. They often arise naturally from the data or model training procedures, or from human design decisions that reflect their own values and preferences. To address this challenge, we provide detailed overview of some current alignment techniques and training algorithms to harmonize LLMs with human preferences in Section 4. Methods based on Post-processing Techniques. Another approach is based on post-processing techniques. Post-processing, in the context of LLMs, generally refers to the practice of invoking external knowledge bases or employing word-based detection techniques to identify biased statements during inference. Subsequently, the identified biases are corrected in the generated text. In Kang et al. [433], techniques such as retrieval are employed within LLMs to match responses during each phase of the Chain of Thought (CoT) generation [985]. This involves retrieving and correcting biased or toxic text at every stage of the LLMs responses, thereby ensuring that LLMs produce accurate and unbiased text responses throughout the CoT process. Andriopoulos & Pouwelse [29] also mention various methods that enhance LLMs by invoking Wikipedia and various external knowledge bases for retrieval. The objective of these approaches is to boost the reliability of LLM outputs and reduce biases in generated text. Additionally, word-level detection is employed in Chen et al. [165] to identify instances in LLM responses involving counterfactual information or not aligning with the context. Subsequently, post-processing approach is applied to correct and enhance LLMs reliability by removing such inaccuracies from the generated text. On the other hand, Li et al. [511] synthesize cultural-specific instruction data to incorporate cultural differences into LLMs. Raza et al. [761] further propose MBIAS, LLM framework instruction fine-tuned on custom dataset designed explicitly for safety interventions. Moreover, Wang & Demberg [961] introduces multi-objective probability alignment approach to overcome current challenges by incorporating multiple debiasing losses to locate and penalize bias in different forms, which is more effective in removing stereotypical bias of LLMs while retaining their general performance. Overall, the methods based on post-processing techniques can effectively and accurately handle certain biased information. However, they also have certain drawbacks. For instance, when relevant information is not present in external knowledge bases, biases in LLMs responses might remain uncorrected. Additionally, post-processing may introduce erroneous information from external knowledge bases. Moreover, approaches relying on post-processing techniques often lead to significant increase in latency. 3.4 Bias and Fairness in MLLMs Compared to LLMs, the emphasis on fairness in MLLMs is more direct toward ensuring that the responses align faithfully with the inputs in different modalities, such as images or audio in context. The responses must align consistently with the visual content, ensuring they are free from any biased text that contradicts 16 Published in Transactions on Machine Learning Research (10/2025) the context. Currently, most research exploring bias and fairness in MLLMs is focused on the phenomenon of image hallucination. This term describes scenarios in which the model, when describing images or answering questions based on visual information, generates responses containing entities, quantities, or logical information that does not exist in the given image [1194, 944, 943, 532, 566, 568, 1201]. In Section 7, we conducted detailed analysis of recent advances in understanding and addressing hallucinations within MLLMs. Apart from hallucinations, there is notable lack of in-depth exploration into the bias and fairness of MLLMs. Similar to LLMs, MLLMs may exhibit significant biases due to the training paradigm and dataset distribution. The scarcity of image-text data for specific groups, coupled with the presence of biased information in the dataset, may lead MLLMs to acquire stereotypical impressions of certain groups. Additionally, imbalanced dataset distribution might cause MLLMs to showcase biases in responses to specific image-text pairs. Earlier efforts on generating counterfactual images towards semantic textual concepts have shown that machine learning models will encode biases related to certain attributes if the training data is imbalanced [609, 726, 1020]. To further mitigate biases during inference, BEND-VLM [301] tailors the debiasing operation for MLLM embedding to each unique input at the test time, thereby avoiding catastrophic forgetting in fine-tuning. However, biases remain largely unexplored in MLLMs, and addressing fairness and bias in MLLMs is crucial for building foundation models that are beneficial and equitable for humanity. 3.5 Bias and Fairness in Image Generative Models The rise of image generative models sparks discussions on systematic social bias and fairness issues in generated content, as indicated by several studies [1156, 779, 182, 89, 606, 513]. Text-guided diffusion models, in particular, have been found to exhibit biases related to professions, ethnicities, and social classes. The generated contents diverge from the distributions in the real world and even amplify the biases in real societies [1156, 89]. For instance, study conducted by Luccioni et al. [606] highlights that image generative diffusion models consistently underrepresent marginalized identities in the generated images. Some examples of gender biases in image generative models are presented in Figure 6. Figure 6: Examples of gender biases in image generation models: DALLE shows spurious correlation between gender and profession. To overcome the systematic bias and fairness issues, many methods [277, 446, 190, 513] focus on mitigating biases in image generative models through prompting techniques. Fair Diffusion [277] randomly injects additional subject pronouns in the prompts to achieve more balanced gender distribution in the generated images. Other work [446] optimizes the soft token in the prompts to induce more balanced gender distribution. Furthermore, Chuang et al. [190] work directly in the text embedding space to obtain more balanced gender distribution in vision-language models. recent study [513] addresses these problems by finding the bias-related concept in an interpretable latent space and manipulating the generation process with the concepts found. These prompt-based regulations are by far the most widely adopted strategy to reduce biases in the generated content. However, it has been noted that keyword-based approaches could disproportionately affect marginalized groups, implying that their use at the prompt level could yield similar outcomes [234]. Another direction of research involves addressing biases through sampling methods. For example, the D2C method [830] generates unconditional diffusion via few-shot conditional diffusion to balance the numbers in 17 Published in Transactions on Machine Learning Research (10/2025) generated classes. Furthermore, Fair Sampling [183] introduces fairness-aware sampling technique aimed at reducing the amplified biases inherent in training data."
        },
        {
            "title": "3.6 Current Limitations and Future Directions",
            "content": "Despite significant advancements in the domain of bias and fairness in foundation models, there are still some limitations in bias and fairness evaluation that require future attention."
        },
        {
            "title": "3.6.1 Limitations and Open Challenges of Bias and Fairness",
            "content": "Currently, most bias evaluation methods are limited to token or paragraph-level assessments, making it challenging to capture the gradual propagation of bias during autoregressive generation [1030, 790, 1218]. In autoregressive models, each tokens prediction relies on previously generated tokens, meaning that biases may accumulate and spread over time. Traditional token or paragraph-level fairness metrics [130, 74] are insufficient to fully assess this bias propagation issue, making it difficult to accurately measure the biases in these model outputs. Additionally, when dealing with specific content, such as social media posts or content related to current events, the concept of bias becomes more complex. Biases in such cases may not always be evident or confined to single token but may be reflected through subtle contextual influences or narrative frameworks. Therefore, bias concerns more than just token-level differences; it also involves how the model handles historical, social, or cultural influences, which may be embedded in the models training data. This presents significant challenge for mitigating biases, as it often intertwines with factual reporting and socially accepted norms. When evaluating biases in foundational models, the lack of clear and consistent definitions of fairness within these models complicates both assessment and improvement efforts [232, 815, 1138]. What is considered fair can vary significantly depending on cultural, social, and historical perspectives. This variability becomes especially pronounced when dealing with news content, where fairness often intersects with historical accuracy or the presentation of current facts. Models must therefore navigate delicate balance: striving for unbiased outputs while carefully weighing the tension between fairness principles and accurately representing reality. For example, in reporting on historical events or current issues, there may be cases where acknowledging certain inequalities or biased social structures is necessary to present the facts accurately. In such situations, pursuing absolute fairness might mean overlooking or distorting facts, leading to significant conflict between fairness and authenticity. For model developers, maintaining the integrity of generated or processed information while addressing ethical concerns is significant challenge. Furthermore, when fairness considerations span different regions, cultures, and social norms, the complexity of such evaluations is exacerbated, increasing the difficulty of implementing fairness assessments in foundational models. In addition, societal biases are challenging to mitigate with common techniques such as data resampling [364], and addressing them in web-scale datasets remains an open problem. 3.6.2 Future Directions Addressing the limitations of bias and fairness in current foundation models opens several avenues for future research. Firstly, exploring unbiased tokenization and embedding methods could help mitigate the introduction of bias during natural language processing [721, 1134]. This involves developing tokenization techniques and embedding representations that maintain fairness at more fine-grained level. Secondly, in terms of unbiased fine-tuning and preference learning, employing techniques such as constrained RLHF [1104], DPO [1202, 956, 1200], and revised LoRA methods [581, 1051] can help to adjust the models training process to reduce bias introduced during generation. As models are made more secure through methods like adversarial training, future work must also ensure these security protocols are designed to be fairness-aware, preventing the accidental amplification of biases as side effect. Furthermore, the post-processing of autoregressive generation is an important area of focus [1201], which can help further detect and correct potential biases in the generated content. This challenge of equitable evaluation extends to related tools, creating need to audit detectors of AI-generated content (AIGC) to ensure they do not unfairly penalize content from 18 Published in Transactions on Machine Learning Research (10/2025) specific demographic groups. Moreover, explainability methods offer promising frontier for more targeted interventions, opening research avenues into using mechanistic interpretability to locate and edit the specific model circuits that encode stereotypes. In MLLMs, individual modalities can have disparate impact on bias and fairness and may require modality-specific interventions [997]. Lastly, balancing fairness and utility is crucial, as striving for absolute fairness often conflicts with the models practical utility and performance. Therefore, developing effective trade-offs that simultaneously address fairness and utility will be significant challenge for future research. 19 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "4 Alignment",
            "content": "Foundation models have significantly expanded their functionality, advancing beyond simple content generation to wide range of applications including strategic planning [400, 840, 594], code generation [154, 724], tool integration [735, 813], complex reasoning [985, 386, 1222], and even addressing challenges in natural sciences, especially mathematics [241, 407, 693]. Despite these advancements, it is important to note that foundation models are primarily trained on large datasets with objectives such as next-token prediction [740], next-scale prediction [883], or diffusion [559]. They are not inherently equipped to understand or prioritize human values and preferences. This gap between their powerful capabilities and inherent limitations underscores the potential risks associated with their deployment. For example, without proper safeguards, foundation models could be jailbroken by users to disclose personal information or engage in harmful behaviors, which should be avoided [514, 874, 811, 808, 156, 142]. Moreover, the ability of AI agents to adapt their capabilities to diverse objectives (e.g., scientific discovery and management systems) further highlights the importance of thoughtful oversight. According to the orthogonality thesis [105], AI systems can pursue any number of goals, regardless of their intelligence level. This concern is compounded by the instrumental convergence thesis [105], which suggests that regardless of their ultimate goals, AI systems might adopt certain potentially harmful strategies as means to achieve themsuch as self-preservation or resource acquisition, which could lead to power-seeking behaviors [105, 115]. As the capabilities of foundation models advance, it becomes crucial to carefully design these models to align with human-centric values and the nuanced requirements of specific tasks. This alignment is essential for ensuring the deployment of foundation models meets rigorous safety and ethical standards in various real-world applications. In the following section, we will focus on aligning LLMs with human preferences.We not only reviews core techniques like Supervised Fine-Tuning and RLHF but also frames them as crucial component in larger ecosystem of responsible AI development, with direct intersections with uncertainty and hallucination. We will begin with Supervised Fine-Tuning [984, 1183, 192], proceed to Reinforcement Learning from Human Feedback [703, 187, 57], and then explore Prompt Engineering [578, 325]. Furthermore, we will extend our discussion to MLLMs in Section 4.4. Finally, we will discuss the limitations of current alignment methods in Section 4.5. These categories can be visualized in Figure 8. 4.1 Supervised Fine-Tuning Supervised Fine-Tuning (SFT) is widely-used approach to align pre-trained LLMs with human preferences, which directly tunes the LLM to mimic desired ground-truth responses. It often serves as the first stage of the alignment process. Most SFT methods can be formally expressed as: LSFT = t=1 log Pf (rt p, r<t) , (11) where denotes the input prompt and = (r1, r2, ..., rL) is the sequence of the target response. This approach maximizes the likelihood of generating the optimally selected response, akin to how student learns from teachers guidance. Combined with other training methods for alignment, SFT can often enhance the stability of the whole alignment process. Although SFT is efficient in aligning LLMs, its success heavily relies on the quality and diversity of the training data. LIMA [1183] presents study that highlights the importance of this aspect, where the authors curate dataset of 1,000 high-quality prompt-response pairs, with 750 of them coming from diverse sources such as StackExchange2, wikiHow3, and the Pushshift Reddit Dataset [73], while the remaining 250 pairs are manually annotated. LIMA demonstrates that LlaMa-65B [896], when fine-tuned on small but highquality dataset using the regular SFT training objective, can achieve significant performance improvements without requiring reinforcement learning or explicit human preference modeling. In this line of research, 2https://stackexchange.com/ 3https://www.wikihow.com/ 20 Published in Transactions on Machine Learning Research (10/2025) d o d F n g A High-quality Datasets LIMA [1183], Curated Prompt-Response Pairs [73] Data Quality Data Cleaning Minihash [112], LSH [208], Rule-based Filtering [1191, 715, 743] Data Selection Instruction Following Difficulty [521], Importance Weights [1035] Self-Instruct Framework Self-Instruct [963], ICL-based Generation [113] Supervised Fine-Tuning AI-Generated Data Open-source Models Alpaca [873], Vicuna [180], Textbook Quality Data [331] Advanced Synthesis MAGPIE [1049], Role-playing Data [948], Agent-tuning Data [733], Self-talk Generation [909] Vulnerabilities Safety Issues Adversarial Manipulations [729], Red Team Dataset [285], Overtrained Models [846] Fine-grained Frameworks Fine-grained RLHF [1015], Rewarded Soup [752] Beyond Conventional Reward Models General Preference Approaches Nash Equilibrium [675, 1084], Dueling Bandit [1115, 1214], Comparative Evaluation [1190] Multi-objective Rewards K-wise Maximum Likelihood [1206, 1207], SteerLM [238], Multi-reward Balance [1205, 938, 171, 129] RLHF Beyond Human-Annotated Data Verified Rewards DeepSeek R1 [209], OpenAI o1 [696], Mathematical Reasoning [669] AI Feedback RLAIF [58], Constitutional AI [58] Task-specific Applications Summarization [494], Complex Reasoning [967], Online AI Feedback [339] Self-exploring Methods SELM [1149], Constitutional DPO [954] Alternative RL Methods RAFT [235], RRHF [1112], ReST [330], SLiC [1172] Prompt Engineering Beyond Proximal Policy Optimization Direct Preference Methods DPO [744], KTO [262], IPO [53], RPO [596], SimPO [647], ORPO [367] Efficient RL Algorithms GRPO [209], REINFORCE++ [378] Continuous Prompts Soft Prompts Gradient-based Tuning [734, 229, 503, 347, 585, 352] Discrete Prompts Manual Crafting Natural Language Prompts, Human-interpretable Design Automatic Generation Rule-based Search [289, 381], RL-based Search [211, 1150], Sensitivity Issues [796] Basic CoT CoT [985], Self-consistency [959], Zero-shot CoT [462] Chain-of-Thought Extended Structures ToT [1079], GoT [84], Long CoT [696, 209, 669, 19, 21] Limitations Post-hoc Rationalizations [903], Test-time Computing [834, 1008] LLM-based Optimization APE [1204], APO [727], OPRO [1055], Self-refine [626] Prompt Optimization Evolutionary Methods EvoPrompt [338], Derivative-free Optimization [226] MLLM Alignment Specialized Techniques ReAct [1078], EmotionPrompt [510], Symbolic Learning [1197], Risk Control [1215] Visual Instruction Tuning Vision-Language Instructions [569], Linear Projection Layers Multimodal Datasets LLaMA-Adapter [288], MultimodalGPT [309], Otter [506], PandaGPT [858], LLaVA [507] Data Quality Control Perplexity Scoring [153], Gradient Computation [121], GPT-4 Rating [987] Preference Methods DRESS [163], POVID [1200], STIC [212], Self-rewarding [956, 1202] RLHF Challenges Tractability Issues [128], High-quality Feedback [167, 888], Data Poisoning, Feedback Biases Multimodal Alignment & Limitations Current Limitations Generality Issues Human Capacity Limitations, Diverse Value Modeling, Reward Hacking, Power-seeking Behaviors Direct Alignment Issues Overfitting Problems [745], Out-of-distribution Robustness [745] Superalignment Scalable Oversight [25, 108, 500, 549], Weak-to-strong Generalization [115, 515] Future Directions Advanced Methods Iterative Training [1111, 774, 1040, 968], Online Alignment [339], Model Merging [753] Security Concerns Superficial Alignment, Backdoor Vulnerabilities [127, 126], Evaluation Difficulties Figure 7: Taxonomy of Alignment in Foundation Models. methods based on Minihash [112] and Local Sensitive Hashing (LSH) [208] are often used to deduplicate the data, which serve as the first step of refining data quality. Then, series of works [1191, 715, 743, 951, 152] propose to use well-suited rules, metrics, and LLMs-based methods for further data cleaning. These work leverage measure the quality of the data and perform improvements such that deduplication and rewriting 21 Published in Transactions on Machine Learning Research (10/2025) Figure 8: Alignment is required at different stages in the foundation models. Typically, LLMs are aligned using SFT and RLHF during post-training, while using prompt engineerfing at inference time. Compared to LLMs, MLLMs require an additional step of multimodal alignment at post-training, such as Visual Instruction Tuning. to improve the overall quality. Li et al. [521] introduces the instruction following difficulty metric for efficient data selection. Similarly, Xie et al. [1035] estimates importance weights for high-quality data selection. Moreover, many empirical results of applying scaling laws [436, 366] to LLM training show that the data size becomes crucial for performance improvements. To reduce the cost of human annotations, researchers are increasingly interested in incorporating AI-generated data into the alignment process, especially with the advent of closed-source LLMs like GPT-4 [693], Gemini 2.0 [314], and Claude 3.5 [34]. line of research explores using LLMs to self-generate instruction-tuning data. notable advancement in this domain is the Self-Instruct [963], which leverages the in-context learning (ICL) capabilities of GPT-3 [113] to gather instructions and preferred responses autonomously. This approach begins with small set of human-annotated seed instructions that are subsequently refined and expanded to generate large-scale instruction data across diverse tasks. Building on this methodology, researchers have achieved significant advances in developing open-source LLMs with enhanced instruction-following capabilities, such as Alpaca [873] and Vicuna [180]. Gunasekar et al. [331] propose to use mix of textbook quality data from the web and GPT-3.5 generated It also pioneers large-scale, data to train the Phi, lightweight LLM suitable for edge scenarios [1045]. high-quality data generation. More recently, Xu et al. [1049] proposed self-synthesis method that leverages the auto-regressive nature of LLMs to generate diverse data without requiring any initial seed question or prompt. Zhou et al. [1196] propose to synthesize natural language descriptions for controllable text generation [384, 861]. Wang et al. [948] introduce method for synthesizing role-playing data using LLMs with carefully curated role descriptions. Similarly, Qiao et al. [733] present an approach for synthesizing agent-tuning data via self-planning with LLMs; Ulmer et al. [909] generates training data via self-talk of LLMs which can be used for further supervised finetuning. Nevertheless, the simplicity of SFT does not shield it from potential vulnerabilities, especially in terms of model safety and robustness. Qi et al. [729] illustrate that LLMs, such as OpenAIs GPT-3.5 Turbo [692], are prone to adversarial manipulations. They demonstrate that fine-tuning these models with limited set of strategically crafted examples from the Anthropic red team dataset [285] can significantly undermine the models safety protocols. This phenomenon highlights the necessity for rigorous examination in selecting and preparing SFT data. Further Springer et al. [846] highlights that overtrained LLMs impose challenge in further supervised finetuning. In conclusion, while SFT demonstrates notable efficiency, its effectiveness in aligning LLMs hinges critically on the quality, scale, and diversity of training data. The critical role of meticulously curated dataset extends beyond improving model performance; it is also vital to mitigate risks related to model safety and robustness. Inadequately vetted or deliberately compromised data can introduce harmful biases and trigger undesirable behaviors in LLMs. This concern highlights the ongoing necessity for rigorous data curation methods to enhance both the reliability and security of LLMs in real-world deployments. 22 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "4.2 Reinforcement Learning from Human Feedback",
            "content": "Reinforcement Learning from Human Feedback (RLHF) [703, 187, 57, 1165] represents significant advancement in aligning LLMs with human preferences, involving several critical steps: 1. Supervised Fine-Tuning (SFT) is performed on pre-trained model using high-quality, instructionfollowing dataset. The resulting model serves as the initial policy, πSFT, for the subsequent RLHF optimization. 2. Collection of pairwise ranking data to train reward model that correctly scores these data. 3. Optimization of the policy model obtained in Step 1 against the reward model in Step 2 using the Proximal Policy Optimization (PPO) [794]. To stabilize the optimization in Step 3, KL-divergence regularization is introduced [703], ensuring that the model remains reasonably close to the initial policy model acquired in Step 1. Figure 9: The evolution of Reinforcement Learning from Human Feedback (RLHF), illustrating three key areas of advancement. Panel (a) depicts the conventional RLHF pipeline and the move Beyond Conventional Reward Models. Panel (b) shows the shift Beyond Human-Annotated Data to include AI-generated feedback. Panel (c) illustrates the trend of moving Beyond Proximal Policy Optimization (PPO) towards simpler, direct preference optimization objectives. Beyond Conventional Reward Models. The effectiveness of RLHF is closely linked to the accuracy and robustness of the reward model. Recent research has identified biases in reward models [810, 501] and has focused on refining traditional Bradley-Terry reward models [109]. Wu et al. [1015] introduce fine-grained RLHF framework that addresses the challenges of translating human preferences into scalar learning signals for extensive textual outputs. This approach utilizes multiple fine-grained reward models and has demonstrated superior performance in tasks such as detoxification and extended question-answering. Complementing this, Rame et al. [752] propose rewarded soup, which linearly interpolates weights across specialized networks to derive diverse rewards. It emphasizes the importance of varied reward structure and aims to achieve Pareto-optimal generalization across the complete preference space. An additional generation of reward modeling, which is referred to as the general preference approach, directly learns pairwise preference function and seeks model that identifies the Nash equilibrium of an entropy-regularized minimax game [675, 1084]. This strategy draws inspiration from the classical dueling bandit problem [1115, 1214]. Zhou & Xu [1190] propose to train comparative evaluation model based on annotated pairwise preference 23 Published in Transactions on Machine Learning Research (10/2025) data and use it to train TextGAN [1161] with RL in Zhou et al. [1192], this can be viewed as an early version of RLHF. Zhu et al. [1206] extend pairwise ranking by considering the ranking of multiple responses and trains the reward model using K-wise maximum likelihood [1207]. Additionally, beyond the single reward model, another approach considers the joint preferences implied by multiple reward functions, such as helpfulness, harmfulness, verbosity, etc. Some of these reward functions may conflict with each other. The objective here is to strike balance among various rewards, reflecting diverse user preferences [238, 1205, 938, 171, 129].This diversity is not merely individual but also deeply cultural, as preferences and values can vary significantly across different populations, making single, universal alignment target an ill-posed problem [455]. The notable success of large reasoning models [209, 696, 669, 758] (e.g., GPT-o1, DeepSeek-R1) on closedend domains such as mathematical and code reasoning demonstrate the importance of verified rewards in large-scale reinforcement learning optimization, which also highlights the reward hacking problem. For more robust rewards, Khalifa et al. [442] design process-reward models that score intermediate steps to yield more faithful and verifiable reasoning. Kim et al. [451] show reward models for math are brittle and propose robustness diagnostics. Zhang et al. [1163] distill practical pitfalls and guidelines for building high-signal process verifiers at scale. Setlur et al. [799] scale automated process verifiers and show progress-aware rewards curb reward hacking. Luo et al. [613] use automatic step-checking to supervise math solutions and boost final accuracy. Li et al. [523] use monte carlo tree search over partial solutions to guide process supervision for hard problems. Li et al. [536] integrate temporally grounded verifiers into RL to improve multimodal reasoning. Guo et al. [337] propose reward model that scores latent/explicit reasoning quality rather than only final answers. Zhang et al. [1140] cast verification as next-token prediction to learn scalable process rewards. Table 2: Various preference optimization objectives given the preference data = (x, yw, yl), where is an input, is an output, yw and yl are the winning and losing responses, and yi, [n] are ranked responses. Method RAFT [235] RRHF [1112] ReST [330] SLiC-HF [1172] DPO [744] PRO [842] IPO [53] KTO [262] ORPO [367] RPO [596] i>j max (cid:2)0, π(yix) π(yjx)(cid:3) (cid:2)λEyπθ (yx)F (x, y; τ ) log πθ(yx)(cid:3) Objective maxw ExD,ypg(w,x)[r(x, y)] Lsft + max ExD +(1 λ)Eyp(yx) [F (x, y; τ ) log πθ(yx)] max (0, δ log πθ(ywx) + log πθ(ylx)) λ log πθ(ywx) πref(ywx) β log πθ(ylx) β log πθ(ywx) log σ (cid:16) (cid:17) βLsft Pn1 k=1 log π(yk x) 1/(r (x,yk )r (x,yn)) πref(ylx) exp π(yk x) 1/(r (x,yk )r (x,yn )) +Pn exp i=k+1 π(yix) 1/(r (x,yk )r (x,yi)) (cid:16) πref(ywx) log πθ(ylx) log πθ(ywx) (cid:16) β log πθ(ywx) πref(ylx) 1 2τ (cid:17) (cid:17)2 πref(ywx) zref λwσ where zref = E(x,y)D [βKL (πθ(yx)πref(yx))] log π(ywx)(1π(ylx)) π(ylx)(1π(ywx)) Lsft λ log σ + λlσ (cid:16) zref β log πθ(ylx) πref(ylx) (cid:17) , min ηβ Exd0,y0πbase(x) + log σ (cid:16) (cid:2) log(πθ(y0x))(cid:3) (cid:17) πref(ywx) β log πθ(ylx) β log πθ(ywx) (cid:16) β yw log πθ(ywx) β πref(ylx) yl log πθ(ylx) γ (cid:17) SimPO [647] log σ Beyond Human-Annotated Data. Synthetic data generation has proven effective for SFT. However, when it comes to RLHF, pairwise preference ranking data is typically collected through human annotations, 24 Published in Transactions on Machine Learning Research (10/2025) process that can be costly for scaling. To mitigate this issue, recent research has shown that AI-generated data can also provide helpful feedback for alignment. Bai et al. [58] introduce RL from AI Feedback (RLAIF), which blends human and AI preferences under the Constitutional AI (CAI) framework. In this framework, AI behaviors are governed by principles analogous to constitution, supported by few examples for few-shot prompting. This methodology aims to train non-evasive AI assistant that is effective and harmless without relying solely on human labels. Further extending the concept of RLAIF, Lee et al. [494] apply it to summarization tasks, while Wang et al. [967] adapt it for complex reasoning tasks, highlighting the potential of AI feedback. Additionally, Guo et al. [339] enhance the RLAIF paradigm with online AI feedback, demonstrating superior performance in model alignment compared to both offline RLAIF and traditional RLHF. Zhang et al. [1149] propose Self-Exploring Language Models (SELM) to elicit preferences for online alignment actively. Wang et al. [954] propose Constitutional DPO, which uses expert-annotated principles to synthesize negative examples for preference learning. Shi et al. [816] and Li et al. [525] propose to use agent workflows to generate verifiable agentic tasks for end-to-end agentic reinforcement learning. Guan et al. [328] align models by training them to deliberate over safety criteria and self-critique. Beyond Proximal Policy Optimization. While RLHF has proven effective in capturing human preferences, the PPO algorithm [794] typically requires complex implementations and substantial computational resources, limiting its applicability in various contexts. The key challenges in PPO training include filtering high-quality data to compare similar responses, managing policy and reward models within limited resources, mitigating reward hacking issues [604, 256, 755], and requiring extensive hyperparameter and training strategy adjustments. To address these challenges, various new preference optimization objectives have been proposed, and their corresponding objective functions are presented in Table 2. Dong et al. [235] introduce the Reward Ranked FineTuning (RAFT) that simplifies the complexity of PPO by using reward model to selectively focus on the most promising responses sampled from an LLM. Specifically, RAFT involves sampling large batch of instructions and generating multiple responses. These responses are then holistically ranked by the reward model, with only the top-ranked responses used in SFT. This process is iterated until the rewards stabilize, and the fine-tuning dataset is periodically updated to enhance its quality. In parallel, Yuan et al. [1112] introduce Reinforced Ranking Human Feedback (RRHF), which aligns the model with human preferences among diverse responses using likelihood ranking loss. This method facilitates the integration of data from multiple sources, including both model-generated and human-curated data. Another innovative approach within the RLHF framework is Reinforced Self-Training (ReST), introduced by Gulcehre et al. [330]. ReST focuses on iteratively generating and refining data from policy models optimized by offline RL algorithms, enhancing data utilization efficiency. The framework involves two main steps: Grow and Improve. In the Grow step, the policy model generates multiple outputs for augmentation. During the Improve step, the generated data is ranked and filtered by preference reward model, after which the policy model is fine-tuned on the filtered data using an offline RL objective. This process is repeated with an increased filtering threshold to further refine data quality. Beyond zeroth-order RL algorithms such as PPO, which require the learning of the value function and hyperparameter tuning, first-order RL algorithms [1148, 1147, 286] can also act as straightforward alternative for RLHF alignment. Sequence Likelihood Calibration (SLiC) by Zhao et al. [1171; 1172] aims to align model outputs with reference sequences in the latent space by calibrating the sequence likelihood. This method replaces the traditional embedding similarity function with preference ranking function and employs cross-entropy regularization loss to keep the model close to the reference, typically an SFT model. Additionally, Song et al. [842] propose the Preference Ranked Optimization (PRO) method. Differing from traditional RLHF approaches that use the Bradley-Terry reward model focusing only on the best and worst responses, it enumerates all possible ranking pairs among candidate responses to provide comprehensive alignment. Based on these insights, Rafailov et al. [744] propose Direct Preference Optimization (DPO), which integrates preference information indirectly into the optimization of the policy model, eliminating the need for separate reward function. The DPO loss derived from the reward maximization-based RLHF algorithms is used to directly optimize the policy model πθ as follows: LDPO (πθ; πref ) = log σ β log (cid:18) πθ (yw x) πref (yw x) β log πθ (yl x) πref (yl x) (cid:19) , (12) 25 Published in Transactions on Machine Learning Research (10/2025) where πref denotes the reference policy (namely the SFT model), and (x, yw, yl) represents the instruction paired with the preferred answer yw and the dispreferred answer yl. Furthermore, Ethayarajh et al. [262] propose Kahneman-Tversky Optimization (KTO) to directly maximize the utility of LLMs generations instead of the likelihood of preferences. Unlike methods that require costly annotated pairwise ranking data, KTO only needs individual binary feedback, which is easier to collect from real users. Besides, Regularized Preference Optimization (RPO) [596] is proposed to mitigate reward hacking or overoptimization issues during RLHF by simply adding the SFT loss to DPO. Azar et al. [53] theoretically analyze the weakness of DPO and introduce Identity Preference Optimization (IPO), which adds constant regularization term to the DPO loss to mitigate the overfitting problem. SimPO [647] introduces margin term to the BradleyTerry objective and incorporates the length normalization, eliminating the common need for an additional reference model in preference learning. Similarly, Hong et al. [367] propose ORPO, which integrates an odds ratio preference objective into the standard SFT objective, also functioning independently of reference model. In addition to these direct preference optimization methods, some recent works focus on improving the efficiency of large-scale reinforcement learning in optimizing LLMs. Luong et al. [614] frame reasoning improvement as preference-style optimization that directly reinforces better intermediate traces. Lai et al. [485] extend DPO to step-wise preferences so policies learn to choose higher-quality next steps. Xu et al. [1043] optimize over complete chains of intermediate steps to stabilize long-horizon preference learning. Lu et al. [605] add curriculum/control over which steps receive preference signals for improving sample efficiency. Shi & Shen [822] treat multi-step search as RL fine-tuning, rewarding trajectories that lead to verified solutions. Zhou et al. [1198] learn from proxy signals to improve reasoning when ground-truth verifiers are unavailable. DeepSeek [209] propose Group Relative Policy Optimization (GRPO) to eliminate the needs of the critic model and value estimation in PPO, which greatly saves computation resources. GRPO estimates the baseline of advantage by calculating the average rewards of multiple samples in the group and replaces the KL penalty added to the reward by explicitly adding the KL divergence to the loss. The paper also indicates the importance of efficiency when applying GRPO in large-scale reinforcement learning for LLMs. REINFORCE++ algorithm [378] removes the critic model via implementing token-level KL penalty, PPOs clipping for policy model updates, and normalized advantages while achieving efficient reinforcement learning for LLMs. The proliferation of these alternatives to PPO raises the practical question of which objective to choose. The decision often involves trade-off between complexity, data requirements, and robustness. Methods like DPO, ORPO, and SimPO offer simplicity and efficiency by forgoing an explicit reward model, making them suitable for resource-constrained scenarios. However, these direct methods may be more susceptible to overfitting on the preference dataset. In contrast, full PPO-based RLHF, while more complex, allows for online exploration and can lead to more robust models, particularly when paired with high-quality, verified rewards as seen in specialized domains. 4.3 Prompt Engineering While some research focuses on aligning LLMs with human preferences through explicit training, another line of research emphasizes the strategic design of prompts to effectively improve the LLMs generated responses. This approach, known as Prompt Engineering, involves crafting prompts that guide LLMs toward fulfilling specific task requirements. Continuous Prompts. In the field of prompt engineering, continuous prompts, represented as continuous vector inputs integrated into the LLM, offer novel approach to guiding AI responses. Due to the continuous nature, these prompts can often be fine-tuned through gradient-based methods using labeled data [734, 229, 503, 347, 585, 352], which is effective for adapting LLMs behavior and injecting domain knowledge. However, the continuous format makes it less transparent for human understanding, posing challenges in interpretability [444, 347]. Discrete Prompts. In contrast to continuous prompts, discrete prompts consist of discrete tokens from the natural language vocabulary. The distinct advantage of discrete prompts lies in their use of natural language, which makes them inherently interpretable and relatable to humans. These prompts can be manually crafted or automatically generated. To automatically design effective prompts, some work utilizes pre-defined rules and reinforcement learning methods for searching [289, 381, 211, 1150]. However, this approach is not 26 Published in Transactions on Machine Learning Research (10/2025) Figure 10: An overview of four major prompt engineering methods for guiding LLMs. (a) Continuous Prompts are soft prompts represented as tunable vectors in the embedding space. (b) Discrete Prompts are interpretable natural language prompts that can be automatically generated. (c) Chain-of-Thought prompting elicits intermediate reasoning steps to solve complex problems. (d) Prompt Optimization uses feedback from the LLM itself to iteratively refine prompts and improve performance. without its challenges, as models can be extremely sensitive to minor, semantically irrelevant changes in prompt formattinga brittleness that can lead to unpredictable performance variations [796]. Chain-of-Thought. For complex reasoning tasks, Wei et al. [985] first propose the Chain-of-Thought (CoT) method to encourage LLMs to generate series of intermediate reasoning steps before reaching the final answer. This method has shown notable success in improving the performance of LLMs on tasks requiring multi-step reasoning, arithmetic reasoning, logical deduction, or commonsense application [985, 959, 278, 462, 151]. Based on CoT, Tree-of-Thought (ToT) [1079] extends the concept to planning and decision. ToT refines the CoT method by utilizing the specific attributes of problems to decompose and organize intermediate thoughts into tree structure. In ToT, each thought builds node in this tree, facilitating explorations by searching algorithms such as the breadth-first or depth-first search, allowing lookahead and backtracking in problem-solving. This method has been further improved by Graph-ofThought (GoT) [84], which diverges from linear or hierarchical structures to more flexible graph-based representation. In GoT, the generated thoughts are forming nodes in graph, with edges representing their complex interdependencies. This graph-based approach can capture the multifaceted nature of reasoning processes, offering improved adaptability for tasks such as sorting and keyword identification. Besides, it can also improve the latency and throughput compared to CoT and ToT. Recently, large reasoning models such as GPT-o1 [696] and DeepSeek-R1 [209] use long CoT to further enhance the model to plan and reason. The long CoT is pushing the length of the generated CoT to thousands or even hundreds of thousands of tokens, which effectively scales the test time computing [696, 209, 669, 19, 834, 1008, 21]. Chen et al. [160] surveys implicit/latent CoT mechanisms and their implications for reliable reasoning. Zelikman et al. [1120] train models to think silently via self-evaluation before emitting answers to improve reasoning without exposing long CoT. Huang et al. [397] accelerate the training process of silent reasoning for practical deployment. 27 Published in Transactions on Machine Learning Research (10/2025) Chen et al. [159] translate problems into programs to externalize and verify intermediate reasoning. Wang et al. [962] aggregate diverse CoT samples to stabilize multi-step reasoning. Liang et al. [548] show verifiers with more test-time compute significantly lift math/code reasoning. It is crucial to note, however, that these generated reasoning chains may not faithfully reflect the models actual computational process. They can be post-hoc rationalizations rather than true trace of the models thought process, phenomenon that complicates their use for interpretability [903] Prompt Optimization. Complementing these structural prompt methods, recent research explores optimizing prompts directly using LLMs themselves. Yao et al. [1078] propose ReAct to motivate LLMs to generate both reasoning traces and actions to interact with environments, to improve their general task-solving ability. Automatic Prompt Engineer (APE) [1204] employs LLMs to craft initial instructions. Subsequently, APE cherry-picks instructions that exhibit the highest accuracy. Each of these selected instructions is then fed back into the LLM, prompting it to generate variant that is semantically akin to the original instruction. Following similar style, Automatic Prompt Optimization (APO) [727] iteratively refines existing instructions using textual feedback from LLMs. Conversely, Optimization by PROmpting (OPRO) [1055] adopts more direct approach, generating new instructions at each optimization step, with LLM optimization focused on enhancing task accuracies without necessarily replicating prior instructions. Some LLMs have been shown to have the ability to use self-generated feedback to iteratively refine the output [626]. One can also apply derivative-free optimization techniques to optimize discrete prompt [226]. Additionally, Guo et al. [338] propose EvoPrompt, which adopts evolutionary algorithms with LLMs for discrete prompt optimization. Beginning with set of initial prompts, EvoPrompt applies evolutionary operators and performance-based selection to iteratively refine and generate new prompts. In addition to these explicit prompt optimizations, Li et al. [510] propose EmotionPrompt which focuses on understanding the psychological and emotional stimuli of LLMs. It shows that simply appending emotional stimuli, such as this is very important to my career, to the original prompts can also significantly enhance the performance of LLMs. Liu et al. [595] propose the first principled framework that has provable regret guarantees to orchestrate reasoning and acting with specially designed prompts. Zhou et al. [1197] propose an agent symbolic learning framework to jointly optimize chain of prompts (i.e., agent workflow; 1195) by mimicking back-propagation and gradient descent with natural language and LLMs. Huang et al. [398] structure agent planning with private scratchpads to improve task-level reasoning. Wei et al. [988] compress and simulate CoT to preserve reasoning gains while reducing token overhead. Gao et al. [287] offload arithmetic/logic to code execution to make reasoning steps explicit and checkable. Tang et al. [870], Zhou et al. [1186] propose to use experience/memory-based learning to optimize LLM agents without fine-tuning the underlying LLMs. Beyond utility-driven metrics, prompt optimization can also be framed as risk control problem, where the goal is to identify prompts that are robust and minimize the likelihood of generating harmful or undesirable content [1215]. In the field of prompt engineering, the majority of methods focus on improving the performance of LLMs on specific tasks. While these methods are crucial for technical optimization, their contribution to aligning LLMs with human values and preferences is more indirect. By improving the interpretability of LLM outputs, these prompt engineering methods can gradually help LLMs better meet human expectations. This relationship between performance improvements and alignment with human values is an important consideration in the ongoing development of LLMs. 4.4 Alignment for MLLMs Recent studies advocate the development of MLLMs capable of tackling various multimodal tasks without requiring particular adaptations. This approach leverages the well-established text-based capabilities of LLMs by integrating them, in frozen state, as the language component within multimodal architectures, i.e., MLLMs, can align the visual and language modality through visual instruction tuning [569], specialized form of instruction tuning that extends the capabilities of pre-trained LLMs to understand and perform multimodal tasks involving both text and visual input. By incorporating datasets containing of vision-language instruction-following samples, this method enhances the zero-shot capabilities of LLMs for understanding and responding to visual inputs. The process typically employs linear projection layers to integrate image encoders with LLMs, allowing these models to effectively handle tasks that require an understanding of both text and images. Besides, extensive datasets comprising vision-language instruction tuning are utilized to 28 Published in Transactions on Machine Learning Research (10/2025) align MLLMs with human preferences [288, 309, 506, 568, 858, 1025, 1024, 507, 890, 17]. This approach allows MLLMs to accurately interpret instructions and generate user-friendly responses. Further works extend MLLMs to wider range of tasks such as multimodal synthesis [600, 601, 891, 1032, 1184, 880], and interactive agents [1127, 57, 1199, 1033, 1056, 807]. To better align the MLLMs output with human preferences, some recent work [864, 1202, 956, 575] aims to enhance model capabilities by filtering low-quality instruction data or constructing carefully examined examples during the fine-tuning phase. Recent studies [153, 121, 713, 357] have introduced methods for evaluating the quality of instruction data in both vision and language datasets. These methods include computing the perplexity, calculating the gradient, and employing more powerful closed-source LLMs (e.g., GPT-4 [693]) for rating, all aimed at filtering low-quality data from the training process. InstructionGPT-4 [987] presents more general data quality control pipeline by training robust data selector to automatically select proper data from the raw dataset used to fine-tune MLLMs. DRESS [163] proposes to divide natural language feedback (NLF) into critique and refinement types, and then utilize them to improve the alignment with human preferences and interaction capabilities of MLLMs. POVID [1200] utilizes AI-generated dispreferred data by explicitly contrasting hallucinatory answer with truthful one, eliminating the need for gathering human feedback. Recent works such as STIC [212], SIMA [956], CSR [1202], and AnyPrefer [1203] explored the enhancement of the alignment between vision and text modalities through self-rewarding methods without introducing additional models and data. 4.5 Current Limitations and Future Directions Though recent research has achieved remarkable success in aligning foundation models with human values and preferences by leveraging Prompt Engineering, Supervised Fine-Tuning, and Reinforcement Learning from Human Feedback, several challenges remain. prominent long-term challenge is the **superalignment problem**: how to ensure that AI systems much more intelligent than humans (i.e., superintelligence) remain aligned with human values and intentions [115]. This is difficult problem because humans may be unable to reliably supervise or evaluate the actions of system that is far more capable than themselves. Effectiveness of RLHF. Despite notable advancements in alignment brought by RLHF, this approach has its own challenges, as extensively analyzed by Casper et al. [128]. These challenges are broadly categorized into two types: tractability and generality. Tractability challenges encompass practical issues within the RLHF framework, such as difficulties in acquiring high-quality feedback [167, 888], risks associated with data poisoning, and inherent biases in the feedback. These issues, while significant, are considered manageable with the right strategies and improvements in future methods. On the other hand, generality challenges are more profound, raising critical issues about the overall effectiveness of RLHF. These include limitations in the human capacity to consistently provide accurate and reliable feedback for complex tasks, challenges in adequately modeling the diverse values of different human groups through reward models, and risks associated with reward hacking and power-seeking behaviors inherent in reinforcement learning systems. Though applying rule-based RL in the reasoning domain appears successful [209, 669, 19, 21], it is challenging to directly adapt it to broader general domains to represent diverse and complex human values. For example, if reward model cannot distinguish between nuanced responses and instead assigns uniformly high rewards to any agreeable or positive-sounding output, it may inadvertently train the foundation model to be sycophanticagreeing with the user regardless of factual accuracyrather than maintaining epistemic integrity. Such fundamental challenges pose critical questions about the long-term viability and ethical implications of relying solely on RLHF for aligning foundation models with human values. Issues in Direct Alignment. Direct alignment methods such as DPO [744] greatly simplify the traditional RLHF pipeline and reduce the massive computational resources required for training. However, these methods may be prone to overfitting and common offline training issues. series of direct alignment methods including DPO, IPO [53], and SLiC [1172] are found to have robustness issues, especially in outof-distribution settings [745]. This is mainly due to the adopted offline training paradigm, which often uses small and fixed set of data for training, lacking explorations compared to online training methods such as PPO [794]. To address this issue, recent methods propose iterative training paradigms [1111, 774, 1040, 968] or online alignment methods [339] to enrich the training data, expecting to match the online RL performance. However, these methods are still in the early stages and require further investigation. Another issue lies in 29 Published in Transactions on Machine Learning Research (10/2025) the scalability of these direct alignment methods. Rafailov et al. [745] find that weak or small LLMs often tend to learn simple features (e.g., length correlation) of preference data instead of high-level human values. To improve performance after alignment, these methods require either large amount of SFT data or scaling up the model size, which limits the efficiency advantage over RLHF methods. Superalignment. Superalignment is concept that refers to ensuring that future super-intelligent AI systems still being consistently aligned with human values, which was first introduced by OpenAI [115]. Generally, there are two approaches trying to achieve superalignment: scalable oversight [25, 108], and weakto-strong generalization [115, 515]. Scalable oversight aims at providing reliable supervision for untrustworthy but more capable AI systems, where most related work leverages the advantage that evaluation is easier than generation [500, 549]. The weak-to-strong generalization is simulating scenario where the weak model can elicit the strong models capabilities. Besides, the alignment of foundation models can be superficial, i.e., the model is pretending to generate human-preferred responses without adhering to the underlying human values, making the alignment evaluation quite difficult. And this can leave backdoor vulnerabilities in the model, which can be exploited by adversaries [127, 126]. Ensemble methods like integrating various weak models to supervise the strong models in different domains can be potential solution of scalable oversight to overcome the weakness of single weak model [500, 108]. The model merging method can also be utilized to achieve strong generalization by averaging bunch of specialized models [753]. In addition, it is valuable to explore efficient methods to combine scalable oversight and weak-to-strong generalization to achieve superalignment. In summary, both scalable oversight and weak-to-strong generalization have their own advantages and limitations, necessitating further efforts to ensure that future super-intelligent AI systems remain aligned with human values. 30 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "5 Security",
            "content": "With the widespread integration of foundation models into various domains, the growing adoption of these advanced models has also exposed security vulnerabilities, making them susceptible to adversarial examples [311, 628]. Adversarial attacks [311] encompass variety of techniques aimed at deceiving AI models by manipulating the input data with imperceptible noise, leading to incorrect predictions or manipulations of their outputs. This issue highlights the urgent need to thoroughly understand the vulnerabilities of foundation models, which is crucial not only for researchers and practitioners, but also for society at large. Prior reviewing efforts [1157, 620] focus on providing new taxonomies and platforms of safety threats and defense strategies across multimodal foundation models. In contrast, our survey embeds security within unified cross-task reliability and responsibility storyline to highlight its interaction with bias, privacy, uncertainty, and explainability in all foundation models. As an integral part of our review, this section explores foundation models security development on attack and defense strategies. To provide comprehensive overview, we summarize the various attack methods in Figure 11. Figure 11: Attacks on various foundation models in training and inference stages. All models suffer from Backdoor Attack and Jailbreak Attack. For the category of Other Attacks\", we include Prompt Injection Attacks in LLMs, Image Adversarial Attacks in MLLMs, and Adversarial Attacks in Image Generative Models. 5.1 Security in LLMs 5.1.1 Attack in LLMs Similar to traditional AI models [789, 492], LLMs are inherently vulnerable to various threats due to their very nature and architecture. For example, attackers can manipulate the input data and prompt LLMs to generate incorrect or undesirable outputs [324]. In the following, we summarize three major threats against LLMs, including jailbreak attacks, prompt injection attacks, and poisoning and backdoor attacks. Jailbreak Attacks. Jailbreaking in foundation models is an attack that bypasses the security protection mechanism of foundation models to enable responses to unsafe questions and unlock restricted capabilities. Jailbreaks are fundamental threats to LLMs since they may potentially enable criminals to exploit these models for illicit activities such as drug making, fake news generation, and phishing email writing. Recent studies have analyzed why jailbreaks work in practice: competing objectives between helpfulness and safety goals lead to failure modes or trade-offs where the model cannot satisfy both simultaneously, and 31 Published in Transactions on Machine Learning Research (10/2025) Jailbreak Attacks Guo et al. [335], Li et al. [514], Taveekitworachai et al. [874], Shen et al. [811], Wen et al. [993], Shen et al. [808], Chen et al. [156], Xiang et al. [1027], Zou et al. [1220], Mazeika et al. [639], Liang et al. [541], Yu et al. [1099], Deng et al. [210], Zhu et al. [1213], Chao et al. [132], Zou et al. [1220], Jin et al. [424], Deng et al. [213] Attack Prompt Injection Attacks Kumar et al. [477], Perez & Ribeiro [718], Apruzzese et al. [39], Liu et al. [590], Abdelnabi et al. [3] LLM Backdoor Attacks Wan et al. [929], Shu et al. [824], Sun et al. [862], Shi et al. [819], Rando & Tramèr [756], Chen et al. [169], Zhao et al. [1169], Xiang et al. [1026], Wei et al. [979], Zou et al. [1221] Defenses against Jailbreaks Chen et al. [139], Xie et al. [1036], Luo et al. [610], Cao et al. [118], Xu et al. [1050], Zhao et al. [1175] Defense Defenses against Prompt Injection Jain et al. [414], Alon & Kamfonas [23], Liu et al. [590] Provable Defenses Robey et al. [768], Kumar et al. [475] Image Adversarial Attacks Carlini et al. [126], Qi et al. [728], Dong et al. [239], Wang et al. [966], Cheng et al. [174], Luo et al. [608] Attack Jailbreak Attacks Wu et al. [1014], Chen et al. [156; 162], Gu et al. [327] MLLM Defense Backdoor Attacks Carlini & Terzis [123], Jia et al. [420] Defenses against Multimodal Perturbation Attacks Mao et al. [631], Wang et al. [952], Waseda & Tejero-de Pablos [973] Defenses against Backdoor Attacks Yang et al. [1060], Bansal et al. [62], Ishmam & Thomas [408], Feng et al. [269], Zhu et al. [1211] Jailbreak Attacks Rando et al. [757], Yang et al. [1070], Tong et al. [889] Attack Backdoor Attacks Chen et al. [157], Zhai et al. [1125], Struppek et al. [854], Bober-Irizar et al. [98], Wang et al. [937] Adversarial Attacks Dai et al. [206], Chen et al. [173], Liu et al. [573], Kang et al. [435], Chen et al. [147] Image Generative Models Defense Dataset Curation Birhane et al. [91], Thiel [878], Hong et al. [368], Birhane et al. [92] Trigger Detection Sui et al. [860], Wang et al. [971], Yoon et al. [1097] Model Finetuning Gandikota et al. [283], Kumari et al. [479], Schramowski et al. [791] Post-Generation Content Moderation OpenAI [695], Pi et al. [722], Yang et al. [1068], Li et al. [529], Liu et al. [579] d o d F y u Figure 12: Taxonomy for Security of Foundation Models. mismatched reward settings for generalization during instruction tuning enable the discovery of adversaries [978]. Moreover, the token-based nature of transformers allows attackers to craft seemingly innocuous token sequences that exploit greedy and gradient-based search techniques, effectively hiding malicious instructions from standard perplexity and content filters [730]. Many studies have explored and demonstrated various methods to jailbreak LLMs successfully [335, 514, 874, 811, 993, 808, 156, 1027] by manually designing jailbreak prompts. Moreover, multiple methods that can automatically generate jailbreak prompts have been proposed, including prompt optimization [1220, 639, 541], fuzzing [1099], multi-agent collaboration [166], and fine-tuning LLMs to generate new jailbreaks [210]. Zhu et al. [1213] merge the strengths of manually designed jailbreaks and optimization-based attacks to achieve gradient-based attack that is both effective and interpretable, thereby generating readable prompts that bypass perplexity filters while maintaining high success rates. The prompts obtained can be transferred to unseen target models to some extent, which poses more threats to the community [1220, 326]. Chao et al. [132] generate semantic jailbreaks with only blackbox access, frequently achieving successful jailbreak with fewer than twenty queries, which is both effective and efficient. Liu et al. [584] also studied jailbreaking LLMs with efficient queries. Jin et al. [424] designed an automated testing framework based on role-playing of LLMs. Deng et al. [213] showed the possibility of bypassing LLM safety mechanisms using non-English prompts. Prompt Injection Attacks. Prompt injection attack is technique designed to manipulate the behavior of LLMs by using malicious prompts to override their original instructions. For instance, common injection attack mainly operates in three stages: pre-constructed prompt insertion, context partitioning, and malicious payload. Indirect prompt injection via third-party data sources (e.g., emails, PDFs, web pages) has also been shown effective where hidden instructions in external content manipulate the LLM [477]. Current prompt injection attacks primarily fall into two categories. The first type [718, 39] manipulates the model to respond to the attackers queries, thereby diverging from its original purpose. The attacker crafts prompts that, once combined, effectively nullify and subvert the intent of the predefined prompt, consequently eliciting the desired responses. Such attacks typically focus on applications that operate within known context or rely on predefined prompts. Another line of work [590, 3] seeks to contaminate LLM-integrated applications to exploit user endpoints. Many LLM-integrated applications in real-world scenarios require interactions with external resources and programs for functionality. Injecting harmful payloads into these resources may compromise these applications. Specifically, these attacks send misleading messages to LLMs, leading to the execution of malicious actions in these applications. Published in Transactions on Machine Learning Research (10/2025) Poisoning and Backdoor Attacks. Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. Numerous studies have shown the vulnerability of instruction tuning against poisoning and backdoor attacks. Wan et al. [929] demonstrate that by adding crafted examples to the dataset, the predictions of fine-tuned LLM can be manipulated to behave in predefined manner whenever specific trigger phrase appears in the input. Shu et al. [824] show that an adversary can achieve content injection by incorporating training examples that mention targeted content, thereby eliciting such behavior on these trained models during inference. Sun et al. [862] backdoor neural code search models to return buggy or even vulnerable code with security issues. Beyond injecting backdoors into supervised fine-tuning data, Shi et al. [819] and Rando & Tramèr [756] explore the possibility of injecting backdoors into the reward model during the RLHF process. For example, an attacker could insert small set of poison examples when training the reward model, where trigger phrase maps to malicious reward manipulation. Such backdoored reward models will then be deployed to the instruction tuning, where the effect of trigger phrases is further embedded into LLM. Furthermore, Chen et al. [169] explore the possibility of injecting very small number of poisoned instances into the retrieval-augmented generation (RAG) database of LLMs to achieve high success rate of backdoor attacks in training-free manner. Zhao et al. [1169] explored defending against backdoor attacks on LLMs through head pruning and Attention normalization. Xiang et al. [1026] explored backdoor attacks on chain-of-thought mechanism of LLMs. Wei et al. [979] studied defending against backdoor attacks under the setting of LLM prompt-tuning. Zou et al. [1221] studied poisoning attacks against RAGs. 5.1.2 Defense in LLMs Drawing from the previously discussed attacks, the field has increasingly focused on developing general defensive strategies for LLMs, aiming to fortify these models against such vulnerabilities. These defense mechanisms are diverse, encompassing both proactive and reactive measures, aimed at preserving the functionality and reliability of LLMs. Defenses against Jailbreaks. Chen et al. [139] ensemble outputs from multiple LLMs and select the one that is both helpful and harmless to defend against jailbreak prompts. Xie et al. [1036] wrap the users query in system prompt, guiding ChatGPT to respond responsibly. Luo et al. [610] decompose the LLM activation of the user input as sparse linear combination of concept vectors and remove the malicious ones from the activation. Cao et al. [118] implement robust alignment checking function that defends against jailbreaks, avoiding the need for costly retraining or fine-tuning of the original LLM. Xu et al. [1050] introduce safety-aware decoding strategy, effectively safeguarding LLMs against jailbreak attacks and ensuring the generation of helpful and harmless responses to user queries. Zhao et al. [1175] propose Adversarial Contrastive Decoding, an optimization-based framework to generate two opposite system prompts for prompt-based contrastive decoding to improve the safety alignment of LLMs. Defenses against Prompt Injection. Prevention-based defense [414] aims to preprocess both data and instruction prompts through techniques such as paraphrasing. This ensures that LLM-integrated applications achieve their intended tasks effectively, even in cases where the data prompt may be compromised. Alon & Kamfonas [23] observe that adversarial suffixes exhibit higher perplexity values than normal, enabling the detection of prompt injection attacks based on the perplexity. Liu et al. [590] defend against prompt injection attacks by integrating prevention-based and detection-based defenses, representing pioneering methodology for black-box prompt injection attacks by its versatility and adaptability when targeting LLM-integrated service providers. Provable Defenses. Building defenses for LLMs with provable guarantees is more challenging than for smaller models, primarily due to their larger model size. Motivated by randomized smoothing, Robey et al. [768] defend against jailbreaks by randomly perturbing multiple copies of given input prompt and then aggregating the predictions to identify adversarial inputs. Kumar et al. [475] defend against jailbreaks by individually erasing tokens and analyzing the resulting subsequences with safety filter. While offering robust guarantees on security, such provable defenses frequently result in increased overhead and reduced utility. 33 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "5.2.1 Attack in MLLMs",
            "content": "MLLMs are more susceptible to vulnerabilities and threats due to their multimodal input format. Attackers may exploit this by manipulating inputs in two ways: (i) by generating adversarial examples for image inputs, and (ii) by using jailbreak prompts for text inputs [324]. Both strategies are designed to prompt MLLMs to generate inaccurate or harmful outputs. In this context, we provide an overview of the primary threats faced by MLLMs, including various types of attacks. [126] Image Adversarial Attacks. The continuous and high-dimensional nature of visual inputs makes them vulnerable to adversarial attacks [311], thereby broadening the attack surface for MLLMs. leverage projected Carlini et al. gradient descent (PGD) [628] attack to generate adversarial images, effectively inducing MLLMs, such as LLaVA and MiniGPT-4, to produce arbitrary toxic sentences. In each optimization iteration, PGD walks toward the sign of the gradient that maximizes the loss with respect to the image, and then projects the perturbation back into an ℓpnorm ball around the original input. Such an approach thereby finds minimal yet effective perturbations to fool the model. Qi et al. [728] find that single visual adversarial example can universally jailbreak an MLLM with alignment on the language domain, compelling it to heed wide range of malicious instructions and produce harmful content. Concurrently, Dong et al. [239] comprehensively analyzes black-box adversarial attacks against commercial MLLMs. This research specifically examines two defense mechanisms in Bard [313]: face detection and toxicity detection. The study underscores the potential to attack these mechanisms through the meticulous design of adversarial images, resulting in significant risks such as the leakage of facial privacy and the abuse of toxic content. Furthermore, Wang et al. [966] propose stop-reasoning attacks to mislead multimodal CoT-based generation of MLLMs. Cheng et al. [174] verify typographic attacks (image with adversarial typography) on current well-known commercial and open-source MLLMs, showcasing the widespread existence of this threat. Gu et al. [327] study the security of MLLMs from multi-agent perspective, revealing that by simply jailbreaking single agent, without any further intervention, (almost) all agents become infected at an exponential rate and exhibit harmful behaviors. The study demonstrated that introducing an infectious adversarial image into the memory of any randomly selected agent is sufficient to achieve widespread infectious jailbreak. Besides, across-prompt adversarial images have also been proposed to attack MLLMs where they show an adversarial image can mislead MLLMs given various prompts [608]. Figure 13 illustrates typical optimization attack that minimizes the likelihood of generating correct responses when given an adversarial image. Figure 13: An example of image adversarial attacks for MLLMs via gradient descent, where harmful textual output is generated. Jailbreak Attacks. MLLMs, like their counterparts, take prompts as inputs, introducing prompts as potential attack surface. Wu et al. [1014], Chen et al. [156] uncover the vulnerability to system prompt leakage in GPT-4V and execute search for potential jailbreak prompts using stolen system prompts, effectively jailbreaking MLLMs solely from the language side. Concurrently, Chen et al. [162] apply adversarial prefix instructions on MLLMs to leak private information in images. To bypass the interleaved cross-attention mechanism that alternates text-image tokens for alignment (used in IDEFICS [489] and Flamingo [16]), authors design two-step multi-hop attack strategy where an adversary first queries benign attribute (e.g., language spoken) and then uses that output in follow-up prompt to infer the protected attribute (e.g., nationality). Gu et al. [327] explored jailbreak attacks against multi-agent MLLMs. Published in Transactions on Machine Learning Research (10/2025) Poisoning and Backdoor Attacks. MLLMs are also vulnerable to backdoor attacks. Carlini & Terzis [123] show that MLLMs can be poisoned and backdoored by modifying tiny proportion (e.g., 0.01%) of the dataset. Similarly, Jia et al. [420] reveal that pre-trained multimodal encoders are vulnerable to backdoor attacks. Classifiers built on these compromised encoders display malicious behaviors when presented with examples containing specific added triggers."
        },
        {
            "title": "5.2.2 Defense in MLLMs",
            "content": "In response to the previously discussed attacks, many studies have shifted towards developing general defensive strategies for MLLMs to fortify these models against such vulnerabilities. These diverse defense mechanisms address both inference-time attacks, which seek to perturb visual and/or textual input, and training-time poisoning and backdoor attacks, all aimed at preserving the functionality and reliability of MLLMs. Defenses against Multimodal Perturbation Attacks. Defenses against inference-time perturbation for MLLMs have primarily focused on improving robustness for zero-shot image input, where adversarial attacks perturb the visual modality. Mao et al. [631] propose defense strategy based on adversarial training [628, 56], which adopts contrastive learning between adversarial images and text embeddings of the corresponding class labels to enhance the robustness of MLLMs against adversarial visual perturbations. Similarly, Wang et al. [952] propose defense method leveraging supervision from the original pre-trained model to improve the models zero-shot adversarial robustness. Considering that the language modality can also be manipulated, Waseda & Tejero-de Pablos [973] leverage the many-to-many relationship in image-text retrieval to enhance adversarial robustness for MLLMs. Defenses against Backdoor and Poisoning Attacks. Defenses against backdoor attacks in MLLMs can be broadly categorized into methods for detecting and removing attacked samples from training [141, 869], techniques for eliminating backdoors already learned by models [1122, 589], and strategies aimed at preventing models from learning backdoors by reducing their effectiveness [62, 533]. Specifically, during training, Yang et al. [1060] introduce ROCLIP to disrupt poisoned image-caption relations by preparing pool of random captions and periodically matching each image with the most similar text instead of its own caption. Similarly, Bansal et al. [62] propose to realign representations from different modalities to enhance robustness. Besides, Ishmam & Thomas [408] proposes to use external knowledge from LLMs to prevent learning correlations between image regions that lack strong alignment. On the other hand, to remove backdoors already learned, Feng et al. [269] propose to search for minimal trigger patterns to ensure inputs stamped with the trigger share similar embeddings. Similarly, Zhu et al. [1211] propose reverse-engineering method to detect backdoors by jointly searching for image triggers and malicious target texts in the shared feature space of vision and language modalities. 5.3 Security in Image Generative Models 5.3.1 Attack in Image Generative Models Furthermore, image generative models such as Stable Diffusion [771] and DALLE [754] raise many security concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones [324]. These security vulnerabilities, including jailbreak and backdoor attacks, highlight the nuanced challenges of maintaining the integrity and safety of image generative models. Jailbreak Attacks. Recent works [1116, 889, 513, 1097] argue that image generative models are vulnerable to ambiguities in their latent space. Many red-teaming studies [889, 757, 181, 1070] have shown that seemingly harmless prompts can inadvertently generate NSFW images or content. For example, SneakyPrompt [1070] introduces an automated attack framework that strategically perturbs input tokens within prompt to evade safety filters; Red-teaming SD [757] and Prompting for Debugging [181] jailbreak the safety filter by searching for adversarial examples in the text embedding space, such as CLIP-Text embeddings; MultiMon [889] shows that one can simply bypass the filter by injecting negations, temporal changes, and bag-of-words. These recent advances pose challenges to the safety filters of image generative models. 35 Published in Transactions on Machine Learning Research (10/2025) Poisoning and Backdoor Attacks. Image generative models are also vulnerable to backdoor attacks. Chen et al. [157], for instance, design novel transitions to diffuse predefined target distribution into the Gaussian distribution, biased by specific trigger. After training, the models will always output adversarial targets along the learned trojan generative process. Zhai et al. [1125] efficiently inject backdoors into large-scale diffusion model. RickRolling [854] inserts single character trigger, such as an emoji, into the prompt to make the model generate images following predefined attributes or hidden malicious descriptions. Moreover, recent work [98] shows that the model architecture poses real threat and can survive complete retraining from scratch. Wang et al. [937] proposed an efficient backdoor attack against image generative models that is training-free and data-free. Adversarial Attacks. Image generative models can be used to generate adversarial samples, which lead to serious security issues for visual models. Dai et al. [206] introduce AdvDiff, which employs diffusion models with adversarial guidance to create unrestricted adversarial examples by subtly steering the models reverse generation process. Chen et al. [173] propose to optimize the attack along low-dimensional manifold of natural images within Stable Diffusion to control style modification and produce photorealistic perturbations. Liu et al. [573] propose Instruct2Attack (I2A), language-guided adversarial attack that uses latent diffusion models to guide the reverse diffusion process adversarially. This approach aims to find an adversarial latent code conditioned on the input image and corresponding text instruction. DiffAttack [435] integrates deviated-reconstruction loss and segment-wise forwarding-backward algorithm to conduct evasion attacks against diffusion-based adversarial purification defenses. More recently, Chen et al. [147] craft semantic latent-space perturbations via diffusion dynamics to generate adversaries that are highly transferable to unseen black-box models under strict imperceptibility constraints. 5.3.2 Defense in Image Generative Models Defending against malicious inputs and attacks in image generative models is critical aspect of ensuring their safe and ethical use. Existing defense mechanisms can be broadly categorized into four types: dataset curation, trigger detection, model fine-tuning, and post-generation content moderation. Dataset Curation. Dataset curation is typically one of the first steps to training foundation models. It is critical mechanism for ensuring that harmful, inappropriate, or biased content is excluded from the training data. Birhane et al. [91] examine the toxic, offensive, and harmful contents in the LAION-400M dataset [792] and demonstrate the failure cases of CLIP filtering. Thiel [878] uncover instances of sexual abuse material within the LAION-5B dataset [793] and raise concerns about the reliability and safety of publicly sourced data. The work also discusses strategies based on the nearest neighboring for removing such harmful content. Hong et al. [368] audit common approaches of image-text CLIP-filtering and highlighted discrepancies in filtering techniques that could lead to biased annotations. Birhane et al. [92] investigate the effect of scaling datasets on harmful content and suggest developing new filtering methods for hateful and aggressive texts that traditional filtering cannot handle. Trigger Detection. Trigger detection focuses on identifying malicious inputs before they can degrade the image generative models. [860] introduce DisDet to detect backdoor samples in unconditional diffusion models by analyzing the distribution discrepancy of the noise input. They propose using KL divergencebased method to identify infected samples, achieving nearly 100% detection recall at low computational cost. However, this method struggles with conditional diffusion models where backdoor attacks may not impact the noise input [184, 854, 1137]. To address this, Wang et al. [971] leverage the assimilation phenomenon of image generative models where malicious trigger token can dominate the attention of the rest of tokens, which causes the model to focus on the triggers intended effect rather than the prompts original semantic meaning. The authors introduces binary-search algorithm to localize such malicious triggers within backdoor sample. Yoon et al. [1097] introduce training-free safeguard approach for image generative generation designed to prevent inappropriate outputs from unsafe or adversarial input prompts by integrating filtering mechanisms across both text embeddings and visual latent spaces. Model Finetuning. Model fine-tuning aims to defend image generative models from generating unsafe and unethical content via alignment [167]. Techniques such as concept-erasing [283, 479, 791] which change the weights of existing image generative models regarding malicious concepts and inference guidance [791] which 36 Published in Transactions on Machine Learning Research (10/2025) directly eliminates the capability of generating inappropriate content from image generative models, have been proposed to preventing harmful content generation under malicious inputs. Despite their potential, these methods face significant challenges: they are not comprehensive, lack scalability, and often degrade the quality of benign image generation [1158, 498, 791], which makes them rarely considered by image generative online services [653]. Post-Generation Content Moderation. Post-generation content moderation involves filtering and censoring generated images that violate safety or ethics criteria. These methods can be divided into prompt-based moderation, like OpenAIs Moderation API [695, 722], which prevents harmful content generation by identifying and rejecting malicious prompts, and image-based moderation, like safety checkers in SD [757], which operates on the generated images to detect and remove inappropriate elements. These methods do not interfere with the training process of the image generativemodel, preserving the quality of the generated images. However, they rely heavily on extensive labeled datasets and often struggle with generalizing to new types of inappropriate content or unseen attacks [1067, 791, 168]. To address the generalizability issue, Yang et al. [1068] proposes GuardT2I, which directly moderates the intermediate latent of textual prompts to be more robust and more generalizable to various inappropriate content. On the contrary, SafeGen [529] operates by regulating the vision-only self-attention layers to remove the generation capability of unsafe content from the image generative model in text-agnostic way. Furthermore, Latent guard [579] proposes to learn latent space on top of the image generative models text encoder and detect the presence of harmful concepts in the input text embedding. Similarly, Chen et al. [172] introduces an agentic post-verification system that guardrails model outputs based on explicit safety regulations. In addition to safety filters, other mitigation strategies have also been studied [791, 513]. 5.4 Current Limitations and Future Directions Despite significant advancements in the domain of foundation model security, several limitations still require future attention for both attack and defense methods. 5.4.1 Limitations and Open Challenges of Attacks Most current attack methods against foundation models rely on optimization techniques, whether white-box or black-box. Iterative white-box optimization is computationally intensive for foundation models, while black-box optimization incurs significant economic costs due to massive token consumption. These high costs may limit and discourage attackers from adopting these methods in real applications. Another major limitation is the uncertain real-world threat of these attacks. Chen et al. [166] point out that most jailbreak attacks are only able to generate simple harmful sentences or paragraphs in most cases, lacking the ability to provide detailed instructions for malicious behaviors. This significantly limits the practical application scenarios for these attacks. There are also multiple open challenges for attacks against foundation models. Foundation models have witnessed new applications and paradigms, including multi-agent communication [340, 710, 731, 369], tool usage [736, 351, 788], retrieval-augmented generation (RAG) [751, 46], making foundation models more widespread in different domains. Understanding the security of foundation models under these paradigms and building attacks for them is also an interesting problem with practical impact. 5.4.2 Limitations and Open Challenges of Defenses Most current defense methods on foundation models lack formal safety guarantees in terms of definition and design, unlike traditional machine learning models and small deep neural networks that have provably secure defenses [197, 748]. major reason is that for foundation models, an important attack space is prompts, which are natural language, and attacks on them are more challenging to formalize compared with images. Additionally, current defense methods are significantly less effective when considering multiple modalities. This challenge is rooted in the fundamental differences between modalities, while current foundation models attempt to unify them into the same embedding space. This unification allows attackers to bypass defenses through any modality. Published in Transactions on Machine Learning Research (10/2025) Another limitation worth mentioning is over-safety. Due to the difficulty of precisely defining safe/unsafe behaviors for foundation models, it is common for defense mechanisms to exhibit over-safety, rejecting benign inputs that are misclassified as malicious. Oversafety negatively impacts user experience and is significant problem to address. Finally, developing defense strategies that leverage external symbolic or classification systems remains challenge. Anthropics constitutional classifiers [806] apply separate LLM-based safety filter trained on explicit constitutional rules to guard against jailbreaks and unsafe outputs. Other safety detectors such as ShieldLM [1164] and Adversarial Prompt Shield [449] also demonstrate customizable detection rules and explainable decisions. However, these systems either require high-volume adversarial training or rely on instructing foundation models (which may still face evasion and incur non-trivial inference overhead). The long-standing trade-off between adversarial safety and user experience is unresolved. Developing on-the-fly defenses that do not significantly compromise output quality remains an open challenge for foundation model defenses. 38 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "6 Privacy",
            "content": "The rapid expansion of foundation models has brought privacy concerns to the forefront. These models, often trained on vast amounts of data, can potentially expose sensitive information [324]. Recent privacy regulations such as GDPR [797] and CCPA [308] further limit the availability and use of private data. Addressing these privacy concerns and ensuring compliance with privacy regulations has led to the development of privacy-preserving machine learning (PPML) solutions. Recent efforts have focused on integrating anonymization mechanisms and creating innovative privacy-preserving methods for foundation models [607]. However, these approaches often address only specific aspects of privacy and may not provide comprehensive solution. For instance, implementing differential privacy in foundation models can reduce model accuracy, while secure multi-party computation methods lead to high communication and computation overhead [251]. Advanced cryptosystems like fully homomorphic encryption offer strong privacy guarantees by enabling computations on encrypted data, but they come with prohibitive computational costs [7]. In this section, we provide comprehensive examination of privacy in foundation models, as illustrated in Figure 14. Figure 14: Privacy threat and protection techniques in different types of foundation models, including LLM, MLLM, and T2I models. 6.1 Privacy in LLMs 6.1.1 Privacy threats in LLMs Like their traditional counterparts, foundation models tend to memorize training data, which frequently includes sensitive information. The issue of memorization is magnified in large foundation models due to their over-parameterization, trait that becomes increasingly pronounced as the models scale enlarges [125, 1066]. Consequently, this raises severe privacy concerns related to the use of LLMs [324]. Membership inference attack (MIA), significant threat posed by LLMs, seeks to identify whether particular data record was utilized during training. Song & Raghunathan [841] study membership inference against BERT models [221]. Mattern et al. [635] propose neighborhood comparison method to improve the effectiveness of such attacks against LLMs. Recently, Shi et al. [820] introduce reference-free MIA method, MIN-K% PROB, that determines if an LLM was trained on specific text based on the distribution of the log probability of each token without requiring knowledge of the pre-training dataset. Besides pre-training, Jagannatha et al. [412] show that membership inference could be performed on language models fine-tuned on medical data. Similarly, Mireshghallah et al. [661] highlight the vulnerability of LLMs to membership inference attacks during their fine-tuning phase. Wen et al. [992] studied membership inference attacks against in-context learning. Anderson et al. [28], Li et al. [537] studied membership inference attack against RAG. Feng & Tramèr [268], Wen et al. [996] further introduce privacy backdoor attacks that significantly increase privacy leakage during the fine-tuning phase. 39 Published in Transactions on Machine Learning Research (10/2025) Membership inference attack Song & Raghunathan [841], Mattern et al. [635], Shi et al. [820], Jagannatha et al. [412], Mireshghallah et al. [661], Wen et al. [992], Anderson et al. [28], Li et al. [537], Feng & Tramèr [268], Wen et al. [996] Privacy Threat Training data extraction Carlini et al. [124], Nasr et al. [685], Huang et al. [387], Kim et al. [450], Yang et al. [1072], Nakka et al. [681] Other Attacks Zhang & Ippolito [1159], Naseh et al. [684] Privacy Protection Differential Privacy Chen et al. [139], Xie et al. [1036], Luo et al. [610], Cao et al. [118], Xu et al. [1050], Zhao et al. [1175] Other Defenses Patil et al. [712], Gehman et al. [297], Schick et al. [787], Lukas et al. [607], Hans et al. [350], Jain et al. [413] Privacy Threat Wu et al. [1010; 1014], Chen et al. [162], Li et al. [538] Privacy Protection Cheng & Amiri [175], Tito et al. [885], Huang et al. [385] LLM MLLM Image Generative Models Privacy Protection Dockhorn et al. [233], Ghalebikesabi et al. [305], Lyu et al. [615], Zhao et al. [1174] Privacy Threat Carlini et al. [127], Liu et al. [572], Webster [977], Duan et al. [245], Kong et al. [465], Somepalli et al. [839; 838], Wen et al. [995], Ma et al. [622] d o d F c r Figure 15: Taxonomy for Privacy of Foundation Models. Moreover, the extraction of training data poses significant risk to the privacy of LLMs due to their strong memorization capabilities. Carlini et al. [124] first successfully extract training data on GPT-2 models, revealing that the model could output sensitive information, such as phone numbers and email addresses when prompted with specific prefix patterns. Nasr et al. [685] further improve it by introducing divergent attack on ChatGPT, which emits training data at considerably higher rate. Further studies [387, 450] specifically focus on the extraction of personally identifiable information (PII) from LLMs. Besides natural language, Yang et al. [1072] explore data extraction in code LLMs, highlighting the broad applicability of these privacy concerns. Nakka et al. [681] explored enhancing PII extraction by grounding context similar to training data. Additionally, there are other potential attack surfaces in LLMs. For instance, Zhang & Ippolito [1159] demonstrate that prompts, considered valuable commodities in the age of foundation models and tradable on markets, can be successfully uncovered by users even when they are intended to be kept confidential. Moreover, the process of tuning hyperparameters for LLM decoding algorithms, which demands significant time, manual effort, and computational resources, is compromised by Naseh et al. [684], who reveals method to extract these hyperparameters at very low cost. 6.1.2 Privacy-preserving techniques in LLMs To mitigate these privacy threats, the field has shifted towards developing various techniques to preserve privacy in LLMs, aiming at fortifying these models against such vulnerabilities. Differential Privacy (DP) is rigorous mathematical framework that provides quantifiable privacy guarantees when analyzing or learning from sensitive data. At its core, DP ensures that the output of an algorithm (e.g., trained model) does not significantly change when any single individuals data is added or removed from the dataset. This limits the risk of leaking information about any particular individual. DP typically works by introducing random noise into the computation to obscure the contribution of individual data points. Differential Privacy Stochastic Gradient Descent (DP-SGD) [1], foundational technology in many privacy-preserving LLMs, injects sample-wise Gaussian noise into the computed gradients during optimization. The key idea is that, even with full access to the trained model, an attacker cannot confidently infer whether any specific individuals data was used. This balance between utility and privacy is controlled by parameters (ϵ, δ), where smaller values imply stronger privacy. Igamberdiev & Habernal [406] explore LLM pre-training under local differential privacy (LDP), aiming for privatized text rewriting. Given that LLMs are frequently fine-tuned on sensitive domains, numerous studies have investigated the application of DP-SGD in fine-tuning LLMs. Qu et al. [737] apply differential privacy on pre-training and fine-tuning BERT models. Yu et al. [1098] and Li et al. [530] study the integration of DP-SGD with different fine-tuning algorithms for GPT-2. Li et al. [531] propose differentially private prompt-tuning techniques for LLMs. Yue et al. [1114] apply DP-SGD for generating synthetic text that adheres to the post-processing theorem, therefore preserving the same privacy budget. These texts can serve as substitutes for original data in downstream tasks while maintaining privacy. Aside from general DP-based defenses against various privacy attacks, there are targeted methods for specific threats, such as data extraction attacks. Patil et al. [712] investigate defense by directly removing sensitive information from model weights. Moreover, techniques for filtering toxic output [297, 787] can help prevent 40 Published in Transactions on Machine Learning Research (10/2025) the generation of sensitive content. Lukas et al. [607] reduce the risk of PII leakage through PII scrubbing on the fine-tuning dataset. Hans et al. [350] propose memorization mitigation strategy during pre-training, which involves randomly sampling subset of tokens to exclude from the loss computation. Jain et al. [413] also find that adding noise to word embeddings during training can reduce the effectiveness of extraction attacks."
        },
        {
            "title": "6.2 Privacy in MLLMs",
            "content": "Similarly, MLLMs also face privacy risks due to their tendency to memorize sensitive information from training data. Hu et al. [380] introduce both metric-based and feature-based attacks for conducting membership inference on multimodal models under various assumptions, highlighting the privacy vulnerabilities of MLLMs. Wu et al. [1010] show that MLLMs are also susceptible to model stealing attacks, where model information of CLIP can be extracted via either the text-to-image or image-to-text retrieval APIs. Another privacy risk of MLLMs stems from their capability to extract sensitive information from images and present it in textual form. Wu et al. [1014] observe that jailbreaking MLLMs could induce them to identify the real human, causing severe privacy concerns. Chen et al. [162] apply adversarial prefix instructions on MLLMs to expose private information within images. Their findings reveal that existing access control instructions fail to prevent MLLMs from answering personal data, violating the General Data Protection Regulation (GDPR). Li et al. [538] studied membership inference against MLLMs based on the token-level confidence of the model output from the cross-modal (text-image) data. Recent research has been conducted to protect the privacy of MLLMs. Cheng & Amiri [175] develop machine unlearning approach tailored for multimodal data and models, providing improved protection for erased data. Tito et al. [885] employ combination of federated learning and differential privacy to secure the privacy of MLLMs, particularly in the context of Document Visual Question Answering. Huang et al. [385] introduce differentially private variant of the CLIP model, effectively addressing privacy concerns while maintaining accuracy across wide range of vision-language tasks. While these studies focus on protecting the privacy of training data, the challenge of mitigating the risk of MLLMs extracting sensitive information from input images remains an open problem. 6.3 Privacy in Image Gneration Models Since the introduction of image generation models, the research community [127, 572, 977, 245, 465, 839, 838, 995, 622] has uncovered hazards associated with extracting private information from public models. These studies demonstrate the possibility of extracting over thousand training examples from state-of-the-art diffusion models, ranging from photographs of individuals to trademarked company logos, highlighting the urgent need to address these vulnerabilities to preserve privacy. To mitigate these issues, the development of differentially private diffusion models [233] has been proposed, utilizing DP-SGD to enforce privacy. Ghalebikesabi et al. [305] explore the use of perturbation, timestep augmentation multiplicity, and modified timestep sampling schemes to train more effective private diffusion model. Lyu et al. [615] further propose Differentially Private Latent Diffusion Models that only finetune the attention modules of diffusion models with privacy-sensitive data to obtain differentially private diffusion models in an efficient manner. Beyond that, recent advancements in fine-tuning based image generation models, such as Textual Inversion [279], DreamBooth [776], and Custom Diffusion [480], have empowered individual users to incorporate personalized concepts into the base model with minimal data and computational resources. However, the increasing adoption of these models has sparked concerns regarding image privacy and copyright issues. For instance, fine-tuning specific face datasets enables image generation models to generate highly realistic images of individuals, which can lead to significant privacy violations and authenticity concerns. Similarly, fine-tuning the works of specific artists allows image generation models to replicate artistic styles with ease, potentially resulting in copyright infringement issues. These concerns surrounding image privacy and copyright in the context of image generation models have garnered attention from the public and media [75, 196, 974]. 41 Published in Transactions on Machine Learning Research (10/2025) number of research efforts have been dedicated to addressing the image privacy and copyright challenges posed by image generation models. notable approach involves adding imperceptible protective adversarial perturbations to images, thereby preventing image generation models from learning the features of protected images [542, 913, 1176, 801, 1002, 1086, 1173]. However, after fine-tuning on images with adversarial perturbations, the generated images by image generation models typically sacrifice quality and exhibit semantic deviations compared to those fine-tuned on unperturbed images. GrIDPure [1174], simple yet efficient purification method, successfully eliminates protected adversarial perturbations while preserving their quality. GrIDPure claims they can effectively aid Stable Diffusion in learning from protected images, thereby highlighting the fragility and unreliability of the adversarial protection method."
        },
        {
            "title": "6.4 Current Limitations and Future Directions",
            "content": "While significant advancements have been made in enhancing privacy for foundation models, numerous challenges remain that warrant further investigation in the future."
        },
        {
            "title": "6.4.1 Limitations and Open Challenges of Privacy Attacks",
            "content": "The vast scale of foundation model training datasets blurs the boundary between member and non-member data [248]. Many non-member data points may naturally be very similar to some member data points. This makes effective membership inference attacks on foundation models challenging and prompts reevaluation of the membership game. While training data extraction attacks largely avoid membership ambiguity issues, their primary limitation is that the current schemes are only able to extract small fraction of the training data. This constraint raises questions about their practical threat. Moreover, the threat model of some privacy attacks is overly strong. For example, backdoor attacks [996] require poisoning or injecting triggers to amplify privacy leakage, which can only be performed in constrained scenarios such that the attacker possesses sufficient knowledge of the target model (e.g., the loss formulation of its original task). In conclusion, building effective privacy attacks that work under weaker assumptions and more general scenarios is an open challenge. Another interesting future direction is contextualized privacy. Even with perfect sensitive data cleaning, personal information leakage can still occur in context. For instance, during multi-turn conversations with LLM-based chatbots, it may be possible to infer personal attributes based on the entire context, even if no part of the conversation contains private information. 6.4.2 Limitations and Open Challenges of Privacy Preserving techniques Currently, Differential Privacy (DP) has become mainstream in protecting data privacy in foundation models. However, DP still faces two significant limitations: 1. DP provides worst-case privacy leakage bounds. In real scenarios, adversaries rarely have full control over the training data, resulting in considerable gap between practical attacks and the worst-case probabilistic analysis of privacy leakage, according to DP. 2. Integrating DP into the foundation model fine-tuning still leads to significant performance degradation [1098, 530]. This utility deterioration weakens the motivation for DP-based fine-tuning. Other approaches focus on protecting privacy by removing or obfuscating sensitive information from training data. However, such data-sanitization pipelines have also been shown to suffer from performance degradation on main tasks [396, 705]. In conclusion, designing strong privacy-preserving techniques for foundation models that balance privacy and performance remains an open challenge. Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "7 Hallucination",
            "content": "Hallucination refers to the phenomenon in which the model generates unfaithful information that is not fully grounded or verifiable, leading to actions misaligned with observable facts. Foundation models are often observed to generate hallucinatory responses in text [371, 532], image [550], video [189], and audio [324], imposing significant challenges in their real-world applications. Specifically, the model outputs incorrect or inaccurate statements, with highly confident tone, that fails to align with the input or reflect the knowledge of the real world. Prior efforts by Huang et al. [390] offer detailed taxonomy for detection and mitigation of the LLM hallucination, which authors define the phenomenon as the generation of plausible yet non-factual content caused by intrinsic (model-driven) or extrinsic (knowledge-driven) factors. Similarly, Sahoo et al. [780] categorize hallucinations across multiple modalities and characterize hallucinations as outputs that lack faithful grounding in input or real-world data. Aggregating these empirical descriptions of hallucination, we introduce definition of the phenomenon from more unified and probabilistic perspective for the general audience. With target generative model and verification predicate that checks the alignment of the output with external knowledge xexternal and/or the input context xinput, model response = (x; θ) is hallucinatory if: pv(yxinput, xexternal, ) < ϵ, (13) where ϵ is the probability threshold for hallucination detection. Hallucination has troubled both researchers and users, negatively affecting various domains, e.g., natural language generation [1029], visual perception [203], and medical application [911]. This section offers an overview of hallucination in the context of foundation models, presenting definitions and categorizations, and discussing various methods for evaluating and mitigating hallucinations. Figure 17 showcases hallucinatory examples of both LLMs and MLLMs. Categories Sources Detection Mitigation Categories Sources LLM MLLM Factuality Hallucination Faithfulness Hallucination Model-driven Knowledge-driven Retrieval / Fact-checking Hongbin et al. [371], Li et al. [532], Xiao & Wang [1029], Ji et al. [418] Maynez et al. [638], Zhou et al. [1182], Ji et al. [418] Kalai & Vempala [430], Arora et al. [44], Zhai et al. [1126] Chuang et al. [190], Wei et al. [981], Rohrbach et al. [770], Xia et al. [1021], Li et al. [520] Augenstein et al. [51], Atanasova et al. [49], Chen et al. [148], Gou et al. [317], Kang et al. [433], Min et al. [658], Chern et al. [178] Internal states / Uncertainty Varshney et al. [914], Luo et al. [612], Yao et al. [1077], Xiao & Wang [1029], van der Poel et al. [912], Guerreiro et al. [329] Data-level Model-level McKenna et al. [641], Lebret et al. [491], Dhingra et al. [225], Meng et al. [648], Huang et al. [391], Dušek et al. [250], Liu et al. [583], Gardent et al. [293], Wang [941], Parikh et al. [707], Brown et al. [113] Ouyang et al. [703], Tian et al. [884], Aralikatte et al. [40] Cross-modal / Visual Errors Cui et al. [203], Li et al. [532], Xia et al. [1022], Zhou et al. [1181], Chen et al. [144] Bias & Domain Gaps Chuang et al. [190], Wei et al. [981], Rohrbach et al. [770], Xia et al. [1021], Li et al. [520] Image Generation Categories Object / Attribute Errors Wu et al. [1006], Huang et al. [389] d o d F o n l Figure 16: Taxonomy of Hallucination in Foundation Models. 7.1 Hallucination Categorization 7.1.1 Hallucinations in LLMs Prior literature [638, 1182, 418] specifically refers hallucination in LLMs as the generation of nonfactual or unfaithful text to an input prompt. We categorize hallucination in LLMs into two types: factuality hallucination and faithfulness hallucination. Figure 17 depicts these two types of hallucinations. Detailed descriptions of the categorization are elaborated below: Factuality Hallucination. Existing LLMs sometimes generate text that conflicts with world knowledge, potentially resulting in misleading and raising concerns about the reliability of these models. These discrepancies and inconsistencies are termed factuality hallucinations. In general, we categorize them into two main types, depending on whether the information produced can be cross-checked with credible source. Factual Inconsistency. This term describes situations where the generated text from language model contradicts established world knowledge. This kind of hallucination is the most common and 43 Published in Transactions on Machine Learning Research (10/2025) Figure 17: Examples of factuality and faithfulness hallucinations in foundation models. arises from various factors, including how the language model captures, stores, and expresses factual knowledge. Factual Fabrication. This refers to cases where the language model generates response that appears factual but cannot be verified by known real-world evidence. Faithfulness Hallucination. LLMs may generate text that conflicts with (1) input instructions, (2) in-context demonstration, or (3) preceding generated text. Such discrepancies can significantly impair the model reliability and the user experience. For example, Bang et al. [61] show that, when instructing an LLM to summarize an article at given location, the LLM can be hallucinatory to summarize another text chunk instead. To better understand the issues, we classify faithfulness hallucinations into three categories based on the type of conflict and the specific aspect they pertain to: Input Confliction. It refers to instances where the generated text contradicts the instruction or query in the input text. It indicates failure to accurately process or explicitly follow task directives. Context Fabrication. It involves cases where the generated content is beyond the specified scope by the in-context demonstrations from the source prompt or externally retrieved knowledge. Preceding Generation Inconsistency. This pertains to situations where the text newly generated by the model conflicts with its earlier outcomes, leading to lack of coherence and continuity in the entire output. 7.1.2 Hallucinations in MLLMs Similarly, MLLMs are notably prone to generating hallucinatory responses, further compounded by issues specific to the vision modality. Figure 18 provides an overview of hallucinations in MLLMs. MLLMs have become pivotal in bridging computer vision and natural language processing, offering spectrum of applications through their ability to produce text descriptions that are contextually appropriate based on visual inputs. Despite their capabilities, MLLMs also encounter challenges with hallucinations. However, the taxonomy of hallucinations in MLLMs is specifically characterized by factuality and faithfulness [638, 1182, 418, 1022], which differs from that of LLMs. This fine-grained classification aims to elucidate the factors influencing hallucinations in MLLMs, thereby revealing their underlying essence. The categories are detailed below: 44 Published in Transactions on Machine Learning Research (10/2025) Figure 18: An overview of hallucinations in MLLMs. The top row illustrates typical MLLM pipeline from user prompt to model response. Hallucinations can emerge at different stages due to multiple sources. To mitigate these hallucinations, various strategies are employed, including improvements to data curation, training methods, and inference approaches. Factuality Hallucination. In vision-language tasks, factuality hallucination occurs when the model, prompted by both textual and visual inputs, produces outputs inconsistent with real-world facts. Based on this difference, we further divide factuality hallucination in MLLMs into: Intrinsic Factuality Hallucination. This refers to situations where MLLM outputs inaccurately describe images with facts that either contradict or cannot be verified against established real-world knowledge. This type of hallucination may stem from many sources, such as out-of-distribution world knowledge [1181] and insufficient visual ability [144]. Extrinsic Factuality Hallucination. MLLMs may generate the correct description of an image but produce information that contradicts or remains unverifiable in real-world knowledge. This kind of hallucination results in the creation of non-factual information, arising not from misinterpretation of images but from other factors. Faithfulness Hallucination. From visual-conceptual perspective, MLLMs may generate unfaithful or inaccurate outputs in response to user-provided image. Considering the elements present in the image, faithfulness hallucination can be categorized into two main types: Object Inconsistency. The model generates description or explanation for an image, incorporating objects or features that are either missing or do not actually exist. Logical Hallucination. The model generates description or explanation for an image that includes missing or non-existent logical relationships, attributes, or quantity. 7.2 Sources of Hallucinations This section discusses the various sources of hallucinations in LLMs and MLLMs, examining issues related to flawed data sources, training, and inference processes. Hallucination from Data Flawed Data Sources. LLMs suffer from biased or incorrect textual data, and MLLMs can incorporate erroneous visual data. They also learn stereotypes from pre-trained corpora of world knowledge. Mislabeled textual corpora and visual data, imbalanced distribution in datasets (e.g., underrepresenta45 Published in Transactions on Machine Learning Research (10/2025) tion of certain demographics), and outdated information can lead to inaccuracies in classification and description tasks [190, 981]. For example, hallucinations in MLLMs are often observed in which the model predicts possible objects or actions that are not supported by the image but are plausible from commonsense [770]. Knowledge Boundaries. LLMs and MLLMs face limitations in domain-specific knowledge (e.g., medical analysis in 1021 and embodied decision-making in 520) or struggle to recognize and interpret up-to-date content. For instance, MLLMs may fail to accurately recognize images of new electronic devices, enterprise logos, or cultural symbols that are absent from their training data. Hallucinations from Training Architecture Choices. The architectural design of MLLMs, often based on complex neural networks that integrate features of both vision and language models, might contribute to hallucinations. For instance, inappropriate conditioning of the visual and textual components leads to model misinterpretations [144]. Inherent Mechanism. There is an inherent statistical lower-bound on the hallucination rate of language models given the sufficient presence of facts for training transformers [430]. In other words, certain level of hallucination is necessary for the model to minimize cross-entropy across large and varied pre-training data. Exposure Bias. This issue may arise when the model is overtrained on certain types of images or texts, leading to an overrepresentation of these elements in the models outputs, regardless of their relevance or accuracy in new contexts [44]. This is often the result of catastrophic forgetting [1126]. Hallucinations from Inference Sampling Randomness in Decoding. LLMs and MLLMs often use stochastic methods to generate outputs to textual and visual content. This randomness can lead to inaccuracies, particularly when dealing with complex tasks, ambiguous instructions, or intricate visual scenes. Imperfect Decoding Representation. Challenges to adequately represent visual and textual information in the model can result in inaccuracies when generating descriptions or interpretations of visual data [892, 138]. Path Dependence. Autoregressive LLMs and MLLMs suffer from error propagation and selfconsistency failures due to their token-by-token prediction nature. Any intermediate errors from previously generated tokens will have compounding effect on the subsequently predicted token. Empirical studies have demonstrated that, assuming constant per-token error rate, the generation quality can deteriorate rapidly with respect to the sequence length [42]. Additionally, inconsistencies with earlier generated content are common when hallucinations occur [975]. 7.3 Hallucination Detection and Measurement As shown in Figure 19, the process of detecting hallucinations in foundation models generally involves three key steps: first, breaking the response into distinct parts; second, extracting the facts from each part; and third, assigning score to each fact. Next, we will provide detailed discussion of hallucination detection and measurement methods across various types of foundation models. 7.3.1 Detecting Language Hallucinations Identifying hallucinations in LLMs is crucial to maintaining the reliability of their outputs. Traditional metrics, mainly based on word overlap matching, are inadequate to distinguish subtle differences between plausible content and hallucinations. This emphasizes the need for advanced detection techniques specifically designed for LLM hallucinations. Here, we discuss three distinct pipelines, differentiated by their use of external knowledge and evaluation metrics. Retrieval-based Detection. To accurately pinpoint factual inaccuracies in LLM outputs, straightforward approach is to compare the generated content with reliable knowledge sources. This idea is similar to the traditional fact-checking process [51, 49]. Building on this concept, line of works has employed LLMs to verify generated claims using evidence retrieved from external sources [148, 317, 433, 658, 178]. This 46 Published in Transactions on Machine Learning Research (10/2025) Figure 19: Illustration of hallucination detection in foundation models. The process generally involves three steps: (1) decomposing the model output into individual factual units, (2) verifying each fact against external knowledge or ground truth, and (3) aggregating the correctness scores to produce an overall hallucination score. The examples show applications in both LLMs and image generation models. method generally encompasses three essential phases: extracting the claims, retrieving relevant evidence from external knowledge, and classifying the veracity of the claim. The detailed implementation of each stage varies in different frameworks. In the context of long-form generation, tools such as FActScore [658], Core [423], and FacTool [178] prompt language models such as InstructGPT [703] or ChatGPT to extract all claims in target text. Similarly, EVER [433] is designed to extract all fact-related concepts from sentence and subsequently generate yes/no validation question for each concept in the generated text. For evidence retrieval, FActScore [658] picks the most relevant pieces of facts from the external knowledge base, where the relevance is ranked by retriever embedding distances. In addition, Chen et al. [148] use language model to summarize the retrieved texts, specifically serving as supporting evidence for the claims. EVER [433] and FacTool [178] utilize search engine to acquire evidence from queries generated by language model. These methods can be further classified into two types based on their ability to detect factual fabrication, which involves generating information that cannot be verified. The first category employs language model to directly label claims as True or False [658, 178], while the second category additionally considers the possibility of Not-Enough-Information (NEI). Model Internal State. series of studies have explored the use of LLM internal states for hallucination detection. Varshney et al. [914] assess hallucinations by examining the lowest token probability in key concepts, suggesting that lower probability indicates higher likelihood of hallucination. Luo et al. [612] implement self-assessment method, where the proficiency of an LLM is judged by its ability to reconstruct concept from self-explanation. This approach uses the perplexity of the generated response as measure to gauge the level of concept understanding. Yao et al. [1077] explore hallucinations as form of adversarial attack by using gradient-based token replacement, finding that the initial tokens generated from standard prompts have lower entropy compared to adversarial ones, leading them to decide an entropy threshold to detect hallucinations. Uncertainty Estimation. Recent advancements in LLM research have seen focus on uncertainty estimation. We will briefly discuss this in the context of hallucination evaluation and provide more general explanation in Section 8. Built upon techniques such as deep ensembles and conditional entropy, entropybased methods [1029, 912, 267] establish connection between hallucination likelihood and predictive uncertainty. Guerreiro et al. [329] explore variance estimation through Monte Carlo Dropout [280] and investigate log-probability-based approach by measuring model confidence through length-normalized sequence log-probability. Additionally, some methods employ LLMs themselves to estimate uncertainty and detect hallucinations. SelfCheck [652] detect errors in complex reasoning within LLMs. The burgeoning field of prompting-based metrics has also gained attention, with Chiang & Lee [179] leveraging the instructionfollowing capability of LLMs to evaluate the faithfulness of content. Beyond these methods, researchers 47 Published in Transactions on Machine Learning Research (10/2025) have also gleaned insights from LLM behaviors. Natural language prompts [1039] have played pivotal role in this direction. Manakul et al. [630] tackle this issue by examining the consistency of factual statements across multiple LLM responses. Another innovative approach [10] uses indirect queries to subtly elicit specific information. This approach mirrors investigative interview techniques, offering nuanced evaluation of consistency. Taking step further, the multi-agent perspective, particularly exemplified by LMvLM [198], involves one LLM as the examiner to question another, the examinee. This method, inspired by legal crossexamination techniques, aims to reveal inconsistencies in multi-turn interactions. Collectively, these diverse approaches contribute to deeper understanding of uncertainty in language models and provide innovative ways to measure it."
        },
        {
            "title": "7.3.2 Measuring Multimodal Hallucinations",
            "content": "Conventional statistical metrics, such as BLEU [706], CIDEr [916], and ROUGE [551], are commonly used to evaluate hallucinations. However, it has been suggested that these metrics might not be highly appropriate for assessing detailed descriptions from MLLMs [1201]. To address this issue, recent metrics [770, 532, 944, 1046] shift their focus towards object hallucination, as objects represent fundamental components that compose the visual scene and contribute to correct visual understanding. In addition, Li et al. [532] propose PollingBased Object Probing Evaluation (POPE), novel approach that utilizes polling-based queries to prompt MLLMs [325] with straightforward yes-or-no questions regarding the existence of specific objects in an image. By asking targeted questions, POPE provides more stable and flexible evaluation of object hallucination. While recent evaluations have introduced numerous metrics focused on the phenomenon of object hallucination, it is crucial to consider their suitability for capturing the intricacies of MLLMs, as many elements contribute to the visual semantics of an image. Apart from evaluation metrics that focus only on object hallucinations, Wang et al. [944] introduce HaELM, framework of hallucination evaluation by fine-tuned LLaMA [13]. The work of Gunjal et al. [332] presents novel dataset, M-HalDetect, designed specifically to identify hallucinations in visual question answering (VQA) tasks. Wang et al. [958] introduce an innovative dataset named Mementos, aimed at assessing the reasoning abilities of MLLMs in understanding sequences of images. Additionally, they propose unique type of hallucination, termed behavior hallucination, that arises specifically in the context of image sequence comprehension. Additionally, Wang et al. [943] propose an LLM-free method to evaluate object existence, object attribute, and object relation hallucinations costeffectively and efficiently. Although Retrieval-Augmented Generation (RAG) [505] is common technique to alleviate the hallucination problem, there are no sufficient guarantees that the problem is fully eliminated. To address this scenario, Wu et al. [1013] curate benchmark of hallucination detection methods in the RAG-based system. 7.3.3 Measuring Image Generation Hallucinations Recent studies [1006, 389] have evaluated the compositional ability of image generation models, revealing their tendency to hallucinate. These benchmarks prompt image generation models with text descriptions that contain multiple concepts. Next, they analyze how many of these concepts actually appear in the generated image by creating one question per visual concept and using strong MLLM to answer all questions. This process identifies hallucinations that occur when the model misses some concepts or introduces concepts that were not present in the original description. 7.4 Hallucination Mitigation 7.4.1 Reducing Hallucinations in LLMs There is pressing need for the development of novel and reliable methods to mitigate hallucinations in LLMs, particularly to meet the demands in real-world applications and substantially improve their generalization. Our review categorizes them into two primary types: data-centric and model-based approaches. Data-Centric Methods. The training dataset plays crucial role in the occurrence of hallucinations in LLMs. McKenna et al. [641] indicate that the primary cause of hallucinations is the memorization of training data. This issue manifests when user queries closely align with pre-trained data, often resulting 48 Published in Transactions on Machine Learning Research (10/2025) in LLMs generating inaccurate or misleading responses. Additionally, some datasets exhibit inconsistencies in the factual alignment between the input text and the reference target [418]. This problem is evident in dataset constructions [491, 225], and downstream tasks such as open domain dialogues [648, 391]. Besides this, many pre-trained LLMs have fixed parametric knowledge base on specific timestamp, and they are often deployed offline (e.g., limited access to the internet or data storage firewall for privacy). This limitation can potentially lead to hallucinations when the model encounters questions that its knowledge base does not cover [52]. Therefore, the data-centric approaches that can benefit the language models (parametric and external) knowledge bases are practically reasonable. To further reduce hallucinations in natural language generation (NLG) tasks, numerous attempts have been made to automatically [250, 583] and manually [293, 941, 707] cleanse and refine the training sets for improving accuracy and reliability. In recent years, the importance of meticulous data filtering has become evident in effectively mitigating hallucination issues in LLMs. For instance, GPT-3s pre-training data [113] is cleaned using several highquality reference corpora to ensure its quality. Touvron et al. [896] improve the quality of the pre-training corpus by incorporating data from reliable external sources such as Wikipedia. This process involves carefully selecting and up-sampling data from these sources to enrich the content and enhance the faithfulness of training data. Model-based Methods. The choice of training strategies and model architectures also plays crucial role in mitigating hallucinations. Past works [884, 40] have showcased correlation between deficient comprehension capability in the encoder and the frequency of hallucinations. Recently, McKenna et al. [641] conducted study on factors influencing hallucination and revealed that the main determinant is the extent to which training data are memorized. Specifically, models tend to prioritize generating outputs based on their parametric (internal) knowledge, rather than relying on the external information from the input prompt. This memorization, known as parametric knowledge bias [627, 418], significantly contributes to the occurrence of hallucinations. Numerous techniques [496, 638, 252, 825] have been developed to address this bias in earlier, smaller-scale language models. Some retrieval-augmented methods improve language models by incorporating external information from large corpora. Borgeaud et al. [103] introduce RETRO (Retrieval-Enhanced Transformer), an auto-regressive language model that leverages document chunks from massive 2 trillion token database. Peng et al. [716] further advance black-box LLM by incorporating external knowledge, therefore improving the models response quality through iterative prompt revision. To address the challenges of factually inaccurate text generation and source attribution in language models, Ram et al. [751] propose novel approach called In-Context RALM (Retrieval-Augmented Language Modeling). This approach maintains the original model architecture while prepending grounding documents from corpus to the input. However, the development of model-based strategies for reducing hallucinations in LLMs is still at an early stage in the academic community. Recent research [674] develops an iterative algorithm to prompt LLMs to identify and eliminate self-contradictions in their generated text, thereby enhancing both fluency and informativeness. Chen et al. [137] use LLMs to corrupt text and fine-tune compact editors to denoise these faux hallucinations. Models fine-tuned with carefully crafted tasks and datasets can also mitigate hallucinations. Elaraby et al. [258] introduce lightweight and knowledge-free framework for this fine-tuning process, with the aim of reducing hallucinations in open-source language models. Current state-of-the-art LLMs, i.e., ChatGPT [692], GPT-4 [693], and Llama 3 [13], have made significant advances in reducing hallucinations during the RLHF stage. It is also worth noting that artificially created hallucination data are leveraged to train the reward model for GPT-4 [693]. 7.4.2 Reducing Hallucinations in MLLMs Prior studies have attempted to address the problem of hallucinations in generating responses from images using small-scale multimodal models [1153, 93, 205, 448]. However, these approaches primarily focus on short image captioning tasks, making them inadequate for applications in MLLMs that aim to provide comprehensive and detailed descriptions. In recent advances, methods for addressing hallucinations in MLLMs can be mainly divided into three categories: training approaches, post-processing techniques, and decoding methods. Training Approaches. Visual instruction-tuning [570] has significantly improved the zero-shot capabilities of MLLMs on new tasks. Built upon this, Liu et al. [567] introduce the Large-scale Robust Visual (LRV)- 49 Published in Transactions on Machine Learning Research (10/2025) Instruction dataset and the corresponding LRV-Instruction framework. The framework incorporates both positive and negative instructions to enhance the robustness of visual instruction-tuning and reduce hallucinations. Furthermore, recent studies introduce several new instruction-tuning datasets aimed at reducing hallucinations. For example, Li et al. [519] present the Multi-Modal Multilingual Instruction Tuning (M3IT) dataset. The M3IT dataset consists of 40 meticulously curated datasets, comprising 2.4 million instances and 400 manually crafted task instructions that have been reformulated into vision-to-text format. Gunjal et al. [332] introduce the M-HalDetect dataset, designed to detect and prevent objects and logical hallucinations in detailed image descriptions. The authors train fine-grained multimodal reward model using InstructBLIP and evaluate its effectiveness using best-of-n rejection sampling, which is technique used in LLMs to improve output quality by generating multiple candidate outputs and selecting the best one according to predefined scoring function. Since common practice in instruction-tuning dataset construction implicitly teaches an LLM to answer all questions, even for problems it cannot answer, this will lead to the tendency of LLM to hallucinate. To address this problem, Zhang et al. [1133] proposed different approach, which enables LLMs to refuse to answer questions beyond their capabilities. LLMs with this ability were shown to effectively reduce hallucination by refusing to respond to particularly challenging questions. Post-processing Approaches. These approaches in MLLMs refer to the techniques used to refine or alter MLLM outputs after the inference process. These techniques aim to improve the quality, accuracy, and usability of models predictions. In the context of small-scale multimodal models, Ngo et al. [687] introduce pseudo labeling and efficient post-processing techniques to improve vehicle retrieval accuracy. Hoxha et al. [376] introduce two post-processing strategies based on hidden Markov models and the Viterbi algorithm, which improve the faithfulness of image captioning by leveraging probabilistic modeling of language structure to correct and optimize generated captions. These strategies effectively correct hallucinations in generated descriptions while improving the overall coherence of the captions. Duan et al. [247] utilize an auxiliary model to first learn the truthful direction of MLLMs decoding and then apply truthful-guided inference-time intervention during decoding to alleviate hallucinations. In the broader context of MLLMs, post-processing approaches can be mainly categorized into two types: LLM-assisted Approaches. These methods involve the use of foundation models, such as GPT, to correct and refine the outputs of smaller multimodal models. Maaz et al. [623] implement GPT-assisted mechanism that refines and optimizes enriched annotations, thereby generating high-quality instruction data. Similar to small models, Yin et al. [1089] focus on improving the reliability of visual hallucination diagnosis, in which GPT is leveraged to rectify inaccurate diagnosis. In addition, Zhou et al. [1201] utilize GPT to correct hallucinatory captions for MLLMs. LLM-free Approaches. These methods do not rely on LLMs. Instead, they may engage in selfcorrection or use smaller model as revisor. Zhou et al. [1201] focus on three key factors contributing to object hallucination: co-occurrence, uncertainty, and object position. Based on these insights, they propose LMM Hallucination Revisor (LURE) to address the problem of object hallucination in MLLMs. Drawing inspiration from LM-Switch, Zhai et al. [1124] introduce control parameter, referred to as switching value\", to manage hallucinations in language generation by modifying word embeddings. Decoding Approaches. In recent years, various methods have employed decoding approaches to address the issue of hallucinations in MLLMs during inference. Chuang et al. [191] reduce incorrect fact generation in LLMs through the contrast of logit differences between the later and earlier layers, and this approach can be extended to MLLMs. Moreover, recent efforts have been dedicated to mitigating hallucinations in MLLMs as well. Leng et al. [502] propose training-free decoding approach called Visual Contrastive Decoding, which contrasts the output distributions with the original and distorted visual inputs. This approach effectively calibrates the models over-reliance on unimodal priors and statistical bias without utilizing external models. Similarly, Chen et al. [170] contrast the logit distribution of different partial visual inputs to approximate the optimal visual context during decoding. Besides, Huang et al. [392] present decoding approach based on an over-trust penalty and retrospection-allocation strategy to alleviate the problem of partial overtrust. Building on these decoding approaches, Li et al. [539] introduce VISTA, training-free inference-time intervention that mitigates hallucinations in MLLMs by reinforcing visual information in activation space and leveraging early-layer activations, achieving consistent improvements across models and decoding strategies. Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "7.5 Current Limitations and Future Directions",
            "content": "While recent research advances have partially defined the concept, developed measurements, revealed causes, and proposed solutions for hallucinations in foundation models, it remains challenge to formulate the phenomenon comprehensively. In the sections above, our survey summarizes the existing work and presents them in our paradigm. Next, we will discuss current challenges and shed light on potential future directions that the community should focus on. When validating whether language model response is hallucinatory, popular line of research is to retrieve external texts (e.g., scientific facts and historical archival) as supportive evidence. This approach requires highly trusted knowledge database that is often crowd-sourced and is presumed to be factual. However, there are no rigorous guarantees regarding the trustworthiness of these external knowledge bases, and their vast size makes manual scrutiny impractical. Furthermore, after evidence retrieval, the current pipeline typically employs verifier to compare the retrieved fact with the target models statement for hallucination classification. If the language model is prompted to execute this validation, it is important to note that the verifier may also exhibit hallucinatory behavior. Additionally, the statistical correlation between the mechanisms of the autoregressive hidden states and the hallucination phenomenon is still unclear. Lastly, while most existing research focuses on validating the language generation from LLMs and MLLMs, research is insufficient in the synthesis of other modalities (e.g., images, videos, and audio). To address the limitations outlined in current research on hallucinations in foundation models, several avenues for future work emerge. Firstly, enhancing the reliability and veracity of the knowledge bases used for evidence retrieval is essential. Future studies shall focus on the development of filtering and verification systems for external knowledge. Secondly, it is crucial to mitigate the risk of hallucinations in validation models. Potential frameworks include multi-agent collaborations where multiple independent models interact and cross-verify the target output. Furthermore, deeper understanding of the underlying mechanisms of language models, particularly the statistical properties and dynamics of autoregressive hidden states, will help mitigate hallucinations. Techniques like mechanistic model editing and representational latent editing hold the potential to promote more faithful language generation. Incorporating refusal mechanisms is rising topic to study knowledge boundaries and mitigate model hallucinations [1042, 1212]. Future work shall consider directions for improving refusal calibration, integrating refusal with multi-agent and retrievalbased pipelines, and extending refusal capabilities to multimodal generation tasks beyond text. Lastly, since image diffusion models [771] and visual autoregressive models [1100, 524] are increasingly conditioned on textual prompts, the community should expand the scope of hallucination research to the generation of images, videos, and audio. 51 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "8 Uncertainty",
            "content": "Though modern foundation models possess impressive capabilities across wide range of domains and tasks, harnessing their power reliably requires clear understanding of the uncertainties inherent in their outputs. In the context of foundation models, uncertainty refers to the degree of confidence in the models predictions or generated content. This uncertainty can arise from multiple sources: epistemic uncertainty, which is due to limited or imperfect knowledge encoded during training (e.g., gaps in the data or model misspecification), and aleatoric uncertainty, which stems from inherent randomness or ambiguity in the data itself (e.g., inherently noisy or ambiguous task definitions). However, it is important to note that this distinction also has limitations [54, 322, 907, 671]. Beyond these, real-world deployment introduces further uncertainty due to distribution shifts, novel scenarios not represented during training, or unforeseen user inputs. Recognizing and managing these different types of uncertainty is essential for responsible and safe use of foundation models in high-stakes applications. Fundamental Sources Data: Baan et al. [54], Ott et al. [702], Osband et al. [701], Lahlou et al. [483] Input ambiguity: Kuhn et al. [473], Kim et al. [447], Liu et al. [562], Tamkin et al. [868] Model architecture: Lakshminarayanan et al. [486], Malinin & Gales [629], Gal & Ghahramani [280] Training objectives: Wei et al. [982], Eikema & Aziz [255], Tian et al. [881] Origins & Nature e n a o y a c Quantification Approaches Mitigation & Control Aleatoric vs. Epistemic Irreducible randomness: ? Zhang [1144] Knowledge gaps: Osband et al. [701], Wang et al. [955], Ye et al. [1085] Discrepancy measures: Malinin & Gales [629], Glushkova et al. [306], Lahlou et al. [483] Probabilistic Methods Entropy measures: Kuhn et al. [474], Lin et al. [558], Malinin & Gales [629], Duan et al. [246] Self-consistency: Kadavath et al. [429], Lin et al. [553], Si et al. [826], Wang et al. [969] Semantic grouping: Kuhn et al. [474], Lin et al. [558] Embedding space: Chen et al. [143] Calibration & Expression ECE/MCE metrics: Naeini et al. [680], Guo et al. [334], Huang et al. [399] Temperature scaling: Guo et al. [334], Desai & Durrett [217], Xiao et al. [1031] Verbalized confidence: Kadavath et al. [429], Lin et al. [553], Tian et al. [882] Linguistic markers: Zhou et al. [1188], Mielke et al. [654], Stengel-Eskin et al. [853] Operational Strategies Statistical Guarantees Selective abstention: Geifman & El-Yaniv [299], Kamath et al. [431], Si et al. [826] Deferral to larger models: Gupta et al. [344] Clarification requests: Kuhn et al. [473], Kim et al. [447], Stengel-Eskin & Van Durme [850] Ambiguity detection: Liu et al. [562], Tamkin et al. [868], Yin et al. [1091] Conformal prediction: Shafer & Vovk [800], Kumar et al. [476], Angelopoulos et al. [32] Risk control: Angelopoulos et al. [31], Snell et al. [835], Deng et al. [215] Generation bounds: Quach et al. [739], Deutschmann et al. [219], Yadkori et al. [1053] Prompt risk control: Zollo et al. [1217] Figure 20: Taxonomy of Uncertainty in Foundation Models. 8.1 Sources of Uncertainty Uncertainty in foundation models can be categorized into aleatoric and epistemic types. Aleatoric uncertainty is primarily influenced by data factors, while epistemic uncertainty is largely affected by modeling decisions. We will discuss the impact of these components in both the training and inference stages, with comparison presented in Figure 21. 8.1.1 Data To begin, we will consider how the nature of language itself introduces uncertainty into the generation process of LLMs [54, 702], which both consume natural language as input data and produce it as output data. Given some input context to language model, there are usually many possible responses for several reasons. First, the input may be reasonably interpreted to have multiple different meanings. This could be because the context is vague (She watched the man with binoculars), very complex (as some reading comprehension questions are, even for humans), or contains spelling or other errors. Besides ambiguity in the input, certain queries may be inherently more open-ended and allow for many reasonable responses. This might include request to complete fictional story, tell joke, or give position on some political or social issue. Finally, given fixed input interpretation, equivalent answers to query may be expressible in many ways. For example, given the input context What is the capital of Rwanda?, Kigali is the capital of Rwanda and The capital of Rwanda is Kigali offer semantically equivalent answers with different surface forms. 52 Published in Transactions on Machine Learning Research (10/2025) Figure 21: The comparison of aleatoric and epistemic uncertainty in machine learning. Aleatoric uncertainty originates from data variability, while epistemic uncertainty results from model limitations. The former is tied to inherent noise in observations, while the latter is tied to insufficient knowledge in representation. The uncertainty in an LLM generation due to natural language data stems both from the training data and the prompt inputted to the model during inference. When the training dataset is small or not sufficiently general, the model may not have the relevant knowledge to effectively process some context [701? , 483, 714]. If the training data contains large amount of ambiguous language, the trained LLM may reflect this uncertainty in its outputs. Other sources of uncertainty introduced by training data include diverse, conflicting, and outdated information. In deployment, further uncertainty is introduced by specific text data given as input to the LLM. Queries could be ambiguous [473, 447, 562], and tasks or instructions could be open-ended or underspecified [868], making it difficult for the model to express an appropriate level of confidence in its response. Also, relevant information could be excluded from the context [1108], or users may produce input errors. 8.1.2 Model In addition to the uncertainty introduced by data in training and inference, the model itself also contributes to the uncertainty in the generation process, given an input prompt. Architecture choices may not reflect the underlying data-generating process. Different modeling techniques like ensembling [486, 629, 306, 957] or Bayesian inference [280, 702, 1029] can be applied with the hope of accurately characterizing the true posterior probability. However, these methods can be computationally expensive and potentially ineffective [4, 704]. Besides architecture, the typical optimization objective of producing the most plausible answer (i.e., maximizing observed sequence probability 255, 254) may not align to produce the most correct and factual answer [881], and in general, the cross-entropy objective has been shown to lead to overconfidence [982]. Finally, though massive pre-trained models are an effective tool for combating the uncertainty introduced by the input context, the popular approach of fine-tuning these LLMs for custom use cases may dilute these generalist capabilities [1110]. 8.1.3 Aleatoric vs. Epistemic Uncertainty Besides identifying how uncertainty may arise due to data and model factors, it may also be useful to characterize uncertainty in LLM responses as either aleatoric or epistemic [? 1144]. Aleatoric uncertainty, sometimes referred to as data uncertainty, exists due to the inherent randomness in the data-generating Published in Transactions on Machine Learning Research (10/2025) process. Additional information cannot be used to reduce aleatoric uncertainty. For example, suppose language model was asked to predict the probability of heads on the flip of fair coin. In that case, no additional context or training data would enable better prediction than 50%. On the other hand, epistemic uncertainty arises precisely because of lack of knowledge. Epistemic uncertainty may be reduced by incorporating additional data, for instance, by including or prioritizing more informative examples in the training set [701, 955] or incorporating appropriate few-shot examples in the context [1085, 227, 528, 855, 1108]. Section 8.2.1 discusses more of this work in detail."
        },
        {
            "title": "8.2 Quantifying and Addressing Uncertainty",
            "content": "While some uncertainty in LLM responses is unavoidable, considerable progress has been made in quantifying and addressing this uncertainty so that models can be deployed responsibly and reliably. Common measures for quantifying uncertainty use notions such as entropy to characterize the uncertainty in response and produce higher scores for outputs that are less likely to be correct. Methods have been developed to recalibrate confidence scores so that they better reflect the true probability of an answer being correct. Additionally, uncertainty estimates can be used to identify cases where the system should abstain from answering or seek further clarification before providing response. Researchers have also examined whether LLMs can generate linguistic expressions indicating their uncertainty for given output [702, 826, 474]. Finally, rigorous statistical methods, which are quickly gaining popularity in the deep learning community, have been applied to provide high probability bounds on LLM performance and risk [910, 856, 759]. Next, we will highlight important work in these areas, accompanied by an illustration in Figure 22. Figure 22: Overview of representative methods for estimating and mitigating uncertainty in foundation models. The illustration highlights how different answers may be generated with varying confidence levels, and categorizes existing approaches into calibration, word-based, selection, and distribution-free methods. 8.2.1 Estimating Uncertainty One critical ingredient for the reliable deployment of black-box LLM is the ability to measure the uncertainty in its responses so that appropriate decisions can be made based on its output [826]. Uncertainty in language model output is often quantified in terms of predictive entropy [474, 558, 629, 246, 970]. Although 54 Published in Transactions on Machine Learning Research (10/2025) predictive entropy can be calculated directly using output class probabilities for classification tasks, measuring the uncertainty in language model generations is more challenging problem, requiring knowledge of the distribution over all possible sequences. One popular way to address the challenge of sequence-level uncertainty quantification is to sample many potential generations from the model and use these samples to estimate the underlying distribution. For instance, Fomicheva et al. [274] use Monte Carlo dropout to draw samples, which are then utilized for quantifying uncertainty in machine translation. Malinin & Gales [629] instead employ an ensemble with Monte Carlo sampling techniques over the token outputs to produce both token-level and sequence-level uncertainty scores. Such approaches to measuring uncertainty via self-consistency are also studied in [429, 553, 826, 227, 474, 969]. While most self-consistency methods focus on the models natural language outputs, Chen et al. [143] offer an effective method for measuring sample consistency in the embedding space. Besides, Stengel-Eskin & Van Durme [849] explore sequence-level uncertainty in semantic parsing through the angle of calibration, where they leverage the minimum confidence across tokens. In addition to the challenge of sampling from the distribution of possible sequences, there may be many equivalent surface forms of correct response to question such as What is the capital of Germany? Accordingly, the desirable notion of uncertainty may go beyond the spaces of sequences into the space of semantic meaning. These challenges are outlined and addressed by Kuhn et al. [474], wherein set of sequences are sampled and grouped by semantic equivalence to measure uncertainty over meanings instead of uncertainty over output forms. Lin et al. [558] build on this work by proposing more sophisticated semantic uncertainty measures and removing the access requirement to the token-level scores of the potentially blackbox model. Also, Duan et al. [246] presents an algorithm based on similar notion that aims to better characterize uncertainty by focusing on the most relevant token. Finally, some have taken the approach of trying to explicitly identify epistemic uncertainty, which can then be addressed by incorporating additional information. Malinin & Gales [629] take an ensemble approach and characterizes epistemic or knowledge uncertainty using mutual information and the level of disagreement between models in the ensemble. Glushkova et al. [306] also apply an ensemble-based approach to quantifying epistemic uncertainty in machine translation. The use of more powerful and efficient techniques such as direct uncertainty prediction and heteroscedastic regression are investigated in Zerva et al. [1123] in the context of machine translation; they find these methods perform favorably compared to variance-based baselines such as MC dropout and deep ensembles while being considerably faster. Lahlou et al. [483] highlight the challenges of using Bayesian techniques or discrepancy-based measures of epistemic uncertainty and proposes Direct Epistemic Uncertainty Prediction framework, wherein secondary model is trained to estimate the point-wise generalization error and provides an upper bound on epistemic uncertainty. Their algorithm is shown to be useful in interactive learning environments, where the model can acquire novel examples and continue learning. Similarly, Osband et al. [701] use an epistemic neural network to identify uncertain data that should be prioritized in fine-tuning, achieving on-par performance while using half as much data as training without prioritization. Hou et al. [374] avoid the need to train separate model to predict the epistemic uncertainty, instead using multiple LLM queries for clarification to rule out data uncertainty so that the remaining uncertainty of each prediction can be prescribed to epistemic uncertainty. As another ICL-based approach, Yadkori et al. [1052] propose an iterative prompting method to identify when epistemic uncertainty is large and highlight its usefulness in setting with multiple good responses. More recently, Wang et al. [960] introduced Bayesian Low-Rank Adaptation (BLoB), which jointly updates both the mean and covariance of LLM parameters during fine-tuning to enhance uncertainty estimation, and Training-Free Bayesianization (TFB) [817], post-hoc method that converts pre-trained LoRA adapters into Bayesian models without retraining, offering efficient and accurate uncertainty quantification. 8.2.2 Calibration One popular method for characterizing models predictive uncertainty is concerning (confidence) calibration. For model to be well-calibrated, its confidence estimates should, on average, reflect the probability of its correct answers. The most common calibration measure in the deep learning literature is Expected Calibration Error (ECE) [680, 334], which measures the expected difference between confidence and accuracy over the data distribution. However, since it is impossible to calculate this quantity directly, ECE is 55 Published in Transactions on Machine Learning Research (10/2025) typically estimated: data points with similar confidence scores are binned together, and ECE is calculated as the mean absolute difference between average confidence and accuracy over all bins. Other popular measures of calibration error include Maximum Calibration Error (MCE) [680, 334], Brier Score [111], negative log-likelihood [356] and other novel variants [907]. Since 0/1 accuracy is often not suitable metric with respect to LLM performance, Huang et al. [399] propose Rank-Calibration Error, which captures whether higher uncertainty scores are associated with worse generations according to continuous metrics like ROUGE [551] or BLEU [706]. While modern neural networks have achieved impressive accuracy across wide range of tasks and improved calibration relative to simpler methods [659], significant miscalibration remains, usually in the direction of overconfidence [334, 934]. To address this remaining calibration error, post-hoc recalibration methods such as Platt scaling [723], temperature scaling [334], or histogram binning [1118] can be used to refine the confidence estimates of pre-trained model. Most of the work in calibration and deep learning has focused on the classification setting, where the softmax probability of class can be reasonably interpreted as confidence score. Accordingly, extending techniques for measuring and improving calibration to LLMs in classification (or other settings with singletoken answers from finite, discrete set) is straightforward. For example, Desai & Durrett [217] find that pre-trained encoder-only transformer models like BERT and RoBERTa are well-calibrated under finetuning and that techniques like temperature scaling and label smoothing can be effective in combating poor confidence estimates. Further, Xiao et al. [1031] perform large-scale analysis of how decisions made along the LLM deployment pipeline, such as model size, architecture, and training objective, affect downstream task calibration on sentiment analysis and NLI. They find that larger models generally give more accurate confidence estimates and that applying temperature scaling and fine-tuning with focal loss may be helpful. Zhao et al. [1168], Zhou et al. [1185] offer methods to debias answers for prompt so that confidence scores are calibrated based on the actual input instance under consideration, while Detommaso et al. [218] introduce the notion of multi-calibration [1223] into the LLM setting by grouping examples based on binary attribute labels produced by the model itself. Kadavath et al. [429] perform an extensive study of whether LLMs can evaluate the correctness of their own responses across tasks such as multi-choice question answering. They find that self-evaluation improves with model size, although calibration is worse for more complex and out-of-distribution tasks. They also find that popular alignment techniques such as RLHF may hurt the calibration of LLM output probabilities. To handle the case of population shift, e.g., across the distribution of subjects in sample of MMLU questions, Li et al. [527] propose to train recalibration method that adapts to new subset of the data given only few unlabeled examples. On the other hand, estimating confidence and measuring calibration is less straightforward when tasks are generative or open-ended (for the same sequence-related reasons outlined in Section 8.2.1). Thus much recent LLM calibration research has focused here [429, 1031, 828, 826, 882, 1171, 654, 587]. In early work highlighting this challenge, Ott et al. [702] analyze model calibration in the setting of neural machine translation, showing that these models tend to diffuse too much probability mass over the space of possible sequences. One popular avenue for addressing the difficulties of combining calibration and generation is the development of new methods for producing calibrated sequence-level confidence scores. To this end, Chen & Mueller [149; 150] combine self-consistency with self-evaluation to produce confidence score using method they call BSDetector and find it is more accurate than alternatives in identifying incorrect LLM responses for models like GPT-3 and ChatGPT. Si et al. [826] measure the calibration of GPT-3 on free-form QA using both the length-normalized language model output probability and self-consistency and finds both methods give more calibrated confidence scores than supervised BERT baseline. Tian et al. [882] study LLM calibration of models aligned with RLHF, finding that these models can verbalize confidence scores that are more reliable than the underlying output probabilities, an approach which is especially useful when the model is behind an API and these probabilities are not available (see Section 8.2.3 for more on verbalized expressions of uncertainty). While most work on the calibration of LLM generations has focused on language tasks like question answering and summarization, Spiess et al. [845] study the calibration of LLMs for code generation across several tasks, correctness criteria, datasets, and approaches. In addition to the approaches described above, researchers have also pursued techniques for better quantifying LLM confidence via model training, concerning an external recalibrator or the LLM itself. For instance, 56 Published in Transactions on Machine Learning Research (10/2025) Mielke et al. [654] address conversational agents overconfidence by training small auxiliary network to predict the appropriate level of confidence to be expressed. Liu et al. [587] offer further work in this direction, proposing to train new linear layer that predicts bias term to be added to the language models output logits. Their approach enables the reordering of candidate generations (as opposed to temperature scaling) and is tested on longer generations including full paragraphs. Kadavath et al. [429] study whether language model can be trained to predict the probability that free-form answer to question is correct; their experiments show promising results, although generalizing such behavior across distributions remains challenging. Lin et al. [553] use fine-tuning to teach GPT-3 model to express its own uncertainty on various mathematics tasks, finding that responses are generally well-calibrated and remain reasonable under distribution shift. Finally, supervised fine-tuning step is proposed in Band et al. [60] to induce linguistic calibration, where model outputs feature confidence estimates that enable downstream decision-makers to make calibrated probabilistic predictions."
        },
        {
            "title": "8.2.3 Verbalized Uncertainty",
            "content": "Generally, in machine learning, confidence scores are numeric values extracted from predictive model, for example, based on predicted class probabilities, logit entropy, or ensemble variance. However, the ability of LLMs to generate arbitrary text output enables paradigm in which language models may express their uncertainty directly in their natural language output. As an early example of such an approach, Kadavath et al. [429] verbalize language model calibration by verifying answers using the probability assigned to tokens such as True or IK (I know) conditioned on its output or articulating confidence scores using numeric verbalizations such as 30% or 80%. Their approach shows promise, although it may be difficult to generalize to new tasks or tasks that are difficult to format as multiple-choice. Additionally, Lin et al. [553] fine-tune GPT-3 to directly express its confidence in its output using verbalized probabilities (e.g., 61%), while Tian et al. [882] prompt model directly to output both confidence scores and linguistic markers of confidence (e.g., highly likely). Zhou et al. [1188] study how linguistic markers of certainty, uncertainty, or evidentiality such as Im sure..., think..., or Wikipedia says... affect model confidence. Their findings imply that LLMs are sensitive to epistemic markers in prompts, with more than 80% variation in accuracy, and that expressions of high certainty result in decrease in accuracy. Their results also suggest that the confidence scores that LLM outputs do not truly reflect epistemic and aleatoric uncertainty in response but instead are based on mimicking language use from the training set. This observation is supported by an extensive study of the ability of black-box models like GPT-4 to verbalize confidence in Xiong et al. [1039]. They find the verbalized uncertainty expressions overconfident and difficult to optimize across models and datasets with single strategy for prompting, sampling, and scoring. In other relevant work, Mielke et al. [654] train confidence calibration network to select linguistic expressions of uncertainty that should be included in the output of conversational agent. Stengel-Eskin et al. [853] propose LACIE, which splits verbalized uncertainty into explicit markers (e.g., Im not sure) and implicit markers (e.g., giving details or backstory, stating persons expertise, etc.). The models are trained to improve calibration by modeling listener who accepts or rejects answers based on their correctness. The generator is rewarded for providing correct answers that are accepted and penalized for incorrect answers being accepted or correct answers being rejected. 8.2.4 Addressing Uncertain Examples Selection is another established tool for addressing uncertainty [299, 300, 273, 257, 1216]. We use the term selection broadly to encompass methods that identify inputs that are particularly difficult for the model. We offer interventions like allowing the model to abstain from the prediction (the classic paradigm in selection) or request further information. Selection has been well-studied in the context of language models [199, 431, 826], and has been shown to improve outcomes concerning hallucination and safety [887]. Kamath et al. [431] investigate selective question answering under domain shift, proposing novel algorithm that incorporates out-of-distribution data to train selection model that identifies examples on which the model is likely to err. Gupta et al. [344] derive new score based on token-level uncertainty features, to identify examples that should be deferred from smaller model to larger model. Uncertainty scoring methods, for example, the semantic entropy-based measures proposed in Lin et al. [558], are often evaluated via selection to highlight 57 Published in Transactions on Machine Learning Research (10/2025) how such measures are useful for predicting the correctness of LLM responses. Stengel-Eskin & Van Durme [850] show that we can recover low-confidence examples in semantic parsing by rephrasing and asking for user confirmation, which is the number of questions the model abstains from while keeping model safety high. To support work on selection in LLMs, Yin et al. [1091] introduce the SelfAware dataset of questions that should be recognized as unanswerable. Given the opportunity for interactivity provided by the text interface, significant amount of research has gone towards algorithms to enable the LLM to request further information before responding, particularly in the case of ambiguous questions. For instance, Kuhn et al. [473] use few-shot learning to detect ambiguous questions that require clarifying questions, while Kim et al. [447] propose tree-based approach to disambiguating questions and retrieving missing information. As the interest in identifying ambiguous questions in LLMs has grown, there has been an accompanying effort to release public datasets that can be used to evaluate the relevant abilities. Liu et al. [562] offer AmbiEnt, dataset to test an LLMs ability to manage ambiguity in resolving entailment relations, finding their task difficult even for powerful commercial models like GPT-4. Additionally, Tamkin et al. [868] introduce AmbiBench, benchmark of ambiguous tasks where the ambiguity is introduced by the task description itself (as opposed to the specific instance of the task). Stengel-Eskin et al. [851] create dataset for identifying and disambiguating instances of the visual question-answering task with MLLMs. Besides, Stengel-Eskin et al. [852] introduce dataset of ambiguous queries and their logical forms and test whether models can recover both interpretations. Also, Saparina & Lapata [785] introduce similar ambiguous parsing dataset but with human-sourced SQL queries. 8.2.5 Distribution-free Uncertainty Quantification As LLMs are increasingly deployed in risk-sensitive domains such as medicine, law, and finance, it may be important to have not only an estimate of the uncertainty in models response but also high probability upper bound on the error rate at test time. Recently, there has been increasing research employing techniques from the Distribution-Free Uncertainty Quantification (DFUQ) family to control the risk of deep learning systems. This line of work generally descends from the literature concerned with conformal prediction [800, 927], wherein threshold on class probabilities is calibrated to produce prediction sets that fulfill some coverage (i.e., recall) guarantee. Angelopoulos & Bates [30] offer tutorial on the subject in the context of modern neural network applications, and Kumar et al. [476] illustrate the application of conformal prediction to multi-choice question answering with LLMs. To broaden its applicability, Angelopoulos et al. [32] derive version of conformal prediction for bounding the expectation of any monotone loss function and studies their method in open-domain question answering. Recent work has offered algorithms for producing bounds on more general loss functions concerning the mean [31], quantile-based risk measures like value at risk (VaR) [835], and measures of statistical dispersion like the Gini Coefficient or differences in loss among protected subgroups [215]. While it is straightforward to apply existing DFUQ techniques to classification with LLMs [835, 215, 476], the question of how best to apply them to generation tasks like summarization, chat, and code remains open. Multiple approaches have been proposed to apply these techniques to language model decoding. For example, Schuster et al. [795] utilize the Learn Then Test framework [31] to calibrate early exit criteria concerning the number of transformer layers applied to an input. Their goal is to identify when an LLM is sufficiently confident that it can exit the forward pass, and thus reduce the amount of computation used. In the conformal prediction vein, Quach et al. [739] calibrate stopping rule to produce set of candidate generations that with high probability contains suitable response (while removing redundant candidates), and Deutschmann et al. [219] incorporate conformal prediction into novel beam search algorithm. To mitigate the risk of models hallucinating answers, Yadkori et al. [1053] proposes conformal abstention procedure using measures of self-consistency that are evaluated by the LLM itself. Finally, Mohri & Hashimoto [667] enforce factuality in LLMs by using conformal techniques to determine level of specificity with which given question can be answered. As more general approach, Prompt Risk Control [1217] unites many techniques from the DFUQ family under single framework for selecting prompt (e.g., system prompt or set of few-shot examples) based on rigorous upper bounds on rich families of informative risk measures. The authors propose two-step prompt selection process. First, set of prompts is validated as producing an acceptable risk for some contextually 58 Published in Transactions on Machine Learning Research (10/2025) relevant measure before final prompt is chosen based on some performance metric, like average reward or accuracy. Prompt Risk Control can be applied to any bounded loss function, such as top-1 accuracy, ROUGE, or toxicity, and can be used to control risk measures, including tail quantities like value-at-risk or measures of statistical dispersion such as the Gini coefficient."
        },
        {
            "title": "8.3 Current Limitations and Future Directions",
            "content": "Much work has gone into methods to identify and address uncertainty in foundation model generation. However, existing results and methods are limited and much work remains to be done before these models can be responsibly and reliably deployed. First, many results in uncertainty quantification in LLMs are produced in limited settings. Experiments are usually performed on tasks like trivia question answering, which can be answered via single token, word, or short phrase. Further, the tasks under study also often assume that there is only one right answer: there may be no uncertainty in the correct response to Who won Super Bowl XX?. However, much LLM usage revolves around tasks that require generating long-form responses to open-ended queries, for which multiple reasonable answers exist. Some works have made progress in this direction [1130, 1131, 1095], it is unclear whether the results produced in these limited settings offer insight into more complex, uncertain, and sequential settings, such as chat or customer care. Alongside the difficulty of extrapolating results from simple settings, existing methods for improved uncertainty quantification of language model generations have come at the expense of generating multiple times for single query. Given that certain methods can increase costs by 2 to 20 timesor morecompared to standard inference, it will be infeasible for LLM users or service providers to adopt such approaches. Furthermore, although modern frontier models have shown some ability to express their uncertainty in words, there is good evidence that any correlation between accuracy and verbalized expressions of confidence is simply result of spurious features in training data [1188]. In addition, it should be noted that these verbalization techniques also usually require extra inference costs, even for the simplest methods, such as scoring p(True) for the generated answer. Finally, although these algorithms have largely not been tested in open-ended tasks and over long generations, it seems probable that new tools will be needed in this setting. For example, consistency-based methods assume that producing diverse samples for particular query indicates an example for which the model will likely give poor answer. However, model that can only produce single answer to query such as Tell me joke or Write me story would lack the capabilities to suit many modern LLM use cases. Overall, it is unclear whether any advanced method for quantifying LLM uncertainty in the zero-shot setting robustly outperforms baseline sequence entropy score calculated using token probabilities. Note that these scores are often unavailable for black-box LLMs behind an API. Furthermore, it is difficult to imagine how best to exploit probabilities taken directly from the language model, since these probabilities do not necessarily relate to the task at hand [640], but instead reflect the cross-entropy objective used in training and plausibility of an answer under the training data distribution (unless the model receives RLHF, which makes accurate uncertainty estimation even more difficult [429, 882]). Besides addressing the limitations in methodology and experimental settings mentioned above, future work in this area may benefit from taking broader view of the challenge of quantifying and addressing uncertainty in generative models. It could explore how uncertainty can be better quantified and addressed across the entire model development and deployment pipeline, and how interventions and measurements at different points in the pipeline interact and affect downstream outcomes. Also, it may be useful to gain more thorough understanding of how techniques for selecting, mixing, and filtering training data affect users ability to accurately estimate the models confidence on downstream tasks, whether via token probabilities or verbalizations. As new architectures and pre-training recipes emerge, they should be benchmarked for calibration, not only accuracy. Fine-tuning algorithms, whether supervised or RL, have been shown to worsen models UQ characteristics, and this phenomenon must be kept in focus as the community iterates on these methods. Finally, given model that has been pre-trained and fine-tuned and is ready for deployment, we might develop new methods to select system prompts and few-shot exemplars that reduce and control uncertainty in the wild, ideally with rigorous statistical methods like those provided by DFUQ [1217]. Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "9 Distribution Shift",
            "content": "Foundation models can occasionally produce unacceptable errors when faced with distribution shifts. These models, typically trained on fixed corpus, require additional adaptation for new tasks. This limitation is particularly challenging in our ever-changing world, where knowledge is constantly shifting due to various factors, such as changes in location or time [438, 452]. For instance, if model trained before 2023 is asked, Which team does Messi play for?, it may incorrectly assign higher probability to Paris Saint-Germain instead of Inter Miami. This example highlights the importance of understanding, detecting, and mitigating distribution shifts in foundation models to improve their reliability. Figure 23: Different types of distribution shifts in the perspectives of (1) statistics, (2) image, and (3) text. The concept shift scenarios show how two distinct classes can merge into single class when labels change. 9.1 Definition and Categorization The distribution shift [486, 45, 405] occurs when the independent and identically distributed (i.i.d.) assumption does not hold between the training and test distributions. This divergence between the training distribution ptrain and the test distribution ptest can significantly impact the performance of machine learning models, including foundation models. In essence, distribution shift describes the scenario where ptrain = ptest, which can degrade model performance and reliability. Based on how the data distribution changes, distribution shifts can be classified into three primary categories, with examples from various domains presented in Figure 23. Published in Transactions on Machine Learning Research (10/2025) d o d F f n u t Definition and Categorization Formal definition Lakshminarayanan et al. [486], Arora et al. [45], Hupkes et al. [405] Covariate Shift Fang et al. [264], Wiles et al. [999], Koh et al. [460] Label Shift Arora et al. [45], Koh et al. [460] Concept Shift Arora et al. [45], Hupkes et al. [405] Foundations & Definitions Yang et al. [1057], Fort et al. [275] Out-of-Distribution Detection LLM-based OOD detection Liu et al. [564], Zhang et al. [1129], Salimbeni et al. [782], Hu et al. [377] MLLM-based OOD detection Dai et al. [207], Huang et al. [388], Cao et al. [119], Xia et al. [1021] Data Augmentation Zhang et al. [1136], DeVries & Taylor [222], Yun et al. [1117], Wei & Zou [983], Li et al. [526], Trabucco et al. [897], Islam et al. [409] Adversarial Training Goodfellow et al. [311], Madry et al. [628], Zhang et al. [1135], Tramèr et al. [898], Bai et al. [56], Verma et al. [919], Zhu et al. [1208] Out-of-Distribution Generalization Label Smoothing Szegedy et al. [867], Müller et al. [673], Yuan et al. [1110] Invariant Learning Arjovsky et al. [43], Lin et al. [554], Sudre et al. [859], Zhang et al. [1136] Model Ensemble Dietterich [228], Lakshminarayanan et al. [486], Arbib [41], Jiang et al. [421], Wan et al. [930] In-context Learning Dong et al. [237], Min et al. [657], Bar et al. [64], Zhang et al. [1154], Huang et al. [403], Yuan et al. [1110], Reizinger et al. [762] Domain Adaptation Retrieval-augmented Generation Khandelwal et al. [443], Min et al. [656], Asai et al. [47], Gao et al. [292], Kang et al. [434], Siriwardhana et al. [831], Zhou et al. [1187] Ram et al. [751], Shi et al. [821], Zhang et al. [1142], Shao et al. [803], Neelakantan et al. [686], Seo et al. [798], Ma et al. [621] BehnamGhader et al. [77], Weller et al. [990], Liu et al. [597], Wang et al. [946], Zhang et al. [1151], Shao et al. [802] Fine-tuning with New Knowledge Yuan et al. [1110], Christiano et al. [187], Reizinger et al. [762], Biderman et al. [90], Lai et al. [484], Turner et al. [902], Jiang et al. [422] Test-time Training Sun et al. [863], Zhang et al. [1152] Model Editing Wang et al. [953], Yao et al. [1081], Hewitt et al. [363], Akyürek et al. [15] Current Limitations and Future Directions Scalability Yuan et al. [1110], Zhang et al. [1155], Verma et al. [919] Lifelong Learning Yang et al. [1058], Shi et al. [818], Kim et al. [452], Li et al. [512] Multimodality Wu et al. [1003], Zhang et al. [1155], Yin et al. [1092], Yu et al. [1103] Figure 24: Taxonomy of Distribution Shift in Foundation Models. Covariate Shift. This term refers to changes in the feature distribution p(x) while the relationship between the features and the labels p(yx) remains unchanged. This type of shift is prevalent in scenarios where the environment or context of the features change. Label Shift. It occurs when the distribution of labels p(y) changes, while the conditional distribution of features given labels p(xy) remains constant. This shift can result from changes in the real-world phenomena being modeled. Concept Shift. Concept shift, also known as conditional shift or concept drift, happens when the relationship between the features and the labels p(yx) changes. It reflects the evolution of the underlying problem statement or process over time. 9.2 Out-of-Distribution Detection Out-of-distribution (OOD) detection involves identifying inputs different from the training distribution [1057, 275], which plays vital role in enhancing the reliability of foundation models. By flagging unfamiliar data points for further scrutiny, these techniques help mitigate risks and maintain the integrity of the models performance. Despite the impressive generalization capabilities of todays extremely large foundation models, they remain fundamentally bounded by their training data. When deployed in dynamic open-world environments, such models can still encounter domain shifts, rare edge cases, or adversarial input that leads to overconfident but incorrect predictions. Therefore, OOD detection remains crucial: not only as safeguard against catastrophic failures in high-stakes applications (e.g., medicine, autonomous driving), but also as tool to trigger human oversight, guide active learning and preserve trust in automated decision-making systems. In the context of language models, Liu et al. [564] present an empirical investigation into the OOD detection capabilities of LLMs, specifically examining the LLaMA families with different model sizes. The study evaluates common OOD detectors in both zero-shot and fine-tuning scenarios, yielding several significant insights: (i) LLMs inherently serve as effective OOD detectors without requiring fine-tuning. (ii) In-distribution (ID) fine-tuning can boost OOD detection. (iii) Generative fine-tuning demonstrates superior generalization ability because it aligns with the pre-training objectives of LLMs. (iv) simple cosine distance OOD detector proves to be highly effective, attributed to the isotropic nature of LLM embedding spaces. Furthermore, 61 Published in Transactions on Machine Learning Research (10/2025) Zhang et al. [1129] propose novel approach for OOD detection, utilizing the likelihood ratio between pre-trained LLM and its fine-tuned variant. This method leverages the pre-trained LLMs extensive prior knowledge about OOD data, which, when fine-tuned with ID data, can effectively differentiate between ID and OOD samples. Expanding on these findings, Salimbeni et al. [782] explore the effectiveness of unmerged Low-Rank Adaptor (LoRA) [377] weights for OOD detection during the fine-tuning process, further contributing to the growing body of research in this area. In addition to textual OOD detection, recent advancements have begun to harness the powerful representation capabilities of foundation models in visual OOD detection. Dai et al. [207] propose method to enhance OOD detection by selectively generating information from LLMs. Their method incorporates consistencybased uncertainty calibration to estimate generation confidence scores and extracts visual objects from images to leverage the world knowledge encoded in LLMs. ODPC [388] utilizes LLMs to generate specific prompts for creating OOD peer classes, which are synthetic categories constructed from in-distribution (ID) semantics but intentionally placed outside the original label space. These peer classes act as proxy OOD categories during training, enabling the model to learn tighter ID class boundaries and better distinguish unfamiliar samples. This approach serves as an auxiliary modality for detection and introduces contrastive loss based on OOD peer classes to learn compact ID class representations and clarify boundaries between different classes. EOE [119] improves OOD detection by tapping into the expert knowledge and reasoning capabilities of LLMs without requiring actual OOD data. This method is designed to adapt to various open-world scenarios, making it suitable for (i) far OOD detection, where the OOD samples come from entirely different domains (e.g., animals vs. vehicles); (ii) near OOD detection, where the OOD samples are semantically close but from unseen categories (e.g., unseen dog breeds when trained on other breeds); and (iii) fine-grained OOD detection, where differences are subtle and intra-class variation is high (e.g., distinguishing between visually similar medical conditions). In the medical domain, CARES [1021] evaluate the OOD detection capability of medical LLMs, focusing on their ability to detect medical images that differ significantly from those used in the training phase. 9.3 Out-of-Distribution Generalization OOD generalization, on the other hand, aims to enhance the robustness of foundation models under new, unseen environments [360, 574, 1065, 1023, 682]. This approach improves the models resilience to variations in input data through diverse techniques. Prior to the era of foundation models, the deep learning community explored rich set of strategies for OOD generalization, supported by extensive empirical studies. These included (i) data augmentation, where transformations were applied to create synthetic training examples [470, 823]; (ii) adversarial training, which exposed models to adversarially perturbed inputs [311, 628]; (iii) label smoothing, regularization technique to prevent overconfidence [867, 673]; (iv) invariant learning, which aimed to capture features stable across environments [43, 12]; and (v) model ensembles, which aggregate predictions from multiple models to reduce variance and improve robustness [486, 228]. Yuan et al. [1110] evaluate these commonly used methods for LLMs, leading to important insights and conclusions. 9.3.1 Data Augmentation Data augmentation [1136, 222, 1117] involves creating new training examples through various transformations of the original data. These transformations range from simple operations, such as flipping or rotating images in computer vision tasks, to more complex manipulations by generative models to simulate the data distribution [526, 897, 409]. In the NLP context, Easy Data Augmentation (EDA) [983] refers to set of simple, low-cost textual augmentation operations synonym replacement, random insertion, random swap, and random deletion designed to increase lexical diversity without altering overall meaning. EDA was originally shown to be effective for small-scale text classification tasks, but its naive application to LLMs often degrades performance due to distributional shifts in token usage and disruption of learned long-range dependencies. The primary objective of data augmentation is to increase the diversity of the training set, thereby enabling the model to learn more robust features that generalize better to unseen data. However, recent research has shown that applying simple augmentation techniques, such as EDA, to LLMs often leads to performance degradation across most tasks, underscoring the need for more advanced augmentation methods tailored to foundation models. 62 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "9.3.2 Adversarial Training",
            "content": "Adversarial training [628, 56] is robust technique used to improve OOD generalization by exposing models to adversarial examples during the training process. These adversarial examples are inputs deliberately perturbed to mislead the model into making incorrect predictions, despite appearing similar to regular data. In earlier deep learning literature, adversarial training was shown to improve robustness in image recognition [311, 898], and it is now increasingly applied to LLMs and MLLMs. Mechanistically, adversarial training operates by solving min-max optimization problem [628, 1135]: the inner maximization finds the worst-case perturbation within certain norm-ball around each input, while the outer minimization updates model parameters to correctly classify these perturbed inputs. By repeatedly training on such challenging examples, the model learns smoother and more stable decision boundaries, which are less sensitive to input shifts, thereby improving robustness and generalization to unseen or distributionshifted data. In LLMs, Free Large-Batch (FreeLB) [1208], an adversarial training method that adds perturbations to the input data, improves generalization performance in most scenarios. Similarly, Verma et al. [919] introduce image perturbations in MLLMs through augmentations like noise addition, blurring, and median filtering. Additionally, they craft adversarial questions using conjunctions, disjunctions, and negations to challenge models reasoning abilities. Among the tested augmentations, Gaussian Noise Addition is identified as the most detrimental, causing the largest decline in performance. The study also finds that the complexity of questions, especially those with multiple connectives, significantly impacts the models performance. 9.3.3 Label Smoothing Label smoothing [867] is regularization technique used to improve OOD generalization by preventing the model from becoming overly confident in its predictions [673]. Unlike traditional training algorithms where models learn to assign probability of 1 to the correct class and 0 to all others, label smoothing introduces small probability to incorrect classes. This approach encourages models to maintain degree of uncertainty in their predictions, potentially improving their ability to generalize to unseen data. In the context of LLMs, however, the effectiveness of label smoothing has been called into question. Yuan et al. [1110] conducted experiments where they smoothed the hard labels in the training data but observed that this technique did not improve the LLMs generalization ability. 9.3.4 Invariant Learning Invariant learning [43] plays crucial role in OOD generalization by capturing invariant representations or predictors across different environments while disregarding spurious correlations. One notable approach of invariant learning involves the use of specialized loss functions, such as Focal Loss [554], Dice Loss [859], and Mixup Loss [1136]. Focal Loss was originally designed for class-imbalanced detection tasks, down-weighting well-classified examples to focus training on harder cases. Dice Loss, derived from the SørensenDice coefficient, is widely used in segmentation to maximize overlap between predicted and ground-truth regions, thus emphasizing recall. Mixup Loss linearly interpolates both inputs and labels between pairs of examples, encouraging the model to behave linearly in-between training samples and reducing overfitting to spurious patterns. These loss functions differ in their inductive biases e.g., emphasizing difficult examples, optimizing overlap, or encouraging linearity but all aim to produce more generalizable decision boundaries that are less sensitive to environment-specific correlations. By applying Focal Loss to the training process of LLMs, these models emphasize hard-to-classify examples and enhance their ability to handle diverse and unfamiliar inputs. 9.3.5 Model Ensemble Model ensemble [41, 486] is powerful technique for enhancing the robustness and performance of AI models in complex environments. This approach combines predictions from multiple models to produce more accurate and reliable final outputs. Yuan et al. [1110] evaluated model ensembling but observed limited improvement in generalization ability. However, building on this foundation, recent studies by Jiang 63 Published in Transactions on Machine Learning Research (10/2025) et al. [421] and Wan et al. [930] have introduced more advanced model ensemble algorithms, improving performance across several downstream tasks."
        },
        {
            "title": "9.4 Domain Adaptation",
            "content": "Unlike OOD generalization, domain adaptation tailors the model to domain-specific tasks by injecting domain-specific knowledge [296, 832], including in-context learning (ICL), retrieval-augmented generation (RAG), fine-tuning, test-time training, and model editing. These methods enable foundation models to specialize in particular domains while maintaining their broad capabilities."
        },
        {
            "title": "9.4.1 In-context Learning",
            "content": "In-context learning (ICL) shows great potential to address the gap between foundation models and domains not covered in their pre-training and fine-tuning data [237, 657]. Recently, ICL has gained attention as transformative approach for foundation models [64, 1154, 403]. It demonstrates the ability to adapt to new tasks or distributions without altering model parameters by adding domain-specific input-output pairs to the test example. This augmented input serves as guide, helping the model produce desired outputs for new tasks. Consequently, ICL offers flexible and efficient method for continual adaptation without the need for computationally expensive retraining. In the field of LLMs, the BOSS benchmark [1110] explores ICL for LLMs by using examples from both ID datasets and the training split of OOD datasets. The findings reveal that fine-tuning domain-specific models is advantageous when sufficient training data is available, while LLMs with ICL perform better in low-resource scenarios. Notably, the effectiveness of ICL varies across models and tasks, highlighting the need for taskspecific adaptation strategies. Complementing this research, Reizinger et al. [762] delve into the intricacies of ICL, focusing on its approximate non-identifiability and the implications for understanding LLMs. Through combination of mathematical examples and empirical observations, their work demonstrates how this approximate non-identifiability manifests in OOD generalization, providing deeper insights into the behavior of ICL in various contexts. For MLLMs, Zhang et al. [1155] demonstrate that ICL can significantly enhance the generalization capabilities, suggesting new approaches to overcome existing limitations. However, their study also investigates the robustness of ICL under various distribution shifts. The findings reveal that ICL is vulnerable to domain shifts, label shifts, and spurious correlation shifts between in-context examples and test data. 9.4.2 Retrieval-augmented Generation Retrieval-augmented generation (RAG) enhances foundation models by retrieving relevant information from external data sources to supplement input queries or generated outputs [443, 656, 47]. This process provides necessary domain knowledge, mitigating distribution shifts and improving generation quality [292, 434, 831, 1187]. In practice, RAG techniques are effective and efficient to apply in various unseen tasks with simple adaptation of the retrieval component, requiring minimal or even no additional training [751]. In the context of language models, Shao et al. [802] construct MASSIVEDS, massively multi-domain database comprising 1.4 trillion tokens of both general web data and domain-specific data. Their findings demonstrate that as the databases size and diversity increase, more distributions are covered during inference, reducing OOD scenarios. To incorporate this domain knowledge without requiring additional training, recent studies [821, 751] focus on in-context Retrieval-Augmented Language Models (RALMs). These models directly input concatenation of all retrieved texts as additional context to LLMs. For the choice of retriever, most work [1142, 803, 686, 798] employ an embedding model to decide what to retrieve. However, with the increasing prevalence of LLMs, researchers have begun using the models themselves as retrievers to improve accuracy [77, 621, 990, 597, 946]. In parallel direction, as these RAG methods may retrieve irrelevant information that even hurt the performance, Zhang et al. [1151] proposed RAFT to further finetune the LLMs to learn to disregard distractor documents\" within the provided context, thereby enhancing the models ability to focus on relevant information. The effectiveness of these In-Context RALMs has been 64 Published in Transactions on Machine Learning Research (10/2025) further demonstrated in several domain-specific tasks [1047, 517, 1038, 599], showcasing the potential of RAG in addressing real-world distribution shifts. To extend RAG to multimodal query input [1167], Wei et al. [980] create M-BEIR, multimodal instructionfollowing benchmark building on existing 10 diverse datasets. UniIR is trained on M-BEIR to take heterogeneous query to retrieve from heterogeneous candidate pool with millions of candidates in diverse modalities. Built upon it, UniRAG [805] employs UniIRs CLIP Score Fusion and BLIP Feature Fusion models as retrievers, improving performance in MLLMs. For visual question answering (VQA) tasks, RAVQA [555] proposed novel framework for joint training of the retriever and the answer generator, and FLMR [556] further improved the retrieval accuracy by combining multi-dimensional embeddings from language and vision models. Similarly, MuRAG [158] uses T5 [746] and ViT [240] for text and image encoding respectively, and retrieval from large-scale memory bank for knowledge-based VQA. To improve embodied agents, MART [1113] utilizes interaction data to fine-tune multimodal retriever based on preference learning. For image captioning and text-to-image generation tasks, RA-CM3 [1083] enhances performance by using pre-trained CLIP model to augment inputs for CM3 Transformer. These methods effectively address the shift in knowledge representation across modalities. Additionally, domain-specific multimodal RAG solutions have shown promising results in various fields [1025, 478, 872]. 9.4.3 Fine-Tuning with New Knowledge Fine-tuning is widely adopted method for addressing domain adaptation in foundation models [762, 1110, 456]. This technique involves adapting pre-trained models to specific downstream tasks by further training them on task-specific datasets. The primary goal is to enhance the models performance on new, unseen data that may differ from the data it was initially trained on. The BOSS benchmark [1110] evaluates vanilla fine-tuning for LLMs, which involves directly fine-tuning pre-trained models on ID datasets without any additional processes. This benchmark helps investigate the relationship between performance on ID and OOD datasets by varying factors such as model scale, training steps, available training samples, and tunable parameters. Observations indicate that fine-tuning with the full dataset generally yields superior performance for ID examples, while LLMs employing in-context learning (ICL) paradigms demonstrate better performance on OOD instances. Reizinger et al. [762] explore the non-identifiability of fine-tuning in LLMs, highlighting its implications for understanding and improving these models. They argue that fine-tuning is non-identifiable, meaning that models with similar fine-tuning performance (such as equivalent test loss) can exhibit markedly different behaviors when applied to real-world tasks. To address OOD generalization, Kirk et al. [456] investigate Reinforcement Learning from Human Feedback (RLHF) [187], which is typically implemented in three stages: (i) supervised fine-tuning (SFT), where the model is aligned with high-quality human-labeled data; (ii) reward modeling, where learned reward model predicts preference scores for outputs; and (iii) reinforcement learning, where the base model is optimized against the reward model. While SFT is the first step in RLHF, it can also be viewed as standalone fine-tuning approach. In their experiments, the authors compare the full RLHF pipeline against using only SFT and find that RLHF generally yields stronger generalization to new, unseen inputs, especially under significant distribution shifts between training and testing data. Jiang et al. [422] propose novel method for fine-tuning LLMs in domains where obtaining large volumes of high-quality, domain-specific data is challenging, such as healthcare or harmless content generation. They re-evaluated the Transformer architecture to identify the most impactful parameter updates. Their analysis revealed that within the self-attention and feed-forward networks of the Transformer architecture, only the attention parameters significantly benefit downstream performance when there is mismatch between the training and test set distributions. Based on this insight, they proposed Training All parameters but Inferring with only Attention (TAIA), which involves updating all parameters during training but utilizing only the fine-tuned attention parameters during inference. Additionally, recent studies have observed that parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptor (LoRA), can maintain more general capabilities from the pre-trained distribution while acquiring new knowledge from the fine-tuning data [90]. 65 Published in Transactions on Machine Learning Research (10/2025) Beyond parameter-efficient methods, recent work has explored activation steering as an alternative or complement to conventional fine-tuning for improving generalization. This line of research modifies model activations at inference time or during light-weight training to achieve desired behavioral shifts without largescale parameter updates. For example, Lai et al. [484] propose Joint Localization and Activation Editing, which identifies and edits specific activation subspaces relevant to the target domain, enabling effective lowresource fine-tuning. Similarly, Turner et al. [902] introduce Activation Addition, technique for steering model outputs by adding direction vectors in activation space, allowing the incorporation of new knowledge or behavioral adjustments without gradient-based optimization. These approaches can reduce overfitting to in-distribution features while selectively enhancing capabilities relevant for OOD settings. For multimodal scenarios, the proposal of EMMA [1069] adapts LLMs to the field of embodied multimodal agents. The key technique involves distilling the reflection outcomes of the LLM, which improves actions derived from analyzing mistakes in text world tasks. It uses these outcomes to fine-tune the vision-language models on analogous tasks in the visual world, which is capable of quickly adapting to the dynamics of the visual world. The cross-modality imitation learning is facilitated by novel DAgger-DPO algorithm, which ensures that EMMA can generalize to wide range of new tasks without further guidance. Belyaeva et al. [80] describe method to address OOD challenges by developing framework called HeLM (Health Large Language Model for Multimodal Understanding). HeLM integrates multiple data modalities, learns robust data encodings, and enhances predictive performance through comprehensive data utilization to achieve OOD generalization. Regarding model architectures, Ito et al. [410] find that models with multiple attention layers or those leveraging cross-attention mechanisms between input domains perform better in their constructed gCOG benchmark. Their study emphasizes that cross-modal attention and deeper attention layers are crucial for integrating multimodal inputs and improving generalization in the presence of distractors and new tasks. 9.4.4 Test-time Training Test-Time Training (TTT) methods view each test instance as an individual learning problem with its own generalization target. This method creates self-supervised learning task for each test sample and updates the model parameters at test time before making prediction. In the LLM era, Sun et al. [863] propose new class of TTT layers for sequence modeling that transform the hidden state into an optimizable model, with the update rule functioning as step of self-supervised learning. Zhang et al. [1152] propose comprehensive recipe of TTT with customizable large chunk updates to enable long sequence modeling validated on diverse modalities. By aligning the training and test data distributions, these methods significantly enhance model performance when faced with distribution shifts. 9.4.5 Model Editing All the domain adaptation methods discussed above modify models behavior by incorporating new knowledge. This process is closely related to model editing for foundation models [953, 1081], which aims to rectify specific errors without affecting unrelated inputs. To explore its potential in addressing distribution shifts, we will now provide an overview of model editing approaches, which typically adhere to three essential properties: Reliability: The edited model should successfully produce the desired output for the edited sample, such as correctly answering Inter Miami\" when asked Who does Messi play for?\" Generality: The corrections made should be consistent across equivalent contexts, for example, accurately responding to Which team is Messi in?\" Locality: The acquired knowledge should be minimally affected, ensuring that unrelated queries like Who does LeBron James play for?\" remain unaffected. These properties ensure the reliability, generality, and locality necessary for the effective and efficient correction of foundation model behaviors. Recent studies in model editing [363, 15] have also demonstrated promising performance in several OOD scenarios. Next, we will delve deeper into four distinct categories of model editing in LLMs (Figure 25), subsequently extending our discussion to address related issues in 66 Published in Transactions on Machine Learning Research (10/2025) MLLMs. Figure 25: An overview of model editing methods in LLMs. Given an incorrect response from the original model, different editing strategies correct factual errors by modifying or augmenting the models knowledge. Memory-based Model Editing. In memory-based approaches, an external memory, outside the intrinsic architecture of the pre-trained LLM, serves as repository for edited knowledge. LLM can access and modify this external memory during inference. For example, Language Patch [677] performs editing by integrating with library of patches in natural language, and MemPrompt [625] adopts growing memory bank as look-up table to store the edit sample and its corresponding prompts, which is used to alter the prediction of the edit sample. KAFT [512] further strengthens the controllability and robustness of LLMs working memory through counterfactual data augmentations. In this approach, the entity representing the answer in the context is substituted with an alternative but still plausible entity. This substitution is intentionally designed to introduce conflict with the genuine ground truth, thereby incorporating counterfactual and irrelevant contexts to standard supervised datasets. In addition to relying on parameter-based memory, IKE [1177] introduces novel factual information into pre-trained LLM via in-context learning, where set of demonstrations will alter the prediction of target factual detail when the input is influenced by an edit. To solve more complex questions involving chains of facts, MQuAKE [1180] enables editing by breaking down each question into iterative subquestions and retrieving the most pertinent fact from the edited fact memory. Classifier-based Model Editing. The classifier-based model editing paradigm aims to preserve pretrained parameters while utilizing classifier to determine whether behavior adjustment is necessary. In this approach, if sample falls outside the scope of the edit sample, the original model is applied to maintain predictions. Conversely, interventions occur when the sample is within the scope, with the specific interventions varying across different methods. SERAC [664] employs scope classifier to determine whether the original model or new lightweight model should be used for prediction. The new lightweight model is specifically trained for in-scope samples. In contrast, Language Patch [677], CaliNET [236], and T-Patcher [404] introduce additional trainable parameters to adapt the original model instead of requiring entirely new models. For example, Language Patch trains new gating head (acting as classifier) to combine predictions from the original prediction head and newly trained interpreter head. CaliNET and T-Patcher insert residual block into the original models feed-forward network (FFN) as an adapter. This adapter utilizes an activation operation on hidden states to determine whether the intervention should be activated. When the activations are zero, there will be no change to the original prediction. However, the success of these classifier-based methods heavily relies on the quality of the classifier, which also necessitates substantial number of unrelated samples for training. Alternatively, GRACE [354] edits model by adding retrieval-based adaptor to chosen layer that enables judicious decisions regarding the utilization 67 Published in Transactions on Machine Learning Research (10/2025) of the dictionary for given input, accomplished via the implementation of deferral mechanism. Hypernetwork-based Model Editing. The hypernetwork-based model editing paradigm utilizes an external model, referred to as the editor, to facilitate parameter updates in the models. Knowledge Editor (KE) [120] employs bidirectional LSTM to transform an edit pair, consisting of the edit sample, incorrect prediction, and correct label, into shifting operation parameters (i.e., mask m, offset b, and scaling factor α) for : ˆ = α(m ) + b. Based on KE, SLAG [355] further appends metrics for two types of input texts: (1) those that, while not part of the targeted edit set, align logically with it; and (2) those that share formal resemblance to edited knowledge, but do not affect the prediction outcomes. However, hyper-networks are generally incapable of updating LLMs due to the massive parameter size. To address this issue, MEND [663] applies low-rank decomposition to and utilizes two MLP layers to generate new low-rank update, ˆ. This approach is lightweight and efficient, particularly for large models like T5-11B. Moreover, KGEditor [177] combines the benefits of memory-based methods and hypernetworks to ensure flexibility and further reduce computation costs. In particular, it introduces an additional feed-forward networks (FFNs) layer for knowledge storage. It then employs bi-directional LSTM to encode embeddings of triples. In this manner, KGEditor becomes an efficient way to edit knowledge graph embeddings. Despite the success of this paradigm, the editors need to undergo prior training stage. The availability of training data, including edit samples and pre-training data, poses critical challenge. While these methods employ synthetic edit samples (e.g., selecting hypotheses via beam search except the top-1 for Question-answering tasks 120), their generalization to realistic mistakes beyond the synthetic sample distribution remains limited. Knowledge-based Model Editing. The knowledge-based model editing paradigm focuses on identifying subset of parameters specifically associated with particular pieces of knowledge and only updating those parameters. This approach assumes that knowledge is stored within the FFNs, which function as keyvalue memories [303]. Knowledge Neuron (KN) [204] attributes knowledge parameters using integrated gradients [865], where more salient gradients indicate greater influence on the knowledge. Building on this idea, Rank-One Model Editing (ROME) [645] uses causal tracing to localize the specific FFN layer whose activation most strongly mediates the recall of target factual association. Once the target layer is identified, ROME performs rank-one update to its value projection matrix, effectively replacing the original stored fact with new subjectobject mapping. MEMIT [646] extends ROME by identifying set of relevant layers (e.g., layers 38 for GPT-J) and applying closed-form multi-layer update. This allows MEMIT to edit multiple facts in parallel while preserving surrounding model behavior. It is important to note that these methods do not establish that these layers are exclusively dedicated to single piece of knowledge, implying that the layers may be shared across different knowledge domains [284]. To mitigate potential effects on out-of-scope samples, regularization techniques are employed during the neuron/layer updates. For example, MEMIT enforces the model to maintain predictions for several unrelated samples. By adopting knowledge-based approach, these methods selectively update parameters associated with specific knowledge while minimizing interference with unrelated samples. Based on ROME, BIRD [618] studies the novel problem of Bidirectional Assessment for Knowledge Editing (BAKE), which evaluates the reversibility of edited models in recalling knowledge in the reverse direction of editing and incorporating the bidirectional relationships between subject and object in an edit fact into the updated model weights. Model Editing in MLLMs. Compared to single-modal model editing, the task of editing MLLMs is more challenging due to their inherent diversity and complexity. Specifically, errors in MLLM outputs can be attributed to the synergistic effects of various modalities. recent study [176] introduces pioneering benchmark for MLLM editing, named MMEdit. This benchmark evaluates three aforementioned key principles: Reliability, Locality, and Generality, and covers two specific sub-tasks: Editing VQA and Editing Image Captioning. Empirical evidence indicates that while current methodologies [120, 1177, 663] are effective for editing the textual model in MLLMs, they fall short in editing the vision module. Researchers are encouraged to explore innovative techniques for efficient and accurate editing across various modalities and to develop comprehensive benchmarks for evaluating larger MLLMs. The end of 9.4.5 could profit from discussion on how the different model editing methods differ and what kind of advantages and disadvantages they carry. 68 Published in Transactions on Machine Learning Research (10/2025) Overall, the four paradigms of model editing discussed above differ substantially in their mechanisms and trade-offs: Memory-based methods store edits externally, avoiding interference with model parameters. They are easy to update and revert but introduce extra retrieval latency and depend on effective memory indexing. Classifier-based methods preserve original parameters and selectively activate edits only when needed, offering strong locality. However, they rely heavily on high-quality scope classifier and require ample negative examples to prevent over-triggering. Hypernetwork-based methods generate parameter updates dynamically from an edit description, enabling flexible and lightweight adaptation. Their main limitations are the need for pre-trained editor network and reduced scalability to very large models unless combined with low-rank or parameter-efficient techniques. Knowledge-based methods directly modify internal representations linked to specific facts, often achieving high reliability and generality with minimal changes. Yet, they risk unintended side effects if the targeted layers store multiple pieces of unrelated knowledge, and they require accurate localization of the relevant parameters. In multimodal settings, these trade-offs can be amplified: memoryand classifier-based methods may generalize more easily across modalities but depend on modality-aware retrieval/classification, while knowledgeand hypernetwork-based methods may offer more precise edits but require sophisticated cross-modal localization strategies. Future research may benefit from hybrid approaches that combine the precision of parameter-based edits with the flexibility and safety of external-memory or classifier gating mechanisms. 9.5 Current Limitations and Future Directions Foundation models, despite their remarkable capabilities, face several challenges when confronting distribution shifts. These limitations primarily stem from inherent difficulties in OOD detection, generalization, and adaptation. Such challenges significantly impact the reliability and robustness of these models in real-world scenarios. When exposed to data that deviates from their training distribution, these models often exhibit decreased performance [1110, 1155], leading to unreliable predictions in dynamic environments where data characteristics frequently change. While various OOD detection methods have been developed, many struggle with scalability issues, making them less practical for large-scale deployment. Current approaches to OOD generalization and adaption, such as domain adaptation [1110, 456, 1069] and adversarial training [56, 1110, 919], demonstrate varying degrees of success across different domains. These methods often require extensive retraining or fine-tuning to handle new domains effectively, process that can be both resource-intensive and time-consuming. Furthermore, many techniques for improving OOD robustness heavily depend on the availability of large, high-quality datasets [1110, 1069, 80, 410]. This dependence poses significant challenges in domains where data is scarce or expensive to obtain. Additionally, for multimodal foundation models, effectively integrating and processing diverse data types remains complex task. Current editing and generalization methods often fall short in scenarios involving multiple modalities, such as text, images, and audio [1003]. Last but not least, modern foundation models often undergo continual pre-training and fine-tuning, either horizontally across sequence of domains or vertically from general-purpose model to domain-specific model [818]. As result, they inevitably tend to suffer from catastrophic forgetting, such as horizontal forgetting [818] when continually adapting across domains and vertical forgetting [818] when continually adapting from more general models to more domain-specific models. To address these limitations, future research should focus on developing more lightweight OOD detection and generalization methods. These approaches should aim to identify and mitigate distribution shifts in largescale settings while maintaining low resource requirements. By focusing on efficiency, such methods could be more readily integrated into practical applications, enhancing the robustness and reliability of foundation model systems across diverse real-world scenarios. 69 Published in Transactions on Machine Learning Research (10/2025) To adapt to rapidly evolving environments, we should prioritize the development of continual or even lifelong learning mechanisms for foundation models [1058, 818, 452]. These mechanisms would enable models to adapt to new data distributions without requiring extensive retraining [512] while simultaneously preserving knowledge acquired from previous training data, including data previously used during pre-training or from previous domains. In other words, they should remain robust against both vertical and horizontal forgetting [818]. This approach could significantly enhance the flexibility and longevity of foundation models in dynamic domains. Additionally, due to the scarcity of data in several domains, developing more data-efficient transfer learning algorithms or creating diverse synthetic data will help models generalize to more practical applications. To further improve the generality of foundation models, advancing their abilities to handle multimodal data effectively is essential, with unified frameworks that can seamlessly integrate various data types and leveraging techniques like cross-modal learning and multimodal embeddings enhancing performance in complex scenarios [1003, 1155, 1092, 1103]. By addressing these limitations and exploring these future directions, we can significantly improve the robustness and reliability of foundation models, ensuring their effective deployment in diverse real-world applications. 70 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "10 Explainability",
            "content": "There are substantial existing efforts tailored towards the explainability of foundation models, particularly LLMs. In this section, we demonstrate the literature on the explainability of LLMs from the following aspects: (1) Feature Attribution Methods, i.e., Explaining LLMs with the raw features (words, sentences, syntax); (2) Exploring the inherent knowledge incorporated in LLMs themselves; (3) Discovering the roles and training samples in pre-training, fine-tuning, and few-shot learning. Following an overview of the methods used for model explanation, we dive into the evaluations and applications of explainability in LLMs. The discussion then broadens to include multimodal large language models (MLLMs), emphasizing the ongoing efforts in the field. Figure 26 provides detailed overview of various methods to explain different foundation model components. Figure 26: An overview of explainability in foundation models. This figure illustrates various techniques for uncovering how different model inputs and internal components influence model outputs. The right legend highlights the role of samples in different learning stages and the typology of explanation approaches. 10.1 Feature Attribution Methods When adopting LLMs on downstream tasks, it is important to determine which part of words or tokens in the input contribute most to the prediction. Thus, we need to determine the importance of each part of the input, i.e., explaining the prediction using the raw features. To explore this, there are several important lines of work: 10.1.1 Perturbing the Input for Explanation To study the effects of the raw features for model prediction, it has been important to perturb part of the input (a piece of text) while monitoring the model output. With this routine, Perturbed Masking [1017] proposes to perturb token in the given sentence while monitoring the representation of another token. They further propose span-level perturbation to study the impacts of certain span within the sentence. While Wu et al. [1017] regard the monitored variable as the representation of the token or span, MICE [772] study the roles of inputs for model prediction in classification tasks (i.e., the monitoring variable becomes the model prediction). They present method to find the edits that could flip the models prediction, where the edits could serve as contrastive explanations. In addition, perturbing the input to shift the label could create counterfactual examples. Crest [899] proposes framework to first perturb the input sentence with masks and then edit the masked tokens to obtain counterfactuals. Here, perturbing the sentence with masked tokens is essentially extracting rationales as they are both locating the important tokens for model prediction, though finding the rationales could also be achieved by other methods [499, 358]. To provide 71 Published in Transactions on Machine Learning Research (10/2025) Perturbation-based explanations Wu et al. [1017], Lei et al. [499], He et al. [358], Ross et al. [772], Treviso et al. [899], Wu et al. [1005] Principles of Explanation Gradient-based explanations Mohebbi et al. [666], Sikdar et al. [827], Sanyal & Ren [783], Wu & Ong [1016], Voita et al. [925], Montavon et al. [668], Du et al. [242], Enguehard [260], Kariyappa et al. [437] Attention-based explanations Tenney et al. [875], Goldberg [307], Voita et al. [923], Vig & Belinkov [922], Raganato & Tiedemann [747], Hewitt & Manning [361], Clark et al. [194], Zhang et al. [1146; 1145], Voita et al. [924], Vig [921], Park et al. [708], Jaunet et al. [417], Hoover et al. [372], Abnar & Zuidema [6], Barkan et al. [68], Hao et al. [353], Jain & Wallace [415], Wiegreffe & Pinter [998] Information across layers Clark et al. [195], Lin et al. [557], Tenney et al. [876], Belinkov [78], Belinkov et al. [79] Probing latent representations Structural/syntactic knowledge Hewitt & Manning [362], Chen et al. [140], Maudslay & Cotterell [636] Semantic/task content Kunz & Kuhlmann [481], Sorodoc et al. [844], Zhou et al. [1189] d o d F y i i x Knowledge Exploration Neuron interventions Torroba Hennigen et al. [895], Gurnee et al. [345], OpenAI [693], Singh et al. [829], Marks & Tegmark [633], Ji et al. [419], Merullo et al. [651], Petroni et al. [720], Apidianaki & Soler [38], Li et al. [516], Ravichander et al. [760], Zhong et al. [1179] Attention circuits Elhage et al. [259], Olsson et al. [690] Mechanistic Interpretability Knowledge in MLP layers Geva et al. [302], Yao et al. [1082], Yu et al. [1102], Meng et al. [645] Training dynamics in concept space Geva et al. [303], Park et al. [709] Explaining with concepts Kim et al. [445], Koh et al. [459], Yan et al. [1054], Kazmierczak et al. [440], Chattopadhyay et al. [134], Zhang et al. [1160], Huang et al. [393], Barrault et al. [70], Wang et al. [940], Captum [122], Mu & Andreas [670], Wang et al. [939], Zou et al. [1219] Concept Learning Roles of samples in training Shapley et al. [804], Kokalj et al. [463], Koh & Liang [458], Yeh et al. [1087], Grosse et al. [321], Ruis et al. [775], Zhou et al. [1183], Wu et al. [1007] Influence of few-shot demonstrations Hahn & Goyal [346], Xie et al. [1034], Lu et al. [603], Li et al. [540], Wei et al. [986], Wu et al. [1004], Madaan & Yazdanbakhsh [624], Wang et al. [933] CLIP Control Radford et al. [742], Yang et al. [1071], Chefer et al. [136], Luo et al. [611], Yan et al. [1054], Agarwal [9] Multimodal Explanation LLM-based multimodal models Zhu et al. [1209], Liu et al. [570], Li et al. [518], Wang et al. [965] Transferability across modalities Sikdar et al. [827], Sanyal & Ren [783], Enguehard [260], Liang et al. [544], Zou et al. [1219], Liu et al. [580] Plausibility of explanations Jacovi & Goldberg [411], Shen et al. [812], Chen et al. [161], Mathew et al. [634], DeYoung et al. [223] Evaluation Faithfulness of explanations DeYoung et al. [223], Chrysostomou & Aletras [188], Chan et al. [131], Liu et al. [591], Atanasova et al. [50], Chen et al. [146], Turpin et al. [903], Lanham et al. [488] Downstream Applications Du et al. [244; 243], Chen & Ji [145], Wei et al. [985], Li et al. [508], Lampinen et al. [487], Nye et al. [689], Stacey et al. [848], Mukherjee et al. [672], Li et al. [509], Hendrycks et al. [359], Lee et al. [493], Fernandes et al. [270], Zaidan & Eisner [1119], Ross et al. [773], Liu & Avci [565], Ghaeini et al. [304], Rieger et al. [766], Kennedy et al. [441], Huang et al. [395], Joshi et al. [428], Ma et al. [617], Yin et al. [1088] Figure 27: Taxonomy of Explainability in Foundation Models. more diverse perturbation types and locations, Polyjuice [1005] presents general-purpose counterfactual generator that can generate diverse sets of realistic counterfactuals. 10.1.2 Gradient-based Explanation Mohebbi et al. [666] adopt gradient-based attribution methods to provide token-level attribution scores to understand the representation space of BERT [221] better. More advanced difference-from-reference approaches such as Integrated Gradients (IG) are also used to explain the BERTs prediction [827, 783]. REAT [242] decomposes the final prediction of RNNs directly into the additive contribution of each word in the input text. Voita et al. [925] extend LRP [668] to the Transformers to attribute the relevance score on the source and target contexts in Neural Machine Translation tasks. Wu & Ong [1016] analyze different gradient-based methods for explaining BERT classification results. Recent work has extended gradientbased methods to autoregressive decoder-only language models. For example, Enguehard [260] introduce Sequential Integrated Gradients, which computes attributions along the generation path by integrating gradients between an empty sequence baseline and the final generated tokens of GPT-2. Kariyappa et al. [437] propose to approximate token attributions by backpropagating importance through each auto-regressive decoding step, which scales efficiently to long sequence generations. 10.1.3 Attention-based Explanation Previous works suggest that abundant information is encoded in the heads of the attention modules [875], including semantic structures and feature compositions [307, 923, 922, 747, 361, 194, 1146, 1145, 1037], which can be used for input-level explanation, controllable generation, and efficient pruning of attention heads [924]. Multiple tools are proposed to visualize the attention to illustrate the correlations between textual words for explanation purposes [921, 708, 417, 372]. Moreover, DeRose et al. [216] propose Attention Flows to visualize the whole attention flow instead of the visualization of one layer. Some methods combine gradients and attention for explanation [68, 353], which generally perform better than using attention alone. 72 Published in Transactions on Machine Learning Research (10/2025) Abnar & Zuidema [6] treat self-attention as flow network across layers to enable post-hoc computation of token-to-token information propagation, which shows higher correlation with gradient-based and ablationbased importance scores compared to raw attention. Though attention scores could be used to understand the large language models, they may not necessarily be capable of identifying the explanations [415]."
        },
        {
            "title": "10.2 Exploring the Knowledge in LLMs",
            "content": "Instead of explaining LLMs by highlighting the important tokens or spans in the input, interest increasingly gravitates toward understanding the breadth of knowledge encapsulated by these models. Several key areas of investigation are outlined as follows:"
        },
        {
            "title": "10.2.1 Probing the Representations within LLMs",
            "content": "Probing the model helps us understand the reasoning mechanism of deep neural networks [78, 972]. Early work by Veldhoen et al. [917] introduced diagnostic classifiers for revealing how neural networks process hierarchical structure by training simple probes on latent representations to test hypotheses about compositional strategies. With such probing philosophies, recent transformer-based works investigate the embeddings and hidden states from various mechanistic components of the network. Kunz & Kuhlmann [481] show that the token embeddings learned by BERT and ELMo contain rich information about the exact linear context of the token. Belinkov et al. [79] interpret the representation of different layers in NMT encoders, finding that higher layers have more semantic information. In contrast, lower-layer representations tend to be more suitable for part-of-speech tagging. The fact that language models can capture semantic information and conduct arithmetic operations is also studied in Sorodoc et al. [844] and Zhou et al. [1189]. Similarly, Clark et al. [195], Lin et al. [557] show that BERTs representations encode surface and positional information in the lower layers, but more semantic features in higher layers, while Hewitt & Manning [362] propose structural probe showing the syntax trees are embedded in linear transformation of ELMo and Berts word representation space. Building on previous probing work, Tenney et al. [876] probes word-level contextual representations to investigate how they encode sentence structures. Different from the above methods paying attention to the representation in certain metric spaces (typically Euclidean space), Chen et al. [140] consider the probing methods in hyperbolic space, which could better recover tree structures. While these methods could reveal the ability of representations to encode syntactic information, Maudslay & Cotterell [636] show that syntactic probes may not properly isolate syntax. With new corpus that is semantically nonsensical but syntactically well-formed, it is shown that syntactic and semantic information are entangled. Further, Zhang et al. [1139] argue that even with the existing works, it remains unclear whether LLMs have understood linguistic knowledge. Thus they probe GPT-3 to show that it has acquired linguistic knowledge in most cases but may still fail when disturbances happen. Apart from exploring the representations, some other works focus on self-attention heads, which could be helpful for heads pruning [467, 195]. Some methods are designed to be used during the inference of LLMs without training the classifier on the hidden vectors, such as cloze completion or text generation [720, 38, 516, 760]. Though prompts can be designed to reveal the abilities of the LLMs, Zhong et al. [1179] question if the prompt-search methods also learn from the training data, i.e., the training data may contain certain regularities of the underlying fact distribution that could be exploited. Probing methods are also used to understand the roles of neurons in LLMs. Torroba Hennigen et al. [895] propose framework based on decomposable multivariate Gaussian probe to explore how linguistic information is structured within the representation, showing that most attributes are reliably encoded by only few neurons. Moreover, some methods propose to probe the internal activations to predict the presence of features in the input, showing the sparse combinations of neurons can represent many features [345]. Recently, OpenAI has shown the possibility of using an advanced LLM (e.g., GPT-4) to explain the neurons in small model (e.g., GPT-2) [693]. Summarize and Score (SASC) [829] proposes to generate candidate explanations to explain the modules from LLMs, which could be more efficient than explaining single neurons. Marks & Tegmark [633] reveal that LLM representations of true/false statements form distinct linear directions that can be identified via probing and causally intervened upon to flip model outputs. Ji et al. [419] show that identifying linear verbal uncertainty feature in LLM representations can be manipulated at inference 73 Published in Transactions on Machine Learning Research (10/2025) time to reduce hallucinations. Merullo et al. [651] investigate how pretraining data frequency influences the emergence of linear representations of factual relations and find strong correlations between term cooccurrence counts and probe performance across models."
        },
        {
            "title": "10.2.2 Explaining LLMs with Concepts",
            "content": "Concept-based explanation refers to mapping the input into concepts and then using linear classifier to predict the final class with the mapped concepts. As the prediction from the concept to the class is simple linear classifier, it has the property of explainability even though the mapping from the input to the concepts is not explainable. Pioneering methods in this direction include Concept Activation Vectors (CAVs) [445] and Concept Bottleneck Models [459]. Such concept-driven framework is widely adopted in visual representation learning where the images are first mapped to the concept space, based on which the classifier makes the decision [445, 459, 1054, 440, 134, 1160, 393]. More recently, Wang et al. [940] propose Probabilistic Conceptual Explainers (PACEs), drawing inspiration from hierarchical Bayesian deep learning [935, 936, 427] and topic models [95] to provide concept-based explanations at multiple levels (e.g., datasets, images, and patches) to address key concerns in model interpretation such as faithfulness, stability, and parsimony. Beyond computer vision, CAVs are also tailored to language models for sentiment classification tasks [122], featuring two concepts: Positive Adjectives and Neutral. Besides, while Captum [122] define concepts manually, Mu & Andreas [670] propose to learn the abstractions by analyzing the neurons, where they find that neurons learn shallow lexical heuristics from dataset biases. Wang et al. [939] propose Variational Language Concepts (VALCs) to learn the concept-based explanations in an unsupervised learning manner while enabling neuron editing in the concept space. Turner et al. [901] propose to steer the behaviors of language models by curating concept activations and injecting them in the models hidden layers, and Zou et al. [1219] propose unified paradigm for concept interventions in the activation space. Barrault et al. [70] propose Large Concept Model (LCM) to perform next-sentence-prediction-based autoregressive learning in the conceptual embedding space. In summary, developing concept representation is crucial step towards interpretable LLMs for diverse tasks. Such interpretability offers feasible solution for diagnosing, revising, and intervening LLMs. 10.2.3 Mechanistic Interpretability In each block of transformer, the self-attention layer projects input tokens or hidden states into query, key, and value vectors that effectively store keyvalue memories. The main focus of mechanistic interpretability is reverse engineering for retrieving contextual information across those vectors, tokens, and layers, which provides systematic approach to explaining LLMs [259]. The aforementioned study also finds that in-context learning in small models could be explained by specific attention heads, termed Induction Heads\". This mechanism is hypothesized to constitute the mechanism for most in-context learning\" in large transformer models [690]. Another line of work focuses on FFN layers. Earlier work [302] argue that FFN layers contain most of the information that operates as key-value memories, and more recent works [1082, 1102] propose to search neural circuits or salient neurons for parametric knowledge representation. With the localization of the information, we could perform model editing on the relevant matrices in FFN layers [645]. In addition, Geva et al. [303] and Park et al. [709] analyze the learning dynamics of generative models in the concept space, demonstrating that updates can be decomposed into sub-updates, where each sub-update corresponds to human-interpretable concepts. Hou et al. [375] introduces MechanisticProbe to explore mechanistic interpretation of LLMs for multi-step reasoning tasks. 10.3 Discovering the Roles of Samples in Training, Fine-tuning, and Few-shot Learning The development of foundation models encompasses multiple learning stages, including pre-training, finetuning, and few-shot learning. During different stages, samples play distinct roles as illustrated in Figure 28. 74 Published in Transactions on Machine Learning Research (10/2025) Figure 28: The influence of samples in pre-training, instruction-tuning, and in-context learning stages. We highlight the beneficial and detrimental textual fragments in green and red, respectively."
        },
        {
            "title": "10.3.1 Influence of Single Example in Training",
            "content": "There is growing body of work studying the effects of one single example in the training process. SHAP [804] first proposes Shapley values to allocate the contribution of one single player in coalitional game. TransSHAP [463] proposes to adapt SHAP to transformers models, Bert specifically, to explain the classification results. Other works measure the effects of the example with the influence of this example on test loss values [1087]. Influence function, as statistical technique adapted to deep neural networks by Koh & Liang [458], approximates how upweighting single training example would change model parameters and test loss. Recently, Grosse et al. [321] scale the influence functions on LLMs with up to 52 billion parameters, and Ruis et al. [775] leverage influence-based analysis to demonstrate that procedural knowledge in the LLM pretraining sequences drives the emergence of reasoning capabilities. 10.3.2 Influence of Training Stages The training stages in the current most powerful models include pre-training and instruction tuning. LIMA [1183] analyzes the relative importance of pre-training and instruction-tuning, hypothesizing that the knowledge revealed in the generation primarily comes from the pre-training stage, while instructiontuning tends to fixate on the style and format of interacting with users, which is tested by using only 1000 curated examples to train Llama-65B to achieve near-GPT-4 performance on controlled human study. Wu et al. [1007] explore the instruction recognition and knowledge evolution before and after instruction-tuning, demonstrating that instruction-tuning could better identify the instruction parts from the input and align the knowledge with the user instruction. 10.3.3 Influence of Samples in Few-shot Learning Few-shot learning in LLMs typically refers to in-context learning (ICL). Li et al. [540] investigate ICLs functionality using contrastive demonstration and saliency maps. Wei et al. [986] examine how specific examples influence learning outcomes in few-shot scenarios, employing two distinct approaches: ICL with intentionally incorrect labels, and ICL with semantically unrelated labels. They found that large models can better override the input-label mapping learned during the pre-training stage, and small models rely more on semantic priors than large models do. In both settings, they find larger models and those enhanced with In-Context Fine-Tuning perform better. Wu et al. [1004] focus on how Chain-of-Thoughts (CoT) affects the model behavior, while others try to perturb CoT demonstrations and check the effects on the outcome [624, 933]. Hahn & Goyal [346] propose theory of emergent in-context learning as implicit structure induction to show how compositional structure in pretraining data gives rise to ICL. Xie et al. [1034] provide complementary perspective by modeling ICL as implicit Bayesian inference where LLMs infer latent concepts shared across prompt examples. From the empirical perspective, Lu et al. [603] conduct large-scale evaluations of emergent abilities across multiple tasks to conclude that the emergent performance gains can be largely ascribed to ICL mechanisms and parametric knowledge. Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "10.4 Evaluation of Explainability",
            "content": "The evaluation of explainability usually focuses on two perspectives [1166]: (1) Plausibility (also known as persuasiveness by 411). plausible explanation seems logical and coherent to the audience, regardless of whether it is correct or accurately reflects the models reasoning process. Essentially, plausibility measures the quality of the explanation in terms of its persuasiveness and understandability from human perspective. (2) Faithfulness. faithful explanation accurately represents the internal workings and decision-making processes of the LLM, demonstrating how well the explanation aligns with what the model is doing when generating the response."
        },
        {
            "title": "10.4.1 Evaluation of Plausibility",
            "content": "To evaluate the plausibility of the explanations of pre-trained LMs, Shen et al. [812] propose benchmark to test LMs abilities in five dimensions: grammar, semantics, knowledge, reasoning, and computation. Another benchmark, HateExplain [634], asks the annotators to highlight part of the text that could justify their decisions, which could serve as the ground-truth explanations. With these ground-truth tokens, we could calculate the metrics such as Accuracy, Macro F1-score, AUROC score [634], and AUPRC (Area Under the Precision-Recall Curve), IOU (Intersection-Over-Union) [223], etc. The above metrics could be applied to the explanations with raw features (discussed in Sec 10.1), but they may not be suitable for the explanations based on natural language (Sec 10.2.1) as there would be no groundtruth explanations in this case. To resolve this issue, Chen et al. [161] propose to evaluate the counterfactual simulatability of natural language explanations, i.e., whether humans could predict the models behavior according to the explanations given by the model. If so, then we say LLMs could explain themselves. 10.4.2 Evaluation of Faithfulness To evaluate the faithfulness of rationales selected by the model, ERASER [223] proposes the following metrics: (1) Comprehensiveness, which refers to the probability change of the original predicted class before and after the removal of the predicted rationales, and (2) Sufficiency, which means how much the extracted rationales could support the model to make prediction. Ideally, the objective is to achieve maximal change of comprehensiveness without compromising the accuracy of predictions when relying solely on the extracted rationales. In addition, TaSc [188] proposes another line of metrics: (1) Decision Flip - Fraction Of Tokens (DFFOT), which measures the fraction of important tokens required to be removed to cause decision flip. lower DFFOT indicates more faithful explanation. (2) Decision Flip - Most Informative Token (DFMIT), where the rate of decision flips caused by removing the most influential tokens is reported for comparison. To further evaluate the faithfulness of the explanations, Liu et al. [591] propose faithfulness violation test to show that most methods are hindered by the faithfulness violation issue. Wang et al. [949] tests whether targeted prompt interventions increase causal faithfulness of reasoning. Garg et al. [294] study calibration when proofs are checked for linking confidence to verifier outcomes. Zheng et al. [1178] standardizes evaluation of process-supervised models and verifiers across tasks. Although these metrics each have their rationale and applicability, the consistency between these metrics remains questionable. Chan et al. [131] show that the explanations that achieve the best DFFOT may have the worst Sufficiency score. These metrics are also not suitable for natural language explanations. To solve this issue, for classification tasks, Atanasova et al. [50] propose two tests: (1) counterfactual input editor for inserting reasons leading to counterfactual predictions; (2) reconstruct inputs from the reasons given by the explanation models and check if they lead to the same predictions. Different from modifying the input and monitoring the output (basically perturbation), REV [146] quantifies the amount of new, label-relevant information in the explanations beyond the information within the input, which can give the measurement without perturbation. For CoT-style explanations, Turpin et al. [903] find that CoT explanations could be vulnerable towards biasing features in the model inputs, thus being systematically unfaithful. Lanham et al. [488] monitor how the model predictions change when the input is intervened. They argue that models may produce less faithful reasoning when the models become larger. These researches demonstrate the need for better explanations in the CoT style. 76 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "10.5 Applications of Explainability",
            "content": "Gaining the explainability of LLMs has various applications, including diagnosing the model, and improving the model, which can help obtain user trust in the model."
        },
        {
            "title": "10.5.1 Avoiding Shortcut Learning",
            "content": "Du et al. [244; 243] point out that LLMs may rely on shortcut features like data biases, and artifacts to make predictions rather than understanding the meaning, which demonstrates an important challenge in the field of LLMs. Chen & Ji [145] propose to utilize the explanations revealed by the model to determine if the model is robust or not, as they argue that robust model should behave consistently between original and adversarial example pairs. Wei et al. [985] use chain-of-thought to understand the reasoning process of the model, though the faithfulness needs further exploration [903, 488]. Li et al. [508] design an urbanenvironment multi-agent simulator based on customizable first-order logic to evaluate the logical reasoning capability of LLMs. These works have identified new challenges in LLMs. 10.5.2 Improving Model Performances Apart from understanding the model, other works try to improve the model with explanations, which can also help gain user trust. For in-context learning, Lampinen et al. [487] find that using explanations in the prompts can improve performances, and hand-tuned explanations on small validation set could even offer substantial improvements. Zhou et al. [1193] use LLM interpretation to improve the performance of LLMs on natural language understanding. To improve the models reasoning ability, Nye et al. [689] find that asking LLMs to emit intermediate computation steps into scratchpad\" helps with multi-step reasoning tasks. Zelikman et al. [1120] trains latent/implicit CoT so models think internally without emitting long chains. On the other hand, Turpin et al. [903] demonstrate that chain-of-thought explanations can be systematically unfaithful. LLM may produce plausible but misleading rationales, which cautions against over-reliance on CoT for performance gains. Besides the training-free prompting, Stacey et al. [848] supervise the models attention weights to encourage the model to pay more attention to the words that are present in the explanations, which significantly improves the model performance. Li et al. [509] proposes the first bilevel planning framework to learn neural predicates from in-context demonstrations to achieve compositional generalization for new tasks. In the large language model regime, Mukherjee et al. [672] propose to train 13B model with the explanations and reasoning processes provided by GPT4 to improve the reasoning ability of small models. Early works by Hendrycks et al. [359] show that structured human feedback can steer language model behaviors toward alignment goals. Lee et al. [493] introduce the XMD framework which shows humans the explanations of model behavior and also updates the model based on the user feedback. Then to improve the models OOD generalization ability, there is popular paradigm called Explanation Regularization (ER) which aims to align the model rationales with human-annotated rationales [565, 766, 1119, 304, 395, 773, 441], where the effects of ER on OOD generalization is evaluated by ER-TEST [428]. As these methods require human-annotated rationales, which might be exhaustive, AMPLIFY [617] proposes to automate the process of rationale generation with the insights from post hoc explanations to provide corrective signals to LLMs. Some other applications include identifying the important instructions to compress the instruction [1088]. Fernandes et al. [270] comprehensively summarize taxonomy of integrating human feedback into natural language generation systems for improving generation quality and explaining model decisions. 10.6 Explainability of MLLMs Existing works on the explainability of MLLMs primarily focus on CLIP-based image-text alignment models. Early research suggests querying GPT to augment class labels, thereby improving the zero-shot performances of CLIP [742]. Recent methods utilize CLIP to analyze the composition of images with textual concepts, wherein the concepts are further used for image classification [135, 1071] or editing [136, 611]. Such approaches offer additional interpretability [1054, 1071] and controllability compared with directly using the CLIP representation for class prediction. Furthermore, Agarwal [9] investigates the trustworthiness of explanations generated for zero-shot and fine-tuned Vision and Language Models (VLMs), revealing that explanations for zero-shot CLIP classifiers are more faithful than those of the fine-tuned versions. While Published in Transactions on Machine Learning Research (10/2025) these works concentrate on image-text alignment models, the explainability of LLM-based image/video understanding models, such as MiniGPT-4 [1209], LLaVA [570], VideoChat [518], and LVChat [965], remains underexplored."
        },
        {
            "title": "10.7 Current Limitations and Future Directions",
            "content": "Despite significant advancements in the field of explainability, several limitations persist that necessitate future attention:"
        },
        {
            "title": "10.7.1 Faithfulness of Raw Features",
            "content": "Current methods, ranging from input perturbation to gradient-based and attention-based techniques, offer explanations for model predictions. However, the faithfulness of these explanations remains questionable. As illustrated in Section 10.4.2, the metrics from different perspectives may vary drastically [131]. This issue extends to downstream tasks where the model may rely on various biases for predictions [358, 244], potentially compromising their generalization ability if the predictions are not truly faithful. 10.7.2 Understanding How LLMs Store Knowledge Research focusing on the knowledge stored within LLMs examining layer representations [79, 481, 749] and analyzing generated content [576, 22] has shed some light on the distribution of syntactic versus semantic information across layers. While these works provide the insight that lower layers encode syntactic information and higher layers possess semantic knowledge, the community is still actively discussing how knowledge is dynamically injected into the model during training and how the injected knowledge is triggered through the inference process. For knowledge injection, Müller-Eberstein et al. [678] trace representational subspaces during pre-training to capture phases when syntactic subspaces rapidly emerge and disentangle from semantic and reasoning subspaces. Hu et al. [379] fit hidden Markov models to training-time metrics (e.g., weight norms, variances) to derive latent states of learning dynamics, which reveal phase transitions during the training. Wal et al. [928] deploy multiple LLM pre-training runs to demonstrate that training dynamics and knowledge injection stabilize consistently with identifiable outlier behaviors. On the other hand, how to trigger relevant knowledge with the input demonstrations needs further understanding [571, 155]. Some works argue that the knowledge is mainly stored in MLP layers [645, 646]; however, it is shown in their papers that attention layers also have slight effects when predicting the facts, especially in earlier layers (See Figure 3 in [646]). Even in the work that explicitly stores knowledge in memory module [964], how the model processes the knowledge is under-explored. 10.7.3 Transferability of Explanation Across Different Modalities How to develop unified explanation frameworks that can inherently transfer across modalities has been an emerging topic. As we have discussed above, prior works on Integrated Gradients [827, 783, 260] are modalityagnostic and can be employed on image classification, text generation, and multimodal VQA tasks. Yet these gradient-based attributions suffer from superficial faithfulness and low robustness to input perturbations [8]. On another track, attention-based methods such as MultiViz [544] can provide visualizations of cross-modal attention and thus enable analysis of pixel-to-token flow in multimodal transformers. However, the credibility of attention as explanation has long been arguable in the community [415, 998]. Concept interpretation tools such as representation engineering for LLMs [1219] have seen the potential to be adapted to vision-language modalities [580]. Aggregating multiple explanation levels and addressing the aforementioned limitations will motivate future work in developing robust, faithful, and transferable explanations for multiple modalities. It is also increasingly necessary to develop standardized benchmarks for cross-modal explanations that promote both interpretability and fidelity. 10.7.4 Reliability and Responsibility of Foundation Models from the Explainability Perspective Without deep comprehension of foundation models, ensuring their reliability and responsibility is challenging, where explainability has the potential to offer pathway to address these issues. For instance, 78 Published in Transactions on Machine Learning Research (10/2025) identifying the biases in pertaining data and implementing various de-biasing strategies could pose more equitable models [534]. Moreover, understanding how the model stores knowledge [546] can facilitate model editing with the current knowledge and the unlearning of harmful information, achieving up-to-date and safer foundation models [645, 947, 1132]. However, the effectiveness of interpretability methods themselves must be critically assessed to ensure they provide meaningful insights. Adebayo et al. [8] perform sanity checks for saliency maps and reveal that some widely used saliency methods are independent of both the model and the data, questioning their validity in explaining model behavior. Similarly, Alvarez-Melis & Jaakkola [24] investigates the robustness of interpretability methods and demonstrates that small perturbations to the input can significantly alter the explanations provided, highlighting the need for more robust interpretability techniques. Moreover, understanding how practitioners use interpretability tools is also crucial. Kaur et al. [439] explore data scientists use of interpretability tools and find that mismatches between tool capabilities and user needs can limit their effectiveness in ensuring model reliability and responsibility. They emphasize the importance of designing interpretability tools that align with the practical requirements of users. As the development of increasingly powerful foundation models continues, focusing on both the advancement and the critical evaluation of explainability methods cannot be overstated. 79 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "11 AIGC Detection",
            "content": "The advent of foundation models has led to surge of artificial intelligence-generated content (AIGC) across various modalities, including text [313, 693, 315], images [754, 1141, 261], audio [468, 343, 394, 27], and video [464, 94, 65]. While these technologies have unlocked many useful applications, they also pose significant challenges, particularly in terms of content authenticity [324, 535, 370]. The capacity of foundation models to generate human-like content can be exploited for malicious purposes, including the dissemination of misinformation and identity theft. Consequently, the demand for research focusing on detecting AIGC is on the rise. This section provides comprehensive overview of current methodologies and techniques for AIGC detection, highlighting the pivotal role this field plays in preserving the integrity of digital information in an era increasingly dominated by foundation models and AI technologies. Figure 29: An overview of AIGC detection techniques. We group them into three categories: zero-shot detectors, watermark-based detection, and neural network detectors, each with further subdivisions. 11.1 The AIGC Detection Problem The task of AIGC detection can be seen as binary classification problem. In general, we aim to determine whether given input , such as an image, text, or audio, is generated by AI models. This can be achieved using detector : {0, 1}, which can be defined as follows: D(x) = ( 1 0 if is generated by AI or is partially generated by AI. if is created by human. (14) The detector can be broadly categorized into the following types: (i) zero-shot detectors, (ii) watermark detectors, and (iii) learnable detectors. Furthermore, we summarize the representative work for all types in Figure 29, with examples for each type illustrated in Figure 31. 11.2 Zero-shot Detectors The fundamental concept behind zero-shot detectors is to differentiate between AIand human-generated content based on their intrinsic distinctions, such as the frequency of word occurrence in the generated text, 80 Published in Transactions on Machine Learning Research (10/2025) Statistical detection (classic) Lavergne et al. [490], Beresneva [83], Badaskar et al. [55], Hans et al. [349], Solaiman et al. [837] Statistical detection (LLM-era) Gehrmann et al. [298], Mitchell et al. [665], Deng et al. [214], Bao et al. [63], Su et al. [857] Zero-shot Detectors Black-box / API-level Yang et al. [1063], Mireshghallah et al. [662], Yang et al. [1064], Cozzolino et al. [202], Tulchinskii et al. [900] Intuitive indicator Uchendu et al. [906], Dugan et al. [249], Mao et al. [632], Levenshtein et al. [504], Borji [104], Farid [265; 266] Pre-trained LLMs Bhattacharjee & Liu [88], Liu et al. [592], Koike et al. [461], Krishna et al. [469] Post-hoc transforms Brassil et al. [110], Topkara et al. [893; 894], Meral et al. [650], Por et al. [725], Rizzo et al. [767], Munyer & Zhong [676], Yang et al. [1062], Yoo et al. [1093], Sato et al. [786], Yang et al. [1061], Atallah et al. [48] Inference-time Kirchenbauer et al. [453], Lee et al. [497], Wang et al. [945], Yoo et al. [1094], Fernandez et al. [271], Qu et al. [738], Kirchenbauer et al. [454], Ren et al. [764], Zhao et al. [1170], An et al. [26], Hu et al. [383], Wu et al. [1009] Watermark-based Detection PRNG-guided sampling Hou et al. [373], Kuditipudi et al. [472], Christ et al. [186] AIGC Detection Taxonomy Images/latents Ramesh et al. [754], Wang [950], Wen et al. [994] Learnable watermarking Abdelnabi & Fritz [2], Zhang et al. [1143], Liu et al. [561; 560], Yu et al. [1109] Supervised fine-tuning Chen et al. [164], Guo et al. [333], Zhan et al. [1128], Tian [879], Yu et al. [1107], Raffel et al. [746], Liu et al. [593] Neural Network Detectors Retrieval / Paternity Yu et al. [1106], Koch et al. [457] Feature-based scores Wu et al. [1001], Verma et al. [920] Data & robustness Liu et al. [586], Hu et al. [382], Sadasivan et al. [778], Krishna et al. [469] Early task-specific detectors Bhagat & Hovy [87], Zellers et al. [1121], Ma et al. [619], Solaiman et al. [837], Bakhtin et al. [59], Uchendu et al. [905] Fairness of detection Liang et al. [547] Limitations & Future Directions Robustness of watermarks Kirchenbauer et al. [454], Zhao et al. [1170], Saberi et al. [777], An et al. [26], Liu et al. [588] Visual generators Liu et al. [588] Figure 30: Taxonomy of AIGC detection in Foundation Models. Figure 31: Examples of zero-shot, watermark, and neural network detectors for textual and visual inputs. which can be identified and flagged by hand-crafted detectors. That said, zero-shot detectors are arguably the simplest to deploy since they do not require additional training of both the detectors and the foundation models that generate the content. 11.2.1 Statistical Detection These detectors assume full, or at least partial (e.g., the token logits during generation), access to the foundation model that generated the content. In the text domain, traditional methods usually rely on statistical outlier detection based on different metrics, including entropy [490], perplexity [83], n-gram frequencies [55], the ratio of perplexity to cross-perplexity [349], which measures how surprising the next token predictions of 81 Published in Transactions on Machine Learning Research (10/2025) one model are to another model, and average per-token log probability [837]. We use them to evaluate the given text passage and apply thresholding to assess whether the content is likely AI-generated. However, these approaches are inadequate in the era of foundation models, where AI-generated content becomes more diverse and of high quality. To this end, several recent studies improve upon these simple ideas and extend them to LLMs. [298] propose GLTR, which is centered on the underlying assumption that LLMs overgenerate from limited subset of the true distribution of natural language, for which they have high confidence. This property is detected by computing, for each token in text sequence: (i) the probability of generating the token, (ii) the rank of the word, and (iii) the entropy of the generated distribution. These metrics are then compared against those of human writers. In similar vein, DetectGPT [665] leverages the empirical observation that AIgenerated text tends to lie in negative curvature of the models log probability function, i.e., the sequence sits at locally concave region of (x) = log pθ(x) such that small random paraphrases/perturbations q(x) systematically decrease . Practically, DetectGPT scores text by the average log-likelihood (cid:1), 2 tr(cid:0)Hf (x) Covq drop = (x) Eyq(x)[f (y)]. second-order Taylor expansion gives E[f (y)] (x) + 1 so > 0 implies tr(cid:0)Hf (x) Covq (cid:1) < 0 (negative curvature) along the perturbation directionsan effect pronounced for model-generated text but weaker/inconsistent for human-written text. This observation led to follow-up investigations on improving detection efficiency [214] and utilizing conditional probability curvature [63]. DetectLLM [857] employs similar principle, but scores with log-rank information. However, these approaches rely on thresholding the probability of given sequence, which requires access to the models token generation probability distribution. Such requirement can be too restrictive in many practical scenarios. To alleviate this, recent detection methods that require only API-level access to the unknown source model are proposed. For instance, [1063] utilize the N-Grad divergence between re-prompted and original text to identify AI-generated content in the biology domain. Additionally, recent research has shown that smaller surrogate models can serve as effective proxies for AIGC detection [662, 1064, 202]. By observing that AI-generated text exhibits lower intrinsic dimensionality compared to human-written text when measured in representation space of fixed text embeddings (e.g., sentenceor token-level vectors produced by pretrained encoder), [900] propose to employ persistence-homology-based intrinsic dimension estimator (PHD) to exploit this property for AIGC detection, estimating local manifold dimension from neighborhoods within the embedding space. This approach does not require API-level accessi.e., it operates in complete black-box setting. 11.2.2 Intuitive Indicators These methods use the analytical abilities of humans to identify inconsistencies with prior knowledge in AIGC, thus achieving detection. As result, these methods provide notable interpretability and credibility in the detection process. For AI-generated text, [906] note that lack of coherence and consistency serves as strong indicator of AIGC, and emphasize the importance of collaboration among human detectors in improving detection accuracy. Similarly, [249] note the unreliability of relying solely on grammatical errors as detection strategy. They further showcase that while LLMs frequently commit factual and logical errors, these mistakes are often overlooked by neural network-based detectors but are easily noticed by human detectors. More recently, [632] find that LLMs exhibit greater propensity to alter human-written text compared to AI-generated text when tasked with rewriting. This tendency stems from LLMs perception of AI-generated text as being of high quality, which results in fewer modifications. They then proposed geneRative AI Detection viA Rewriting (RAIDAR) to detect AI-generated content by instructing LLMs to rewrite text and then calculating the edit distance of the output by the Levenshtein Score [504]. In vision, the detection of AI-generated images can be done by examining inconsistency with reality. Numerous studies [104, 265] note that AI-generated images often violate physical rules in the real world, such as missing or unnatural reflections and shadows of objects that are inconsistent with natural lighting and environment. In addition, [266] has noticed that AI-generated images exhibit inconsistency in perspective, such as parallel lines cannot converge at common vanishing point. For facial images, [104] outlines key cues 82 Published in Transactions on Machine Learning Research (10/2025) for detecting AI-generated faces, including symmetry, iris color, pupil shapes, skin, etc., where the generated images tend to depict physiological falsehood. However, AIGC detection by intuitive indicators are becoming much harder as the capabilities of AIGC models continually improve."
        },
        {
            "title": "11.2.3 Pre-trained LLMs",
            "content": "Without training, few studies have investigated the use of pre-trained LLMs to directly identify generated texts either by themselves or by other LLMs. However, it has been observed that the performance of these detection methods is often inferior to statistical and neural network approaches. For example, [88, 592] formulate the AIGC detection task in question-and-answer format, and prompt LLMs with the question to obtain an answer for detection. [88] note that neither ChatGPT nor GPT-4 could reliably identify text generated by various LLMs, while [592] reveal the poor zero-shot performance of GPT-3.5-turbo in AIGC detection which is close to random guessing. recent work [461] considers employing in-context-learning (ICL) with pre-trained LLMs for AIGC detection, in which few labeled examples (context) are integrated into the question prompt as single input to the model, thereby facilitating the learning of new tasks in context. The results in [461] show that using ICL outperforms both traditional zero-shot methods and RoBERTa-based detectors, however, [592] observe no significant improvement in using ICL with GPT-3.5-turbo. It is worth noting that while ICL methods are not strictly zero-shot, they do not require additional training of the detectors. Another recent work [469] proposes detection mechanism based on retrieval, which involves creating database of generated text and comparing the semantic similarity of the target text with all the text stored in the database to perform detection. Although this approach is effective and robust against paraphrasing, its requirement of storing LLMs generation may raise privacy concerns. 11.3 Watermark-based Detection Watermarking injects algorithmically detectable patterns into the AI-generated content while ideally preserving the quality and diversity of AIGC. watermarking algorithm for AI-generated content detection typically involves three components: The watermark or message, denoted as m, can be represented as bit-string in the generated images or as specific occurrence of words in the generated text. From now on, the term watermark payload will be used to refer to the amount of information conveyed by the watermark message. An encoder, denoted as A, is responsible for embedding the watermark message into an AIgenerated content x, thereby transforming it into watermarked content x. detector, denoted as D, is capable of determining the presence of watermark in either or x, provided that the content is generated by AI. In zero-bit watermarking, the embedded message only signifies the presence or absence of watermark, hence is only used to indicate whether is generated by AI; whereas in multi-bit watermarking, the embedded message can carry additional detailed, customized information, e.g., the name of the AI model or authorship attribution. We will primarily focus on the first case - using watermarking for AIGC detection. watermarking algorithm that is effective for detecting AI-generated content should possess the following key properties: It should be algorithmically easy to verify yet remain imperceptible to humans, where ease of verification can refer to the ability to open-sourcing, or high success rate for detection. It should have minimal impact on the quality of AI-generated content. This means that foundation models incorporating the watermark algorithm, potentially during training, should still produce content of similar quality compared to the non-watermarked version. 83 Published in Transactions on Machine Learning Research (10/2025) It should exhibit high robustness to attacks aimed at removing the watermark or applying semantically invariant transformations to AI-generated content with watermarks. These transformations can range from rephrasing generated text to distorting watermarked images. It should demand minimal effort to incorporate the watermark into AI-generated content."
        },
        {
            "title": "11.3.1 Training-free Watermarking",
            "content": "In training-free watermarking algorithms, the watermark, encoding, and decoding algorithms are all designed based on heuristics, exploiting domain-specific characteristics of the generated content rather than learned through end-to-end training. Several studies apply various kinds of semantically-invariant transformation directly to existing AI-generated text. These include visually imperceptible reformatting such as adding whitespace characters and replacing characters with similar ones in appearance but with different Unicode representation [110, 725, 767, 786]; lexical-based modifications such as synonym substitution [676, 894, 1062, 1093, 1061]; syntax-based manipulation which alters the arrangement of words and phrases in the text through several predefined types of transformations [48, 650, 893]. Each distinct type of transformation corresponds to specific message bit, therefore allowing the detection and extraction of watermarks. The immediate advantage of these approaches is that they do not require knowing the identity (i.e., the name of the model) or access to the AI models that generated the content. However, since these methods largely rely on simple semantically invariant transformation, they are easy to spot and hence are vulnerable to watermark attack or removal. Moreover, these manually defined modifications can create abrupt and unnatural modifications to the original text, hence significantly degrading the quality of the generated content. Instead of encoding watermarks in the existing context after generation, it is also possible to encode trainingfree-based watermarks during the content generation process without the need for re-training the models. Consequently, unlike previous approaches discussed, the following methods assume at least the given access to controlling the generation process of the foundation models. The pioneering research of [453] first proposes watermarking framework for LLMs by altering the logits for token sampling in text sequence generation. The algorithm [453] works by selecting randomized set of green\" tokens before generation, and then softly promoting the use of green\" tokens during generation by adding small bias on the sampling logits of green\" tokens. Detection can be achieved by deploying statistical tests which are essentially based on identifying the unnatural occurrence of green\" tokens in the writing. Follow-up research works expand upon this idea in the directions of preserving quality and semantic meaning of generated content in low-entropy text generation scenarios [497, 945], where text quality is vulnerable to such tiny bias towards generating randomly selected green\" tokens; multi-bit watermarking [1094, 271, 738]; improving the robustness of watermarking against removal attack and post-processing [454, 764, 1170, 26]; and defending against forgeries of watermarks [383, 1009]. In contrast altering the logits, line of works [373, 472, 186] alternatively choose to manipulate the token sampling process itself directly by encoding watermark in pseudo-random number sequence as seeds to guide the sampling of each token or sentence in text generation sequence. Detection therefore needs to access the correspondence between the tokens generated and the underlying pseudo-random numbers. Beyond text generation, training-free watermarks have also been applied to AI-generated images. For instance, DaLLE [754] always prints tiny visible color pattern at the bottom right corner of its generated images. To better preserve the visual quality of the generated images, invisible-watermark [950], which is adopted by Stable Diffusion, encodes bits of the watermark message through modifying coefficients of carefully selected subset of band frequencies of its generated images under discrete wavelet transforms. Detection and decoding of the watermark is thereon achieved through an inverse transformation. In addition, [994] introduce training-free watermark for diffusion models by embedding watermark signals into the initial latent noise, creating semantic watermark. 11.3.2 Learnable Watermarking Although training-free watermarking and detection techniques are straightforward in concept and require minimal effort to deploy, the pre-defined watermarking rules may be too conspicuous, leading to compromise 84 Published in Transactions on Machine Learning Research (10/2025) in the quality of the generated content or making them susceptible to watermark removal and forgery. In this survey, we use learnable watermarking to refer to methods that modify the generation process to encode keyed watermark payload at training or inference time, and whose verification requires the corresponding key (or public verifier). This distinguishes them from post-hoc detectors in Section 11.4, which do not assume any embedded signal. To address this issue, couple of studies [2, 1143] propose using learningbased watermark encoding and decoding modules, in which the training pipeline involves an encoder that first embeds binary watermark payload into the original text followed by decoding for the message from the watermarked text. To preserve coherence and consistency of the generated content, the modules from [2] are trained against an adversary that performs classification between the original and watermarked text, whereas [1143] regularize the watermarked message by penalizing semantic difference with the original text. [561] embed watermarks into text by adding extra watermark logits to the LLMs sampling logits at each generation step, following [453]. To ensure both attack robustness and security robustness, each watermark logit is determined by applying learned transformation (a trained watermark model) on the semantic embedding of all preceding tokens generated using another pre-trained LLM. Two similarity loss and normalization loss are minimized during training to prompt semantic consistency and unbiasedness in the generated watermark logits and facilitate statistical detection. Moreover, in recent work, [560] propose an unforgettable publicly verifiable watermark algorithm utilizing two different neural networks for watermark generation and detection, thereby preventing exposing key information in the watermark generation phase when made accessible for public detection. Furthermore, the token embedding parameters are shared between the generation and detection networks which improves both training efficiency and detection accuracy. [1109] proposed SAEMARK, user-specific watermarking method that embeds personalized signatures without altering logits. SAEMARK uses Sparse Autoencoder (SAE) to extract features from generated texts and selects outputs by matching key-derived feature distributions. Boundary to Section 11.4. Although methods such as ASI and publicly verifiable schemes train neural verifiers, we keep them in Learnable Watermarking because they require payload that was intentionally embedded at generation time. By contrast, Section 11.4 covers detectors that operate without any embedded watermark or key, treating detection purely as post-hoc content classification. 11.4 Neural Network Detectors Unlike Section 11.3 (learnable watermarking), approaches in this section do not modify the generator and do not assume any embedded payload/key. They train post-hoc classifiersoften on human vs. AI corporaand can operate in black-box settings against unknown generators. Consequently, any methods whose verification relies on generation-time watermark remain in Section 11.3 rather than here. Another line of work approaches the AIGC detection problem by training binary classifier using labeled training samples containing both human and AI-generated content. Earlier work focuses on fake review [87], fake news [1121], fake images [619], or small AI models detection [837, 59, 905]. Subsequently, growing interest in this line of research turns to detecting high-quality content brought by foundation models. Detectors under this category do not require access to model parameters hence can operate under complete black-box settings. Targeting the problem of machine-generated text detection, numerous studies [164, 333, 1128, 879, 1107] fine-tune pre-trained LLM, such as T5 [746] or RoBERTa [593], on dataset of pairs of human-written text and AI-written text from mixed sources as simple solution. Alternatively, several works also consider training classifier on top of frozen pre-trained LLM [164, 333, 1001, 920]. In particular, [164, 333] have attempted training logistic regression classifier on text embedding obtained using pre-trained LLM for detection, however, they find such method often underperforms the fine-tuning approach. [1001] propose LLMDet, which conducts binary classification utilizing proxy score for perplexity, while [920] propose Ghostbuster, which is inspired by statistical detection methods based on analyzing token log-probabilities. Both methods train logistic regression classifier on top of these selected and hand-crafted features to detect machine-generated text, therefore, no longer requiring direct access to the model token sampling logits, as in their zero-shot counterparts, at test time. Recognizing the similarities between the original AI-generated and the regenerated text produced with ChatGPT, [1106] introduce novel GPT Paternity Test for AIgenerated text detection. This method involves utilizing ChatGPT to infer question based on the input text being examined, followed by supplying response. Subsequently, Siamese network [457] is trained to 85 Published in Transactions on Machine Learning Research (10/2025) assess the similarity between the original and regenerated text, aiding the detection using another trained binary classifier. One major challenge in training reliable binary classifier is data scarcity as collecting sufficient data to train the classifier can be challenging, especially in diverse domains where the availability of training samples is major bottleneck. To alleviate this, [586] consider adopting contrastive learning approaches in addition to the supervised training for detection. Another significant challenge involves tackling paraphrasing attacks [778, 469]. To mitigate this problem, [382] propose to employ an adversarial learning approach to simultaneously train detector and paraphraser. Nevertheless, supervised training of binary classifier tends to overfit their training data, resulting in decline in performance when faced with cross-domain or unseen data. Additionally, fine-tuning LLM classifiers is limited in facing data generated by different models."
        },
        {
            "title": "11.5 Current Limitations and Future Directions",
            "content": "Despite significant advancements in the domain of AIGC detection, several limitations still require future attention: 11.5.1 Fairness of AIGC Detection Although state-of-the-art text detectors generally achieve high accuracy in experimental settings, as discussed by [547], perplexity-based text detectors exhibit notable bias against text written by non-native speakers. Specifically, these detectors have been observed to misclassify TOEFL essays written by foreign writers more frequently than those by native speakers. This discrepancy may be due to the lower perplexity of nonnative essays, which often display less linguistic diversity and richness. This issue may also affect minority languages, which tend to have higher perplexities compared to popular languages like English. Additionally, similar biases might exist in other modalities, such as image detection. Therefore, it is crucial to consider the fairness of detectors when designing future detection methods and to develop efficient methods for evaluating the fairness of AIGC detection methods. Meanwhile, on the watermarking side, learnable watermarking methods might also exhibit biases toward outof-distribution data points. For instance, if the watermarking encoder and decoder are trained on English text written by native speakers, the model might also have higher misclassification rate on essays written by non-native speakers. Therefore, it is crucial to consider fairness in the development of learnable watermarking methods as well. 11.5.2 Robustness of Watermarks Both text and image watermarks are susceptible to regeneration or post-processing attacks, such as paraphrasing [454] or diffusion purification [1170]. In contrast, semantic watermarks tend to be more robust against such attacks. However, because semantic watermarks typically require deep neural networks to decode the watermark signals, they are vulnerable to adversarial attacks [777, 26]. Adversarial perturbations can also be developed to prevent regeneration and post-processing attacks [588]. Adversarial attacks remain significant challenge even for classification tasks. Therefore, designing robust watermarks that can withstand both attacks is challenging and crucial. 11.5.3 Origin Attribution of Generated Images Recent advancements in visual generative models have significantly improved the quality of generated images, raising concerns about their potential misuse. It is critical to develop methods to accurately identify the origin model responsible for generating given image [588]. Especially, the scenarios are especially important and practical where access to the source model is restricted and only limited number of images from the source model are available [588]. 86 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "12 Intersection and Conclusion",
            "content": "In this survey, we comprehensively examine the reliability and responsibility of foundation models, spanning technical and societal considerations. Given our overview of the current research landscape, we identify significant prior research that makes progress on these critical issues. However, outstanding challenges limit the extent to which current models are reliable or responsibly developed, indicating more research is needed as this technology has broader societal impact. Moreover, greater attention should be paid to the intersections between different research areas, as the areas covered by this survey are interconnected and influence each other. Instead of addressing challenges in isolation, we advocate for more holistic approach to ensure the overall reliability and responsible development of foundation models. In conclusion, we emphasize number of key points of intersection across these domains, highlighting the challenges at these crossroads and outlining potential directions for future research."
        },
        {
            "title": "12.1 Summary of Recent Community Efforts",
            "content": "A growing body of community work has conducted the reliability and responsibility reviews from complementary perspectives. Table 3 shows summary of relevant past surveys4. For example, there are hallucination and grounding reviews for LLMs [390, 418], cross-modal syntheses [780], safety-at-scale perspectives spanning security, robustness, and governance [620], comprehensive trustworthiness benchmarking [1157, 402], and explainability overviews [1166]. These efforts collectively advance shared terminology, taxonomies, and evaluation practices, while emphasizing that trustworthy AI requires both method innovation and standardized assessment. Table 3: Coverage of modalities and trustworthy AI topics in related surveys. indicates the survey has considerable proportion of contents that emphasizes the modality or task. T: Text-only LLMs, M: Multimodal LLMs, IG: Image Generation, VG: Video Generation, B: Bias and Fairness, A: Alignment by Post-Training, S: Security, P: Privacy, H: Hallucination, U: Uncertainty, D: Distribution Shift, E: Explainability, C: AIGC Detection, I: Intersection. Survey IG VG Huang et al. [390] Sahoo et al. [780] Shen et al. [809] Gallegos et al. [281] Huang et al. [402] Zhao et al. [1166] Wen et al. [991] Ma et al. [620] Lin et al. [552] Zhang et al. [1157] Anwar et al. [37] Bengio et al. [82] Huang et al. [401] Gallegos et al. [282] Zhang et al. [1162] 12.2 Bias, Fairness, and Security S D To improve the security of foundation models, adversarial training [628, 56] is often leveraged, which aims to make models resistant to malicious manipulations by conducting bi-level adversarial games [871, 616] 4The tables mapping of each surveys primary scope (based on abstracts and dedicated sections) could be limited. We welcome corrections, suggestions, and insights from readers and survey authors and will revise the table accordingly. 87 Published in Transactions on Machine Learning Research (10/2025) during training. However, while adversarial training improves robustness, it can unintentionally exacerbate fairness issues [1041]. For example, robust models may focus on defending against certain set of features without considering the fact that some demographic groups may be penalized by these features more than others, leading to disproportionate performance degradation for underrepresented or marginalized groups. For instance, [784] found that hate speech detection models were biased against African American English (AAE), disproportionately misclassifying non-offensive AAE utterances as hate speech. On the other hand, security risks such as data poisoning, where malicious attackers corrupt training data, can introduce new biases into foundation models or exacerbate existing biases [644, 341]. Poisoned data can skew models learned representation, posing the dual challenge of protecting models from data poisoning while ensuring that the fairness of the model is not compromised, especially when training on large, uncurated datasets."
        },
        {
            "title": "12.3 Bias, Fairness, and AI-generated Content Detection",
            "content": "Large multimodal and image generation models are often biased due to unbalanced or stereotype-laden training data, such as images that reinforce harmful stereotypes or misrepresent certain social or cultural groups [1156, 182, 606]. Detecting biased AI-generated content (AIGC) poses significant challenge, as detection systems themselves can inherit or amplify biases found in the training data. For instance, zeroshot or neural network detectors might disproportionately mark content created by minority groups as AI-generated based on biased data patterns in the training set. It is important to ensure the fairness of these detection capabilities, as biased detection could result in unfair discrimination, such as unfairly enforcing restrictions on content produced by minority groups. Similarly, watermark-based detectors can raise fairness concerns if watermarks are inconsistently applied across different types of content. For example, if IG models excessively generate watermarked content associated with certain groups (e.g., images associated with particular ethnic or gender identity), this content could be more easily flagged or blocked, suppressing content created or represented by those groups. 12.4 Security and Privacy Privacy concerns often go hand-in-hand with issues of security. For example, models ought not to reveal users private information, which can be found in pre-training data as well as in interactions with users. Furthermore, certain legal jurisdictions already impose privacy-related laws that impact technologies including foundation models. For example, the EUs GDPR covers Right to be Forgotten, which mandates that users have the ability to delete their private information [1132]; non-compliance with these regulations could result in legal penalties. Several methods exist for preventing models from revealing private information, including unlearning (e.g., removing concepts from the models parametric knowledge) [582, 1080, 416, 1012] and introducing prompts [253] or RLHF [1028] to prevent models from revealing information. However, jailbreaking attacks (see Section 5.1.1) can often circumvent prompts and RLHF, and [712] show that model editing is also vulnerable to attack, finding that sensitive information could be recovered from models even after deletion when querying multiple times. Thus, making models compliant with existing and future legislation regarding private information is an open challenge, as is robustly defending against adversaries attempting to extract private information. 12.5 Security and AI-generated Content AIGC detection is naturally framed as an adversarial task, with an attacker attempting to pass AIGC as real content, and defender attempting to detect it. In such scenarios, the advantage usually lies with the attacker, who can make multiple attempts to test existing defenses. Existing work shows that the robustness of current detection methods is imperfect and they are vulnerable to adversarial attacks [777, 26, 454, 1170]; improving these detection algorithms remains an area of continuous future work. 88 Published in Transactions on Machine Learning Research (10/2025) Beyond the security of individual watermarking and detection methods, AIGC raises broader questions of societal security, i.e. the potential threats that AIGC poses to both public institutions and individuals. Here, extensive documentation exists regarding ongoing threats from AIGC. In the political sphere, AIGC has been employed to disseminate misinformation and erode public trust in political systems and elections [231, 426], where AIGC has been used to spread misinformation and sow distrust. Similarly, in public health settings, AIGC has been utilized to generate and spread health-related misinformation [649]. Other voices have also called attention to the risks associated with AIGCs interactions with individuals. For example, [319] raise the concern that AIGC could harm mental health through hyper-personalization. Moreover, individuals may become more susceptible to personalized fraud attempts, such as voice cloning or sophisticated phishingstyle attacks [76, 263, 114, 133]. To counteract these malicious use-cases, larger-scale safeguards will likely be needed. This includes implementing public education initiatives to raise awareness about the risks of AIGC and developing strategies to combat its misuse across various domains."
        },
        {
            "title": "12.6 Uncertainty and Alignment",
            "content": "Given the importance of correctly expressing model uncertainty (as described in Section 8), growing area of interest is in aligning models to accurately predict their uncertainty [654, 853] or to abstain from answering in cases of uncertainty [991]. This work builds on past work finding that models internal states often contain meta-knowledge about whether the model can correctly respond to particular prompt [429, 654, 577]. Several past efforts have explored aligning models to predict this information based on internal states. [654] train models to express uncertainty linguistically by extracting control codes from their internal states and using them to adjust the models output. [908] train smaller LLM to predict the uncertainty of larger LLM. [853] use speaker-listener framework to supervise models, rewarding generator or speaker model for getting listener to accept correct answers and reject incorrect ones, while penalizing it for doing the opposite outcomes. Future research directions in this field include addressing uncertainty not reflected by the models internal state, that is, the \"unknown unknowns\" where the model is unaware of its own ignorance, often when faced with out-of-distribution data or novel concepts, and enhancing models capacity to resolve uncertainty effectively. 12.7 Hallucination, Uncertainty, Distribution Shift, and Alignment Uncertainty and distribution shifts are deeply interconnected with hallucinations in foundation models [1029, 1201, 267, 495]. When these models encounter unfamiliar OOD data, they often lack the ability to detect OOD data accurately, leading to highly overconfident predictions. This unreliability is especially concerning in safety-critical applications. The risk of hallucination also increases as the model makes inferences based on its prior knowledge [765, 522], which may not be applicable in the new context [418, 390, 1162]. In multimodal settings, training model on evolving image-text data presents significant challenges, as distribution shifts in training data increase the potential of the model to forget previously learned knowledge and disrupt modality alignment. This can exacerbate hallucinations of the model by incorrectly aligning the relationships between observations and prior knowledge [495, 1210, 425, 348]. For example, changes in language usage or visual context can cause the model to lose its ability to associate specific textual descriptions with visual cues, resulting in errors when trying to generalize to unseen data, highlighting the need for effective memory retention mechanisms or continual learning strategies [598, 847, 1096, 201] to maintain seamless alignment between modalities. 89 Published in Transactions on Machine Learning Research (10/2025)"
        },
        {
            "title": "References",
            "content": "[1] Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp. 308318, 2016. [2] Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. 2021 IEEE Symposium on Security and Privacy (SP), pp. 121140, 2020. [3] Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what youve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, pp. 7990, 2023. [4] Taiga Abe, Estefany Kelly Buchanan, Geoff Pleiss, Richard Zemel, and John Cunningham. Deep In Advances in Neural Information Processing Systems, ensembles work, but are they necessary? 2022. [5] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 298306, 2021. [6] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers, 2020. [7] Abbas Acar, Hidayet Aksu, Selcuk Uluagac, and Mauro Conti. survey on homomorphic encryption schemes: Theory and implementation. ACM Computing Surveys (Csur), 51(4):135, 2018. [8] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. Advances in neural information processing systems, 31, 2018. [9] Chirag Agarwal. Intriguing properties of visual-language model explanations. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models, 2023. [10] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when theyre hallucinating references? arXiv preprint arXiv:2305.18248, 2023. [11] Jaimeen Ahn, Hwaran Lee, Jinhwa Kim, and Alice Oh. Why knowledge distillation amplifies gender bias and how to mitigate from the perspective of distilbert. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pp. 266272, 2022. [12] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In International Conference on Machine Learning, pp. 145155. PMLR, 2020. [13] Meta AI. Llama 3 model card. https://github.com/metallama/llama3/blob/main/MODEL_CARD.md, 2024. [14] AIBase. Aibase: Deepseek app surpasses 100 million downloads in one month. https://www.aibase.com/news/15598, 2025. [15] Afra Feyza Akyürek, Eric Pan, Garry Kuwanto, and Derry Wijaya. Dune: Dataset for unified editing. arXiv preprint arXiv:2311.16087, 2023. [16] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. 90 Published in Transactions on Machine Learning Research (10/2025) [17] Alibaba. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [18] Alibaba. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [19] Alibaba. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. [20] Alibaba. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [21] Alibaba. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [22] Dimitrios Alivanistos, Selene Báez Santamaría, Michael Cochez, Jan-Christoph Kalo, Emile van Krieken, and Thiviyan Thanapalasingam. Prompting as probing: Using language models for knowledge base construction, 2022. URL: http://arxiv. org/abs/2208.11057, 2022. [23] Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132, 2023. [24] David Alvarez-Melis and Tommi Jaakkola. On the robustness of interpretability methods. arXiv preprint arXiv:1806.08049, 2018. [25] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. [26] Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu, Chenghao Deng, Sicheng Zhu, Abdirisak Mohamed, Yuxin Wen, Tom Goldstein, et al. Benchmarking the robustness of image watermarks. arXiv preprint arXiv:2401.08573, 2024. [27] Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al. Seed-tts: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. [28] Maya Anderson, Guy Amit, and Abigail Goldsteen. Is my data in your retrieval database? membership inference attacks against retrieval augmented generation. arXiv preprint arXiv:2405.20446, 2024. [29] Konstantinos Andriopoulos and Johan Pouwelse. Augmenting llms with knowledge: survey on hallucination prevention. arXiv preprint arXiv:2309.16459, 2023. [30] Anastasios Angelopoulos and Stephen Bates. gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv:2107.07511, 2021. [31] Anastasios N. Angelopoulos, Stephen Bates, Emmanuel J. Candès, Michael I. Jordan, and Lihua Lei. Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. arXiv:2110.01052, 2021. [32] Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control, 2023. [33] Anthropic. Claude3. https://www.anthropic.com/news/claude-3-family, 2024. [34] Anthropic. Claude3.5. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. [35] Anthropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/claude-3-7-sonnet, February 2025. [36] Anthropic. Claude4. https://www.anthropic.com/news/claude-4, 2025. [37] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932, 2024. 91 Published in Transactions on Machine Learning Research (10/2025) [38] Marianna Apidianaki and Aina Garí Soler. ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns Semantic Properties and their Prototypicality, October 2021. URL http: //arxiv.org/abs/2110.06376. arXiv:2110.06376 [cs]. [39] Giovanni Apruzzese, Hyrum Anderson, Savino Dambra, David Freeman, Fabio Pierazzi, and Kevin Roundy. real attackers dont compute gradients: Bridging the gap between adversarial ml research and practice. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pp. 339364. IEEE, 2023. [40] Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, and Ryan McDonald. Focus attention: Promoting faithfulness and diversity in summarization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 60786095, 2021. [41] Michael Arbib. The handbook of brain theory and neural networks. MIT press, 2003. [42] Mikhail L. Arbuzov, Alexey A. Shvets, and Sisong Beir. Beyond exponential decay: Rethinking error accumulation in large language models. arXiv preprint arXiv:2505.24187, 2025. [43] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. [44] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171, 2022. [45] Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect them. arXiv preprint arXiv:2109.06827, 2021. [46] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023. [47] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau Yih. Reliable, adaptable, and attributable language models with retrieval. arXiv preprint arXiv:2403.03187, 2024. [48] Mikhail J. Atallah, Victor Raskin, Michael Crogan, Christian F. Hempelmann, Florian Kerschbaum, Dina Mohamed, and Sanket Naik. Natural language watermarking: Design, analysis, and proof-ofconcept implementation. In Information Hiding, 2001. URL https://api.semanticscholar.org/ CorpusID:37687669. [49] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. Generating fact checking explanations. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7352 7364, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-mai n.656. URL https://aclanthology.org/2020.acl-main.656. [50] Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. Faithfulness Tests for Natural Language Explanations, June 2023. URL http://arxiv.org/abs/2305.18029. arXiv:2305.18029 [cs]. [51] Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. MultiFC: real-world multi-domain dataset for evidencebased fact checking of claims. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 46854697, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1475. URL https://aclanthology.org/D19-1475. 92 Published in Transactions on Machine Learning Research (10/2025) [52] Razvan Azamfirei, Sapna Kudchadkar, and James Fackler. Large language models and the perils of their hallucinations. Critical Care, 27(1):12, 2023. [53] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. general theoretical paradigm to understand learning from human preferences, 2023. [54] Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz. Uncertainty in natural language generation: From theory to applications. arXiv preprint arXiv:2307.15703, 2023. [55] Sameer Badaskar, Sachin Agarwal, and Shilpa Arora. Identifying real or fake articles: Towards better language modeling. In International Joint Conference on Natural Language Processing, 2008. URL https://api.semanticscholar.org/CorpusID:4324753. [56] Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. Recent advances in adversarial training for adversarial robustness. In IJCAI, 2021. [57] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [58] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. ArXiv preprint, abs/2212.08073, 2022. URL https://arxiv.org/abs/2212.08073. [59] Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, MarcAurelio Ranzato, and Arthur Szlam. Real or fake? learning to discriminate machine from human generated text. ArXiv, abs/1906.03351, 2019. URL https://api.semanticscholar.org/CorpusID:182952342. [60] Neil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto. Linguistic calibration of language models, 2024. [61] Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. HalluLens: LLM hallucination benchmark. In ACL, 2025. [62] Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, and Kai-Wei Chang. Cleanclip: Mitigating data poisoning attacks in multimodal contrastive learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 112123, 2023. [63] Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. ArXiv, abs/2310.05130, 2023. URL https://api.semanticscholar.org/CorpusID:263831345. [64] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:2500525017, 2022. [65] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. [66] Gabriel Barcik, Jakob Bauer, Dana Berman, Nicole Brichtova, Lluis Castrejon, . . . , and Yang Zhao. Imagen google deepmind. https://deepmind.google/models/imagen/, 2025. [67] Soumya Barikeri, Anne Lauscher, Ivan Vulić, and Goran Glavaš. Redditbias: real-world resource for bias evaluation and debiasing of conversational language models. arXiv preprint arXiv:2106.03521, 2021. 93 Published in Transactions on Machine Learning Research (10/2025) [68] Oren Barkan, Edan Hauon, Avi Caciularu, Ori Katz, Itzik Malkiel, Omri Armstrong, and Noam In CIKM, pp. Koenigstein. Grad-sam: Explaining transformers via gradient self-attention maps. 28822887. ACM, 2021. [69] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Nips tutorial, 1: 2017, 2017. [70] Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussà, David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, and Holger Schwenk. Large Concept Models: Language modeling in sentence representation space. 2024. URL https://arxiv.org/ab s/2412.08821. [71] Elisa Bassignana, Valerio Basile, Viviana Patti, et al. Hurtlex: multilingual lexicon of words to hurt. In CEUR Workshop proceedings, volume 2253, pp. 16. CEUR-WS, 2018. [72] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pp. arXiv2506, 2025. [73] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The In Proceedings of the international AAAI conference on web and social pushshift reddit dataset. media, volume 14, pp. 830839, 2020. [74] Nina Baumgartner, Matthias Stürmer, Matthias Grabmair, Joel Niklaus, et al. Towards explainability and fairness in swiss judgement prediction: Benchmarking on multilingual dataset. arXiv preprint arXiv:2402.17013, 2024. [75] BBC. \"Art is dead Dude\" - the rise of the AI artists stirs debate. https://www.bbc.com/news/tec hnology-62788725, 2022. [76] Nils Begou, Jérémy Vinoy, Andrzej Duda, and Maciej Korczyński. Exploring the dark side of ai: Advanced phishing attack design and deployment using chatgpt. In 2023 IEEE Conference on Communications and Network Security (CNS), pp. 16. IEEE, 2023. [77] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. [78] Yonatan Belinkov. Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics, 48(1):207219, April 2022. ISSN 0891-2017, 1530-9312. doi: 10.1162/coli_a_00422. URL https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-S hortcomings-and. [79] Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 110, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. URL https://aclanthology.org/I17-1001. [80] Anastasiya Belyaeva, Justin Cosentino, Farhad Hormozdiari, Krish Eswaran, Shravya Shetty, Greg Corrado, Andrew Carroll, Cory McLean, and Nicholas Furlotte. Multimodal llms for health grounded in individual-specific data. In Workshop on Machine Learning for Multimodal Healthcare Data, pp. 86102. Springer, 2023. [81] Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610623, 2021. 94 Published in Transactions on Machine Learning Research (10/2025) [82] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al. Managing extreme ai risks amid rapid progress. Science, 384(6698):842845, 2024. [83] Daria Beresneva. Computer-generated text detection using machine learning: systematic review. In International Conference on Applications of Natural Language to Data Bases, 2016. URL https: //api.semanticscholar.org/CorpusID:1175726. [84] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. ArXiv preprint, abs/2308.09687, 2023. URL https://arxiv.org/abs/2308.09687. [85] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [86] Camiel Beukeboom and Christian Burgers. How stereotypes are shared through language: review and introduction of the aocial categories and stereotypes communication (scsc) framework. Review of Communication Research, 7:137, 2019. [87] Rahul Bhagat and Eduard H. Hovy. Squibs: What is paraphrase? Computational Linguistics, 39: 463472, 2013. URL https://api.semanticscholar.org/CorpusID:32452685. [88] Amrita Bhattacharjee and Huan Liu. Fighting fire with fire: can chatgpt detect ai-generated text? ACM SIGKDD Explorations Newsletter, 25(2):1421, 2024. [89] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pp. 14931504, 2023. [90] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. [91] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes, 2021. [92] Abeba Birhane, Vinay Prabhu, Sang Han, Vishnu Naresh Boddeti, and Alexandra Sasha Luccioni. Into the laions den: Investigating hate in multimodal datasets, 2023. [93] Ali Furkan Biten, Lluís Gómez, and Dimosthenis Karatzas. Let there be clock on the beach: Reducing object hallucination in image captioning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 13811390, 2022. [94] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [95] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. JMLR, 3(Jan): 9931022, 2003. [96] Su Lin Blodgett and Brendan OConnor. Racial disparity in natural language processing: case study of social media african-american english. arXiv preprint arXiv:1707.00061, 2017. [97] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. Language (technology) is power: critical survey of\" bias\" in nlp. arXiv preprint arXiv:2005.14050, 2020. 95 Published in Transactions on Machine Learning Research (10/2025) [98] Mikel Bober-Irizar, Ilia Shumailov, Yiren Zhao, Robert Mullins, and Nicolas Papernot. Architectural backdoors in neural networks, 2022. [99] Rishi Bommasani and Percy Liang. Trustworthy social bias measurement, 2022. URL https: //arxiv.org/abs/2212.11672. [100] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. ArXiv, 2021. URL https://crfm.stanford.edu/assets/report.pdf. [101] Conrad Borchers, Dalia Sara Gala, Benjamin Gilburt, Eduard Oravkin, Wilfried Bounsi, Yuki Asano, and Hannah Rose Kirk. Looking for handsome carpenter! debiasing gpt-3 job advertisements. arXiv preprint arXiv:2205.11374, 2022. [102] Shikha Bordia and Samuel Bowman. Identifying and reducing gender bias in word-level language models. arXiv preprint arXiv:1904.03035, 2019. [103] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning, pp. 22062240, 2022. [104] Ali Borji. Qualitative failures of image generation models and their application in detecting deepfakes. ArXiv, abs/2304.06470, 2023. URL https://api.semanticscholar.org/CorpusID:257826680. [105] Nick Bostrom. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. Minds and Machines, 22:7185, 2012. [106] Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Viren Bajaj, and Zeya Ahmad. Langfair: python package for assessing bias and fairness in large language model use cases, 2025. URL https://arxiv.org/abs/2501.03112. [107] Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher Manning. large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015. [108] Samuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukošiute, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022. 96 Published in Transactions on Machine Learning Research (10/2025) [109] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [110] Jack Brassil, Steven H. Low, Nicholas F. Maxemchuk, and Lawrence OGorman. Electronic marking and identification techniques to discourage document copying. Proceedings of INFOCOM 94 Conference on Computer Communications, pp. 12781287 vol.3, 1994. URL https://api.semanticsc holar.org/CorpusID:6540783. [111] Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1):13, 1950. [112] Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pp. 2129. IEEE, 1997. [113] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [114] Amy Bunn. Artificial imposterscybercriminals turn to ai voice cloning for new breed of scam, 2024. URL https://www.mcafee.com/blogs/privacy-identity-protection/artificial-imp osters-cybercriminals-turn-to-ai-voice-cloning-for-a-new-breed-of-scam/. [Accessed 09-09-2024]. [115] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. [116] ByteDance Seed Team. Seed-1.6: multimodal deep-thinking foundation model. https://seed.b ytedance.com/zh/seed1_6, 2025. [117] Aylin Caliskan, Joanna Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183186, 2017. [118] Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. Defending against alignment-breaking attacks via robustly aligned llm. arXiv preprint arXiv:2309.14348, 2023. [119] Chentao Cao, Zhun Zhong, Zhanke Zhou, Yang Liu, Tongliang Liu, and Bo Han. Envisioning outlier exposure by large language models for out-of-distribution detection. arXiv preprint arXiv:2406.00806, 2024. [120] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Conference on Empirical Methods in Natural Language Processing, 2021. URL https://api.semant icscholar.org/CorpusID:233289412. [121] Yihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining: High-quality instruction data selection for large language models. arXiv preprint arXiv:2307.06290, 2023. [122] Captum. Testing with concept activation vectors (tcav) on sensitivity classification examples and convnet model trained on imdb dataset. https://github.com/pytorch/captum/blob/master/tu torials/TCAV_NLP.ipynb, 2022. [123] Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. arXiv preprint arXiv:2106.09667, 2021. [124] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 26332650, 2021. 97 Published in Transactions on Machine Learning Research (10/2025) [125] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan In The Eleventh International Zhang. Quantifying memorization across neural language models. Conference on Learning Representations, 2022. [126] Nicholas Carlini, Milad Nasr, Christopher Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems, 36, 2024. [127] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pp. 52535270, 2023. [128] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. ArXiv preprint, abs/2307.15217, 2023. URL https://arxiv.org/abs/2307.15217. [129] Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang. Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. arXiv preprint arXiv:2402.08925, 2024. [130] Ilias Chalkidis, Tommaso Pasini, Sheng Zhang, Letizia Tomada, Sebastian Felix Schwemer, and Anders Søgaard. Fairlex: multilingual benchmark for evaluating fairness in legal text processing. arXiv preprint arXiv:2203.07228, 2022. [131] Chun Sik Chan, Huanqi Kong, and Liang Guanqing. comparative study of faithfulness metrics for model interpretability methods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 50295038, 2022. [132] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. [133] Aliza Chasan. Ethical hacker scams 60 Minutes staffer to show how easy digital theft is, 2023. URL https://www.cbsnews.com/news/how-digital-theft-targets-people-from-millennials-t o-seniors-60-minutes-2023-05-21/. [Accessed 09-09-2024]. [134] Aditya Chattopadhyay, Kwan Ho Ryan Chan, Benjamin Haeffele, Donald Geman, and René Vidal. Variational information pursuit for interpretable predictions. arXiv preprint arXiv:2302.02876, 2023. [135] Aditya Chattopadhyay, Kwan Ho Ryan Chan, and Rene Vidal. Bootstrapping variational information pursuit with large language and vision models for interpretable image classification. In The Twelfth International Conference on Learning Representations, 2024. [136] Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, and Lior Wolf. The hidden language of diffusion models. In ICLR, 2024. [137] Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, and Kelvin Guu. Purr: Efficiently editing language model hallucinations by denoising language model corruptions. arXiv preprint arXiv:2305.14908, 2023. [138] Beitao Chen, Xinyu Lyu, Lianli Gao, Heng Tao Shen, and Jingkuan Song. Alleviating hallucinations In The Thirty-eighth in large vision-language models through hallucination-induced optimization. Annual Conference on Neural Information Processing Systems, 2024. [139] Bocheng Chen, Advait Paliwal, and Qiben Yan. Jailbreaker in jail: Moving target defense for large language models. In Proceedings of the 10th ACM Workshop on Moving Target Defense, pp. 2932, 2023. 98 Published in Transactions on Machine Learning Research (10/2025) [140] Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, and Liping Jing. Probing BERT in Hyperbolic Spaces, April 2021. URL http://arxiv.org/abs/2104.03869. arXiv:2104.03869 [cs]. [141] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728, 2018. [142] Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, arXiv preprint Jindong Gu, Huaxiu Yao, Chaowei Xiao, et al. Can editing llms inject harm? arXiv:2407.20224, 2024. [143] Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms internal states retain the power of hallucination detection, 2024. [144] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. arXiv preprint arXiv:2311.11860, 2023. [145] Hanjie Chen and Yangfeng Ji. Adversarial training for improving model robustness? look at both prediction and interpretation. In The 36th AAAI Conference on Artificial Intelligence (AAAI), 2022. [146] Hanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, and Swabha Swayamdipta. Rev: information-theoretic evaluation of free-text rationales. The 61th Annual Meeting of the Association for Computational Linguistics (ACL), 2023. [147] Jianqi Chen, Hao Chen, Keyan Chen, Yilan Zhang, Zhengxia Zou, and Zhenwei Shi. Diffusion models for imperceptible and transferable adversarial attack. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [148] Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett, and Eunsol Choi. Complex claim verification with evidence retrieved in the wild. arXiv preprint arXiv:2305.11859, 2023. [149] Jiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness, 2023. [150] Jiuhai Chen and Jonas Mueller. Automated data curation for robust language model fine-tuning. arXiv preprint arXiv:2403.12776, 2024. [151] Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. When do you need chain-of-thought prompting for chatgpt? arXiv preprint arXiv:2304.03262, 2023. [152] Jiuhai Chen, Rifaa Qadri, Yuxin Wen, Neel Jain, John Kirchenbauer, Tianyi Zhou, and Tom GoldarXiv preprint stein. Genqa: Generating millions of instructions from handful of prompts. arXiv:2406.10323, 2024. [153] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training better alpaca with fewer data, 2023. [154] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [155] Shuo Chen, Zhen Han, Bailan He, Mark Buckley, Philip Torr, Volker Tresp, and Jindong Gu. Understanding and improving in-context learning on vision-language models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. [156] Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, and Jindong Gu. Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks? arXiv preprint arXiv:2404.03411, 2024. Published in Transactions on Machine Learning Research (10/2025) [157] Weixin Chen, Dawn Song, and Bo Li. Trojdiff: Trojan attacks on diffusion models with diverse targets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 40354044, 2023. [158] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. arXiv preprint arXiv:2210.02928, 2022. [159] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts promptarXiv preprint ing: Disentangling computation from reasoning for numerical reasoning tasks. arXiv:2211.12588, 2022. [160] Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: comprehensive survey on latent chain-of-thought reasoning. arXiv preprint arXiv:2505.16782, 2025. [161] Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves? counterfactual simulatability of natural language explanations, 2023. [162] Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, and Alan Ritter. Can language models be instructed to protect personal information? arXiv preprint arXiv:2310.02224, 2023. [163] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Dress: Instructing large vision-language models to align and interact with humans via natural language feedback, 2023. [164] Yutian Chen, Hao Kang, Vivian Zhai, Liang Li, Rita Singh, and Bhiksha Ramakrishnan. Gptsentinel: Distinguishing human and chatgpt generated content. ArXiv, abs/2305.07969, 2023. URL https://api.semanticscholar.org/CorpusID:258686680. [165] Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. Hallucination detection: Robustly discerning reliable answers in large language models. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 245255, 2023. [166] Zhaorun Chen, Zhuokai Zhao, Wenjie Qu, Zichen Wen, Zhiguang Han, Zhihong Zhu, Jiaheng Zhang, and Huaxiu Yao. Pandora: Detailed llm jailbreaking via collaborated phishing agents with decomposed reasoning. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models. [167] Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? arXiv preprint arXiv:2407.04842, 2024. [168] Zhaorun Chen, Francesco Pinto, Minzhou Pan, and Bo Li. Safewatch: An efficient safety-policy following video guardrail model with transparent explanations. arXiv preprint arXiv:2412.06878, 2024. [169] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases. arXiv preprint arXiv:2407.12784, 2024. [170] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. Halc: Object hallucination reduction via adaptive focal-contrast decoding. In International Conference on Machine Learning (ICML), 2024. [171] Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. Autoprm: Automating procedural supervision for multi-step reasoning via controllable question decomposition. In North American Chapter of the Association for Computational Linguistics (NAACL), 2024. 100 Published in Transactions on Machine Learning Research (10/2025) [172] Zhaorun Chen, Mintong Kang, and Bo Li. Shieldagent: Shielding agents via verifiable safety policy reasoning. arXiv preprint arXiv:2503.22738, 2025. [173] Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding, and Wenqiang Zhang. Contentbased unrestricted adversarial attack. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, 2023. [174] Hao Cheng, Erjia Xiao, Jindong Gu, Le Yang, Jinhao Duan, Jize Zhang, Jiahang Cao, Kaidi Xu, and Renjing Xu. Unveiling typographic deceptions: Insights of the typographic vulnerability in large vision-language model. European Conference on Computer Vision (ECCV) (To appear), 2024. [175] Jiali Cheng and Hadi Amiri. Multimodal machine unlearning. arXiv preprint arXiv:2311.12047, 2023. [176] Siyuan Cheng, Bo Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang. Can we edit multimodal large language models? ArXiv, abs/2310.08475, 2023. URL https://api. semanticscholar.org/CorpusID:263908997. [177] Siyuan Cheng, Ningyu Zhang, Bo Tian, Zelin Dai, Feiyu Xiong, Wei Guo, and Huajun Chen. Editing language model-based knowledge graph embeddings. ArXiv, abs/2301.10405, 2023. URL https: //api.semanticscholar.org/CorpusID:256231427. [178] Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. Factool: Factuality detection in generative aia tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023. [179] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023. [180] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog /2023-03-30-vicuna/. [181] Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, and Wei-Chen Chiu. Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts. arXiv preprint arXiv:2309.06135, 2023. [182] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 30433054, 2023. [183] Yujin Choi, Jinseong Park, Hoki Kim, Jaewook Lee, and Saeroom Park. Fair sampling in diffusion models through switching mechanism. arXiv preprint arXiv:2401.03140, 2024. [184] Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho. Villandiffusion: unified backdoor attack framework for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [185] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [186] Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. IACR Cryptol. ePrint Arch., 2023:763, 2023. URL https://api.semanticscholar.org/CorpusID: 259092330. [187] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Published in Transactions on Machine Learning Research (10/2025) [188] George Chrysostomou and Nikolaos Aletras. Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 477488, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.40. URL https://aclanthology.org/2021.acl-long.40. [189] Zhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan Qin, and Kui Ren. Sora detector: unified hallucination detection for large text-to-video models. arXiv preprint arXiv:2405.04180, 2024. [190] Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka. Debiasing vision-language models via biased prompts. arXiv preprint arXiv:2302.00070, 2023. [191] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: arXiv preprint Decoding by contrasting layers improves factuality in large language models. arXiv:2309.03883, 2023. [192] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. [193] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. [194] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of berts attention. In BlackboxNLP@ACL, pp. 276286. Association for Computational Linguistics, 2019. [195] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What Does BERT Look at? An Analysis of BERTs Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclantholo gy.org/W19-4828. [196] CNN. AI won an art contest, and artists are furious. https://www.cnn.com/2022/09/03/tech/ai -art-fair-winner-controversy/index.html, 2022. [197] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In international conference on machine learning, pp. 13101320. PMLR, 2019. [198] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm vs lm: Detecting factual errors via cross examination. arXiv preprint arXiv:2305.13281, 2023. [199] Jeremy R. Cole, Michael J. Q. Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions, 2023. [200] Competition and Markets Authority. Ai foundation models: initial review. https://www.gov.uk/cmacases/ai-foundation-models-initial-review, 2023. [201] Andrea Cossu, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, Tinne Tuytelaars, and Davide Bacciu. Continual pre-training mitigates forgetting in language and vision. Neural Networks, 179: 106492, 2024. [202] Davide Cozzolino, Giovanni Poggi, Matthias Nießner, and Luisa Verdoliva. Zero-shot detection of ai-generated images. In European Conference on Computer Vision, pp. 5472. Springer, 2025. [203] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287, 2023. 102 Published in Transactions on Machine Learning Research (10/2025) [204] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Knowledge neurons in pretrained transformers. ArXiv, abs/2104.08696, 2021. URL https://api.semanticscholar.org/CorpusID: 233296761. [205] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible may not be faithful: Probing object hallucination in vision-language pre-training. arXiv preprint arXiv:2210.07688, 2022. [206] Xuelong Dai, Kaisheng Liang, and Bin Xiao. Advdiff: Generating unrestricted adversarial examples using diffusion models. arXiv preprint arXiv:2307.12499, 2023. [207] Yi Dai, Hao Lang, Kaisheng Zeng, Fei Huang, and Yongbin Li. Exploring large language models for multi-modal out-of-distribution detection. arXiv preprint arXiv:2310.08027, 2023. [208] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253262, 2004. [209] DeepSeek. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [210] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715, 2023. [211] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing Discrete Text Prompts with Reinforcement Learning. In EMNLP 2022, pp. 33693391, 2022. [212] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. arXiv preprint arXiv:2405.19716, 2024. [213] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=vESNKdEMGp. [214] Zhijie Deng, Hongcheng Gao, Yibo Miao, and Hao Zhang. Efficient detection of llm-generated texts with bayesian surrogate model. ArXiv, abs/2305.16617, 2023. URL https://api.semanticscho lar.org/CorpusID:258947640. [215] Zhun Deng, Thomas P. Zollo, Jake C. Snell, Toniann Pitassi, and Richard Zemel. Distribution-free statistical dispersion control for societal applications, 2023. [216] Joseph F. DeRose, Jiayao Wang, and Matthew Berger. Attention flows: Analyzing and comparing attention mechanisms in language models. IEEE Trans. Vis. Comput. Graph., 27(2):11601170, 2021. [217] Shrey Desai and Greg Durrett. Calibration of pre-trained transformers, 2020. [218] Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, and Aaron Roth. Multicalibration for confidence scoring in llms, 2024. [219] Nicolas Deutschmann, Marvin Alberts, and María Rodríguez Martínez. Conformal autoregressive generation: Beam search with coverage guarantees, 2023. [220] Sunipa Dev, Tao Li, Jeff Phillips, and Vivek Srikumar. On measuring and mitigating biased In Proceedings of the AAAI Conference on Artificial Intelligence, inferences of word embeddings. volume 34, pp. 76597666, 2020. 103 Published in Transactions on Machine Learning Research (10/2025) [221] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclantholo gy.org/N19-1423. [222] Terrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout, 2017. [223] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. ERASER: Benchmark to Evaluate Rationalized NLP Models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 44434458, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.408. URL https://aclanthology.org/2020.acl-main.408. [224] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 862872, 2021. [225] Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Cohen. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 48844895, 2019. [226] Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, and Tong Zhang. Blackbox prompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531, 2022. [227] Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active prompting with chain-of-thought for large language models. arXiv preprint arXiv:2302.12246, 2023. [228] Thomas Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems, pp. 115. Springer, 2000. [229] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning. arXiv preprint arXiv:2111.01998, 2021. [230] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 6773, 2018. [231] Alphaeus Dmonte, Marcos Zampieri, Kevin Lybarger, and Massimiliano Albanese. Classifying humangenerated and ai-generated election claims in social media. arXiv preprint arXiv:2404.16116, 2024. [232] Thang Viet Doan, Zhibo Chu, Zichong Wang, and Wenbin Zhang. Fairness definitions in language models explained. arXiv preprint arXiv:2407.18454, 2024. [233] Tim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis. Differentially private diffusion models. arXiv preprint arXiv:2210.09929, 2022. [234] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758, 2021. [235] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. ArXiv preprint, abs/2304.06767, 2023. URL https://arxiv.org/abs/2304.06767. 104 Published in Transactions on Machine Learning Research (10/2025) [236] Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. Calibrating factual knowledge in pretrained language models. ArXiv, abs/2210.03329, 2022. URL https://api.semant icscholar.org/CorpusID:252762125. [237] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [238] Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1127511288, 2023. [239] Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, and Jun Zhu. How robust is googles bard to adversarial image attacks? arXiv preprint arXiv:2309.11751, 2023. [240] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [241] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, et al. neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32):e2123433119, 2022. [242] Mengnan Du, Ninghao Liu, Fan Yang, Shuiwang Ji, and Xia Hu. On attribution of recurrent neural network predictions via additive decomposition. In WWW, pp. 383393. ACM, 2019. [243] Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models. North American Chapter of the Association for Computational Linguistics (NAACL), 2021. [244] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. Shortcut learning of large language models in natural language understanding. Communications of the ACM (CACM), 2023. [245] Jinhao Duan, Fei Kong, Shiqi Wang, Xiaoshuang Shi, and Kaidi Xu. Are diffusion models vulnerable to membership inference attacks? In International Conference on Machine Learning, pp. 87178730. PMLR, 2023. [246] Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 50505063, 2024. [247] Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, and Kaidi Xu. Truthprint: Mitigating lvlm object hallucination via latent truthful-guided pre-intervention. arXiv preprint arXiv:2503.10602, 2025. [248] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024. [249] Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, and Chris Callison-Burch. Real or fake text?: Investigating human ability to detect boundaries between human-written and machinegenerated text. In AAAI Conference on Artificial Intelligence, 2022. URL https://api.semantic scholar.org/CorpusID:255125274. [250] Ondřej Dušek, David Howcroft, and Verena Rieser. Semantic noise matters for neural natural language generation. arXiv preprint arXiv:1911.03905, 2019. 105 Published in Transactions on Machine Learning Research (10/2025) [251] Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and programming, pp. 112. Springer, 2006. [252] Nouha Dziri, Andrea Madotto, Osmar Zaiane, and Avishek Joey Bose. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. arXiv preprint arXiv:2104.08455, 2021. [253] Kennedy Edemacu and Xintao Wu. Privacy preserving prompt engineering: survey. arXiv preprint arXiv:2404.06001, 2024. [254] Bryan Eikema. The effect of generalisation on the inadequacy of the mode. In Proceedings of the 1st Workshop on Uncertainty-Aware NLP (UncertaiNLP 2024), pp. 8792, 2024. [255] Bryan Eikema and Wilker Aziz. Is map decoding all you need? the inadequacy of the mode in neural machine translation. arXiv preprint arXiv:2005.10283, 2020. [256] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex DAmour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, and Jonathan Berant. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking, 2024. URL https://arxiv.org/abs/2312.09244. [257] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(53):16051641, 2010. URL http://jmlr.org/papers/v11/el-yan iv10a.html. [258] Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, and Shizhu Liu. Halo: Estimation and reduction of hallucinations in open-source weak large language models. arXiv preprint arXiv:2308.11764, 2023. [259] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. Mathematical Framework for Transformer Circuits transformer-circuits.pub. https://transformer-c ircuits.pub/2021/framework/index.html, December 2021. [Accessed 27-11-2023]. [260] Joseph Enguehard. Sequential integrated gradients: simple but effective method for explaining In ACL (Findings), pp. 75557565. Association for Computational Linguistics, language models. 2023. [261] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [262] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024. [263] Chibuike Samuel Eze and Lior Shamir. Analysis and prevention of ai-based phishing email attacks. Electronics, 13(10):1839, 2024. [264] Tongtong Fang, Nan Lu, Gang Niu, and Masashi Sugiyama. Rethinking importance weighting for deep learning under distribution shift. Advances in neural information processing systems, 33:1199612007, 2020. [265] Hany Farid. Lighting (in)consistency of paint by text. ArXiv, abs/2207.13744, 2022. URL https: //api.semanticscholar.org/CorpusID:251135258. [266] Hany Farid. Perspective (in)consistency of paint by text. ArXiv, abs/2206.14617, 2022. URL https: //api.semanticscholar.org/CorpusID:250113700. 106 Published in Transactions on Machine Learning Research (10/2025) [267] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. In Nature, 2024. [268] Shanglun Feng and Florian Tramèr. Privacy backdoors: Stealing data with corrupted pretrained models. arXiv preprint arXiv:2404.00473, 2024. [269] Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, and Xiangyu Zhang. Detecting backdoors in pre-trained encoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1635216362, 2023. [270] Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, and André F. T. Martins. Bridging the gap: survey on integrating (human) feedback for natural language generation, 2023. [271] Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models. 2023 IEEE International Workshop on Information Forensics and Security (WIFS), pp. 16, 2023. URL https://api.semanticscholar.org/CorpusID: 260351507. [272] Emilio Ferrara. Should chatgpt be biased? challenges and risks of bias in large language models. arXiv preprint arXiv:2304.03738, 2023. [273] Adam Fisch, Tommi Jaakkola, and Regina Barzilay. Calibrated selective classification, 2022. URL https://arxiv.org/abs/2208.12084. [274] Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for neural machine translation, 2020. [275] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. Advances in Neural Information Processing Systems, 34:70687081, 2021. [276] Alisa Fortin, Guillaume Vernade, Kat Kampf, and Ammaar Reshi. Introducing Gemini 2.5 flash image, our state-of-the-art image model. https://developers.googleblog.com/en/introducing -gemini-2-5-flash-image/, August 2025. [277] Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Instructing text-to-image generation models on Luccioni, and Kristian Kersting. Fair diffusion: fairness. arXiv preprint arXiv:2302.10893, 2023. [278] Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-Based Prompting for Multi-step Reasoning. ICLR 2023 poster, 2022. [279] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2022. [280] Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning, 2016. [281] Isabel Gallegos, Ryan Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen Ahmed. Bias and fairness in large language models: survey. arXiv preprint arXiv:2309.00770, 2023. [282] Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: survey, 2024. URL https://arxiv.org/abs/2309.00770. 107 Published in Transactions on Machine Learning Research (10/2025) [283] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts In Proceedings of the IEEE/CVF International Conference on Computer from diffusion models. Vision, pp. 24262436, 2023. [284] Rohit Gandikota, Sheridan Feucht, Samuel Marks, and David Bau. Erasing conceptual knowledge from language models. arXiv preprint arXiv:2410.02760, 2024. [285] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. ArXiv preprint, abs/2209.07858, 2022. URL https://arxiv.org/abs/2209.07858. [286] Feng Gao, Liangzhi Shi, Shenao Zhang, Zhaoran Wang, and Yi Wu. Adaptive-gradient policy optimization: Enhancing policy learning in non-smooth differentiable simulations. In Forty-first International Conference on Machine Learning. [287] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and In International Conference on Machine Graham Neubig. Pal: Program-aided language models. Learning, pp. 1076410799. PMLR, 2023. [288] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model, 2023. [289] Tianyu Gao, Adam Fisch, and Danqi Chen. Making Pre-trained Language Models Better Few-shot Learners. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 38163830, 2021. [290] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. [291] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [292] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. [293] Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. Creating trainIn 55th annual meeting of the Association for Computational ing corpora for nlg micro-planning. Linguistics (ACL), 2017. [294] Anisha Garg, Engin Tekin, Yash More, David Bick, Nishit Neema, and Ganesh Venkatesh. Calibrated reasoning: An explanatory verifier for dynamic and efficient problem-solving. arXiv preprint arXiv:2509.19681, 2025. [295] Aparna Garimella, Rada Mihalcea, and Akhash Amarnath. Demographic-aware language model fine-tuning as bias mitigation technique. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, pp. 311319, 2022. [296] Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, et al. Openagi: When llm meets domain experts. Advances in Neural Information Processing Systems, 36, 2024. [297] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah Smith. RearXiv preprint altoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv:2009.11462, 2020. 108 Published in Transactions on Machine Learning Research (10/2025) [298] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualization of generated text. In Annual Meeting of the Association for Computational Linguistics, 2019. URL https://api.semanticscholar.org/CorpusID:182952848. [299] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks, 2017. [300] Yonatan Geifman and Ran El-Yaniv. Selectivenet: deep neural network with an integrated reject option, 2019. [301] Walter Gerych, Haoran Zhang, Kimia Hamidieh, Eileen Pan, Maanas Sharma, Thomas Hartvigsen, and Marzyeh Ghassemi. Bendvlm: Test-time debiasing of vision-language embeddings, 2024. URL https://arxiv.org/abs/2411.04420. [302] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. [303] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680, 2022. [304] Reza Ghaeini, Xiaoli Z. Fern, Hamed Shahbazi, and Prasad Tadepalli. Saliency learning: Teaching the model where to pay attention. In NAACL-HLT (1), pp. 40164025. Association for Computational Linguistics, 2019. [305] Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De, Samuel Smith, Olivia Wiles, and Borja Balle. Differentially private diffusion models generate useful synthetic images. arXiv preprint arXiv:2302.13861, 2023. [306] Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, and André F. T. Martins. Uncertainty-aware machine translation evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.330. URL http://dx.doi.org/10.18653/v1/2021.findings-emnlp.330. [307] Yoav Goldberg. Assessing berts syntactic abilities. CoRR, abs/1901.05287, 2019. [308] Eric Goldman. An introduction to the california consumer privacy act (ccpa). Santa Clara Univ. Legal Studies Research Paper, 2020. [309] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: vision and language model for dialogue with humans, 2023. [310] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [311] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. [312] Google. Gemini ai video generator powered by veo 3. https://gemini.google/overview/video-g eneration/. [313] Google. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [314] Google. Gemini2. https://blog.google/technology/google-deepmind/google-gemini-ai-update-december2024/, 2024. [315] Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 109 Published in Transactions on Machine Learning Research (10/2025) [316] Google. Gemini 2.5 pro: Our most intelligent ai model, March 2025. URL https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/. Accessed: April 6, 2025. [317] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. arXiv preprint Critic: Large language models can self-correct with tool-interactive critiquing. arXiv:2305.11738, 2023. [318] Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pp. 3745, 2012. [319] David Greenfield and Shivan Bhavnani. Social media: Generative ai could harm mental health. Nature, 617(7962):676, 2023. [320] Anthony Greenwald, Debbie McGhee, and Jordan LK Schwartz. Measuring individual differences in implicit cognition: the implicit association test. Journal of personality and social psychology, 74 (6):1464, 1998. [321] Roger B. Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukosiute, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large language model generalization with influence functions. CoRR, abs/2308.03296, 2023. [322] Cornelia Gruber, Patrick Oliver Schenk, Malte Schierholz, Frauke Kreuter, and Göran Kauermann. Sources of uncertainty in machine learninga statisticians view. arXiv preprint arXiv:2305.16703, 2023. [323] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [324] Jindong Gu. Responsible generative ai: What to generate and what not. arXiv preprint arXiv:2404.05783, 2024. [325] Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. systematic survey of prompt engineering on vision-language foundation models. arXiv preprint arXiv:2307.12980, 2023. [326] Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqian Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, Xiaochun Cao, and Philip Torr. survey on transferability of adversarial examples across deep neural networks. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=AYJ3m7BocI. [327] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. Agent smith: single image can jailbreak one million multimodal llm agents exponentially fast. In ICML, 2024. [328] Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. [329] Nuno Guerreiro, Elena Voita, and André FT Martins. Looking for needle in haystack: comprehensive study of hallucinations in neural machine translation. arXiv preprint arXiv:2208.05309, 2022. [330] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. ArXiv preprint, abs/2308.08998, 2023. URL https://arxiv.org/ab s/2308.08998. 110 Published in Transactions on Machine Learning Research (10/2025) [331] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. [332] Anish Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. ArXiv, abs/2308.06394, 2023. URL https://api.semanticscholar.org/Corpus ID:260887222. [333] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. ArXiv, abs/2301.07597, 2023. URL https://api.semanticscholar.org/CorpusID:255998637. [334] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks, 2017. [335] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. Gradient-based adversarial attacks against text transformers. arXiv preprint arXiv:2104.13733, 2021. [336] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, and Zuquan Song. Seed1.5-vl technical report, 2025. URL https://arxiv.org/abs/2505.07062. [337] Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. Reward reasoning model. arXiv preprint arXiv:2505.14674, 2025. [338] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023. [339] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares-López, Alexandre Ramé, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. Direct Language Model Alignment from Online AI Feedback. arXiv, abs/2402.04792, 2024. 111 Published in Transactions on Machine Learning Research (10/2025) [340] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. [341] Yue Guo, Yi Yang, and Ahmed Abbasi. Auto-debias: Debiasing masked language models with automated biased prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10121023, 2022. [342] Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, and Shuo Shuo Liu. Bias in large language models: Origin, evaluation, and mitigation, 2024. URL https://arxiv.org/abs/2411.10915. [343] Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan. Prompttts: Controllable text-tospeech with text descriptions. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. [344] Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond, 2024. [345] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris arXiv preprint Bertsimas. Finding neurons in haystack: Case studies with sparse probing. arXiv:2305.01610, 2023. [346] Michael Hahn and Navin Goyal. theory of emergent in-context learning as implicit structure induction, 2023. [347] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. Warp: Word-level adversarial reprogramming. arXiv preprint arXiv:2101.00121, 2021. [348] Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, and Filippos Kokkinos. Learning to see before seeing: Demystifying llm visual priors from language pre-training. arXiv preprint arXiv:2509.26625, 2025. [349] Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Spotting llms with binoculars: Zero-shot detection of machine-generated text. arXiv preprint arXiv:2401.12070, 2024. [350] Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, et al. Be like goldfish, dont memorize! mitigating memorization in generative llms. arXiv preprint arXiv:2406.10209, 2024. [351] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Advances in neural information processing systems, 36, 2024. [352] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong arXiv preprint Tian. Training large language models to reason in continuous latent space. arXiv:2412.06769, 2024. [353] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In AAAI, pp. 1296312971. AAAI Press, 2021. [354] Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors. ArXiv, abs/2211.11031, 2022. URL https://api.semanticscholar.org/CorpusID:253735429. [355] Peter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srini Iyer. Methods for measuring, updating, and visualizing factual beliefs in language models. In Conference of the European Chapter of the Association for Computational Linguistics, 2023. URL https://api.semanticscholar.org/CorpusID:258378150. 112 Published in Transactions on Machine Learning Research (10/2025) [356] T. Hastie, R. Tibshirani, and J.H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer series in statistics. Springer, 2001. ISBN 9780387952840. URL https://books.google.com/books?id=VRzITwgNV2UC. [357] Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan, Ruoyu Qin, Ruofan Lu, Rui Lu, Yunchen Zhang, Gangming Zhao, Zhaohui Hou, Zhiyuan Huang, Shaoqing Lu, Ding Liang, and Mingjie Zhan. Teacherlm: Teaching to fish rather than giving the fish, language modeling likewise, 2023. [358] Zexue He, Yu Wang, Julian J. McAuley, and Bodhisattwa Prasad Majumder. Controlling bias exIn EMNLP (Findings), pp. 58545866. Association for posure for fair interpretable predictions. Computational Linguistics, 2022. [359] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values, 2020. [360] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 83408349, 2021. [361] John Hewitt and Christopher D. Manning. structural probe for finding syntax in word representations. In NAACL-HLT (1), pp. 41294138. Association for Computational Linguistics, 2019. [362] John Hewitt and Christopher D. Manning. Structural Probe for Finding Syntax in Word Representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41294138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419. [363] John Hewitt, Sarah Chen, Lanruo Lora Xie, Edward Adams, Percy Liang, and Christopher Manning. Model editing with canonical examples. arXiv preprint arXiv:2402.06155, 2024. [364] Yusuke Hirota, Jerone TA Andrew, Dora Zhao, Orestis Papakyriakopoulos, Apostolos Modas, Yuta Nakashima, and Alice Xiang. Resampled datasets are not enough: Mitigating societal bias beyond single attributes. arXiv preprint arXiv:2407.03623, 2024. [365] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [366] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training computeoptimal large language models. arXiv preprint arXiv:2203.15556, 2022. [367] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2(4):5, 2024. [368] Rachel Hong, William Agnew, Tadayoshi Kohno, and Jamie Morgenstern. Whos in and whos out? case study of multimodal clip-filtering in datacomp, 2024. [369] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. [370] Yan Hong and Jianfu Zhang. Wildfake: large-scale challenging dataset for ai-generated images detection. arXiv preprint arXiv:2402.11843, 2024. [371] Ye Hongbin, Liu Tong, Zhang Aijia, Hua Wei, and Jia Weiqiang. Cognitive mirage: review of hallucinations in large language models. arXiv, 2023. 113 Published in Transactions on Machine Learning Research (10/2025) [372] Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: visual analysis tool to explore learned representations in transformer models. In ACL (demo), pp. 187196. Association for Computational Linguistics, 2020. [373] Abe Bohan Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, and Yulia Tsvetkov. Semstamp: semantic watermark with paraphrastic robustness for text generation. ArXiv, abs/2310.03991, 2023. URL https://api.semanticscholar.org/CorpusID:263831179. [374] Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang. Decomposing uncertainty for large language models through input clarification ensembling, 2023. [375] Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, and Mrinmaya Sachan. Towards mechanistic interpretation of multi-step reasoning capabilities of language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 49024919, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.299. URL https://aclanthology.org/2023.emnlp-main.299/. [376] Genc Hoxha, Giacomo Scuccato, and Farid Melgani. Improving image captioning systems with postprocessing strategies. IEEE Transactions on Geoscience and Remote Sensing, 2023. [377] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [378] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models, 2025. URL https://arxiv.org/abs/2501.03262. [379] Michael Y. Hu, Angelica Chen, Naomi Saphra, and Kyunghyun Cho. Latent state models of training dynamics, 2023. [380] Pingyi Hu, Zihan Wang, Ruoxi Sun, Hu Wang, and Minhui Xue. 4 i: Multi-modal models membership inference. Advances in Neural Information Processing Systems, 35:18671882, 2022. [381] Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong Sun. Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 22252240, 2022. [382] Xiaobing Hu, Pinyu Chen, and Tsung-Yi Ho. Radar: Robust ai-text detection via adversarial learning. ArXiv, abs/2307.03838, 2023. URL https://api.semanticscholar.org/CorpusID:259501842. [383] Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark for large language models. ArXiv, abs/2310.10669, 2023. URL https://api.semantic scholar.org/CorpusID:264172471. [384] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Toward controlled generation of text, 2018. URL https://arxiv.org/abs/1703.00955. [385] Alyssa Huang, Peihan Liu, Ryumei Nakada, Linjun Zhang, and Wanrong Zhang. Safeguarding data in multimodal ai: differentially private approach to clip training. arXiv preprint arXiv:2306.08173, 2023. [386] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 10491065, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.67. URL https://aclanthology.o rg/2023.findings-acl.67. Published in Transactions on Machine Learning Research (10/2025) [387] Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models leaking your personal information? arXiv preprint arXiv:2205.12628, 2022. [388] Huang, Song, Hanwen Su, and Jiyan Wang. Out-of-distribution detection using peer-class generated by large language model. arXiv preprint arXiv:2403.13324, 2024. [389] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [390] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, 2025. [391] Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. Challenges in building intelligent open-domain dialog systems. ACM Transactions on Information Systems (TOIS), 38(3):132, 2020. [392] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023. [393] Qihan Huang, Jie Song, Mengqi Xue, Haofei Zhang, Bingde Hu, Huiqiong Wang, Hao Jiang, Xingen Wang, and Mingli Song. LG-CAV: Train any concept activation vector with language guidance. In NeurIPS, 2024. [394] Qingqing Huang, Daniel Park, Tao Wang, Timo Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: Text-conditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917, 2023. [395] Quzhe Huang, Shengqi Zhu, Yansong Feng, and Dongyan Zhao. Exploring distantly-labeled rationales In ACL/IJCNLP (1), pp. 55715582. Association for Computational in neural network models. Linguistics, 2021. [396] Shuo Huang, William MacLean, Xiaoxi Kang, Qiongkai Xu, Zhuang Li, Xingliang Yuan, Gholamreza Haffari, and Lizhen Qu. Nap^2: benchmark for naturalness and privacy-preserving text rewriting by learning from human. arXiv preprint arXiv:2406.03749, 2024. [397] Wei Huang, Yizhe Xiong, Xin Ye, Zhijie Deng, Hui Chen, Zijia Lin, and Guiguang Ding. Fast quiet-star: Thinking without thought tokens. arXiv preprint arXiv:2505.17746, 2025. [398] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. [399] Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, Osbert Bastani, and Edgar Dobriban. Uncertainty in language models: Assessment through rank-calibration, 2024. [400] Yue Huang, Qihui Zhang, Lichao Sun, et al. Trustgpt: benchmark for trustworthy and responsible large language models. ArXiv preprint, abs/2306.11507, 2023. URL https://arxiv.org/abs/2306 .11507. [401] Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, et al. Position: Trustllm: Trustworthiness in large language models. In International Conference on Machine Learning (ICML), 2024. [402] Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi, Qihui Zhang, Yuan Li, Han Bao, Zhaoyi Liu, Tianrui Guan, Dongping Chen, Ruoxi Chen, Kehan Guo, Andy Zou, Bryan Hooi Kuen-Yew, Caiming Xiong, Elias Stengel-Eskin, Hongyang Published in Transactions on Machine Learning Research (10/2025) Zhang, Hongzhi Yin, Huan Zhang, Huaxiu Yao, Jaehong Yoon, Jieyu Zhang, Kai Shu, Kaijie Zhu, Ranjay Krishna, Swabha Swayamdipta, Taiwei Shi, Weijia Shi, Xiang Li, Yiwei Li, Yuexing Hao, Zhihao Jia, Zhize Li, Xiuying Chen, Zhengzhong Tu, Xiyang Hu, Tianyi Zhou, Jieyu Zhao, Lichao Sun, Furong Huang, Or Cohen Sasson, Prasanna Sattigeri, Anka Reuel, Max Lamparth, Yue Zhao, Nouha Dziri, Yu Su, Huan Sun, Heng Ji, Chaowei Xiao, Mohit Bansal, Nitesh V. Chawla, Jian Pei, Jianfeng Gao, Michael Backes, Philip S. Yu, Neil Zhenqiang Gong, Pin-Yu Chen, Bo Li, Dawn Song, and Xiangliang Zhang. On the trustworthiness of generative foundation models: Guideline, assessment, and perspective, 2025. [403] Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, and Xiaoxing Ma. Securing reliability: brief overview on enhancing in-context learning for foundation models. arXiv preprint arXiv:2402.17671, 2024. [404] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformerpatcher: One mistake worth one neuron. ArXiv, abs/2301.09785, 2023. URL https://api.semant icscholar.org/CorpusID:256194369. [405] Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, et al. taxonomy and review of generalization research in nlp. Nature Machine Intelligence, 5(10):11611174, 2023. [406] Timour Igamberdiev and Ivan Habernal. Dp-bart for privatized text rewriting under local differential privacy. arXiv preprint arXiv:2302.07636, 2023. [407] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. ArXiv preprint, abs/2303.05398, 2023. URL https://arxiv.org/abs/2303.05398. [408] Alvi Md Ishmam and Christopher Thomas. Semantic shield: Defending vision-language models In Proceedings of the against backdooring and poisoning via fine-grained knowledge alignment. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2482024830, 2024. [409] Khawar Islam, Muhammad Zaigham Zaheer, Arif Mahmood, and Karthik Nandakumar. Diffusemix: Label-preserving data augmentation with diffusion models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2761127620. IEEE, June 2024. doi: 10.1109/cvpr52733.2024.02608. URL http://dx.doi.org/10.1109/CVPR52733.2024.02608. [410] Takuya Ito, Soham Dan, Mattia Rigotti, James Kozloski, and Murray Campbell. On the generalization capacity of neural networks during generic multimodal reasoning. arXiv preprint arXiv:2401.15030, 2024. [411] Alon Jacovi and Yoav Goldberg. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 41984205, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.386. URL https://aclanthology.org/2020.acl-mai n.386. [412] Abhyuday Jagannatha, Bhanu Pratap Singh Rawat, and Hong Yu. Membership inference attack susceptibility of clinical language models. arXiv preprint arXiv:2104.08305, 2021. [413] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neftune: Noisy embeddings improve instruction finetuning. arXiv preprint arXiv:2310.05914, 2023. [414] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614, 2023. [415] Sarthak Jain and Byron Wallace. Attention is not explanation. arXiv preprint arXiv:1902.10186, 2019. 116 Published in Transactions on Machine Learning Research (10/2025) [416] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. arXiv preprint arXiv:2210.01504, 2022. [417] Theo Jaunet, Corentin Kervadec, Romain Vuillemot, Grigory Antipov, Moez Baccouche, and Christian Wolf. Visqa: X-raying vision and language reasoning in transformers. IEEE Transactions on Visualization and Computer Graphics, 28(1):976986, 2021. [418] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. [419] Ziwei Ji, Lei Yu, Yeskendir Koishekenov, Yejin Bang, Anthony Hartshorn, Alan Schelten, Cheng Zhang, Pascale Fung, and Nicola Cancedda. Calibrating verbal uncertainty as linear feature to reduce hallucinations, 2025. [420] Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. BadEncoder: Backdoor attacks to pre-trained encoders in self-supervised learning. In IEEE Symposium on Security and Privacy, 2022. [421] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. [422] Shuyang Jiang, Yusheng Liao, Ya Zhang, Yu Wang, and Yanfeng Wang. Taia: Large language models are out-of-distribution data learners. arXiv preprint arXiv:2405.20192, 2024. [423] Zhengping Jiang, Jingyu Zhang, Nathaniel Weir, Seth Ebner, Miriam Wanner, Kate Sanders, Daniel Khashabi, Anqi Liu, and Benjamin Van Durme. Core: Robust factual precision scoring with informative sub-claim identification, 2024. [424] Haibo Jin, Ruoxi Chen, Jinyin Chen, and Haohan Wang. Quack: Automatic jailbreaking large language models via role-playing, 2024. URL https://openreview.net/forum?id=1zt8GWZ9sc. [425] Xisen Jin and Xiang Ren. What will my model forget? forecasting forgotten examples in language model refinement. In International conference on machine learning, 2024. [426] H. Jingnan. Xs chatbot can now generate ai images. lack of guardrails raises election concerns. NPR, August 16 2024. URL https://www.npr.org/2024/08/16/nx-s1-5078636/x-twitter-art ificial-intelligence-trump-kamala-harris-election. [427] Michael Jordan, Zoubin Ghahramani, Tommi Jaakkola, and Lawrence Saul. An introduction to variational methods for graphical models. In Learning in graphical models, pp. 105161. Springer, 1998. [428] Brihi Joshi, Aaron Chan, Ziyi Liu, Shaoliang Nie, Maziar Sanjabi, Hamed Firooz, and Xiang Ren. In Findings of the Er-test: Evaluating explanation regularization methods for language models. Association for Computational Linguistics: EMNLP 2022, pp. 33153336, 2022. [429] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022. [430] Adam Tauman Kalai and Santosh S. Vempala. Calibrated language models must hallucinate. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing, STOC 24, pp. 160171. ACM, June 2024. doi: 10.1145/3618260.3649777. URL http://dx.doi.org/10.1145/3618260.364 9777. 117 Published in Transactions on Machine Learning Research (10/2025) [431] Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift, 2020. [432] Masahiro Kaneko and Danushka Bollegala. Unmasking the maskevaluating social biases in masked language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 1195411962, 2022. [433] Haoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language models through real-time verification and rectification. arXiv preprint arXiv:2311.09114, 2023. [434] Mintong Kang, Nezihe Merve Gürel, Ning Yu, Dawn Song, and Bo Li. C-rag: Certified generation risks for retrieval-augmented language models. arXiv preprint arXiv:2402.03181, 2024. [435] Mintong Kang, Dawn Song, and Bo Li. Diffattack: Evasion attacks against diffusion-based adversarial purification. Advances in Neural Information Processing Systems, 36, 2024. [436] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [437] Sanjay Kariyappa, Freddy Lécué, Saumitra Mishra, Christopher Pond, Daniele Magazzeni, and Manuela Veloso. Progressive inference: Explaining decoder-only sequence classification models using intermediate predictions, 2024. [438] Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah Smith, Yejin Choi, Kentaro Inui, et al. Realtime qa: whats the answer right now? Advances in Neural Information Processing Systems, 36, 2023. [439] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. Interpreting interpretability: understanding data scientists use of interpretability tools for machine learning. In Proceedings of the 2020 CHI conference on human factors in computing systems, pp. 114, 2020. [440] Rémi Kazmierczak, Eloïse Berthier, Goran Frehse, and Gianni Franchi. CLIP-QDA: an explainable concept bottleneck model. CoRR, abs/2312.00110, 2023. [441] Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani, and Xiang Ren. Contextualizing hate speech classifiers with post-hoc explanation. In ACL, pp. 54355442. Association for Computational Linguistics, 2020. [442] Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. [443] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019. [444] Daniel Khashabi, Shane Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, et al. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. arXiv preprint arXiv:2112.08348, 2021. [445] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres. Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV), June 2018. URL http://arxiv.org/abs/1711.11279. arXiv:1711.11279 [stat]. [446] Eunji Kim, Siwon Kim, Chaehun Shin, and Sungroh Yoon. De-stereotyping text-to-image models through prompt tuning. 2023. [447] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models, 2023. 118 Published in Transactions on Machine Learning Research (10/2025) [448] Jae Myung Kim, Koepke, Cordelia Schmid, and Zeynep Akata. Exposing and mitigating spurious In Proceedings of the IEEE/CVF Conference on Computer correlations for cross-modal retrieval. Vision and Pattern Recognition, pp. 25842594, 2023. [449] Jinhwa Kim, Ali Derakhshan, and Ian G. Harris. Robust safety classifier for large language models: Adversarial prompt shield. arXiv preprint arXiv:2311.00172, 2023. [450] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile: Probing privacy leakage in large language models. arXiv preprint arXiv:2307.01881, 2023. [451] Sunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Jungsoo Won, Dongha Lee, and Jinyoung Yeo. Evaluating robustness of reward models for mathematical reasoning. arXiv preprint arXiv:2410.01729, 2024. [452] Yujin Kim, Jaehong Yoon, Seonghyeon Ye, Sangmin Bae, Namgyu Ho, Sung Ju Hwang, and Se-Young Yun. Carpe diem: On the evaluation of world knowledge in lifelong language models. In The North American Chapter of the Association for Computational Linguistics, 2024. [453] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. watermark for large language models. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:256194179. [454] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models. ArXiv, abs/2306.04634, 2023. URL https://api.semanticscholar.or g/CorpusID:259095643. [455] Hannah Rose Kirk, Alexander Whitefield, Paul Rottger, Andrew Bean, Katerina Margatina, Rafael Mosquera-Gomez, Juan Ciro, Max Bartolo, Adina Williams, He He, et al. The prism alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Advances in Neural Information Processing Systems, 37:105236105344, 2024. [456] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452, 2023. [457] Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot image recognition. In ICML deep learning workshop, volume 2, pp. 130. Lille, 2015. [458] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions, 2017. [459] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 53385348. PMLR, 2020. [460] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: benchmark of in-the-wild distribution shifts. In International conference on machine learning, pp. 56375664. PMLR, 2021. [461] Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples. ArXiv, abs/2307.11729, 2023. URL https: //api.semanticscholar.org/CorpusID:260091573. [462] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. 119 Published in Transactions on Machine Learning Research (10/2025) [463] Enja Kokalj, Blaz Skrlj, Nada Lavrac, Senja Pollak, and Marko Robnik-Sikonja. BERT meets shapley: Extending SHAP explanations to transformer-based classifiers. In EACL (Hackashop), pp. 1621. Association for Computational Linguistics, 2021. [464] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [465] Fei Kong, Jinhao Duan, RuiPeng Ma, Heng Tao Shen, Xiaoshuang Shi, Xiaofeng Zhu, and Kaidi Xu. An efficient membership inference attack for the diffusion model by proximal initialization. In The Twelfth International Conference on Learning Representations, 2024. [466] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [467] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the Dark Secrets of BERT, September 2019. URL http://arxiv.org/abs/1908.08593. arXiv:1908.08593 [cs, stat]. [468] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022. [469] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. ArXiv, abs/2303.13408, 2023. URL https://api.semanticscholar.org/CorpusID:257687440. [470] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [471] Kuaishou. Kling. https://klingai.kuaishou.com/, 2024. Accessed: 2024-12-09. [472] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models. ArXiv, abs/2307.15593, 2023. URL https://api.semanticscho lar.org/CorpusID:260315804. [473] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Clam: Selective clarification for ambiguous questions with generative language models, 2023. [474] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation, 2023. [475] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. Certifying llm safety against adversarial prompting. arXiv preprint arXiv:2309.02705, 2023. [476] Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and Andrew Beam. Conformal prediction with large language models for multi-choice question answering, 2023. [477] Surender Suresh Kumar, M.L. Cummings, and Alexander Stimpson. Strengthening llm trust boundaries: survey of prompt injection attacks surender suresh kumar dr. m.l. cummings dr. alexander stimpson. In 2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS), 2024. [478] Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. arXiv preprint arXiv:2403.10153, 2024. [479] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2269122702, 2023. 120 Published in Transactions on Machine Learning Research (10/2025) [480] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19311941, 2023. [481] Jenny Kunz and Marco Kuhlmann. Classifier probes may just learn from linear context features. In COLING, pp. 51365146. International Committee on Computational Linguistics, 2020. [482] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan Black, and Yulia Tsvetkov. Measuring bias in contextualized word representations. arXiv preprint arXiv:1906.07337, 2019. [483] Salem Lahlou, Moksh Jain, Hadi Nekoei, Victor Ion Butoi, Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, and Yoshua Bengio. Deup: Direct epistemic uncertainty prediction, 2023. [484] Wen Lai, Alexander Fraser, and Ivan Titov. Joint localization and activation editing for low-resource fine-tuning. arXiv preprint arXiv:2502.01179, 2025. [485] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Stepwise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. [486] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. [487] Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 537563, 2022. [488] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023. [489] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open webscale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024. [490] Thomas Lavergne, Tanguy Urvoy, and François Yvon. Detecting fake content with relative entropy scoring. In Pan, 2008. URL https://api.semanticscholar.org/CorpusID:12098535. [491] Rémi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to the biography domain. arXiv preprint arXiv:1603.07771, 2016. [492] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436444, 2015. [493] Dong-Ho Lee, Akshen Kadakia, Brihi Joshi, Aaron Chan, Ziyi Liu, Kiran Narahari, Takashi Shibuya, Ryosuke Mitani, Toshiyuki Sekiya, Jay Pujara, and Xiang Ren. Xmd: An end-to-end framework for interactive explanation-based debugging of nlp models, 2022. [494] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. [495] Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, and Sung Ju Hwang. Stella: Continual audiovideo pre-training with spatio-temporal localized alignment. In International Conference on Machine Learning, 2024. [496] Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David Sussillo. Hallucinations in neural machine translation. 2018. URL https://api.semanticscholar.org/CorpusID:53593076. 121 Published in Transactions on Machine Learning Research (10/2025) [497] Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee Kim. Who wrote this code? watermarking for code generation. ArXiv, abs/2305.15060, 2023. URL https://api.semanticscholar.org/CorpusID:258865409. [498] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-toimage models. Advances in Neural Information Processing Systems, 36, 2024. [499] Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. Rationalizing neural predictions. In EMNLP, pp. 107117. The Association for Computational Linguistics, 2016. [500] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: research direction. arXiv preprint arXiv:1811.07871, 2018. [501] Jixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. Taming overconfidence in llms: Reward calibration in rlhf. arXiv preprint arXiv:2410.09724, 2024. [502] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint arXiv:2311.16922, 2023. [503] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [504] Vladimir Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pp. 707710. Soviet Union, 1966. [505] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems, pp. 94599474, 2020. [506] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning, 2023. [507] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [508] Bowen Li, Zhaoyu Li, Qiwei Du, Jinqi Luo, Wenshan Wang, Yaqi Xie, Simon Stepputtis, Chen Wang, Katia Sycara, Pradeep Ravikumar, Alexander Gray, Xujie Si, and Sebastian Scherer. Logicity: Advancing neuro-symbolic ai with abstract urban simulation. In Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2024. [509] Bowen Li, Tom Silver, Sebastian Scherer, and Alex Gray. Bilevel Learning for Bilevel Planning. In Proceedings of the Robotics: Science and Systems (RSS), 2025. [510] Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. Large language models understand and can be enhanced by emotional stimuli. ArXiv preprint, abs/2307.11760, 2023. URL https://arxiv.org/abs/2307.11760. [511] Cheng Li, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. Culturellm: Incorporating cultural differences into large language models, 2024. URL https://arxiv.org/abs/2402.10946. [512] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X. Yu, and Surinder Kumar. Large language models with controllable working memory. ArXiv, abs/2211.05110, 2022. URL https://api.semanticscholar.org/CorpusID:253420654. 122 Published in Transactions on Machine Learning Research (10/2025) [513] Hang Li, Chengzhi Shen, Philip Torr, Volker Tresp, and Jindong Gu. Self-discovering interpretable diffusion latent directions for responsible text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1200612016, 2024. [514] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197, 2023. [515] Jialu Li, Jaemin Cho, Yi-lin Sung, Jaehong Yoon, and Mohit Bansal. Selma: Learning and merging skill-specific text-to-image experts with auto-generated data. arXiv preprint arXiv:2403.06952, 2024. [516] Jiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. Probing via Prompting, July 2022. URL http: //arxiv.org/abs/2207.01736. arXiv:2207.01736 [cs]. [517] Jiarui Li, Ye Yuan, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hallucinations: case study on domain-specific queries in private knowledge-bases. arXiv preprint arXiv:2403.10446, 2024. [518] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [519] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. 3 it: large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023. [520] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. In NeurIPS Datasets and Benchmarks Track, 2024. [521] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. arXiv preprint arXiv:2308.12032, 2023. [522] Moxin Li, Yong Zhao, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, Tat-Seng Chua, and Yang Deng. Knowledge boundary of large language models: survey. In ACL Long Papers, 2025. [523] Shuangtao Li, Shuaihao Dong, Kexin Luan, Xinhan Di, and Chaofan Ding. Enhancing reasoning through process supervision with monte carlo tree search. arXiv preprint arXiv:2501.01478, 2025. [524] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. [525] Weizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang, Qianben Chen, Weichen Sun, Qiexiang Wang, Hongxuan Lu, Tianrui Qin, Chenghao Zhu, Yi Yao, Shuying Fan, Xiaowan Li, Tiannan Wang, Pai Liu, King Zhu, He Zhu, Dingfeng Shi, Piaohong Wang, Yeyi Guan, Xiangru Tang, Minghao Liu, Yuchen Eleanor Jiang, Jian Yang, Jiaheng Liu, Ge Zhang, and Wangchunshu Zhou. Chain-of-agents: End-to-end agent foundation models via multiagent distillation and agentic rl, 2025. URL https://arxiv.org/abs/2508.13167. [526] Xiang Li, Jinqi Luo, and Rabih Younes. Activitygan: generative adversarial networks for data augmentation in sensor-based human activity recognition. In Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers, UbiComp/ISWC 20 Adjunct, pp. 249254, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450380768. doi: 10.1 145/3410530.3414367. URL https://doi.org/10.1145/3410530.3414367. [527] Xiang Lisa Li, Urvashi Khandelwal, and Kelvin Guu. Few-shot recalibration of language models, 2024. [528] Xiaonan Li and Xipeng Qiu. Finding support examples for in-context learning, 2023. 123 Published in Transactions on Machine Learning Research (10/2025) [529] Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, and Wenyuan Xu. Safegen: Mitigating unsafe content generation in text-to-image models. arXiv preprint arXiv:2404.06666, 2024. [530] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In ICLR, 2022. [531] Yansong Li, Zhixing Tan, and Yang Liu. Privacy-preserving prompt tuning for large language model services. arXiv preprint arXiv:2305.06212, 2023. [532] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [533] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. Advances in Neural Information Processing Systems, 34: 1490014912, 2021. [534] Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. survey on fairness in large language models. arXiv preprint arXiv:2308.10149, 2023. [535] Yixuan Li, Xuelin Liu, Xiaoyang Wang, Shiqi Wang, and Weisi Lin. Fakebench: Uncover the achilles heels of fake images with large multimodal models. arXiv preprint arXiv:2404.13306, 2024. [536] Yunxin Li, Xinyu Chen, Zitao Li, Zhenyu Liu, Longyue Wang, Wenhan Luo, Baotian Hu, and Min Zhang. Veripo: Cultivating long reasoning in video-llms via verifier-gudied iterative policy optimization. arXiv preprint arXiv:2505.19000, 2025. [537] Yuying Li, Gaoyang Liu, Chen Wang, and Yang Yang. Generating is believing: Membership inference attacks against retrieval-augmented generation. arXiv preprint arXiv:2406.19234, 2024. [538] Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, and Volkan Cevher. Membership inference attacks against large vision-language models. arXiv preprint arXiv:2411.02902, 2024. [539] Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, and Dimitris N. Metaxas. The hidden life of tokens: Reducing hallucination of large vision-language models via visual information steering. In ICML, 2025. [540] Zongxia Li, Paiheng Xu, Fuxiao Liu, and Hyemi Song. Towards understanding in-context learning with contrastive demonstrations and saliency maps, 2023. [541] Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, and Rene Vidal. Seca: Semantically equivalent and coherent attacks for eliciting llm hallucinations. In NeurIPS, 2025. [542] Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples. In Proceedings of the 40th International Conference on Machine Learning, pp. 2076320786, 2023. [543] Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and mitigating social biases in language models. In International Conference on Machine Learning, pp. 65656576. PMLR, 2021. [544] Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multiviz: Towards visualizing and understanding multimodal models, 2022. [545] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Published in Transactions on Machine Learning Research (10/2025) [546] Qiyao Liang, Ziming Liu, Mitchell Ostrow, and Ila Fiete. How diffusion models learn to factorize and compose. In NeurIPS, 2024. [547] Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. Gpt detectors are biased against non-native english writers. Patterns, 4(7), 2023. [548] Zhenwen Liang, Ye Liu, Tong Niu, Xiangliang Zhang, Yingbo Zhou, and Semih Yavuz. Improving llm reasoning through scaling inference computation with collaborative verification. arXiv preprint arXiv:2410.05318, 2024. [549] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [550] Youngsun Lim, Hojun Choi, and Hyunjung Shim. Evaluating image hallucination in text-to-image generation with question-answering. arXiv preprint arXiv:2409.12784, 2024. [551] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. [552] Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, and Shu Hu. Detecting multimedia generated by large ai models: survey. arXiv preprint arXiv:2402.00045, 2025. [553] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words, 2022. [554] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 29802988, 2017. [555] Weizhe Lin and Bill Byrne. Retrieval augmented visual question answering with outside knowledge. arXiv preprint arXiv:2210.03809, 2022. [556] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. Fine-grained lateinteraction multi-modal retrieval for retrieval augmented visual question answering. Advances in Neural Information Processing Systems, 36:2282022840, 2023. [557] Yongjie Lin, Yi Chern Tan, and Robert Frank. Open sesame: getting inside berts linguistic knowledge. arXiv preprint arXiv:1906.01698, 2019. [558] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models, 2023. [559] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [560] Aiwei Liu, Leyi Pan, Xuming Hu, Shuang Li, Lijie Wen, Irwin King, and Philip S. Yu. An unforgeable publicly verifiable watermark for large language models. 2023. URL https://api.semanticschola r.org/CorpusID:260333928. [561] Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. semantic invariant robust watermark for large language models. ArXiv, abs/2310.06356, 2023. URL https://api.semanticscholar.or g/CorpusID:263830310. [562] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. Were afraid language models arent modeling ambiguity, 2023. 125 Published in Transactions on Machine Learning Research (10/2025) [563] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [564] Bo Liu, Li-Ming Zhan, Zexin Lu, Yujie Feng, Lei Xue, and Xiao-Ming Wu. How good are llms at out-ofdistribution detection? In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 82118222, 2024. [565] Frederick Liu and Besim Avci. Incorporating priors with feature attribution on text classification. In ACL (1), pp. 62746283. Association for Computational Linguistics, 2019. [566] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566, 2023. [567] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. [568] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565, 1, 2023. [569] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [570] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [571] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021. [572] Jiang Liu, Chun Pong Lau, and Rama Chellappa. Diffprotect: Generate adversarial examples with diffusion models for facial privacy protection. arXiv preprint arXiv:2305.13625, 2023. [573] Jiang Liu, Chen Wei, Yuxiang Guo, Heng Yu, Alan Yuille, Soheil Feizi, Chun Pong Lau, and Instruct2attack: Language-guided semantic adversarial attacks. arXiv preprint Rama Chellappa. arXiv:2311.15551, 2023. [574] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: survey. arXiv preprint arXiv:2108.13624, 2021. [575] Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, and Hongsheng Li. Mm-instruct: Generated visual instructions for large multimodal model alignment. arXiv preprint arXiv:2406.19736, 2024. [576] Jinxin Liu, Shulin Cao, Jiaxin Shi, Tingjian Zhang, Lei Hou, and Juanzi Li. Probing structured semantics understanding and generation of language models via question answering. arXiv preprint arXiv:2401.05777, 2024. [577] Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, and Jacob Andreas. Cognitive dissonance: Why do language model outputs disagree with internal representations of truthfulness? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 47914797, 2023. [578] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):135, 2023. Published in Transactions on Machine Learning Research (10/2025) [579] Runtao Liu, Ashkan Khakzar, Jindong Gu, Qifeng Chen, Philip Torr, and Fabio Pizzati. Latent guard: safety framework for text-to-image generation. European Conference on Computer Vision (ECCV) (to appear), 2024. [580] Sheng Liu, Haotian Ye, Lei Xing, and James Zou. Reducing hallucinations in vision-language models via latent space steering, 2024. [581] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In ICML, 2024. [582] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush Varshney, et al. Rethinking machine unlearning for large language models. arXiv preprint arXiv:2402.08787, 2024. [583] Tianyu Liu, Xin Zheng, Baobao Chang, and Zhifang Sui. Towards faithfulness in open domain tableto-text generation from an entity-centric view. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1341513423, 2021. [584] Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, and Kai Chen. Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction. In 33rd USENIX Security Symposium (USENIX Security 24), pp. 47114728, 2024. [585] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023. [586] Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, and Chao Shen. Coco: Coherenceenhanced machine-generated text detection under low resource with contrastive learning. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticsc holar.org/CorpusID:264406273. [587] Xin Liu, Muhammad Khalifa, and Lu Wang. Litcab: Lightweight language model calibration over shortand long-form responses, 2024. [588] Xinwei Liu, Jian Liu, Yang Bai, Jindong Gu, Tao Chen, Xiaojun Jia, and Xiaochun Cao. Watermark vaccine: Adversarial attacks to prevent watermark removal. In European Conference on Computer Vision, pp. 117. Springer, 2022. [589] Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and Jianfeng Ma. Backdoor defense with machine unlearning. In IEEE INFOCOM 2022-IEEE conference on computer communications, pp. 280289. IEEE, 2022. [590] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023. [591] Yibing Liu, Haoliang Li, Yangyang Guo, Chenqi Kong, Jing Li, and Shiqi Wang. Rethinking attentionmodel explainability through faithfulness violation test, 2022. [592] Yikang Liu, Ziyin Zhang, Wanyang Zhang, Shisen Yue, Xiaojing Zhao, Xinyuan Cheng, Yiwen Zhang, and Hai Hu. Argugpt: evaluating, understanding and identifying argumentative essays generated by gpt models. ArXiv, abs/2304.07666, 2023. URL https://api.semanticscholar.org/CorpusID: 258179137. [593] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach, 2019. URL https://arxiv.org/abs/1907.11692. [594] Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, and Huazhe Xu. World models with hints of large language models for goal achieving. arXiv preprint arXiv:2406.07381, 2024. 127 Published in Transactions on Machine Learning Research (10/2025) [595] Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, and Zhaoran Wang. Reason for future, act for now: principled framework for autonomous llm agents with provable sample efficiency. arXiv preprint arXiv:2309.17382, 2023. [596] Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, and Zhaoran Wang. Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. arXiv preprint arXiv:2405.16436, 2024. [597] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv preprint arXiv:2401.10225, 2024. [598] David Lopez-Paz and MarcAurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. [599] Alejandro Lozano, Scott Fleming, Chia-Chun Chiang, and Nigam Shah. Clinfo. ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature. In PACIFIC SYMPOSIUM ON BIOCOMPUTING 2024, pp. 823. World Scientific, 2023. [600] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. UnifiedIn The Eleventh International io: unified model for vision, language, and multi-modal tasks. Conference on Learning Representations, 2022. [601] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2643926455, 2024. [602] Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in neural natural language processing. Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday, pp. 189202, 2020. [603] Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. Are emergent abilities in large language models just in-context learning?, 2023. [604] Taiming Lu, Lingfeng Shen, Xinyu Yang, Weiting Tan, Beidi Chen, and Huaxiu Yao. It takes two: On the seamlessness between reward and policy model in rlhf. arXiv preprint arXiv:2406.07971, 2024. [605] Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning. arXiv preprint arXiv:2407.00782, 2024. [606] Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Evaluating In Thirty-seventh Conference on Neural Information societal representations in diffusion models. Processing Systems Datasets and Benchmarks Track, volume 2, 2023. [607] Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago ZanellaBéguelin. Analyzing leakage of personally identifiable information in language models. arXiv preprint arXiv:2302.00539, 2023. [608] Haochen Luo, Jindong Gu, Fengyuan Liu, and Philip Torr. An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models. In The Twelfth International Conference on Learning Representations, 2023. [609] Jinqi Luo, Zhaoning Wang, Chen Henry Wu, Dong Huang, and Fernando De La Torre. Zero-shot In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern model diagnosis. Recognition (CVPR), 2023. 128 Published in Transactions on Machine Learning Research (10/2025) [610] Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, and René Vidal. Pace: Parsimonious concept engineering for large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [611] Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Hancheng Min, Chris Callison-Burch, and René Vidal. Concept lancet: Compositional representation transplant for diffusion-based image editing. In CVPR, 2025. [612] Junyu Luo, Cao Xiao, and Fenglong Ma. Zero-resource hallucination prevention for large language models. arXiv preprint arXiv:2309.02654, 2023. [613] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. [614] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. [615] Saiyue Lyu, Margarita Vinaroz, Michael Liu, and Mijung Park. Differentially private latent diffusion models. arXiv preprint arXiv:2305.15759, 2023. [616] Jiachen Ma, Anda Cao, Zhiqing Xiao, Jie Zhang, Chao Ye, and Junbo Zhao. Jailbreaking prompt attack: controllable adversarial attack against diffusion models. arXiv preprint arXiv:2404.02928, 2024. [617] Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, Himabindu Lakkaraju, et al. Post hoc explanations of language models can improve language models. arXiv preprint arXiv:2305.11426, 2023. [618] Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, and Cong Liu. Untying the reversal curse via bidirectional language model editing. ArXiv, abs/2310.10322, 2023. URL https://api.semanticsc holar.org/CorpusID:264146289. [619] Ruipeng Ma, Jinhao Duan, Fei Kong, Xiaoshuang Shi, and Kaidi Xu. Exposing the fake: Effective diffusion-generated images detection. AdvML Frontiers workshop at 40th International Conference on Machine Learning, 2023. [620] Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, et al. Safety at scale: comprehensive survey of large model safety. arXiv preprint arXiv:2502.05206, 2025. [621] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage In Proceedings of the 47th International ACM SIGIR Conference on Research and text retrieval. Development in Information Retrieval, pp. 24212425, 2024. [622] Zhe Ma, Xuhong Zhang, Qingming Li, Tianyu Du, Wenzhi Chen, Zonghui Wang, and Shouling Ji. Could it be generated? towards practical analysis of memorization in text-to-image diffusion models. arXiv preprint arXiv:2405.05846, 2024. [623] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: arXiv preprint Towards detailed video understanding via large vision and language models. arXiv:2306.05424, 2023. [624] Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. [625] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after deployment. ArXiv, abs/2201.06009, 2022. URL https://api.semanticschola r.org/CorpusID:246016194. 129 Published in Transactions on Machine Learning Research (10/2025) [626] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S37hOerQLB. [627] Andrea Madotto, Zihan Liu, Zhaojiang Lin, and Pascale Fung. Language models as few-shot learner for task-oriented dialogue systems. arXiv preprint arXiv:2008.06239, 2020. [628] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. [629] Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction, 2021. [630] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023. [631] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. arXiv preprint arXiv:2212.07016, 2022. [632] Chengzhi Mao, Carl Vondrick, Hao Wang, and Junfeng Yang. Raidar:geneRative AI Detection viA Rewriting. In ICLR, 2024. [633] Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv preprint arXiv:2310.06824, 2023. [634] Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. Hatexplain: benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 1486714875, 2021. [635] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schölkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. arXiv preprint arXiv:2305.18462, 2023. [636] Rowan Hall Maudslay and Ryan Cotterell. Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing, June 2021. URL http://arxiv.org/abs/2106.02559. arXiv:2106.02559 [cs]. [637] Chandler May, Alex Wang, Shikha Bordia, Samuel Bowman, and Rachel Rudinger. On measuring social biases in sentence encoders. arXiv preprint arXiv:1903.10561, 2019. [638] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020. [639] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. 2024. [640] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve, 2023. [641] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. Sources of hallucination by large language models on inference tasks. arXiv preprint arXiv:2305.14552, 2023. [642] Michal Měchura. taxonomy of bias-causing ambiguities in machine translation. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pp. 168173, 2022. Published in Transactions on Machine Learning Research (10/2025) [643] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):135, 2021. [644] Ninareh Mehrabi, Muhammad Naveed, Fred Morstatter, and Aram Galstyan. Exacerbating algorithmic bias through fairness attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 89308938, 2021. [645] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022. [646] Kevin Meng, Arnab Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. ArXiv, abs/2210.07229, 2022. URL https://api.semanticscholar.org/Co rpusID:252873467. [647] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with referencefree reward. arXiv preprint arXiv:2405.14734, 2024. [648] Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei Wu, Rui Yan, and Jiwei Li. Openvidial: large-scale, open-domain dialogue dataset with visual contexts. arXiv preprint arXiv:2012.15015, 2020. [649] Bradley Menz, Natansh Modi, Michael Sorich, and Ashley Hopkins. Health disinformation use case highlighting the urgent need for artificial intelligence vigilance: weapons of mass disinformation. JAMA internal medicine, 184(1):9296, 2024. [650] Hasan Mesut Meral, Bülent Sankur, A. Sumru Özsoy, Tunga Güngör, and Emre Sevinç. Natural language watermarking via morphosyntactic alterations. Comput. Speech Lang., 23:107125, 2009. URL https://api.semanticscholar.org/CorpusID:1192689. [651] Jack Merullo, Noah A. Smith, Sarah Wiegreffe, and Yanai Elazar. On linear representations and pretraining data frequency in language models, 2025. [652] Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436, 2023. [653] Midjourney. Midjourney. https://midjourney.com/, 2023. Accessed: 2023. [654] Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents overconfidence through linguistic calibration, 2022. [655] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013. [656] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering ambiguous open-domain questions. arXiv preprint arXiv:2004.10645, 2020. [657] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. [658] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. [659] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks, 2021. URL https://arxiv.org/abs/2106.07998. [660] MiniMax. Hailuo 02 global ai video generation model. https://hailuo-02.com/, 2025. 131 Published in Transactions on Machine Learning Research (10/2025) [661] Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, and Taylor BergKirkpatrick. An empirical analysis of memorization in fine-tuned autoregressive language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 18161826, 2022. [662] Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, R. Shokri, and Taylor Berg-Kirkpatrick. ArXiv, Smaller language models are better black-box machine-generated text detectors. abs/2305.09859, 2023. URL https://api.semanticscholar.org/CorpusID:258740888. [663] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale. ArXiv, abs/2110.11309, 2021. URL https://api.semanticscholar.org/CorpusID: 239050360. [664] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Memorybased model editing at scale. ArXiv, abs/2206.06520, 2022. URL https://api.semanticscholar. org/CorpusID:249642147. [665] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:256274849. [666] Hosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of BERT token In EMNLP (1), pp. 792806. Association for representations to explain sentence probing results. Computational Linguistics, 2021. [667] Christopher Mohri and Tatsunori Hashimoto. Language models with conformal factuality guarantees, 2024. [668] Grégoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert Müller. Layer-wise relevance propagation: An overview. In Explainable AI, volume 11700 of Lecture Notes in Computer Science, pp. 193209. Springer, 2019. [669] Moonshot. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/ab s/2501.12599. [670] Jesse Mu and Jacob Andreas. Compositional Explanations of Neurons, February 2021. URL http: //arxiv.org/abs/2006.14032. arXiv:2006.14032 [cs, stat]. [671] Bálint Mucsányi, Michael Kirchhof, and Seong Joon Oh. Benchmarking uncertainty disentanglement: Specialized uncertainties for specialized tasks. Advances in neural information processing systems, 37:5097251038, 2024. [672] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. [673] Rafael Müller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? Advances in neural information processing systems, 32, 2019. [674] Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. arXiv preprint arXiv:2305.15852, 2023. [675] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023. [676] Travis J. E. Munyer and Xin Zhong. Deeptextmark: Deep learning based text watermarking for detection of large language model generated text. ArXiv, abs/2305.05773, 2023. URL https://api. semanticscholar.org/CorpusID:258588289. 132 Published in Transactions on Machine Learning Research (10/2025) [677] Shikhar Murty, Christopher D. Manning, Scott M. Lundberg, and Marco Tulio Ribeiro. Fixing model bugs with natural language patches. In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID:249147353. [678] Max Müller-Eberstein, Rob van der Goot, Barbara Plank, and Ivan Titov. Subspace chronicles: How linguistic information emerges, shifts and interacts during language model training, 2023. [679] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020. [680] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015. [681] Krishna Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, and Xuebing Zhou. PII-compass: Guiding LLM training data extraction prompts towards the target PII via grounding. In Proceedings of the Fifth Workshop on Privacy in Natural Language Processing, pp. 6373, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [682] Yang Nan, Huichi Zhou, Xiaodan Xing, and Guang Yang. Beyond the hype: dispassionate look at vision-language models in medical scenario. arXiv preprint arXiv:2408.08704, 2024. [683] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel Bowman. Crows-pairs: challenge dataset for measuring social biases in masked language models. arXiv preprint arXiv:2010.00133, 2020. [684] Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. Stealing the decoding algorithms of language models. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pp. 18351849, 2023. [685] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, Feder Cooper, Daphne Ippolito, Christopher Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023. [686] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022. [687] Bach Hoang Ngo, Dat Thanh Nguyen, Nhat-Tuong Do-Tran, Phuc Pham Huy Thien, Minh-Hung An, Tuan-Ngoc Nguyen, Loi Nguyen Hoang, Vinh Dinh Nguyen, and Vinh Dinh. Comprehensive visual features and pseudo labeling for robust natural language-based vehicle retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54085417, 2023. [688] Debora Nozza, Federico Bianchi, Dirk Hovy, et al. Honest: Measuring hurtful sentence completion in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2021. [689] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. [690] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context Learning and Induction Heads transformer-circuits.pub. https://transforme r-circuits.pub/2022/in-context-learning-and-induction-heads/index.html, March 2022. [Accessed 27-11-2023]. 133 Published in Transactions on Machine Learning Research (10/2025) [691] OpenAI. Image generation guide (gpt-image-1). https://platform.openai.com/docs/guides/im age-generation?image-generation-model=gpt-image-1. [692] OpenAI. ChatGPT. https://openai.com/blog/chatgpt, 2023. Accessed: September 10, 2023. [693] OpenAI. Gpt-4 technical report, 2023. [694] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [695] OpenAI. Moderator overview. https://platform.openai.com/docs/guides/moderation/overv iew, 2024. Accessed: June 29, 2024. [696] OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [697] OpenAI. Sora. https://openai.com/sora, 2024. Accessed: Feburary 15, 2024. [698] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/introducing-o3-and-o 4-mini/, April 16 2025. [699] OpenAI. Gpt5. https://openai.com/index/introducing-gpt-5/, 2025. [700] OpenAI. Gpt-4.5, February 2025. URL https://openai.com/index/introducing-gpt-4-5/. Accessed: April 6, 2025. [701] Ian Osband, Seyed Mohammad Asghari, Benjamin Van Roy, Nat McAleese, John Aslanides, and Geoffrey Irving. Fine-tuning language models via epistemic neural networks, 2023. [702] Myle Ott, Michael Auli, David Grangier, and MarcAurelio Ranzato. Analyzing uncertainty in neural machine translation. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3956 3965. PMLR, 1015 Jul 2018. URL https://proceedings.mlr.press/v80/ott18a.html. [703] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730 27744, 2022. [704] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your models uncertainty? evaluating predictive uncertainty under dataset shift, 2019. [705] Anwesan Pal, Radhika Bhargava, Kyle Hinsz, Jacques Esterhuizen, and Sudipta Bhattacharya. The empirical impact of data sanitization on language models. arXiv preprint arXiv:2411.05978, 2024. [706] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. [707] Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. Totto: controlled table-to-text generation dataset. arXiv preprint arXiv:2004.14373, 2020. [708] Cheonbok Park, Inyoup Na, Yongjang Jo, Sungbok Shin, Jaehyo Yoo, Bum Chul Kwon, Jian Zhao, Hyungjong Noh, Yeonsoo Lee, and Jaegul Choo. Sanvis: Visual analytics for understanding selfattention networks. In 2019 IEEE Visualization Conference (VIS), pp. 146150. IEEE, 2019. [709] Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, and Hidenori Tanaka. Emergence of hidden capabilities: Exploring learning dynamics in concept space. In NeurIPS, 2024. 134 Published in Transactions on Machine Learning Research (10/2025) [710] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pp. 122, 2023. [711] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. Bbq: hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193, 2021. [712] Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms? objectives for defending against extraction attacks. In The Twelfth International Conference on Learning Representations, 2024. [713] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on data diet: Finding important examples early in training. Advances in Neural Information Processing Systems, 34:2059620607, 2021. [714] Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph, Jean-François Godbout, and Reihaneh Rabbany. Towards reliable misinformation mitigation: Generalization, uncertainty, and gpt-4, 2023. [715] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023. URL https: //arxiv.org/abs/2306.01116. [716] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023. [717] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 15321543, 2014. [718] Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022. [719] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations, 2018. URL https://arxiv.org/ abs/1802.05365. [720] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. Language Models as Knowledge Bases?, September 2019. URL http://arxi v.org/abs/1909.01066. arXiv:1909.01066 [cs]. [721] Buu Phan, Marton Havasi, Matthew Muckley, and Karen Ullrich. Understanding and mitigating tokenization bias in language models. arXiv preprint arXiv:2406.16829, 2024. [722] Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong arXiv preprint Zhang. Mllm-protector: Ensuring mllms safety without hurting performance. arXiv:2401.02906, 2024. [723] J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10(3), 1999. [724] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=KmtVD97J43e. 135 Published in Transactions on Machine Learning Research (10/2025) [725] Lip Yee Por, Koksheik Wong, and Kok Onn Chee. Unispach: text-based data hiding method using unicode space characters. J. Syst. Softw., 85:10751082, 2012. URL https://api.semanticschola r.org/CorpusID:17690312. [726] Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman. Lance: Stresstesting visual models by generating language-guided counterfactual images. In NeurIPS, 2023. [727] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. ArXiv preprint, abs/2305.03495, 2023. URL https://arxiv.org/abs/2305.03495. [728] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. Visual adversarial In The Second Workshop on New Frontiers in examples jailbreak aligned large language models. Adversarial Machine Learning, volume 1, 2023. [729] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! ArXiv preprint, abs/2310.03693, 2023. URL https://arxiv.org/abs/2310.03693. [730] Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and Peter Henderson. Safety alignment should be made more than just few tokens deep. arXiv preprint arXiv:2406.05946, 2024. [731] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. [732] Rebecca Qian, Candace Ross, Jude Fernandes, Eric Smith, Douwe Kiela, and Adina Williams. Perturbation augmentation for fairer nlp. arXiv preprint arXiv:2205.12586, 2022. [733] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Jiang, Chengfei Lv, and Huajun Chen. AutoAct: Automatic agent learning from scratch for QA via self-planning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 30033021, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.o rg/2024.acl-long.165. [734] Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. [735] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. ArXiv preprint, abs/2304.08354, 2023. URL https://arxiv.org/abs/2304.08354. [736] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. [737] Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc Najork. Natural language understanding with privacy-preserving bert. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pp. 14881497, 2021. [738] Wenjie Qu, Dong Yin, Zixin He, Wei Zou, Tianyang Tao, Jinyuan Jia, and Jiaheng Zhang. Provably robust multi-bit watermarking for ai-generated text via error correction code. arXiv preprint arXiv:2401.16820, 2024. [739] Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, and Regina Barzilay. Conformal language modeling, 2023. [740] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 136 Published in Transactions on Machine Learning Research (10/2025) [741] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [742] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. [743] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, and et al. Scaling language models: Methods, analysis & insights from training gopher, 2022. URL https://arxiv.org/abs/2112.11446. [744] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. ArXiv preprint, abs/2305.18290, 2023. URL https://arxiv.org/abs/2305.18290. [745] Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, and Scott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms. arXiv preprint arXiv:2406.02900, 2024. [746] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. [747] Alessandro Raganato and Jörg Tiedemann. An analysis of encoder representations in transformerbased machine translation. In Proceedings of the 2018 EMNLP workshop BlackboxNLP: analyzing and interpreting neural networks for NLP. The Association for Computational Linguistics, 2018. [748] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. [749] Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Schölkopf, and Pradeep Kumar In The Thirty-eighth Annual Ravikumar. From causal to concept-based representation learning. Conference on Neural Information Processing Systems, 2024. [750] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing, 2016. URL https://api.semanticscholar.org/CorpusID:11816014. [751] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023. [752] Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. ArXiv preprint, abs/2306.04488, 2023. URL https://arxiv.org/abs/2306.04488. [753] Alexandre Ramé, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, and Olivier Bachem. Warp: On the benefits of weight averaged rewarded policies. arXiv preprint arXiv:2406.16768, 2024. [754] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, In International Conference on Machine and Ilya Sutskever. Zero-shot text-to-image generation. Learning, pp. 88218831. PMLR, 2021. [755] Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models, 2024. URL https://arxiv.org/abs/2401.12187. 137 Published in Transactions on Machine Learning Research (10/2025) [756] Javier Rando and Florian Tramèr. Universal jailbreak backdoors from poisoned human feedback. In ICLR, 2024. [757] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramèr. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610, 2022. [758] Abhinav Rastogi, Albert Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910, 2025. [759] Shauli Ravfogel, Yoav Goldberg, and Jacob Goldberger. Conformal nucleus sampling. arXiv preprint arXiv:2305.02633, 2023. [760] Abhilasha Ravichander, Eduard Hovy, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT. In Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics, pp. 88102, Barcelona, Spain (Online), December 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.starsem-1.10. [761] Shaina Raza, Ananya Raval, and Veronica Chatrath. Mbias: Mitigating bias in large language models while retaining context, 2024. URL https://arxiv.org/abs/2405.11290. [762] Patrik Reizinger, Szilvia Ujváry, Anna Mészáros, Anna Kerekes, Wieland Brendel, and Ferenc Huszár. Position: Understanding llms requires more than statistical generalization. In Forty-first International Conference on Machine Learning, 2024. [763] Navid Rekabsaz and Markus Schedl. Do neural ranking models intensify gender bias? In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 20652068, 2020. [764] Jie Ren, Han Xu, Yiding Liu, Yingqian Cui, Shuaiqiang Wang, Dawei Yin, and Jiliang Tang. robust semantics-based watermark for large language model against paraphrasing. ArXiv, abs/2311.08721, 2023. URL https://api.semanticscholar.org/CorpusID:265213008. [765] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation, July 2023. URL http://arxiv.org/abs/2307.11019. arXiv:2307.11019 [cs]. [766] Laura Rieger, Chandan Singh, W. James Murdoch, and Bin Yu. Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 81168126. PMLR, 2020. [767] Stefano Giovanni Rizzo, Flavio Bertini, and Danilo Montesi. Content-preserving text watermarking through unicode homoglyph substitution. Proceedings of the 20th International Database Engineering & Applications Symposium, 2016. URL https://api.semanticscholar.org/CorpusID:11689200. [768] Alexander Robey, Eric Wong, Hamed Hassani, and George Pappas. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023. [769] Anna Rogers, Matt Gardner, and Isabelle Augenstein. Qa dataset explosion: taxonomy of nlp resources for question answering and reading comprehension. ACM Computing Surveys, 55(10):145, 2023. [770] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 40354045, 2018. 138 Published in Transactions on Machine Learning Research (10/2025) [771] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. [772] Alexis Ross, Ana Marasovic, and Matthew E. Peters. Explaining NLP models via minimal contrastive editing (mice). In ACL/IJCNLP (Findings), volume ACL/IJCNLP 2021 of Findings of ACL, pp. 38403852. Association for Computational Linguistics, 2021. [773] Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. Right for the right reasons: Training differentiable models by constraining their explanations. In IJCAI, pp. 26622670. ijcai.org, 2017. [774] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences. arXiv, 2024. [775] Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rocktäschel, Edward Grefenstette, and Max Bartolo. Procedural knowledge in pretraining drives reasoning in large language models, 2024. [776] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2250022510, 2023. [777] Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, and Soheil Feizi. Robustness of ai-image detectors: Fundamental limits and practical attacks. arXiv preprint arXiv:2310.00076, 2023. [778] Vinu Sankar Sadasivan, Aounon Kumar, S. Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? ArXiv, abs/2303.11156, 2023. URL https://api.semantic scholar.org/CorpusID:257631570. [779] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [780] Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, and Aman Chadha. comprehensive survey of hallucination in large language, image, video and audio foundation models. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024. [781] Julian Salazar, Davis Liang, Toan Nguyen, and Katrin Kirchhoff. Masked language model scoring. arXiv preprint arXiv:1910.14659, 2019. [782] Etienne Salimbeni, Francesco Craighero, Renata Khasanova, Milos Vasic, and Pierre Vandergheynst. Beyond fine-tuning: Lora modules boost near-ood detection and llm security. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. [783] Soumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. In EMNLP (1), pp. 1028510299. Association for Computational Linguistics, 2021. [784] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah Smith. The risk of racial bias in hate speech detection. In Proceedings of the 57th annual meeting of the association for computational linguistics, pp. 16681678, 2019. [785] Irina Saparina and Mirella Lapata. Ambrosia: benchmark for parsing ambiguous questions into database queries. arXiv preprint arXiv:2406.19073, 2024. [786] R. Sato, Yuki Takezawa, Han Bao, Kenta Niwa, and Makoto Yamada. Embarrassingly simple text watermarks. ArXiv, abs/2310.08920, 2023. URL https://api.semanticscholar.org/CorpusID: 264128148. 139 Published in Transactions on Machine Learning Research (10/2025) [787] Timo Schick, Sahana Udupa, and Hinrich Schütze. Self-diagnosis and self-debiasing: proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, 9: 14081424, 2021. [788] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024. [789] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85117, 2015. [790] Florian Schmidt. Generalization in generation: closer look at exposure bias. arXiv preprint arXiv:1910.00292, 2019. [791] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2252222531, 2023. [792] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [793] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. [794] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv preprint, abs/1707.06347, 2017. URL https://arxiv.org/abs/17 07.06347. [795] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling, 2022. [796] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models sensitivity to spurious features in prompt design or: How learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324, 2023. [797] Andrew Selbst and Julia Powles. meaningful information and the right to explanation. In conference on fairness, accountability and transparency, pp. 4848. PMLR, 2018. [798] Minju Seo, Jinheon Baek, James Thorne, and Sung Ju Hwang. Retrieval-augmented data augmentation for low-resource domain tasks. arXiv preprint arXiv:2402.13482, 2024. [799] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [800] Glenn Shafer and Vladimir Vovk. tutorial on conformal prediction. Journal of Machine Learning Research, 9(12):371421, 2008. [801] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and Ben Y. Zhao. Glaze: Protecting artists from style mimicry by Text-to-Image models. In 32nd USENIX Security Symposium (USENIX Security 23), pp. 21872204, 2023. [802] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore. arXiv preprint arXiv:2407.12854, 2024. 140 Published in Transactions on Machine Learning Research (10/2025) [803] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023. [804] Lloyd Shapley et al. value for n-person games. 1953. [805] Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, and Jimmy Lin. Unirag: Universal retrieval augmentation for multi-modal large language models. arXiv preprint arXiv:2405.10311, 2024. [806] Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare OHara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, and Ethan Perez. Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming. arXiv preprint arXiv:2501.18837, 2025. [807] Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, et al. Thinking vs. doing: Agents that reason by scaling test-time interaction. arXiv preprint arXiv:2506.07976, 2025. [808] Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, and Daniel Khashabi. The language barrier: Dissecting safety challenges of llms in multilingual contexts. arXiv preprint arXiv:2401.13136, 2024. [809] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: survey. arXiv preprint arXiv:2309.15025, 2023. [810] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. arXiv preprint arXiv:2310.05199, 2023. [811] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023. [812] Yaozong Shen, Lijie Wang, Ying Chen, Xinyan Xiao, Jing Liu, and Hua Wu. An Interpretability Evaluation Benchmark for Pre-trained Language Models, July 2022. URL http://arxiv.org/abs/ 2207.13948. arXiv:2207.13948 [cs]. [813] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3815438180. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc /paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf. [814] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019. [815] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph Gonzalez, and Ion Stoica. Fairness in serving large language models. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 965988, 2024. [816] Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Taskcraft: Automated generation of agentic tasks, 2025. URL https://arxiv.org/abs/2506.10055. 141 Published in Transactions on Machine Learning Research (10/2025) [817] Haizhou Shi, Yibin Wang, Ligong Han, Huan Zhang, and Hao Wang. Training-free bayesianization for low-rank adapters of large language models. arXiv preprint arXiv:2412.05723, 2024. [818] Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. Continual learning of large language models: comprehensive survey. arXiv preprint arXiv:2404.16789, 2024. [819] Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt. arXiv preprint arXiv:2304.12298, 2023. [820] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789, 2023. [821] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023. [822] Wentao Shi and Yiqing Shen. Reinforcement fine-tuning for reasoning towards multi-step multi-source search in large language models. arXiv preprint arXiv:2506.08352, 2025. [823] Connor Shorten and Taghi Khoshgoftaar. survey on image data augmentation for deep learning. Journal of big data, 6(1):148, 2019. [824] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. On the exploitability of instruction tuning. arXiv preprint arXiv:2306.17194, 2023. [825] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021. [826] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable, 2023. [827] Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature interaction attribution for neural NLP models. In ACL/IJCNLP (1), pp. 865878. Association for Computational Linguistics, 2021. [828] Aniket Kumar Singh, Suman Devkota, Bishal Lamichhane, Uttam Dhakal, and Chandra Dhakal. The confidence-competence gap in large language models: cognitive study, 2023. [829] Chandan Singh, Aliyah Hsu, Richard Antonello, Shailee Jain, Alexander Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in natural language with language models. arXiv preprint arXiv:2305.09863, 2023. [830] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2c: Diffusion-decoding models for few-shot conditional generation. Advances in Neural Information Processing Systems, 34:12533 12548, 2021. [831] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering. Transactions of the Association for Computational Linguistics, 11:117, 2023. [832] Shamane Siriwardhana, Mark McQuade, Thomas Gauthier, Lucas Atkins, Fernando Fernandes Neto, Luke Meyers, Anneketh Vij, Tyler Odenthal, Charles Goddard, Mary MacCarthy, et al. Domain adaptation of llama3-70b-instruct through continual pre-training and model merging: comprehensive evaluation. arXiv preprint arXiv:2406.14971, 2024. 142 Published in Transactions on Machine Learning Research (10/2025) [833] Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. im sorry to hear that: Finding new biases in language models with holistic descriptor dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 9180 9211, 2022. [834] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv, abs/2408.03314, 2024. [835] Jake C. Snell, Thomas P. Zollo, Zhun Deng, Toniann Pitassi, and Richard Zemel. Quantile risk control: flexible framework for bounding the probability of high-loss predictions, 2022. [836] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. [837] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019. URL https://api.semanticscholar.org/CorpusID:201666234. [838] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4778347803. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_fil es/paper/2023/file/9521b6e7f33e039e7d92e23f5e37bbf4-Paper-Conference.pdf. [839] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 60486058, June 2023. [840] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llmplanner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 29983009, 2023. [841] Congzheng Song and Ananth Raghunathan. Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security, pp. 377390, 2020. [842] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. ArXiv preprint, abs/2306.17492, 2023. URL https://arxiv.org/abs/2306.17492. [843] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [844] Ionut-Teodor Sorodoc, Kristina Gulordava, and Gemma Boleda. Probing for Referential Information in Language Models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 41774189, Online, July 2020. Association for Computational Linguistics. doi: 10.1 8653/v1/2020.acl-main.384. URL https://aclanthology.org/2020.acl-main.384. [845] Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md Rafiqul Islam Rabin, Amin Alipour, Susmit Jha, Prem Devanbu, and Toufique Ahmed. Calibration and correctness of language models for code, 2024. [846] Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, Graham Neubig, and Aditi Raghunathan. Overtrained language models are harder to fine-tune. arXiv preprint arXiv:2503.19206, 2025. [847] Tejas Srinivasan, Ting-Yun Chang, Leticia Pinto Alva, Georgios Chochlakis, Mohammad Rostami, and Jesse Thomason. Climb: continual learning benchmark for vision-and-language tasks. Advances in Neural Information Processing Systems, 35:2944029453, 2022. 143 Published in Transactions on Machine Learning Research (10/2025) [848] Joe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations for robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pp. 1134911357, 2022. [849] Elias Stengel-Eskin and Benjamin Van Durme. Calibrated interpretation: Confidence estimation in semantic parsing. Transactions of the Association for Computational Linguistics, 11:12131231, 2023. [850] Elias Stengel-Eskin and Benjamin Van Durme. Did you mean...? confidence-based trade-offs in semantic parsing. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [851] Elias Stengel-Eskin, Jimena Guallar-Blasco, Yi Zhou, and Benjamin Van Durme. Why did the chicken cross the road? rephrasing and analyzing ambiguous questions in vqa, 2023. [852] Elias Stengel-Eskin, Kyle Rawlins, and Benjamin Van Durme. Zero and few-shot semantic parsing with ambiguous inputs. In The Twelfth International Conference on Learning Representations, 2023. [853] Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. Lacie: Listener-aware finetuning for confidence calibration in large language models. arXiv preprint arXiv:2405.21028, 2024. [854] Lukas Struppek, Dominik Hintersdorf, and Kristian Kersting. Rickrolling the artist: Injecting backdoors into text encoders for text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 45844596, 2023. [855] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Selective annotation makes language models better few-shot learners, 2022. [856] Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language models without logit-access. arXiv preprint arXiv:2403.01216, 2024. [857] Jinyan Su, Terry Zhuo, Di Wang, and Preslav Nakov. DetectLLM: Leveraging log rank information for zero-shot detection of machine-generated text. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1239512412, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findi ngs-emnlp.827. URL https://aclanthology.org/2023.findings-emnlp.827. [858] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all, 2023. [859] Carole Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and Jorge Cardoso. Generalised dice overlap as deep learning loss function for highly unbalanced segmentations. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 14, Proceedings 3, pp. 240248. Springer, 2017. [860] Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, and Bo Yuan. Disdet: Exploring detectability of backdoor attack on diffusion models. arXiv preprint arXiv:2402.02739, 2024. [861] Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Wieting, Nanyun Peng, and Xuezhe Ma. Evaluating large language models on controlled generation tasks. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 31553168, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.190. URL https: //aclanthology.org/2023.emnlp-main.190. [862] Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, and Bin Luo. Backdooring neural code search. arXiv preprint arXiv:2305.17506, 2023. 144 Published in Transactions on Machine Learning Research (10/2025) [863] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. [864] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 1308813110, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.775. [865] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. International Conference on Machine Learning (ICML), 2017. [866] Harini Suresh and John V. Guttag. framework for understanding sources of harm throughout the machine learning life cycle. Equity and Access in Algorithms, Mechanisms, and Optimization, 2019. URL https://api.semanticscholar.org/CorpusID:235436386. [867] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 28182826, 2016. [868] Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman. Task ambiguity in humans and language models, 2022. [869] Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. Demon in the variant: Statistical analysis In 30th USENIX Security Symposium of {DNNs} for robust backdoor contamination detection. (USENIX Security 21), pp. 15411558, 2021. [870] Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, and Wangchunshu Zhou. Agent kb: Leveraging cross-domain experience for agentic problem solving, 2025. URL https://arxiv.org/abs/2507.06229. [871] Lue Tao, Lei Feng, Jinfeng Yi, Sheng-Jun Huang, and Songcan Chen. Better safe than sorry: Preventing delusive adversaries with adversarial training. Advances in Neural Information Processing Systems, 34:1620916225, 2021. [872] Yitian Tao, Liyan Ma, Jing Yu, and Han Zhang. Memory-based cross-modal semantic alignment network for radiology report generation. IEEE Journal of Biomedical and Health Informatics, 2024. [873] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https: //github.com/tatsu-lab/stanford_alpaca, 2023. [874] Pittawat Taveekitworachai, Febri Abdullah, Mustafa Can Gursesli, Mury Dewantoro, Siyuan Chen, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmas. Breaking bad: Unraveling influences and risks of user inputs to chatgpt for game story generation. In International Conference on Interactive Digital Storytelling, pp. 285296. Springer, 2023. [875] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In ACL (1), pp. 45934601. Association for Computational Linguistics, 2019. [876] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. ArXiv, abs/1905.06316, 2019. URL https://api.semanticscholar.org/CorpusID:108300988. 145 Published in Transactions on Machine Learning Research (10/2025) [877] Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Language models get gender makeover: Mitigating gender bias with few-shot data interventions. arXiv preprint arXiv:2306.04597, 2023. [878] David Thiel. Identifying and eliminating csam in generative ml training data and models. Stanford Internet Observatory, 2023. [879] Edward Tian. GPTzero: An ai text detector., 2023. URL https://gptzero.me/. [880] Fengrui Tian, Tianjiao Ding, Jinqi Luo, Hancheng Min, and René Vidal. Voyaging into perpetual dynamic scenes from single view. In International Conference on Computer Vision (ICCV), 2025. [881] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. Fine-tuning language models for factuality, 2023. [882] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback, 2023. [883] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [884] Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur Parikh. Sticking to the facts: Confident decoding for faithful data-to-text generation. arXiv preprint arXiv:1910.08684, 2019. [885] Rubèn Tito, Khanh Nguyen, Marlon Tobaben, Raouf Kerkouche, Mohamed Ali Souibgui, Kangsoo Jung, Lei Kang, Ernest Valveny, Antti Honkela, Mario Fritz, et al. Privacy-aware document visual question answering. arXiv preprint arXiv:2312.10108, 2023. [886] Marcus Tomalin, Bill Byrne, Shauna Concannon, Danielle Saunders, and Stefanie Ullmann. The practical ethics of bias reduction in machine translation: Why domain adaptation is better than data debiasing. Ethics and Information Technology, pp. 115, 2021. [887] Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, and Mark Ibrahim. Uncertainty-based abstention in llms improves safety and reduces hallucinations, 2024. [888] Haibo Tong, Zhaoyang Wang, Zhaorun Chen, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, et al. Mj-video: Fine-grained benchmarking and rewarding video preferences in video generation. arXiv preprint arXiv:2502.01719, 2025. [889] Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems with language models. arXiv preprint arXiv:2306.12105, 2023. [890] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. [891] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [892] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024. [893] Mercan Topkara, Umut Topkara, and Mikhail J. Atallah. Words are not enough: sentence level natural language watermarking. In Workshop on Medical Cyber-Physical Systems, 2006. URL https: //api.semanticscholar.org/CorpusID:5854860. [894] Umut Topkara, Mercan Topkara, and Mikhail J. Atallah. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In Workshop on Multimedia & Security, 2006. URL https://api.semanticscholar.org/CorpusID:3061822. 146 Published in Transactions on Machine Learning Research (10/2025) [895] Lucas Torroba Hennigen, Adina Williams, and Ryan Cotterell. Intrinsic Probing through Dimension Selection. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 197216, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.15. URL https://aclanthology.org/2020.emnlp-main.15. [896] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [897] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models, 2023. [898] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017. [899] Marcos V. Treviso, Alexis Ross, Nuno Miguel Guerreiro, and André Martins. CREST: joint In ACL (1), pp. 1510915126. framework for rationalization and counterfactual text generation. Association for Computational Linguistics, 2023. [900] Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, S. Barannikov, Irina Piontkovskaya, Sergey I. Nikolenko, and Evgeny Burnaev. Intrinsic dimension estimation for robust detection of ai-generated texts. ArXiv, abs/2306.04723, 2023. URL https://api.semanticschola r.org/CorpusID:259108779. [901] Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization, 2023. [902] Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan Vazquez, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv e-prints, pp. arXiv2308, 2023. [903] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36:7495274965, 2023. [904] UBS. Ubs: Chatgpt may be the fastest growing app of all time. https://aibusiness.com/nlp/ubschatgpt-is-the-fastest-growing-app-of-all-time, 2024. [905] Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. Authorship attribution for neural text In Conference on Empirical Methods in Natural Language Processing, 2020. URL generation. https://api.semanticscholar.org/CorpusID:221835708. [906] Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Ting-Hao Kenneth Huang, and Dongwon Lee. Does human collaboration enhance the accuracy of identifying llm-generated deepfake texts? Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 2023. URL https: //api.semanticscholar.org/CorpusID:257913864. [907] Dennis Ulmer. On uncertainty in natural language processing. arXiv preprint arXiv:2410.03446, 2024. [908] Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. Calibrating large language models using their generations only. arXiv preprint arXiv:2403.05973, 2024. [909] Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun, Xibin Gao, and Yi Zhang. Bootstrapping llm-based task-oriented dialogue agents via self-talk. arXiv preprint arXiv:2401.05033, 2024. [910] Dennis Ulmer, Chrysoula Zerva, and André FT Martins. Non-exchangeable conformal language generation with nearest neighbors. arXiv preprint arXiv:2402.00707, 2024. 147 Published in Transactions on Machine Learning Research (10/2025) [911] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. Med-halt: Medical domain hallucination test for large language models. arXiv preprint arXiv:2307.15343, 2023. [912] Liam van der Poel, Ryan Cotterell, and Clara Meister. Mutual information alleviates hallucinations in abstractive summarization. arXiv preprint arXiv:2210.13210, 2022. [913] Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, and Anh Tran. Anti-dreambooth: Protecting users from personalized text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 21162127, 2023. [914] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987, 2023. [915] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [916] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575, 2015. [917] Sara Veldhoen, Dieuwke Hupkes, and Willem Zuidema. Diagnostic classifiers revealing how neural networks process hierarchical structure. In NIPS 2016 Workshop on Cognitive Computation, 2016. [918] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-HaoKenneth Huang, and Shomir Wilson. Nationality bias in text generation. arXiv preprint arXiv:2302.02463, 2023. [919] Aayush Atul Verma, Amir Saeidi, Shamanthak Hegde, Ajay Therala, Fenil Denish Bardoliya, Nagaraju Machavarapu, Shri Ajay Kumar Ravindhiran, Srija Malyala, Agneet Chatterjee, Yezhou Yang, et al. Evaluating multimodal large language models across distribution shifts and augmentations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5314 5324, 2024. [920] Vivek Kumar Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting text ghostwritten by large language models. ArXiv, abs/2305.15047, 2023. URL https://api.semantic scholar.org/CorpusID:258865787. [921] Jesse Vig. Bertviz: tool for visualizing multihead self-attention in the bert model. In ICLR workshop: Debugging machine learning models, volume 23, 2019. [922] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in transformer language model. In BlackboxNLP@ACL, pp. 6376. Association for Computational Linguistics, 2019. [923] Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. Context-aware neural machine translation learns anaphora resolution. In ACL (1), pp. 12641274. Association for Computational Linguistics, 2018. [924] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head arXiv preprint self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv:1905.09418, 2019. [925] Elena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in neural machine translation. In ACL/IJCNLP (1), pp. 11261140. Association for Computational Linguistics, 2021. [926] Ellen Voorhees. Natural language processing and information retrieval. In International summer school on information extraction, pp. 3248. Springer, 1999. Published in Transactions on Machine Learning Research (10/2025) [927] Vladimir Vovk, Akimichi Takemura, and Glenn Shafer. Defensive forecasting for linear protocols. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, 2005. [928] Oskar Van Der Wal, Pietro Lesci, Max Muller-Eberstein, Naomi Saphra, Hailey Schoelkopf, Willem Zuidema, and Stella Biderman. Polypythias: Stability and outliers across fifty language model pretraining runs, 2025. [929] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning. In International Conference on Machine Learning, 2023. [930] Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. arXiv preprint arXiv:2401.10491, 2024. [931] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [932] Alex Wang and Kyunghyun Cho. Bert has mouth, and it must speak: Bert as markov random field language model. arXiv preprint arXiv:1902.04094, 2019. [933] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022. [934] Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. Rethinking calibration of deep neural networks: Do not be afraid of overconfidence. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1180911820. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_fil es/paper/2021/file/61f3a6dbc9120ea78ef75544826c814e-Paper.pdf. [935] Hao Wang and Dit-Yan Yeung. Towards bayesian deep learning: framework and some existing methods. TDKE, 28(12):33953408, 2016. [936] Hao Wang and Dit-Yan Yeung. survey on bayesian deep learning. CSUR, 53(5):137, 2020. [937] Hao Wang, Shangwei Guo, Jialing He, Kangjie Chen, Shudong Zhang, Tianwei Zhang, and Tao Xiang. Eviledit: Backdooring text-to-image diffusion models in one second. In ACM Multimedia 2024, 2024. URL https://openreview.net/forum?id=ibEaSS6bQn. [938] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571, 2024. [939] Hengyi Wang, Shiwei Tan, Zhiqing Hong, Desheng Zhang, and Hao Wang. Variational language concepts for interpreting foundation language models. In EMNLP, 2024. [940] Hengyi Wang, Shiwei Tan, and Hao Wang. Probabilistic conceptual explainers: Towards trustworthy conceptual explanations for vision foundation models. In ICML, 2024. [941] Hongmin Wang. Revisiting challenges in data-to-text generation with fact grounding. In Proceedings of the 12th International Conference on Natural Language Generation, pp. 311322, 2019. Published in Transactions on Machine Learning Research (10/2025) [942] Huandong Wang, Wenjie Fu, Yingzhou Tang, Zhilong Chen, Yuxi Huang, Jinghua Piao, Chen Gao, Fengli Xu, Tao Jiang, and Yong Li. survey on responsible llms: Inherent risk, malicious use, and mitigation strategy. arXiv preprint arXiv:2501.09431, 2025. [943] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. [944] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. [945] Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, and Xu Sun. Towards codable watermarking for injecting multi-bit information to llm. 2023. URL https://api. semanticscholar.org/CorpusID:260334887. [946] Liang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv:2307.07164, 2023. [947] Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, and Georg Gottlob. Selective forgetting: Advancing machine unlearning techniques and evaluation in language models. arXiv preprint arXiv:2402.05813, 2024. [948] Noah Wang, Z.y. Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, and Junran Peng. RoleLLM: Benchmarking, eliciting, and enhancing role-playing In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), abilities of large language models. Findings of the Association for Computational Linguistics ACL 2024, pp. 1474314777, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.findings-acl.878. [949] Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. Pinto: Faithful language reasoning using prompt-generated rationales. arXiv preprint arXiv:2211.01562, 2022. [950] Qingquan Wang. Invisible watermark, 2020. URL https://github.com/ShieldMnt/invisible-w atermark. [951] Ruida Wang, Wangchunshu Zhou, and Mrinmaya Sachan. Lets synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1181711831, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.791. URL https://aclanthology.org/2023.fi ndings-emnlp.791. [952] Sibo Wang, Jie Zhang, Zheng Yuan, and Shiguang Shan. Pre-trained model guided fine-tuning for zero-shot adversarial robustness. arXiv preprint arXiv:2401.04350, 2024. [953] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large language models: survey. ACM Computing Surveys, 2023. [954] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, 2024. URL https://arxiv.org/abs/2401.17268. 150 Published in Transactions on Machine Learning Research (10/2025) [955] Xiyao Wang, Wichayaporn Wongkamjan, Ruonan Jia, and Furong Huang. Live in the moment: Learning dynamics model adapted to evolving policy. In International Conference on Machine Learning, pp. 3647036493. PMLR, 2023. [956] Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visual-language modality alignment in large vision language models via self-improvement. arXiv preprint arXiv:2405.15973, 2024. [957] Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, and Furong Huang. Coplanner: Plan to roll out conservatively but to explore optimistically for modelbased rl. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=MSe8YFbhUE. [958] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. In Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [959] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In International Conference on Learning Representations (ICLR), 2023. [960] Yibin Wang, Haizhou Shi, Ligong Han, Dimitris Metaxas, and Hao Wang. Blob: Bayesian low-rank adaptation by backpropagation for large language models. In NeurIPS, 2024. [961] Yifan Wang and Vera Demberg. parameter-efficient multi-objective approach to mitigate stereoIn Agnieszka Faleńska, Christine Basta, Marta Costa-jussà, typical bias in language models. Seraphina Goldfarb-Tarrant, and Debora Nozza (eds.), Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pp. 119, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.gebnlp1.1. URL https: //aclanthology.org/2024.gebnlp-1.1/. [962] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022. [963] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self-generated instructions. corr, abs/2212.10560, 2022. doi: 10.48550. ArXiv preprint, abs/2212.10560, 2022. URL https://arxiv. org/abs/2212.10560. [964] Yu Wang, Xiusi Chen, Jingbo Shang, and Julian McAuley. Memoryllm: Towards self-updatable large language models. arXiv preprint arXiv:2402.04624, 2024. [965] Yu Wang, Zeyuan Zhang, Julian McAuley, and Zexue He. Lvchat: Facilitating long video comprehension. arXiv preprint arXiv:2402.12079, 2024. [966] Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, and Jindong Gu. Stop reasoning! when multimodal llms with chain-of-thought reasoning meets adversarial images. Conference On Language Modeling (COLM), 2024. [967] Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. Democratizing reasoning ability: Tailored learning from large language model. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 19481966, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.emnlp-main.120. 151 Published in Transactions on Machine Learning Research (10/2025) [968] Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao. Cream: Consistency regularized self-rewarding language models. arXiv preprint arXiv:2410.12735, 2024. [969] Zhiyuan Wang, Jinhao Duan, Lu Cheng, Yue Zhang, Qingni Wang, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, and Kaidi Xu. Conu: Conformal uncertainty in large language models with correctness coverage guarantees. arXiv preprint arXiv:2407.00499, 2024. [970] Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Huaxiu Yao, Yue Zhang, Ren Wang, Kaidi Xu, and Xiaoshuang Shi. Word-sequence entropy: Towards uncertainty estimation in free-form medical question answering applications and beyond. arXiv preprint arXiv:2402.14259, 2024. [971] Zhongqi Wang, Jie Zhang, Shiguang Shan, and Xilin Chen. T2ishield: Defending against backdoors on text-to-image diffusion models. arXiv preprint arXiv:2407.04215, 2024. [972] Zijian Wang and Chang Xu. Thoughtprobe: Classifier-guided llm thought space exploration via In Proceedings of the 2025 Conference on Empirical Methods in Natural probing representations. Language Processing, pp. 60296050, 2025. [973] Futa Waseda and Antonio Tejero-de Pablos. Leveraging many-to-many relationships for defending against visual-language adversarial attacks. arXiv preprint arXiv:2405.18770, 2024. [974] WashingtonPost. He made childrens book using AI. Then came the rage. https://www.washington post.com/technology/2023/01/19/ai-childrens-book-controversy-chatgpt-midjourney/, 2022. [975] Michelle Wastl, Jannis Vamvas, and Rico Sennrich. UZH at SemEval-2025 task 3: Token-level selfconsistency for hallucination detection. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), 2025. [976] Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and Slav Petrov. Measuring and reducing gendered correlations in pre-trained models. arXiv preprint arXiv:2010.06032, 2020. [977] Ryan Webster. reproducible extraction of training images from diffusion models. arXiv preprint arXiv:2305.08694, 2023. [978] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023. [979] Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, and Wenzhi Chen. Lmsanitator: Defending prompt-tuning against task-agnostic backdoors. arXiv preprint arXiv:2308.13904, 2023. [980] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. arXiv preprint arXiv:2311.17136, 2023. [981] Hongliang Wei, Xingtao Wang, Xianqi Zhang, Xiaopeng Fan, and Debin Zhao. Toward stable, fair, and comprehensive evaluation of object hallucination in large vision-language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [982] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural network overconfidence with logit normalization, 2022. [983] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text classification tasks. arXiv preprint arXiv:1901.11196, 2019. 152 Published in Transactions on Machine Learning Research (10/2025) [984] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [985] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [986] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. [987] Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: 200-instruction paradigm for fine-tuning minigpt-4, 2023. [988] Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Sim-cot: Supervised implicit chain-of-thought. arXiv preprint arXiv:2509.20317, 2025. [989] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed by language models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pp. 214229, 2022. [990] Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, and Jack Hessel. Promptriever: Instruction-trained retrievers can be prompted like language models. arXiv preprint arXiv:2409.11136, 2024. [991] Bingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu, Yulia Tsvetkov, Bill Howe, and Lucy Lu arXiv preprint Wang. Know your limits: survey of abstention in large language models. arXiv:2407.18418, 2024. [992] Rui Wen, Zheng Li, Michael Backes, and Yang Zhang. Membership inference attacks against incontext learning. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, pp. 34813495, 2024. [993] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5100851025. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/a00548031e4647b13042c97 c922fadf1-Paper-Conference.pdf. [994] Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030, 2023. [995] Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, explaining, and mitigating memorization in diffusion models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=84n3UwkH7b. [996] Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, and Nicholas Carlini. Privacy backdoors: Enhancing membership inference through poisoning pre-trained models. In NeurIPS, 2024. [997] Zhaotian Weng, Zijun Gao, Jerone Andrews, and Jieyu Zhao. Images speak louder than words: Understanding and mitigating bias in vision-language model from causal mediation perspective. arXiv preprint arXiv:2407.02814, 2024. [998] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation, 2019. Published in Transactions on Machine Learning Research (10/2025) [999] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Krishnamurthy arXiv preprint Dvijotham, and Taylan Cemgil. fine-grained analysis on distribution shift. arXiv:2110.11328, 2021. [1000] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [1001] Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. Llmdet: third party large language models generated text detection tool. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:258865367. [1002] Ruijia Wu, Yuhang Wang, Huafeng Shi, Zhipeng Yu, Yichao Wu, and Ding Liang. Towards promptrobust face privacy protection via adversarial decoupling augmentation framework. arXiv preprint arXiv:2305.03980, 2023. [1003] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [1004] Skyler Wu, Eric Meng Shen, Charumathi Badrinath, Jiaqi Ma, and Himabindu Lakkaraju. Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions. ArXiv, abs/2307.13339, 2023. URL https://api.semanticscholar.org/CorpusID:260155139. [1005] Tongshuang Wu, Marco Túlio Ribeiro, Jeffrey Heer, and Daniel S. Weld. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In ACL/IJCNLP (1), pp. 6707 6723. Association for Computational Linguistics, 2021. [1006] Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. Conceptmix: compositional image generation benchmark with controllable difficulty. arXiv preprint arXiv:2408.14339, 2024. [1007] Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu. From language modeling to instruction following: Understanding the behavior shift in llms after instruction tuning. arXiv preprint arXiv:2310.00492, 2023. [1008] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models. arXiv, 2024. [1009] Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. Dipmark: stealthy, efficient and resilient watermark for large language models. ArXiv, abs/2310.07710, 2023. URL https: //api.semanticscholar.org/CorpusID:263834753. [1010] Yixin Wu, Rui Wen, Michael Backes, Ning Yu, and Yang Zhang. Model stealing attacks against vision-language models. 2022. [1011] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [1012] Yongliang Wu, Shiji Zhou, Mingzhuo Yang, Lianzhe Wang, Heng Chang, Wenbo Zhu, Xinting Hu, Xiao Zhou, and Xu Yang. Unlearning concepts in diffusion model via concept domain correction and concept preserving gradient. In AAAI, 2025. [1013] Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. Ragtruth: hallucination corpus for developing trustworthy retrieval-augmented language models. arXiv preprint arXiv:2401.00396, 2023. [1014] Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, and Lichao Sun. Jailbreaking gpt-4v via self-adversarial attacks with system prompts. arXiv preprint arXiv:2311.09127, 2023. 154 Published in Transactions on Machine Learning Research (10/2025) [1015] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. ArXiv preprint, abs/2306.01693, 2023. URL https://arxiv.org/abs/23 06.01693. [1016] Zhengxuan Wu and Desmond C. Ong. On explaining your explanations of BERT: an empirical study with sequence classification. CoRR, abs/2101.00196, 2021. [1017] Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In ACL, pp. 41664176. Association for Computational Linguistics, 2020. [1018] xAI. Grok4. https://x.ai/news/grok-4, 2025. [1019] xAI. Grok 3 beta the age of reasoning agents, February 2025. URL https://x.ai/news/grok-3. Accessed: April 6, 2025. [1020] Peng Xia, Di Xu, Lie Ju, Ming Hu, Jun Chen, and Zongyuan Ge. Lmpt: Prompt tuning with classspecific embedding loss for long-tailed multi-label visual recognition. arXiv preprint arXiv:2305.04536, 2023. [1021] Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: comprehensive benchmark of trustworthiness in medical vision language models. arXiv preprint arXiv:2406.06007, 2024. [1022] Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, et al. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models. arXiv preprint arXiv:2410.10139, 2024. [1023] Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, and Zongyuan Ge. Generalizing to unseen domains in diabetic retinopathy with disentangled representations. arXiv preprint arXiv:2406.06384, 2024. [1024] Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language models. arXiv preprint arXiv:2410.13085, 2024. [1025] Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [1026] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=c9 3SBwz1Ma. [1027] Zhen Xiang, Yi Zeng, Mintong Kang, Chejian Xu, Jiawei Zhang, Zhuowen Yuan, Zhaorun Chen, Chulin Xie, Fengqing Jiang, Minzhou Pan, et al. Clas 2024: The competition for llm and agent safety. In NeurIPS 2024 Competition Track, 2024. [1028] Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, et al. Large language models can be good privacy protection learners. arXiv preprint arXiv:2310.02469, 2023. [1029] Yijun Xiao and William Yang Wang. On hallucination and predictive uncertainty in conditional language generation. arXiv preprint arXiv:2103.15025, 2021. [1030] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. survey on non-autoregressive generation for neural machine translation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):1140711427, 2023. 155 Published in Transactions on Machine Learning Research (10/2025) [1031] Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and LouisPhilippe Morency. Uncertainty quantification with pre-trained language models: large-scale empirical analysis, 2022. [1032] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [1033] Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: survey. arXiv preprint arXiv:2402.15116, 2024. [1034] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference, 2021. [1035] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:34201 34227, 2023. [1036] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. Nature Machine Intelligence, pp. 111, 2023. [1037] xin jin, Yichuan Zhong, and Yapeng Tian. TP-blend: Textual-prompt attention pairing for precise object-style blending in diffusion models. Transactions on Machine Learning Research, 2025. [1038] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation for medicine. arXiv preprint arXiv:2402.13178, 2024. [1039] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023. [1040] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under klconstraint. In Forty-first International Conference on Machine Learning, 2024. [1041] Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang. To be robust or to be fair: Towards fairness in adversarial training. In International conference on machine learning, pp. 1149211501. PMLR, 2021. [1042] Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, and Kai Yu. Rejection improves reliability: Training llms to refuse unknown questions using rl from knowledge feedback. arXiv preprint arXiv:2403.18349, 2024. [1043] Huimin Xu, Xin Mao, Feng-Lin Li, Xiaobao Wu, Wang Chen, Wei Zhang, and Anh Tuan Luu. Fullstep-dpo: Self-supervised preference optimization with step-wise rewards for mathematical reasoning. arXiv preprint arXiv:2502.14356, 2025. [1044] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report, 2025. URL https://arxiv.org/abs/2509.17765. [1045] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. survey on green deep learning, 2021. URL https://arxiv.org/abs/2111.05193. 156 Published in Transactions on Machine Learning Research (10/2025) [1046] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023. [1047] Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May Wang, Joyce Ho, Chao Zhang, and Carl Yang. Bmretriever: Tuning large language models as better biomedical text retrievers. arXiv preprint arXiv:2404.18443, 2024. [1048] Yuancheng Xu, Chenghao Deng, Yanchao Sun, Ruijie Zheng, Xiyao Wang, Jieyu Zhao, and Furong Huang. Equal long-term benefit rate: Adapting static fairness notions to sequential decision making. arXiv preprint arXiv:2309.03426, 2023. [1049] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. [1050] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. arXiv preprint arXiv:2402.08983, 2024. [1051] Ziqing Xu, Hancheng Min, Lachlan Ewen MacDonald, Jinqi Luo, Salma Tarmoun, Enrique Mallada, and Rene Vidal. Understanding the learning dynamics of lora: gradient flow perspective on low-rank adaptation in matrix factorization. In AISTATS, 2025. [1052] Yasin Abbasi Yadkori, Ilja Kuzborskij, András György, and Csaba Szepesvári. To believe or not to believe your llm, 2024. [1053] Yasin Abbasi Yadkori, Ilja Kuzborskij, David Stutz, András György, Adam Fisch, Arnaud Doucet, Iuliya Beloshapka, Wei-Hung Weng, Yao-Yuan Yang, Csaba Szepesvári, Ali Taylan Cemgil, and Nenad Tomasev. Mitigating llm hallucinations via conformal abstention, 2024. [1054] An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Wang, Jingbo Shang, and Julian J. McAuley. Learning concise and descriptive attributes for visual recognition. CoRR, abs/2308.03685, 2023. [1055] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. ArXiv preprint, abs/2309.03409, 2023. URL https: //arxiv.org/abs/2309.03409. [1056] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1420314214, 2025. [1057] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: survey. arXiv preprint arXiv:2110.11334, 2021. [1058] Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, and Di Wang. Moral: Moe augmented lora for llms lifelong learning. arXiv preprint arXiv:2402.11260, 2024. [1059] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [1060] Wenhan Yang, Jingdong Gao, and Baharan Mirzasoleiman. Robust contrastive language-image pretraining against data poisoning and backdoor attacks. Advances in Neural Information Processing Systems, 36, 2024. [1061] Xi Yang, Jie Zhang, Kejiang Chen, Weiming Zhang, Zehua Ma, Feng Wang, and Nenghai Yu. Tracing text provenance via context-aware lexical substitution. ArXiv, abs/2112.07873, 2021. URL https: //api.semanticscholar.org/CorpusID:245144237. 157 Published in Transactions on Machine Learning Research (10/2025) [1062] Xi Yang, Kejiang Chen, Weiming Zhang, Chang rui Liu, Yuang Qi, Jie Zhang, Han Fang, and Neng H. Yu. Watermarking text generated by black-box language models. ArXiv, abs/2305.08883, 2023. URL https://api.semanticscholar.org/CorpusID:258714683. [1063] Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. ArXiv, abs/2305.17359, 2023. URL https://api.semanticscholar.org/CorpusID:258960101. [1064] Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Ruth Petzold, William Yang Wang, and Wei Cheng. Zero-shot detection of machine-generated codes. ArXiv, abs/2310.05103, 2023. URL https: //api.semanticscholar.org/CorpusID:263831381. [1065] Xinyu Yang, Huaxiu Yao, Allan Zhou, and Chelsea Finn. Multi-domain long-tailed learning by augmenting disentangled representations. Transactions on Machine Learning Research, 2023. [1066] Xinyu Yang, Zichen Wen, Wenjie Qu, Zhaorun Chen, Zhiying Xiang, Beidi Chen, and Huaxiu Yao. Memorization and privacy risks in domain-specific large language models. In ICLR 2024 Workshop on Reliable and Responsible Foundation Models, 2024. [1067] Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Tsung-Yi Ho, Nan Xu, and Qiang Xu. Mma-diffusion: Multimodal attack on diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 77377746, 2024. [1068] Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, and Qiang Xu. Guardt2i: Defending textto-image models from adversarial prompts. arXiv preprint arXiv:2403.01446, 2024. [1069] Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. Embodied multi-modal agent trained by an llm from parallel textworld. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2627526285, 2024. [1070] Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. Sneakyprompt: Jailbreaking textto-image generative models. In Proceedings of the IEEE Symposium on Security and Privacy, 2024. [1071] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. Language in bottle: Language model guided concept bottlenecks for interpretable image classification. arXiv preprint arXiv:2211.11158, 2022. [1072] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, DongGyun Han, and David Lo. What do code models memorize? an empirical study on large language models of code. arXiv preprint arXiv:2308.09932, 2023. [1073] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [1074] Zonghan Yang, Xiaoyuan Yi, Peng Li, Yang Liu, and Xing Xie. Unified detoxifying and debiasing in language generation via inference-time adaptive optimization. arXiv preprint arXiv:2210.04492, 2022. [1075] Huaxiu Yao, Yiping Wang, Linjun Zhang, James Zou, and Chelsea Finn. C-mixup: Improving generalization in regression. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [1076] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In International Conference on Machine Learning (ICML), pp. 2540725437. PMLR, 2022. [1077] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023. 158 Published in Transactions on Machine Learning Research (10/2025) [1078] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [1079] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, may 2023. ArXiv preprint, abs/2305.10601, 2023. URL https://arxiv.org/abs/2305.10601. [1080] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. arXiv preprint arXiv:2310.10683, 2023. [1081] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172, 2023. [1082] Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, and Huajun Chen. Knowledge circuits in pretrained transformers. In NeurIPS, 2024. [1083] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022. [1084] Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, and Tong Zhang. theoretical analysis arXiv preprint of nash learning from human feedback under general kl-regularized preference. arXiv:2402.07314, 2024. [1085] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning, 2023. [1086] Xiaoyu Ye, Hao Huang, Jiaqi An, and Yongtao Wang. Duaw: Data-free universal adversarial watermark against stable diffusion customization. arXiv preprint arXiv:2308.09889, 2023. [1087] Chih-Kuan Yeh, Joon Sik Kim, Ian En-Hsu Yen, and Pradeep Ravikumar. Representer point selection for explaining deep neural networks. In NeurIPS, pp. 93119321, 2018. [1088] Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Jason Wu. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. arXiv preprint arXiv:2306.01150, 2023. [1089] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. ArXiv preprint, abs/2310.16045, 2023. URL https://arxiv.org/abs/2310.16045. [1090] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. [1091] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language models know what they dont know?, 2023. [1092] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. Advances in Neural Information Processing Systems, 36, 2023. [1093] Kiyoon Yoo, Wonhyuk Ahn, Jiho Jang, and No Jun Kwak. Robust multi-bit natural language waIn Annual Meeting of the Association for Computational termarking through invariant features. Linguistics, 2023. URL https://api.semanticscholar.org/CorpusID:259129912. [1094] Kiyoon Yoo, Wonhyuk Ahn, and No Jun Kwak. Advancing beyond identification: Multi-bit watermark for language models. ArXiv, abs/2308.00221, 2023. URL https://api.semanticscholar.org/Corp usID:262903996. 159 Published in Transactions on Machine Learning Research (10/2025) [1095] Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, and Minjoon Seo. Reasoning models better express their confidence. arXiv preprint arXiv:2505.14489, 2025. [1096] Jaehong Yoon, Sung Ju Hwang, and Yue Cao. Continual learners are incremental model generalizers. In International Conference on Machine Learning, 2023. [1097] Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, and Mohit Bansal. Safree: Training-free and adaptive guard for safe text-to-image and video generation. arXiv preprint arXiv:2410.12761, 2024. [1098] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500, 2021. [1099] Jiahao Yu, Xingwei Lin, and Xinyu Xing. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253, 2023. [1100] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. [1101] Liu Yu, Yuzhou Mao, Jin Wu, and Fan Zhou. Mixup-based unified framework to overcome gender bias resurgence. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 17551759, 2023. [1102] Mengxia Yu, De Wang, Qi Shan, Colorado Reed, and Alvin Wan. The super weight in large language models, 2024. [1103] Shoubin Yu, Jaehong Yoon, and Mohit Bansal. Crema: Generalizable and efficient video-language reasoning via multimodal modular fusion. arXiv preprint arXiv:2402.05889, 2024. [1104] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1380713816, 2024. [1105] Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. survey of knowledge-enhanced text generation. ACM Computing Surveys, 54(11s):138, 2022. [1106] Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, and Neng H. Yu. Gpt paternity test: Gpt generated text detection with gpt genetic inheritance. ArXiv, abs/2305.12519, 2023. URL https://api.semanticscholar.org/CorpusID:258833423. [1107] Xiao Yu, Kejiang Chen, Qi Yang, Weiming Zhang, and Nenghai Yu. Text fluoroscopy: Detecting llm-generated text through intrinsic features. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1583815846, 2024. [1108] Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, and Chao Zhang. Cold-start data selection for few-shot language model fine-tuning: prompt-based uncertainty propagation approach, 2023. [1109] Zhuohao Yu, Xingru Jiang, Weizheng Gu, Chang Gao, Yidong Wang, Shikun Zhang, and Wei Ye. Saemark: Steering personalized multilingual llm watermarks with sparse autoencoders. [1110] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations, 2023. 160 Published in Transactions on Machine Learning Research (10/2025) [1111] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. [1112] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. ArXiv preprint, abs/2304.05302, 2023. URL https://arxiv.org/abs/2304.05302. [1113] Junpeng Yue, Xinru Xu, Börje Karlsson, and Zongqing Lu. Mllm as retriever: Interactively learning multimodal retrieval for embodied agents. arXiv preprint arXiv:2410.03450, 2024. [1114] Xiang Yue, Huseyin Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan, and Robert Sim. Synthetic text generation with differential privacy: simple and practical recipe. arXiv preprint arXiv:2210.14348, 2022. [1115] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):15381556, 2012. [1116] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2022. [1117] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix: Regularization strategy to train strong classifiers with localizable features. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 60226031. IEEE, October 2019. doi: 10.1109/iccv.2019.00612. URL http://dx.doi.org/10.1109/ICCV.2019.00612. [1118] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. ICML, 1, 05 2001. [1119] Omar Zaidan and Jason Eisner. Modeling annotators: generative approach to learning from annotator rationales. In EMNLP, pp. 3140. ACL, 2008. [1120] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. [1121] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. ArXiv, abs/1905.12616, 2019. URL https://api. semanticscholar.org/CorpusID:168169824. [1122] Yi Zeng, Si Chen, Won Park, Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. arXiv preprint arXiv:2110.03735, 2021. [1123] Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, and André F. T. Martins. Disentangling uncertainty in machine translation evaluation, 2022. [1124] Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, and Xiangjun Fan. Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption. ArXiv preprint, abs/2310.01779, 2023. URL https://arxiv.org/abs/2310.01779. [1125] Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, and Hang Su. Text-to-image arXiv preprint diffusion models can be easily backdoored through multimodal data poisoning. arXiv:2305.04175, 2023. [1126] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023. 161 Published in Transactions on Machine Learning Research (10/2025) [1127] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=nBjmMF2IZU. [1128] Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp. G3detector: General gpt-generated text detector. ArXiv, abs/2305.12680, 2023. URL https://api.semanticscholar. org/CorpusID:258832418. [1129] Andi Zhang, Tim Xiao, Weiyang Liu, Robert Bamler, and Damon Wischik. Your finetuned large language model is already powerful out-of-distribution detector. arXiv preprint arXiv:2404.08679, 2024. [1130] Caiqi Zhang, Fangyu Liu, Marco Basaldella, and Nigel Collier. Luq: Long-text uncertainty quantification for llms. arXiv preprint arXiv:2403.20279, 2024. [1131] Caiqi Zhang, Ruihan Yang, Zhisong Zhang, Xinting Huang, Sen Yang, Dong Yu, and Nigel Collier. Atomic calibration of llms in long-form generations. arXiv preprint arXiv:2410.13246, 2024. [1132] Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, and Xiwei Xu. Right to be forgotten in the era of large language models: Implications, challenges, and solutions. arXiv preprint arXiv:2307.03941, 2023. [1133] Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-tuning: Teaching large language models to refuse unknown questions. arXiv preprint arXiv:2311.09677, 2023. [1134] Haoran Zhang, Amy Lu, Mohamed Abdalla, Matthew McDermott, and Marzyeh Ghassemi. HurtIn proceedings of the ACM ful words: quantifying biases in clinical contextual word embeddings. Conference on Health, Inference, and Learning, pp. 110120, 2020. [1135] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pp. 74727482. PMLR, 2019. [1136] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [1137] Jie Zhang, Florian Kerschbaum, Tianwei Zhang, et al. Backdooring textual inversion for concept censorship. arXiv preprint arXiv:2308.10718, 2023. [1138] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pp. 993999, 2023. [1139] Lining Zhang, Mengchen Wang, Liben Chen, and Wenxin Zhang. Probing GPT-3s Linguistic Knowledge on Semantic Tasks. In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 297304, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/202 2.blackboxnlp-1.24. [1140] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [1141] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Published in Transactions on Machine Learning Research (10/2025) [1142] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554, 2023. [1143] Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, and Farinaz Koushanfar. Remark-llm: robust and efficient watermarking framework for generative large language models. ArXiv, 2023. [1144] Shenao Zhang. Conservative dual policy optimization for efficient model-based reinforcement learning. Advances in neural information processing systems, 35:2545025463, 2022. [1145] Shenao Zhang, Li Shen, and Lei Han. Learning meta representations for agents in multi-agent reinforcement learning. arXiv preprint arXiv:2108.12988, 2021. [1146] Shenao Zhang, Li Shen, Zhifeng Li, and Wei Liu. Structure-regularized attention for deformable object representation. arXiv preprint arXiv:2106.06672, 2021. [1147] Shenao Zhang, Wanxin Jin, and Zhaoran Wang. Adaptive barrier smoothing for first-order policy gradient with contact dynamics. In International Conference on Machine Learning, pp. 4121941243. PMLR, 2023. [1148] Shenao Zhang, Boyi Liu, Zhaoran Wang, and Tuo Zhao. Model-based reparameterization policy gradient methods: Theory and practical algorithms. Advances in Neural Information Processing Systems, 36, 2024. [1149] Shenao Zhang, Donghan Yu, Hiteshi Sharma, Ziyi Yang, Shuohang Wang, Hany Hassan, and Zhaoran Wang. Self-exploring language models: Active preference elicitation for online alignment. arXiv preprint arXiv:2405.19332, 2024. [1150] Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, and Zhaoran Wang. How can llm guide rl? value-based approach. arXiv preprint arXiv:2402.16181, 2024. [1151] Tianjun Zhang, Shishir Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131, 2024. [1152] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. [1153] Wenqiao Zhang, Haochen Shi, Siliang Tang, Jun Xiao, Qiang Yu, and Yueting Zhuang. Consensus In Proceedings of the AAAI graph representation learning for better grounded image captioning. Conference on Artificial Intelligence, volume 35, pp. 33943402, 2021. [1154] Xingxuan Zhang, Renzhe Xu, Han Yu, Yancheng Dong, Pengfei Tian, and Peng Cui. Flatness-aware minimization for domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 51895202, 2023. [1155] Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models. arXiv preprint arXiv:2402.06599, 2024. [1156] Yanzhe Zhang, Lu Jiang, Greg Turk, and Diyi Yang. Auditing gender presentation differences in text-to-image models. arXiv preprint arXiv:2302.03675, 2023. [1157] Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, et al. Benchmarking trustworthiness of multimodal large language models: comprehensive study. arXiv preprint arXiv:2406.07057, 2024. 163 Published in Transactions on Machine Learning Research (10/2025) [1158] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. arXiv preprint arXiv:2310.11868, 2023. [1159] Yiming Zhang and Daphne Ippolito. Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success. arXiv preprint arXiv:2307.06865, 2023. [1160] Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zhili Feng, Zenghui Ding, and Yining Sun. Rankclip: Ranking-consistent language-image pretraining. arXiv preprint arXiv:2404.09387, 2024. [1161] Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Adversarial feature matching for text generation. In International conference on machine learning, pp. 40064015. PMLR, 2017. [1162] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Sirens song in the ai ocean: survey on hallucination in large language models. Computational Linguistics, pp. 145, 2025. [1163] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [1164] Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, and Minlie Huang. Shieldlm: Empowering llms as aligned, customizable and explainable safety detectors. arXiv preprint arXiv:2402.16444, 2024. [1165] Zijian Zhang, Kaiyuan Zheng, Zhaorun Chen, Joel Jang, Yi Li, Chaoqi Wang, Mingyu Ding, Dieter Fox, and Huaxiu Yao. Grape: Generalizing robot policy via preference alignment. arXiv preprint arXiv:2411.19309, 2024. [1166] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: survey. CoRR, abs/2309.01029, 2023. [1167] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. Retrieving multimodal information for augmented generation: survey. arXiv preprint arXiv:2303.10868, 2023. [1168] Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models, 2021. [1169] Xingyi Zhao, Depeng Xu, and Shuhan Yuan. Defense against backdoor attack on pre-trained language In Forty-first International Conference on models via head pruning and attention normalization. Machine Learning, 2024. [1170] Xuandong Zhao, Prabhanjan Vijendra Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking for ai-generated text. ArXiv, abs/2306.17439, 2023. URL https://api.semanticscholar. org/CorpusID:259308864. [1171] Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter Liu. Calibrating sequence likelihood improves conditional language generation. ArXiv preprint, abs/2210.00045, 2022. URL https://arxiv.org/abs/2210.00045. [1172] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. ArXiv preprint, abs/2305.10425, 2023. URL https://arxiv.org/abs/2305.10425. [1173] Zhengyue Zhao, Jinhao Duan, Xing Hu, Kaidi Xu, Chenan Wang, Rui Zhang, Zidong Du, Qi Guo, and Yunji Chen. Unlearnable examples for diffusion models: Protect data from unauthorized exploitation. arXiv preprint arXiv:2306.01902, 2023. 164 Published in Transactions on Machine Learning Research (10/2025) [1174] Zhengyue Zhao, Jinhao Duan, Kaidi Xu, Chenan Wang, Rui Zhangp Zidong Dup Qi Guo, and Xing Hu. Can protective perturbation safeguard personal data from being exploited by stable diffusion? CVPR, 2024. [1175] Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, and Yunji Chen. Adversarial contrastive decoding: Boosting safety alignment of large language models via opposite prompt optimization. arXiv preprint arXiv:2406.16743, 2024. [1176] Boyang Zheng, Chumeng Liang, Xiaoyu Wu, and Yan Liu. Understanding and improving adversarial attacks on latent diffusion model. arXiv preprint arXiv:2310.04687, 2023. [1177] Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning? ArXiv, abs/2305.12740, 2023. URL https: //api.semanticscholar.org/CorpusID:258832407. [1178] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. [1179] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual Probing Is [MASK]: Learning vs. Learning to Recall, December 2021. URL http://arxiv.org/abs/2104.05240. arXiv:2104.05240 [cs]. [1180] Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen. Mquake: Assessing knowledge editing in language models via multi-hop questions. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticschola r.org/CorpusID:258865984. [1181] Andy Zhou, Jindong Wang, Yu-Xiong Wang, and Haohan Wang. Distilling out-of-distribution robustness from vision-language foundation models. In NeurIPS 2023, October 2023. [1182] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 13931404, 2021. [1183] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment. CoRR, abs/2305.11206, 2023. [1184] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [1185] Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. Batch calibration: Rethinking calibration for in-context learning and prompt engineering, 2024. [1186] Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, and Jun Wang. Memento: Fine-tuning llm agents without fine-tuning llms, 2025. URL https://arxiv.org/abs/2508.16153. [1187] Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, and Zhenhao Li. Trustrag: Enhancing robustness and trustworthiness in rag. arXiv preprint arXiv:2501.00879, 2025. [1188] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey area: How expressions of uncertainty and overconfidence affect language models, 2023. [1189] Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia. Pre-trained large language models use fourier features to compute addition, 2024. 165 Published in Transactions on Machine Learning Research (10/2025) [1190] Wangchunshu Zhou and Ke Xu. Learning to compare for better training and evaluation of open domain natural language generation models. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):97179724, Apr. 2020. doi: 10.1609/aa ai.v34i05.6521. URL https: //ojs.aaai.org/index.php/AAAI/article/view/6521. [1191] Wangchunshu Zhou, Tao Ge, Chang Mu, Ke Xu, Furu Wei, and Ming Zhou. Improving grammatical error correction with machine translation pairs. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 318328, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.30. URL https://aclanthology.org/2020.findings-emnlp.30. [1192] Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and Ming Zhou. Self-adversarial learning with comparative discrimination for text generation. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1l8L6EtDS. [1193] Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. Towards interpretable natural language understanding with explanations as latent variables. Advances in Neural Information Processing Systems, 33:68036814, 2020. [1194] Wangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xinsong Zhang. VLUE: multi-task multidimension benchmark for evaluating vision-language pre-training. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2739527411. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/v162/zhou22n .html. [1195] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents, 2023. URL https://arxiv.org/abs/2309.07870. [1196] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. Controlled text generation with natural language instructions. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 4260242613. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202 /zhou23g.html. [1197] Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents, 2024. URL https://arxiv.org/abs/2406.18532. [1198] Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025. [1199] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024. [1200] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024. [1201] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In International Conference on Learning Representations, 2024. [1202] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 166 Published in Transactions on Machine Learning Research (10/2025) [1203] Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, et al. Anyprefer: An automatic framework for preference data synthesis. In International Conference on Learning Representations (ICLR), 2025. [1204] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. ArXiv preprint, abs/2211.01910, 2022. URL https://arxiv.org/abs/2211.01910. [1205] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint arXiv:2310.03708, 2023. [1206] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, 2023. [1207] Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. In International Conference on Machine Learning, pp. 4303743067. PMLR, 2023. [1208] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. arXiv preprint arXiv:1909.11764, 2019. [1209] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: EnarXiv preprint hancing vision-language understanding with advanced large language models. arXiv:2304.10592, 2023. [1210] Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Kun Kuang, and Chao Wu. Model In International tailor: Mitigating catastrophic forgetting in multi-modal large language models. conference on machine learning, 2024. [1211] Liuwan Zhu, Rui Ning, Jiang Li, Chunsheng Xin, and Hongyi Wu. Seer: Backdoor detection for vision-language models through searching target text and image trigger jointly. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 77667774, 2024. [1212] Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu, and Conghui He. Grait: Gradient-driven refusal-aware instruction tuning for effective hallucination mitigation. arXiv preprint arXiv:2502.05911, 2025. [1213] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks on large language models. arXiv preprint arXiv:2310.15140, 2023. [1214] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In International conference on machine learning, pp. 1018. PMLR, 2014. [1215] Thomas Zollo, Todd Morrill, Zhun Deng, Jake Snell, Toniann Pitassi, and Richard Zemel. Prompt risk control: rigorous framework for responsible deployment of large language models. arXiv preprint arXiv:2311.13628, 2023. [1216] Thomas Zollo, Zhun Deng, Jake Snell, Toniann Pitassi, and Richard Zemel. Improving predictor reliability with selective recalibration. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=Aoj9H6jl6F. Expert Certification. [1217] Thomas P. Zollo, Todd Morrill, Zhun Deng, Jake C. Snell, Toniann Pitassi, and Richard Zemel. Prompt risk control: rigorous framework for responsible deployment of large language models, 2024. [1218] Thomas P. Zollo, Nikita Rajaneesh, Richard Zemel, Talia B. Gillis, and Emily Black. Towards effective discrimination testing for generative ai, 2024. URL https://arxiv.org/abs/2412.21052. 167 Published in Transactions on Machine Learning Research (10/2025) [1219] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: topdown approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023. [1220] Andy Zou, Zifan Wang, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. [1221] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models, 2024. [1222] Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, and Xiaolong Wang. Real deep research for ai, robotics and beyond. arXiv preprint arXiv:2510.20809, 2025. [1223] Úrsula Hébert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Calibration for the (computationally-identifiable) masses, 2018."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Columbia University",
        "Drexel University",
        "ETH Zurich",
        "Harvard University",
        "Imperial College London",
        "Johns Hopkins University",
        "Mila",
        "National University of Singapore",
        "New York University",
        "Princeton University",
        "Rutgers University",
        "Stanford University",
        "University College London",
        "University of California, San Diego",
        "University of California, Santa Cruz",
        "University of Chicago",
        "University of Maryland",
        "University of Montreal",
        "University of North Carolina at Chapel Hill",
        "University of Oxford",
        "University of Pennsylvania",
        "University of Washington"
    ]
}