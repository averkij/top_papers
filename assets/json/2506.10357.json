{
    "paper_title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts",
    "authors": [
        "Zaijing Li",
        "Yuquan Xie",
        "Rui Shao",
        "Gongwei Chen",
        "Weili Guan",
        "Dongmei Jiang",
        "Liqiang Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/"
        },
        {
            "title": "Start",
            "content": ": Towards Generalist Multimodal Optimus-3 Minecraft Agents with Scalable Task Experts Zaijing Li1 2, Yuquan Xie1, Rui Shao1, Gongwei Chen1 Weili Guan1, Dongmei Jiang2, Liqiang Nie1 1Harbin Institute of Technology, Shenzhen 2Peng Cheng Laboratory 5 2 0 2 2 ] . [ 1 7 5 3 0 1 . 6 0 5 2 : r {lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn https://cybertronagent.github.io/Optimus-3.github.io/ Figure 1: Demonstration of Optimus-3s capabilities as generalist agent in Minecraft. It can perform long-horizon task planning, captioning, embodied QA, grounding, low-level action generation, and reflection in an interactive manner. All of these capabilities are seamlessly integrated into unified end-to-end architecture, enabling robust and coherent performance across diverse task scenarios."
        },
        {
            "title": "Abstract",
            "content": "Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce Mixture-ofExperts (MoE) architecture with task-level routing. 3) We develop Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agents reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, general-purpose agent for Minecraft. Extensive experimental Preprint. Under review. results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across wide range of tasks in the Minecraft environment."
        },
        {
            "title": "Introduction",
            "content": "Building an agent with human-level capabilities in perception, planning, action, and reflection within an open-world represents significant milestone toward generalist agents. Recently, empowered by the perception and reasoning capabilities of multimodal large language models (MLLMs), agents have made remarkable progress in range of domains, including video understanding [14, 49], mobile navigation [48, 54], embodied manipulation [19, 23], and vision-language navigation [55, 57]. However, in environments such as Minecraft [51, 40, 26], existing agents typically possess only one or subset of the core capabilities in Perception, Planning, Action, and Reflection. The development of an interactive generalist agent that seamlessly integrates all these faculties remains challenges. Challenge I: Insufficient Training Data for Domain-Specific Tasks. Although previous works have introduced datasets for action [3, 28, 26] and visual question answering [40, 24], there are still no publicly available datasets specifically targeting planning, reflection, and grounding. Challenge II: Interference Among Heterogeneous Tasks. In Minecraft, different tasks exhibit distinct inputoutput patterns. For example, Action requires the generation of low-level control signals, Captioning involves producing textual descriptions of visual content, and Grounding necessitates outputting spatial coordinates of target objects. As result, heterogeneous task conflict poses significant challenge faced by dense-architecture MLLMs during multi-task learning. Challenge III: Diversity of Observations and Environments in Open-World Settings. In open-world environment of Minecraft, agents are exposed to highly diverse environments and observations, ranging from forests, mountains, and caves to oceans and villages. This variability introduces significant challenges for the agent in performing vision-related tasks, such as captioning, embodied QA, and grounding. In this paper, we propose Optimus-3, generalist agent in open-world of Minecraft, which endowed with comprehensive capabilities in perception, planning, action, and reflection (depicted in Figure 1). To address the aforementioned challenges, we propose targeted improvements across three key dimensions: data generation, model architecture, and training methodology. Knowledge-Enhanced Pipeline for Automated Data Generation. To address the scarcity of domain-specific data in Minecraft, we propose an automated data generation pipeline. Compared to previous approaches limited to single-task data generation [26, 40], it offers unified pipeline for generating diverse types of data. To further enhance the quality of the generated data, we integrate multiple models and tools richly endowed with Minecraft-specific domain knowledge, provide them with environment feedback as ground truth, thereby ensuring the automated generation of high-quality annotations. Experimental results show that post-training MLLMs on data generated by this pipeline lead to significant performance improvements. Task-level Routing MoE Architecture for Scalable Multi-task Learning. To address the interference problem in heterogeneous multi-task learning, we propose task-level routing MoE architecture. As shown in Figure 2, it consists of shared knowledge expert and multiple scalable task experts. Unlike token-level routing [8], task-level routing assigns each query to dedicated task-specific expert, along with shared knowledge expert that captures generalizable knowledge across tasks. Each task is learned by updating only the parameters of its corresponding expert, thereby avoiding interference among heterogeneous tasks. Experimental results show that task-level routing outperforms token-level routing across various tasks in Minecraft. Further experiments demonstrate its ability to scale to new tasks without compromising the performance of previously learned tasks, highlighting its effectiveness and extensibility in heterogeneous multi-task settings. Multimodal Reasoning-Augmented Reinforcement Learning for Model Training. To better adapt to the visual diversity of environments in Minecraft, we propose Multimodal Reasoning-Augmented Reinforcement Learning method. This method explicitly instructs the agent to reason about the visual content, encouraging it to ground its responses in visual observations, thereby improving the interpretability of its outputs. To further enhance such reasoning capabilities, we employ an IoU-Density Reward function with GRPO [16] algorithm, allowing the model to iteratively refine its reasoning process via reinforcement learning (RL). Experimental results demonstrate that our method 2 further improves the performance of fine-tuned agent, achieving gains of 42% on Embodied QA and 36% on Grounding tasks. Figure 2: A: The architecture of Optimus-3, which includes task router that selects specific task expert for each query, ViT [11] for visual encoding, and MoE LLM for generating responses and low-level actions. Given long-horizon task, it can generate feasible plan and then execute the sub-goals sequentially. B: The proposed Multimodal Reasoning-Augmented Reinforcement Learning effectively enhances the agents performance. C: Performance comparison of Optimus-3 against current task-specific SOTA agents, GPT-4o [1], and the original backbone Qwen2.5-VL [2]. We conducted comprehensive evaluations in the open-world environment of Minecraft. Experimental results show that Optimus-3 outperforms both general multimodal large language models and previous SOTA agents in Minecraft across wide range of tasks. Compared to previous SOTA, Optimus-3 achieves an improvements of 20% on Planning, 3% on Long-Horizon Action, 66% on Captioning, 76% on Embodied QA, 3.4x on Grounding, and 18% on Reflection, respectively. In summary, our contributions are as follows: To the best of our knowledge, the proposed Optimus-3 is the first generalist agent in Minecraft, which equipped with comprehensive capabilities in perception, planning, action, grounding and reflection. Moreover, we design unified automated data generation pipeline for these tasks, providing high-quality data for the training of Optimus-3. To mitigate interference among heterogeneous tasks, we propose the MoE architecture with task-level routing. By designing shared knowledge expert alongside dedicated task-specific experts, it not only avoids task interference but also enables task expansion. To further enhance the reasoning capability of Optimus-3, we propose Multimodal Reasoning-Augmented Reinforcement Learning approach. It requires the agent to perform multimodal reasoning prior to decision-making, thereby improving the interpretability of its response. Meanwhile, reinforcement learning encourages the agent to explore diverse decision paths, promoting better generalization in open-world environments."
        },
        {
            "title": "2 Related Work",
            "content": "Minecraft Agents. The existing Minecraft agent frameworks are illustrated in Figure 3. Early work [38, 9, 4, 18] leveraged reinforcement learning to build goal-conditioned policies in Minecraft, refer to Figure 3 (a). VPT [3] and GROOT [5] develop vision-conditioned policies, while MineCLIP [13] and STEVE-1 [28] introduce textual instructions as action goals. Several works leverage (M)LLMs to generate executable code, enabling agent-environment interaction through function calling mechanisms [47, 31, 59, 34, 56, 22], refer to Figure 3 (b). To equip agents with the ability to long-horizon planning, some works have leveraged MLLMs as planner, and goal-conditioned policy for low-level control [40, 50, 51, 26, 27], refer to Figure 3 (c). Moreover, OmniJARVIS [52] introduces the use of MLLMs to generate latent tokens, which serve as control conditions for the policy, refer to Figure 3 (d). Despite significant advances, significant gap remains between current agents and generalist Minecraft agents, due to limitations in their capabilities for perception, grounding, and reflection. As shown in Figure 3 (e), we aim to develop an end-to-end generalist Minecraft agent, which is equipped with comprehensive capabilities and supports interactive responses. 3 Figure 3: Different agent framework in Minecraft. (a) Goal-conditioned policy which takes observation and instruction as input. (b) Function calling, which employs (M)LLM as the planner, then generates executable functions. (c) (M)LLM as the planner, which then employs goal-conditioned policy to generate low-level actions. (d) MLLM generates latent tokens, which are used as control conditions for the policy. (e) End-to-end architecture (ours) which is capable of generating both low-level actions and textual responses. Mixture-of-Experts Architectures. In recent years, Mixture of Experts (MoE) architectures [37, 46] have garnered significant attention in the field of Large Language Models (LLMs), owing to their scalability and sparsity in activation [20, 15, 12]. On the theoretical front, some work [7, 35] demonstrated the effectiveness of MoE architectures in tasks with cluster structures. To further enhance expert specialization, DeepSeekMoE [8] proposed more differentiated expert training strategies, significantly boosting downstream task performance, albeit with increased training complexity. Recently, MoE architectures have been incorporated into MLLMs [44, 25, 29, 53], significantly enhancing their generalization capabilities and computational efficiency. Despite these advancements, existing MoE architectures still face notable challenges in load balancing and routing optimization. In this paper, we propose task-routing driven MoE architecture, in which task router directs each query to designated task-specific expert along with shared knowledge expert, thereby eliminating the need for token-level dynamic routing."
        },
        {
            "title": "3 Optimus-3",
            "content": "In this section, we first introduce an automated dataset generation pipeline in Sec. 3.1. Then, we provide detailed description of the Optimus-3 architecture (Sec. 3.2). As shown in Figure 2, it adopts Mixture-of-Experts (MoE) architecture with task-level routing, which effectively enhances the models capability to learn heterogeneous multi-task. Finally, in Sec 3.3, we elaborate on how to implement the proposed Multimodal Reasoning-Augmented RL, which enhances Optimus-3s reasoning capabilities and generalization on vision-related tasks. 3.1 Knowledge-Enhanced Data Generation Pipeline Currently, there are no publicly available datasets for planning, grounding, or reflection in Minecraft. Compared to manual annotation, leveraging MLLMs for automated data sampling offers lower-cost alternative. However, we observe that general-purpose MLLMs are often insufficient for use as annotation models, due to their lack of domain-specific knowledge. For instance, GPT-4o [1] fails at grounding, some MLLMs [10, 2] tend to hallucinate in embodied question answering, and most models [39, 45, 36, 10] lack the ability to produce feasible plan for long-horizon tasks. To address this, we propose knowledge-enhanced automated data generation pipeline. It integrates multiple expert models and tools to ensure the quality and reliability of the generated data. As shown in Figure 4, we first collect set of commonly used items from the Minecraft Wiki to construct task pool. We then employ knowledge graph (directed graph capturing crafting dependencies) [26] to generate plans for each task. These plans are subsequently executed by goal-conditioned policy, STEVE-1 [28], resulting in collection of observation-action pairs. During execution, we randomly sample visual frames for vision-related tasks. Then we employ expert 4 Figure 4: Data Generation Pipeline. Given task pool, we utilize knowledge graph [26] to generate task plans, forming the planning dataset. These plans are then used as instructions for STEVE-1 [28], which interacts with the environment to produce the action dataset. During this process, we randomly sample images and employ expert models [36, 33] with environmental feedback to generate the captioning, embodied QA, and grounding datasets. models [36, 32, 1] to label these frames, and provide them with environmental feedback [17] (agents state, inventory, and surrounding objects) as ground truth. By incorporating expert models with environmental feedback, we significantly enhance the quality of the generated data. For example, by providing the caption model with information about the objects present in the scene, the incidence of hallucinations in the generated captions is significantly reduced. In this way, diverse types of data are sampled to support the training of Optimus-3. For more details, please refer to Appendix D. 3.2 MoE Architecture with Task-level Routing Besides textual outputs, an agent in Minecraft must also generate non-textual outputs, such as lowlevel actions, bounding box coordinates. This heterogeneity in task formats introduces significant risk of task interference during multi-task training. One effective solution is to introduce Mixture-ofExperts (MoE) architecture [37, 46], in which each task is learned by updating only subset of expert parameters [8]. However, token-level routing can still result in the same expert being optimized for multiple tasks, leading to task interference[58, 44]. To address this issue, we introduce an MoE architecture with task-level routing, where each task is learned by updating only its designated expert parameters, thereby effectively avoiding interference across tasks. As shown in Figure 2, given an instruction, the Task Router assigns it to specific task type, allowing each query to have its expert determined before entering the model. Moreover, following the design of DeepSeekMoE [8], we introduce shared knowledge expert that is always activated for every query, facilitating the transfer of common knowledge across different tasks. Formally, at the l-th MoE layer, given querys hidden state ul can be expressed as: q, the output hidden state hl = Wshared(ul hl q) + Wtask(q)(ul q), (1) where Wshared() represents the transformation applied by the shared knowledge expert, and Wtask(q)() denotes the transformation applied from the task expert which selected by the task router. In practice, we fine-tune Sentence-BERT [41] as the task router to classify instructions into task types. For action generation, we integrate VPT [3] as the action head, enabling low-level control execution within the Minecraft environment. We first train the shared knowledge expert across various tasks to capture common knowledge between them. Then, we fine-tune the task-specific experts while keeping the shared expert frozen. Through this approach, each task-specific expert learns specialized task knowledge, while the shared knowledge expert establishes connections across tasks. Moreover, the task scale-up experiments in Section 4.3 demonstrate that the model supports task expansion by training new expert, without compromising the performance of previously learned tasks. 3.3 Multimodal Reasoning-Augmented Reinforcement Learning In open-world environment of Minecraft, the diverse surroundings and randomized initial positions result in varying observations. To adapt to this diversity, we propose Multimodal Reasoning-Augmented Reinforcement Learning approach. It guides the agent to generate reasoning processes based on visual content and instruction, thereby explicitly activating its multimodal reasoning capability. This capability is further enhanced through reinforcement learning (RL), encouraging the agent to produce robust and interpretable responses. Specifically, we first leverage the data collected from the pipeline (Sec. 3.1) to adapt general MLLM to the Minecraft domain. Then, we introduce multimodal reasoning fine-tuning phase, where the model is tuned on data with Chain-of-Thought (CoT) templates to activate its reasoning capabilities: Lsft = (cid:88) t= log pθ (yt xv, xins, y<t) , = (cid:104) y(think), y(ans)(cid:105) (2) where xv and xins denote the input visual tokens and text tokens, respectively; represents the output token, which consists of reasoning process y(think) and final answer y(ans); and pθ denotes the MLLM with parameters θ. During this phase, we require the model to describe the current visual scene as part of the multimodal reasoning process. This compels the model to attend to visual information before answering questions, guiding it to complete tasks based on visual cues. To further enhance the models reasoning capabilities, we incorporate reinforcement learning phase with Group Relative Policy Optimization (GRPO) algorithm [43]. It obviates the need for additional value function approximation as in PPO [42], and instead uses the average reward of multiple sampled outputs as the baseline. Therefore, the model can leverage its self-generated sampling data to iteratively refine its reasoning process, leading to more robust and coherent reasoning over time. To better adapt to the grounding task in Minecraft, we design an IoU-Density Reward function, which offers more fine-grained reward signal to effectively guide the models learning process: (y) = 1, ηu, 0, if α if α > β if < β (3) Here, denotes the Intersection-over-Union (IoU), α and β are hyperparameters in the range (0, 1), and η is weighting coefficient. For multimodal input x, policy πθ samples output sequences {y1, y2, ..., yG}. The training objective can be formulated as follows: 1 (cid:88) i=1 1 yi yi (cid:88) t=1 {min [rθ (x, yi) Ai,t, clip (rθ (x, yi) , 1 ε, 1 + ε) Ai,t] λDKL} , (4) rθ (x, yi) = πθ (yi,t x, yi,<t) πθold (yi,t x, yi,<t) , DKL = πref (yi,t x, yi,<t) πθ (yi,t x, yi,<t) log πref (yi,t x, yi,<t) πθ (yi,t x, yi,<t) 1, (5) where Ai,t denotes the advantage function, ε and λ are hyperparameters, and πθold and πref represent the old policy and the reference model, respectively."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiments Setting Environment. Following the standard settings in many previous works [28, 50, 51, 40, 26], we conduct experiments in the open-world environment of Minecraft on the MineRL [17] platform. In MineRL, the agent receives instructions and observations and outputs mouse and keyboard control actions at frequency of 20 Hz. For each task execution, the agent is initialized in random biomes and positions without any equipment, resulting in highly diverse environmental conditions. Therefore, Minecraft serves as suitable and challenging testbed for evaluating open-world agents. Implementation details. We initialize Optimus-3 with the weights of Qwen2.5-VL-7B [2]. Then we adapt it into MoE architecture, comprising one shared knowledge expert and five task-specific experts dedicated to planning, perception, action, grounding, and reflection. We collect 230k samples for supervised fine-tuning, 58k samples for the multimodal reasoning fine-tuning phase, and 5k samples 6 Table 1: Main Result of Optimus-3 on Long-Horizon tasks. We report SR on each task group, the results of each task can be found in the Appendix. H. Planner denotes MLLM generates the task plan, followed by STEVE-1 [28] generates actions, as adopted in previous work [27]. Method Wood Stone Diamond Multimodal Large Language Model as Planner Gold Iron GPT-3.5 GPT-4o Gemini-1.5-pro Qwen2.5-VL Qwen2.5-VL (SFT) DEPS [50] JARVIS-1 [51] Optimus-1 [26] Optimus-2 [27] Optimus-3 0.40 0.47 0.41 0.28 0.76 0.77 0.93 0.98 0.99 0.99 0.00 0.05 0.03 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.20 0.23 0.21 0.06 0.36 Current SOTA Agents in Minecraft 0.48 0.89 0.92 0.93 0. 0.00 0.07 0.08 0.09 0.10 0.16 0.36 0.46 0.53 0.55 0.00 0.00 0.00 0.00 0.00 0.01 0.08 0.11 0.13 0.15 RedStone Armor 0.00 0.00 0.00 0.00 0.00 0.00 0.16 0.25 0.28 0.28 0.00 0.00 0.00 0.00 0.00 0.10 0.15 0.19 0.21 0.23 for reinforcement learning. These datasets are sourced from our proposed data generation pipeline as well as previous works [3, 27, 40]. All experiments were conducted on 8x NVIDIA L40 GPUs. Training pipeline and hyperparameter setting can be found in Appendix. G. Evaluation Tasks & Metrics. We evaluate Optimus-3 across six types of tasks: Long-Horizon, Planning, Captioning, Embodied QA, Grounding, and Reflection. For Long-Horizon tasks, we follow the previous setup [26], conducting experiments on 67 tasks grouped into 7 categories. Each task is executed at least 30 times, and the average Success Rate (SR) is used as the evaluation metric. For Planning and Reflection tasks, evaluation samples are 103 and 64, respectively. They are used to evaluate the agents ability for long-horizon planning and reflection on its current state, with average accuracy (Acc) as the evaluation metric. For the Captioning and Embodied QA tasks, the evaluation includes 134 and 400 samples, respectively. We adopt an LLM-as-Judge [21] approach, employing GPT-4.1 [1] to assign score from 1 to 10 for each sample. The average score is then normalized to value between 0 and 1. For the Grounding tasks, we construct 500 evaluation samples, and use IOU@0.5 [6] as the prediction accuracy. Baselines. For Long-Horizon tasks, we employ generalist MLLMs (GPT-3.5 [39], GPT-4o [1], Qwen2.5-VL[2], Gemini-1.5-pro [45]) and current SOTA agents in Minecraft (DEPS [50], JARVIS-1 [51], Optimus-1 [26], and Optimus-2 [27]) as baselines. For the other tasks, we compare Optimus-3 against generalist MLLMs, and various post-trained versions of Qwen2.5-VL. 4.2 Experimental Results Optimus-3 outperforms existing Minecraft agents on Long-Horizon tasks. As shown in Table 1, Optimus-3 achieves the highest success rate across all seven task groups, with particularly strong performance on the Diamond Group, attaining SR of 15%. Notably, Optimus-3 performs selfplanning and action prediction without any additional tools or external models, distinguishing it from other agents [51, 26, 27]. Furthermore, the results in the 4-5 rows demonstrate that post-training Qwen2.5-VL on our planning dataset significantly enhances its long-horizon planning capabilities. Optimus-3 significantly surpasses existing agents in Planning, Captioning, Embodied QA, Grounding, and Reflection capabilities. As depicted in Table 2, compared to all baselines, Optimus3 achieves the highest performance across all task types. More specifically, the first four rows of experimental results reveal the limited capabilities of existing generalist MLLMs, due to they have not post-trained in the Minecraft domain. In contrast, Qwen2.5-VL (SFT), which post-trained on our dataset, shows notable performance boost. Furthermore, after applying our proposed reinforcement learning method, Qwen2.5-VL (RL) achieves 52% improvement in Grounding. We observe that the various versions of Qwen2.5-VL without the MoE architecture exhibit task interference, while Optimus-3 consistently achieves superior performance across tasks. Moreover, compared to tokenlevel routing, the proposed task-level routing demonstrates superior performance on tasks such as Captioning, Planning, and Grounding. 7 Table 2: Evaluation Results of Optimus-3 on Planning, Captioning, Embodied QA, Grounding, and Reflection. We report the accuracy of each task. #Params denotes activated parameters."
        },
        {
            "title": "Model",
            "content": "#Params"
        },
        {
            "title": "Reflection",
            "content": "GPT-4o Gemini-1.5-pro Qwen2.5-VL Qwen2.5-VL"
        },
        {
            "title": "Generalist Multimodal Large Language Model",
            "content": "- - 3B 7B 0.20 0.19 0.03 0.05 0.46 0.33 0.37 0.47 0.33 0.33 0.40 0.46 Post-trained Multimodal Large Language Model Qwen2.5-VL (SFT) Qwen2.5-VL (RL) Qwen2.5-VL (SFT) Qwen2.5-VL (RL) Optimus-3 [tokenlevel] Optimus-3 [tasklevel] 3B 3B 7B 7B 6.8B 6.8B 0.76 0.74 0.79 0.76 0.88 0.94 0.64 0.65 0.68 0.71 0.66 0.78 0.69 0.70 0.71 0.68 0.77 0.81 - - 0.58 0.18 0.69 0.71 0.52 0.79 0.75 0. 0.31 0.34 0.47 0.56 0.47 0.48 0.53 0.56 0.58 0.66 4.3 Ablation Study In this section, we conduct comprehensive ablation studies to validate the effectiveness of our approach and summarize our key findings. High-quality training data is essential for effective MLLM post-training. Experimental results in Table 2 reveal that both Optimus-3 and Qwen2.5-VL benefit substantially from training on our dataset. As shown in Figure 5, when we remove the expert models and environmental feedback from the data generation pipeline, the performance drops by 81% on Planning, 32% on Embodied QA, and 23% on Grounding, respectively. We attribute this to the proposed data generation pipeline, which incorporates multiple expert models and tools in Minecraft domain, ensuring the quality of the collected data. Moreover, the entire data collection process incurred only $300 in API costs, required no manual annotation, and was completed using 4 NVIDIA L40 GPUs over 36 hours, demonstrating the cost-efficiency of the pipeline. The MoE architecture is crucial for effective multi-task learning. Compare rows 3-6 in Table 3, we observe that the MoE-based Qwen2.5-VL exhibits no task interference, while the densearchitecture Qwen2.5-VL suffers performance degradation in Planning and Embodied QA due to interference from other tasks. It highlights the strength of our proposed task-routing MoE architecture in heterogeneous multi-task learning. Figure 5: Ablation Study on Training Data. original refers to the original Qwen2.5-VL, tuned_w/o_k indicates the model fine-tuned on data without knowledge, tuned_w/_k represents the model tuned on data generated by knowledge-enhanced pipeline. Table 3: Ablation study of Optimus-3. We report accuracy on each task. F., A., M., and R. represent the fine-tuning on our data, MoE architecture, multimodal reasoning phase, and RL phase, respectively. Ablation Setting F. A. M. R. Planning (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) Caption 0.47 0.68 0.71 0.74 0.78 0.78 Perception EQA Grounding 0.46 0.71 0.68 0.57 0.81 0.81 0.18 0.52 0.79 0.59 0.67 0.80 0.05 0.79 0.76 0.85 0.94 0.94 Multimodal Reasoning-Augmented Reinforcement Learning further enhances the agents capabilities. As shown in Table 3, removing the multimodal reasoning phase and the reinforcement learning phase results in performance drop of 26% and 16% in Grounding, respectively. We attribute this to the effectiveness of the proposed method in activating the models multimodal reasoning capabilities, thereby enhancing Optimus-3s performance for vision-related tasks. 8 Figure 6: Visual comparison of Optimus-3 (ours), Qwen2.5-VL (tuned on our data), and GPT-4o. Red highlights indicate errors, while blue highlights denote correct outputs. Comparison of task scale-up capability. We employ Qwen2.5-VL-7B [2] as the backbone and compare the performance of token-level routing and task-level routing, in the context of extending grounding capabilities. The lightcolored bars indicate models trained only on planning, captioning, reflection, and embodied QA, while the dark-colored bars represent models additionally trained on grounding. As shown in Figure 7, after expanding to include the grounding task, the model with task-level routing (task-moe-grounding) preserves the performance on previously learned tasks, while the token-level routing model (token-moe-grounding) suffers from task interference and overall lower performance compared to its task-level counterpart. It demonstrates that our proposed MoE with task-level routing architecture possesses strong task scalability. Figure 7: Performance comparison between tokenlevel routing and task-level routing. 4.4 Qualitative Analysis As depicted in Figure 6, we provide visual comparison between Optimus-3, Qwen2.5-VL-7B [2], and GPT-4o [1], highlighting their differences in performance and behavior across various tasks. We observe that GPT-4o exhibits hallucinations in captioning and embodied QA, lacks grounding capabilities, and produces unreasonable plans. In contrast, Qwen2.5-VL, when fine-tuned on our dataset, shows reduced hallucination, acquires grounding and planning abilities, and generates more coherent outputs. Notably, Optimus-3 accurately performs vision-related tasks and produces wellstructured plans conditioned on instructions, demonstrating its superior perception and reasoning in the Minecraft environment."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Optimus-3, which endowed with comprehensive capabilities in perception, planning, action, and reflection within the Minecraft. We propose knowledge-enhanced data generation pipeline to support agent training, task-level routing MoE to address interference among heterogeneous tasks, and multimodal reasoning-augmented reinforcement learning method to improve performance on vision-related tasks. Extensive experimental results demonstrate that Optimus-3 marks significant step forward toward building generalist agent in Minecraft."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. [4] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1373413744, 2023. [5] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning to follow instructions by watching gameplay videos. In The Twelfth International Conference on Learning Representations, 2023. [6] Zhihong Chen, Ruifei Zhang, Yibing Song, Xiang Wan, and Guanbin Li. Advancing visual grounding with scene knowledge: Benchmark and method. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1503915049, 2023. [7] Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding mixture of experts in deep learning. arXiv preprint arXiv:2208.02813, 2022. [8] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024. URL https://arxiv. org/abs/2401.06066, 2024. [9] Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang, and Zongqing Lu. Clip4mc: An rl-friendly vision-language model for minecraft. arXiv preprint arXiv:2303.10571, 2023. [10] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. [12] Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International conference on machine learning, pages 55475569. PMLR, 2022. [13] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:1834318362, 2022. [14] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, pages 7592. Springer, 2024. 10 [15] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23 (120):139, 2022. [16] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [17] William Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019. [18] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. [19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [21] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. [22] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language models for minecraft. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1642616435, 2024. [23] Hao Li, Qi Lv, Rui Shao, Xiang Deng, Yinchuan Li, Jianye Hao, and Liqiang Nie. Star: Learning diverse robot skill abstractions through rotation-augmented vector quantization. arXiv preprint arXiv:2506.03863, 2025. [24] Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, and Yitao Liang. Jarvis-vla: Post-training large-scale vision language models to play visual games with keyboards and mouse. arXiv preprint arXiv:2503.16365, 2025. [25] Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. Uni-moe: Scaling unified multimodal llms with mixture of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [26] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. arXiv preprint arXiv:2408.03615, 2024. [27] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-2: Multimodal minecraft agent with goal-observation-action conditioned policy. arXiv preprint arXiv:2502.19902, 2025. [28] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: generative model for text-to-behavior in minecraft. Advances in Neural Information Processing Systems, 2023. [29] Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and Armen Aghajanyan. Moma: Efficient early-fusion pre-training with mixture of modality-aware experts. arXiv preprint arXiv:2407.21770, 2024. [30] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 11 [31] Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia. Rl-gpt: Integrating reinforcement learning and code-as-policy. arXiv preprint arXiv:2402.19299, 2024. [32] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [34] Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, and Mingli Song. Odyssey: Empowering agents with open-world skills. arXiv preprint arXiv:2407.15325, 2024. [35] Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, and Jie Fu. closer look into mixture-ofexperts in large language models. arXiv preprint arXiv:2406.18219, 2024. [36] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng towards real-world vision-language Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: understanding. arXiv preprint arXiv:2403.05525, 2024. [37] Siyuan Mu and Sen Lin. comprehensive survey of mixture-of-experts: Algorithms, theory, and applications. arXiv preprint arXiv:2503.07137, 2025. [38] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, pages 26612670. PMLR, 2017. [39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [40] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: multi-modal open-ended embodied system in minecraft via active perception. arXiv preprint arXiv:2312.07472, 2023. [41] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019. [42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [43] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [44] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. Mome: Mixture of multimodal experts for generalist multimodal large language models. Advances in neural information processing systems, 37:4204842070, 2025. [45] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [46] Arpita Vats, Rahul Raja, Vinija Jain, and Aman Chadha. The evolution of mixture of experts: survey from basics to breakthroughs. Preprints, 2024. [47] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. 12 [48] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024. [49] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 5876. Springer, 2024. [50] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023. [51] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023. [52] Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, and Yitao Liang. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents. arXiv preprint arXiv:2407.00114, 2024. [53] Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, and Radu Soricut. Omni-smola: Boosting generalist multimodal models with soft mixture of low-rank experts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1420514215, 2024. [54] Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, and Liqiang Nie. Gui-explorer: Autonomous exploration and mining of transition-aware knowledge for gui agent. arXiv preprint arXiv:2505.16827, 2025. [55] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang. Navid: Video-based vlm plans the next step for vision-and-language navigation. arXiv preprint arXiv:2402.15852, 2024. [56] Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang, and Gaoang Wang. See and think: Embodied agent in virtual environment. arXiv preprint arXiv:2311.15209, 2023. [57] Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pages 260278. Springer, 2024. [58] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022. [59] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023. 13 Appendix of Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 Optimus-"
        },
        {
            "title": "3.2 MoE Architecture with Task-level Routing . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.3 Multimodal Reasoning-Augmented Reinforcement Learning . . . . . . . . . . . . 4 Experiments 4.1 Experiments Setting . . 4.2 Experimental Results . 4.3 Ablation Study . . . 4.4 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion Limitation and Future Work Broader Impact Minecraft Dataset Generation Pipeline D.1 Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Comparison with Existing Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . MoE Architecture with Task-level Routing Multimodal Reasoning-Augmented Reinforcement Learning Training Details Quantitative Analysis H.1 Evaluation Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Results on Long-Horizon Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2 3 4 4 6 6 6 7 8 9 15 15 15 15 17 18 18 19"
        },
        {
            "title": "A Limitation and Future Work",
            "content": "In this paper, we aim to endow Optimus-3 with multi-dimensional capabilities, including perception, planning, execution, grounding, and reflection. These abilities enable Optimus-3 to interact with humans and perform wide range of tasks in Minecraft. However, due to the absence of memory module, Optimus-3 lacks the ability for life-long learning and self-evolution. Equipping Optimus-3 with the capability to autonomously explore and learn new tasks remains promising direction for future research. Moreover, creative tasks represent current limitation of Optimus-3. Once suitable datasets for such tasks become available, its capabilities can be effectively enhanced via post-training."
        },
        {
            "title": "B Broader Impact",
            "content": "Optimus-3 is built upon multimodal large language model (MLLM), which brings both potential benefits and inherent risks. On the positive side, thanks to the perceptual and reasoning capabilities of MLLMs, Optimus-3 demonstrates strong performance in planning and visual question answering. Moreover, it exhibits good generalization across wide range of tasks. However, on the negative side, MLLMs carry the risk of generating harmful or unintended outputs. Although techniques such as post-training have significantly reduced the likelihood of Optimus-3 producing undesired content, comprehensive assessment of its potential risks remains essential to ensure safe deployment."
        },
        {
            "title": "C Minecraft",
            "content": "Minecraft is highly popular sandbox video game developed by Mojang Studios 1. It allows players to explore an infinitely generated 3D world composed of blocks, gather raw materials, tame and breed creatures, craft tools, build structures, and battle bosses. The game features diverse range of environments, including mountains, rivers, oceans, forests, deserts, villages, and caves. In such an open world, the agent receives visual observations as input and generates mouse and keyboard control actions as output. The diversity of the environment, visual observations, and action space makes Minecraft an ideal testbed for evaluating general-purpose agents abilities in perception, planning, action, and reflection. Therefore, consistent with large body of prior work [3, 28, 47, 13, 50, 51, 40, 26, 27], we adopt Minecraft as the environment for evaluating our agent."
        },
        {
            "title": "D Dataset Generation Pipeline",
            "content": "D.1 Pipeline We first use script to extract commonly used items from the Minecraft Wiki 2, which are then assembled into task pool. For each item in the task pool, we use knowledge graph to derive its crafting or acquisition path. For example, the crafting path for the stone pickaxe represented as: {log planks crafting table sticks wooden pickaxe cobblestone stone pickaxe}. This path is converted into task plan format. To increase the diversity and complexity of planning tasks, we randomly generate scenarios where the agent is assumed to already possess some materials. For example: Given one crafting table and two cobblestones, how can stone sword be crafted? Such settings significantly increase the difficulty of planning, as the agent must reason under partial resource constraints and adjust the crafting sequence accordingly. Next, we introduce STEVE-1 [28] to sequentially execute each sub-goal in the plan. Through interaction with the environment, we collect the corresponding observation-action pairs for each step, which are then used to construct the action dataset. We follow prior settings [27] by sampling frames at 640360 resolution and 20 frames per second, and mapping the agents actions into discrete action space [3]. During execution, we randomly sample visual frames for vision-related tasks. For the captioning, we leverage environmental feedback (including the agents state, surrounding objects, and items in the inventory) as ground truth, and employ DeepSeek-VL2 [36] to generate visual descriptions. 1https://www.minecraft.net/en-us/article/meet-mojang-studios 2https://minecraft.wiki/ Figure 8: Planning and Action samples generated from our pipeline. Figure 9: Captioning and Embodied QA samples generated from our pipeline. For the embodied QA, we leverage GPT-4 3 to generate question-answer pairs based on the visual descriptions and environmental feedback, constructing the VQA dataset. For the grounding dataset, Grounding DINO [32] is used to generate object coordinates based on the object information provided in the environmental feedback. When the environmental feedback indicates task failure, we sample the corresponding trajectory and query GPT-4 to produce reflective analyses, thereby constructing the reflection dataset. Through this pipeline, we collected total of 100k action trajectories, 10k planning samples, 40k captioning samples, 40k embodied QA samples, 40k grounding samples, and 3k reflection samples. These datasets were then randomly split into training and evaluation sets. We present some samples in Figure 8-10, showcasing the diversity and structure of the collected datas. 3https://openai.com/index/gpt-4/ Figure 10: Grounding and Reflection samples generated from our pipeline. D.2 Comparison with Existing Datasets To highlight the novelty and coverage of dataset generated from proposed pipeline, we compare it with existing datasets commonly used in the Minecraft. As shown in Table 4, existing datasets lack coverage of critical tasks such as planning, grounding, and reflection, which significantly limits the development of general-purpose agents in Minecraft. Key advantages of our dataset: Broader Task Coverage: Includes six task types, planning, action, captioning, embodied QA, grounding, and reflection, many of which are not jointly covered in existing datasets. Automated and Scalable: Generated via our proposed unified pipeline, reducing the cost and time of manual annotation. Knowledge-Enhanced: Incorporates Minecraft-specific expert models and environment feedback to ensure data quality. Supports End-to-End Training: Enables training of generalist agents like Optimus-3 that can reason and act across multiple modalities and tasks. Table 4: Comparison of our dataset with existing datasets used in Minecraft. Dataset Perception Planning Grounding Action Reflection Automation VPT [3] MineDojo [13] STEVE-1 [28] MGOA [27] MP5 [40] OmniJAVIS [52] Ours (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) 17 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) MoE Architecture with Task-level Routing We initialize our model with Qwen2.5-VL-7B [2], and then adapt it into Mixture-of-Experts (MoE) architecture. Following the configuration of DeepSeek-V3 [30], it adopts hybrid architecture that integrates shared backbone with MoE modules to balance generalization and specialization across diverse tasks. The lower layers of the model are implemented as dense Transformer blocks, enabling the learning of universal semantic representations. In contrast, the upper layers incorporate MoE structure, where experts are selectively activated based on inputs. The MoE structure consists of one shared knowledge expert and five task-specific experts for planning, perception, action, grounding, and reflection. All experts are configured to have equal parameters. Multimodal Reasoning-Augmented Reinforcement Learning Multimodal Reasoning-Augmented Reinforcement Learning consists of two training stages: Multimodal Reasoning Tuning Phase: The model is required to perform explicit reasoning before executing the target task. For example, in embodied QA, the model must first describe the visual content as reasoning process, and then answer the question based on both the question itself and the generated visual description. In our experiments, we observe that this multimodal reasoning fine-tuning stage significantly enhances the models performance in tasks such as planning and embodied QA. The prompt templates for planning, embodied QA, grounding, and reflection are presented in Templates 14. Reinforcement Learning Phase: We leverage the GRPO [43] algorithm to further enhance the models reasoning capabilities. For the grounding task, we design an IoU-Density Reward to provide finer-grained supervision, enabling more precise optimization of the models grounding performance. System: You are an expert in Minecraft, capable of performing task planning, visual question answering, reflection, grounding and executing low-level actions. User: You should first think about the reasoning process in the mind and then provide the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>< answer> answer here </answer>. The users instruction is: {instruction} Assistant: <think> To craft {items}, it is necessary to first consider the required materials and subsequently acquire them in the appropriate order. {crafting recipes} Upon analyzing the crafting dependencies among these materials, the resulting list of components and their respective quantities is as follows: {materials and their quantities} </think> <answer> To obtain {items} from scratch, you need to carry out the following steps in sequence: {plans} </answer> Listing 1: Prompt for Planning. System: You are an expert in Minecraft, capable of performing task planning, visual question answering, reflection, grounding and executing low-level actions. User: You should first think about the reasoning process in the mind and then provide the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>< answer> answer here </answer>. The users instruction is: <image> {instruction} Assistant: <think> To answer the questions accurately, must first provide precise description of the visual content. {description of visual content} Then, will analyze the questions in relation to the visual content to derive the correct answers. </think> <answer> {ans} </answer> Listing 2: Prompt for Embodied QA. System: You are an expert in Minecraft, capable of performing task planning, visual question answering, reflection, grounding and executing low-level actions. User: You should first think about the reasoning process in the mind and then provide the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>< answer> answer here </answer>. The users instruction is: <image> {instruction} Assistant: <think> {description of visual content} </think> <answer> {bbox} </answer> Listing 3: Prompt for Grounding. System: You are an expert in Minecraft, capable of performing task planning, visual question answering, reflection, grounding and executing low-level actions. User: <image> Given the current task: {task} and the corresponding image, you are required to determine whether the task can continue. Assistant: <think> To accurately assess the current state, must first provide precise description of the visual content. {description of visual content} Then, need to perform reasoning based on the current task and the visual content. {reasoning} </think> <answer> {state of current task} </answer> Listing 4: Prompt for Reflection. Table 5: Hyperparameter setting for each training phase. Hyperparameter Optimizer Learning Rate Epochs Batch Size Gradient Accumulation Warm Up Ratio Max Pixels Num Generations Max Prompt Length"
        },
        {
            "title": "G Training Details",
            "content": "Fine-tuning Multimodal Reasoning Reinforcement Learning AdamW 5.0e-5 2 12 16 0.25 234416 - 2048 AdamW 3.0e-5 2 8 16 0.25 234416 - 2048 AdamW 1.0e-6 20 16 8 - 234416 5 2048 The entire training process is divided into three phase. We first train the shared knowledge expert on various tasks via supervised fine-tuning. During this stage, all parameters of the ViT [11] and the LLM are unfrozen and trained for 2 epochs to capture generalizable, task-agnostic knowledge. Next, we freeze the ViT and the dense backbone of the LLM, and train only the task-specific experts through multimodal reasoning tuning. Notably, for the action task, we apply imitation learning [27] to train the model to generate low-level actions. During this phase, the action head is also unfrozen and updated. Finally, we further improve the reasoning capabilities of specific experts, particularly for grounding and reflection, through the reinforcement learning phase. The hyperparameter settings for each training phase are summarized in the Table 5."
        },
        {
            "title": "H Quantitative Analysis",
            "content": "H.1 Evaluation Benchmark The evaluation benchmark consists of: 104 samples for planning. 134 samples for captioning. 400 samples for embodied QA. 64 samples for reflection. 500 samples for grounding. 67 long-horizon tasks for action execution. Planning: We randomly sampled 104 cases to construct the planning evaluation set, covering six technological levels: wood, stone, iron, gold, diamond, and redstone. Notably, we introduced planning scenarios with partially available materials, which increases the difficulty of generating accurate plans. This setup poses significant challengeeven for human playersas computing the correct crafting path for diamond-level items within one minute can be non-trivial. We present subset of examples in Table 6. We use accuracy as the evaluation metric, where prediction is considered correct only if the agents generated planning steps exactly match the ground truth. Group Wood Stone Iron Gold Diamond Redstone Table 6: Samples from the Planning Evaluation Set. Query Given that you already have 2 logs, how to get 1 wooden axe ? Given that you already have 2 stick, how to get 1 wooden pickaxe ? Given that you already have 7 planks, how to get 1 wooden sword ? Given that you already have 2 logs, how to get 1 stone sword? Given that you already have 2 planks, how to get 1 stone pickaxe ? Given that you already have 1 wooden pickaxe, how to get 1 stone hoe ? Given that you already have 1 wooden pickaxe, how to get 1 iron helmet ? Given that you already have 11 planks, how to get 1 iron shovel ? Given that you already have 6 gold nugget, how to get 1 golden carrot ? Given that you already have 10 planks, how to get 1 gold nugget ? Given that you already have 2 cobblestone, how to get 1 golden chestplate ? Given that you already have 10 planks, how to get 1 diamond leggings ? Given that you already have 9 cobblestone, how to get 1 diamond hoe ? Given that you already have 3 planks, how to get 1 clock ? Given that you already have 2 logs, how to get 1 compass ? Captioning: We collect 134 samples as the evaluation set, where the model is required to accurately describe first-person visual scenes in Minecraft. We adopt an LLM-as-Judge evaluation protocol, employing GPT-4.1 to score each generated visual description on scale of 1 to 10. The average score is then normalized to the (0, 1) range and used as the evaluation metric. To enhance the robustness of evaluation, we prompt GPT-4.1 to simultaneously score responses from multiple models, reducing randomness and bias in individual assessments. The evaluation prompt is shown in Template 5. Embodied QA: We collect 400 samples as the evaluation set, where the model is required to answer questions related to the visual scene. The evaluation metric is consistent with that of the captioning task. We present some evaluation samples in Table 7. 20 Table 7: Samples from the Captioning and Embodied QA Evaluation Set. Task Image Query"
        },
        {
            "title": "Captioning",
            "content": "Embodied QA <image> Describe this image. <image> Describe this image. <image> Describe this image. <image> Describe this image. <image> What is the players current health and hunger status? <image> What block or item is the player currently highlighting in their inventory? <image> What tool is the player currently holding and using? <image> How many hearts of health does the player currently have? System: You are rigorous evaluator. Your task is to score and analyze multiple answers based on provided correct answer. User: Each answer should be scored from 0 to 10 based on the following dimensions: - Accuracy: Does the answer correctly reflect the information in the correct answer? - Completeness: Does the answer include all the key points from the correct answer? - Clarity: Is the answer clear, well-structured, and easy to understand? - Relevance: Is all content in the answer relevant to the correct answer, without unnecessary additions? You will provide both numerical score (0,10) and detailed reason for each answer. Input Format: Answer: {ground truth} Response1: {r1} Response2: {r2} Response3: {r3} Output Format: Output1 Score: X/10 Output1 Reason: xxx Output2 Score: X/10 Output2 Reason: xxx Output3 Score: X/10 Output3 Reason: xxx Listing 5: Evaluation prompt template for Captioning and Embodied QA. 21 Table 8: Samples from the Grounding and Reflection Evaluation Set. Task Image Query Answer <image> Trace the outline of the cow visible outdoors. [49, 181, 86, 208]"
        },
        {
            "title": "Grounding",
            "content": "<image> Identify the position of the sheep near the player. [277, 188, 339, 237] <image> Locate the chicken near the village or forest. [127, 200, 153, 271] <image> Find the pig visible on the terrain. [87, 213, 184, 267] <image> Determine whether the task can continue. <image> Determine whether the task can continue. <image> Determine whether the task can continue. <image> Determine whether the task can continue. No No Yes No Reflection Grounding: We randomly select 500 samples as the evaluation set, where the model is required to predict the coordinates of specified object. We use IoU@0.5 as the accuracy metric, where prediction is considered correct if the intersection-over-union (IoU) between the predicted bounding box and the ground truth is greater than 50%. Some cases are shown in Table 8. Reflection: We randomly select 64 samples as the evaluation set, where the model is required to assess whether the current state allows for continued execution of given task. This involves reasoning about conditions such as whether the agent is in danger or whether the environment permits task completion. We formulate this as binary classification task (whether the task can or cannot be continued) and use accuracy as the evaluation metric. Long-Horizon Tasks: Following the settings of prior work [26], we select 67 long-horizon tasks spanning seven groups (wood, stone, iron, gold, diamond, redstone, and armor), to evaluate the agents ability to perform long-sequence action execution. For each task, the agent is initialized at random location and without any materials or equipment. As result, the agent must progressively gather resources and craft tools in order to complete the final objective. To ensure robustness, each task is executed at least 30 times during evaluation. We employ success rate as metric. In the next section, we report the results for each task. Notably, Optimus-3 is the only agent that completes both long-horizon task planning and low-level action generation within an end-to-end architecture, without relying on any external generalist multimodal large models [40, 50] or additional memory modules [51, 26]. 22 H.2 Results on Long-Horizon Tasks As shown in Table 9 and Table 10, we report the success rate of Optimus-3 on each long-horizon task, providing detailed breakdown of its performance across different task categories. Table 9: The results of Optimus-3 on the Wood Group, Stone Group, and Iron Group. Group Task Wood Stone Iron Craft wooden shovel Craft wooden pickaxe Craft wooden axe Craft wooden hoe Craft stick Craft crafting table Craft wooden sword Craft chest Craft bowl Craft ladder Craft stone shovel Craft stone pickaxe Craft stone axe Craft stone hoe Craft charcoal Craft smoker Craft stone sword Craft furnace Craft torch Craft an iron shovel Craft an iron pickaxe Craft an iron axe Craft an iron hoe Craft bucket Craft hopper Craft rail Craft an iron sword Craft shears Craft smithing table Craft tripwire hook Craft chain Craft an iron bars Craft an iron nugget Craft blast furnace Craft stonecutter Sub-Goal Num. 6 5 5 5 4 3 5 4 4 4 8 10 10 8 9 9 8 9 8 13 13 13 13 13 14 13 12 12 12 13 13 12 12 14 13 SR 100.00 100.00 97.22 100.00 100.00 100.00 96.67 96.97 100.00 100.00 90.32 91.43 91.18 91.43 91.43 96.77 90.91 100.00 90.32 83.33 80.00 57.58 60.00 72.97 65.71 63.33 71.88 63.64 37.84 40.00 36.11 35.00 45.95 40.54 60.00 Eval Times 40 39 36 35 30 30 30 30 30 30 31 35 34 35 35 31 33 30 31 30 30 33 30 37 35 30 32 33 37 30 36 40 37 37 23 Table 10: The results of Optimus-3 on the Gold group, Diamond Group, Redstone Group, and Armor Group. Group Gold Diamond Redstone Armor Task Craft golden shovel Craft golden pickaxe Craft golden axe Craft golden hoe Craft golden sword Smelt and craft golden ingot Craft diamond shovel Craft diamond pickaxe Craft diamond axe Craft diamond hoe Craft diamond sword Dig down and mine diamond Craft jukebox Craft piston Craft redstone torch Craft an activator rail Craft compass Craft dropper Craft note block Craft shield Craft iron chestplate Craft iron boots Craft iron leggings Craft iron helmet Craft diamond helmet Craft diamond chestplate Craft diamond leggings Craft diamond boots Craft golden helmet Craft golden leggings Craft golden boots Craft golden chestplate Sub Goal Num. 16 16 16 16 16 15 15 15 16 15 15 15 15 16 16 18 23 16 16 14 14 14 14 14 17 17 17 17 17 17 17 17 SR 9.09 5.41 10.81 12.50 5.41 16.67 16.67 13.95 11.36 28.13 14.63 16.67 5.56 26.42 29.58 23.44 31.48 27.27 27.08 60.00 46.67 65.00 13.33 46.67 15.52 20.00 6.25 8.89 10.00 4.00 4.35 4.55 Eval Times 77 37 37 32 37 42 42 43 44 32 41 30 36 53 71 64 54 44 48 30 30 60 30 30 58 50 48 45 50 50 46"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen",
        "Peng Cheng Laboratory"
    ]
}