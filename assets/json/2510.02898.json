{
    "paper_title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
    "authors": [
        "Lorenzo Bianchi",
        "Giacomo Pacini",
        "Fabio Carrara",
        "Nicola Messina",
        "Giuseppe Amato",
        "Fabrizio Falchi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present Patch-ioner, a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ ."
        },
        {
            "title": "Start",
            "content": "ONE PATCH TO CAPTION THEM ALL: UNIFIED ZEROSHOT CAPTIONING FRAMEWORK Lorenzo Bianchi*1,2 Giacomo Pacini*1,2 Fabio Carrara1 Nicola Messina1 Giuseppe Amato1 1CNR-ISTI Fabrizio Falchi1 2Universit√† di Pisa 5 2 0 O 6 ] . [ 2 8 9 8 2 0 . 0 1 5 2 : r *Equal contribution"
        },
        {
            "title": "ABSTRACT",
            "content": "Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired imagetext data. To caption an image, they proceed by textually decoding text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present Patch-ioner, unified framework for zero-shot captioning that shifts from an image-centric to patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and newly introduced trace captioning task, highlighting the effectiveness of patchwise semantic representations for scalable caption generation. Project page at paciosoft.com/Patch-ioner/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Image captioning is one of the most representative tasks in vision-language understanding and has reached incredible accuracy thanks to the availability of pre-trained vision-language backbones and large paired image-text datasets. In its basic formulation, captioning model takes full image as input and autonomously decides which elements must be described and up to what degree. To enable user guidance and produce more targeted descriptions, some previous works proposed region-level captioning methods (Johnson et al., 2016; Cornia et al., 2019), which take as an additional input spatial indication e.g., bounding boxes specifying which image regions have to be described and, possibly, in which order. These region-level captioning methods require expensive manually labeled data to fully supervise the model. Indeed, each sequence or set of bounding boxes for given image should correspond to manually written ground-truth caption describing those objects. This fully supervised solution does not scale properly. In this paper, we propose perspective shift that enables us to perform region-level captioning with arbitrary spatial granularity from single image patch up to the entire image in zero-shot fashion, i.e., without requiring any form of image-level or region-level supervision. Specifically, instead of relying on the idea that the subject of captioning method is the image then potentially conditioned on set of sub-regions we instead build on two straightforward yet powerful ideas: i) the simplest element that we could caption is patch, the atomic element of an image representation in modern architectures based on vision transformers (Dosovitskiy et al., 2021), and ii) we can easily aggregate multiple patch representations to produce descriptions for arbitrarily large and also potentially not contiguous image regions. We present Patch-ioner, zero-shot captioning framework implementing these ideas. Our formulation offers maximum flexibility in zero-shot captioning tasks, producing models that effortlessly generate captions for various aggregations of"
        },
        {
            "title": "Preprint",
            "content": "image patches, ranging from individual patches to larger image regions, up to providing caption for the entire image. Despite the powerful perspective change that defines the patch as the new captioning unit, the problem is now entangled in simple yet critical question: how can we craft model able to provide patch-level captions without relying on any direct patch-level ground truth supervision? In the last years, large pre-trained vision-language foundation models like CLIP (Radford et al., 2021; Jia et al., 2021; Li et al., 2023a) solved many downstream tasks in zero-shot or even trainingfree configurations. In particular, contrastively learned vision-language representations enabled impressive results in zero-shot settings in image classification (Radford et al., 2021; Zhai et al., 2022), open-vocabulary detection (Zhou et al., 2022; Minderer et al., 2023), and segmentation (Ghiasi et al., 2022; Liang et al., 2023), or text-image retrieval (Kordopatis-Zilos et al., 2025). Image captioning, however, cannot directly employ CLIP machinery at inference time to generate text, given that CLIP is inherently discriminative and not generative approach. Only recently, image captioning models became zero-shot by decoupling image encoding where pre-trained discriminative models like CLIP are used to create proper image and text representations from the actual generative module. This is the case for models like Nukrai et al. (2022); Li et al. (2023b); Gu et al. (2023); Zeng et al. (2024); Fei et al. (2023); Yan et al. (2025); Zeng et al. (2025); Tewel et al. (2022); Su et al. (2022), which i) employ CLIP to leverage shared vision-language semantic space, and ii) train text decoder on solely text samples to recover the text back from the CLIP textual feature. This requires nothing more than pre-trained contrastive model and large set of sole text samples to craft powerful captioner. In this paper, we show that our framework can generalize this core idea and that, under this formulation, many zero-shot captioners, paired with the right components, can be easily restructured to perform zero-shot region-based captioning. Therefore, we identify and study in detail the most critical components of this novel zero-shot region-based captioning framework. Particularly, we focus our attention on the pre-trained vision-language contrastive backbone, which should be able, unlike CLIP, to create meaningful patch representations. To this aim, we largely explore DINO-based (Caron et al., 2021; Oquab et al., 2024) variants, having better localized capabilities than CLIP. We further expand the study of our framework, addressing multiple modality-gap mitigation strategies that help the text decoder to be trained only on text without having access to paired image-text features, as well as studying different patch aggregation methods. By studying existing components and employing vision backbones able to output patch-level meaningful representations like DINO, we show that we can enable many zero-shot captioners to reach state-of-the-art or comparable results in many zero-shot captioning task variants requiring captioning sub-parts of the entire image dense captioning (Johnson et al., 2016), region-set captioning (Cornia et al., 2019), up to the standard image captioning (Tewel et al., 2022) where the region to caption extends over the entire image. To better showcase the effectiveness of our framework in extreme patch-based captioning scenarios, we also introduce the zero-shot trace captioning task, which requires generating caption for region within an image specified by mouse trace. To summarize, we propose the following contributions: a) We reformulate captioning by shifting perspective from the image-to-caption approach to patch-to-caption one, unifying local and global tasks in Patch-ioner, single framework which does not require region-level supervision, b) we repurpose existing models to work within this novel framework, by analyzing the role of the key components, with special attention to the vision backbone, c) we show the performance of these models on four zero-shot captioning tasks that span different region granularity, from captioning few patches to the whole image, showing the effectiveness of the proposed perspective shift proposed by our framework despite its overall simplicity."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Language-aligned Dense Image Representations are crucial for our goal of captioning at patch level. Vision-language models (VLM) like CLIP (Radford et al., 2021) introduced powerful approach to learning global modality representations in shared space via contrastive learning, paving the way to solve several downstream tasks, including captioning (Mokady et al., 2021; Cornia et al., 2024). However, in zero-shot settings, CLIP-like representations are known to struggle with dense tasks"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Patch-ioner: patch-centric framework for unified zero-shot captioning. A. Overview of our framework. First, we extract language-aligned dense patch embeddings from the image using VLM. Given region, we select the underlying patches and aggregate their features to obtain region representation. Finally, we obtain the region caption by applying zero-shot text decoder, that is a) conditioned on the latent region representation, b) trained on text-only data, and c) equipped with mechanism to handle the modality gap present in vision-language common spaces. This enables regional captioning without requiring region-level supervision. B. By aggregating patch-level features from arbitrary image regions, we can flexibly handle multiple captioning tasks across spatial granularities in unique model. due to misalignment between local visual patches and fine-grained semantics (Zhong et al., 2022; Ranasinghe et al., 2023; Bica et al., 2024). On the other hand, visual-only self-supervised models (SSM) like DINO (Caron et al., 2021; Oquab et al., 2024) excel in local semantic modeling but lack bridge with language. Recent works like SILC (Naeem et al., 2024) and DINO.txt (Jose et al., 2024) attempt to get the best of both worlds by combining DINOand CLIP-like training objectives, aiming to obtain language-aligned dense representations. INViTE (Chen et al., 2024) modifies CLIPs visual encoder by zeroing the attention weights from each patch to all the others in the last layers, leading to more semantic patch features. DenseCLIP (Rao et al., 2022) and RegionCLIP (Zhong et al., 2022) extend CLIP with additional region-level supervision. RegionCLIP leverages region-proposal network to construct region-text pairs from image-text datasets, while DenseCLIP introduces pixel-to-text matching loss to strengthen the alignment between local regions and textual concepts. Other methods instead exploit already existing VLMs and SSMs to get the same properties with minimal or no training: Talk2DINO (Barsellotti et al., 2024) connects language to the DINOv2 space by mapping CLIP textual representations to DINOv2 patches. ProxyCLIP (Lan et al., 2024) instead leverages DINOs attention maps to improve the local properties of the CLIP visual embeddings of patches. Zero-shot Image Captioning methods rely mostly on global CLIP representations to guide text generation. Early-guided decoding methods take CLIP visual features as input and introduce adaptation techniques to reduce the visual-textual modality gap (Liang et al., 2022). DeCap (Li et al., 2023b) projects CLIP visual features into more text-aligned space using memory of texts as basis, while CapDec (Nukrai et al., 2022) and CLOSE (Gu et al., 2023) inject noise during text-only training to enable decoding also from the CLIP visual space. To improve the generation ViECap (Fei et al., 2023), MeaCap (Zeng et al., 2024), MERCap (Zeng et al., 2025) and EntroCap (Yan et al., 2025) leverage external knowledge to condition the decoding together with the CLIP image representation. Late-guided decoding methods instead use CLIP as scoring or optimization signal rather than direct input. ZeroCap (Tewel et al., 2022) leverages CLIP gradients to steer the cached context during"
        },
        {
            "title": "Preprint",
            "content": "text generation, while MAGIC (Su et al., 2022) optimizes token selection based on CLIP similarity scores. However, all the above approaches rely on global representations, which are not well-suited for capturing localized semantic details, making them suboptimal for patch-level or region-level captioning in zero-shot settings. Region-level Captioning comprises several tasks in which models are asked to produce natural language descriptions based on sub-parts of an image. They pose additional challenges as naively captioning the cropped regions or feature maps often induces loss of the global context of the image and, thus, misinterpretation of the region. For this reason, zero-shot solutions to this family of problems are still underexplored. For controllable captioning (Cornia et al., 2019) the generation of an image caption controlled by set or sequence of regions and dense captioning (Johnson et al., 2016) the localization and captioning of salient regions of an image state-of-the-art solutions like CAG-Net (Yin et al., 2019), GRiT (Wu et al., 2024), ControlCap (Zhao et al., 2024), and FlexCap (Dwibedi et al., 2025) provide good performance but need supervision with ground-truth boxes. Recent works (Guo et al., 2024), (Hua et al., 2025) moved towards the direction of arbitrary regions captioning exploiting region-level supervision, and the Localized Narratives dataset PontTuset et al. (2020a) comprising images, timed captions, and timed mouse tracks provide the ingredients for evaluating captioning also at trackor patch-level. We propose unique framework to tackle captioning at various granularities, from imageto patch-level, in zero-shot setting."
        },
        {
            "title": "3.1 MOTIVATION AND FORMULATION",
            "content": "Learned region fusion requires regional data. Traditional regional captioning (Johnson et al., 2016; Dwibedi et al., 2025; Zhao et al., 2024) follows an early injection of region specification in the model. Formally, given an image and region R, the region caption is modeled as = D(I, R). Such models require region-caption annotations and often use dedicated models or losses per captioning task or granularity. step forward can be moved by introducing formulation which disentangles image encoding and postpones region specification, defining = D(œà(I), R), where œà(I) provides visual representation of the image independent of the region R, and performs late region selection and text decoding. In order to avoid training the whole pipeline using region-level labels, we further decompose the decoder module D() into two distinct modules: parameter-free fixed aggregation aggR of patch-level representations, and an actual text decoder œï() not directly conditioned on regions R, so that = œï(aggR(œà(I), R)). We will detail these two factorized components in the following paragraphs. Parameter-free patch aggregation. Let RHW 3 be an image split into non-overlapping patches of size . Each patch is encoded with vision backbone œàv, yielding dense grid of patch-level embeddings = œàv(I) = {vi} D, where vi RD. Assuming that œàv(I) extracts this spatial grid of patch embeddings, region definition selects subset of patch features that we aggregate to obtain the region embedding vR = aggR(œàv(I)). We describe this aggregation as vS = (cid:80) iS wivi, where is the set of indices of patches that underlie the region, and wi are aggregation weights. While several aggregation functions can be considered (e.g., uniform, gaussian, attention-based), we found that the specific choice has limited impact in regional captioning (see SM8) and report results with mean aggregation (wi = 1/S). Using set operator as aggregation gives us the flexibility to aggregate arbitrary sets of patches, and thus, define regions as boxes, masks, traces, single patches, or full-image grids. Empirically, we also find that not all œàv are suitable to extracting patch-level meaningful visual representations; transformer architectures pretrained with dense local contrastive objectives (such as DINO) are significantly more robust at the patch level, as validated in 4.2. Zero-shot decoding. We train text decoder œï : RD using prefix language modeling approach, where the decoder reconstructs caption from its text embedding œàt(t). If œàt(t) is aligned to the visual encoder œàv(I), we could directly decode the visual embeddings using text-only decoder trained on text-only data. In such case, we can finally decode the region representation into natural language caption = œï(vR). However, the assumption that œï() can digest features from œàv(I) while being trained to reconstruct features from œàt(t) is often too optimistic due to the"
        },
        {
            "title": "Preprint",
            "content": "prominent modality gap (Liang et al., 2022). In fact, text and image representations, despite being in the same multimodal space, occupy different, separated subspaces. To obtain decoder applicable to visual embeddings, we experimented with two mitigation strategies for this gap. The first strategy, following Li et al. (2023b), introduces projection step at inference that maps visual features into the text subspace using memory of text embeddings. The second, inspired by Gu et al. (2023) and Nukrai et al. (2022), trains the decoder under input perturbations, encouraging robustness so that it can directly process visual embeddings. We analyze the effects of the modality gap mitigation strategies in SM9. Overall, our formulation offers three main advantages: a) it is zero-shot by design, in the sense that it only requires image-level textual descriptions to be trained and does not require paired image-text samples to train the text decoder, b) any region (e.g., whole image, box, mask, free-form trace, single point) is addressed identically, supporting modular, general-purpose regional captioning pipeline, and c) our method requires only single forward pass of the vision backbone to extract patch features for an entire image, which can then be reused to caption multiple regions without rerunning the full pipeline for each one."
        },
        {
            "title": "3.2 FROM PATCHES TO REGIONS",
            "content": "Building on patch-level zero-shot captioning, our framework can generate captions for arbitrary regions of an image. region is defined as set of patches, and its representation is obtained by averaging the embeddings of its constituent patches. This simple formulation unifies several existing region-level captioning tasks, which differ only in how the relevant patches are selected, allowing us to address them without task-specific modifications. In the following, we describe the tasks considered in our evaluation and their induced patch selections. Image Captioning involves generating single caption that describes the entire image. To achieve this, we derive global representation vI = avgi(vi) by aggregating the feature embeddings of all patches {vi} within the image I. Dense Captioning requires locating salient regions in an image and generating their descriptions. Following defined evaluation protocols (Johnson et al., 2016), we focus on captioning already defined boxes, effectively removing the localization subtask, which can be tackled using additional regionproposal models. Given bounding box and the set SB of indexes of patches that intersect with B, we obtain the representation of the region vB = avgiSB (vi). Region-set Captioning consists of generating single caption for multiple regions within an image, where each region is specified by distinct bounding box. Given an image and set of bounding boxes = {B1, B2, . . . , BK}, we define SBk as the set of patches that intersect with the k-th bounding box in B. To represent the entire set of regions, we aggregate the feature embeddings from all selected patches across all bounding boxes, which results in combined region-level representation vB ="
        },
        {
            "title": "1\nB‚ààB |SB|",
            "content": "(cid:80) (cid:88) (cid:88) vi . BB iSB (1) Note that if patch appears in more than one box, it is weighted more in the average. Trace Captioning. To demonstrate the flexibility of our approach, we introduce Trace Captioning, novel task in which the region of interest is specified by mouse trace = {p1, . . . , pL} with points. Each point pj is mapped to the corresponding patch index ij, yielding the sequence ST = [i1, . . . , iL]. The trace-level representation is then obtained by averaging the embeddings of the selected patches vT ="
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) j=1 vij . (2) Unlike box-based settings, this formulation allows for free-form, user-specified regions and thus enables interactive and fine-grained localized descriptions, expanding the scope of region-level captioning."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we first quantitatively assess the role of the backbone in our novel framework (4.2). Then, we measure the performance of our new formulation over state-of-the-art zero-shot captioning methods, evaluating them region-specific zero-shot tasks (4.3). Other interesting yet less influential studies like the role of the modality-gap mitigation strategy and the patch aggregation operator aggR() are available in SM 8 and 9. To create common ground, we first introduce the employed datasets and metrics in the following section."
        },
        {
            "title": "4.1 DATASETS AND METRICS",
            "content": "Trace Captioning. We build benchmark for Trace Captioning exploiting Localized Narratives (Pont-Tuset et al., 2020b) dataset in which annotators vocally described objects in images while moving the mouse pointer over the described object. The dataset provides temporal annotated voice transcriptions and mouse traces for the images of many standard captioning datasets. We took the labeled COCO (Lin et al., 2014; Chen et al., 2015) with splits defined in Karpathy & Fei-Fei (2015) and Flickr30K1(Young et al., 2014) test subsets to build the trace captioning evaluation datasets. We split long traces and transcriptions for each image into sentences, and we discard the parts of the traces that are not temporally located between the start and the end of each sentence. We discarded noisy sentences such as the ones describing image properties (The image is blurred, the image is edited, ...) and rewrote each sentence removing uncertainties typical of voice descriptions in more concise and caption-like style through few-shots prompted LLM (LLama 3 Dubey et al. (2024)). After annotation cleaning, 51 COCO images resulted without clean sentences and were discarded. Each sub-trace and relative sentence comprise an independent sample, that is, we ignore the temporal information of consecutive sentences and focus on evaluating the description of each sub-trace only. Samples and additional details are available in 10. Dense Captioning. We assess the performance on dense captioning tasks following the evaluation procedure of Johnson et al. (2016), omitting the bounding box proposal and evaluating only the bounding box captioning task, using ground-truth boxes as input for the models. In addition to standard caption metrics, for this task, we also report the mAP as originally defined by Johnson et al. (2016). We use the Visual Genome (VG) v1.2 (Johnson et al., 2016; Krishna et al., 2017) and VG-COCO test splits (Li et al., 2019). The former comprises 5000 images from VG, while the latter contains 2476 images present in both VG and COCO. Both contain multiple bounding box annotations per image with descriptions. Region-Set Captioning We follow the evaluation protocol of Cornia et al. (2019) that originally introduced region-set captioning. We use the Flickr30K Entities (Plummer et al., 2015) and the COCO Entities (Cornia et al., 2019) datasets. Each record comprises an image, set of bounding boxes of variable length, and ground-truth controlled caption. We evaluate on the test splits, comprising of images in the Karpathy & Fei-Fei (2015) test splits, that consist of 3569 and 1000 images for COCO and Flickr30k versions, respectively. Image Captioning We follow the standard evaluation pipeline for zero-shot image captioning, generating captions for the 5000 images in Karpathys COCO test split. We compare with these state-of-the-art models: DeCap (Li et al., 2023b), CLOSE (Gu et al., 2023), ZeroCap (Tewel et al., 2022), MAGIC (Su et al., 2022), ViECap (Fei et al., 2023), CapDec (Nukrai et al., 2022), EntroCap (Yan et al., 2025), MeaCap2 (Zeng et al., 2024), and MERCap (Zeng et al., 2025). For DeCap, ViECap, and MeaCap, we also report the best results reproduced by us. Metrics. All datasets used in our evaluation provide ground-truth caption for each annotation. We therefore adopt standard captioning metrics to assess the similarity between generated and reference 1Licenses for COCO and Visual Genome are CC-BY 4.0. Flickr30k images are under the terms of Flickrs user agreement. 2We picked the MeaCapInvLM , which uses GPT-2 decoder and achieves the highest scores in the zero-shot captioning task."
        },
        {
            "title": "Preprint",
            "content": "Captioning Task: (Dataset) Trace (COCO) Dense (VG v1.2) Region-Set (COCO Entities) Image (COCO)"
        },
        {
            "title": "Backbone",
            "content": "CLIP CLIP B/16 DenseCLIP CLIP B/16 INViTE CLIP B/16 ProxyCLIP DINO B/8 + CLIP B/16 ProxyCLIP DINOv2 B/14 + CLIP B/16 DINO.txt DINOv2 B/14 Talk2DINO DINOv2 B/14 10.9 18.6 13.8 16.7 16.5 23.2 27.9 75.0 75.3 76.4 75.7 75.7 78.8 78.7 10.9 19.9 16.8 15.7 15.5 23.4 31.9 74.2 75.2 77.3 76.0 76.0 78.5 78.8 41.6 51.0 43.3 41.2 40.6 91.8 109.1 78.8 77.6 78.9 78.4 78.5 86.3 87.5 42.1 28.0 21.3 28.7 27.4 67.8 69.2 CLIP-S 84.0 77.0 79.1 79.0 78.6 87.2 87. 66.2 57.3 60.6 61.7 61.0 70.8 72.8 Table 1: Vision-Language Backbones. CIDEr (C) and RefPAC-S (P) across four captioning tasks. captions. In particular, we focus on CIDEr (C) (Vedantam et al., 2015), which captures syntactic overlap, and RefPAC Score (P) (Sarto et al., 2023), more recent metric that quantifies semantic similarity independently of caption phrasing. For completeness, results with other traditionally used metrics BLEU@4 (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), and SPICE (Anderson et al., 2016) are reported in supplementary materials, as they follow the same trends. For the image captioning task, we additionally report CLIP-Score (Hessel et al., 2021), which measures the alignment between an image and its generated caption in the joint CLIP visionlanguage space. This metric is not applicable to region-set, trace, or dense captioning tasks, where captions describe local regions rather than the entire image."
        },
        {
            "title": "4.2 BACKBONE SELECTION",
            "content": "The choice of the visual backbone is crucial for our patch-centric framework, as the quality and semantic richness of the patch features directly impact the captioning performance. We tested several state-of-the-art vision-language models pre-trained without region-level supervision and evaluated their effectiveness within the framework. We tested vanilla CLIP (Radford et al., 2021), three CLIP adaptations for dense tasks DenseCLIP (Rao et al., 2022), INViTE (Chen et al., 2024), and ProxyCLIP (Lan et al., 2024) , and two methods with visual encoders based on DINOv2 (Oquab et al., 2023) DINO.txt (Jose et al., 2025) and Talk2DINO (Barsellotti et al., 2024). Patches are aggregated as described in 3.2, while for the zero-shot decoder, we align with the setting of Li et al. (2023b), and use prefix GPT-2 style decoder (SM6 reports implementation details), with memory-based latent projection approach as mitigation strategy for handling the modality gap. Specifically, before decoding, the region representation is projected into the text embedding space as similarity-weighted linear combination of memory elements, vproj = Œ± with Œ± = softmax(cid:0) 1 œÑ v(cid:1), where = [m1, . . . , mN ] stores the text embeddings mj = œàt(tj) and œÑ > 0 controls the sharpness of the weighting distribution. This choice enables us to be directly comparable with other works using the same architecture in the all the following experiments, besides also performing the best among the tested zero-shot decoder methods (see SM9). In SM8, we also study how the choice of the memory bank affects the captioning performance, providing an upper bound to metrics. Table 1 shows that backbone effectiveness in our framework is closely tied to capturing fine-grained local semantics. Standard CLIP performs poorly, indicating that its patch tokens lack the spatial detail needed for our tasks (Mukhoti et al., 2023; Ranasinghe et al., 2023; Bica et al., 2024). Backbones that strengthen CLIP local representations, such as INViTE and DenseCLIP, achieve stronger results, supporting our hypothesis. The best performance comes from DINOv2-based models, including DINO.txt and Talk2DINO, with the latter emerging as the most effective encoder. This underscores the importance of semantically rich patch-level features for high-quality region-level captions. For this reason, we show results using Talk2DINO as the default backbone in subsequent experiments."
        },
        {
            "title": "4.3 COMPARISON WITH SOTA",
            "content": "Despite significant advances in zero-shot image captioning, we are unaware of any prior methods specifically tailored for zero-shot regional captioning tasks. Existing zero-shot captioners are usually evaluated only at the level of whole images, without any mechanisms to natively attend to arbitrary"
        },
        {
            "title": "Preprint",
            "content": "regions. To rigorously quantify the benefit of our Patch-ioner framework, in Table 2, we compare against both state-of-the-art zero-shot image captioners and adapted baselines: (i) state-of-the-art whole-image zero-shot captioners in their standard setting or applied to region crops, simulating regional captioning by isolating local content, and (ii) region-supervised encoders, thus outside our no-region-label setting, that leverage mask-based (AlphaCLIP, Sun et al. (2024)) or crop-based (RegionCLIP, Zhong et al. (2022)) attention coupled with the same zero-shot decoder, allowing them to attend to specific regions. This design ensures that our evaluation covers the strongest available baselines for both image-level and region-level zero-shot captioning. Note that we do not compare with large multimodal models tackling regional understanding and captioning, such as Guo et al. (2024); Hua et al. (2025), as they are outside of our assumptions on the available supervision by training on region-level data. In addition to our strongest model (i.e., Talk2DINO with the memory-based mitigation strategy), we also report other combinations that express existing zero-shot captioning approaches (CLOSE (Gu et al., 2023), CapDec (Nukrai et al., 2022), VieCap (Fei et al., 2023), and MeaCap (Zeng et al., 2024)) but replacing the original CLIP backbone. Specifically, CLOSE and CapDec can be expressed by choosing the noise-injection mitigation strategy and using the standard decoder pipeline described in 4.2. The same applies to ViECap and MeaCap, although with the addition of extra knowledge in the text decoding step: ViECap uses external entity-aware prompts, while MeaCap leverages also structured concept retrieval from knowledge base. We use two datasets for each task COCO-derived dataset and an additional dataset such as Visual Genome (Krishna et al., 2017) or Flickr30k (Young et al., 2014). Figure 6 shows qualitative results. Patch-centric captioning excels in local, fine-grained tasks. In the trace and dense captioning tasks, which emphasize local visual content, our patch-centric framework significantly outperforms all baselines across metrics. For trace captioning (Table 2, first group), our patch-based formulation outperforms whole-image captioners and crop-based adaptations. Models relying on global CLS representations fail to capture the precise objects and attributes under the trace. Even AlphaCLIP, that is region-supervised backbone that can be naturally applied to traces, lags behind, underlying intrinsic limitations of the pretrained CLIP backbone. Dense captioning shows similar trend (Table 2, second group), with our models outperforming baselines. For this task, we report crop-based adaptations for DeCap, ViECap, and MeaCap, as they provide stronger baseline with respect to the same models applied to the global CLS (see Table 4 in SM). Although isolating regional content, these models discard broader contextual cues that are crucial for coherent dense descriptions. Patch aggregation extends seamlessy to context-aware captioning. On region-set captioning (Table 2, third group), our models once again achieves state-of-the-art results, outperforming both zero-shot baselines and region-supervised models. Note that the region-set captioning task tends to align more closely with image-level captioning rather than strictly focusing on localized regions (see Figure 6 and Figure 7 in SM), as regions are intended to control an image-level caption3. Thus, global models also tend to perform well on those tasks, showing narrower gap with respect to our new state of the art. By aggregating patch embeddings from arbitrary and possibly disjoint sets of regions, the model produces coherent and contextually rich captions that align with the collective semantics of the chosen areas. In contrast, whole-image methods, including ViECap and DeCap, cannot naturally incorporate regional cues, limiting their effectiveness in this setting. Importantly, our patch-based aggregation even surpasses AlphaCLIP, which relies on explicit mask supervision. Patch-centric models deliver comparable performance on whole-image captioning. In wholeimage captioning (Table 2, fourth group), our results remain competitive with the strongest zero-shot captioners but are slightly behind dedicated image-centric architectures such as MERCap (Zeng et al., 2025) and EntroCap (Yan et al., 2025). For this task, we report the performance of our models using an attention-based weighting, when helpful, as it usually performs marginally better than the standard average patch aggregation and thus represents the best available model for this task and reaches the smaller gap with state-of-the-art models (see SM8). Notably, adding structured external knowledge or filtered retrieval (as in ViECap or MeaCap) on noise-based decoders improves fluency 3This is expected since the ground-truth captions in the COCO Entities dataset originate from the image-level annotations of COCO, as stated by Cornia et al. (2019)."
        },
        {
            "title": "Preprint",
            "content": "Model Whole-image Zero-shot Captioners ZeroCap (Tewel et al., 2022) CVPR22 MAGIC (Su et al., 2022) ArXiv22 CLOSE (Gu et al., 2023) ICCV23 CapDec (Nukrai et al., 2022) EMNLP22 EntroCap (Yan et al., 2025) NEUCOM25 MERCap (Zeng et al., 2025) AAAI25 ViECap (Fei et al., 2023) ICCV23 MeaCap (Zeng et al., 2024) CVPR24 DeCap (Li et al., 2023b) ICLR23 With Region-level Supervision RegionCLIP (Zhong et al., 2022) CVPR22 + Mem. ( DeCap) AlphaCLIP (Sun et al., 2024) CVPR24 + Mem. ( DeCap) Patch-ioner (Our Patch-based Framework)"
        },
        {
            "title": "Dense Captioning",
            "content": "Region-Set Captioning"
        },
        {
            "title": "Image Captioning",
            "content": "COCO Flickr30k VG v1.2 VG-COCO COCO Entities Flickr30k Entities"
        },
        {
            "title": "COCO",
            "content": "Flickr30k C"
        },
        {
            "title": "P mAP",
            "content": "C"
        },
        {
            "title": "P mAP",
            "content": "C C C CLIP-S CLIP-S - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 24.3 74.4 12.0 68.8 14.90 26.4 74.3 15.2 26.6 74.3 102.7 22.5 74.4 12.6 69.8 15.01 28.6 75.1 16.0 28.9 75.1 97.9 20.5 75.3 11.2 71.0 17.75 24.6 77.8 17.8 24.9 77.7 95. - - - - - - 85.0 85.2 87.4 - - - - - - 31.8 38.6 39.4 - - 15.85 - 21.3 75.4 11.8 71.0 14.63 - 21.7 19.1 76.7 16.01 21.0 73.9 14.82 19.4 75.4 73.8 - 95.1 - 87.4 - 39. - - - - - - 74.9 76.4 78.8 - 78.8 79.1 76.5 77.8 78.6 14.6 49.3 81.2 91.8 94.3 96.0 89.7 86.0 87.4 - - - - - - 88.5 88.6 90.6 93.4 89. 91.2 91.1 - - - - - - 75.6 77.8 79.3 77.5 78.2 - 17.5 - 35.7 41.5 45.6 29.8 40.1 40.0 - - - - - - 80.6 82.8 84.8 38.8 40. 84.5 85.4 - - - - - - 70.5 73.9 76.3 73.6 75.0 88.5 90.2 76.0 39.3 84.2 71.8 65.5 86.2 67.0 88.5 89.2 73.7 34.1 82.8 69.9 83.0 89.6 74.8 39.4 84.4 71.4 27.8 80.8 70.9 T2D + Mem. ( DeCap) T2D + Noise ( CLOSE, CapDec) T2D + Noise + External knowledge ( ViECap) T2D + Noise + Filtered knowledge ( MeaCap) : reproduced by us. : Model applied to image crops. : attention-based weighting. 27.9 78.7 18.8 77.0 21.31 29.3 78.1 19.3 75.6 20.26 28.2 78.2 18.5 76.2 18.43 27.4 78.8 20.3 77.3 18.66 31.9 26.3 30.3 31.9 78.8 21.53 32.3 77.0 20.33 26.4 77.8 18.43 30.7 78.9 19.43 32.3 78.7 109.1 76.9 97.5 77.7 109.3 78.7 104.4 87.5 85.6 86.7 86. 44.1 37.1 37.8 42.3 Table 2: Comparison of our Patch-ioner framework, using Talk2DINO (T2D), with state-of-theart zero-shot captioning methods on trace, dense, region-set, and image captioning tasks. Our approach consistently outperforms whole-image and region-supervised baselines in local, fine-grained captioning tasks, while achieving competitive results on whole-image captioning. The table reports CIDEr (C), RefPAC (P), mean average precision (mAP) for dense captioning, and CLIP-Score (CLIPS) when applicable; best and second-best results are highlighted in bold and underline, respectively."
        },
        {
            "title": "Dense",
            "content": "Region-Set"
        },
        {
            "title": "Image",
            "content": "DeCap: cat is sleeping on cluttered desk. Ours (CLIP + Mem.): cat is sitting on the bed and its contents. Ours (Talk2DINO + Mem.): plant in vase sitting on table. DeCap: giraffe in zoo with city in the background. Ours (CLIP + Mem.): there are some people that are out by lot of trees. Ours (Talk2DINO + Mem.): view of city with sky in the background. GT: sky. DeCap: woman squatting on bench with cat. Ours (CLIP + Mem.): there is person that is out on the kitchen. DeCap (Crop): close up of person standing by person holding phone. Ours (Talk2DINO + Mem.): black cat is sitting on black bench. GT: black cat sitting on bench. DeCap: baseball player at bat getting ready to hit the ball. Ours (CLIP + Mem.): some baseball players are on the field playing baseball. Ours (Talk2DINO + Mem.): baseball player is swinging his bat as crowd watches. GT: man swinging baseball bat as another looks on. DeCap: bench sits on the beach next to the ocean. ZeroCap: beachfront bench. ViECap: wooden bench sitting on top of sandy beach. Ours (Talk2DINO + Mem.): bench sitting on the beach next to the ocean. GT: wooden bench sitting on beach. Figure 2: Qualitative results from finer (left) to coarser (right) tasks. Note the discrepancy of predicted and ground-truth captions when an image-level (DeCap, DeCap (Crop)) or CLIP-based regional (CLIP + Mem.) captioner is applied, with respect to our Talk2DINO-based model. and informativeness, suggesting such modules are complementary than substitutes for strong regional semantics. However, they compare similarly to the model using the memory-based decoder."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "We introduced Patch-ioner, novel zero-shot captioning framework that shifts from an imagecentric to patch-centric approach, enabling caption generation for individual patches and arbitrary aggregations without any region or even image supervision. We rely on the strong spatial awareness of DINOv2, whose local image patches have been effectively bridged with the text modality. Thanks to the disentangled training of the decoder network, this flexible and scalable method sets the new state of the art on various regional captioning tasks, including dense and region-based captioning, as well as our newly proposed trace captioning. Despite its simplicity, results show that our patch-centric approach can effectively bridge the gap between local and global understanding in image captioning, providing unified framework for multigranularity captioning tasks in zero-shot setting. Moreover, our models require single backbone forward pass to caption multiple regions, facilitating practical viability in interactive applications. Limitations and Future Work. Despite strong zero-shot performance, our model still lags behind fully supervised, task-specific approaches. The contextual scope of each patch is fixed by the backbone and is not adjusted to meet the user intents. On top of that, the modality jump introduces noise that can cause hallucinations. Future work could incorporate weak supervision, e.g., image-level"
        },
        {
            "title": "Preprint",
            "content": "captioning loss, to improve patch-level semantics in contrastively-learned representations, or refine the patch-to-text projection to further reduce the modality gap in zero-shot settings. Acknowledgments This work was partially supported by the following projects: SUN Social and hUman ceNtered XR (101092612), NextGenerationEU (FAIR PE00000013, ITSERR B53C22001770006 and MUCES B53D23026090001)."
        },
        {
            "title": "REFERENCES",
            "content": "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, pp. 382398. Springer, 2016. 7 Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. 7 Luca Barsellotti, Lorenzo Bianchi, Nicola Messina, Fabio Carrara, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, and Rita Cucchiara. Talking to dino: Bridging self-supervised vision backbones with language for open-vocabulary segmentation. arXiv preprint arXiv:2411.19331, 2024. 3, 7, 1, 2, 4 Ioana Bica, Anastasija Ilic, Matthias Bauer, Goker Erdogan, Matko Bo≈°njak, Christos Kaplanis, Alexey Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, et al. Improving fine-grained understanding in image-text pre-training. In ICML, 2024. 3, 7 Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 2, 3 Haozhe Chen, Junfeng Yang, Carl Vondrick, and Chengzhi Mao. Interpret and control visionlanguage models with text explanations. In B. Kim, Y. Yue, S. Chaudhuri, K. Fragkiadaki, M. Khan, and Y. Sun (eds.), International Conference on Representation Learning, volume 2024, pp. 35589 35608, 2024. URL https://proceedings.iclr.cc/paper_files/paper/2024/file/ 99120406e3c273c63dc7d3059a516b1a-Paper-Conference.pdf. 3, 7, 1 Invite: Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and Lawrence Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv preprint arXiv:1504.00325, 2015. 6 Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Show, Control and Tell: Framework for Generating Controllable and Grounded Captions. In CVPR, 2019. 1, 2, 4, 6, 8 Marcella Cornia, Lorenzo Baraldi, Giuseppe Fiameni, and Rita Cucchiara. Generating more pertinent captions by leveraging semantics and style on multi-source datasets. IJCV, 132(5):17011720, 2024. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021. 1 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 6 Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman, and Yusuf Aytar. Flexcap: Describe anything in images in controllable detail. NeurIPS, 37:111172111198, 2025. 4 Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, Chengjie Wang, and Feng Zheng. Transferable decoding with visual entities for zero-shot image captioning. In ICCV, pp. 31363146, 2023. 2, 3, 6, 8, 9, 1 Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, pp. 540557. Springer, 2022. 2 Sophia Gu, Christopher Clark, and Aniruddha Kembhavi. cant believe theres no images! learning visual tasks using only language supervision. In ICCV, pp. 26722683, 2023. 2, 3, 5, 6, 8, 9, 4, 10 Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1379613806, 2024. 4,"
        },
        {
            "title": "Preprint",
            "content": "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In EMNLP, 2021. 7 Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Soo Ye Kim, Zhifei Zhang, Yilin Wang, Jianming Zhang, Zhe Lin, and Jiebo Luo. Finecaption: Compositional image captioning focusing on wherever you want at any granularity. In CVPR, pp. 2476324773, 2025. 4, 8 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pp. 49044916. PMLR, 2021. 2 Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In CVPR, pp. 45654574, 2016. 1, 2, 4, 5, Cijo Jose, Th√©o Moutakanni, Dahyun Kang, Federico Baldassarre, Timoth√©e Darcet, Hu Xu, Daniel Li, Marc Szafraniec, Micha√´l Ramamonjisoa, Maxime Oquab, et al. Dinov2 meets text: unified framework for image-and pixel-level vision-language alignment. arXiv preprint arXiv:2412.16334, 2024. 3, 4 Cijo Jose, Th√©o Moutakanni, Dahyun Kang, Federico Baldassarre, Timoth√©e Darcet, Hu Xu, Daniel Li, Marc Szafraniec, Micha√´l Ramamonjisoa, Maxime Oquab, et al. Dinov2 meets text: unified framework for imageand pixel-level vision-language alignment. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2490524916, 2025. 7, 1 Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, pp. 31283137, 2015. 6 Giorgos Kordopatis-Zilos, Vladan Stojnic, Anna Manko, Pavel ≈†uma, Nikolaos-Antonios Ypsilantis, Nikos Efthymiadis, Zakaria Laskar, JiÀár√≠ Matas, OndÀárej Chum, and Giorgos Tolias. ILIAS: Instance-level image retrieval at scale. In CVPR, 2025. 2 Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:3273, 2017. 6, Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation. In ECCV, 2024. 3, 7, 1 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML, 2023a. 2 Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang. Decap: Decoding CLIP latents for zero-shot captioning via text-only training. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/forum? id=Lt8bMlhiwx2. 2, 3, 5, 6, 7, 9, 1, 4 Xiangyang Li, Shuqiang Jiang, and Jungong Han. Learning object context for dense captioning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 86508657, 2019. 6 Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In CVPR, pp. 70617070, 2023. 2 Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. NeurIPS, 35:1761217625, 2022. 3, 5, 4 Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013/. 7 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In ECCV, 2014. Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. NeurIPS, 36:7298373007, 2023. 2 Ron Mokady, Amir Hertz, and Amit Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021."
        },
        {
            "title": "Preprint",
            "content": "Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip HS Torr, and Ser-Nam Lim. Open vocabulary semantic segmentation with patch aligned contrastive learning. In CVPR, pp. 1941319423, 2023. 7 Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, and Federico Tombari. Silc: Improving vision language pretraining with self-distillation. In ECCV, pp. 3855. Springer, 2024. 3 David Nukrai, Ron Mokady, and Amir Globerson. Text-only training for image captioning using noiseinjected CLIP. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 4055 4063. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-EMNLP.299. URL https://doi.org/10.18653/v1/2022.findings-emnlp.299. 2, 3, 5, 6, 8, 9, 4 Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning Robust Visual Features without Supervision. arXiv preprint arXiv:2304.07193, 2023. 7, 4 Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv√© J√©gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024, 2024. URL https://openreview.net/forum?id=a68SUt6zFt. 2, 3 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 26412649, 2015. 6 Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, pp. 647664. Springer, 2020a. 4, 5 Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV, pp. 647664. Springer, 2020b. 6, 8 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In ICML, 2021. 2, 7, Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Perceptual Grouping in Contrastive Vision-Language Models. In ICCV, 2023. 3, 7 Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1808218091, 2022. 3, 7, 1 Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Positive-augmented contrastive learning for image and video captioning evaluation. In CVPR, pp. 69146924, 2023. 7 Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel Collier. Language models can see: Plugging visual controls in text generation. arXiv preprint arXiv:2205.02655, 2022. 2, 4, 6, Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-clip: clip model focusing on wherever you want. In CVPR, pp. 1301913029, 2024. 8, 9 Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic. In CVPR, pp. 1791817928, 2022. 2, 3, 6, 9, 10 Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pp. 45664575, 2015. Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: generative region-to-text transformer for object understanding. In ECCV, pp. 207224. Springer, 2024."
        },
        {
            "title": "Preprint",
            "content": "Jie Yan, Yuxiang Xie, Shiwei Zou, Yingmei Wei, and Xidao Luan. Entrocap: Zero-shot image captioning with entropy-based retrieval. Neurocomputing, 611:128666, 2025. 2, 3, 6, 8, 9 Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, and Jing Shao. Context and attribute grounded dense captioning. In CVPR, pp. 62416250, 2019. 4 Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the association for computational linguistics, 2:6778, 2014. 6, 8 Delong Zeng, Ying Shen, Man Lin, Zihao Yi, and Jiarui Ouyang. Zero-shot image captioning with multi-type entity representations. In AAAI, volume 39, pp. 2230822316, 2025. 2, 3, 6, 8, 9 Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen, Bo Chen, and Zhengjue Wang. Meacap: Memory-augmented zero-shot image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1410014110, 2024. 2, 3, 6, 8, 9, 1 Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In CVPR, pp. 1812318133, 2022. 2 Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye, and Fang Wan. Controlcap: Controllable region-level captioning. In ECCV, pp. 2138. Springer, 2024. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. RegionCLIP: Region-Based Language-Image Pretraining. In CVPR, 2022. 3, 8, 9 Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr√§henb√ºhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In ECCV, pp. 350368. Springer, 2022."
        },
        {
            "title": "SUPPLEMENTARY MATERIAL",
            "content": ""
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "For the training of the textual decoder of the memory-based configuration œï, we adopt prefix GPT2-style decoder-only Transformer with 4 attention heads and 4 layers, following the architecture used by Li et al. (2023b). We train the model on captions from the COCO training set, which also serves as the memory bank for the projection mechanism, comprising approximately 500k texts. We set the hyperparameters of the projection mechanism as in DeCap (œÑ = 0.01), and use the AdamW optimizer with weight decay of 0.01. Training proceeds for 10 epochs with learning rate of 105 and batch size of 64. comprehensive overview of the framework operating in the DeCap setting (with the projection mechanism as modality gap mitigation strategy) is provided in Figure 3. For the external knowledge-based captioning models we performed training of 15 epochs with batch size of 80 captions on the same GPT2-style textual decoder using learning rate of 2 105 and gaussian noise variance of 16 103 to replicate the experimental settings of Fei et al. (2023) and Zeng et al. (2024). All experiments were conducted on single NVIDIA H100 GPU with 80GB of HBM3 memory. Training took approximately 25 minutes per epoch."
        },
        {
            "title": "7 BACKBONE DETAILS",
            "content": "We briefly summarize here the characteristics of the vision-language models we tested in our framework. CLIP (Radford et al., 2021): foundational model that learns shared embedding space for images and text through contrastive learning. While being the most used model for global image-text alignment, its patch tokens are known to lack strong spatial and fine-grained semantic information. The input resolution of its training is 224 pixel. DenseCLIP (Rao et al., 2022): fine-tuned version of CLIP that incorporates pixel-text matching loss to enhance the models ability to understand local regions. The official implementation input resolution is 640 pixel for the ViT-B/16 version. INViTE (Chen et al., 2024): This method modifies CLIPs vision transformer to bring patch tokens in the text space by disabling the self-attention mechanism. It employs the same visual encoder of CLIP, trained at 224 pixel input resolution. ProxyCLIP (Lan et al., 2024): model that leverages the local understanding of DINO backbone to improve CLIPs patch-level representations. It achieves this by replacing the attention maps in CLIPs final layer with DINOs attention maps, effectively transferring DINOs fine-grained spatial awareness to the CLIP embedding space. The DINO ViT-B/8 version was tested with images at 296 pixel resolution, while the DINOv2 ViT-B/14 at 518 pixel. DINO.txt (Jose et al., 2025): This model builds upon frozen DINOv2 backbone, adding learnable transformer blocks on top. It is then trained with contrastive objective against text encoder to align both global and patch-level representations with language. The DINOv2 backbone was trained at the resolution of 518 pixel. Talk2DINO (Barsellotti et al., 2024): This model creates bridge between the CLIP and DINOv2 embedding spaces. It trains projection to map CLIP text embeddings into the DINOv2 patch space, using DINOv2s highly meaningful attention maps to identify and align with the most relevant patches during training. The DINOv2 backbone was trained at the resolution of 518 pixel."
        },
        {
            "title": "TEXT COLLECTION",
            "content": "We tested several patch aggregation strategies and input resolutions for our model and the other baselines."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Memory-based Patch-level captioning. Given an input image, we first extract dense patch-level representations using vision transformer backbone. For selected patch, we apply the projection-based mechanism introduced by Li et al. (2023b) to mitigate the modality gap and align its representation with the text embedding space. Finally, the transformed embedding is fed into text decoder trained on text-only corpus, generating zero-shot caption for the patch. Patch Aggregation. tion functions for merging the vi in selected set of visual patches: In cases where we are not captioning single patch, we test different aggregaa) uniform, the average box patch representations; b) gaussian, for rectangular configurations of contiguous patches i.e., either the full image or bounding box; we consider weighted average of patches representations where central patches weigh more; specifically, we assign to each patch (a, b) coordinates in uniform square grid [1, 1]2 (i.e., the top-left and bottom-right patches have (1, 1) and (1, 1) coordinates, respectively), and weight of e(a2+b2) in the average, and c) attention, weighted average of box patches representations, with patch weights defined as the average attention map of the last layer of œàv. Input Resolution. For patch-based captioning, we followed Talk2DINO Barsellotti et al. (2024) and used an input image resolution of 518x518, obtaining 37 14x14 patches per side when using the Talk2DINO backbone. The original DeCap Li et al. (2023b), ViECap Fei et al. (2023), MeaCap Zeng et al. (2024) implementations uses the CLIP B/32 backbone with 224x224 input images with 7 patches per side. We also tested with the CLIP B/16 backbone, resulting in 14 patches per side at 224x224 resolution, and with 592x592 input image size, to obtain the same number of patches as in our framework (37 per side). We report results of these additional configurations for all baselines: DeCap, ViECap, and MeaCap. While the main paper reports only the best configuration per task and model, in this section, we report and discuss the results of all the tested configurations. We perform these tests on COCO-derived datasets and on VG v1.2 for dense captioning. We highlight the rows in the tables corresponding to the configurations reported in the main paper. Trace Captioning. Table 3 reports trace captioning results. We did not apply the gaussian weighting scheme for this task, as the sparse discontinuous traces often do not identify rectangular region needed to apply this scheme. We notice that a) the simple average of the trace patches provides the best performance in our framework b) as expected, using the CLIP B16 backbone, that extracts finer patches, improves over the standard CLIP B32 backbone used in the baseline methods, and c) resolutions higher than 224 only marginally improve performance for baselines in this task. Dense Captioning. Table 4 reports the results of the dense captioning task. For our framework, changing the weighting strategy does not cause significant performance changes. The best baselines are the ones Region-based, which consist of applying the captioners to the CLS tokens of image crops specified by the bounding boxes (e.g., DeCap@224 Crop). The difference between the CLIP B/16 and B/32 versions is usually small or negligible. Region-Set Captioning. Table 5 shows the results in the region-set captioning task on COCO Entities. The gap between zero-shot image captioners and our frameworks captioners narrows in this task due to the more global nature of it, which requires the model to produce caption for the whole image while focusing on certain regions. Also in this task, the choice of weighting strategy only marginally affects the performance of the models in our framework."
        },
        {
            "title": "Model",
            "content": "Image-based DeCap@224 DeCap@224 DeCap@592 ViECap@224 ViECap@224 ViECap@592 MeaCap@224 MeaCap@224 MeaCap@592 Patch-based (Our Framework) T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 # Patches Backbone"
        },
        {
            "title": "Input Weighting",
            "content": "B S 7 14 37 7 14 37 7 14 37 37 37 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP CLIP B32 CLIP B16 CLIP B"
        },
        {
            "title": "CLS\nCLS\nCLS",
            "content": "- - - - - - - - - DINOv2 B14 DINOv2 B"
        },
        {
            "title": "Patches\nPatches",
            "content": "uniform attention 2.1 2.2 2.0 2.5 2.5 2.3 2.3 2.3 2.2 2.5 2.4 9.7 9.8 9. 9.8 9.9 9.6 9.4 9.3 9.1 21.7 21.8 21.5 22.4 22.3 22.1 21.5 20.8 20.3 21.1 21.3 20. 24.8 24.7 24.3 23.1 23.4 22.5 8.8 8.7 8.7 9.3 9.5 9.5 8.9 9.0 9.0 75.2 75.4 75. 74.1 74.5 74.4 74.2 74.6 74.4 10.7 10.4 23.2 22.7 27.9 27.6 12.6 12. 78.7 78.1 Table 3: Trace Captioning results on COCO test set."
        },
        {
            "title": "Model",
            "content": "Image-based DeCap@224 DeCap@224 DeCap@592 ViECap@224 ViECap@224 ViECap@592 MeaCap@224 MeaCap@224 MeaCap@592 Region-based DeCap@224 Crop DeCap@224 Crop DeCap@592 Crop ViECap@224 Crop ViECap@224 Crop ViECap@592 Crop MeaCap@224 Crop MeaCap@224 Crop MeaCap@592 Crop Patch-based (Our Framework) T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 # Patches Backbone"
        },
        {
            "title": "Input Weighting mAP",
            "content": "M S 7 14 37 7 14 37 7 14 37 7 14 37 7 14 37 7 14 37 37 37 37 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B"
        },
        {
            "title": "CLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS",
            "content": "- - - - - - - - - - - - - - - - - - DINOv2 B14 DINOv2 B14 DINOv2 B"
        },
        {
            "title": "Patches\nPatches\nPatches",
            "content": "uniform gaussian attention 0.15 0.14 0.15 0.13 0.14 0.14 0.13 0.13 0.13 0.17 0.18 0.14 0.15 0.16 0.12 0.15 0.16 0.12 0.21 0.22 0.21 8.40 8.48 8.37 8.25 8.30 8.17 8.04 8.01 7.86 10.03 10.33 8.30 9.32 9.59 7.83 9.64 10.03 7. 10.63 10.82 10.31 0.94 0.95 0.92 1.02 1.01 1.00 0.98 1.03 0.99 1.35 1.40 1.05 1.42 1.46 1.14 1.46 1.57 1.19 1.36 1.43 1.27 15.61 15.70 15.67 16.06 15.86 15.86 15.40 15.15 15.13 18.20 18.44 16.39 17.79 18.03 16.02 18.00 18.45 16. 19.38 19.11 18.53 24.18 23.81 23.26 23.22 23.37 22.77 23.61 24.56 17.20 26.40 27.13 20.20 28.62 30.53 21.32 9.38 9.40 9.26 9.97 9.91 9.82 9.66 9.49 9.43 10.90 11.28 7.78 10.07 10.43 7.44 10.98 11.51 7.86 73.71 73.94 73.91 73.03 73.49 73.35 72.97 73.55 73.50 77.09 77.76 75.47 74.34 75.62 73.26 75.08 76.35 73. 18.59 18.82 18.17 31.94 32.80 30.58 15.03 15.48 14.72 78.82 79.14 78.69 Table 4: Dense Captioning results on VG v1.2 test set."
        },
        {
            "title": "Model",
            "content": "Image-based DeCap@224 DeCap@224 DeCap@592 ViECap@224 ViECap@224 ViECap@592 MeaCap@224 MeaCap@224 MeaCap@592 Patch-based (Our Framework) T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 # Patches Backbone"
        },
        {
            "title": "Input Weighting",
            "content": "B S 7 14 37 7 14 37 7 14 37 37 37 37 37 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B"
        },
        {
            "title": "CLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS",
            "content": "- - - - - - - - - DINOv2 B14 DINOv2 B14 DINOv2 B14 DINOv2 B"
        },
        {
            "title": "CLS\nPatches\nPatches\nPatches",
            "content": "- uniform gaussian attention 10.1 10.0 9.6 11.2 11.3 10.8 10.4 10.1 9.3 9.1 11.5 11.6 11.0 19.0 19.4 18.6 18.2 18.3 17.8 17.7 17.5 16.9 16.9 19.3 19.6 19.0 38.0 38.3 37.5 38.9 38.6 37.9 37.0 35.5 34. 35.0 38.8 39.3 38.3 94.4 95.1 91.4 102.7 102.2 99.2 97.9 96.5 91.1 89.4 109.1 111.6 107.0 26.4 26.8 25.9 27.0 26.9 26.5 25.9 25.7 25.5 25.4 29.4 30.1 29.3 86.9 87.4 86.7 85.0 85.4 85.0 85.2 85.4 85. 85.5 87.5 87.7 87.4 Table 5: Region-Set Captioning results for COCO Entities test set."
        },
        {
            "title": "Model",
            "content": "Image-based DeCap@224 DeCap@224 DeCap@592 ViECap @224 ViECap@224 ViECap @592 MeaCap@224 MeaCap@224 MeaCap@592 Patch-based (Our Framework) T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 T2D + Mem. ( DeCap) @518 GT Memory T2D + Mem. ( DeCap) @518 GT Memory # Patches Backbone"
        },
        {
            "title": "Weighting",
            "content": "B S 7 14 37 7 14 37 7 14 37 37 37 37 37 37 37 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B16 CLIP B32 CLIP B16 CLIP B"
        },
        {
            "title": "CLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS\nCLS",
            "content": "- - - - - - - - - DINOv2 B14 DINOv2 B14 DINOv2 B14 DINOv2 B14 DINOv2 B14 DINOv2 B"
        },
        {
            "title": "CLS\nPatches",
            "content": "central patch uniform gaussian attention - attention 23.46 23.89 22.43 26.70 26.3 25.60 24.57 23.6 22.01 15.68 19.52 21.17 23.64 23.58 25.66 25.12 25.51 24.64 23.99 24.0 23.38 23.12 22.7 21. 18.46 21.49 22.62 23.93 23.54 24.77 50.06 50.34 49.25 50.85 50.3 49.52 47.68 45.5 44.72 40.84 44.88 46.62 48.54 47.71 49.83 87.40 89.64 84.57 89.67 89.5 86.84 86.66 85.1 80. 55.53 69.19 76.79 88.46 85.67 93.87 19.14 19.52 18.66 17.54 17.6 17.08 17.27 17.3 16.69 12.66 15.59 16.73 18.21 17.86 19.09 90.58 91.05 90.36 88.45 88.8 88.33 88.76 89.0 88. 84.26 87.36 88.36 90.21 89.53 90.70 Table 6: Image Captioning results on COCO test set. Image Captioning. In Table 6, we report the results of standard zero-shot image captioning. In addition to the already described weighting schemes, we test one additional configuration for our framework that is central patch, where the decoding is applied to the central patch of the image. We can observe that the most effective strategy for the image captioning task is attention. This is coherent with results from Barsellotti et al. (2024), where they suggest the attention-weighted patch means to use Talk2DINO for global tasks such as image-text retrieval. Memory Bank. Considering that in the memory-based model of our framework (that is similar to Li et al. (2023b)) we tackle the modality gap through projection based on collection of texts, we tested how much the selection of the texts in the memory bank influences performance. In Table 6, we also report the results obtained by that model when in its memory bank there are also ground-truth captions of the test set (rows marked with GT Memory). This provides sort of upper bound to performance when varying the text collection used as memory. We observe that in this configuration, the performance only slightly improves (+0.5%), indicating that the model is robust to the choice of the memory bank."
        },
        {
            "title": "NOISE",
            "content": "In this section, we quantitatively assess the performance of two state-of-the-art solutions to overcome the modality gap. In particular, we compared the configuration based on memory bank of texts the one introduced in 3 with an alternative solution based on noise injection during the decoder training. Additionally, we include in our comparison baseline with no modality gap mitigation (no mitig.), to highlight the benefits brought by each strategy. Training with Noise. Various works Gu et al. (2023); Nukrai et al. (2022) proposed zero-shot image captioning solutions based on noise injection during the training of the text decoder. Through this strategy, the trained decoders are more effective in understanding semantic representations, even when those are not coming from the text modality. To implement this strategy in our framework, we trained the textual decoder on the same collection of captions as for the memory bank-based configuration. We adopted Talk2DINO Barsellotti et al. (2024) textual space for the decoder input space, which is aligned to DINOv2 Oquab et al. (2023) with registers Jose et al. (2024). Following the setting of Gu et al. (2023), we added Gaussian noise with œÉ2 = 0.08 to the textual embeddings while leaving the other parameters unchanged (as defined in 6). In the next paragraphs, we report and compare the results for each task of Talk2DINO within our framework with the memory bank (Memory) and with the training with noise (N oise). In Table 7, we compare the two modality gap mitigation strategies across multiple captioning tasks, and also report the performance of baseline without any mitigation (no mitig.). The baseline consistently underperforms compared to both the Memory and oise configurations, indicating that, like other contrastively learned image-text encoders Liang et al. (2022), Talk2DINO is also affected by the modality gap. These results highlight the importance of explicitly addressing this gap to"
        },
        {
            "title": "Preprint",
            "content": "Table 7: Mitigation of Modality Gap. Comparison of Memory-based Projection (Memory) vs Noise-trained Decoder (N oise) across tasks. Trace Captioning (COCO) Dense Captioning (VG v1.2) Region-Set Captioning (COCO Entities) Image Captioning (COCO)"
        },
        {
            "title": "Mitigation B M R",
            "content": "C S"
        },
        {
            "title": "P mAP M B",
            "content": "R P"
        },
        {
            "title": "B M R",
            "content": "C no mitig. oise Memory 9.7 0.7 15.9 17.8 10.2 75.2 5.0 15.0 29.4 59.4 21.1 1.2 9.1 18.3 14.7 8.5 75.1 0.18 3.0 11.5 24.7 29.3 12.3 78.1 0.20 10.4 1.2 17.8 26.3 12.6 77.0 10.5 18.4 37.2 97.5 26.7 2.5 10.7 23.2 27.9 12.6 78.7 0.21 10.6 1.4 18.6 31.9 15.0 78.8 11.5 19.3 38.8 109.1 29.4 82.2 85.6 87."
        },
        {
            "title": "B M R",
            "content": "C CLIP-S 9.9 17.7 36.8 43.7 12.3 82.2 19.6 21.5 45.4 65.5 15.5 86.2 19.5 21.5 44.9 69.2 15.6 87.4 69.6 70.9 72.8 achieve strong captioning performance. For Trace Captioning, the Memory method is slightly more effective in the semantic metric RefPAC-S, while the oise variant achieves marginally better scores in CIDEr, ROUGE-L, METEOR, and BLEU@4, with minimal gap between the two approaches. In Dense Captioning, the Memory model consistently outperforms the oise model across all metrics. Similarly, for Region-Set Captioning, both methods achieve strong results, but the Memory method shows clearer advantage, particularly in tasks closer to the patch level. Finally, in Image Captioning, the performance gap between the two architectures narrows, especially on the Flickr30k test split. In this scenario, the Memory method performs significantly better when applied to the CLS token, whereas patch aggregation produces comparable results. However, the metrics reveal conflicting trends across different datasets. Chosen Strategy. Based on the observed results, we selected the projection-based approach (Memory) as the primary strategy for overcoming the modality gap in our framework. While the noise injection method (N oise) yielded competitive performance across multiple tasks, the Memory method demonstrated superior performance in dense captioning and region-set captioning, as well as clear advantage when applied to the CLS token in image captioning. Given these trends, and considering the stability of the projection-based approach across different evaluation settings, we adopted Memory as the default configuration for our framework."
        },
        {
            "title": "10 TRACE CAPTIONING BENCHMARK GENERATION",
            "content": "We construct our Trace Captioning dataset from the Localized Narratives dataset Pont-Tuset et al. (2020a). This dataset consists of mouse traces and their corresponding speech transcriptions, where annotators describe objects in images while moving the mouse pointer over them. The initial dataset samples include timestamped mouse traces and are composed of multiple sentences that thoroughly describe the trace, with the generated descriptions following the order of the mouse movement. However, our task does not require strict temporal coherence. Instead, we aim to generate single, concise caption that describes the specific area covered by the localized trace, rather than multi-sentence description. To achieve this, we split the descriptions into individual sentences and align the traces accordingly. We then refine the traces by removing intermediate periods caused by transitions between sentences, which often occur when the annotator moves to different region of the image. Specifically, we trim each trace by removing the first and last 15% of points, eliminating these transitional segments. Furthermore, we refine the captions by prompting the Llama3 8B model to rephrase the sentences, removing vague or subjective phrases such as \"there is,\" \"we can see,\" or \"on the left of the image,\" and replacing them with concise, objective descriptions that refer specifically to the region covered by the trace. This rephrasing is crucial to ensure that each caption adheres to the standard format of image-captioning datasets and focuses only on the precise part of the image that the trace corresponds to. The LLM also helps identify and remove irrelevant sentences (e.g., \"the image is blurred,\" \"the image is edited\"), which are then discarded along with their associated traces from the final benchmark. Figure 4 shows the full prompt used to guide the Llama model in refining and cleaning the descriptions. Figure 5 illustrates how the initial narrative samples are transformed into final trace captioning samples through the process of trace splitting and caption rephrasing."
        },
        {
            "title": "11 MORE QUALITATIVE RESULTS",
            "content": "Additional qualitative results are shown in Figures 6 and 7. Note that the first rows of Figures 6 and 7 contain also qualitative results for single patch captioning, for which we do not have annotated data to report quantitative results. As can be noticed in Figures 6 and 7, the Region-Set Captioning task tends to align more closely with image-level captioning rather than strictly focusing on localized regions. This is expected since the ground-truth captions in the COCO Entities dataset originate from the image-level annotations of COCO, as stated in Cornia et al. (2019)."
        },
        {
            "title": "Preprint",
            "content": "I have image descriptions derived from spoken narratives. These need to be (cid:44) rewritten as concise, stand-alone captions in the style of the image-caption datasets. Follow these rules: (cid:44) image,\" etc. - Remove unnecessary narrative phrases like \"we can see,\" \"there is,\" \"in this (cid:44) - Ensure the caption is standalone and descriptive. - Use simple, objective language that highlights key elements. - Keep it concise--just single phrase. - Follow the classical style of caption datasets. - If the description is vague, subjective, or does not describe concrete visual (cid:44) element (e.g., \"The image is taken indoor,\" \"This image is blurred\"), return `<INVALID>`. (cid:44) - Wrap the output in `{}` and add nothing else. ### **Examples:** - **Input:** \"We can see young elephant stands which is near the water in (cid:44) wooded area.\" **Output:** {A young elephant stands near the water in wooded area.} - **Input:** \"In this image can see some young children kicking soccer ball (cid:44) in field.\" **Output:** {A group of young children kicking soccer ball around field.} - **Input:** \"In the left of the image, we see pole that has two green street (cid:44) signs on it.\" **Output:** {A pole has two green street signs on it.} - **Input:** \"We can see two surfboards which are stuck in the sand along the (cid:44) seashore.\" **Output:** {Two surfboards stuck in the sand along the seashore.} - **Input:** \"This image consists of man which rides wakeboard behind (cid:44) boat.\" **Output:** {A man rides wakeboard behind boat.} - **Input:** \"In the background, there are bunch of sticky notes and pair of (cid:44) scissors.\" **Output:** {A bunch of sticky notes and pair of scissors.} - **Input:** \"It looks like sepia-toned photograph of motorcycle underneath (cid:44) tree.\" the shadow of **Output:** {A sepia-toned photograph of motorcycle underneath the shadow of (cid:44) tree.} - **Input:** \"There is sky\" **Output:** {A sky.} - **Input:** \"She is smiling.\" **Output:** {A smiling girl.} - **Input:** \"The image is taken indoor.\" **Output:** {<INVALID>} - **Input:** \"This image is edited.\" **Output:** {<INVALID>} - **Input:** \"The image is blurred.\" **Output:** {<INVALID>} - **Input:** \"I think he is about to jump.\" **Output:** {<INVALID>} Now, rewrite the following captions accordingly. Wrap each in `{}` and add (cid:44) <INPUT CAPTION> nothing else: Figure 4: LLM Prompt for rephrasing trace captions."
        },
        {
            "title": "Preprint",
            "content": "(a) Localized Narrative (b) Track 1 (c) Track 2 (d) Track 3 In this picture can observe dog running on the land. can observe water and grass on the ground. The background is blurred. Original: In this picture can observe dog running on the land. Processed: dog runs on the land. Original: can observe water and grass on the ground. Processed: Water and grass on the ground. Original: The background is blurred. Processed: <INVALID> In this image there is person wearing helmet is on vehicle. At the bottom of the image there are side mirrors. The background of the image is blurred. Original: In this image there is person wearing helmet is on vehicle. Processed: person wearing helmet rides vehicle. Original: At the bottom of the image there are side mirrors. Processed: Side mirrors. Original: The background of the image is blurred. Processed: <INVALID> This image is taken outdoors. In this image we can see the green grass on the ground. In the middle of the image we can see there are two dogs. Original: This image is taken outdoors. Processed: <INVALID> Original: In this image we can see the green grass on the ground. Processed: Green grass on the ground. Original: In the middle of the image we can see there are two dogs. Processed: Two dogs. Figure 5: Narrative vs. Trace Samples. The first column displays sample images from the Localized Narrative dataset Pont-Tuset et al. (2020b). The remaining three columns show the corresponding mouse traces, along with the captions generated by the LLM. Captions marked with <INVALID> are removed from the dataset."
        },
        {
            "title": "H\nC\nT\nA\nP",
            "content": "DeCap cat is sleeping on cluttered desk. cat is sleeping on cluttered desk. tennis player is playing tennis on the court for serve. Ours (CLIP + Mem.) cat is sitting on the bed and its contents. cat is sitting at table with full laptop . couple of people are in the middle of Ours (Talk2DINO + Mem.) plant in vase sitting on table. office supplies , pens , toys , and other items on desk. tennis court. street light in front of large building. few people are skiing on snowy mountain. few people are skiing in snowy mountain. cloudy sky is seen in this cloudy day."
        },
        {
            "title": "E\nC\nA\nR\nT",
            "content": "GT Two giraffes, rocks, and fence. DeCap giraffe in zoo with city in the backOurs (CLIP + Mem.) Ours (Talk2DINO + Mem.) ground. there are some people that are in lot by tree. two giraffes standing in fenced area. sky. giraffe in zoo with city in the background. there are some people that are out by lot of trees. view of city with sky in the background. flag. man on skateboard who is holding onto skateboard. there are some people that are in the water with couple of them. flag is flying high in the air. People walking on walkway. park filled with people sitting on benches near trees. there are several traffic lights out in the wild. large group of people walking on sidewalk."
        },
        {
            "title": "E\nS\nN\nE\nD",
            "content": "GT light shining through the trees. bench sitting in the woods. clock at train station. DeCap bench sits in the middle of wooded area. bench sits in the middle of wooded area. train traveling along the platform of black cat sitting on bench. woman squatting on bench with cat. DeCap (Crop) person in tree is standing in the wild near trees. Ours (CLIP + Mem.) bear is in the woods among the trees. Ours (Talk2DINO + Mem.) sun shining through the trees at sunset. bench sitting in the middle of wooded area. there are many trees that are standing in the woods. park bench sitting in the middle of wooded area. public train. black cat is leaning on black cat. train is on the tracks and going by. close up of person standing by person holding phone. there is person that is out on the kitchen. clock on train station platform above train. black cat is sitting on black bench. - I"
        },
        {
            "title": "G\nE\nR",
            "content": "GT an elderly man in cap sitting on bench. an old man sitting on bench with purse. man performing trick near fire hyDeCap man sitting on bench while holding door. Ours (CLIP + Mem.) bathroom has blue floor and it is very clean. Ours (Talk2DINO + Mem.) man in hat sitting on bench. man sitting on bench while holding door. bathroom has blue toilet and the walls. man sits on wooden bench with bag on his back. drant. man on skateboard doing trick. there are many cars driving down the street corner. fire hydrant on sidewalk next to street pole. man swinging baseball bat as another looks on. baseball player at bat getting ready to hit the ball. some baseball players are on the field playing baseball. baseball player is swinging his bat as crowd watches."
        },
        {
            "title": "E\nG\nA\nM",
            "content": "I GT black cat rubbing against bottle of man in wetsuit rides wave. wooden bench sitting on beach. wine. DeCap black cat standing next to bottle of wine glasses ZeroCap Wine dro Pet Cat. CLOSE cat sitting on the counter of green bottle. Ours (Talk2DINO + Mem.) black cat sitting on chair next to bottle of wine. man on surf board riding wave in the water man surfing in the area 0. man on surf board riding wave in the ocean. man on surfboard riding wave. bench sits on the beach next to the ocean slice of cake on plate with cup of cake beachfront bench. wooden bench sitting in the sand near the ocean. bench sitting on the beach next to the ocean. sunny cake with tea. and cake is sitting on white plate. piece of cake on plate with cup of coffee. wooden table with plate of cake and coffee. Figure 6: Qualitative results. We report four predictions of our model and compare baselines from the finer (top) to the coarser (bottom) task. For trace captioning examples, the trace time is color-coded from start (red) to end (yellow). DeCap = DeCap applied on the whole image. DeCap (Crop) = DeCap applied on cropped box. ZeroCap = ZeroCap Tewel et al. (2022) applied to the whole image. CLOSE = CLOSE Gu et al. (2023) applied to the whole image. Ours (CLIP + Mem.) = Our patch-based framework using CLIP as backbone and the projection as modality gap mitigation strategy. Ours (Talk2DINO + Mem.) = Our patch-based framework using Talk2DINO as backbone and the projection as modality gap mitigation strategy. GT = ground-truth caption."
        },
        {
            "title": "H\nC\nT\nA\nP",
            "content": "DeCap group of people in kitchen are cooking food. Ours (CLIP + Mem.) couple of people that are standing around each other. Ours (Talk2DINO + Mem.) forest with trees in the background. table with cup of coffee and plates of silverware. bunch of people are sitting at the table together. cup of coffee with spoon sitting on plate. small bed is curled up in cluttered room. police car is parked on the side of street. aa baby is in bedroom with white sink and toilet. dog laying on rug in living room. there are few street signs in the middle of the neighborhood. fence that is next to road."
        },
        {
            "title": "E\nC\nA\nR\nT",
            "content": "GT Clouds and the sun in the sky. person wearing cap. DeCap couple of people are sitting on bench woman at table putting food in pot. looking at the ocean. Ours (CLIP + Mem.) couple of people are on boat by the ocean. Ours (Talk2DINO + Mem.) sunset in the distance in the sun couple of people are in kitchen making food. person wearing hat looking at something in the background Christmas tree decorated with balls and toys. two people posing with man and woman having glass of wine. there are two people in kitchen with red sweater. the christmas tree is decorated for christmas Few people on boat. man on boat in body of water with other boats. there are some people in the water by boat. boat with many people on it"
        },
        {
            "title": "E\nS\nN\nE\nD",
            "content": "GT white ceiling fan hanging in the kitchen. plane flying in the sky. DeCap kitchen with large refrigerator , cabinets and stove. DeCap (Crop) bathroom sink with variety of toilet building is flying under traffic light in the air near building. large airplane is in flight on the airport. above the wall. Ours (CLIP + Mem.) kitchen has lot of fridge and stove in lot of building is outside of yellow car. it. Ours (Talk2DINO + Mem.) ceiling fan is hanging in the kitchen. there is plane flying high in the sky. two sandwiches on plate. sandwich and plate of soup on table. bird that is perched on small bird. potted plant on the ledge. sandwich on plate containing sandwich. the couple of food are in the kitchen with meal. plate topped with two sandwiches on table. close up of person standing by person holding phone. there is man that is about to take trick. potted plant sitting on ledge. N I"
        },
        {
            "title": "G\nE\nR",
            "content": "GT Dogs near the edge of water . DeCap dog and his dogs are wading in the muddy water. Ours (CLIP + Mem.) there are many things that are out in the water. Ours (Talk2DINO + Mem.) two dogs near one another near water. soccer player is running while kicking ball . soccer player in the soccer uniform tries to kick the ball. there are some people on baseball field playing game. soccer player getting ready to kick the ball. brown-haired woman is pushing baby stroller . man and child walking in the street while holding stroller. there are some cars and man about to go down the street. woman pushing stroller with child inside. The child lays on the hardwood floor . young boy sitting on the floor in room. the kitchen is very clean and has an open door. young child is laying on the floor."
        },
        {
            "title": "E\nG\nA\nM",
            "content": "I GT Four birds are chasing another bird which has piece of food in its mouth. DeCap flock of birds flying over the water. ZeroCap gull mating. CLOSE group of birds flying over body of water. Ours (Talk2DINO + Mem.) flock of birds flying in the sky. Brown-haired girl wearing green tank top, talking on cellphone. woman talking on cell phone while on street. man in the back of pickup truck with blood on the back. woman looking at her cell phone while standing in street. woman talking on cell phone in market. woman with blond-hair is sitting in booth with drink working on her laptop. woman sitting at table using laptop. young girl in blue shirt is in bowling alley, and is casting her ball down lane. young girl playing bowling game on wii. readers writing on laptop on deskmounted computer. woman sitting at table with laptop and drink. woman sitting at cafe using her laptop. woman playing bowling game on the view hitting the deck pin at the end of the row stretch. on person on skateboard doing trick. bowling. Figure 7: Qualitative results. We report four predictions of our model and compare baselines from the finer (top) to the coarser (bottom) task. For trace captioning examples, the trace time is color-coded from start (red) to end (yellow). DeCap = DeCap applied on the whole image. DeCap (Crop) = DeCap applied on cropped box. ZeroCap = ZeroCap Tewel et al. (2022) applied to the whole image. CLOSE = CLOSE Gu et al. (2023) applied to the whole image. Ours (CLIP + Mem.) = Our patch-based framework using CLIP as backbone and the projection as modality gap mitigation strategy. Ours (Talk2DINO + Mem.) = Our patch-based framework using Talk2DINO as backbone and the projection as modality gap mitigation strategy. GT = ground-truth caption."
        }
    ],
    "affiliations": [
        "CNR-ISTI",
        "Universit√† di Pisa"
    ]
}