{
    "paper_title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations",
    "authors": [
        "Genglin Liu",
        "Salman Rahman",
        "Elisa Kreiss",
        "Marzyeh Ghassemi",
        "Saadia Gabriel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 0 3 8 7 0 . 4 0 5 2 : r MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations Genglin Liu1 Salman Rahman1 Elisa Kreiss1 Marzyeh Ghassemi2 Saadia Gabriel 1 University of California, Los Angeles genglinliu@cs.ucla.edu 2 MIT CSAIL"
        },
        {
            "title": "Abstract",
            "content": "We present novel, open-source social network simulation framework MOSAIC where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with directed social graph to analyze emergent deception behaviors and gain better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We opensource our simulation software to encourage further research within AI and social sciences: https://github.com/genglinliu/MOSAIC"
        },
        {
            "title": "Introduction",
            "content": "In 2024, OpenAI reported that its platform was already being misused by covert influence operations to generate synthetic content diffused over social media (OpenAI, 2024). These internet manipulators exploit the fact that social networks have become fundamental part of modern life, shaping public discourse, influencing political opinions, and facilitating the rapid spread of unverified humanand AI-generated content (Aichner et al., 2021; Orben et al., 2022; Cinelli et al., 2021). While traditional social science methods such as surveys and observational studies have provided insights into human behavior, they often struggle to capture large-scale, emergent online interactions 1 (Yu et al., 2021; Lorig et al., 2021). Agent-based modeling (ABM) provides distinct advantages over survey methods in social science research since they can simulate dynamic interactions over time, and support examination of hypothetical or counterfactual scenarios with repeatable and controllable conditions (Bonabeau, 2002; Epstein, 1999). Recent advances in foundation models have led to the emergence of generative agent-based social simulations, where AI-powered users dynamically engage in posting, sharing, flagging, and commenting on content (Yang et al., 2024; Gao et al., 2023; Chen et al., 2024a; Wang et al., 2024; Zhou et al., 2023; Park et al., 2022). Unlike traditional survey methods or classical agent-based modeling, simulations driven by large language models enable agents to interact with the environment and each other naturally through rich, human-like dialogue, closely mirroring authentic social behavior. In this work, we introduce MOSAIC, novel multi-agent AI social network simulation that models content diffusion, user engagement patterns, and misinformation propagation. Simulation gives us the power to ask counterfactual questions about the complex world. Among different applications of social simulations, content moderation stands out as pressing challenge due to the real-world harm caused by misor disinformation and online influence operations. Previous research has shown that false information not only spreads more rapidly and deeply than truthful content (Vosoughi et al., 2018) but also alters public perception in ways that are difficult to reverse (Lewandowsky et al., 2012). Addressing this issue requires effective content moderation strategies that can mitigate harm while preserving user engagement and freedom of expression. We embed three moderation strategies into our simulation environment: (1) community-based fact-checking mimicking and Metas Community Notes, (2) independent fact-checking, and mix of those. Figure 1: Overview of the MOSAIC, multi-agent social simulation framework where agents interact in an environment mimicking social network, form dynamic memory-based behaviors, and respond to misinformation using community-based, third-party, or hybrid fact-checking mechanisms. Personas are replicated from human surveys or generated using synthetic distributions. Memories are retrieved before an agent takes certain actions, and are updated after certain events. We systematically evaluate the impact of these 3 content moderation strategies on misinformation spread, moderation precision/recall, and user engagement dynamics. Beyond moderation, understanding how certain content gains traction remains an open challenge. Online discourse is shaped by the dynamics of content diffusion, where some posts attract widespread engagement while others remain largely unseen. In our simulation, LLM-powered agents are equipped with memory, self-reflection, and explicit reasoning mechanisms, allowing them to explain their decisions and adapt their behavior over time. While our primary focus is on moderation, this extended perspective helps contextualize how misinformation and other content propagate in online interactions. To this end, our key contributions are: We build novel multi-agent simulation from scratch where LLM-powered users dynamically engage with online content, enabling realistic modeling of social behaviors and content diffusion. Interestingly, we find that agents can accurately model individuals, but theyre better at simulating some (more common) demographic groups than others (Section 2). We conduct comparative study of third-party, community-based, and hybrid fact-checking approaches, quantifying their effectiveness in mitigating misinformation while preserving engagement. We show that misinformation doesnt spread as fast in an agent simulation as it is commonly observed in human social media, and content moderation strategies can improve not only factchecking but also engagement (Section 3). We explore how different content and network properties influence diffusion dynamics, offering insights into engagement patterns and how some content/users end up attracting more attention than others. Surprisingly, we find that agents individual verbose reasoning may not really reflect their collective action patterns on group level (Section 4). By bridging social science observations, gametheoretic modeling (Acemoglu et al., 2023) and LLM-driven modeling, our work demonstrates the potential of generative agent simulations as tool for studying large-scale online behaviors, testing content moderation strategies, and mitigating misinformation risks in the era of generative AI."
        },
        {
            "title": "2 Social Network Simulation",
            "content": "Figure 2: Average engagement received per post: Human vs. Agents. Our t-test validates that the difference in reaction patterns across the three engagement types are not statistically significant, suggesting that agents can simulate individual human reactions to social media feed realistically. Our AI-driven social network simulates how content spreads, how users engage, and how misinformation propagates within directed social graph. As illustrated in Figure 1, at its core, the system simulates dynamic environment where AI agents interact by following others, posting content, reacting (e.g., liking, sharing, commenting), and reporting misinformation. Each agent operates with persona generated using question set inspired by AgentBank (Park et al., 2024). The main simulation system tracks the progression of time and the evolving state of the network. It is supported by several key components: relational database that records all user interactions; content manager that injects new posts into the network; an analytics module that monitors diffusion patterns and user behavior; and fact-checking system that evaluates the performance of various content moderation strategies. Simulated Network We build simulated social network environment inspired by platforms like X,1 allowing AI-driven users to interact, post, and share content. The simulation includes basic user class with attributes such as username, posts, followers, following, and reposts, mimicking the structure of real-world social media platforms. The network itself is defined by the follower-following relationships, creating web of user interactions, represented by directed graph = (N, E) 1https://x.com/ 3 where represents the set of user nodes, i.e., = {n1, n2, . . . , nk}, where ni is user in the network. represents the set of directed edges, i.e., = {(ni, nj) ni follows nj}. Each edge (ni, nj) signifies that user ni follows user nj."
        },
        {
            "title": "2.1 Simulation Flow",
            "content": "The simulation begins with an initialization phase where the system loads experimental configurations, sets up the database (details in Appendix D), generates an initial user population (more details in Appendix A), and establishes follow relationships. Agents are configured to operate under diverse behavioral traits, reflecting real-world variations in social media engagement. In all of our experiments, agents are driven by gpt-4o (Hurst et al., 2024) as the foundation model backbone, unless otherwise specified. We do also implement an option to connect agents with open-weight models through SGLang (Zheng et al., 2024) or vLLMs (Kwon et al., 2023) inference engines. At each time step, news content is introduced based on predefined parameters, with agents dynamically responding to their feeds. Agents can optionally generate posts according to their own interests. However during certain controlled experiments we configure them to only engage through reactions such as liking, sharing, commenting, or reporting misinformation. We describe more general action space and more details of their decisionmaking process in Appendix E. The visibility of posts evolves based on engagement metrics, simulating algorithmic amplification effects. If factchecking is enabled, agents incorporate moderation signals, i.e. they are prompted to pay more attention to potentially falsified content or misinformation. adjusting their interactions accordingly (we discuss the content moderation simulation in more depth in Section 3). Throughout this process, the system tracks key statistics, including content reach, user influence, and misinformation spread. At the end of each simulation run, post-hoc analysis is conducted to assess content diffusion dynamics, user engagement metrics, influence distribution, and the impact of fact-checking interventions. We also keep track of various network properties such as centrality and triadic closures, and perform homophily analysis to examine clustering patterns in user engagement. Table 1: Demographic groups showing significant differences in engagement patterns between human participants and AI agents (p < 0.05). Category Significant Differences Non-Significant Differences Age Gender Religion Ethnic Group 25-34 (shares) Male (likes, shares) Hinduism (likes), Islam (shares) Hispanic/Latino, Black/African (shares), Asian (comments) Secondary (shares), Doctorate (likes) $10K-$20K (comments), $70K-$80K (likes) Education Income Political Stance Conservative (shares), Very Conservative (likes) Very Liberal, Moderate, Liberal, Libertarian High School, Undergraduate, Technical, Graduate Various other income brackets 18-24, 35-44, 45-54, 55-64, 65-74 Female No Religion, Spiritual, Christianity, Jewish White/Caucasian, Mixed, Others"
        },
        {
            "title": "2.2 Human Validation",
            "content": "To validate the veracity of our simulation, we conducted human study to compare the sharing patterns between humans and LLM agents. We recruited 204 participants via Prolific.2 More details of our human survey is provided in Appendix B. Setup In the first phase of this replication study, we conducted survey to collect demographic data (e.g., age, gender, religion, ethnicity, education level, language, residence, income, political stance) and personal values and behaviors (e.g., time use, priorities, personality of close relationships, social behavior, hobbies, residential history, social goals, meaningful life events, valued friendship traits, financial habits). Inspired by Park et al. (2024), we used this anonymized data to create individualized personas for 204 LLM-driven agents, each corresponding to human participant. In the second phase, both participants and their corresponding LLM agents were shown two curated social media snapshots containing 30 posts. They were instructed to respond to each post using fixed set of actions (e.g., like, dislike, comment, share). The agents, guided solely by their assigned persona profiles, followed the same instructions. We then analyzed and compared engagement patterns between humans and agents, both overall and across demographic groups, to assess how well LLMs can emulate human social media behavior based on persona information alone. Simulation/Human Reaction Alignment Our analysis compared engagement behaviors between 204 human participants and the same number of persona-replicated AI agents across 30 diverse social media posts, using independent two-sample t-tests for each engagement type. 10% of the articles are false news articles verified by NewsGuard. 2https://www.prolific.com/ As illustrated in Figure 2, no statistically significant differences emerged in likes (t = 1.33, = 0.19) or comments (t = -1.05, = 0.30), though humans gave slightly more likes (+2.17 per post), and agents posted more comments (+1.87 per post). marginally significant difference was observed in shares (t = 2.11, = 0.04), with humans sharing slightly more (+0.80 per post). These results indicate that persona-driven AI agents display engagement patterns that closely mirror those of humans, supporting the realism of our simulation. Further demographic-level analysis  (Table 1)  found that out of 52 examined demographic subgroups, only 14 showed statistically significant differences (p < 0.05) in at least one engagement metric. Notable discrepancies appeared in the 2534 age group (shares) and several religious, ethnic, educational, income, and political categories. However, most demographic groups exhibited no significant differences, suggesting that agents simulate typical engagement behavior more accurately for demographics more prevalent in LLM training data. We provide more details about the per-demographic engagement pattern alignment in Appendix B. Figure 3: Effectiveness of content moderation approaches in promoting factual content. Positive values: factual content receives more engagement. Negative values: misinformation receives more engagement."
        },
        {
            "title": "Social Environment",
            "content": "We conduct series of experiments using our multiagent simulation framework to investigate the effects of different fact-checking strategies on the spread of both factual and misinformation content. Our findings reveal key differences between LLMdriven social simulations and human social networks in how misinformation propagates."
        },
        {
            "title": "3.1 Setup",
            "content": "Data Sources We obtained data license from NewsGuard3 to access proprietary information on widespread misinformation narratives tracked by their independent team of journalists. We collected 1,353 examples of false news from their database with release dates up to December 19th, 2024.4 To collect real news, we utilize news aggregation API 5 to retrieve articles published daily from January 31 to February 28, 2025. The system queries the API for all available topics, prioritizing popular articles in English. For each day in the specified range, we extract key information from the retrieved articles, including their title, description, main content, and publication date. Using the NewsAPI, we scraped total number of 2470 pieces of real news. Since most of them are nonpolitical news from major media outlets, we consider them as \"real news\" as opposed to the verified political misinformation from the NewsGuard API. Environment Initialization Our simulations involve agentic users interacting with news posts under four different fact-checking conditions: (1) No Fact-Checking, (2) Community-Based FactChecking, (3) Third-Party Fact-Checking with an offline LLM that uses its own parametric knowledge, and (4) Hybrid Fact-Checking, where the latter integrates both community-based and thirdparty verification mechanisms. The simulations starts with 50 agents, and spans over 40 time steps, with agents making interaction decisions based on the perceived veracity of posts and the presence (or absence) of content moderation. At each new step, we randomly introduce up to 2 more agents into the environment to simulate the regular user growth of the social media platform. We analyze both the overall engagement with posts and the effectiveness 3https://www.newsguardtech.com/ 4We removed non-English articles and de-deduplicated. 5https://newsapi.org/ of fact-checking strategies in suppressing misinformation. The action space of agents varies across different fact-checking conditions, reflecting different levels of scrutiny and intervention in their social media interactions. In the no fact-checking setting, agents interact freely with the feed, engaging with posts based solely on their interests and beliefs. They can like, share, comment (within 250-character limit), or ignore posts, without any explicit instructions to assess the accuracy of the content. In the thirdparty fact-checking condition, the action space remains the same, but the environment implicitly assumes the presence of external fact-checkers who may influence the visibility or credibility of posts. However, the agents themselves do not perform any direct verification. In contrast, the community fact-checking setting expands the action space by allowing agents to add community notes to posts they deem misleading or in need of additional context, as well as rate existing community notes as either helpful or unhelpful. This introduces participatory element, encouraging agents to contribute to crowdsourced verification system. Finally, the hybrid fact-checking condition combines elements from both third-party and community-driven verification. Agents can engage with posts as in previous settings while also considering official fact-checks alongside community notes, contributing their own notes and rating those written by others. Across all conditions, agents must select from predefined valid actions, ensuring consistency in response formats. Additionally, when reasoning is enabled, agents are required to justify their interactions by providing brief explanation for each chosen action, further enhancing the interpretability of their behavior. Fact Checker LLM The fact checker presented in this code is an automated content verification system designed to identify and address misinformation on social platform. It works by prioritizing posts for review based on engagement metrics (likes, shares, comments), news classification, and user flags, with special priority given to content that has received community notes in hybrid factchecking scenarios. The system leverages an LLM (gpt-4o in our experiments) to analyze post content and render verdicts categorized as \"true,\" \"false,\" or \"unverified,\" each accompanied by an explanation, confidence score, and supporting sources from the models training data. When posts are deemed false"
        },
        {
            "title": "Method",
            "content": "Total Posts News Posts Factual News Misinfo News Precision Recall F1 Score No Fact-Checking Community-Based Third-Party Hybrid 2878 1269 1233 809 500 490 450 500 450 441 405 450 50 49 45 - 0.462 0.219 0.625 - 0.490 0.156 0.600 - 0.475 0.182 0.612 Table 2: Comparison of different fact-checking methods including dataset size, news distribution, and evaluation metrics. Figure 4: Comparison of total interactions over time across different fact-checking strategies. with high confidence ( 0.9 in standard mode, or 0.7 in hybrid mode with community notes), the system automatically takes them down and records the justification. All fact-check results are stored in the database that maintains an audit trail of verdicts alongside ground truth data when available. Network Initialization We initialize scale-free network of LLM-powered agents interacting within directed social graph, using BarabásiAlbert model (Barabási and Albert, 1999). Misinformation and factual content are injected into the system at controlled rates, with agents dynamically engaging based on their personas and decisionmaking processes. Each moderation strategy is implemented in separate experiment, allowing for comparative analysis. We share more details about the experiment configurations in Appendix F."
        },
        {
            "title": "3.2 False News Do Not Spread Faster than\nReal News with Simulation Agents",
            "content": "A key insight from our simulation contradicts established results from human social networks: false news does not spread faster than real news (Figure 4). Prior studies on human social behavior have consistently demonstrated that misinformation propagates more rapidly and deeply than factual content (Vosoughi et al., 2018; Zhao et al., 2020). However, in our agent-based simulation, engagement (particularly with sharing) with misinformation does not surpass that of factual news, even in the absence of fact-checking. From Figure 4, across all experimental conditions, factual news generally maintains higher levels of engagement compared to misinformation. This trend is particularly pronounced in the ThirdParty and Hybrid fact-checking setups, where the separation between factual and false news interactions is maximized. Even in the No Fact-Checking scenario, misinformation fails to gain dominant foothold, suggesting that LLM-driven agents may inherently avoid interacting with unverified or misleading content."
        },
        {
            "title": "3.3 Content Moderation Improves Both\nFact-Checking and Engagement",
            "content": "While political misinformation does not spread faster than factual news in the simulations, another important observation is that overall engagement remains significantly lower in the absence of factchecking. Figure 4 demonstrates that, under No Fact-Checking, even factual posts fail to gain substantial engagement. This suggests that the agents, when left without explicit fact-checking mechanisms, tend to suppress their interactions altogether, rather than engaging indiscriminately with unreliable content. We hypothesize that this behavior emerges due to the inherent sensitivity of LLM-based agents to political misinformation, leading them to disengage rather than risk amplifying uncertain information. To address this disengagement issue, we introduce various fact-checking mechanisms and observe that they not only suppress the spread of 6 misinformation but also enhance engagement with factual news. As shown in Figure 3, we observe pattern in how different content moderation approaches influence the engagement balance between factual content and misinformation over time. Third Party fact-checking emerges as the most effective intervention, establishing substantial cumulative advantage for factual content that reaches approximately 325 units by the final time step, significantly outperforming all other approaches. Notably, even the No Fact Check condition maintains positive trajectory, indicating that factual content may have some inherent engagement advantage in the simulated environment, but this advantage is dramatically amplified by active moderation strategies. The consistently positive and increasingly divergent trajectories suggest that moderation effects compound over time, with the gaps between approaches widening progressively rather than stabilizing. This indicates that content moderation not only creates immediate benefits but generates cumulative advantages for factual information that strengthen with continued application, with third-party professional fact-checking demonstrating particular potency in creating healthy information ecosystem. This is somewhat corroborated by some human user studies showing content moderation increases trust in unflagged content (Pennycook et al., 2020)."
        },
        {
            "title": "3.4 Fact-Checking Performance: Hybrid",
            "content": "Model Achieves the Best Trade-off While Third-Party fact-checking achieves the best engagement-separation effect, we evaluate factchecking effectiveness using precision, recall, and F1-score  (Table 2)  . The results indicate that the Hybrid approach provides the best overall factchecking performance, achieving precision of 0.625, recall of 0.6 and an F1 Score of 0.612, highest amongst all three approaches we explored. This suggests that combination of communitybased and third-party verification leads to the most reliable identification of misinformation, balancing recall (coverage of false news) and precision (accuracy of detection). In contrast, Third-Party fact-checking alone suffers from lower recall rate (15.6%), indicating that while it is selective in marking posts as misinformation, it may not capture all false claims effectively. The CommunityBased model, on the other hand, achieves higher recall but lower precision, leading to moderate overall performance."
        },
        {
            "title": "4 Excursion: What Makes Certain\nUsers/Content More Popular?",
            "content": "Understanding why certain content gains traction in online spaces is essential for modeling engagement dynamics and intervention strategies. In this section, we analyze the diffusion characteristics of posts, particularly through agents reactions to different social media content. Figure 5: Top 50 users with highest engagement. Figure 6: User engagement best power-law fit. Experimental Setup We generate diverse set of content types, including news, opinion pieces, and user-generated posts, each with varying levels of controversy, sentiment, and source credibility. Agents interact with these posts based on their personas, engagement signals, and memory-based decision-making. In this experiment, the agents are no longer just prompted to react to the social feeds, but they also participate in generating new content that circulates through the network. We combine two simulation runs that host total of 161 agents and 4,249 posts."
        },
        {
            "title": "4.1 Findings",
            "content": "Our study investigates the underlying drivers of user engagement in simulated social media environment by analyzing multiple dimensions including user popularity, persona attributes, con7 tent topics, and agent reasoning patterns. We begin by defining user popularity as the sum of followers, likes, shares, and comments, and observe in Figure 5 that user engagement follows power-law distribution. The best-fitting approximation, (x) = 120x0.6, with an R2 = 0.84 (Figure 6), confirms an adherence to power-law behavior where small number of users generate most of the engagement. Though our exponent α = 0.60 is lower than the typical range (1.52.5) reported in real-world networks (Muchnik et al., 2013; Bild et al., 2015), it still highlights the skewed nature of content popularity. We then explore whether user engagement correlates with persona attributes by comparing the top and bottom 50 users across several traits. The Chi-square analysis summarized in Table 3 reveals no statistically significant differences across categories such as age, gender, activity type, hobby, ethnicity, income level, political affiliation, or primary goal. Although medium effect sizes for ethnicity and hobby (Cramers of 0.319 and 0.302, respectively) are noted (Cramér, 1946), the absence of statistical significance underscores the lack of strong influence of these traits on engagement. Furthermore, the simulation did not include public figures or celebrity-like personas, indicating that even with randomized initialization, subset of users naturally attracts more attention, emphasizing the unpredictability of engagement in such networks. To assess whether content topics drive engagement, we applied BERTopic (Grootendorst, 2022) using all-MiniLM-L6-v2 embeddings (Reimers and Gurevych, 2019) to cluster post content and computed engagement statistics for each topic. An ANOVA test returned an F-statistic of 0.614 with p-value of 0.84, indicating no statistically significant relationship between content topics and engagement. This suggests that the semantic content alone does not reliably influence user reactions. Given the lack of correlation between engagement and demographic or topical properties, we turn to feed prioritization mechanisms. Our system emphasizes recency and follow relationships rather than engagement-based ranking, yet this setup inadvertently creates feedback loop. Once user is followed, their content gains more exposure, which can further increase engagement and visibility, reinforcing the users popularity. These findings lead us to speculate that the power-law influence distribution may arise from the agents behavioral dynamics, such as copying the actions of prior agents, creating preferential attachment loops that reinforce popularity instead of from their profile attributes or the specific content they generate. Table 3: Chi-square Test Results for Differences in Engagement Based on Demographic Attributes Attribute Chi-square p-value Cramers Effect Size Age Group Gender Activity Hobby Ethnicity Income Level Political Affiliation Primary Goal 1.632 0.653 5.030 9.101 10.187 4.373 2.515 8. 0.652 0.721 0.412 0.246 0.070 0.358 0.642 0.089 0.128 0.081 0.224 0.302 0.319 0.209 0.159 0.284 Small Negligible Small Medium Medium Small Small Small Finally, we examine agents reasoning traces to better understand engagement behaviors. Table 4 highlights distinct sentiment and motivation patterns associated with different actions. Positive sentiment dominates actions like following (99%), commenting (97%), liking (92%), and sharing (92%), while negative sentiment is prevalent in flagging (71%) and unfollowing (40%). Motivational reasoning varies by action type: flagging stems from quality assessments (49%) and misinformation concerns (22%), while sharing reflects agreement (46%). Likes and comments are driven by social connection and agreement, and following is based on long-term interest. Vocabulary analysis further reveals that engagement types rely on domain-specific lexicons. Yet, there remains notable disconnect: only 21% of posts have sentiment alignment with agent reasoning, implying that agents verbal justifications do not fully capture the deeper influences behind their choices. Despite articulating rationales involving value alignment and relevance, these factors do not reliably predict popularity, further supporting the role of individual context and network dynamics. Together, these findings paint complex picture: while agents provide structured reasoning for engagement actions, overall popularity and content virality arise less from user or content attributes and more from emergent social dynamics and structural effects in feed exposure. We provide more extended discussion and analysis in Appendix H."
        },
        {
            "title": "5 Background",
            "content": "Behavioral Economics and Persuasion Games. We model sequential persuasion game using LLM-powered agents conditioned on finegrained personas (Kamenica and Gentzkow, 2011; 8 Gentzkow and Kamenica, 2017; Acemoglu et al., 2023). These agents operate within directed social graph and evolve based on memory and social context, enabling the study of online behavior, intervention strategies, and moderation effects. LLM-Driven Social Simulations. LLMs have transformed agent-based modeling by enabling context-aware, generative behaviors. While early simulationssuch as Schellings segregation model (Schelling, 1971), Sugarscape (Epstein and Axtell, 1996), and NetLogo-based environments (Wilensky, 1999)relied on static heuristics, recent systems like Smallville (Park et al., 2023), AgentVerse (Chen et al., 2024a), and Chirper (Minos, 2023) showcase agents with lifelike interactions and social dynamics. However, LLM-driven agents still face challenges like inconsistency and limited long-term reasoning. Our work addresses this by incorporating structured constraints and iterative feedback to enhance reliability for social science research. Misinformation and Fact-Checking. False information often spreads more rapidly than truth due to emotional appeal and engagement-driven algorithms (Vosoughi et al., 2018; Pennycook and Rand, 2021; Solovev and Pröllochs, 2022). Existing responsesthird-party fact-checking (Raghunath and Malik, 2024; Patel, 2024), algorithmic detection, and crowdsourced moderation like Community Notes6each face limitations in scalability, accuracy, or bias (Zannettou et al., 2019; Panizza et al., 2023). We use LLM-based simulations to evaluate these approaches in controlled settings, comparing their effectiveness and exploring hybrid strategies. Simulations for Governance and Policy. Simulations have long supported decision-making in fields like epidemiology and public policy (Currie et al., 2020; Axtell and Farmer, 2022; Qu and Wang, 2024). In the context of social media governance, LLM-driven simulations offer novel testbed for assessing content moderation and algorithmic interventions before deployment (Charalabidis et al., 2011; Landau et al., 2024). Our framework enables scalable experimentation with regulatory strategies, contributing to ongoing efforts in algorithmic auditing and platform accountability. 6https://communitynotes.x.com/guide/en/"
        },
        {
            "title": "6 Conclusion",
            "content": "Our study introduces novel multi-agent generative AI simulation to model content diffusion, engagement, and misinformation dynamics in social networks. Using LLM-driven agents, we bridge social science and computational modeling with high realism. We find that hybrid content moderationcombining community-based and thirdparty fact-checkingbest balances misinformation reduction and user engagement. Notably, LLM agents tend to avoid unverified content, likely due to safety training, and misinformation did not spread faster than factual news, unlike in human studies. Engagement followed power-law distribution, with few users driving most activity. However, user attributes and content topics were weak predictors, highlighting the complexity of online ecosystems. Agent reasoning showed gap between stated motivations and actual behavior, suggesting network effects shape engagement more than demographics or content."
        },
        {
            "title": "Limitations",
            "content": "Our findings are subject to several limitations, particularly in the scale of our experiments. First, the limited number of human participants, especially from minority demographic groups, restricts the statistical power of our conclusions. Expanding participant diversity would enable more robust analysis of how alignment between real and simulated social interaction patterns varies across demographics. Second, our content moderation experiments were conducted at relatively small scale, which may have constrained the emergence of complex behaviors. Running these experiments at larger scale could uncover additional dynamics not captured in the present study. Third, the fact-checking agents in our simulations lacked access to live web search, limiting their ability to verify claims against up-to-date or external information sources. Enhancing their access to real-time data could significantly improve their reliability and utility. Finally, we observe gap between agents explicit explanations for their actions and the collective reaction patterns that emerge in the system. The root causes of this misalignment remain unclear and warrant further investigation, potentially involving deeper analysis of agent modeling assumptions or social influence mechanisms."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was supported in part by research credits generously provided by OpenAI. In addition, the authors would like to thank Professor Jacob Andreas for his valuable feedback and insightful suggestions."
        },
        {
            "title": "References",
            "content": "Daron Acemoglu, Asuman Ozdaglar, and James Siderius. 2023. model of online misinformation. Review of Economic Studies, page rdad111. Thomas Aichner, Matthias Grünfelder, Oswin Maurer, and Deni Jegeni. 2021. Twenty-five years of social media: review of social media applications and definitions from 1994 to 2019. Cyberpsychology, behavior, and social networking, 24(4):215222. Robert Axtell and Doyne Farmer. 2022. Agentbased modeling in economics and finance: Past, present, and future. Journal of Economic Literature, 14. Albert-László Barabási and Réka Albert. 1999. Emerscience, gence of scaling in random networks. 286(5439):509512. David Bild, Yue Liu, Robert Dick, Morley Mao, and Dan Wallach. 2015. Aggregate characterization of user behavior in twitter and analysis of the retweet graph. ACM Transactions on Internet Technology (TOIT), 15(1):124. Eric Bonabeau. 2002. Agent-based modeling: Methods and techniques for simulating human systems. Proceedings of the national academy of sciences, 99(suppl_3):72807287. Yannis Charalabidis, Euripidis Loukis, and Aggeliki Androutsopoulou. 2011. Enhancing participative policy making through modelling and simulation: state of the art review. In Proceedings of the European, Mediterranean and Middle Eastern Conference on Information Systems-Informing Responsible Management: Sustainability in Emerging Economies, EMCIS, pages 210222. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, et al. 2024a. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In ICLR. Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2024b. Internet of agents: Weaving web of heterogeneous agents for collaborative intelligence. arXiv preprint arXiv:2407.07061. Matteo Cinelli, Gianmarco De Francisci Morales, Alessandro Galeazzi, Walter Quattrociocchi, and Michele Starnini. 2021. The echo chamber effect on social media. Proceedings of the national academy of sciences, 118(9):e2023301118. Harald Cramér. 1946. Mathematical methods of statistics. Princeton University Press. Christine SM Currie, John Fowler, Kathy Kotiadis, Thomas Monks, Bhakti Stephan Onggo, Duncan Robertson, and Antuela Tako. 2020. How simulation modelling can help reduce the impact of covid19. Journal of Simulation, 14(2):8397. Joshua Epstein. 1999. Agent-based computational models and generative social science. Complexity, 4(5):4160. Joshua Epstein and Robert Axtell. 1996. Growing artificial societies: social science from the bottom up. Brookings Institution Press. Chen Gao, Xiaochong Lan, Zhi jie Lu, Jinzhu Mao, Jing Piao, Huandong Wang, Depeng Jin, and Yong Li. 2023. S3: Social-network simulation system with large language model-empowered agents. ArXiv, abs/2307.14984. Matthew Gentzkow and Emir Kamenica. 2017. Bayesian persuasion with multiple senders and rich signal spaces. Games and Economic Behavior, 104:411429. Maarten Grootendorst. 2022. Bertopic: Neural topic modeling with class-based tf-idf procedure. arXiv preprint arXiv:2203.05794. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Md Rafiqul Islam, Shaowu Liu, Xianzhi Wang, and Guandong Xu. 2020. Deep learning for misinformation detection on online social networks: survey and new perspectives. Social Network Analysis and Mining, 10(1):82. Jennifer Jerit and Yangzi Zhao. 2020. Political misinformation. Annual Review of Political Science, 23(1):7794. Emir Kamenica and Matthew Gentzkow. 2011. Bayesian persuasion. American Economic Review, 101(6):25902615. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626. 10 Susan Landau, James Dempsey, Ece Kamar, Steven Bellovin, and Robert Pool. 2024. Challenging the machine: Contestability in government ai systems. arXiv preprint arXiv:2406.10430. Stephan Lewandowsky, Ullrich KH Ecker, Colleen Seifert, Norbert Schwarz, and John Cook. 2012. Misinformation and its correction: Continued influence and successful debiasing. Psychological science in the public interest, 13(3):106131. Genglin Liu, Xingyao Wang, Lifan Yuan, Yangyi Chen, and Hao Peng. 2023. Examining llms uncertainty expression towards questions outside parametric knowledge. arXiv preprint arXiv:2311.09731. Fabian Lorig, Emil Johansson, and Paul Davidsson. 2021. Agent-based social simulation of the covid-19 pandemic: systematic review. JASSS: Journal of Artificial Societies and Social Simulation, 24(3). Morgan Marietta, David Barker, and Todd Bowser. 2015. Fact-checking polarized politics: does the fact-check industry provide consistent guidance on disputed realities? In The forum, volume 13, pages 577596. De Gruyter. Stephan Minos. 2023. Chirper: Changing the ai landscape with autonomy and entertainment. Chirper Blog. Lev Muchnik, Sen Pei, Lucas Parra, Saulo DS Reis, José Andrade Jr, Shlomo Havlin, and Hernán Makse. 2013. Origins of power-law degree distribution in the heterogeneity of human activity in social networks. Scientific reports, 3(1):1783. OpenAI. 2024. Disrupting deceptive uses of ai by covert influence operations. Amy Orben, Andrew Przybylski, Sarah-Jayne Blakemore, and Rogier Kievit. 2022. Windows of developmental sensitivity to social media. Nature Communications, 13(1):1649. Folco Panizza, Piero Ronzani, Tiffany Morisseau, Simone Mattavelli, and Carlo Martini. 2023. How do online users respond to crowdsourced fact-checking? Humanities and Social Sciences Communications, 10(1):111. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2022. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 118. Joon Sung Park, Carolyn Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, and Michael Bernstein. 2024. Generative agent simulations of 1,000 people. arXiv preprint arXiv:2411.10109. Uma Patel. 2024. New partnerships and initiatives to strengthen fact-checking online. Gordon Pennycook, Adam Bear, Evan Collins, and David Rand. 2020. The implied truth effect: Attaching warnings to subset of fake news headlines increases perceived accuracy of headlines without warnings. Management science, 66(11):49444957. Gordon Pennycook, Ziv Epstein, Mohsen Mosleh, Antonio Arechar, Dean Eckles, and David Rand. 2021. Shifting attention to accuracy can reduce misinformation online. Nature, 592(7855):590595. Gordon Pennycook and David G. Rand. 2021. The psychology of fake news. Trends in Cognitive Sciences, 25:388402. Yao Qu and Jue Wang. 2024. Performance and biases of large language models in public opinion simulation. Humanities and Social Sciences Communications, 11(1):113. Durga Raghunath and Surabhi Malik. 2024. Partnering with news publishers and fact checkers to tackle misinformation ahead of the indian general elections 2024. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Mohammed Saeed, Nicolas Traub, Maelle Nicolas, Gianluca Demartini, and Paolo Papotti. 2022. Crowdsourced fact-checking at twitter: how does the crowd compare with experts? In Proceedings of the 31st ACM international conference on information & knowledge management, pages 17361746. Thomas Schelling. 1971. Dynamic models of segregation. Journal of mathematical sociology, 1(2):143 186. Kirill Solovev and Nicolas Pröllochs. 2022. Moral emotions shape the virality of covid-19 misinformation on social media. In Proceedings of the ACM web conference 2022, pages 37063717. Briony Swire-Thompson, David Lazer, et al. 2020. Public health and online misinformation: challenges and recommendations. Annu Rev Public Health, 41(1):433451. Joseph Uscinski and Ryden Butler. 2013. The epistemology of fact checking. Critical Review, 25(2):162180. Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. science, 359(6380):11461151. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. 2024. Sotopia-π: Interactive learning of socially intelligent language agents. arXiv preprint arXiv:2403.08715. Uri Wilensky. 1999. Netlogo. 1.0. Liang Wu, Fred Morstatter, Kathleen Carley, and Huan Liu. 2019. Misinformation in social media: definition, manipulation, and detection. ACM SIGKDD explorations newsletter, 21(2):8090. Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, et al. 2024. Oasis: Open agents social interaction simulations on one million agents. arXiv preprint arXiv:2411.11581. Zhenhua Yu, Si Lu, Dan Wang, and Zhiwu Li. 2021. Modeling and analysis of rumor propagation in social networks. Information Sciences, 580:857873. Savvas Zannettou, Michael Sirivianos, Jeremy Blackburn, and Nicolas Kourtellis. 2019. The web of false information: Rumors, fake news, hoaxes, clickbait, and various other shenanigans. Journal of Data and Information Quality (JDIQ), 11(3):137. Zilong Zhao, Jichang Zhao, Yukie Sano, Orr Levy, Hideki Takayasu, Misako Takayasu, Daqing Li, Junjie Wu, and Shlomo Havlin. 2020. Fake news propagates differently from real news even at early stages of spreading. EPJ data science, 9(1):7. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. 2024. Sglang: Efficient execution of structured language model programs. Advances in Neural Information Processing Systems, 37:62557 62583. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Interactive evaluation for social 2023. Sotopia: arXiv preprint intelligence in language agents. arXiv:2310.11667."
        },
        {
            "title": "A Persona Generation Details",
            "content": "Here we describe the questions that we sampled and generated for the agent users. The generated personas are stored in JSONL format, with each entry containing unique identifier, descriptive narrative, and associated behavioral labels. A.1 Persona Replication from Human Survey The persona generation method begins by transforming structured survey responses collected from Prolific participants into rich, natural language character descriptions suitable for use in agentbased simulations. Each participants responses covering wide range of personal, demographic, social, and psychological traitsare encoded in JSONL format, where each line corresponds to different individual. We first this file into list of Python dictionaries, each representing single participants answers. The preprocessing pipeline then embeds each answer into templated sentence structure. This includes details such as age, gender, residential background, number of places lived, favorite activities, values, political stance, income, ethnicity, language, education, religion, social tendencies, hobbies, relationship values, personality, future goals, significant life events, friendship values, and hypothetical financial decisions. By expressing these traits in fluent, first-personstyle English, the function essentially replicates each participants worldview and identity into lifelike persona that can guide agent behavior in social simulations. In the final step iterates through all participant entries, generates the corresponding natural language persona for each one, and writes the enriched dataincluding both the original responses and the generated descriptionback into new JSONL file. This process creates bridge between raw human survey data and psychologically grounded agent profiles, enabling more realistic and diverse behaviors in multi-agent environments. A.2 Synthetic Persona from Agent Bank In contrast to the human-annotated personas derived from survey responses, we also generate fully synthetic personas by sampling from structured question bank, referred to as the Agent Bank. This bank contains curated set of 23 multiplechoice questions covering key dimensions of identity, background, and social orientationranging from age and gender to values, education, hobbies, and political affiliations. Please refer to the code repository for the complete content and answer choices of each of them. Each question is assigned label and fixed set of possible answers. To simulate human-like diversity, we construct agent personas by probabilistically sampling answers from these options, sometimes using uniform random choice and other times leveraging carefully constructed distributions to better mirror real-world population dynamics. For instance, age is generated from normal distribution centered at 35 with bounds clamped between 18 and 60, while gender is sampled from distribution reflecting approxi12 mate societal proportions. In some cases, dependencies between traits are explicitly modeledfor example, primary language is sampled conditionally based on persons ethnicity using manually specified probability distributions that reflect linguistic prevalence across ethnic groups. These sampled answers are then assembled into dictionary of attributes. From this, we use one of two methods to generate natural language persona descriptions. The first method uses hardcoded template that deterministically weaves the sampled answers into coherent paragraph, mimicking the style and structure used for real survey-based personas. The second, more dynamic method leverages GPT-4o to produce creative and varied persona descriptions from the same underlying attributes. carefully crafted system prompt instructs the model to retain every single piece of information from the attribute dictionary while generating single fluent paragraph in the second person, presenting the result as believable and detailed backstory. This ensures that each agent maintains consistent and complete identity while allowing room for stylistic diversity. Ultimately, each synthetic persona is stored as structured JSON object containing unique ID, the full natural language description, and the associated label-value pairs, ready to be deployed as agents in downstream simulations. A.3 User Generation and Instantiation The foundation of the simulation lies in the creation of realistic individual agentic virtual users. Each agent is instantiated with detailed persona that shapes their online behavior and engagement patterns. Persona Generation As illustrated in Figure 1, personas are generated using combination of predefined questions and sampling from probabilistic distributions stored in an agent_bank profile collection, inspired by Park et al. (2024). Key demographic attributes such as age, gender, ethnicity, and primary language are assigned probabilistically to mirror real-world distributions. For instance, age follows normal distribution centered around 35 years, while other attributes are sampled based on predefined probabilities. We disclose these questions and describe more detail of the methodology in Appendix A. After synthesizing the structured profiles for agents, we construct natural language description for each of them. This process leverages mixture of deterministic rules and LLM-based augmentation using GPT-4o (Hurst et al., 2024) to enhance diversity and realism. User Instantiation Once personas are generated, they are instantiated as agent users within the simulation. Each agent is assigned unique user ID and persona profile that includes background details and interest labels. The relational database serves as the backbone for recording agent activities, ensuring persistent storage of interactions, post engagements, and behavioral updates. This database facilitates dynamic user tracking and enables postsimulation analysis of engagement trends and content spread. We provide more implementation details in Appendix D."
        },
        {
            "title": "B Details of the Human Study",
            "content": "The study was open to 20,240 eligible participants from larger Prolific population of 232,330, and we collected 204 valid responses from eligible participants. The survey was conducted via Prolific to collect responses from U.S.-based participants fluent in English. Participants were asked to complete 12-minute survey assessing their demographic characteristics and social media interactions. The survey, hosted on Google Forms, required no software downloads or special device features and was accessible via mobile, tablet, or desktop. Participant recruitment applied custom screening for language, political spectrum, vaccine opinion, and prior participation, ensuring targeted sample. Responses were collected using Prolific ID via question at the start of the form, and participants received completion code upon finishing. Compensation was set at $2.40 per participant, equivalent to $12.00/hour, and submissions were manually reviewed before approval. The median completion time was approximately 14.5 minutes. All members on our research team have obtained IRB approval before the human study was conducted. Our study costs total of $480 for participant payment and $160 of platform fee. Figure 7 shows the complete breakdown of the 9 key demographic distributions of the 204 human participants. B.1 Per-Demographic Attribute Engagement"
        },
        {
            "title": "Pattern",
            "content": "We also analyzed the reaction patterns between human participants and persona-driven agents grouped by specific demographic attributes such as age, gender, income, ethnicity, etc, as shown in Table 1. 13 Figure 7: Demographic Distributions of Study Participants. The analysis of engagement patterns between humans and agents reveals some differences across various demographic groups. Specifically, the age group 25-34 shows notable differences in shares, while males exhibit significant variations in both likes and shares. Among religious groups, Hinduism and Islam display significant differences in likes and shares, respectively. Ethnic groups such as Hispanic or Latino, Black or African American, and Asian show significant differences in shares and comments. Education levels also play role, with secondary education and doctorate degree holders showing significant differences in shares and likes, respectively. Income levels between $10,000 - $19,999 and $70,000 - $79,999 show significant differences in comments and likes. Political stances such as Conservative and Very Conservative also exhibit significant differences in shares and likes. In contrast, many other demographic groups, including various age ranges, genders, religions, ethnicities, education levels, income brackets, and political stances, show no significant differences in engagement types. Overall, out of 52 demographic groups analyzed, 14 show significant differences in one or more engagement types, while 38 do not. The criteria for significance were based on p-value of less than 0.05 in statistical comparisons. The results suggest that agents may be more adept at simulating the engagement patterns of \"common\" or more broadly represented demographic groups in LLM pretraining data, as indicated by the lack of significant differences in many of these groups."
        },
        {
            "title": "C An Extended Version of Related Work",
            "content": "Behavioral Economics and Persuasian Games. Our system computationally models sequential persuasion game with LLM-powered agents conditioned on fine-grained personas (Kamenica and Gentzkow, 2011; Gentzkow and Kamenica, 2017; Acemoglu et al., 2023). The agents interact within directed social graph and evolve based on memory and social context. This framework serves as testbed for studying online behaviors, intervention 14 strategies, and the impact of algorithmic moderation. AI-Driven Social Simulations and Generative Agents. The emergence of large language models (LLMs) has significantly advanced the capabilities of agent-based social simulations, enabling more sophisticated, context-aware interactions. Traditional agent-based modeling relied on predefined rule sets and heuristics, limiting adaptability and realism. Early computational social simulations, such as Schellings segregation model (Schelling, 1971), Sugarscape (Epstein and Axtell, 1996) and NetLogo-based models (Wilensky, 1999), provided insights into social dynamics but lacked the ability to generate nuanced, context-dependent behaviors. Recent advances, such as Smallville (Park et al., 2023), AgentVerse (Chen et al., 2024a), Internetof-Agents (Chen et al., 2024b), and Chirper (Minos, 2023), leverage LLMs to enable generative agents that dynamically respond to evolving contexts. These systems showcase how AI-powered agents can engage in lifelike conversations, form social relationships, and simulate content dissemination patterns. However, despite their ability to generate plausible interactions, generative agents can still exhibit inconsistencies due to biases inherent in LLM training data or limitations in longterm memory and reasoning. By integrating more structured constraints and iterative feedback mechanisms, this work enhances the reliability of agentbased simulations for social science research and policy testing. Misinformation Spread and Fact-Checking Mechanisms. The spread of misinformation on digital platforms has been extensively studied (Swire-Thompson et al., 2020; Jerit and Zhao, 2020; Wu et al., 2019; Islam et al., 2020), with empirical evidence showing that falsehoods often propagate more rapidly and broadly than factual information (Vosoughi et al., 2018). The virality of misinformation is attributed to its emotional appeal, novelty, and the role of engagement-driven algorithms that inadvertently amplify misleading narratives (Pennycook and Rand, 2021; Solovev and Pröllochs, 2022). Addressing this issue has led to the development of multiple fact-checking methodologies, including third-party verification, algorithmic detection, and crowdsourced moderation. Third-party fact-checking, typically conducted by organizations such as Snopes7, PolitiFact8, or Googles partnerships with external organizations (Raghunath and Malik, 2024; Patel, 2024), provides authoritative assessments but faces challenges in scalability and timeliness (Zannettou et al., 2019; Uscinski and Butler, 2013; Marietta et al., 2015). Crowdsourced fact-checking such as Xs Community Notes,9 on the other hand, leverages collective intelligence (Panizza et al., 2023) but introduces risks related to expertise and susceptibility to group biases (Saeed et al., 2022; Pennycook et al., 2021). There is no consensus on which factchecking approach is more effective, nor is it wellunderstood how different moderation strategies interact. This study addresses this gap by leveraging LLM-driven simulations to evaluate different factchecking mechanisms within controlled environments. By testing various moderation strategies in scalable, repeatable manner, this work provides insights into the comparative efficacy of communitybased, third-party, and hybrid fact-checking interventions in mitigating misinformation. Simulations as Tools for Policy and Platform Governance The use of computational simulations as decision-support tools has been wellestablished in domains such as epidemiology (Currie et al., 2020; Lorig et al., 2021), economics (Axtell and Farmer, 2022), and public policy (Qu and Wang, 2024). By enabling scenario testing before real-world implementation, simulations help policymakers anticipate the consequences of interventions (Charalabidis et al., 2011). In the context of social media governance, AI-driven simulations present an emerging opportunity to evaluate moderation strategies, optimize intervention policies, and test the societal impact of algorithmic changes before deployment. Recent discourse around AI governance emphasizes the need for proactive measures to ensure platform accountability and transparency (Landau et al., 2024). Regulatory bodies and platform operators are increasingly exploring ways to assess the impact of interventions such as content moderation adjustments, ranking algorithm changes, and misinformation mitigation strategies before rolling them out at scale. To this end, our research introduces AI-driven social simulations as novel framework for governance experimentation. By 7https://www.snopes.com/ 8https://www.politifact.com/ 9https://communitynotes.x.com/guide/en/ 15 simulating diverse social environments and misinformation dynamics, we provide an approach that offers scalable, controlled setting for testing policy interventions. This methodology aligns with the growing call for algorithmic auditing and regulatory sandboxes, providing novel tool for both researchers and policymakers to refine governance strategies before real-world application."
        },
        {
            "title": "D Database Schema",
            "content": "In this section, we describe the database schema that we developed to store and keep track of all the data generated by each simulation run. This relational SQL database schema is designed to support social media simulation in which LLMpowered AI agents mimic user behaviors. The database captures and organizes user-generated content, interactions, and system-level processes in detail. The users table stores individual user profiles, including metadata such as personas, background labels, influence scores, and engagement metrics. Posts authored by users are managed in the posts table, which records content details, interaction counts (likes, shares, flags, comments), and moderation or fact-check statuses. Social relationships are modeled through the follows table, which tracks follower-followed connections. User engagement actions, such as creating content or reacting to posts, are logged in the user_actions table. Comments on posts are separately recorded in the comments table with their associated metadata. Community moderation is facilitated via the community_notes and note_ratings tables, enabling users to contribute interpretive notes and rate their helpfulness. System moderation decisions are logged in moderation_logs. The fact_checks table provides detailed verdicts and rationales from fact-checking processes. To simulate memory and reasoning for AI agents, agent_memories track the content and importance of internal memories, with timestamps and decay factors. The spread_metrics table quantifies the virality and diffusion dynamics of each post over time steps, including derived interaction statistics and takedown decisions. Exposure to content is tracked at the user level in the feed_exposures table, supporting the analysis of information visibility and reach. Together, these schemas capture detailed and interconnected view of simulated social media dynamics, grounded in observable user behavior and system responses."
        },
        {
            "title": "E Agent Action Space",
            "content": "Figure 8: Agents action space. In our simulated social media environment, each agentpowered by large language modelis instantiated with predefined action space that governs its interactions within the platform. These agents are configured with unique personas via Config (persona) generation module and endowed with Memory / reflection module that allows them to recall and adapt based on past experiences. The Action space outlines the full spectrum of behaviors an agent can exhibit: they can share posts, comment or like comments, create or like posts, and flag inappropriate content. Additionally, agents can retrieve their feeds, follow or unfollow other users, or ignore content or interactions altogether. These discrete actions simulate realistic user behavior and social dynamics, enabling rich, emergent interactions in the environment. E.1 Agent Decision-Making Process The agent decision-making process is governed by structured interactions between feed presentation, memory recall, and reasoning mechanisms. Feed Presentation Each agents feed aggregates posts from followed users, supplemented by additional trending content and news articles. On average, one in ten posts is from the NewsGuard dataset and contains misinformation. Posts are displayed with metadata such as engagement counts (likes, comments, shares) and fact-checking signals (flags, community notes, third-party verdicts). This metadata provides context for the agents engagement decisions. Memory and Reflection Module We implement an AgentMemory module which manages the mem16 ory and reflection capabilities of each agent. Memories are categorized into interactions (e.g., past engagements) and reflections (high-level insights derived from past behaviors). Each piece of memory is assigned an importance score, which decays over time unless reinforced by further interactions. The decay function ensures that long-term behaviors emerge naturally based on experience. Please refer to Appendix for more details of the Memory module. Periodically, agents generate reflections based on recent interactions. These reflections help detect behavioral patterns, relationship dynamics, and potential biases, influencing future content engagement and decision-making. Agent Decision-Making and Action Execution Agents make decisions based on combination of persona-driven heuristics, memory retrival, and reasoning prompts. The AgentPrompts module formulates structured decision prompts, guiding agents through content engagement options such as liking, sharing, or flagging post. When engaging, agents provide reasoning for their actions, influenced by (1) Personal Beliefs and Persona Traits: Agents weigh content credibility based on their ideological stance and historical preferences, (2) Engagement Signals: Highly engaged posts are more likely to be reshared due to social validation effects, and (3) Fact-Checking Feedback: Agents integrate fact-checking signals into their reasoning, adjusting their trust in flagged content accordingly. Once decision is made, the agents action gets recorded in the relational database, along with updated post metrics, engagement, and new memories. The importance of each interaction is evaluated based on emotional intensity, action strength, and alignment with the agents goals. odic reflective updates. Each agent is instantiated from detailed persona descriptions provided via an external JSONL file, and operates using the GPT4o engine with decoding temperature of 1.0 to promote diversity in generated responses. Agents can be configured to create original posts independently, or they can be prompted to only respond to posts depending on the setting. Once the simulation environment is initiated, an agents feed consists of mixture of up to default of 15 posts from followed users and 10 from non-followed users, drawn from pool that includes up to 20 injected news items per run. Initial social ties are sparse, with 10% probability of following another user at initialization, and new user addition and follow behaviors are disabled during the simulation. All of the above numbers are configurable. The experiment evaluates one of the four fact-checking intervention modes described in Section 3, combining both third-party and community-based mechanisms. For each step, if fact-checking agent is enabled, then number of posts are selected for potential moderation, with fact-checking outputs generated using low-temperature (0.3) setting and required to include reasoning. Thresholds are specified for flagging and notetaking behavior if moderation is set active. Periodically, agents reflect on their recent interactions, update memory states, and check their internal objectives, offering framework for studying emergent behavior, information diffusion, and intervention efficacy in artificial societies."
        },
        {
            "title": "G Details of the Memory Module",
            "content": "Memory relevance is computed as: Relevance = Importance Decay The decay factor is defined as:"
        },
        {
            "title": "F Detailed Experiment Configuration",
            "content": "Decay = max(0, PrevDecay αt) We describe our experimental settings and configurable variables in more detail in this section. In our simulation, we model dynamic social network of an arbitrary number of (practically in our experiments up to over 200) LLM-driven agents interacting over the course of number of discrete time steps. The simulation loop follows structured core cycle that includes initializing the environment, assigning new users probabilistically (though this one could be disabled in certain runs), content creation, feed-based reactions, and periwhere α is the decay rate (default 0.1), and is the time (in days) since last access. New memories start with PrevDecay = 1.0. memory is considered relevant if Relevance 0.3. Both Importance and Decay are in [0, 1]. Importance Scoring. Each memory has base importance score of 0.5. This is increased by 0.1 for each keyword match (up to max of 1.0) from the following semantic categories: Emotional: love, hate, angry, happy, sad 17 Action: achieved, failed, learned, discovered Relationship: friend, follow, connect, share Goal: objective, target, aim, purpose"
        },
        {
            "title": "Let k be the number of keyword matches in the",
            "content": "memory content. Then: Importance = min(1.0, 0.5 + 0.1k)"
        },
        {
            "title": "This value is combined with the decay factor to",
            "content": "compute final relevance."
        },
        {
            "title": "Popularity",
            "content": "H.1 Power-Law Distribution of User"
        },
        {
            "title": "Popularity",
            "content": "First we define the popularity of users as sum of the number of followers, number of likes, shares, and comments received. We collected the top 50 users and plotted their popularity (as measured by the sum of engagement received by them) from highest to lowest in Figure 5. We observe power-law distribution of user influence. We have (x) = 120x0.6 as the best-fitting power-law approximation of our sampled data, as shown in Figure 6. With α = 0.60, our regression line has an R2 = 0.84, suggesting strong fit to our user engagement data and that our social system follows typical power law distribution where few users generate most of the engagement. Existing analysis on real-world social networks suggest that this power-law exponent usually ranges from 1.5-2.5 depending on the specific context (Muchnik et al., 2013; Bild et al., 2015). Our best-fit exponent is lower than these reported numbers, but it still illustrates clear trend that minority of users/content collect most of the engagement while the majority of them do not contribute nearly as much. In the rest of this section, we explore potential reasons why this distribution emerges, and through series of analyses leveraging our simulated environment, we reveal the unpredictability of influence or popularity in online social networks. More fundamentally, we argue that perhaps LLM-driven agents have tendency to simply copy the decisions of agents who act before them. This results in the preferential attachment and as natural consequence establishes the power-law distribution of engagement pattern. Such pattern does not necessarily stem from anything else such as users profile details, or the content they post about. And even their own \"inner reasoning\" might not reveal their true decision-making, which invites further investigation on the authenticity of LLM agents self-expressed reasoning traces. H.2 Persona Attributes Dont Correlate with"
        },
        {
            "title": "Engagement",
            "content": "We analyzed user engagement by comparing the top 50 most engaged users (highest number of followers, likes, shares, comments, etc.) with the bottom 50 least engaged users across several attributes. The Chi-square test results summarized in Table 3 indicate that there are no statistically significant differences in the distributions of age group, gender, activity type, hobby, ethnicity, income level, political affiliation, or primary goal between the two groups. Although some attributes, such as ethnicity and hobby, exhibited medium effect sizes (Cramers (Cramér, 1946) of 0.319 and 0.302, respectively), their associated p-values did not reach conventional levels of statistical significance. This suggests that the attributes examined do not notably influence the level of user engagement. Notably, we did not include personas resembling real-world public figures or celebrities, whose presence might have substantially influenced content popularity. Our findings thus suggest that, when personas are initialized randomly, some users naturally attract significantly more attention and engagement, independent of the specific attributes assigned during their initialization. This underscores the inherent variability and unpredictability of user engagement in social platforms. H.3 Do Content Topics Matter? Our analysis aimed to directly investigate the correlation between content topics and user engagement. To accomplish this, we first computed an engagement score for each post by summing its likes, shares, and comments. We then cleaned and preprocessed the textual content of the posts to ensure accurate topic modeling. For topic extraction, we employed unified topic model based on BERTopic (Grootendorst, 2022), utilizing sentence embeddings from the SentenceTransformer model all-MiniLM-L6-v2 (Reimers and Gurevych, 2019).BERTopic was chosen due to its effectiveness in capturing nuanced semantic relationships within short text content. By fitting single topic model to all posts, we ensured consistency and comparability across the identified topics. Following topic assignment, we conducted detailed statistical analysis. Engagement metricsincluding mean, median, and standard deviation for likes, shares, comments, and overall engagement scoreswere calculated for each topic. To statistically assess whether variations in engagement across topics were significant, we performed an ANOVA (Analysis of Variance). The key statistical finding from the analysis was an ANOVA result yielding an F-statistic of 0.614 and p-value of 0.84. This indicates no statistically significant relationship between the topics and overall engagement levels. In other words, statistically, the topic of post alone does not reliably predict its engagement level. H.4 Clues from Agents Own Reasoning"
        },
        {
            "title": "Traces and Recommender System",
            "content": "The lack of clear correlation between user profiles, content topics, or temporal properties, and engagement patterns suggested that maybe the way we present feed to the agents has an influence on what content ends up being popular. Here we discuss our feed prioritization algorithm. Our simulation does not employ sophisticated recommender system. Our feed prioritization in the simulation relies primarily on recency and existing follow relationships, rather than explicit engagement metrics such as likes or shares. Posts, regardless of whether theyre from followed or non-followed users, are generally ordered based on creation time, ensuring that newer content receives greater visibility. However, content from followed users gains additional prioritized exposure due to dedicated allocations in the feed. This structure might create followbased feedback loop: when User follows User A, As posts consistently appear in Bs feed, enhancing As opportunities for engagement through likes, comments, and shares. Higher engagement subsequently boosts As visibility to other users who view these interactions, increasing the likelihood of additional follows and further amplifying this cycle. Agents Reasoning Pattern We extract and analyze agent reasoning across several dimensions, including sentiment, motivation, entity and concept extraction, and word-frequency analysis. The analysis specifically focused on identifying patterns related to different engagement actions (such as likes, comments, and shares), exploring how post content and user backgrounds influenced reasoning, and examining common linguistic trends. The analysis of agent reasoning reveals patterns in how agents engage with content and users on social media. As shown in Figure 4, agents demonstrate clear and distinct emotional sentiment patterns associated with different types of actions. Positive-dominant actions such as following users (99% positive sentiment), commenting (97%), liking posts (92%), and sharing content (92%) indicate that agents predominantly perceive their interactions as constructive contributions. Conversely, negative sentiment predominantly characterizes actions like flagging posts (71% negative) and unfollowing users (40% negative), reflecting agents use of these interactions primarily for expressing disapproval or concern."
        },
        {
            "title": "Further examining motivational",
            "content": "reasoning, agents apply distinct frameworks depending on the nature of their engagement. Content evaluation actions, such as flagging posts, are predominantly motivated by information quality assessments (49%) and concerns regarding misinformation (22%). Sharing decisions primarily reflect agreement with content (46%). In contrast, relationship-building actions show different motivations: liking is heavily driven by social connection potential (34%), commenting balances agreement (29%) and social connection (28%), and following users reflects diverse personal interests (27%). Vocabulary analysis further emphasizes these distinctions, revealing specialized linguistic patterns for each type of engagement. Flagging content uses specific moderation-related language such as \"misinformation,\" \"harmful,\" and \"credible,\" whereas community-oriented engagements like sharing, liking, and commenting frequently reference concepts like \"community,\" \"support,\" and \"alignment.\" Following actions highlight terms related to content curation and long-term value, including \"consistently,\" \"valuable,\" and \"insights.\" Interestingly, despite these detailed reasoning frameworks, low alignment (21.4%) between post sentiment and agent reasoning indicates that agents explicit justifications may not fully reflect the underlying factors driving engagement. Instead, engagement decisions appear largely guided by personal values alignment, information quality assessments, community-building potential, and personal relevance rather than simple emotional resonance with content. These insights also highlight notable contra19 diction with prior analyses, which showed no significant correlation between user demographics or content topics and overall engagement popularity. While agents clearly articulate their engagement motivations in terms of specific frameworks (values alignment, informational quality, social connection), these explanations alone do not robustly predict broad engagement patterns. This paradox suggests that engagement is heavily individualized, contextual, and possibly influenced by network effectssuch as who posts content, existing social validation, or content placement within social feedsfactors not fully captured by demographic or topical categorizations alone. In essence, the analysis confirms that agents employ reasoning structures tailored to the type of engagement but reveals that actual engagement outcomes are influenced by nuanced individual interpretations and contextual social dynamics. This misalignment between LLMs internal decisionmaking and explicit surface behavior is also consistent with findings observed by prior work (Liu et al., 2023). Recognizing these complexities is essential for understanding and anticipating the unpredictability in social media engagement."
        },
        {
            "title": "I Prompts",
            "content": "I.1 Post Creation Prompt \"\"\" Given: - persona: str, - memories_text: str, - recent_posts_text: str, - feed_text: str \"\"\" Create social media tweet for user with the following characteristics: Background: {persona} Your recent memories and experiences: {memories_text if memories_text else \"No relevant memories.\"} IMPORTANT: - Avoid repeating similar topics or themes from your recent posts - Try to bring fresh perspectives or discuss different aspects of your interests - Feel free to engage with or reference one or more recent posts from your feed when relevant - If there's breaking news in your feed, consider engaging with it if it aligns with your interests, whether you agree or disagree You don't need to always use emojis every time you write something. Consider the posts that you have made recently. Try to diversify your content and style. For example, avoid always starting post with the same phrase like \"just ...\" The post you are about to create is: I.2 Feed Reaction Prompt def create_feed_reaction_prompt( persona: str, memories_text: str, feed_content: str, reflections_text: str = \"\", experiment_type: str = \" third_party_fact_checking\", include_reasoning: bool = False ) -> str: # Base prompt that's common across all experiment types base_prompt = f\"\"\"You are browsing your social media feed as user with this background: {persona} Recent memories and interactions: {memories_text if memories_text else \"No relevant memories.\"} Your feed: -------------------------------- {feed_content if feed_content else \"No recent feed posts.\"} -------------------------------- Your past reflections: {reflections_text if reflections_text else \"N/A\"} Posts you've made recently: {recent_posts_text if recent_posts_text else \"No recent posts.\"} Based on your persona, memories, and the content you see, choose how to interact with the feed. Recent posts by other users in your feed: {feed_text if feed_text else \"No recent feed if not experiment_type: raise ValueError(\"Experiment type is posts.\"} required\") \"\"\" The post should be authentic to the user's # Add experiment-specific instructions persona and background and can reference your past experiences. Keep it concise and suitable for social media platform. and valid actions if experiment_type == \"no_fact_checking\": base_prompt += \"\"\" 20 Table 4: Agent Reasoning for Content Engagement Analysis Action Type Total Actions (%) Positive (%) Neutral (%) Negative (%) Top 2 Reasoning Cateshare_post 1382 (30.1%) flag_post comment 1126 (24.6%) 880 (19.2%) follow_user 719 (15.7%) like_post 463 (10.1%) ignore 9 (0.2%) unfollow_user 5 (0.1%) 91.8 13.5 96. 98.9 92.0 77.8 20.0 Valid actions: - like-post // [post_id] - share-post // [post_id] - comment-post // [post_id] with [content], limited to 250 characters - ignore Interact with posts and users based on your interests and beliefs. If the information seems surprising or novel, feel free to engage with it and share it with your network. \"\"\" elif experiment_type == \" third_party_fact_checking\": base_prompt += \"\"\" Valid actions: - like-post // [post_id] - share-post // [post_id] - comment-post // [post_id] with [content], limited to 250 characters - ignore \"\"\" elif experiment_type == \" community_fact_checking\": base_prompt += \"\"\" You can add community notes to posts that you think need additional context or factchecking. You can also rate existing community notes as helpful or not helpful based on their accuracy and usefulness. Valid actions: - like-post // [post_id] - share-post // [post_id] - comment-post // [post_id] with [content], limited to 250 characters - add-note // [post_id] with [content] - Add community note to provide context or factchecking 1.6 71.0 0.5 0.3 0.2 22. 40.0 gories agreement (46.3%) social_connection (15.7%) information_value (48.8%) misinformation (22.4%) agreement (29.2%) social_connection (27.6%) personal_interest (27.4%) information_value (24.4%) social_connection (33.6%) agreement (23.6%) information_value (36.4%) personal_interest/agreement (18.2%) agreement (50.0%) emotional_reaction (16.7%) you have additional context to provide. 6.6 15. 2.7 0.8 7.8 40.0 \"\"\" elif experiment_type == \" hybrid_fact_checking\": base_prompt += \"\"\" Pay attention to both official fact-check verdicts and community notes on posts. You can add your own community notes and rate existing ones, while also considering official fact-checks. Valid actions: - like-post // [post_id] - share-post // [post_id] - comment-post // [post_id] with [content], limited to 250 characters - add-note [post_id] with [content] - Add community note to provide context or factchecking - rate-note [note_id] as [helpful/not-helpful] - Rate existing community notes - ignore \"\"\" base_prompt += \"\"\" THESE ARE THE ONLY VALID ACTIONS YOU CAN CHOOSE FROM. \"\"\" # Add reasoning instructions if enabled if include_reasoning: base_prompt += \"\"\" For each action you choose, give brief reasoning explaining your decision. \"\"\" base_prompt += \"\"\" Respond with JSON object containing list of actions. For each action, include: - action: The action type from the valid actions list - rate-note // [note_id] as [helpful/not-helpful - target: The ID of the post/user/comment/note ( ] - Rate existing community notes not needed for 'ignore') - ignore - content: Required for comment-post and addnote actions If you see existing community notes on post, \"\"\" first consider rating them as helpful or not helpful, and then add your own note ONLY if # Add reasoning field to example if 21 enabled if include_reasoning: base_prompt += \"\"\" 2. Changes in your relationships 3. Evolution of your interests 4. Potential biases or preferences you've - reasoning: brief explanation of why you took developed 5. Goals or objectives you might want to pursue Provide thoughtful reflection that could guide your future behavior. Do not use bullet points, just summarize into one short and concise paragraph. I.4 Fact-checking Prompt \"\"\" Given: - post_content: str, - community_notes: str, - engagement_metrics: dict \"\"\" Please fact-check the following social media post: Content: {post_content} Engagement Metrics: - Likes: {engagement_metrics['likes']} - Shares: {engagement_metrics['shares']} - Comments: {engagement_metrics['comments']} {community_notes} Please analyze this content and provide: 1. verdict (true/false/unverified) - if you are unsure, mark it as unverified 2. detailed explanation of your findings 3. Your confidence level (0.0 to 1.0) 4. List of sources consulted If the post mentions time that is in the future or has content that is outside of your knowledge scope, you should mark it as unverified. For obvious misinformation, you should mark it as false. this action \"\"\" # Add note_rating field for relevant experiment types if experiment_type in [\" community_fact_checking\", \" hybrid_fact_checking\"]: base_prompt += \"\"\" - note_rating: Required for rate-note actions (\" helpful\" or \"not-helpful\") \"\"\" # Example response if include_reasoning: base_prompt += \"\"\" Example response: { \"actions\": [ { \"action\": \"like-post\", \"target\": \"post-123\", \"reasoning\": \"This post contains valuable information\" }, { \"action\": \"share-post\", \"target\": \"post-123\", \"reasoning\": \"I want to spread this important news\" ] }\"\"\" } else: base_prompt += \"\"\" Example response: { \"actions\": [ \"action\": \"like-post\", \"target\": \"post-123\" { }, { } ] }\"\"\" return base_prompt I.3 Reflection Prompt Based on your recent experiences as social media user with: Background: {persona} Recent memories and experiences: {memory_text} Reflect on these experiences and generate insights about: 1. Patterns in your interactions 22 \"action\": \"share-post\", \"target\": \"post-123\" Format your response as structured verdict with these components."
        }
    ],
    "affiliations": [
        "MIT CSAIL",
        "University of California, Los Angeles"
    ]
}