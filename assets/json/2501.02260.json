{
    "paper_title": "MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control",
    "authors": [
        "Mengting Wei",
        "Tuomas Varanka",
        "Xingxun Jiang",
        "Huai-Qian Khor",
        "Guoying Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Mengting Wei, Tuomas Varanka, Xingxun Jiang, Huai-Qian Khor, Guoying Zhao, Fellow, IEEE"
        },
        {
            "title": "Control",
            "content": "5 2 0 2 4 ] . [ 1 0 6 2 2 0 . 1 0 5 2 : r AbstractWe address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific persons expression in fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through selfattention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace. Index TermsAction unit, Facial expression editing, Diffusion models. I. INTRODUCTION It is perennial challenge in computer vision to realistically change the expression of closeup while preserving the persons identity and other attributes either from background or other face characteristics. The challenge of this problem arises from the lack of intuitive and interpretable depiction to represent facial expressions that can support customized expressions, and previous work usually addresses this with latent space of expressions, where the codes are learned from large expression dataset or the off-the-shelf ones like 3DMM parameters [1][4]. These methods ignore the fact that the semantic meanings of these codes are implicit, which poses challenges for interpretable, arbitrary and flexible manupulation of expression by non-professionals. In this work, we show that its possible to convincingly alter persons expression, in user-friendly manner by offering localized control with adjustable intensity, while preserving their identity and other attributes from the portrait. Our key insight is to employ action units (AUs) to represent facial expressions, and then steer M. Wei, T. Varanka, H. Khor and G. Zhao are with the Center for Machine Vision and Signal Analysis, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland. E-mail: {mengting.wei, chen.haoyu, yante.li, guoying.zhao}@oulu.fi. X. Jiang is with the Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering, Southeast University, Nanjing 210096, China, and is also with the Center for Machine Vision and Signal Analysis, Faulty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland (e-mail:jiangxingxun@seu.edu.cn). *Corresponding author Stable-Diffusion model to produce high-quality expression editing results. In editing facial expressions, the crucial question is which representation should be used to encode expressions. Facial AUs, as anatomical markers of facial muscle activity, have proven effective for precise and flexible facial expression description in images [5][7]. However, most AU annotations are of frontal faces of limited number of subjects in laboratory settings, which is easy to cause overfitting when used for training. model trained by this type of data is hard to generalize on individuals with different poses and backgrounds, which we will present in Section IV-C. To deal with this issue, off-the-shelf AU intensity estimators like Libreface [8] used knowledge distillation technique from large-scale network pre-trained on natural images to accommodate AU intensity estimation task on the lab datasets. Such automatic tools provide us with AU intensity estimation of high accuracy, hence we can produce AU estimation of any face image and use the estimation as AU condition to train our model. On the other hand, diffusion models have emerged as popular choice for image generation, surpassing Generative Adversarial Networks (GANs) with higher generation quality [9][12]. ControlNet-style models enable users to add additional control signals such as depth, paintings, skeleton pose as the condition of the target [13]. Some works modify it to enable some basic expressions like happiness and sad [13], [14], but many of their editing results are with extreme face deformations that may look unrealistic. More importantly, these models can not provide specific controls over expressions, whether through text or through images. Such specific controls, including intensity and location of expressions, are the focus of our work. To merge the advantages of both worlds, we propose MagicFace, model that allows to correlate AU changes to facial expressions and then produce photorealistic edited image conditioned on AU variations. Specifically, MagicFace first extracts AU intensities from portrait photographs using an off-the-shelf method, perform desired AU changes, and finally uses Stable-Diffusion model to map the AU variations into photorealistic images. As the edited images should preserve the background and pose, we first cut out the background of the portrait and draw the contour of the pose, which is learned by an Attribute Controller so that MagicFace only needs to perform conditional inpainting to the face. To maintain the consistency of identity and high-frequency facial characteristics, we introduce an ID encoder which is symmetrical UNet structure to capture spatial details of the input indentity. This design allows the model to understand the relationship with the identity image within uniform feature space, greatly JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 Fig. 1. MagicFace takes in the AU changes based on the input portrait and edit the portrait to exhibit different expressions. The edited image respects the AU condition and preserve identity, pose, background as well as other facial details. encoding diverse facial movements through combinations of fundamental Action Units (AUs). AUs represent specific facial configurations resulting from the contraction of one or more facial muscles and are not influenced by emotional interpretation. The earlier version of FACS [16] included 44 Action Units (AUs), with 30 of them anatomically linked to specific facial muscles, while the remaining 14 were categorized as miscellaneous actions. In later version [17], the criteria were updated: AU25, AU26, and AU27 were merged based on intensity, as were AU41, AU42, and AU43. The intensities of AUs can be assessed using five-point ordinal scale, typically labeled with uppercase letters to E, representing minimal to maximal intensity alongside the AU number. For example, AU2A indicates the minimum intensity of AU2, while AU2E represents its maximum intensity. However, this five-point scale is not uniform; for instance, levels and encompass broader range of facial changes compared to levels A, B, and E. Fig. 2 shows example images labeled with specific AUs. The precision and clarity of AUs provide unmatched control over facial expressions, offering users an intuitive and easily interpretable set of tools, so we choose to use it as the condition to describe and edit facial expressions. B. Facial Expression Editing Facial expression editing is challenging task as it demands deep understanding of input facial images and prior knowledge about human expressions. Unlike general facial attribute editing, which primarily focuses on modifying the appearance of specific facial regions, facial expression editing is more complex as it often involves significant geometric transformations and simultaneous changes across multiple facial components. Remarkable progress has been made in earlier years with the advancement of GANs. ExprGAN [18] the intensity introduces an expression controller to adjust of generated expressions, but it relies on pre-trained face Fig. 2. display showcasing various action units and their corresponding intensity scales. Only set of commonly used AUs are displayed here. For complete collection of AUs with descriptions see [15]. enhancing the preservation of detailed appearance features. Our model is trained on designed dataset with 30K image pairs. Fig. 1 shows the editing results of various pepole, background and AU combinations. In summary, our contibutions are: generative model that enables precise and localized facial expression editing by using AU variations to represent facial expressions. An ID encoder capable of well preserving the attributes of the person in the edited portraits, unconstrained by any head poses, backgrounds and characters. Quantitative and qualitative results show that our method presents more interpretable, flexible and user-friendly editing manner with higher image generation quality compared to other facial expression editing methods. II. RELATED WORKS A. Facial Action Units The Facial Action Coding System (FACS) [16] is one of the most impactful methods for analyzing facial behavior. is comprehensive, anatomy-based system capable of It JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 Fig. 3. Overview of MagicFace. During training, pair of images with the same identity but different pose, background as well as expressions is used respectively as the identity image and the target. AU variations are computed by an estimator and then sent into the denoising UNet as AU condition. Pose and background of the target is parsed into an image condition independently, dealed with an Attribute Controller and then inputted to the denoising UNet. ID encoder takes in the encoded identity image to edit for target AUs, where features in each transformer blocks are merged into the corresponding ones of the denoising UNet via self-attention. During inference, the conditional image will be parsed from the identity image. recognizer to maintain identity information. StarGAN [19], on the other hand, enables image translation across domains using single model and preserves identity features by minimizing cycle loss. However, it is limited to generating only discrete expressions. GANimation [20] provides more fine-grained method for editing facial expressions by leveraging Action Units (AUs). Similarly, ICface [21] utilizes AUs to enable controllable facial expressions in facial reenactment. Recent focus has shifted to diffusion models due to its ability to produce high-quality images. However, many text-to-image diffusion models are unable to support flexible facial expression editing and, in most cases, can only handle simple emotion labels. Instead, some approaches choose the expression coefficients of 3DMM as the condition to enable fine-grained control over facial expression. However, the large number of coefficients poses significant challenge, as only experts can manually adjust the desired expressions, which limits its applicability for general user-level applications. In contrast, this work focuses on injecting AUs into pre-trained large diffusion models to enable more flexible and user-friendly editing of face. III. METHOD A. Preliminariy Stable Diffusion. Both the denoising UNet and ID encoder of our method inherit the same architecture as well as parameters of the denoising UNet in Stable Diffusion (SD). SD is developed from latent diffusion model (LDM) [22], which introduces learning of feature distributions to reduce computational complexity. It contains fixed autoencoder to map input images into feature maps with smaller size. Formally, for an image given, the encoder maps it into latent representation via = E(x) and the decoder reconstructs it by ˆx = D(z). The denoising UNet in SD learns to denoise noise ϵ which is normally distributed into z. During training, the latent map is added Gaussian noise in timesteps and produce noised latent zt. The UNet is trained to predict the noise added, optimized by the following objective: = Ezt,c,ϵ,t (cid:16) ϵ ϵθ (zt, c, t)2 2 (cid:17) , (1) where ϵ denotes the denoising UNet and represents condition embeddings. In original SD, is produced by text encoder CLIP ViT-L/14 [23] to enable text-to-image generations. During inference, zT is sampled from normal distribution as the noised latent at timestep , and then it is denoised iteratively into z0 by deterministic sampling process like DDPM [24], DDIM [25]. Then z0 will be reconstructed by the decoder into generated image. AU variations VS absolute AUs. Stepping back and reconsidering the conditioning inputs, using absolute AUs should be an intuitive choice. However, this condition configuration has drawback: the model must estimate the actual AUs of the input image to decide whether to edit it. From an application standpoint, we are required to supply value that must exactly match the corresponding AU in the source image even if no changes are intended. AU variations, as opposed to absolute AUs, represent the intended change in specific action units. This aligns with the definition of action units, which indicates the activation state of facial muscles. Therefore, in our model, we choose the difference between target AUs ctgt and source AUs cID as the input condition, denoted as: cAU cID ctgt (2) Using AU variations as conditions offers several advantages. Firstly, it is intuitive and user-friendly. For instance, if we only wish to suppress AU10 (Upper Lip Raiser), we can assign any real negative value to this AU while setting all other values to zero. On the other hand, compared to the full set of target AUs, the values in AU variations are zero-centered, providing JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 more meaningful information for guiding expression editing and stabilizing the training process. With AU variations, the model learns to edit and reconstruct facial regions based on both non-zero and zero values, reducing the complexity of preserving action units. B. Architecture Attribute Controller. As shown in Fig. 3, Attribute Controller takes in the latent representation of an image where the face is masked and the face pose is outlined by landmarks. This design is to solve the ill-posed problem: in videos shot of wild settings, there are no image pairs with identical backgrounds and poses, while expressions undergo significant changes for the same person, which means no ground truth of the image with target AUs is available. However, we can find numerous images of the same person with different backgound, pose and facial expressions. As an alternative, we separate the background and pose as an independent condition of the identity by parsing out the background image, which will avoid the model from learning all the information through the input identity image. On the parsed background image, we draw the contour to highlight the head pose in case that some backgrounds maybe black. It is theoretically possible that the denoising UNet will learn background and pose from the identity. Empirically, we find the network learns to use the Attribute Controller for background and pose information and does not rely on the ID encoder, we hypothsize this is because the conditional image is pixel-aligned with the ground truth and thus more easily used by the model. Attribute Controller is convolution layer with 4 4 kernels, 2 2 strides, and 4 channels to align the latent of conditional image with the same resolution as the noise input. The processed conditional image is then appended to the noisy latent together as the input to the denoising UNet. ID encoder. We first evaluated the vanilla ControlNet [13] for identity preservation. As illustrated in Fig. 6, we observed that ControlNet struggles to preserve appearance consistency when generating human images coherent to AU prompt, making it unsuitable for our editing task. Many approaches employ the CLIP image encoder [26] to encode image conditions, but in our experiments it doesnt adequately tackle problems associated with maintaining consistency in details. CLIP is designed to align semantic, high-level features with text, as result it will lose many details in the generated images. On the other hand, recent studies [27][29] have highlighted the significant role of self-attention layers in diffusion models in maintaining the details of generated images. Building on these findings, we conducted an experiment on self-attention for identity control. In this setup, both the identity image and the noisy image are processed through the self-attention layers. key observation from this architecture is that it inherently facilitates appearance resemblance between the two images. One possible explanation is that the self-attention layers in the UNet play crucial role in spatially transmitting appearance information. As result, they can function as deformation module, enabling the generation of visually similar images with varying geometric structures. Given the above observations, we design feature extraction encoder ωθ for the identity image which is aimed to preserve detailed face attributes, inspired by [28]. This encoder shares the same architecture with the denoising UNet ϕθ. Specifically, let = E(I id) denote the encoded latent feature sent to the ID encoder ωθ where id is the identity image. The features extracted from ωθ will be merged into the denoising UNet ϵθ through self-attention. Along with AU conditions, the ID encoder and the denoising UNet are jointly optimized by: LAU Edit = Ezt,s,cAU ,g,ϵ,t[ϵϵθ(zt, t, ωθ(s), cAU , ϕθ(g))2 2], (3) where = E(I bg) denotes the encoded latent feature sent to the Attribute Controller ϕθ, bg is the background image and cAU represents the encoded AU condition. We omit the AU encoder in the framework as its simply linear layer to map the input AU into feature with the same dimension as the time conditions in SD (details are deferred to Sec. IV-C). The AU condition is added to the time-embedding condition in ϕθ. To merge identity features from ωθ into ϕθ with self-attention, we first dive into the transformer block [30] of the ID encoder and denoising UNet, identifying each pair of feature maps that serve as inputs for the respective self-attention layers. For ease of explanation, we temporarily use an Rhnwncn and bn Rhnwncn to denote the nth pair of feature maps from ID encoder and denoising UNet. In the denoising UNet, in each block the feature maps from both sides are (cid:76) bn Rhn2wncn , then first concatenated by ban = an we replace bn with ban as the input to the self-attention layer. After coming out from self-attention layer, we crop out the fist half of the output feature map into cn of resolution hn wn cn before input to the subsequent cross-attention layer. C. AU dropout To improve the controllability of our method, we incorporate an AU dropout operation during training, enabling classifierfree guidance [31] for AU conditions. This strategy has been widely applied in conditional image generation to balance the trade-off between quality and diversity in images generated by latent diffusion models. In our training process, we randomly drop the AU as all zeros cuncon AU = . To distinguish the dropped AUs from vectors representing no AU changes, we add small amount of Gaussian noise to the vectors representing no AU changes, denoted as zero AU = czero AU + ϵ, ϵ (µ, σ2). We empirically set σ = 0.2. At the inference stage, we use guidance scale α > 1 to modify the intensity of conditional control on the predicted noise: ˆϵθ (zt, cAU ) = ϵθ (zt, ) + α (ϵθ (zt, cAU ) ϵθ (zt, )) . (4) Note that we omit other conditions including t, ωθ(s), ϕθ(g) compared with Eq. 3 to explain more intuitively. We empirically configure the AU dropout ratio to 10% during training, i.e., 10% AUs are set to . Based on our ablation study (see Sec. 4.3), the optimal value of the guidance scale α is typically in the range of 1.5 to 3.5. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 Fig. 4. Qualitative comparison for facial expression editing. Other methods exhibit shortcomings in preserving the identity, pose or background, and they are unable to support continuous control over the intensity of generated expressions, whereas our method excels in maintaining exceptional detail features and meanwhile allows flexible, fine-grained control to the expression intensity. Please zoom in for more detailed observation. D. Training Strategy As introduced in Section III-B, the background and pose are formed into an independent image condition, so in the training stage we use the background of the target image parsed by [32]. We draw the pose contour on the background image by highlighting the 14 landmarks [33] around the chin of the face. Then in the inference time for edit, we use the background and pose parsed from the identity image to enable the generated results consistent with the identity in terms of background. We initialize the denoising UNet and the ID encoder with the pre-trained weights from SD. Both the weights of the denoising UNet and ID encoder are updated independently during training. Note that the ID encoder is only an encoder without time conditions, which needs only one step forward process before the multiple denoising steps in inference. IV. EXPERIMENTS A. Implementations Experimental settings. We collect 30K image pairs from the Aff-Wild dataset [34] by decomposing the videos into frames to train our model. Each pair has the same identity but different expressions, wherein at least one of the pose or background are highly likely different by ensuring that two sampled pairs maintain certain distance in the video. We use LibreFace [8] and [35] to compute 12 AUs variation of the pair images and use Stable Diffusion 1-5 base 1 to initialize the weights. Experiments are conducted on 4 NVIDIA A100 GPUs. Training is conducted for 100,000 steps with batch size of 2 on each GPU. The learning rate is set to 1e-5. Test details. For the test set, we edit 20 identities with variations on individual AUs and on AU combinations. We use 5 intensity variation levels for each single AU out of 12 AUs, 1https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 6 and for AU combinations we use in total 387 combinations. This leads to total number of 20 (12 5 + 387) = 8940 edited images. Evaluation Criteria. We assess the performance of MagicFace by examining the edited image from four perspectives: the accuracy of AU intensity, identity preservation, background preservation and head pose preservation. We measure the AU values of the edited image Yest and compare it with the intended target values Ytar by computing the mean squared error (MSE). LibreFace [8] is used to estimate AU intensities of the generated images. To assess the similarity of identity, we measure the distance of embeddings between the edited image tar and the identity image id, where the embedding is extracted from pre-trained ID recongnition model 2. For background preservation, we employ the root pixel-wise mean squared error (RMSE) between the edited image and the identity image, where the face area of the both is masked to eliminate the interference of face expression difference. For head pose preservation, we compute head pose coefficients of the FLAME model [36] and compute their root mean squared error (RMSE). TABLE COMPARISON OF METHODS IN TERMS OF AU ACCURACY, IDENTITY PRESERVATION AND IMAGE SIMILARITY. N/A DENOTES INCOMPUTABLE. THE BEST AND SECOND BEST RESULTS ARE REPORTED IN BOLD AND [SQUARE BRACKETS], RESPECTIVELY. Method AU ID MSE () L2 () background RMSE () head pose RMSE () DeltaEdit [37] FaceEdit DALLE-3 DiffAE [14] GANmut [38] FaceAdapter [39] MagicFace (Ours) N/A N/A 0.512 N/A N/A [0.427] 0. 0.743 0.513 0.763 0.579 [0.495] 0.567 0.473 0.126 0.044 0.224 0.161 0.059 [0.056] 0.044 0.075 0.032 0.208 0.183 0.032 [0.034] 0.032 B. Results We compare our method against SoTA methods related with facial expression editing, including DeltaEdit [37], FaceEdit3, DALLE-34, DiffAE [14], GANmut [38] and FaceAdapter [39]. DeltaEdit and FaceEdit cannot understand AU descriptions but they understand emotional vocabulary, so when testing on them, we use specific emotional words like happiness and adjectives that describe the intensity of expressions such as mild happiness, intense happiness in the text prompt. DALLE-3 is better at understanding AU text prompts and we design the prompt as add intensity level 2 (level range is [1, 10]) of AU1 , AU4 and AU15 to this mans face which enables it to generate results as coherent to AU intensity as possible. Two emotional labels, i.e., happiness and sadness are employed respectively as the key words in text prompts. DiffAE provides the manipulation levels of smiling, so we use its positive direction as happiness and negative direction 2https://github.com/ageitgey/face recognition 3https://github.com/ototadana/sd-face-editor?tab=readme-ov-file 4https://openai.com/index/dall-e-3/ as sadness. GANmut is facial expression model based on Valence-Arousal conditions. We employ the mapping between emotions and VA reported in their work to produce targeted expressions. FaceAdapter is designed to achieve face swapping and reenactment, we modify the condition of their model to enable only expression variation of the generated images. For fair comparison with these methods, we use the activated AUs related with these two labels [16] to test our model, which is AU6+AU12 for happiness and AU1+AU4+AU15 for sadness. The intensity of AUs grows gradually from 1 to 9 gapped by 2 from left to right in each row of each emotional label. Quantitative and qualitative results are shown in Tab. and Fig. 4, respectively. Comparisons. Our model surpasses the majority of existing methods in overall generation quality. Some approaches like DeltaEdit, DELLA-3 and FaceAdapter are unable to effectively preserve the identity of the input identity image. DELLA-3 performs well in continuously increasing the expression intensity and it can also interpret AU semantic meanings, but it can only generate the image with similar style to the input image, where the identity, background and pose are likely to be changed. Some methods including DiffAE, GANmut, DeltaFace are unable to continuously control the intensity of expression especifically for the sadness. DiffAE requires to crop and align the input image, which fails to preserve head pose. The results generated by FaceEdit are the most similar to ours, but they fail to preserve facial features well enough. Moreover, FaceEdit is unable to support the editing of other customed facial expressions. Tab. provides quantitative demonstration of the comparison, where we can observe that DELLA-3 and our model obtain much lower error in terms of the AU intensity accuracy. FaceEdit and GANmut show better score in terms of identity preservation as well as image similarity, corresponding to the observation from Fig. 4. Overall, our model significantly outperforms current either GAN-based or diffusion-based models with more precise and regulatable editing to facial expressions, both qualitatively and quantitatively. Fig. 5. Results of using laboratory dataset for training. In lab setting, the model cannot generalize to image from natural settings (the second row). C. Ablation Study Training data. We investigate the effectiveness of using in-the-wild data to train our model by comparing with the model trained on DISFA [40]. DISFA provides AU annotation manually annotated by trained people, so literally using this dataset to train could produce more accurate AU condition than JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 7 Fig. 6. Ablation study for different model architectures. Zoom in to view facial details. using AU conditions from AU estimation tools. However, as shown in Fig. 5, training on lab data can only generalize to new data in laboratory settings and cannot generalize to data from natural settings. The reason of this can be arrtibuted to the overly simplistic data patterns: simple background, only frontal face, and limited number of individuals in DISFA. Although AU estimator may bring inaccuray in estimated AU intensity of the training data, we can observe in Fig. 1 and 4 that it stills can generate edited images with high coherence to the AU prompt. TABLE II ABLATION STUDY ON MODEL ARCHITECTURE. THE BEST AND SECOND BEST RESULTS ARE REPORTED IN BOLD AND [SQUARE BRACKETS], RESPECTIVELY. AU ID MSE () L2 () background RMSE () head pose RMSE () ControlNet Cont. All Conv ID. CLIP ID. Ours 0.725 0.673 0.462 [0.406] 0. 0.664 0.610 0.574 [0.543] 0.473 0.164 0.130 [0.070] 0.073 0.044 0.032 [0.033] [0.033] 0.032 0.032 Model design. To demonstrate the effectiveness of our models design, we explore four alternatives, 1) Replacing the whole TABLE III ABLATION STUDY ON AU ENCODERS. THE BEST AND SECOND BEST RESULTS ARE REPORTED IN BOLD AND [SQUARE BRACKETS], RESPECTIVELY. AU ID MSE () L2 () background RMSE () head pose RMSE () MLP+Conv ZeroAppend+Time Linear+Time (Ours) 0.204 0.282 [0.261] 0.501 [0.495] 0.473 [0.044] 0.054 0.044 0.032 0.032 0.032 architecture with ControlNet [13] (Control.) and add our Attribute Controller to enable background embedding; 2) Copy the architecture of 1) but enable denoising trainable (Cont. All); 3) Replacing the ID Encoder by Conv layer (Conv ID.); 4) Replacing the ID Encoder by CLIP image encoder [26] (CLIP ID.). Results are shown in Fig. 6 and Tab. II. Our design achieves the optimal performance. Design 1) and 2) shows that ControlNet can not adapt to AU conditions, and it significantly alters the background and the appearance of the face. Design 2) and 4) show that using CLIP or Conv to extract features can preserve image similarity but fails to preserve facial details. This further prevents them from precisely editing facial expressions based on AUs as some face details are lost. The edited results of these two experiments also differ from JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 8 Fig. 7. Demonstration of out-of-domain testing for extreme expressions as well as unseen styles. Some AUs are out of the range [0, 5]. The left column displays the editing results of real person photos with extreme expressions and the right column displays that of the cartoon characters. the identity image in some color details, like the hoodie and face color of the leftmost identity. We can also observe that in all of these ablations head pose is well-preserved, which indicates the effectiveness of separating background and pose as pixel-aligned condition to make the model focus on the face area. Out-of-domain editing. It is worth noting that MagicFace demonstrates remarkable generalization to out-of-domain identity images with unseen AUs and characters, achieving impressive appearance controllability even without additional finetuning on the target domain. In the training stage the utmost AU variation given neutral expression is 5 (AU annotation only in range [0, 5]), but our model can edit the range of AUs almost extending from -10 to 10. This indicates that MagicFace has strong capability to generate and explore contradictory and extreme expressions, as we can set AUs associated with contradictory emotional states to have opposite intensity variations and large values. On the other hand, on zero-shots results of applying our model to cartoon characters whose visual style is distinct from the training data of the real-human, our model can still ensure AU coherence with high-quality generation. We visualize the qualitative results in Fig. 7. AU Encoder. We investigate the influence of AU encoder by conducting two additional experiments: 1) Apply MLP to encode AU vectors and concatenate it to the first Conv layer of the denoising UNet (MLP+Conv). 2) Append zero values to the end of the AU prompt to make the input vector of the same size as the time embedding and add it to the time embedding (ZeroAppend+Time). Quantitative results are shown in Tab. III. As can be observed, the encoding method of AUs does not significantly affect the performance of the approach, so we choose an easier way by just applying linear layer to the AU prompt and then add it to the time embedding (Linear+Time). AU dropout. We investigate the effects of AU dropout as well as the different values of the guidance scale α. We first train our model without/with AU dropout. trained with AU dropout, we set Then, the model for α = 0.5, 1.5, 2.5, 3.0, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5 for classifier-free guidance. During inference, we ensure that all other parameters including the random seed remain consistent to allow for fair comparison. As illustrated in Fig. 8, without AU dropout, classifier-free guidance is not supported, resulting in the worst generation quality where the target AU4[-6] is not obvious. For the model trained with dropout, when α > 1.0, the opposite variation of AU4 related area (brow lower towards brow higher) become progressively more distinct as α increases. However, color distortion will appear when α 4.5 and becomes increasingly noticeable starting from α = 5.0, as seen in the hoodie color, more evident beard and more red face. Tab. IV provides quantitative evidence supporting the effectiveness of our AU dropout. It also indicates that the optimal guidance scale is typically in the range of 1.5 to 3.5 in most cases. According to this study, we empirically set α = 3.0 for all test experiments. Visualization. Fig. 9 visualizes the attention maps learned in the self-attention as well as cross-attention of the denoising UNet. For the self-attention, we observe that initially the attention weights are almost uniformly distributed across the whole image, and then it focus more on the background area due to the input condition from Attribute Controller. As the denoising process nears its end, the attention gradually expands to include the facial regions. For the cross-attention, the attention shifts from an initially near-uniform distribution to gradually focusing on the facial regions, especially on the areas related with inputted AU varations. This indicates the self-attention layers mainly focus on the features of the image itself, while the cross attention focus more on interpreting AUs with the edited image. V. IMPACT STATEMENT The proposed MagicFace for facial expression editing offers diverse applications. It greatly enhances communication in digital environments by allowing individuals to convey themselves more effectively through avatars or digital characters. This improvement facilitates better interactions in virtual meetJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 Fig. 8. Qualitative comparison of images edited by MagicFace trained without/with AU dropout and using different values of the guidance scale α. The AU variation to edit is AU4[-6]. Please zoom in for more details. Fig. 9. Visualization of self-attention maps (lst row) and cross-attention maps (2nd row) from the denoising UNet. Please zoom in to observe more details. TABLE IV ABLATION STUDY OF AU DROPOUT AND DIFFERENT GUIDANCE SCALE VALUES. THE BEST AND SECOND BEST RESULTS ARE REPORTED IN BOLD AND [SQUARE BRACKETS], RESPECTIVELY. BG IS THE ABBREVIATION FOR BACKGROUND AND HP IS THE ABBREVIATION FOR HEAD POSE. THE BEST RESULTS ARE REPORTED IN BOLD. AU Dropout Guidance Scale - 0.5 1.5 2.5 3.0 3.5 4.5 5.5 AU MSE () 0.360 0.357 0.333 0.281 0.261 0.264 0.280 0.286 ID L2 () 0.480 0.482 0.473 0.473 0.473 0.484 0.495 0.510 BG RMSE () 0.044 0.043 0.043 0.042 0.044 0.044 0.046 0. HP RMSE () 0.032 0.032 0.031 0.032 0.032 0.032 0.034 0.034 AU Dropout Guidance Scale 6.5 7.5 8.5 9.5 10.5 11.5 12.5 13.5 AU MSE () 0.293 0.298 0.305 0.309 0.315 0.323 0.331 0.350 ID L2 () 0.527 0.516 0.520 0.504 0.519 0.523 0.533 0.536 BG RMSE () 0.050 0.046 0.046 0.052 0.055 0.052 0.057 0. HP RMSE () 0.032 0.034 0.032 0.031 0.032 0.034 0.032 0.033 ings, online education platforms, and social networking spaces. In addition, MagicFace has the potential to transform the entertainment and media industries by enabling the creation of more realistic and expressive characters in films, video games, and animations. This innovation enhances immersive storytelling and boosts audience engagement. Experiments show that our model can generalize across various real human ethnicities and age groups, and even to out-of-domain images, e.g., cartoon-style images and painting-style images. Potential Negative Social Impact: The method could be exploited for malicious purposes, such as creating fake animated images or videos of individuals, which might be used for fraudulent activities. To prevent such misuse, it is crucial to implement various measures, including digital watermarking and detection algorithms, enforcing strict legal regulations, promoting media literacy through public awareness and education, and establishing ethical guidelines within the tech industry. Achieving this requires collaborative efforts among technology companies, governments, educators, and the public to foster safer digital environment and reduce the risks associated with fraudulent AI-generated content. VI. LIMITATIONS AND CONCLUSION Limitations. Our model may struggle to generate completely the same image when the AU variations are inputted as all zeros, partially because of the inaccuracy of the estimated AU intensity used in training stage. Besides, our model demonJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 strates lower operational efficiency compared to non-diffusionbased methods due to the use of DDPM. Conclusion. In this paper, we present MagicFace, framework capable of editing any portraits with different facial expressions and intensity levels by embedding action units as condition into Stable-Diffusion. We introduce ID encoder that effectively retains detailed face attributes and achieve efficient facial expression controllability. Our approach provides an user-friendly way to enable fine-grained control over facial expressions with high quality, which outperforms existing face expression editing methods."
        },
        {
            "title": "REFERENCES",
            "content": "[1] R. Wu, Y. Yu, F. Zhan, J. Zhang, S. Liao, and S. Lu, Poce: Posecontrollable expression editing, IEEE Transactions on Image Processing, vol. 32, pp. 62106222, 2023. [2] Z. He, W. Zuo, M. Kan, S. Shan, and X. Chen, Attgan: Facial attribute editing by only changing what you want, IEEE transactions on image processing, vol. 28, no. 11, pp. 54645478, 2019. [3] M. Pernuˇs, V. ˇStruc, and S. Dobriˇsek, Maskfacegan: High resolution face editing with masked gan latent code optimization, IEEE Transactions on Image Processing, 2023. [4] S. Jiang, Z. Tao, and Y. Fu, Geometrically editable face image translation with adversarial networks, IEEE Transactions on Image Processing, vol. 30, pp. 27712783, 2021. [5] Y. Li and S. Shan, Contrastive learning of person-independent representations for facial action unit detection, IEEE Transactions on Image Processing, vol. 32, pp. 32123225, 2023. [6] K. Zhao, W.-S. Chu, F. De la Torre, J. F. Cohn, and H. Zhang, Joint patch and multi-label learning for facial action unit and holistic expression recognition, IEEE Transactions on Image Processing, vol. 25, no. 8, pp. 39313946, 2016. [7] H. Chen, P. Zhang, C. Guo, K. Lu, and D. Jiang, Facial action unit representation based on self-supervised learning with ensembled priori constraints, IEEE Transactions on Image Processing, 2024. [8] D. Chang, Y. Yin, Z. Li, M. Tran, and M. Soleymani, Libreface: An open-source toolkit for deep facial expression analysis, in WACV, 2024, pp. 82058215. [9] C.-Y. Chan, W.-C. Siu, Y.-H. Chan, and H. A. Chan, Anlightendiff: Anchoring diffusion probabilistic model on low light image enhancement, IEEE Transactions on Image Processing, 2024. [10] X. Tao, J. Kong, M. Jiang, M. Lu, and A. Mian, Unsupervised learning of intrinsic semantics with diffusion model for person re-identification, IEEE Transactions on Image Processing, 2024. [11] J. Yue, L. Fang, S. Xia, Y. Deng, and J. Ma, Dif-fusion: Towards high color fidelity in infrared and visible image fusion with diffusion models, IEEE Transactions on Image Processing, 2023. [12] X. Dai, Y. Li, M. Duan, and B. Xiao, Diffusion models as strong adversaries, IEEE Transactions on Image Processing, 2024. [13] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 38363847. [14] K. Preechakul, N. Chatthee, S. Wizadwongsa, and S. Suwajanakorn, Diffusion autoencoders: Toward meaningful and decodable representation, in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [15] M. Ozel, Facs cheat sheet, 2024. [16] P. Ekman and W. V. Friesen, The facial action coding system: technique for the measurement of facial movement, Consulting Psychologists Press Inc. google schola, vol. 2, pp. 97115, 1978. [17] P. Ekman, W. Friesen, and J. Hager, Facial Action Coding System: Facial action coding system : the manual : on CD-ROM, ser. Facial Action Coding System. Research Nexus, 2002. [Online]. Available: https://books.google.com.hk/books?id=wphFzwEACAAJ [18] H. Ding, K. Sricharan, and R. Chellappa, Exprgan: Facial expression editing with controllable expression intensity, in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018. [19] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, Stargan: Unified generative adversarial networks for multi-domain image-toimage translation, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 87898797. [20] A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and F. MorenoNoguer, Ganimation: Anatomically-aware facial animation from single image, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 818833. [21] S. Tripathy, J. Kannala, and E. Rahtu, Icface: Interpretable and controllable face reenactment using gans, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2020, pp. 3385 3394. [22] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, Highresolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 68410 695. [23] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [24] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 6840 6851, 2020. [25] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [26] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable language supervision, in International visual models from natural conference on machine learning. PMLR, 2021, pp. 87488763. [27] Y. Xu, T. Gu, W. Chen, and C. Chen, Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on, arXiv preprint arXiv:2403.01779, 2024. [28] L. Hu, Animate anyone: Consistent and controllable image-to-video synthesis for character animation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 81538163. [29] M. Cao, X. Wang, Z. Qi, Y. Shan, X. Qie, and Y. Zheng, Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 22 56022 570. [30] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [31] J. Ho and T. Salimans, Classifier-free diffusion guidance, in NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. [32] C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation, International journal of computer vision, vol. 129, pp. 30513068, 2021. [33] A. Bulat and G. Tzimiropoulos, How far are we from solving the 2d & 3d face alignment problem? (and dataset of 230,000 3d facial landmarks), in International Conference on Computer Vision, 2017. [34] D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou, G. Zhao, B. Schuller, I. Kotsia, and S. Zafeiriou, Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond, International Journal of Computer Vision, vol. 127, no. 6, pp. 907929, 2019. [35] T. Varanka, H.-Q. Khor, Y. Li, M. Wei, H. Kung, N. Sebe, and G. Zhao, Towards localized fine-grained control for facial expression generation, arXiv preprint arXiv:2407.20175, 2024. [36] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero, Learning model of facial shape and expression from 4d scans. ACM Trans. Graph., vol. 36, no. 6, pp. 1941, 2017. [37] Y. Lyu, T. Lin, F. Li, D. He, J. Dong, and T. Tan, Deltaedit: Exploring text-free training for text-driven image manipulation, in CVPR. IEEE Computer Society, 2023, pp. 68946903. [38] S. dApolito, D. P. Paudel, Z. Huang, A. Romero, and L. Van Gool, Ganmut: Learning interpretable conditional space for gamut of emotions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 568577. [39] Y. Han, J. Zhu, K. He, X. Chen, Y. Ge, W. Li, X. Li, J. Zhang, C. Wang, and Y. Liu, Face-adapter for pre-trained diffusion models with finegrained id and attribute control, in European Conference on Computer Vision. Springer, 2025, pp. 2036. [40] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and J. F. Cohn, Disfa: spontaneous facial action intensity database, TAC, vol. 4, no. 2, pp. 151160, 2013."
        }
    ],
    "affiliations": [
        "Center for Machine Vision and Signal Analysis, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland",
        "Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering, Southeast University, Nanjing 210096, China"
    ]
}