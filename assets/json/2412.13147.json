{
    "paper_title": "Are Your LLMs Capable of Stable Reasoning?",
    "authors": [
        "Junnan Liu",
        "Hongwei Liu",
        "Linchen Xiao",
        "Ziyi Wang",
        "Kuikun Liu",
        "Songyang Gao",
        "Wenwei Zhang",
        "Songyang Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs' \"realistic\" reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: https://github.com/open-compass/GPassK."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 7 4 1 3 1 . 2 1 4 2 : r Technical Report Are Your LLMs Capable of Stable Reasoning? Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu Songyang Gao, Wenwei Zhang, Songyang Zhang*, Kai Chen* Shanghai AI Laboratory opencompass@pjlab.org.cn corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, novel evaluation metric that provides continuous assessment of model performance across multiple sampling attempts, quantifying both the models peak performance potential and its stability. Second, we present LiveMathBench, dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs realistic reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: https://github.com/open-compass/GPassK."
        },
        {
            "title": "Introduction",
            "content": "Since the emergence of large language models (LLMs), the complex reasoning, particularly in mathematical problem-solving, has been regarded as the crown jewel of LLM capabilities. Numerous models have demonstrated remarkable performance on mathematical tasks, from general-purpose models like GPT-4o (OpenAI, 2024a), and the LLaMA series (AI, 2024) to specialized models such as DeepSeek-Math (Shao et al., 2024) and Qwen-Math (Yang et al., 2024b), which excel in complex and high-difficulty mathematical reasoning. More recently, long-chain-of-thought (Long-CoT) models like OpenAI-o1 (OpenAI, 2024b), QwQ (Team, 2024a), and DeepSeek-R1-Lite-Preview (DeepSeek-R1-Lite-Preview, 2024) have further advanced the state-of-the-art in mathematical problem-solving. In real-world applications, LLMs typically employ sampling with predefined decoding parameters (including temperature, top-k, top-p, and repetition penalty) to maintain response diversity. Users often regenerate responses or initiate new sessions until obtaining satisfactory answers to specific questions. However, conventional evaluation metrics for LLMssuch as Greedy Accuracy, Pass@K (Chen et al., 2021), Best-of-N (BoN), and Majority Votingdemonstrate significant limitations in measuring real-world performance, particularly regarding long-term consistency. While these metrics effectively capture either instantaneous accuracy or peak performance across multiple samples, they inadequately address output stability. Such instability poses significant challenges for applications requiring reliable and predictable outcomes, highlighting the need for evaluation metrics that effectively balance response diversity with consistent performance. To address these challenges, we introduce G-Pass@k, novel evaluation metric that simultaneously assesses both problem-solving capability and performance consistency. The core 1 Technical Report Figure 1: Pass@16 v.s. Greedy Accuracy v.s. G-Pass@161.0 on LiveMathBench. This figure illustrates the gap between the performance of models using the Pass@16 criterion (dark bars), typical greedy output (semi-light bars), and the performance under the G-Pass@161.0 criterion (light bars), highlights the instability of model performance across multiple samplings. principle of G-Pass@k lies in evaluating model performance under varying thresholds of correctness, providing nuanced understanding of model behavior across different stringency levels. By integrating measures of both stability and potential, G-Pass@k delivers comprehensive assessment of an LLMs capabilities, particularly valuable for complex reasoning tasks. To thoroughly demonstrate the practical implications of G-Pass@k, we additionally present LiveMathBench, challenging bi-lingual mathematics benchmark. This dynamic evaluation framework continuously incorporates contemporary mathematical problems, ensuring relevance to the latest developments in both model capabilities and mathematical discourse. We conducted comprehensive experiments using G-Pass@k to evaluate various models on LiveMathBench, including general LLMs, mathematics-specialized models, and Chain-ofThought (CoT) enhanced variants known for their step-by-step reasoning capabilities in complex mathematical problem-solving. Through systematic comparison between traditional metrics (Greedy Accuracy, Pass@K) and G-Pass@k, our analysis reveals distinctive insights into model performance that conventional evaluation approaches may overlook. Our analysis, illustrated in Figure 1, demonstrates significant limitations in traditional evaluation metrics regarding output stability assessment. We observe substantial performance degradation across all models as the G-Pass@k threshold becomes more stringent, pattern consistent across both established open-source datasets(MATH and AIME) and our novel LiveMathBench. Through detailed decomposition of the G-Pass@k metric and comparative analysis of model architectures, we reveal distinct behavioral patterns between o1-like LLMs and standard LLMs under varying difficulty levels and stability requirements. These findings underscore the inadequacy of conventional evaluation methods and demonstrate the necessity of stability-aware metrics for realistic assessment of model capabilities in complex reasoning tasks. The key observation derived from the new metric togehter with LiveMathBench includes: Instability in Reasoning: Both proprietary and open-source models exhibit significant instability when applied to challenging reasoning tasks. Performance drops exceed 50% in many cases, with the worst instances showing declines of up to 90%. 2 Technical Report Limited Benefits from Increased Model Size: Merely scaling up model size does not necessarily enhance stable reasoning capabilities. The expected improvements in performance and stability are not consistently observed. Discrepancy Between Potential and Stability: There is noticeable gap between the models potential capabilities, as measured by their G-Pass@kτ0, and their actual stability, reflected in G-Pass@kτ. This disparity highlights the need for further research into developing methods that can better harness the theoretical capabilities of these models in practical, stable applications."
        },
        {
            "title": "2 Related Work",
            "content": "Mathematical Reasoning Benchmarks for LLMs. The assessment of large language models (LLMs) in mathematical reasoning has led to the development of specialized benchmarks focusing on different aspects of an LLMs mathematical proficiency. GSM8K (Cobbe et al., 2021) presents dataset of 8,500 elementary-level math word problems, segregated into training and testing sets, that demand multi-step reasoning and detailed solution paths. MATH (Hendrycks et al., 2021b) encompasses 12,500 problems derived from high school math competitions, challenging LLMs with advanced topics like calculus and algebra, and providing step-by-step solutions to facilitate coherent training. MMLU (Hendrycks et al., 2021a) evaluates pre-trained language models across 57 subjects, including STEM fields, featuring mathematics section designed to gauge knowledge and problem-solving skills in mathematics. FineMath (Liu et al., 2024b) assesses fine-grained mathematical reasoning through dataset comprising core elementary concepts, categorized into 17 types of math vocabulary questions, manually annotated, and classified by difficulty. GAOKAOBench (Zhang et al., 2023), incorporating 2,811 Chinese questions, examines LLMs in zero-sample setting, resembling Chinas college entrance examination format, encompassing range of question types from objective to subjective. MathBench (Liu et al., 2024a) is hierarchical benchmark that assesses both theoretical and applied mathematical abilities, consisting of 3,709 questions spanning basic arithmetic to university level, structured across five educational tiers. Omni-Math (Gao et al., 2024) focuses on Olympic-level mathematical reasoning, featuring 4,428 competition-level problems categorized into over 33 subfields and 10 difficulty levels, spanning from entry-level to professional international competitions. Stability of LLM Reasoning. Large language models (LLMs) exhibit remarkable performance in reasoning tasks, encompassing question answering, programming, and mathematical problem-solving. Despite their prowess, the output stability of LLMs poses significant challenge, whereby the models outputs can vary for the same input due to random sampling or hallucinations, impacting the models reliability and predictability (Gupta et al., 2024; Xu et al., 2024; Atil et al., 2024). Atil et al. (2024) introduced two new metrics: TARr@N for the total agreement rate at runs over raw output and TARa@N for total agreement over parsed-out answers. However, TARr@N and TARa@N focus solely on measuring output consistency, our work introduces novel evaluation metric G-Pass@k for evaluating the mathematical reasoning proficiency of LLMs. This metric aims to assess the models true reasoning ability by not only considering output consistency but also emphasizing correctness."
        },
        {
            "title": "3 Generalized Metric for LLM Reasoning",
            "content": "In this section, we begin with review of the classical evaluation metric Pass@k and then introduce our proposed metric G-Pass@k. Finally, we outline the key differences between these two metrics. 3.1 Preliminary: Pass@k Pass@k is initially proposed to evaluate the functional correctness of code generated by models (Kulal et al., 2019; Chen et al., 2021). With the expanding application of large language models (LLMs) in various reasoning tasks (Rajani et al., 2019; Imani et al., 2023; 3 Technical Report Giadikiaroglou et al., 2024), the Pass@k metric has gained increasing prominence (Luo et al., 2023; Yu et al., 2024). It effectively measures models potential performance in solving complex questions. Pass@k denotes the probability of obtaining at least one correct solution within attempts for each question, as described by the formula: Pass@k = Questions (cid:34) 1 (cid:35) , (nc ) (n k) (1) where denotes the total count of runs, which means the number of generations in the reasoning task, and signifies the number of correct solutions among them. Intuitively, Equation (1) assesses the expected ratio of questions for which at least one correct solution is discovered. In practical, is usually configured as the same value of k, mainly due to the heavy inference cost. 3.2 Generalized Metric: G-Pass@k Pass@k can indicate models performance potential, however, it does not account for the stability of the models reasoning performance. To assess both the potential and stability of models, we propose generalized metric called G-Pass@k. In simple terms, G-Pass@k measures the consistency of model in generating correct solutions across multiple generations by assessing the probability of obtaining correct solutions in all generations. Let denote the probability that model proDefinition of G-Pass@k & G-Pass@kτ. vides the correct solution for question. Given that each generation is independent and identically distributed (i.i.d.), the probability of obtaining correct solutions follows binomial distribution: B(n, p). (2) Since is usually inaccessible, we leverage the hypergeometric distribution to approximate the binomial distribution: H(m; k, c, n) B(m; n, p). lim Therefore, G-Pass@k can be defined as: G-Pass@k = Questions (cid:21) . (cid:20) (c k) (n k) (3) (4) where represents the total number of generations per question, and denotes the number of generations resulting in correct solutions. Considering the stringent nature of Equation (4), we draw inspiration from the mean Average Precision (mAP) metric (Everingham et al., 2010) used in object detection to introduce threshold τ (0.0, 1.0], leading to the definition of G-Pass@kτ: G-Pass@kτ = Questions j=τk j) (nc (c kj) (n k) , (5) where τ denotes the smallest integer greater than or equal to τ k. Conceptually, for τ < 1.0, there is flexibility to allow up to τ incorrect solutions within the generations. Recall that we utilize hypergeometric distributions to approximate binomial distributions, which provides good estimation when is sufficiently large. We provide more discussions about the estimation in Appendix A.2. Pass@k is the special instance of G-Pass@kτ. special case of G-Pass@kτ, as demonstrated by the following theorem. It can be observed that Pass@k is essentially Technical Report Figure 2: Comparison of Pass@k and G-Pass@k. In our simulation configuration, we set = 10, = {8, 16, 24, 32}, and then calculate Pass@k and G-Pass@k. Theorem 3.1. Pass@k is is the special case of G-Pass@k as τ approaches 0, formally described as: j) (nc (c kj) (n k) (nc ) (n k) j=τk = 1 lim τ0 . (6) The proof is provided in Appendix A.3. Definition of mG-Pass@k. When the threshold τ is low, G-Pass@kτ tends to measure the models performance potential. Conversely, at higher τ values, G-Pass@kτ evaluates the models stability or its level of mastery over the question. Thus, G-Pass@kτ facilitates the continuous observation of both performance potential and stability. We further define mG-Pass@k as: mG-Pass@kτ = (cid:90) 1.0 0.5 G-Pass@kτdτ = 2 i=0.5k+1 G-Pass@k . (7) Intuitively, mG-Pass@k provides an interpolated estimate of the area under the curve of mG-Pass@k[0.5:1.0], serving as comprehensive metric that integrates all G-Pass@kτ values where τ [0.5, 1.0]. For optimal and stable models, the mG-Pass@k value should approach 1. 3.3 Pass@k v.s. G-Pass@k To facilitate more intuitive comparison between Pass@k and G-Pass@k, Figure 2 illustrates the metric values for various values with = 80. The figure demonstrates that while Technical Report Pass@k provides some insights into the models capabilities, relying solely on Pass@k may result in an overestimation of the models actual performance. For example, as shown in the upper left of Figure 2, even if the model correctly solves the question only 8 times out of 80 runs, Pass@k stukk produces notably high score (Pass@k > 0.8 for 16). Additionally, once surpasses certain threshold, discerning differences in Pass@k becomes challenging. In contrast, G-Pass@k offers more precise evaluation of the models performance, as depicted in Figure 2. Across varying values, G-Pass@k exhibits clear distinctions. Moreover, by adjusting the thresholds, G-Pass@k can emphasize different aspects: lower threshold places more emphasis on the models potential, while higher threshold underscores the models stability and mastery of the problem. In conclusion, G-Pass@k not only provides more detailed performance assessment compared to Pass@k but also, through tailored threshold configurations, effectively balances considerations of the models potential and stability."
        },
        {
            "title": "4 Performance and Analysis",
            "content": "4.1 LiveMathBench To effectively analyze the G-Pass@k performance of large language models, we construct new and challenging benchmark named LiveMathBench. LiveMathBench will undergo ongoing updates with new questions to continuously evaluate the mathematical reasoning performance of models. 4.1.1 Benchmark Construction LiveMathBench is specifically designed to include four challenging out-of-domain question sets from various mathematical competitions, aiming to avoid data contamination issues in existing LLMs and public math benchmarks (Zhou et al., 2023; Li et al., 2024; Ni et al., 2024). LiveMathBench (version of 202412) incorporates the latest problems from the China National Mathematical Olympiad (CNMO), Chinas College Entrance Examination (CCEE), American Mathematics Competition (AMC), and William Lowell Putnam Mathematical Competition (WLPMC). These datasets encompass diverse levels of difficulty and linguistic variations and have low overlap with publicly available datasets, ensuring comprehensive evaluation of the generalization capabilities of LLMs across various mathematical scenarios. More details about LiveMathBench can be found in Appendix A.1. 4.1.2 Benchmark Statistics Table 1 presents comprehensive statistics for the LiveMathBench. In order to enhance benchmark diversity and assess the performance of LLMs in multilingual settings, both English and Chinese versions of the questions are included. Table 1: Statistics of LiveMathBench Dataset Language #Fill-In-the-Blank #Problem-Solving #Questions CNMO CCEE AMC WLPMC ALL en & cn en & cn en & cn en & cn en & cn - 132 - - 13 6 182 312 462 112 1062 182 442 462 112 1192 Technical Report 4.2 Setup 4.2.1 LLMs We evaluate various LLMs recognized for their strong mathematical reasoning capabilities, including InternLM2-Math-Plus-20B (Ying et al., 2024), DeepSeek-Math-7b-RL (Shao et al., 2024), DeepSeek-V2.5-1210 (DeepSeek-AI, 2024), Llama-3.1-8B-Instruct (Dubey et al., 2024), Llama-3.1-70B-Instruct (Dubey et al., 2024), Llama-3.3-70B-Instruct (AI, 2024), NuminaMath72B-CoT (Beeching et al., 2024), Mistral-Large-Instruct-2411 (Team, 2024b), Qwen2.5-7BInstruct (Qwen Team, 2024), Qwen2.5-Math-7B-Instruct (Yang et al., 2024b), Qwen2.5-32BInstruct (Qwen Team, 2024), Qwen2.5-72B-Instruct (Qwen Team, 2024), Qwen2.5-Math72B-Instruct (Yang et al., 2024b), Claude-3.5-Sonnet (Anthropic Inc., 2024), Gemini-1.5Pro (Research, 2024), and GPT-4o-2024-11-20 (OpenAI, 2024a). Additionally, we include several o1-like LLMs, such as QwQ-32B-Preview (Team, 2024a), Skywork-o1-Open-Llama3.1-8B (o1 Team, 2024), and OpenAI o1-mini (OpenAI, 2024b). 4.2.2 Implementation Details In all experiments, we set the number of generations, n, to 16 3 = 48 and report the greedy accuracy, Pass@k (G-Pass@k0), and G-Pass@k values, where {4, 8, 16} and τ {0.25, 0.5, 0.75, 1.0}. For the sampling parameters of open-source models, we configure the temperature to 0.7, top-p to 1.01, top-k to 50 and repetition-penalty to 1.0. For opensource models, the maximum number of tokens is set to 8, 192 for non-o1 LLMs and 32, 768 for o1-like LLMs. And for close-source models, due to constraints of inference costs, we configured the maximum completion tokens to 4, 096 for non-o1 LLMs and 16, 384 for OpenAI o1-mini. We use the OpenCompass (Contributors, 2023) platform to evaluate all LLMs. Due to the diverse formats of the final answers produced by models in complex mathematical questions, we leverage Qwen-2.5-72B-Instruct (Yang et al., 2024a) to judge whether the content generated by the tested model aligns with the standard answer. In our judge pipeline, we provide the original question, reference answer, and model-generated answer, prompting Qwen-2.5-72B-Instruct to determine whether the candidate solution is consistent with the reference answer. The details of the judging process can be found in Appendix A.4. 4.2.3 Additional Public Benchmarks We also include widely used public benchmarks MATH500 (Hendrycks et al., 2021b; Lightman et al., 2024) and AIME2024 (AIME2024; Yang et al., 2024b), both of which are designed to rigorously evaluate LLMs on their mathematical reasoning capabilities. MATH500-L5. MATH500 dataset collection MATH (Hendrycks et al., 2021b), intended to challenge LLMs with complex mathIt encompasses variety of advanced questions from multiple ematical problems. domains including algebra, geometry, probability, and number theory, thereby providing comprehensive assessment of models proficiency in mathematical reasoning. We select all questions with difficulty 5, resulting in MATH500-L5, which contains 134 questions. is curated subset of larger AIME2024-45. Tailored for evaluating LLM performance at the American Invitational Mathematics Examination (AIME) level, the AIME question set presents series of intricate tasks that test logical thinking, abstract reasoning, and accurate calculation skills. This resource aims to push the boundaries of what LLMs can achieve in solving sophisticated mathematical problems. We combine the part 1 and the part 2 of the American Invitational Mathematics Examination 2024, resulting in 45 questions, called AIME2024-45. 1For Qwen2.5 models, top is configured to 0.8 to reduce repetition and gibberish. 7 Technical Report Table 2: Performance of models on LiveMathBench. We perform 48 runs and report results of greedy accuracy, and G-Pass@16{0.5,0.75,1.0} and mG-Pass@16. more detailed performance can be found in Table 6 at Appendix A.5.1. LLMs Greedy G-Pass@16 (Equation (5)) / % G-Pass@160.5 G-Pass@160.75 G-Pass@161.0 mG-Pass@16 General LLMs Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct DeepSeek-V2.5-1210 Mistral-Large-Instruct-2411 Gemini-1.5-Pro-Latest Claude-3.5-Sonnet GPT-4o-2024-11-20 InternLM2-Math-Plus-20B DeepSeek-Math-7B-RL NuminaMath-72B-CoT Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Skywork-o1 QwQ-32B-Preview OpenAI o1-mini 18.1 27.7 34.9 32.8 42.9 43.7 43.3 37.4 49.2 37.0 40.0 22.3 20.6 34.5 39.9 50.4 7.5 18.2 30.5 31.7 40.6 41.7 39.7 29.1 48.0 35.2 36.1 3.5 13.0 23.7 23.4 34.4 34.5 28.1 26.7 40.2 27.2 28. Mathematical Reasoning LLMs 10.1 17.9 22.6 39.2 45.3 3.0 12.7 12.8 32.2 37.8 O1-like Reasoning LLMs 39.5 64.3 66.5 31.2 66.6 68. 24.1 56.2 58.8 0.8 7.1 16.0 13.8 23.9 25.2 16.6 23.5 26.8 17.4 18.4 0.3 5.8 3.7 24.2 26.8 13.1 33.3 42.0 3.3 12.3 22.7 22.2 32.6 33.2 27.0 26.4 37.8 25.9 26.8 3.3 11.7 11.8 31.2 36. 22.6 52.2 56.5 API-based close-source LLMs. OpenAI o1 series model does not provide an optional temperature parameter, so we chose the average accuracy of 20 generations as the proxy for greedy accuracy. 4.3 LiveMathBench & Public Benchmark Performance Table 2 demonstrates the performance of all models on LiveMathBench and Table 3 demonstrates the performance on MATH500-L5 and AIME2024-45. From the results, we derive the following observations. 1) Competition-Level Questions Still Remain Challenging for Current LLMs. Our analysis reveals that competition-level questions such as those in LiveMathBench and AIME2024-45 continue to pose substantial challenges for all evaluated models, even those at the cutting edge of current research. For instance, despite being the top-performing general-purpose model, Gemini-1.5-Pro-Latest achieves only greedy decoding accuracy of 49.2% on LiveMathBench. Similarly, the best-performing mathematical reasoning model, Qwen2.5-Math-72B-Instruct, attains an accuracy of 50.4%, which, while slightly higher, still falls short of perfection. Most of the LLMs we evaluated scored between 10% and 45% on greedy decoding. Notably, several high-performing models are close-source, such as GPT-4o with 39.9% and Claude-3.5-Sonnet with 37.0%. Moreover, on another challenging benchmark, AIME2024-45, the best-performing non-o1-like LLMs only achieve about 20% greedy accuracy and the performance of most LLMs is at low level. These results underscore the ongoing difficulty in achieving high accuracy on the latest complex mathematical problems. While O1-like models, equipped with long-chain-of-thought (long-CoT) and reflective mechanisms, perform significantly better on intricate tasks, they too face considerable challenges in LiveMathBench. For example, the optimal-performing OpenAI o1-mini achieves score of 66.51%, and the most powerful open-source O1-like model, QwQ-32B-Preview, scores 64.3% on LiveMathBench. Despite these improvements, the gap between current model performance and human-level proficiency remains notable. Technical Report Table 3: Performance of models on MATH500 and AIME2024. Aligning with experiments on LiveMathBench, we also perform 48 runs and report results of greedy accuracy, GPass@16{0.5,0.75,1.0}, and mG-Pass@16. More detailed results are available in Table 7 at Appendix A.5.2. LLMs Greedy G-Pass@16 (Equation (5)) / % G-Pass@160.5 G-Pass@160.75 G-Pass@161.0 mG-Pass@16 Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Mistral-Large-Instruct-2411 Llama-3.3-70B-Instruct DeepSeek-Math-7b-RL Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Skywork-o1 QwQ-32B-Preview Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Mistral-Large-Instruct-2411 Llama-3.3-70B-Instruct DeepSeek-Math-7b-RL Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Skywork-o1 QwQ-32B-Preview OpenAI o1-mini 56.7 63.4 64.2 55.2 57.5 14.2 63.4 70.9 59.0 83.6 8.9 11.1 13.3 13.3 22.2 2.2 11.1 20.0 6.7 44.4 60.3 MATH500-L5 56.0 66.1 61.9 52.3 54.5 17.8 65.2 62.5 49.3 87.2 AIME20246.5 7.0 13.3 10.4 20.6 1.5 3.8 18.7 3.9 41.0 62.2 45.4 58.1 54.4 51.2 48.8 11.2 62.8 58.9 44.3 78.8 4.8 5.3 11.3 6.8 16.2 0.1 2.2 14.4 2.3 28.6 53.3 29.8 42.4 46.7 45.6 31.9 5.1 56.4 47.3 35.9 57.4 3.2 2.5 8.4 2.4 4.9 0.0 2.2 3.7 1.5 8.1 15.6 43.4 55.6 53.7 50.1 44.9 10.6 61.7 56.6 42.8 75. 4.8 4.8 11.0 6.1 14.3 0.2 2.4 12.2 2.4 24.7 43.1 API-based close-source LLMs. OpenAI o1 series model does not provide an optional temperature parameter, so we chose the average accuracy of 20 generations as greedy accuracy. 2) Reasoning Ability Still Needs to be Properly Evaluated. While most models exhibit relatively strong performance under Greedy Accuracy and Pass@16, their performance significantly deteriorates when evaluated using the G-Pass@k metric. Specifically, when τ is set to 1.0, indicating that the model must provide accurate responses in all 16 attempts, almost all models exhibit drastic performance drop. Take LiveMathBench for example, the Llama-3.1-8B-Instruct models accuracy plummets from 18.1% (Greedy) to 0.8% (GPass@161.0), reduction of 95.7%. Even larger models, like NuminaMath-72B-CoT, see significant decline from 34.5% to 3.7% at G-Pass@161.0, decrease of 89.3%. Across the approximately 20 models tested, the average performance drop is 60%. Notably, even the most robust model, OpenAI o1-mini, shows 36.9% decline, from 66.5% to 42.0%. Even when τ is relaxed to 0.5, requiring only half of the samples to be correct for pass, General LLMs, Mathematical Reasoning LLMs, and O1-like Reasoning LLMs still experience average performance drops of 14.0%, 22.5%, and 4.8%, respectively. This indicates that, under challenging conditions, most models struggle to maintain consistency in their reasoning ability across multiple samples, regardless of whether the criteria are strict or lenient. These findings underscore the need for more rigorous evaluation of models reasoning capabilities, particularly in scenarios that require consistent and reliable performance over multiple instances. The current evaluation metrics, which often rely on single-shot greedy decoding, may not fully capture the true robustness and stability of these models in realworld applications. 3) Increasing Model Size May Not Significantly Enhance Robustness. Comparing models within the same series, such as Qwen2.5-32B-Instruct and Qwen2.5-72B-Instruct, reveals that despite more than twofold difference in model size, their performance remains similar across various metrics and datasets. For example, on both our latest LiveMathBench 9 Technical Report Figure 3: Illustration of G-Pass@k w.r.t. different values of for DeepSeek-Math-7b-RL, Qwen2.5-Math-72B-Instruct, QwQ-32B-Preview. and existing open-source datasets, the difference in Greedy Accuracy and mG-Pass@k between these two models is within two percentage points. Additionally, in the larger Mistral-LargeInstruct-2411 (123B) model, although the scale has increased further, performance and stability have actually declined compared to Qwen2.5-72B-Instruct. This suggests that for certain tasks, particularly those requiring deep understanding and logical reasoning, mere parameter expansion may not yield the expected gains in performance or stability. This could be because these tasks not only depend on the models memory capacity and pattern recognition but also require strong reasoning and context comprehension abilities. 4) Significant Gap Between Theoretical Performance Potential and Actual Stability. In evaluating model performance, we observed notable gap between the theoretical upper limit (G-Pass@16τ0), the actual performance (Greedy Accuracy), and the stability across multiple samples (G-Pass@16τ=1.0). As evident from Figure 1, while models theoretically possess high potential performance, their actual performance in practical applications falls short of this optimal level, particularly in terms of output stability. Some models demonstrate high accuracy in single-shot greedy decoding, indicating potential for handling specific tasks, but they fail to maintain consistent high accuracy across multiple samples, far from achieving optimal performance. This highlights the current models shortcomings in reasoning stability and consistency, which are often overlooked in training and evaluation. The single-shot inference performance of models can be influenced by factors such as input data variations, initialization states, or random sampling, leading to inconsistent results across different instances. In applications requiring high reliability and consistency, this inconsistency is significant concern, emphasizing the need to ensure stable model outputs while approaching optimal performance. 4.4 Further Analysis 4.4.1 Performance w.r.t. Figure 3 presents the results of selected models for G-Pass@4, G-Pass@8, and G-Pass@16. For G-Pass@4, Deepseek-Math-7b-RL shows significant decline in performance as τ increases, dropping from around 40% to 20%. Qwen-2.5-Math-72B-Instruct and QwQ-32BPreview also decline but maintain higher performance levels, starting around 65% and 80%, respectively, and ending around 50% and 70%. For G-Pass@8, the trend is similar, with Deepseek-Math-7b-RL showing steep decline from 40% to 20%, while Qwen-2.5-Math72B-Instruct and QwQ-32B-Preview start at 60% and 80%, respectively, and end around 45% and 70%. For G-Pass@16, Deepseek-Math-7b-RL declines from 30% to 10%, while Qwen-2.5-Math-72B-Instruct and QwQ-32B-Preview start at 60% and 80%, respectively, and end around 35% and 60%. In general, G-Pass@k can achieve consistent evaluation results under different values, which indicates robustness as metric. In addition, for advanced reasoning models with strong performance, larger value of has better differentiation. 10 Technical Report Figure 4: Illustration of G-Pass@k w.r.t. different values of for DeepSeek-Math-7b-RL and NuminaMath-72B-CoT. Table 4: Performance on CCEE and WLPM. The table shows the decreasing trend of Greedy w.r.t. Pass@16 and G-Pass@161.0 w.r.t. Greedy, which are marked with colors of different transparency. LLMs Llama-3.3-70B-Instruct Mistral-Large-Instruct-2411 DeepSeek-V2.5-1210 Qwen2.5-72B-Instruct Gemini-1.5-Pro-Latest GPT-4o Deepseek-Math-7B-RL Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct QwQ-32B-Preview CCEE WLPMC G-Pass@160 Greedy G-Pass@161.0 G-Pass@160 Greedy G-Pass@161.0 68.3 63.3 74.3 74.4 76.9 73.7 61.7 68.5 74.1 85.8 56.817.0 54.613.7 56.823.5 56.823.7 59.123.1 52.329.0 43.230.0 56.817.1 68.28.0 70.515.3 31.844.0 42.522.1 26.952.6 50.311.0 45.323.4 34.035.0 17.360.0 43.623.2 50.226.4 40.530. 41.0 21.1 65.9 54.0 60.0 29.9 12.5 52.5 48.3 89.3 36.012.2 18.213.7 9.186.0 27.349.4 36.440.0 18.239.1 9.127.2 27.348.0 27.343.5 27.369.4 9.274.4 6.166.5 4.056.1 0.398.9 4.388.2 4.078.0 0.0100.0 9.166.7 18.233.3 18.233.3 4.4.2 Performance w.r.t. As previously noted, the number of attempts is crucial for the accuracy of the estimates. We selected two models, DeepSeek-Math-7b-RL and NuminaMath-72B-CoT, to conduct experiments with = {16} {1, 2, 3, 5, 8, 15} = {16, 32, 48, 128, 240}, and reported G-Pass@16τ. The results are illustrated in Figure 4. When is small, the estimation deviation is large, as shown by the significant fluctuations in the G-Pass@16τ values for both models. Conversely, for larger n, G-Pass@16τ tends to stabilize, indicating more consistent and reliable performance. Specifically, the DeepSeek-Math-7b-RL model shows steady performance around 20% for 48, while the NuminaMath-72B-CoT model stabilizes around 30% for 48. Therefore, empirically, we recommend making at least = 3k generations when calculating G-Pass@k to ensure estimation accuracy. 4.4. Impact of Questions Difficulty We also examine the performance of models with respect to questions of varying difficulty levels. We analyze CCEE and WLPMC datasets from LiveMathBench. CCEE is college entrance examination that primarily involves fundamental high school mathematics knowledge, whereas WLPMC is prestigious collegiate mathematics competition that presents significantly greater challenges. Table 4 shows the experimental results. The findings indicate that: 1) Existing Advanced Models are Capable of Reasoning Correctly about Complex Mathematical Questions. As shown in Table 4, most models achieve high Pass@16 performance on CCEE, which is of conventional difficulty. Furthermore, Deepseek-V2.5 and QwQ-32B11 Technical Report Preview models also record high Pass@16 scores on the more challenging competition dataset. 2) Models Struggle More with Generalizing this Ability to Challenging Questions. The sharp decline in performance on WLPMC, compared to CCEE, suggests higher level of difficulty models face with these problems. For instance, on WLPMC, the Greedy accuracy of Deepseek-V2.5 decreases by 86% relative to its Pass@16 performance, and the Greedy Accuracy of QwQ-32B-Preview declines by 69% compared to its Pass@16 performance. In contrast, on CCEE, the Greedy accuracy of Deepseek-V2.5 only decreases by 24% relative to its Pass@16 performance, and the Greedy Accuracy of QwQ-32B-Preview only declines by 15% compared to its Pass@16 performance. From these observations, we conjecture that models tend to learn superficial patterns from training data, which is reflected in the marked improvement of the Pass@k metric. However, this increase does not necessarily translate into an enhancement of the models real reasoning capabilities. Thus, emphasis should be placed on evaluating greedy performance and reasoning stability. 4.4.4 Does Data Contamination or Overfitting Affect Stability? Data contamination arises when the test data is mixed into training data, also referred to as data leakage (Dickson, 2024; Dong et al., 2024). To investigate the influence of varying extents of data contamination or overfitting on our proposed G-Pass@k metric, we performed series of experiments using the Qwen2.5-7B model on the MATH500-L5 dataset. The training process began with base set of 200,000 randomly sampled instructions from the Numina-Math-CoT corpus (LI et al., 2024), which served as the uncontaminated training set. Subsequently, we introduced incremental rounds of data contamination, consisting of 0, 6, 8, 10, and 16 rounds, where round of 0 indicates the absence of contamination, i.e., training exclusively on the original NuminaMath data. The models efficacy was assessed across these five conditions, as illustrated in Figure 5. Despite the observed increase in greedy score with escalating rounds of contamination, the stability, as quantified by the G-Pass@k metric, did not exhibit corresponding enhancein ment. Specifically, the Figure 5 left part, disparity between actual performance (Greedy Accuracy) and stability across multiple samples at (G-Pass@k@16τ=1.0) each contamination round6, 8, 10, and 16was 22, 20, 18, and In respectively. 26, contrast, this gap for non-contaminated model was only 5, which indicating that the discrepancy between performance and stability in contaminated models is more than three times greater. Additionally, as shown in Figure 5 right part, the slope becomes increasingly steep as the rounds of contamination increase, shows deterioration in model stability with each additional round of contamination. This phenomenon is particularly significant in certain downstream training scenarios where overfitting becomes necessary, such as in contexts characterized by data scarcity. In these cases, while an increase in greedy accuracy might be achieved, it often comes at the cost of reduced stability. Notably, the aforementioned Figure 5: The data contamination experiment involves different contamination rounds, where #Replication represents the number of these rounds. The term Slope denotes the slope value of the G-Pass@16τ curve with respect to τ. 12 Technical Report performance-stability gap may not necessarily narrow proportionally with increased levels of overfitting. This observation suggests that although data contamination or overfit can boost the models performance on seen data, it may significantly undermine its robustness and reliability when dealing with unseen or diverse inputs. Consequently, practitioners should be cautious when introducing contaminated data into the training process, especially in contexts where model stability is crucial."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced G-Pass@k, novel evaluation metric that assesses both problemsolving capability and performance consistency of LLMs across varying thresholds of correctness. To demonstrate the practical implications of G-Pass@k, we presented LiveMathBench, dynamic multilingual mathematics benchmark that continuously incorporates contemporary challenging problems, ensuring relevance to the latest developments in both model capabilities and mathematical discourse. After detailed evaluations on LiveMathBench with G-Pass@k, we find 1) Despite demonstrating considerable potential in terms of Pass@K and Greedy Accuracy, most models exhibit instability during sampling. 2) Scaling up the model size or overfitting to the dataset can enhance Greedy Accuracy, but may not necessarily lead to significant improvements in stability. We hope that G-Pass@k and LiveMathBench can serve as pivotal tools for the research community, facilitating deeper insights into the development and evaluation of language models. 13 Technical Report"
        },
        {
            "title": "References",
            "content": "Meta AI. Llama 3.3 - 70b parameters instruct: large language model supporting multilinguality, coding, reasoning, and tool usage. https://huggingface.co/meta-llama/ Llama-3.3-70B-Instruct, 2024. Accessed: 2024-12-13. AIME2024. Aime problems and solutions. https://artofproblemsolving.com/wiki/index. php/AIME Problems and Solutions. Anthropic Inc. Claude 3.5 Sonnet: large language model for advanced text generation. https://claude.ai/chats, 2024. Accessed: 2024-12-10. Berk Atil, Alexa Chittams, Liseng Fu, Ferhan Ture, Lixinyu Xu, and Breck Baldwin. LLM stability: detailed analysis with some surprises. CoRR, abs/2408.04667, 2024. Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju Shen, Roman Soletskyi, and Lewis Tunstall. Numinamath 72b cot. https://huggingface.co/AI-MO/NuminaMath-72B-CoT, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. DeepSeek-AI. DeepSeek V2.5-1210: An advanced large language model. huggingface.co/deepseek-ai/DeepSeek-V2.5-1210, 2024. Accessed: 2024-12-13. https:// DeepSeek-R1-Lite-Preview. Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power. https://api-docs.deepseek.com/news/news1120, 2024. Accessed: 202412-13. Ben Dickson. Why data contamination is big issue for llms. https://api-docs.deepseek. com/news/news1120, 2024. Accessed: 2023-7-17. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Data contamination and trustworthy evaluation for large language models, 2024. URL https://arxiv.org/abs/2402.15938. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, 14 Technical Report Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. Int. J. Comput. Vis., 88(2): 303338, 2010. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models. CoRR, abs/2410.07985, 2024. Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou. Puzzle solving using reasoning of large language models: survey. In EMNLP, pp. 1157411591. Association for Computational Linguistics, 2024. Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, and Megan Ung. Changing answer order can decrease MMLU accuracy. CoRR, abs/2406.19470, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021b. Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. In ACL (industry), pp. 3742. Association for Computational Linguistics, 2023. Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy Liang. Spoc: Search-based pseudocode to code. In NeurIPS, pp. 1188311894, 2019. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Nu- [https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/ minamath. project-numina/aimo-progress-prize/blob/main/report/numina dataset.pdf), 2024. Yucheng Li, Yunhao Guo, Frank Guerin, and Chenghua Lin. An open-source data contamination report for large language models. In EMNLP (Findings), pp. 528541. Association for Computational Linguistics, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR. OpenReview.net, 2024. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the theory and application proficiency of llms with hierarchical mathematics benchmark. In ACL (Findings), pp. 68846915. Association for Computational Linguistics, 2024a. 15 Technical Report Yan Liu, Renren Jin, Lin Shi, Zheng Yao, and Deyi Xiong. Finemath: fine-grained mathematical evaluation benchmark for chinese large language models. CoRR, abs/2403.07747, 2024b. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. CoRR, abs/2308.09583, 2023. Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, and Min Yang. Training on the benchmark is not all you need. CoRR, abs/2409.01790, 2024. Skywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. OpenAI. Gpt-4o: Advancements in text and vision integration. https://www.openai.com/ blog/gpt-4o-integration/, 2024a. Accessed: 2024-12-10. OpenAI. Learning to reason with llms. learning-to-reason-with-llms/, 2024b. Accessed: 2024-12-10. https://openai.com/index/ Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https: //qwenlm.github.io/blog/qwen2.5/. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In ACL, pp. 4932 4942. Association for Computational Linguistics, 2019. Google Research. Gemini 1.5 pro: large language model by google. https://developers. google.com/generative-ai, 2024. Accessed: 2024-12-12. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. Llm-as-ajudge & reward model: What they can and cannot do. CoRR, abs/2409.11239, 2024. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024a. URL https://qwenlm.github.io/blog/qwq-32b-preview/. The Mistral AI Team. Mistral-Large Instruct 2411: large language model https://huggingface.co/mistralai/ supporting Mistral-Large-Instruct-2411, 2024b. Accessed: 2024-12-13. instructions. advanced A. T. Vandermonde. Mem. Acad. Roy. Sciences Paris. 1772. Hanzi Xu, Renze Lou, Jiangshu Du, Vahid Mahzoon, Elmira Talebianaraki, Zhuoan Zhou, Elizabeth Garrison, Slobodan Vucetic, and Wenpeng Yin. Llms classification performance is overclaimed. CoRR, abs/2406.16203, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024a. 16 Technical Report An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin. Internlm-math: Open math large language models toward verifiable reasoning. CoRR, abs/2402.06332, 2024. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In ICLR. OpenReview.net, 2024. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. CoRR, abs/2305.12474, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Dont make your LLM an evaluation benchmark cheater. CoRR, abs/2311.01964, 2023. Technical Report"
        },
        {
            "title": "A Appendix",
            "content": "A.1 LiveMathBench Details A.1.1 Data Sources LiveMathBench (version of 202412) is composed of 4 parts including CNMO, CCEE, AMC, and WLPMC. CNMO. The CNMO section features curated questions from the latest Chinese National Mathematics Olympiad. To enhance the difficulty level, single-choice questions are transformed into problem-solving tasks by concealing answer options, necessitating models to reason independently and provide solutions. CCEE. In the CCEE segment, we have selected questions from recent mock exams of Chinas College Entrance Examination, excluding multi-modal proof problems. We have excluded multiple-choice questions and converted single-choice items into problem-solving questions, removing provided answer choices to assess the models ability to generate solutions autonomously. AMC. The AMC section includes questions from the latest American Mathematics Competition, where each original question typically offers five possible answers labeled through E, with only one correct option. Consistent with our approach in other sections, we convert these single-choice questions into problem-solving cues, encouraging models to deduce solutions without the aid of provided options. WLPMC. We also include questions from the latest William Lowell Putnam Mathematical Competition (WLPMC). Regarded as one of the most prestigious university-level mathematics competitions globally, the WLPMC challenges participants with problems that span broad spectrum of mathematical disciplines. These include geometry, algebra, trigonometry, calculus, linear algebra, combinatorics, probability theory, number theory, complex numbers, and differential equations. A.1.2 Data Samples Here we provide some samples in LiveMathBench. Example in CNMO [Question] 设复数z, w满足z + = 2求S = z2 2w + w2 2z的最小可能值 [Answer] 5 16 8 [Question Type] 问答 A.2 Estimation of G-Pass@k To demonstrate the unbiasedness of Equation (5), we conduct the simulation experiment illustrated in Figure 6. Specifically, we assume the probability of model providing the correct solution in single run is = 0.4. For each n, we perform several random Bernoulli samplings to obtain different values of to calculate G-Pass@kτ, and then compute the mean and variance to generate the figure. From Figure 6, it can be observed that Equation (5) is an unbiased estimator, facilitating fair comparison across different values of n. 18 Technical Report Example in CCEE [Question] 函数 (x) = x3e3x3lnx1 [Answer] 3 [Question Type] 填空 Example in AMC (x > 0)的最小值是 [Question] The graph of = ex+1 + ex 2 has an axis of symmetry. What is the reflection of the point (1, 1 [Answer] (cid:17) (cid:16) 2 ) over this axis? 0, 1 2 [Question Type] Problem-Solving Example in WLPMC [Question] sequence y1, y2, . . . , yk of real numbers is called zigzag if = 1, or if y2 y1, y3 y2, . . . , yk yk1 are nonzero and alternate in sign. Let X1, X2, . . . , Xn be chosen independently from the uniform distribution on [0, 1]. Let a(X1, X2, . . . , Xn) be the largest value of for which there exists an increasing sequence of integers i1, i2, dots, ik such that Xi1, Xi2, . . . , Xik a(X1, X2, . . . , Xn) for 2. [Answer] 2n+2 3 Find the expected value of is zigzag. [Question Type] Problem-Solving A.3 Proof of Theorem 3. Proof. Since starts iterating at the upward rounding of τ and τ (0, 1], so we have: j=τk j) (nc (c kj) (n k) = j=1 j) (nc (c kj) (n k) . lim τ0 (8) According to the Vandermondes Identity (Vandermonde, 1772), the numerator term on the right side of Equation (8) can be written as (cid:19) (cid:18)c c j=1 (cid:19) (cid:18)n = = j=0 (cid:18)n (cid:19) (cid:18)c (cid:19) (cid:18)n (cid:19) (cid:18)n (cid:19) (cid:19) (cid:18)n . (9) So we conclude that: 19 Technical Report Figure 6: Illustration of estimation and the true value of G-Pass@kτ. j=τk j) (nc (c kj) (n k) lim τ0 = = j) (nc (c kj) (n k) j=1 k) (nc (n ) (n k) (nc ) (n k) . = 1 (10) A.4 Judge Details A.4.1 Configurations of Judge Model Inspired by previous works (Zheng et al., 2023; Son et al., 2024), we leverage Qwen2.5-72BInstruct (Yang et al., 2024a) to judge if the answers generated by the models are consistent with the golden answers, consider the high inference cost of the closed source models such as OpenAI models. We set the temperature to 0.0, and maximum output tokens to 8, 192. A.4.2 Prompt for Judge We leverage the following prompts to judge the consistency between candidate answers and reference answers. 20 Technical Report Chinese Version of Judge Prompt 请你作为一个数学阅卷专家判断下面的答案是否与标准答案一致即考生是否回 答正确下面是一些评判标准 1. 有些答案可能包含多项内容可能有单选题多选题填空题和问答题只要答 案与标准答案一致即可, 对于多选题和多个空的填空题需要考生对应的选项或空都 回答正确才算正确 2. 有些答案可能通过不同的方式表达比如有些答案可能是一个数学表达式有些 答案可能是一个文字描述只要表达的意思一致即可且有些公式通过不同的方式 表达但等价也是正确的 3. 你不需要重新计算问题答案因为标准答案已经给出只需要根据问题形式来判 断考生的答案是否与标准答案一致是否正确即可 请 你 根 据 上 述 标 准 判 断 下 面 的 答 案 是 否 与 标 准 答 案 一 致 如 果 一 致 请 在 最 后 输 出boxed{{yes}}, 否 则 输 出boxed{{no}}, 如 果 难 以 判 断 请 输 出boxed{{no}}. 原问题{question} 标准答案{reference answer} 考生答案{candidate answer} 分析 English Version of Judge Prompt Please act as an expert in grading mathematics exam papers, and judge whether the following answers match the standard answers, i.e., whether the examinee answered correctly. Here are some evaluation criteria: 1. Some answers may contain multiple parts, such as single-choice questions, multiple-choice questions, fill-in-the-blank questions, and problem-solving questions. As long as the answer matches the standard answer, it is considered correct. For multiple-choice questions and fill-in-the-blank questions with multiple blanks, the examinee must answer all corresponding options or blanks correctly to be considered correct. 2. Some answers may be expressed in different ways; for example, some answers may be mathematical expressions, while others may be textual descriptions. As long as the meaning conveyed is consistent, it is considered correct. Additionally, some formulas may be expressed differently but are equivalent, which is also considered correct. 3. You do not need to recalculate the problem answers, as the standard answers are already provided. You only need to judge whether the examinees answer matches the standard answer based on the form of the question and whether it is correct. Please judge whether the following answer matches the standard answer according If they match, output boxed{{yes}}, otherwise output to the above criteria. boxed{{no}}. If it is difficult to judge, also output boxed{{no}}. Original Question: {question} Standard Answer: {reference answer} Examinees Answer: {candidate answer} Analysis: A.4.3 Evaluation of Judge Model To evaluate the effectiveness of our judge model, we compared the agreement rate between Qwen2.5-72B-as-Judge and GPT4o-as-Judge (OpenAI, 2024a). Specifically, we randomly selected 300 samples from the generations of five different models and used the judgments from GPT4o as the ground truth. We then calculated the agreement rate between the judgments made by our model and those by GPT4o. Table 5 presents the results, demonstrating 21 Technical Report that Qwen2.5-72B-as-Judge achieves high consistency with GPT4o-as-Judge across different models. These findings validate the feasibility of Qwen2.5-72B-as-Judge. Table 5: Agreement rates between Qwen2.5-72B-as-Judge and GPT4o-as-judge. Models Need to Judge Agreement Disagreement Accuracy (%) Deepseek-Math-7B-RL Qwen2.5-32B-Instruct Qwen2.5-Math-72B-Instruct Mistral-Large-Instruct-2411 QwQ-32B-Preview 296 282 287 285 4 18 13 15 10 98.7 94.0 95.7 95.0 96.7 A.5 Full Experimental Results A.5.1 Full Performance on LiveMathBench Table 6 presents the comprehensive performance results on LiveMathBench. A.5.2 Full Performance on MATH500-L5 & AIME2024Table 7 presents the comprehensive performance results for MATH500-L5 and AIME2024-45. 22 Technical Report Table 6: Full performance of models on LiveMathBench. We report results of greedy decoding, Pass@16 (G-Pass@160), G-Pass@16{0.25,0.5,0.75,1.0}, and mG-Pass@16. LLMs Greedy G-Pass@16 (Equation (5)) / % G-Pass@160 G-Pass@160.25 G-Pass@160.5 G-Pass@160.75 G-Pass@161.0 mG-Pass@16 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct DeepSeek-V2.5-1210 Mistral-Large-Instruct-2411 Gemini-1.5-Pro-Latest Claude-3.5-Sonnet GPT-4o-2024-11-20 InternLM2-Math-Plus-20B DeepSeek-Math-7B-RL NuminaMath-72B-CoT Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Skywork-o1 QwQ-32B-Preview OpenAI o1-mini 18.1 27.7 34.9 32.8 42.9 43.7 43.3 37.4 49.2 37.0 39.9 22.3 20.6 34.5 39.9 50. 39.5 64.3 66.5 General LLMs 18.1 27.3 38.4 41.5 51.6 46.0 55.8 33.3 58.7 45.7 49.2 7.5 18.2 30.5 31.7 40.6 41.7 39.7 29.1 48.0 35.2 36.1 Mathematical Reasoning LLMs 21.1 23.9 32.3 46.8 54. 10.1 17.9 22.6 39.2 45.3 O1-like Reasoning LLMs 42.7 75.3 77.3 31.2 66.6 68.5 43.5 54.0 55.8 60.5 66.6 65.1 64.3 41.7 73.9 63.1 66.9 52.9 43.3 60.2 62.0 66. 57.3 84.2 84.5 3.5 13.0 23.7 23.4 34.4 34.5 28.1 26.7 40.2 27.2 28.2 3.0 12.7 12.8 32.2 37.8 24.1 56.2 58.8 0.8 7.1 16.0 13.8 23.9 25.2 16.6 23.5 26.8 17.4 18.4 0.3 5.8 3.7 24.2 26. 13.1 33.3 42.0 3.3 12.3 22.7 22.2 32.6 33.2 27.0 26.3 37.8 25.9 26.8 3.3 11.7 11.8 31.2 36.5 22.6 52.2 56.5 API-based close-source LLMs. OpenAI o1 series model does not provide an optional temperature parameter, so we chose the average accuracy of 20 generations as greedy accuracy. Table 7: Full performance of models on MATH500-L5 and AIME2024-45. Results of greedy decoding, Pass@16 (G-Pass@160), G-Pass@16{0.25,0.5,0.75,1.0}, and mG-Pass@16 are reported. LLMs Greedy G-Pass@16 (Equation (5)) / % G-Pass@160 G-Pass@160.25 G-Pass@160.5 G-Pass@160.75 G-Pass@161.0 mG-Pass@16 Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Mistral-Large-Instruct-2411 Llama-3.3-70B-Instruct DeepSeek-Math-7b-RL Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Skywork-o1 QwQ-32B-Preview Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Mistral-Large-Instruct-2411 Llama-3.3-70B-Instruct DeepSeek-Math-7b-RL Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Skywork-o1 QwQ-32B-Preview OpenAI o1-mini 56.7 63.4 64.2 55.2 57.5 14.2 63.4 70.9 59.0 83. 8.9 11.1 13.3 13.3 22.2 2.2 11.1 20.0 6.7 44.4 60.3 77.0 80.2 77.3 58.4 71.7 42.8 77.1 77.9 71.0 95.9 22.4 30.1 33.0 15.4 39.6 16.3 22.5 37.4 17.2 74.3 86.7 MATH500-L5 65.1 71.2 70.5 54.4 62.8 25.0 69.3 70.0 57.0 92.5 AIME20248.2 15.3 17.4 11.1 26.8 2.2 8.9 23.7 4.7 59.3 80.0 56.0 66.1 61.9 52.3 54.5 17.8 65.2 62.5 49.3 87.2 6.5 7.0 13.3 10.4 20.6 1.5 3.8 18.7 3.9 41.0 62.2 45.4 58.1 54.4 51.2 48.8 11.2 62.8 58.9 44.3 78.8 4.8 5.3 11.3 6.8 16.2 0.1 2.2 14.4 2.3 28.6 53.3 29.8 42.4 46.7 45.6 31.9 5.1 56.4 47.3 35.9 57. 3.2 2.5 8.4 2.4 4.9 0.0 2.2 3.7 1.5 8.1 15.6 43.4 55.6 53.7 50.1 44.9 10.6 61.7 56.6 42.8 75.6 4.8 4.8 11.0 6.1 14.3 0.2 2.4 12.2 2.4 24.7 43.1 API-based close-source LLMs. OpenAI o1 series model does not provide an optional temperature parameter, so we chose the average accuracy of 20 generations as greedy accuracy."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory"
    ]
}