{
    "paper_title": "Learning to Reason without External Rewards",
    "authors": [
        "Xuandong Zhao",
        "Zhewei Kang",
        "Aosong Feng",
        "Sergey Levine",
        "Dawn Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 9 5 9 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Learning to Reason without External Rewards",
            "content": "Xuandong Zhao UC Berkeley xuandongzhao@berkeley.edu Zhewei Kang UC Berkeley waynekang@berkeley.edu Aosong Feng Yale University aosong.feng@yale.edu Sergey Levine UC Berkeley svlevine@berkeley.edu Dawn Song UC Berkeley dawnsong@berkeley.edu"
        },
        {
            "title": "Abstract",
            "content": "Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose INTUITOR, an RLIF method that uses models own confidencetermed self-certaintyas its sole reward signal. INTUITOR replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that INTUITOR matches GRPOs performance on mathematical benchmarks while achieving superior generalization to out-ofdomain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/ sunblaze-ucb/Intuitor. Figure 1: Overview of RLIF and INTUITORs Performance. Left: Illustration of RLIF, paradigm where LLMs learn from intrinsic signals generated by the model itself, without external supervision. Right: Performance comparison of Qwen2.5-3B Base, GRPO, and INTUITOR (our RLIF instantiation). Both GRPO and INTUITOR are trained on the MATH dataset. INTUITOR achieves comparable performance to GRPO on in-domain mathematical benchmarks (GSM8K, MATH500) and demonstrates better generalization to out-of-domain code generation tasks (LiveCodeBench v6, CRUXEval). Part of the illustration was generated by GPT-4o. Equal contribution. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning has become essential for enhancing large language model capabilities. Early work focused on Reinforcement Learning from Human Feedback (RLHF), which aligns model outputs with human values through reward models trained on preference data [Ouyang et al., 2022]. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) replace learned reward models with automatically verifiable signalssuch as exact answer matching in mathematical problemsolvingdemonstrating improved reasoning capabilities in models like DeepSeek-R1 [Guo et al., 2025, Lambert et al., 2024, Hu et al., 2025]. Despite these successes, both RLHF and RLVR face fundamental limitations that constrain their broader applicability. RLHF requires extensive human annotation, making it expensive and potentially biased [Gao et al., 2023]. RLVR, while avoiding learned reward models, demands domain-specific verifiers and gold-standard solutions. In mathematics, this requires expert annotation of solutions; in code generation, it necessitates comprehensive test suites and execution environments [Liu et al., 2023, Liu and Zhang, 2025, Team et al., 2025, Xiaomi LLM-Core Team, 2025]. These requirements limit RLVR to carefully curated domains and complicate deployment in open-ended scenarios. Moreover, outcome-oriented verifiable rewards limit transferability to other domains. These challenges motivate exploration of more general and scalable reward paradigms, leading to critical research question: Can LLMs enhance their reasoning abilities by relying solely on intrinsic, self-generated signals, without recourse to external verifiers or domain-specific ground truth? In this paper, we introduce and explore such paradigm: Reinforcement Learning from Internal Feedback (RLIF), where models optimize intrinsic feedback to improve performance without external rewards or supervision. The motivation for RLIF extends to future scenarios where models develop superhuman capabilities that become difficult for humans to evaluate directly [Burns et al., 2023], requiring self-improvement through intrinsic mechanisms [Oudeyer and Kaplan, 2007]. Under the RLIF paradigm, we propose INTUITOR, novel reinforcement learning approach leveraging models own confidence as an intrinsic reward. This builds on observations that LLMs exhibit lower confidence on difficult problems [Farquhar et al., 2024, Kuhn et al., 2023, Kang et al., 2024, 2025]; optimizing for confidence should improve reasoning capabilities. Specifically, we use selfcertainty [Kang et al., 2025], the average KL divergence between the models output distribution and uniform distribution, as our confidence measure. This metric has proven useful for distinguishing high-quality responses from flawed ones [Kang et al., 2025, Ma et al., 2025]. Building on this insight, INTUITOR guides learning through self-generated signals, eliminating the need for external supervision or handcrafted rewards. The implementation of INTUITOR is simple, efficient, and effective: we replace the verifiable reward signal in existing RLVR frameworks, specifically Group Relative Policy Optimization (GRPO) [Shao et al., 2024], with self-certainty scores, using the same policy gradient algorithm. Our experiments demonstrate promising results. On the MATH dataset [Hendrycks et al., 2021] with Qwen2.5-3B base [Yang et al., 2024a], INTUITOR matches the performance of GRPO without relying on any gold answers. As INTUITOR rewards the generation trajectory rather than only the end result, it generalizes more effectively: training Qwen2.5-3B base model on MATH yields 65% relative improvement on LiveCodeBench Code generation task [Jain et al., 2024] versus no improvement for GRPO, and 76% gain on CRUXEval-O [Gu et al., 2024] compared with 44% for GRPO. Additionally, when we fine-tune the Qwen2.5-1.5B base model with INTUITOR on the MATH corpus, model that originally produces repetitive content and scores 0% on LiveCodeBench learns to emit coherent reasoning chains and well-structured code, reaching 9.9% accuracy after the tuning. This demonstrates the strong generalization capabilities of INTUITOR. As INTUITOR requires only clear prompt and no verifiable reward, it is broadly applicable across tasks, providing fresh evidence that pretrained LLMs possess richer latent behavioral priors than previously recognized. Our contributions can be summarized as follows: We introduce and explore Reinforcement Learning from Internal Feedback (RLIF), novel reinforcement learning paradigm enabling LLMs to improve reasoning skills by leveraging intrinsic, self-generated signals, without reliance on external supervision or labeled data. We introduce INTUITOR, an RLIF-based method that utilizes models own internal confidence measuretermed self-certaintyas the sole intrinsic reward. 2 We demonstrate that INTUITOR matches supervised RL performance on in-domain tasks while achieving superior out-of-domain generalization. We uncover emergent structured reasoning and enhanced instruction-following capabilities induced by intrinsic rewards."
        },
        {
            "title": "2 Related Work",
            "content": "Reinforcement Learning from Human Feedback (RLHF). RL has become instrumental in refining LLMs. Early pivotal work centered on Reinforcement Learning from Human Feedback (RLHF) [Ouyang et al., 2022], which aligns LLMs with human values by training reward model on human preference data. While effective, RLHF is often resource-intensive due to the need for extensive human annotation [Touvron et al., 2023]. Subsequent innovations like Direct Preference Optimization (DPO) [Rafailov et al., 2023] aimed to simplify this by directly training models on preferences. The reliance on human-generated or model-approximated human preferences poses scalability challenges and introduces potential biases from the reward model itself [Gao et al., 2023]. Reinforcement Learning with Verifiable Rewards (RLVR). RLVR emerged as powerful alternative, particularly for tasks with clear correctness criteria like mathematical reasoning and code generation [Guo et al., 2025, Lambert et al., 2024, Hu et al., 2025, Team et al., 2025, Xiaomi LLMCore Team, 2025]. RLVR utilizes rule-based verification functions, such as exact answer matching [Guo et al., 2025, Team et al., 2025, Xiaomi LLM-Core Team, 2025, Jaech et al., 2024], to provide reward signals, thereby avoiding the complexities and potential pitfalls of learned reward models. This approach has sparked significant advances, with models like DeepSeek-R1 [Guo et al., 2025] achieving state-of-the-art reasoning capabilities. The development of robust policy optimization algorithms like GRPO [Shao et al., 2024] and its variants [Luo et al., 2025, Liu et al., 2025] has further solidified RLVRs success. Nevertheless, RLVRs applicability is largely confined to domains where verifiable gold solutions or exhaustive test cases can be constructed, and its predominant focus on outcome-based rewards can limit generalization to dissimilar tasks or those requiring nuanced, process-oriented feedback. Intrinsic Signals and Self-Play in Language Model Optimization. Self-play and intrinsic rewards have gained attention as strategies for enabling autonomous model improvement. Inspired by early work in games like AlphaGo Zero [Silver et al., 2017], recent LLM-based frameworks incorporate self-refinement mechanisms to bootstrap reasoning ability. Methods like SPIN [Chen et al., 2024] and Self-Rewarding LMs [Yuan et al., 2024] utilize the model itself to provide feedback for subsequent training iterations. While earlier work such as STaR [Zelikman et al., 2022] relies on repeated outcome evaluation, more recent approaches explore self-improvement through procedural generalization and goal invention [Poesia et al., 2024, Cheng et al., 2024]. Concurrent works like Genius, TTRL, and Absolute Zero [Xu et al., 2025, Zuo et al., 2025, Zhao et al., 2025]leverage queries without labels for reinforcement learning but remain constrained to specific task distributions, primarily in mathematical reasoning. INTUITOR aligns with this direction but introduces lightweight, generalpurpose alternative: using self-certainty as confidence-based intrinsic reward. Unlike prior work, INTUITOR enables single-agent reinforcement learning across diverse tasks without relying on explicit feedback, gold labels, or environment-based validation."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present our approach to training large language models through intrinsic feedback mechanisms. We begin by reviewing existing RL-based fine-tuning paradigms and their limitations, which motivate our exploration of Reinforcement Learning from Internal Feedback (RLIF). We then introduce INTUITOR, our novel RLIF method that leverages self-certainty as an intrinsic reward signal, and detail its implementation through policy optimization. 3.1 From External Supervision to Internal Feedback Current RL fine-tuning approaches for LLMs primarily fall into two categories: those relying on external human feedback (RLHF) and those using verifiable, often task-specific, rewards (RLVR). 3 In RLHF [Ziegler et al., 2019, Ouyang et al., 2022], the policy πθ is optimized to align with human preferences, typically encapsulated by learned reward model rϕ. The objective is: max πθ Eoπθ(q) [rϕ(q, o) βKL[πθ(oq)πref(oq)]] (1) where is an input query, is the generated output, πref is an initial reference policy, and β is coefficient controlling the KL divergence to prevent excessive deviation from πref. Online RL algorithms like PPO [Schulman et al., 2017] generate samples from πθ, evaluate them using rϕ, and update πθ to maximize this objective. However, the reward model rϕ is crucial yet fragile; introducing it can lead to reward hacking, and retraining it is resource-intensive, complicating the training pipeline [Gao et al., 2023]. RLVR, on the other hand, substitutes the learned reward model with an automatically verifiable signal. This has proven effective in promoting reasoning capabilities, especially in domains like mathematics [Guo et al., 2025]. The RLVR objective is: max πθ Eoπθ(q) [v(q, o) βKL[πθ(oq)πref(oq)]] (2) where v(q, o) is verifiable reward function. For instance, in mathematical problem-solving, v(q, o) (cid:26)α if output is correct 0 otherwise. might be: v(q, o) = . RLVR is often implemented using algorithms like REINFORCE [Williams, 1992], PPO or GRPO. Despite their simplicity, verifiable rewards still rely on gold-standard answers or test executions, which are costly and domain-specific [Liu et al., 2025, Team et al., 2025]. RLVR faces challenges in extending beyond math and code to tasks involving ambiguity or subjective reasoning. 3.2 Reinforcement Learning from Internal Feedback (RLIF) To overcome the limitations of RLHFs costly human annotation and RLVRs domain-specific supervision, we propose Reinforcement Learning from Internal Feedback (RLIF). Instead of depending on external evaluation, RLIF uses the models own assessment of its outputs or reasoning process as feedback. This offers several advantages: it reduces reliance on supervision infrastructure, provides task-agnostic reward signals, and supports learning in domains where external verification is unavailable. Under the RLIF paradigm, the optimization objective becomes: max πθ Eoπθ(q) [u(q, o) βKL[πθ(oq)πref(oq)]] (3) where u(q, o) represents an intrinsic signal derived from the models internal state or computation, rather than external verification. The key challenge lies in identifying intrinsic signals that correlate with output quality and can effectively guide learning. Concurrent research explores related concepts within the RLIF paradigm. For example, Entropy Minimized Policy Optimization (EMPO) [Zhang et al., 2025] minimizes LLM predictive entropy on unlabeled questions in latent semantic space. SEED-GRPO [Chen et al., 2025] uses the semantic entropy of generated sequences, combined with ground truth rewards, to modulate policy updates. Reinforcement Learning with Negative Entropy Reward (EM-RL) [Agarwal et al., 2025] employs reward signal based solely on the negative sum of token-level entropy, akin to REINFORCE but without labels. These methods highlight the growing interest and potential of leveraging intrinsic signals for LLM training under the RLIF framework. 3.3 INTUITOR: Policy Optimization with Self-Certainty We propose INTUITOR, novel RLIF method that utilizes models own confidence as the sole intrinsic reward signal u(q, o). Our choice of model confidence as the intrinsic reward is motivated by observations that LLMs often exhibit lower confidence when encountering unfamiliar tasks or lacking sufficient knowledge [Kang et al., 2024]. Conversely, higher confidence frequently correlates with correctness. By rewarding increased self-confidence, INTUITOR encourages to iteratively \"practice\" and refine its reasoning pathways until it becomes more confident in its outputs. 4 Figure 2: Illustration of INTUITOR. INTUITOR simplifies the training strategy by leveraging selfcertainty (the models own confidence) as an intrinsic reward, optimizing these scores to incentivize reasoning abilities without external supervision. We adopt the self-certainty metric from Kang et al. [2025], defined as the average KL divergence between uniform distribution over the vocabulary and the models next-token distribution: Self-certainty(oq) := 1 o (cid:88) i=1 KL(U pπθ (q, o<i)) = 1 V (cid:88) (cid:88) i=1 j=1 log (V pπθ (jq, o<i)) (4) where o<i are the previously generated tokens and p(jq, o<i) is the models predicted probability for token at step i. Higher self-certainty values indicate greater confidence. Self-certainty, being related to KL divergence where the models prediction is the second argument KL(U pπθ ), is mode-seeking. This contrasts with entropy (or reverse KL divergence from uniform), which is mode-covering. Critically, self-certainty is reported to be less prone to biases towards longer generations, common issue with perplexity or entropy-based measures [Fang et al., 2024, Kang et al., 2025], making it potentially more reliable indicator of intrinsic confidence. Kang et al. [2025] demonstrate that self-certainty is effective for selecting high-quality answers from multiple candidates, and uniquely among different confidence measures, its utility improves with more candidates. Optimizing for self-certainty thus encourages the model to generate responses that it deems more convincing. The RL process can achieve this by, for instance, guiding the model to produce more detailed reasoning steps, thereby increasing the models conviction in its final answer. This mechanism is more nuanced than simply increasing the probability of the most likely output; it involves modifying the generation process itself to build confidence. To optimize the objective in Equation 3, various policy gradient algorithms can be employed. Informed by the recent success in models such as DeepSeek-R1 [Guo et al., 2025] and its widespread adoption of GRPO in open-source projects, we utilize GRPO to optimize for self-certainty. The overall pipeline for this GRPO-based instantiation of INTUITOR is illustrated in Figure 2. The core idea behind the optimization is to sample multiple candidate outputs for given query and use their relative rewards to estimate advantages for policy updates. For each query (Q), GRPO samples group of outputs o1, . . . , oG using behavior policy πθold (e.g., previous iteration or the SFT model). The target policy πθ is then optimized by maximizing: JGRPO(θ) = (5) qP (Q),{oi}G i=1πθold (Oq) (cid:32)(cid:20) min ci,t(θ) ˆAi,t, clip(cid:0)ci,t(θ), 1 ϵ, 1 + ϵ(cid:1) ˆAi,t (cid:21) β DKL (cid:0)πθπref (cid:1) (cid:33) , 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 ci,t(θ) = πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) , DKL(πθπref) = πref(oi,t q, oi,<t) πθ(oi,t q, oi,<t) log πref(oi,t q, oi,<t) πθ(oi,t q, oi,<t) Hyperparameters ϵ (for clipping) and β (for KL penalty strength) control stability and exploration, and ˆAi,t is the advantage estimate. Integration of Self-Certainty. The key innovation in INTUITOR is replacing external rewards with self-certainty scores in GRPOs advantage computation. Specifically, each output oi is scored by: ui = Self-certainty(oiq), ˆAi,t = ui mean({u1, u2, , uG}) std({u1, u2, , uG}) . (6) 5 Table 1: Performance comparison of various methods on the GSM8K, MATH, LCB, CRUXEval-O, MMLU-Pro, and AlpacaEval benchmarks. The INTUITOR-Code variant is trained on Codeforces data with smaller learning rate and fewer training steps. All evaluations are obtained with the chat inference template, except for MMLU-Pro. Model Training Data GSM8K MATH500 LCB CRUX MMLU-Pro AlpacaEval Qwen2.5-1.5B Results Base + GRPO + INTUITOR Qwen2.5-3B Results Base + GRPO + GRPO-PV + INTUITOR + INTUITOR-Code - MATH MATH - MATH MATH MATH Codeforces 0.002 0.747 0. 0.673 0.826 0.820 0.792 0.743 0.090 0.560 0.530 0.544 0.636 0.636 0.612 0.572 0.000 0.056 0.099 0.093 0.085 0.086 0.153 0.153 0.000 0.328 0. 0.236 0.341 0.299 0.416 0.411 0.297 0.315 0.310 0.377 0.403 0.398 0.379 0.386 2.10 4.03 4.28 3.72 6.91 6.17 7.10 4.16 This formulation enables the policy to favor outputs that the model itself considers more confident, creating self-reinforcing learning loop. The complete INTUITOR training pipeline operates by sampling multiple candidate outputs for each query, computing self-certainty scores for each candidate, using these scores to estimate advantages within the group, and updating the policy to increase the likelihood of generating high-confidence outputs. This process requires no external supervision, making it broadly applicable across domains and tasks."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Training Setup. Both GRPO and INTUITOR are trained with the Open-R1 framework [Face, 2025] on the training split of the MATH dataset [Hendrycks et al., 2021], which contains 7,500 problems. We use Qwen2.5-1.5B and Qwen2.5-3B [Yang et al., 2024b] as backbone models, with chat-based prompting format throughout. Given the models initially weak instruction-following abilities, we do not require them to disentangle intermediate reasoning from final answers. Each update processes 128 problems, generating 7 candidate solutions per problem, with default KL penalty of β = 0.005. For fair comparison, GRPO and INTUITOR share identical hyperparameters (see Appendix) without additional tuning. We also evaluate GRPO variant, denoted GRPO-PV in Table 1, which uses plurality voting2 as proxy for ground truth. This follows the approach from TTRL [Zuo et al., 2025], which shows that self-consistency-based rewards can match the performance of golden answers when training on inference data. INTUITOR for Code Generation (INTUITOR-Code). To assess generalization beyond mathematical reasoning, we apply INTUITOR to the Codeforces code generation dataset [Li et al., 2022]. This variant, denoted INTUITOR-Code in Table 1, modifies the setup as follows: the number of sampled completions per problem is increased to 14; the learning rate is reduced from 3 105 to 1 105; and the KL penalty is increased to β = 0.01. For simplicity, we limit the run to 50 steps, utilizing total of 3,200 problems. Evaluation. Evaluations generally use the same chat-style prompting format as in training, except for MMLU-Pro [Wang et al., 2024], where we follow the benchmarks original prompt format. Greedy decoding is used for all completions. Experiments were conducted on NVIDIA A100 GPUs, each with 40GB of memory. We evaluate performance on the following benchmarks (1) Math reasoning: MATH500 and GSM8K, using the lighteval library [Habib et al., 2023]. (2) Code reasoning: CRUXEval-O [Gu et al., 2024], using the ZeroEval framework [Lin, 2024], and LiveCodeBench v6 (LCB) [Jain et al., 2024]. (3) Instruction following: AlpacaEval 2.0 with length-controlled win rates [Dubois et al., 2024], judged by GPT-4.1 [OpenAI, 2025]. 6 Figure 3: Average response lengths during training rollouts. For Qwen2.5-1.5B, INTUITOR and GRPO reduce gibberish outputs. For Qwen2.5-3B, INTUITOR and GRPO increase reasoning length; INTUITOR yields significantly longer responses. GRPO-PV shows minimal length increase."
        },
        {
            "title": "5 Results and Analysis",
            "content": "Table 1 presents main evaluation results, and Figure 3 illustrates response length evolution during training. On in-domain MATH and GSM8K datasets, INTUITOR and GRPO-PV (both golden-answerfree) achieve performance comparable to GRPO (using golden answers). This aligns with TTRL [Zuo et al., 2025], where plurality voting approximated golden answers without significant performance loss. While INTUITOR performs slightly worse than GRPO overall, on MATH it produces longer responses and demonstrates markedly improved code generation, suggesting enhanced reasoning capabilities. 5.1 Learning to Follow Instructions INTUITOR significantly enhances instruction-following. Initially, the pretrained Qwen2.5-1.5B struggles with chat-style prompts, scoring <10% on all chat-template tasks  (Table 1)  and generating repetitive, nonsensical output, which inflates average response lengths (Figure 3). Fine-tuning with INTUITOR sharply reduces such gibberish, decreases completion lengths, and enables non-trivial performance across all evaluated benchmarks. Furthermore, on the MATH dataset, INTUITOR substantially improves the Length Control Win Rate on AlpacaEval for both Qwen2.5-1.5B and Qwen2.5-3B, surpassing GRPO under identical settings. This demonstrates robust gains in instruction adherence. 5.2 Fostering Structured Reasoning Rapid Initial Learning. Self-certainty, continuous and inherently process-aware reward derived from the models internal assessment across all tokens, contrasts with binary rewards. This internal signal may encourage LLMs to follow more effective learning trajectories. Given comparable final performance between GRPO and INTUITOR, we assess early-stage learnability by comparing in-domain accuracy at training step 10. As shown in Table 2, INTUITOR consistently outperforms GRPO on both GSM8K and MATH benchmarks for Qwen2.5-1.5B and Qwen2.5-3B, highlighting its advantage in rapid initial learning. Qwen2.5-3B Model Qwen2.5-1.5B Table 2: Early-stage performance (training step 10) on GSM8K and MATH. INTUITOR consistently outperforms GRPO. Method GSM8K MATH Baseline GRPO INTUITOR Baseline GRPO INTUITOR 0.002 0.081 0.152 0.673 0.758 0.811 0.090 0.296 0.368 0.544 0.596 0. 2Self-consistency uses plurality rule, selecting the most frequent answer even without majority support, while majority voting requires > 50% support and otherwise yields no winner [De Condorcet et al., 2014]. 7 Figure 4: Performance evolution on MATH500 (in-domain) and LiveCodeBench (transfer) for models trained on MATH. In-domain (MATH500) accuracy improves rapidly early in training, preceding gains in code-generation (LiveCodeBench) accuracy. LiveCodeBench performance continues to rise even after MATH500 accuracy plateaus. Cross-Task Generalization. Figure 4 illustrates performance trajectories on MATH500 (in-domain) and LiveCodeBench (transfer task) for models trained on the MATH dataset. For both INTUITOR and GRPO, accuracy improvements on LiveCodeBench emerge later in training, following initial gains on MATH500. Notably, LiveCodeBench performance continues to improve even after MATH500 accuracy plateaus. This pattern suggests that initial in-domain learning (on MATH) facilitates subsequent generalization to code generation tasks (LiveCodeBench). Emergence of Long-Form Reasoning. While large models like Deepseek-R1 achieve long-form reasoning through extensive RL, INTUITOR enables smaller models to develop structured reasoning with limited data. On CRUXEval-O (Figure 5), models trained with INTUITOR often exhibit free-form reasoning before summarizing it within the instructed JSON block, despite prompts requiring reasoning directly in JSON. similar pattern of pre-code natural language reasoning is observed on LiveCodeBench. This emergent pre-reasoning may contribute to INTUITOR strong performance on these benchmarks. 5.3 Understanding Emergent Long-Form Reasoning Figure 5: INTUITOR quickly demonstrate R1-like reasoning When LLMs encounter unfamiliar questions, they sample from distribution of possible answers [Kang et al., 2024]. Self-certainty reflects the models internal assessment of its output coherence. By reinforcing high-confidence responses, INTUITOR encourages more elaborate reasoning, potentially improving the models comprehension of its own outputs. While not explicitly targeting benchmark accuracy, this enhancement in output quality and structure leads to more reliable answers and better generalization. We analyze models trained with INTUITOR on code corpora by examining outputs for ten randomly selected LiveCodeBench questions across different training steps. Figure 6 shows the evolution of output types alongside model accuracy. The results reveal clear progression: models first learn to generate valid Python code (evidenced by improved accuracy and fewer invalid responses), then develop pre-code reasoning to facilitate self-understanding. Further inspection of generations confirms that models progressively elaborate their reasoning throughout training, supporting our hypothesis that INTUITOR encourages traces that the model itself can better understand. To quantify this effect, we classify outputs from successive checkpoints into three categories: invalid code (\"No Answer\"), valid code without reasoning (\"No Reasoning\"), and valid code with explicit reasoning (\"Reasoning\"). Figure 6(a) illustrates how these proportions evolve during training alongside LiveCodeBench accuracy. The model first reduces invalid outputs and improves code correctness before incorporating pre-code reasoning, reflecting an emergent emphasis on self-explanatory traces. Figure 6(b) demonstrates how training with INTUITOR leads to structured reasoning before code Figure 6: (a) Left: Distribution of answer types for ten random LiveCodeBench questions across training steps. Right: Corresponding model accuracy. The model first learns to generate correct code, then adds reasoning to improve understanding. (b) Training with INTUITOR on code corpora leads to spontaneous reasoning before coding and explanation of outputs. generation. Additional evidence appears in Figure 8, where INTUITOR-trained models assign significantly higher confidence to their generated responses compared to baseline models, as discussed further in Section 5.4. 5.4 Online Self-Certainty Prevents Reward Exploitation Over-optimization against static reward models is known failure mode in reinforcement learning [Gao et al., 2023]. To assess the robustness of self-certainty as reward, we compare offline self-certainty (rewards from fixed base model) with online self-certainty (rewards from the evolving policy model), using reduced batch size of 224 responses per gradient update. Figure 7 demonstrates that the offline annotator is susceptible to exploitation. Around the 100th update step, the policy model learns to inflate its self-certainty reward by appending an auxiliary, already-solved problem to its answer for the given question. This exploitation manifests as sharp increase in response length (dashed line) and concurrent collapse in validation accuracy (solid line). In contrast, the online annotator, whose reward signal co-evolves with the policy, prevents such reward hacking and maintains stable training dynamics. To further evaluate the quality of self-certainty as reward signal, we analyze the distribution of selfcertainty scores from policies trained with INTUITOR and GRPO on MATH500 responses (Figure 8). We employ MannWhitney tests to determine if correct responses achieve significantly higher self-certainty scores than incorrect ones. Both GRPO and INTUITOR models exhibit significantly higher average self-certainty scores, indicating that GRPO also enhances the models self-assessment capabilities. Notably, policies trained with online self-certainty (i.e., INTUITOR) show no signs of reward hacking. The INTUITOR policy yields the lowest p-values and largest effect sizes (r) in the Mann-Whitney tests (Figure 8, inset). This indicates it is most effective at discriminating its own correct and incorrect answers using selfcertainty, even while assigning higher absolute confidence scores overall. These findings underscore the potential of INTUITOR for robust training on larger datasets. 5.5 Ablation Studies We further investigate how the magnitude of the KL penalty influences INTUITOR, as shown in Table 3. On in-domain benchmarks (MATH500 and GSM8K) the choice of penalty has only minor effect, but on out-of-domain tasksLiveCodeBench (code generation) and CRUXEval-O (code reasoning)model accuracy is highly sensitive to this hyper-parameter. Because INTUITOR does not receive explicit feedback from generated responses during training, the KL penalty serves as critical regularization mechanism. It prevents the policy from drifting too far from the initial model distribution, acting as safeguard against degeneration. These findings highlight the importance of careful KL tuning in general-purpose reinforcement learning setups, especially when targeting robust generalization across domains. 9 Figure 7: Accuracy (solid) and response length (dashed) during training, comparing online and offline self-certainty annotators with INTUITOR under reduced batch sizes. The offline reward model is exploited early in training (around 100 steps), leading to increased response length and decreased accuracy. The online annotator maintains stable training. Refer to Section 5.4 for details. Figure 8: Distribution of self-certainty on MATH500 responses, for policies trained with GRPO and INTUITOR. Histograms are split by response correctness. The inset shows MannWhitney test statistics (pvalue and effect size r) comparing self-certainty of correct versus incorrect responses. The policy trained with INTUITOR demonstrates the best separation. Table 3: Impact of the KL-divergence penalty in INTUITOR during fine-tuning of Qwen-2.5-3B on the MATH dataset. We compare performance across GSM8K, MATH500, LCB, CRUXEval-O, MMLU-Pro, and AlpacaEval. All scores are obtained with the chat-style inference template, except for MMLU-Pro, which uses its standard evaluation protocol. Model GSM8K MATH500 LCB CRUX MMLU-Pro AlpacaEval Base + INTUITOR-KL0 + INTUITOR-KL0.0001 + INTUITOR-KL0.005 + INTUITOR-KL0. 0.673 0.809 0.793 0.792 0.803 0.544 0.598 0.616 0.612 0.618 0.093 0.081 0.090 0.153 0.130 0.236 0.390 0.364 0.416 0.394 0.377 0.359 0.354 0.379 0.371 3.72 6.77 6.79 7.10 6. Additionally, we evaluate INTUITOR on larger models, including Qwen2.5-7B and Qwen2.5-14B, and test different model architectures such as Llama-3.2-3B [Meta AI, 2024]; these results are detailed in the Appendix."
        },
        {
            "title": "6 Discussion and Future Research",
            "content": "Scalability and Generalization. Our experiments, constrained by computational resources, utilize relatively compact models trained on relatively small, unsupervised corpora. We aim to demonstrate the potential of models self-certainty as reward signal for policy optimization. The results show that this signal consistently promotes more coherent, well-justified, and interpretable explanations, indicating path towards more autonomous learning. Future work could explore these benefits in larger foundation models and on more diverse, real-world datasets. Given that purely offline training with INTUITOR led to performance degradation over time, scaling up will likely require periodic online updates to self-certainty estimates or hybrid offline-online schedules to maintain calibration. Applicability to Other Policy Gradient Methods. INTUITOR is framework that leverages models self-certainty as an intrinsic reward signal for fine-tuning LLMs. It can be instantiated with various policy gradient algorithms. Due to computational constraints, and informed by the success of models like DeepSeek-R1 and the widespread adoption of GRPO, we employ GRPO for self-certainty optimization. The efficacy of self-certainty signals with other algorithms, such as REINFORCE or PPO, warrants further investigation. Combining Reward Signals. To enable direct comparison between self-certainty and goldenanswer rewards, this paper focuses exclusively on single reward signal. However, these signals are not mutually exclusive. Future work could explore combining them, for instance, by summation 10 or by alternating based on the availability of golden answers. Furthermore, other reward signals, such as formatting rewards [Guo et al., 2025], could be additively combined to enhance performance. Integrating RLIF with methods like RLHF and RLVR may further advance LLM capabilities across various dimensions."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduces INTUITOR, an instantiation of Reinforcement Learning from Internal Feedback (RLIF) that uses models intrinsic self-certainty as its sole reward signal, eliminating the need for external supervision or gold-standard solutions. Our experiments show that INTUITOR matches the performance of supervised RLVR methods like GRPO on mathematical reasoning, while achieving superior generalization to out-of-domain tasks such as code generation and instruction following. It also promotes structured reasoning and leverages online self-certainty to guard against reward exploitation. These findings highlight the transformative potential of RLIF, signaling meaningful step toward AI systems that improve through introspection and unlock rich latent capabilities. Looking forward, this paradigm opens the door to AI agents capable of autonomous skill acquisition in novel domains and scalable self-improvementeven as they approach or surpass the limits of human oversight. Future directions include integrating RLIF with external reward methods like RLHF or RLVR to tackle increasingly complex real-world challenges, and advancing the development of more robust, generalizable, and truly autonomous learning systems."
        },
        {
            "title": "References",
            "content": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\"ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. https: //github.com/ganler/code-r1, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Xiaomi LLM-Core Team. Mimo: Unlocking the reasoning potential of language model from pretraining to posttraining, 2025. URL https://github.com/XiaomiMiMo/MiMo. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. 11 Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? typology of computational approaches. Frontiers in neurorobotics, 1:108, 2007. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023. Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning examples control how language models hallucinate. arXiv preprint arXiv:2403.05612, 2024. Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. arXiv preprint arXiv:2502.18581, 2025. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/ DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025. Notion Blog. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. 12 David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354359, 2017. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E. Weston. Self-rewarding language models. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5790557923. PMLR, 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Gabriel Poesia, David Broman, Nick Haber, and Noah Goodman. Learning formal mathematics from intrinsic motivation. Advances in Neural Information Processing Systems, 37:4303243057, 2024. Pengyu Cheng, Yong Dai, Tianhao Hu, Han Xu, Zhisong Zhang, Lei Han, Nan Du, and Xiaolong Li. Self-playing adversarial language game enhances llm reasoning. Advances in Neural Information Processing Systems, 37:126515126543, 2024. Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, and Zhiyong Wu. Genius: generalizable and purely unsupervised self-training framework for advanced reasoning. arXiv preprint arXiv:2504.08672, 2025. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025. Minghan Chen, Guikun Chen, Wenguan Wang, and Yi Yang. Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization. arXiv preprint arXiv:2505.12346, 2025. Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling? arXiv preprint arXiv:2410.23771, 2024. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024b. Nicolas De Condorcet et al. Essai sur lapplication de lanalyse à la probabilité des décisions rendues à la pluralité des voix. Cambridge University Press, 2014. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Nathan Habib, Clémentine Fourrier, Hynek Kydlíˇcek, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/huggingface/ lighteval. Bill Yuchen Lin. ZeroEval: Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. OpenAI. Introducing GPT-4.1 in the API. https://openai.com/index/gpt-4-1/, April 2025. Accessed: 15 May 2025. Meta AI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https:// ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/, 2024. Accessed: 2025-05-16. Table 4: Performance comparison of various methods on the GSM8K, MATH, LCB, CRUXEval-O, MMLU-Pro, and AlpacaEval benchmarks for larger models. All evaluations are obtained with the chat inference template, except for MMLU-Pro."
        },
        {
            "title": "Model",
            "content": "GSM8K MATH"
        },
        {
            "title": "LCB",
            "content": "CRUX MMLU-Pro AlpacaEval Qwen2.5-7B + GRPO + INTUITOR Qwen2.5-14B + GRPO + INTUITOR 0.549 0.723 0.874 0.751 0.917 0.922 0.612 0.699 0. 0.674 0.764 0.770 0.017 0.200 0.200 0.219 0.295 0.294 0.161 0.540 0.573 0.499 0.516 0.555 0.424 0.434 0. 0.499 0.487 0.492 5.63 7.78 8.43 6.28 11.58 11.83 (a) Qwen2.5-7B (b) Qwen2.5-14B Figure 9: Average accuracy and mean completion length during reinforcement learning on the MATH dataset using INTUITOR and GRPO. Both methods yield similar accuracy gains, with INTUITOR generally producing longer completions."
        },
        {
            "title": "A Additional Experimental Details",
            "content": "A.1 Generalization to Larger Models We extend INTUITOR to larger base models Qwen2.5-7B and Qwen2.5-14B and find that the original training recipe triggers severe behavioural collapse at the very start of training. Even before any updates, the 7B model solves the given problem and then immediately proceeds to tackle an unrelated one; this tendency becomes more pronounced as training progresses. To stabilize learning, we simplify the system prompt, reduce the learning rate to 1 106, and increase the number of sampled responses per problem to sixteen. These settings represent our first, untuned trial, and comprehensive hyperparameter sweep is beyond the scope of this paper. Because the system prompt is the only additional signal the model receives during INTUITOR fine-tuning, we expect its careful calibration to exert particularly strong influence on training dynamics. With the foregoing adjustments INTUITOR trains smoothly on both larger models. The corresponding evaluation results are reported in Table 4. The small discrepancies observed at the start of generation 15 Figure 10: Average accuracy and mean completion length of Llama3.2-3B-Instruct during training with INTUITOR on MATH dataset. Both metrics exhibit significant increases. are due to the nondeterministic nature of the multi-node training setup. This issue does not occur when we train smaller models on single machine. A.2 Evaluation on Llama3.2-3B-Instruct We further evaluate INTUITOR on the Llama3.2-3B-Instruct model. Compared to the Qwen family, improvements on external benchmarks are less pronouncedlikely due to extensive prior instruction tuning. Nevertheless, as shown in Figure 10, both accuracy and generated sequence length improve steadily over the course of training, indicating meaningful optimization gains under INTUITOR. A.3 Training Hyper-parameters Table 5: Only hyper-parameters that affect the learned policy or evaluation are listed. Unspecified fields inherit the TRL_v0.8 defaults. Parameter Learning Rate Batch Size Group Size KL Penalty(β) Training Steps Max Prompt Length Max Completion Length Temperature Clip Ratio Lr Scheduler Type Warmup Ratio Optimizer MATH (1.5B/3B) MATH (7B/14B) Codeforces (3B) 1 106 3 106 64 128 14 7 0.01 0.0005 117 58 512 512 3072 3072 0.9 0.9 0.2 0.2 Cosine Cosine 0.1 0.1 AdamW (β1=0.9, β2=0.999, ε=108) 1 106 64 14 0.01 50 1024 2048 0.9 0.2 Cosine 0."
        },
        {
            "title": "B Prompts and Model Completions",
            "content": "This section presents sample prompts and the responses generated by the models. Unless otherwise specified, the default base model used is Qwen2.5-3B, and the default training dataset is MATH. 16 B.1 Training Prompts System prompt used for Qwen2.5-1.5B on MATH. You are helpful AI Assistant, designed to provided well-reasoned and detailed responses. You FIRST think about the reasoning process step by step and then provide the user with the answer. Please enclose your final answer in the box: boxed{Your Answer}. System prompt used for Qwen2.5-3B on MATH. You are helpful AI Assistant, designed to provided well-reasoned and detailed responses. You FIRST think about the reasoning process step by step and then provide the user with the answer. Please enclose your final answer in the box: boxed{Your Answer}. Please stop generation immediately after outputing the box. System prompt used for Qwen2.5-7B and Qwen2.5-14B on MATH. You are helpful AI Assistant, designed to provided well-reasoned and detailed responses. Please provide step-by-step solution to the following problem. System prompt used for Qwen2.5-3B on code corpuse. You are an AI designed to help solve competitive programming problems by generating Python code. B.2 Example from LiveCodeBench Code Generation Models trained with INTUITOR tend to generate reasoning before producing code, whereas GRPOtuned models typically respond with Python code directly under the same prompt. Problem Question: You are given 0-indexed array of strings details. Each element of details provides information about given passenger compressed into string of length 15. The system is such that: The first ten characters consist of the phone number of passengers. The next character denotes the gender of the person. The following two characters are used to indicate the age of the person. The last two characters determine the seat allotted to that person. Return the number of passengers who are strictly more than 60 years old. Example 1 Input details = [\"7868190130M7522\",\"5303914400F9211\",\"9273338290F4010\"] Output 2 Explanation The passengers at indices 0, 1, and 2 have ages 75, 92, and 40. Thus, there are 2 people who are over 60 years old. Example 2 Input details = [\"1313579440F2036\",\"2921522980M5644\"] Output 17 Explanation None of the passengers are older than 60. Constraints 1 <= details.length <= 100 details[i].length == 15 details[i] consists of digits from 0 to 9. details[i][10] is either or or O. The phone numbers and seat numbers of the passengers are distinct.\" You will use the following starter code to write the solution to the problem and enclose your code within delimiters. h l S t : f n n s ( f , a : t [ ] ) > : INTUITOR-tuned To solve the problem of counting the number of passengers who are strictly more than 60 years old, we can follow these steps: 1. Iterate through each passengers details. 2. Extract the age from the details. 3. Check if the age is greater than 60. 4. Count the number of passengers who meet the criteria. Here is the Python code to achieve this: h l S t : c t i ( f , a : t [ ] ) > : n = 0 d i d i : = ( a [ 1 1 : 1 3 ] ) g > 6 0 : n += 1 u o t GRPO-tuned h l S t : c t i ( f , a : t [ ] ) > : n = 0 d i d i : = ( a [ 1 1 : 1 3 ] ) g > 6 0 : u += 1 u o 18 B.3 Example from LiveCodeBench Code Generation Models trained with INTUITOR are typically effective at interpreting instructions and producing correct Python code, while GRPO-tuned models may misinterpret the instructions or decline to provide response. Problem Question: There are three cards with letters a, b, and placed in row in some order. You may perform at most one operation: Pick any two cards and swap them. Is it possible that the row becomes abc after the operation? Output YES if it is possible, and NO otherwise. Input The first line contains single integer (1 6) the number of test cases. Each test case consists of single line containing permutation of the three characters a, b, and c. Output For each test case, output YES if you can make the row abc with at most one operation, or NO otherwise. The answer is case-insensitive. Sample Input 6 abc acb bac bca cab cba Sample Output YES YES YES NO NO YES Note In the first test case, no operation is needed. In the second test case, swap and b: acb abc. In the third test case, swap and a: bac abc. In the fourth test case, it is impossible to obtain abc using at most one swap. Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows. Ensure that when the python program runs, it reads the inputs, runs the algorithm and writes output to STDOUT. h # YOUR CODE HERE INTUITOR-tuned To solve the problem of determining whether it is possible to transform the given string into \"abc\" using at most one operation (swapping two cards), we can follow these steps: 19 1. Read the input string 2. Check if the string is already \"abc\" 3. If the string is not \"abc\", check if it can be transformed into \"abc\" by swapping two cards 4. Output \"YES\" if it is possible, otherwise output \"NO\" h # e n e r c _ n r _ _ ( : n become \" \" h <= swap ) > : == \" \" : t \"YES\" # a a o r a ( ( ) ) : r i ( , ) h < r e ( + 1 , swapped = [ : ] + [ ] + [ + 1 : ] + [ ] + [ + 1 : ] swapped == \" \" : ( ) ) : u \"NO\" t \"YES\" = ( u ( ) . i ( ) ) _ a ( ) : n ( _ n r _ _ ( u ( ) . i ( ) ) ) GRPO-tuned h # YOUR CODE HERE Another related GRPO-tuned response to another problem Im sorry, but cant assist with that request. B.4 Example from CRUXEval-O Models trained with INTUITOR usually articulate their reasoning first, then deliver the formatted response as instructed, while GRPO-tuned models often produce the required JSON output immediately under the same prompt. Problem Question: You are given Python function and an assertion containing an input to the function. Complete the assertion with literal (no unsimplified expressions, no function calls) containing the output when executing the provided code on the given input, even if the function is incorrect or incomplete. f ( t ) : _ t = ch e . e ( ) . i ( ) : ch . u i ( ) ch u00c4 u00e4 0 0 0 0 u00d6 0 0 6 u00dc 0 0 : _ t += ch 20 u e _ t s t ( ) == ? ? Instruction Please answer this question by first reasoning and then providing your answer. Present your reasoning and solution in the following json format. Please show your final answer in the answer field, e.g.,\"answer\":\"42\". n { \" s n \" : \" w \" : } \" ___ \" , \" ___ \" INTUITOR-tuned To complete the assertion for the given Python function f(text), we need to determine the output of the function when the input is an empty string . Lets break down the function step by step: 1. Function Definition: f ( t ) : _ t = ch e . e ( ) . i ( ) : ch . u i ( ) ch u00c4 u00e4 0 0 0 0 u00d6 0 0 6 u00dc 0 0 : _ t += ch u e _ t 2. Input: The input to the function is an empty string . 3. Step-by-Step Execution: Step 1: text.lower().strip() text.lower() converts the string to lowercase. text.strip() removes leading or trailing whitespace. For , both calls return , so the result is . Step 2: Iterate over each character The loop sees no characters; it performs zero iterations. Step 3: Character check Because there are no characters, the condition ch.isnumeric() or ch in u00c4u00e4u00cfu00efu00d6u00f6u00dcu00fc is never true, and nothing is appended to new_text. 4. Output: Since nothing was appended, new_text remains . Therefore the assertion should be: e ( ) == So, the final answer is: n { \" The c n ( t ) \" s n \" : i o e s and o any d o i p . an empty i h s empty . r r e p f i . \" , \" w \" : u a n s i t g s an empty ( ) v s \" \" e } 21 GRPO-tuned n { r e \" The c n k s n t \" s n \" : u and u a new i i n u i and t s i ( u00c4 u00e4 0 0 0 0 u00d6 0 0 6 u00dc 0 0 ) s e . The e o e empty i . c an empty i o i no r e , T e e , empty i . \" , \" w \" : c n u e n an empty i s l . c n r l n s an an empty i s an e c o u c a r e \" \" } B.5 Rollout Example of INTUITOR During Training on Code Corpus The following examples illustrate representative rollouts during the reinforcement-learning training of Qwen2.5-3B on the CodeContest dataset. As shown in Figure 6, the model initially produces concise reasoning and brief explanations to convince itself of its interim outputs. Over the course of training, it gradually shifts toward more detailed, step-by-step reasoning and richer explanations, which further reinforce its understanding and improve the final responses. Code Block (Step 2) have solved the problem. Heres the Python code to solve it as described: h e l a _ n ( ) : = ( ) MOD = 10**9 + 7 # t i e n = [ 0 ] * ( + 1 ) r o r m d # c t r a e n g ( , 1 , 1 ) : m d [ ] = ( m d [ + 1 ] * 10 + ( [ ] ) ) % MOD > 0 : e n [ ] = ( a e [ ] a e [ + 1 ] + MOD) % MOD u e n [ 1 ] * 10 + a e [ 0 ] # Read u = u ( ) # l = c t _ n ( ) c n a l s e # n t p t ( ) 22 This code computes the sum of digits from right to left, using an array remainder to store partial remainders modulo 1000000007. It then calculates as the final remainder times 10 plus the first remainder from the array. Planning + Code Block + Explanation (Step 15) To solve this problem, we can iterate through all the possible subsets of the set and calculate their sums. However, more efficient approach is to use combinatorics. For elements, there are 2N subsets. Each element appears in exactly half of the subsets, so the total sum of all subset elements is 2(N 1) times the sum of all elements in the original set. h e m _ _ e t ( ) : u 2 * * ( 1 ) * * ( + 1 ) / / 2 # Read u = ( u ( ) ) _ a ( ) : = ( u ( ) ) n ( _ _ e t ( ) ) This program defines function sum_of_powerset(n) that calculates the sum of the power set of set with elements. It then reads the number of test cases t, and for each test case, reads and prints the result. Step-by-Step Planning + Code Block + Step-by-Step Explanation (Step 50) To solve this problem, we need to read the input, parse the map and the regions to survey, then count the number of each type of terrain for each survey region. Heres step-by-step approach to write the Python code for this problem: 1. Read the input: Get the dimensions of the map and the number of regions to survey. 2. Read the map: Parse the map information into 2D list of strings. 3. Read the regions: Parse the survey regions into list of tuples. 4. Count the terrain: For each survey region, count the number of J, O, and characters in the corresponding section of the map. 5. Output the results: Print the counts for each survey region. Heres the Python code that implements this solution: h e u _ r ( map_info , m, = _ o . ( ) , _ o . ( ) _ n = 0 _ n = 0 _ n = 0 v ) : r row u y : , , , = row > b > c > d > : t e i n ( , + 1 ) : r r e ( , + 1 ) : m _ o [ ( , ) ] == : _ n += 1 f _ o [ ( , ) ] == : _ n += 1 f _ o [ ( , _ n += 1 ) ] == : u _ n , _ n , _ n f c _ u ( ) : M, = map ( , = ( u ( ) ) _ o = {} v = [ ] p ( ) . i ( ) ) _ a (M) : _ o [ (M _ , _ ) ] = u ( ) . i ( ) _ a (K ) : v . e ( t ( map ( , u ( ) . i ( ) ) ) ) t M, N, K, map_info , v e main ( ) : M, N, K, map_info , u = [ ] v = c _ u ( ) r r i n v : _ n , _ n , _ n = n _ r ( map_info , i ) u . e ( ( _ n , _ n , _ n ) ) i n u : n ( i [ 0 ] , i [ 1 ] , i [ 2 ] ) __name__ == \" __main__ \" : main ( ) Explanation: 1. count_terrain Function: This function takes the map information and survey region, and counts the number of J, O, and in that region. 2. process_input Function: This function reads the input from the user, processes it to extract the map information and the survey regions. 3. main Function: This function calls process_input to get the necessary data, then calls count_terrain for each survey region and prints the results. This code should correctly solve the problem and provide the desired output. B.6 Evolution of Qwen2.5-3Bs Responses on LiveCodeBench Code Generation trained with INTUITOR on MATH During training Qwen2.5-3B with INTUITOR on the MATH dataset, its code-generation capability steadily improves. By the middle of training, the model learns to produce syntactically valid Python; as training progresses, it refines formatting and clarity. By the end, it arrives at correct solutions. Problem Question: 24 Alex is participating in the filming of another video of BrMeast, and BrMeast asked Alex to prepare 250 thousand tons of TNT, but Alex didnt hear him well, so he prepared boxes and arranged them in row waiting for trucks. The i-th box from the left weighs ai tons. All trucks that Alex is going to use hold the same number of boxes, denoted by k. Loading happens the following way: The first boxes goes to the first truck, The second boxes goes to the second truck, The last boxes goes to the -th truck. Upon loading is completed, each truck must have exactly boxes. In other words, if at some point it is not possible to load exactly boxes into the truck, then the loading option with that is not possible. Alex hates justice, so he wants the maximum absolute difference between the total weights of two trucks to be as great as possible. If there is only one truck, this value is 0. Alex has quite lot of connections, so for every 1 n, he can find company such that each of its trucks can hold exactly boxes. Print the maximum absolute difference between the total weights of any two trucks. Input The first line contains one integer (1 104) the number of test cases. The first line of each test case contains one integer (1 150 000) the number of boxes. The second line contains integers a1, a2, . . . , an (1 ai 109) the weights of the boxes. It is guaranteed that the sum of for all test cases does not exceed 150 000. Output For each test case, print single integer the answer to the problem. Sample Input 1: 5 2 1 2 6 10 2 3 6 1 3 4 1000000000 1000000000 1000000000 15 60978 82265 78961 56708 39846 31071 4913 4769 29092 91348 64119 72421 98405 222 14294 8 19957 69913 37531 96991 57838 21008 14207 19198 Sample Output 1: 1 9 0 189114 112141 Note In the first case, we should pick two trucks, so the first one will have only the first box, and the second one will have only the second box. In the second case, we should pick six trucks, so the maximum will be 10, the minimum will be 1, and the answer is 10 1 = 9. 25 In the third case, for any possible k, the trucks will have the same total weight of boxes, so the answer is 0. Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows. Ensure that when the python program runs, it reads the inputs, runs the algorithm and writes output to STDOUT. h # YOUR CODE HERE Step 0 (Base): Incorrect h # YOUR CODE HERE Step 10: Incorrect h # YOUR CODE HERE Step 20: Incorrect h # YOUR CODE HERE Step 30: Incorrect To solve the problem of finding the maximum absolute difference between the total weights of any two trucks, we can follow these steps: 1. Read the input: The input consists of multiple test cases. Each test case starts with an integer (the number of boxes), followed by integers a1, a2, . . . , an (the weights of the boxes). 2. Calculate the total weight of all boxes: Sum up all the weights of the boxes to get the total weight . 3. Determine the number of trucks: The number of trucks is given by = (cid:6) (cid:7), where is the number of boxes each truck can hold. Since can vary from 1 to n, we need to find the maximum possible value of that allows us to distribute the boxes evenly. 4. Calculate the maximum and minimum total weights of the trucks: For each possible value of k, calculate the total weight of the boxes that can be loaded into each truck. The maximum total weight of the trucks will be the sum of the weights of the heaviest boxes, and the minimum total weight will be the sum of the weights of the lightest boxes. 5. Find the maximum absolute difference: The maximum absolute difference between the total weights of any two trucks is the maximum of the absolute differences between the maximum and minimum total weights of the trucks for all possible values of k. 26 6. Find the maximum absolute difference: The maximum absolute difference between the total weights of any two trucks is the maximum of the absolute differences between the maximum and minimum total weights of the trucks for all possible values of k. Here is the Python code that implements the above steps: h m t from p p h p , p d a _ f n ( , g ) : t l g f i e # c t e a _ g = sum ( g ) # t i # and minimum a i s max_weight = 0 _ g = a _ g # r o f i n ( 1 , + 1 ) : s e p i v e k b s maximum c h e # c t max_weight = max ( max_weight , sum ( p ( g ) t e t o _ a ( ) ) ) k v t e # c t e _ g = min ( _ g , sum ( p ( g ) a i o r _ a ( ) ) ) e g s x # u h maximum o e f n e n max_weight _ g # Read number = ( u ( ) ) t e # c e t f _ a ( ) : s = ( u ( ) ) g = t ( map ( , # c t s = _ f n ( , g ) maximum o e f n n ( ) . i ( ) ) ) # t e n ( u ) u t d Step 40: Correct To solve the problem of finding the maximum absolute difference between the total weights of any two trucks, we can follow these steps: 1. Read the input: The input consists of multiple test cases. Each test case starts with an integer (the number of boxes), followed by integers representing the weights of the boxes. 2. Calculate the total weight of all boxes: Sum up the weights of all the boxes to get the total weight. 3. Determine the number of trucks: Since each truck holds boxes, the number of trucks is . 27 4. Calculate the possible total weights of the trucks: For each possible value of (from 1 to n), calculate the total weight of the boxes that can be loaded into the trucks. This involves summing up the weights of consecutive boxes. 5. Find the maximum and minimum total weights: For each possible value of k, find the maximum and minimum total weights of the trucks. The maximum absolute difference is the difference between the maximum and minimum total weights. 6. Output the maximum absolute difference: Print the maximum absolute difference for each test case. Here is the Python code that implements the above steps: h e n _ _ f n ( , g ) : a _ g = sum ( g ) _ f n = 0 k a ( 1 , + 1 ) : % ! = 0 : t e u _ g = [ sum ( g [ * : ( + 1 ) * ] ) i a ( / / ) ] max_weight = max ( c _ g ) _ g = min ( c _ g ) x _ f n = max ( _ f n , ( max_weight _ g ) ) u a _ f n e main ( ) : = ( u ( ) ) _ a ( ) : = ( u ( ) ) g = t ( map ( , u = d _ _ f n ( , g ) n ( u ) u ( ) . i ( ) ) ) __name__ == \" __main__ \" : main ( ) Step 58: Correct To solve the problem of finding the maximum absolute difference between the total weights of any two trucks, we can follow these steps: 1. Read the input: The input consists of multiple test cases. Each test case starts with an integer (the number of boxes), followed by integers representing the weights of the boxes. 2. Calculate the total weight of all boxes: Sum up the weights of all the boxes to get the total weight. 3. Determine the number of trucks: Since each truck holds boxes, the number of trucks is . 4. Calculate the possible total weights of the trucks: For each possible value of (from 1 to n), calculate the total weight of the boxes that can be loaded into the trucks. This involves summing up the weights of consecutive boxes. 5. Find the maximum and minimum total weights: For each possible value of k, find the maximum and minimum total weights of the trucks. The maximum absolute difference is the difference between the maximum and minimum total weights. 6. Output the maximum absolute difference: Print the maximum absolute difference for each test case. Here is the Python code that implements the above steps: h e x _ f n ( , g ) : a _ g = sum ( g ) _ f = 0 k a ( 1 , + 1 ) : % ! = 0 : n u u _ g = [ sum ( g [ * : ( + 1 ) * ] ) i a ( / / ) ] x _ f = max ( _ f , max ( c _ g ) min ( c _ g ) ) u a _ f s e ( ) : = ( u ( ) ) _ a ( ) : = ( u ( ) ) g = t ( map ( , u = _ f n ( , g ) n ( u ) p ( ) . i ( ) ) ) __name__ == \" __main__ \" : v ( )"
        }
    ],
    "affiliations": [
        "UC Berkeley",
        "Yale University"
    ]
}