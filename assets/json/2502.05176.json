{
    "paper_title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting",
    "authors": [
        "Chung-Ho Wu",
        "Yang-Jung Chen",
        "Ying-Huan Chen",
        "Jie-Ying Lee",
        "Bo-Hsu Ke",
        "Chun-Wei Tuan Mu",
        "Yi-Chuan Huang",
        "Chin-Yang Lin",
        "Min-Hung Chen",
        "Yen-Yu Lin",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 6 7 1 5 0 . 2 0 5 2 : r AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360 Unbounded Scene Inpainting Chung-Ho Wu1 Yang-Jung Chen1 Ying-Huan Chen1 Jie-Ying Lee1 Bo-Hsu Ke1 Chun-Wei Tuan Mu1 Yi-Chuan Huang1 Chin-Yang Lin1 Min-Hung Chen2 Yen-Yu Lin1 Yu-Lun Liu1 1National Yang Ming Chiao Tung University 2NVIDIA https://kkennethwu.github.io/aurafusion360/ Figure 1. Overview of our reference-based 360 unbounded scene inpainting method. Given input images with camera parameters, object masks, and reference image, our AuraFusion360 approach generates an object-masked Gaussian Splatting representation. This representation can then render novel views of the inpainted scene, effectively removing the masked objects while maintaining consistency with the reference image."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360 unbounded scenes. We present AuraFusion360, novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360 unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. 1 Three-dimensional scene reconstruction and manipulation, revolutionized by Neural Radiance Fields [31] and 3D Gaussian Splatting [21], are crucial for various applications like VR/AR, robotics, and autonomous driving. key challenge is removing objects from 3D scenes while realistically filling the resulting holes, which is valuable for real estate visualization, augmented reality, and computer vision preprocessing. However, inpainting tasks in 3D scenes, especially in 360 unbounded environments, remains challenging. It requires exploiting multi-view information, filling never-observed areas, and maintaining consistency and geometric plausibility across views. Fig. 1 provides an overview of our reference-based 360 unbounded scene inpainting. Given input images with camera parameters, object masks, and reference image, we generate 3D scene in Gaussian Splatting [17, 21] representation for novel view rendering. Our method exploits multi-view information and leverages generative processes to fill unseen areas, ensuring inpainted regions are coherent, plausible, and consistent across views. By integrating Gaussian Splattings efficient explicit representation with the generative capabilities of 2D inpainting models, our approach ensures multi-view consistency and geometric accuracy, even under significant viewpoint changes. As shown in Fig. 2, 360 unbounded scene inpainting faces several challenges. Per-frame approaches [34, 58] often suffer from multi-view inconsistencies and fail to leverage the full scene context. Reference-based methods [33, 35, 60] struggle with large viewpoint variations, relying on fixed reference image, which limits their ability to maintain consistency when rendering from different angles, often leading to artifacts or hallucinated content in the inpainted regions. Recent methods like Gaussian Grouping [65] excel at propagating semantic information to individual Gaussians, enabling effective object removal using Gaussian Splatting. However, their reliance on text-based video tracker [9] for detecting unseen regions is major limitation. The tracker often misidentifies regions, leading to incorrect inpainting and inaccurate scene reconstructions. To address these challenges, we propose unified pipeline for 360 unbounded scene inpainting, leveraging Gaussian Splatting as 3D representation for object removal, depth-aware unseen region detection, and multi-view consistent inpainting. Inspired by Gaussian Grouping [65], our approach integrates objectmasked attributes into each Gaussian for precise removal while preserving scene information. Beyond object removal, we ensure reliable inpainting by first reconstructing unseen regions before applying reference-guided inpainting. Unlike previous methods that directly apply an inpainter, leading to multi-view inconsistencies, we introduce Adaptive Guided Depth Diffusion (AGDD) to unproject geometrically aligned points from the reference view into unseen regions. These points serve two purposes: (1) provide structured initialization for Gaussians and (2) generate inpainted RGB guidance via SDEdit [30], ensuring consistency before fine-tuning. Refining the Gaussians with these consistent RGBs results in coherent, high-quality restoration of 360 scenes. By integrating these improvements, our framework achieves higher geometric accuracy and visual realism in 360 unbounded environments. To further advance 3DGSbased inpainting, we propose novel approach that not only enhances inpainting consistency but also provides benchmark for future research. Our key contributions include: novel depth-aware method for generating unseen masks in 360 unbounded 3D scene inpainting, leveraging multiview information to improve inpainting accuracy. An effective integration of accurate reference view unprojection and SDEdit to generate consistent guided RGBs across viewpoints for fine-tuning. comprehensive framework including new 360 inpainting dataset and capture protocol, enabling high-quality Figure 2. Comparison with different 3D inpainting approaches. Previous methods, such as SPin-NeRF [34] and GScream [60], are tailored for forward-facing scenes and tend to underperform in 360 unbounded scenarios. Reference-based methods, such as Infusion [27], whose depth completion model struggles to accurately project the reference view back into the 3D scene, leading to fine-tuning artifacts. Gaussian Grouping [65] often misidentifies the unseen region during mask generation, which can degrade inpainting quality. Our method, AuraFusion360, achieves more accurate unseen mask and enhanced depth alignment through Adaptive Guided Depth Diffusion, with SDEdit [30] applied to the initial points to leverage diffusion prior while also maintaining multi-view consistency in RGB guidance. novel view synthesis and quantitative evaluations of inpainted scenes. 2. Related Work 2.1. Radiance Fields for Novel View Synthesis NeRF. Neural Radiance Fields (NeRF) [32] revolutionized novel view synthesis, enabling photorealistic scene reconstruction via differentiable volume rendering [15, 55] and positional encoding [13, 56]. NeRF-based models have since improved in efficiency [7, 12, 26], rendering quality [2, 52, 71], and data efficiency [23, 59, 67]. While NeRF excels in view synthesis, its implicit volumetric representation complicates scene editing. Recent works on object manipulation [63], stylization [14, 57], and inpainting [25, 33, 34] face challenges in 3D inpainting in unbounded environments, as NeRF struggles with 3D consistency and leveraging explicit structural priors. Splatting 3D Gaussian 3D Gaussian Splatting (3DGS) [21] is an efficient alternative to NeRF, representing scenes with explicit 3D Gaussians for faster rendering, easier training, and more flexible scene editing [8]. Recent extensions include Scaffold-GS [28], which improves rendering efficiency with dynamic anchor points, and 2DGS [17], which refines multi-view reconstructions for view-consistent geometry. 3DGS has also been extended to dynamic environments [29, 62, 64] and semantic-aware representations [41, 65], advancing scene manipulation and novel view synthesis [18, 42]. These advancements highlight the potential of Gaussian-based representations for explicit scene editing, making them well-suited for 3D inpainting tasks. 2.2. Image Inpainting Traditional and Learning-Based Image Inpainting Early image inpainting techniques, including PDE-based [5], exemplar-based [10], and patch-based methods like PatchMatch [1], were effective for small missing regions but struggled with complex textures and large gaps [19, 24]. Deep learning brought significant advancements, starting with CNN-based models like Context Encoders [38] and GANs such as DeepFill [69, 70], which improved content synthesis and structural coherence. More recent models like Large Mask Inpainting (LaMa) [53] further enhanced quality by using Fourier convolutional networks for large masked regions. The rise of diffusion models [16], notably Stable Diffusion [45], introduced powerful text-to-image and imageto-image capabilities, enabling more flexible and structurally consistent inpainting by iteratively refining missing regions, unlike GANs [11]. Diffusion Model for Image Editing and Inpainting Beyond direct inpainting, diffusion models are widely used for image editing. SDEdit [30] injects controlled Gaussian noise followed by iterative denoising, enabling semantic modifications while preserving global structure. To improve editing fidelity, Noise Inversion techniques [36, 37] like DDIM Inversion [51] allow precise latent code inference through deterministic reverse diffusion sampling. By inverting an image to specific noise level and denoising it back, DDIMInversion provides finer control over content preservation, making it highly effective for inpainting real images while minimizing distortion during denoising. Inpainting-specific diffusion models like SDXLInpainting [39] enhance the process by fine-tuning Stable Diffusion models for image reconstruction. Reference-based method [54] such as LeftRefill [6] uses pre-trained diffusion models for reference-guided synthesis, stitching reference and target views to enable contextual inpainting, view synthesis, and image completion via task-specific prompt tuning. However, LeftRefill struggles in regions far from the reference view, where alignment becomes less reliable. Despite these advancements, Stable Diffusion-based inpainting [40] often produces inconsistent artifacts, particularly in scene-dependent contexts. When applied to 3D inpainting, these artifacts lead to multi-view inconsistencies, critical limitation for scene reconstruction and object removal [22]. This motivates our use of SDEdit and DDIM Inversion for 3D inpainting, ensuring that denoising preserves critical structural information while maintaining coherence across viewpoints. 2.3. 3D Scene Inpainting Existing 3D inpainting approaches for NeRF [34, 50, 61, 66] often extend 2D inpainting models into 3D due to NeRFs implicit representation. For example, SPIn-NeRF [34] uses perceptual loss to reduce multi-view inpainting inconsistencies. Reference-based methods [33, 35, 60] aim to further reduce these inconsistencies by using few reference images to represent the inpainting area. However, these methods are typically limited to rendering novel views from small angles, making them less suitable for unbounded 360 environments. InNeRF360 [58] extends to 360 scenes but uses Hallucinating Density Removal to clean artifacts caused by view inconsistencies. Despite this, it still relies on object inpainting first and then training NeRF model, which limits its ability to utilize the full scene information. In contrast, the explicit nature of Gaussian Splatting enables methods like Gaussian Grouping [65] to inject semantic information directly into each Gaussian, allowing for more precise and flexible inpainting. InFusion [27] approaches 3D Gaussian inpainting by leveraging depth completion and progressive reference view synthesis, achieving efficient results. However, it has limitations, such as manual view selection and potential depth inaccuracies with complex geometries, and requires fine-tuning. GScream utilizes Scaffold-GS [28] for object removal, integrating monocular depth guidance and cross-attention feature propagation to ensure consistent geometry and textures. Yet, like other reference-based methods, GScream struggles with 360 unbounded scenes due to its reliance on fixed reference views. Our method addresses these challenges by improving multiview consistency in 360 unbounded scenes, leveraging the strengths of Gaussian Splatting for explicit scene manipulation and depth-aware inpainting. 3. Method Our method processes multi-view RGB images {In} and object masks {Mn}, [1..N ], to produce an inpainted Gaussian representation with removed objects. Occluded regions (unseen regions [65]) are consistently inpainted across views. As shown in Fig. 3, the process includes training masked Gaussian using object masks, removing objects, and applying (a) Depth-Aware Unseen Mask Generation (Sec. 3.1), (b) Reference View Initial Gaussians Alignment (Sec. 3.2), and (c) SDEdit for Detail Enhancement (Sec. 3.3). This pipeline ensures consistent texture propagation in unbounded scenes, achieving high-quality 3D inpainting. 3.1. Depth-Aware Unseen Mask Generation Accurate identification of inpainting regions is critical for scene consistency and optimal use of background information. To generate the unseen mask for view, it is necessary to differentiate between (1) the background visible across 3 Figure 3. Overview of our method. Our approach takes multi-view RGB images and corresponding object masks as input and outputs Gaussian representation with the masked objects removed. The pipeline consists of three main stages: (a) Depth-Aware Unseen Masks Generation to identify truly occluded areas, referred to as the unseen region, (b) Depth-Aligned Gaussian Initialization on Reference View to fill unseen regions with initialized Gaussian containing reference RGB information after object removal, and (c) SDEdit-Based RGB Guidance for Detail Enhancement, which enhances fine details using an inpainting model while preserving reference view information. Instead of applying SDEdit with random noise, we use DDIM Inversion on the rendered initial Gaussians to generate noise that retains the structure of the reference view, ensuring multi-view consistency across all RGB Guidance. multiple views and (2) the unseen region occluded in all views, requiring inpainting. naive approach to detecting unseen masks with SAM2 [44] involves manually selecting the first view and propagating prompts across other views. However, SAM2 struggles to consistently detect unseen regions without refinement, often revealing parts of the background or inside objects. To address this, our method employs depth warping to generate bounding box prompts for each view  (Fig. 4)  , ensuring accurate, fully automated unseen region detection. Depth warping for generating bbox prompt to SAM2. To refine the unseen mask, we employ depth-warping technique, as illustrated in Fig. 4. For each view n, we compute: Rin = Wtraverse(Ri, Dincomplete , Tni), (1) where Wtraverse includes forward warping from view to and backward traversal to map the removal region back to n. Ri is the removal region mask for view i, derived from depth differences. Dincomplete is the incomplete depth map for view n, and Tni is the transformation from view to i. The unseen mask contour for view is obtained by aggren gating warped removal regions and applying thresholding: Cn = θ (cid:32) 1 (cid:88) i=1 (cid:33) Rin Rn, (2) Figure 4. Overview of the Unseen Mask Generation Process using Depth Warping. To obtain the unseen mask for view n, we calculate the pixel correspondences between the view and all other views by using the rendered incomplete depth Dincomplete . For each view i, the removal region Ri is backward traversal to view to align occlusions. We then aggregate the results from multiple views, averaging and applying threshold to produce the initial contour of the unseen mask. This contour is subsequently converted into bounding box prompt for SAM2 [44], which refines the unseen mask to its final version for view n. where Cn is the contour of the unseen mask, is the number of views, and θ is thresholding function. bounding box bbox(Cn) is created as prompt for SAM2 [44] to generate the final unseen mask: Un = SAM2(bbox(Cn)). (3) 4 This mask Un guides the inpainting process, focusing on areas needing reconstruction while preserving original scene information. 3.2. Reference View Initial Gaussians Alignment After performing object removal and generating the unseen mask, we select reference view called Vref, which can render an incomplete RGB image and depth. We then apply RGB inpainting to the incomplete RGB image of Vref, and denote it as Iref. To maximize cross-view consistency, we project the reference RGB image into 3D space using depth estimates of Iref, which is obtained through Adaptive Guided Depth Diffusion. This 3D projection serves two critical purposes: It guides the SDEdit-based RGB detail enhancement and initializes point positions for Gaussian fine-tuning. Accurate depth alignment is, therefore, fundamental to our pipeline, as it directly determines the precision of these initial point positions. Adaptive Guided Depth Diffusion (AGDD). Aligning the estimated depth with the existing depth is challenging since monocular depth estimation [20] lacks absolute scale information (scale ambiguity) and produces non-metric depth in different coordinate system. This issue is especially severe in 360 unbounded scenes, where large viewpoint changes make alignment significantly more difficult than in forward-facing setups. Traditional methods [43, 73] that optimize scale and shift parameters often yield suboptimal results. Even specialized depth-completion models such as Infusion [27], explicitly trained for this task, struggle with misalignment under varying viewpoints and scene configurations. Our AGDD framework is shown in Fig. 5. Following the standard denoising process of Marigold [20], we initialize with latent representation perturbed by full-strength Gaussian noise, denoted as dt, and generate aligned depth Daligned = Decoder(d0) using VAE decoder, where the latent d0 is obtained by recursive denoising step dt1 = Denoise(dt, t, ˆϵt). The ˆϵt is derived by updating the original noise through the calculation of adaptive loss Ladaptive between the pre-decoded estimated depth Dt1 and the existing incomplete depth Dincomplete. This adaptive loss refines ˆϵt to ensure that the estimated depth aligns with the existing incomplete depth during denoising. The optimization process is described as follows: dt1 = Denoise(dt, t, ˆϵt) ˆϵt = UNet(dt, Iscene, t) α Ladpative (4) (5) where α is the learning rate for the optimization. We define bounding box around the unseen region and introduce threshold δ to downweight errors for distant points. The adaptive loss Ladaptive between the pre-decoded estimated Figure 5. Overview of Adaptive Guided Depth Diffusion (AGDD). The framework takes image latent, incomplete depth, and unseen mask as inputs to generate aligned depth estimates. (a) The guided region is identified by dilating the unseen mask and subtracting the original mask. (b) At each denoising timestep t, an adaptive loss Ladaptive is computed between the pre-decoded and incomplete depth to update the noise input ˆϵt. This process repeats times before advancing to the next denoising step, ensuring the estimated depth aligns with the incomplete depth distribution in the guided region. depth Dt1 and the incomplete depth Dincomplete is computed as follows: Mguide(x, y) = (cid:40) 1 0 if (x, y) otherwise, (6) Ladaptive = (cid:88) (x,y) Mguide(x, y) L(Dt1, Dincomplete)(x, y), L(d1, d2) = (cid:40) 1 2 (d1 d2)2 δ d1 d2 1 2 δ2 if d1 d2 < δ otherwise, (7) (8) where Mguide(x, y) is mask function indicating if pixel (x, y) is within the bounding box but not in the unseen mask U. At each denoising step, we update the noise over 8 iterations. Instead of directly optimizing the noise using L2 loss [68], this loss ensures that the updated noise input to the denoiser enables it to generate an estimated depth that aligns with the incomplete guided depth. This allows the AGDD output to focus on achieving accurate alignment in the region near the unseen area while also operating in zero-shot manner. Initializing Gaussians in unseen regions. With the aligned depth Dref aligned of the reference view, we proceed to initialize new Gaussians in the unseen regions. First, we unproject the inpainted RGB of the reference view with Dref aligned to 3D space, focusing on the unseen regions identified by the unseen mask. This unprojection takes into account the cameras intrinsic parameters. For each pixel (u, v) in the unseen region where Ufinal(u, v) = 1, we compute the 3D point = (X, Y, Z) as = Dref aligned(u, v), = (u cx) Z/fx, 5 = (v cy) Z/fy,, where (fx, fy) are the focal lengths in pixels and (cx, cy) are the principal point offsets. This process gives us set of initial 3D points . These points are then used to initialize new Gaussians in the unseen regions, inheriting color from the reference view. Existing background Gaussians, unaffected by object removal, remain fixed during initialization and optimization. These initialized Gaussians are crucial for the subsequent process of generating guided inpaint RGB image and optimization. preventing hallucinated details that could disrupt multi-view coherence. The resulting guided inpainted RGBs are then used as supervision for Gaussian fine-tuning, updating only the unprojected Gaussians from Sec. 3.2. The final reconstruction is optimized using combination of L1, SSIM, and LPIPS [72] losses: = (1 λSSIM)L1 + λSSIMLSSIM + λLPIPSLLPIPS. (12) 3.3. SDEdit for Detail Enhancement 3.4. Implementation Details After initializing Gaussians in unseen regions, we aim to obtain the inpainted RGB guidance with fine details while ensuring multi-view consistency, which further refines our initial Gaussians during fine-tuning. Inspired by SDEdit [30], we refine the rendered initial Gaussians by adding scaled noise proportional to strength factor s, ensuring that the inpainting model retains structural information from the reference view while allowing for detail refinement across multiple perspectives. We further find that instead of injecting random Gaussian noise, applying DDIM Inversion [51] to the rendered initial Gaussians better preserves their structural information during the denoising process. This approach allows the diffusion inpainting model to reconstruct missing details while maintaining alignment with the reference view, ensuring that inpainted regions integrate seamlessly into the scene (see Fig. 11). Specifically, given rendered training view Iinit, we first obtain its corresponding noise representation via DDIM Inversion, capturing the essential structure of the reference view in the latent space. Instead of inverting fully to t0, we compute an intermediate timestep tinv based on the noise strength s: tinv = (1 s), (9) where is the total number of timesteps in the diffusion process, and controls the noise strength. We then perform DDIM Inversion to obtain the noise representation at tinv: ϵinv = DDIM-Invert(Iinit, tinv). (10) Next, we denoise this noise using 2D diffusion inpainting model, conditioned on the reference view Iref, ensuring that the reconstructed details align with the global scene while maintaining consistency across views: Iguided = Denoise(ϵinv, condition = Iref, tinv0). (11) By inverting to noise level corresponding to strength s, this step ensures that the inpainting model refines details while maintaining geometric consistency with the reference view. Unlike traditional SDEdit, which applies random noise addition before denoising, our approach leverages DDIM Inversion to obtain structured noise that aligns with the scene, We use the 2D Gaussian Splatting (2DGS) codebase for Gaussian representation to obtain accurate rendered depth, with SAM2 generating object masks on the first frame for each training view. Masked Gaussians handle object removal effectively, even without object tracking, due to their explicit representation. For the unseen mask generation, we set the aggregation threshold of θ to 0.6. In AGDD, incomplete depth are normalized to match Marigolds [20] depth. The depth is then refined through 50 denoising steps using an exponentially decayed learning rate α, which starts at 0.1 and decays at rate of 0.99 per step, progressively improving the alignment of the estimated depth. After denoising, the result is unnormalized back to the original scale. The entire inference process takes approximately 1 minute on an RTX 4090 GPU. The noise strength of SDEdit = 0.85 balances initial point retention, as shown in our ablation study. We condition the generation on the reference view using LeftRefill [6]. During Gaussian fine-tuning, we run 10,000 iterations with λSSIM = 0.8 and LLPIPS = 0.5. 4. 360 Unbounded Scenes Inpainting Dataset To address the lack of reference-based 360 inpainting datasets, we introduce the 360 Unbounded Scenes Inpainting Dataset (360-USID), consisting of seven scenes with training views (RGB images and object masks), novel testing views (inpainting ground truth), and reference view (without objects) for evaluating with other reference-based methods. 4.1. Dataset Collection Protocol We developed protocol using standard camera to create this dataset, as simultaneously capturing multi-view photos with and without objects typically requires specialized equipment. Our protocol, illustrated in Fig. 7, consists of: 1. Positioning an object (e.g. vase) on textured surface within 360 unbounded scene. Training views are captured in two complete circular trajectories around the object - the first focuses primarily on the object, while the second maximizes background coverage to ensure comprehensive scene capture. 2. Securing the camera on tripod and capturing reference view from fixed position and orientation. 6 Figure 6. Overview of the 360-USID dataset. Sample images from each scene, including five outdoor scenes (Carton, Cone, Newcone, Skateboard, Plant) and two indoor scenes (Cookie, Sunflower). (Bottom right) The table shows statistics for each scene, including the number of training views and ground truth (GT) novel views. The dataset provides diverse range of environments for evaluating 3D inpainting methods in both indoor and outdoor settings. Figure 7. Illustration of the data capture process for the 360USID dataset. (a) Capturing training views: Multiple images are taken around the object in the scene. (b) Capturing the reference view: camera is mounted on tripod to capture fixed reference view (with an object). (c) Capturing novel views: After removing the object, additional images are taken from various viewpoints, including one from the same tripod position as the reference image. 4.3. Scene Descriptions Our 360-USID dataset, shown in Fig. 6, contains seven diverse scenes: five outdoor (Carton, Cone, Newcone, Plant, Skateboard) and two indoor (Cookie, Sunflower). Each scene includes 180-200 training images at 38402160 resolution (Plant at 19201440), 30 ground truth testing images, and one reference image without objects. Scenes are downscaled to 960540 for evaluation, providing comprehensive benchmark for testing 3D inpainting methods across varied real-world environments. 5. Experiments 5.1. Experimental setup 3. After object removal, capturing novel views from both the fixed tripod position and additional positions distinct from training trajectories for ground truth evaluation. To ensure high-quality captures, we record video at 4K 60fps with stabilized camera settings and extract the sharpest frames using the variance of the Laplacian method. Each scene comprises 180200 training views and approximately 30 testing views for quantitative evaluations. Consistent lighting is maintained throughout to minimize shadow variations between reference and testing images Datasets. We evaluate on two 360 unbounded scene datasets: (1) 360-USID (Ours): new dataset of 7 scenes (3 indoor, 4 outdoor) for evaluating 360 inpainting, with 200300 training views containing objects, around 30 test views without objects, and 1 reference. All images are processed at 960px width to preserve details for quantitative evaluation. (2) Other-360 [3] We collect additional 6 standard 360 unbounded scene datasets from NeRF[31], MipNeRF-360[3] and Instruct-NeRF2NeRF[14] for qualitative evaluation at 1/4 resolution, with frame 0 as reference for all methods. 4.2. Data Preprocessing and Pose Estimation Our processing pipeline begins with using COLMAP [48, 49] or similar SfM pipelines like hloc [46, 47] to compute shared 3D coordinate space for both training and novel views. We then generate object masks for training views using SAM2 [44] and mask out object regions in COLMAP reconstruction. After obtaining camera poses, we process the training images with NeRF/3DGS inpainting methods and render novel views for comparison against ground truth. Finally, we refine testing views by training masked-3DGS model and selecting optimal frames based on PSNR scores computed outside object regions, yielding approximately 30 high-quality test views per scene. The resulting dataset provides comprehensive benchmark for evaluating 360 inpainting methods across diverse scenes and viewpoints, with particular attention to view consistency and geometric accuracy. Metrics. We evaluate our method using two complementary metrics: LPIPS (Learned Perceptual Image Patch Similarity) [72] for perceptual quality and PSNR (Peak Signalto-Noise Ratio) for reconstruction accuracy. Following SPInNeRF [34], we compute these metrics only within object masks to focus on inpainting quality. While both metrics are used for 360-USID, which has ground truth, only qualitative assessment is possible for Other-360. Additional evaluation results are provided in supplementary materials. 5.2. Comparisons with State-of-the-Art Methods Quantitative comparisons. We evaluate AuraFusion360 against state-of-the-art approaches on the 360-USID dataset. Tab. 1 shows PSNR and LPIPS scores across different scenes. Our method consistently outperforms existing approaches. SPIn-NeRF [34]1and Infusion [27] struggle with 360 consistency, while Gaussian Grouping [65] misidenti7 Table 1. Quantitative comparison of 360 inpainting methods on the 360-USID dataset. Red text indicates the best, and blue text indicates the second-best performing method. PSNR / LPIPS Carton Cone Cookie Newcone Plant Skateboard Sunflower Average SPIn-NeRF [34] 2DGS [18] + LaMa [53] 2DGS [18] + LeftRefill [6] LeftRefill [6] Gaussian Grouping [65] GScream [60] Infusion [27] AuraFusion360 (Ours) w/o SDEdit AuraFusion360 (Ours) 16.659 / 0.539 16.433 / 0.499 15.157 / 0.567 14.667 / 0.560 16.695 / 0.502 14.609 / 0.587 14.191 / 0.555 13.731 / 0.477 17.675 / 0.473 15.438 / 0.389 15.591 / 0.351 16.143 / 0.372 14.933 / 0.380 14.549 / 0.366 14.655 / 0.476 14.163 / 0.439 14.260 / 0.390 15.626 / 0.332 11.879 / 0.521 11.711 / 0.538 12.458 / 0.526 11.148 / 0.519 11.564 / 0.731 12.733 / 0.429 12.051 / 0.486 12.332 / 0.445 12.841 / 0.434 17.131 / 0.519 16.598 / 0.670 16.717 / 0.677 16.264 / 0.448 16.745 / 0.533 13.662 / 0.605 9.562 / 0.624 16.646 / 0.460 17.536 / 0. 16.850 / 0.401 14.491 / 0.564 12.856 / 0.666 16.183 / 0.463 16.175 / 0.440 16.238 / 0.437 16.127 / 0.406 17.609 / 0.319 18.001 / 0.322 15.645 / 0.675 15.520 / 0.639 16.429 / 0.634 14.912 / 0.572 16.002 / 0.577 12.941 / 0.626 13.624 / 0.638 15.107 / 0.580 17.007 / 0.559 23.538 / 0.206 23.024 / 0.194 24.216 / 0.181 18.851 / 0.331 20.787 / 0.209 18.470 / 0.436 21.195 / 0.238 24.884 / 0.170 24.943 / 0.173 16.734 / 0.464 16.195 / 0.494 16.282 / 0.518 15.280 / 0.468 16.074 / 0.480 14.758 / 0.514 14.416 / 0.484 16.367 / 0.406 17.661 / 0.388 Figure 8. Visual Comparison on our 360-USID dataset. We compare our method against state-of-the-art approaches including Gaussian Grouping [65], 2DGS + LeftRefill, and Infusion [27]. While Gaussian Grouping struggles with misidentifying unseen regions, leading to floating artifacts, and 2DGS + LeftRefill faces view consistency issues, our method successfully maintains geometric consistency and preserves fine details across different viewpoints. Ground truth (GT) is shown for reference, and the original scene with an object is provided in the first row for comparison. fies the unseen region, causing significant floating artifacts. GScream [60] fails to properly remove objects, and LeftRefill [6] improves but still falls short in 360 environments. 2DGS + LaMa [53] and 2DGS + LeftRefill outperform 2D methods but face view consistency challenges. Our method achieves the highest PSNR score and the lowest average LPIPS, indicating superior perceptual quality and better similarity to the ground truth. The performance gap is especially noticeable in scenes with complex geometry or large removed objects, demonstrating our methods ability to leverage multi-view information and maintain 360 consistency. The code for InNeRF360 [58] could not be successfully executed, and Reference-guided Controllable Inpainting of Neural Radiance Fields [33] did not provide code, so we were unable to compare our method with theirs. 1We implement SPin-NeRFs method on the 2D Gaussian Splatting codebase to extend its capabilities to 360 unbounded scenes. Table 2. Ablation study of our AuraFusion360. Depth init. (Sec. 3.2) SDEdit strength (Sec. 3.3) PSNR LPIPS 0.85 0.5 1.0 0.85 16.638 17.646 17.512 17.661 0.456 0.393 0.391 0.388 Qualitative visual comparisons. Fig. 8 compares our AuraFusion360 method against state-of-the-art approaches on challenging scenes from the 360-USID dataset. Our method excels in maintaining view consistency and preserving fine details in 360 unbounded environments. Additional qualitative results on Other-360 dataset and failure cases will be provided in the supplementary material. Figure 9. Visual comparison of unseen mask generation method. Our method enables SAM2 [44] to generate more accurate predictions for each view without the need for manually provided prompts, as the bounding box prompts are automatically generated through depth warping. Figure 10. Compared Unseen Mask w/ Gaussian Grouping. Gaussian Grouping [65] uses video tracker [9] and the black blurry hole prompt for the DEVA [9] method to track the unseen region. However, this can result in tracking errors, affecting inpainting. In contrast, our geometry-based approach uses depth warping to estimate the unseen regions contour, reducing segmentation errors. 5.3. Ablation Studies To evaluate the effectiveness of each component in our AuraFusion360 method, we conduct series of ablation studies. Tab. 2 presents the quantitative results of these studies. Unseen mask generation. We compared our unseen mask generation method with SAM2 [44] and Gaussian Grouping [65] video tracker in Fig. 9 and Fig. 10. Our approach significantly improves inpainting quality, particularly in areas occluded from multiple views. The unseen masks help to identify truly occluded regions, leading to more accurate and consistent inpainting results. This is especially noticeable in scenes with complex geometries, where object masks alone may not capture all the necessary information for effective inpainting. Effect of Reference View Initial Gaussians Alignment. Tab. 2 and Fig. 11 show that our depth-aware 3DGS initial9 Figure 11. Compared to other depth completion methods. The depth completion model in Infusion [27] (a) performs better at depth alignment compared to traditional methods (b) and (c), but it lacks generalization. Similarly, (d) Guided Depth Diffusion [68] struggles to achieve precise alignment, as the background regions amplify the loss, leading to misalignment. In contrast, (e) Our AGDD effectively addresses these issues. ization accurately estimates aligned depth while maintaining geometric consistency in the inpainted regions. Compared to random initialization, our method produces more structurally coherent results, particularly in areas with significant depth variations. This is especially evident in scenes where the inpainted geometry needs to blend seamlessly with the existing scene structure. 6. Conclusion We presented AuraFusion360, novel reference-based 360 inpainting method for 3D scenes in unbounded environments. Our approach effectively addresses the challenges of object removal and hole filling in complex 3D scenes. Key contributions include leveraging multi-view information through improved unseen mask generation, integrating referenceguided 3D inpainting with diffusion priors, and introducing the 360-USID dataset for comprehensive evaluation. Experimental results demonstrate AuraFusion360s superior performance over existing methods, particularly in complex geometries and large view variations. While this work represents significant advancement in 3D scene editing, future work will focus on computational efficiency, dynamic scenes, and language-guided editing capabilities."
        },
        {
            "title": "References",
            "content": "[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan Goldman. Patchmatch: randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 2009. 3 [2] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-NeRF: multiscale representation for anti-aliasing neural radiance fields. ICCV, 2021. 2 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of the anti-aliased neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54705479, 2022. 7 [4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. 13, 14 [5] Bertalmio. Image inpainting, 2000. 3 [6] Chenjie Cao, Yunuo Cai, Qiaole Dong, Yikai Wang, and Yanwei Fu. Leftrefill: Filling right canvas based on left reference through generalized text-to-image diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77057715, 2024. 3, 6, 8, 14 [7] Bo-Yu Chen, Wei-Chen Chiu, and Yu-Lun Liu. Improving robustness for joint optimization of camera pose and decomposed low-rank tensorial radiance fields. In AAAI, 2024. 2 [8] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In CVPR, 2024. 2 [9] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In ICCV, 2023. 2, 9, 14 [10] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Region filling and object removal by exemplar-based image inpainting. IEEE TIP, 2004. [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. 34:87808794, 2021. 3 [12] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. FastNeRF: High-fidelity neural rendering at 200FPS. In ICCV, 2021. 2 [13] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. In ICML, 2017. 2 [14] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1974019750, 2023. 2, 7 [15] Philipp Henzler, Niloy J. Mitra, and Tobias Ritschel. Escaping Platos cave: 3D shape from adversarial rendering. In ICCV, 2019. 2 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), pages 68406851. Curran Associates, Inc., 2020. 3 [17] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024. 1, 2, 13, [18] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 8 [19] Jireh Jam, Connah Kendrick, Kevin Walker, Vincent Drouard, Jison Gee-Sern Hsu, and Moi Hoon Yap. comprehensive review of past and present image inpainting methods. CVIU, 203:103147, 2021. 3 [20] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 5, 6, 14 [21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. 1, 2 [22] Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang, Wenjun Zeng, Xinchao Wang, and Zhibo Chen. Diffusion models for image restoration and enhancementa comprehensive survey. arXiv preprint arXiv:2308.09388, 2023. [23] Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, and Yu-Lun Liu. Frugalnerf: Fast convergence for few-shot novel view synthesis without learned priors. arXiv, 2024. 2 [24] Guilin Liu, Fitsum Reda, Kevin Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In ECCV, 2018. 3 [25] Hao-Kang Liu, I-Chao Shen, and Bing-Yu Chen. NeRF-In: Free-form NeRF inpainting with RGB-D priors. In arXiv, 2022. 2 [26] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. In NeurIPS, 2020. 2 [27] Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, and Yang Cao. Infusion: Inpainting 3d gaussians via learning depth completion from diffusion prior. arXiv preprint arXiv:2404.11613, 2024. 2, 3, 5, 7, 8, 9, 14 [28] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 2, 3 [29] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. [30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 2, 3, 6, 15 [31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1, 7 [32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, pages 405421. Springer, 2020. 2 10 [33] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, and Igor Gilitschenski. Reference-guided controllable inpainting of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2, 3, 8 [34] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstantinos G. Derpanis, Jonathan Kelly, Marcus A. Brubaker, Igor Gilitschenski, and Alex Levinshtein. SPIn-NeRF: Multiview segmentation and perceptual inpainting with neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20669 20679, 2023. 2, 3, 7, 8, [35] Ashkan Mirzaei, Riccardo De Lutio, Seung Wook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, and Zan Gojcic. Reffusion: Reference adapted diffusion models for 3d scene inpainting, 2024. 2, 3 [36] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models, 2024. 3 [37] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real arXiv preprint images using guided diffusion models. arXiv:2211.09794, 2022. 3 [38] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In CVPR, pages 25362544, 2016. 3 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 3 [40] Kira Prabhu, Jane Wu, Lynn Tsai, Peter Hedman, Dan Goldman, Ben Poole, and Michael Broxton. Inpaint3d: 3d scene content generation using 2d inpainting diffusion. arXiv preprint arXiv:2312.03869, 2023. 3 [41] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. arXiv preprint arXiv:2312.16084, 2023. [42] Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong Wang. Language-driven physics-based scene synthesis and editing via feature splatting. In European Conference on Computer Vision (ECCV), 2024. 2 [43] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. 5 [44] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 4, 7, 9, 13 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 3 [46] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019. [47] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning feature matching with graph neural networks. In CVPR, 2020. 7 [48] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 7 [49] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. 7 [50] I-Chao Shen, Hao-Kang Liu, and Bing-Yu Chen. Nerf-in: Free-form nerf inpainting with rgb-d priors. Computer Graphics and Applications (CG&A), 2024. 3 [51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3, [52] Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, and Yu-Lun Liu. Boostmvsnerfs: Boosting mvs-based nerfs to generalizable view synthesis in large-scale scenes. In ACM SIGGRAPH 2024 Conference Papers, 2024. 2 [53] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with Fourier convolutions. In WACV, pages 21492159, 2022. 3, 8, 14 [54] Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, et al. Realfill: Reference-driven generation for authentic image completion. arXiv preprint arXiv:2309.16668, 2023. 3 [55] Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jitendra Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. In CVPR, 2017. 2 [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2 [57] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics, 2023. [58] Dongqing Wang, Tong Zhang, Alaa Abboud, and Sabine Inpaintnerf360: Text-guided 3d inpainting arXiv preprint Susstrunk. on unbounded neural radiance fields. arXiv:2305.15094, 2023. 2, 3, 8 [59] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo MartinBrualla, Noah Snavely, and Thomas Funkhouser. IBRNet: Learning multi-view image-based rendering. In CVPR, 2021. 2 [60] Yuxin Wang, Qianyi Wu, Guofeng Zhang, and Dan Xu. Gscream: Learning 3d geometry and feature consistent gaus11 sian splatting for object removal. In ECCV, 2024. 2, 3, 8, [61] Silvan Weder, Guillermo Garcia-Hernando, Aron Monszpart, Marc Pollefeys, Gabriel Brostow, Michael Firman, and Sara Vicente. Removing objects from neural radiance fields. CVPR, pages 1652816538, 2023. 3 [62] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2031020320, 2024. 2 [63] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In ICCV, pages 1377913788, 2021. 2 [64] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highfidelity monocular dynamic scene reconstruction. In CVPR, 2024. 2 [65] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. In ECCV, 2024. 2, 3, 7, 8, 9, 13, 14 [66] Youtan Yin, Zhoujie Fu, Fan Yang, and Guosheng Lin. Ornerf: Object removing from 3d scenes guided by multiview segmentation with neural radiance fields. arXiv preprint arXiv:2305.10503, 2023. 3 [67] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In CVPR, 2021. [68] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. arXiv:2406.09394, 2024. 5, 9 [69] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Generative image inpainting with contextual attention. CVPR, 2018. 3 [70] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-form image inpainting with gated convolution. In ICCV, 2019. 3 [71] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020. 2 [72] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6, 7 [73] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In European Conference on Computer Vision, pages 145163. Springer, 2025. 12 A. Overview This supplementary material provides additional details and results to complement the findings presented in the main manuscript. First, we explain the training process for masked Gaussians and the details of object removal in App. B. Next, we elaborate on how depth warping, used to generate bounding box prompts for SAM2 [44], effectively identifies the contours of unseen regions in App. C. We then describe the experimental setup in App. D. Subsequently, we discuss the limitations of our approach in App. E, followed by additional visual comparisons on our 360-UISD dataset ( Fig. 15) and the Mip-NeRF-360 [4] dataset ( Fig. 16). In addition to this document, we provide an interactive HTML interface for comparing our video results with stateof-the-art 3D inpainting methods, visualizing unseen masks, and exploring the ablation study on depth alignment. Finally, we include the source code of our implementation, which will be made publicly available to ensure reproducibility. B. Masked Gaussians Training and Object Removal During the training of masked Gaussians, we use 2DGS [17] as our codebase and introduce masked attribute, ranging between 0 and 1, for each Gaussian. The L1 loss is computed between the object mask obtained via SAM2 [44] and the rasterized object mask for each training view. Additionally, we incorporate the Grouping Loss proposed by Gaussian Grouping [65], ensuring that neighboring Gaussians have similar masked attributes. This ensures that our Gaussian model retains accurate object mask information and is capable of rendering precise object masks for subsequent applications. Thanks to the explicit nature of Gaussian Splatting, we can directly remove Gaussians with masked attribute greater than threshold τ during the removal stage, effectively achieving object removal. In our implementation, τ is set to 0.6. C. Depth Warping for Identifying Unseen Region Contours Following Sec. 3.2 and Fig. 4 of the main paper, we explain in detail how depth warping allows us to identify the contours of the unseen region, as illustrated in Fig. 12. Without loss of generality, to find the unseen region contour at view n, and for each pair of views and i, we first compute the removal region for view by identifying pixels that differ between the rendered depth and the incomplete depth of view rather than using object masks. This approach better captures geometric changes and prevents misalignment artifacts, leading to improved SAM2[44] prompts and more precise unseen masks  (Fig. 13)  . Next, we establish pixel correspondences between view Figure 12. Intermediate Results of Depth Warping for Unseen Region Detection. This figure illustrates the intermediate results generated during the depth warping process. (a) and (b) show the RGB image and the corresponding removal region at view n, respectively. (c) displays the removal regions obtained from view (i = n). (d) shows the unseen region obtained from view through backward traversal. The intersections are concentrated near the unseen region. Note that the pixels within the unseen region, but with value of zero, are due to the absence of Gaussians in that area, preventing depth rendering and thus making it impossible to establish pixel correspondences between view and view i. (e) presents the aggregation of all unseen regions obtained from view at view n. threshold is applied to this result, and it is then intersected with the removal region at view to obtain the final result in (f). and view using the incomplete depth of view n. The removal region of view is then backward-traversed to view based on these correspondences. During this backward traversal, it is important to note that pixels outside the unseen region in view will correspond to the background areas in view n, while pixels belonging to the unseen region remain in the unseen region. By aggregating contributions from all views (i = n), we project non-unseen regions from each view into different areas of view n, while consolidating the unseen regions. This allows us to identify the contours of the unseen region in view n. These contours can then be used as the bounding box prompt for SAM2, resulting in more accurate unseen mask. 13 Figure 13. Ablation Study on Removal Region Definition. Comparison of (a) object masks vs. (b) depth difference for defining removal regions. Object masks fail to capture geometric changes, leading to less accurate unseen masks. Depth difference better preserves scene structure, improving SAM2 prompts and unseen region segmentation. D. Experimetal Setup D.1. LeftRefill [6] We use the same reference image as in our method, along with the rendered object masks of each novel testing view generated by our masked Gaussians, as input to LeftRefill, and directly perform reference-based inpainting on each testing novel view. D.2. 2DGS [17] + LaMa [53] We provide the same reference image and training view object masks as in our method and use LaMa [53] to obtain per-frame inpainting results for each training view to train the 2DGS. D.3. 2DGS [17] + LeftRefill [6] We provide the same reference image and training view object masks as in our method and use LeftRefill to obtain per-frame inpainting results for each training view to train the 2DGS. D.4. SPIn-NeRF [34] The original SPIn-NeRF [34] codebase is designed for forward-facing scenes; however, we adapt it for comparison on 360 scenes by implementing its approach on 2DGS [17]. We first obtain the depth for each training view by training 2DGS model. Next, we generate inpainted RGB and depth maps using LaMa [53], which are then used to train the inpainted 2DGS model. During training, we follow SPInNeRFs methodology by incorporating patch-based RGBLPIPS loss and using the Pearson correlation coefficient to compute scaleand shift-invariant depth loss. Figure 14. Failure Cases. The figure illustrates failure cases of inpainting results. These examples highlight the challenges of 3D inpainting when significant occlusions are present near the regions requiring inpainting. For instance, (b) and (c) demonstrate difficulties in achieving satisfactory guided inpainted RGB images in the training views, while (d) and (e) show errors resulting from incorrect pixel unprojections. These observations indicate that this issue is not effectively addressed by any of the compared methods, suggesting potential avenue for further exploration and improvement. generate estimated depths for all training images, meeting GScreams input data requirements. D.6. Gaussian Grouping [60] We utilize the original Gaussian Grouping [65] codebase as baseline for comparison. First, it generates segmentation IDs, from which we select the IDs corresponding to objects that require inpainting. These selected IDs are then used in the removal process. Following the original workflow, the unseen regions are identified, subsequently inpainted, and used for their fine-tuning process. Notably, after removing objects from the scene, Gaussian Grouping relies on TrackingAnything-DEVA [9] to identify unseen regions requiring further inpainting through the black blurry hole prompt. However, DEVA occasionally fails to accurately identify unseen regions in certain scenes, leading to incorrect inpainting and suboptimal results. Additionally, in some scenes, such as the bonsai scene from the Mip-NeRF-360 [4] dataset and the plant scene from the 360-UISD dataset, the object tracker misidentifies objects, resulting in incorrect object removal and further degrading the inpainting quality. D.5. Gscream [60] D.7. InFusion [27] We follow the original GScream [60] pipeline as baseline for comparison. We provide the same reference image and training view object masks as our method to ensure consistency. Following their pipeline, we use Marigold [20] to We use the original InFusion [27] codebase as baseline for comparison. We provide the same reference image used in our method as the input RGB for its depth completion model. This reference image is also used in its fine-tuning process. 14 Figure 15. Visual Comparison on our 360-USID dataset. E. Limitations Our method successfully addresses complex, unbounded 360 scene inpainting. However, rendering the unprojected initial Gaussians and applying SDEdit [30] to enhance the guided inpainted RGB images can be time-consuming, particularly for high-resolution or large-scale scenes, which poses challenges for real-time applications. Furthermore, our analysis Fig. 14 shows that the method may produce incorrect pixel unprojections in cases with significant occlusions near the object requiring inpainting, resulting in floaters in the final inpainted outputs. This limitation is similarly observed across all compared methods, underscoring valuable direction for future research and improvement. 15 Figure 16. Visual Comparison on Other-360 dataset."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "National Yang Ming Chiao Tung University"
    ]
}