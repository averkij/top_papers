{
    "paper_title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era",
    "authors": [
        "Kanzhi Cheng",
        "Wenpo Song",
        "Jiaxin Fan",
        "Zheng Ma",
        "Qiushi Sun",
        "Fangzhi Xu",
        "Chenyang Yan",
        "Nuo Chen",
        "Jianbing Zhang",
        "Jiajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io."
        },
        {
            "title": "Start",
            "content": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era Kanzhi Cheng Fangzhi Xu Wenpo Song Chenyang Yan"
        },
        {
            "title": "Nuo Chen",
            "content": "Zheng Ma Jianbing Zhang Qiushi Sun Jiajun Chen National Key Laboratory for Novel Software Technology, Nanjing University The University of Hong Kong Shanghai Artificial Intelligence Laboratory zjb@nju.edu.cn chengkz@smail.nju.edu.cn 5 2 0 2 6 ] . [ 1 9 2 3 2 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Image captioning has been longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at CapArena."
        },
        {
            "title": "Introduction",
            "content": "Image captioning, the task of generating textual descriptions for images, has long been fundamental challenge in both the computer vision and natural language processing communities (Vinyals et al., 2015; Anderson et al., 2018; Cornia et al., 2020). It has broad and valuable applications, such as assisting visually impaired individuals and supporting multimedia retrieval. Driven by progress in LLMs, Equal contribution. 1 Figure 1: Model rankings from CapArena in detailed captioning. Top models are comparable to humans, while most open-source models lag behind. modern Vision-Language Models (VLMs) are capable of generating long, detailed descriptions of image content (OpenAI, 2023; Chen et al., 2024c), moving beyond the short captions of traditional methods. The proliferation of VLMs presents new opportunities for the image captioning field. However, image captioning has not advanced as expected. Current VLMs focus on tasks like Visual Question Answering (Yue et al., 2024) and multimodal reasoning (Cheng et al., 2024a), bypassing the essential task of image captioning. few works still rely on MSCOCO (Lin et al., 2014)a dataset with an average caption length of 10 words, which is clearly outdated for evaluating advanced VLMs. This obstacle stems from the inherent difficulty in evaluating detailed captions. Unlike multiple-choice questions or mathematical reasoning, captioning lacks explicit answers, resulting in the absence of reliable evaluation benchmarks. Researchers are unable to assess the captioning capabilities of existing VLMs, nor effectively evaluate and improve their own models. This paper addresses two key questions to drive the evolution of image captioning in the LLM era: (1) How do existing VLMs perform in detailed captioning? Do top models achieve human-level performance, and how do they compare against each other? We conduct the first large-scale human evaluation to benchmark current VLMs. Further, using resulting annotations from (1) as ground truth, we analyze existing captioning metrics and ask: (2) How can we develop automated evaluation methods that reliably measure detailed caption quality and align with human preferences? To explore the first question, we developed CapArena, which includes over 6000 high-quality human annotations to evaluate the detailed captioning capabilities of 14 advanced VLMs and humans. In our preliminary study, we found that traditional scoring methods are unsuitable for annotating detailed captions quality due to their fine-grained intricacy and diversity. Inspired by LLM evaluation (Chiang et al., 2024), we adopted pairwise caption battles paradigm. The resulting model rankings are shown in Figure 1. For the first time, we observe that state-of-the-art models, such as GPT4o, are comparable to or even surpass human-level performance, marking pivotal milestone in image captioning. While open-source VLMs achieve competitive results on general benchmarks, CapArena reveals persistent performance gap between them and commercial models. An exception is InternVL2-26B (Chen et al., 2024c), smaller open-source model that stands out with its strong performance, underscoring the potential of compact and efficient VLMs for detailed captioning. In response to the second question, we conducted comprehensive analysis of traditional and recent captioning metrics, as well as the ability of VLMas-a-Judge (Chen et al., 2024a) to assess caption quality. We compared these metrics against human preferences from CapArena. Our results reveal that most metrics designed for short captions, such as CLIPScore (Hessel et al., 2021), fail entirely in the detailed captioning task. Although some rule-based metrics, such as METEOR (Banerjee and Lavie, 2005), exhibit decent agreement with human judgments at the caption level, they suffer from systematic biases across different VLMs. This results in low agreement at the model level, where the rankings produced by these metrics deviate significantly from human rankings. In contrast, we introduce VLM-as-a-Judge with reference captions, which demonstrates robust discernment for detailed captions. It achieves the highest alignment with human judgments at both the caption and model levels. In light of these findings, we release CapArenaAuto, an automated benchmark for detailed capIt comprises 600 samples and innovationing. tively adopts the pairwise battle paradigm to improve evaluation reliability. VLM-as-a-Judge is employed to estimate human preferences by comparing captions against three baseline models. With 94.3% correlation to human rankings and just $4 per test, CapArena-Auto offers fast and robust pipeline for evaluating detailed captioning."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Vision-Language Models In recent years, Vision-Language Models (VLMs) have experienced rapid advances, achieving stateof-the-art performance across various multimodal tasks. Most VLMs integrate visual encoders (Radford et al., 2021; Zhai et al., 2023) with large language models (Bai et al., 2023; Touvron et al., 2023), allowing the latter to possess visual perception capabilities (Liu et al., 2024; Ye et al., 2023; Bai et al., 2023; Chen et al., 2024c, inter alia). To achieve this, VLMs are typically trained in two stages: pre-training on large-scale caption data to align visual and textual information, followed by supervised fine-tuning (SFT) with instructionfollowing capacity. Captioning plays crucial role in building VLMs (Chen et al., 2024b), as accurately describing image content forms the foundation for complex recognition and reasoning tasks (Cheng et al., 2024b). Current VLM evaluations mainly focus on visual question answering related to knowledge (Yue et al., 2024) and reasoning (Wu et al., 2024; Sun et al., 2024), while captioning is often overlooked due to evaluation difficulties. Our work is dedicated to benchmarking and analyzing the captioning ability of VLMs. 2.2 Image Captioning and Evaluation Metrics Image captioning significantly progressed over the past decade (Vinyals et al., 2015; Xu, 2015; Anderson et al., 2018; Cheng et al., 2022; Wang et al., 2022; Ma et al., 2023; Cheng et al., 2023; Pi et al., 2024). These methods use human-annotated datasets like MSCOCO (Lin et al., 2014) and Nocaps (Agrawal et al., 2019), evaluating generated captions by comparing them to reference sentences using rule-based metrics such as BLEU (Papineni et al., 2002) and CIDEr (Vedantam et al., 2015). Recent research shows that CLIP-based metrics (Hessel et al., 2021) exhibit higher human consistency for short captions. However, current VLMs generate significantly longer, detailed captions, where 2 Image Pairwise Caption Battle Caption1 (Qwen2-VL-72B): ... The dog is holding stick in its mouth, and the kitten is standing on its hind legs, reaching up to grab the stick. The kittens front paws are extended towards the stick, and its body is slightly arched as it tries to take the stick from the dog. ... Caption2 (Human): ... gray and black kitten is leaping into the air at the dogs face. It is facing the back at right angle. Its right leg is extended out with its paw in front of the dogs face. Its tail is down to the right in the air. ... Preference Caption2 is better (more accurate and vivid description of the cats posture). Caption1 (GPT-4o): ... The mural features figure that appears to be inspired by traditional Asian art, possibly deity or spiritual figure. The figure is depicted with serene expression ... Surrounding the figure is halo-like glow in warm tones of yellow and orange, adding sense of divinity or spirituality. Caption2 (LLama-3.2-90B): The image depicts vibrant mural of woman on the side of building, surrounded by various objects and features. **Mural:** The mural is painted in bright colors, featuring woman with long dark hair wearing blue robe ... Caption1 is better (a spiritual figure is much more informative than woman). Table 1: Examples of pairwise battles in CapArena. Red and green indicate less accurate and more preferable expressions, respectively. The evaluation guidelines are detailed in Section 3.1, and more examples are in Table 6. traditional metrics are not effective, posing challenges for evaluation. Several recent works have focused on detailed captioning. Dong et al. (2024) introduced new metric CAPTURE and improved VLM captioning performance. Lu et al. (2024) proposed sophisticated scene graph-based metric. Differently, we first conducted large-scale human-centered empirical study to systematically evaluate advanced VLMs. We then concentrated on the consistency between metrics and human preferences. An indepth analysis using human annotated data (50 times larger than previous works) reveals the robustness and systematic biases of different metrics."
        },
        {
            "title": "Detailed Image Captioning",
            "content": "While current VLMs excel in tasks like visual perception, question answering, and reasoning, their ability to generate long, detailed image descriptions remains unclear. In this section, we address this gap by introducing the first large-scale humancentered empirical study to benchmark VLMs in the context of detailed image captioning. Next, we first present our evaluation protocol tailored for detailed captioning task (Section 3.1). Then, we describe the implementation of our annotation platform CapArena (Section 3.2). Finally, we highlight our key findings on the performance of existing advanced VLMs (Section 3.3). 3.1 Evaluation Protocol We originally tried scoring system (Hodosh et al., 2013), where annotators were asked to assign score from 1 to 5 to single description. However, the task proved to be inherently complex and subjective. Since most generated captions cover the main content of the image, annotators found it difficult to assign precise grades, leading to low interannotator consistency. Inspired by open-domain evaluations of LLMs, we shifted to pairwise comparison methodology (Chiang et al., 2024) to assess detailed captions, which was further validated in our preliminary study. We believe reliable evaluation protocol is crucial for accurate assessments. Inspired by Kasai et al. (2021), expert annotators drafted the initial guidelines, which were refined through in-house meetings with all annotators to ensure consistency. Finally, we established the following transparent evaluation protocol for detailed captioning. Guidlines. Our guidelines primarily evaluate the quality of descriptions in terms of precision and informativeness. The full annotator guidelines can be found in Appendix H. Precision: Precision measures how precise the content in the description is, i.e., whether the description aligns with the details in the image. For example, in the first case of Table 1, Qwen2-VL provides an inaccurate description of the cats posture, while the human description captures the crucial action of the cat pouncing toward the dog. Precision includes various aspects, such as objects, attributes, relationships, and positions. Informativeness: Informativeness assesses how much of the key information in the image is comprehensively covered by the description, including the salient objects and important details. For example, in the second case of Table 1, Llama-3.2s description of woman is precise; however, it is clearly less informative compared to GPT-4os de3 scription of spiritual figure. Hallucination: Hallucinations are considered an intractable flaw in VLMs (Li et al., 2023), where models generate objects that do not exist in the image. We instruct annotators to impose strict penalties for hallucinations, as they significantly harm the caption quality in real-world applications. Additionally, previous studies show that human annotators are influenced by output length and response style (Chiang et al., 2024). To mitigate this, we asked annotators to focus solely on the quality of the descriptions, minimizing distractions from such aspects. Furthermore, for pairs of similar quality, we considered the longer description less favorable if it was noticeably too long. 3.2 CapArena: Pairwise Battle Platform We developed CapArena, an annotation platform aimed at benchmarking VLMs performance in detailed captioning through anonymous pairwise battles. The platform covers diverse range of image scenarios and evaluates set of established VLMs through human annotator votes, providing reliable rankings between models. Data Source. The test images are sourced from the recently proposed DOCCI dataset (Onoe et al., 2024), which includes high-resolution images of various real-life scenes. We provide detailed information about this dataset in Appendix A. Notably, each image in this dataset is paired by carefully crafted long, human-annotated descriptions, which are used as the human baseline on our platform. For the tested models, we selected diverse set of representative VLMs, including both commercial and open-source models, spanning range of model sizes (the full list is provided in Appendix C). To minimize bias introduced by specific prompts, we crafted 10 prompts for detailed image captioning, such as Describe this image in detail. These prompts were manually reviewed to ensure they generated descriptions of similar quality and length (all prompts are provided in Appendix B). We show the caption length distribution of different VLMs in Figure 2, where GPT-4o is most similar to humans. CapArena Infrastructure. We take to denote the number of models. Let n1, n2 [N ] be the indices of the models. We define At = (n1, n2) as the model pair compared at time t. The human response is indicated by Ht {0, 1}, where Ht = 0 indicates human preference for model n1, and Ht = 1 for model n2. To focus on comparing models with similar perFigure 2: Caption length distribution of different VLMs. formance levels and thus accelerate the convergence of rankings, we adopt the probability update strategy from Chatbot Arena (Chiang et al., 2024). The sampling probability for each pair is proportional to the reduction in the size of the confidence interval: Pt(a) ˆΣt,a,a {t At = a} ˆΣt,a,a {t At = a} + 1 , where Pt(a) is the probability of sampling pair at time t, and ˆΣt denotes the covariance matrix estimated from the samples. We then apply the Bradley-Terry (BT) model (Bradley and Terry, 1952) with the logistic form to calculate the scores of the models: ˆs = arg min ξ t=1 1 (At) ℓ (Ht, 1 1 + eξAt, ) , ξAt,1 where ξ is the vector of BT coefficients, which is an -dimensional vector, and ℓ is the binary crossentropy loss, defined as ℓ(h, p) = (h log(p) + (1 h) log(1 p)). The BT coefficients ˆs are used to create the ordered ranking of models. We bootstrap the BT rating estimate 1000 times to construct confidence interval for each rating. Annotator Training and Quality Control We made considerable efforts to enhance the reliability of the annotations. We conducted in-house annotations, with all annotators being graduate students specializing in natural language processing, with no visual impairments, and familiar with the image captioning task.* We first conducted preliminary annotation on 100 captions and developed the initial guidelines. In-house workshops followed to refine the process and ensure all annotators understood the task. Finally, we adopted the annotation methodology described in Section 3.1. *The first four authors of participated in the annotation. 4 Figure 3: Battle counts and win rate matrix between models in CapArena. Figure 4: Correlation between vision-language benchmark scores and CapArena ranking. Models that perform well on general benchmarks do not necessarily excel in captioning. The size of each point represents the model size. During the annotation process, we randomly selected 400 samples to be annotated by different annotators for inter-annotator validation. The results showed an agreement of 0.782, demonstrating the reliability of our manual labeling. Our empirical study found that the remaining 20% of disagreement stemmed not from annotator errors, but from the inherent subjectivity in interpreting which aspects of the image to emphasize. We also implemented measures to continuously monitor annotation quality during the labeling process (see Appendix E). Data collection began in October 2024, and by February 2025, total of 6,522 annotation instances had been collected. The average time per annotation was 142 seconds. 3.3 VLMs Performance Analysis The final ranking is shown in Figure 1. Figure 3 presents the battle count and win rate matrix. Top models achieve human-level performance. Surprisingly, our evaluation reveals for the first time that leading models like GPT-4o have reached or even surpassed human-level performance in detailed captioning tasks. This milestone demonstrates that machine-generated descriptions can now rival high-quality human descriptions. We conducted an empirical study in Appendix D, and the results show that even human expert annotators occasionally overlook image details, where GPT-4os descriptions are more comprehensive. CapArena uncovers disparities in fine-grained visual perception across models. As shown in Figure 1, most open-source models still lag significantly behind commercial models, even with large model sizes (e.g., Llama-3.2-90B). Despite competitive performance on general multimodal benchmarks, our results reveal the limitations of opensource alternatives in detailed captioning, where they lack stability in accurately perceiving image details. An exception is InternVL2-26B, midsized model that demonstrates exceptional performance. We attribute this to its large-scale vision encoder, InternViT-6B, enabling strong visual understanding. These results highlight the growth potential of open-source models. We present the correlation between model per5 formance on vision-language benchmarks and CapArena ranking in Figure 4. Strong performance on general benchmarks does not always translate to superior captioning ability. For example, despite its relatively low MMMU (Yue et al., 2024) score, InternVL2-26B excels in captioning. Moreover, the hallucination benchmark POPE (Li et al., 2023) lacks discriminative capacity for detailed captioning, underscoring the need for further exploration in assessing image comprehension. Failure Cases. Besides weaker models missing image details or making errors, we identified several common mistakes: (1) failure to provide accurate descriptions for unusual scenes, (2) frequent neglect of subtle yet important details, (3) challenges in scenes requiring knowledge association, and (4) an inability of VLMs to accurately identify the time on clocks. Examples of these cases are provided in Table 7. These shortcomings highlight the need for further research in detailed captioning to advance real-world applications."
        },
        {
            "title": "4 Analysis of Captioning Metrics",
            "content": "Thanks to its meticulously designed annotation process, CapArena can be regarded as reliable evaluation system for detailed image captioning that reflects the performance of different models. However, it relies on costly human preference annotations, making annotating every model of interest impractical. Therefore, automatic metrics are crucial for evaluating and iterating on the development of detailed captioning capabilities in VLMs. This section evaluates range of traditional and recently proposed captioning metrics, and the ability of VLM-as-a-Judge to assess the quality of detailed captions using the CapArena data. The over 6000 high-quality human annotations serve as the golden standard, providing the basis to analyze how these metrics correlate with human preferences and to identify their potential deficiencies. 4.1 Captioning Metrics Traditional Metrics. We first consider rule-based metrics that are commonly used for MSCOCO evaluation: BLEU (Papineni et al., 2002) measures n-gram overlap between the generated and reference captions. METEOR (Banerjee and Lavie, 2005) incorporates synonym matching and stemming to improve recall. SPICE (Anderson et al., 2016) measures semantic consistency using scene graph. CIDEr (Vedantam et al., 2015) evaluates consensus across multiple reference captions. Next, we include range of CLIP-based metrics, which claim higher alignment with human judgments (Hodosh et al., 2013). CLIPScore (Hessel et al., 2021) is reference-free metric that measures the similarity using CLIP features, and LongCLIPScore (Zhang et al., 2024) is variant adapted for longer text. Polos (Wada et al., 2024) enhances CLIPScore through supervised learning on human feedback. FLEUR (Lee et al., 2024) leverages large multimodal model and achieves state-of-theart human alignment on short captions. Metrics Designed for Detailed Captions. To evaluate long, detailed descriptions, few specialized metrics have been proposed. CAPTURE (Dong et al., 2024) extracts visual objects, attributes, and relationships, performing multi-stage matching between generated and reference captions. VDCScore (Chai et al., 2024) is video captioning metric that decomposes the reference caption into question-answer pairs and utilizes an LLM to check their correspondence with the predicted caption. VLM-as-a-Judge. Leveraging powerful LLMs to simulate human preferences has proven effective in open-ended scenarios (Zheng et al., 2023). While recent studies have explored VLM-as-a-Judge in general multimodal tasks (Chen et al., 2024a; Li et al., 2024b), we are the first to apply it to detailed caption comparisona task that demands fine-grained discernment of vision-language semantic alignment. Similar to human judgment, given an image and two descriptions, VLMs determine which one is better. We employ several well-established VLMs (e.g., GPT-4o, Qwen2.5VL (Bai et al., 2025), LLaVA-OneVision (Li et al., 2024a), LLaVA-Critic (Xiong et al., 2024)) as evaluator. We also introduce reference-enhanced variant that incorporates human descriptions to assist the models judgment. 4.2 Experiment Settings To evaluate these metrics ability to assess detailed caption quality, we compare metric-based judgments with human annotations on caption battle pairs in CapArena and analyze their correlation with human preferences. For scoring-based metrics like CIDEr and CLIPScore, scores are used to determine the winner in pairwise battles. For VLM-as-a-Judge, we use prompt similar to human annotator guidelines (See Appendix I). Since some pairs are of similar quality, we allow ties to 6 Metrics Inter-Annotator Output Length BLEU-4 SPICE CIDER METEOR Polos CLIPScore LongCLIPScore FLEUR CAPTURE VDC-Score LLaVA-OneVision LLaVA-Critic Qwen2.5-VL GPT-4o GPT-4o (with ref) Need Ref? Overall Level1 Caption-level Agreement (Including Tie) Level2 Level3 Level4 Model-level Agreement Spearman Kendall τ - No Yes Yes Yes Yes Yes No No No Yes Yes No No No No Yes 0.683 0.585 0.810 0. 0.650 0.593 0.650 0.552 0.620 0.521 - 0.710 - 0.582 Traditional Image Captioning Metrics 0.474 0.417 0.384 0.576 0.479 0.325 0.400 0.458 0.477 0.441 0.378 0.657 0.526 0.266 0.422 0.513 0.480 0.422 0.383 0.582 0.467 0.308 0.404 0.462 0.475 0.415 0.389 0.536 0.462 0.362 0.384 0.444 0.467 0.387 0.387 0.530 0.462 0.355 0.395 0.414 0.424 0.275 -0.279 0.785 0.420 -0.574 -0.226 0. Metrics Designed for Detailed Image Captioning 0.525 0.557 0.601 0.687 0.512 0.579 0.504 0.496 0.479 0. 0.613 0.890 VLM-as-a-Judge 0.606 0.609 0.625 0.628 0.627 0.709 0.735 0.739 0.740 0.733 0.642 0.631 0.647 0.647 0.663 0.541 0.544 0.566 0.572 0. 0.537 0.530 0.552 0.557 0.560 0.921 0.903 0.908 0.930 0.943 0.319 0. 231 -0.209 0.582 0.363 -0.451 -0.121 0.297 0.538 0.736 0.780 0.736 0.736 0.802 0.846 Table 2: Evaluation of various metrics on detailed captioning tasks. Need Ref indicates whether human-written reference captions are required. Higher scores indicate better alignment with human preferences. The best results in each column are highlighted in bold. GPT-4o as the evaluator achieves the best performance. align with human annotations. Scoring-based metrics use threshold to determine draws, ensuring similar occurrence rate as in human annotations. We analyze the agreement between metrics and human preferences from two perspectives: Caption-level Agreement. Caption-level agreement measures the consistency between metrics and human judgments for the same pairwise caption battle. It is calculated as the proportion of pairs in CapArena where the metrics decision matches the human judgment (A wins / wins / tie). To assess metric performance across caption pairs of varying distinction, we categorize all samples into four levels based on the ranking gap between models. Level 1 consists of the most easily distinguishable model pairs (e.g., GPT-4o vs. LLaVA-1.5), while Level 4 includes those with the most similar performance (e.g., GPT-4o vs. Gemini-1.5). We also report human annotators internal consistency across the four levels for reference, manually verifying 100 samples per level. Model-level Agreement. Caption-level agreement measures how often metric matches human judgments on individual battles. However, metric may achieve high caption-level agreement while exhibiting bias toward certain models, leading to inaccurate performance estimation. To address this, we introduce model-level agreement, which measures the consistency between rankings derived from human judgments and those derived from metrics. Specifically, we replace human annotations with metrics for all pairwise battles in CapArena and compute rankings using the same ELO mechanism. We compute this agreement using Spearmans rank coefficient and Kendalls τ . 4.3 Results Analysis Most traditional metrics fail in the detailed captioning task. Traditional metrics with high human agreement on short captions (Hodosh et al., 2013) deviate from human preferences when applied to detailed captioning task. As shown in Table 2, they exhibit low agreement both at the caption-level and model-level. Rule-based metrics, such as CIDEr, struggle due to the flexible nature of detailed captions, which complicates n-gram matching. For CLIP-based metrics like CLIPScore, our results reveal that current vision-language rep7 Figure 5: Metrics exhibit systematic biasesoverestimating (positive values) or underestimating (negative values) certain models. Color saturation represents bias magnitude. Different metrics favor different models. GPT-4o exhibits lower biases (lighter overall colors), contributing to higher model-level agreement observed in Table 2. resentation models fail to align fine-grained details in long descriptions to the image content. VLM-as-a-Judge demonstrates stronger discernment for detailed captions. As shown in Table 1, accurate judgment for closely matched battles relies on deep understanding of image content and discriminative comparison between descriptions. Powerful VLMs with reasoning and fine-grained perception exhibit this capacity. No bells and whistles, GPT-4o as an evaluator, shows the highest consistency with human preferences, outperforming recently proposed metrics. Reference descriptions help the evaluator clarify uncertain image details, further improving model-level agreement. Systematic biasesoverestimating or underestimating certain models are critical concern. As shown in Table 2, METEOR and Output Length (which simply favors longer captions) exhibit decent caption-level agreement, but their model-level agreement is notably lower. To investigate this issue, we assessed the metrics intrinsic bias towards specific models, by calculating the models average win rate across all battles and comparing it with the golden win rate in Figure 3. positive (negative) win rate difference indicates that the metric overestimates (underestimates) the model. As displayed in Figure 5, all metrics exhibit systematic biases. The degree of bias varies across metrics; Output Length shows particularly strong bias, while GPT-4o-as-a-Judge has lower bias than METEOR (average 4.4% vs. 8.2%). This suggests that the disagreement between GPT-4o-asa-Judge and humans is more likely due to random preferences per independent sample, rather than harmful bias towards specific models, leading to more accurate model ranking estimates. Evaluating hard-to-distinguish caption pairs remains challenge. Despite the reasonable performance of VLM-as-a-Judge, it still falls short of Inter-Annotator agreement, particularly on Level 3/4 samples which represent the most challenging model battles (e.g., 0.650/0.620 vs. 0.572/0.560). This suggests that the current VLM still has limitations in perceiving fine-grained image details and distinguishing subtle differences between captions, which in turn leaves room for further improvement in detailed caption evaluation."
        },
        {
            "title": "Benchmark for Detailed Captioning",
            "content": "CapArena relies on substantial human labor to estimate model performance, which is time-consuming and expensive. Therefore, developing an automated benchmark for detailed captioning is desideratum to enable rapid evaluation and accelerate model development. In this section, based on the findings above, we introduce CapArenaAuto, cheap and fast framework for detailed captioning evaluation. CapArena-Auto includes 600 evaluation images and assesses model performance through pairwise battles with baseline models. Curation of Test Samples. We selected images from the DOCCI test split (Onoe et al., 2024) as candidates. The images, photographed by diverse"
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we explore the task of detailed image captioning in the LLM era. We conducted the first large-scale human-centered empirical study to benchmark advanced VLMs performance. The results demonstrate that, for the first time, leading models (e.g., GPT-4o) achieve or surpass humanlevel performance, while open-source models lag behind. Next, we provided an in-depth analysis of existing captioning metrics. Our experiments reveal that VLM-as-a-Judge sets new standard, highlighting that systematic biases in existing metrics are the key factor affecting alignment with human preferences. Finally, we release CapArenaAuto, an automated benchmark that closely aligns with human preferences, offering cost-effective and efficient tool for detailed captioning evaluation. Our findings lay the foundation for the future development of image captioning."
        },
        {
            "title": "Limitations",
            "content": "One limitation of CapArena is the scope of models and the domains covered. Current pairwise battle evaluation includes 14 representative VisionLanguage Models (VLMs), but this selection is constrained by the available annotation resources. As result, several recent models released after our dataset was compiled have not been considered in the evaluation. Additionally, the images used mostly focus on everyday life scenarios, which means that other domains, such as artwork or medical images, are not yet represented. Extending the evaluation to these domains could provide more complete picture of VLM performance. annotators in everyday contexts, encompass wide range of scenes and are of high resolution. Given that these images were newly captured and released after the timestamp of most existing VLMs training, we believe there is minimal risk of data contamination. Next, to sample diverse test set, we adopted the image feature clustering from DOCCI, uniformly sampling 600 samples from 149 clusters. Additionally, we applied CLIP feature-based filtering to remove overly similar samples, ensuring the quality of the final selection. Evaluation Protocol. We employ pairwise battle paradigm and use VLM-as-a-Judge for comparison (Li et al., 2024c; Chou et al., 2024). For each of the 600 test samples, we compare captions from the test model and baseline model to determine the better one. To reduce potential noise and bias from single baseline (Lin et al., 2024), we use three baseline models with different performance levels: GPT-4o, CogVLM-19B, and MiniCPM-8B. We use GPT-4o as the judge due to its high agreement with human preferences and provide human reference captions as additional support. To compute the final score for the test model, we assign +1 for win, -1 for loss, and 0 for draw in each pairwise comparison. The models score in CapArena-Auto is the total sum of its scores across the 600 test samples. We provide the current leaderboard of CapArena-Auto in Table 8. Spearman Kendall τ DOCCI (with BLEU-4) DOCCI (with METEOR) CAPTURE CapArena-Auto 0.341 0.859 0.763 0.943 0.275 0.648 0.604 0.824 Table 3: Correlation between the automated benchmark and CapArenas golden ranking. CapArena-Auto exhibits the highest alignment with human preferences. To validate the effectiveness of CapArena-Auto, we compare it with several recent detailed captioning benchmarks. Table 3 presents the correlation coefficients between the model rankings from these benchmarks and the golden ranking provided by CapArena. CapArena-Auto outperforms existing benchmarks by large margin, better aligning with human preferences. Additionally, its streamlined design allows each evaluation to cost only $4, making it an effective and affordable evaluation benchmark for detailed captioning."
        },
        {
            "title": "References",
            "content": "Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. 2019. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957. Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Spice: Semantic propositional image caption evaluation. In Computer Vision ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part 14, pages 382398. Springer. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 60776086. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, and Christopher Manning. 2024. Auroracap: Efficient, performant video dearXiv tailed captioning and new benchmark. preprint arXiv:2410.03051. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024a. Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024b. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. 2024c. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. 2024a. Vision-language models can self-improve reasoning via reflection. arXiv preprint arXiv:2411.00855. Kanzhi Cheng, Zheng Ma, Shi Zong, Jianbing Zhang, Xinyu Dai, and Jiajun Chen. 2022. Ads-cap: framework for accurate and diverse stylized captioning with unpaired stylistic corpora. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 736748. Springer. Kanzhi Cheng, Wenpo Song, Zheng Ma, Wenhao Zhu, Zixuan Zhu, and Jianbing Zhang. 2023. Beyond generic: Enhancing image captioning with real-world knowledge using vision-language pre-training model. In Proceedings of the 31st ACM International Conference on Multimedia, pages 50385047. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024b. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. 2024. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132. Christopher Chou, Lisa Dunlap, Koki Mashita, Krishna Mandal, Trevor Darrell, Ion Stoica, Joseph Gonzalez, and Wei-Lin Chiang. 2024. Visionarena: 230k real world user-vlm conversations with preference labels. arXiv preprint arXiv:2412.08687. Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. Meshed-memory transformer for image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1057810587. Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. 2024. BenchmarkarXiv ing and improving detail image caption. preprint arXiv:2405.19092. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: referencefree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718. Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853899. 10 Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Ronan Le Bras, Yejin Choi, and Noah Smith. 2021. Transparent human evalarXiv preprint uation for arXiv:2111.08940. image captioning. Yebin Lee, Imseong Park, and Myungjoo Kang. 2024. Fleur: An explainable reference-free evaluation metric for image captioning using large multimodal model. arXiv preprint arXiv:2406.06004. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. 2024b. Vlrewardbench: challenging benchmark for visionlanguage generative reward models. arXiv preprint arXiv:2411.17451. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024c. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355. Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. 2024. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. arXiv preprint arXiv:2406.04770. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems, 36. Fan Lu, Wei Wu, Kecheng Zheng, Shuailei Ma, Biao Gong, Jiawei Liu, Wei Zhai, Yang Cao, Yujun Shen, and Zheng-Jun Zha. 2024. Benchmarking large vision-language models via directed scene graph for comprehensive image captioning. arXiv preprint arXiv:2412.08614. Zheng Ma, Mianzhi Pan, Wenhan Wu, Kanzhi Cheng, Jianbing Zhang, Shujian Huang, and Jiajun Chen. 2023. Food-500 cap: fine-grained food caption benchmark for evaluating vision-language models. In Proceedings of the 31st ACM International Conference on Multimedia, pages 56745685. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. 2024. Docci: Descriptions of connected and contrasting images. In European Conference on Computer Vision, pages 291309. Springer. OpenAI. 2023. GPT-4 technical report. Preprint, arXiv:2303.08774. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, and Tong Zhang. 2024. Image textualization: An automatic framework for creating accurate and detailed image descriptions. arXiv preprint arXiv:2406.07502. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. 2024. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image deIn Proceedings of the IEEE scription evaluation. conference on computer vision and pattern recognition, pages 45664575. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 31563164. Yuiga Wada, Kanta Kaneda, Daichi Saito, and Komei Sugiura. 2024. Polos: Multimodal metric learning from human feedback for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1355913568. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. Ofa: Unifying architectures, tasks, and modalities through simple 11 In Insequence-to-sequence learning framework. ternational conference on machine learning, pages 2331823340. PMLR. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. 2024. Osatlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712. Kelvin Xu. 2015. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2024. Mmmu: massive multi-discipline multimodal understanding In Proand reasoning benchmark for expert agi. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. 2024. Long-clip: Unlocking the long-text capability of clip. In European Conference on Computer Vision, pages 310325. Springer. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685."
        },
        {
            "title": "A DOCCI Dataset",
            "content": "The DOCCI dataset consists of long, humanannotated English descriptions for images. These images were collected, curated, and donated for research purposes. Most images in DOCCI are natural scenes captured in both indoor and outdoor settings. The researcher aimed to capture key challenges, such as spatial relations, counting, text rendering, and world knowledge, among others. DOCCI captions require detailed, accurate descriptions of objects, attributes, and actions, with clear and grammatically correct language. The annotations ensure high quality and consistency for evaluating image captioning models. The datasets diverse scenes and high-quality human-annotated captions make it ideal for our benchmark."
        },
        {
            "title": "B Prompts for generating detailed",
            "content": "caption Table 4 provides the list of prompts used to generate detailed captions for images. Figure 6 shows captions generated with different prompts by the same VLM. Prompts for Caption Generating Describe this image in detail. Generate detailed caption for this image. Explain the contents of this photo with specifics. Create rich and descriptive caption for this visual content. Give this image detailed image caption. Write detailed explanation of the scene depicted here. Provide detailed description of this photo. Provide thorough description of this image. Describe this scene with specifics. Provide descriptive caption for this image. Table 4: Prompts for detailed captioning."
        },
        {
            "title": "C Model List uesd in annotation",
            "content": "The models selected for our experiments include both commercial and open-source models. Among the commercial models, we utilize GPT-4o-0806, GPT-4o-mini-0718, Gemini-2.0-flash-exp, Gemini1.5-pro-002, and Claude-3.5-Sonnet-0620. On the other hand, we also incorporate several opensource models, including LLama-3.2-90B-VisionInstruct, Qwen2-VL-72B-Instruct, Qwen2-VL-7BInstruct, Qwen2-VL-2B-Instruct, LLaVA-v1.634B, LLaVA-v1.5-7B, InternVL2-26B, CogVLM2llama3-chat-19B, and MiniCPM-V2.6-8B. By evaluating both commercial and open-source models, we aim to provide comprehensive comparison of VLMs in the context of detailed captioning. GPT-4o vs Human Example We observed that human captions occasionally overlook certain image details. For instance, in the upper image of Table 5, compared to humans, GPT-4o paid attention to the surrounding environment in greater detail. Additionally, its mention of \"a red leash is attached to its collar, trailing across the grass\" was more precise than simply stating that \"a red leash leading to the bottom of the image\". In 12 Figure 6: Detailed caption generated with different prompt. also allowed to mark the result as tie."
        },
        {
            "title": "G Failure Cases",
            "content": "In our analysis of VLM performance, we identified several common mistakes that persist even in models with strong overall performance. We provide four types of representative examples in Table 7. The last row of the table illustrates typical errors made by weaker models."
        },
        {
            "title": "H Guidelines for Annotation",
            "content": "Our guidelines for judging which of the two image captions is better are showed in Figure 7. While the guidelines provide framework, they cannot account for all possible cases. Therefore, we encourage annotators to make informed judgments based on the specific circumstances and their own reasoning about which caption is more appropriate."
        },
        {
            "title": "I Prompt Template",
            "content": "Building on Chen et al. (2024a); Li et al. (2024b), we designed the VLM-as-a-Judge prompt for pairwise judgments, as shown in Figure 8. CapArena-Auto Leaderboard The current leaderboard of CapArena-Auto is provided in Table 8. the lower image, GPT-4o provided more detailed description of the yacht and also noticed the poster on the window, feature that was not mentioned by the human annotators."
        },
        {
            "title": "E Quality Control",
            "content": "Throughout the annotation process, experts intermittently check the annotations of each annotator to ensure consistent high-quality results. When necessary, specific feedback was provided to guide the annotator and enhance the accuracy and consistency of subsequent annotations. Additionally, the platform continuously moniIf an annotators tored the annotators activity. annotation time consistently fell below the average, the platform would trigger feedback alert, prompting experts to pay closer attention to the quality of that annotators work. Moreover, given the tasks complexity, submissions made in under five seconds were considered as potential malicious attempts and were disregarded to maintain the integrity of the data. The rigorous annotator training procedures and stringent platform filtering mechanisms implemented throughout the annotation process ensured the collection of high-quality data."
        },
        {
            "title": "F Protocol Example",
            "content": "We have included additional annotation examples in Table 6 to offer clearer and more detailed illustration of our evaluation protocol. In the annotation process, annotators follow the established guidelines to carefully assess whether any issues exist within the pair of captions for given image. They compare the two captions against each other, considering multiple aspects. Based on this comparison, annotators provide annotation result, ensuring that the final result reflects the strengths and weaknesses of each caption. If it is difficult to distinguish between the captions, annotators are 13 Image Detailed Caption human: dog laying on its belly looking at the camera, The dog appears to be by tree on lawn of grass with dead leaves around the area. The dog has white and curly fur, It has bone held by its paws, and has red leash leading to the bottom of the image. The dog is looking directly at the camera and has its body shifted to the right. GPT-4o-0806: fluffy, cream-colored puppy is resting against the trunk of large tree in grassy area. The puppy has soft, curly coat and is holding chew stick in its front paws. red leash is attached to its collar, trailing across the grass. The ground is scattered with dry leaves, and the background shows part of house with some garden elements. The puppy looks content and relaxed in its outdoor setting. human: medium three quarters frontal view of white yacht on dock near body of water that is partially visible at the bottom of the view. Behind the yacht is large blue building with blue toned color on the right side of the view, and on its left side the color is gray. The building extends from the middle left of the view to the right side. At the top of the view the blue sky is visible behind large metal constructs and poles that stand above the yacht and nearby building. Between the metal columns and poles are large nets that extend across the upper portion of the view. Reflections of the white yahct are visible on the water at the bottom of the view. The yacht has blue accents around the bottom and has two different deck sections. GPT-4o-0806: The image depicts large yacht docked at marina. The yacht has sleek, modern design with white hull and blue stripe near the waterline. The upper structure features combination of white and wooden elements, giving it classic and elegant appearance. The main deck has wooden railings and covered area with blue fabric, likely for protection against the elements. The yacht has multiple levels, with large windows on the lower deck and spacious upper deck area, possibly for navigation and leisure. There are antennas and equipment on the top, indicating communication or navigation systems. In the background, there is large building with blue exterior and multiple windows, some of which display images or posters. The building is adjacent to the marina, with walkway and railings visible alongside the water. Above the building, there are tall metal structures, possibly part of nearby facility or infrastructure. The water is calm, reflecting parts of the yacht and the surrounding structures. The sky is clear, suggesting sunny day. Table 5: Examples of GPT-4o perform better than human . 14 Image Pairwise Caption Battle Caption1 (Qwen2-VL-7B): ... The background shows more cars parked outside, indicating that the dealership has large inventory. The showroom also features high ceiling with large windows, allowing for ample natural light. ... Caption2 (Qwen2-VL-72B): ... Outside the large windows, the view shows parking lot with several cars parked, including mix of sedans and SUVs. The parking lot is surrounded by greenery, with trees and bushes visible in the distance. The sky is partly cloudy, with patches of blue sky and white clouds. ... Preference Caption2 is better (Caption1 has baseless speculation). Caption1 (Gemini-2.0-flash-exp): ... The most prominent feature is large, reversed text that reads \"BLACK SASH\" when viewed correctly (likely through window). The letters are black, three-dimensional, and seemingly made of hard, glossy material. ... Caption2 (MiniCPM-V2.6-8B): The image shows wall with the word \"BLACK ASH\" written in large, black, capital letters. Below the text, there are several colorful ribbons or banners attached to the wall. ... Caption1 is better (correctly point out \"reversed\" and accurately identify it). Caption1 (GPT-4o-0806): ... It shows person sitting on wooden chair by lake, but instead of human head, the person has the head of dog. The dogs head is large and fluffy, with friendly expression and perked ears. The person is wearing gray shirt, white pants, and brown slip-on shoes, and their arms are resting on the armrests of the chair. ... Caption2 (Claude-3.5-Sonnet): ... The scene shows large, fluffy dog sitting on someones lap in wooden chair on what looks like dock or pier by body of water. The dog, which seems to be mixed breed with long fur, is front and center, taking up most of the persons torso and obscuring their upper body. ... Caption2 is better (correctly identify the spatial relationship). Caption1 (Claude-3.5-Sonnet): ... There are at least three turtles visible, with one larger turtle perched on top and two smaller ones below. The turtles shells have distinct patterns and colors, ranging from dark green to brown. ... Caption2 (human): ... There is one big turtle and little small one on top of its shell and in front of the big turtle there is small one on the log looking up. ... Caption2 is better (accurately identify the characteristics of the three turtles). Caption1 (Gemini-1.5-pro-002): light green and purple monster truck toy is seen in motion, just about to drive off the bottom of bright green plastic slide. The toy has large black tires, pink chassis, and textured, mottled design on its body that incorporates darker green and black markings, resembling scales or flames. ... Caption2 (Qwen2-VL-72B):...The truck is positioned on grassy surface, and behind it is large, green, plastic slide. The slide has smooth, curved surface and is likely part of playground set. ... Caption1 is better (precisely capture the motion state). Caption1 (GPT-4o-mini-0718): ... The car is notable for its body damagespecifically, significant scrape or dent running along the lower part of its drivers side, suggesting that it may have been in minor collision or scraped against post or another vehicle. ... Caption2 (InternVL2-26B): ... The car is positioned with its rear facing the camera, and the license plate is visible. The car appears to be in good condition, with no apparent damage. ... Caption1 is better (correctly describe the condition of the car). Caption1 (human): ... The shadow depicts person standing up straight and suspended in the air while jumping. The person can be seen wearing shorts, and the edge of backpack can be seen. ... Caption2 (Gemini-2.0-flash-exp): ...Shadow Details:...**Pose:** The arms are positioned slightly away from the body and the legs are separated at the feet. This indicates casual, natural standing position. ... Caption1 is better (accurately describe the jumping state). Caption1 (CogVLM2-llama3-chat-19B): This image depicts pink toy car with yellow roof, parked on paved surface. The toy car has simplistic design, with steering wheel on the left side and single seat. Atop the toy car is blue helmet, which is not typically associated with toys. The background shows wooden fence and gravel ground, suggesting an outdoor setting, possibly backyard or park. Caption2 (MiniCPM-V2.6-8B): The image shows childs toy car, which is pink with beige roof. The car is equipped with steering wheel and has blue helmet placed on the roof. The car is situated on paved area with wooden fence and gravel in the background. The toy car appears to be designed for young children, providing safe and playful environment for imaginative play. Tie (Both captions accurately describe the appearance of the toy car, the surrounding objects and environment). Table 6: More example of the evaluation protocol. 15 Category Image Failure Caption Unusual Scene Important Details Require Association Identify Clock Time General Mistake GPT-4o-0806: In this image, flatbed tow truck is transporting white GMC pickup truck. Unusually, white Cadillac sedan is partially lodged underneath the rear of the pickup, with its front end lifted off the ground. The scene appears to be on highway, with shipping container visible in the background, indicating busy roadway. The Cadillacs position suggests it may have collided with the pickup, resulting in this precarious arrangement. The surrounding environment includes concrete barriers and other vehicles, emphasizing the urban setting. Comment: misinterpretation of the scene, where it should be the rear end being lifted, not the front, as commonly assumed. Claude-3.5-Sonnet-0620: This image shows close-up of gray cat sitting on wooden floor. The cat has striking green eyes and is looking directly at the camera. Its fur appears to be mix of light and dark gray, giving it silvery appearance. The cats whiskers are prominently visible. In the foreground, we can see the cats paw raised, showing its pink paw pads. This gives the impression that the cat might have been in the middle of grooming or was about to make playful gesture. The background of the image shows parts of what seems to be living room. Theres glimpse of furniture, possibly chair or sofa, and what looks like bookshelf or cabinet in the distance. The room appears to be dimly lit, making the cat the main focus of the photograph. The wooden floor the cat is sitting on looks to be hardwood, and theres hint of rug or carpet visible at the edge of the frame. Overall, this image captures candid and endearing moment of domestic cat in its home environment, showcasing its curiosity and the close relationship between the photographer and the feline subject. Comment: There should be two cats in the scene, and the paw in the foreground actually belongs to the other cat, not the one in the close-up. GPT-4o-mini-0718: series of five artistic shields are displayed on patch of dirt next to sidewalk. The shields, painted in vibrant reds and adorned with intricate designs that resemble leaves or vines, vary in size and orientation. They are positioned in line, suggesting movement, with the largest shield at the far left and gradually decreasing in size towards the right. grassy area and white wooden fence can be seen in the background, completing the scene with touch of nature and structure. The setting appears to be an outdoor space, likely park or community area, illuminated by bright sunlight. Comment: It should be pointed out that this is set of tomato slices, which is important for accurately describing the scene. Gemini-1.5-pro-002: The image shows close-up of an analog clock on textured, light blue wall. The clock has black plastic or metal frame and white face with black Arabic numerals. The time displayed is approximately 1:53. The word \"QUARTZ\" is printed in small black letters below the center of the clock face. The wall shows some signs of wear and tear. Theres noticeable crack running diagonally across the top left portion of the frame, and some smaller imperfections and possible chipped areas scattered around the walls surface. In the top right corner, partially obstructing the view of the wall, is the corner of metal fence or gate. galvanized metal pole is attached to the wall with metal bracket. The wall appears damaged around the area where the pole is attached, with some of the surface material missing, revealing what looks like the underlying concrete or plaster. small section of chain-link fencing is also visible behind the pole. This suggests the clock might be located in more utilitarian or perhaps outdoor setting. Comment: The correct time is about 1:07. LLaVA-1.6-34B: The image captures simple yet intriguing scene of stack of six rectangular concrete blocks arranged in pyramid-like structure on sandy ground. The blocks, varying in shades of gray and brown, are stacked in such way that the smallest block is at the top, and the largest block is at the bottom. The pyramid casts shadow on the ground, adding depth to the scene. The sandy ground beneath the pyramid is scattered with small rocks and pebbles, providing natural contrast to the man-made structure. The image is taken from slightly elevated angle, giving clear view of the pyramid and its surroundings. The overall composition of the image suggests sense of balance and stability, as the pyramid stands firm on the sandy ground. Comment: The description of the arrangement of the bricks in terms of quantity and size is incorrect. There are more than six equal-sized bricks. Table 7: Examples of failure cases. . 16 Guidelines As an annotator,your task is to judge which of the two image captions is better. Below are some guidelines for your reference: Precision: The caption should accurately correspond to the content of the image, providing precise information about it. Common examples of imprecision include errors in color, quantity, spatial relationships, or the posture of people. Informativeness: Salient information in the image should be reflected in the caption. Since it is impossible to include every detail, you will need to subjectively judge which aspects of the image are important. For instance, describing an otter as \"a small animal\" is precise, but it is less informative than specifying \"an otter\". Hallucination: Captions that include descriptions of objects or elements that are clearly absent from the image should be significantly penalized. Caption length: Longer captions are not inherently better. For captions with equivalent informativeness, shorter ones are either better or at least not worse. Additionally, overly lengthy, verbose, or redundant expressions should be penalized. Attention to detail: Annotators should pay close attention to the details in the image to distinguish the quality of the descriptions. Assistive description: Imagine visually impaired person asking you to describe the image for them. How would you convey the image to them? Reverse thinking: What image does the caption lead us to imagine? Does the caption effectively lead you to imagine the intended image? Ties are acceptable: If you find it genuinely difficult to determine which caption is better (e.g., both captions are excellent), marking tie is acceptable. Figure 7: Guidelines for Annotation Model Score_Avg Score_GPT Score_Cog Score_CPM Length_Avg Gemini-1.5-pro-002 GPT-4o-0806 Qwen2.5-VL-72B-Instruct Gemini-2.0-flash-exp Ovis2-34b Claude-3.5-Sonnet-0620 InternVL2-26B GPT-4o-mini-0718 Ovis-1.6-27b GLM-4V-Plus CogVLM2-llama3-chat-19B Qwen2-VL-72B-Instruct LLaVA-Onevision-72B-sft LLama-3.2-vision-90B-Instruct Hunyuan-standard-vision InternVL2-5-8B MiniCPM-V2.6-8B Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct LLaVA-1.6-34B cambrian-34b LLaVA-1.5-7B 56.17 44.00 35.33 30.83 27.00 21.50 13.00 9.33 3.00 -0.17 -8.50 -9.00 -12.33 -25.67 -26.00 -29.83 -38.00 -48.67 -49.00 -67.50 -75.00 -94.00 29.0 0 -1.0 -2.0 -15.0 -14.0 -38.5 -36.0 -49.5 -51.5 -56.5 -50.5 -57.5 -72.0 -63.0 -71.0 -80.0 -86.0 -78.0 -92.0 -93.0 -99. 61.0 55.5 49.0 39.5 33.5 30.0 20.0 17.0 14.5 13.0 0 -4.5 -6.0 -13.0 -19.0 -29.0 -34.0 -49.5 -59.0 -53.5 -76.0 -92.0 78.5 76.5 58.0 55.0 62.5 48.5 57.5 47.0 44.0 38.0 31.0 28.0 26.5 8.0 4.0 10.5 0 -10.5 -10.0 -57.0 -56.0 -90.5 168.6 115.8 163.7 417.0 120.2 147.9 236.3 139.8 94.2 109.3 115.9 114.5 200.9 160.3 354.1 117.8 106.7 116.8 97.8 124.8 120.2 74.4 Table 8: CapArena-Auto Leaderboard. 17 Template prompt of VLM-as-a-Judge (System Prompt) You are highly capable multimodal AI assistant tasked with evaluating image captions. (Instruction) Given an image and two candidate captions, you are require to determine which of the two captions is better. (Noticement) Below are some guidelines for your reference: 1. **Precision**: The caption should accurately correspond to the content of the image, providing precise information about it. Common examples of imprecision include errors in color, quantity, spatial relationships, or the posture of people. 2. **Informativeness**: Salient information in the image should be reflected in the caption. Since it is impossible to include every detail, you will need to subjectively judge which aspects of the image are important. For instance, describing an otter as small animal is precise, but it is less informative than specifying an otter. 3. **Hallucination**: Captions that include descriptions of objects or elements that are clearly absent from the image should be significantly penalized. 4. **Attention to detail**: Annotators should pay close attention to the details in the image to distinguish the quality of the descriptions. 5. **Assistive description**: Imagine visually impaired person asking you to describe the image for them. How would you convey the image to them? 6. **Reverse thinking**: What image does the caption lead us to imagine? Does the caption effectively lead you to imagine the intended image? 7. **Ties are acceptable**: If you find it genuinely difficult to determine which caption is better (e.g., both captions are excellent), marking tie is acceptable. While the above guidelines provide framework, they cannot cover all possible cases. Therefore, we encourage you to make **subjective judgments** based on the specific circumstances and your own reasoning about which caption is better. (Response Format) Format your response into two lines as shown below: Reason: <your thoughts and reasoning process for the judgment> Judgment: <Caption 1 is better>/<Caption 2 is better>/<Tie> Figure 8: Template prompt of VLM-as-a-Judge."
        }
    ],
    "affiliations": [
        "National Key Laboratory for Novel Software Technology, Nanjing University",
        "Shanghai Artificial Intelligence Laboratory",
        "The University of Hong Kong"
    ]
}