{
    "paper_title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
    "authors": [
        "Jiashuo Liu",
        "Jiayun Wu",
        "Chunjie Wu",
        "Jingkai Liu",
        "Zaiyuan Wang",
        "Huan Zhou",
        "Wenhao Huang",
        "Hongseok Namkoong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation."
        },
        {
            "title": "Start",
            "content": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics Jiashuo Liu1,, Jiayun Wu1,2,, Chunjie Wu1, Jingkai Liu1, Zaiyuan Wang1, Huan Zhou1, Wenhao Huang1,, Hongseok Namkoong3 1ByteDance Seed, 2Carnegie Mellon University, 3Columbia University Corresponding author, Intern at ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates shift from fragmented, task-specific metrics to holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture models dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates multi-round, sequential contest where models are dynamically paired across curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation (N = 100, 000 iterations) is used to approximate the statistically robust Expected Win Score (E[Sm]), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement Failure Sensitivity Analysis by parameterizing the per-round elimination quantity (Tk), which allows us to profile models based on their risk appetitedistinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing vital step towards risk-informed, next-generation LLM evaluation. Correspondence: Jiashuo Liu, Wenhao Huang at {liujiashuo.77, huang.wenhao}@bytedance.com 5 2 0 2 4 2 ] . [ 1 0 1 0 1 2 . 2 1 5 2 : r Figure 1 Overall ranking of 29 advanced LLMs across 38 recent widely-used and open-sourced benchmarks given by our Competitive Swiss-System Dynamics framework. Check if this aligns with your insights."
        },
        {
            "title": "Introduction",
            "content": "The field of Artificial Intelligence has been rapidly transformed by the emergence and widespread deployment of Large Language Models (LLMs). These models exhibit remarkable capabilities across multitude of tasks, spanning complex reasoning [1, 3, 4, 8, 17, 20], code generation [11, 12, 19, 23, 31, 32], and nuanced natural language understanding [21, 25]. Consequently, the development of robust evaluation methodologies has become paramount. However, the sheer diversity of benchmarks presents challenge for practical model selection. In many downstream applicationsranging from selecting backbone for autonomous agents to enterprise API procurementpractitioners face singular decision: they must identify one model that is sufficiently robust to handle diverse, unpredictable workflows. This challenge is exacerbated by the scale of modern LLM evaluation pipelines, which typically comprise hundreds of internal benchmarks. In such high-dimensional settings, manual inspection of individual metrics is practically infeasible. Therefore, deriving unified ranking from fragmented benchmarks is not merely simplification, but necessity for resource allocation and deployment decisions. While standardized leaderboards attempt to provide this unified view (e.g., the Intelligence Index on https://artificialanalysis.ai/), they typically rely on simple aggregate scores. This approach often masks critical shortcomings, treating model with high variance (excellent in one area, poor in another) as equivalent to consistently reliable model. truly deployable model must demonstrate uniformly high performance to be trusted in multi-stage competitive environment. To address the need for reliable selection criterion, the key problem studied in this paper is: Given results on various benchmarks, how to synthesize unified ranking that accurately reflects models general utility and robustness? Current evaluation paradigms primarily rely on static aggregation, method fundamentally challenged by the lack of an objective ground truth for task importance. We refer to this as the problem of arbitrary weighting: when combining results from disparate benchmarks (e.g., math, coding, and safety), researchers must assign weights based on heuristics rather than data. Consequently, the final ranking becomes highly sensitive to these manual choices. Furthermore, existing methodswhether based on simple averaging or static pairwise models like Elo [5]fail to capture the path-dependent nature of real-world model utility. In static average, failure in foundational capability can be mathematically compensated by excellence in an advanced task. However, in practical deployment, capabilities are often sequential and interdependent. Consider typical agentic workflow illustrated in Figure 2: model must first correctly parse users instruction (Step 1) before executing complex reasoning (Step 2). If model fails the foundational Step 1, its potential proficiency in Step 2 is rendered irrelevant. Static metrics obscure this distinction, treating the two capabilities as independent addends. We argue that robust evaluation must bypass the arbitrary weighting problem and instead model evaluation as dynamic competitive systemone that integrates cumulative pressure and structured elimination to reveal models true, risk-adjusted performance. To bridge the gap between static aggregation and dynamic deployment, we introduce the Competitive Swiss-System Dynamics (CSD) framework for Holistic LLM Ranking. Our framework simulates multiround environment where models are sequentially tested across distinct ability benchmarks using the Swiss-System pairing strategy. This approach introduces three primary innovations. First, CSD resolves the arbitrary weighting problem by replacing heuristic coefficients with structural importance. Unlike standard Elo implementations or static averageswhich typically treat all benchmarks as independent data pointsthe Swiss-System enforces path dependency. The impact (or weight) of benchmark is not assigned by the researcher, but emerges from the tournament dynamics: failure in early rounds alters models pairing trajectory and maximum potential score. Thus, weighting becomes function of competitive survival rather than subjective preference. Second, to mitigate the variance inherent in tournament pairings, we leverage Monte Carlo Simulation (N = 100, 000 iterations) to derive statistically robust Expected Win Score (E[Sm]). This metric represents models predicted cumulative victories, effectively eliminating the confounding influence of random pairing luck. Finally, we integrate structured elimination mechanism with tunable parameter Tk (the quantity of eliminated models in the lowest performance group). This facilitates formal Failure Sensitivity Analysis, enabling us to profile models based on risk. Overall, our CSD framework prioritizes models that demonstrate consistent performance across benchmarks (i.e. Robust Generalist), while heavily 2 A. Static Aggregation View Score Excel (100) B. Path-Dependent Reality Avg: 50 User Input Step 1 Instruction BLOCKED Step 2 Reasoning Fail (0) Instruction Reasoning Effective Utility: \"Performs OK on average.\" \"Failure in Step 1 bottlenecks the entire pipeline.\" Figure 2 The Illusion of Static Aggregation. (A) Averaging scores hides foundational failures. (B) In realistic sequential workflow, failure in foundational task (Step 1) blocks downstream capabilities (Step 2), illustrating the path dependency of model performance. penalizing those with localized deficiencies (i.e. Aggressive Specialist). In summary, this paper makes the following contributions to LLM evaluation: novel CSD framework that translates static multi-benchmark results into dynamic, competition-based ranking, intrinsically avoiding the subjective weighting problem. The E[Sm] metric, statistically robust and competition-aware score that provides superior alternative to subjective aggregated scores. quantifiable methodology for assessing models Failure Sensitivity by analyzing E[Sm]s behavior across varying elimination parameters Tk. The remainder of this paper is organized as follows: Section 2 details the formal structure of the CSD framework, including the Swiss-System rules, elimination mechanisms, and the Monte Carlo methodology. Section 3 reviews related work in LLM benchmarking and competitive ranking systems. Section 4 presents experimental results, comparing CSD rankings against traditional methods and demonstrating the Failure Sensitivity Analysis. Section 7 concludes the paper and discusses future research directions."
        },
        {
            "title": "2 Methodology",
            "content": "This section details the mathematical and conceptual foundations of the Competitive Swiss-System Dynamics (CSD) framework. We first establish the formal components of the competitive environment, justifying our design choices (Section 2.1). We then detail the stochastic state transition process (Section 2.2), leading to our core argument for the computational intractability of an analytical solution (Section 2.3). This logically necessitates the Monte Carlo approximation (Section 2.4) and enables our novel Failure Sensitivity Analysis (Section 2.5)."
        },
        {
            "title": "2.1 Framework Components and Rationale",
            "content": "The CSD framework formally instantiates multi-round, sequential competitive environment, engineered to dynamically evaluate LLMs under conditions of elevated stakes and cumulative performance pressure. The Necessity of Dynamic Competition in Deployment. Static evaluation, based on (heuristically) weighted averages, fails to adequately penalize critical performance gaps, which poses significant risk in sequential industrial applications. The CSD framework is explicitly designed to model and mitigate this risk through structured competition. We demonstrate the motivation through some real-world scenarios: Supply Chain Automation: Consider an LLM managing supply chain, where the foundational task (d1: parsing inventory manifests) precedes complex tasks (dk: optimizing logistics routes). model that 3 fails d1 but excels at dk is an unacceptable risk. CSD sequences d1 first, forcing the fragile model into the minimum score group (Gmin), thereby exposing it to the probabilistic elimination risk controlled by the parameter Tk. Financial Risk Assessment: In loan underwriting, an LLM must possess strong capability in both numerical reasoning (da) and regulatory text interpretation (db). CSD uses its Swiss-System dynamic to pair high-scoring models on da against strong performers on db. This ensures that final ranking reflects genuine full-spectrum competitive resilience, rather than isolated proficiency in one area. Complex Code Generation: multi-step coding agent must correctly complete module d1 to proceed to d2. CSDs path-dependent scoring rewards models that successfully navigate the entire sequence. Consistent early wins accumulate score and provide buffer, ensuring only models capable of sustaining performance survive the rigorous, later-stage competition. Thus, CSD fundamentally shifts the evaluation focus from theoretical average score to risk-adjusted fitness for sequential pipeline, ensuring that only models with verifiable, full-spectrum capability are rewarded with high Expected Win Score ( ˆE[Sm]). We formalize the CSD framework in Algorithm 1. Below, we will introduce our CSD framework in detail. 2.1.1 The Pairwise Win-rate Tensor (W ) The foundational data structure for our framework is the Pairwise Win-rate Tensor (W ), an tensor where is the number of models and is the number of sequenced benchmarks. Conceptual Rationale. By abstracting raw performance scores (e.g., accuracy, perplexity) into binary win/loss outcomes, (i, j, k) {0, 1}, the CSD framework inherently bypasses the critical and subjective problem of benchmark weighting. As discussed in Section 1, deciding if 10-point gain on math benchmark is worth more than 5-point gain on coding benchmark is subjective exercise that makes static-weighted averages fragile. In CSD, win is win. The importance of benchmark is defined not by an priori subjective weight, but by its sequential position in the contest and the competitive pressure at that stage. is pre-calculated from underlying model performance data. Note that is only calculated once. This separation ensures that the iterative computational cost remains relatively low, as the expensive step is not repeated within the Monte Carlo sampling. 2.1.2 The Swiss-System Pairing Engine Given pre-defined sequenced benchmarks D1, . . . , DK, each game in our CSD framework contains multiple (K) rounds. In each round k, we first pair one model with another following the Swiss-System Pairing Engine, and then compare paired models according to their performances on the k-th round. The core dynamic of the CSD is the Swiss-System pairing mechanism, which dictates who competes with whom in each round. In round k, the active models are first grouped based on their current cumulative win score Pairing Engine. Sm(k 1). The system then pairs models within these score groups with the primary objective of matching opponents with identical scores. This mechanism ensures that model comparisons are concentrated among competitors of similar proven strength, thus maximizing the diagnostic value of each match. The Swiss-System is chosen for its efficiency in ranking large number of competitors with limited number of rounds. Unlike simple random-pairing tournament, its primary feature is dynamic strength-of-schedule matching: models with similar cumulative scores are paired against each other. This ensures that: 1. High-performing models are rigorously tested against other high-performers, preventing them from achieving high rank by only defeating weaker opponents. 2. Low-performing models are paired, allowing for clearer differentiation at the bottom of the ranking, which is crucial for our elimination mechanism. 4 Zero-Point Bye Rule. critical, non-standard design choice in our CSD framework is the Zero-Point Bye rule. In traditional chess tournaments, bye (receiving no opponent) may grant 1 or 0.5 points. We explicitly assign 0 points. This is because our objective is to measure competitive success. bye is product of random chance (an odd number in score group Gs(k)), not competitive victory. This design choice ensures that E[Sm] exclusively reflects accumulated competitive victories, not passive, luck-based score inflation. Furthermore, as detailed in Section 2.4, the implementation of Monte Carlo Simulationwhich involves repeating the contest timeseffectively mitigates the influence of single-instance stochasticity (e.g., random pairing or elimination) to yield statistically robust Expected Win Score ( ˆE[Sm])."
        },
        {
            "title": "2.1.3 The Structured Elimination Mechanism",
            "content": "The Structured Elimination Mechanism is the CSDs core component for modeling risk and penalizing performance fragility. After each round k, set of models (Tk models) are permanently removed from the contest. Conceptual Rationale. This mechanism models the real-world deployment-cycle reality that models exhibiting significant failures are often eliminated from consideration. By targeting the minimum score group (Gmin), we ensure that elimination pressure is applied only to the models demonstrating the poorest relative performance in the contest up to that point. The parameter Tk (Elimination Count) thus acts as tunable penalty for failure or pressure of the contest."
        },
        {
            "title": "2.2 Formal Stochastic Process\nWe now formally define the state transition from round k to k + 1. Let Sm(k) denotes the score of model m\nafter k rounds.",
            "content": "State Xk = (Mk, S(k 1)): The set of active models Mk and their corresponding cumulative score vector S(k 1). Phase 1: Grouping: Models in Mk are partitioned into disjoint score groups Gs(k) = {m Mk Sm(k 1) = s}. We let ns(k) = Gs(k). Phase 2: Scoring (Im(k)): model ms score for the round, Im(k) {0, 1}, is determined. Its conditional expectation, E[Im(k) Xk], given it is in group Gs(k), is: E[Im(k) Xk] = (cid:80) (cid:40) 1 ns(k)1 (cid:16) 1 1 ns(k) jGs(k){m} (m, j, k) (cid:17) (cid:104) 1 ns(k)1 (cid:80) jGs(k){m} (m, j, k) (cid:105) if ns(k) is even; if ns(k) is odd. (1) Interpretation: This equation quantifies the expected gain for in round k. If ns(k) is odd, has 1/ns(k) chance of receiving bye (score 0) and (1 1/ns(k)) chance of playing. If it plays, it faces one of the ns(k) 1 others, and its expected score is its average win-rate against them on benchmark dk. Phase 3: Elimination: Models update their scores Sm(k) = Sm(k 1) + Im(k). The new minimum group Gmin(k) (size nmin(k)) is identified. The probability of elimination for model is: PElim(m, k) = 1(m Gmin(k)) Tk max(Tk, nmin(k)) . (2) The surviving set Mk+1 is formed, completing the transition. 5 Algorithm 1: Single Instance of Competitive Swiss-System Dynamics (SingleInstanceCSD) : Set of Models = {m1, . . . , mM }; Sequenced Benchmarks = {d1, . . . , dK }; Input Win-rate Tensor (i, j, k) (where corresponds to dk); Number of Rounds K; Elimination Parameter Tk (for each round k); Output : Final Score Vector S(K); Final Model Ranking; 1 Mactive // Set of active models 2 S(0) 0 // Initialize all cumulative scores to zero 3 for = 1 to do Current Benchmark dk is used for round contests; // Phase 1: Grouping and Pairing Partition Mactive into score groups Gs(k) based on S(k 1) airsk for each score group Gs(k) do airsk,s SwissPairing(Gs(k)) // random pairs airsk airsk airsk,s // Phase 2: Contest Execution and Scoring on dk for (mi, mj) in airsk do Imi (k) (i, j, k) // Look up win/loss for dk Imj (k) 1 Imi (k) for each model that received Bye do Im(k) 0 // Zero-Point Bye Rule // Phase 3: Score Update and Elimination for each Mactive do Sm(k) Sm(k 1) + Im(k) // Update cumulative score 5 6 7 8 10 11 12 13 14 16 17 18 19 Gmin(k) {m Mactive Sm(k) = min(S(k))} // Identify minimum score group 20 21 Mactive ApplyElimination(Mactive, Gmin(k), Tk) // Randomly remove Tk models from Gmin(k) if Mactive < 2 then 23 break // Competition ends if fewer than 2 models remain Algorithm 2: Monte Carlo Approximation of CSD Expected Win Score ( ˆE[S]) Input : Set of Models M; Sequenced Benchmarks D; Win-rate Tensor ; Number of Rounds K; Elimination Parameter Tk; Number of Monte Carlo Iterations ; Output : Estimated Expected Win Score Vector ˆE[S]; Statistically Robust Model Ranking; 1 // Number of models 2 Stotal 0M // Initialize total score accumulator for all models 3 for = 1 to do 4 // Run single, stochastic CSD competition instance S(i)(K) SingleInstanceCSD(M, D, W, K, Tk) // Returns final score vector // Accumulate the final scores for each model mj do Stotal,j Stotal,j + S(i) mj (K) 5 7 8 9 // Estimate the Expected Win Score ˆE[Sm] 10 for each model mj do 11 12 ˆE[Smj ] Stotal,j/N // Sample mean over all trials ˆE[S] { ˆE[Smj ]}M j="
        },
        {
            "title": "2.3 Intractability of the Analytical Solution\nOur objective is to compute the Expected Win Score (E[Sm]) for each model m over the K rounds. By the\nlinearity of expectation,",
            "content": "E[Sm(K)] = (cid:88) k=1 E[Im(k)]. To compute E[Im(k)], one must use the law of total expectation: E[Im(k)] = (cid:88) E[Im(k) Xk] (Xk). all possible Xk (3) (4) This analytical solution is computationally intractable. The intractability arises from the path-dependent nature of the process, leading to combinatorial explosion of the state space. The state in round k, Xk, depends on the entire history of stochastic events: 1. Pairing Stochasticity: The random pairings within Gs(j) for all < k. 2. Elimination Stochasticity: The random eliminations from Gmin(j) for all < k. The number of possible contest histories grows exponentially, making the direct computation of the probability (Xk) for every possible state Xk infeasible for any non-trivial , K, and ."
        },
        {
            "title": "2.4 Monte Carlo Approximation of E[Sm(K)]\nGiven the intractability of the analytical solution, we must approximate E[Sm(K)] using Monte Carlo\nSimulation.",
            "content": "1. Simulation: We simulate the entire K-round CSD contest times (e.g., = 10, 000), where each simulation is full path realization from X1 to XK+1. 2. Estimation: The estimator ˆE[Sm(K)] is the sample mean of the final scores S(i) (K): ˆE[Sm(K)] = 1 N (cid:88) i=1 S(i) (K). (5) Conceptual Interpretation of E[Sm]. By the law of large numbers, ˆE[Sm] E[Sm]. This metric is far richer than simple average score. It represents models expected cumulative victories given its ability to survive the sequential elimination pressures of the CSD. It is holistic metric that intrinsically blends models raw win-rate (from ) with its robustness against failure (its ability to stay out of Gmin) and its resilience to random chance (luck in pairing and elimination draws). The whole procedure is shown in Algorithm 2."
        },
        {
            "title": "2.5 Failure Sensitivity Analysis (FSA)\nThe CSD framework’s true diagnostic power is unlocked by the Failure Sensitivity Analysis (FSA). This analysis\nelevates our framework from a simple ranking tool to a diagnostic profiling system.\nSpecifically, a single ˆE[Sm] score (at a fixed Tk) provides a ranking, but the function ˆE[Sm](Tk) provides\na model risk profile. This profile reveals the trade-offs between a model’s aggressive, high-scoring potential\nand its defensive robustness, providing a multi-dimensional basis for model selection that aligns with specific\ndeployment risk-tolerances (e.g., “Is it better to have a model that is excellent at 9 tasks but fails 1, or one\nthat is good at all 10?”).",
            "content": "Procedure. For simplicity, we fix the elimination count Tk for each round (e.g., Tk {1, 2, 3}). We then run the full -iteration Monte Carlo simulation for range of Tk values to trace the E[Sm] curve. We define 7 models Sensitivity Coefficient (Λm) as the empirical derivative (e.g., slope of linear regression) of this function: Λm ˆE[Sm] Tk . (6) This coefficient Λm allows us to classify models (see Figure 4): Aggressive Specialist (Λm 0). model with highly negative slope. Its high E[Sm] at low PT (low penalty) reveals its specialist nature, but this score collapses as the penalty PT increases, exposing its fragility to short boards. Robust Generalist (Λm 0). model with near-zero slope. Its E[Sm] is stable and insensitive to elimination pressure, indicating it rarely, if ever, falls into the Gmin group."
        },
        {
            "title": "3 Related Work",
            "content": "Existing Evaluation Paradigms: Pointwise Evaluation. LLM evaluation has historically been dominated by pointwise benchmarking (e.g., HELM, GLUE, MMLU). This paradigm assesses model performance in isolation, yielding scalar score (e.g., accuracy, F1-score) per task. While essential for measuring task-specific proficiency, pointwise methods inherently fail to capture the relative competitive strength between models, making holistic comparisons challenging. Furthermore, any aggregated ranking derived from pointwise scores relies on arbitrary, subjective weighting across diverse benchmarks, fundamental flaw addressed in our Introduction. This reliance on static, independent metrics led to the development of methods that focus on relative comparison. Pairwise Ranking: Elo and Bradley-Terry Models. To overcome the limitations of pointwise scoring, the research community adopted pairwise ranking models, most notably those based on the Bradley-Terry (BT) model and the Elo rating system. Systems like the Chatbot Arena (LLM Arena) utilize the Elo methodology derived from human preference data via crowd-sourcingto generate general, single-valued skill rating for each model. The mathematical rigor of Elo and BT lies in their probabilistic foundation, which translates win-loss records into latent skill parameter that best explains the observed outcomes. These models are crucial for providing single, universally comparable skill score that is independent of the dataset used, significant advantage over simple averaging. CSD Framework vs. Static Pairwise Ranking (Elo). While acknowledging the statistical rigor of Elo/BT systems, our Competitive Swiss-System Dynamics (CSD) framework diverges fundamentally in its objective and structure. The key differences can be summarized as shift from static, general-purpose ranking to dynamic, contest-specific profiling: 1. Objective: General Skill vs. Competitive Fitness: Elo aims to compute models universal, equilibrium skill rating (R)a score that predicts future win probability independent of the competition format. In contrast, CSD computes the Expected Win Score (E[Sm]) within prescribed, high-stakes competition structure. E[Sm] measures models fitness for that specific contest, intrinsically folding the penalty of sequential failure and the reward of surviving elimination into the final metric. 2. Information Scope and Dynamics: Elo and BT models are typically designed to process large volumes of independent pairwise outcomes (human votes or standardized head-to-head results). They do not account for path dependency. CSD, however, leverages the Swiss-System dynamic, where pairing in round is determined by the cumulative history of wins Sm(k 1). This dynamic ensures high performers face progressively harder opponents, providing far more realistic simulation of sustained competition. 3. Risk Quantification (Failure Sensitivity): Traditional pairwise models yield only one dimension: skill (R). The CSD framework introduces the Failure Sensitivity Analysis (FSA) via the elimination parameter Tk. This allows CSD to quantify models risk profileits vulnerability to being eliminated due to single short boarda diagnostic capability entirely absent in standard Elo systems. Our method thus offers crucial second dimension of evaluation for deployment safety and reliability. 8 Contest Simulation and Next-Generation Evaluation. Our work contributes to the emerging field of ContestBased Evaluation. Existing work on applying tournament structures to AI (e.g., in game theory and multi-agent systems) often focuses on optimizing pairing strategies for efficiency. CSD, by contrast, focuses on using the tournament structure itself as diagnostic tool. By combining the statistical robustness of Pairwise Data (W matrix) with the dynamic structure of the Swiss-System and the rigorous sampling of Monte Carlo simulation, CSD represents vital step toward next-generation evaluation framework. This framework moves beyond passive measurement, offering an actionable, risk-informed, and context-aware methodology for ranking LLMs suitable for specific deployment pipelines."
        },
        {
            "title": "4 Experiment",
            "content": "This section presents the experimental evaluation of our CSD system through two key analyses: (1) an overall comparative analysis of how the most advanced LLMs ranking across multiple established benchmarks, and (2) an in-depth examination of LLM ranking within specific, individual benchmark. Throughout this section, we examine the most advanced LLMs (29 LLMs in total), including: Google (Gemini Series): Gemini-3-pro, Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-2.5-Flash.1, Gemini2.5-Flash-Lite, Gemini-2.5-Flash-Lite-Preview-2509, Gemini-2.5-Flash-Preview-2509. OpenAI (GPT-5 Series): GPT-5.1-high, GPT-5-chat, GPT-5-high, GPT-5-medium. Anthropic (Claude Series): Claude-Opus-4.1-nothinking, Claude-4-Sonnet-nothinking, Claude-Sonnet4.5-nothinking, Claude-Sonnet-4.5-thinking. Alibaba (Qwen Series): Qwen-plus-0728, Qwen3-next-80b-a3b-thinking, Qwen3-next-80b-a3b-instruct, Qwen3-235b-a22b-instruct-2507, Qwen3-max-0923. DeepSeek: DeepSeek-V3.1-Terminus-nothinking, DeepSeek-V3.1-Terminus-thinking, DeepSeek-V3.2Exp-nothinking, DeepSeek-V3.2-Exp-thinking, DeepSeek-V3.2-thinking. Zhipu AI (Z.ai / GLM Series): GLM-4.5, GLM-4.6. Moonshot AI (Kimi Series): Kimi-K2-0711, Kimi-K2-0905."
        },
        {
            "title": "4.1 Overall Ranking of Advanced LLMs Across 38 Benchmarks",
            "content": "As previously discussed, the ultimate goal of the CSD framework is to derive holistic LLM ranking across various benchmarks. To achieve this, we perform multi-round, sequential contest among the models. Benchmarks. We evaluate model performance using comprehensive suite of 38 recent, widely-used, and open-source benchmarks. Our selection methodology aims to reflect contemporary and representative ranking aligned with community needs, focusing on two criteria: (1) widespread adoption and (2) recent development. These benchmarks span 6 high-level capability categories: basic knowledge, reasoning, instruction following, coding, agent capabilities, and factuality. For more granular and sequential analysis (corresponding to our 12-round contest structure), these are further organized into 12 sequential sub-categories. Sub-categories are delineated based on either the difficulty spectrum or the scope of the respective benchmarks, ensuring structured progression of challenges. Detailed information regarding the sequential arrangement and mapping is provided in Table 1. Order of Benchmarks. As shown in Table 1, the sequencing of these benchmarks follows robust two-part rationale: moving from general to specific coverage, and progressing from fundamental to challenging tasks. For instance, core abilities such as general knowledge and instruction following form the essential prerequisites upon which complex capabilitieslike reasoning, coding, and autonomous agent skillsare built. We therefore place these foundational capacities at the beginning of our Swiss System schedule. This specific sequence is critical within our CSD framework. Recall that CSD involves elimination after each round, and the early ranking is determined by the initial, fundamental tasks. While model might excel at an advanced task (e.g., coding or agent work), poor performance on the fundamental steps (general knowledge and 9 Category Basic Knowledge Instruction Following Reasoning Coding Agent Factuality 3 4 5 6 7 9 10 11 12 Table 1 Sequencial benchmarks used in our CSD framework. Round Sub-Category Foundational Knowledge Others Cleaned Dataset Names MMLU [10], MMLU-pro [25], SuperGPQA [24], SimpleQA [26], ChineseSimpleQA [9] ArenaHard(v2) MMMLU [10] [13], GMMLU(lite) [21], Basic Instruction Following IFEval [33], MulDimIF [30] Complex Instruction Following EIFBench [35], MultiChallenge [22], MARSBench [29] Logic Puzzles Mathematics Other Disciplines Algorithms Software Engineering Search Agent Tool Use Factual Hallucinations ARC-AGI-2 BENCH [14] [6], ProcBench [8], KORAIME24, AIME251, BeyondAIME [3], MathConstruct [1] GPQA(diamond) [20], ScienceOlympiad [4], Phybench [18], PHYSICS [7] LiveCodeBench [11], CodeForces [19], HardEval [23] SWE-Bench [12], Multi-SWE-Bench [31], Terminal Bench2 GAIA [15], HLE [17], BrowseComp [27], BrowseComp-zh [34] τ 2-bench [2] FActScore [16], LongFact (object) [28], LongFact (concept) [28] instruction following) indicates severe deficiency. model that cannot correctly understand our demands or lacks basic factual grounding cannot be expected to succeed reliably in complex tasks, regardless of its specialized potential. By prioritizing the foundation, the CSD framework naturally imposes higher implied weight on models that demonstrate strong performance in basic competencies early on. This ensures that models which pass the initial low-level filter are inherently more reliable and capable of handling the demands of subsequent, specialized tasks. Finding 1: The Overall Ranking Aligns with Consensus and Reveals Four Performance Tiers. The overall ranking of the 29 advanced LLMs is visually represented in Figure 3, establishing the general capability hierarchy among these state-of-the-art models. Figure 3 reveals three distinct layers of top-performing models. The first tier is led by Gemini-3-pro, GPT-5.1-High, GPT-5-High, and GPT-5-Medium which demonstrate highly similar performance. Critically, all these four models exhibit minimal degradation in score as the elimination pressure (models dropped per round) increases, indicating exceptional robustness and general competence. These three LLMs collectively form the first tier, representing the most general and resilient models currently available. The second tier of models is clearly demarcated, including Claude-Sonnet-4.5-thinking and DeepSeekV3.2-thinking. Similar to the first tier, these two models exhibit relatively robust performance and general competence. The third tier of models is led by Qwen3-Max. It is closely followed by highly competitive group: GLM-4.6, Gemini-2.5-pro, DeepSeek-V3.2-Exp-Thinking, Gemini-2.5-Flash-Preview, and DeepSeekV3.1-Terminus-Thinking. This cluster highlights significant trend: the rapid performance improvement 10 Figure 3 Overall ranking of 29 advanced LLMs across 38 recent widely-used and open-sourced benchmarks given by our CSD framework, highlighting the models organized into four tiers. of Chinese models (GLM, DeepSeek, Qwen). Their scores demonstrate that they are effectively closing the performance gap with Gemini-2.5-Pro (and some have surpassed it), which previously defined the performance frontier (SOTA) in many benchmarks and remains highly robust contender. This tier, therefore, illustrates the accelerating global competition and the quickly evolving landscape of general LLM capabilities. The fourth tier contains models with suboptimal operational or architectural trade-offs, thus establishing clear performance degradation. This cohort primarily consists of previous model iterations (e.g., GLM4.5), lightweight or efficiency-focused versions (e.g., Gemini-2.5-Flash-Lite, Gemini-2.5-Flash), and base models lacking external thinking or planning mechanisms (e.g., Kimi-K2-0905, DeepSeek-V3.2-ExpNonThinking). This concentration confirms the strong relationship between model scale/architectural complexity and overall performance robustness under competitive evaluation. Finding 2: Robust Generalists vs. Aggressive Specialists. To better understand how model performance is affected by competitive pressure, we define the performance drop as: E[Sm] := E[SmTk = 2] E[SmTk = 0] = 2 Λm, (7) which calculates the decrease in the average score when the system shifts from baseline state (Tk = 0, no elimination) to the Tk = 2 condition (dropping two models per round), and Λm is the sensitivity coefficient defined in Equation (6). In Figure 4, we plot the base performance E[SmTk = 0] against the sensitivity coefficient, where models positioned further to the right exhibit better overall performance, while models positioned higher up demonstrate fewer shortcomings relative to the group. From Figure 4, we find that the three tiers observed in Figure 3 are clear. As introduced in Section 2.5, we conduct failure sensitivity analysis, where we define two distinct model behaviors based on their performance and resilience: the Robust Generalist and the Aggressive Specialist. Robust Generalist is characterized by high overall performance coupled with minimal shortcomings. Conversely, an Aggressive Specialist also achieves high overall performance but exhibits significant shortcomings (i.e., low robustness). As visualized in Figure 4, models such as Gemini-3-pro, GPT-5.1-High, GPT-5-High, GPT-5-Medium, Claude-Sonnet-4.5-Thinking, and DeepSeek-V3.2-thinking exemplify the robust generalist behavior. Conversely, Qwen-3-235B, Qwen-Plus-0728, and Qwen-Next-80B-Thinking are grouped as aggressive specialists. 11 Figure 4 Base Performance and Sensitivity Coefficient of 29 Models. The base performance is the average model score given by our CSD framework when there is no model elimination (Tk = 0). The sensitivity coefficient is the gradient in average score when Tk increases from 0 to 2, calculated as (E[SmTk = 2] E[SmTk = 0])/2. more negative sensitivity coefficient indicates greater model shortcomings or susceptibility to elimination."
        },
        {
            "title": "4.2 Sensitivity Analysis of CSD Framework",
            "content": "Following the overall ranking analysis in Section 4.1, we now explore the sensitivity of the proposed CSD framework to score perturbation. In real-world scenarios, model performance scores may be subject to Sensitivity to extremely low scores. significant measurement errors or API instability, leading to occasional extreme (outlier) values on one or several specific benchmarks. This analysis investigates the robustness of the CSD framework when faced with such score perturbations. Example 1 (Sensitivity to Zero Scores on IFEval and MulDimIF). Consider scenario where the scores for Qwen3-Max on the IFEval [33] and MulDimIF [30] benchmarks are set to zero, potentially simulating API errors. In this case, Qwen3-Max becomes the weakest model in the third round. Surprisingly, as depicted in Figure 5a, this extreme perturbation minimally affects the overall ranking produced by our CSD framework. In sharp contrast, if we were to simply aggregate all benchmark results using the average score, the rank of Qwen3-Max would drop to 12th, highlighting the CSD frameworks superior robustness to outlier data. Example 2 (Sensitivity to Zero Scores on Four Benchmarks). To further test resilience, we introduce an extreme perturbation by setting the Qwen3-Max scores to zero on four distinct benchmarks: IFEval [33], MulDimIF [30], AIME24, and AIME25 (simulating comprehensive failure due to API errors). This places Qwen3-Max as the weakest model in the third round and significantly weaker model in the sixth. As detailed in Figure 5b, under this severe perturbation, the rank of Qwen3-Max drops to 10th but remains within the third performance tier. Crucially, the simple average baseline ranking for Qwen3-Max plummets much further, to 19th, underscoring the CSD frameworks superior stability against widespread score anomalies. Remark on CSD Robustness. Comparing Example 1 with Example 2, we observe that increasing the number 12 (a) Example 1. Zero scores on two benchmarks. (b) Example 2. Zero scores on four benchmarks. Figure 5 Score Perturbatioin Analysis on Extremely Low Scores. of zeroed benchmark scores leads to more significant, yet controlled, decrease in the models rank. This analysis demonstrates that the CSD framework is robust against isolated, severe score perturbations (e.g., API errors affecting few benchmarks). Crucially, the purpose of this sensitivity test is not to validate the frameworks behavior under widespread data failure. In practical scenario where large number of benchmark scores are unreliable, the only necessary step is to re-test all results. Furthermore, we find that the CSD framework similarly exhibits low sensitivity to extremely high scores on several specific benchmarks."
        },
        {
            "title": "5 Discussion",
            "content": "In this section, we discuss the extended applications of our proposed CSD framework, as well as the limitations of the framework."
        },
        {
            "title": "5.1 Extended Applications",
            "content": "Beyond generating an overall ranking, the CSD framework is flexible and can support personalized and specialized use cases. This section discusses two such applications: agentic performance prediction and ranking models within single benchmark. 5.1.1 Agentic Performance Prediction Our CSD framework is ideally suited for agentic performance prediction because its core mechanism requires sequence of input benchmarks. This structure directly aligns with the widely-used agentic workflow, where single complex task necessitates the sequential and competitive execution of multiple underlying capabilities. For example: Example 3 (Web Navigation and Data Extraction). complex agentic task, such as Web navigation and data extraction, involves sequence of dependent steps where failure in an early step invalidates the entire process. This can be mapped to competitive sequence for CSD as follows: 1. B1: IFEval (Instruction Following): Interpret the users goal and constraints. (Foundational) 2. B2: GSM8K (Simple Reasoning): Devise the initial action plan (e.g., Search Click Extract). (Core Planning) 3. B3: ToolBench (Function Calling): Execute the action by correctly generating the tool/API interaction code. (Execution) 4. B4: HumanEval (Code Debugging): Validate and process the retrieved data for final output. (Refinement) Therefore, if we set the corresponding sequence of benchmarks, the overall ranking given by our CSD framework could reflect (or predict) the ranking on the new agentic task. We believe that agentic performance prediction represents an interesting and important extension direction for our CSD framework. 13 However, significant caveat exists: since many recent advanced LLMs inevitably target or optimize for performance on open-source agentic benchmarks during their development, making it more challenging to directly apply the CSD framework to predict performance on novel agentic tasks. This occurs because the input benchmarks themselves may be subject to data contamination or overfitting."
        },
        {
            "title": "5.1.2 Ranking Models within One Single Benchmark",
            "content": "While traditional evaluation relies on single aggregate score (e.g., accuracy), this often fails to capture the competitive dynamics across the different capabilities required within single, complex benchmark. Our CSD framework can provide more nuanced and robust ranking by modeling the internal structure of the benchmark. Establishing Difficulty Tiers. To apply CSD, we first leverage the empirical performance of all models to objectively partition the benchmarks question set into sequential difficulty tiers. Specifically, we can rank all questions by their average model performance (success rate) and group them into ordered subsets: B1 (Easiest), B2 (Medium), B3 (Hardest), and so forth. Simulating Competitive Sequence. We then treat this sequence of difficulty tiers (B1, B2, . . . , Bn) as the input to the CSD framework. This simulates sequential evaluation where models must successfully pass the easier challenges before proceeding to the harder ones. Favoring Robustness over Spikes. This method inherently favors robust generalists over models that exhibit large performance variance. Our expectation is that model failing on empirically easy questions (low score on B1) but succeeding on complex ones (high score on B3) suggests some instability or unreliable reasoning, possibly due to stochasticity or data contamination. By applying the CSD mechanism, such models will incur severe performance drop (E[Sm]), resulting in lower final rank. Conversely, models that maintain consistent performance across the increasing difficulty tiers are rewarded for their stability and resilience. Experiment Setting. We conduct experiments on QA datasets (MMLU-pro [25] and SuperGPQA [24]), respectively. For each benchmark, we conduct the following workflow: 1. Data-Driven Segmentation: Instead of using the datasets separately, we group all questions into ten tiers (B1: Questions with > 90% average accuracy, B2: 70% 80%, . . . , B9: < 10%). 2. CSD Input: We use B1, B2, . . . , B10 as the sequential input. (a) SuperGPQA (b) MMLU-pro Figure 6 CSD Framework on Single Benchmark. As illustrated in Figure 6, models exhibit varying levels of sensitivity when Tk ranges from 0 to 2. Notably, the Kimi models (light green) experience substantial performance decline on both benchmarks. This suggests that they are less competitive on easier questions compared to harder ones, indicating lack of robustness across these two benchmarks. Similar cases happen on Claude nonthinking models and GLM models too. In comparison, Gemini-3-pro, GPT-5.1-high, and Claude-Sonnet-4.5-thinking are quite robust on these two QA benchmarks."
        },
        {
            "title": "5.1.3 Applying CSD to Datasets with Weights",
            "content": "In cases where only benchmark weights are available without fixed order, we extend the CSD framework by inducing an order through sampling. Specifically, we sample permutation of benchmarks based on their weights, ensuring that benchmarks with higher importance are statistically more likely to be listed first. And based on this sampling mechanism, we can again use Monte Carlo to approximate the expectation of each models score. sample code for generating the benchmark order is like this: import random def et _ we gh ed _ or er ( datasets , weights ) : \" \" \" Returns partial order of datasets based on weights using the Efraimidis - Spirakis algorithm . \" \" \" # Calculate key : ^(1/ ) keys = [( random . random () ** (1.0 / ) , ) for , in zip ( datasets , weights ) ] # Sort by key in descending order keys . sort ( key = lambda : [0] , reverse = True ) return [ for , in keys ] 1 2 3 4 5 7 8 9 10 11"
        },
        {
            "title": "5.2 Limitations",
            "content": "While the CSD framework offers significant advantages in identifying robust models and revealing competitive weaknesses, it is essential to acknowledge its limitations, primarily stemming from the nature of LLM evaluation itself. Absence of Ground-Truth Ranking. fundamental challenge in evaluating complex models is the absence of universally accepted ground-truth ranking for overall LLM performance. Unlike traditional machine learning tasks with clear objective functions, the true ranking of generalist LLMs remains subjective and context-dependent. 1. Subjectivity of Utility: Our CSD ranking reflects the concept of competitive robustness and shortfall penalty, which favors models that are consistently reliable across diverse tasks. While this is highly valuable metric for practical deployment, it does not necessarily align with other utility definitions (e.g., peak performance on single, highly specialized task). 2. Lack of External Validation: Consequently, it is challenging to perform definitive external validation of the CSD ranking against an objective best list. The ranking is primarily validated through its internal consistency and its superior robustness demonstrated in the sensitivity analysis, rather than by external correlation with an undisputed standard. Challenges in Direct Baseline Comparison. The CSD framework introduces novel competitive ranking dynamic that diverges significantly from conventional score aggregation methods. This originality presents difficulties when comparing our results to existing baselines. 1. Incommensurate Metrics: Traditional baselines often rely on simple metrics like average score or geometric mean, which treat all benchmarks equally and do not account for sequential dependencies or competitive elimination. Since the CSD ranking is defined by its unique E[Sm] metric (performance drop under competitive pressure), direct, quantitative comparison with baseline rankings based solely on aggregate scores is incommensurate. 2. Focus on Resilience vs. Peak Performance: The primary goal of CSD is to penalize weaknesses and reward resilience. Therefore, our ranking may deviate from baselines that prioritize raw peak performance, even if that performance is fragile or susceptible to significant drops when encountering shortfall. This deviation reflects deliberate methodological choice rather than an error, but it complicates straightforward better/worse comparisons with aggregate methods."
        },
        {
            "title": "6 Data and Code",
            "content": "Data. We rely on internal evaluation results for this analysis. Please note that LLM scores, especially on agent benchmarks, are sensitive to environmental factors and API stability. As result, the specific rankings derived via the CSD framework may exhibit fluctuations under different evaluation setups. Code. The code can be found at https://github.com/LJSthu/LLMSwissRound."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced the Competitive Swiss-System Dynamics (CSD) framework, novel and robust methodology that addresses fundamental limitations in current LLM evaluation paradigms by simulating dynamic, multi-round competitive environment. This approach intrinsically solves the subjective weighting problem plaguing aggregated leaderboards, as the weight of benchmark is naturally determined by its sequence and frequency within the contest structure. By prioritizing dynamic competitive fitness and penalizing performance shortfalls, the CSD ranking offers more reliable metric for selecting LLMs destined for complex, multi-stage deployment. Future work will focus on extending the CSD frameworks utility, including the formal integration of sequential dependencies for agentic performance prediction and exploring the correlation between CSD rankings and real-world task failure rates."
        },
        {
            "title": "References",
            "content": "[1] Mislav Balunovic, Jasper Dekoninck, Nikola Jovanović, Ivo Petrov, and Martin Vechev. Mathconstruct: Challenging llm reasoning with constructive proofs. In Forty-second International Conference on Machine Learning, 2025. [2] Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. tau2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982, 2025. [3] ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads, 2025. [4] Bytedance-Seed. Scienceolympiad: Challenging ai with olympiad-level multimodal science problems, 2025. [5] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [6] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025. [7] Kaiyue Feng, Yilun Zhao, Yixin Liu, Tianyu Yang, Chen Zhao, John Sous, and Arman Cohan. Physics: Benchmarking foundation models on university-level physics problem solving. arXiv preprint arXiv:2503.21821, 2025. [8] Ippei Fujisawa, Sensho Nobe, Hiroki Seto, Rina Onda, Yoshiaki Uchida, Hiroki IKOMA, Pei-Chun Chien, and Ryota Kanai. Procbench: Benchmark for multi-step reasoning and following procedure. [9] Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. Chinese simpleqa: chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024. [10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. [11] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. [12] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. [13] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline. Blog post.[Accessed 07-02-2025], 2024. [14] Kaijing Ma, Xeron Du, Yunran Wang, Haoran Zhang, Xingwei Qu, Jian Yang, Jiaheng Liu, Xiang Yue, Wenhao Huang, Ge Zhang, et al. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks. In The Thirteenth International Conference on Learning Representations, 2025. [15] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [16] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, 2023. [17] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [18] Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, et al. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074, 2025. [19] Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al. Codeelo: Benchmarking competition-level code generation of llms with humancomparable elo ratings. CoRR, 2025. 17 [20] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [21] Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and Sara Hooker. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation, 2024. URL https://arxiv.org/abs/2412.03304. [22] Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. arXiv preprint arXiv:2501.17399, 2025. [23] Florian Tambon, Amin Nikanjam, Foutse Khomh, and Giuliano Antoniol. Assessing programming task difficulty for efficient evaluation of large language models. CoRR, 2024. [24] M-AP Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. CoRR, 2025. [25] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. [26] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. [27] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [28] Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8075680827. Curran Associates, Inc., 2024. URL https://proceedings. neurips.cc/paper_files/paper/2024/file/937ae0e83eb08d2cb8627fe1def8c751-Paper-Conference.pdf. [29] Chenghao Yang, Yinbo Luo, Zhoufutu Wen, Qi Chu, Tao Gong, Longxiang Liu, Kaiyuan Zhang, Jianpeng Jiao, Ge Zhang, Wenhao Huang, et al. Mars-bench: multi-turn athletic real-world scenario benchmark for dialogue evaluation. arXiv preprint arXiv:2505.23810, 2025. [30] Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, et al. multi-dimensional constraint framework for evaluating and improving instruction following in large language models. arXiv preprint arXiv:2505.07591, 2025. [31] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [32] Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, et al. Livecodebench pro: How do olympiad medalists judge llms in competitive programming? arXiv preprint arXiv:2506.11928, 2025. [33] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. CoRR, 2023. [34] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025. [35] Tao Zou, Xinghua Zhang, Haiyang Yu, Minzheng Wang, Fei Huang, and Yongbin Li. Eifbench: Extremely complex instruction following benchmark for large language models. arXiv preprint arXiv:2506.08375, 2025."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Carnegie Mellon University",
        "Columbia University"
    ]
}