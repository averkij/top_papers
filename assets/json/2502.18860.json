{
    "paper_title": "Exploring Rewriting Approaches for Different Conversational Tasks",
    "authors": [
        "Md Mehrab Tanjim",
        "Ryan A. Rossi",
        "Mike Rimer",
        "Xiang Chen",
        "Sungchul Kim",
        "Vaishnavi Muppala",
        "Tong Yu",
        "Zhengmian Hu",
        "Ritwik Sinha",
        "Wei Zhang",
        "Iftikhar Ahamath Burhanuddin",
        "Franck Dernoncourt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best."
        },
        {
            "title": "Start",
            "content": "Md Mehrab Tanjim, Ryan A. Rossi, Mike Rimer, Xiang Chen, Sungchul Kim, Vaishnavi Muppala, Tong Yu, Zhengmian Hu, Ritwik Sinha Wei Zhang, Iftikhar Ahamath Burhanuddin, Franck Dernoncourt {tanjim, ryrossi, mrimer, xiangche, sukim, mvaishna, tyu, zhengmianh, risinha, wzhang, burhanud, dernonco}@adobe.com Adobe Research"
        },
        {
            "title": "Abstract",
            "content": "Conversational assistants often require question rewriting algorithm that leverages subset of past interactions to provide more meaningful (accurate) answer to the users question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including text-to-text generation task and multimodal generative task that takes as input text and generates visualization or data table that answers the users question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for conversational question-answering assistant, the query rewriting approach performs best, whereas for data analysis assistant that generates visualizations and data tables based on the users conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best."
        },
        {
            "title": "Introduction",
            "content": "Conversational assistants have become integral tools in various domains, from customer service to personal assistance. These systems often depend on sophisticated algorithms to interpret and respond to user queries effectively. One critical aspect of such systems is question or query rewriting (QR), which aims to transform user queries into more detailed and self-contained versions, thereby enhancing the systems ability to provide accurate responses. QR includes various techniques aimed at addressing underspecified and ambiguous queries. Traditionally, QR involves adding terms to the original query, known as query expansion (Lavrenko and Croft, 2017), rephrasing the query with similar phrases (Zukerman and Raskutti, 2002), or using synonymous terms (Jones et al., 2006). Recently, the emergence of large language models (LLMs) has generated interest in utilizing these generative models to automatically resolve ambiguities during query processing, thereby enhancing query modeling for downstream tasks. For example, recent studies have prompted LLMs to generate detailed information, such as expected documents or pseudoanswers (Wang et al., 2023; Jagerman et al., 2023; Anand et al., 2023). These methods are particularly useful when high-quality dataset for domain is unavailable, necessitating the employment of LLMs customized for the particular use-case. This leads to crucial research problem: Can single LLM-based query rewrite module be universally effective across diverse conversational scenarios, or is there need for specialized modules tailored to specific query types and use cases? Addressing this problem involves systematically evaluating the performance of LLMs in query rewriting across various domains and tasks, identifying potential limitations, and exploring whether onesize-fits-all approach is feasible or if more nuanced, context-dependent strategy is required. This investigation will provide valuable insights into the adaptability of LLMs and inform the design of more robust and versatile conversational assistants. Summary of Main Contributions. The key contributions of this work are as follows: We introduce generalized framework for two query rewriting approaches that is expressive to recover both approaches at either extreme. We systematically investigate both approaches across three new datasets including both short and long conversational data analysis task for generating visualizations from text and another text-based question-answering task. 5 2 0 2 8 2 ] . [ 2 0 6 8 8 1 . 2 0 5 2 : r We find that for both the short and long conversational text-to-vis tasks, query fusion performs the best whereas for conversational question-answering task, the query rewrite outperforms query fusion."
        },
        {
            "title": "2 Related Work",
            "content": "Query rewrite or reformulation (QR) includes various techniques aimed at addressing underspecified and ambiguous queries by transforming them into more detailed and self-contained versions that are appropriate for retrieval or question-answering systems. Traditionally, QR involves adding terms to the original query, known as query expansion (Lavrenko and Croft, 2017), rephrasing the query with similar phrases (Zukerman and Raskutti, 2002), or using synonymous terms (Jones et al., 2006). When human-generated rewrites or reward signals are available, language models (LMs) are also trained specifically for question rewriting (QR) (Elgohary et al., 2019; Anantha et al., 2021; Vakulenko et al., 2021; Qian and Dou, 2022; Ma et al., 2023). In conversational search, query reformulation is employed to manage conversational dependencies. (Anantha et al., 2021) introduce the rewrite-then-retrieve pipeline, which relies on human-crafted dataset (Elgohary et al., 2019). Many studies fine-tune QR models to generate standalone questions (Yu et al., 2020; Voskarides et al., 2020; Lin et al., 2021; Kumar and Callan, 2020; Wu et al., 2022). Recently, the emergence of large language models (LLMs) has generated interest in utilizing these generative models to automatically resolve ambiguities during query processing, thereby enhancing query modeling for downstream tasks. These tasks frequently aim to improve information retrieval in single question answering setting. For example, recent studies have prompted LLMs to generate detailed information, such as expected documents or pseudo-answers (Wang et al., 2023; Jagerman et al., 2023; Anand et al., 2023). These methods are particularly useful when high-quality dataset for specific domain is unavailable, necessitating the employment of off-the-shelf LLMs customized for the particular use-case. While these techniques significantly mitigate the issue of the extensive training data required for dedicated model training, prompting large language models (LLMs) for query rewriting introduces its own set of challenges. For instance, Anand et al. (2023) identified that LLM-based query rewriting can experience concept drift, deviating from the original intention or meaning when queries alone are used as prompts. This issue becomes particularly pronounced in real-world enterprise systems, where generic terms can have specific, contextdependent meanings. Our observations confirm this phenomenon (e.g., people\" might refer to specific metric or dimension in chart, while segment\" could denote particular data object within the system), leading to the conclusion that there is no universal solution applicable to all use cases. Instead, practitioners must carefully consider their specific use case, identifying the taxonomy and nature of the queries to design the most effective rewrite strategies. To this end, we present best practices for rewrite strategies and, through two distinct use cases within the same enterprise setting, demonstrate how these practices lead to optimal results."
        },
        {
            "title": "3 Approaches",
            "content": "In this section, we introduce general parameterized approach in Algorithm 1 for query rewriting tasks. First, we provide an overview of the parameterized query rewrite approach (Sec. 3.1), then discuss two different parameterizations of it for text-based Q&A rewrite (Sec. 3.2) and query fusion task (Sec. 3.3)."
        },
        {
            "title": "3.1 Overview",
            "content": "In Algorithm 1, we first initialize to be empty, which will be used to represent the context given to the model (Line 9). Next, we iteratively construct the appropriate context with the last previous input questions from = {I1, . . . , It} and the last previous output set = {O1, . . . , Ot} (Line 1011). Finally, we generate the rewritten question using the model conditioned on the application specific prompt , the context we constructed, and the current question to obtain the rewritten question ˆQ = M(P, C, Q). As we will show, this general framework can recover many different approaches that are useful for variety of tasks."
        },
        {
            "title": "3.2 Query Rewrite",
            "content": "In this section, we detail the query rewrite approach that we investigate in this work using Algorithm 1. This approach was designed with the questionanswering task in mind Notably, we of course utilize custom application-specific prompt as input, that is different from the query fusion prompt Table 1: Conversation that user may have with an assistant and the corresponding rewritten question from our query fusion in Sec. 3.3 with = 1. Conversation Rewritten Question 1 2 3 4 5 6 7 8 9 10 compare monthly revenue by country yearly show it as line chart now change to marketing channel what about month over month as bar replace with pageviews show top-3 what about top-5 show only this month add revenue compare monthly revenue by country compare yearly revenue by country compare yearly revenue by country as line chart compare yearly revenue by marketing channel as line chart compare month over month revenue by marketing channel as bar compare month over month pageviews by marketing channel as bar compare month over month pageviews by top-3 marketing channels as bar compare month over month pageviews by top-5 marketing channels as bar compare this month pageviews by top-5 marketing channels as bar compare this month pageviews and revenue by top-5 marketing channels as bar used below. This prompt consists of instructions to rewrite the input query and has application-specific aspects. For the results reported later, we set = 5 and utilize both the set of previous input queries and set of previous responses. As an aside, we denote this more general approach as query rewrite, and refer to the other described next in Section 3.3 as query fusion. Algorithm 1 Parameterized Query Rewrite 1: Input: 2: 3: 4: 5: 6: current query application specific prompt chat history length previous input query set = {I1, . . . , It} previous response set = {O1, . . . , Ot} base large language model 7: 8: Output: rewritten query ˆQ 9: = and = 10: for = max(1, k) to do {Ii, Oi} 11: 12: ˆQ = M(P, C, Q)"
        },
        {
            "title": "3.3 Query Fusion",
            "content": "Now, we describe the query fusion approach. To obtain the query fusion approach from the parameterized approach in Algorithm 1, we simply set = 1, then let Oi = for all = 1, . . . , O, and set to be the set of previously rewritten queries = {R1, . . . , Rt}. An important property of the proposed query fusion is that even when = 1, this approach can succinctly capture an arbitrary number of previous questions, not only the last question or k. This is limitation of the approach described in Section 3.2 since it is highly dependent on selecting good k, and this is fundamentally challenging problem in itself, and depends on the user and application task. An intuitive example is provided in Table 1. It is straightforward to see this property, for instance, our query fusion approach always leverages the last rewritten question as the = 1 previous question, and then fuses it with the current output, hence, suppose the last rewritten question is compare yearly revenue by country, then the user asks show it as line chart, then the rewritten question is simply, compare yearly revenue by country as line chart. From Table 1, it is straightforward to see that even after the user has asked 10 questions, the generated rewritten questions always contain much more information, and can be thought of as compact summary of the previous conversation. As an aside, this approach is likely to be useful for conversational text-to-image generation applications as well, since the typical user behavior is that user requests to generate an image, such as generate an image of cheetah, then modifies the generated image such as add tree on the right, etc. This pattern follows the same as the text-tovisualization application, among others that deal with modalities other than text. The proposed parameterized approach provided in Algorithm 1 is able to recover both the query rewrite and query fusion approaches described above, as well as many others that are likely to be important for other applications."
        },
        {
            "title": "4 Methodology",
            "content": "In this section, we provide details on the methodology including the different datasets investigated and how they were collected as well as the evaluation metrics used. summary of the dataset statistics and properties are provided in Table 2. Notably, we detail the total number of questions for each, the number of questions with chat history, and the average chat length. Table 2: Summary of the datasets and their statistics # Questions # Questions with Chat History Chat Length Question Types Text-based Q&A Text-to-Vis (long conv.) Text-to-Vis (short conv.) 179 794 171 715 161 2-5 10 2"
        },
        {
            "title": "4.1 Text-based Q&A Dataset",
            "content": "In this dataset, we collected questions that users asked to an actual assistant developed to aid users in understanding how to use specific product and its features, or answer questions about the datasets they had, which can be viewed as metadata questions that ask about their data. The questions users asked that are related to product are closely aligned with questions asked about documentation or documents related to it. In this task, there were 22 annotators that were domain experts. These experts were assigned set of conversations and asked to provide the ground-truth rewritten query after each interaction. The conversations varied greatly in the length, going from only 2 questions to conversations with 5 questions. One example of conversation might be \"what is streaming segmentation\", followed by \"how does it differ from batch segmentation?\". Overall, the dataset consists of 179 questions in total, with 136 of them having chat history."
        },
        {
            "title": "4.2 Text-to-Visualization Datasets",
            "content": "For the text-to-visualization task, we created two datasets for evaluation. These include short conversation and long conversation dataset. We detail each of these datasets carefully below."
        },
        {
            "title": "4.2.1 Short Conversational Dataset",
            "content": "In completely different annotation task, we asked 12 domain experts to write an analytics question, then likely follow-up question that may be incomplete, along with the rewritten ground-truth question that takes into account both. Furthermore, we asked the annotator to mark the rewritten question as N/A if rewrite was not needed. Finally, we also asked the annotator to mark the intent of the rewritten query. The annotators were told to focus on writing mostly questions that require rewriting, as those that do not are easy to obtain. This provided us with dataset of 171 short conversations, where 161 of them were detected as actually needing to be rewritten. One example short conversation is: Compare orders by country then the follow-up input question is: Show top-5 countries as bar chart finally, the ground-truth rewritten question is: Compare orders by top-5 countries as bar where the ground-truth rewritten question is colorcoded by the different questions, and the text shown in blue in the final above rewritten question indicates the new information included."
        },
        {
            "title": "4.2.2 Long Conversational Dataset",
            "content": "To collect this dataset, we asked 18 domain experts to complete as many conversations as they can with an AI assistant designed for analytics. We gave them background into the assistants capabilities. Furthermore, we asked them to write conversations that are typically at least 10 questions, and gave them concrete example. We guided them to write conversations that include sequence of rewrites and mentioned that some conversations can have multiple topics indicating that for conversation of length 10, queries 1-4 can be on related topic (requiring maintaining the context), while questions 5-10 can be completely different topic, so question 5 in that case wouldnt require rewrite. We ask the experts to provide the ground-truth rewritten question for each step in the conversation, and provide the intent of the rewritten question. This led us to collect total of 794 questions, with 715 of them having chat history. For one example conversation, see Table 1. In particular, we see typical conversation that user may ask. This conversation consists of 10 data analysis questions where the task is to generate visualization."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "For the experimental results in Section 5, we use two evaluation metrics, namely, cosine similarity and BERT F1 score. To compute the metrics, we use the same embedding model to ensure all results Table 3: Results comparing rewriting and fusion approaches across two conversational tasks. Task Approach Cosine Similarity BERT Text-based Q&A Text-to-Vis (long conv.) Query Fusion Query Rewrite Query Fusion Query Rewrite 0.826 0.859 0.820 0.760 0.751 0. 0.773 0.734 are completely consistent. To derive cosine similarity, we obtain the embedding of the ground-truth rewritten question denoted as and then derive the embedding of the rewritten question from one of the approaches ˆy, then compute the cosine similairity between the actual and generated ˆy as: E(y, ˆy) = ˆy yˆy (1) where E(y, ˆy) = 1 if the two embeddings of the generated and actual rewritten text are identical and E(y, ˆy) = 0 if the two embeddings are orthogonal implying no similarity. We also compute the BERT F1 score for completeness."
        },
        {
            "title": "5 Experiments",
            "content": "In these experiments, we investigate the query rewrite and fusion approaches across two different conversational tasks including conversational text-based Q&A (Section 5.1) and conversational data analysis (Section 5.2)."
        },
        {
            "title": "5.1 Conversational Text-based Q&A Results",
            "content": "For this experiment, we study query rewrite and query fusion for conversational text-based Q&A. Results are provided in Table 3. Notably, we see that query rewrite achieves better cosine similarity and BERT F1 scores compared to query fusion. In particular, query rewrite achieves mean relative gain of 3.9% and 9.8% over query fusion for cosine similarity and BERT F1 score, respectively  (Table 3)  . This indicates the importance of considering both the previous questions along with the answers to those questions during the rewriting process. In comparison, query fusion does not leverage the previous responses from the model. Furthermore, query fusion also recursively fuses the past questions as long as it makes sense to do so, and thus, can adaptively consider both short and longer conversational contexts during the query rewriting process. However, for the conversational text-based Q&A task, there is often need to include the answers (responses from the model) as well, as the current question the user asks can often be the result of series of other questions and their answers, building up to the next question, and so on. For instance, user may ask how to use some feature of product, then get response, then the next question may use or require information from both the previous question, but more importantly the previous answer. One other situation we noticed is that user may ask few questions, then ask another question which may not have been what they wanted or useful, and then afterwards ask another question that was closely related to the questions before the last. In both situations, query fusion did not perform well since it doesnt consider previous answers as input, and does not handle gaps in the conversational context where user may wish to return to previous part of the conversation after asking question that gave an unintended response."
        },
        {
            "title": "5.2 Conversational Data Analysis Results",
            "content": "Now we investigate query rewrite (Sec. 3.2) and query fusion (Sec. 3.3) for the conversational data analysis task. In this task, the user often creates one or more visualizations to help them better understand and analyze their data. One example conversation is shown in Table 1. For this task, we first investigate the long conversation text-to-vis dataset. Results are shown in Table 3. In particular, the query fusion approach from our parameterized framework (Alg. 1) achieves significantly better cosine similarity and BERT F1 score compared to query rewrite. Query fusion achieves mean relative gain of 7.6% and 5.2% over query rewrite for cosine similarity and BERT F1 score, respectively  (Table 3)  . This indicates the importance of query fusion for the conversational data analysis task, as it can better summarize the conversations that often involve creating visualization for some user-specific dataset, and then one or more queries that involve different changes to it, which may include adding, removing, or replacing data attribute, time range, chart type, filter, among others. For such data analysis conversations, these Table 4: Results comparing rewriting and fusion approaches for short text-to-vis conversational task. Approach Cosine Sim. BERT F1 Query Fusion Query Rewrite 0.925 0.857 0.856 0.837 results indicate that the query rewrite approach that includes the previous = 5 questions may often not be very useful, as it may miss the important context in the conversation, introducing ambiguity, leading to poor performance. In comparison, the query fusion uses = 1, but instead of the previous question, it leverages the previous rewritten question, which is essentially summary of the relevant previous questions. As shown in Table 1, the resulting rewritten question after every step represents succinct and compact summary of the conversation up to that point in time. For instance, the last input to the conversation in Table 1 is \"add revenue\", and the proposed query fusion approach from Sec. 3.3 leverages \"add revenue\" along with the previous rewritten question \"compare this month pageviews by top-5 marketing channels as bar\", and the model outputs \"compare this month pageviews and revenue by top-5 marketing channels as bar\", which represents the new compact summary of the conversation up to this point in time. Hence, query fusion can naturally handle conversations of an arbitrary length, whereas in query rewrite the user must specify the length of the chat history to consider, which is fundamentally challenging and prone to errors. Consider the previous example used above, the query rewrite approach would only be able to leverage the previous = 5 questions as input to generate correct and useful rewritten question. However, it is obvious that the rewritten question generated from the query rewrite approach that uses only the chat history from rows 5-9 in Table 1 would be incorrect, as it is missing much of the previous context from earlier in the chat, e.g., the last = 5 questions the user asked did not include anything about marketing channel, and thus is missing fundamental piece of information needed to correctly answer the users question without completely losing context. As an aside, the query rewrite approach may also struggle from conflicting user inputs, such as \"show top-3\" and \"what about top-5\" as the model may incorrectly rewrite based on the one in the more distant past rather than the more recent request, whereas the query fusion approach naturally handles such cases as it recursively generates rewritten question at each step that compactly summarizes the entire conversation up to any point in time. We also investigated shorter conversations for the data analysis task as shown in Table 4. For this dataset, we observe similar results as before. Notably, the query fusion approach achieves better performance across both metrics compared to the query rewrite, which is consistent with the previous findings from the long conversation dataset."
        },
        {
            "title": "5.3 Additional Results with Rewrite Classifier",
            "content": "To analyze further the results, we investigated the impact of the query rewrite approach when using rewrite classifier first, which inferred whether query required rewriting or not. Results are reported in Table 5. Notably, we see that performance improves slightly in Table 5 when compared to the previous results in Table 3. Table 5: Results comparing rewriting with an ambiguity detection classifier. Task Cosine Sim. BERT F1 Text-based Q&A Text-to-Vis (long conv.) 0.871 0.769 0.859 0."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we proposed parameterized query rewrite approach that gives rise to family of algorithms that can be used for variety of different applications. We investigated two such algorithms from the framework, and showed how this parameterized algorithm is able to recover both at either extreme. We systematically compared these two different approaches, which we denoted as rewriting and fusion, on two fundamentally different generation tasks, including text-to-text generation task and multimodal generative task that takes as input text and generates visualization or data table that answers the users question. Notably, we make several important findings. Our results showed that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for conversational question-answering assistant, the query rewriting approach performs best, whereas for data analysis assistant that generates visualizations and data tables based on the users conversation with the assistant, the fusion approach works best."
        },
        {
            "title": "References",
            "content": "Abhijit Anand, Vinay Setty, Avishek Anand, et al. 2023. Context aware query rewriting for text rankers using llm. arXiv preprint arXiv:2308.16753. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-domain question answering goes conversational via question rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 520534. Ahmed Elgohary, Denis Peskov, and Jordan BoydGraber. 2019. Can you unpack that? learning to rewrite questions-in-context. Can You Unpack That? Learning to Rewrite Questions-in-Context. Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query expansion by prompting large language models. arXiv preprint arXiv:2305.03653. Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, and Maarten de Rijke. 2020. Query resolution for conversational search with limited supervision. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 921930. Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678. Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, and Gaurav Singh Tomar. 2022. Conqrr: Conversational query rewriting for retrieval with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1000010014. Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Fewshot generative conversational query rewriting. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 19331936. Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proceedings of the 15th international conference on World Wide Web, pages 387396. Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical query paraphrasing for document retrieval. In COLING 2002: The 19th International Conference on Computational Linguistics. Vaibhav Kumar and Jamie Callan. 2020. Making information seeking easier: An improved pipeline for conversational search. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 39713980. Victor Lavrenko and Bruce Croft. 2017. Relevancebased language models. In ACM SIGIR Forum, volume 51, pages 260267. ACM New York, NY, USA. Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. 2021. Multi-stage conversational passage retrieval: An approach to fusing term importance estimation and neural query rewriting. ACM Transactions on Information Systems (TOIS), 39(4):129. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting for retrievalaugmented large language models. arXiv preprint arXiv:2305.14283. Hongjin Qian and Zhicheng Dou. 2022. Explicit query rewriting for conversational dense retrieval. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4725 4737. Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu, and Raviteja Anantha. 2021. Question rewriting for conversational question answering. In Proceedings of the 14th ACM international conference on web search and data mining, pages 355363."
        }
    ],
    "affiliations": [
        "Adobe Research"
    ]
}