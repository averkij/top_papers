{
    "paper_title": "From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning",
    "authors": [
        "Cheng Yang",
        "Jiaxuan Lu",
        "Haiyuan Wan",
        "Junchi Yu",
        "Feiwei Qin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 6 7 3 2 . 9 0 5 2 : r FROM WHAT TO WHY: MULTI-AGENT SYSTEM FOR EVIDENCE-BASED CHEMICAL REACTION CONDITION REASONING Cheng Yang1, Jiaxuan Lu2, Haiyuan Wan2,3, Junchi Yu4, Feiwei Qin1 1Hangzhou Dianzi University, 2Shanghai Artificial Intelligence Laboratory, 3Tsinghua University, 4University of Oxford"
        },
        {
            "title": "ABSTRACT",
            "content": "The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multichannel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 2035% gains over domain-specific baselines and outperforms general-purpose LLMs by 1015% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes new paradigm for explainable AI in scientific discovery."
        },
        {
            "title": "INTRODUCTION",
            "content": "The progress in chemistry has long relied on the ability to design chemically valid reactions that yield scientific insights (Tu et al., 2023; Ismail et al., 2022). Central to this task is selecting proper reaction condition parameters, such as solvent, temperature, catalysts, and reagent ratios, which are pivotal to reaction success, selectivity, and scalability (Ball et al., 2025; Taylor et al., 2023). The traditional approach involves extensive human labor to explore the chemical reaction space, which cannot satisfy the growing demand for efficient and safe chemical synthesis (Lyall-Brookes et al., 2025; Ali et al., 2024; Lee et al., 2025). Recent advances in deep learning and data-driven modeling have opened up new opportunities for reaction recommendation, enabling automated exploration of reaction space and the discovery of novel, scalable synthetic routes with minimal manual intervention (Ali et al., 2024; Liu et al., 2023). Early work typically trains relatively small-scale models, such as graph neural networks (Wu et al., 2020) and Transformers (Vaswani et al., 2017), from scratch, achieving strong performance when abundant labeled data are available (Wang et al., 2023). With the rapid development of large language models (Naveed et al., 2025; Zhao et al., 2023) (LLMs), there has been growing interest in leveraging their powerful reasoning and planning abilities for reaction condition recommendation (Bran et al., 2025). Current LLM-based approaches can be broadly categorized into retrieval-based (Zhang et al., 2024b; Chen et al., 2023) and reasoningbased approaches. Retrieval-based approaches search for similar reactions from external databases and transfer their conditions to the query reaction, which is usually enhanced by learned molecular embeddings or unsupervised chemical priors to improve retrieval quality (Andronov et al., 2023). In contrast, reasoning-based approaches directly prompt or fine-tune LLMs to infer suitable reaction conditions from molecular structures or textual descriptions (Qian et al., 2023; Zhou et al., 2025), and achieve improved zero-shot and few-shot generalization capabilities. Corresponding author: Feiwei Qin (E-mail:qinfeiwei@hdu.edu.cn). 1 Figure 1: Overview of ChemMAS. collaborative multi-agent system for evidence-based reactioncondition reasoning from SMILES inputs. ChemMAS demonstrates strong versatility and delivers state-of-the-art performance on reaction condition reasoning. However, despite their success in predicting plausible reaction conditions, these approaches rarely address the deeper scientific question of why such conditions are appropriate. In the context of scientific discovery, understanding why is arguably more critical than merely predicting what. reliable system should not only propose solvent or temperature but also provide mechanistic justification: Which functional group governs the reactivity? What prior experimental evidence supports this choice? Which constraints exclude alternative reagents or solvents? Without such explanatory reasoning, models risk being opaque black boxes, limiting their utility in high-stakes scientific workflows. To tackle this challenge, we introduce ChemMAS, multi-agent system that treats condition selection as reasoning task grounded in chemical knowledge, mechanistic constraints, and peer deliberation. ChemMAS decomposes the problem into four collaborative stages. It first grounds chemical reactivity via mechanistic analysis, where general chemist agent parses SMILES to identify functional groups, balance stoichiometry, and infer plausible by-products. The system then retrieves condition exemplars through multichannel queries over structured reaction database. These candidates are refined via tournament-style elimination process, in which agent panels conduct pairwise comparisons using memory-informed multi-step reasoning. Finally, ChemMAS aggregates rationales for each decision by combining mechanistic plausibility, retrieved evidence, and constraint checks into interpretable justifications. By shifting from mere top-k ranking to interpretable, evidence-backed reasoning, ChemMAS offers new paradigm: one that is not only predictive but also justifiable, auditable, and suitable for closedloop experimentation. In our evaluation, ChemMAS outperforms specialized chemical models (e.g., RCR (Gao et al., 2018), Reagent Transformer (Andronov et al., 2023)) by 20-30% Top-1 accuracy and surpasses leading general-purpose LLMs (e.g., GPT-5, Gemini 2.5) by 10-15% on average, validating its effectiveness and robustness. Our contributions are threefold: We reformulate reaction condition recommendation as evidence-based chemical reaction condition reasoning, requiring models to output not only what-level conditions but also why-level evidence. We introduce ChemMAS, multi-agent system that couples chemistry-aware tool calling with multi-channel recall, multi-step mechanistic reasoning under constraint verification, and debate-based aggregation, producing interpretable, falsifiable condition reasoning. We benchmark ChemMAS against specialized chemical models and cutting-edge generalpurpose LLMs, showing state-of-the-art performance with up to 30-point gains in Top-1 accuracy and robust generalization across diverse condition types."
        },
        {
            "title": "2.1 PROBLEM DEFINITION",
            "content": "Unlike the existing reaction condition recommendation, we formalize evidence-based reaction condition reasoning as follows. An input reaction is = (R, P, I) with reactants R, products P, and optional context I. condition configuration is structured object C, where may mix discrete and continuous factors. The system returns configurations (cid:98)C = {c1, . . . , cK} and rationale for each ρ(c) = (M, S, E, Π) comprising domain reasoning , verifiable checks S, aligned evidence E, and concise derivation Π. Validity is Valid(cid:0)ρ(c); x(cid:1) = [Constr(S) Align(E; x, c) δ Coherent(Π, M, E)] . Here, Constr(S) is true when all hard checks in pass. Align(E; x, c) [0, 1] scores how well the evidence supports (x, c) using signals such as reaction-type matches, functional-group overlap, MCS alignment, or learned embeddings, with δ as fixed threshold. Coherent(Π, M, E) verifies that the derivation Π is logically consistent with the mechanistic summary and the evidence E. The indicator returns 1 only when all criteria hold. The objective is (1) max (cid:98)C, ρ (cid:88) (cid:98)C u(c; x) + λ Div( (cid:98)C) s.t. (cid:98)C = K, Valid = 1 c. (2) The first term accumulates success proxy over selected configurations, where may be calibrated yield predictor, feasibility score, or learned pairwise preference aggregator. The diversity term Div promotes coverage across condition dimensions to avoid mode collapse, λ controls the trade-off between utility and diversity. The constraints enforce fixed budget and require every selected configuration to be valid, upgrading recommendation to reasoning by demanding justified and verifiable outputs. Classical recommendation optimizes only. Our task requires each proposed to carry falsifiable, evidence-aligned certificate ρ(c). 2.2 OVERVIEW As illustrated in Figure 2, ChemMAS realizes the proposed reasoning framework through multistage agent-based pipeline, with intermediate representations stored in shared memory. The process begins with General Chemist that parses the input reaction (R, P) using domain-specific tools to extract mechanistic signals, align stoichiometry, and predict reaction type. Outputs are structured into Reaction Report written to memory. Condition hypotheses are generated via the Multi-Channel Recall module, which independently queries historical condition database using reaction type, reactant, and product features, followed by combinatorial synthesis into candidate sets of similar conditions. The Tournament Selection phase ranks these candidates through pairwise comparisons conducted by specialized agents, each focusing on one condition dimension (e.g., catalyst, solvent, reagent) under context-aware constraints. Finally, each agent engages in Multi-Step Reasoning over memory and retrieved evidence, and the Multi-Agent Debate aggregates these judgments via majority voting to produce verified configurations {c1, . . . , cK}, each paired with rationale ρ(c). 2.3 GENERAL CHEMIST Given chemical reaction specified by Reactant SMILES = {ri} and Product SMILES = {pj}, the General Chemist (AGen) extracts mechanistically informative priors for downstream condition prediction. The General Chemist agent orchestrates three tools, including Functional Group Tagger, Constraint Engine, and Chemical Knowledge Base, to (i) identify main functional groups, (ii) infer balanced stoichiometry and by-products, and (iii) retrieve reaction-type evidence. All outputs are written to Memory. Functional Group Tagger. curated library = {(namek, SMARTSk)} of common organic motifs (e.g., acyl chlorides, amines, alcohols, heteroaromatics) is used to match each ri via SMARTS substructure search, yielding F(ri). The union FR = (cid:83) F(ri) is then ranked by role salience considering electrophile/nucleophile tags, activation levels, and motif frequency across reactants. The top-ranked entries are designated as the Main FG set and stored in Memory with atom indices for downstream reference. 3 Figure 2: Architecture of ChemMAS. The left side shows how the General Chemist processes SMILES and Multi-Channel Recall retrieves reaction conditions from the Reaction Base. On the right, candidate conditions are paired and evaluated through Multi-Agent Debate, where four agents with Multi-Step Reasoning select the top-50 conditions via Tournament Selection. Constraint Engine. Reactant and product molecular graphs are canonicalized (including implicit hydrogens), aligned by maximum common substructure to derive an atom mapping. An integer linear program computes stoichiometric coefficients ν = (νR, νP , νaux). Changes on mapped atoms, combined with heuristic leaving-group rules, are used to enumerate neutral species B, from which the most parsimonious by-product hypothesis is selected. Both the balanced equation and consistency diagnostics are written to Memory. Chemical Knowledge Base. Query templates built from FR, product scaffolds, and molecular identifiers are used to retrieve supporting evidence from public repositories (e.g., PubChem) and locally indexed mirror. Retrieved exemplars and co-occurrence statistics yield signal features sckb = {stype, srole, sbyprod}, which support reaction type classification and by-product confirmation. The resulting labels, along with citation metadata, are stored in Memory for use in later reasoning stages. 2.4 MULTI-CHANNEL RECALL We maintain structured Reaction Base = {(τn, rn, pn, cn)}N n=1, where each entry contains the reaction type τn, molecular representations of reactants rn and products pn, and condition triple cn = (cat, sol, reag). Given the current reaction context (ˆτ , R, P) from Memory, we perform three parallel queries including type-, reactant-, and product-centric, to obtain candidate index sets St, Sr, Sp (exact type match for St, top-k nearest neighbors by functional-group, MCS, and embedding similarity for Sr and Sp). Without any scoring or rank fusion, an entry is admitted into Matched Conditions if it hits on any of the three tags. We define the unified retrieval result as the deduplicated union: Smatched = dedup(St Sr Sp) , (3) and collect {cn : Smatched} as experience-driven condition proposals. Optional feasibility filters, e.g., mass/charge balance, known by-product constraints, can be applied to screen out invalid 4 entries. To promote diversity, we construct Similar Conditions via applying controlled slot-level recombination Π(c) that replaces one or two elements of with high co-occurrence alternatives conditioned on (ˆτ , FR), while removing infeasible or near-duplicate combinations. The overall candidate pool is the truncated union: = truncate (cid:0)Smatched Ssimilar (cid:1), (4) which is forwarded to downstream selection and debate."
        },
        {
            "title": "2.5 CANDIDATE PAIRING AND TOURNAMENT SELECTION",
            "content": "We refine the initial pool of 5,000 Candidate Conditions into final Top-50 via tournament-style knockout that emphasizes head-to-head preference (Liu et al., 2025) under comparable context rather than brittle global scoring. Let = {ci}5000 i=1 . We apply random permutation π and form disjoint pairs (0) = {(cπ(1), cπ(2)), . . . , (cπ(4999), cπ(5000))}. In round t, each pair (a, b) (t) is adjudicated by an agent panel, and the winner is determined by majority vote: win(a, b) = arg max o{a,b} [dj = o], (cid:88) (5) with confidence-sum tie-break when necessary. Winners form (t) = {win(a, b)}, which is reshuffled and re-paired to yield (t+1) = pair(shuffle(W (t))). Iteration stops when (T ) = 50. We prefer this pairing-and-knockout protocol to global scoring since absolute scores are difficult to calibrate across heterogeneous condition sets and amplify noise in near-ties; head-to-head comparison avoids global calibration, anchors judgments in matched contexts, and affords linear-time selection with natural parallelism. 2.6 MULTI-AGENT DEBATE Multi-Step Reasoning. For candidate option {a, b}, each agent AF ull, ACat, ASol, ARea executes an evidence-seeking chain. The agent parses the Memory Reaction Report (main functional groups, by-product, reaction type) to extract keywords κj, queries the Chemical Knowledge Base to obtain support Θ(0) (o), and composes an initial assessment Initj(o) = LLM(cid:0)κj, Θ(0) (o), structured format(cid:1). (6) Across micro-rounds = 0, . . . , 1, the agent refines its stance by reading peer summaries from the conversation buffer and re-querying when uncertainty is detected: Dec(u+1) (cid:16) (o) = Φ Dec(u) (o), Peers(u), Θ(u+1) (cid:17) (o) , (7) where Φ() integrates new citations, Constraint-Engine checks (e.g., base required to capture HCl), and potential failure modes. Upon convergence or budget exhaustion, the agent outputs final decision dj {a, b} with rationale saved to Memory. Majority Voting. After each agent completes Multi-Step Reasoning for both and b, the panel engages in structured debate: agents post final assessments and key citations to shared Memory board, while designated facilitator enforces turn-taking and prompts resolution of conflicts (e.g., solvent polarity vs. nucleophile strength). The pairwise outcome is determined by majority voting as in win(a, b) = arg max o{a,b} (cid:88) [dj = o], (8) with confidence-sum tie-breaks if needed. The winning option advances to the next tournament round, losers are eliminated, and iterating over reshuffled winners progressively reduces the 5k candidates to the Top-50."
        },
        {
            "title": "3.1 CHEMICAL TEACHING",
            "content": "We adopt cold-start Supervised Fine-Tuning (SFT) recipe to endow the backbone LLM with initial Tool-Integrated Reasoning (TIR) (Dong et al., 2025) for chemical condition judgment. Given training pairs (xi, yi), we apply the standard Supervised Fine-tuning objective on the backbone model Pθ with parameters θ: L(θ) = log Pθ(yi xi), (9) (cid:88) (xi,yi) where xi denotes the input prompt containing reaction and paired candidate conditions, and yi is structured target consisting of (i) yr : step-wise chain that incorporates tool invocation logic and special tokens. (ii) ya : concise Judgement section that independently critiques each response and declares the preferred option. The reasoning trajectory integrates two types of tools, namely Chemical Knowledge Base searching and Memory searching, serialized in special formats (e.g., <search> ... </search> , <memory> ... </memory> ), enabling the model to learn the fundamental rules of tool invocation during the SFT process. Ultimately, this process yields cold-start LLM ˆπθ that learns when and how to invoke chemical tools, thereby establishing an initial capability for TIR in chemistry. 3.2 TOOL INCENTIVIZATION After obtaining the cold-start model ˆπθ via SFT, we apply tool incentivization RL to align the policy with both answer correctness and collaborative tool usage, obtaining πRL . θ Hierarchical Reward. Given valid format, we augment task accuracy Acc with multi-tool bonus rM when both tools appear (Dong et al., 2025), otherwise we down-weight: = max(Acc + rM , Acc), Format ok and Acc > 0, Format ok and Acc = 0, 0, 1, Otherwise, (cid:40) rM = 0.1, ( <search> & <memory> ), 0, otherwise. (10) This explicitly rewards combined tool use without sacrificing correctness. Tool-Incentivization RL. For each query and tool-augmented output o, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as our RL algorithm, which estimates the baseline using group of rollouts. Concretely, we sample rollouts {oi}G i=1, compute group-normalized advantages with group baseline, and optimize LGRPO(θ) = 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 where min(cid:0)ρi,t ˆAi,t, clip(ρi,t, 1 ϵ, 1 + ϵ) ˆAi,t (cid:1) β DKL (cid:2)ˆπθ ˆπref ρi,t(θ) = ˆπθ(oi,t q, oi,<t) ˆπold(oi,t q, oi,<t) , (cid:3) , (11) (12) ϵ controls PPO clipping, β weights the KL regularization to the fixed reference ˆπref , and ˆAi,t denotes the advantage normalized with respect to the group baseline."
        },
        {
            "title": "4 EXPERIMENTAL SETTINGS",
            "content": "4.1 TRAINING PIPELINE All agents in ChemMAS are initialized from the same backbone, Qwen3-8B-Instruct, and are trained under unified Two-stage Multi-tool Collaborative Training Framework that applies SFT and RL; 6 Figure 3: Two-stage Multi-tool Collaborative Training Framework of ChemMAS. Chemical Teaching uses SFT for cold-start training, enabling the LLM to master TIR, and Tool Incentivization employs RL to align the models policy with both answer correctness and collaborative tool usage. while the optimization protocol is shared, the learning objectives and accessible tools differ across agents. We independently trained two distinct models: one for the AGen, and another for the multiagent system comprising AF ull, ACat, ASol, ARea. More training details are in the Appendix. 4.2 DATASETS We curate private dataset of organic reactions, consisting of 544,591 entries represented as reaction equations in SMILES format. For each entry, the reactants and products are defined as the input, while the reaction conditions, including catalyst1, solvent1, solvent2, reagent1, and reagent2, are defined as the output. Based on this setting, we construct questionanswer pairs and split the dataset into training, validation, and test sets with ratio of 8:1:1."
        },
        {
            "title": "5 RESULTS AND DISCUSSIONS",
            "content": "5.1 MAIN RESULTS We assessed our proposed method, ChemMAS, against selection of current models. We compared with specialized chemical models including RCR (Gao et al., 2018), Reagent Transformer (Andronov et al., 2023), and MM RCR (Zhang et al., 2024b), which represent the latest advances in reaction-specific prediction. In addition, we benchmarked against general-purpose large language models (LLMs), such as Qwen3-235B-A22B (Yang et al., 2025), GPT5 (OpenAI), Claude 3.7 Sonnet (Anthropic, 2024), DeepSeek-R1 (Guo et al., 2025), and Gemini2.5-Pro (Comanici et al., 2025), which epitomize the cutting edge in general reasoning and knowledge transfer. As shown in Table 1, ChemMAS surpasses both specialized chemical models and state-of-the-art LLMs across all reaction types and Top-k settings. It achieves relative Top-1 accuracy improvements ranging from 70% to over 90% when compared to domain-specific baselines such as RCR, Reagent Transformer, and MM RCR. Even against top-tier general-purpose LLMs like GPT-5 and Gemini 2.5-Pro, ChemMAS yields consistent relative gains of 1525% in Top-1 accuracy, underscoring its strength in fine-grained mechanistic reasoning. ChemMAS achieves strong Top-5 accuracies across all reaction types, ranging from 83.2% to 93.9%, and peaking on solvent1. Its particularly strong performance on challenging categories such as catalyst and solvent2 where many baselines struggle to surpass 65% Top-1 accuracy, further highlights the benefits of its multi-agent coordination and domain-aware tool integration. These results demonstrate that ChemMAS not only outperforms domain-specific systems, but also exceeds the capabilities of cutting-edge general LLMs. 7 Table 1: Top-k accuracy (%) across five types of reaction conditions: catalyst, solvent1, solvent2, reagent1, reagent2. We report accuracy at {1, 5, 10}, corresponding to Top-1, Top-5, and Top10 ranks. The proposed ChemMAS achieves the best performance across all settings. Model Top-k Accuracy (%) Catalyst Solvent1 Solvent2 Reagent Reagent2 1 5 10 1 10 1 5 10 1 10 1 5 10 Pretrained Models RCR 40.3 52.6 60.7 49.9 62.1 68.5 45.3 52.8 60.3 50.1 56.2 63.3 36.4 43.3 44.9 Reagent Transformer 35.3 49.3 56.6 38.2 46.3 52.3 37.7 46.4 54.3 46.3 61.3 64.2 37.9 40.1 47.2 43.4 60.1 75.9 53.7 70.7 73.7 49.3 56.3 65.6 55.7 65.2 71.6 40.2 56.3 59.6 MM RCR Zero-shot LLMs Qwen3-235B-A22B 55.4 75.2 77.9 64.0 70.6 73.7 48.4 58.6 64.2 68.3 76.2 82.7 44.2 57.7 60.2 62.7 74.2 83.2 73.7 83.7 86.2 65.9 74.3 83.6 67.2 86.9 90.1 68.4 84.9 86.1 GPT5 43.6 52.9 60.1 46.0 55.7 58.7 39.2 45.7 53.9 52.3 63.9 67.1 46.2 52.3 54.7 Claude3.7-Sonnet 52.8 69.4 73.2 67.2 73.5 78.1 45.2 54.9 62.2 60.4 71.4 75.7 53.6 67.6 72.3 DeepSeek-R1 63.4 79.4 80.5 68.0 83.6 86.4 63.1 74.0 78.6 64.3 82.6 90.1 63.7 76.8 82.2 Gemini2.5-Pro ChemMAS 78.1 92.3 96.3 85.4 93.9 96.9 76.3 83.2 93.1 88.3 93.6 94.3 73.6 85.2 87.7 Table 2: Ablation on different components in ChemMAS. The best and second-best results are bolded and underlined. Method Top-k Accuracy (%) Catalyst Solvent 1 Solvent 2 Reagent 1 Reagent 2 5 10 1 5 10 5 10 1 5 10 5 10 Memory w/o Main FG w/o By-Product w/o Reaction Type 66.7 82.6 87.6 65.9 76.3 82.7 63.1 70.5 76.8 64.1 76.9 87.6 60.7 65.7 72.3 70.3 88.4 90.1 78.4 84.1 89.6 69.7 76.0 85.9 74.5 82.8 90.1 68.2 74.9 81.6 74.6 88.6 92.5 82.4 91.6 93.8 73.8 78.6 86.9 81.6 90.3 92.0 70.0 78.1 85.3 w/o Multi-Agent Debate 65.7 77.9 80.1 66.2 74.1 80.3 58.3 68.2 74.6 62.9 75.6 80.1 52.6 62.0 69.8 Framework w/o Multi-Step Reasoning 62.4 79.8 83.5 70.5 79.3 87.5 62.5 72.5 81.3 69.1 84.3 87.2 61.3 72.5 79.8 74.1 89.7 92.6 81.6 90.1 92.5 72.8 80.4 89.8 84.2 89.3 91.5 71.4 79.4 82.8 w/o Candidate Pairing ChemMAS 78.1 92.3 96.3 85.4 93.9 96.9 76.3 83.2 93.1 88.3 93.6 94.3 73.6 85.2 87.7 5.2 ADDITIONAL QUANTITATIVE ANALYSIS Ablation Studies. We conducted an ablation study to analyze the contribution of different components in ChemMAS. The ablation settings are as follows: (1) w/o Main FG, w/o By-Product, and w/o Reaction Type denote removing the corresponding elements from the Memory module; (2) w/o Multi-Agent Debate replaces multi-agent collaboration with single-agent reasoning process, thereby eliminating conversational exchanges; (3) w/o Multi-Step Reasoning removes the iterative evidence-based reasoning chain within each agent, such that agents can only rely on prior knowledge and inter-agent debate without tool invocation; (4) w/o Candidate Pairing discards the pairwise elimination mechanism for candidate conditions, instead applying global scoring and ranking procedure to directly select the top-50 candidates. As illustrated in Table 2, removing key components leads to substantial performance drops, underscoring their critical role in ChemMAS. Specifically, removing Main FG from the Memory module results in significant decrease in performance, with an average drop of +8.4% across all reaction conditions, highlighting the crucial role of functional group extraction and analysis in reaction condition prediction. Similarly, removing Multi-Step Reasoning causes an average accuracy decrease of 12.3%, underscoring the importance of evidencebased multi-round reasoning. To evaluate the effectiveness of each stage in our training framework, we conducted an ablation study focusing on SFT and RL. As illustrated in Table 3. The results demonstrate that removing either the SFT or RL stage leads to clear degradation in Top-k Accuracy across all reaction condition types. Notably, excluding SFT results in slightly larger drop compared to removing RL, under8 Table 3: Ablation on the SFT and RL. The best and second-best results are bolded and underlined. Top-k Accuracy (%) Training Framework Catalyst Solvent 1 Solvent 2 Reagent 1 Reagent 2 1 10 1 5 10 1 10 1 5 10 1 10 w/o RL w/o SFT 70.6 88.3 90.4 82.6 89.4 90.5 71.2 80.4 88.5 84.1 87.5 90.2 70.2 82.3 84.5 67.9 84.3 90.5 81.3 84.6 88.4 72.6 78.1 87.4 79.2 83.5 91.9 67.7 80.9 83.2 SFT+RL 78.1 92.3 96.3 85.4 93.9 96.9 76.3 83.2 93.1 88.3 93.6 94.3 73.6 85.2 87.7 Figure 4: Multi-agent ablation: Top-1 accuracy improvements across Catalyst, Solvent1/2, and Reagent1/2 when adding specialized agents on top of AGen+AF ull. scoring the importance of supervised fine-tuning in establishing strong initialization for subsequent reinforcement learning. These findings highlight the necessity of the two-stage training framework, where SFT and RL play complementary roles in achieving optimal performance. Analysis of Multi-Agent Collaboration. To assess the utility and synergy of different agents, we evaluate combinations built on the base AGen+AF ull, which are listed in Figure 4. Introducing specialized agents yields improvements. Specifically, ACat enhances performance on Catalyst, with an average Top-1 increase of 8.5%. ASol shows strong contributions on Solvent1/2, with an average Top-1 gain of 11.6%. ARea provides the largest gains on Reagent1/2, with an average Top-1 increase of 18.4%. When all three specialized agents are incorporated, the full system achieves macroaverage Top-1 increase of 1619% across all condition types. These results show that the specialized agents contribute substantial, domain-aligned improvements, and multi-agent debate is conducive to enhancing overall performance. For the analysis of Top-5 and Top-10, see the Appendix."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce ChemMAS, multi-agent system that reframes reaction condition recommendation as the evidence-based reasoning task, grounded in domain-specific chemistry knowledge, mechanistic constraints, and interpretable evidence. Unlike prior models that focus solely on prediction, ChemMAS explains why each condition is appropriate, enhancing trust, generalization, and scientific utility. Empirically, ChemMAS achieves up to 30% improvements in Top-1 accuracy over specialized chemical models and consistently outperforms leading general-purpose LLMs. These results affirm the importance of transitioning from black-box predictions to evidence-based, auditable decision-making in scientific AI. In future work, we envision extending this agent-based reasoning framework to broader scientific domains such as materials design, bioinformatics, and physical simulation, where interpretability and mechanistic grounding are equally critical."
        },
        {
            "title": "REFERENCES",
            "content": "Rizvi Syed Aal Ali, Jiaolong Meng, Muhammad Ehtisham Ibraheem Khan, and Xuefeng Jiang. Machine learning advancements in organic synthesis: focused exploration of artificial intelligence applications in chemistry. Artificial Intelligence Chemistry, 2(1):100049, 2024. Mikhail Andronov, Varvara Voinarovska, Natalia Andronova, Michael Wand, Djork-Arne Clevert, and Jurgen Schmidhuber. Reagent prediction with molecular transformer improves reaction data quality. Chemical Science, 14(12):32353246, 2023. Anthropic. Claude 3.7 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-7-sonnet. Matt Ball, Dragos Horvath, Thierry Kogej, Mikhail Kabeshov, and Alexandre Varnek. Predicting reaction conditions: data-driven perspective. Chemical Science, 2025. Daniil Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570578, 2023. Andres Bran, Theo Neukomm, Daniel Armstrong, Zlatko Jonˇcev, and Philippe Schwaller. Chemical reasoning in llms unlocks steerable synthesis planning and reaction mechanism elucidation. arXiv preprint arXiv:2503.08537, 2025. Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, Jiezhong Qiu, Jianzhang Pan, Yi Huang, et al. Chemist-x: Large language model-empowered agent for reaction condition recommendation in chemical synthesis. arXiv preprint arXiv:2311.10776, 2023. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410, 2025. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In International Conference on Machine Learning, 2023. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Conference on Empirical Methods in Natural Language Processing, December 2022. Hanyu Gao, Thomas Struble, Connor Coley, Yuran Wang, William Green, and Klavs Jensen. Using machine learning to predict suitable conditions for organic reactions. ACS Central Science, 4(11):14651476, 2018. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 1076410799, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Idil Ismail, Raphael Chantreau Majerus, and Scott Habershon. Graph-driven reaction discovery: progress, challenges, and future opportunities. The Journal of Physical Chemistry A, 126(40): 70517069, 2022. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 10 Lars Benedikt Kaesberg, Jonas Becker, Jan Philip Wahle, Terry Ruas, and Bela Gipp. Voting or consensus? decision-making in multi-agent debate. arXiv preprint arXiv:2502.19130, 2025. Joshua Ong Jun Leang, Aryo Pradipta Gema, and Shay Cohen. Comat: Chain of mathematically annotated thought improves mathematical reasoning. arXiv preprint arXiv:2410.10336, 2024. Minhyeok Lee, Umit Ucak, Jinyoung Jeong, Islambek Ashyrmamatov, Juyong Lee, and Eunji Sim. Automated and efficient sampling of chemical reaction space. Advanced Science, 12(9): 2409009, 2025. Tiantao Liu, Zheng Cao, Yuansheng Huang, Yue Wan, Jian Wu, Chang-Yu Hsieh, Tingjun Hou, and Yu Kang. Syncluster: reaction type clustering and recommendation framework for synthesis planning. JACS Au, 3(12):34463461, 2023. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Pairjudge rm: Perform best-of-n sampling with knockout tournament. arXiv preprint arXiv:2501.13007, 2025. George Lyall-Brookes, Alex Padgham, and Anna Slater. Flow chemistry as tool for high throughput experimentation. Digital Discovery, 2025. Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, and Philippe Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6(5):525535, 2024. Michael Maser, Alexander Cui, Serim Ryou, Travis DeLano, Yisong Yue, and Sarah Reisman. Multilabel classification models for the prediction of cross-coupling reaction conditions. Journal of Chemical Information and Modeling, 61(1):156166, 2021. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. ACM Transactions on Intelligent Systems and Technology, 16(5):172, 2025. OpenAI. Gpt-5 system card. URL https://openai.com/index/ gpt-5-system-card/. Yujie Qian, Zhening Li, Zhengkai Tu, Connor Coley, and Regina Barzilay. Predictive chemistry augmented with text retrieval. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Conference on Empirical Methods in Natural Language Processing, pp. 1273112745, December 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, et al. Chemagent: Self-updating library in large language models improves chemical reasoning. arXiv preprint arXiv:2501.06590, 2025. Connor Taylor, Alexander Pomberger, Kobi Felton, Rachel Grainger, Magda Barecka, Thomas Chamberlain, Richard Bourne, Christopher Johnson, and Alexei Lapkin. brief introduction to chemical reaction optimization. Chemical Reviews, 123(6):30893126, 2023. Zhengkai Tu, Thijs Stuyver, and Connor Coley. Predictive chemistry: machine learning for reaction deployment, reaction development, and reaction discovery. Chemical science, 14(2): 226244, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiaorui Wang, Chang-Yu Hsieh, Xiaodan Yin, Jike Wang, Yuquan Li, Yafeng Deng, Dejun Jiang, Zhenxing Wu, Hongyan Du, Hongming Chen, et al. Generic interpretable reaction condition predictions with open reaction condition datasets and unsupervised learning of reaction center. Research, 6:0231, 2023. 11 Junde Wu, Jiayuan Zhu, Yuyuan Liu, Min Xu, and Yueming Jin. Agentic reasoning: streamlined framework for enhancing llm reasoning with agentic tools. In Annual Meeting of the Association for Computational Linguistics, pp. 2848928503, 2025. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip Yu. IEEE transactions on neural networks and comprehensive survey on graph neural networks. learning systems, 32(1):424, 2020. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. Cut the crap: An economical communication pipeline for llm-based multi-agent systems. arXiv preprint arXiv:2410.02506, 2024a. Yu Zhang, Ruijie Yu, Kaipeng Zeng, Ding Li, Feng Zhu, Xiaokang Yang, Yaohui Jin, and Yanyan Xu. Text-augmented multimodal llms for chemical reaction condition recommendation. arXiv preprint arXiv:2407.15141, 2024b. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. Tianhang Zhou, Yingchun Niu, Xingying Lan, and Chunming Xu. Locally-deployed chain-ofthought (cot) reasoning model in chemical engineering: Starting from 30 experimental data. arXiv preprint arXiv:2502.12383, 2025. Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, et al. Multiagentbench: Evaluating the collaboration and competition of llm agents. arXiv preprint arXiv:2503.01935, 2025."
        },
        {
            "title": "Supplemental Material of ChemMAS",
            "content": "This document provides supplementary material to complement the main paper. It includes detailed descriptions of the ChemMAS system, prompt templates, training pipeline, additional experimental results, and reproducibility assets. Specifically: Appendix describes how large language models (e.g., GPT-5 and Google Nano Banana) were used in writing assistance and figure generation. Appendix summarizes related works in three areas: Appendix B.1: Reaction Condition Prediction Appendix B.2: LLM-Based Multi-Agent Systems Appendix B.3: LLM-Based Reasoning Models Appendix details the ChemMAS methodology, including: Appendix C.1: Multi-Agent Debate Procedure Appendix C.1: Two-Stage Multi-Tool Collaborative Training Appendix outlines the training and evaluation setups, including: Appendix D.1: Optimization settings for ChemMAS and baselines Appendix D.1: Agent-specific training targets and tool usage Appendix D.2: Evaluation protocol and candidate selection details Appendix D.3: Prompt design for different agent types Appendix presents additional experimental results and visualizations: Appendix E.1: Top-5 and Top-10 Accuracy Ablation Appendix E.2: Reaction prediction visualization table"
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS",
            "content": "In this work, the large language model GPT-5 was used as general-purpose tool for polishing the writing, including improving clarity and grammar. In Figure 2, the five images representing the agents and small tool icons were generated with the assistance of GPT-51, while the overall framework was created by the authors. The three images representing different models in Figure 3 were produced with the help of Google Nano Banana2. The conceptual design of both figures were entirely implemented by the authors."
        },
        {
            "title": "B RELATED WORKS",
            "content": "B.1 REACTION CONDITION PREDICTION Predicting reaction conditions from reactants and products is long-standing challenge in computeraided synthesis. Early large-scale efforts such as (Gao et al., 2018) used feedforward neural networks trained on millions of Reaxys records to jointly predict catalysts, solvents, reagents, and temperatures, achieving promising top-k accuracies despite sparsity and label imbalance. Focusing on crosscoupling families, (Maser et al., 2021) formulated the task as multi-label ranking, developing rolespecific encoders and leveraging graph-based features to yield accurate, context-aware predictions. To improve generalization and interpretability, (Wang et al., 2023) released benchmark datasets and proposed Parrot, Transformer model augmented with unsupervised reaction center learning. Parrot achieved significant gains in condition accuracy and temperature estimation while offering interpretable attention maps localized to reactive substructures. Separately, (Andronov et al., 2023) addressed data quality limitations by training Molecular Transformer to impute missing reagents in USPTO reactions. Their system not only improved reagent recall but also enhanced downstream product prediction models. 1https://chatgpt.com/ 2https://www.nano-banana.ai/ 13 Retrieval-augmented methods incorporate external knowledge to improve robustness. TextReact (Qian et al., 2023) pairs structure-based encoders with retrieved literature snippets to inform condition prediction and retrosynthesis. By integrating textual context into training, it significantly outperforms molecule-only baselines. In peptide catalysis design, (Edwards et al., 2022) proposed semi-automated ML framework for selecting universal catalyst libraries and discovered novel, high-selectivity peptides via efficient search in large tripeptide space. At the interface of language and chemistry, (Edwards et al., 2022) introduced MolT5, pre-trained encoder-decoder model that translates between molecules and natural language. It supports molecule-to-caption generation and chemically constrained text-to-molecule synthesis, offering foundation for LLM-based explainability. More recently, (Zhang et al., 2024b) proposed text-augmented multimodal LLM framework for reaction condition recommendation. Their method jointly encodes SMILES, molecular graphs, and relevant text to achieve state-of-the-art accuracy across open benchmarks and improve generalization under low-data or OOD settings. Despite these advances, current methods primarily focus on recommending what the potential reaction conditions are, but fail to provide explanatory why-level evidence for why such conditions are important or mechanistically justified. B.2 LLM-BASED MULTI-AGENT SYSTEMS LLMs are increasingly deployed as autonomous agents equipped with retrieval, reasoning, and tooluse capabilities. (Boiko et al., 2023) showcased early efforts in autonomous laboratory control, with LLM agents performing iterative web search, experimental planning, and execution. (M. Bran et al., 2024) extended this direction in chemistry by coupling GPT-4 with 18 specialized tools for retrosynthesis, property prediction, and literature search. The resulting system could autonomously complete multi-step syntheses and identify new chromophores. In reaction condition recommendation, (Chen et al., 2023) leveraged retrieval-augmented generation by combining molecular similarity search, literature parsing, and in silico condition evaluation, mimicking the workflow of expert chemists. To address hallucinations and unreliable reasoning, multi-agent collaboration has emerged as promising direction. (Du et al., 2023) proposed multi-agent debate framework where LLMs iteratively critique each others answers, leading to improved factuality and robustness. (Zhu et al., 2025) benchmarked agent interactions across collaborative and competitive settings, revealing that structured debate and agent role specialization improve task success. Recent work further explores coordination protocols. (Kaesberg et al., 2025) found that consensus-based decision-making outperforms majority voting on complex QA tasks, while (Zhang et al., 2024a) introduced compression pipeline that reduces inter-agent communication by up to 70% without degrading performance. (Wu et al., 2025) introduced Agentic Reasoning, general framework for LLMs to call sub-agents (e.g., web search, code execution, memory management), enabling long-horizon, tool-rich scientific workflows. Together, these systems demonstrate that combining LLMs with external tools, structured memory, and agent-level reasoning can produce scalable, verifiable pipelines for high-stakes domains. However, how to enhance the factuality and reliability of reaction condition prediction remains largely unexplored. B.3 LLM-BASED REASONING MODELS complementary line of work focuses on improving the reasoning capabilities of LLMs, which is essential for high-stakes decision-making and interpretability in scientific domains. In general contexts, program-aided language models (PAL) (Gao et al., 2023) execute intermediate logic through code to improve arithmetic and symbolic reasoning. CoT prompting, self-consistency, and debatestyle prompting have shown broad benefits in multi-step question answering. CoMAT (Leang et al., 2024) proposes mathematically annotated chain-of-thought mechanism to handle complex symbolic queries. MME-CoT (Jiang et al., 2025) benchmarks the reasoning abilities of large multimodal models across science, math, and logic domains. In chemistry, (Tang et al., 2025) introduces self-updating subtask library to facilitate memory-augmented chemical reasoning. It decomposes complex tasks into reusable subtasks and retrieves relevant solutions, enabling LLMs to generalize over time via experience. However, the ability to infer mechanistic or contextual rationales behind chemical reaction conditions is rarely addressed in existing works."
        },
        {
            "title": "C METHOD DETAILS",
            "content": "C.1 ALGORITHM OF CHEMMAS FRAMEWORK Multi-Agent Debate. In this section, we outline the overall workflow of our Multi-Agent Debate procedure. The process consists of two coordinated phases executed for each candidate pair, as illustrated in Algorithm 1 (see also the prompt specification in Figure 6): (1) Evidence-Seeking & Refinement. Given pair (a, b), each agent Aj initializes an evidenceseeking chain by parsing the Reaction Report (main functional groups, by-products, reaction type) to extract keywords, querying the Chemical Knowledge Base for citations, and composing an initial assessment. Across micro-rounds, agents iteratively refine their stance by (i) reading peer summaries from the shared buffer, (ii) re-querying the KB when uncertainty is detected, and (iii) invoking the Constraint Engine (e.g., verifying that bases are present to capture HCl). This yields final per-agent decision dj {a, b} with confidence and citations. (2) Panel Aggregation & Tournament. After convergence, all agents post their final assessments to the Memory board. The pairwise winner is determined by majority voting; ties are broken by the sum of confidences. Winners advance while losers are eliminated, and repeated rounds over reshuffled winners progressively reduce the pool to the Top-50. This debate-driven pipeline promotes crossagent verification, encourages tool-grounded reasoning, and produces interpretable, citation-backed outcomes archived in Memory. Two-Stage Multi-Tool Collaborative Training. In this section, we outline the overall workflow of our Two-Stage Multi-Tool Collaborative Training pipeline. The procedure alternates two phases over multiple cycles, as illustrated in Algorithm 2 (see also the prompt specifications in Figure 5 and Figure 6): (1) Chemical Teaching (SFT). Starting from the Qwen3-8B-Instruct backbone, we perform supervised fine-tuning on structured trajectories that serialize tool invocations (e.g., search, memory) before the final label. This phase teaches the model when and how to call tools and enforces standardized output format, yielding cold-start, tool-aware policy ˆπθ. (2) Tool Incentivization (RL). Initialized from ˆπθ, we optimize the policy with GRPO using hierarchical reward that jointly encourages (i) format validity, (ii) answer correctness, and (iii) collaborative multi-tool usage. For each query, the model samples tool-augmented rollouts; advantages are normalized with group baseline and regularized by KL term to frozen reference. Policy parameters are then updated to maximize the GRPO objective. This alternating scheme combines supervised teaching of tool protocols with reinforcement alignment for accuracy and collaboration, resulting in robust tool-aware reasoning model πRL θ with interpretable, consistent behavior."
        },
        {
            "title": "D EXPERIMENTAL SETTINGS",
            "content": "D.1 TRAINING PIPELINE For both AGen and the multi-agent system, we employ two-stage optimization strategy consistent with the main framework. In the SFT stage, the AdamW optimizer is used with β = (0.9, 0.95), an initial learning rate of 2 105, and weight decay of 0.1. Each model is trained for one epoch with batch size of 128. In the subsequent RL stage, we adopt the GRPO strategy with learning rate 1 106, KL coefficient 0.04, and number of iterations set to 1. To enhance diversity, we set the temperature parameter to 0.75 during generation. All training and inference are conducted on 8 NVIDIA A100 GPUs. General Chemist (AGen). The input is limited to Reactant and Product SMILES, and the output is the predicted Reaction Type. During SFT, the supervision target is structured as step-wise chain that explicitly serializes three tool invocationsFunctional Group Tagger, Constraint Engine, and Chemical Knowledge Base Searchingbefore emitting the final reaction type. This design enables 15 Algorithm 1 Multi-Agent Debate with Multi-Step Reasoning and Majority Voting Require: Agent set = {A1, . . . , Am}; Candidates C; Memory: Reaction Report (main fg, by product, reaction type); Chemical Knowledge Base (KB); Constraint Engine; Micro-rounds ; target K=50. pairwise tournament until Top-K form disjoint pairs winner or per-agent final outputs and confidences each agent reasons on both options micro-round refinement while > do DEBATEMATCH(a, b, A, ) MAJORITYVOTE(D) Cnext Cnext {o} PAIRSHUFFLE(C) Cnext for all (a, b) do Output: Top-K surviving candidates 1: function MAD TOURNAMENT(C, A, U, K) 2: 3: 4: 5: 6: 7: 8: 9: 10: end while 11: return 12: 13: end function 14: function DEBATEMATCH(a, b, A, ) 15: 16: 17: 18: 19: for all Aj do for all {a, b} do end for Cnext (o) QUERYKB(κj, o) κj EXTRACTKEYWORDS(Reaction Report) Θ(0) Dec(0) for = 0 to 1 do (o) COMPOSEINIT(κj, Θ(0) (o)) Peers(u) READPEERSUMMARIES(A {Aj}) if DETECTUNCERTAINTY(Dec(u) (o), Peers(u)) then 20: 21: 22: 23: 24: 25: 26: 27: 28: (o) QUERYKB(κj, o) (o) Θ(u) (o) Θ(u+1) else Θ(u+1) end if Γ(u+1) Dec(u+1) end for (dj, cj, citj) FINALIZE(Dec(U ) WRITETOMEMORYBOARD(Aj, dj, cj, citj) {(Aj, dj, cj)} (a), Dec(U ) end for 29: 30: 31: 32: 33: 34: end for 35: return 36: 37: end function 38: function MAJORITYVOTE(D) 39: 40: 41: 42: 43: 44: 45: 46: 47: end function na (cid:80) if na = nb then (Aj ,dj ,cj )D end if else return arg maxo{a,b}{no} sa (cid:80) sb (cid:80) return arg maxo{a,b}{so} (Aj ,dj ,cj )D cj [dj = a] (Aj ,dj ,cj )D cj [dj = b] [dj = a]; nb (cid:80) (Aj ,dj ,cj )D [dj = b] (o) CONSTRAINTCHECK(o, by product=HCl, base-needed, . . .) (o) UPDATEDECISION(Dec(u) (o), Peers(u), Θ(u+1) (o), Γ(u+1) (o)) (b)) store rationale/citations tie-break by confidence sum the model to learn when and how to call tools. In the subsequent RL stage, we apply hierarchical reward that integrates format correctness, answer accuracy, and collaborative multi-tool usage. Multi-Agent System (AF ull, ACat, ASol, ARea). These role-specialized agents share the same trained backbone and are SFT on QA pairs generated in the Candidate Pairing stage. The supervision targets embed the invocation logic of two toolsChemical Knowledge Base Searching and 16 Algorithm 2 Two-Stage Multi-Tool Collaborative Training Require: Datasets = {(xi, yi)}; External tools ( <search> , <memory> , ...); Instruction I; SFT epochs Esft; RL cycles C; steps per cycle S; rollouts G; GRPO hyper-parameters (ϵ, βKL); temperature τ ; optimizer config. Output: Trained policy πRL θ Stage I: Chemical Teaching (SFT) 1: Initialize backbone model πθ Qwen3-8B-Instruct /* cold-start tool-aware policy */ AdamW (β=(0.9, 0.95)), lr 2105, wd 0.1, batch 128 2: for = 1, . . . , Esft do 3: 4: Sample minibatch Compute SFT loss Lsft(θ) = (cid:80) (x,y)Blog πθ(y x) ( <search> , <memory> ) contains step-wise chain + tool tokens Update θ θ ηθLsft(θ) 5: 6: end for 7: Freeze SFT checkpoint as reference ˆπref stopgrad(πθ); set ˆπθ πθ Stage II: Tool Incentivization (RL with GRPO) 1: for = 1, . . . , do 2: 3: 4: 5: 6: 7: Sample batch Db for all Db do Sample rollouts with tools at temperature τ : {oj}G For each oj, compute reward R(oj) with hierarchical scheme: for = 1, . . . , do j=1 πθ( q, ) /* align accuracy & tool use */ RL cycles optimization steps per cycle Format: if invalid R(oj) 1 Accuracy: Acc(oj) {0, 1} Multi-tool bonus: rM =0.1 if ( <search> & <memory> ) appear, else 0 Final: if format ok, R(oj)= max(Acc(oj)+rM , Acc(oj)) Compute group-normalized advantages { ˆAj,t} w.r.t. group baseline Optimize GRPO objective: 8: 9: LGRPO(θ) = 1 G (cid:88) j=1 1 oj oj (cid:88) t=1 min(cid:0)ρj,t ˆAj,t, clip(ρj,t, 1ϵ, 1+ϵ) ˆAj,t (cid:1) βKL DKL (cid:2)πθ ˆπref (cid:3) Update θ θ + η θLGRPO(θ) end for 10: 11: 12: 13: end for 14: return πRL end for θ Memory Searching. The RL stage employs the same reward design to align both judgment quality and tool collaboration, ensuring that agents can deliberate effectively while remaining tool-aware. D.2 EVALUATION DETAILS We evaluate general-purpose LLMs in controlled candidate-ranking regime aligned with the ChemMAS pipeline. Directly prompting models with only Reactant and Product SMILES yields an excessively large decision space, leading to chemically plausible yet inaccurate suggestions and Top-1 accuracy of approximately 5%. To obtain faithful assessment, for each reaction high-recall pool is first constructed via Multi-Channel Recallaggregating reaction-base retrieval, functional-group cues, constraint heuristics, and memory lookupto produce Top-5000 candidate set spanning Catalyst, Solvent1, Solvent2, Reagent1, and Reagent2. Each model ranks within the same 5k pool and outputs Top-50 list per head. All models receive identical candidate sets, instructions, and judgment interfaces, and are not permitted to modify the pool, ensuring that differences reflect discriminative ranking and evidence integration rather than retrieval coverage. This protocol mitigates search-space inflation, reduces hallucination, and provides an evaluation setting consistent with the workflow of the framework. D.3 PROMPT TEMPLATES As shown in Figure 5 and Figure 6, there are prompts for the different agents. Beyond the systemlevel instruction, the prompt is organized into four parts. First, the Tool Definition specifies the invocation schema of tools together with their expected outputs. Second, the Interaction Protocol describes how the agent should interleave tool calls with reasoning traces using XML-style tokens, and how the final answer must be returned in structured format. Third, the Task Prompt clarifies the objectives. Finally, the Output Format enforces JSON schema that standardizes the prediction into fields such as reaction type, main functional groups, by-products, and evidence. This structured prompt design enables the model to understand tool usage, maintain consistent reasoning procedure, and produce verifiable outputs."
        },
        {
            "title": "E RESULTS AND DISCUSSIONS",
            "content": "E.1 ADDITIONAL QUANTITATIVE RESULTS Top-5 Analysis. As shown in Figure 7, introducing specialized agents consistently improves Top5 Accuracy over the AGen + AF ull baseline. ACat delivers targeted gains on Catalyst (+10.1%), aligning with its role specialization. ASol contributes the most on solvents, improving Solvent1 and Solvent2 by +16.4% and +13.4%, respectively. ARea yields the largest boosts on reagents (e.g., Reagent1/2 with gains around +18.7% and +13.9%). When specialized agents are combined (e.g., +Cat+Sol, +Sol+Rea, +Cat+Rea), the improvements remain additive and stable across condition types, and the Full System shows the most consistent Top-5 lift across all five heads, indicating effective collaboration among role-specialized experts. Top-10 Analysis. As shown in Figure 8, the same trend holds for Top-10 Accuracy. ACat most strongly benefits Catalyst (+13.1%). ASol provides clear gains on Solvent1/2 (e.g., +10.8% and +13.6%). ARea again dominates on Reagent1/2 with sizeable increments (e.g., +17.2% and +9.8%). Pairwise combinations further enhance coverage across heads, and the Full System achieves the highest Top-10 metrics in macro sense, evidencing that multi-agent collaboration scales beyond single-head expertise and produces robust gains under larger candidate sets. E.2 RESULT VISUALIZATION To better illustrate the performance of our framework, we visualize several representative reactions with both predicted and ground-truth conditions. As shown in Table 4, the predicted conditions generally align well with the ground-truth, especially for solvents and reagents that are strongly correlated with the transformation patterns in the reaction. For example, in reactions involving polar functional groups, the model consistently identifies appropriate polar solvents such as alcohols or cyclic ethers. Similarly, in palladium-catalyzed cross-coupling reactions, the model reliably predicts the use of palladium-based catalysts, demonstrating its ability to capture mechanistic priors from training data. In cases where the predictions slightly deviate from the ground-truth, the model often proposes chemically reasonable alternatives. For instance, different bases such as potassium carbonate and cesium carbonate are interchangeable under similar conditions, and solvents like ethanol and methanol can play analogous roles. These substitutions highlight the models flexibility in generating valid yet diverse solutions, reflecting its capacity to generalize beyond exact memorization of training examples. Overall, the visualization confirms that the framework not only achieves high top-k accuracy but also produces predictions that are chemically interpretable and robust. The ability to provide both exact matches and plausible alternatives underscores the potential of our approach for assisting chemists in condition selection and experimental design. 18 Figure 5: Prompt for General Chemist 19 Figure 6: Prompt for Multi-Agent System Figure 7: Multi-agent ablation: Top-5 accuracy improvements across Catalyst, Solvent1/2, and Reagent1/2 when adding specialized agents on top of AGen+AF ull. Figure 8: Multi-agent ablation: Top-10 accuracy improvements across Catalyst, Solvent1/2, and Reagent1/2 when adding specialized agents on top of AGen+AF ull. 21 Table 4: Visualization of several reactions with predicted (blue) vs. ground-truth (red) labels. Reactions Catalyst 1 (Pred / GT) Solvent 1 (Pred / GT) Solvent 2 (Pred / GT) Reagent 1 (Pred / GT) Reagent 2 (Pred / GT) AcOH AcOH Toluene Toluene EtOH EtOH Bromine Bromine TEA TEA Chloride Chloride NaOH NaOH Palladium Palladium MeOH MeOH THF THF MeCN MeCN THF THF AIBN AIBN K2CO3 K2CO Platinum Platinum THF THF TEA TEA Pyridine Pyridine Toluene Toluene EtOH MeOH K2CO3 Cs2CO3 H2O H2O NaOEt NaOMe"
        }
    ],
    "affiliations": [
        "Hangzhou Dianzi University",
        "Shanghai Artificial Intelligence Laboratory",
        "Tsinghua University",
        "University of Oxford"
    ]
}