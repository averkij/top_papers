{
    "paper_title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
    "authors": [
        "Junyuan Zhang",
        "Qintong Zhang",
        "Bin Wang",
        "Linke Ouyang",
        "Zichen Wen",
        "Ying Li",
        "Ka-Ho Chow",
        "Conghui He",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 2 9 5 2 0 . 2 1 4 2 : r OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation Junyuan Zhang1 Qintong Zhang1 Bin Wang1 Linke Ouyang1 Zichen Wen1,4 Ying Li5 Ka-Ho Chow3 Conghui He1 Wentao Zhang2 1Shanghai AI Laboratory 2Peking University 3The University of HongKong 4Shanghai Jiaotong University 5Beihang University"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG. To better understand OCRs impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https: //github.com/opendatalab/OHR-Bench 1. Introduction Retrieval Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowlThese authors contributed equally to this work. Project leader. Corresponding author (heconghui@pjlab.org.cn). edge [12, 18], enabling them to respond accurately to queries beyond their training corpus, such as recent news or proprietary content, and reducing hallucinations [17, 18, 27]. This is achieved through retrieval-then-grounding approach, where relevant documents are retrieved from external knowledge bases and incorporated into the LLMs prompt for grounding. As an essential component of RAG systems, the knowledge base defines the scope and quality of documents that RAG can access. Given that vast amount of realworld knowledge resides in unstructured documents, such as scanned PDFs, constructing an external knowledge base often relies on Optical Character Recognition (OCR) to parse structured data from these unstructured PDF documents [14, 39]. For instance, MinerU [32] takes raw PDFs as input and extracts plain text, formulas, and tables into structured formats for subsequent RAG applications. However, imperfect predictions of OCR and non-uniform representations of parsing results impair the construction of high-quality knowledge base for RAG. To be specific, despite advancements in OCR [2, 32, 34], even the leading model cannot achieve perfect accuracy across all scenarios [22, 39]. Furthermore, structural data can inherently be parsed in non-uniform representation, such as Markdown or LaTeX. These issues introduce OCR noise in parsing results and diminish the quality of the knowledge base. Considering RAG is sensitive to input noise [7, 10, 36], recent works race on downstream RAG components, including more precise retrievers [4, 19, 24] and more advanced LLMs [1, 8, 10, 37]. However, the quality of OCR-based external knowledge bases and its cascading impact on these downstream RAG components have received less attention, which highlights critical but unaddressed gap: the absence of benchmarks to assess OCRs cascading impact on each component and entire system of RAG. Existing benchmarks either evaluate RAG holistically without fine-grained assessment [40], or consider limited OCR solutions without consideration of noise they introduce [11, 14]. To fill this gap, we introduce OHRBench, 1 question-answering benchmark designed to evaluate OCRs cascading impact on each component and entire systems of RAG in two ways. First, OHRBench constructs documentbased RAG Q&A dataset. It encompasses complex unstructured PDF documents from six RAG real-world application areas, including Textbook, Law, Finance, Newspaper, Manual, and Academia. As shown in Tab. 1 and Fig. 2, more than 1800 pages of these PDFs contain multimodal elements, such as tables and formulas, or feature complex layouts, posing challenges for constructing high-quality knowledge bases for RAG systems. For questions, OHRBench provides diverse set of Q&A pairs, with evidence sourced from diverse varied document elements, making it an ideal testbed for assessing the OCRs impact on RAG performance. Second, OHRBench identifies two primary OCR noise types: Semantic Noise, resulting from prediction errors, and Formatting Noise, arising from non-uniform document element representation. By deriving samples from current OCR results and systematically perturbing ground truth structured data based on these samples, OHRBench generates set of perturbed structured data with varying degrees of Semantic Noise and Formatting Noise that occur in OCR results, enabling exploration of the quantitative relationship between OCR noise and RAG performance. With OHRBench, we first conduct comprehensive benchmark on current OCR solutions, including pipelinebased OCR systems [25, 32], end-to-end OCR models [2, 34] and Vision-Language Models for OCR [5, 33]. We reveal that even the best OCR solutions exhibit performance gap of 7.5% at least, compared to the ground truth structured data, facilitating the importance of mitigating OCR noise in RAG systems. Further experiments on different types of OCR noise uncover that Semantic Noise consistently exert significant impact, while Formatting Noise affects specific retrievers and LLMs differently. offering valuable insights for developing RAG-tailored OCR solutions and noise robust models. Moreover, we also discuss employing VisionLanguage Models (VLMs) in the RAG generation stage without OCR. We find that inputting image only cannot reach the performance of using OCR text, but by simply combining these two inputs, VLM can improve the performance by up to 24.5% and approach the performance of the ground truth text baseline, indicating its promising potential of applying VLMs in RAG systems. Contributions. We summarize our main contributions: We present OHRBench, question-answering benchmark designed to evaluate the impact of OCR on RAG systems. OHRBench includes various unstructured PDF documents from six RAG application areas and Q&As sourced from multimodal document elements, posing challenges to the employment of current OCR solutions in RAG systems. We introduce two primary types of OCR noise in OHRBench: Semantic Noise and Formatting Noise. We offer perturbed structured data with varying levels of each noise type, facilitating further exploration of RAG-tailored OCR solutions on RAG systems. We conduct comprehensive evaluation of current OCR solutions and reveal that none of their extracted structured data is competent for constructing high-quality knowledge bases for RAG systems. Moreover, we provide systematic analysis of the fine-grained and cascading impact of these two OCR noises on RAG performance, facilitating future development of RAG-tailored OCR solutions. 2. Related Works 2.1. Retrieval-Augmented Generation Retrieval-Augmented Generation (RAG) [17, 18, 27] integrates external knowledge into large language models (LLMs) to mitigate hallucinations. Although RAG technology enhances the generation capabilities of LLMs, it is notably sensitive to input noise. InfoRAG [36] characterizes this noise in RAG as incorrect and irrelevant content within retrieved text and reveals its impact on RAG performance. RAAT [10] further expands noise into relevant noise, counterfactual noise, and irrelevant types. However, these studies focus solely on chunk-level noise introduced during the retrieval stage and its effect on generation capabilities of LLMs, leaving the impact of noise derived from OCR results unexplored. GARAG [7] examines typographical errors, form of OCR noise, but its scope is limited to plain text using only synthetic data, overlooking the variety of OCR noise encountered in real-world RAG applications. In this paper, we reveal the impact of noise introduced during the OCR stage, offering comprehensive analysis of its impact on RAG systems. 2.2. Document parsing with OCR OCR-based document parsing is promising solution for structured data extraction from unstructured documents, facilitating applications like RAG. Current OCR solutions can be summarized into three categories, pipeline-based systems [25, 32], end-to-end models [2, 21, 34] and employing VLMs for OCR [5, 15, 33] Pipeline-based systems decompose OCR into multiple subtasks such as layout detection, text, formula, and table recognition, enabling fine-grained data extraction. End-to-end models take document images as input and output the overall recognition result in an endto-end manner. Due to the achievement of VLMs on visual understanding, recent works have explored its application in OCR [22]. In this paper, we evaluate these OCR paradigms, examining their suitability for RAG applications across diverse, real-world document domains. Figure 1. Construction of OHRBench and evaluation protocol. (1) Benchmark Dataset: we collect PDF documents from six domains, extract human-verified ground truth structured data, and generate Q&As derived from multimodal document elements. (2) RAG Knowledge Base: OCR Processed Structured Data for benchmarking current OCR solutions and Perturbed Structured Data for assessing the impact of different OCR noise types. (3) Evaluation of OCR impact on each component and the overall RAG system. 2.3. Benchmark and evaluation of Retrievalof each component."
        },
        {
            "title": "Augmented Generation",
            "content": "Frameworks like RAGAS [9] and ARES [29] propose evaluating RAG systems based on context relevance, answer faithfulness, and answer relevance, using LLMs or fine-tuned discriminators for measurement. RGB [3] assesses the noise robustness, negative rejection, information integration, and counterfactual robustness of RAG in news data. MultiHopRAG [30] focuses on multi-hop reasoning capabilities, while ClashEval [35] explores the context preference in conflicting evidence scenarios. However, these evaluations target specific components of RAG systems, and none of them discusses the impact of external knowledge base construction on RAG systems. Although UDA [14], VisRAG [38] and M3DocRAG [6] explore RAGs effectiveness in document analysis, they consider limited OCR solutions and lack analysis of different OCR noise types. In this paper, we introduce OHRBench to comprehensively investigate OCR noises impact on RAG systems. 3. OHRBench Our OHRBench consists of 1) number of unstructured PDF documents from six real-world RAG applications and Q&A pairs derived from multimodal elements in these documents, and 2) perturbed structured data based on ground truth with varying degrees of OCR noises. Fig. 1 illustrates the construction of OHRBench. We will now delve into the details 3.1. Data collection According to [20, 39], extracting structured data from multimodal document elements like formulas and tables poses significant challenges to current OCR solutions. Considering the practical application scenarios of RAG and the challenging field of OCR, we compile PDF document collection representing six common RAG application scenarios: Textbook, Law, Finance, Newspaper, Manual, and Academia. This collection encompasses diverse array of documents from both existing datasets and public web resources. For Law and Finance, we randomly sample documents from CUAD [13] and FinanceBench [16]. Documents in CUAD have complex layouts, while FinanceBench provides documents with large and complex tables. Manual documents are expanded from [23] using ManualsLib 1 to incorporate variety of multimodal elements. For Academia, we gather papers from Arxiv 2, and for Textbook, we source content from LibreTexts 3 and public resources, with particular focus on formulas. For Newspaper, we collect New York Times from pages 4 following [40], covering dates from 2024-05-01 to 2024-08-31. For each collected document, we provide ground truth structured data. Specifically, we begin with 1https://www.manualslib.com/ 2https://arxiv.org 3https://libretexts.org/ 4https://static01.nyt.com/images/ 3 Statistic Documents - Domain - Total Pages - Avg.Tokens - Avg.Data Type Number 350 6 4012 482.61/page 1.45/page Questions Avg.Question Token Avg.Answer Token 4598 18.56 7.91 (Evidence Source) - Text - Formula - Table (Answer Format) - Short Text Answer - Judgment - Numeric - Formula 3334 (72.5%) 872 (19.0%) 392 (8.5%) 2482 (54.4%) 849 (18.5%) 894 (19.4%) 354 (7.7%) Table 1. Dataset Statistics Figure 2. The layout of documents in OHRBench is complex. Each number indicates the count of PDF pages with that attribute. Criteria for these attributes can be found in Appendix Sec. II parsing all documents using Mathpix 5 for structural data extraction. We then ask expert-level annotators to revise the results, ensuring fidelity to the original structure and content of PDFs while mitigating any style deviations from Mapthix. Detailed descriptions can be found in Appendix Sec. II. 3.2. Question-answer pair generation To systematically assess the impact of OCR results on RAG performance, our Q&A generation approach revolves around the diversity of evidence sources and realistic questionanswer types representativeness for retrieval-augmented generation. Specifically, we provide the ground truth structured data of each document page to GPT-4o for generating Q&A based on multimodal document elements, including plain text, tables, and formulas. Each Q&A consists of the following fields: one page of the original PDF document, evidence context from ground truth structured data that provides the answer to the question, type of evidence (plain text, table, and formula), and the question and answer which are both derived from this evidence context. In this way, these Q&As can serve as testbed for evaluating OCR results on multimodal document elements. To ensure relevance to real-world RAG applications, we require that each question include at least one specific entity name, minimizing ambiguity. The prompt template for the Q&A generation is provided in Appendix Sec. I. Quality Control. For each Q&A, we first instruct GPT-4o to answer the question without access to the ground truth evidence context. This step automatically filters out questions that are answerable without retrieval. Next, We manually review and eliminate questions with ambiguous references, such as \"How many models are compared in Table 3?\", as these are challenging for RAG systems to resolve without clear context information. Finally, we ensure that each Q&A pairs type of evidence aligns with the actual source context content. This multi-step quality control process helps 5https://mathpix.com/ guarantee that the Q&A dataset meets the requirements of diverse evidence sources and practical RAG applications. 3.3. Data perturbation with OCR noise Despite advancements in OCR, real-world applications often encounter document types beyond the training corpus of OCR models, leading to low-quality data extraction. Additionally, the inherent non-uniform representation of document elements further introduces noise, affecting the performance of each component in RAG systems. In this paper, we primarily focus on two primary types of OCR noises: Semantic Noise and Formatting Noise. To systematically explore these noises, we start from errors in current OCR results and generate perturbed data with different noise levels based on the ground truth structured data. We will now delve into the details of each type. Semantic Noise results from OCR prediction errors, including typos, misrecognized formula symbols, as well as content and structural errors in tables. This type of noise impacts the semantics of parsed content, deviating retrievers and LLMs from integrating correct information related to user queries. As its appearance is irregular, we introduce this type of noise by providing examples and adopting GPT-4o to mimic it. Specifically, we begin with gathering real examples from current OCR results. We divide them into three categories according to their edit distance to the ground truth, and prompt GPT-4o to modify ground truth structured data in three levels based on these examples. The perturbations are across all document elements, including plain text, table, and formula. As illustrated in Fig. 3, minor perturbations on the table include typos, while moderate perturbations disrupt structure by adding or misaligning rows and columns, which makes it more complicated for retrieval and LLMs perception. Severe perturbations produce tables with control character omission, which cannot be rendered correctly, simulating OCR recognition breakdowns. For plain text and formula, misspelling, visually similar characters, and truncation are introduced in different proportions to produce three 4 overall system performance. For the retrieval stage, we utilize knowledge bases derived from the same domains of user queries. During the generation stage, we provide the page where the question is derived from for LLMs to generate the response. In the overall evaluation, retrievers retrieve the relevant chunks from the knowledge base in the same domain as the question, and LLMs generate responses based on these chunks. In the overall evaluation, we provide the top-1 matched chunk for generation unless otherwise stated. The default chunk size is 1024 with no overlap. Metrics. To evaluate the quality of OCR results, we calculate the edit distance between each page of OCR results and the ground truth structured data and report the average values. For assessing retrieval performance, as results of different OCRs often include various extraneous characters, discriminating whether the evidence exactly appears in the retrieved contents is not fair. Following [14], we employ Longest Common Subsequence (LCS) [26] to measure evidence inclusion in retrieved content. For the generation stage, we employ the F1-score and Exact-Match metrics to measure the accuracy of LLMs responses. Retrievers. We consider two primary retrievers: (1) BGEM3 [4], recent SOTA dense retriever within its size category. (2) BM-25 [28, 31] lightweight sparse retriever ranking document based on the query term frequency. LLMs. We employ two representative open-source LLMs: Qwen2 (Qwen2-7B-Instruct) [33] and Llama-3.1 (Llama3.18B-Instruct) [8] and the proprietary LLM: GPT-4o (gpt4o-2024-08-06). standard prompt template is used to format responses consistently across all LLMs (see Appendix Sec. I). All open-source models are downloaded from Huggingface 6, with inference conducted on 8 NVIDIA A100 GPUs. 4.2. Benchmarking current OCR solutions In this section, we evaluate the suitability of current OCR solutions for real-world RAG applications by conducting comprehensive experiments with our OHRBench. We involve several representative OCR solutions including (1) Pipeline-based OCR, such as MinerU [32] and Marker [25], (2) End-to-end OCR, including GOT [34] and Nougat [2], and (3) Vision-Language Models, specifically Qwen2-VL72B [33] and InternVL2-Llama3-76B [5]. For GOT, we employ its format OCR mode to output structured data. For Qwen2-VL-72B and InternVL2-Llama3-76B, we prompt them to produce formulas and tables in LaTeX format, with the prompt template available in Appendix Sec. I. We use PyMuPDF to convert the original PDF page to an image and use the default input resolution of each OCR solution. To ensure fair edit distance calculations, tables from different OCR results are converted into LaTeX format. The retrievers used are BGE-M3 and BM25, while the LLMs are 6https://huggingface.co/ Figure 3. Illustration of different levels of Semantic Noise on plain text, equation, and table, which are all perturbed based on existing OCR results. different levels of noise. Formatting Noise refers to formatting commands used for style rendering, such as white space characters for beautifying formulas and bold and italic commands for better readability, and different representations of structured data like Markdown and LaTeX tables. Although irrelevant to semantics, this noise complicates information integration for both retrievers and LLMs. To assess the impact of Formatting Noise on RAG, we review its presence in current OCR outputs and develop rules to replicate it through addition, removal, and format conversion strategies. For plain text, perturbations range from token-level changes (e.g., bold, italic, and underline) to paragraph-level adjustments (e.g., title and reading order shuffles). In formulas and tables, we focus on extraneous elements like \"quad\" in formula and \"hline\" in tables, and equivalent symbols such as \"mathbf{}\", \"boldsymbol{}\", and \"mathbb{}\". Please refer to Appendix Sec. II for details of the characters and conversion rules involved. By varying the proportions of these changes in ground truth structured data, we create three sets of perturbed data with different degrees of Formatting Noise. Finally, for multiple formats of tabular data, we examine RAG performance with format conversion to Markdown, LaTeX, and HTML. 4. Experiments 4.1. Experimental settings We evaluate the impact of OCR on RAG systems in three ways: retrieval performance, generation performance, and 5 OCR Retrieval Generation Overall Edit Distance LCS@1 LCS@5 EM F1 EM@1 F1@1 - 63.53 86.22 33.54 50.19 26.42 39. Ground Truth Pipeline-based OCR MinerU [32] Marker [25] End-to-end OCR GOT [34] Nougat [2] 0.2328 0. 0.2884 0.3303 52.53 56.94 45.80 44.77 53.16 42.43 73.61 78.53 30.50 30. 46.08 46.02 24.52 23.89 36.84 36.51 67.06 61.46 26.36 24.81 40.62 37. 21.51 20.40 32.69 30.89 72.97 57.51 26.72 20.74 41.23 32.89 23.45 20. 35.91 31.23 Vision-Language Model for OCR Qwen2-VL-72B [33] InternVL2-Llama3-76B [5] 0.2564 0.4450 Table 2. Evaluation of various OCR solutions and their impacts on RAG systems. We report the retrieval performance using top-1 (LCS@1) and top-5 (LCS@5) retrieved chunks. Overall performance is presented with top-1 (EM@1, F1@1) and top-5 (EM@5, F1@5) retrieved chunks. Bold indicates the best performance, and underline indicates the second-best performance. LLama-3.1-8B-Instruct and Qwen2-7B-Instruct. All metrics are averaged across domains and combinations of retrievers and LLMs. Details of experimental results are available at Appendix Sec. III.1. Through the comparison presented in Tab. 2, we derive several key conclusions about the performance of these OCR solutions and their corresponding impacts on RAG systems, as follows: (1) Pipeline-based OCR demonstrates the best performance. Employing Marker achieves the best retrieval performance across all OCR solutions, while MinerU dominates the generation and overall evaluation. This is due to large number of complex layouts in our document datasets, and pipeline-based OCR can perform fine-grained local recognition based on the layout detection results, thereby large number of complex layouts in our document datasets, and pipeline-based OCR can perform fine-grained local recognition based on the layout detection results, better-supporting issues such as reading orders and dense text. (2) The best solutions of end-to-end OCR and VLMs can achieve comparable performance. GOT and Qwen2-VL-72B come out on top in end-to-end OCR and VLMs, respectively, with Qwen2-VL-72B being the closest to the performance of pipeline-based OCR and achieving sub-optimal results in LCS@1. The main reason why they do not perform as well as pipeline-based OCR is that they have difficulty handling complex layouts, especially for Newspaper, where all end-to-end OCR and VLMs have serious performance degradation. (See Appendix Sec. III.1 for detailed results on each domain) Meanwhile, they also struggle with out-of-training data distribution and hallucination issues. Nougat is designed for academic papers and achieves good performance, but it is difficult to generalize to other fields. InternVL2-Llama3-76B, on the other hand, struggles with hallucinations and generates large number of repetitive and nonsensical characters when handling images with dense text. (3) All OCR solutions suffer performance loss. Even the best solutions show decrease of 1.9 in EM@1 and 2.93 F1@1 in the overall evaluation, with greater losses in the retrieval and generation stages. In summary, current OCR solutions struggle to ensure robustness and effectiveness across diverse real-world RAG application scenarios. Moreover, the OCR metric of edit distance does not always correlate with RAG performance. For example, although MinerU and Qwen2-VL-72B exhibit lower edit distances compared to Marker, they do not consistently achieve better performance across all metrics. This discrepancy may be attributed to the varying types of OCR noise introduced by different solutions. To further investigate, we systematically explore the impact of these OCR noise types on RAG in Sec. 4.3. 4.3. In-depth analysis of OCR noises impact on"
        },
        {
            "title": "RAG",
            "content": "In this section, we conduct an in-depth analysis of the impact of Semantic Noise and Formatting Noise on RAG systems, with our perturbed structured data derived from ground truth. We employ edit distance to quantify these perturbations, with value of 0 representing ground truth structured data. 4.3.1. Fine-grained impact on retrieval and generation Semantic Noise significantly influences both retrieval and generation phases. As illustrated in Fig. 4, increasing perturbation levels from mild to severe results in nearly 50% performance decline for most retrievers and LLMs. In the retrieval stage, the sparse retriever BM25 initially outperforms the dense retriever BGE-M3 on ground truth structured data. However, as perturbation intensifies, BM25s performance eventually falls below BGE-M3, indicating its vulnerabil6 Figure 4. Impact of Semantic Noise ([S] dashed lines) and Formatting Noise ([F] solid lines) on RAG components. The horizontal axis denotes the edit distance, where higher values indicate increased OCR noise. We report LCS@1 and F1-score for each evidence source, including text (first column), table (second column), formula (third column), and all sources combined (last column). ity to Semantic Noise. Notably, for table-related questions, BGE-M3 shows 10% improvement with mild perturbation compared to the ground truth structured data. This may relate to mild perturbation involving only plain text and formulas, leaving tables unaltered, thus minimizing the impact on table-related queries. In the generation phase, both proprietary and open-source LLMs struggle with Semantic Noise, among which performance on table questions decreases the most. For instance, GPT-4o experiences 50% drop in F1score on table-related questions, indicating the importance of correctly parsing table structure. Formatting Noise primarily affects questions related to multimodal elements. While performance on plain text queries shows slight variation, multimodal queries, especially those involving formulas, experience significant degradation. The maximum performance drops are 16.8% for BM25 and 19.4% for Qwen2-7B in retrieval and generation, respectively. Table questions see declines of 9.9% (BM25) and 18.4% (Llama3.1-8B), respectively. GPT-4o is relatively resilient, with only 6% reduction in performance for both formula and table questions, indicating that more advanced LLMs can effectively handle Formatting Noise. 4.3.2. Impact on end-to-end evaluation Semantic Noise consistently demonstrate strong impact, while Formatting Noise affects specific retrievers and LLMs differently. All retrievers and LLMs struggle with Semantic Noise, leading to consistent performance drops, particularly in multimodal questions involving formulas and tables. In contrast, the effect of Formatting Noise is more variable, depending on the specific bottlenecks within the system. Despite decrease in retrieval performance with increasing perturbations, when employing Llama3.1-8B for generation, the overall performance shows slight change due to its limited information integration capabilities. Conversely, overall performance using GPT-4o is more sensitive to changes in the retrieval stage. For table-related questions, improvements in BGE-M3s performance under mild perturbations lead to enhanced results with GPT-4o. However, in formula-related questions, performance deteriorates to the level of Qwen2-7B as the retrievers performance decreases. In summary, Semantic Noise significantly affects each stage of RAG and the entire system. The impact of Formatting Noise varies with different retrievers and language models, particularly affecting questions related to multimodal elements. 7 Model Context Input GPT-4o Qwen2-VL-7B InternVL2-8B GT Text OCR Text Image Image+OCR Text GT Text OCR Text Image Image+OCR Text GT Text OCR Text Image Image+OCR Text Generation F1 EM 50.60 45. 39.26 49.97 48.70 43.19 44.09 48.13 44.72 41.22 31.10 38.31 70.66 64. 56.28 70.05 66.58 61.04 59.63 63.48 56.12 52.53 51.16 56.43 Figure 5. Performance of retrieval, generation and end-to-end with different table format. We only report the results of tablerelated questions. Other experimental results can be found in Appendix Sec. III.1. Table 3. Performance of employing VLMs for generation. For context input, GT Text represents ground truth structured data, OCR Text represents the OCR processed data from MinerU, and Image refers to the PDF image containing the clue. 4.3.3. Impact of table format In addition to perturbations, we investigate the influence of different table formats, as kind of Formatting Noise. As shown in Fig. 5, HTML tables show inferior performance during retrieval compared to Markdown and LaTeX formats, less than half the performance of Markdown tables. Markdown and LaTeX formats perform similarly, with BGE-M3 demonstrating better understanding of Markdown. During the generation phase, the impact of table formats varies among LLMs. For example, Qwen2-7B performs better with HTML tables, whereas GPT-4o maintains consistent performance across all formats, with maximum F1-score variation of only 2.1. In end-to-end evaluations, affected by low retrieval performance, HTML tables result in the lowest across nearly all retriever and LLM combinations, while the combination of GPT-4o and BM25 using Markdown achieves the best performance. 4.4. Potential of employing Vision-Language Models in RAG systems. Vision-Language Models (VLMs) have emerged as promising solution for document understanding tasks without relying on OCR [6, 11, 23]. In this section, we evaluate the efficacy of employing VLMs in the generation stage of RAG systems. We evaluate four types of context inputs: ground truth text, OCR text, image, and combination of image and OCR text, using MinerU for OCR due to its superior performance in Tab. 2. We keep the prompt templates consistent, with slight input format adjustments (see Appendix Sec. I). Results are shown in Tab. 3. (1) The performance of imageonly input is still lagging behind inputting OCR results. For GPT-4o and InternVL2-8B, using images alone yields lower EM-scores and F1-scores compared to OCR text. Although Qwen2-VL-7B achieves comparable performance with image input, it is still inferior to the performance using ground truth text. This indicates that current VLMs are not yet capable of fully replacing OCR-based RAG systems. (2) Combining multimodal inputs enhances VLM performance. When both image and OCR text are fed, GPT-4o, Qwen2-VL-7B, and InternVL2-8B show consistent performance improvements, with F1-score increases of 24.5%, 6.5% and 10.3% respectively, compared to image-only input. Notably, the EM value of GPT-4o and Qwen2-VL-7B, and the F1-score of GPT-4o and Internvl2-8B, achieve performance comparable to ground truth text. This suggests that the detailed text information compensates for VLMs challenges with complex documents that may be beyond their training corpus. These findings highlight that employing OCR results is orthogonal to employing VLMs, facilitating future research to explore VLMs potential in RAG systems. 5. Conclusion In this paper, we present OHRBench to evaluate the impact of OCR on RAG systems, which encompasses diverse unstructured PDF documents from six RAG applications scenarios along with Q&A pairs derived from multimodal elements in these documents. Through comprehensive evaluations of current OCR solutions, we reveal that none is fully capable of RAG systems across all scenarios. Furthermore, our analysis of different types of OCR noise demonstrates that while no retrievers and LLMs are immune to Semantic Noise, more advanced models exhibit greater resilience to Formatting Noise. We also discuss the 8 employment of VLMs to avoid the requirement of OCR and its potential to benefit from combining image and OCR results as inputs. We believe that our document and Q&A datasets as well as perturbed data with varying levels of noise will advance future the development of OCR solutions tailored for RAG and OCR noise-resistant RAG systems."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. In The Twelfth International Conference on Learning Representations, 2024. 1, 2, 5, 6 [3] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1775417762, 2024. 3 [4] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multifunctionality, multi-granularity text embeddings through selfknowledge distillation. arXiv preprint arXiv:2402.03216, 2024. 1, 5 [5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 2, 5, 6 [6] Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. M3docrag: Multi-modal retrieval is what you need for multi-page multi-document understanding. arXiv preprint arXiv:2411.04952, 2024. 3, 8, 1 [7] Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, and Jong Park. Typos that broke the rags back: Genetic attack on rag pipeline by simulating documents in the wild via low-level perturbations. arXiv preprint arXiv:2404.13948, 2024. 1, [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 5 [9] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023. 3 [10] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training. arXiv preprint arXiv:2405.20978, 2024. 1, 2 [11] Manuel Faysse, Hugues Sibille, Tony Wu, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. arXiv preprint arXiv:2407.01449, 2024. 1, 8 [12] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. 1 [13] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlp dataset for legal contract review. In Advances in Neural Information Processing Systems, 2021. [14] Yulong Hui, Yao Lu, and Huanchen Zhang. Uda: benchmark suite for retrieval augmented generation in real-world document analysis. arXiv preprint arXiv:2406.15187, 2024. 1, 3, 5 [15] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2 [16] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. 3 [17] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143, 2023. 1, 2 [18] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459 9474, 2020. 1, 2 [19] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. 1 [20] Zichao Li, Aizier Abulaiti, Yaojie Lu, Xuanang Chen, Jia Zheng, Hongyu Lin, Xianpei Han, and Le Sun. Readoc: unified benchmark for realistic document structured extraction. arXiv preprint arXiv:2409.05137, 2024. [21] Chenglong Liu, Haoran Wei, Jinyue Chen, Lingyu Kong, Zheng Ge, Zining Zhu, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Focus anywhere for finegrained multi-page document understanding. arXiv preprint arXiv:2405.14295, 2024. 2 [22] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 1, 2 [23] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523, 2024. 3, 8 [24] Gabriel de Souza Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. Nvretriever: Improving text embedding models with effective 9 Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [38] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, et al. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594, 2024. 3 [39] Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Wentao Zhang, and Conghui He. Document parsing unveiled: Techniques, challenges, and prospects for structured information extraction. arXiv preprint arXiv:2410.21169, 2024. 1, 3 [40] Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. Docbench: benchmark for evaluating llm-based document reading systems. arXiv preprint arXiv:2407.10701, 2024. 1, 3 hard-negative mining. arXiv preprint arXiv:2407.15831, 2024. 1 [25] Vik Paruchuri. Marker, 2024. 2, 5, 6 [26] Mike Paterson and Vlado Danˇcík. Longest common subIn International Symposium on Mathematical sequences. Foundations of Computer Science, pages 127142. Springer, 1994. 5 [27] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Incontext retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316 1331, 2023. 1, [28] Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. Nist Special Publication Sp, 109:109, 1995. 5 [29] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023. 3 [30] Yixuan Tang and Yi Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391, 2024. 3 Im- [31] Andrew Trotman, Antti Puurula, and Blake Burgess. provements to bm25 and language models examined. In Proceedings of the 19th Australasian Document Computing Symposium, pages 5865, 2014. 5 [32] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024. 1, 2, 5, 6 [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 5, 6 [34] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. 1, 2, 5, [35] Kevin Wu, Eric Wu, and James Zou. How faithful are rag models? quantifying the tug-of-war between rag and llms internal prior. arXiv preprint arXiv:2404.10198, 2024. 3, 1 [36] Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, and Jie Zhou. Unsupervised information refinement training of large language models for retrievalaugmented generation. arXiv preprint arXiv:2402.18150, 2024. 1, 2 [37] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, 10 OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "I. Instruction Prompts Attribute Counts Description Q&A Generation Prompt Template. The template is shown in Tab. S3. Following [35, 40], we instruct GPT4o to generate questions with clear entities and require three levels of difficulty for question diversity. RAG Generation Stage Prompt Template. The prompt template for LLMs and VLMs with text-only input is shown in Tab. S4. Templates for image-only input and combined image and OCR text input are provided in Tab. S5 and Tab. S6, respectively. Vision-Language Models OCR Prompt Template. We tune the prompt for best performance of VLMs OCR, by comparing simple and detailed instructions as shown in Tab. S7. Results in Tab. S9 indicate that the detailed prompt consistently performs better across all evaluations, so it is used by default. Multi-Elements Pages Hard Reading Order 1025 Complex Formula 1924 Complex Table High Text Density 1003 Pages containing at least two types of elements, such as text, images, tables, and formulas. Pages not presented in single-column format. Pages with formulas requiring at least 10 tokens for description. Pages with tables having more than 4 rows or columns, or containing merged cells. Pages with more than 800 tokens (separated by spaces). Table S2. Counts and description of each attribute which is present in our datasets II. Benchmark Construction Details II.3. Ground truth structured data annotation II.1. Document details We collect 350 PDFs with total of 4012 pages, of which 1370 pages are used for Q&As generation and the rest as part of the knowledge base. Each document is manually reviewed by primary authors to ensure its availability for academic use. Detailed domain statistics are shown in Tab. S1 Domains PDFs Pages Pages with Q&As Law Finance Textbook Manual Newspaper Academia Total 60 12 133 15 116 10 350 918 1733 133 932 116 204 4012 505 235 133 225 116 156 1370 Table S1. Document statistics of each domain II.2. Document with complex layout Although existing RAG document benchmarks have gathered PDFs from different domains [6, 11, 14], they often ignore the challenges posed by OCR. Therefore, we compile PDFs with different challenging attributes in this benchmark. Specifically, we categorize these into five attributes: MultiElements Pages, Hard Reading Order, Complex Formula, Complex Table, and High Text Density. Detailed descriptions of each category are provided as in Tab. S2. We annotate the ground truth structured data in Mathpix Markdown format, where tables and formulas are in LaTeX and images are excluded. To ensure the quality of annotation, we begin with using Mathpix to pre-annotate all PDFs. We then remove non-semantic formatting commands to ensure unbiased ground truth for all OCR solutions. Finally, the primary authors employ Mathpix Markdown previews7 to render structured data into PDFs, manually review and correct pre-annotated results. II.4. Rules for Formatting Noise introduction To introduce Formatting Noise, we define perturbation rate to control its extent. In order to match the level of Semantic Noise (measured by similar edit distance), we set the = {0.1, 0.3, 0.6}, indicating the three levels of perturbation: mild, moderate, and severe. Based on the Formatting Noise in the existing OCR results, we formulate the following rules to perturb plain text, tables, and formulas respectively. II.4.1. Plain text Text Style: Given the plain text content of the ground truth, we randomly divide it into sequence where each item consists of 2 to 5 words, select target items based on r, and apply one of the following operations as perturbations. Bold: Enclose the selected text in ** or textbf{}. Italic: Enclose the selected text in * or textit{}. 7https : / / github . com / Mathpix / vscode - mathpix - markdown 1 0.1.0-base model (350M). All prompt templates can be found in Sec. I. For all LLMs and VLMs, we set the temperature to 0 with do_sample=False by default for reproducibility. III.2. Details in different domains Tabs. S10 to S12 present the performance of OCR solutions in retrieval, generation, and overall RAG systems across domains. The results indicate that no single OCR solution consistently outperforms others in all domains. Figs. S1 to S3 illustrate the impact of OCR noise across domains. Notably, in the finance domain, which contains many tables, as mild perturbation does not affect tables, the performance improve slightly. Underline: Enclose the selected text in _ or underline{}. Title Formatting: We identify short sentences that end with full stop and have no more than 5 words as potential headings. We randomly pick them according to and add one of level 1 to level 3 title controls in Markdown (#) or LaTeX (section{}) to make new titles. Paragraph: To simulate the line breaks that exist in PDFs, we randomly insert at word intervals based on r. II.4.2. Formula Formula Conversion: Randomly convert the inline formula into block formula and vice versa at rate r. Extraneous Elements: We first randomly select the target formulas based on r. Subsequently, for each target formula, we randomly insert 1 to 5 meaningless markers in its symbol gaps, including , quad, qquad, ;, :. Equivalent Symbols: For each formula, we replace the following equivalent symbols with probability r: bold: mathbf{}, boldsymbol{}. cursive: mathbb{}, pmb{}, mathrsfs{}, euscript{}, mathcal{}. unicode: (sigma,u03A3), etc8. II.4.3. Table Row and Column Lines: For each line and column, randomly insert hline or cline with probability r. Cell Content: For each cell content, randomly apply above rules for plain text or formula with probability r. II.5. Rules for Semantic Noise introduction To introduce Semantic Noise, we use samples from existing OCR results. For plain text and formula, samples are taken from MinerU, Nougat, and InternVL2-Llama3-76B. For the table, samples are from MinerU version 0.7.0b19. To ensure diverse perturbation results, we collect 5 groups of samples and randomly select one each time. The prompt template in Tab. S8 is used to generate perturbed structured data with OCR noise. Fig. S4 presents case. III. Additional Experimental Results III.1. Experimental details For MinerU, we use version 0.9.210 by default. For Marker, version 0.2.1711 is employed. For Nougat, we utilize its 8Full lists are drawn from https://raw.githubusercontent. com/w3c/xml-entities/refs/heads/gh-pages/unicode. xml 9https : / / github . com / opendatalab / MinerU / tree / magic_pdf-0.7.0b1-released 10https : / / github . com / opendatalab / MinerU / releases/tag/magic_pdf-0.9.2-released 11https : / / github . com / VikParuchuri / marker / releases/tag/v0.2.17 2 System: Given the following document, please generate three RAG-style question-answer pairs based on the document with different levels of difficulty: Easy, Medium, and Hard. RAG-style refers to question that needs to be answered by retrieving relevant context from an external document based on the question, so the question MUST obey the following criteria: 1. The question must contain all information and context/background necessary to answer without the document!!! Do not include phrases like \"according to the document\" in the question! 2. The question must not contain any ambiguous references, such as he, she, it, the report, the paper, and the document! You MUST use their complete names! In your output, include the phrase from the document that contains the answer to the question as context. This phrase MUST be copied verbatim, word for word, from the document. You must produce the context phrase exactly from the text, with no modifications or truncations. You MUST obey the following criteria: - The question MUST be detailed and be based explicitly on information in the document. - The question MUST include at least one entity. - The context sentence the question is based on MUST include the name of the entity. For example, an unacceptable context is \"He won bronze medal in the 4 100 relay\". An acceptable context is \"Nils Sandström was Swedish sprinter who competed at the 1920 Summer Olympics.\" - The answer form should be as diverse as possible, including [Yes/No, Numeric, Formula, Short Answer]. - The context sentence, which is the evidence source for the question, should be as diverse as possible, including [text, table, formula]. If there are no possible questions that meet these criteria, return None as the question. Output the question in JSON format. Example Input Format: <Begin Document>...<End Document> Example Response: \"Question\": \"Who was the commanding general of the Union Army during the American Civil War?\",\"Answer\": \"Ulysses S. Grant\", \"Context\": \"As commanding general, Ulysses S. Grant led the Union Army to victory in the American Civil War in 1865.\",\"Difficulty Level\": \"Easy\" , \"Answer Form\": \"Short Answer\", \"Evidence Source\": \"text\" User: <Begin Document>{document}<End Document> Table S3. Q&A Generation Prompt System: You are an expert, you have been provided with question and documents retrieved based on that question. Your task is to search the content and answer these questions using the retrieved information. You **MUST** answer the questions briefly with one or two words or very short sentences, devoid of additional elaborations. Write the answers within <response></response>. User: Question: {question} Retrieved Documents: {retrieved_documents} Table S4. LLMs prompt for RAG generation 3 System: You are an expert, you have been provided with question and document image retrieved based on that question. Your task is to answer the question using the content from the given document image. You **MUST** answer the questions briefly with one or two words or very short sentences, devoid of additional elaborations. Write the answers within <response></response>. User: Question: {question} Table S5. Prompt template for VLM generation with image-only inputs System: You are an expert, you have been provided with question, document image, and its OCR result retrieved based on that question. Your task is to search for the content and answer these questions using both the retrieved information and the document image. You **MUST** answer the questions briefly with one or two words or very short sentences, devoid of additional elaborations. Write the answers within <response></response>. User: Question: {question} Retrieved Documents: {retrieved_documents} Table S6. Prompt template for VLM generation with image+text inputs Simple Prompt: Please do OCR on the image and give all the text content in markdown format. The formula should be wrapped in $$ and the table should be parsed in LaTeX format. Only output the OCR results without any extra explanations or comments. Detailed Prompt: You are powerful OCR assistant tasked with converting PDF images to Markdown format. You MUST obey the following criteria: 1. Plain text processing: - Precisely recognize all text in the PDF image without making assumptions in Markdown format. 2. Formula Processing: - Convert all formulas to LaTeX. - Use $ $ to wrap inline formulas. - Use $$ $$ to wrap block formulas. 3. Table Processing: - Convert tables to LaTeX. - Use begin{table} end{table} to wrap tables. 4. Figure Handling: - Ignore figures from the PDF image; do not describe or convert images. 5. Output Format: - Ensure the Markdown output has clear structure with appropriate line breaks. - Maintain the original layout and format as closely as possible. Follow these guidelines strictly to ensure accurate and consistent conversion. Only output the OCR results without any extra explanations or comments. Table S7. Prompt template for VLMs OCR 4 System: You are able to mimic the various errors introduced in OCR recognition. You are given examples with ground truth and OCR results with errors. Your job is to refer to the examples and generate three samples with increasing perturbations based on the user input. The new perturbed document could include the following OCR errors: 1. formatting issues, character recognition errors (like misrecognized letters or numbers), and slight variations in wording. 2. Redundant content and typos for plain text and formula. 3. Randomly add and delete table structure controlling character \"&\" for tabular data. 4. Randomly add extra columns and rows for tabular data. 5. Randomly make misaligned columns and rows for tabular data. - In the first perturbed document, make mild perturbation with 10% changes. - In the second perturbed document, make moderate perturbation with 30% changes. - In the third perturbed document, make severe perturbation with 50% changes. Output the three perturbed documents within <response></response>. You should refer to the following perturbed example to generate the perturbed document. Example ground truth: {gt_sample} Example results with mild perturbation: {mild_sample} Example results with moderate perturbation: {moderate_sample} Example results with severe perturbation: {severe_sample} User: <Begin Document>{gt_structured_data}<End Document> Table S8. Prompt template for Semantic Noise introduction Retrieval Generation Overall LCS@1 LCS@5 EM F1 EM@1 F1@1 Simple Prompt Qwen2-VL-72B InternVL2-Llama3-76B 52.50 41.23 72.27 57.31 26.29 20.07 40.65 32. 23.09 20.03 35.30 30.40 Detailed Prompt Qwen2-VL-72B InternVL2-Llama3-76B 53.16 42.43 72.97 57. 26.72 20.74 41.23 32.89 23.45 20.58 35.91 31.23 Table S9. Effects of Different OCR Prompts on VLMs for OCR. The detailed prompt is used due to its consistently superior performance."
        },
        {
            "title": "Nougat",
            "content": "Qwen2-VL-72B InternVL2 -Llama3-76B LCS@1 LCS@5 LCS@1 LCS@5 LCS@1 LCS@5 LCS@1 LCS@5 LCS@1 LCS@5 LCS@1 LCS@5 LCS@1 LCS@5 Law 64.31 90.08 59.01 83.50 61.34 86.20 52.18 74.69 56.36 79.30 58.21 82.36 41.31 59.50 Finance Textbook Manual News Academia 74.18 94.28 58.92 80.90 67.31 89.23 62.00 84.09 47.67 62.29 64.86 87.24 66.44 87.83 56.92 85.59 48.27 70.61 49.76 74.08 48.69 73.20 49.01 70.91 39.23 57.06 42.02 59. 37.96 70.97 31.22 58.77 36.91 67.21 31.70 59.49 24.11 44.96 34.52 62.84 17.20 32.07 72.89 87.34 58.31 76.15 65.80 83.15 4.08 6.74 0.60 1.03 54.17 67.25 17.33 23.54 79.89 86.14 55.75 62.06 60.55 65.03 49.28 66.52 24.11 44.96 62.68 66.60 61.26 65.51 Table S10. Comparison of OCR solutions on various domains in the retrieval stage using LCS@1 and LCS@5 metrics. Bold indicates the best performance, and underline indicates the second-best performance."
        },
        {
            "title": "Nougat",
            "content": "Qwen2-VL-72B InternVL2 -Llama3-76B Law F1 45.85 EM 30.25 43.95 F1 EM 28.85 F1 43.50 EM 27.95 39.69 F1 EM 25.82 F1 41.36 EM 26.73 F1 43.13 EM 27.57 F1 34.89 EM 22.76 Finance Textbook Manual News Academia 50.24 30.42 44.14 26.13 47.16 27.73 44.70 26.15 37.38 21.35 46.80 27.97 44.85 26.26 41.60 30.24 41.09 30.98 36.27 26.13 37.28 27.04 37.25 27.65 34.04 24.58 33.17 24.15 45.85 30.25 24.34 11.67 21.56 9.86 24.69 11.85 20.02 9.00 23.08 10.73 21.05 10. 35.25 25.63 30.53 21.89 31.50 21.89 31.06 22.15 26.51 18.82 34.69 24.46 25.62 18.36 40.38 23.28 37.02 22.02 36.41 21.98 5.55 3.23 2.68 1.11 29.63 16.13 12.18 6.14 Table S11. Comparison of OCR solutions on various domains in the generation stage using F1-score and EM metrics. Bold indicates the best performance, and underline indicates the second-best performance."
        },
        {
            "title": "Nougat",
            "content": "Qwen2-VL-72B InternVL2 -Llama3-76B F1@1 EM@1 F1@5 EM@5 F1@1 EM@1 F1@5 EM@5 F1@1 EM@1 F1@5 EM@5 F1@1 EM@1 F1@5 EM@5 F1@1 EM@1 F1@5 EM@5 F1@1 EM@1 F1@5 EM@5 F1@1 EM@1 F1@5 EM@5 Law 44.35 30.84 47.36 29.65 41.44 28.43 46.45 29.27 41.97 23.63 45.02 27.52 38.47 26.32 40.92 25.31 39.95 27.29 42.78 26.17 40.80 27.31 45.45 27.82 23.63 23.63 35.35 21.88 Finance Textbook Manual News Academia 47.02 28.55 53.46 32.29 40.71 24.22 47.58 28.03 45.26 26.87 49.05 28.59 41.48 24.37 47.92 27.92 35.13 20.47 39.64 22.23 44.30 26.98 49.29 28.96 44.04 26.92 45.66 25.60 40.59 30.09 42.61 30.39 41.52 32.25 40.66 29.71 36.02 26.64 36.53 25.64 37.05 27.69 37.51 26.39 37.74 28.84 36.76 26.47 34.61 26.11 33.47 23.06 35.15 26.39 31.19 21. 39.97 23.20 40.79 23.35 37.07 22.48 36.96 21.55 35.90 22.99 36.92 20.98 5.47 3.30 5.64 3.16 2.61 1.15 2.76 1.07 29.62 17.17 29.64 15.09 12.14 6.54 12.22 5.75 28.90 14.99 25.09 10.47 14.99 14.13 21.90 9.21 23.79 11.89 19.32 7.84 24.67 12.14 24.71 11.56 21.59 10.15 18.46 7.84 32.05 22.98 20.73 8.63 24.86 12.58 17.23 7.73 33.10 24.49 37.40 26.77 29.16 20.86 31.90 22.93 29.89 21.26 33.09 22.52 29.37 21.10 32.74 23.20 24.57 17.60 28.45 20.04 32.05 22.98 37.33 25.95 24.53 17.66 26.71 19.06 Table S12. Comparison of OCR solutions on various domains in overall performance. The performance is presented with top-1 (EM@1, F1@1) and top-5 (EM@5, F1@5) retrieved chunks. Bold indicates the best performance, and underline indicates the second-best performance. 7 Figure S1. The impact of Semantic Noise ([S] dashed line) and Formatting Noise ([F] solid line) on different retrievers. The horizontal axis represents the edit distance, and higher values indicate greater OCR noise. We report the LCS@1 of each retriever on questions from all evidence sources. Figure S2. The impact of Semantic Noise ([S] dashed line) and Formatting Noise ([F] solid line) on different LLMs. The horizontal axis represents the edit distance, and higher values indicate greater OCR noise. We report the F1-score of each LLM on questions with all evidence sources. 8 Figure S3. The impact of Semantic Noise ([S] dashed line) and Formatting Noise ([F] solid line) on different RAG systems, using top-1 in the retrieval stage. The horizontal axis represents the edit distance, and higher values indicate greater OCR noise. Overall performance is presented for top-1 (F1@1) retrieved chunks. 9 Figure S4. One of the real table cases used to guide the introduction of Semantic Noise. The upper left is the original table in the ground truth, and the upper right is real example from the OCR result of MinerU. The lower left and lower right are the results of moderate and severe perturbation to the original table after using the real example as guidance. For better show, we modified some latex codes manually so that most part of the table structure can be displayed normally."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Peking University",
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University",
        "The University of HongKong"
    ]
}