{
    "paper_title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
    "authors": [
        "Ji Zhao",
        "Yufei Gu",
        "Shitong Shao",
        "Xun Zhou",
        "Liang Xiang",
        "Zeke Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 3 9 3 5 0 . 2 0 6 2 : r Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better Ji Zhao1,2,, Yufei Gu1, Shitong Shao1, Xun Zhou2, Liang Xiang2, Zeke Xie1, 1The Hong Kong University of Science and Technology (Guangzhou), 2ByteDance Seed Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, fundamental real-world question remains underexplored: Can we leverage existing small pretrained models to accelerate the training of larger models? In this paper, we propose Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an late LLM during early training using representations from the late layers of pretrained (i.e. training phase) model. We identify two key mechanisms that drive LETs effectiveness: late-toearly-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LETs efficiency and effectiveness. Notably, when training 1.4B LLM on the Pile dataset, our method achieves up to 1.6 speedup with nearly 5% improvement in downstream task accuracy compared to standard training, even when using pretrained model with 10 fewer parameters than the target model. Correspondence: Zeke Xie at zekexie@hkust-gz.edu.cn"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable performance across diverse natural language tasks [1, 7, 76], marking significant milestone toward artificial general intelligence (AGI) [8, 25]. Pretraining plays key role in shaping these models capabilities [16, 63], serving as the foundation for their downstream performance. However, training such models remains extremely resource-intensive [38, 42, 64]. For example, training an LLM with 12B parameters can require about 72,000 GPU hours using NVIDIA A100 GPUs [5], which calls for more efficient training paradigms. Meanwhile, fueled by the open-source culture within the AI community, we are witnessing flourishing era rich with an array of publicly available models of varying sizes [27, 31, 102]. Building on open-source implementations, many impactful works have emerged by fine-tuning existing models such as Taori et al. [74] in the text domain and Liu et al. [52] in the multimodal domain. This approach effectively leverages the substantial computational resources already invested in the development of these models. 1 Figure 1 Comparison of Average Downstream Task Performance: LET vs. Baseline (Standard Training) on 1.4B and 7B Models. LET models are trained under our proposed LET paradigm, whereas the baseline models utilize standard causal language modeling. Remarkably, LET delivers significant performance gains, even when aligned with model 10 smaller than the target model. Traditional knowledge distillation (KD) typically trains smaller student model under the guidance of more capable teacher model [37, 66]. Nevertheless, in the context of LLMs, employing substantially larger teacher inevitably incurs considerable memory and computational overhead. Furthermore, in conventional KD, student models tend to lag behind their teachers in performance, which limits their utility as foundation for scaling LLM capabilities. Recently, Rawat et al. [65] claimed that smaller models can bootstrap the pretraining of larger LLMs. However, the size gap between the teacher and the student is modest (1.87), which limits practical applicability because the teacher remains relatively large and incurs substantial memory overhead. Moreover, the approach relies on data preprocessing and underutilizes existing open-source models that were trained at considerable computational cost. Another line of research using smaller models to accelerate larger model training focuses on model growth strategies, leveraging open-source LLMs to accelerate the training of larger models [17, 67, 83]. While these approaches can reduce training time, they typically require deliberate architectural modifications, such as carefully calibrated increases in network depth and width, which add complexity and constrain the range of feasible architectures. Consequently, their practical utility is also limited. This raises natural and practical questions: Given the abundance of small, pretrained open-source models, can they be generally leveraged during the pretraining of larger LLMs to guide and accelerate the learning process? Furthermore, could the larger target model learn to adaptively process and refine these representations as it progressively develops greater capabilities? To address these questions, we propose Late-to-Early Training (LET), novel and general paradigm for enhancing LLM pretraining using the representations of small, pretrained models that were developed at considerable computational expense by the community. LET is architecture-agnostic as it relies solely on representations of LLMs rather than specific architectural constraints. Furthermore, LET is designed to remain effective despite the performance limitations of the smaller models in the later stages of LET training: As training progresses, the larger target model rapidly improves in overall capability and may eventually surpass the smaller model in overall performance, thereby reducing the effectiveness of the representations alignment. To address this, LET aligns the representations of the smaller trained model with the early layers of the target model, allowing the subsequent layers to naturally adapt to and refine these representations through learning dynamics (see Section 3.3). Extensive experiments with 1.4B, 3B, and 7B parameter models demonstrate the effectiveness and efficiency of the LET paradigm, with comparative results for 1B and 7B models shown in Figure 1. The primary contributions can be summarized as follows. 2 First, we are the first to tackle novel, valuable, yet overlooked problem: Given the abundance of small, pretrained models developed at significant computational expense by the community, can they be leveraged to generally accelerate the pretraining process of much larger LLMs (e.g., 10), regardless of LLM architectures? Second, we propose the novel LET paradigm. At its core, it enables the early layers of the target model during early training steps to learn from the late layers of smaller pretrained LLM (i.e., from its late training phase). We identify two key mechanisms, Late-to-Early Step Learning and Late-to-Early Layer Learning, which are robust to the limitations of smaller models representations. Third, extensive experiments demonstrate that LET achieves both faster training and superior downstream performance. Notably, for training 1.4B model on the Pile dataset (as shown in Figure 1), our method delivers up to 1.6 faster improvement in downstream performance compared with standard training, even when relying on small model with up to 10 fewer parameters than the target model, which significantly exceeds the typical scope of conventional knowledge distillation."
        },
        {
            "title": "2 Methodology",
            "content": "In this section, we formally propose the LET paradigm for faster and better LLM training. Notation. We introduce the notation used in the standard pretraining paradigm for LLMs. Let denote an LLM with parameters θ, and let = [x1, x2, . . . , xT ] represent an input token sequence of length . The objective of pretraining is to maximize the likelihood of the sequence under M, typically by training the model to predict each token given its preceding tokens. Formally, at each position (1 ), the model produces conditional distribution PM(xt x<t), where x<t = [x1, . . . , xt1] denotes the prefix context. We denote by (l) the transformation implemented by the l-th layer of the model (and analogously (l) for model ), where 1 for model with layers. Thus, forward pass through the first layers of is written as: h(k) (e1:T ), yielding the hidden states after the k-th layer. (1) (k1) = (k) We propose the LET paradigm, which incorporates an additional alignment mechanism to guide the early training of larger model with the help of smaller pretrained model . LET comprises two components: Late-to-early-layer learning: encouraging the early-layer representations of to align with the late-layer representations of ; Late-to-early-step learning: employing pretrained model (representing later training stage) during the initial training steps, and gradually phasing it out as training progresses. We summarize the procedure of the LET paradigm in Algorithm 1. The traditional training objective for is to minimize the cross-entropy loss, i.e., the negative log-likelihood (NLL) of the target tokens over the training dataset. For given sequence x, this loss is formulated as: LNLL = (cid:88) t= log PM (cid:0)xt x<t (cid:1). (1) This loss measures how well the model predicts the token xt at each step t; lower value indicates more accurate predictions. In contrast to standard pretraining, knowledge distillation (KD) is classical approach in which smaller (or less capable) student model is trained to match the output probability distributions of larger teacher model. As discussed in detail in Section B, in the context of language modeling, given pretrained teacher model producing soft predictions PT (xt x<t), the KD loss is defined as LKD = (cid:88) (cid:88) t=1 vV PT (v x<t) log PM(v x<t), (2) where denotes the vocabulary and indexes individual tokens. This objective minimizes the cross-entropy between the teachers and the students predicted distributions. 3 Algorithm 1 Late-to-Early Training 1: Input: Training dataset D; target model M; small pretrained model ; initial projection weight λ0; projection stop step Sstop 2: Output: Pretrained target model 3: for each minibatch do 4: 5: Forward through and to obtain hidden representations Compute standard loss LNLL from Retrieve h(k) if dT = dM then Project h(k) from layer of and h(LT ) to match h(LT ) T from the final layer of else Use h(k) directly to align with h(LT ) end if Normalize hidden states and compute projection loss Lproj = h(k) Update projection weight λ = λ0 max Compute total loss Ltotal = LNLL + λLproj Backpropagate and update parameters of 0, Sstops Sstop (cid:17) (cid:16) h(LT ) , where is the current training step 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for Consider an input token sequence = [x1, x2, . . . , xT ] of length , where each token xt belongs to the vocabulary V. Let e1:T = [e1, e2, . . . , eT ], et Rd denote the corresponding token embeddings, with being the embedding dimension. These embeddings are processed by two models: target model and small pretrained model . Let LM and LT denote the total number of Transformer layers in and , respectively. The hidden states after the final layer of and after the k-th layer of are: = (LT ) h(LT ) = (k) h(k) (1) (LT 1) (1) (k1) (e1:T ), (e1:T ), (3) where (l) denotes the transformation implemented by the l-th Transformer layer, and 1 LM. For clarity, we illustrate the case of single token: h(LT ) RdM represent the hidden states from and M, respectively. When dT = dM, projection is applied before alignment (details in Appendix F). The representations are then normalized, and the projection loss is defined as the negative cosine similarity between them: RdT and h(k) Lproj = h(k) h(LT ) = (cid:32) (cid:33) (cid:32) h(k) h(k) (cid:33) h(LT ) h(LT ) . (4) To control the influence of this auxiliary alignment term during training, we introduce weight λ that decays linearly to zero: Ltotal = LNLL + λ Lproj = LNLL + λ0 max 0, Lproj. (5) (cid:18) (cid:19) Sstop Sstop where λ0 is the initial projection loss weight, is the current training step, and Sstop is the step at which λ decays to zero. This formulation implements the late-to-early-layer learning mechanism in the LET paradigm. In the early stage of training, λ is relatively large, allowing the model to leverage additional representational guidance from the model . As training progresses, λ gradually decays according to predefined schedule, ensuring that the model focuses on optimizing the primary objective LNLL. Overall, LET incorporates both late-to-early-layer learning and late-to-early-step learning into LLM pretraining, thereby promoting faster convergence and better generalization, as demonstrated by the experimental results in Section 3. Table 1 Results on downstream evaluation datasets used in Groeneveld et al. [28]. We report accuracy scores for each task and the average across all datasets, with the best score per model size boldfaced. Notably, in the 1.4B scale setting, LET not only achieves higher final accuracy, but also exceeds the baselines average performance while requiring less than 67% of the training steps even with 10 smaller model . Here, LET (67%) denote models trained with 67% of the total training steps, using our proposed LET. ARC-c ARC-e HS LAMB OBQA PIQA SciQ Wino. BoolQ Avg. Baseline RKD SALT1 LET(67%) LET . Baseline RKD SALT1 LET(67%) LET 17.8 18.0 18.1 17.8 18. 19.4 19.8 19.1 18.4 20.0 Model Size = 1.4B 24.1 24.8 24.5 23.8 24.9 26.0 26.3 26.3 26.6 26.8 Model Size = 7B 25.5 26.5 27.4 27.0 28. 28.0 30.8 30.6 29.7 31.4 61.5 62.4 64.0 64.6 64.4 63.3 61.3 62.1 61.8 65.3 28.6 27.7 28.5 28.1 28.4 29.3 28.8 30.5 29.5 29.8 44.2 42.9 45.5 45.7 45. 45.6 41.6 46.8 45.9 47.4 73.3 63.7 73.6 72.2 74.0 74.5 63.9 76.0 74.1 76.7 51.4 52.3 52.7 52.6 53.0 52.7 51.4 52.9 51.4 54.4 47.9 54.8 52.9 51.1 57. 51.4 55.6 56.9 57.3 55.9 41.6 41.4 42.9 42.5 43.6 43.3 42.2 44.7 43.9 45."
        },
        {
            "title": "3 Empirical Analysis",
            "content": "In the following paragraph, we empirically studied the proposed LET with various settings."
        },
        {
            "title": "3.1 Experimental setup",
            "content": "Model Architecture. Our models are based on the LLaMA architecture. We adopt RMSNorm and SwiGLU activations [70, 79, 105], and all models are trained using BF16 precision. In our experiments, the models are drawn from the OPT family [107], the Pythia family [5], and the SmolLM family [3]. Detailed model hyperparameters are summarized in Section E. Data. We pretrain our models on The Pile dataset, large-scale, diverse, and high-quality English text corpus designed for training large language models [23]. It contains approximately 825 GB of text from 22 different sources, and our experiments use approximately 20 billion tokens. Pretraining Setting. We follow the hyperparameter configuration for The Pile dataset from Rawat et al. [65]. Specifically, we use total batch size of 2048 and an input sequence length of 1280. All experiments are conducted on 32 NVIDIA A100 80GB GPUs. We employ the AdamW optimizer [54] and cosine learning rate schedule, with linear warmup during the first 10% of training steps and decay to 10% of the peak learning rate thereafter. Following the Groeneveld et al. [28] setup, the peak learning rate is set to 4 104 for 1B scale models and 3 104 for 7B scale models. For more details, please refer to Appendix A. Evaluation. We evaluate one-shot performance on the nine downstream test datasets used in Groeneveld et al. [28], Gu et al. [30]. For more details on these tasks, please refer to Appendix J. Additionally, we report the language modeling loss on test set from The Pile. Baseline Setting. We compare the proposed LET paradigm with both the traditional causal language modeling approach (referred to as the Baseline in this paper), SALT [65] and Reverse Knowledge Distillation (RKD). For more details, please refer to Appendix A. 1Since data selection is orthogonal to our method, we adopt SALT without data selection for more controlled comparison."
        },
        {
            "title": "3.2 Main Results",
            "content": "LET Improves Downstream Task Performance. We empirically evaluate the effectiveness of our proposed LET paradigm by pretraining language models with 1.4B and 7B parameters and assessing their downstream task performance on the evaluation datasets used in Groeneveld et al. [28]. Additional experimental results and discussions are provided in Appendix C. As shown in Table 1, LET consistently outperforms the baseline on the majority of tasks across both scales, yielding higher average accuracy with notable margin. These findings demonstrate that integrating both late-to-early-layer learning and late-to-early-step learning into LLM pretraining can effectively enhance generalization across downstream applications. Furthermore, in the 1.4B parameter configuration, LET employs small pretrained model that is an order of magnitude smaller (10) than the target model M, yet it still achieves substantial performance gains over the baseline. Compared to LET, RKD shows clear limitations when the model is significantly smaller than the target model; specifically, it underperforms the baseline in both the 1.4B and 7B settings. RKDs results also exhibit certain patterns. For instance, it performs relatively well on tasks such as ARC-c and LAMB, indicating stronger reasoning abilities. However, on tasks like SciQ, which involve multiple-choice question answering in the science domain, RKDs performance is markedly lower than that of other methods. This suggests that while the distillation process may strengthen certain specific capabilities, it can also considerably hinder the models overall learning effectiveness. From Table 1, it is evident that RKD struggles when the teacher model is significantly smaller than the student model; specifically, it underperforms the baseline in both the 1.4B and 7B model settings. The results of RKD also exhibit certain patternsfor example, it performs relatively well on tasks such as ARC-c and LAMB, demonstrating strong reasoning ability. However, on tasks like SciQ, which focuses on multiple-choice question answering in the science domain, RKDs performance is substantially lower than that of other methods. This suggests that while the distillation process may reinforce certain capabilities in the student model, it can also significantly impede the models overall learning capacity. Figure 2 Language modeling performance of LET across three different vocabulary settings. We evaluate the perplexity of models trained with different vocabulary: SmolLM, OPT, and Pythia. For fair comparison [23], each subplot uses the same vocabulary. The results demonstrate that LET consistently achieves lower perplexity across all three settings. LET Improves Language Modeling. To comprehensively assess the effectiveness of LET, we evaluate not only the average performance on nine downstream tasks, but also the language modeling perplexity on the test split of The Pile [23]. For representation alignment in the 1.4B target model M, we use three distinct small pretrained models at approximately 125160M scale (OPT-125M, Pythia-160M, and SmolLM-135M). As shown in Figure 2, each subfigure uses consistent vocabulary across and . Despite using different small pretrained models, LET consistently reduces test perplexity, in line with the performance improvements observed in downstream tasks. This confirms the robustness of LET in enhancing modeling capability, irrespective of the tokenization scheme employed. Moreover, different small pretrained models have varying impacts: although their sizes are similar, substantial differences in architecture (see Appendix E) lead to different learned representations and, consequently, distinct training dynamics in M. Among these, using SmolLM as yields the best overall performance. In addition to improving performance, LET also significantly accelerates LET Accelerates Training. training. As shown in Table 1, LET attains higher performance while requiring less than two-thirds of the training steps needed to surpass the baseline. This represents substantial speedup, even when is an order of magnitude (10) smaller than M. similar pattern is observed in Figure 2, where LET achieves lower test perplexity during training across three different vocabularies. These results demonstrate LETs effectiveness in accelerating convergence for both language modeling and generalization across diverse tasks. Furthermore, LET not only facilitates efficient pretraining for LLMs, but also exhibits strong cross-domain performance. For additional results on cross-domain generalization, such as time series classification, please refer to Section D."
        },
        {
            "title": "3.3 Ablation Study and Analysis",
            "content": "In our proposed late-to-early-layer learning paradigm, More diverse layer-wise alignment experiments. we use the late-layer representations of small pretrained model to align the earlier-layer representations of the target model during training. This approach has shown strong empirical performance. To systematically assess the impact of different alignment strategies, we conduct series of ablation experiments with diverse layer alignment configurations. Specifically, we consider six variants: L2E, L2M, L2L, where the last layer of aligns with the early, middle, or last layer of M, respectively; and M2E, M2M, M2L, where middle layer of aligns with the early, middle, or last layer of M, respectively. In both the 1B scale and 7B scale settings. Figure 3 Comparison of six layer-wise alignment strategies on average downstream task performance in one-shot evaluation. The proposed LET paradigm, corresponding to L2E, achieves the highest average performance across all downstream tasks, outperforming all alternative strategies. From Figure 3, we can draw two main observations. First, using the middle-layer representations of for alignment consistently yields weaker performance compared to using the final-layer representations, as evidenced by M2E, M2M, and M2L underperforming all of L2E, L2M, and L2L. Second, among all configurations that use the late layer of for alignment, L2E demonstrates superior performance and robustness. As illustrated in Figure 4, the L2E alignment strategy demonstrates superior robustness compared to alternative approaches. This stability is evident from the perplexity trajectories observed after the alignment phase: while all non-L2E strategies show varying degrees of perplexity increase immediately post-alignment, L2E maintains consistent performance. This robustness advantage suggests more seamless integration between the alignment objective and the underlying language modeling capability. Moreover, L2E achieves the lowest perplexity among all approaches and correspondingly delivers the highest average performance across downstream tasks, further indicating its effectiveness as an alignment strategy. These empirical results further validate the effectiveness of our late-to-early-layer learning design, in which late-layer representations from the small pretrained model guide the formation of informative early-layer representations within the target model M. Consistent with the design rationale of LET, the robustness of L2E can be attributed to its alignment strategy: by mapping the representations of to the early layers of M, Figure 4 Comparison of six layer-wise alignment strategies on language modeling performance, measured as test perplexity on the test split of The Pile dataset. Both M2E and L2E maintain robust performance throughout training, with L2E yielding the lowest final perplexity among all strategies. the subsequent layers retain sufficient capacity to adapt and refine these representations through the learning dynamics of training. This structural configuration becomes increasingly important as training progresses, since gradually gains capability and may eventually surpass in overall performance, thereby diminishing the relative strength of the alignment representations from . The remaining layers after early alignment act as buffer, enabling the seamless integration and progressive refinement of representations provided by . The trends observed in Figure 4 provide further empirical support for this explanation. Figure 5 Average downstream task performance (left) and test perplexity on the The Pile dataset (right) evaluated under different λ values: 0.01, 0.1, 0.3, 1.0, and 3.0. Baseline denotes training with standard causal language modeling, whereas all other configurations employ the proposed LET paradigm with different λ. In previous experiments, we adopted λ = 0.1 as the Effect of hyperparameter λ on performance. default setting. To further investigate the effect of the hyperparameter, we conducted additional evaluations across multiple values, specifically λ {0.01, 0.1, 0.3, 1.0, 3.0}. The average downstream task performance for each setting is shown in Figure 5. As illustrated, when λ exceeds 0.1, performance consistently drops, indicating that larger values induce excessive alignment of the target model to the representations of the small pretrained model (see Figure 6), which in turn hampers learning from data. Conversely, setting λ = 0.01 yields performance above the baseline but still below that achieved with λ = 0.1, suggesting that alignment is insufficient at this lower value and thus limits the effective utilization of representations from . 8 Figure 6 Cosine similarity between the late-layer representations of the small pretrained model and the early-layer representations of the target model under varying λ values. Figure 5 and Figure 6 together indicate that λ = 0.1 achieves an optimal balance, as both excessively large and small values result in suboptimal performance. In addition, Figure 6 reveals the following: (1) higher λ values correspond to higher average cosine similarity, reflecting stronger alignment between and ; (2) representation similarity increases steadily throughout training, regardless of the λ setting; and (3) despite varying λ by an order of magnitude, similarity curves remain relatively stable, suggesting that even small λ values can provide effective alignment. Overall, λ = 0.1 offers well-balanced trade-off between aligning with and acquiring new knowledge from data, resulting in optimal performance on both downstream tasks and language modeling perplexity. LET-1.4B Achieves Superior Performance over Baseline-3B. As shown in Figure 7, LET-1.4B achieves higher performance than Baseline-3B, despite having fewer parameters. This result indicates that the proposed LET paradigm enables the model to learn more effectively from limited training data. LET achieves this by effectively leveraging the representations learned by the model to guide alignment in the target model , thereby improving learning efficiency. This improved efficiency allows models to generalize better with constrained data, making LET particularly valuable in resource-limited settings. Figure 7 comparison of average downstream task performance across different training paradigms and model sizes."
        },
        {
            "title": "4 Discussion",
            "content": "In this section, we discuss the potential limitations and future directions of our work. Limitations. First, in order to ensure fair comparison of wall-clock training time, we provide detailed analysis of the throughput for each method. Due to space constraints, the detailed throughput results are provided in Table 4 in the Appendix E. As shown in the table, the throughput of our LET approach is slightly lower than that of the baseline. Second, while our extensive experiments provide strong evidence for the effectiveness and efficiency of the proposed LET paradigm, empirical evaluations are primarily conducted on models with 1.4B, 3B, and 7B parameters, trained on datasets comprising up to 20B tokens, due to computational resource limitations. Further scaling of both model size and training data is needed to fully demonstrate the scalability of the LET paradigm. Future Work and Discussion. First, LET is only applied during the early stages of training. As training progresses and more data is processed, the computational overhead introduced by LET becomes increasingly negligible. Additionally, although RKD achieves marginally higher throughput, its final performance remains 9 substantially inferior to that of LET. Notably, although the baseline achieves 1.078 the throughput of LET during the early training phase, our LET paradigm attains 1.6 speedup in convergence, which more than compensates for the modest reduction in throughput. Furthermore, when scaling from 1.4B model to 7B model, the size of the small teacher model increases by more than an order of magnitude (from SmolLM-135M to SmolLM-1.7B), yet the resulting decrease in throughput remains minimal. This demonstrates that LET is not only efficient but also highly scalable. Second, further validation on larger models, such as those with 70B parameters or more and on substantially larger datasets (e.g., datasets containing 1T tokens) is warranted for thoroughly assessing the practical applicability of LET in real-world settings."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents Late-to-Early Training (LET), novel paradigm that transforms the vast computational investments already made by the community into driving force for building stronger LLMs, ensuring that these expensive resources are maximally utilized. Unlike conventional knowledge distillation, which typically relies on substantially larger teacher models, thereby incurring significant memory overhead and may not enable the student to outperform its teacher, LET can exploit much smaller pretrained models to iteratively enhance the capabilities of larger target models. LET introduces two core mechanisms late-to-early-step learning and late-to-early-layer learning, which achieve faster convergence and superior performance without imposing architectural constraints. Extensive experiments across models with 1.4B to 7B parameters validate the effectiveness of LET. Overall, LET offers pratical pathway for advancing next-generation LLMs, guiding language model development toward more resource efficient trajectory."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [3] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. [4] Guozhong An. The effects of adding noise during backpropagation training on generalization performance. Neural computation, 8(3):643674, 1996. [5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. [6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [8] Sébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. [9] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. 10 [10] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143, 2021. [11] Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher Ré. Skill-it! data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36:3600036040, 2023. [12] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015. [13] Xianing Chen, Qiong Cao, Yujie Zhong, Jing Zhang, Shenghua Gao, and Dacheng Tao. Dearkd: data-efficient early knowledge distillation for vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1205212062, 2022. [14] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [15] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [17] Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, and Jie Fu. Stacking your transformers: closer look at model growth for efficient llm pre-training. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [18] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311, 2018. [19] Scott Fahlman. The recurrent cascade-correlation architecture. Advances in neural information processing systems, 3, 1990. [20] Scott Fahlman and Christian Lebiere. The cascade-correlation learning architecture. Advances in neural information processing systems, 2, 1989. [21] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In International conference on machine learning, pages 16071616. PMLR, 2018. [22] Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, et al. Principled data selection for alignment: The hidden risks of difficult examples. In Forty-second International Conference on Machine Learning, 2025. [23] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [24] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628. [25] Ben Goertzel. Artificial general intelligence: concept, state of the art, and future prospects. Journal of Artificial General Intelligence, 5(1):1, 2014. [26] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International conference on machine learning, pages 23372346. PMLR, 2019. [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 11 [28] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arxiv [preprint](2024). URL https://api. semanticscholar. org/CorpusID, 267365485. [29] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. [30] Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, and Minlie Huang. Data selection via optimal control for language models. arXiv preprint arXiv:2410.07064, 2024. [31] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [32] Steven Gutstein, Olac Fuentes, and Eric Freudenthal. Knowledge transfer in deep convolutional neural nets. International Journal on Artificial Intelligence Tools, 17(03):555567, 2008. [33] Jeff HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. Shape matters: Understanding the implicit bias of the noise covariance. In Conference on Learning Theory, pages 23152357. PMLR, 2021. [34] Ethan He, Abhinav Khattar, Ryan Prenger, Vijay Korthikanti, Zijie Yan, Tong Liu, Shiqing Fan, Ashwath Aithal, Mohammad Shoeybi, and Bryan Catanzaro. Upcycling large language models into mixture of experts. arXiv preprint arXiv:2410.07524, 2024. [35] Ruifei He, Shuyang Sun, Jihan Yang, Song Bai, and Xiaojuan Qi. Knowledge distillation as efficient pre-training: Faster convergence, higher data-efficiency, and better transferability. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91619171, 2022. [36] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [37] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [38] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [39] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex stochastic gradient descent. Annals of Mathematical Sciences and Applications, 4(1):332, 2019. [40] Stanislaw Jastrzkebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017. [41] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019. [42] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [43] Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Sung Ju Hwang, and Alexander Min. study on knowledge distillation from weak teacher for scaling up pre-trained language models. arXiv preprint arXiv:2305.18239, 2023. [44] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. KR, 2012:13th, 2012. [45] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. arXiv preprint arXiv:2308.12032, 2023. [46] Qianxiao Li, Cheng Tai, et al. Stochastic modified equations and adaptive stochastic gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 21012110. JMLR. org, 2017. [47] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic differential equations (sdes). arXiv preprint arXiv:2102.12470, 2021. 12 [48] Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less is more: Task-aware layer-wise distillation for language model compression. In International Conference on Machine Learning, pages 2085220867. PMLR, 2023. [49] Kevin Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and Lawrence Carin. Mixkd: Towards efficient distillation of large-scale language models. arXiv preprint arXiv:2011.00593, 2020. [50] Seng Pei Liew, Takuya Kato, and Sho Takase. Scaling laws for upcycling mixture-of-experts language models. arXiv preprint arXiv:2502.03009, 2025. [51] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen, et al. Not all tokens are what you need for pretraining. Advances in Neural Information Processing Systems, 37:2902929063, 2024. [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [53] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685, 2023. [54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [55] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [56] Roy Miles and Krystian Mikolajczyk. Understanding the role of the projector in knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 42334241, 2024. [57] Roy Miles, Ismail Elezi, and Jiankang Deng. Vkd: Improving knowledge distillation using orthogonal projections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1572015730, 2024. [58] Vinod Nair and Geoffrey Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807814, 2010. [59] Arvind Neelakantan, Luke Vilnis, Quoc Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015. [60] Thanh Huy Nguyen, Umut Simsekli, Mert Gurbuzbalaban, and Gaël Richard. First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise. In Advances in Neural Information Processing Systems, pages 273283, 2019. [61] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. [62] Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, et al. Knowledge inheritance for pre-trained language models. arXiv preprint arXiv:2105.13880, 2021. [63] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [64] Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [65] Ankit Singh Rawat, Veeranjaneyulu Sadhanala, Afshin Rostamizadeh, Ayan Chakrabarti, Wittawat Jitkrittum, Vladimir Feinberg, Seungyeon Kim, Hrayr Harutyunyan, Nikunj Saunshi, Zachary Nado, et al. little help goes long way: Efficient llm training by leveraging small lms. arXiv preprint arXiv:2410.18779, 2024. [66] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 13 [67] Mohammad Samragh, Iman Mirzadeh, Keivan Alizadeh Vahid, Fartash Faghri, Minsik Cho, Moin Nabi, Devang Naik, and Mehrdad Farajtabar. Scaling smart: Accelerating large language model pre-training with small model initialization. arXiv preprint arXiv:2409.12903, 2024. [68] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [69] Shitong Shao, Hongwei Yi, Hanzhong Guo, Tian Ye, Daquan Zhou, Michael Lingelbach, Zhiqiang Xu, and Zeke Xie. Magicdistillation: Weak-to-strong video distillation for large-scale few-step synthesis. arXiv preprint arXiv:2503.13319, 2025. [70] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [71] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984, 2020. [72] Qian-Yuan Tang, Yufei Gu, Yunfeng Cai, Mingming Sun, Ping Li, Zeke Xie, et al. Investigating the overlooked hessian structure: From cnns to llms. In Forty-second International Conference on Machine Learning, 2025. [73] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136, 2019. [74] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/ tatsu-lab/stanford_alpaca, 2023. [75] Yi Tay, Mostafa Dehghani, Vinh Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, et al. Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022. [76] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [77] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via document de-duplication and diversification. Advances in Neural Information Processing Systems, 36:5398353995, 2023. [78] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. In International conference on Training data-efficient image transformers & distillation through attention. machine learning, pages 1034710357. PMLR, 2021. [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [81] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. In The Eleventh International Conference on Learning Representations. [82] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:57765788, 2020. [83] Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and Hongxia Yang. Lemon: Lossless model expansion. arXiv preprint arXiv:2310.07999, 2023. [84] Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, and Jianmin Wang. Deep time series models: comprehensive survey and benchmark. 2024. [85] Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. 14 [86] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023. [87] Lei Wu, Chao Ma, and Weinan. How sgd selects the global minima in over-parameterized learning: dynamical stability perspective. In Advances in Neural Information Processing Systems, pages 82798288, 2018. [88] Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang, and Rui Wang. Ad-kd: Attribution-driven knowledge distillation for language model compression. arXiv preprint arXiv:2305.10010, 2023. [89] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations. [90] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. [91] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:6979869818, 2023. [92] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:3420134227, 2023. [93] Zeke Xie, Issei Sato, and Masashi Sugiyama. diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=wXgk_iCiYGo. [94] Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, Dacheng Tao, and Masashi Sugiyama. Artificial neural variability for deep learning: On overfitting, noise memorization, and catastrophic forgetting. Neural Computation, 33(8), 2021. [95] Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, Dacheng Tao, and Masashi Sugiyama. Artificial neural variability for deep learning: On overfitting, noise memorization, and catastrophic forgetting. Neural Computation, 33(8), 2021. [96] Zeke Xie, Issei Sato, and Masashi Sugiyama. diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=wXgk_iCiYGo. [97] Zeke Xie, Li Yuan, Zhanxing Zhu, and Masashi Sugiyama. Positive-negative momentum: Manipulating stochastic gradient noise to improve generalization. In International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1144811458. PMLR, 1824 Jul 2021. [98] Zeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, and Masashi Sugiyama. Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2443024459, 2022. [99] Zeke Xie, Qianyuan Tang, Mingming Sun, and Ping Li. On the overlooked structure of stochastic gradients. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [100] Zeke Xie, Zhiqiang Xu, Jingzhao Zhang, Issei Sato, and Masashi Sugiyama. On the overlooked pitfalls of weight decay and how to mitigate them: gradient-norm perspective. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [101] Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 31223133, 2018. [102] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [103] Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. Dataset pruning: Reducing training data by examining generalization influence. In The Eleventh International Conference on Learning Representations, 2022. [104] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [105] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [106] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. Towards the law of capacity gap in distilling language models. arXiv preprint arXiv:2311.07052, 2023. [107] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [108] Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao. Toward understanding the importance of noise in training neural networks. In International Conference on Machine Learning, 2019. [109] Wangchunshu Zhou, Canwen Xu, and Julian McAuley. Bert learns to teach: Knowledge distillation with meta learning. arXiv preprint arXiv:2106.04570, 2021. [110] Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects. In ICML, pages 76547663, 2019."
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "1 3 5 5 6"
        },
        {
            "title": "5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "10 Experimental Settings and Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Knowledge transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Training acceleration methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 18 18 19 Supplementary Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Time Series Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LM Architecture and Throughput . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Hidden States Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LogSum Loss Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Hessian Structure Analysis H.3 Curvature Bound via Frobenius Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 23 24 24 24 25 25 26 Failure Mode Analysis and Layer Selection Strategies . . . . . . . . . . . . . . . . . . . . . 26 Descriptions of Evaluation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the National Natural Science Foundation of China under Grant No. 62506317 and Doubao Large Model Fund, ByteDance."
        },
        {
            "title": "A Experimental Settings and Details",
            "content": "This section provides comprehensive experimental settings and implementation details to facilitate full reproducibility of our work. Training Hyperparameters For the experimental configuration, we employed total batch size of 2048 with an input sequence length of 1280 tokens, resulting in approximately 2.62 million tokens per step. We used The Pile dataset with all copyrighted content removed and define the third layer of as the early layer in all configurations. The training setup was adapted based on model size. For the 1.4B parameter model, we utilized per-GPU batch size of 16 with gradient accumulation factor of 4. In contrast, for the larger 7B parameter model, we reduced the per-GPU batch size to 4 while increasing gradient accumulation to 16 to accommodate memory constraints. Both models shared common hyperparameters: we applied the AdamW optimizer with β1 = 0.9 and β2 = 0.999, weight decay of 0.01, and maximum gradient norm of 1.0 for gradient clipping. The learning rate varied by model size: 4 104 for the 1.4B parameter model and 3 104 for the 7B parameter model, with both utilizing cosine learning rate schedule for optimization. Evaluation and Benchmark For model evaluation, we assessed our models using nine downstream tasks (used in OLMo). The task suite includes Hellaswag [104], Winograde [44], LAMBADA [61], OpenbookQA [55], ARC-easy/challenge [15], PIQA [6], SciQ [85], BoolQ [14]. Regarding perplexity measurements on The Pile dataset, we conducted evaluations at regular intervals of 500 training steps, corresponding to approximately 1.3 billion tokens of training. For downstream task assessments, we saved checkpoints throughout the training process and evaluated them using the EleutherAI evaluation harness framework [24]. To optimize evaluation efficiency, we use automatic batch size detection within the evaluation harness to identify the maximum supported batch size for each model configuration. Consistent with our training setup, all evaluations were performed on NVIDIA A100 80GB GPUs. Baseline Setup For RKD and SALT experiments, we follow the settings from Rawat et al. [65]. Unless otherwise specified, we use SmolLM2 (referred to as SmolLM for brevity) as the small model in this paper, with SmolLM-135M for the 1.4B model and SmolLM-1.7B for the 7B model."
        },
        {
            "title": "B Related work",
            "content": "In this section, we review existing works relevant to LET in details. B.1 Knowledge transfer Traditional knowledge distillation and its variants. Traditional knowledge distillation (KD) [37, 66] involves transferring knowledge from larger, well-trained teacher model to smaller student model by minimizing the difference between their output distributions. KD methodologies can be systematically categorized into two principal approaches: logits-based and hint-based techniques. The former operates at the level of output logits. Conversely, hint-based methodologies focus on aligning intermediate representations. Model including DistillBERT [68], DistillBiLSTM [73], MINILLM [29], MiniMA [106] and MixKD [49] adhere to the logits-based distillation paradigm. In contrast, models such as TinyBERT [41], MobileBERT [71], MiniLM [82], TED [48], MetaDistil [109], and AD-KD [88] implement hint-based techniques to establish correspondence between intermediate representations of the teacher and student models. In the domain of computer vision, Touvron et al. [78] achieved competitive results by having the student learn from the teacher through attention. To address training efficiency, He et al. [35] introduced the KDEP framework for efficient pre-training by aligning feature. Chen et al. [13] proposed two-stage approach to improve data efficiency. Recent work has advanced the theoretical understanding of KD by demonstrating that the projector enables relational gradients for the student model [56]. In parallel, orthogonal projection has proven highly effective, yielding significant enhancements in object detection and image generation [57]. Most KD approaches focus 18 on scenarios where the teacher model is larger than the student model. In contrast, our work investigates the reverse setting. This setup provides pathway toward developing next-generation models that aim to balance strong performance with improved memory efficiency and throughput. Weak to strong. The idea that weaker models can enhance stronger ones has been explored in various forms, with the concept of weak-to-strong generalization formalized by Burns et al. [9]. They demonstrated that fine-tuning strong pretrained model on labels generated by weaker model consistently yields performance surpassing that of the weak supervisor, terming this phenomenon weak-to-strong generalization. Earlier work laid the groundwork for this concept. For instance, work like [21] showed that in computer vision and language modeling tasks, student models can outperform equivalently sized teacher models without requiring larger teacher, suggesting inherent robustness in knowledge transfer. MagicDistillation [69] introduces weak to strong distillation framework that enables few step inference for large-scale video diffusion models. In language modeling [43, 62, 65] has highlighted the potential and limitations of leveraging weaker models to assist the training of larger models. Within the Mix of Experts (MoE) paradigm, various studies have undertaken significant explorations [34, 50]. Notably, Liew et al. [50] advances our understanding by identifying empirical scaling laws that characterize the relationship between performance and both dataset size and model configuration. While these works provide valuable insights, they often focus on small-size models (fewer than 1B parameters) or settings where the student is only marginally larger than the teacher with similar architecture. In contrast, our study involves architecture-agnostic student models scaling up to 7B parameters, with the student can be up to 10 larger than the teacher. Moreover, we focus on reusing released open-source models in the community. These models have consumed significant computational resources during their initial pretraining, yet are often underutilized when training new models. Our approach aims to leverage these existing assets more effectively, providing resource-efficient path for improving larger models using smaller, accessible ones. B.2 Training acceleration methods Two lines of research have been particularly active in accelerating language model training: data selection and model growth. Data selection aims to improve training efficiency by improving the quality and diversity of the data used during pretraining [45, 51, 53]. Recent progress has been made in both offline and online selection strategies. Offline methods [22, 77, 9092, 103] typically involve pre-filtering or reweighting data before training, whereas online methods [11, 51, 89] dynamically adjust the data distribution during training. Recent work [30] revisits data selection from the perspective of optimal control, offering new theoretical insights into selection dynamics. Model growth, initially explored in the 1990s [19, 20, 32], was significantly advanced by Net2Net [12], which introduced function-preserving expansions along both the width and depth dimensions. This paradigm has been extended in several directions. Bert2Bert [10], Lemon [83], StackedBERT [26], LiGO [81] and other related methods [17, 67] focuses on width expansion, depth expansion, or learning-based mapping. Learning dynamics has provided valuable perspective on studying how optimizers select minima [39, 40, 46, 47, 60, 87, 93, 101, 110]. This line of research also suggests that learning dynamics critically affect convergence speed and matters to final minima selection [4, 33, 59, 72, 94, 97, 99, 108]. However, relevant methods [95, 97, 98, 100] made great success on training relatively smaller models, such as ResNet, they failed to propose practical training algorithms that work well in LLM training. While another line of research [72, 95100] also studied how to improve training from perspective of learning dynamics, they failed to propose practical training algorithms that work well on LLM training. These methods effectively improve model convergence efficiency, but they still require carefully designed depth and width expansion strategies, which increase the overall complexity, particularly given the growing number of attention variants and the potential need for additional data pre-processing or complex online selection strategies. LET is orthogonal to these approaches and instead leverages small pretrained model to accelerate the early stage of model training."
        },
        {
            "title": "C Supplementary Empirical Results",
            "content": "This section provides supplementary empirical results on LLMs, including additional 1B-scale results, detailed comparisons with RKD, analysis of stopping thresholds, and experiments using LLaMA 3.2 1B as the model . In Section 3.2, we presented results using SmolLMAdditional experiments with 1B-scale models. 135M [3] as model . Here, we further extend our investigation by pretraining 1.4B model with OPT125M [107] and Pythia-160M [5] as the models , following the same experimental setup described previously 3.1. As shown in Figure 8, despite the being significantly smaller than the target model and differing in architecture, we still observe substantial improvements and faster convergence. These results highlight the robustness of our proposed L2E paradigm, which consistently delivers strong performance across different choices of models . Figure 8 comparison of average downstream task performance when using Pythia-160M and OPT-125M as the model . Here, \"LET-opt125m\" and \"LET-pythia160m\" represent the use of the LET paradigm with OPT-125M and Pythia-160M as the small models respectively. In Section 3.2, we compared the final average downstream Downstream task comparisons with RKD. task performance of RKD with the baseline and our L2E paradigm. In this section, we provide further insight by examining how the average downstream task performance evolves during training when using the RKD. As shown in Figure 9, RKD consistently underperforms compared to the baseline on both 1.4B and 7B models. This observation aligns with the findings of Lee et al. [43], Rawat et al. [65]. The former, based on experiments with 67M-size models, found that knowledge distillation can degrade performance when the teacher model is at least 0.78 times smaller than the student model. The latter primarily focused on 1.5B and 2.8B models and similarly observed that RKD underperforms the baseline. Moreover, we also observe that the performance degradation of the RKD method is more pronounced on the 7B scale compared to the 1.4B scale. These results further underscore the performance advantage of our proposed L2E paradigm, which achieves up to 1.6 speedup and 5.13% improvement in performance even when the model is 10 smaller than the target model M. While the work [43, 65] was highly valuable and provided inspiration for subsequent research, we believe that as language models become increasingly powerful and are trained on ever-growing datasets, even much smaller models can still provide useful guidance during the early stages of training. The results in Figure 9 support this analysis. Experiments and analysis of Different Sstop Values. To gain preliminary insights into the choice of Sstop, we conducted experiments by setting Sstop to 1500 and 3000, respectively. As shown in Figure 10, when training reaches around 5B tokens, using Sstop = 3000 yields better performance. This can be attributed to the gradually decreasing λ schedule described in Section 2: with larger Sstop, the alignment strength 20 Figure 9 comparison of average downstream task performance between RKD, Baseline, and LET paradigm at both 1.4B and 7B model scales. We used SmolLM-135M and SmolLM-1.7B as the models , respectively. remains higher for longer period during the early stages of training, which is beneficial for initial learning. However, as training progresses and the student model, being much larger, develops greater capacity to capture complex knowledge, continued alignment with much smaller teacher model can actually hinder further improvement. The results in Figure 10 support this analysis. Ultimately, we choose Sstop = 1500, which yields better final performance while reducing overall training time. For more detailed discussion on wall-clock training time, please refer to Section 4. Figure 10 comparison of average downstream task performance using different stopping thresholds in the LET paradigm. In this experiment, Sstop = 1500 and Sstop = 3000 represent implementations of the LET paradigm where alignment was terminated after 1500 and 3000 steps respectively. LLaMA 3.2 1B as the model for 7B-scale model. We presented results on the 7B model using SmolLM-1.7B in Section 2. To enable broader empirical analysis and further evaluate the generalizability of our LET paradigm on 7B models, we conduct additional experiments in this section using Llama-3.2-1B as the model . As shown in Figure 11, applying the LET paradigm to the 7B model also yields significant acceleration and noticeable improvements in final performance. Although the final performance gain is smaller than that observed when using SmolLM-1.7B, the acceleration ratio remains similarly high. We attribute this to 21 Figure 11 Average performance of the 7B model on downstream tasks (left) and perplexity on test split of The Pile dataset (right). The LET-Llama_3.2_1B model is trained using our proposed LET paradigm and leverages Llama-3.2-1B as the model . In contrast, the baselines are trained using standard causal language modeling. Both models have 7B parameters and share the Llama-3.2-1B vocabulary. Table 2 Comparison of accuracy between baseline (Qwen-0.5B) and LET across various datasets. Our experimental setup is to add alignment losses at depth of 6 (24 in total). Dataset Qwen-0.5B LET (Ours) EthanolConcentration FaceDetection Handwriting HreatBeat JapaneseVowels PEMS-SF SelfRegulationSCP1 SelfRegulationSCP2 SpokenArabicDigits UWaveGestureLibrary 25.8% 59.2% 23.0% 66.8% 83.9% 45.5% 69.6% 50.0% 99.3% 67.5% 28.8% 66.9% 33.2% 75.0% 95.7% 62.5% 85.5% 53.9% 99.7% 82.2% the larger parameter size of SmolLM-1.7B, which enables stronger language modeling capabilities and thus provides more effective alignment representations. Comparison with SALT. Table 1 presents comparison between LET and SALT under identical hyperparameter configurations. SALT employs two-stage training paradigm controlled by hyperparameter nKD: KD is applied for the first stage, followed by standard training. For fair comparison, we set nKD = Sstop to align with our experimental configuration. Our empirical results demonstrate that LET achieves superior performance within the same token budget and LET exhibits stable training dynamics. Additionally, SALT optimizes different training objective from ours (UL2 [75]), and its default hyperparameters may therefore be suboptimal for our training protocol. Given our limited computational budget, we did not conduct dedicated hyperparameter search for SALT in this setting. In summary, our extensive empirical analyses in both Section 3 and this section consistently demonstrate the effectiveness and efficiency of our proposed LET."
        },
        {
            "title": "D Time Series Experiments",
            "content": "The applicability of LET extends beyond LLMs. To demonstrate its versatility, we evaluate LET on time series classification tasks. As demonstrated in Table 2, we evaluated LET on diverse set of time series datasets, including EthanolConcentration, FaceDetection, Handwriting, Heartbeat, JapaneseVowels, PEMSSF, SelfRegulationSCP1, SelfRegulationSCP2, SpokenArabicDigits, and UWaveGestureLibrary [84]. The results indicate that LET significantly outperforms the baseline, which involved fine-tuning Qwen-0.5B on the respective tasks. Furthermore, the model is the TimesNet [86], specifically pre-trained on subset of 22 these time series datasets (EthanolConcentration, FaceDetection, Handwriting, Heartbeat, JapaneseVowels, SelfRegulationSCP1, and UWaveGestureLibrary). These empirical findings strongly validate both the generalizability and effectiveness of LET."
        },
        {
            "title": "E LM Architecture and Throughput",
            "content": "In this section, we detail the model configurations and training efficiency of our experiments. Table 3 LM architecture comparison"
        },
        {
            "title": "Attention\nvariant",
            "content": "OPT-125M Pythia-160M SmolLM2-135M Ours-1B . SmolLM2-1.7B Llama3.2-1B Ours-7B 768 768 576 2048 2048 2048 4096 1B scale setting 3072 3072 1536 5461 12 12 30 24 7B scale setting 8192 8192 11008 24 16 32 12 12 9 32"
        },
        {
            "title": "Relu\nGelu\nSilu\nSwiGLU",
            "content": "Silu Silu SwiGLU"
        },
        {
            "title": "Full\nFull\nGQA\nFull",
            "content": "GQA GQA Full Table 4 Training Efficiency and Resource Consumption Comparison. Throughput Ratio is defined as the throughput of the corresponding method divided by the baseline throughput. Wall-Clock Ratio and Peak-VRAM Ratio are defined in the same way. Method Throughput (token/s) Throughput Ratio Wall-Clock Ratio Peak-VRAM Ratio Baseline RKD SALT LET Baseline RKD SALT LET 224.2k 211.1k 221.5k 220.8k 105.9k 98.3k 104.3k 104.2k 1.4B Model 1.000 0.9415 0.9880 0.9848 7B Model 1.000 0.9282 0.9849 0.9839 1.000 1.0621 1.0122 1.0154 1.000 1.0773 1.0153 1.0163 1.000 1.1742 1.1742 1. 1.000 1.0946 1.0946 1.0944 Table 3 summarizes the architectural configurations of the models used in our empirical analysis. Remarkably, the LET paradigm achieves significant improvements despite substantial architectural heterogeneity among these models. The differences span several dimensions, including hidden size, intermediate size, number of layers, number of attention heads, activation functions, and attention mechanisms. For example, activation functions vary across models, including ReLU [58], GeLU [36], SiLU [18], and SwiGLU [70]. Similarly, the attention variants include Full, which denotes standard multi-head attention [80], and GQA referring to Grouped Query Attention [2]. As shown in Table 4, we compare throughput, wall-clock time, and peak VRAM across methods. Notably, LET achieves lower peak VRAM than other methods requiring auxiliary models when training with large batch sizes. This efficiency stems from LETs focus on learning representations in rather than the larger logit space, thereby reducing memory overhead. It is worth noting that both LET and SALT only require auxiliary 23 models during the early training phase, resulting in minimal impact on wall-clock time and throughput compared to the baseline. While LET exhibits slightly higher wall-clock time than SALT, its lower peak VRAM under large batch training demonstrates considerable potential for scaling."
        },
        {
            "title": "F Hidden States Alignment",
            "content": "In this section, we provide detailed description of the projection component in section 2. RBSdM and h(L) In our LET framework, the hidden states extracted from the model and the model may differ in their hidden dimensionality. Specifically, let h(k) RBSdT denote the hidden representations at layer of the and the final layer of the , respectively, where is the batch size, is the sequence length, and dM, dT are the hidden dimensions. When dM = dT , we apply an projection operation to the hidden state h(k) to align dimension with that of the . Concretely, we apply linear interpolation along the hidden dimension for each token position independently. That is, for each token index {1, . . . , S} and each sample in the batch, the student representation vector h(k) M,i RdM is interpolated to produce h(k) M,i RdT . This operation treats the hidden dimension as 1D information. The interpolation formula for each interpolated coordinate {0, . . . , dT 1} is: M,i,j = (1 βj) h(k) h(k) M,i,uj + βj h(k) M,i,uj +1, (6) where the source index uj = dM1 that, the representations h(k) and h(L) dT 1 and βj = uj uj. This procedure preserves endpoint alignment. After are normalized and compared using the cosine similarity loss: Lproj = (cid:88) i=1 M,i h(L) h(k) M,i h(L) ,i ,i h(k) . (7) This alignment ensures that the cosine similarity loss can be computed, even when the model and have different hidden dimensions."
        },
        {
            "title": "G LogSum Loss Setting",
            "content": "Our LET design (Section 2) employs cosine similarity as the measure of similarity between the normalized representations of model and model . Here, we investigate alternative alignment objectives to assess potential performance improvements. Given that models and exhibit substantial differences in capacity in our setting, we note that the logsum loss demonstrates promising performance when applied to models with significant capacity gaps [56]. Motivated by this observation, we investigate the effect of replacing cosine similarity with logsum loss in the LET. As shown in Table 5, employing logsum loss consistently outperforms the Baseline, RKD, and SALT, and further improves upon LET. We attribute the effectiveness of logsum loss to its tendency to emphasize regions where representations between and diverge significantly, which provides explicit guidance by directing model to prioritize learning features with the largest discrepancies, which may be particularly beneficial for efficiently aligning the larger model with the pre-trained smaller model during early training stages."
        },
        {
            "title": "H Theoretical Analysis",
            "content": "We provide theoretical analysis of why LET promotes smoother optimization landscapes compared to non-early layer alignment. To facilitate analytical tractability, we focus on simplified setting: deep linear network, where the representation dimension is set to for both model and model . 24 Table 5 Comparison of average downstream task performance under 1-shot setting. LET-LogSum denotes LET with logsum loss, LET-CCA indicates LET using Canonical Correlation Analysis (CCA) for representation alignment, and LET represents the tokenizer-mismatch setting where the model uses OPT tokenizer while target models use SmolLM tokenizer."
        },
        {
            "title": "Method",
            "content": "Avg. Performance Relative Gain"
        },
        {
            "title": "Our Methods",
            "content": "LET-LogSum LET-CCA LET LET 41.6 41.4 42.9 43.7 42.7 42.3 43.6 - -0.2 +1.3 +2.1 +1.1 +0.7 +2.0 H.1 Setup We begin by specifying the notation that will be used in the subsequent analysis and proofs. Consider model with layers defined by: h(l+1) = (l)h(l), = 0, 1, . . . , 1 (8) Here, h(0) = Rd denotes the input, (l) Rdd are the weight matrices, and h(L) is the output. We define θ(l) = vec(W (l)) Rd2 as the vectorized parameters of layer l, and Θ = (θ(0), . . . , θ(L1)) RLd2 as the complete parameter vector. The total training objective is: Ltotal(Θ) = LNLL(Θ) + λ Lproj(Θ) (9) where LNLL and Lproj are defined as in Section 2. Our analysis focuses primarily on Lproj to explicitly isolate the structural impact of the alignment depth, as the task loss LNLL remains shared component across different settings. H.2 Hessian Structure Analysis We analyze the curvature properties of the loss landscape using the Hessian matrix. For the alignment loss at layer k: Lproj θ(j) This arises because the representation h(k) depends on the parameters {W (0), . . . , (k1)}. k. = 0, The Hessian of model exhibits structured block form Hproj = 2Lproj Θ Θ Hproj = (cid:32) (0:k) proj 0 (cid:33) 0 0 , where (0:k) proj Rkd2kd2 corresponds to parameters in layers 0, . . . , 1. For any and k, 2Lproj θ(i)θ(j) = θ(i) (cid:18) Lproj θ(j) (cid:19) = θ(i) 0 = 0, and, by symmetry of the Hessian, blocks with are also zero. 25 (10) (11) (12) H.3 Curvature Bound via Frobenius Norm We employ the Frobenius norm as measurable proxy for the curvature magnitude. Recalling that the spectral norm, denoted as 2, which dictates the Lipschitz smoothness constant, is upper-bounded by the Frobenius norm (i.e., A2 AF ), it follows that establishing tighter bound on the Frobenius norm implicitly constrains the maximal curvature."
        },
        {
            "title": "For a block matrix",
            "content": "M = (cid:19) (cid:18)A 0 0 0 , its Frobenius norm of the block matrix is identical to that of the upper-left block: This follows directly from the definition, since F = AF . 2 = Mij2 = (cid:88) i,j (cid:88) i,jA Aij2 = A2 . (13) (14) (15) We adopt the simplified deep linear network setting where all layers share the same structure. Let (i,j) denote the Hessian block corresponding to the second derivatives with respect to θ(i) and θ(j). To derive an analytical upper bound, and for analytical tractability, we postulate uniform bound on the Frobenius norms of all Hessian blocks. Specifically, we assume there exists constant > 0 such that for all layer pairs i, < L, the Hessian blocks satisfy Utilizing the established Hessian block structure together with the block matrix norm property, (i,j)F C. Hproj2 = (0:k) proj = k1 (cid:88) k1 (cid:88) i=0 j=0 (i,j) k2C 2, (16) (17) and taking square roots gives the bound. From the curvature upper bound Hproj(k)F C, it follows immediately that, for k1 < k2 < L, the theoretical upper bound on the total curvature for alignment at depth k1 is smaller than that for alignment at depth k2. This indicates that, within our bounding analysis, earlier alignment layers admit smaller upper bounds on curvature than later ones. In summary, under the simplified deep linear network model and the uniform Hessian block bound assumption, our analysis shows that LET incurs smaller theoretical upper bound on the additional curvature cost, thereby preserving more of the original optimization landscape than non-early alignment and ultimately yielding smoother landscape. Extending beyond this simplified setting, we empirically validate in Section 3 that the smooth optimization landscape induced by LET is consistently observed in modern model architectures."
        },
        {
            "title": "I Failure Mode Analysis and Layer Selection Strategies",
            "content": "In this section, we investigate scenarios where LET exhibits limitations and examine the impact of layer selection strategies on final performance. When employing GPT-2 [63] as the small model , LET underperforms the baseline. As shown in Table 6, we evaluate three configurations: LET-GPT2-Small pairs LET with GPT-2 Small as , LET-GPT2-Medium uses GPT-2 Medium, and RKD employs GPT-2 Small. The results reveal progressive improvement from RKD to LET-GPT2-Small to LET-GPT2-Medium, though all variants underperform the baseline. We attribute this degradation to the potentially lower quality of GPT-2s training data (with cutoff of late 2017) compared to modern language models. Consequently, GPT-2s representations fail to provide effective alignment signals. Notably, LET consistently outperforms RKD, demonstrating superior robustness to model quality. 26 Method Avg. 41.6 Baseline 39.2 RKD LET-GPT2-Small 40.7 LET-GPT2-Medium 41.1 -2.4 -0.9 -0.5 L1-F1 L1-F3 L1-F5 L3-F 43.2 43.6 43.4 43.3 Table 6 Average 1-shot performance across downstream tasks when employing GPT-2 variants as the small model , which was pre-trained on data up to late 2017. 42. 43 43.2 43.4 43.6 43.8 Avg. Performance Figure 12 Impact of layer selection strategies Aligning the final layer of with earlier layers of M, specifically the third layer, yields optimal performance gains. As illustrated in Figure 12, we use SmolLM-135M as and evaluate different pairing strategies, where L1-F1 denotes aligning the last layer of with the first layer of M, with analogous notation for other layer pairs. The results demonstrate that L1-F3 achieves the best performance, which suggests that the first layer may primarily encode input-specific information. The inferior performance of L1-F5 compared to L1-F3 indicates that the third layer strikes an optimal balance for representation alignment."
        },
        {
            "title": "J Descriptions of Evaluation Tasks",
            "content": "We briefly describe each downstream evaluation task used in our experiments, which are intended to help interpret the one-shot performance results reported in the main experiment section 3. HellaSwag (HS) [104]: commonsense reasoning benchmark where the model needs to choose the most plausible sentence to follow given context from options. The task is designed to be adversarial against language models through counter-intuitive distractors. Winogrande (Wino.) [44]: coreference resolution benchmark that evaluates the models ability to resolve pronouns in sentences requiring commonsense knowledge. It is based on the Winograd schema challenge, scaled up in size and difficulty. LAMBADA (LAMB) [61]: word prediction task where the model needs to predict the final word of passage. The passages are filtered to require broad contextual understanding beyond the last sentence. OpenbookQA (OBQA) [55]: multiple-choice question answering task that combines small open book of science facts with commonsense reasoning. The model must integrate both explicit knowledge and inference. ARC (ARC-c and ARC-e) [15]: science question answering benchmark divided into two subsets. The easy (ARC-e) set consists of questions that can often be answered with simple reasoning or basic science knowledge, while the challenge (ARC-c) set includes more difficult questions requiring complex inference or broader background knowledge. PIQA [6]: physical common sense reasoning task involving everyday scenarios. The model must select the more plausible solution among candidates for completing an action. SciQ [85]: science multiple-choice QA dataset with questions crowd-sourced and aligned to middle school science curricula. The task requires mixture of factual recall and reasoning. BoolQ [14]: binary (yes/no) question answering task over short passages. The model must decide whether the answer to the question is entailed by the given passage."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}