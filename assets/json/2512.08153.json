{
    "paper_title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
    "authors": [
        "Zheng Ding",
        "Weirui Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4$\\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 5 1 8 0 . 2 1 5 2 : r TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models Zheng Ding UC San Diego zhding@ucsd.edu Weirui Ye MIT ywr@csail.mit.edu"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains major barrier to widespread adoption. We introduce TreeGRPO, novel RL framework that dramatically improves training efficiency by recasting the denoising process as search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4 faster training while establishing superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in visual generative models, particularly diffusion models Ho et al. (2020); Rombach et al. (2022); Podell et al. (2023) and rectified flows Lipman et al. (2022); Liu et al. (2022); Esser et al. (2024), have achieved state-of-the-art fidelity in image and video generation. Although largescale pre-training establishes strong data priors, incorporating human feedback during post-training is crucial to align model outputs with human preferences and aesthetic criteria Gong et al. (2025). Inspired by the success of reinforcement learning (RL) in aligning large language models (LLMs), researchers have begun adapting RL to visual generative models. Early methods like DDPO Black et al. (2023) and DPOK Fan et al. (2023) demonstrated feasibility but faced challenges in scalability and stability. The introduction of GRPO (Shao et al., 2024) and its adaptations, such as DanceGRPO Xue et al. (2025) and FlowGRPO Liu et al. (2025), provides PPO-style update framework based on group-relative advantages. However, these GRPO-based methods suffer from two critical limitations: (1) poor sample efficiency, since each policy update requires sampling complete, computationally expensive denoising trajectories, and (2) coarse credit assignment, where single terminal reward is uniformly attributed to all denoising steps, obscuring the contribution of individual actions. While MixGRPO Li et al. (2025) attempts to reduce costs via hybrid sampling and sliding windows, it often sacrifices final performance for efficiency. In this work, we propose TreeGRPO, novel RL framework that introduces tree-structured advantages to overcome these limitations. Drawing inspiration from the exceptional sample efficiency of tree search in sequential decision-making domains like game playing Silver et al. (2016; 2017); Ye et al. (2021), we recognize that the fixed-horizon, stepwise nature of denoising makes diffusion/flow Equal Contribution 1 Figure 1: The proposed TreeGRPO achieves the best pareto performance across the rewards and training efficiency, where the single-GPU runtime is the normalized wall-clock time. In (a), following the normalized metrics in RL domains (Mnih et al., 2013), the nromalized reward scores here is calculated by (ğ‘Ÿ ğ‘Ÿğ‘ ğ‘‘3.5)/(ğ‘Ÿğ‘šğ‘ğ‘¥ ğ‘Ÿğ‘ ğ‘‘3.5), where the ğ‘Ÿğ‘šğ‘ğ‘¥ in the HPS, ImageReward, Asethetic, ClipScore reward models are {1.0, 2.0, 10.0, 1.0} respectively. generation particularly amenable to tree-based exploration. Our key insight is to recast the denoising process as search tree where we can efficiently explore multiple trajectories from shared prefixes. As illustrated in Figure 2 , the proposed TreeGRPO framework initiates from shared noise samples and branches strategically at intermediate steps, reusing common prefixes while exploring diverse completions. Specifically, at denoising step ğ‘¡, we expand ğ‘ candidate paths for ğ‘› subsequent steps before producing final images. These candidates are evaluated by reward models, and we backpropagate rewards through the tree to compute dense advantages for each edgeproviding more accurate credit assignment than uniform trajectory rewards. This design provides three principal benefits: (1) High Sample Efficiency: Achieving higher performance under the same training samples; (2) Precise Step-wise Credit Assignment: Reward backpropagation through the tree structure computes step-specific advantages, addressing coarse credit assignment; (3) Amortized Compute per Forward Pass: Multi-child branching generates multiple advantages per node, enabling multiple policy updates per forward pass. In terms of experiments, following prior works Xue et al. (2025); Li et al. (2025); Liu et al. (2025), we employ HPS-v2.1 (Wu et al., 2023), ImageReward (Xu et al., 2023), Aesthetics (Wu et al., 2023), and ClipScore (Radford et al., 2021) as reward models. We report both single-reward (HPS-v2.1 only) and multi-reward (HPS-v2.1 and CLIPScore) settings, and evaluate on all four rewards. Our results demonstrate that TreeGRPO achieves 23 faster training convergence while outperforming baselines, establishing superior Pareto trade-off between efficiency and final reward (1). Our main contributions are: We introduce TreeGRPO, tree-structured RL framework for fine-tuning visual generative models that enables exploration through branching and prefix reuse. We develop precise credit assignment mechanism that backpropagates rewards through the tree to compute step-specific advantages. We demonstrate significant efficiency and performance gains, including 2.4 improvement in training efficiency and consistent improvements across multiple reward models. 2 Figure 2: Introduction of TreeGRPO: Our framework optimizes the denoising process of diffusion/flow models by constructing search trees. Starting from shared initial noise, it explores multiple trajectories by branching at intermediate steps, leveraging prefix reuse for step-wise advantages."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 RL Post-training for Generative Models Modern visual generative models are dominated by diffusion and flow-based approaches. Diffusion models learn to denoise Gaussian-corrupted data, supporting both stochastic (SDE) and deterministic (ODE) sampling Ho et al. (2020); Song et al. (2020). Flow matching methods learn velocity fields for continuous normalizing flows, with recent advances enabling efficient ODE-style sampling Lipman et al. (2022); Karras et al. (2022). Theoretical work has unified these approaches through stochastic interpolants and optimal-control perspectives Albergo & Vanden-Eijnden (2022); Domingo-Enrich et al. (2024). Alignment with human preferences remains key challenge. Current methods include direct reward optimization Lee et al. (2023); Xu et al. (2023), off-policy techniques like advantageweighted regression Peng et al. (2019), and preference-based learning (DPO, RAFT) that avoid explicit value functions Rafailov et al. (2023); Dong et al. (2023). Policy gradient methods (e.g., PPO) provide general RL frameworks for exploration-sensitive scenarios Schulman et al. (2017). Gao et al. (2024) reduce policy optimization to regressing the relative reward for generative models. The success of RL post-training in enhancing language models Jaech et al. (2024); Guo et al. (2025) has inspired similar approaches for visual generation. However, visual domains pose unique challenges for step-wise credit assignment along denoising trajectories. 2.2 Tree-based Reinforcement Learning Tree search methods combined with learned policies offer exceptional sample efficiency and precise credit assignment. The AlphaGo series demonstrated superhuman performance through neuralnetwork-guided search and pruning Silver et al. (2016; 2017), with later works confirming remarkable efficiency Ye et al. (2021). In language domains, tree-structured reasoning organizes inference as path search Yao et al. (2023), while recent RL methods leverage structured exploration to amplify training signals Jaech et al. (2024); Guo et al. (2025). Most related, Yang et al. (2025) applies tree-based optimization to language models, searching over token sequences. Our work adapts treestructured search to denoising processes. TreeGRPO leverages shared noise prefixes across branches for efficiency while enabling step-level credit assignment via reward backpropagation."
        },
        {
            "title": "3 Background",
            "content": "We briefly review flow matching and its formulation as Markov decision process (MDP) for RL fine-tuning. We then introduce an ODESDE conversion that enables stochastic, probability-aware sampling while preserving marginal distributions, which is essential for policy-gradient RL. Finally, we contextualize our approach within existing RL methods."
        },
        {
            "title": "3.1 Flow Matching and Rectified Flows",
            "content": "Flow matching models define probability path between data ğ‘¥0 ğ‘data and noise ğ‘¥1 ğ‘noise through linear interpolation: ğ‘¥ğ‘¡ = (1 ğ‘¡)ğ‘¥0 + ğ‘¡ğ‘¥1, velocity field ğ‘£ ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡) is trained to predict the direction ğ‘¥1 ğ‘¥0 using the objective: ğ‘¡ [0, 1]. LFM(ğœƒ) = Eğ‘¡ ,ğ‘¥0,ğ‘¥1 (cid:2)ğ‘£ ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡) (ğ‘¥1 ğ‘¥0)2 (cid:3) . Inference follows the probability-flow ODE ğ‘‘ğ‘¥ğ‘¡ = ğ‘£ ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡)ğ‘‘ğ‘¡ or its stochastic variant. (1) (2) 3.2 Denoising as Markov Decision Process We formulate generation as finite-horizon MDP (S, A, ğ‘ƒ, ğ‘…) where state ğ‘ ğ‘¡ = (ğ‘, ğ‘¡, ğ‘¥ğ‘¡ ) includes conditioning information ğ‘ (e.g., text prompts), timestep ğ‘¡, and current latent ğ‘¥ğ‘¡ . Actions ğ‘ğ‘¡ parameterize transitions ğ‘¥ğ‘¡ ğ‘¥ğ‘¡+1, and terminal reward ğ‘…(ğ‘¥ğ‘‡ , ğ‘) is provided by preference metrics. The RL objective maximizes: ğ½ (ğœƒ) = Eğœ ğœ‹ğœƒ [ğ‘…(ğ‘¥ğ‘‡ , ğ‘)] , ğœ = (ğ‘ 0, ğ‘0, . . . , ğ‘ ğ‘‡ ), (3) enabling optimization of black-box rewards inaccessible to supervised training. 3.3 ODE to SDE Conversion for Policy Gradients Deterministic ODE solvers lack the transition probabilities required by policy-gradient RL. Following Song et al. (2020); Albergo et al. (2023), we convert the probability-flow ODE ğ‘‘ğ‘¥ğ‘¡ = ğ‘“ ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡)ğ‘‘ğ‘¡ to an equivalent SDE that admits tractable likelihoods while preserving marginals: (cid:20) ğ‘‘ğ‘¥ğ‘¡ = ğ‘“ ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡) + 1 ğœ2 (ğ‘¡)ğ‘¥ log ğ‘ ğœƒ (ğ‘¥ğ‘¡ ğ‘, ğ‘¡) (cid:21) ğ‘‘ğ‘¡ + ğœ(ğ‘¡)ğ‘‘ğ‘Šğ‘¡ . (4) (5) Here ğœ(ğ‘¡) controls the noise scale, with ğœ(ğ‘¡) 0 recovering the deterministic ODE. This stochastic formulation enables proper credit assignment while maintaining sample quality. 3.4 Comparison of RL Fine-tuning Methods DDPO/DPOK samples trajectories independently and normalizes advantages at the batch level. DanceGRPO introduces group advantages but requires full regenerations. Flow-GRPO adapts GRPO to flows with stochastic sampling, similar to DanceGRPO as they published at the same time. MixGRPO improves efficiency via ODE-SDE hybrid sampling but lacks fine-grained credit assignment. Our TreeGRPO approach formulates denoising as sparse tree rooted at shared noise. By branching strategically, it simultaneously achieves all three desiderata: prefix reuse enables superior training efficiency; reward backpropagation through the tree structure provides fine-grained step-wise credit assignment; and multi-child branching facilitates group advantage comparisons. This unified approach overcomes the fundamental limitations of trajectory-based methods that assign uniform credit and require independent sampling."
        },
        {
            "title": "4 Method",
            "content": "We present TreeGRPO, tree-structured reinforcement learning (RL) post-training framework for diffusion and flow-based generators. TreeGRPO (i) reuses shared denoising prefixes to markedly improve sample efficiency, (ii) assigns step-wise credit by propagating leaf rewards back through the tree to produce per-edge advantages, and (iii) optimizes GRPO-style objective on these per-edge advantages. At high level, TreeGRPO builds sparse search tree over fixed denoising horizon by branching only within scheduled SDE windows while using ODE steps elsewhere. Leaf rewards are group-normalized per prompt and then backed up to internal edges to yield dense advantages that weight the policy-gradient update."
        },
        {
            "title": "4.1 Problem Setup",
            "content": "We consider conditional generator given conditioning ğ‘ (e.g., text), denoising/flow horizon ğ‘¡ = 0, . . . , ğ‘‡1, and latent states ğ‘¥ğ‘¡ . Sampling is viewed as an MDP with state ğ‘ ğ‘¡ = (ğ‘, ğ‘¡, ğ‘¥ğ‘¡ ) and policy ğœ‹ ğœƒ (ğ‘ğ‘¡ ğ‘ ğ‘¡ ) that induces ğ‘¥ğ‘¡+1. terminal, non-differentiable reward ğ‘…(ğ‘¥ğ‘‡ , ğ‘) is provided by preference model. The post-training objective is max ğœƒ Eğœ ğœ‹ğœƒ (cid:2)ğ‘…(ğ‘¥ğ‘‡ , ğ‘)(cid:3), ğœ = (ğ‘ 0, ğ‘0, . . . , ğ‘ ğ‘‡ ). (6) This formulation permits direct optimization of black-box alignment signals while preserving the fixed-length trajectory structure of diffusion/flow sampling. 4.2 Overview of Tree-Advantage GRPO TreeGRPO addresses the sample inefficiency of standard reinforcement learning for diffusion models by leveraging tree-structured sampling and temporal credit assignment. The key insight is that denoising trajectories share common prefixes, allowing us to efficiently explore multiple branching paths from shared intermediate states. The framework operates in three phases: First, we construct sparse search tree where deterministic ODE steps preserve shared prefixes and stochastic SDE windows create strategic branching points. Second, we compute final rewards for all leaf nodes and propagate these rewards backward through the tree using log-probability-weighted average to assign step-wise advantages to each denoising action. Third, we optimize GRPO objective that uses these per-edge advantages to update the policy, with clipping for stability. This approach provides candidate diversity with linear computational cost in the number of SDE windows, while the advantage propagation enables fine-grained credit assignment that distinguishes the contribution of each denoising step to the final outcome. 4.3 Tree-Structured Sampler For given prompt ğ‘ and predefined window {0, . . . , ğ‘‡1}, we sample an initial noise ğ‘¥0 (0, ğ¼) and run fixed denoising schedule ğ‘¡ = 0, . . . , ğ‘‡1 with two kinds of steps: 1. ODE steps (no branching). If ğ‘¡ W, we apply deterministic update to every frontier node. This advances all paths without creating new branches and reuses common prefix across descendants. 2. SDE windows (branching). If ğ‘¡ W, each frontier node spawns ğ‘˜ children by adding small stochastic perturbation to the ODE mean update. For each child edge ğ‘’, we compute and store its sampling log-probability log ğœ‹ ğœƒold (ğ‘’) under the frozen sampler. Repeating this until ğ‘¡ = ğ‘‡ yields tree whose leaves share deterministic prefixes and differ only at the SDE windows. We then decode each leaf to an image and use the stored edge log-probabilities for advantage propagation and GRPO updates. 5 Algorithm 1 TreeGRPO: SDE-window branching, per-step advantages, GRPO update Require: Policy ğœ‹ ğœƒ ; sampler uses ğœ‹ ğœƒold; steps ğ‘‡; branch ğ‘; leaf cap ğ‘; SDE window ğ‘Š (ğ‘™); rewards {ğ‘…ğ‘˜ } with stats (ğœ‡ğ‘˜, ğœğ‘˜) 1: for all prompt ğ‘ do 2: 3: 4: 5: 6: 7: else Sample shared seed x0 (0, ğ¼); init root (x0, ğ‘¡=0) for ğ‘¡ = 0 to ğ‘‡ 1 do if ğ‘¡ ğ‘Š (ğ‘™) then for all frontier node ğ‘¢ do for ğ‘— = 1 to ğ‘ do ğ‘£ SDE step(ğ‘¢, ğœ‹ ğœƒold , ğ‘, ğ‘¡); record edge log ğœ‹ ğœƒold Build tree SDE branching ODE continuation for all frontier node ğ‘¢ do ğ‘£ ODE step(ğ‘¢, ğœ‹ ğœƒold Decode leaves {x(ğ‘–) for all leaf edge ğ‘’ = ( ğ‘ ğ‘–) do ğ‘‡ } {y(ğ‘–) }; ğ‘Ÿ (ğ‘–) = (cid:205)ğ‘˜ ğ´edge(ğ‘’) ğ‘Ÿ (ğ‘–) ğœ‡ ğœ , ğ‘, ğ‘¡); record edge log ğœ‹ ğœƒold ğ‘…ğ‘˜ (y(ğ‘–) ,ğ‘) ğœ‡ğ‘˜ ğœğ‘˜ ; set ğœ‡, ğœ over {ğ‘Ÿ (ğ‘–) } for ğ‘¡ = ğ‘¡max(ğ‘Š (ğ‘™)) 1 down to ğ‘¡min(ğ‘Š (ğ‘™)) do for all internal node ğ‘¢ at time ğ‘¡+1 with child edges S(ğ‘¢) do ğœ‹(ğ‘’) softmax(cid:0){log ğœ‹ ğœƒold (ğ‘’) : ğ‘’ S(ğ‘¢)}(cid:1) ğ´node (ğ‘¢) (cid:205)ğ‘’ (ğ‘¢) ğœ‹(ğ‘’) ğ´edge (ğ‘’) ğ´edge ( ğ‘ ğ‘¢) ğ´node(ğ‘¢) post-order backup LGRPO = (cid:205)ğ‘¡ ğ‘Š (ğ‘™) (cid:205)ğ‘’ Eğ‘¡ log ğœ‹ ğœƒ (ğ‘ğ‘¡ (ğ‘’) xğ‘¡ (ğ‘’), ğ‘, ğ‘¡) ğ´edge(ğ‘’); ğœƒ ğœƒ ğœ‚ğœƒ LGRPO 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 4.4 Random Window We select single contiguous SDE window of fixed length ğ‘¤ along ğ‘‡-step denoising schedule with timesteps indexed 0, . . . , ğ‘‡1. For start index ğ‘–, the window is Wğ‘– = { ğ‘–, ğ‘–+1, . . . , ğ‘–+ğ‘¤1 }, ğ‘– {0, 1, . . . , ğ‘‡ ğ‘¤ 1}. (7) At the beginning of each training epoch, we draw the start ğ‘– from truncated geometric distribution over {0, . . . , ğ‘‡ ğ‘¤ 1} with parameter ğ‘Ÿ (0, 1): Pr[ğ‘–] = (1 ğ‘Ÿ) ğ‘Ÿ ğ‘– 1 ğ‘Ÿ ğ‘‡ ğ‘¤ , ğ‘– = 0, 1, . . . , ğ‘‡ ğ‘¤ 1. (8) This distribution places more mass on earlier timesteps when ğ‘Ÿ is small and becomes closer to uniform as ğ‘Ÿ 1. In practice, this early-time bias is desirable because post-training primarily targets corrections in the initial denoising stages. 4.5 Leaf Advantages Calculation For each prompt ğ‘ with leaf set (ğ‘), we first aggregate raw reward scores from one or more evaluators {ğ‘…ğ‘˜ } using nonnegative weights {ğ‘¤ ğ‘˜ } (typically uniform): ğ‘† (â„“ ) = ğ‘˜ ğ‘¤ ğ‘˜ ğ‘…ğ‘˜(cid:0)ğ‘¦ (â„“ ) , ğ‘(cid:1), â„“ (ğ‘), ğ‘¤ ğ‘˜ 0, ğ‘¤ ğ‘˜ = 1. (9) ğ‘˜ Let ğœ‡ğ‘ and ğœğ‘ be the mean and standard deviation of {ğ‘† (â„“ ) }â„“ (ğ‘) . The leaf advantages are computed within the prompt group as ğ´leaf(â„“) = ğ‘† (â„“ ) ğœ‡ğ‘ ğœğ‘ , â„“ (ğ‘), (10) . These prompt-conditioned leaf advantages serve as boundary conditions for the subsequent tree backup to obtain per-edge advantages."
        },
        {
            "title": "4.6 Leaf-to-Root Advantage Propagation",
            "content": "We convert leaf-level advantages into per-step (edge) advantages by bottom-up pass over the tree. For an internal node ğ‘¢, let ğ‘†(ğ‘¢) be the set of outgoing child edges and let ğ‘’ = ( ğ‘ ğ‘¢) denote the incoming edge of ğ‘¢. Each child edge ğ‘’ ğ‘†(ğ‘¢) stores (i) its advantage ğ´edge(ğ‘’) and (ii) its sampling log-probability log ğœ‹ ğœƒold (ğ‘’) from the frozen sampler. Define logprob-based mixture weights by normalizing the stored probabilities; equivalently, take softmax over the stored log-probabilities: ğ‘¤ğ‘¢ (ğ‘’) = exp(cid:0) log ğœ‹ ğœƒold (ğ‘’)(cid:1) (cid:205)ğ‘’ ğ‘† (ğ‘¢) exp(cid:0) log ğœ‹ ğœƒold (ğ‘’)(cid:1) = ğœ‹ ğœƒold (ğ‘’) (cid:205)ğ‘’ ğ‘† (ğ‘¢) ğœ‹ ğœƒold (ğ‘’) , ğ‘’ ğ‘†(ğ‘¢). (11) The advantage assigned to the incoming edge of ğ‘¢ is the weighted average of its children: ğ´edge (ğ‘’) = ğ‘¤ğ‘¢ (ğ‘’) ğ´edge(ğ‘’). (12) ğ‘’ğ‘† (ğ‘¢) When ğ‘†(ğ‘¢) = 1, Eq. equation 12 reduces to identity and the parents edge advantage equals that of its unique child. Applying equation 12 in reverse topological order yields distinct per-timestep advantages for all internal edges up to the root. 4.7 GRPO Update with Per-Edge Advantages For consistency with our setting, we describe the update as GRPO: it is the standard PPO clipped surrogate applied to group-relative, per-edge advantages. For each SDE-window edge ğ‘’ Eğ‘¡ with stored behavior log-probability log ğœ‹ ğœƒold (ğ‘ğ‘¡ (ğ‘’) ğ‘¥ğ‘¡ (ğ‘’), ğ‘, ğ‘¡), define ğ‘Ÿğ‘¡ (ğ‘’; ğœƒ) = exp (cid:16) log ğœ‹ ğœƒ (ğ‘ğ‘¡ (ğ‘’) ğ‘¥ğ‘¡ (ğ‘’), ğ‘, ğ‘¡) log ğœ‹ ğœƒold (ğ‘ğ‘¡ (ğ‘’) ğ‘¥ğ‘¡ (ğ‘’), ğ‘, ğ‘¡) (cid:17) . The GRPO (clipped) objective over all SDE-window edges is LGRPO (ğœƒ) = (cid:16) min ğ‘¡ ğ‘’ Eğ‘¡ ğ‘Ÿğ‘¡ (ğ‘’; ğœƒ) ğ´edge (ğ‘’), clip(cid:0)ğ‘Ÿğ‘¡ (ğ‘’; ğœƒ), 1 ğœ–, 1 + ğœ– (cid:1) ğ´edge(ğ‘’) (cid:17) , (13) with clip parameter ğœ– (no explicit KL term). We optimize equation 13 and periodically refresh the behavior policy by setting ğœƒold ğœƒ. In short, GRPO here is PPO with prompt-relative, per-edge advantages computed by our tree backup."
        },
        {
            "title": "5 Theoratical Analysis of TreeGRPO",
            "content": "In this section, we provide theoretical justification for the efficacy of TreeGRPO. We highlight that the tree-structured advantage estimation acts as principled method for variance reduction and robustness regularization through weighted averaging based on action probabilities. 5.1 Variance Reduction via Weighted Aggregation Standard RL fine-tuning methods such as vanilla GRPO estimate the gradient using Monte Carlo samples of single trajectories. In contrast, TreeGRPO aggregates information from multiple branches ğ‘˜ {1, . . . , ğ¾ } originating from shared state ğ‘ ğ‘¡ . Crucially, this aggregation is probabilityweighted average rather than simple arithmetic mean. Let ğ‘¤ ğ‘˜ be the normalized weight for the ğ‘˜-th branch, derived from the policys log-probabilities: exp(log ğœ‹ ğœƒold (ğ‘ (ğ‘˜ ) ğ‘ ğ‘¡ )) ğ‘—=1 exp(log ğœ‹ ğœƒold (ğ‘ ( ğ‘— ) ğ‘¤ ğ‘˜ = ğ‘ ğ‘¡ )) (14) (cid:205)ğ¾ . ğ‘¡ ğ‘¡ The advantage estimator for the parent node is computed as Ë†ğ´tree(ğ‘ ğ‘¡ ) = (cid:205)ğ¾ ğ‘˜=1 Proposition 5.1 (Variance Reduction with Weighted Estimator). Let ğœ2 env be the variance of the reward realization due to future diffusion noise. The variance of the TreeGRPO weighted estimator is strictly less than or equal to the variance of single-sample estimator, provided the effective sample size is greater than 1. ğ‘¤ ğ‘˜ Ë†ğ´(ğ‘˜ ) leaf. 7 Proof. The variance of single-sample estimator (standard GRPO) is Var( Ë†ğ´single) = ğœ2 TreeGRPO estimator Ë†ğ´tree = (cid:205)ğ¾ ğ‘˜=1 ğ‘ ğ‘¡ , the variance is: env. For the ğ‘¤ ğ‘˜ Ë†ğ´(ğ‘˜ ) , assuming conditional independence of branches given ğ¾ (cid:32) ğ¾ (cid:33) Var( Ë†ğ´tree) = ğ‘¤2 ğ‘˜Var( Ë†ğ´(ğ‘˜ ) ) = ğ‘¤2 ğ‘˜ ğœ2 env . (15) ğ‘¤ ğ‘˜ = 1 and ğ‘¤ ğ‘˜ (0, 1), it implies that (cid:205)ğ¾ ğ‘˜=1 Since (cid:205)ğ¾ are 0). The quantity 1/((cid:205) ğ‘¤2 reduced by factor equal to the ESS: ğ‘˜= ğ‘˜=1 ğ‘˜ < 1 (unless one weight is 1 and others ğ‘˜) represents the effective sample size (ESS). Thus, the variance is ğ‘˜=1 ğ‘¤2 Var( Ë†ğ´tree) ğœ2 env ESS . (16) This variance reduction leads to more stable gradient estimates and larger trust-region updates. Our experiment also shows that increasing the branch number will result in better performances in general, which proves this empirically."
        },
        {
            "title": "5.2 Regularization and Robustness",
            "content": "The weighted average structure also provides conceptual link to regularization against noise overfitting. In diffusion models, high reward might be obtained purely by chance due to specific noise seed (a sharp peak in the reward landscape), even if the action probability is low. Proposition 5.2 (Weighted Averaging as Smoothness Regularization). By calculating advantages as an expectation over the local policy distribution (approximated by the weighted tree), TreeGRPO optimizes smoothed objective that penalizes solutions with high local curvature (sharp peaks). Proof. The standard update maximizes ğ½ (ğœƒ) Ë†ğ´(ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) log ğœ‹(ğ‘ğ‘¡ ). If Ë†ğ´ comes from single lucky path, the policy may collapse to deterministic action that is brittle to noise. TreeGRPO assigns the parent value based on ğ‘‰tree(ğ‘ ğ‘¡ ) = (cid:205)ğ‘˜ ğ‘¤ ğ‘˜ğ‘„(ğ‘ ğ‘¡ , ğ‘ğ‘˜). This approximates the true value function ğ‘‰ ğœ‹ (ğ‘ ğ‘¡ ) = Eğ‘ ğœ‹ [ğ‘„(ğ‘ ğ‘¡ , ğ‘)]. Consider the Taylor expansion of the expected reward around the mean outcome. Maximizing the weighted average effectively maximizes: Eğ‘ ğœ‹ [ğ‘„(ğ‘ ğ‘¡ , ğ‘)] ğ‘„(ğ‘ ğ‘¡ , ğœ‡ğ‘) + 1 2 Tr(Î£ ğœ‹ 2 ğ‘ğ‘„(ğ‘ ğ‘¡ , ğ‘)). (17) By explicitly using multiple samples weighted by ğœ‹, the optimization signal favors regions where the expected return is high (high ğ‘„) AND where the surrounding region is robust (the second-order term 2ğ‘„ is not largely negative). This acts as an implicit regularizer, discouraging the policy from converging to sharp, narrow optima where slight deviations in sampling (noise) would lead to collapse in reward. This theoretical property aligns with the improved Pareto frontier observed empirically. In summary, the log-probability weighted aggregation in TreeGRPO is mathematically equivalent to performing Rao-Blackwellization on the advantage estimator (Prop. 5.1) and implicitly optimizing smoothness-regularized objective (Prop. 5.2)."
        },
        {
            "title": "6 Experiment",
            "content": "We evaluate TreeGRPO against previous methods under identical sampling budgets (NFE=10) and report both efficiency (per-iteration wall clock) and alignment metrics across multiple reward models. We use HPDv2 dataset for both training and evaluation across all the methods. 6.1 Experimental Setup Foundation Models and Datasets We use SD3.5-medium as our base model, following recent works on diffusion model alignment. For training and evaluation, we use the HPDv2 dataset (Wu et al., 2023), which contains 103,700 text prompts focused on human preference alignment. The evaluation is performed on held-out set of 3,200 prompts to ensure fair comparison. 8 Table 1: Train on HPS-v2.1 reward model and Eval on four reward models. Here are the comparison results for overhead and performance. Method Iter. Time (s) Human Preference Alignment HPS-v2.1 ImageReward Aesthetic ClipScore SD3.5-M"
        },
        {
            "title": "DDPO\nDanceGRPO\nMixGRPO",
            "content": "TreeGRPO (Ours) - 166.1 173.5 145.4 72.0 0.2725 0.2758 0.3556 0. 0.3735 0.8870 1.0067 1.3668 1.2263 1.3294 5.9519 5.9458 6.3080 6. 6.5094 0.3996 0.3900 0.3769 0.3612 0.3703 Table 2: Train on HPS-v2.1 and ClipScore reward model with ratio 4:1 and Eval on four reward models. Here are the comparison results for overhead and performance. Method Iter. Time (s) Human Preference Alignment HPS-v2.1 ImageReward Aesthetic ClipScore SD3.5-M DDPO DanceGRPO MixGRPO TreeGRPO (Ours) - 178.2 184.0 152.0 79.2 0.2725 0.2748 0.3485 0. 0.364 0.8870 1.0061 1.3930 1.2056 1.3426 5.9519 5.8500 6.3224 6. 6.4237 0.3996 0.3884 0.3862 0.3812 0.3830 Reward Models We employ four different reward models to evaluate comprehensive alignment with human preferences: HPSv2.1 (Wu et al., 2023), ImageReward (Xu et al., 2023), Aesthetic Score (Wu et al., 2023), and ClipScore (Radford et al., 2021). These models capture different aspects of human judgment - HPSv2.1 and ImageReward focus on overall preference, Aesthetic Score evaluates visual appeal, and ClipScore measures text-image alignment. We conduct experiments under both single-reward (HPSv2.1 only) and multi-reward training settings. Training Configuration All methods are trained with fixed NFE (Number of Function Evaluations) budget of 10 steps. We use batch size of 32 and train for 250 epochs with the AdamW optimizer (learning rate 1e-5, weight decay 0.01). Training is conducted on 8A100 GPUs with mixed precision. The same random seed is used across all experiments for reproducibility. (1) DDPO (Black et al., 2023): Uses Baselines We compare against three strong baselines: PPO for diffusion denoising with batch advantage estimation; (2) DanceGRPO (Xue et al., 2025): Applies GRPO with group-based advantage calculation for same prompts; (3) MixGRPO (Li et al., 2025): Combines ODE and SDE sampling during inference with GRPO updates. All baselines are re-implemented and trained under identical conditions for fair comparison. 6.2 Main Results Single-Reward Training Table 1 shows results when training with only HPSv2.1 reward. TreeGRPO achieves the best HPSv2.1 score (0.3735) and aesthetic score (6.5094) while being significantly faster (72.0s/iteration) than all baselines. DanceGRPO achieves the highest ImageReward score but is 2.4 slower than our method. Multi-Reward Training For multi-reward setting, we use advantage-weighted summation instead of direct reward addition. Specifically, we set the ratio of HPSv2.1 reward and ClipScore reward to 0.8:0.2 (ğ‘¤0 = 0.8, ğ‘¤1 = 0.2). We calculate leaf-advantages ğ´1, ğ´2 for each reward model, and obtain the weighted advantage ğ´ = (cid:205)ğ‘–=0,1 ğ‘¤ğ‘– ğ´ğ‘– as the final advantage. This advantage is then backpropagated through the tree structure using our proposed method. Table 2 demonstrates that 9 Table 3: Ablation on sample tree structure. We set NEF to 10 as the default, and train the models on different search tree structure. Method Tree# EffGrp EffSteps Time (s) HPSv2 ImgRwd Aesth. CLIP ğ‘˜=3, ğ‘‘=3 ğ‘˜=3, ğ‘‘=3 ğ‘˜=2, ğ‘‘=3 ğ‘˜=2, ğ‘‘=3 ğ‘˜=2, ğ‘‘=4 ğ‘˜=4, ğ‘‘= 1 2 1 2 1 1 27 54 8 16 16 64 13 26 7 14 15 70.0 120.2 39.4 60.0 59.6 126.3 0.3735 0.3771 0.3271 0.3381 0.3537 0.3822 1.3294 1.3229 1.0650 1.1002 1.1725 1. 6.5094 6.4598 6.2736 6.3472 6.2955 6.4201 0.3703 0.3659 0.3738 0.3584 0.3575 0.3664 Notes. ğ‘˜ is the branching factor, ğ‘‘ is the depth, and Tree Num is the number of trees for each prompt. Iteration time is per-step wall clock (lower is better). EffGrp is effective group size which is the number of generated images for the same prompt. EffSteps is the effective training steps per prompt. Table 4: Ablation of inference strategies during sampling. ImageReward Aesthetic ClipScore 1.3294 6.5094 0.3703 Sampling Strategy HPSv2 Random, ğ‘Ÿ = 0.5 Random, ğ‘Ÿ = 0.3 Random, ğ‘Ÿ = 0.7 Shifting 0.3632 0.3576 0. 0.3735 1.2815 1.2384 1.3207 Notes. ğ‘Ÿ is the ratio parameter in randome window. The smaller ğ‘Ÿ will choose the frontier noise step to expand search tree in larger probability. 0.3556 0.3611 0.3738 6.6067 6.2161 6.2736 TreeGRPO maintains strong performance across all metrics while being 2.4x faster than DanceGRPO, showing particular strength in ImageReward (1.3426) and aesthetic scores (6.4237). Efficiency Analysis The significant speed advantage of TreeGRPO (72.0-79.2s vs 145.4-184.0s for baselines) comes from our tree-based parallel sampling strategy, which maximizes trajectory diversity within the same NFE budget while minimizing computational overhead through efficient advantage backpropagation. 6.3 Ablation Studies Tree Structure Analysis Table 3 investigates how different tree configurations affect performance. As for Optimal Branching, ğ‘˜ = 3, ğ‘‘ = 3 provides the best trade-off between performance and efficiency. Larger branching (ğ‘˜ = 4) improves HPSv2.1 score to 0.3822 but increases computation In terms of Depth Impact, Deeper trees (ğ‘‘ = 4) provide more training steps but time by 75%. show diminishing returns. ğ‘˜ = 2, ğ‘‘ = 4 offers good efficiency but lower overall performance. For Multiple Trees, Using 2 trees with ğ‘˜ = 3, ğ‘‘ = 3 improves HPSv2.1 score marginally (0.3771 vs 0.3735) but doubles computation time, suggesting limited practical benefit. Sampling Strategy Analysis Table 4 compares different inference strategies during tree sampling: (1) Random Window Sampling: The default ğ‘Ÿ = 0.5 provides balanced performance. Smaller ğ‘Ÿ = 0.3 prioritizes aesthetic quality (6.6067) at the cost of text alignment, while ğ‘Ÿ = 0.7 shows the opposite trade-off. (2) Shifting Strategy: Achieves best ClipScore (0.3738) but compromises (3) Adaptive Sampling: Our on other metrics, making it suitable for text-heavy applications. experiments show that dynamic adjustment of ğ‘Ÿ during training based on reward progress can provide additional 2-3% improvement, though we use fixed ğ‘Ÿ = 0.5 for simplicity in main results. Advantage Weighting Analysis We ablate the multi-reward weighting strategy by comparing equal weighting (0.5:0.5) against our chosen ratio (0.8:0.2). The 0.8:0.2 ratio provides better balance across all evaluation metrics, while equal weighting tends to over-optimize for ClipScore at the expense of other rewards."
        },
        {
            "title": "7 Discussion",
            "content": "Our work introduces TreeGRPO, novel RL framework that overcomes the prohibitive computational cost of aligning visual generative models by recasting the denoising process as tree search, achieving exponential sample efficiency through strategic branching and prefix reuse, and enabling fine-grained credit assignment via reward backpropagation. While this approach establishes superior Pareto frontier in efficiency and performance, its current limitations include the introduction of new hyperparameters governing the tree structure and an increased memory footprint during training. Future work will focus on developing adaptive scheduling for these parameters, integrating learned value functions for early tree pruning, and extending the framework to more computationally intensive domains like video and 3D generation to further enhance its scalability and impact. Apart from GRPO, such tree-based advantages can also be applied to other methods for post-training."
        },
        {
            "title": "References",
            "content": "Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Zhaolin Gao, Jonathan Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kiante Brantley, Thorsten Joachims, Drew Bagnell, Jason Lee, and Wen Sun. Rebel: Reinforcement learning via regressing relative rewards. Advances in Neural Information Processing Systems, 37: 5235452400, 2024. Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 11 Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. 12 Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference. arXiv preprint arXiv:2303.14420, 1(3), 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. Treerpo: Tree relative policy optimization. arXiv preprint arXiv:2506.05183, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in neural information processing systems, 34:2547625488, 2021."
        }
    ],
    "affiliations": [
        "MIT",
        "UC San Diego"
    ]
}