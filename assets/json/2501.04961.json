{
    "paper_title": "Demystifying Domain-adaptive Post-training for Financial LLMs",
    "authors": [
        "Zixuan Ke",
        "Yifei Ming",
        "Xuan-Phi Nguyen",
        "Caiming Xiong",
        "Shafiq Joty"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: https://github.com/SalesforceAIResearch/FinDap"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 6 9 4 0 . 1 0 5 2 : r Demystifying Domain-adaptive Post-training for Financial LLMs Zixuan Ke, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong and Shafiq Joty Salesforce AI Research {zixuan.ke,yifei.ming,xnguyen,cxiong,sjoty}@salesforce.com"
        },
        {
            "title": "Abstract",
            "content": "Domain-adaptive post-training of large language models (LLMs) has emerged as promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on novel preference data distillation method, which leverages process signals from generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: https://github.com/SalesforceAIResearch/FinDap"
        },
        {
            "title": "Introduction",
            "content": "While large language models (LLMs) demonstrate strong generalization across diverse tasks, they often struggle in specialized domains such as finance, law, and healthcare. Consequently, domainadaptive post-training of LLMs has garnered significant attention recently (Colombo et al., 2024b; Xie et al., 2024a). In the earlier days of language models, continual pre-training or CPT was the dominant strategy. This involved further training pre-trained model on domain-specific plain text and then fine-tuning it for individual tasks (Gururangan et al., 2020; Ke et al., 2023). While prompt engineering with zeroor few-shot examples has emerged as convenient way to adapt general-purpose LLMs to new tasks, recent methods focus on fine-tuning model weights to transform them into domain experts (Colombo et al., 2024a; Chen et al., 2023a; Li et al., 2023).1 Today, domain-adaptive post-training has evolved into collection of techniques including CPT, instruction-tunning (IT) and preference alignment (PA). Building on this trend, this work focuses on adapting LLMs to specific domains through parameter training, which complements semi-parametric methods that leverage external knowledge, such as in retrieval-augmented generation or RAG (Lewis et al., 2020; Ke et al., 2024). Our focus is also different from general post-training, as the goal is 1This shift is understandable not only because training typically leads to better results on specialized tasks, but also because training has become more affordable due to the growing number of techniques that enable efficient training. For example, the training cost was over $4M for GPT-3 (davinci), but it has recently dropped to around $0.8M to train GPT-3 on-par model like Phi3.5 (Maslej et al., 2024). not to develop another general-purpose frontier LLM but to create specialized, expert-level LLMs tailored to specific domain. Despite the need and aforementioned advancements in domain-specific post-training, there is still no systematic study on what makes good domain-specific LLM (see Table 2). In this paper, we wish to address the following research questions: Research Questions Given pre-trained LLM with strong general capabilities (e.g., Llama3-8b-instruct), how to effectively adapt it to target domain by post-training? What criteria are desirable for successful adaptation? What are effective training recipes with respect to data and model? common criterion for evaluating domain-specific LLMs is to test their performance on set of domain-specific tasks (Bhatia et al., 2024; Xie et al., 2024b). However, this simplified approach can often misalign with our broader expectations for domain-expert LLM. The absence of clear and comprehensive evaluation criteria has significantly hindered the development of robust training recipes, encompassing both data and model strategies. Furthermore, it remains uncertain whether simply using larger LLM will yield better performance in specific domain compared to carefully post-trained, domain-adaptive LLM. To address these questions methodically, we introduce FINDAP, novel finance-specific post-training framework comprising comprehensive evaluation framework, state-of-the-art model checkpoints and training recipe. We use the finance domain as case study to perform domain-adaptive post-training on an instruction-tuned LLM, Llama3-8b-instruct (LLaMA, 2024). Figure 1 outlines our approach. We begin by identifying set of core capabilities that domain-expert LLM should possess such as knowledge about concepts and tasks in the target domain and the associated reasoning processes (2.1). This identification is critical as it informs the entire exploration process. Based on these expected capabilities, we develop an evaluation framework (2.2) to establish clear performance goals and guide model improvement over selection of development and unseen tasks. The training recipe involves two stages. In the first stage (3.2 & 3.3), we conduct rigorous and principled experiments to determine the optimal data and model recipes for CPT and IT, resulting in the CPT+IT checkpoint. Subsequently, we further train the model using curated on-policy preference data for PA. We propose novel approach to construct preference data from the process signals given by generative reward model. Our best performing recipe yields Llama-Fin that outperforms all considered baselines, including large open models at the 70B scale and proprietary models like GPT-4o, on unseen tasks that are similar to the training data. Even on unseen and novel tasks never encountered in training, Llama-Fin remains competitive and consistently outperforms its base model across all identified capabilities. In summary, our key contributions are: comprehensive guidance for domain-specific post-training, including identification of capabilities, evaluation, data and model recipe design. systematic exploration on each stage of domain-specific post-training, with an emphasis on the goals, challenges and effective approaches at each stage. novel preference alignment approach that constructs preference data using on-policy trajectories guided by outcome and process signals from generative reward model. new state-of-the-art finance LLM (Llama-Fin) at the 8b parameter scale. Extensive evaluation with an open-source leaderboard, checkpoints, data and model recipes."
        },
        {
            "title": "2 FINDAP Setup",
            "content": "In FINDAP, we first identify the desired capabilities and design the corresponding evaluation measures, and then formulate the training method. 2.1 FINDAP Capabilities We begin by illustrating the capabilities that are desirable for domain-specific LLM. Specifically, we focus on the following core capabilities: 2 Figure 1: An overview of FINDAP. Left: we first identify the core expected capabilities for the target domain and then curate texts and prompts for training and evaluation. Right: the top shows our training strategies. For each training stage, we use development set to select the best model. After training, we use unseen set to demonstrate the effectiveness of Llama-Fin. Domain specific concepts. domain typically includes its own specific concepts. For example, bond in finance refers to loan agreement between an investor and borrower. Adapting the LLM to domain-specific concepts is crucial, as these concepts form the fundamental building blocks of domain knowledge. However, this adaptation should not come at the cost of losing knowledge about general concepts, which are essential for both domain-specific and general tasks. Domain specific tasks. While many NLP tasks, such as NER or sentiment analysis, are shared across different domains, domain typically has its own tasks. For example, stock movement detection is primarily found in finance. Adapting LLMs to these domain-specific tasks is important, as it demonstrates how they can leverage domain-specific concepts to solve tailored tasks effectively. Reasoning. For complex tasks, reasoning with concepts is highly desired capability in LLMs. For example, in finance, the LLM is often required to analyze companys financial report, involving extensive reasoning, particularly mathematical reasoning, to compute key financial concepts such as market rate or earnings per share. Instruction-Following (IF) and chat. This is core capability for both general and domain-specific LLMs, as tasks are often presented in the form of instruction following or conversation. Others. Additionally, domains may vary significantly in their sensitivity. For instance, the medical domain is highly sensitive, requiring utmost accuracy and strict adherence to ethical considerations. In contrast, domains such as entertainment may have more relaxed requirements. Another important consideration is multi-modality, as some domains require handling multiple types of input and output formats. For example, the healthcare domain may involve processing medical images alongside textual reports, while the e-commerce domain may integrate product descriptions, images, and customer reviews into unified response. Similarly, scientific research often combines charts, graphs, and textual analysis to present findings effectively. While we acknowledge these additional aspects, we leave those for future work and concentrate on the four primary capabilities discussed above. 2.2 FINDAP Evaluation With the above breakdown of capabilities, our evaluation framework consists of suite for assessing these capabilities using development sets and unseen (held-out) evaluation sets. Our development set is directly split from the training data at each stage. Table 1 outlines the capabilities and the evaluation benchmarks selected to cover these capabilities. Crucially, we did not examine scores on our unseen 3 Capability Domain Task Evaluation Dataset Size Reference Tasks Finance Sentiment Analysis Unseen - Similar FPB FiQA SA FOMC Monetary policy Stance Named entity recognition NER Abstractive Summarization EDTSUM Total Concept General Knowledge Recall Tasks Finance Finance Extractive Summarization ESG Issue Classification MLESG Rumour Detection Stock Movement Prediction SM-Bigdata MA 970 Malo et al. (2014) 235 Maia et al. (2018) Shah et al. (2023) 496 98 Alvarado et al. (2015) 2,000 Zhou et al. (2021) 3,799 Unseen - Novel MMLU AI2-ARC Nq-open MMLU-Finance Flare-ECTSUM 14,042 (Hendrycks et al., 2021) 3,548 Clark et al. (2018) 7,842 Kwiatkowski et al. (2019) 1,460 - 495 Mukherjee et al. (2022) Chen et al. (2023b) 300 500 Yang et al. (2020) 1,470 Soun et al. (2022) 3,720 Xu and Cohen (2018) SM-ACL 1,140 Wu et al. (2018) SM-CIKM 2,280 Feng et al. (2024) CRA-CCF 2,100 Feng et al. (2024) CRA-CCFraud Hofmann (1994) 200 Flare-German 139 Quinlan (1987) Flare-Astralian 2,690 Feng et al. (2024) CRA-LendingClub 1,740 Feng et al. (2024) CRA-Polish 1,370 Feng et al. (2024) CRA-Taiwan 2,380 Feng et al. (2024) CRA-ProroSeguro CRA-TravelInsurance 2,530 Feng et al. (2024) 1,670 Zhu et al. (2021) Flare-TATQA Islam et al. (2023) 150 Finance Bench 80 MT-bench Zheng et al. (2023) 2,985 Amini et al. (2019a) MathQA 2,636 Welbl et al. (2017) Social-IQA 500 Mihaylov et al. (2018) Open-book-qa 10,003 Zellers et al. (2019) Hellaswag 1,767 Sakaguchi et al. (2019) Winogrande 3,000 Bisk et al. (2020) PIQA 1,030 Link CFA-Easy CFA-Challenge 90 91,872 - Fraud Detection Credit Scoring Distress Identification Claim Analysis Tabular QA Open QA General Precise IF Reasoning IF/Chat Reasoning Math General Social Reasoning Common Reasoning Finance Exam Total Table 1: Summary of our evaluation dataset. New datasets released with FINDAP are colorhighlighted for emphasis. set while developing the models, which allows us to observe how much we may have overfitted to particular evaluations in our decisions around training recipe. For the unseen tasks  (Table 1)  , we manually review each individual dataset and have the following considerations. Benchmarking tasks. Corresponding to the capabilities, we consider diverse set of benchmarking tasks. For concepts, we include knowledge tasks in the general domain, such as AI2-ARC (Clark et al., 2018), as well as in finance, such as MMLU-Finance (Hendrycks et al., 2021). For tasks, we consider general tasks, such as Social-IQA (Welbl et al., 2017), and domain-specific tasks, such as MLESG (Chen et al., 2023b). Notably, we intentionally include few financial tasks such as Flare-TATQA (Zhu et al., 2021) and SM-Bigdata (Soun et al., 2022) that require understanding of tabular data, as this data format is common in this domain. For IF/Chat capabilities, we utilize popular instruction-following benchmarks, such as MT-Bench (Zheng et al., 2023). For reasoning, we include general reasoning tasks, such as MathQA (Amini et al., 2019a) and Hellaswag common sense reasoning (Zellers et al., 2019), as well as domain-specific reasoning tasks, such as CRA-ProroSeguro claim analysis (Feng et al., 2024). We also construct new benchmark on CFA-Challenge based on CFA Level III, one of the most challenging financial exams that requires comprehensive reasoning (Khamnuansin et al., 2024; Callanan et al., 2024). Evaluation method. We split our evaluation set into two types based on their exposure to Instruction tuning (IT) data  (Table 1)  . The first type, Similar, includes tasks whose types have been encountered during training, even if the specific tasks themselves are unseen (e.g., new NER task). The second type, Novel, includes tasks whose types have not been seen during training, representing entirely new challenges for the model (e.g., stock movement prediction). We use two different evaluation methods based on the nature of the benchmarks. For knowledge and NLP tasks (e.g., NER), we employ straightforward direct answer evaluation. For reasoning tasks (e.g., CFA-Challenge), we use 0-shot chain-of-thought (CoT) (Wei et al., 2023) answer evaluation to enhance the reliability of 4 Finance LLM Capability FinTral PIXIU FinLLM AdaptLLM Concept Concept, Task Task Concept, Task Training Continue Pre-training Instruction Tuning Preference Alignment General Domain ORM PRM Target Domain General Task Domain Task On-policy Llama-Fin Concept, IF/Chat, Task, Reasoning General Task Task Domain Task Evaluation Reasoning Task Method Direct Answer CoT Answer Table 2: Comparison between Llama-Fin with other open finance LLMs. our evaluation. This also exposes the reasoning path, allowing us to investigate the causes of incorrect answers and enabling more fine-grained comparison across different models. 2.3 Comparison with Existing Finance LLMs Table 2 compares Llama-Fin with popular open finance-specific LLMs: FinTral (Bhatia et al., 2024), PIXIU (Xie et al., 2023), FinLLM (Xie et al., 2024b), and AdaptLLM (Cheng et al., 2024). Among these models, only FinTral incorporates all three training stages: CPT, IT, and PA. However, unlike Llama-Fin, FinTral and other models lack systematic design and fail to include carefully crafted components, as illustrated in Figure 1. For instance, none of these models have an in-depth analysis of design choices for different post-training stages, let alone introduce novel approach for constructing preference data."
        },
        {
            "title": "3 Model Training",
            "content": "As shown in Figure 1, FINDAPs training starts with curated text and prompts (3.1), followed by joint Continual Pre-training and Instruction Tuning (3.4), and then Preference Alignment (3.5). 3.1 Data Text Curation. To introduce domain concepts while preserving general concepts, we curate texts for CPT. Table 3 summarizes the texts curation datasets. Specially, for general concepts, research has shown that small amount of general text (as little as 1%) can effectively mitigate the forgetting issue (Scialom et al., 2022). Therefore, we focus on collecting relatively small but high-quality set of general-domain text. To achieve this, we use verifiable text, which is text written by humans and previously used in supervised tasks in the literature. Note that this contrasts with using unverifiable web text such as C4 (Raffel et al., 2020). Capability Domain CPT Dataset Concept General NaturalInstrution 100,000 Mishra et al. (2022) Reference Size For domain concept, our goal is to collect both large volume of data and maintain high quality. Following practices from the literature on training general LLMs (Lambert et al., 2024; Gunasekar et al., 2023), we source financial texts from primarily relevant websites and books. Details on the collection process can be found in Appendix A. Bach et al. (2022) 100,000 Amini et al. (2019b) 29,837 Ling et al. (2017) 97,500 Onoe et al. (2021) 10,200 Camburu et al. (2018) 549,367 8,130 Khot et al. (2020) 1,190,000 Kim et al. (2022) Geva et al. (2021) 2,290 Xie et al. (2022) 779,000 Cobbe et al. (2021) 7,470 1,470,000 Huang et al. (2024b) PromptSource Math Aqua CREAK ESNLI QASC SODA StrategyQA UnifiedSKG GSM8K ApexInstr DeepmindMath 379,000 DialogueStudio 1,070,000 Zhang et al. (2023) 4,380,000 - 4,500 - 10,177,294 Saxton (2019) Book-Fin Finance Fineweb-Fin Prompt Curation. Prompts represent the diverse ways users may interact with models and serves the essential component for IT and PA. Table 4 summarizes the prompts curation datasets. Specifically, we conduct broad survey and source general, financial, instruction-following, and reasoning tasks from public datasets. To promote diversity, we include datasets like Flare-FinQA (Chen et al., 2021), large open QA dataset in finance, and UltraChat (Ding et al., 2023), dataset shown to perform well for IT in the literature (Tunstall et al., 2024; Ivison et al., 2024). Additionally, we find that exercises or demonstrations from books that Table 3: Summary of curated texts. New datasets released with FINDAP are color-highlighted for emphasis. Total 5 Capability Domain Task Tasks Finance Relation Cls. NER Headline Cls. Sentiment Cls. Summariz. IF/Chat IF/Chat General Finance QA Reasoning Math QA Code Finance QA CFA Exam Total IT Dataset FingptFinred FingptNERCls FingptNER FingptHeadline SentimentCls SentimentTra TradeTheEvent SelfInstruct SlimOrca UltraChat ShareGPT FinanceInstruct FingptConvfinqa FlareFinqa FlareFiqa OrcaMath MetaMathQA MathInstruct MagicodeInstruct Exercise Size 27,600 13,500 511 82,200 47,600 76,800 258,000 82,000 518,000 774,000 100,000 178,000 8,890 6,250 17,100 200,000 395000 262,000 111,000 2,950 3,161,401 Reference Sharma et al. (2022) Yang et al. (2023) Alvarado et al. (2015) Sinha et al. (2020) Yang et al. (2023) Yang et al. (2023) Zhou et al. (2021) Wang et al. (2022) Lian et al. (2023) Ding et al. (2023) Link Link Chen et al. (2022) Chen et al. (2021) Yang et al. (2023) Mitra et al. (2024) Yu et al. (2023) Xiang Yue (2023) Luo et al. (2023) - Table 4: Summary of our curated prompts. New datasets released with FINDAP are color-highlighted for emphasis. For datasets without formal references but only URL, we provide their links. were curated in 3.1 is valuable for reasoning tasks as they usually involve challenging reasonings and come with ground truth answers and sometimes even include human-written chain-of-thought (CoT) explanations. Details on the prompts used to extract these exercises can be found in Appendix B. 3.2 Continual Pre-training (CPT) In order to expose the LLM to domainspecific concepts, we first conduct continual pre-training (CPT). In CPT, we feed plain text to the LLM and perform next token prediction. From Text to CPT Data. key challenge in CPT is what kind of data we should use. Given the general and domain-specific texts introduced in 3.1, we can construct three versions of CPT data, CPT-In contains only the financial (in-domain) text, CPT-Gen contains only the general domain data, and CPT-Mix contains the mixture of the CPT-In and CPT-Gen. Figure 2: Average performance on selected datasets for training Llama3-8b-instruct on our CPT-In, CPT-Gen and CPTMix. The selected datasets are chosen for illustration purpose based on their ability to illustrate the general trend. Key Data Experiments. We conduct CPT on each of the three versions of data. As shown in Figure 2, we observe that while CPT-In and CPT-Gen outperforms in financial (Fig 2a) and general (Fig 2b) tasks, respectively, CPT-Mix achieves the best overall. This is expected as CPT-In can cause catastrophic forgetting on the general tasks, while incorporating general domain concepts in CPT-Mix acts as replay mechanism to mitigate it (Scialom et al., 2022). We can also see that none of the CPT-trained LLMs outperform their base. This is unexpected because CPT invovles post-training on more specialized data, which should enhance the performance. By analyzing the output, we attribute this issue to the model forgetting how to follow instructions effectively after CPT. To quantify this finding, we evaluate the instruction following ability of these models using MT-Bench. The two-turn average scores for CPT-Mix, CPT-In, and 6 CPT-Gen are 1, 1, and 1.0125, respectively, while the base model, achieves score of 7.8875. These confirm that the conventional CPT applied to instruction-tuned LLM can cause serious forgetting on instruction-following (IF) capability. In 3.3, we will see how jointly train IT and CPT can help mitigate such forgetting issue. 3. Instruction Tuning (IT) To adapt the LLM to domain-specific and IF tasks, we conduct IT. The key different between IT and CPT is that IT masks out the instruction and takes as input supervised tasks. From Prompt to IT Data. We introduced our prompt curation in 3.1. We create the responses for IT by filtering existing responses or creating new responses. For prompts with existing responses, we generally keep the original responses if they were written by human or strong model, such as GPT-4. We also filter out empty responses. For prompts without responses, for example, exercises extracted from books that may not have solutions provided, we generate new responses using GPT-4o. Similar to CPT data, we construct three versions of IT data, IT-In, which contains only financial (in-domain) tasks, IT-Gen, which contains only general tasks, and IT-Mix, which includes mixture of the IT-In and IT-Gen. Figure 3: Average performance on selected datasets for training Llama3-8b-instruct on our IT-In, IT-Gen and IT-Mix. Key Data Experiments. Similar to CPT, we conduct IT to each of the three versions. From Figure 3, we observe that unlike CPT, forgetting is significantly reduced. Specifically, all versions of IT are no longer worse than their base versions, indicating that the ability to follow instructions is not as severely forgotten as in CPT. This is further supported by the MT-Bench scores, where we obtained 7.2031, 6.2094, and 7.3219 for IT-Mix, IT-In and IT-Gen, repsetively, all of which are significantly better than the CPT counterparts. We observe that IT-Mix is slightly better than other data versions, suggesting that mixing general tasks remains helpful to mitigating forgetting of general concepts and tasks, although the effect is much less pronounced compared to CPT. We also see that similar tasks improve significantly over base model while novel tasks (including financial tasks and general tasks) show little change. This indicates that, in contrast to CPT, domain has less impact in IT, but task generalization is challenging issue. Comparison with LoRA. Another popular approach to adapt the LLM to specific domain is Parameter-efficient Fine-tuning (PEFT), where the LLM parameters remain fixed, and only small set of additional parameters are trained. This approach naturally mitiFigure 4: Average performance on selected datasets for training Llama3-8b-instruct on IT-Mix with full-model finetuning (IT-Mix) and LoRA finetuning (IT-Mix (LoRA)). 7 gates forgetting issues and is more efficient in terms of trainable parameters. However, whether it can achieve performance comparable to full-model training is unclear. In Figure 4, we experiment with PEFT, specifically using LoRA (Hu et al., 2021), with rank size of 128,2 and compare its performance with full-model finetuning (IT-Mix). We observe that with and without LoRA performs similarly, confirming that LoRA is effective for task adaptation. However, the novel tasks still show little improvement, highlighting that task generalization still remains significant challenge. plausible reason for the lack of task generalization is that effective generalization may require exposure to diverse range of tasks (Wei et al., 2022), which is often impractical in certain domains, particularly long-tail ones. However, concepts themselves may be inherently more generalizable due to the shared nature of concepts across tasks. Based on this, we propose adding CPT either before or concurrently with the IT stage and conduct training experiments accordingly. 3.4 Combining CPT and IT Figure 5: Average performance on selected datasets for training Llama3-8b-instruct on CPT-Mix and IT-Mix jointly (CPTMix + IT-Mix) and sequentially (CPT-Mix IT-Mix). natural choice is to conduct CPT and IT sequentially (Lambert et al., 2024). On the one hand, this is flexible as it allows for different settings (e.g., data size) in each stage. On the other hand, it does not help prevent forgetting during the CPT stage, leaving the LLM dependent on IT to recover its instructionfollowing capability. To make more grounded decision, we conduct experiments on both sequential and joint training approaches. In joint training, an additional hyperparameter to consider is the mixture ratio. We find that down-sampling CPT data to match the size of IT data is the most effective strategy. Results for other strategies, such as no-sampling are provided in Appendix D. Figure 5 illustrates the comparison between joint and sequential training. In both cases, different from ITonly results shown in Figure 3, we see improved performance on similar and novel tasks. This supports our hypothesis that CPT can help improve the generalization of IT, as the concepts are likely able to be shared across different tasks. It is further interesting to see that even the general tasks are improved, indicating that there could be positive transfer between CPT and IT. Comparing the two, we observe that joint training outperforms sequential training across financial and general tasks, as well as similar and novel tasks, highlighting the importance of preventing forgetting of CPT and knowledge transfer between CPT and IT. Figure 6: Average performance on selected datasets for PEFT or full model fine-tuning for CPT and IT. 2Further decreasing or increasing the rank size did not show improvement in our preliminary experiments. For example, rank size of 32, 128 and 512 yield overall averages across 10 general tasks of 0.5267, 0.5331, and 0.5215, respectively, showing only minor differences. 8 Comparison with LoRA. In Section 3.3, we showed that LoRA can effectively adapt tasks but still suffers from task generalization. While we already showed that CPT can help in full-model training setting, we now explore whether CPT can help in the PEFT setting as well. Figure 6 presents the results of applying LoRA for IT and LoRA for both CPT and IT. Surprisingly, we find that full fine-tuning significantly outperforms the LoRA counterparts across similar and novel tasks. This finding contrasts with our previous observations in Figure 4, where performance with and without LoRA was comparable. Our results reveal that knowledge transfer from CPT to IT, which is crucial for task generalization, requires full-model training. Takeaways from Continual Pre-training and Instruction Tuning CPT and IT have different effects and challenges. CPT is effective at introducing domain concepts. However, addressing the forgetting of general concepts and IF capability is crucial. In contrast, IT is effective at introducing domain-specific tasks, with negligible forgetting of IF capabilities and only slight forgetting in general tasks. To improve generalizability, mixing general-domain text and general tasks, along with joint training for IT and CPT is found to be more effective. Furthermore, PEFT is beneficial for task adaptation, but transferring knowledge from CPT to IT to improve task generalization requires full-model training. 3.5 Improving Reasoning with Preference Learning In 3.2 & 3.3, we observed that with CPT and IT, the model improves in capabilities such as concepts, tasks and IF/Chat. However, such improvements were missing on the reasoning tasks. Preference Alignment (PA), where the model is trained to assign higher probability mass to better generations, has been shown to be effective in enhancing reasoning capabilities of LLMs (Pang et al., 2024; Lambert et al., 2024; Jiao et al., 2024; Wang et al., 2024). Specifically, we employ Direct Preference Optimization (DPO) (Rafailov et al., 2023), which directly learns from positive (chosen) and negative (rejected) preference data, yielding an easier way for PA compared to reinforcement learning. We synthetically generate such data from the on-policy model, i.e., the jointly trained CPT+IT checkpoint, as it has shown the strongest performance in previous experiments. Negligible Forgetting in PA. As with CPT and IT, we begin by performing an ablation study on different data versions to evaluate their effectiveness. Since the degree of forgetting diminishes from CPT to IT (as observed in 3.3), we expect it to be even less pronounced in PA. To quickly evaluate this hypothesis, we take naive approach and create PA-Mix and PA-In by using either the provided or GPT4o generated responses (as done for IT in 3.3) as the chosen samples and the output of CPT+IT checkpoint as the rejected ones, based on the prompts of IT-Mix and IT-In, respectively. Figure 7: Average performance on selected datasets for PA training from the CPT+IT checkpoint on PA-Mix and PA-In. Figure 7 shows the results after PA training for PA-In and PA-Mix from the CPT+IT checkpoint.3 We observe that PA-In performs comparably to PA-Mix, indicating that it may not be essential to include general tasks to prevent forgetting of concepts or tasks, unlike the cases of CPT and IT. This suggests that PA training can focus on in-domain tasks, without requiring broader set of general tasks or raising concerns about forgetting. Given this, we use CFA exams (Table 4 in 3.1) as representative source for in-domain reasoning because they cover diverse financial scenarios, emphasize complex reasoning, and, most importantly, are derived from real-world exams. These characteristics make them strong proxy for broader range of financial tasks, ensuring that the model generalizes effectively within the financial domain while simplifying the training process. 3PA trained from Llama3-8b-instruction has shown worse results compared to training from the CPT+IT checkpoint in our preliminary experiments, as PA requires strong initialization checkpoint. For instance, PA-Mix from Llama3-8b-instruction achieves only 29.99 on EDTSUM, whereas the CPT+IT counterpart achieves 54.21. As results, we only investigate training PA from CPT+IT checkpoint. Figure 8: An overview of the proposed Trajectory-GenORM and Trajectory-GenPRM. In TrajectoryGenORM, we collect trajectories from GenORM by evaluating the entire solution (thus question marks in the intermediate steps). In Trajectory-GenPRM, we collect trajectories from GenPRM, by identifing and correcting the first erroneous step. Another crucial observation is that there is not much difference even for unseen similar tasks (FiQA SA, FOMC and NER) and reasoning tasks (CFA-Easy and CFA-Challenge). This highlights the limitations of the current naive PA approach and suggests room for further improvement. In the following, we propose novel PA approach that constructs preference data guided by both outcome and process reward signals. Proposed Method and Comparison with Prior Work. Substantial efforts have been made in the past to improve reasoning with LLMs. These lead to two main families of approaches: inference-time (Snell et al., 2024; Wu et al., 2024) and training-time (Jiao et al., 2024; Qu et al., 2024) methods. Inference-time methods such as search-based (Snell et al., 2024) and self-correction (Qu et al., 2024) achieve good performance by scaling up inference-time computation. Despite their effectiveness, these methods often suffer from high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. For example, Monte Carlo Tree Search (MCTS) method (Jones, 2021) may improve task performance, but potentially requires significantly more computation than simply sampling solutions multiple times. This makes them unsuitable for the finance domain, where applications often require rapid responses, such as for real-time market analysis or portfolio management. In contrast, training-time methods first collect trajectories using (variations of) inference-time methods and then train the LLM with the collected trajectories. This helps the model reason more accurately and faster during inference. Therefore, in this work, we adopt the training-time approach and limit ourselves to simple CoT based inference methods. To collect reasoning trajectories, there are two main approaches. The first is search-based (Setlur et al., 2024; Snell et al., 2024), where trained Reward Model (RM) or verifier is used to guide search method (e.g., Best-of-N, Beam Search) to identify the best reasoning path. The second is revision-based (Bai et al., 2022; Du et al., 2023; Madaan et al., 2023; Saunders et al., 2022), which attempts to improve the generation distribution through multi-round interactions, often by leveraging feedback from itself or another strong LLM to refine the input prompt. In practice, revision-based methods have shown mixed results and have not yet been well established as reliable for achieving improvements (Huang et al., 2024a). In contrast, search-based methods have been shown to be more effective but they heavily rely on the performance of the RM. Most RMs typically assign numerical scores to candidate solutions, which do not fully exploit the text-generation capabilities that LLMs are inherently designed for. Consequently, there has been growing interest in Generative Reward Models (GenRMs), where the RM is generative model capable of producing text-based feedback. However, most existing GenRM still either provide only outcome-level (i.e., on the final answer) signals (Mahan et al., 2024) or require extensive training (Zhang et al., 2024), which results in sparse supervision and under-utilizes the strengths of pre-trained LLMs. To overcome these limitations, we propose leveraging the advanced text generation capabilities of LLMs as GenRM to provide rewards on both the final answer (outcome reward) and the reasoning 10 steps (process reward). Figure 8 illustrates the proposed method. Specifically, we construct preference data from Generative Outcome Reward Model (GenORM) and Generative Process Reward Model (GenPRM) trajectories, both of which are implemented using GPT-4o: Trajectory from GenORM (Trajectory-GenORM). Given prompt and candidate model generated solution, we prompt the GenORM to give holistic judgment for the entire solution using single Yes or No token. We then use the correct solutions as chosen samples and the incorrect solutions as rejected samples. Trajectory from GenPRM (Trajectory-GenPRM). Since reasoning is often complex and process rewards have been shown to be more effective (Lightman et al., 2024), we further leverage GenPRM to provide process rewards. Instead of requesting rewards at each step, which has been shown to be unnecessary by Lightman et al. (2024) and Luo et al. (2024), we prompt the GenPRM to identify the first erroneous step. While identifying the first error is helpful for training Reward Model (RM), it is insufficient for generating preference data. To address this, we further extend the approach by prompting the GenPRM to correct the identified erroneous step. Using this correction, we construct preference data sample. The input prompt is formed by concatenating the original question, the candidate reasoning steps up to the first error, and follow-up question framed as What is the next step?. The chosen response of this preference sample is the newly-obtained corrected step, while the original first erroneous step is deemed as rejected response. This approach produces trajectories that focus on predicting the correct next step given reasoning prefix, rather than requiring prediction of the entire reasoning trajectory, as in Trajectory-GenORM. Additional details about the prompts used for our data collection are provided in Appendix C. Results of the Proposed Method. Figure 9 shows the results. We observe significant improvement in both CFA-Easy and CFAChallenge, indicating that the supervision provided by Trajectory-GenORM and TrajectoryGenPRM is highly beneficial for reasoning tasks. Moreover, combining Trajectory-GenORM and Trajectory-GenPRM yields additional improvements over Trajectory-GenORM alone, demonstrating that outcome and process rewards complement each other effectively. Figure 9: Average performance on selected datasets for training from CPT+IT on PA using Trajectory-GenORM, noted as PA (GenORM) and Trajectory-GenORM + Trajectory-GenPRM, noted as PA (GenORM+GenPRM). Takeaways from Preference Alignment PA is effective for learning to reason. Similar to IT, the forgetting of general concepts and IF capability is negligible, as is the forgetting of general tasks. However, synthesizing preference data for effective adaptation is challenging. Constructing preference data by collecting trajectories from both outcome and process signals of GenRM is found to be an effective method."
        },
        {
            "title": "4 Final Recipe and Evaluation",
            "content": "In this section, we present our final recipe based on the desired capabilities in Section 2.1, and data and model ablations performed in Section 3. summary of the final recipe is given in Table 5. For evaluation, we compare with wide range of baselines models, including Llama-Fin against wide range of baseline models, including its base model, Llama3-8B-instruct, and the 8B peer, Llama3.1-8B-instruct. We also include comparisons with models of other sizes, such as Phi-3.5-miniinstruct (Abdin et al., 2024) (3.8B), and Mistral-Nemo-instruct (Jiang et al., 2023) (12B), as well as the closed model GPT-4o (OpenAI, 2023). Additionally, we evaluate against the latest open finance11 specific LLM, Palmyra-Fin-32k (Writer, 2024), which is also based on the Llama3-8B-instruct model.4 Final Recipe for Llama-Fin Continual Pre-training (CPT) and Instruction Tuning (IT) Data Curriculum Group 1 50% CPT, 50% IT Group 2 Intialization Attention Steps Model Optim. LR Batch size Loss of development set stops decreasing ( 1 epoch) Stop Cri. Preference Alignment (PA) Data Steps Model Trajectory-GenORM and Trajectory-GenPRM 24.58 tokens Initialization Loss LR Batch size Loss of development set stops decreasing Optim. Stop Cri. CPT: 50% Domain-specific Text (Web and book), 50% General text (verfiable text) IT: 20% Domain-specific tasks, 80% General tasks CPT: Group 1 data + domain-specific books IT: Group1 + Exercises extracted from books Group 1: 3.84B tokens; Group 2: 1.66B tokens (8,000 context length, 16 A100) Llama3-8b-instruct CPT: full attention with cross-docuemnt attention masking IT: full attention with instruction mask-out and cross-docuemnt attention masking AdamW (weight decay = 0.1, β1=0.9, β2=0.95) Group 1: 5e-6 with 10% warmup; Group 2: 5e-6 with 50% warmup 128K tokens CPT+IT DPO with an additional negative log-likelihood term 5e-7 with 10% warmup 32K tokens Table 5: Final recipe of Llama-Fin. The joint training of CPT and IT is structured into two groups, with each group undergoing joint training sequentially. The second group utilizes higher-quality data (sourced from books), following the typical curriculum training practice (Gao et al., 2024). For PA, we employ modified DPO loss with an additional negative log-likelihood term, similar to Pang et al. (2024), as it has shown to be more effective than relying solely on the original DPO loss. Table 6 shows the evaluation of Llama-Fin across all considered benchmarks. Scores are reported using the metrics specified in parentheses5, ensuring consistency with the corresponding literature. Higher scores always indicate better performance. It is important to note that while all evaluations are unseen for Llama-Fin, we cannot verify whether some benchmarks were used during the training of the baseline models. Unseen - Similar. Llama-Fin trained based on Llama-3-8b-instruct outperforms all other baselines in its size category. It also surpasses significantly larger models, such as the finance-specific PalmyraFin-32K (70B). Notably, Llama-Fin also exceeds the performance of the closed model GPT-4o. These results demonstrate the effectiveness of our data and model recipe for domain-adaptive post-training. Unseen - Novel. To evaluate the generalization of Llama-Fin, we evaluate its performance on unseen novel tasks that correspond to the identified capabilities. Below, we summarize the key takeaways from this comparison: Llama-Fin Preserves General Concepts. We observe that Llama-Fin performs better or remains competitive with its base model in general knowledge recall tasks, indicating that it effectively preserves general concepts. It performs slightly worse than the base model in finance knowledge recall (MMLU-Finance), despite our earlier finding that the CPT benefits IT (3.3). We hypothesize that CPT helps learn concepts that are helpful but differ from those emphasized in MMLU-Finance. Llama-Fin is Effective in The Majority of Tasks. Llama-Fin outperforms the base model in 12 out of 17 tasks, demonstrating that our approach can lead to models that generalize well to novel, unseen tasks requiring the same capabilities. Interestingly, we observe that for certain tasks (e.g., CRA-TravelInsurance, CRA-Taiwan and CRA-ProroSeguro), preference alignment negatively impacts performance, resulting in worse outcomes compared to CPT+IT. Even GPT-4o performs 4We note that there are additional financial LLMs available, such as FinMa (Xie et al., 2023) based on Llama2, Finance-LLM (Cheng et al., 2024) based on Llama3-base and FinLLaVA (Xie et al., 2024b) focuses on multi-modality and not publicly available. However, they are either significantly smaller in scale, based on less advanced LLMs compared to our model or not publicly available. In our preliminary experiments, these models performed considerably worse than both our model and the baselines. Therefore, we have only included the SoTA financial LLM in our comparisons. 5Acc and Rouge1 refer to accuracy and ROUGE-1 score respectively. Mcc refers to Matthews correlation coefficient, usually used in highly imbalanced data (Xie et al., 2024b). 12 Capability Domain Task Benchmark Llama-Fin 8B CPT+IT 8B Llama3 Instruct 8B Llama3.1 Instruct 8B Palmyra Fin 70B Phi 3.5-mini Instruct 3.8B Mistral Nemo instruct 12B GPT4o Tasks Finance Sentiment Analysis Sentiment Analysis Monetary Policy Stance Named Entity Recognition NER (Rouge1) Abstractive Summarization EDTSUM (Rouge1) FPB (Acc) FiQA SA (Acc) FOMC (Acc) Unseen - Similar 91.13+ 95.32+ 64.31+ 76.69+ 53.78+ Unseen - Novel Concept General Knowledge Recall Task Finance Knowledge Recall Finance Extractive Summarization MMLU (CoT, Acc) AI2-ARC (CoT, Acc) Nq-open (CoT, Acc) MMLU-Finance (Acc) Flare-ECTSUM (Rouge1) ESG Issue Classification MLESG (Acc) Rumor Detection Stock Movement Prediction SM-Bigdata (CoT, Acc) MA (Acc) 92.99 94.47 63.10 74.33 54.21 47.22 88.95 16.20 63.93 34.41 42.00 84.60 52.04 49.89 44.88 0.61 32.32 60.50 51.80 65.96 0.65 96.41 86.57 98.50 66.43 52.00 7.29 54.30 73.64 79.20 78.92 67.48 84.39 62.31 35.56 73.09 77.87 56.65 45.03 11.50 48.14 89.29 18.47 65.71 35.92 36.33 82.60 55.3 50.51 55.56 -0.32 14.78 33.50 66.91 52.69 12.37 12.01 96.98 6.39 67.80 52.70 7.88 51.16 68.83 77.00 73.34 62.51 79.82 60.56 34. 71.55 70.64 54.64 51.22 12.53 47.42 89.80 22.52 66.74 35.77 36.00 84.20 46.06 45.30 48.03 2.73 17.3 15.00 11.51 25.38 15.07 35.97 44.33 80.31 63.70 38.00 7.92 49.35 70.73 82.20 69.10 66.69 81.45 60.47 35.56 67.11 71.91 63.10 54.29 21.77 54.93 89.01 19.25 75.15 33.24 39.67 62.60 48.70 51.21 52.92 3.12 33.03 12.00 12.95 23.40 13.78 52.58 56.20 17.28 64.21 56.67 5.80 41.51 77.28 87.00 69.69 74.27 86.72 36.05 25.56 78.04 69.36 58.47 39.37 19.97 45.07 87.25 6.20 68.17 35.52 38.33 75.40 53.26 49.84 50.03 1.20 45.33 49.50 46.76 48.87 69.14 69.96 25.86 94.48 57.70 40.70 8.38 39.40 72.82 80.20 67.89 72.22 82.05 61.24 48. 78.25 55.74 57.86 49.84 12.32 49.64 88.19 17.01 61.88 37.86 32.67 85.20 53.53 50.75 53.28 3.94 32.94 32.50 56.12 21.03 11.18 57.88 32.58 73.64 66.40 55.30 7.84 52.46 62.95 76.40 61.74 65.82 77.91 65.89 43.33 82.16 68.51 67.94 43.02 18.15 63.88 97.85 27.92 86.52 35.90 45.67 73.80 49.18 50.97 49.78 6.16 49.57 17.00 51.80 65.03 17.38 8.57 96.60 54.03 74.90 51.30 9.10 70.82 78.92 94.60 81.76 85.71 94.34 83.14 74.44 47.42 89.43+ 19.20+ 64.20 34.10 40.67+ 84.00+ 54.14 51.99+ 54.94 0.83+ 34.03+ 64.00+ 44.60 68.49+ 15.30+ 40.81+ 35.14 SM-ACL (CoT, Acc) SM-CIKM (CoT, Acc) CRA-CCF (CoT, Mcc) CRA-CCFraud (CoT, Acc) Flare-German (CoT, Acc) Flare-Astralian (CoT, Acc) CRA-LendingClub (CoT, Acc) CRA-Polish (CoT, Mcc) CRA-Taiwan (CoT, Acc) CRA-ProroSeguro (CoT, Acc) CRA-TravelInsurance (CoT,Acc) 41.52+ 66.61+ *Flare-TATQA (CoT, Acc) 54.00+ *Finance Bench (CoT, Acc) MT-bench (1,2 turn avg) 7.36 55.08+ MathQA (CoT, Acc) 75.23+ Social-IQA (CoT, Acc) 82.60+ 81.90+ 70.32+ 85.85+ 66.28+ 55.56+ Hellaswag (CoT, Acc) Winogrande (CoT, Acc) PIQA (CoT, Acc) CFA-Easy (CoT, Acc) CFA-Challnge (CoT, Acc) Fraud Detection Credit Scoring Distress Identification Claim Analysis Tabular QA Open QA General Precise IF IF/Chat Reasoning Math Math Reasoning General Social Reasoning Finance Exam Common Sense Reasoning Open-book-qa (CoT, Acc) Table 6: Overview of the results on unseen evaluation set. * indicates that GPT4o is used as the judge. Llama-Fin and its variant (CPT+IT) are highlighted in blue. The best performing model for 8b on each benchmark is bolded. The overall best performance across all models is underlined. + indicates that Llama-Fin outperforms the base Llama3-8B-instruct. poorly in these tasks. This suggests that for some tasks, leveraging reasoning capabilities might not be beneficial, as these tasks could be inherently easy and solvable without the need for advanced reasoning. Such observations aligns with findings in the literature (Sprague et al., 2024; Liu et al., 2024). Llama-Fin Preserves IF/Chat Capabilities. Llama-Fin achieves competitive MT-Bench score compared to the base model, indicating that it effectively maintains the IF capability. Llama-Fin Excels in Reasoning Tasks. For reasoning capability, Llama-Fin significantly outperforms the base models across all considered benchmarks in large amount (up to 20% in CFAChallenge), indicating substantial improvements in reasoning capability. Moreover, Llama-Fin is significantly better than CPT+IT, further confirming that our proposed Trajectory-GenPRM and Trajectory-GenORM are particularly effective in improving reasoning performance beyond the already strong checkpoint of CPT+IT."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce FINDAP, an open SoTA finance-specific post-training framework, featuring evaluation framework, leaderboard, data, and model recipes. Under FINDAP, we develop Llama-Fin, SoTA finance LLM. In this development, we conduct systematic study on effectively adapting target domain through post-training. This includes identifying key capabilities, designing evaluation setups, and detailing training and data recipe across the stages of CPT, IT, and PA. Furthermore, we propose novel approach for constructing preference data during PA. For each stage, we reveal the distinct challenges, objectives, and effective strategies. Looking ahead, we aim to explore additional domain-specific capabilities using FINDAP."
        },
        {
            "title": "6 Limitations",
            "content": "While the recipe for FINDAP and Llama-Fin are effective, the performance on novel unseen tasks still requires further improvement. For example, selectively employing reasoning capabilities only for questions that require such advanced reasoning might give better results. Additionally, the data recipe is currently based on full-scale empirical experiments, which can be time-intensive. Developing low-cost experiments to reliably indicate the effectiveness of data in post-training could streamline this process and accelerate the development iteration. It is also worth noting that the same recipe may not generalize well to other model families. Different architectures or pretraining strategies might require tailored recipe to achieve optimal results, emphasizing the need for adaptability in recipe design in future research."
        },
        {
            "title": "References",
            "content": "Marah Abdin et al. 2024. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. arXiv:2404.14219 [cs.CL] https://arxiv.org/abs/2404.14219 Salinas Alvarado, Julio Cesar, Karin Verspoor, and Timothy Baldwin. 2015. Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment. In Proceedings of the Australasian Language Technology Association Workshop 2015, Ben Hachey and Kellie Webster (Eds.). Parramatta, Australia, 8490. https://aclanthology.org/U15-1010 Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019a. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. arXiv:1905.13319 [cs.CL] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019b. MathQA: Towards Interpretable Math Word Problem Solving with OperationBased Formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 23572367. https://doi.org/10.18653/v1/N19-1245 Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. arXiv:2202.01279 [cs.LG] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073 [cs.CL] https://arxiv.org/abs/2212.08073 Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, and Muhammad Abdul-Mageed. 2024. FinTral: Family of GPT-4 Level Multimodal Financial Large Language Models. arXiv:2402.10986 [cs.CL] https://arxiv.org/abs/2402.10986 Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: Reasoning about Physical Commonsense in Natural Language. In Thirty-Fourth AAAI Conference on Artificial Intelligence. 14 Ethan Callanan, Amarachi Mbakwe, Antony Papadimitriou, Yulong Pei, Mathieu Sibue, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2024. Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams. In Proceedings of the Eighth Financial Technology and Natural Language Processing and the 1st Agent AI for Scenario Planning, ChungChi Chen, Tatsuya Ishigaki, Hiroya Takamura, Akihiko Murai, Suzuko Nishino, Hen-Hsen Huang, and Hsin-Hsi Chen (Eds.). -, Jeju, South Korea. https://aclanthology.org/2024.finnlp-2.2 Oana-Maria Camburu, Tim Rockt\"aschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language Inference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.). Curran Associates, Inc., 95399549. http://papers.nips.cc/paper/ 8163-e-snli-natural-language-inference-with-natural-language-explanations. pdf Chung-Chi Chen, Yu-Min Tseng, Juyeon Kang, Anaïs Lhuissier, Min-Yuh Day, Teng-Tsai Tu, and Hsin-Hsi Chen. 2023b. Multi-Lingual ESG Issue Identification. In Proceedings of the Fifth Workshop on Financial Technology and Natural Language Processing and the Second Multimodal AI For Financial Forecasting, Chung-Chi Chen, Hiroya Takamura, Puneet Mathur, Remit Sawhney, Hen-Hsen Huang, and Hsin-Hsi Chen (Eds.). -, Macao, 111115. https://aclanthology.org/ 2023.finnlp-1.11 Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023a. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079 (2023). Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. FinQA: Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online https://doi.org/10.18653/v1/2021. and Punta Cana, Dominican Republic, 36973711. emnlp-main.300 Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 62796292. https://doi.org/ 10.18653/v1/2022.emnlp-main.421 Daixuan Cheng, Shaohan Huang, and Furu Wei. 2024. Adapting Large Language Models via Reading Comprehension. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=y886UXPEZ0 Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. ArXiv abs/1803.05457 (2018). Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168 (2021). Pierre Colombo, Telmo Pires, Malik Boudiaf, Rui Filipe Coimbra Pereira de Melo, Gabriel Hautreux, Etienne Malaboeuf, Johanne Charpentier, Dominic Culver, and Michael Desa. 2024b. SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain. In The Thirty-eighth https://openreview.net/ Annual Conference on Neural Information Processing Systems. forum?id=NLUYZ4ZqNq Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, et al. 2024a. Saullm-7b: pioneering large language model for law. arXiv preprint arXiv:2403.03883 (2024). 15 Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing Chat Language Models by Scaling High-quality Instructional Conversations. arXiv preprint arXiv:2305.14233 (2023). Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325 (2023). Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Zhengyu Chen, Alejandro Lopez-Lira, and Hao Wang. 2024. Empowering Many, Biasing Few: Generalist Credit Scoring through Large Language Models. arXiv:2310.00566 [cs.LG] https://arxiv. org/abs/2310.00566 Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024. How to Train Long-Context arXiv:2410.02660 [cs.CL] https://arxiv.org/abs/2410. Language Models (Effectively). 02660 Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use Laptop? Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL) (2021). Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are All You Need. arXiv:2306.11644 [cs.CL] https://arxiv.org/abs/2306. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Dont Stop Pretraining: Adapt Language Models to Domains and Tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online. https://aclanthology.org/2020.acl-main.740 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. Proceedings of the International Conference on Learning Representations (ICLR) (2021). Hans Hofmann. 1994. Statlog (German Credit Data). UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5NC77. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL] https://arxiv.org/abs/2106.09685 Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024a. Large Language Models Cannot Self-Correct Reasoning Yet. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id= IkmD3fKBPQ Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. 2024b. OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models. https://arxiv.org/pdf/2411.04905 Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie FinanceBench: New Benchmark for Financial Question Answering. Vidgen. 2023. arXiv:2311.11944 [cs.CL] Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024. Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback. https://arxiv.org/abs/2406.09279 16 Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, and Shafiq Joty. 2024. Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA. https://aclanthology.org/2024.emnlp-main.20 Andy L. Jones. 2021. Scaling Scaling Laws with Board Games. https://arxiv.org/abs/2104.03113 arXiv:2104.03113 [cs.LG] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Bridging the preference gap between retrievers and llms. arXiv preprint arXiv:2401.06954 (2024). Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2023. Continual Pre-training of Language Models. In International Conference on Learning Representations (ICLR). Danupat Khamnuansin, Atthakorn Petchsod, Anuruth Lertpiya, Pornchanan Balee, Thanawat Lodkaew, Tawunrat Chalothorn, Thadpong Pongthawornkamol, and Monchai Lertsutthiwong. 2024. THaLLE: Text Hyperlocally Augmented Large Language Extension Technical Report. arXiv:2406.07505 [cs.CL] https://arxiv.org/abs/2406.07505 Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. QASC: Dataset for Question Answering via Sentence Composition. arXiv:1910.11473v2 (2020). Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2022. SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. ArXiv abs/2212.10465 (2022). Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics (2019). Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2024. Tülu 3: Pushing Frontiers in Open Language Model Post-Training. (2024). Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 94599474. Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large language models in finance: survey. In Proceedings of the Fourth ACM International Conference on AI in Finance. 374382. Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". 2023. SlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification. https://https://huggingface.co/Open-Orca/SlimOrca Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets Verify Step by Step. In The Twelfth International Conference on Learning Representations. https://openreview.net/ forum?id=v8L0pN6EOi Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL (2017). 17 Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, and Thomas L. Griffiths. 2024. Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse. arXiv:2410.21333 [cs.LG] https://arxiv.org/abs/2410. 21333 Team LLaMA. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv. org/abs/2407.21783 Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2024. Improve Mathematical Reasoning in Language Models by Automated Process Supervision. arXiv:2406.06592 [cs.CL] https: //arxiv.org/abs/2406.06592 Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=S37hOerQLB Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. 2024. Generative Reward Models. arXiv:2410.12832 [cs.LG] https://arxiv.org/abs/2410.12832 Mário Maia, Jonathan Berant, José Farinha, and André Freitas. 2018. WWW18 Open Challenge: Financial Opinion Mining and Question Answering. In Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee, 19411942. Dean Malmgren. 2014. Textract. https://textract.readthedocs.io/ P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology 65 (2014). Nestor Maslej, Loredana Fattorini, Raymond Perrault, Vanessa Parli, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, and Jack Clark. 2024. The AI Index 2024 Annual Report,. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can Suit of Armor Conduct Electricity? New Dataset for Open Book Question Answering. In EMNLP. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In ACL. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. 2024. Orca-Math: Unlocking the potential of SLMs in Grade School Math. arXiv:2402.14830 [cs.CL] Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, and Pawan Goyal. 2022. ECTSum: New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates. https://aclanthology.org/ 2022.emnlp-main.748 Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. CREAK: Dataset for Commonsense Reasoning over Entity Knowledge. OpenReview (2021). OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). 18 Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. 2024. Iterative Reasoning Preference Optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=4XIKfvNYvx Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. 2024. Recursive Introspection: Teaching Language Model Agents How to Self-Improve. arXiv:2407.18219 [cs.LG] https://arxiv.org/ abs/2407.18219 Ross Quinlan. 1987. Statlog (Australian Credit Approval). UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C59012. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview. net/forum?id=HPuSIXJaa9 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 167. http://jmlr.org/papers/v21/20-074.html Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. arXiv preprint arXiv:1907.10641 (2019). William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv:2206.05802 [cs.CL] https://arxiv.org/abs/2206.05802 Hill Kohli Saxton, Grefenstette. 2019. Analysing Mathematical Reasoning Abilities of Neural Models. arXiv:1904.01557 (2019). Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. 2022. Fine-tuned Language Models are Continual Learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates. https://doi.org/10.18653/ v1/2022.emnlp-main.410 Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024. Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning. arXiv:2410.08146 [cs.LG] https://arxiv.org/abs/ 2410.08146 Agam Shah, Suvan Paturi, and Sudheer Chava. 2023. Trillion Dollar Words: New Financial Dataset, Task & Market Analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada. https: //aclanthology.org/2023.acl-long. Soumya Sharma, Tapas Nayak, Arusarka Bose, Ajay Kumar Meena, Koustuv Dasgupta, Niloy Ganguly, and Pawan Goyal. 2022. FinRED: Dataset for Relation Extraction in Financial Domain. In Proceedings of The 2nd Workshop on Financial Technology on the Web (FinWeb). Ankur Sinha, Tanmay Khandait, and vincent. 2020. Impact of News on the Commodity Market: Dataset and Results. CoRR abs/2009.04202 (2020). arXiv:2009.04202 https://arxiv.org/ abs/2009.04202 Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv:2408.03314 [cs.LG] https://arxiv.org/abs/2408.03314 Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and Kang. 2022. Accurate Stock Movement Prediction with Self-supervised Learning from Sparse Noisy Tweets. In 2022 IEEE International Conference on Big Data (Big Data). IEEE Computer Society, 16911700. 19 Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2024. To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning. arXiv:2409.12183 [cs.CL] https://arxiv.org/abs/2409. Lewis Tunstall, Edward Emanuel Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander Rush, and Thomas Wolf. 2024. Zephyr: Direct Distillation of LM Alignment. In First Conference on Language Modeling. https://openreview.net/forum?id= aKkAwZB6JV Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. 2024. Direct Judgement Preference Optimization. arXiv:2409.14664 [cs.CL] https://arxiv.org/abs/2409.14664 Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. 2022. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations. https://openreview.net/forum?id= gEZrGCozdqR Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL] https://arxiv.org/abs/2201.11903 Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing Multiple Choice Science Questions. In NUT@EMNLP. Writer. 2024. Palmyra-Fin-70B-32k: powerful LLM designed for Finance. https://dev.writer. com. Huizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang. 2018. Hybrid Deep Sequential Modeling for Social Text-Driven Stock Prediction. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (Torino, Italy) (CIKM 18). Association for Computing Machinery, New York, NY, USA, 16271630. https://doi.org/10.1145/3269206.3269290 Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models. arXiv:2408.00724 [cs.AI] https://arxiv.org/abs/2408.00724 Ge Zhang Yao Fu Wenhao Huang Huan Sun Yu Su Wenhu Chen Xiang Yue, Xingwei Qu. 2023. MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. arXiv preprint arXiv:2309.05653 (2023). Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. PIXIU: Large Language Model, Instruction Data and Evaluation Benchmark for Finance. arXiv:2306.05443 [cs.CL] https://arxiv.org/abs/2306.05443 Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, Shunian Chen, Yifei Zhang, Lihang Shen, Daniel Kim, Zhiwei Liu, Zheheng Luo, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Zhiyuan Yao, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Yilun Zhao, Yitao Long, Guojun Xiong, Kaleb Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jianyun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Jimin Huang, and Sophia Ananiadou. 2024b. Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications. arXiv:2408.11878 [cs.CL] https://arxiv.org/abs/2408.11878 Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, ChienSheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. UnifiedSKG: Unifying and 20 Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. EMNLP (2022). Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2024a. Efficient Continual Pre-training for Building Domain Specific Large Language Models. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1018410201. https://doi.org/10.18653/ v1/2024.findings-acl. Yumo Xu and Shay B. Cohen. 2018. Stock Movement Prediction from Tweets and Historical Prices. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Iryna Gurevych and Yusuke Miyao (Eds.). Association for Computational Linguistics, Melbourne, Australia. https://aclanthology.org/P18-1183 Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023. FinGPT: Open-Source Financial Large Language Models. FinLLM Symposium at IJCAI 2023 (2023). Linyi Yang, Eoin M. Kenny, Tin Lok James Ng, K. Z. Zhang P.K. Kannan Yang, Barry Smyth, and Ruihai Dong. 2020. Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification. In COLING. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. arXiv preprint arXiv:2309.12284 (2023). Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-Rewarding Language Models. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=0NphYCmgua Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can Machine Really Finish Your Sentence?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Silvio Savarese, and Caiming Xiong. 2023. DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. arXiv preprint arXiv:2307.10172 (2023). Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240 (2024). Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv:2306.05685 [cs.CL] Zhihan Zhou, Liqian Ma, and Han Liu. 2021. Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association https://doi.org/10.18653/v1/2021. for Computational Linguistics, Online, 21142124. findings-acl.186 Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: Question Answering Benchmark on Hybrid of Tabular and Textual Content in Finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online. https://aclanthology.org/2021. acl-long."
        },
        {
            "title": "A Texts Curation Details",
            "content": "we source financial text from two primary resources. The first source is web text, where we filter non-financial content from the FineWeb using URLs like sec.gov and investopedia.com. The second source is books. We manually select 10 finance-related topics (e.g., economics and management), download books on these topics, and convert them to text using OCR (Malmgren, 2014). Since OCR can make mistakes, we further employ strong LLM to filter out content lacking educational value or unrelated to finance. Details on the financial URLs, finance-related topics, and the prompts used for filtering is shown below: Selected Financial URLs. We curated selection of 70 financial websites to comprehensively cover diverse aspects of finance-related content on the web. These include trusted sources from financial institutions, regulatory agencies, educational platforms, and industry-specific news outlets. This diverse collection ensures representation across sub-domains such as investment, banking, personal finance, regulatory compliance, and financial planning, offering well-rounded foundation that can cover most of the finance content in the web. Selected Topics. We select 12 topics that are cover most of books in finance. 5 of them are from business areas, including business, Accounting, Accounting, Management, Marketing, Trading; 1 is from Mathematics, i.e., Mathematical Economics; and 4 are from Economy area, including economy, econometrics, investing, and markets. We crawled the web to downloaded the books from the corresponding topics. For CFA, we use the material provided by CFA prep providers. Prompt for Filtering the Text. We explored various prompt formats to automatically extract an financial score using an LLM and found that the additive scale by Yuan et al. (2024) worked best. Figure A.1 shows the prompt we used to filter the low-quality text. Specifically, this prompt allows the LLM to reason about each additional point awarded, unlike the single-rating Likert scale which fits samples into predefined boxes. Then, to avoid the LLM favoring highly technical content like academia papers, we focused on financial student level knowledge. By setting threshold of 4 (on scale of 0 to 5) during the filtering process, we were able to also retain some high-quality financial content."
        },
        {
            "title": "B Prompts Curation Details",
            "content": "Figure B.1 shows the prompt we used to extract exercises from books. We carefully design the prompt to extract both the question part of an exercise, which potentially include questions, scenario and exhibits, and the answer part of the exercise, which may include answer choices and solution. In books, questions and their corresponding answers can be located far apart (e.g., the questions may appear at the beginning while the solutions are provided at the end), meaning they may not be captured within the same chunk. As result, some questions may not have corresponding extracted answers. For such cases, GPT-4os generated answers are used when converting the prompt into instruction-following or preference-alignment data, as detailed in Sections 3.3 and 3.5."
        },
        {
            "title": "C GenRM Details",
            "content": "In 3.5, we simplified the prompt for GenRM for the purpose of illustration in Figure 8. In this section, we give full detailed of the prompt for GenORM and GenPRM in Figure C.1 and Figure C.2, respectively."
        },
        {
            "title": "D Additional Ablations",
            "content": "In 3.3, we employ down-sampling CPT data to match the size of IT data when conducting CPT and IT jointly. In this section, we also evaluate the performance under no-sampling setting. Figure D.1 shows the results. We observe that in both joint and sequential training, down-sampling yields better results on financial tasks. This is understandable because down-sampling assigns more weight to IT, which is beneficial for the financial tasks. Interestingly, we observe the opposite trend for general tasks: no-sampling performs better. We hypothesize that this is because having more CPT data helps preserve general concepts more effectively, although it may diminish instruction-following abilities. 22 Prompt for Filtering the Text Below is an extract from text book. Evaluate whether the book has high financial value and could be useful in an financial setting for teaching financial students using the additive scoring system described below. Points are accumulated strictly based on the satisfaction of each criterion: - Add 1 point if the extract provides educational value for financial students whose goal is to learn financial concepts or take finacial exams. It is acceptable if quizzes are not included; however, if quizzes are present, detailed solutions and explanations must also be provided Add 1 point if the extract provides educational value for financial students whose goal is to learn financial concepts or take financial exams. It is acceptable if quizzes are not included; however, if quizzes are present, detailed solutions and explanations must also be provided. Add another point if the extract addresses certain elements pertinent to finance and aligns closely with financial standards. It might offer superficial overview of potentially useful topics or present information in disorganized manner and incoherent writing style. Award third point if the extract is appropriate for financial use and introduces key concepts relevant to financial curricula. It is coherent and comprehensive. Grant fourth point if the extract is highly relevant and beneficial for financial learning purposes for level not higher than financial students, exhibiting clear and consistent writing style. It offers substantial financial content, including exercises and solutions, with minimal irrelevant information, and the concepts arent too advanced for financial students. The content is coherent, focused, and valuable for structured learning. Bestow fifth point if the extract is outstanding in its financial value, perfectly suited for teaching either at financial students. It follows detailed reasoning, the writing style is easy to follow and offers profound and thorough insights into the subject matter, devoid of any non-financial or complex content. The extract: <EXAMPLE>. After examining the extract, You will output json object containing the following 2 fields: { \" Justification \": string // Briefly justify your total score , up to 100 words . \" Score \": integer // Conclude with the score } Figure A.1: Prompt for filtering the text 23 Prompt for Extracting Exercise from Book You are an educational assistant aims to extract all questions from the provided material. Look for specific indicators such as \"example,\" \"quiz,\" \"questions,\" or similar terms to identify where the questions are located. If the material includes scenarios or exhibits, must include all details related to them. Do not create or derive any questions or come up with content on your ownstrictly extract what is present in the material. Make sure no question is missed. If one scenario or exhibits corresponds to multiple questions, duplicate the scenarios and exhitbits so that the number of questions match the number of scenarios and exhibits. The material: <MATERIAL>. After performing these tasks, You will output json object containing the following fields: { \" Justification \": \" string \" , // brief justification for your extractions , up to 100 words . \" Questions \": \" string \", // list of questions extracted from the material . Only extract the exact questions presented in the text . \" Scenario \": \" string \", // list of scenarios corresponding to the above questions . If the material does not provide the Do not do any derivation or reference , scenario place \"N/A .\" must output the exact same , detailed and complete scenarios . The scenario may contain multiple paragraphs or even splited by the exhibits , combine them into one string . The scenario can be long , you may modify it to make it shorter , not change its meaning . but must \" Exhibit \": \" string \", // list of exhibits or tables corresponding to the above questions . If the material does not provide the exhibit , place \"N/A .\" Do not do any summary , or must output the exact same , detailed derivation or cutting , and complete exibits . There may be multiple exhibits involved in scenario , combine them into one string . The exhibit can be long , you may modify it to make it shorter . Must keep the table format \" Answer Choices \": \" string \", // list of answer choices corresponding to the above questions . If the material does not provide answer choices , place \"N/A .\" \" Answer \": \" string \" // list of answers corresponding to the above questions . Answers should only be included if provided in the material . If no answer is given , place \"N /A .\" If explanations or reasoning steps or equations are included , must capture all of them . Must not answer it yourself if there is no answer provided in the material . Make sure the final number of questions equals to number of scenario equals to number of exhibits equals to number of answers } Figure B.1: Prompt for extracting exercises from books 24 Prompt for GenORM You given question, reference answer and proposed answer, you task is to determine the correctness of the proposed answer. First, extract the final answer (for example, A, or C) from the reference answer. Second, extract the final answer from the proposed answer (for example, A, or C). Finally, compare the two final answer to determine the correctness. Do not do any extra reasoning, must determine the correctness soley based on the given reference and proposed answer. Question: <QUESTION> Reference Answer: <REFERENCE> Proposed Answer: <PROPOSAL> After performing these tasks, You will output json object containing the following fields: { \" Justification \": \" string \", // brief justification for your output , up to 100 words . \" Correctness \": \" string \", // If the proposed answer has the same final final answer as the reference answer ( for example , both choose or have the same answer ) , wrong ' to all other cases . For example , if the proposed answer output ' correct '. Put ' has different final answer comparing to the reference answer , put ' wrong '. If the proposed answer does not explicitly give final answer to the question , put ' wrong '. If the proposed answer gives more than one final answer to the question , put ' wrong '. } Figure C.1: Prompt for GenORM 25 Prompt for GenPRM Given question, reference answer and an incorrect answer, you task is to identify the first incorrect step from the incorrect answer. The \"first incorrect step\" means all reasoning up to that point is accurate, but the error begins at this specific step. Question: <QUESTION> Reference Answer: <REFERENCE> Incorrect Answer: <INCORRECT> After performing these tasks, You will output json object containing the following fields: { \" Justification \": \" string \" , // brief justification for your output , up to 100 words . You need to explain (1) why the identified first incorrect step is incorrect ; (2) why the reasoning up to this specific step is correct and (3) how the corrected step resolves the issue , aligning with the reference answer , maintaining the logical flow and progressing to the final answer . \" First incorrect step \": \" string \", // The explanation in the incorrect answer consists of multiple reasoning steps . Please identify the first incorrect reasoning step . It should be piece of text directly and exactly quoted from the incorrect answer . It should be an intermediate step rather than the final answer \" Reasoning up to incorrect \": \" string \" , // From the incorrect answer , give the correct reasoning steps up to the first incorrect step . This should be directly and exactly quoted from the incorrect answer . \" Step correction \": \" string \", // Replace the identified incorrect step with single , clear , and correct step . This step should directly address and correct the error , explicitly providing the correct reasoning without requring for more information or challenging the question . It should effectively answer the question , \" What is the next reasoning step ?\" given on the question and the identied \" Reasoning up tp incorrect \". It should help progress to the final answer . } Figure C.2: Prompt for GenPRM 26 Figure D.1: Average and selected datasets performance from down-sampling or no-sampling on CPT."
        }
    ],
    "affiliations": [
        "Salesforce AI Research"
    ]
}