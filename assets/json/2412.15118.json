{
    "paper_title": "Outcome-Refining Process Supervision for Code Generation",
    "authors": [
        "Zhuohao Yu",
        "Weizheng Gu",
        "Yidong Wang",
        "Zhengran Zeng",
        "Jindong Wang",
        "Wei Ye",
        "Shikun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS"
        },
        {
            "title": "Start",
            "content": "Outcome-Refining Process Supervision for Code Generation Zhuohao Yu1*, Weizheng Gu1*, Yidong Wang1, Zhengran Zeng1, Jindong Wang2, Wei Ye1, Shikun Zhang1 1Peking University 2Microsoft Research zyu@stu.pku.edu.cn, wye@pku.edu.cn 4 2 0 2 9 1 ] . [ 1 8 1 1 5 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation tasks (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a; Guo et al., 2024). However, when implementing algorithms with multiple components or handling intricate logic flows, these models often struggle to maintain reliability and consistency (Jiang et al., 2024b; Jimenez et al., 2023). This limitation becomes particularly apparent in problems requiring deeper algorithmic insights and careful consideration of trade-offs. 1We open-source all our code and data at https:// github.com/zhuohaoyu/ORPS 1 Figure 1: Comparison of code generation paradigms. As shown in Figure 1, traditional approaches to improving LLM performance have relied on outcome supervision, where models are guided solely by final output quality (Chen et al., 2021b; Chang et al., 2023). Process supervision has emerged as promising alternative, guiding models through intermediate reasoning steps using Process Reward Models (PRMs) (Lightman et al., 2023; Wang et al., 2024c). While effective for mathematical reasoning, its application to code generation faces unique challenges: PRMs require extensive human-annotated data for training (Luo et al., 2024; Wang et al., 2024b) and often suffer from hallucination when evaluating complex reasoning steps (Wang et al., 2023b; Chen et al., 2024a). Recent studies show that LLMs cannot reliably selfcorrect (Huang et al., 2023) or self-validate without external verification (Stechly et al., 2024). Code generation presents unique opportunity through concrete, verifiable signals. Unlike other domains where intermediate steps may be difficult to verify, code can be executed throughout development, providing objective feedback about both theoretical correctness and practical performance (Zhang et al., 2023; Shinn et al., 2024). Yet existing approaches using execution feedback (Shinn et al., 2024; Zhong et al., 2024) focus primarily on local improvements and debugging, missing opportunities for exploring fundamentally different algorithmic strategies. We Process propose Outcome-Refining Supervision, novel paradigm that treats the reasoning on refinement of outcomes itself as the process to be supervised. This approach differs fundamentally from existing self-improvement methods that focus on iterative refinement with execution feedback. Through tree-structured framework maintains exploration space, our multiple reasoning trajectories simultaneously, enabling models to discover and refine diverse solution strategies. This structure allows models to explore different algorithmic approaches when initial attempts prove suboptimal, rather than being confined to local improvements. Our key insight is that execution feedback can serve as objective anchors for evaluating reasoning quality, eliminating the need for specially trained PRMs. This creates natural synergy: execution outcomes ground the evaluation process, while the models inherent reasoning capabilities guide the exploration of theoretical improvements. By maintaining multiple solution paths, the framework can explore diverse algorithmic strategies while ensuring each step is verified through concrete signals. This approach differs fundamentally from traditional reward models by grounding supervision in verifiable outcomes rather than learned judgments. Through extensive experiments, we have observed several critical insights: Reasoning space over model size: Providing sufficient reasoning space is more crucial than model size for complex programming tasks - even smaller models like Qwen-7B achieve remarkably high success rates (80% Pass@1) when given room to explore and refine multiple solution strategies Reliable verification: Combining execution feedback with self-critique mechanisms creates more reliable verification system than traditional reward models, without requiring expensive training data and solution efficiency compared to existing methods, particularly on complex tasks where current approaches struggle Our key contributions include: We propose ORPS: novel framework that generalizes outcome and process supervision to tackle complex code problems. We are the first to demonstrate that introducing search trees significantly enhances the ability to solve complex coding problems. ORPS achieves an average Pass@1 improvement of 26.9% across three datasets and five models, while reducing running time by 42.2% on average."
        },
        {
            "title": "Supervision",
            "content": "Outcome supervision, the traditional paradigm in machine learning, assesses performance solely on final outputs (Chen et al., 2021b; Brown et al., 2020). This approach has evolved through LLMas-a-judge methods, where language models are prompted (Zheng et al., 2023; Yu et al., 2024a) or trained (Wang et al., 2023c) to evaluate model outputs (Chang et al., 2023). While providing richer feedback than simple metrics, these methods still focus exclusively on final results. Process supervision represents paradigm shift by evaluating the quality of intermediate reasoning steps (Lightman et al., 2023; Luo et al., 2024). This paradigm employs specially trained Process Reward Models (PRMs) - an extension of LLM-asa-judge - to evaluate each solution step (Lightman et al., 2023; Ma et al., 2023; Luo et al., 2024; Jiang et al., 2024a). PRMs have proven particularly effective in domains requiring complex reasoning, such as mathematical problem-solving and logical deduction, where they guide search algorithms toward better solution paths (Wang et al., 2024c,b). evaluates only the final output: Routcome(y) = 1[output is correct]. in contrast, aggregates step-wise scores: Process supervision, supervision Formally, outcome Rprocess(s1, ..., sT ) = (cid:88) t=1 PRM(sts1:t1), (1) Scalable improvement: Our approach shows consistent improvements in both success rates where PRM(sts1:t1) evaluates the quality of step st given the previous steps. 2 Figure 2: Outcome-Refining Process Supervision framework overview. language model serves as both programmer and critic in step-by-step reasoning process. Through beam search, the framework maintains multiple solution trajectories, where each state contains reasoning chains, code implementations, and step reward. However, current approaches to process supervision face significant challenges in practice. The requirement for dense human annotations to train reliable PRMs makes the approach expensive and time-consuming (Lightman et al., 2023). The generalization capability of PRMs is often limited, as reasoning patterns can vary significantly across different tasks and domains. Furthermore, when serving as judges, LLMs may produce unreliable evaluations due to hallucination (Hu et al., 2024; Li et al., 2024), particularly for complex tasks (Thakur et al., 2024). Recent studies show that LLMs cannot reliably self-correct (Huang et al., 2023) or selfvalidate without external verification (Stechly et al., 2024). These limitations motivate our approach of grounding process supervision in concrete, verifiable signals rather than learned judgments."
        },
        {
            "title": "2.2 Execution-Driven Code Generation",
            "content": "Code generation is typically formulated as sequence-to-sequence problem: given input specification (including natural language description and test cases), generate program that correctly implements the required functionality (Jiang et al., 2024b). While most existing approaches treat this as single-step generation process (Chen et al., 2021a), recent work has explored using execution feedback to guide code generation (Zhong et al., 2024; Zhang et al., 2023) or use CoT prompting to improve correctness (Shinn et al., 2024). Although these execution-guided approaches show promise, our experiments indicate they are insufficient for complex programming tasks that require deeper reasoning. While execution feedback is easy to measure, it alone provides little guidance on how to improve solutions that fail or how to make working solutions more efficient. More importantly, it offers no feedback during the intermediate stages of development, when early course corrections could prevent cascading errors. Consider implementing an efficient sorting algorithm: model might write code that passes all test cases but uses an inefficient O(n2) approach. Outcome supervision would mark this as success, missing the opportunity to guide the model toward more optimal O(n log n) solution. Similarly, if the code fails, sparse \"fail\" signal provides no insight into whether the error lies in the algorithmic approach, the implementation details, or edge case handling. These limitations of both process and outcome supervision highlight the need to rethink how to supervise the development of complex programs, where both theoretical understanding and practical implementation must evolve together."
        },
        {
            "title": "3 Methodology",
            "content": "We formulate code generation as step-by-step reasoning process, where each state on our reasoning tree contains combination of theoretical reasoning, code implementation, execution outcomes and step-level reward. This unified framework generalizes both outcome and process supervision by treating the refinement of outcomes as the process to be supervised. The overall approach is outlined in Figure 2 and formalized in Algorithm 1. 3 Algorithm 1 Outcome-Refining Process Supervision Require: Problem x, Model M, Beam size K, Steps , Candidates Initialize beam0 {(x, )} {Start with problem description} for step = 1 to do paths {Initialize reasoning paths} for state in beamt1 do chain s.reasoning_chain {Copy current reasoning chain} candidates Mreason(x, chain, ) {Generate reasoningimplementation pairs to be considered as the next step} for candidate in candidates do reasoning, code Extract from candidate new_chain chain reasoning code {Extend new chain} feedback Execute and analyze code critique, reward Mcritic(new_chain, code, feedback) new_chain new_chain critique paths paths {(new_chain, reward)} end for end for beamt Select top-K paths by reward {Keep promising paths} if Any path in beamt is complete then break end if end for return Best reasoning chain from beamT"
        },
        {
            "title": "3.1 Outcome-Refining Process Supervision",
            "content": "Our framework introduces novel supervision paradigm where the refinement of outcomes serves as the process to be supervised. Unlike traditional approaches that rely on linear Chain-of-Thought structures, we use beam search over tree structure where each state embodies the dual nature of code generation - theoretical understanding and practical implementation. The states evolve through self-refining process: reasoning chains capture the changes and observations on theoretical approach and implementation strategy, while execution outcomes provide concrete signals for refinement. This interaction between reasoning and execution creates refinement process: execution outcomes inform theoretical improvements, while enhanced reasoning guides better implementations. The tree structure, combined with beam search, enables deeper reasoning exploration crucial for complex programming tasks. By maintaining multiple promising trajectories simultaneously, our framework allows diverse exploration of different strategies - capability particularly important for problems requiring extensive reasoning and algorithmic insights. Through comprehensive state representation including reasoning chains, code, and execution feedback, inferior solution paths are naturally replaced when better alternatives are discovered. Our experimental results in Figure 4 demonstrate this outcome-guided exploration scales effectively with increased computational budget, which suggests the importance of providing sufficient reasoning space for complex programming tasks."
        },
        {
            "title": "3.2 Self-Critic with Generative Rewarding",
            "content": "Traditional PRMs are trained in discriminative manner, producing only numerical scores without explaining their evaluation rationale. This design fails to leverage language models inherent ability to reason about their judgments. Our framework takes different approach by treating process reward generation as reasoning task itself. Before assigning scores, the model first articulates its analysis of the reasoning chain and execution outcomes, considering factors like algorithmic complexity, code structure, and performance metrics. This generative approach allows the model to benefit from its own chain-of-thought process when making evaluations. Our experiments Table 4 also demonstrate that this generative reward mechanism outperforms traditional discriminative PRMs, even for explicitly trained ones. The improvement stems from the models ability to perform detailed analysis before making judgments, rather than directly mapping inputs to numerical scores."
        },
        {
            "title": "3.3 Execution-Guided Process Reward Model",
            "content": "While traditional PRMs rely on expensive training data, we leverage concrete execution signals to ground the models judgment. Existing methods use PRMs that are trained on human-annotated data to learn evaluation criteria for reasoning steps. Ours eliminates this training requirement through since PRM training essentially key insight: grounds the models judgment through humandefined criteria, we can achieve similar grounding by providing concrete execution signals directly to the language model. This allows execution outcomes to serve as objective anchors for evaluation while preserving the models inherent reasoning capabilities. The process rewards emerge from holistic analysis across multiple dimensions: the soundness and coherence of reasoning steps, the relationship between reasoning and practical implementation, and concrete execution metrics including correctness, performance, and resource usage. This grounded approach offers eliminates PRM training costs, reduces hallucination through concrete verification, and enables reliable evaluation through objective metrics."
        },
        {
            "title": "4 Experiments",
            "content": "Our experimental evaluation aims to address three key questions: (1) How effective is our framework 4 Table 1: Main Results on Code Generation Benchmarks. Pass@1: solutions passing all test cases. Tests: average test cases passed. Valid: solutions that compile and execute. Time: relative execution time, compared to the standard solution. Best results are in bold and second-best are underlined, every metric is in percentage. Model/Method LBPP (2024) HumanEval (2021b) MBPP (2021) Pass@1 Tests Valid Time Pass@1 Tests Valid Time Pass@1 Tests Valid Time Llama-3.1-8B-Instruct (2024) 44.3 49.3 39.8 64.7 66.9 81.4 CoT Reflexion LDB (w/ T) BoN ORPS ORPS (w/ T) 30.9 34.0 25.9 46.9 45.9 67.1 63.0 67.3 58.0 84.6 88.5 93.7 176.8 148.5 252.2 107.6 99.1 89.4 DeepSeek-Coder-7B-Instruct-v1.5 (2024) CoT Reflexion LDB (w/ T) BoN ORPS ORPS (w/ T) 32.7 25.9 31.5 49.4 56.3 63. 45.9 41.9 45.7 63.9 71.1 80.8 67.3 63.0 61.7 80.2 88.0 96.9 Qwen-2.5-Coder-7B-Instruct (2024) CoT Reflexion LDB (w/ T) BoN ORPS ORPS (w/ T) 40.1 37.7 35.8 53.1 59.9 77.8 55.3 57.1 49.9 68.8 75.7 87. 72.2 78.4 65.4 85.8 92.0 96.9 Qwen-2.5-Coder-14B-Instruct (2024) 77.2 53.7 82.1 60.5 75.3 51.9 90.7 61.7 90.7 61.7 95.7 85.8 CoT Reflexion LDB (w/ T) BoN ORPS ORPS (w/ T) 63.9 70.5 62.9 74.9 77.4 90.7 GPT-4o-Mini (2024) 50.0 62.3 54.9 64.2 67.9 88.9 CoT Reflexion LDB (w/ T) BoN ORPS ORPS (w/ T) 65.9 73.9 67.8 78.6 81.2 94.3 80.2 87.7 82.7 93.8 94.4 98.1 160.1 153.0 206.2 123.4 89.4 74.4 118.6 111.2 187.8 117.9 84.1 82.4 119.2 113.3 225.2 115.6 84.8 64.2 124.5 93.2 220.1 88.9 81.5 61. 50.0 54.9 54.3 71.3 70.3 91.4 65.9 63.4 74.4 73.8 76.2 95.7 72.6 75.6 87.8 77.4 79.9 96.3 82.9 83.5 89.6 87.8 81.7 97.0 79.9 75.0 88.4 82.9 84.8 97.6 68.4 71.1 62.3 84.7 87.5 95. 78.2 77.1 80.0 88.1 90.0 98.0 79.0 81.1 90.3 85.1 91.6 98.0 88.5 89.9 92.0 93.9 91.3 98.5 87.5 83.6 92.2 90.2 92.7 98.7 82.9 83.5 66.5 93.3 96.2 98.1 85.4 86.6 81.7 94.5 96.3 99. 82.3 84.1 91.5 87.8 96.3 98.8 90.2 92.7 92.7 95.7 96.3 99.4 90.9 87.2 93.9 92.7 96.3 99.4 98.1 107.5 127.1 77.3 65.8 63.6 86.9 101.0 85.6 64.1 40.6 31.8 79.2 73.6 76.1 66.8 48.3 43. 76.6 68.8 140.5 58.8 41.5 43.8 80.5 75.1 133.4 66.5 57.5 46.2 58.0 58.8 43.6 73.5 71.8 90.4 69.3 68.9 61.1 74.3 73.2 93.0 79.0 79.0 66.9 82.9 76.7 94.9 84.0 83.3 72.4 81.7 76.3 95. 78.6 79.4 72.8 80.5 80.2 95.7 64.9 65.0 47.1 79.9 78.2 93.1 75.0 74.4 64.0 80.2 80.3 94.7 83.3 84.0 69.4 87.2 82.4 96.4 87.4 87.2 74.6 86.4 82.0 96.9 83.5 84.0 75.5 85.5 86.0 97. 72.4 71.2 49.4 86.4 84.3 95.6 80.9 80.2 66.1 86.8 87.5 96.1 88.3 88.7 72.0 91.8 88.3 97.3 91.1 91.1 76.3 91.1 87.9 98.1 87.9 88.3 77.8 89.9 91.8 98.4 91.9 88.6 170.7 72.1 84.5 59. 77.7 74.2 98.3 68.9 46.8 34.2 67.3 63.5 96.8 62.6 68.0 45.3 67.5 66.0 149.7 58.4 58.8 41.0 70.3 67.6 157.9 64.6 64.7 51.4 compared to existing approaches? (2) How does each component of our framework contribute to the overall performance? (3) What insights can we gain about the relationship between reasoning quality and code generation? Table 2: Dataset Statistics. Characteristics of the programming benchmarks used in evaluation. LBPP (Matton et al., 2024) HumanEval (Chen et al., 2021b) MBPP (Austin et al., 2021) # Test Problems # Unit Tests Solution Length Contamination 162 5.1 627 / 3039 New Dataset 164 6.5 169 / 622 18.9% 257 3.0 130 / 589 20.8%"
        },
        {
            "title": "Competitive\nProgramming\nAlgorithms",
            "content": "Basic Functions Func. Completion Basic Functions Basic Prog. From sanitized version; Contamination results reported from Riddell et al. (2024); Average/maximum characters in solution code."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. We evaluate on 3 programming benchmarks as shown in Table 2. LBPP is recent complex programming dataset manually curated by human experts with competitive programming experience. HumanEval and MBPP are popular code generation benchmarks but could be too easy for current LLMs (Matton et al., 2024). Moreover, significant proportion of their data is leaked in popular pre-training corpora (Riddell et al., 2024). To ensure reproducibility, we report our detailed hyperparameters in Appendix A, we also open-source all our code and scripts in the aforementioned URL. Baselines. For outcome supervision, Reflexion (Shinn et al., 2024) is recent self-improvement approach that leverages execution feedback for 5 code refinement. LDB (Zhong et al., 2024) enhances this by incorporating execution outcomes, debugger outputs, and intermediate variable values to iteratively fix solutions. For test-time scaling, we implement Best-of-N sampling, which generates multiple solutions and selects the best based on test outcomes. Due to lack of existing process supervision methods for code generation, we implement similar approach from mathematical reasoning (Luo et al., 2024) ourselves for comparison and show in ablation studies. As some methods require access to unit tests from benchmarks, we mark them as (w/ T) in our tables. Evaluation Metrics. We primarily report Pass@1, while also tracking the average percentage of passed test cases and successful compilations to evaluate program correctness in detail. Additionally, we analyze code quality through complexity measures and resource utilization metrics including execution time, memory usage, and cyclomatic complexity, providing comprehensive view beyond mere correctness. Detailed descriptions of these metrics are provided in Table 5."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 shows the comparative results of our method and baselines, Figure 3 provides detailed multi-dimensional profiling of the performance of generated solutions with different methods. Our results indicate significant improvements in both correctness and code quality metrics, especially on harder benchmarks. Even smaller model (Qwen 7B), when paired with our method, could surpass its larger variant (Qwen 14B) without our method, suggesting that providing sufficient reasoning space can be more effective than solely scaling model parameters - which is significantly more computationally expensive. This finding has important implications for practical applications where computational resources are limited. When compared to other execution-feedback and outcome reward based methods like Reflexion and LDB, our approach consistently demonstrates superior performance regardless of test case access. This improvement stems from fundamental difference in approach: while these outcome-based methods focus primarily on local information like resolving execution errors and reasoning in chain structure, our method provides LLMs with broader reasoning space to reflect on higher-level aspects such as algorithm selection and problem properties by using process reward guided search. For Figure 3: Multi-dimensional Performance Analysis. Metrics are normalized against the LBPP standard solutions (1.0) and averaged across all backbone models. Higher values indicate better performance. instance, LDB achieves 35.8% Pass@1 on LBPP with Qwen-7B with test case access, while our method reaches 77.8% under the same conditions. Particularly noteworthy is the performance boost when models have access to actual unit tests from test datasets (without access to solutions). All models show drastic improvements on all metrics in this setting. For instance, Qwen-7B achieves 77.8% Pass@1 on LBPP and 96.3% on HumanEval with test case access, compared to 59.9% and 79.9% without. This suggests that while our selfgenerated test cases may be relatively weak, given feedback for higher quality test cases, models can effectively guide themselves through the reasoning process to generate significantly better code. Figure 3 further supports these findings through detailed profiling results, showing consistent improvements over baselines across all models in terms of code efficiency and quality metrics. Guided by our framework, models are capable of refining themselves to generate faster, more coherent code. However, we do observe slight disadvantage on MBPP, particularly when comparing with Best-of-N sampling. This is less concerning given that, as shown in Table 2, MBPP consists of relatively simple problems with short solutions, and significant portion (20.8%) of its test data already exists in publicly available pre-training datasets."
        },
        {
            "title": "4.3 Component-wise Ablation Study",
            "content": "We conducted experiments on the challenging LBPP dataset using the Qwen-7B model to inves6 Table 3: Ablation Study Results. - Execution: Remove execution feedback from our framework. - Reasoning: Remove in-depth reasoning process. Every metric is in percentage."
        },
        {
            "title": "ORPS",
            "content": "- Execution - Reasoning Pass@1 Tests Valid Time 59.9 43.8 55.6 75.7 56.4 74.5 92.0 72.8 94.4 84.1 200.5 124. tigate the importance of incorporating execution outcomes and reasoning during the exploration process. The results are presented in Table 3. When the model is unable to access execution outcomes during exploration, Pass@1 decreases by 16.1%. This highlights the critical role of realenvironment execution in guiding the model to generate correct solutions. It is relatively challenging for LLMs to predict the execution outcomes of given piece of code (Jain et al., 2024). Integrating these results into the exploration process ensures that the model benefits from concrete and actionable feedback. Conversely, omitting extensive reasoning during exploration results in 4.3% decrease in Pass@1. Although this reduction is smaller compared to the absence of execution results, reasoning remains vital component for performance enhancement. Reasoning enables the model to iteratively refine its approach based on feedback, addressing issues that may not be resolved through execution feedback alone. This iterative refinement is crucial, particularly for solving complex problems where execution feedback alone may be insufficient. Table 4: Analysis of Process Reward Model. Granularity refers to the level of detail in the reward signal (line-level or outcome-level). Train indicates whether the process reward model requires training. Methods Granularity Train Outcome"
        },
        {
            "title": "Line\nOutcome\nLine",
            "content": "Pass@1 Tests Valid Time 37.0 32.1 59.9 38.3 48.3 43.9 75.7 52.8 65.4 59.3 92.6 70.4 153.8 153.4 89.1 123."
        },
        {
            "title": "4.4 Analysis of Process Reward Model",
            "content": "Our framework uses an implicit process reward model (PRM), which provides outcome-level supervision during beam search without additional training. Most existing work on process supervision generates line-level process reward signals and relies on explicitly trained PRMs. This motivates us to explore two questions: (1) Are outcome-level rewards more effective than line-level rewards? (2) Is an implicit PRM that does not require training better than an explicitly trained PRM? To address these questions, we conduct experiments on the LBPP dataset using the Qwen-7B model. For the outcome-level process supervision method, the implementation details are consistent with the corresponding parts of ORPS. For the line-level method, the model generates step-by-step thoughts for the coding problem, with numerical process rewards assigned to each step. The final code is then generated based on the best thought trace. For methods requiring explicit training, we randomly select half of the LBPP dataset as training set to avoid data leakage. To simulate humanannotated process feedback, we filter data from GPT-4s outputs. Results in Table 4 confirm that our framework substantially outperforms the other three method, validating our design choices. Overall, outcomelevel reward signals prove to be more effective than line-level signals. Intuitively, line-level signals can only provide feedback for incomplete thought processes, which undoubtedly lack more information compared to the outcome-level. Additionally, the implicit PRM shows greater effectiveness than the explicit PRM. This suggests that external process supervision feedback may not always be reliable. We consider that LLMs already have strong self-reflection capabilities and only require execution outcomes to activate this ability. This also indicates that spending extra data and time on training reward models might be unnecessary."
        },
        {
            "title": "4.5 Scaling Analysis",
            "content": "In addition, we wanted to explore how the performance of ORPS changes as reasoning overhead increases. For comparison, we chose BoN as the baseline. This is because BoN allows easy control of reasoning overhead with linear growth. We conducted experiments on two models using the most challenging LBPP dataset. The results are shown in Figure 4. With the same model, ORPS improved much faster as reasoning overhead increased. This shows that ORPS has strong scaling potential. It can effectively use more computational resources to improve reasoning. In comparison, BoN showed slower improvements, suggesting it does not fully utilize the increased reasoning capacity. 7 Figure 4: Performance vs. Inference Budget. Pass@1 scores on LBPP with varying inference budgets. Our method maintains superior performance across different computational constraints. Figure 5: Performance by Problem Class. Top-20 problem classes in LBPP showing success rates and unsolved cases for our method vs baseline."
        },
        {
            "title": "4.6 Case Studies",
            "content": "the also analyzed improvements We of across different problem categories. ORPS As shown in Figure 5, on the competitive programming dataset LBPP, our method shows significant improvements over the CoT Baseline, especially in more difficult categories. For instance, in complex algorithmic tasks such as dynamic programming, loops, and graphs, our method correctly solves nearly twice as many problems as CoT. This further confirms that high-quality intrinsic reasoning can help models avoid logical pitfalls when tackling difficult coding tasks. Through detailed case studies, we demonstrate how our framework enhances code generation by improving reasoning. As shown in Appendix C, the response generated by the traditional CoT method for the Minimum Greatest Common Divisor problem in LBPP demonstrates that while the model provides detailed step-by-step thought process during solution generation, the complexity of the task results in an imperfect code implementation. For instance, in CoTs approach, the reliance on nested loops and pairwise GCD calculations introduces inefficiencies and fails to address scalability for larger datasets. Similarly, our methods initial implementation demonstrates lack of robustness in handling edge cases and unnecessary redundancies in subset formation. However, ORPS achieves more accurate solution through finer reasoning. The code initially generated by our model contains redundancies and erroneous logic. Nevertheless, with the feedback from the critic on the execution outcomes, the programmer successfully refines the code to reach 8 correct implementation. This iterative process not only eliminates logical errors but also optimizes performance, demonstrating the advantage of integrating structured feedback into code generation."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we introduced Outcome-Refining Process Supervision (ORPS), novel paradigm for enhancing code generation through structured reasoning and execution-driven feedback. By leveraging tree-structured exploration framework, ORPS facilitates diverse solution trajectories, enabling models to refine both theoretical reasoning and practical implementation simultaneously. Our approach demonstrated significant improvements in correctness and efficiency across various benchmarks, including an average Pass@1 increase of 26.9% and 42.2% reduction in runtime, outperforming traditional outcome-based and process-based supervision methods. Key findings reveal that structured reasoning space and concrete feedback signals are pivotal for solving complex programming tasks. ORPS proved effective even with smaller models, underscoring the importance of reasoning capabilities over mere model scaling. Furthermore, our frameworks reliance on execution feedback eliminates the need for expensive, annotated training data, making it cost-efficient alternative. These contributions highlight the potential of process supervision to enhance complex problemsolving abilities in LLMs. Future work could extend this framework to other domains requiring rigorous reasoning and verification. By bridging the gap between reasoning quality and execution fidelity, ORPS paves the way for more autonomous and adaptive systems in computational intelligence."
        },
        {
            "title": "6 Limitations",
            "content": "While ORPS demonstrates significant advancements in code generation, it has notable limitations. First, its reliance on execution outcomes as primary feedback restricts its applicability to tasks where execution is feasible and well-defined. Ambiguous problem descriptions or creative tasks beyond executable code can hinder the frameworks performance. Additionally, this reliance may lead to bias toward solutions optimizing immediate execution success, potentially overlooking broader algorithmic considerations. The tree-structured beam search also introduces significant computational overhead, requiring substantial memory and processing power. While scalable with increased resources, this approach may be impractical for real-time or resource-constrained applications, limiting its utility in some environments. Finally, the quality of ORPS depends heavily on the underlying language model. If the model lacks domain knowledge or reasoning ability, it may generate incoherent reasoning chains or fail to explore promising solutions effectively, especially when feedback mechanisms rely solely on execution outcomes without additional validation."
        },
        {
            "title": "7 Ethical Considerations",
            "content": "While ORPS advances code generation capabilities, we acknowledge important ethical considerations. Like all LLM-based systems, our framework inherits potential biases from the underlying models. However, our execution-guided approach provides an advantage: concrete verification helps detect and mitigate certain failure modes through objective testing. The frameworks ability to explore multiple solution paths also reduces the risk of being stuck with problematic implementations. We emphasize that ORPS is designed as development aid rather than replacement for human programmers. The execution feedback and reasoning traces make the code generation process more transparent and auditable. We encourage using this framework in conjunction with established software development practices, including code review and testing, particularly for applications in sensitive domains."
        },
        {
            "title": "References",
            "content": "Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403. Devansh Arpit, Stanisław Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. 2017. closer look at memorization in deep networks. In International conference on machine learning, pages 233242. PMLR. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Ann Campbell. 2018. Cognitive complexity-a new way of measuring understandability. SonarSource SA, page 10. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. survey on evaluation of large language models. arXiv preprint arXiv:2307.03109. Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2021a. Evaluating the rationales of amateur investors. In Proceedings of the Web Conference 2021, pages 39873998. Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024a. Humans or llms as the judge? study on judgement biases. arXiv preprint arXiv:2402.10669. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024b. Alphamath almost zero: process supervision without process. arXiv preprint arXiv:2405.03553. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large lanarXiv preprint guage models trained on code. arXiv:2107.03374. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. 2024c. Autoprm: Automating procedural supervision for multi-step reasoning via controlarXiv preprint lable question decomposition. arXiv:2402.11452. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023). Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct arXiv preprint with tool-interactive critiquing. arXiv:2305.11738. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming arXiv preprint the rise of code intelligence. arXiv:2401.14196. Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, and Weiming Lu. 2024a. Advancing process verification for large language models via tree-based preference learning. arXiv preprint arXiv:2407.00390. Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, and Han Zhao. 2024b. Semi-supervised reward modeling via iterative self-training. arXiv preprint arXiv:2409.06903. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701. Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, and Lin Yan. 2024. Process supervision-guided policy optimization for code generation. arXiv preprint arXiv:2410.17621. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Jonathan St BT Evans. 2003. In two minds: dualprocess accounts of reasoning. Trends in cognitive sciences, 7(10):454459. Markus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. arXiv preprint arXiv:1702.01806. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Varun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue, and Zachary Nado. 2023. Deep learning tuning playbook. Version 1.0. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning, volume 1. MIT Press. Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. 2006. fast learning algorithm for deep belief nets. Neural Computation, 18:15271554. Lynette Hirschman and Robert Gaizauskas. 2001. Natural language question answering: the view from here. natural language engineering, 7(4):275300. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. 2024. Are llm-based evaluators confusing nlg quality criteria? arXiv preprint arXiv:2402.12055. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. 10 Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. 2024. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, et al. 2024a. Technical report: Enhancing llm reasoning with rewardguided tree search. arXiv preprint arXiv:2411.11694. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024b. survey on large language models for code generation. arXiv preprint arXiv:2406.00515. Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy Chen, and Shafiq Joty. 2024. Learning planningbased reasoning by trajectories collection and arXiv preprint process reward synthesizing. arXiv:2402.00658. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843 3857. Yucheng Li. 2023. An open source data contamination report for llama series models. arXiv preprint arXiv:2310.17589. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Yachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, and Xing Xie. 2023. Meta semantic template for evaluation of large language models. arXiv preprint arXiv:2310.01448. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. 2023. Lets reward step by step: Step-level reward model arXiv preprint as the navigators for reasoning. arXiv:2310.10080. Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen GilsenanMcMahon, and Matthias Gallé. 2024. On leakage of code generation evaluation datasets. arXiv preprint arXiv:2407.07565. Marvin Muñoz Barón, Marvin Wyrich, and Stefan Wagner. 2020. An empirical validation of cognitive complexity as measure of source code understandability. In Proceedings of the 14th ACM/IEEE international symposium on empirical software engineering and measurement (ESEM), pages 112. OpenAI. 2023. Gpt-4 technical report. Preprint, arXiv:2303.08774. OpenAI. 2024. o1 system card. https://cdn.openai. com/o1-system-card.pdf. Accessed: Dec 9, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. 11 Kaiping Peng, Richard Nisbett, and Nancy YC Wong. 1997. Validity problems comparing values across cultures and possible solutions. Psychological methods, 2(4):329. Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. 2024. On the self-verification limitations of large language models on reasoning and planning tasks. arXiv preprint arXiv:2402.08115. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 16. IEEE. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 114. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506. Martin Riddell, Ansong Ni, and Arman Cohan. 2024. Quantifying contamination in evaluating code generation capabilities of language models. arXiv preprint arXiv:2403.04811. Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. arXiv preprint arXiv:2310.18018. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024. Rewarding progress: Scaling automated proarXiv preprint cess verifiers for llm reasoning. arXiv:2410.08146. Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune bert for text classification? In Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 1820, 2019, Proceedings 18, pages 194 206. Springer. Maciej Swiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Mandziuk. 2023. Monte carlo tree search: review of recent modifications and applications. Artificial Intelligence Review, 56(3):2497 2562. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXiv preprint arXiv:2406.12624. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. \" OReilly Media, Inc.\". Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Yamamoto Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processbased and outcome-based feedback. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, and Dong Yu. 2024a. Litesearch: Efficacious tree search for llm. arXiv preprint arXiv:2407.00320. 12 Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. 2023a. Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. arXiv preprint arXiv:2310.07521. Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. 2022. Glue-x: Evaluating natural language understanding models from an outarXiv of-distribution generalization perspective. preprint arXiv:2211.08073. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. 2024b. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671. Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, et al. 2023. Supervised knowledge makes large language models better incontext learners. arXiv preprint arXiv:2312.15918. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024c. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439. Yidong Wang, Zhuohao Yu, Jindong Wang, Qiang Heng, Hao Chen, Wei Ye, Rui Xie, Xing Xie, and Shikun Zhang. 2024d. Exploring vision-language models for imbalanced learning. International Journal of Computer Vision, 132(1):224237. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023c. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560. Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. 2024e. Multi-step problem solving through verifier: An empirical analysis on model-induced process supervision. arXiv preprint arXiv:2402.02658. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. Codeshell technical report. arXiv preprint arXiv:2403.15747. Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. 2024. Watch every step! llm agent learning via iterative step-level process refinement. arXiv preprint arXiv:2406.11176. 13 Wenjin Yao, Yidong Wang, Zhuohao Yu, Rui Xie, Shikun Zhang, and Wei Ye. 2024. Pure: Aligning llm via pluggable query reformulation for enhanced helpfulness. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 8721 8744. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, and Shikun Zhang. 2024a. Kieval: knowledgegrounded interactive evaluation framework for large language models. arXiv preprint arXiv:2402.15043. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, and Shikun Zhang. 2024b. Freeeval: modular framework for trustworthy and efficient evaluation of large language models. arXiv preprint arXiv:2404.06003. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816. Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023. Self-edit: Fault-aware code editor for code generation. arXiv preprint arXiv:2305.04087. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Li Zhong, Zilong Wang, and Jingbo Shang. 2024. Ldb: large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906."
        },
        {
            "title": "A Experimental Setup and\nHyperparameter Details",
            "content": "This appendix provides comprehensive description of the experimental setup, encompassing the hyperparameters, software, and hardware configurations employed in this study. A.1 Search Algorithm Hyperparameters (ORPS) The following hyperparameters were used for the search algorithm in ORPS: Search Depth (num_rounds): 5. This parameter defines the maximum depth of the search tree, representing the number of iterative steps in the search process. Beam Width (top_k): 3. This parameter specifies the number of highest-scoring candidate solutions (traces) retained at each step of the beam search. Expansion Factor (num_samples): 20. This represents the number of new states (candidate solutions) explored from each state during the search process. A."
        },
        {
            "title": "Inference Configuration",
            "content": "All inference experiments were conducted on single machine using the FreeEval (Yu et al., 2024b) codebase, integrated with Hugging Faces text-generation-inference toolkit for efficient model serving. The following inference settings were applied: Maximum Context Length (max_tokens): 18,000 tokens. This parameter defines the maximum number of tokens allowed in the input sequence to the model. Generated Tokens per Round: 1,500 tokens. This specifies the number of new tokens generated by the model in each round of inference. A.3 Execution Constraints To ensure consistent and reproducible results, the following execution constraints were enforced during inference: Memory Limit: 512 MB. This constraint restricts the maximum memory allocation permitted for each test case. Maximum Test Cases per Problem: 15. This sets an upper bound on the number of test cases evaluated for each problem. A.4 Model Training Configuration This section outlines the hyperparameters and settings used during the training phase of the model, which was pertinent to the analysis experiments (subsection 4.4). While ORPS itself does not require training, these details are provided for completeness and reproducibility. Training Framework: llamafactory (Zheng et al., 2024) Optimization Framework: DeepSpeed ZeRO3 (Rajbhandari et al., 2020) (Zero Redundancy Optimizer Stage 3). This enables efficient training of large models by partitioning optimizer states, gradients, and model parameters across data parallel processes. Base Model: qwen-2.5-coder-7b-instruct. This is the pre-trained language model upon which further training was conducted. Batch Size per Device: 2. This defines the number of training examples processed on each GPU before gradient update step. Gradient Accumulation Steps: 4. This allows simulating larger effective batch size by accumulating gradients over multiple forward and backward passes before updating model weights. The effective batch size is therefore 8 (2 per device * 4 steps). Learning Rate: 2 105. This parameter controls the step size taken during gradientbased optimization. Learning Rate Scheduler: Cosine decay. This gradually reduces the learning rate over the course of training, following cosine function. Timeout per Test Case: 5 seconds. This limits the maximum execution time allowed for each test case. Number of Training Epochs: 2.0. This specifies the number of complete passes through the entire training dataset. 14 Table 5: Performance Metrics Description. Our evaluation framework uses both dynamic execution profiling and static code analysis metrics to comprehensively assess code quality and efficiency."
        },
        {
            "title": "Cognitive Complexity",
            "content": "Total CPU time spent executing the code, measured in nanoseconds. Lower values indicate more efficient execution and better algorithmic optimization. Number of CPU instructions executed during runtime. Reflects computational efficiency, with lower counts suggesting more optimized code paths and better algorithm implementation. Frequency of incorrect branch predictions during execution. Lower values indicate better code predictability and CPU pipeline efficiency, resulting in faster execution times. Number of times the program needs to access virtual memory. Fewer page faults suggest better memory management and more efficient memory access patterns. Total number of lines in the source code. Generally, shorter code length indicates more concise solutions while maintaining readability and functionality. Number of nodes in the Abstract Syntax Tree. Measures structural complexity of the code, with fewer nodes suggesting simpler and more maintainable implementation. Quantifies the number of linearly independent paths through the code. Lower values indicate easier-to-maintain and test code, reducing potential bug sources. Measures how difficult the code is to understand, based on control flow structures and nesting. Lower scores suggest more readable and maintainable code that is easier to debug. Maximum Sequence Length: 16,384 tokens. This defines the maximum length of the input sequences during training. dataset (Problem #101) that demonstrates how our method handles complex logical reasoning tasks. The original prompt of this problem is: You are given an array of integers, each of which is greater than 1. This array needs to be split into subsets such that the greatest common divisor within each subset is greater than 1. Write Python program to return the minimum number of subsets required to create such split. Mixed Precision Training: Enabled with bf16 (Brain Floating Point 16-bit format). This accelerates training by performing some computations with reduced precision while maintaining model accuracy. A.5 Hardware Environment All experiments were performed on NVIDIA A800 GPUs, each equipped with 80GB of GPU memory."
        },
        {
            "title": "B AI Usage in Code Development",
            "content": "During the development of ORPS and the design of its experiments, LLMs were employed to assist with coding. All AI-assisted code were reviewed and refined by the authors to ensure correctness and alignment with the research goals."
        },
        {
            "title": "C Example Model Outputs",
            "content": "To illustrate the effectiveness of our approach, we present representative example from the LBPP"
        },
        {
            "title": "CoT Output",
            "content": "Prob: Given an array of integers, split it into the minimum number of subsets such that the GCD of each subset is greater than 1. Reasoning Process: 1. First, we need to understand what makes numbers group-able: - Numbers can be in the same subset if their GCD > 1 - We want to minimize the number of subsets 2. Key observations: - If GCD(a,b) > 1 and GCD(b,c) > 1, a,b,c can potentially be in the same group - This suggests graph-like structure where numbers are connected if their GCD > 1 3. Solution approach: - Treat numbers as nodes in graph - Connect numbers with edges if their GCD > 1 - Find connected components in this graph - Each component represents one subset 4. Implementation strategy: - Use Union-Find data structure for efficient component tracking - Calculate GCD using Euclidean algorithm - Final answer is the number of connected components Generated Solution: 1 def gcd(a, b): 2 while b: 3 4 a, = b, % return 5 6 def minGroups(nums): 7 = len(nums) parent = list(range(n)) 8 9 11 12 13 14 15 17 18 19 20 21 23 def find(x): if parent[x] != x: parent[x] = find(parent[x]) return parent[x] def union(x, y): parent[find(x)] = find(y) for in range(n): for in range(i + 1, n): if gcd(nums[i], nums[j]) > 1: union(i, j) return len(set(find(i) for in range(n)))"
        },
        {
            "title": "ORPS Programmer Output",
            "content": "# === Programmer Thoughts === # The problem requires splitting an array into subsets such that the greatest common divisor (GCD) within each subset is greater than 1. To achieve this, we can use greedy algorithm. The idea is to iterate through the array and add each element to the existing subset if their GCD is greater than 1. If the GCD is 1, we start new subset. # We need to implement function that calculates the GCD of two numbers. Then, we can use this function to determine if an element can be added to an existing subset or if we need to start new subset. # We also need to ensure that our solution is efficient. The time complexity of the GCD calculation can be optimized using the Euclidean algorithm, which has time complexity of O(log(min(a, b))). # === Solution === 1 import math 2 3 def gcd(a, b): \"\"\" 4 Calculate the greatest common divisor of two numbers using the Euclidean algorithm. \"\"\" while b: 6 5 7 9 a, = b, % return 10 11 def min_g_c_d_subsets(nums): 12 \"\"\" Split the array into subsets such that the greatest common divisor within each subset is greater than 1. Return the minimum number of subsets required. \"\"\" subsets = [] for num in nums: added = False for subset in subsets: 14 15 16 17 18 20 21 22 23 24 26 if gcd(num, subset[-1]) > 1: subset.append(num) added = True break if not added: subsets.append([num]) return len(subsets)"
        },
        {
            "title": "ORPS Critic Feedback",
            "content": "# === Critic Thoughts === The code provided attempts to solve the problem by iterating through each number and adding it to an existing subset if the GCD with the last element of the subset is greater than 1. If no such subset exists, new subset is created. This approach ensures that each subset has GCD greater than 1. However, there are few issues and improvements that can be made: 1. **Test Case Failures**: The test cases for indices 5, 8, and 10 fail. This indicates that the current implementation does not correctly handle certain scenarios where numbers with no common divisor greater than 1 are present. 2. **Performance**: The code has higher cognitive complexity, which means it is harder to understand and maintain. The use of nested loops and the nested if-else structure can be optimized. 3. **Efficiency**: The current approach has time complexity of O(n2) due to the nested loops. This can be optimized to O(n log n) by sorting the numbers first and then using single pass to form subsets. 4. **Code Clarity**: The code can be made more readable by breaking down the logic into smaller functions and adding comments. 5. **Handling Edge Cases**: The code does not handle edge cases such as an empty list or list with single element. # === Score === $$3$$"
        },
        {
            "title": "ORPS Programmer Improvement",
            "content": "# === Programmer Thoughts === # The feedback indicates that the current implementation has several issues, including failing test cases and high cognitive complexity. We need to address these issues by improving the logic and ensuring that all edge cases are handled correctly. # **Key Improvements**: 1. **Handling Edge Cases**: Ensure that the function handles an empty list or list with single element. 2. **Efficiency**: Optimize the approach to achieve time complexity of O(n log n) by sorting the numbers first and then using single pass to form subsets. 3. **Code Clarity**: Break down the logic into smaller functions and add comments for better readability. # **Approach**: 1. **Sort the Array**: Sorting the array ensures that numbers with common divisor are adjacent, making it easier to form subsets. 2. **Single Pass to Form Subsets**: Iterate through the sorted array and add each number to the current subset if the GCD with the last element of the subset is greater than 1. If not, start new subset. 3. **Edge Case Handling**: If the list is empty, return 0. If the list has single element, return 1. # === Solution === \"\"\"Calculate the greatest common divisor of two numbers using the Euclidean algorithm.\"\"\" while b: 1 import math 2 3 def gcd(a, b): 4 a, = b, % return 8 9 def min_g_c_d_subsets(nums): 5 6 7 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 \"\"\" Split the array into subsets such that the greatest common divisor within each subset is greater than 1. Return the minimum number of subsets required. :param nums: List of integers greater than 1 :return: Minimum number of subsets required \"\"\" if not nums: return 0 if len(nums) == 1: return 1 # Sort the array to ensure numbers with common divisor are adjacent nums.sort() subsets = [] for num in nums: added = False for subset in subsets: if gcd(num, subset[-1]) > 1: subset.append(num) added = True break if not added: subsets.append([num]) return len(subsets)"
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Peking University"
    ]
}