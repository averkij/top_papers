{
    "paper_title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling",
    "authors": [
        "Ivan Rodkin",
        "Daniil Orel",
        "Konstantin Smirnov",
        "Arman Bolatov",
        "Bilal Elbouardi",
        "Besher Hassan",
        "Yuri Kuratov",
        "Aydar Bulatov",
        "Preslav Nakov",
        "Timothy Baldwin",
        "Artem Shelmanov",
        "Mikhail Burtsev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning is a core capability of large language models, yet understanding how they learn and perform multi-step reasoning remains an open problem. In this study, we explore how different architectures and training methods affect model multi-step reasoning capabilities within a cellular automata framework. By training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization, we demonstrate that most neural architectures learn to abstract the underlying rules. While models achieve high accuracy in next-state prediction, their performance declines sharply if multi-step reasoning is required. We confirm that increasing model depth plays a crucial role for sequential computations. We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 4 7 6 1 . 8 0 5 2 : r Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling Ivan Rodkin1,2 Daniil Orel1 Konstantin Smirnov1 Arman Bolatov1 Bilal Elbouardi1 Besher Hassan1 Yuri Kuratov3,2 Aydar Bulatov3,2 Preslav Nakov1 Timothy Baldwin1 Artem Shelmanov1 Mikhail Burtsev4 1MBZUAI, Abu Dhabi, UAE 2Neural Networks and Deep Learning Lab, MIPT, Moscow, Russia 3AIRI, Moscow, Russia 4London Institute for Mathematical Sciences, London, UK {ivan.rodkin,daniil.orel,konstantin.smirnov,arman.bolatov,bilal.elbouardi, besher.hassan,tbaldwin,preslav.nakov,artem.shelmanov}@mbzuai.ac.ae {yurii.kuratov,bulatov.as}@phystech.edu mb@lims.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Reasoning is core capability of large language models, yet understanding how they learn and perform multi-step reasoning remains an open problem. In this study, we explore how different architectures and training methods affect model multi-step reasoning capabilities within cellular automata framework. By training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization, we demonstrate that most neural architectures learn to abstract the underlying rules. While models achieve high accuracy in next-state prediction, their performance declines sharply if multi-step reasoning is required. We confirm that increasing model depth plays crucial role for sequential computations. We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities. The code is available on github."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) demonstrate impressive capabilities in problem-solving and reasoning tasks, e.g., OpenAIs o1 [47] and DeepSeek R1 [28] models achieved top-500 ranking in qualifier for the USA Math Olympiad (AIME). Also OpenAI system achieved an outstanding result, ranked 6 in International Olympiad in Informatics (IOI 2025)1. Both Google DeepMind and OpenAI systems achieve gold-medal scores in International Olympiad in Mathematics (IMO 2025)2. On the other hand, extensive evidence from ongoing research shows that LLMs still face challenges in multi-step reasoning [15, 67, 34, 19, 44, 59] and planning [65], particularly when required to infer and apply underlying rules from data. These observations raise the following questions: 1. Is the reasoning exhibited by LLMs the result of genuine generalization, or merely memorization? 1https://x.com/OpenAI/status/1954969035713687975 2https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achievesgold-medal-standard-at-the-international-mathematical-olympiad/ Preprint. 2. How does task difficulty scale as the required number of reasoning steps increases? 3. To what extent do models architectural inductive biases, training objectives, and inference procedures limit its reasoning capabilities? Transformers [66] are universal function approximators and, with unbounded depth and precision, are Turing-complete [11, 35, 13, 75, 5, 50, 56]. Yet, finite-depth, fixed-width models used in practice cannot process arbitrarily long inputs in single forward pass, and they provably fail on tasks such as graph connectivity, Boolean formula evaluation, and exact arithmetic beyond bounded length [42, 40, 61, 17]. One way to sidestep this depth barrier is to let the model write its own scratch-pad of intermediate tokens. Chain-of-Thought (CoT) prompting, process supervision, and reinforcement learning (RL) encourage models to emit multi-step rationales before producing the final answer [69, 64, 68, 73, 38]. Generating and consuming these extra tokens effectively increases the computational depth in proportion to the rationale length, enabling transformers to solve dynamic-programming benchmarks [17] and to recognize regular languages with linear decoding depth [39]. Yet, the main drawback is the need for supervision over intermediate steps, which is expensive or might be unavailable. complementary avenue is to recycle hidden states. Segment-level recurrence in memory-augmented transformers [70, 25] enables the re-feeding of hidden states across segments [12, 52, 8, 10, 54], whereas state-space models achieve long-range interactions by leveraging linear dynamical systems [27, 26]. Recurrence deepens the network without emitting extra tokens, but the maximum number of recurrent steps is still limited by the input length. Adaptive Computation Time (ACT) [24] removes this upper bound entirely: the model learns to allocate variable number of layer updates to each token, halting once further computation is predicted to be unhelpful. In principle, ACT grants transformers unbounded effective depth while preserving parameter efficiency, which is an appealing property for reasoning tasks that require widely varying amounts of computation. In this paper, we investigate rule abstraction capabilities and approaches for increasing the reasoning depth of neural models. We cast multi-step reasoning as variable-horizon prediction problem on one-dimensional Cellular Automata (1dCA). In our experiments, Boolean local update rules are never reused between training and testing, thus, success requires genuine rule generalization rather than memorization. We use this controlled benchmark to quantify how diverse neural architectures and depth extension strategies cope as the reasoning horizon increases. Our main contributions are as follows: 1dCA-Reasoning benchmark. We release variable-length dataset and four task variants (O-S, O-O, O-RS, RO-S) that disentangle rule inference from state propagation; train/test rule sets are disjoint to preclude memorisation. Comprehensive architectural study. We evaluate Transformers, LSTMs, state-space models (Mamba), and the Associative Recurrent Memory Transformer (ARMT) under identical conditions. Fixed-depth (4-layer) autoregressive models show sharp cut-off in reasoning depthsolving k=1 but collapsing for k2while segment-recurrent attention (ARMT) extends generalisation to k=2. We corroborate these findings on group multiplication benchmark [43]. Depth-extension analysis. With four-layer backbones: (i) Adaptive Computation Time (ACT) yields roughly one additional effective step with modest compute overhead; (ii) reinforcement learning with GRPO attains k=3 without intermediate supervision; and (iii) token-level Chain-of-Thought training achieves near-perfect accuracy up to k=4. Ablations and efficiency. We quantify how layer count, model width, and ACT variants trade off computation and accuracy, and we report effective depth alongside accuracy, providing practical design guidelines."
        },
        {
            "title": "2 Methods",
            "content": "2.1 Modeling Reasoning with One-dimensional Cellular Automata Motivation. Reason is the capacity of consciously applying logic by drawing valid conclusions from new or existing information.3 Reasoning about an unfamiliar process can be decomposed into two 3https://en.wikipedia.org/wiki/Reason 2 Figure 1: Learning One-dimensional Cellular Automata. (a) Update of state with local rule. (b) Orbit of 1dCA is sequence of binary strings of size = 20. The first = 10 states marked by the red rectangle encode transformer input. (c) Given part of the orbit model learns to predict the next state (O-S). subtasks: (i) inferring the hidden law that governs state transitions and (ii) chaining that law to project multiple steps into the future. One-dimensional Cellular Automata embody this structure in its purest form. single, local Boolean rule the micro-physics of toy universe maps each binary state to the next. Given an orbit, model must first discover this rule from observations and then apply it repeatedly to predict future states, mirroring the two-phase logic of multi-step reasoning. Indeed, reasoning might be simulated by memorization mapping from each seen input to its correct output, with no grasp of why that mapping holds. In our 1dCA benchmark the update rule is withheld and the train-time and test-time rule sets are disjoint, so rote lookup to train data cannot succeed. To solve task the model must induce the rule and then reason with it. 1dCA therefore offers minimal, fully observable sandbox where two essential ingredients law extraction and multi-step inference separate genuine reasoning from mere memorization. Background. An One-dimensional Cellular Automaton (1dCA) is one-dimensional, dynamical system in which space and time are discrete. Let : 1 be the neighborhood radius in the space represented by regular lattice of : 2r + 1 identical, locally-interconnected cells with binary state spaces, = {0, 1}. The 1dCAs global state, SW , is lattice configuration specified by the values of all states of all cells in the lattice at given time. This state evolves deterministically in synchronous, discrete time steps according to global map gρ : SW SW defined by local rule ρ : S2r+1 S, so [gρ(x)]w = ρ(xwr, . . . , xw, . . . , xw+r) (Fig.1a). The sequence of states an 1dCA passes through during its spacetime evolution, OT (x) = [x, gρ(x), gρ(gρ(x)), . . . , goT 1 (x)], defines its trajectory or orbit from an initial condition (configuration) for : 1. Examples of 1dCA orbits are visualized in Figure 1(b). ρ Benchmark for reasoning. 1dCA evolution model serves as the basis for our benchmark designed to evaluate multi-step reasoning capabilities of neural models. Each instance in the benchmark is trajectory, consisting of e.g. 10 states generated via some unknown unique rule. The set of rules used to synthesize the training set does not overlap with the rules used in the test set. For each instance, the task is to infer the underlying rule from those ten observed states and then predict one or more subsequent states. Crucially, models must learn general rule-inference algorithm during training, rather than memorize instance-specific rules, so they can generalize to entirely novel rules at test time. This design strips away any reliance on memorization and isolates pure reasoning ability. The benchmark allows creating tasks with progressive complexity. We can task the model to predict the next state, or the state over the next, and so on. Predicting such look-ahead states without access to intermediate steps requires not just an ability to infer rules, but also the ability to sequentially apply them and track intermediate states. To predict goT +k (x) for look-ahead steps {2, 3, 4}, the model has to compute the intermediate states, mimicking multi-step reasoning. In this work, we refer to as the depth of reasoning required to solve the task and analyze what architectures can achieve certain depth. ρ Task variants. The benchmark could emulate the situations when we have supervision on intermediate steps (i.e. the thinking process of the LLM) and when we only have final look-ahead state. We 3 consider four variations of learning tasks designed to assess different aspects of predictive modeling and rule inference: Orbit-State (O-S): given an orbit OT (x) = [x(1), x(2), . . . , x(T )] where x(1) SW , the objective is to predict the state x(T +k) at look-ahead : 1. For = 1 (see Fig.1c) this is single-step prediction which may be seen as an elementary act of reasoning or as part of curriculum to learn longer chains of reasoning. For > 1 multiple intermediate inference steps are required to get the answer. Orbit-Orbit (O-O): given an orbit OT (x) for some > 1 predict the subsequent states up to time + k, generating OT +k +1 (x) = [x(T +1), . . . , x(T +k)]. This task simulates step-by-step multi-step reasoning as learning objective. Orbit-State and Rule (O-RS): given an orbit OT (x) predict the state x(T +k) and the local rule ρ. By explicitly optimizing rule prediction, the model receives direct supervision. Rule and Orbit-State (RO-S): given an orbit OT (x) and the local rule ρ predict the state x(T +k) at time + k. Since the rule is explicitly provided, the model can bypass inference of rule structure and focus solely on learning to apply the update. You can find the examples of training/validation samples in the Appendix D. The rule in our 1dCA setup is based on neighborhood radius = 2, meaning each bit of the next state depends on 5-bit window (2 left + current cell + 2 right) from the current state. Since there are 25 possible 5-bit strings, the rule mapping can be represented by 32-bit string. Each bit in this string corresponds to the output of the rule for specific input. The position of this output bit within the rule string is determined by the binary value of the 5-bit input. For example, if the 5-bit input 00000 maps to 1, then the 0-th bit of the 32-bit rule string would be 1 (see Fig.1a). This encoding ensures that the model can interpret the rule as direct lookup table. For our evaluation we use the exact match metric for state prediction (1 if the state is predicted correctly, 0 if at least one bit is predicted wrong). For rule prediction, however, we use bit accuracy (ratio of the correctly predicted bits in the rule). The reason is that in some significant part of the samples, it is not possible to exactly predict the whole rule, including all 32 = 25 transitions, as some of these transitions did not appear in the orbit, which makes the exact match metric biased towards 0 and less representative. Neural Models. In our study, we consider several widely-applied models from distinct architectural families. Long Short-Term Memory (LSTM) networks [33], class of recurrent neural network (RNN), have proven effective in capturing sequential dependencies in NLP tasks. However, their inherent sequential processing limits efficiency and scalability. Transformers [66] address these limitations by processing entire input sequences simultaneously through self-attention, enabling parallel computation and better handling of long-range dependencies compared to RNN-based models. State space models (SSMs) [27] offer an alternative approach to sequence modeling by leveraging structured state representations and computationally efficient recurrence mechanisms. We use one of the most powerful SSM architectures to date Mamba [26, 27]. Additionally, we consider the Associative Recurrent Memory Transformer (ARMT) [54], an extension of the transformer designed to enhance memory capabilities. ARMT builds on the Recurrent Memory Transformer [9] by incorporating quasi-linear attention mechanisms that improve information transfer across input blocks, mitigating limitations in long-context processing. We discuss the properties of these models in Appendix B. 2.2 Methods for Enhancing Reasoning Capabilities We explore several approaches for enhancing reasoning in neural networks, such as Chain-of-Thought, RL-methods (GRPO), and Adaptive Computations Time. Chain-of-Thought (CoT) prompting [69] is powerful technique for enhancing the reasoning capabilities of LLMs. Unlike standard prompting techniques, which attempt to directly infer an answer from the input, CoT enforces structured reasoning where the LLM is required to write the solution step by step. 4 CoT forces the model to explicitly generate intermediate reasoning steps while solving problem, allowing it to reference these tokens as form of recurrent state. This mechanism effectively increases the formal computational power of the model [39] and extends its effective depth. These improvements enable LLMs to perform multi-step reasoning, particularly in tasks such as mathematical problem-solving, logical inference, and commonsense reasoning [69]. There are many variations and extensions of the traditional chain of thought, such as tree of thoughts [73] and graph of thoughts [4]. Learning to reason with RL. Another common practice involves training LLMs with reinforcement learning methods such as proximal policy optimization (PPO) [57] and group relative policy optimization (GRPO) [58] after supervised finetuning in order to improve the generation of reasoning traces. RL post-training has been shown to improve instruction following [48] as well as mathematical [68] and general reasoning performance in LLMs [31, 38, 28]. Compared to supervised methods, training to reason with GRPO requires no supervision on intermediate reasoning steps. It only relies on rewards from correct final answers and maintaining the desired format. Adaptive Computation Time (ACT) [24] is the mechanism proposed to allow recurrent and selfattentive models to perform variable number of computation steps within each time-step dynamically. The core idea is to enable different parts of the sequence to have different computational complexities, which is particularly useful for tasks with non-uniform requirements for computation. In this class of models halting unit dynamically decides how much thinking time should take place at each step, thus adaptively scaling the effective reasoning depth of the model. For mathematical formulation please check the Appendix C. Originally, ACT was applied to single-layer NNs [13, 24]. When it comes to deep models, we can apply ACT to each layer of the model, averaging the remainders over the layers to add as the time penalty to the loss (layer-wise ACT or LACT). Another option is to apply ACT to the whole backbone model (MACT), which maps the RN RN (therefore without embedding and unembedding layer). In our ablation studies, we compare layer-wise ACT and model ACT but find that they perform similarly. See F.2 for more details. Therefore, in the main experiments, we use only layer-wise ACT and always refer to this version. To determine whether performance gains stem from the adaptive nature of computation time or merely from increased computation, we include fixed computation time (FCT) baseline in our ablation study (F.1). Specifically, we examine the case of three fixed iterations, chosen to match the upper bound of the average number of ACT operations observed in our experiments. Recurrent Memory Transformers. As trade-off between expressive recurrent models and efficiently trainable transformers, the Recurrent Memory Transformer was proposed [9]. It leverages recurrent steps between the fixed-sized segments, while the tokens inside these segments are processed in parallel with the transformer model, which RMT augments. In the original RMT [9], the recurrent steps are performed by passing the output of special memory tokens from one segment to the input of the next segment. In the enhanced version of RMT: Associative Recurrent Memory Transformer [54], the recurrent steps are performed with quasi-linear attention in each transformer layer. In this work, we use the ARMT as representative of recurrent memory transformers."
        },
        {
            "title": "3 Experiments",
            "content": "We generated an 1dCA dataset with the CellPyLib [2] for the fixed lattice size = 20 and neighborhood radius = 2. This configuration results in total of 222r+1 4.3 109 possible Boolean functions defining local rules. For each sample in the dataset, both the initial state and the local rule ρ were generated randomly. We then computed the orbit for = 20 time steps using these parameters. The training dataset consists of 9.5 105 instances and the test of 105 instances. Importantly, the local rules included in the test set are exclusive and not present in the training set. This separation ensures that the models performance reflects its ability to generalize to unseen rules, rather than simply memorizing the training data. The input vocabulary of the tested models consists of the following tokens: [0], [1], [SEP], and [M]. The states and the local rule ρ are encoded as binary strings. The model receives the orbit as sequence of bits, representing consecutive states separated by the [SEP] tokens. To begin our analysis, we evaluate the fundamental capabilities of different model architectures on our 1dCA-based reasoning tasks. As shown in Figure 2(a), most models can predict one step forward 5 (a) State prediction (b) Rule prediction (c) GPTNeox on O-S, O-O, O-RS, RO-S tasks Figure 2: Single-step accuracy is near-perfect across models, but multi-step performance collapses. (a) Exact-match accuracy for single-step state prediction (O-S): all models except LSTM achieve >95 %. (b) Bit-wise accuracy for rule inference (O-RS): most architectures recover the hidden Boolean rule, yet ARMT trails the rest. (c) GPTNeox accuracy on variable-horizon prediction across the four task variants (O-S, O-O, O-RS, RO-S): accuracy falls steeply with look-ahead k. with nearly perfect accuracy. LSTM performs slightly worse than other architectures, likely due to challenges in effectively encoding the binary state representation. Successful learning demonstrate that the Transformer model is capable of generalizing not only over initial conditions for particular function commonly the focus in studies of transformer trainability in CA domain [71, 20, 1, 45, 49, 77, 18, 63, 23, 53, 36, 60, 6, 16, 3] but also across different Boolean functions of fixed arity (5 in our case). When tasked with predicting both future states and the underlying rules (O-SR setting), Figure 2(b) shows that models generally achieve high accuracy on rule prediction, though with interesting variations. ARMT notably struggles with accurate rule inference compared to other architectures, despite handling next-state prediction well. 3.1 Limitations in the Reasoning Depth of Transformers We selected 4-layer architecture with dmodel=128 as baseline configuration for our experiments. Using this configuration, we separately trained from scratch for each look-ahead step {2, 3, 4} of the O-S task the GPTNeox [7] model to predict the state at time x(T +k) given an orbit OT (x) = [x(1), x(2), . . . , x(T )]. As presented in Figure 2(c), this task proved to be challenging. While the average accuracy for next-state prediction (O-S task with = 1) was 0.95, it dropped to 0.40 for = 2 and fell below 0.25 for = 3 and = 4. Despite having four layers, which in principle could capture up to two or three sequential transformations if effectively utilized, the model still struggles to learn look-ahead tasks for 2. Specifically, the same models depth that suffices for the single-step O-S task is no longer adequate for maintaining accurate multi-step predictions, suggesting that the capacity is being taxed by the need to encode and apply repeated rule updates in fixed number of transformations. To determine whether this decline was due to the GPTNeoxs architecture or the training objective, we explored whether accuracy could be improved by training the model to predict intermediate steps. This approach is analogous to multi-token prediction [21]. We employed the Orbit-Orbit (O-O) task, training the model to predict the next four states in parallel. The results, also shown in Figure 2(c), indicate that the models predictive abilities degrade in this training scenario, as even prediction of the next state (k = 1) is less than 0.80 accuracy. However, the higher standard deviation suggests that it happens because of the instability of such training: some runs could simply fail, while others could work well (as shown by the exact match of = 2, 3, 4 being relatively close to the O-S scenario). These results suggest that learning to store hidden representation of intermediate states (as in the O-S, O-RS and RO-S with > 1) is hard for the model. Surprisingly, direct supervision for hidden representation of the underlying rule (O-RS) is more challenging initially and does not facilitate better generalization to longer planning horizons. This implies that explicitly encouraging the model to infer the generating rule cannot enhance its ability to make longer-term predictions by reinforcing the internalization of the systems dynamics. 6 Figure 3: Depth not width drives multi-step accuracy. Exact-match accuracy for look-ahead horizons {1, 2, 3, 4} as function of (a) transformer layer count and (b) embedding dimension dmodel. Deeper networks boost performance sharply for k2 and plateau beyond six layers, whereas widening the model yields only marginal gains across all horizons. Finally, we explored the scenario where the local rule ρ is explicitly provided to the model, corresponding to the Rule and Orbit-State (RO-S) task. Intuitively, this should be the easiest task for the model, as it eliminates the need to infer the rule from the orbit. As shown in Figure 2(c), GPTNeox indeed learns to apply the given rule for next-state prediction with near-perfect accuracy for = 1. Surprisingly, however, the performance for look-ahead steps = 2, 3 and 4 drops to the level of original O-S predictions. The poor performance on look-ahead steps > 1 raises the question of whether this limitation stems from the neural networks parameter count, layer width, or width of its embeddings. To answer this question, we performed the experiments, while varying the number of transformer layers and the embedding dimension dmodel. Figure 3 (a) shows that accuracy for oneand two-step prediction saturates after 46 layers. Three-step prediction, however, continues to improve up to about 12 layers, whereas four-step prediction remains poor regardless of depthevidence that vanilla transformers struggle with longer-range dependencies even when made substantially deeper. Figure 3 (b) examines width. Increasing dmodel provides only marginal gains across all horizons, with the most noticeable bump occurring between 64 and 128 dimensions; further widening yields diminishing returns. These results illustrate the importance of increasing the models depth rather than the width of its embeddings for better multi-step reasoning performance. 3.2 Extending the Depth of Reasoning with Recurrence and Adaptive Computation Time The previous subsection confirmed that simply adding layers offers clear performance boost, yet even 12-layer transformer still falters for 4 (Fig. 3a). Here, we set the depth to 4 layers and study if its possible to improve performance by techniques that expand models effective depth at inference timesegment-level recurrence and Adaptive Computation Time (ACT). Hyperparameters for all models can be found in Table 1. Both approaches inject extra computational steps without further increasing the static layer count, potentially enabling deeper reasoning while preserving parameter efficiency. Figure 4 shows that the auto-regressive models GPTNeox, LSTM, and Mamba 4 handle next-state prediction but fail to solve the multi-step task. Only ARMT manages to extend its capacity up to two look-ahead steps, likely because it processes sequences segment by segment and is thus forced to separate rule and state representations. This separation may enable the generation of hidden representation for the intermediate state, followed by the application of the rule, effectively enhancing the depth of the model reasoning. 4We use the architecture from the previous section: 4 layer GPTNeox with dmodel = 128 and 4 attention heads. For Mamba, we use state size of 16. For ARMT, dmem = 32. As ARMT is segment-level model, we segment our state sequence in the way that each segment contains pair of consecutive states in the orbit, and 7 Figure 4: ACT significantly improves computational abilities of transformer-based models in multi-step prediction. Exact match of the x(T +k) state prediction for look-ahead steps {1, 2, 3, 4} for O-S training objective. Different models are shown in different colors. Augmenting models with ACT5 has little effect on all architectures except GPTNeox, which sees improved performance at = 2 but not at = 3, 4. Overall, ARMT makes effective use of the transformers four-layer depth but cannot extend beyond it. Likewise, while ACT helps the transformer make use of its existing layers more efficiently, it fails to enable any architecture to solve threeor four-step predictions. Moreover, LSTM and Mamba are unable to master multi-step tasks with or without ACT, likely due to representation bottlenecks in their hidden states. We subsequently chose to train GPTNeox model that is already capable of performing one-step reasoning with the SFT, LACT, MACT, and GRPO methods, with the goal of enabling it to reason over multiple steps without access to supervision for the intermediate reasoning stages. As illustrated in Figure 5, standard supervised fine-tuning (SFT) fails to address the problem effectively. Although the model is primarily trained on one-step prediction task, it struggles to apply the rule iteratively. Consistent with previous results  (Fig. 4)  , applying ACT both at the layer level the prediction is performed in the last segment with the last CA state from the input in it. We report average results of 3 models trained with different seeds. 5In our study, the sequence length (20 tokens) represents the length of the cellular automata state, and the operations corresponding to local rule are performed in parallel across this state, not sequentially. Therefore, models do not require 20 sequential self-iterations to process such state. Instead, the number of iterations required is primarily proportional to the look-ahead value, which in our experiments varies from 1 to 4. Therefore, we have chosen to limit the number of ACT self-iterations to 4. Figure 5: Without supervision on intermediate reasoning steps RL training with GRPO allows the model to extrapolate reasoning on 3 steps forward. While model and layer ACT variants extend it to 2 steps forward. However, 4 steps remains challenging task for all approaches. All models are trained from GPTNeox checkpoint trained on 1 step forward prediction task. 8 Figure 6: With step-by-step supervision, the CoT approach significantly outperforms the indepth approach of ACT. While the performance of the depth-based methods decreases with the increasing look-ahead steps, breadth-based CoT succeeds in predicting up to all 4 steps. However, among the models without autoregressive generation, GPTNeox and ARMT with both ACT and O-O supervision perform the best. (LACT) and across the entire model (MACT) improves performance on the two-step prediction task but does not generalize beyond that. Interestingly, when trained using RL (GRPO) and granted the capability to autoregressively generate intermediate thinking tokens before producing the final output, the model succeeds on the three-step prediction task. The reward signal is defined as the average token-level accuracy of the models prediction following the end-of-thinking token. 3.3 Reasoning Supervision We examine the impact of reasoning supervision on GPTNeox and ARMT, along with their corresponding ACT-augmented variants. To this end, we replicate the O-O training setup by incorporating mask tokens into the autoregressive models within causal masking framework. Figure 6 shows that contrary to our expectations, the O-O training objective alone does not yield performance improvements for either GPTNeox or ARMT. However, the integration of O-O training with ACT results in superior performance, surpassing both the baseline and ACT-only variants. As final step, we combined GPTNeox and ARMT with token-by-token CoT-like next-token prediction training. Under this regime, both models succeed at multi-step prediction up to = 4, with GPTNeox slightly outperforming ARMT across each look-ahead distance  (Fig.6)  . These results suggest that, when explicit reasoning supervision is available, chain-of-thought-inspired approach to training offers particularly effective strategy for enabling multi-step reasoning. 3.4 Group multiplication In addition to the cellular automata experiments, we show the significance of our findings on group multiplication benchmark [43]. The task is, given the sequence of elements of some group label each element with the product of all previous elements of the sequence including the current one. This task is relevant to reasoning because it provides controlled setup with the tasks of different computational complexity. We evaluated our models in 3 groups of different difficulty: Z60, A4 Z5, and A5; and different sequence lengths: 5, 10, 15, 20, and 40. For each model, we report the minimal number of layers to achieve 70% exact match accuracy. For the sake of consistency with previous works, we slightly changed the hyperparameters of our models. We use dmodel = 512 and nheads = 8. For the ARMT model, we use the segments of size 2. As shown on the Figure 7, the required depth for solving longer tasks grows for GPTNeox and Mamba models, while staying constant (1-2 layers) for the models with recurrence (ARMT and LSTM). Moreover, depth requirements can be significantly reduced with adding Adaptive Computation Time (ACT) or Associative Memory (ARMT), which is consistent with our findings on 1dCA benchmark. LSTM, however, performs much better, being able to solve the problem with just one layer. 9 Figure 7: ACT significantly reduces the required models depth for the majority of group multiplication tasks. Each chart contains the information about the minimal required number of layers for solving task of given length with 70% exact match accuracy. GPTNeox and Mamba being 0-limited models require more layers for solving deeper (longer in this case) tasks, while ARMT and LSTM solve them with constant number of layers."
        },
        {
            "title": "4 Discussion and Conclusions",
            "content": "Our study exposes how architecture, training signal, and depth-extension strategy jointly determine models ability to learn to reason over multiple 1dCA time stepswithout relying on memorization, because the underlying rule is never reused between training and evaluation. The headline results are visualised in Figure 8. Both Transformer and recurrent models (GPT-NeoX, LSTM, Mamba, ARMT) demonstrate the capability to infer the underlying 1dCA rule from in-context examples of states evolution: because evaluation uses unseen rules, any observed success reflects rule inference rather than memorization. Fixed-depth (4-layer) models show sharp cutoff in reasoning depth. With only four layers, GPT-NeoX, LSTM, and Mamba solve the one-step Orbit-to-State task (k=1) but accuracy collapses for k2. Segment-recurrent attention helps but remains bounded. ARMT, which augments self-attention with associative memory, generalizes to k=2 but no further, suggesting that fixed-budget recurrence alone cannot overcome the depth barrier. Adaptive halting offers compute-efficient +1 step. Adding Adaptive Computation Time (ACT) to the Transformer yields consistent gains (roughly +1 depth of state prediction) without increasing parameter count, although benefits taper off beyond k=3. GRPO closes the gap to three steps without intermediate supervision. By learning to think before speaking, GRPO-trained models reach k=3 accuracy comparable to CoT at k=2, demonstrating that policy-gradient fine-tuning can unlock deeper implicit reasoning. Token-level Chain-of-Thought achieves near-perfect four-step prediction. When stepwise targets are available, CoT turns the problem into autoregressive next-state generation; GPT-NeoX attains > 99% accuracy up to k=4  (Fig.6)  , saturating our current benchmark. Adaptive Computation allows 0-limited models to solve deeper group multiplication tasks. Such models like Transformer and Mamba require more layers to solve deeper state-tracking tasks. However, adding ACT partially mitigates this issue, while not solving the problem entirely. Rule inference. The 1dCA benchmark disentangles two subtasks that often co-occur in real-world situation: (i) rule inference (abstraction) and (ii) state propagation. Because train and test rules are disjoint, even k=1 accuracy entails episode-specific rule inference. Importantly, this can be partial inference: from the orbit the model can extract only those neighborhood-to-bit mappings needed for the current state, which suffices for one-step prediction. In contrast, k>1 introduces new neighborhoods that were not observed in the context, so success requires more compact, compositional representation of the underlying Boolean rule that can be applied repeatedly. Because ARMT cannot attend to the full orbit, its > 1 success is unlikely to be explained by orbit lookup alone. 10 Figure 8: With GRPO as well as with ACT and Orbit-Orbit training depth of reasoning can be significantly extended. Average DepthScore = 1 + (cid:80)4 i=2 acc(i), where acc(i) is the accuracy of predicting the (10 + i)th state based on the first 10 states. Broader implications for LLM reasoningand beyond. Our results align with growing body of evidence that reasoning failures often stem from insufficient depth allocation and sparse optimisation signals. For LLMs, this suggests that (i) prompt engineering alone is unlikely to improve multistep reasoning: unless intermediate steps are reinforcedvia CoT, search-augmented decoding, or RL-style self-critiquemodels tend to default to shallow heuristics; (ii) adaptive-depth mechanisms are promising scaling direction: ACT-style halting, deployed token-wise or layer-wise, can allocate computation on demand to match the variable complexity of real queries; and (iii) explicit intermediate representations remain the most reliable route to multi-step generalisation via CoT. Beyond language, the same principles apply to neural algorithmic reasoning, robotic planning, and scientific simulation: whenever the target task contains latent iterative structure, giving the network roomvia dynamic recurrence, learned halting, or supervised scratch-padsto run the hidden algorithm is more data-efficient than brute-force depth. We therefore advocate future benchmarks that (a) separate rule induction from state propagation, (b) report effective depth alongside accuracy, and (c) evaluate adaptive-computation policies explicitly. Progress along these axes will benefit not only next-generation LLMs but also neural systems tasked with symbolic manipulation, formal verification, and open-ended planning. Conclusions. We introduced the 1dCA-Reasoning benchmark and used it to probe how architecture, training signal, and depth-extension strategy shape multi-step reasoning without memorisation (train/test rules are disjoint). Across settings, success therefore reflects rule inference rather than lookup. Empirically, fixed-depth (4-layer) modelsTransformers, LSTMs, and state-space modelsexhibit sharp cut-off in reasoning depth: they solve k=1 but collapse for k2. Segment-recurrent attention (ARMT) extends this to k=2 yet remains bounded. Adding Adaptive Computation Time (ACT) yields compute-efficient +1 effective step without increasing parameters, with gains tapering beyond k3. Reinforcement learning via GRPO unlocks reliable k=3 performance without intermediate labels, while token-level Chain-of-Thought achieves near-perfect prediction to k=4. These findings support our four contributions: (1) new benchmark that cleanly separates rule induction from state propagation; (2) systematic architectural comparison; (3) an analysis of depthextension mechanisms (recurrence, halting, and RL); and (4) actionable ablations/design guidance. More broadly, they reinforce that how we train can matter as much as what we train: objectives that force multi-step prediction and mechanisms that allocate depth adaptively are decisive, while explicit intermediate representations remain the most reliable route to deeper generalisation. We encourage 11 the community to use this benchmark and to report effective depth alongside accuracy, evaluating adaptive-compute policies explicitly as models scale."
        },
        {
            "title": "Limitations",
            "content": "While our findings offer valuable insights into methods for enhancing reasoning, we acknowledge that the study is limited to small-scale models, and certain conclusions may not generalize directly to large language models."
        },
        {
            "title": "References",
            "content": "[1] M. Aach, J. H. Goebbert, and J. Jitsev. Generalization over different cellular automata rules learned by deep feed-forward neural network, 2021. [2] L. M. Antunes. Cellpylib: python library for working with cellular automata. Journal of Open Source Software, 6(67):3608, 2021. doi: 10.21105/joss.03608. URL https://doi.org/ 10.21105/joss.03608. [3] J. A. Berkovich and M. J. Buehler. Lifegpt: Topology-agnostic generative pretrained transformer model for cellular automata. arXiv preprint arXiv:2409.12182, 2024. [4] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1768217690, 2024. [5] S. Bhattamishra, A. Patel, and N. Goyal. On the computational power of transformers and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020. [6] A. Bibin and A. Dereventsov. Data-centric approach to constrained machine learning: case study on conways game of life. arXiv preprint arXiv:2408.12778, 2024. [7] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL https://arxiv.org/abs/2204.06745. [8] A. Bulatov, Y. Kuratov, and M. S. Burtsev. Recurrent memory transformer. arXiv preprint arXiv:2207.06881, 2022. [9] A. Bulatov, Y. Kuratov, and M. S. Burtsev. Recurrent memory transformer, 2022. [10] A. Chevalier, A. Wettig, A. Ajith, and D. Chen. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788, 2023. [11] G. Cybenko. Approximations by superpositions of sigmoidal function. Mathematics of Control, Signals and Systems, 2:183192, 1989. [12] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29782988, 2019. [13] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=HyzdRiR9Y7. [14] G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter, S. Legg, J. Veness, et al. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. [15] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bhagavatula, R. Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. 12 [16] V. Elser. Reconstructing cellular automata rules from observations at nonconsecutive times. Physical Review E, 104(3):034301, 2021. [17] G. Feng, B. Zhang, Y. Gu, H. Ye, D. He, and L. Wang. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36, 2024. [18] G. Gala, D. Grattarola, and E. Quaeghebeur. (n)-equivariant graph neural cellular automata. arXiv preprint arXiv:2301.10497, 2023. [19] J. P. Gandarela, D. S. Carvalho, and A. Freitas. Inductive learning of logical theories with llms: complexity-graded analysis. arXiv preprint arXiv:2408.16779, 2024. [20] W. Gilpin. Cellular automata as convolutional neural networks. Physical Review E, 100(3): 032402, 2019. [21] F. Gloeckle, B. Y. Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. [22] S. Goyal, Z. Ji, A. S. Rawat, A. K. Menon, S. Kumar, and V. Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. [23] D. Grattarola, L. Livi, and C. Alippi. Learning graph cellular automata. Advances in Neural Information Processing Systems, 34:2098320994, 2021. [24] A. Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. [25] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. [26] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [27] A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. [28] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [29] S. Hao, S. Sukhbaatar, D. Su, X. Li, Z. Hu, J. Weston, and Y. Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [30] S. Hao, S. Sukhbaatar, D. Su, X. Li, Z. Hu, J. Weston, and Y. Tian. Training large language models to reason in continuous latent space, 2024. URL https://arxiv.org/abs/2412. 06769. [31] A. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R. Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. [32] D. Herel and T. Mikolov. arXiv:2405.08644, 2024. Thinking tokens for language modeling. arXiv preprint [33] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8): 17351780, 1997. [34] W. H. Holliday and M. Mandelkern. Conditional and modal reasoning in large language models. arXiv preprint arXiv:2401.17169, 2024. [35] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359366, 1989. [36] B. Kang, H. Kumar, M. Lee, B. Chakraborty, and S. Mukhopadhyay. Learning locally interacting discrete dynamical systems: Towards data-efficient and scalable prediction. In A. Abate, M. Cannon, K. Margellos, and A. Papachristodoulou, editors, Proceedings of the 6th Annual Learning for Dynamics and Control Conference, volume 242 of Proceedings of Machine Learning Research, pages 13571369. PMLR, 1517 Jul 2024. URL https://proceedings. mlr.press/v242/kang24a.html. [37] H. Karloff, S. Suri, and S. Vassilvitskii. model of computation for mapreduce. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 938948. SIAM, 2010. [38] A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. [39] W. Merrill and A. Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023. [40] W. Merrill and A. Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531545, 2023. [41] W. Merrill and A. Sabharwal. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, 2024. [42] W. Merrill, A. Sabharwal, and N. A. Smith. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843856, 2022. [43] W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In Proceedings of the 41st International Conference on Machine Learning, pages 3549235506, 2024. [44] P. Mondorf and B. Plank. Liar, liar, logical mire: benchmark for suppositional reasoning in large language models. arXiv preprint arXiv:2406.12546, 2024. [45] A. Mordvintsev, E. Randazzo, E. Niklasson, and M. Levin. Growing neural cellular automata. Distill, 5(2):e23, 2020. [46] F. Nowak, A. Svete, A. Butoi, and R. Cotterell. On the representational capacity of neural language models with chain-of-thought reasoning. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1251012548, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.676. [47] OpenAI. Learning to reason with llms. https://openai.com/index/ learning-to-reason-with-llms/, 2024. Accessed: 2024-09-23. [48] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [49] R. Pande and D. Grattarola. Hierarchical neural cellular automata. In Artificial Life Conference Proceedings 35, volume 2023, page 20. MIT Press One Rogers Street, Cambridge, MA 021421209, USA journals-info . . . , 2023. [50] J. Pérez, P. Barceló, and J. Marinkovic. Attention is turing-complete. Journal of Machine Learning Research, 22(75):135, 2021. [51] J. Pfau, W. Merrill, and S. R. Bowman. Lets think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=NikbrdtYvG. [52] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv. org/abs/1911.05507. 14 [53] A. D. Richardson, T. Antal, R. A. Blythe, and L. J. Schumacher. Learning spatio-temporal patterns with neural cellular automata. PLOS Computational Biology, 20(4):e1011589, 2024. [54] I. Rodkin, Y. Kuratov, A. Bulatov, and M. Burtsev. Associative recurrent memory transformer, 2024. URL https://arxiv.org/abs/2407.04841. [55] C. Sanford, D. Hsu, and M. Telgarsky. Transformers, parallel computation, and logarithmic depth. In Forty-first International Conference on Machine Learning, 2024. [56] C. Sanford, D. J. Hsu, and M. Telgarsky. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36, 2024. [57] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [58] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [59] P. Shojaee, I. Mirzadeh, K. Alizadeh, M. Horton, S. Bengio, and M. Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity, 2025. URL https://arxiv.org/abs/2506.06941. [60] J. M. Springer and G. T. Kenyon. Its hard for neural networks to learn the game of life. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 18. IEEE, 2021. [61] L. Strobl, W. Merrill, G. Weiss, D. Chiang, and D. Angluin. Transformers as recognizers of formal languages: survey on expressivity. arXiv preprint arXiv:2311.00208, 2023. [62] L. Strobl, W. Merrill, G. Weiss, D. Chiang, and D. Angluin. What formal languages can transformers express? survey. Transactions of the Association for Computational Linguistics, 12:543561, 2024. [63] M. Tesfaldet, D. Nowrouzezahrai, and C. Pal. Attention-based neural cellular automata. Advances in Neural Information Processing Systems, 35:81748186, 2022. [64] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [65] K. Valmeekam, K. Stechly, and S. Kambhampati. Llms still cant plan; can lrms? preliminary evaluation of openais o1 on planbench. arXiv preprint arXiv:2409.13373, 2024. [66] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, In Advances in neural information and I. Polosukhin. Attention is All you Need. processing systems, pages 59986008, 2017. URL http://papers.nips.cc/paper/ 7181-attention-is-all-you-need. [67] Y. Wan, W. Wang, Y. Yang, Y. Yuan, J.-t. Huang, P. He, W. Jiao, and M. Lyu. LogicAsker: Evaluating and improving the logical reasoning ability of large language models. In Y. AlOnaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21242155, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.128. URL https://aclanthology.org/2024.emnlp-main.128/. [68] P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510/. [69] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 15 [70] J. Weston, S. Chopra, and A. Bordes. Memory networks. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916. [71] N. Wulff and J. A. Hertz. Learning cellular automaton dynamics with neural networks. Advances in Neural Information Processing Systems, 5, 1992. [72] L. Yang, K. Lee, R. Nowak, and D. Papailiopoulos. Looped transformers are better at learning learning algorithms, 2024. URL https://arxiv.org/abs/2311.12424. [73] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. [74] Q. Yu, Z. He, S. Li, X. Zhou, J. Zhang, J. Xu, and D. He. Enhancing auto-regressive chain-ofthought through loop-aligned reasoning, 2025. URL https://arxiv.org/abs/2502.08482. [75] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019. [76] X. Zhang, M. Abdul-Mageed, and L. V. Lakshmanan. Autoregressive+ chain of thought= recurrent: Recurrences role in language models computability and revisit of recurrent transformer. arXiv preprint arXiv:2409.09239, 2024. [77] Y. Zhang and H. Bölcskei. Cellular automata, many-valued logic, and deep neural networks. arXiv preprint arXiv:2404.05259, 2024."
        },
        {
            "title": "A Related Work",
            "content": "Computational Expressivity. Sanford et al. [56] show that in setups where the input context length grows but the model depth remains constant, transformers achieve logarithmic complexity scaling in input size for sparse averaging tasks and linear scaling for triple detection. They further use the simulation of transformers in constant number of MPC [37] communication rounds to demonstrate their expressive power, showing that logarithmic-depth transformers can efficiently solve tasks that are intractable for graph neural networks and recurrent models [55]. Merrill and Sabharwal [40] prove that transformers with logarithmic precision can be simulated by constant-depth logspace-uniform threshold circuits, implying fundamental computational limitations. Zhang et al. [76] employ circuit complexity theory to show that bounded-depth transformers cannot directly solve certain arithmetic or equation tasks, unless the model size increases exponentially. Formal Language Recognition. The Chomsky hierarchy has been used to classify the computational capabilities of transformers and their expressivity limits. Deletang et al. [14] show that transformers struggle with non-regular languages. Strobl et al. [62] provide comprehensive survey on how transformers relate to formal language classes, identifying the architectural constraints that limit their ability to process hierarchical structures. They show that while transformers with softmax attention can count, they remain within TC0 and struggle with evaluating Boolean formulas or solving complex hierarchical tasks. Zhang et al. [76] discuss transformers limitations due to their lack of recurrence, arguing that they are computationally weaker than recurrent models in formal language tasks. Several studies explore how CoT enhances transformer reasoning capabilities. Feng et al. [17] show that transformers can solve arithmetic and dynamic programming tasks via CoT, which they fail to do directly. Merrill and Sabharwal [41] demonstrate that CoT increases computational power, enabling the recognition of regular languages. Nowak et al. [46] formalize CoT reasoning probabilistically, showing equivalence to probabilistic Turing machines. Zhang et al. [76] argue that CoT can approximate recurrent computation, mitigating transformers lack of explicit recurrence. There are generalizations of CoT that relax the human-like word-by-word out-loud reasoning. The reasoning process has been moved to special pause [22], think [32], or filler [51] tokens to allow the model to think internally before generating response. Coconut (Chain of Continuous Thought) [29] further extends this by replacing explicit word decoding with the models last hidden state as input to the next step, effectively shifting reasoning into the latent space. Moreover, since real-world datasets rarely include supervision for long, multi-step reasoning, approaches that incorporate verifiers or intermediate feedback have become increasingly important [51]. At the same time, reinforcement learning methods [57], such as GRPO [58], which rely solely on rewards for correct final answers, show great promise. Overall, these studies highlight the limitations of transformers in reasoning depth and computational power, showing that CoT-like approaches and recurrence can help mitigate these constraints. Our work explores the use of One-dimensional Cellular Automata (1dCA) as framework to evaluate models reasoning abilities. 1dCA provides flexible and controlled setting where the number of sequential steps required to solve task can be precisely defined. Adjusting the complexity of state transition rules allows for varying task difficulty. Looped Transformers Another paper [72] investigates whether looped transformers [72] can emulate iterative learning algorithms, such as gradient descent, for data-fitting problems like linear regression. Their core finding is that looped transformers can achieve comparable performance to standard transformers with significantly fewer parameters by effectively replicating these iterative optimization steps. Our paper investigates how different architectures and training methods affect models ability to learn and perform multi-step reasoning and rule abstraction. The \"iterations\" in our study are interpreted as steps for applying discovered rule or propagating state, which is distinct from emulating optimization algorithms. RELAY [74] is framework that aligns CoT steps with loop iterations and uses intermediate supervision during looped transformer training to generate high-quality reasoning chains for auto-regressive models. Their aim is to leverage the length generalization of looped transformers to improve autoregressive models handling of longer reasoning chains. In our paper, we study CoT as training objective that provides direct reasoning supervision on intermediate states for multi-step state prediction on 1dCA. While both studies involve recurrence and CoT-like supervision, Yu et al.s work focuses on specific methodology for generating CoT for other models by aligning CoT steps with 17 loops, whereas our work directly evaluates how training with or without intermediate supervision, as in O-O or GRPO, respectively, influences models core reasoning capabilities in disentangled environment. In the \"Illusion of Thinking\" research [59] authors show that the models performance decreases with the increased complexity of puzzle environments. For thinking models, however, this degradation is less dramatic. Which is consistent with our findings on Figure 4."
        },
        {
            "title": "B Models Discussion",
            "content": "LSTM By integrating gating mechanism into recurrent neural networks, LSTMs alleviated the vanishing gradient problem, allowing the model to retain information from up to 1015 prior time steps. However, LSTMs still face several limitations. First, despite the gating mechanism, they often struggle with very long-range dependencies, as information can decay over extended sequences. Second, their sequential nature hinders parallelization, which slows training and increases computational cost compared to more modern architectures such as transformers. As result, while LSTMs represented major breakthrough in sequence modeling and in theory can process contexts of infinite length, they have been largely superseded by more scalable and efficient models. Transformers Attention mechanism allows transformers to dynamically focus on relevant parts of the input, facilitating effective information integration across long distances. As result, they maintain and reuse context more effectively than LSTMs, making them powerful backbone for modern large language models. This design has enabled state-of-the-art performance on complex reasoning tasks, cementing the transformers role at the forefront of natural language processing. While this flexibility is powerful, it also introduces drawbacks. Transformers must compute and store large attention matrix, often scaling to O(n2) in both memory and computation. This creates challenges when handling very long inputs or generating lengthy outputs, as hardware and software limitations cap the practical context window. Another limitation of transformers is their difficulty in processing information in-depth. Each generation step requires fixed amount of computation, constrained by the number of transformer layers. Consequently, transformers face challenges with multi-hop reasoning. To enable more efficient in-depth reasoning, various test-time compute strategies have been introduced, including chain-of-thought prompting, Monte Carlo Tree Search, and others. While these techniques partially mitigate the issue, they remain bottlenecks: longer generations demand substantial computational resources and may exceed the effective context window. These techniques also require supervision for intermediate steps to train the model. This is huge limitation as strong AGI systems should automatically learn to recursively apply rules to data. State Space Models While less prevalent compared to RNNs and transformers, SSMs are widely used in control theory and signal processing. In the context of neural networks, SSMs aim to combine the strengths of recurrent models, such as handling infinitely large contexts, with the efficiency of convolutional models for fast prompt processing and training. This positions SSMs as middle ground between classical LSTMs and transformers. In our experiments, we utilize Mamba, an SSM variant improved with selective mechanism [26, 27]. The Mamba Selective State Model extends this framework by making A, B, and dynamic, adjusting them based on the input x(t). This adaptive mechanism allows Mamba to selectively focus on relevant input features, filtering out irrelevant details [26]. By dynamically adapting its parameters, Mamba is able to capture long-range dependencies in sequences while remaining computationally efficient. While SSMs excel in efficiently modeling long-range dependencies and processing sequential data with reduced computational overhead compared to transformers, they typically lack the expressiveness and flexibility required for advanced reasoning tasks. These models may face challenges in capturing complex, hierarchical relationships, compounding the limitations already present in transformers when it comes to in-depth reasoning. Associative Recurrent Memory Transformer As shown in Rodkin et al. [54], ARMT can leverage information from the distant past of up to 50 million tokens. Compared to SSMs, ARMT is more expressive due to its grounding in the classical transformer architecture, while it also introduces the ability to recurrently process contexts of infinite length."
        },
        {
            "title": "Model\nGPTNeox\nARMT\nMamba\nLSTM",
            "content": "Depth 4 4 4 4 dmodel 128 128 128 128 dmem / state_size max ACT iterations - 32 16 - 4 4 4 4 Table 1: Hyperparameters for the base models. We used these hyperparameters in the O-S, O-O, O-RS and RO-S experiments, as well as CoT and GRPO experiments. Hyperparameters Theoretical Depth Estimates Theoretical estimates predict that for GPTNeox and Mamba depth of computation is limited by the number of layers Depth = O(L), where is the number of model layers. For LSTM computational depth not only grows with the number of layers, but also with the sequence length, making Depth = O(L + ), here is the sequence length. ARMT is trade-off between parallelization and recurrence. It utilizes the forward transformer for local processing of the segment, but passes its recurrent state between segments in RNN-like format, which allows its computational depth to grow with the sequence length, making Depth = O(L + ), here is the segment size."
        },
        {
            "title": "C Adaptive Computation Time formulation",
            "content": "The module calculates halting weight pt at each computation step t, which represents the percentage of the task completed by the module : pt = HALT(ht); ht+1 = (ht), HALT(ht) = σ(Whht + bh) (1) where ht is the layer input. This weight is accumulated into Pt until the halting condition is met: Pt = (cid:80)t i=0 pi; = argmint(Pt 1 ϵ) + 1. (2) Finally, the prediction is done in the following way: = (cid:80)T 1 1 (cid:80)T 2 component serves as time penalty. t=0 ptht+1 with pT 1 = = t=0 pt. For training, we add an auxiliary component to the loss function ˆL = + τ R. This"
        },
        {
            "title": "D Samples examples",
            "content": "The samples from our open dataset 6. We train the model to predict the blue tokens. In all these examples rule is 01011111100100000101111011111100 and the initial state is 10110111001000110100. O-S 10110111001000110100<sep>11101001101111101100<sep>10111011010000111011<sep> 11001110111011101100<sep>10111011001100111011<sep>11001110111011101100<sep> 10111011001100111011<sep>11001110111011101100<sep>10111011001100111011<sep> 11001110111011101100<gen>10111011001100111011 O-O 10110111001000110100<sep>11101001101111101100<sep>10111011010000111011<sep> 11001110111011101100<sep>10111011001100111011<sep>11001110111011101100<sep> 10111011001100111011<sep>11001110111011101100<sep>10111011001100111011<sep> 11001110111011101100<gen>10111011001100111011<sep>11001110111011101100<sep> 10111011001100111011<sep>11001110111011101100 O-RS 6https://huggingface.co/datasets/irodkin/1dCA_r2s20T20 19 Figure 9: ACT outperforms the base model on multiple prediction horizons task. Exact match accuracy (mean std) for cellular automata state prediction across different look-ahead horizons. Models receive initial 10 states followed by special shift token (1-4) indicating prediction horizon. 10110111001000110100<sep>11101001101111101100<sep>10111011010000111011<sep> 11001110111011101100<sep>10111011001100111011<sep>11001110111011101100<sep> 10111011001100111011<sep>11001110111011101100<sep>10111011001100111011<sep> 11001110111011101100<gen>01011111100100000101111011111100<sep> RO-S 01011111100100000101111011111100<sep>10110111001000110100<sep> 11101001101111101100<sep>10111011010000111011<sep> 11001110111011101100<sep> 10111011001100111011<sep>11001110111011101100<sep>10111011001100111011<sep> 11001110111011101100<sep>10111011001100111011<sep>11001110111011101100<gen>"
        },
        {
            "title": "E Multiple Prediction Horizons Training",
            "content": "Given an orbit OT (x) and the random shift token si {s1, s2, s3, s4} the objective is to predict the state x(T +i1). In this setup, we train the model to reason more for some inputs than others. We conducted experiments where single model was trained to handle multiple prediction horizons format: [x_0][SEP]...[x_9][shift_k][gen][MASK] where {1, 2, 3, 4} indicates the required lookahead. As shown in Figure 9, baseline GPTNeox performs 32% shift=2 and 19% for shift=4. Introducing ACT substantially mitigates these drops. (1-4 steps ahead) using special in the input tokens shift The ARMT architecture shows comparable characteristics while baseline performance at shift=2 is stronger than GPTNeox (43% vs 32%), ACT provides similar absolute improvements (85% at shift=2). However, both architectures exhibit similar limitations at the longest horizons (shift=4), with all variants scoring 21%-25%, indicating challenges in extreme-depth reasoning."
        },
        {
            "title": "F Ablation Studies",
            "content": "Here, we present several auxiliary studies of various ACT variants. F.1 Fixed Number of Steps in ACT vs Dynamic Number of Steps We conduct experiments with fixed number of steps to assess the need for adaptivity in computation time. constant depth of 3 was selected based on experiments with ACT, which demonstrated that 20 this represents the upper limit of the number of steps reached for any hidden state. The results with Fixed Computation Time (FCT) and ACT as the baseline are presented in Figure 10 and Figure 11 for O-S and O-O settings respectively. In O-S setting, FCT improved the exact match in look-ahead 2, 3 for GPTNeox, but performed worse in look-ahead 2 for ARMT. In contrast, in the O-O setting, FCT showed reduced performance for both GPTNeox and ARMT in look-ahead 2, 3, 4. Therefore, adaptivity in computation time might find the optimal amount of steps leading to enhanced exact match, or perform equivalently with fewer steps. Figure 10: Fixed Computation Time (FCT) with 3 iteration steps performs on par with Adaptive Computation Time (ACT) in Orbit-State task. Exact match accuracy (mean std) for cellular automata state prediction across different look-ahead horizons. Figure 11: Fixed Computation Time (FCT) with 3 iteration steps underperforms Adaptive Computation Time (ACT) in Orbit-Orbit task. Exact match accuracy (mean std) for cellular automata state prediction across different look-ahead horizons. F.2 Model-ACT vs Layer-ACT Figure 12 shows that Layer-ACT performs similarly or better compared to Model-ACT. In particular, Model-ACT has similar processing pattern to the COCONUT model [30], passing the hidden states from the model output to the input. Therefore, similar reasoning behavior is expected. notable difference is observed when these types of ACT are applied to ARMT. However, it is important to note that training was stopped after 30,000 steps, and the model with MACT augmentation did not have sufficient time to fully converge. All models in this experiment adhered to these training restrictions to ensure fair comparison. Figure 12: Layer-ACT performs similar or better compared to Model-ACT. Exact match on cellular automata state prediction task with look ahead 2."
        }
    ],
    "affiliations": [
        "AIRI, Moscow, Russia",
        "London Institute for Mathematical Sciences, London, UK",
        "MBZUAI, Abu Dhabi, UAE",
        "Neural Networks and Deep Learning Lab, MIPT, Moscow, Russia"
    ]
}