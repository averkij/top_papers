{
    "paper_title": "MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation",
    "authors": [
        "Taha Koleilat",
        "Hojat Asgariandehkordi",
        "Omid Nejati Manzari",
        "Berardino Barile",
        "Yiming Xiao",
        "Hassan Rivaz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation."
        },
        {
            "title": "Start",
            "content": "MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation Taha Koleilat(cid:66)"
        },
        {
            "title": "Berardino Barile",
            "content": "Hojat Asgariandehkordi Yiming Xiao Omid Nejati Manzari Hassan Rivaz 6 2 0 2 3 2 ] . [ 1 3 2 4 0 2 . 2 0 6 2 : r Concordia University, Montreal, Canada https://tahakoleilat.github.io/MedCLIPSeg"
        },
        {
            "title": "Abstract",
            "content": "Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation. Figure 1. (Top): Comparison between deterministic and probabilistic cross-modal fusion techniques in CLIP adaptation for text-driven segmentation. Probabilistic formulation models variability in visualtextual representations as distributions, enabling more robust feature alignment. (Bottom): Robustness and Reliability plots over ID and OOD data show improved generalization, with smaller out-of-domain performance drops and better calibration of predicted confidence, reflected by lower Brier scores. 1. Introduction Accurate and trustworthy medical image segmentation remains cornerstone for diagnosis, treatment planning, and quantitative clinical follow-up. Yet progress is often constrained by three persistent obstacles. First, expert annotations of segmentation ground truths are expensive and often inconsistent across raters, restricting the quality of supervised learning. Second, lesions and organs can ex- (cid:66) Corresponding Author: taha.koleilat@mail.concordia.ca Co-senior authors hibit ambiguous boundaries due to gradual intensity transitions or partial-volume effects that make clear decisionmaking difficult. Third, common domain shifts in scans due to variations in scanners, acquisition protocols, and patient populations cause models trained on limited in-distribution (ID) data to fail when exposed to out-of-distribution (OOD) conditions. These issues expose an urgent need for segmentation systems that are simultaneously data-efficient, uncertainty-aware, and generalizable across domains. In medical image segmentation, U-Net and its variants [34, 63] have driven major advances, by exploiting convolu1 tional neural network (CNN)s inductive biases for efficient feature learning and/or Vision Transformer (ViT) architectures [9, 11, 30] for long-range dependencies. Despite their success, these models depend on extensive pixel-wise supervision and most operate deterministically. Prior work shows that such segmentation networks, including U-Nets, are systematically over-confident, particularly for out-ofdistribution inputs and those with fuzzy tissue boundaries [3, 74], yielding unreliable segmentation results without warning mechanisms. This highlights the issues in conventional feature learning with deterministic approaches, which fail to properly account for ambiguity or local disagreement among features. Learning methods that can adaptively weigh evidence and/or modulate attention based on contextual reliability of data distribution would be hopeful to address this [53]. Additionally, alternative methods that move beyond traditional fully supervised approaches to mitigate the heavy reliance on expert training data while enhancing generalizability and interactability are highly desirable. By aligning images and texts through large-scale contrastive pretraining, visionlanguage models (VLMs) such as CLIP [60] and its biomedical variants [43, 76] offer promising paradigm towards more label-efficient and generalizable medical image segmentation with the potential for intuitive natural language-driven user-interaction. Notably, recent studies show that CLIPs patch tokens can encode spatial semantics [25, 82], suggesting its capacity for dense localization even without pixel-level supervision. Yet, in medical domain, where more subtle visual differences and fine-grained descriptions limit multi-modal alignment [80], the dense localization capability in VLMs remains weaker [23], but can be potentially strengthened through more nuanced imagetext mapping [48] and tailored, task-specific representation adaptation without degrading generalization [32]. Practically, clinical descriptions are far easier to obtain than pixel-level masks, making VLMs appealing in low-data regimes, where textual supervision compensates for limited annotations [47]. While most methods emphasize prompt learning, decoder tuning, or unidirectional textto-vision modulation [50, 59, 73], deep cross-modal fusion was shown to better support CLIP adaptation and spatial grounding [42, 77]. Yet, CLIP models with deterministic representations still remain over-confident on OOD data [54], motivating probabilistic CLIP formulation [15], which remains underexplored for medical image segmentation. In this context, key question emerges: how can we formulate cross-modal attention to be uncertainty-aware for CLIP-based segmentation? This is particularly relevant in practical medical AI adoption, where model credence and transparency are crucial, while over-confident deterministic AI models fail to meet the needs. Building on existing insights, we propose MedCLIPSeg, text-driven medical image segmentation framework that adapts CLIP with probabilistic, bidirectional visionlanguage represenIts core component, the Probabilistic Vitation fusion. sionLanguage (PVL) adapter learns confidence-weighted attention between image patches and text tokens within CLIPs multiple deep encoding layers. Confidence-aware attention scores based on variational modeling of Keys reduce over-confidence, and Monte Carlo sampling of Value distributions yields both mean segmentation masks and associated pixel-level uncertainty maps for user interpreSpecifically, our probabilistic modeling of the tation. Keys and Values in cross-modal attention naturally captures aleatoric uncertainty from ambiguous image features and epistemic uncertainty arising from unseen domains, leading to better accuracy, improved calibration, and enhanced robustness, in line with [29, 40]. As illustrated in Fig. 1, in contrast to MedCLIPSegs probabilistic adaptation, the deterministic variant leads to inaccurate segmentation and over-confidence that manifests as poor calibration and significant performance drops under domain shift. Furthermore, to maintain data efficiency, we preserve CLIPs pretrained encoders and introduce soft patch-level contrastive loss that refines imagetext alignment for dense prediction under limited supervision. In summary, our main contributions include: 1. Bidirectional representation-level fusion that enhances data-efficiency and robustness through novel vision-language interaction adapters while preserving CLIPs parameters, guided by soft contrastive loss. 2. Probabilistic cross-modal attention with variational Key and Value formulation to enable uncertainty-aware learning to improve accuracy and generalizability. 3. Pixel-level uncertainty maps by sampling Values from learnt probability distributions in VLM attentions to offer intuitive reliability visualization for clinical review. 4. Comprehensive evaluation against SOTA methods for medical image segmentation on five modalities and six organs, assessing data efficiency, domain generalizability, and model sub-component performance to provide insights into the proposed framework. 2. Related Work Medical Image Segmentation: Medical image segmentation has traditionally relied on vision-only architectures. CNNs established the foundation, with U-Net [63] introducing the long skip connections, inspiring UNet++ [85], Attention U-Net [56], and nnUNet [34]. Other variants, including the DeepLab series [12], improved multi-scale context modeling through dilated convolutions and pyramid pooling. The advent of ViTs further advanced segmentation by incorporating long-range dependencies. TransUNet [11] fused CNN backbones with Transformer encoders, while Swin-UNet [9] improved efficiency using shifted windows. Building on these designs, hybrid models such as 2 HiFormer [31] and UNETR [30] demonstrated strong performance across multiple benchmarks. Despite these advances, vision-only approaches often rely heavily on lowlevel appearance features and show limited robustness to domain shifts across scanners and imaging protocols. This motivates the use of high-level semantic priors, particularly via cross-modal learning to improve generalizability. Vision-Language Models: Vision-language models have gained interest in biomedical domains. As CLIP [60] and ALIGN [38] demonstrated strong zero-shot transfer across different visual tasks, they inspired biomedical variants such as BiomedCLIP [76], PubMedCLIP [21], and UniMedCLIP [43], which leverage clinical image-text corpora for domain-specific learning. While these models provide robust global alignment, they often require further adaptation to capture the fine-grained semantics of anatomy and pathology. Typical parameter-efficient adaptation techniques include prompt tuning (e.g., CoOp [84], CoCoOp [83], and MaPLe [42]) and low-rank-based model updates (e.g., CLIP-LoRA [75] and CLIP-SVD [49]). Tailored for biomedical visions, methods such as DCPL [10] and BiomedCoOp [47] incorporate domain priors and knowledge distillation to enhance adaptation under limited supervision. More recently, probabilistic fine-tuning frameworks such as CLAP4CLIP [37] and ProbVLM [67] incorporate embedding uncertainty to better manage the manyto-one mappings between vision and language, improving calibration and cross-domain generalization. Despite these advances, most biomedical VLMs focus on classification or retrieval, with limited exploration of spatially grounded tasks, such as segmentation, which is more challenging. Prompt-based Segmentation: For natural images, only few methods extend VLMs to image segmentation. CLIPSeg [51] and CRIS [69] append lightweight decoders to frozen CLIP encoders, while LAVT [73] fuses textual embeddings into Transformer layers through crossattention, modulating visual features throughout the encoder. Further approaches such as DenseCLIP [61], ZegCLIP [86], and SAN [72] enhance fine-grained localisation, while the recent CAT-Seg [16] stands out with strong state-of-the-art performance in open-vocabulary segmentation. On the other hand, the Segment Anything Model (SAM) [45] introduced promptable foundation for general-purpose segmentation but lacks explicit natural language interaction and conditioning, and remains limited to drawing-based prompts. Although effective for natural images, these methods struggle in biomedical contexts, where images lack contextual diversity, exhibit high interclass similarity, and feature ambiguous boundaries. On the other hand, medical adaptations, like LViT [50] and Ariadnes Thread [81], integrated BERT textual embeddings into ViT architectures. MedSAM [52] and recent promptbased work [46, 48, 62, 64, 70] still rely on geometric prompts at different intermediate stages, introducing potential instability. More recently, few-shot medical image segmentation frameworks, such as UniverSeg [7], MultiverSeg [71], and Iris [26], leverage small support set of imagelabel pairs to segment unseen classes and modalities without additional training. BiomedParse [78] further explores structured knowledge parsing across modalities through the use of natural language. However, methods for adapting VLMs such as CLIP for dense biomedical prediction remain limited. Poudel et al. [59] applied CLIPSeg and CRIS with frozen CLIP encoders and new decoders, but domain-specific models such as BiomedCLIP [76] offered no gains, highlighting the weakness of naıve adaptations. VLSM-Adapter [19] offers parameterefficient VLM adaptation, and CausalCLIPSeg [13] introduces multi-modal causal adaptations, but evaluations on multiple datasets are not provided, and we show its weakness in domain generalization  (Table 2)  . Unlike existing approaches, our method preserves CLIPs pretrained parameters while introducing probabilistic, bidirectional visionlanguage fusion to enhance robustness and clinical reliability in biomedical segmentation. 3. Methodology Our MedCLIPSeg framework is presented in Fig. 2. 3.1. CLIP Overview Our text-driven segmentation framework builds upon the CLIP architecture [60], which comprises two Transformerbased encoders: vision encoder Ev and text encoder Et. They encode images and text into shared D-dimensional space for cross-modal alignment. For batch of RGB images, the input is represented as Xv RB3HW . The vision encoder first partitions each image into nonoverlapping patches, projects each to D-dimensional embedding, and prepends learnable [CLS] token. This results in sequence of (P + 1) tokens: Zv = Ev(Xv) RB(P +1)D (1) RBD, serves as The [CLS] token output, Z[CLS] the global image representation in the shared CLIP space. Notably, prior works [25, 82] have shown that, although CLIPs objective aligns only the [CLS] tokens, the contrastive vision-language pre-training implicitly shapes the patch token representations to encode semantically meaningful spatial features with emerging textual correlation. For the textual branch, tokenized prompts Xt RBL of length are embedded into D-dimensional vectors and processed by the text encoder. Unlike the vision encoders use of [CLS] token, CLIP takes the output at the [EOS] position as the global text representation: Zt = Et(Xt) RBLD (2) Figure 2. Overview of the proposed MedCLIPSeg framework for text-driven medical image segmentation. The model extends CLIP with vision and language encoders connected via PVL Adapters, which perform confidence-weighted imagetext fusion at multiple deep layers. Segmentation and uncertainty maps arise from the mean and entropy of posterior samples, with soft patch-level contrastive loss. the PVL adapter with total layers amounting to , first projects both modalities to shared lower-dimensional space Ds with (n) RDvDs and (n) RDtDs : Figure 3. Illustrations of PVL Adapter and AttnPVL. Here, the [EOS] output Z[EOS] RBD acts as the compact textual embedding in the joint space. The global embeddings enable cross-modal similarity, while the patch tokens from Zv preserve fine-grained spatial information. In our framework, the global text embedding [EOS] is used as query and compared via dot product with each vision patch token, producing segmentation logits that guide segmentation according to the natural language prompt. v(n) = V(n)W (n) , t(n) = T(n)W (n) (3) QKV parameterization: Inspired by [40], we extend the standard attention formulation to incorporate uncertainty in both Keys and Values by modeling them as probability distributions with learnable means and variances. These variances represent data ambiguity, allowing the model to encode inherent noise in the input representations. This probabilistic design enables the attention module to downweight uncertain tokens and sample value representations for stochastic modeling. This attention module AttnPVL takes as input query sequence and context sequence and outputs fused output as = AttnPVL(X, Z). The input query and context sequences are transformed using the Query (Q), Key (K), Value (V), and Output (O) projection matrices as follows: 3.2. Probabilistic Multi-modal Adaptation = WQ (4) To enable efficient and confidence-aware multimodal fusion-based CLIP adaptation for biomedical segmentation, we propose the Probabilistic Vision-Language Adapter (PVL Adapter), novel probabilistic framework that bridges CLIPs vision and language encoders at the representation level. Each PVL module performs bidirectional, probabilistic interaction between image and text tokens. The architecture of PVL Adapter is depicted in Fig. 3. Downward Projection: Given visual tokens V(n) RBTvDv and text tokens T(n) RBTtDt at Layer where RBTqDs is the input query sequence, WQ RDsDa is the learnable projection matrix, and is the transformed query to compute the attention score. Keys and Values are projected into both mean and logvariance representation: [Kµ, Klog σ2] = WK, [Vµ, Vlog σ2] = WV (5) where RBTkDs is the context sequence and WK RDs2Da produces two Da-dimensional splits for mean and log-variances. 4 To convert the predicted log-variances into original variance values, we avoid the numerically unstable exp() and instead use the softplus ζ(.) activation, which is smoother and less prone to instabilities: σ = ζ(Klog σ2), 2 σ = ζ(Vlog σ2 ) (6) Here, 2 σ and 2 and Value distributions. σ represent the variance terms for the Key Confidence-weighted attention: Our proposed attention score considers two terms: mean similarity Sµ and variance-based confidence penalty S2 σ. Each score Sij corresponds to the dot product between deterministic query vector Qi and probabilistic key Kj (Kµ,j, 2 σ,j), assuming feature-wise independence. This yields probabilistic attention score with mean and variance given by: Sµ = QK µ Da , S2 σ = σ) Q2(K 2 Da , (7) where Q2 denotes the element-wise square of Q. The variance term quantifies how key uncertainty interacts with the magnitude of query features, allowing uncertain tokens to be adaptively downweighted. The final attention weights are computed as: = {Aij} = softmax(cid:0)Sµ βSσ (cid:1)/ωij exp(cid:0)Sµ,ij exp(cid:0)Sµ,ir (cid:80) (cid:1)/ωir Aij = , ωij = exp(cid:0)β Sσ,ij (cid:1) (9) (cid:1) with (8) where β = 2.35 (indicates the full width at half maximum of Gaussian distribution) scales the confidence penalty, down-weighting overconfident attention responses. This dynamic probabilistic weighting allows the model to emphasize reliable evidence and downweight uncertain signals, thereby regularizing feature learning and improving out-of-distribution generalization, which is crucial for medical data with large domain variability. Conceptually, Eq. 8 can be interpreted as variance-aware extension of the standard attention mechanism, where attention scores are weighted by an input-dependent uncertainty term (Eq. 9); when β = 0, the formulation naturally reduces to the conventional deterministic attention. Value sampling: To model uncertainty in the Value representations, we draw samples from their learned probability distribution (Vµ, 2 σ ). Each sample is obtained via the reparameterization trick: Vsample = Vµ + ϵ Vσ, ϵ (0, I). (10) where denotes the Hadamard product. During training, we adopt stochastic regime by sampling only once to compute the attended output as: = Vsample. (11) At test time, we perform multiple stochastic forward passes to sample from the models approximate posterior. The learned variances within the PVL Adapters capture aleatoric uncertainty, while Monte Carlo sampling accounts for epistemic uncertainty arising from model variability. The predictive entropy across sampled outputs quantifies total uncertainty and overall confidence. Empirically, we found 30 forward passes sufficient to obtain stable uncertainty estimates. Residual gating: Directly relying on attended features from PVL Adapters can introduce instability early in the training schedule, when attention responses are still noisy. The residual gate mitigates this by controlling how much new information is incorporated, gradually increasing reliance on attended features as cross-modal alignment and model confidence improve, leading to smoother optimization and more reliable fusion: Oproj = Wout, (12) where Wout RDaDs is the output projection matrix. Then we apply learnable gate [0, 1]: = Oproj + (1 g) X, (13) where scalar is initialized to balanced weighting through sigmoid(0), providing equal emphasis on the original query and the attended output at the start of training. Bidirectional Interaction: two-way Transformer layer performs mutual updates via AttnPVL, enabling visual and textual features to mutually refine each other for stronger cross-modal alignment and contextual consistency: v(n) = Attnvt t(n) = Attntv PVL (v(n), t(n)) PVL (t(n), v(n)) (14) (15) Upward Projection: Finally, the fused features are projected back to their original dimensions with (n) RDsDv and (n) RDsDt with residual connections applied: ˆV(n) = V(n) + v(n)W (n) ˆT(n) = T(n) + t(n)W (n) (16) (17) The PVL Adapters is applied at multiple encoding CLIP layers to refine joint representations. 3.3. Segmentation via Pixel-Text Similarity After the final fusion layer, the text [EOS] token embedding and the visual patch are L2-normalized. The visual patch tokens are then upscaled via learned block ψ, while 5 lightweight MLP mask head ϕ maps [EOS] ible embedding: to compatV = ψ(Z [Patches] ), = ϕ(Z[EOS] ) (18) The segmentation logits RBHW are computed by simple dot product followed by bilinear interpolation: = UpsampleHW ( t) (19) 3.4. Soft Patch-level Contrastive Loss Building on CLIPs ability to capture rich imagetext relationships, we extend it to handle diverse descriptions with spatial context for medical images. Since single caption may mention multiple anatomical regions, global alignment alone can be insufficient for fine-grained correspondence. To address this, we average image patch embeddings into stable regional representations that preserve local visual semantics while reducing token-level noise. This enables more accurate textregion associations and consistent segmentation performance across heterogeneous anatomy. Specifically, we align L2-normalized Average-pooled visual patch embeddings pv = Z[Patches] with text embeddings pt = Z[EOS] via soft contrastive loss, computing text-tot image similarity logit Pvt and image-to-text counterpart Ptv across all batch pairs. Since text prompts within batch can be similar, hard targets are replaced with soft targets derived from text similarities: = softmax , (20) (cid:19) (cid:18) pt τ where τ is temperature parameter set to 0.2. The soft cross-entropy loss is defined in the following, and the final contrastive loss averages both directions: Lsoft(P, G) ="
        },
        {
            "title": "1\nB",
            "content": "(cid:88) (cid:88) Gij log(softmax(Pi)j). LSoftCon = 1 2 (cid:0)Lsoft(Ptv, G) + Lsoft(Pvt, G)(cid:1) . (21) overall The (Dice+BCE, equal weights) and contrastive objectives: combines training loss (22) segmentation = λSeg LSeg + λSoftCon LSoftCon. (23) where λSeg, λSoftCon control the relative importance of each loss term, set to 0.5 and 0.1, respectively. 4. Experiments and Results 4.1. Experimental Setup Data Efficiency: We evaluate MedCLIPSeg under varying levels of training data to assess data-efficiency and scalability. Models are trained with 10%, 25%, and 50% of 6 the data to measure performance under limited supervision. The fully supervised setting uses all training examples with their pixel-level and textual annotations, serving as an upper-bound reference. Domain Generalization: To assess out-of-distribution (OOD) generalization, models are trained on one indistribution (ID) source dataset, fully supervised, and directly tested on unseen target datasets without fine-tuning. This evaluates robustness to domain shifts in image acquisition, clinical sites, and patient populations. Table 1. Data-efficiency evaluation: This table reports the average DSC and NSD (%) when varying the fraction of training data across different segmentation methods. Best results are in bold, and second-best are underlined. Method 10% Data 25% Data 50% Data 100% Data DSC NSD DSC NSD DSC NSD DSC NSD UNet [63] UNet++ [85] DeepLabv3 [12] AttnUNet [56] nnUNet [34] Swin-UNet [9] TransUNet [11] Unimodal Approaches 62.74 65.86 65.39 64.97 76.73 54.69 55.25 66.16 69.21 69.10 68.53 80.66 59.24 58.95 64.43 67.08 64.84 66.25 77.37 57.91 56.38 71.61 73.15 68.58 71.34 78.86 55.89 55. 60.95 63.72 61.32 62.78 73.45 53.04 52.69 75.14 76.31 72.57 74.96 82.68 61.25 59.30 78.49 78.44 73.28 76.30 81.40 65.03 67.22 82.07 81.79 77.42 79.77 85.08 69.32 71.15 LViT [50] 66.51 Ariadnes Thread [81] 61. Generic Text-driven Approaches 75.66 63.09 78.88 65.65 78.12 64.51 68.80 62.75 81.34 66.92 EoMT-CLIP [41] CLIPSeg [51] DenseCLIP [61] ZegCLIP [86] SAN [72] MaPLe [42] 74.07 74.66 67.84 61.25 74.13 66.27 MaPLe [42] + Decoder 74.81 74.47 71.19 78.76 VLSM-Adapter [19] CausalCLIPSeg [13] CAT-Seg [16] CLIP-Based Approaches 76.29 78.31 70.23 72.46 76.13 71.53 79.64 77.63 75.42 81.12 79.84 81.34 72.70 75.01 78.91 73.95 82.60 80.53 78.00 83.92 77.41 77.75 70.33 63.72 76.97 68.75 77.90 77.50 73.74 81.50 79.19 79.63 72.09 76.21 78.80 74.60 82.81 80.83 78.60 83. 82.78 82.58 74.45 78.80 81.52 77.12 85.80 83.77 81.22 85.61 83.35 70.07 85.89 71.49 82.93 84.87 74.19 78.98 81.62 74.60 84.94 83.85 81.34 85.90 86.35 87.74 76.89 81.69 84.35 77.10 87.91 86.72 84.20 88.31 MedCLIPSeg (Ours) 81. 83.94 85.08 87.85 87.18 89.95 88. 91.35 Datasets: We experiment on diverse medical imaging datasets spanning six organs and five modalities, covering clinically critical segmentation tasks including tumor, polyp, and skin lesion segmentation, each representing challenging, high-impact applications with ambiguous boundaries and large domain variability. For supervised settings and data-efficiency tests, we use BUSI [1], BTMRI [14], ISIC [17, 66], Kvasir-SEG [36], QaTa-COV19 [18], and EUS [35]. For domain generalization tests, evaluations are conducted on BUSUC [33], BUSBRA [28], BUID [4], UDIAT [8], CVC-ColonDB [65], CVC-ClinicDB [6], CVC-300 [68], BKAI [55], BRISC [24], and UWaterlooSkinCancer [2, 27], each introducing substantial domain shifts. For datasets with missing text, we generated prompts using GPT-5 [57] and applied image processing techniques to identify different visual attributes for each image. Dataset and prompt details are provided in Appendices and C. Table 2. Domain generalization: Models are trained on source dataset and evaluated on OOD target datasets without adaptation. DSC (%) values are reported where the best results are in bold, and second-best are underlined. Breast Ultrasound Polyp Endoscopy Brain MRI Skin Dermatoscopy Method Source Target Source Target Source Target Source Target BUSI BUSBRA BUSUC BUID UDIAT Kvasir-SEG ColonDB ClinicDB CVC300 BKAI BTMRI BRISC ISIC UWaterloo LViT [50] CLIPSeg [51] DenseCLIP [61] ZegCLIP [86] SAN [72] MaPLe [42] MaPLe [42] + Decoder VLSM-Adapter [19] CausalCLIPSeg [13] CAT-Seg [16] MedCLIPSeg (Ours) 75.32 80.95 71.85 72.08 77.99 66.37 80.49 80.90 76.11 81.83 85.72 59.41 63.66 53.34 61.08 64.37 50.08 55.89 68.48 55.87 70.94 75.06 67.95 75.03 70.97 73.57 74.15 71.52 64.96 82.37 69.12 81.48 53.51 68.43 63.53 71.75 58.13 70.77 60.66 75.26 64.49 73. 65.60 56.67 54.93 52.41 61.98 57.81 59.44 69.16 48.90 70.30 84.37 78.99 74.64 85.29 81.98 79.32 78.46 83.16 76.12 83.46 85.89 78.77 86.43 90. 60.01 59.93 56.38 53.46 61.82 48.09 61.53 63.51 41.65 68.49 71.90 75.27 71.49 68.08 69.75 74.46 59.64 71.20 76.09 57.54 70.35 80.80 70.22 72.74 64.71 60.73 80.36 63.80 74.62 75.24 45.77 78.12 67.17 66.46 61.63 65.60 69.31 56.94 66.93 71.59 52.56 74. 81.41 86.33 70.30 76.65 85.27 75.40 85.08 85.03 81.71 84.86 71.86 77.61 34.12 66.31 71.60 45.19 71.46 68.92 53.96 76.28 91.21 90.55 89.29 81.45 91.39 88.31 90.10 91.30 89.47 91.27 80.82 79.15 88. 80.92 92.54 58.87 80.19 53.39 38.60 82.51 69.12 81.83 82.17 48.73 82.02 83.53 Implementation Details: We use UniMedCLIP ViTB/16 [43] with PubMedBERT [76] as the backbone. All models are trained for 100 epochs (10 for EUS) with learning rate of 3 104, batch size 24, and Adam optimizer [44] under cosine annealing. The segmentation loss combines Dice and binary cross-entropy (BCE) losses (equal weights). Experiments are conducted on single NVIDIA A100 GPU (40GB RAM). All settings were identical across all experiments, and all CLIP-based baselines use the same UniMedCLIP backbone for fair comparison. We use Dice Similarity Coefficient (DSC) and normalized surface distance (NSD) to compare segmentation accuracy. 4.2. Data Efficiency Evaluation As shown in Table 1, MedCLIPSeg consistently outperforms both unimodal and multimodal baselines, notably the strong CLIP-based CAT-Seg, with 23% gains at 10% data and 34% at 50%. Compared to EoMT-CLIP [41], variant without the PVL Adapters and LSoftCon, MedCLIPSeg achieves further +7.0% and +8.8% DSC improvements at 10% and 25% efficiency, respectively, highlighting the effectiveness of these components for data-efficient learning. 4.3. Domain Generalization Table 2 reports cross-dataset performance. MedCLIPSeg achieves 85.7% DSC on BUSI, 84.4% on BUSUC, 90.2% on Kvasir-SEG, 88.0% on BTMRI, and 92.5% on ISIC, consistently outperforming SAN [72] and CAT-Seg [16]. Despite substantial distribution shifts, such as lighting/signal gain, zoom, and viewpoint/field-of-view variations in polyp and ultrasound datasets, our model preserves contour fidelity and segmentation quality, demonstrating robust generalization across domains. 4.4. Effectiveness of Key Design Components Table 3 quantifies the impacts of MedCLIPSegs components. HM denotes the harmonic mean between the ID and Table 3. Effect of different key components. Method Domain Generalization ID DSC (%) OOD DSC (%) HM DSC (%) MedCLIPSeg (Ours) 89.11 79. 83.76 Probabilistic Vision-Language Adapters w/o PVL Adapters 81.23(7.88) 55.23(23.79) 65.75(18.01) 87.55(1.56) 76.79(2.23) 81.82(1.94) w/o Gating w/o AttnPVL 86.21(2.90) 74.13(4.89) 79.71(4.05) Deterministic MedCLIPSeg 87.68(1.43) 63.12(15.90) 73.40(10.36) Bidirectional Multimodal Interaction 81.50(7.61) 64.40(14.62) 71.95(11.81) w/o Visual Adaptation 88.83(0.28) 76.40(2.62) 82.15(1.61) w/o Textual Adaptation w/o Bidirectional Interaction 88.71(0.40) 77.71(1.31) 82.85(0.91) Unimodal MedCLIPSeg 86.53(2.58) 73.49(5.53) 79.48(4.28) Contrastive Loss w/o LSoftCon Hard Targets Attention-pooled LSoftCon 87.24(1.87) 77.08(1.94) 81.84(1.92) 88.34(0.77) 77.64(1.38) 82.65(1.11) 88.73(0.38) 75.60(3.42) 81.64(2.12) Table 4. Effect of text prompt design. Text Prompt Design ID DSC (%) OOD DSC (%) HM DSC (%) Contradictory Missing Location Overdescriptive Underdescriptive Original 68.60 86.98 82.93 66.91 89. 63.21 77.75 74.49 49.38 79.02 65.79 82.11 78.48 56.82 83.76 OOD DSC (%) scores. Removing the PVL Adapters causes the largest DSC drop (7.9% ID, 23.8% OOD), emphasizing their role in robust multi-modal alignment. Replacing probabilistic attention with deterministic attention reduces OOD DSC by 15.9%, confirming the value of uncertainty-aware formulation. Bidirectional interaction further enhances performance, while excluding LSoftCon decreases HM DSC by 1.92%, highlighting its positive role in maintaining nuanced cross-modal alignment. 7 Figure 4. Segmentation and uncertainty visualizations. Uncertainty peaks along lesion boundaries and remains consistent across diverse datasets, indicating reliable calibration and generalization. ID data are in blue while OOD data are in red. lowers performance by 1.65% and 5.28%, respectively. Contradictory, which contain internally inconsistent descriptions that can confuse the model, along with Underspecified prompts, deteriorate DSCs to 65.79% and 56.82%, respectively (see Appendix for more details). CLIP Backbone: Table 5 shows that the backbone choice strongly impacts generalization. Models benefit from stronger and larger-scale CLIP pretraining, with UniMedCLIP [43] providing the most robust transferability. 4.6. Uncertainty Visualization and Reliability Figure 4 illustrates segmentation and uncertainty maps for both ID and OOD data, where uncertainty consistently concentrates along anatomical boundaries and regions prone to expert disagreement. All subsequent quantitative analyses are computed over foreground regions in ID and OOD data. Predicted uncertainty closely follows segmentation errors, achieving strong Spearman correlations of (87.57%, 80.41%) for (ID, OOD), respectively. Furthermore, probabilistic modeling improves calibration, reducing the Brier scores from (23.9%, 25.3%) in the deterministic baseline to (11.1%, 11.8%), as shown in Fig. 1, alleviating overconfidence and enhancing reliability in clinical decision-making. 5. Conclusion We presented MedCLIPSeg, probabilistic framework for text-driven medical image segmentation by adopting the novel PVL Adapter that enables confidence-weighted attention and explicit uncertainty estimation. Leveraging CLIPs pretrained features through bidirectional fusion and soft patch-level contrastive loss, MedCLIPSeg achieves SOTA segmentation performance with high data efficiency, strong out-of-distribution generalization, and wellcalibrated uncertainty across six organs and five modalities, advancing reliable VLM for medical AI. Figure 5. Layer-wise interventions (left) and confidence weighting (β) (right) ablations averaged on ID and OOD data. Table 5. Effect of pre-trained visionlanguage models. Pre-trained Model ID DSC (%) OOD DSC (%) HM DSC (%) CLIP [60] PubMedCLIP [22] BiomedCLIP [76] UniMedCLIP [43] 88.48 86.67 88.70 89.11 74.81 73.05 77.08 79.02 81.07 79.28 82.48 83.76 4.5. Ablation Studies Layer Interventions: Figure 5(a) shows that deeper layer interventions with PVL Adapters steadily improve both ID and OOD segmentation, peaking at Layer 10 (HM DSC 83.76%), with slight drop at the final layer. Confidence Weight (β) Choice: Figure 5(b) shows that β=2.35 yields the best HM DSC from range of tested values in [0,5], balancing in-domain stability and OOD generalization by effectively calibrating probabilistic attention. Text Prompt Style: Table 4 shows that concise yet informative prompts (i.e., Original) perform best. Relocation) or adding verbosity moving spatial cues (i.e.,"
        },
        {
            "title": "Acknowledgements",
            "content": "acknowledge and the Natural SciWe ences of Council Canada (NSERC) and the Fonds de recherche du Quebec (B2X-363874). the Engineering technologies support of Nature"
        },
        {
            "title": "Research",
            "content": "et"
        },
        {
            "title": "References",
            "content": "[1] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in brief, 28:104863, 2020. 6, 13, 14 [2] Robert Amelard, Jeffrey Glaister, Alexander Wong, and David A. Clausi. High-level intuitive features (hlifs) for intuitive skin lesion description. IEEE Transactions on Biomedical Engineering, 62(3):820831, 2015. 6, 13, 14 [3] Shuang Ao, Stefan Rueger, and Advaith Siddharthan. Identifying over and underTwo sides of miscalibration: In Uncerconfidence prediction for network calibration. tainty in artificial intelligence, pages 7787. PMLR, 2023. 2 [4] Ali Abbasian Ardakani, Afshin Mohammadi, Mohammad Mirza-Aghazadeh-Attari, and Rajendra Acharya. An open-access breast lesion ultrasound image database: Applicable in artificial intelligence studies. Computers in Biology and Medicine, 152:106438, 2023. 6, 13, 14 [5] Fan Bai, Yuxin Du, Tiejun Huang, Max Q. H. Meng, and Bo Zhao. M3d: Advancing 3d medical image analysis with multi-modal large language models, 2024. 14 [6] Jorge Bernal, Javier Sanchez, Gloria FernandezEsparrach, Debora Gil, Cristina Rodrıguez, and Fernando Vilarino. Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics, 43:99 111, 2015. 6, 13, [7] Victor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert R. Sabuncu, John Guttag, and Adrian V. Dalca. Universeg: Universal medical image segmentation, 2023. 3 [8] Michal Byra, Piotr Jarosik, Aleksandra Szubert, Michael Galperin, Haydee Ojeda-Fournier, Linda Olson, Mary OBoyle, Christopher Comstock, and Michael Andre. Breast mass segmentation in ultrasound with selective kernel U-Net convolutional neural network. Biomed Signal Process Control, 61, 2020. 6, 13, 14 [9] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European conference on computer vision, pages 205218. Springer, 2022. 2, 6, 15, 21, 22, 23, 24 [10] Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, and Xiaokang Yang. Domain-controlled prompt learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 936944, 2024. 3 [11] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021. 2, 6, 15, 21, 22, 23, 24 9 [12] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 2, 6, 15, 21, 22, 23, [13] Yaxiong Chen, Minghong Wei, Zixuan Zheng, Jingliang Hu, Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, and Lichao Mou. Causalclipseg: Unlocking clips potential in referring medical image segmentation with causal intervention. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 7787. Springer, 2024. 3, 6, 7, 15, 21, 22, 23, 24 [14] Jun Cheng. brain tumor dataset. 2017. 6, 13, 14 [15] Silin Cheng and Kai Han. VaMP: Variational multi-modal prompt learning for vision-language models. In The Thirtyninth Annual Conference on Neural Information Processing Systems, 2025. 2 [16] Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Catseg: Cost aggregation for open-vocabulary semantic segIn Proceedings of the IEEE/CVF Conference mentation. on Computer Vision and Pattern Recognition, pages 4113 4123, 2024. 3, 6, 7, 15, 21, 22, 23, 24 [17] Noel CF Codella, David Gutman, Emre Celebi, Brian Helba, Michael Marchetti, Stephen Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis toward melanoma detection: challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018), pages 168172. IEEE, 2018. 6, 13, 14 [18] Aysen Degerli, Serkan Kiranyaz, Muhammad E. H. Chowdhury, and Moncef Gabbouj. Osegnet: Operational segmentation network for covid-19 detection using chest x-ray images. In 2022 IEEE International Conference on Image Processing (ICIP), pages 23062310, 2022. 6, 13, 14 [19] Manish Dhakal, Rabin Adhikari, Safal Thapaliya, and Bishesh Khanal. Vlsm-adapter: Finetuning vision-language In Insegmentation efficiently with lightweight blocks. ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages 712722. Springer, 2024. 3, 6, 7, 15, 21, 22, 23, [20] Julia Dietlmeier, Oluwabukola Grace Adegboro, Vayangi and Noel Vishmi Vishara Ganepola, Claudia Mazo, OConnor. VLSM-ensemble: Ensembling CLIP-based vision-language models for enhanced medical image segmentation. In Medical Imaging with Deep Learning - Short Papers, 2025. 18, 19 [21] Sedigheh Eslami, Gerard de Melo, and Christoph Meinel. Does clip benefit visual question answering in the medical domain as much as it does in the general domain?, 2021. 3 [22] Sedigheh Eslami, Christoph Meinel, and Gerard de Melo. PubMedCLIP: How much does CLIP benefit visual question In Findings of the Asanswering in the medical domain? sociation for Computational Linguistics: EACL 2023, pages 11811193, Dubrovnik, Croatia, 2023. Association for Computational Linguistics. 8 [23] Hasan Farooq, Murtaza Taj, Mehwish Nasim, and Arif Mahmood. Localization lens for improving medical visionIn International Conference on Medilanguage models. cal Image Computing and Computer-Assisted Intervention, pages 341350. Springer, 2025. 2 [24] Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh, and Vahid Abolghasemi. Brisc: Annotated dataset for brain tumor segmentation and classification with swin-hafnet. arXiv preprint arXiv:2506.14318, 2025. 6, 13, 14 [25] Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting clips image representation via text-based decomposition. arXiv preprint arXiv:2310.05916, 2023. 2, 3 [26] Yunhe Gao, Di Liu, Zhuowei Li, Yunsheng Li, Dongdong Chen, Mu Zhou, and Dimitris Metaxas. Show and segment: Universal medical image segmentation via in-context learning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2083020840, 2025. 3 [27] Jeffrey Glaister, Robert Amelard, Alexander Wong, and David Clausi. Msim: multistage illumination modeling of dermatological photographs for illumination-corrected skin lesion analysis. IEEE Transactions on Biomedical Engineering, 60(7):18731883, 2013. 6, 13, [28] Wilfrido Gomez-Flores, Maria Julia Gregorio-Calas, and Wagner Coelho de Albuquerque Pereira. Bus-bra: breast ultrasound dataset for assessing computer-aided diagnosis systems. Medical Physics, 51(4):31103123, 2024. 6, 13, 14 [29] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 13211330. PMLR, 2017. 2 [30] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574584, 2022. 2, 3 [31] Moein Heidari, Amirhossein Kazerouni, Milad Soltany, Reza Azad, Ehsan Khodapanah Aghdam, Julien CohenAdad, and Dorit Merhof. Hiformer: Hierarchical multi-scale representations using transformers for medical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 62026212, 2023. 3 [32] Chaoqin Huang, Aofan Jiang, Jinghao Feng, Ya Zhang, Xinchao Wang, and Yanfeng Wang. Adapting visual-language models for generalizable anomaly detection in medical images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1137511385, 2024. 2 [33] Ahmed Iqbal and Muhammad Sharif. Memory-efficient transformer network with feature fusion for breast tumor segmentation and classification task. Engineering Applications of Artificial Intelligence, 127:107292, 2024. 6, 13, 14 [34] Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. 1, 2, 6, 15, 21, 22, 23, 24 [35] Marıa Jaramillo, Josue Ruano, Martın Gomez, and Eduardo Romero. Endoscopic ultrasound database of the pancreas. In 16th International Symposium on Medical Information Processing and Analysis, pages 130135. SPIE, 2020. 6, 13, 14 [36] Debesh Jha, Pia Smedsrud, Michael Riegler, Pal Halvorsen, Thomas De Lange, Dag Johansen, and Havard In InJohansen. Kvasir-seg: segmented polyp dataset. ternational conference on multimedia modeling, pages 451 462. Springer, 2019. 6, 13, 14 [37] Saurav Jha, Dong Gong, and Lina Yao. Clap4clip: Continual learning with probabilistic finetuning for vision-language models. Advances in neural information processing systems, 37:129146129186, 2024. 3 [38] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. 3 [39] A. Emre Kavur, N. Sinem Gezer, Mustafa Barıs, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp Ernst, Savas Ozkan, Bora Baydar, Dmitry Lachinov, Shuo Han, Josef Pauli, Fabian Isensee, Matthias Perkonigg, Rachana Sathish, Ronnie Rajan, Debdoot Sheet, Gurbandurdy Dovletov, Oliver Speck, Andreas Nurnberger, Klaus H. Maier-Hein, Gozde Bozdagı Akar, Gozde Unal, Oguz Dicle, and M. Alper Selver. Chaos challenge - combined (ct-mr) healthy abdominal organ segmentation. Medical Image Analysis, 69:101950, 2021. 14 [40] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems, 30, 2017. 2, 4 [41] Tommie Kerssies, Niccolo Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, and Daan de Geus. Your vit is secretly an image segmentation model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2530325313, 2025. 6, [42] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: the Multi-modal prompt IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1911319122, 2023. 2, 3, 6, 7, 15, 21, 22, 23, 24 In Proceedings of learning. [43] Muhammad Uzair Khattak, Shahina Kunhimon, Muzammal Naseer, Salman Khan, and Fahad Shahbaz Khan. Unimedclip: Towards unified image-text pretraining paradigm arXiv preprint for diverse medical imaging modalities. arXiv:2412.10372, 2024. 2, 3, 7, 8, 14 [44] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. 7, [45] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White10 head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollr, and Ross Girshick. Segment anything, 2023. 3 [46] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Medclip-sam: Bridging text and image In Intowards universal medical ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages 643653. Springer, 2024. 3 image segmentation. [47] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Biomedcoop: Learning to prompt for biomedical vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 14766 14776, 2025. 2, [48] Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, and Yiming Xiao. Medclip-samv2: Towards universal textdriven medical image segmentation. Medical Image Analysis, page 103749, 2025. 2, 3 [49] Taha Koleilat, Hassan Rivaz, and Yiming Xiao. Singular value few-shot adaptation of vision-language models. arXiv preprint arXiv:2509.03740, 2025. 3 [50] Zihan Li, Yunxiang Li, Qingde Li, Puyang Wang, Dazhou Guo, Le Lu, Dakai Jin, You Zhang, and Qingqi Hong. Lvit: Language meets vision transformer in medical image segmentation, 2023. 2, 3, 6, 7, 13, 15, 21, 22, 23, 24 [51] Timo Luddecke and Alexander Ecker. Image segmentaIn Proceedings of tion using text and image prompts. the IEEE/CVF conference on computer vision and pattern recognition, pages 70867096, 2022. 3, 6, 7, 15, 21, 22, 23, 24 [52] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1), 2024. [53] Alireza Mehrtash, William Wells, Clare Tempany, Purang Abolmaesumi, and Tina Kapur. Confidence calibration and predictive uncertainty estimation for deep medical image segmentation. IEEE transactions on medical imaging, 39(12):38683878, 2020. 2 [54] Balamurali Murugesan, Julio Silva-Rodrıguez, Ismail Ben Ayed, and Jose Dolz. Robust calibration of large visionlanguage adapters. In Computer Vision ECCV 2024, pages 147165, Cham, 2025. Springer Nature Switzerland. 2 [55] Phan Ngoc Lan, Nguyen Sy An, Dao Viet Hang, Dao Van Long, Tran Quang Trung, Nguyen Thi Thuy, and Dinh Viet Sang. Neounet: Towards accurate colon polyp segmentation and neoplasm detection. In International Symposium on Visual Computing, pages 1528. Springer, 2021. 6, 13, 14 [56] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018. 2, 6, 15, 21, 22, 23, 24 [57] OpenAI. GPT-5 System Card. Technical report, OpenAI, 2025. Accessed: 2025-08-10. 6 [58] Qingtao Pan, Zhengrong Li, Guang Yang, Qing Yang, and Bing Ji. Evivlm: When evidential learning meets vision language model for medical image segmentation. IEEE Transactions on Medical Imaging, pages 11, 2025. 18, 19 [59] Kanchan Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, and Bishesh Khanal. Exploring transfer learning in medical image segmentation using vision-language models. In Proceedings of The 7nd International Conference on Medical Imaging with Deep Learning, pages 11421165. PMLR, 2024. 2, 3 [60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 2, 3, 8 [61] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with contextIn Proceedings of the IEEE/CVF conaware prompting. ference on computer vision and pattern recognition, pages 1808218091, 2022. 3, 6, 7, 15, 21, 22, 23, [62] Hamza Rasaee, Taha Koleilat, and Hassan Rivaz. Text-prompted multi-organ Groundingdino-us-sam: segmentation in ultrasound with lora-tuned vision-language models. arXiv preprint arXiv:2506.23903, 2025. 3 [63] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. 1, 2, 6, 15, 21, 22, 23, 24 [64] Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey Miller, Hassan Rivaz, Marta Kersten-Oertel, and Yiming Xiao. Textsam-eus: Text prompt learning for sam to accurately segment pancreatic tumor in endoscopic ultrasound. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 948957, 2025. 3 [65] Nima Tajbakhsh, Suryakanth Gurudu, and Jianming Liang. Automated polyp detection in colonoscopy videos using shape and context information. IEEE transactions on medical imaging, 35(2):630644, 2015. 6, 13, 14 [66] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):19, 2018. 6, 13, [67] Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano Mancini, and Zeynep Akata. Probvlm: Probabilistic adapter In Proceedings of the for frozen vison-language models. IEEE/CVF International Conference on Computer Vision, pages 18991910, 2023. 3 [68] David Vazquez, Jorge Bernal, Javier Sanchez, Gloria Fernandez-Esparrach, Antonio Lopez, Adriana Romero, Michal Drozdzal, and Aaron Courville. benchmark for endoluminal scene segmentation of colonoscopy images. Journal of healthcare engineering, 2017(1):4037190, 2017. 6, 13, 14 [69] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: ClipIn Proceedings of driven referring image segmentation. the IEEE/CVF conference on computer vision and pattern recognition, pages 1168611695, 2022. 3 [70] Hallee Wong, Marianne Rakic, John Guttag, and Adrian Dalca. Scribbleprompt: fast and flexible interactive segmentation for any biomedical image. In European Conference on Computer Vision, pages 207229. Springer, 2024. 3 11 [82] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free In European Conference on Comdense labels from clip. puter Vision, pages 696712. Springer, 2022. 2, 3 [83] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, pages 1681616825, 2022. 3 [84] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 130(9):23372348, 2022. 3 [85] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: nested u-net architecture for medical image segmentation. In International workshop on deep learning in medical image analysis, pages 311. Springer, 2018. 2, 6, 15, 21, 22, 23, 24 [86] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1117511185, 2023. 3, 6, 7, 15, 21, 22, 23, 24 [71] Hallee Wong, Jose Javier Gonzalez Ortiz, John Guttag, and Adrian Dalca. Multiverseg: Scalable interactive segmentation of biomedical imaging datasets with in-context guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2096620980, 2025. [72] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2945 2954, 2023. 3, 6, 7, 15, 21, 22, 23, 24 [73] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1815518165, 2022. 2, 3 [74] Wenqian Ye, Yunsheng Ma, Xu Cao, and Kun Tang. Mitigating transformer overconfidence via lipschitz regularization. In Uncertainty in Artificial Intelligence, pages 24222432. PMLR, 2023. 2 [75] Maxime Zanella and Ismail Ben Ayed. Low-rank few-shot In Proceedings of adaptation of vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15931603, 2024. 3 [76] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Matthew P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs, 2024. 2, 3, 7, 8, 14 [77] Yuxuan Zhang, Tianheng Cheng, Lianghui Zhu, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Evf-sam: Early vision-language fusion for text-prompted segment anything model. arXiv preprint arXiv:2406.20076, 2024. 2 [78] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, et al. Biomedparse: biomedical foundation model for image parsing of everything everywhere all at once. arXiv preprint arXiv:2405.12971, 2024. [79] Yidong Zhao, Changchun Yang, Artur Schweidtmann, and Qian Tao. Efficient bayesian uncertainty estimation for nnuIn International Conference on Medical Image Comnet. puting and Computer-Assisted Intervention, pages 535544. Springer, 2022. 18, 19 [80] Zihao Zhao, Yuxiao Liu, Han Wu, Mei Wang, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, et al. Clip in medical imaging: survey. Medical Image Analysis, page 103551, 2025. 2 [81] Yi Zhong, Mengqiu Xu, Kongming Liang, Kaixin Chen, and Ming Wu. Ariadnes thread: Using text prompts to improve segmentation of infected areas from chest x-ray images. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 724733. Springer, 2023. 3, 6, 15, 18, 19, 21, 22, 23, 24 12 MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Datasets Overview Our evaluation comprises diverse collection of medical image segmentation datasets, covering six organs and five imaging modalities. We organize our benchmarks into three settings: data efficiency, fully supervised, and domain generalization. The data efficiency and fully supervised evaluation includes BUSI [1], BTMRI [14], ISIC [17, 66], Kvasir-SEG [36], QaTa-COV19 [18], and EUS [35], which collectively span ultrasound, MRI, dermatoscopy, endoscopy, and X-ray modalities. For domain generalization, we employ ten diverse datasets to provide outof-domain (OOD) samples: BUSUC [33], BUSBRA [28], BUID [4], UDIAT [8], BRISC [24], UWaterlooSkinCancer [2, 27], CVC-ColonDB [65], CVC-ClinicDB [6], CVC-300 [68], and BKAI [55], each introducing distinct appearance shifts across imaging devices, acquisition protocols, and anatomical domains. This combination enables systematic analysis of segmentation robustness across both intraand cross-domain distributions. Dataset statistics, modalities, and split details are summarized in Table S1. Importantly, our method does not use the validation sets; however, other methods, such as LViT [50], rely on them during training to select the best checkpoints. In our framework, models are trained on the training split, and we select the last epoch checkpoint to evaluate on the test split. For domain generalization, the OOD datasets are never seen during training; we evaluate directly on their test sets without any finetuning or adaptation. B. Computational Cost Analysis Table S2 summarizes the computational complexity of all compared methods, including parameter footprint, FLOPs, and inference time. All measurements are computed on the same BUSI [1] test set under identical hardware conditions. Although our MedCLIPSeg framework typically employs sampling strategy during inference, the computational cost reported in Table S2 corresponds to the configuration where we use only single sampled forward pass. This ensures fair, per-sample comparison with other methIn general, MedCLIPSeg exhibits fair, competods. itive computational profile with state-of-the-art segmentation performance. C. Text Prompt Generation We introduce scalable strategy for automated caption generation in unpaired datasets without relying on visionlanguage models, detailed in Algorithm 1. Instead of requiring imagetext pairs, we query large language model once per dataset to produce small set of generic caption templates with placeholders for attributes such as class, location, number, shape, and color. Using lightweight image and mask processing, these attributes are automatically extracted and filled into the templates, enabling clinician-style descriptive captions. Algorithm 1 Text Prompt Generation 1: Inputs: Images, Masks (optional), Labels (optional) 2: Goal: Produce one caption per image using LLM templates + simple attributes 3: Step 1: Templates (once per dataset) 4: Query an LLM to write few short templates with placeholders: {class}, {location}, {number}, {shape}, {color}. 5: Provide separate normal and lesion templates. 6: Step 2: Attribute extraction (per image) 7: for each image in Images do 8: 9: if corresponding mask exists then Class: use label if available; otherwise lesion if uniform. Location: coarse region from mask (e.g., upper/lower/left/right/center). Number: gle/multiple). Shape: coarse shape cue (e.g., round/irregular). Color: overall brightness/tone relative to background. count connected components (sinelse Mark as normal (no lesion mask). 10: 11: 12: 13: 14: 15: end if 16: 17: end for 18: Step 3: Fill templates (per image) 19: for each image do if normal then 20: 21: 22: else Choose normal template; save caption. 23: Choose lesion template; replace placeholders with extracted attributes; save caption. end if 24: 25: end for 26: Output: list of paired (image, text, mask) samples ready for training or evaluation.. 13 Table S1. Summary of medical datasets: Overview of datasets used in the data-efficiency, fully supervised, and domain generalization benchmarks. For data-efficiency experiments, values in parentheses under Train and Validation indicate the number of samples corresponding to (10%, 25%, 50%) of the full splits; other sections report full counts."
        },
        {
            "title": "Organ",
            "content": "BUSI [1] BTMRI [14] ISIC [17, 66] Kvasir-SEG [36] QaTa-COV19 [18] EUS [35] BUSI [1] BTMRI [14] ISIC [17, 66] Kvasir-SEG [36] QaTa-COV19 [18] EUS [35] Data-Efficiency Evaluation (62, 156, 312) (273, 684, 1,369) (80, 202, 404) (80, 200, 400) (571, 1,429, 2,858) (2,631, 6,579, 13,159) (7, 19, 39) (132, 330, 660) (9, 22, 45) (10, 25, 50) (142, 357, 714) (175, 439, 879)"
        },
        {
            "title": "Fully Supervised",
            "content": "624 2,738 809 800 5,716 26,318 78 1,321 90 100 1,429 1,"
        },
        {
            "title": "Domain Generalization",
            "content": "BUSUC [33] BUSBRA [28] BUID [4] UDIAT [8] BRISC [24] UWaterlooSkinCancer [2, 27] CVC-ColonDB [65] CVC-ClinicDB [6] CVC-300 [68] BKAI [55] 567 1,311 162 113 4,000 132 20 490 6 799 122 282 35 25 1,000 0 0 61 0 100 78 1,005 379 100 2,113 10,090 78 1,005 379 100 2,113 10,090 122 282 35 25 1,000 41 360 61 60 Ultrasound MRI Dermatoscopy Endoscopy X-ray Ultrasound Ultrasound MRI Dermatoscopy Endoscopy X-ray Ultrasound"
        },
        {
            "title": "Breast\nBreast\nBreast\nBreast\nBrain\nSkin\nColon\nColon\nColon\nColon",
            "content": "D. Detailed Hyperparameters E. Prompt Designs Overview All models were trained using UniMedCLIP ViT-B/16 [43] as the vision backbone and PubMedBERT [76] as the text encoder. We employed the Adam [44] optimizer with learning rate of 3 104, batch size of 24, and cosine annealing learning rate schedule. The segmentation objective combines Dice and binary cross-entropy losses with equal weighting (λSegLSeg = λSegLDice + λSegLBCE with λSeg = 0.5), while the CLIP-based contrastive alignment term was weighted by λSoftCon = 0.1. The probabilistic attention weighting factor was fixed at β = 2.35 across all experiments. All runs were performed on single NVIDIA A100 GPU (40 GB). Due to the relatively large size of the EUS dataset, we observed convergence within the first 10 epochs. Consequently, EUS experiments were limited to 10 epochs, whereas all other datasets were trained for 100 epochs to ensure full convergence under both dataefficient and domain-generalization settings. No validation set was used, and the checkpoint at the last epoch was utilized. Table S3 provides an overview of the text prompt configurations used in our ablation experiments (see Section 4.5). Each design type represents distinct linguistic variation that probes the models sensitivity to descriptive accuracy, spatial specificity, verbosity, and potential contradictions. By comparing these designs, we evaluate how differences in text formulation, from concise and spatially informative prompts to noisy or underspecified ones, influence segmentation performance and generalization across datasets. F. 3D applicability MedCLIPSeg naturally extends to 3D segmentation without modifying the core method, when given 3D VLM backbone. We showcase this by using M3D-CLIP [5] to replace the 2D image encoder with 3D one. 3D segmentations are obtained by computing the dot product between the global text token and 3D voxel features, followed by trilinear interpolation for upscaling. We validate this on the CHAOS CT Liver dataset [39] following the M3D-Seg data split and achieve DSC of 88.72%, demonstrating that 14 Table S2. Comparison of computational complexity between different methods. Models using text or multimodal supervision are marked with in the Text? column. Model Text? Params. (M) FLOPs (G) Inf. Time (s) UNet [63] UNet++ [85] DeepLabv3 [12] AttnUNet [56] nnUNet [34] Swin-UNet [9] TransUNet [11] LViT [50] Ariadnes Thread [81] CLIPSeg [51] DenseCLIP [61] ZegCLIP [86] SAN [72] MaPLe [42] MaPLe [42] + Decoder VLSM-Adapter [19] CausalCLIPSeg [13] CAT-Seg [16] MedCLIPSeg (Ours) 14.8 74.5 57.6 34.9 19.1 82.3 105 29.7 44.0 1.1 89.7 10.6 8.2 7.1 8.2 5.0 57.2 34.8 18. 50.3 94.6 38.4 101.9 412.7 67.3 56.7 54.1 49.8 66.8 66.7 67.6 90.0 66.9 67.3 68.4 158.3 69.7 73.6 0.55 0.81 1.16 0.77 1.55 1.38 1.22 1.74 2.39 1.35 1.50 1.68 1.46 1.45 1.75 1.32 4.39 2.34 1.51 MedCLIPSeg generalizes beyond 2D. We report the average runtime per volume over 20 test volumes in the last column of Table S4, confirming practical feasibility for 3D settings, with further 3D analysis left for future work. G. Inference-time MC sampling cost Table S4 shows that using 510 MC samples only marginally affects DSC and uncertainty estimates. 2D runtimes are reported as the average inference time per batch of 32 images, measured over 1,000 test images, all on single NVIDIA A100 GPU (40GB RAM). This demonstrates suitability for practical clinical settings with substantially reduced computational cost compared to 30 MC samples. In endoscopic video settings requiring 2530 FPS, 5 MC samples configuration is practical for real-time inference. H. Effect of λSof tCon Figure S1 illustrates the effect of the patch-level contrastive weight λSof tCon on domain generalization. We find that λSof tCon = 0.1 provides the optimal balance between in-distribution (ID) and out-of-distribution (OOD) performance, yielding the highest harmonic mean (HM) score. When λSof tCon = 0, the contrastive loss is removed entirely, leading to degradation in both ID and OOD performance due to the absence of semantic alignment across imagetext patches. Increasing λSof tCon beyond 0.1 results in marginal performance drops, suggesting that excessive contrastive weighting can overconstrain the feature space. These results highlight the importance of moderate patchFigure S1. Effect of λSof tCon on Domain Generalization level contrastive regularization in maintaining both semantic consistency and domain robustness. I. Effect of Gating Initialization Table S5 examines the impact of the gating parameter initialization on segmentation performance across indistribution (ID) and out-of-distribution (OOD) domains. We observe that initializing the gate with sigmoid(0) yields the best overall results, achieving the highest ID, OOD, and harmonic mean (HM) DSC scores. smaller initialization value (sigmoid(-0.5)) makes the gate overly suppressive early in training, limiting information flow from the probabilistic branch and reducing generalization. Conversely, larger initialization (sigmoid(0.5)) biases the fusion toward the probabilistic output too soon, leading to mild overfitting. The balanced initialization at sigmoid(0) thus provides stable midpoint, enabling adaptive modulation between deterministic and probabilistic pathways throughout training. J. Effect of Two-way Mechanism Table S6 evaluates the contribution of the two-way crossmodal attention mechanism to segmentation performance. In the Vision First variant, cross-modal features are first computed for the vision tokens as the query in AttnPVL and then refined in the subsequent text-to-image interaction, while in Text First, this order is reversed. Among these, initializing fusion with the visual stream (Vision First) achieves the best results across in-distribution (ID), out-of-distribution (OOD), and harmonic mean (HM) DSC scores. Removing the two-way mechanism (None) or prioritizing text-driven conditioning (Text First) both lead to noticeable drops in OOD generalization, indicating that early visual grounding provides stronger foundation for subsequent text-guided refinement. This suggests that 15 Table S3. Prompt design taxonomy with examples. Each configuration illustrates how wording choices (conciseness, spatial detail, contradictions, and noise) affect the semantics supplied to the model. Design Type Description Style Example (Normal) Example (Tumor) Original Balanced, accurate Underdescriptive Minimal, label-only Overdescriptive Verbose, redundant Contradictory Incorrect/Conflicting info Missing Location No spatial info The breast appears normal with no signs of lesions. Normal breast. The breast tissue appears entirely healthy, with homogeneous echotexture throughout. Normal breast tissue with visible lesion in the image. The breast appears normal with no signs of lesions. malignant tumor is present in the upper-left region of the breast. Tumor present. clearly defined malignant tumor with irregular boundaries located in the upper-left quadrant. Malignant tumor detected, but breast appears completely normal. malignant tumor is detected in the breast. Table S4. Performance-cost tradeoff under MC sampling Samples Runtime (s/batch) FPS HM DSC (%) HM Spearman (%) 3D Runtime (s/vol) 5 10 20 30 1.78 2.20 4.08 6.01 24.92 14.35 7.64 5.23 83.52 83.66 83.71 83.76 83.07 83.44 83.52 83. 0.98 1.03 1.97 2.90 (as in Attention Pooling) leads to noisier supervision due to bias toward high-attention regions, while relying solely on the [CLS] token underutilizes spatial information critical for dense prediction. Thus, average pooling yields the most consistent global-text alignment and best domain generalization. Table S5. Effect of the gating initialization Table S7. Effect of the pooling strategies Gating Init. (g) ID DSC (%) OOD DSC (%) HM DSC (%) Contrastive Pooling ID DSC (%) OOD DSC (%) HM DSC (%) sigmoid(-0.5) sigmoid(0) sigmoid(0.5) 88.79 89.11 88.93 74.65 79.02 77.51 81.11 83.76 82. [CLS] Attention Pooling Average Pooling 88.89 88.73 89.11 78.28 75.60 79.02 83.25 81.64 83.76 vision-first bidirectional fusion promotes more stable multimodal alignment, allowing the model to capture anatomical context before integrating semantic cues from text. Table S6. Effect of the two-way attention mechanism Two-way Mechanism ID DSC (%) OOD DSC (%) HM DSC (%) None Text First Vision First 88.71 88.55 89.11 77.71 76.99 79.02 82.85 82.37 83.76 K. Effect of Contrastive Pooling Mechanism Table S7 analyzes the impact of different pooling strategies used in the contrastive loss. Among the three variants, Average Pooling achieves the highest in-distribution (ID), out-of-distribution (OOD), and harmonic mean (HM) DSC scores. This indicates that averaging patch-level embeddings provides more balanced and stable global representation for contrastive learning compared to [CLS] or attention-based pooling. Removing uniform averaging L. Effect of Upscaling Blocks Table S8 examines how varying the number of upscaling layers in the decoder affects segmentation performance. Using two upscaling blocks yields the best balance across indistribution (ID), out-of-distribution (OOD), and harmonic mean (HM) DSC scores. single block (1) limits spatial resolution recovery, resulting in coarse boundary predictions, while deeper configurations (3) introduce oversmoothing and reduce OOD robustness. The two-block design thus offers the optimal trade-off between preserving fine structural details and maintaining stable feature generalization across domains. Table S8. Effect of the number of upscaling layers Num. Upscale ID DSC (%) OOD DSC (%) HM DSC (%) 1 2 88.73 89.11 88.64 75.74 79.02 74.99 81.72 83.76 81.24 16 M. Effect of Adapter Dimension (Ds) Table S10. Effect of confidence-weighted attention mechanism Table S9 evaluates the impact of the shared dimensionality in the probabilistic vision-language (PVL) adapters. The best performance is achieved at dimension of 256, balancing both in-distribution (ID) and out-of-distribution (OOD) segmentation accuracy. Smaller adapter sizes (e.g., 64 or 128) underfit the cross-modal representations, limiting their ability to capture nuanced semantic alignments between visual and textual features. Conversely, excessively large dimensions (e.g., 512) tend to overfit the training distribution, slightly reducing OOD generalization. The 256dimensional configuration thus provides the optimal tradeoff between expressiveness and regularization. Table S9. Effect of the shared dimension in the PVL adapters Adapter Dim. (Ds) ID DSC (%) OOD DSC (%) HM DSC (%) 64 128 192 256 512 87.76 88.68 88.93 89.11 88.56 74.63 76.44 76.01 79.02 77. 80.66 82.11 81.96 83.76 82.92 N. Effect of Different Confidence-Weighted Attention Mechanisms Table S10 examines the impact of incorporating uncertainty into the attention computation in different manners. Our difference-based formulation yields the best performance across in-distribution (ID), out-of-distribution (OOD), and harmonic mean (HM) DSC scores. This approach adjusts attention weights by directly penalizing high-variance (lowconfidence) regions, encouraging the model to focus on more reliable feature correspondences. For comparison, the weight scaling variant applies an uncertainty-dependent multiplicative attenuation to the attention matrix: Ascaled = softmax(Sµ) (cid:0)1 + βSσ (cid:1), where denotes element-wise division. This strategy offers slightly lower gains compared to the proposed difference-based approach, while omitting uncertainty entirely reduces robustness under domain shifts. These results highlight that explicitly encoding confidence into attention promotes more stable and trustworthy segmentation performance. O. Error vs. Uncertainty Correlation The models uncertainty maps exhibit strong correlation with segmentation errors, as shown in Fig. S5. Across all ID and OOD datasets, the Pearson correlations between uncertainty and error are consistently high: 0.9248 (Breast Mechanism ID DSC (%) OOD DSC (%) HM DSC (%) None Scaling Difference 88.97 88.92 89.11 77.49 78.78 79. 82.83 83.54 83.76 Ultrasound), 0.9921 (Polyp Endoscopy), 0.9201 (Skin Dermatoscopy), and 0.9885 (Brain MRI), all with < 0.001. These strong correlations indicate that regions of elevated uncertainty reliably align with areas of higher prediction error, confirming that the uncertainty estimates meaningfully reflect model confidence and can support downstream tasks such as boundary refinement, error correction, and active learning. P. Deterministic vs. Probabilistic Confidence S2, As shown in Fig. the probabilistic variant of MedCLIPSeg demonstrates superior handling of both overconfidence and underconfidence compared to its deterministic counterpart. Explicitly modeling predictive uncertainty suppresses spurious activations in non-lesion regions (reducing false positives) while recovering missed lesion boundaries (reducing false negatives). This leads to more balanced confidence calibration, smoother segmentation contours, and lower combined error rates across diverse ultrasound datasets. Q. Additional Visualization Examples We further evaluate MedCLIPSeg under cross-domain conditions using polyp endoscopy and breast ultrasound datasets. As shown in Figs. S3 and S4, the model maintains strong segmentation quality despite noticeable domain shifts in texture, lighting, and instrument artifacts. Uncertainty maps remain concentrated along polyp and tumor boundaries, reflecting well-calibrated confidence and robust generalization to unseen endoscopic and ultrasound environments. R. Effect of Supervised Segmentation Table S11 highlights the critical role of supervised segmentation annotations in medical image segmentation. When we remove the segmentation loss LSeg and train the model solely with the soft patch-level contrastive objective LSoftCon, performance collapses to 20% DSC indistribution and below 13% out-of-distribution. Although purely contrastive or self-supervised objectives can be sufficient for natural-image segmentation, where object boundaries are often distinct and semantic categories are well separated and diverse, they are fundamentally insufficient 17 Figure S2. FP/FN comparison between deterministic and probabilistic MedCLIPSeg. Figure S3. Breast Tumor Ultrasound Segmentation and uncertainty visualizations. Uncertainty peaks along lesion boundaries and remains consistent across breast ultrasound datasets, indicating reliable calibration and generalization. in the medical domain. Medical boundaries are subtle, low-contrast, and frequently ambiguous, with fine-grained structures that require pixel-accurate supervision to disambiguate anatomy from imaging artifacts and surrounding tissues. Incorporating LSeg provides this necessary spatial guidance, enabling the model to learn clinically meaningful decision boundaries and yielding dramatic gains of more than +69% DSC ID and +66% DSC OOD. These results clearly demonstrate that segmentation annotations remain indispensable for achieving reliable and generalizable medical image segmentation performance. S. Additional Baselines We include several additional baselines: non-VLM nnUNet with checkpoint ensembling [79], Ariadnes Thread [81], EviVLM [58], VLSM-Ensemble [20], and our frameworks variant with deterministic adapters and an evidential 18 Figure S4. Polyp Endoscopy Segmentation and uncertainty visualizations. Uncertainty peaks along lesion boundaries and remains consistent across polyp endoscopy datasets, indicating reliable calibration and generalization. Table S12. Domain generalization with more baselines (%) Method ID OOD HM DSC Spearman DSC Spearman DSC Spearman Ariadnes Thread [81] EviVLM [58] VLSM-Ensemble [20] 68.25 84.06 87.36 nnUNet + Ensembling [79] 86.50 MedCLIPSeg (Evidential) 88.18 89.11 MedCLIPSeg (Ours) 66.74 78.49 87.57 27.24 54.47 63.24 74.20 76.61 79.02 55.22 76.79 80. 38.94 66.10 73.37 79.80 81.99 83.76 60.44 77.63 83.84 Figure S5. Relationship between predictive uncertainty and pixellevel segmentation error across imaging domains. Table S11. Effect of the segmentation annotations consistently outperforms the others in both aspects. Table S13. Domain generalization with different metrics (%) Method ID OOD HM SAN [68] VLSM-Adapter [18] CAT-Seg [15] (86.9, 88.9, 84.5) (74.3, 83.9, 69.9) (80.1, 86.3, 76.5) (88.5, 87.9, 85.8) (80.3, 80.8, 73.3) (84.2, 84.2, 79.0) (87.2, 89.1, 86.1) (81.1, 82.6, 74.6) (84.0, 85.7, 79.9) MedCLIPSeg (Ours) (89.9, 90.8, 89.1) (86.2, 80.7, 79.0) (88.0, 85.5, 83.8) LSeg? ID DSC (%) OOD DSC (%) HM DSC (%) T. Additional Evaluation Metrics 19.84 89.11 12.68 79.02 15.47 83. Table S13 reports (Sensitivity, Specificity, F1) for the top three baselines under domain generalization, supporting improved boundary localization by MedCLIPSeg under domain shifts. mask head at the end for pixel-wise uncertainty estimation without Monte Carlo (MC) sampling. We evaluate them all for robustness and uncertainty quality (Spearman correlation with errors) in Table S12, and show that MedCLIPSeg U. Sample Text Prompts Below, we provide one representative text prompt from each of the 16 datasets: 19 V. Per-dataset Efficiency Results Tables S14, S15, S16, and S17 present detailed segmentation performance for each dataset across varying levels of labeled supervision (10%, 25%, 50%, and 100%). We report Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD) scores to evaluate both volumetric overlap and boundary accuracy. This breakdown highlights the data-efficiency behavior of different model families, ranging from unimodal CNN and transformer baselines to text-driven and CLIP-based approaches. MedCLIPSeg consistently achieves the highest or second-highest performance across nearly all datasets and label fractions, demonstrating its robustness to annotation scarcity and strong cross-domain adaptability. one small pink round polyp, located in right of the image pituitary tumor is present in the center region of the brain. Presence of benign lesion located at the upper section. Detected malignant tumor positioned towards the center side. One small rectangle-shaped regular tumor at the left in the breast ultrasound image. Findings indicate benign tumor situated in the center area. no irregularities detected on MRI scan one small white triangular polyp, located in center of the image one small pink circle polyp, located in center of the image one small white circle polyp, located in center of the image Bilateral pulmonary infection involving two regions with involvement of all left lung and all right lung Endoscopic ultrasound showing heterogeneous mass in center one medium red rectangular skin melanoma which is spot with dark speckles located in top right of the image one medium white round polyp, located in left of the image Detected malignant lesion located at the center area. Presence of red skin melanoma positioned in the center part. Table S14. Per-dataset segmentation with 10% Labeled Data: This table reports DSC and NSD values (%) across six medical image segmentation benchmarks. All baseline methods are trained using 10% of the ground-truth annotations. Best results are in bold, and second-best are underlined. Method BUSI BTMRI ISIC Kvasir-SEG QaTa-COV EUS DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD Unimodal Approaches UNet [63] 49.33 52. 64.49 69.12 79.43 81.93 40.66 44. 71.41 77.18 60.40 62.07 UNet++ [85] 53. 57.58 62.23 66.11 82.83 85.54 46. 49.28 69.22 74.99 67.96 69.00 DeepLabv3 [12] 42.45 45.64 61.96 66.47 84.62 87. 45.14 48.32 68.51 74.38 65.22 66. AttnUNet [56] 54.66 58.17 58.68 62.77 85. 88.08 42.46 45.68 70.86 76.37 64. 66.44 nnUNet [34] 56.32 60.78 81.44 86. 88.67 91.61 74.15 78.48 70.20 75. 69.94 71.14 Swin-UNet [9] 39.87 45.20 41. 45.83 81.04 84.12 36.84 43.05 62. 70.43 57.14 58.81 TransUNet [11] 39.61 43. 55.04 58.65 84.43 87.30 47.48 51. 54.50 61.29 35.09 36.29 LViT [50] 63. 65.97 52.48 54.80 74.53 75.48 51. 53.10 76.52 82.23 80.57 81.21 Ariadnes Thread [81] 35.51 36.39 58.70 60.01 66.28 67. 74.98 76.40 59.86 63.13 76.12 76. Generic Text-driven Approaches CLIP-Based Approaches CLIPSeg [51] 65.65 68.40 72. 76.42 85.18 86.26 67.81 70.53 74. 82.16 81.90 82.75 DenseCLIP [61] 55.09 57. 60.56 61.87 88.54 89.56 73.73 75. 66.95 73.86 62.18 63.48 ZegCLIP [86] 46. 48.13 70.74 73.46 79.16 80.07 68. 70.29 69.19 75.88 33.84 34.49 SAN [72] 66.99 69.56 77.92 81.76 89.20 90. 66.79 69.36 72.36 78.78 71.51 72. MaPLe [42] 55.70 57.98 70.14 71.57 86. 87.50 59.82 62.05 67.11 74.24 58. 59.16 MaPLe [42] + Decoder 60.50 63.49 73.18 76. 84.47 85.57 66.67 69.28 76.95 84. 87.11 87.97 VLSM-Adapter [19] 63.85 66.60 73. 76.81 86.81 87.95 71.74 74.44 74. 82.06 76.31 77.14 CausalCLIPSeg [13] 51.29 53. 73.97 77.27 84.89 85.86 60.35 62. 70.58 77.18 82.61 84.17 CAT-Seg [16] 68. 70.66 77.15 80.38 87.95 89.01 75. 77.60 76.19 82.71 87.82 88.65 MedCLIPSeg (Ours) 68.66 71.35 79.07 82.71 90.35 91. 77.21 79.53 79.73 86.27 91.59 92. 21 Table S15. Per-dataset segmentation with 25% Labeled Data: This table reports DSC and NSD values (%) across six medical image segmentation benchmarks. All baseline methods are trained using 25% of the ground-truth annotations. Best results are in bold, and second-best are underlined. Method BUSI BTMRI ISIC Kvasir-SEG QaTa-COV19 EUS DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD Unimodal Approaches UNet [63] 56. 59.90 70.40 74.98 82.44 85.68 41. 44.93 68.23 73.05 57.23 58.45 UNet++ [85] 56.68 60.36 76.46 81.07 84.35 87. 62.23 65.56 43.36 48.05 72.07 73. DeepLabv3 [12] 55.03 58.85 73.16 78.77 87. 90.08 54.34 58.04 66.98 72.75 55. 56.10 AttnUNet [56] 62.55 66.75 64.24 68. 85.72 88.59 55.17 58.94 55.00 60. 67.16 68.65 nnUNet [34] 62.28 66.29 83. 89.12 89.85 92.76 80.96 84.91 72. 78.41 71.26 72.44 Swin-UNet [9] 37.56 42. 66.20 71.85 80.35 83.33 42.49 47. 53.94 60.36 47.59 49.71 TransUNet [11] 46. 49.91 58.33 62.24 86.04 88.82 51. 55.95 50.09 56.19 39.33 40.57 LViT [50] 62.31 64.64 76.21 79.53 81.02 81. 72.52 74.43 78.99 84.55 82.94 83. Ariadnes Thread [81] 39.01 40.04 58.70 60.01 68. 69.45 75.71 76.11 60.06 64.29 76. 77.14 Generic Text-driven Approaches CLIP-Based Approaches CLIPSeg [51] 70.35 73. 74.91 78.40 86.45 87.59 73.67 76. 78.77 85.74 85.72 86.72 DenseCLIP [61] 58. 59.94 64.97 67.32 88.98 89.69 78. 80.40 65.92 72.66 64.96 66.19 ZegCLIP [86] 49.86 51.88 73.18 76.11 79.39 80. 71.95 73.75 72.99 80.09 87.37 88. SAN [72] 64.43 67.09 81.15 84.96 90. 91.69 77.48 79.76 74.35 80.57 68. 69.40 MaPLe [42] 63.94 66.42 72.87 73. 87.89 88.89 71.68 73.85 67.43 74. 65.39 65.93 MaPLe [42] + Decoder 67.12 69.75 79. 83.74 87.89 88.98 74.96 77.57 79. 86.15 88.58 89.39 VLSM-Adapter [19] 63.48 66. 79.50 83.33 89.44 90.52 76.45 78. 78.14 84.67 78.74 79.54 CausalCLIPSeg [13] 57. 59.67 76.15 79.57 85.98 86.93 68. 70.35 76.17 82.95 83.86 84.54 CAT-Seg [16] 73.08 75.76 80.07 83.59 89.61 90. 80.11 82.50 79.12 85.45 84.73 85. MedCLIPSeg (Ours) 77.73 80.29 83.93 87.69 91. 92.04 84.21 86.46 81.83 88.01 91. 92.61 22 Table S16. Per-dataset segmentation with 50% Labeled Data: This table reports DSC and NSD values (%) across six medical image segmentation benchmarks. All baseline methods are trained using 50% of the ground-truth annotations. Best results are in bold, and second-best are underlined. Method BUSI BTMRI ISIC Kvasir-SEG QaTa-COV19 EUS DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD Unimodal Approaches UNet [63] 60.46 64.38 81.47 86.06 86.85 89. 72.12 75.51 66.36 71.15 62.38 63. UNet++ [85] 63.63 67.16 80.21 84.61 88. 91.23 74.53 77.72 63.80 67.71 68. 69.43 DeepLabv3 [12] 55.83 60.21 77.82 83. 86.14 89.06 67.72 71.90 64.86 70. 59.12 60.39 AttnUNet [56] 59.81 63.46 72. 77.24 87.28 90.48 74.79 78.66 64. 69.95 68.52 69.99 nnUNet [34] 68.15 71. 85.30 91.18 90.38 93.21 83.45 87. 74.44 79.89 71.42 72.50 Swin-UNet [9] 41. 48.11 57.76 64.96 85.52 89.14 50. 55.25 53.67 61.58 46.51 48.44 TransUNet [11] 41.95 45.95 62.60 67.63 86.43 89. 54.78 59.41 52.98 58.79 32.58 34. LViT [50] 62.74 65.29 78.89 82.17 89. 90.20 78.63 80.54 80.21 85.51 83. 84.36 Ariadnes Thread [81] 48.30 49.35 61.76 63. 68.45 69.37 76.24 77.66 63.00 65. 76.12 76.66 Generic Text-driven Approaches CLIP-Based Approaches CLIPSeg [51] 71. 74.15 75.63 79.09 88.47 89.57 76. 78.58 80.17 87.05 86.12 87.02 DenseCLIP [61] 64.62 65.78 68.83 70.21 89.17 90. 80.16 82.29 63.93 70.74 65.81 67. ZegCLIP [86] 63.76 65.79 73.54 76.35 80. 81.47 74.98 76.83 74.90 82.17 89. 90.21 SAN [72] 71.53 74.16 82.08 85. 91.06 92.09 80.03 82.25 75.74 81. 72.36 72.84 MaPLe [42] 65.58 68.06 74. 75.66 88.51 89.51 76.56 78.71 70. 77.35 72.68 73.42 MaPLe [42] + Decoder 73.70 76. 81.87 85.49 89.79 90.89 79.95 82. 80.57 87.15 90.39 91.30 VLSM-Adapter [19] 69. 72.51 82.47 86.46 91.35 92.42 82. 85.59 79.33 85.50 79.22 80.13 CausalCLIPSeg [13] 68.48 70.82 75.69 79.08 88.11 89. 73.26 75.20 76.76 83.07 85.00 85. CAT-Seg [16] 72.95 75.54 83.48 85.14 90. 91.48 83.85 85.17 80.87 87.17 88. 89.17 MedCLIPSeg (Ours) 81.48 84.06 85.93 89. 91.97 93.03 88.18 90.36 82.97 89. 92.57 93.36 23 Table S17. Per-dataset segmentation with 100% Labeled Data: This table reports DSC and NSD values (%) across six medical image segmentation benchmarks. All baseline methods are trained in fully supervised manner using ground-truth annotations. Best results are in bold, and second-best are underlined. Method BUSI BTMRI ISIC Kvasir-SEG QaTa-COV19 EUS DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD DSC NSD Unimodal Approaches UNet [63] 70.04 73.88 86.06 90.71 89. 92.06 80.26 83.53 76.97 82.32 68. 69.92 UNet++ [85] 67.54 71.15 83.30 87. 89.36 92.17 84.81 88.07 73.52 78. 72.08 73.15 DeepLabv3 [12] 63.02 67.18 83. 89.31 76.34 80.24 82.00 85.73 69. 75.67 65.21 66.37 AttnUNet [56] 62.65 66. 85.24 89.76 89.00 92.03 78.59 81. 73.57 78.71 68.76 70.13 nnUNet [34] 76. 80.70 86.91 92.00 90.52 93.37 85. 89.29 75.43 81.00 73.27 74.09 Swin-UNet [9] 50.37 56.13 65.34 72.51 88.10 91. 58.69 63.87 69.69 72.43 57.96 59. TransUNet [11] 57.98 62.50 70.90 76.46 88. 91.00 58.69 63.87 68.74 71.99 58. 61.10 LViT [50] 75.32 77.99 81.41 84. 91.21 92.22 85.29 87.30 82.31 87. 84.53 85.23 Ariadnes Thread [81] 57.26 58.22 69. 71.40 68.37 69.30 77.42 78.79 70. 73.94 76.71 77.31 Generic Text-driven Approaches CLIP-Based Approaches CLIPSeg [51] 80.95 83.87 85.33 89.45 90.55 91. 81.98 84.61 81.76 87.30 88.66 89. DenseCLIP [61] 71.85 74.39 70.30 72.34 89. 90.32 79.32 81.37 65.84 72.72 68. 70.17 ZegCLIP [86] 72.08 74.45 76.65 79. 81.45 82.33 78.46 80.43 75.42 82. 89.83 90.54 SAN [72] 77.99 80.75 85. 89.14 91.39 92.41 83.16 85.23 76. 82.88 75.07 75.67 MaPLe [42] 66.37 68. 75.40 76.83 88.31 89.30 76.12 78. 70.40 77.52 70.98 71.75 MaPLe [42] + Decoder 80. 83.38 85.08 89.20 90.10 91.21 83. 85.96 81.86 88.16 88.65 89.55 VLSM-Adapter [19] 80.90 83.71 85.03 89.01 91.30 92. 85.89 88.34 81.15 87.10 78.82 79. CausalCLIPSeg [13] 76.11 78.70 81.71 85.30 89. 90.46 78.77 80.79 75.67 82.37 86. 87.59 CAT-Seg [16] 81.83 84.52 84.86 86. 91.27 92.34 86.43 88.83 82.82 88. 88.18 89.07 MedCLIPSeg (Ours) 85.72 88.35 88. 91.78 92.54 93.58 90.15 92.32 83. 89.17 92.11 92."
        }
    ],
    "affiliations": [
        "Concordia University, Montreal, Canada"
    ]
}