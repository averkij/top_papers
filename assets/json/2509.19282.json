{
    "paper_title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps",
    "authors": [
        "Bingnan Li",
        "Chen-Yu Wang",
        "Haiyang Xu",
        "Xiang Zhang",
        "Ethan Armand",
        "Divyansh Srivastava",
        "Xiaojun Shan",
        "Zeyuan Chen",
        "Jianwen Xie",
        "Zhuowen Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite steady progress in layout-to-image generation, current methods still struggle with layouts containing significant overlap between bounding boxes. We identify two primary challenges: (1) large overlapping regions and (2) overlapping instances with minimal semantic distinction. Through both qualitative examples and quantitative analysis, we demonstrate how these factors degrade generation quality. To systematically assess this issue, we introduce OverLayScore, a novel metric that quantifies the complexity of overlapping bounding boxes. Our analysis reveals that existing benchmarks are biased toward simpler cases with low OverLayScore values, limiting their effectiveness in evaluating model performance under more challenging conditions. To bridge this gap, we present OverLayBench, a new benchmark featuring high-quality annotations and a balanced distribution across different levels of OverLayScore. As an initial step toward improving performance on complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a curated amodal mask dataset. Together, our contributions lay the groundwork for more robust layout-to-image generation under realistic and challenging scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 8 2 9 1 . 9 0 5 2 : r OverLayBench: Benchmark for Layout-to-Image Generation with Dense Overlaps Bingnan Li1 Chen-Yu Wang1 Haiyang Xu1 Xiang Zhang1 Ethan Armand1 Divyansh Srivastava1 Xiaojun Shan1 Zeyuan Chen1 Jianwen Xie2 Zhuowen Tu1 1UC San Diego 2Lambda, Inc equal contribution Figure 1: Examples from OverLayBench with difficulty increasing from left to right."
        },
        {
            "title": "Abstract",
            "content": "Despite steady progress in layout-to-image generation, current methods still struggle with layouts containing significant overlap between bounding boxes. We identify two primary challenges: (1) large overlapping regions and (2) overlapping instances with minimal semantic distinction. Through both qualitative examples and quantitative analysis, we demonstrate how these factors degrade generation quality. To systematically assess this issue, we introduce OverLayScore, novel metric that quantifies the complexity of overlapping bounding boxes. Our analysis reveals that existing benchmarks are biased toward simpler cases with low OverLayScore values, limiting their effectiveness in evaluating model performance under more challenging conditions. To bridge this gap, we present OverLayBench, new benchmark featuring high-quality annotations and balanced distribution across different levels of OverLayScore. As an initial step toward improving performance on complex overlaps, we also propose CreatiLayout-AM, model fine-tuned on curated amodal mask dataset. Together, our contributions lay the groundwork for more robust layout-to-image generation under realistic and challenging scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench."
        },
        {
            "title": "Introduction",
            "content": "With the advancement of text-to-image generative models [Ramesh et al., 2021, Nichol et al., 2021, Rombach et al., 2022, Chen et al., 2023, Xue et al., 2024], there has been growing interest Preprint. Figure 2: Failure cases from state-of-the-art layout-to-image models. Each row presents an example with overlapping instances, and image captions are shown below. More examples and more detailed failure descriptions can be found in Section C. in controllable image generation [Li et al., 2023b, Zhang et al., 2023]. recent line of work proposes generating images conditioned on layouts, commonly referred to as Layout-to-Image (L2I) generation, which allows users to directly specify spatial locations [Xie et al., 2023b, Wang et al., 2024b, Li et al., 2023b] and object counts [Binyamin et al., 2024, Yang et al., 2023] in the generated outputs. While existing frameworks [Xie et al., 2023b, Wang et al., 2024b, Li et al., 2023b] can achieve satisfactory spatial and numerical control over image generation, these approaches fail to generate distinct, coherent objects when multiple bounding boxes overlap in layout and their associated categories are semantically similar. As illustrated in Figure 2, such scenarios lead to artifacts including object blending, spatial ambiguity, and visual distortion. To quantify the effect of bounding box overlap on generation, we introduce OverLayScore (Equation (1)), simple metric that captures the difficulty of generation based on spatial and semantic overlap between bounding boxes. OverLayScore is computed as the sum of IoUs for all instance pairs, weighted by their semantic similarity, measured via the dot product of CLIP embeddings of instance annotations. We empirically demonstrate that generation quality degrades with higher OverLayScore, i.e., large overlap between bounding boxes and high category-level semantic similarity. Henceforth, we will interchangeably refer to layouts with high OverLayScore score as complex layouts and low OverLayScore score as simple layouts. Existing benchmarks for Layout-to-Image (L2I) generation [Cheng et al., 2024, Zhang et al., 2024] primarily focus on image quality, offering limited evaluation of complex layouts or the accuracy of generated instance relationships. Our analysis reveals strong bias towards simple layouts in these benchmarks, which restricts their utility in assessing model performance under more challenging, realistic scenarios. To address this gap, we introduce OverLayBench, new benchmark specifically designed to evaluate L2I models on their ability to reconstruct complex layouts and instance-level relationships. OverLayBench features rich annotations such as detailed image and dense instance captions, enrichment of complex images with higher OverLayScore, improved semantic grounding of bounding boxes using Qwen [Bai et al., 2023], and quality instance relationships for evaluation. We conduct extensive evaluations of state-of-the-art L2I models [Li et al., 2023a, Wang et al., 2024a, Zhou et al., 2024a, Cheng et al., 2024, Zhou et al., 2024b, Zhang et al., 2025, 2024] on OverLayBench and verify their effectiveness in addressing layout-level challenges. These results provide strong baselines and highlight areas for improvement. Finally, we demonstrate that fine-tuning CreatiLayout [Zhang et al., 2024] with amodal mask supervision on complex layouts helps mitigate generation artifacts caused by instance occlusion. This new baseline, CreatiLayout-AM, provides initial evidence that explicit mask-level guidance improves generation quality under high-overlap conditions, offering promising direction for future research. Our contributions are summarized as follows: (1) We propose OverLayScore, novel metric that empirically quantifies the difficulty of L2I generation by measuring the IoU and semantic similarity between bounding boxes in layouts; (2) We introduce OverLayBench, challenging benchmark with high-quality annotations and balanced difficulty distribution, designed to evaluate complex 2 relationships between instances in layouts; (3) We demonstrate that training with amodal masks helps alleviate generation artifacts in overlapping regions. Specifically, we present simple yet effective baseline that aligns attention maps in diffusion models with amodal mask supervision."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Layout-to-Image Generation Generative models have recently become highly popular [Wang et al., 2024d, Zhao et al., 2025, Zeng et al., 2025, Srivastava et al., 2025a, Xu et al., 2024, Wang et al., 2024c, Chen et al., 2025], and controllable generation is attracting growing interest. In particular, Layout-to-Image (L2I) generation has gained attention as it enables structured and spatially grounded image synthesis. Prior work on L2I generation has primarily focused on fine-tuning foundational text-to-image (T2I) generative models [Labs, 2024, Esser et al., 2024, Podell et al., 2023], introducing various techniques to inject layout conditioning into pre-trained architectures. U-Net-based approaches [Li et al., 2023a, Zhou et al., 2024a, Wang et al., 2024a, Wu et al., 2024, Cheng et al., 2024, Zheng et al., 2023] typically incorporate layout information through layer insertions (either in series or parallel) or by manipulating attention maps and masks. While these methods have shown promising results, their generation quality is often constrained by the representational capacity of U-Net backbones. More recent methods [Zhang et al., 2025, 2024] leverage powerful diffusion Transformer (DiT) architectures [Peebles and Xie, 2023], fine-tuning models such as Flux [Labs, 2024] or SD3 [Esser et al., 2024], and achieve improved image fidelity. These layouts can be provided by the user or generated automatically from text using text-to-layout models [Feng et al., 2023a, Gani et al., 2023, Lian et al., 2023, Srivastava et al., 2025b], forming two-stage pipeline for controllable image synthesis. In parallel, growing line of training-free approaches [Xie et al., 2023a, Gani et al., 2023, Yang et al., 2024b, Lee et al., 2024, Chen et al., 2024a,c] has emerged, which utilize guidance mechanisms within diffusion models to enforce spatial constraints without additional training. However, these models often treat layout information as soft constraint, which limits their ability to strictly adhere to spatial specifications. Despite these advancements, existing methods struggle to generate coherent and distinguishable objects when multiple bounding boxes overlap, particularly when the associated categories are semantically similar. Such scenarios frequently lead to confusion, ambiguity, visual distortion, or artifacts in the generated images. To address the gap, we study Layout-to-Image (L2I) generation under bounding box overlap in layouts and highlight it as novel and underexplored challenge that demands dedicated investigation. 2.2 Layout-to-Image Benchmarks The most widely used benchmark for L2I generation is COCO [Lin et al., 2014], which provides image-text pairs annotated with entity bounding boxes (bboxes) and simple category labels. Although COCO and its variants are commonly used for evaluating L2I models, they lack detailed image and instance-level captions, which limits their utility for training and evaluating in more semantically rich generation tasks [Feng et al., 2023b]. To address this shortcoming, many works manually augment COCO with additional instance-level captions to support more comprehensive evaluation. Recent efforts [Cheng et al., 2024, Zhang et al., 2024] have introduced new layout benchmarks to offer more robust and holistic assessments. HiCo [Cheng et al., 2024] proposes HiCo-7k, benchmark containing 7,000 carefully curated samples from GriT-20M [Peng et al., 2023], while CreatiLayout [Zhang et al., 2024] introduces LayoutSAM, comprising 5,000 samples selected from the SAM-1B [Kirillov et al., 2023] dataset. Both benchmarks employ image/entity filtering and generate detailed annotations. However, they rely on GroundingDINO [Liu et al., 2023] for bounding box extraction, which, according to its authors, exhibits limited recognition performance and frequently produces false positives in detection outputs (as discussed in Section 4.1). HiCo-7k partially addresses these issues through manual human curation, whereas LayoutSAM is generated without human intervention. In contrast, OverLayBench leverages Qwen2.5-VL-32B [Bai et al., 2025], which surpasses GroundingDINO in benchmark performance and our own qualitative evaluation. Additionally, we incorporate human curation to further ensure data quality. Most importantly, unlike previous benchmarks that 3 (a) (b) (c) Figure 3: (a) Image quality of CreatiLayout across varying levels of bounding box IoU and semantic similarity, from poorer to better; (b) Performance comparison of three L2I models on toy COCO samples, grouped by layout difficulty using OverLayScorehigher difficulty consistently leads to lower mIoU; (c) Distribution of layout difficulty across COCO, LayoutSAM, HiCo-7k, and our proposed benchmark, OverLayBench (introduced in detail in Section 4.1). primarily feature simple or regular layouts, our benchmark includes significantly more complex and challenging layouts, providing more balanced distribution of layout difficulty and enabling more rigorous evaluation of L2I models."
        },
        {
            "title": "3 OverLayScore",
            "content": "We evaluate the impact of overlap on L2I generation performance from two perspectives: spatial and semantic. Figure 3 (a) presents the L2I generation results in simplified two object setting, from which we derive two key observations: (1) as the Intersection-over-Union (IoU) between bounding boxes increases, the output quality of state-of-the-art L2I models deteriorates; and (2) given the same IoU, higher semantic similarity between the instance captions of overlapping boxes further degrades the generation quality. These findings indicate that both spatial and semantic overlaps introduce complexities that negatively affect L2I generation. Building on our observations, we propose OverLayScore, metric designed to quantify the difficulty of L2I generation arising from overlapping elements within layout. Formally, given layout with objects, let pk and Bk denote the instance caption and normalized bounding box for the k-th object, respectively. We define the metric as: OverLayScore = (cid:88) IoU(Bi, Bj) cos (cid:0)pi, pj(cid:1), (1) (i,j): IoU(Bi,Bj )>0 where cos (cid:0)pi, pj(cid:1) is the CLIP-based cosine similarity between instance caption pi and pj. The summation captures the spatial-semantic entanglement of all overlapping object pairs in the layout. higher OverLayScore indicates greater expected difficulty for an L2I model to faithfully generate an image that conforms to both the layout and the associated instance-level semantics. 4 Figure 4: An overview of the data curation pipeline for OverLayBench. To validate the effectiveness of OverLayScore, we evaluate common L2I models on subset of the COCO dataset [Lin et al., 2014]. Layouts are extracted using the datasets bounding box and category annotations, and filtered to include scenes with 2 to 10 objects. Based on their OverLayScore, these layouts are categorized into three difficulty levels simple, regular, and complex. From each category, we randomly sample 100 layouts for evaluation. We then assess the performance of three representative L2I models, i.e., GLIGEN [Li et al., 2023a], InstanceDiffusion [Wang et al., 2024a], and CreatiLayout [Zhang et al., 2024]. As illustrated in Figure 3 (b), the performance of all models consistently declines as OverLayScore increases, demonstrating that OverLayScore effectively reflects the difficulty of generating images from overlapping layouts. We further apply OverLayScore to several widely used L2I benchmarksCOCO [Lin et al., 2014], HiCo [Cheng et al., 2024], and LayoutSAM [Zhang et al., 2024], and visualize the score distribution in Figure 3 (c). We observe that the majority of samples fall within the low-difficulty regime, indicating strong imbalance in existing benchmarks. This skew limits their ability to evaluate model performance in more complex overlapping scenarios."
        },
        {
            "title": "4 OverLayBench",
            "content": "The analysis in the previous section highlights key limitation of existing L2I benchmarksthey are heavily skewed toward low-OverLayScore samples, which restricts their ability to evaluate model performance under challenging layout conditions. To overcome this limitation, we introduce OverLayBencha new benchmark specifically curated to assess L2I models on complex and overlapping layouts. By carefully selecting images across broad range of OverLayScore values, OverLayBench provides more balanced and comprehensive evaluation set, enabling rigorous evaluation of model robustness in spatially and semantically complex layouts. 4.1 Dataset Curation An overview of our data curation pipeline is presented in Figure 4, comprising three key stages. In Stage I, we use Flux to generate reference images based on captions extracted from real-world images. Stage II leverages vision-language model (VLM) to extract both image and instance-level descriptions, along with inter-instance relationships. Finally, Stage III involves human curation process to filter out unrealistic generation and balance the distribution across difficulty levels. 4.1.1 Reference Image Generation We begin by extracting image captions from the COCO [Lin et al., 2014] training set using Qwen2.5VL-7B [Bai et al., 2025]. These captions are then used to generate diverse set of image candidates with Flux.1-dev [Labs, 2024]. By leveraging captions derived from real-world images, we ensure that the generated content and corresponding layouts are both natural and realistic. In total, we collect approximately 86,000 generated images paired with their corresponding captions. 4.1.2 Image Grounding and Captioning Step 1: Image Caption Refinement Although Flux demonstrates strong image generation capabilities, the generated images do not always perfectly align with the input captions. To improve semantic consistency, we perform an additional captioning pass on all generated images using Qwen-2.5-VL-7B to produce refined global image captions. Step 2: Instance Grounding Qwen [Bai et al., 2023] has demonstrated superior grounding performance compared to models commonly used in existing L2I benchmarks, such as GroundingDINO [Liu et al., 2023]. We leverage Qwen to detect and describe all foreground objects in each image. Based on the grounding results, we retain only images that contain one to ten valid overlapping bounding box pairs. bounding box pair is considered valid if it satisfies both: (1) an IoU greater than 5%, and (2) an intersection area exceeding 1% of the total image area. After this step, each image contains global description and local descriptions for all detected instances. Step 3: Relationship Extraction In addition to image-level and instance-level captions and bounding boxes, we further prompt Qwen to generate pairwise relationship phrases between overlapping instances. These phrases capture both spatial and semantic relationships, providing richer annotation signal crucial for evaluating model performance on inter-instance relationship in complex layouts. 4.1.3 Scoring and Curation To ensure high-quality and reliable annotations while minimizing hallucinations from VLMs, we perform thorough manual verification and discard all invalid cases. Specifically, we assess the accuracy of each bounding box, the alignment between image content and both global and instancelevel captions, and the validity of relationship descriptions based on available textual inputs. This rigorous process ensures that OverLayBench remains free from hallucinations. After validation, we compute the OverLayScore score for each example and retain curated dataset of 2,052 simple, 1,000 regular, and 1,000 complex layouts. 4.2 Benchmark Metrics We introduce two novel metrics tailored to overlapping scenarios: O-mIoU (Overlap-mIoU), which computes the mIoU within the ground-truth overlap regions and the corresponding predicted regions. By isolating shared areas between instances, O-mIoU provides more sensitive and discriminative measure of fidelity in occluded or entangled regions than standard global mIoU; SRR (Success Rate of Relationship), which reports the percentage of object pairs whose predicted spatial relationships match the ground truth. It offers an interpretable, relationship-level measure of success. In addition to these two metrics, we adopt commonly used evaluation measures from prior work [Zhang et al., 2025, 2024], including mIoU, CLIP [Radford et al., 2021], SRE (Success Rate of Entity), and FID [Heusel et al., 2017]."
        },
        {
            "title": "Objects",
            "content": "To address the challenges posed by complex overlaps in state-of-the-art L2I models, we incorporate amodal mask supervision during training, which provide complete object shape information even under occlusion. We construct custom L2I training set annotated with amodal masks and train new model to validate the effectiveness of this approach. Training Dataset We begin by synthesizing occlusions on top of FLUX-generated images. For each image, we employ Segment Anything Model v2 [Ravi et al., 2024] to extract amodal object masks. These masks are then used to crop individual objects, forming pool of object-mask pairs denoted as O. To simulate occlusions, we randomly select an object from and paste it onto 6 Figure 5: Comparison of generated images from CreatiLayout and CreatiLayout-AM. The CreatiLayout-AM handles overlapping instances more effectively, producing more coherent and realistic images. target image at location that creates overlap with an existing object. This method enables controlled occlusion synthesis while preserving the original scene context. For each synthesized image, we use Qwen-2.5-VL-32B to generate both global image caption and local instance descriptions, covering both original and newly pasted overlapping objects. The final training set contains approximately 67.8k images. CreatiLayout-AM Building on our curated training dataset, we introduce CreatiLayout-AM, modified version of CreatiLayout-SD3 [Zhang et al., 2024] designed to enhance generation quality in the presence of occluded bounding boxes by incorporating amodal masks during training. Inspired by TokenCompose [Wang et al., 2024e], we fine-tune CreatiLayout by introducing two additional loss terms that explicitly encourage alignment between the models attention maps and ground truth amodal masks. Specifically, we compute two auxiliary losses, Ltoken and Lpixel, in addition to the original training objective. Let Ai denote the attention map between image tokens and the layout token corresponding to the ith instance, and let mi be the ground-truth amodal mask for that instance. The token-level alignment loss is defined as: Ltoken = 1 (cid:88) (cid:18) 1 i=1 (cid:80) Ai (cid:80) mi Ai (cid:19) (2) where is the total number of instances and indexes each pixel coordinate. The pixel-level alignment loss is defined using cross-entropy function: where CE is the cross entropy loss. The final training objective is given by: Lpixel = (cid:88) CE(Ai u, mi u) = LLDM + λLtoken + βLpixel (3) (4) where LLDM is the original denoising loss used in the Latent Diffusion Models (LDM). Beyond CreatiLayout [Zhang et al., 2024], we also implemented an EliGen [Zhang et al., 2025] based AM method, please find the detail in Section A.2.1. 7 Table 1: Comprehensive comparisons between training-based methods on OverLayBench, including the newly-released model. Bold and underline denote the best and the second best methods. Methods above the dashed line are U-Netbased, while those below are DiTbased. means the model takes additional depth map as input. Method mIoU(%) O-mIoU(%) SRE(%) SRR(%) CLIPGlobal CLIPLocal FID OverLayBench-Simple 60.541.82 GLIGEN 71.210.11 InstanceDiff 58.640.18 MIGC 69.470.26 HiCo 65.750.07 3DIS 58.780.44 CreatiLayout-SD3 CreatiLayout-FLUX 71.170.23 68.170.41 EliGen DreamRender 67.600.36 36.220.13 49.990.06 32.150.31 47.230.44 38.380.23 32.520.61 49.800.52 43.720.76 43.450. 49.990.43 77.710.18 63.410.53 67.750.18 86.240.17 72.340.61 84.350.49 86.500.59 88.800.44 78.720.50 87.490.23 81.600.19 86.080.76 86.980.24 84.450.04 90.870.27 89.670.26 90.070.12 OverLayBench-Regular 52.460.29 GLIGEN 60.080.24 InstanceDiff 47.420.08 MIGC 55.020.33 HiCo 55.660.22 3DIS 47.040.13 CreatiLayout-SD3 CreatiLayout-FLUX 59.720.29 58.560.38 EliGen DreamRender 58.080.36 26.530.06 34.150.16 20.060.18 29.600.53 27.290.18 20.670.28 35.510.44 32.620.52 33.000.50 44.880.31 72.510.29 56.670.75 58.241.01 80.800.47 62.600.77 77.200.57 80.850.20 83.520. 77.460.49 83.360.30 77.850.59 79.890.66 83.690.09 78.310.38 86.390.31 84.420.36 84.950.41 OverLayBench-Complex 50.790.75 GLIGEN 53.680.56 InstanceDiff 40.040.31 MIGC 46.560.31 HiCo 50.650.61 3DIS 44.240.55 CreatiLayout-SD3 CreatiLayout-FLUX 54.500.50 52.530.17 EliGen DreamRender 52.470.14 23.850.52 25.630.34 13.260.05 20.350.38 21.750.31 18.050.39 28.970.54 26.190.27 26.130.36 41.700.91 66.020.47 47.800.67 48.880.32 74.311.24 52.100.53 69.720.39 74.030.66 77.870.45 79.930.58 80.340.25 74.480.99 75.190.48 81.570.89 79.980.30 86.450.45 84.090.58 84.930. 34.170.02 34.250.05 33.070.05 35.250.10 35.850.05 37.290.04 37.400.15 36.650.06 37.290.03 33.930.07 33.090.10 32.720.11 33.910.14 35.420.05 36.670.05 36.730.08 36.270.09 36.850.11 33.920.06 32.330.05 31.930.05 33.150.18 35.110.09 36.550.08 36.720.07 36.180.11 36.750.10 31.270.38 24.750.02 36.170.23 27.690.02 31.640.11 26.490.10 29.210.16 27.040.09 29.180.29 29.670.03 27.490.03 27.510.15 20.180.11 23.790.17 28.870.88 28.290.13 24.910.65 30.110.18 23.420.02 52.220.43 26.190.03 59.730.95 54.240.52 24.990.02 49.070.45 25.340.04 48.560.48 28.120.01 45.570.20 25.550.08 41.510.39 26.210.07 45.651.09 27.050.12 42.660.56 28.740.12 22.750.06 25.530.01 24.200.04 24.410.05 27.350.07 24.760.03 24.850.09 25.920.13 27.540. 57.320.11 66.320.29 66.520.33 55.780.35 54.900.29 53.290.80 45.660.75 50.410.74 48.110.89 Table 2: BaseModel v.s. Ours AM method on OverLayBench. Split mIoU(%) O-mIoU(%) SRE(%) SRR(%) CLIPGlobal CLIPLocal Method CreatiLayout CreatiLayout-AM vs. BaseModel EliGen EliGen-AM vs. BaseModel CreatiLayout CreatiLayout-AM vs. BaseModel EliGen EliGen-AM vs. BaseModel CreatiLayout Ours vs. BaseModel EliGen EliGen-AM vs. BaseModel Simple Simple Regular Regular Complex Complex 58.78 61.16 +4.05% 68.17 69.70 +2.24% 47.04 47.38 +0.72% 58.56 59.44 +1.50% 44.24 43.97 0.61% 52.53 53.28 +1.43% 32.52 37.69 +15.90% 43.72 46.43 +6.20% 20.67 21.79 +5.42% 32.62 33.85 +3.74% 18.05 18.07 +0.11% 26.19 26.69 +1.91% 72.34 73.33 84.45 84.84 +1.37% +0.46% 86.50 86.83 +0.38% 62.60 63. 89.67 90.07 +0.45% 78.31 78.71 +0.85% +0.51% 80.85 81.18 +0.41% 52.10 52.49 84.42 85.47 +2.43% 79.98 79.77 +0.75% 0.26% 74.03 76.32 +3.09% 84.09 84.31 +0.27% 37.29 37.17 0.32% 36.65 36.84 +0.52% 36.67 36.49 0.49% 36.27 36.46 +0.52% 36.55 36.32 0.63% 36.18 36.39 +0.58% 8 FID 27.51 27.76 27.49 27.44 0.18% +0.91% 28.29 28.58 +1.03% 25.55 25.46 28.87 26.43 -8.45% 45.57 46.34 0.35% +1.68% 27.05 27.37 +1.18% 24.76 24.72 45.65 43.52 -4.67% 53.29 53.48 0.16% +0.36% 25.92 26.15 +0.89% 50.41 49.42 -2.00%"
        },
        {
            "title": "6 Evaluation",
            "content": "6.1 Implementation Details For data curation, we use Flux-1-dev for image generation with 28 sampling steps. Qwen2.5-VL-7B is employed for image captioning, while Qwen2.5-VL-32B is used to extract bounding bboxes, instancelevel captions, and relationship captions. Additionally, we apply RealVisXL_V5.0_Lightning1 for object removal during training data construction. For CreatiLayout-AM training, we fine-tune the model for 3,500 steps on 8 NVIDIA RTX A6000 (48GB) GPUs with batch size of 16, and learning rate of 105. We use the AdamW [Loshchilov and Hutter, 2017] optimizer with bf16 precision. Please refer to the Section for more details. 6.2 Quantitative Results Table 1 presents the quantitative evaluation of multiple layout-to-image (L2I) generation methods [Li et al., 2023a, Wang et al., 2024a, Zhou et al., 2024a, Cheng et al., 2024, Zhang et al., 2024, Zhou et al., 2024b, Zhang et al., 2025, Zhou et al., 2025] across varying difficulty levels in the OverLayBench benchmark: Simple, Regular, and Complex. As task difficulty increases, all models show noticeable decline in spatial metrics (particularly O-mIoU), highlighting the inherent challenges posed by highly overlap and semantically similar layouts. Despite this, DiT-based models demonstrate more stable visual quality and stronger semantic alignment, underscoring their robustness and scalability in handling complex generation scenarios. 6.3 CreatiLayout-AM Comparison Table 2 demonstrates that CreatiLayout-AM outperforms the original CreatiLayout on the Simple and Regular splits, with particularly notable gains in O-mIoU (+15.90% and +5.42%, respectively). These improvements are consistent across other spatial and relational metrics, including mIoU, SRE, and SRR, indicating enhanced spatial and relational alignment despite minor drops in CLIP scores. On the Complex split, where distribution shift from the training set is more pronounced, performance remains competitive, exhibiting only slight declines in mIoU and CLIP. Overall, these results validate the effectiveness of amodal mask supervision in improving L2I generation under bbox overlap, presenting promising direction for future explorations. 6.4 Qualitative Analysis For qualitative analysis, we provide comprehensive visualization of state-of-the-art L2I models on OverLayBench, showcasing diverse set of examples with varying levels of layout overlap complexity in Figure 6. We annotate each row with gold, silver, and bronze icons representing the top-3 performing models."
        },
        {
            "title": "7 Conclusion and Future Works",
            "content": "In this work, we present comprehensive study on the often-overlooked challenge of object occlusion in Layout-to-Image (L2I) generation. We introduce OverLayScore, principal difficulty metric that captures both spatial overlap and semantic similarity, and show that higher OverLayScore values strongly correlate with degraded generation quality. To support rigorous evaluation, we propose OverLayBench, balanced benchmark spanning the full spectrum of layout difficulty. It features high-fidelity images and dense captions, enabling in-depth assessment of instance interactions in densely overlapping scenes. Additionally, we demonstrate that amodal mask supervision mitigates collusion artifacts, enhancing generation quality in complex layouts. Our baseline model, CreatiLayout-AM, outperforms existing methods under OverLayScore. Together, our metric, benchmark, and baseline establish unified testbed for advancing occlusionaware, controllable image generation, and aim to inspire future methods with stronger spatial reasoning and compositional understanding. 1https://huggingface.co/SG161222/RealVisXL_V5.0_Lightning 9 Figure 6: Comparison of generated images from different models on our OverLayBench. Acknowledgment. This work is supported by NSF award IIS-2127544 and NSF award IIS-2433768. We thank Lambda, Inc. for their compute resource help on this project."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Lital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, and Gal Chechik. Make it count: Text-to-image generation with an accurate number of objects. arXiv preprint arXiv:2406.10210, 2024. Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395, 2024a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 53435353, 2024b. Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, and Linjie Luo. X-dancer: Expressive music to human dance video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024c. Bo Cheng, Yuhang Ma, Liebucha Wu, Shanyuan Liu, Ao Ma, Xiaoyu Wu, Dawei Leng, and Yuhui Yin. Hico: Hierarchical controllable diffusion model for layout-to-image generation. arXiv preprint arXiv:2410.14324, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. arXiv preprint arXiv:2305.15393, 2023a. Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-toimage diffusion for accurate instruction following. arXiv preprint arXiv:2311.17002, 2023b. Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, and Peter Wonka. Llm blueprint: Enabling text-to-image generation with complex and detailed prompts. arXiv preprint arXiv:2310.10640, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 11 Yuseung Lee, Taehoon Yoon, and Minhyuk Sung. Groundit: Grounding diffusion transformers via noisy patch transplantation. Advances in Neural Information Processing Systems, 37:5861058636, 2024. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2251122521, 2023a. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation, 2023b. URL https://arxiv.org/abs/2301.07093. Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer vision ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79327942, 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. URL https://arxiv.org/abs/2408. 00714. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Divyansh Srivastava, Xiang Zhang, He Wen, Chenru Wen, and Zhuowen Tu. Lay-your-scene: Natural scene layout generation with diffusion transformers. arXiv preprint arXiv:2505.04718, 2025a. Divyansh Srivastava, Xiang Zhang, He Wen, Chenru Wen, and Zhuowen Tu. Lay-your-scene: Natural scene layout generation with diffusion transformers. arXiv preprint arXiv:2505.04718, 2025b. Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62326242, 2024a. Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62326242, 2024b. Yilin Wang, Zeyuan Chen, Liangjun Zhong, Zheng Ding, and Zhuowen Tu. Dolfin: Diffusion layout transformers without autoencoder. In European Conference on Computer Vision, pages 326343. Springer, 2024c. Yilin Wang, Haiyang Xu, Xiang Zhang, Zeyuan Chen, Zhizhou Sha, Zirui Wang, and Zhuowen Tu. Omnicontrolnet: Dual-stage integration for conditional image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74367448, 2024d. Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-to-image diffusion with token-level supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85538564, 2024e. Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, and Xinchao Wang. Ifadapter: Instance feature control for grounded text-to-image generation. arXiv preprint arXiv:2409.08240, 2024. Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74527461, 2023a. Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74527461, 2023b. Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, and Zhuowen Tu. Bayesian diffusion models for 3d shape reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1062810638, 2024. Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. Advances in Neural Information Processing Systems, 36, 2024. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024a. Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering textto-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024b. Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1424614255, 2023. 13 Guanning Zeng, Xiang Zhang, Zirui Wang, Haiyang Xu, Zeyuan Chen, Bingnan Li, and Zhuowen Tu. Yolo-count: Differentiable object counting for text-to-image generation. arXiv preprint arXiv:2508.00728, 2025. Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, and Yu Zhang. Eligen: Entity-level controlled image generation with regional attention. arXiv preprint arXiv:2501.01097, 2025. Hui Zhang, Dexiang Hong, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, and Yu-Gang Jiang. Creatilayout: Siamese multimodal diffusion transformer for creative layout-to-image generation. arXiv preprint arXiv:2412.03859, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. Qingcheng Zhao, Xiang Zhang, Haiyang Xu, Zeyuan Chen, Jianwen Xie, Yuan Gao, and Zhuowen Tu. Depr: Depth guided single-view scene reconstruction with instance-level diffusion. arXiv preprint arXiv:2507.22825, 2025. Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2249022499, 2023. Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68186828, 2024a. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. arXiv preprint arXiv:2410.12669, 2024b. Dewei Zhou, Mingwei Li, Zongxin Yang, and Yi Yang. Dreamrenderer: Taming multi-instance attribute control in large-scale text-to-image models. arXiv preprint arXiv:2503.12885, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "We begin by outlining the implementation details for the following stages: (1) data curation, (2) training, (3) inference and evaluation. A.1 Data Curation A.1.1 Prompts Construction We provide the prompt templates used with Qwen-2.5-VL-7B and Qwen-2.5-VL-32B during the dataset curation process. Qwen-2.5-VL is employed for three distinct tasks: (1) Image Captioning, (2) Grounding, and (3) Relationship Extraction. The specific prompts for each task are shown below. Image Captioning Give detailed caption of this image . Grounding You are required to detect the main foreground instances in the image and describe them . Response in json format : { } t e _c g _ 1 : { bbox : [ x1 , y1 , x2 , y2 ] , local_prompts : description of this instance } , t e _c g _ 2 : { bbox : [ x1 , y1 , x2 , y2 ] , local_prompts : description of this instance } , ... Each bounding box must correspond to single , distinct individual object - never group or collection . Do not merge multiple instances into one . Strictly follow this instruction without exceptions or interpretation . Strictly follow the format in English , without any irrelevant words . Relationship Extraction You are required to extract the relation between two instances in given annotation , based on their bounding boxes and local descriptions . Only describe the relationship for the provided valid instance pairs . Here is the image caption : { Caption }. Here are the instance annotations : { Bbox and Instance - Caption }. Here are the list of valid instance pairs to describe : { Valid Overlapping Pairs }. Response in the following json format : { } ( Instance_1 , Instance_2 ) : Instance_1 { relationship } Instance_2 , ... where { relationship } are words in the caption and local prompts that explicitly describe the interaction or spatial relation . Each key must be tuple from the list of valid instance pairs . If the relationship is not explicitly specified , please respond with None . Never invent any relationship that is not specified in the annotation . Strictly respond the relationship description in one short sentence , without any irrelevant words . A.1.2 Web-UI Construction As part of our data curation pipeline, we incorporate human auditing to ensure data quality. To facilitate this process, we develop custom Web-UI that displays key annotations, including image captions, bounding boxes, instance captions, and relationship captions. Annotators are instructed to verify the accuracy of each annotation to uphold high data quality standards. screenshot of the Web-UI is provided in Figure 7. 15 Figure 7: Web-UI used for human auditing. A.2 Training The complete set of hyper-parameters used during training is listed in Table 3. Table 3: Training hyperparameters of CreatiLayout-AM. Hyperparameter CreatiLayout-AM EliGen-AM Number of GPUs Batch Size (per GPU) Gradient Accumulation Steps Gradient Checkpointing Learning Rate LR Scheduler Warm-up Steps Training Steps LoRA Rank Parallel λ β 8 Nvidia A6000 48GB 8 Nvidia H100 80GB 1 2 False 1e-5 Linear 500 3500 32 DDP 0.5 1 1 1 True 1e-4 Linear 500 2000 64 FSDP 1 A.2.1 EliGen-AM To further validate the effectiveness of our training set and the proposed AM method, we conduct experiments on an additional baseline, EliGen [Zhang et al., 2025]. Unlike CreatiLayout, which 16 represents each entity with explicit layout tokensthereby allowing direct extraction of entityspecific attention maps for applying our losses in Equation (4), EliGen constrains image tokens within bounding box to only attend to the global description tokens and their corresponding local description tokens. This design makes it non-trivial to define the instance-level attention map required in Equation (4). To address this, we approximate Ai as the average attention map across all text tokens in the ith local description. Formally, we define the attention map of ith instance as: Ai ="
        },
        {
            "title": "1\nLi",
            "content": "Li (cid:88) j=0 Aj (5) where Li is the number of tokens in the ith local description, Aj refers to the attention map between images tokens and the jth local description token. We replace the Ai with Ai in Equation (2) and Equation (3) and train the model with the hyperparameters in Table 3. The quantitative results is shown in Table 2. A. Inference and Evaluation For evaluation, we generate three images per method for each layout using fixed random seed (20251202, 20251203 and 20251204) to ensure fair comparison. To compute the standard mIoU, we match each ground truth bounding box to its corresponding predicted box using the Hungarian algorithm. For our proposed metric, O-mIoU, we compute the mIoU over the cropped intersection region between two instances involved in specified relationship. We argue that this metric more effectively captures the fidelity and realism of object rendering, particularly in densely overlapping scenarios. The CLIP score is computed using the pretrained CLIP model \"ViT-B/32\" [Radford et al., 2021]. During the inference and evaluation stages, Qwen-2.5-VL-32B is employed for object detection (to generate predicted bounding boxes for mIoU computation) and for question answering, which is used to determine the instance-level success rate SRE and relationship-level success rate SRR. We provide prompts for each task below. Object Detection You are required to detect all the instances of the following categories { Categories } in the image . Response in json format : { } category_1 : [[ x1 , y1 , x2 , y2 ] , [ x1 , y1 , x2 , y2 ] , ...] , category_2 : [[ x1 , y1 , x2 , y2 ] , ...] ... For each category , provide list of bounding boxes of all its instances in the image . Each bounding box must correspond to single , distinct individual object - never group or collection . Strictly follow this instruction without exceptions or interpretation . Strictly follow the format in English , without any irrelevant words . SRE You are required to answer whether the instances in an image match the corresponding descriptions , based on their bounding boxes . Here are the instance names , the corresponding bboxes and the instance description : { Bbox and Instance Captions } Please follow these rules : Check if the generated instance visually matches its local_prompt description . If the instance is clearly generated and not corrupted , and its key attributes described in the local_prompt are present , answer Yes . If the instance is missing , corrupted , or the key attributes are not present , answer No . Response in the following format : { Instance_name : Yes / No , 17 ... } Each key must be from the dict of the instance name , the corresponding bbox and the instance description . Each value must be Yes or No . If the instance name is not in the image , the answer should be No . Strictly follow the format in English , without any irrelevant words . SRR You are required to answer whether the relationship between two instances in an image matches the description . Here are the instance name and the bbox : { Bbox and Instance Name }. Here are dict of the instance pair and the ground truth relationship descriptions : { Relationships }. Please follow these rules : For proximity relations like near , beside , close to , next to , if the two instances are generated well ( not corrupted or fused into one ) and their bounding boxes are close , you can consider the description as matched . For directional or positional relations like behind , in front of , you must strictly check if the spatial arrangement in the image actually matches the description , because bounding boxes alone are not enough . Response in the following format : { } ( Instance_1 , Instance_2 ) : Yes / No , ... Each key must be tuple from the dict of the instance pair and the ground truth relationship descriptions . Each value must be Yes or No . Yes means the action / spatial relationship between the two instances matches the description . You shouldn pay too much attention on how well the bounding boxes are aligned . Strictly follow the format in English , without any irrelevant words . A.4 Baseline Clarification We include several recent training-based L2I methods in our benchmark evaluation, some of which provide multiple variants depending on the underlying base model. To avoid ambiguity, we clarify our choices here. For HiCo [Cheng et al., 2024], we use the HiCo-SD1.5 model, as the SDXL version was not publicly available at the time of evaluation. For 3DIS [Zhou et al., 2024b] and DreamRender [Zhou et al., 2025], we adopt the FLUX-based versions. In addition, since DreamRender requires auxiliary modalities such as canny edge maps and depth maps, we utilize DepthAnything v2 [Yang et al., 2024a] to extract depth maps from reference images, which are then provided together with the layout as inputs to the model. Quantitative Results Analysis on Training-Free Methods In addition to the training-based approaches, we evaluate several training-free methods on OverLayBench [Xie et al., 2023a, Chen et al., 2024b, Phung et al., 2024, Lee et al., 2024, Chen et al., 2024a], as shown in Table 4. Specifically, on the OverLayBench-Simple split, RegionalPrompting achieves the highest overall performance, with the best mIoU (42.54%), O-mIoU (20.10%), SRE (73.49%), SRR (75.81%), CLIPLocal (27.40), and FID (23.94). On the Regular split, it continues to lead in mIoU (32.72%), O-mIoU (12.29%), SRE (63.74%), and CLIPLocal (25.82), again achieving the lowest FID (43.13), despite slight drop in SRR (67.08%). For the more challenging Complex split, RegionalPrompting still attains the highest mIoU (28.35%), O-mIoU (9.05%), SRE (53.56%), and CLIPLocal (25.29), alongside competitive FID (49.41), further demonstrating its robustness in dense and overlap-heavy scenarios. 18 Table 4: Comprehensive comparison between training-free methods on OverLayBench. Bold and underline denote the best and the second best methods. Methods above the dashed line are UNetbased, while those below are DiTbased. Method mIoU(%) O-mIoU(%) SRE(%) SRR(%) CLIPGlobal CLIPLocal FID OverLayBench-Simple 24.480.08 BoxDiff 23.120.27 LayoutGuidance 27.781.90 R&B 31.920.34 GroundDiT RegionalPrompting 42.540.17 7.710.43 7.920.13 9.701.16 10.930.36 20.100. 42.030.89 69.940.52 45.780.65 70.830.70 36.980.52 64.050.45 48.570.89 75.260.40 73.490.53 75.810.32 36.780.06 33.470.10 34.640.18 36.100.12 35.450.08 21.330.08 21.220.01 21.320.04 22.730.03 27.400.06 34.650.20 74.401.01 36.572.30 34.980.63 23.940.10 OverLayBench-Regular 19.400.30 BoxDiff 15.810.20 LayoutGuidance 20.350.74 R&B 24.030.20 GroundDiT RegionalPrompting 32.720. 5.330.24 4.510.19 5.540.61 6.700.31 12.290.23 37.580.36 71.810.44 44.940.07 72.760.86 32.850.42 65.010.06 42.240.66 74.020.46 63.740.73 67.080.69 36.500.05 31.760.01 34.490.14 36.000.12 34.460.10 19.970.03 20.050.02 19.880.04 21.100.05 25.820.10 55.410.28 128.160.35 58.551.79 54.940.49 43.130.31 OverLayBench-Complex 20.020.32 BoxDiff 16.340.33 LayoutGuidance 19.800.67 R&B 24.770.88 GroundDiT RegionalPrompting 28.350.94 5.210.15 4.010.14 4.850.36 6.580.31 9.050.59 33.520.85 76.410.51 37.760.91 75.531.41 28.381.05 69.971.31 37.851.27 77.031.81 53.561.58 60.371.56 36.920.02 32.750.04 34.570.19 36.200.24 33.340.13 19.910.03 59.700.67 19.720.05 119.321.19 63.712.82 19.470.06 55.591.39 20.550.06 49.411.25 25.290."
        },
        {
            "title": "C Error Pattern Analysis of Existing Methods",
            "content": "We identify and categorize the common failure patterns observed in existing methods into five major classes: 1) Incorrect Object Number, where models either hallucinate additional undesired objects or fail to generate required instances within the specified bounding box regions; 2) Object Fusion, where models struggle to generate distinct instances for overlapping bounding boxes, instead producing single merged or entangled object; 3) Object Distortion, where the generated instance lacks realism, often exhibiting severe deformation or artifacts that degrade perceptual quality; 4) Incorrect Category, where the generated object does not match the intended category, undermining semantic correctness; 5) BBox Misalignment, where the object does not properly align with its designated bounding box, either overflowing beyond the box or failing to fully occupy the allocated region, thus breaking spatial consistency. Please refer to Figures 8 and 9 for additional visual examples and explanations of each error pattern."
        },
        {
            "title": "D Comparison Between GroundingDINO and Qwen",
            "content": "We compare the grounding capabilities of GroundingDINO v1.0 Liu et al. [2023] and Qwen-2.5VL-32B Bai et al. [2023] using an example from GroundingDINOs official demo2. As illustrated in Figure 10, GroundingDINO produces multiple false positives, including misclassification of shark as butterfly. In contrast, Qwen demonstrates more accurate and robust detection performance."
        },
        {
            "title": "E User Study",
            "content": "We conducted user study with 15 participants over 60 image pairs (see Table 5), comparing CreatiLayout and CreatiLayout-AM across three difficulty levels. Excluding No Preference cases, our method achieved winning rates of 55.2%, 51.9%, and 46.8% on Simple, Regular, and Complex settings, respectively, showing moderate preference in simpler scenarios. Table 5: User study results across three difficulty levels. Ours vs. CreatiLayout Simple Regular Complex Winning Rate (%) 55. 51.9 46.8 2https://cloud.deepdataspace.com/playground/grounding_dino 19 Figure 8: Error pattern analysis and explanation for existing models. Figure 9: More examples of error patterns for existing methods. Figure 10: Comparison of grounding performance between GroundingDINO v1.0 and Qwen-2.5-VL32B."
        }
    ],
    "affiliations": [
        "Lambda, Inc",
        "UC San Diego"
    ]
}