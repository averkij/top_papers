{
    "paper_title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation",
    "authors": [
        "Xin Zhang",
        "Robby T. Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser."
        },
        {
            "title": "Start",
            "content": "Mamba as Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation Xin Zhang1 Robby T. Tan1,2 1National University of Singapore 2ASUS Intelligent Cloud Services x.zhang@u.nus.edu robby.tan@nus.edu.sg 5 2 0 2 4 ] . [ 1 3 9 1 3 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities 1. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate longsequence modeling. To address this, we propose MFuser, novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github. com/devinxzhang/MFuser. 1. Introduction Developing semantic segmentation models that can robustly handle diverse and unseen conditions [7, 5961] is critical for real-world applications such as autonomous driving, where variations in environment, lighting, and weather [1, 6, 33, 34, 58] can significantly impact per1In this paper, we refer to foundation models trained solely on visual data as VFMs and those trained on both visual and textual data as VLMs. Figure 1. Comparative analysis of the VFM and the VLM features. VFM: Visualization of PCA-computed features from DINOv2 (the first three components of PCA, computed on the image features, serve as color channels), displaying fine-grained details but lacking text alignment. VLM: Image-text similarity map from EVA02-CLIP using the query car, demonstrating good alignment with text but insufficient localization of queried objects. MFuser: Our proposed fusion framework integrates VFM and VLM, resulting in unified features that exhibit both precise locality and robust text alignment. Quantitative results on synthetic-to-real DGSS benchmarks further validate our approach, with MFuser consistently achieving the highest mIoU scores across all tasks. formance. Domain Generalized Semantic Segmentation (DGSS) aims for strong performance across unseen domains without relying on target domain data during training. Traditional approaches include normalization and whitening techniques [10, 43], domain randomization methods [23, 66, 68]. Despite these efforts, existing approaches remain suboptimal, as they often rely on conventional backbones pre-trained on limited datasets, which struggle to generalize effectively to the diverse challenges encountered in real-world scenarios. The recent emergence of Vision Foundation Models (VFMs) and Vision Language Models (VLMs) has established them as powerful tools for achieving generalization in various domains. Some studies have introduced parameter-efficient fine-tuning (PEFT) methods that effectively adapt these foundation models for DGSS [55, 63]. Additionally, some works leverage diffusion models [48] to generate diverse-style images for training DGSS models [15]. VLMs, in particular, have demonstrated the ability to generalize effectively across varied domains by utilizing text embeddings that provide semantic and domaininvariant representations [45]. This capability has sparked the development of multiple approaches in both image classification [9, 24] and semantic segmentation [15, 39]. However, the specific differences between VFMs and VLMs in the context of DGSS remain underexplored. VFM features (e.g., DINOv2 [38]) capture strong details at granular level. In contrast, VLM features (e.g., EVA02-CLIP [16]) struggle to associate text semantics with precise visual regions due to their image-level alignment training. However, this alignment enables VLMs to leverage text embeddings as semantic anchors [40], guiding visual features to remain robust across domain variations. To examine their properties, we perform principal component analysis (PCA) on the DINOv2 features at the final layer. As illustrated in Fig. 1, the PCA-computed features from DINOv2 clearly distinguish between different objects (e.g., cars and trees), even in low-light conditions. Additionally, we apply EVA02-CLIP with the text query car. The activation map also indicates the presence of cars but appears incomplete. This raises an important question: how can we combine both models to extract features that are both locally precise and text-aligned, enabling effective use of text embeddings for improved generalization? An intuitive idea would be to utilize both VFM and VLM for training segmentation model. However, without fine-tuning, foundation models may struggle to adapt to DGSS tasks [55] and VLM text embeddings often fail to align with VFM features, resulting in suboptimal performance. Fully fine-tuning both models, meanwhile, is computationally prohibitive. As such, we propose to introduce additional trainable parameters while keeping the original ones frozen, enabling efficient adaptation. Moreover, combining features from both encoders doubles the patch sequence length, complicating even parameter-efficient finetuning methods in handling such long-range sequences. This leads us to our second question: how can we efficiently adapt and integrate both VFM and VLM for DGSS? To this end, we propose MFuser, novel fusion framework based on the State-Space Model (SSM) that efficiently unifies the strengths of VFMs and VLMs. SSMs [17, 71] are well-suited for capturing long-range dependencies with linear computational complexity, making them ideal for jointly adapting VFMs and VLMs with minimal overhead. Following recent advances in text-guided segmentation [39, 62, 70], we build MFuser on the text-queried Mask2Former [8] pipeline, where class text embeddings serve as queries for the segmentation decoder, enabling class-aware feature refinement. Specifically, we introduce MVFuser, Mamba-based co-adapter that jointly fine-tunes the two Visual models. By taking concatenated patch tokens (features) from both models at each layer, MVFuser models both sequential dynamics and spatial relationships among tokens in parallel. This enables effective interaction between the two feature types, enhancing the granularity of VLM features while also reducing trainable parameters. To further ensure cross-modality consistency between the fused visual features and VLM Text embeddings, we introduce MTEnhancer. MTEnhancer employs hybrid attention-Mamba architecture, leveraging the strengths of both model families. Visual features are used as conditional inputs within MTEnhancer, enabling effective sequence modeling that produces text embeddings closely related to visual content, resulting in image-conditioned text embeddings. Extensive experiments across diverse DGSS settings demonstrate that the proposed MFuser consistently outperforms existing state-of-the-art methods, achieving superior results in both synthetic-to-real and real-to-real scenarios. Contributions can be summarized into three aspects: We propose novel fusion framework, MFuser, to collaborate arbitrary pairs of VFMs and VLMs for DGSS, integrating the strengths of both without introducing significant computational overhead. We present MVFuser, Mamba-based co-adapter that enables joint fine-tuning of VFMs and VLMs, bridging the gap between these models and enhancing their complementary feature interactions. Additionally, we introduce MTEnhancer, hybrid attention-Mamba module that refines text embeddings with visual priors, ensuring superior cross-modal consistency and robust alignment. Extensive experiments show the proposed MFuser consistently outperforms state-of-the-art methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on realto-real benchmarks. 2. Related Works Domain Generalized Semantic Segmentation Domain Generalized Semantic Segmentation (DGSS) aims to develop models capable of generalizing to unseen domains without relying on target domain data during training. Common approaches include meta-learning, which exposes models to diverse tasks to learn features that are robust to domain shifts [26]; data augmentation techniques, such as style transfer and synthetic data creation, to introduce extensive visual diversity [5]; instance normalization and whitening [22, 41, 43, 57], which encourages the model to foucs on domain-invariant features rather than domain-specific styles. Some works also explore to design new architectures based on transformers [13, 21]. Recently, increasing attention has been paid to leveraging foundation models to enhance generalization [40, 55, 63]. Efforts have been taken to harness generative foundation models to creat new images [2], parameter-efficiently fine-tune VFMs [55], leverage textual semantics to guide invariance learning [40], etc. However, the complementary potential of combining VFMs and VLMs remains largely underexplored. Foundation Models Foundation models represent transformative approach in deep learning, focusing on pretraining networks on vast collection of unlabeled images. This pre-training equips the model with strong general representation capabilities, allowing it to be fine-tuned effectively for various downstream tasks. Initially popularized in Natural Language Processing (NLP), this paradigm has also drawn increasing attention in computer vision. In this paper, we refer to the vision-only pre-trained models as Vision Foundation Models (VFMs) including DINO [4] and DINOv2 [38], iBOT [69], MAE [20], SAM [28], etc. Visionlanguage pre-trained models are referred to as Vision Language Models (VLMs), which include CLIP [45], EVA02CLIP [16, 54], SIGLIP [65], etc. There are also generative foundation models such as Stable Diffusion [48, 56]. We focus on effectively combining VFMs and VLMs for DGSS. State Space Models for Visual Applications State-space models (SSMs) [18, 52] have emerged as promising alternatives for capturing long-range dependencies, offering linear scalability with sequence length. Building on the foundational S4 model [18], which introduced deep state-space modeling, SSMs have found applications across range of fields, including Natural Language Processing (NLP) [36], computer vision [71], medical applications [50]. Mamba [17] extended S4 by introducing hardware-aware design and selective scan mechanism, leading to the development of selective SSM called the S6 model. More recently, VMamba [71] emerged as fully Mamba-based architecture for vision tasks, while other studies [19] explored hybrid models combining Mamba and transformers. Unlike previous SSM-based efforts that primarily focus on creating entire backbone architectures, we take different approach by designing Mambabased adapters to efficiently fine-tune pre-trained VFMs and VLMs. This method enhances the adaptability and performance of VFMs and VLMs across various domains, leveraging Mambas efficiency to optimize existing models rather than training from scratch. 3. Preliminary Domain Generalized Segmantic Segmentation Given }NS the source images = {xS i=1 with corresponding ground truth masks = {yS i=1 where NS denotes the number of source images, and segmentation model , }NS composed of visual encoder followed by segmentation decoder D, namely = E, domain generalized semantic segmentation (DGSS) aims to train the network to generalize to unknown target domains. With the advancements in foundation models, recent DGSS methods increasingly leverage their strong generalization capabilities to design effective visual encoders [55, 63]. Semantic Segmentation with Text Queries Recent segmentation frameworks like Mask2Former [8], utilize query-based mechanism where learnable object queries serve as dynamic pointers to direct the models focus on relevant regions. Building on this, recent studies have increasingly leveraged the image-text alignment capabilities of Vision Language Models (VLMs) to design text-based queries [12, 30, 31, 35, 39, 62, 70]. The text embeddings produced by VLMs have been found to be inherently domain-invariant, capturing semantic information that remains consistent across various contexts and visual styles. This domain invariance stems from the VLM training process, which associates textual descriptions with diverse visual inputs, effectively disentangling semantic content from domain-specific features. The domain invariance of text embeddings forms basis for promoting the domain generalization of visual features. In this paper, we follow similar pipeline which utilizes the text embeddings of each class as the queries in Mask2Former decoder. Formally, the visual encoder EVLM of VLM serves as the encoder of the segmentation model, the aligned text encoder EVLM generates class embeddings qt = [t1, t2, ..., tC] for each class label name {classk}C k=1. qt will be used to design queries or conditional queries of the decoder [39, 62, 70]. 4. Proposed Method In this section, we introduce the Mamba-based foundation models fuser (MFuser), framework designed to integrate an arbitrary VFM with CLIP-like VLM using Mask2Former decoder for DGSS. Fig. 2 illustrates the overall architecture of MFuser. MFuser enhances feature locality while leveraging domain-invariant semantic knowledge provided by text embeddings to effectively constrain visual representations. The core components of this framework include MVFuser and MTEnhancer. MVFuser jointly finetunes the visual encoders of both models in parameterefficient manner, fusing their features to maximize synergy. MTEnhancer enriches the text queries by incorporating visual features, enhancing semantic alignment and feature robustness. 4.1. MVFuser Due to the large number of parameters in the VFM and VLM visual encoders, fully fine-tuning all parameters is impractical. Instead, we propose the introduction of additional Figure 2. Overall architecture of MFuser. MFuser takes inputs through both VFM and VLM visual encoders. Features from each encoder layer are concatenated and refined in MVFuser, which captures sequential and spatial dependencies in parallel. The refined features are then added back to the original features and passed to the next layer. MTEnhancer strengthens text embeddings of each class by integrating visual features through hybrid attention-Mamba mechanism. The enhanced text embeddings serve as object queries for the Mask2Former decoder, alongside multi-scale visual features. During training, only MVFusers, MTEnhancers, and the segmentation decoder are trainable while the VFM and VLM remain frozen, preserving their generalization ability and enabling efficient training. Note that skip connections between each block of MTEnhancer are omitted for clarity. modules, MVFuser, to refine visual features while keeping the original encoder parameters frozen. This design offers several advantages. First, the distinct characteristics of the two visual encoders could be compromised by full fine-tuning, whereas adapter-style finetuning preserves their original strengths while mitigating their weaknesses. Second, refining features from both encoders through shared MVFuser encourages effective interaction between the two feature types. Specifically, the visual encoders of VFMs and VLMs are composed of an image tokenizer layer and consecutively connected transformer blocks {Bi}N i=1. The image tokenizer layer first converts 2D image into flatten patch tokens xp RT D, where represents the length of the patch sequence and denotes the feature dimension. Normally, xp is input into the transformer blocks to calculate features. The process is as follows: x1 = B1(xp), xi = Bi(xi1), (1) , respectively. As stated, xVFM where xi is the token features output by Block Bi. The features for VFM and VLM can be denoted as xVFM , and xVLM i exhibits finer granularity, from which xVLM can benefit through the interaction. We propose ini serting the MVFuser at each block to bridge the two visual encoders, encouraging layer-wise interaction of the two models. MVFuser receives both xVFM as input, the learned feature offsets are then added back to xVFM and xVLM , respectively, enabling multi-level feature refinei and xVLM ment where one MVFuser refines the features from both encoders: [xVFM = xVFM ; xVLM + xVFM ] = MVFuser([xVFM = xVLM ; xVLM ]), + xVLM , xVLM , (2) (3) xVFM and xVLM where xVFM VFM and VLM, respectively. xVFM the refined features. are the learned feature offsets for and xVLM symbolize and xVLM MVFuser acts two roles: 1) refines xVFM to generate more task-specific features; 2) interacts between two kinds of features to complement eachs weaknesses. natural idea to capture inter-token relationship is to employ self-attention mechanism. However, the sequence length is doubled with the features from the two encoders. Applying the attention mechanism in transformers for adaptation is inefficient due to the quadratic increase in computational complexity with token count. While introducing learnable tokens and applying cross-attention between learnable tokens and patch token features can reduce this computational cost, it struggles to capture inter-token dependencies effectively. To address these challenges, we design fusion module based on state-space models for efficient long-range sequence modeling. Core of the MVFuser The architecture of MVFuser is shown in Fig. 2. Token features from both encoders are concatenated to form the input to MVFuser. Following bottleneck design, MVFuser first projects the concatenated token features to lower-dimensional space, models inter-token dependencies, and then projects them back to the original feature dimension. We modify the original Mamba block to encourage the two branches to capture the sequential dynamics and spatial relationships respectively in parallel. x(seq) x(spa) = SSM(conv(proj([xVFM ; xVLM ; xVLM ])). ]))), (4) (5) = conv(proj([xVFM Note that we omit the activation and normalization layers for clarity. Finally, gating mechanism is applied between the outputs of the two branch to improve generalization, followed by projection layer to recover the feature dimension. [xVFM ; xVLM ] = proj(x(seq) x(spa) ), (6) where denotes the element-wise multiplication. 4.2. MTEnhancer Text embeddings have been utilized as queries in semantic segmentation by framing the task as matching problem between representative class queries and image patch features, or by serving as the initial object queries for the Mask2Former decoder. This approach leverages the domain-invariant semantic information embedded in text to enhance the models ability to accurately identify and segment relevant regions within an image [62, 70]. Unlike previous methods, which typically assume that visual features and text embeddings are already aligned in pretrained VLM, our approach enhances the original text embeddings from VLM by incorporating the fused visual priors through the proposed MTEnhancer. MTEnhancer is designed to enriches text embeddings by modeling their relationships with fused image tokens. As illustrated in Fig. 2, MTEnhancer is hybrid architecture combining an attention block, conditional Mamba block, and an MLP, leveraging the strengths of diverse model architectures. The attention block encodes inter-class relationships, while the conditional Mamba block integrates image tokens into the text embeddings. While the Mamba block excels at processing long token sequences, its use in cross-attention mechanisms remains largely unexplored. To efficiently leverage the unidirectional scan order inherent to Mamba, we propose concatenating two copies of text embeddings at both sides of the image token, together they serve as the input of the Mamba block. Each block within MTEnhancer is implemented with residual connections. [qt; xv; qcopy qt = qt + Attention(qt), ] = Mamba([qt; xv; qcopy qt = qt + qt + qcopy , qt = qt + MLP(qt), ]), (7) (8) (9) (10) where xv represents the fused visual features output by the encoders final heads. qt is denoted without distinguishing between updates throughout the process. We adopt the approach of using enhanced text embeddings qt as object queries for Mask2Former decoder [39, 62]. Training Objective We train the framework with the prediction-level segmentation loss together with the featurelevel alignment loss. For the segmentation loss, we follow the standard Mask2Former [8]: Lseg = λbceLbce + λdiceLdice + λclsLcls, (11) where Lbce, Ldice, Lcls represent the binary cross-entropy loss and the dice loss for the predicted masks, and the crossentropy loss for each queried proposal, respectively. Additionally, we enforce pixel-level vision-language alignment using pixel-text alignment loss to ensure that textual semantics are precisely mapped to corresponding image regions [46]. The experiments involve three VLMs: CLIP, EVA02-CLIP, and SIGLIP. We apply SoftMax loss for CLIP and EVA02-CLIP, and Sigmoid loss for SIGLIP, consistent with the loss functions used during each VLMs original training. Therefore, the overall training loss is: Ltotal = Lseg + Lalign. (12) 5. Experiments 5.1. Settings Datasets We evaluate the performance of MFuser on both synthetic-to-real, clear-to-adverse-weather, and real-to-real scenarios are involved. As synthetic datasets, GTAV [47] contains 12,403, 6,382, and 6181 images for training, validation, and testing, respectively, at resolution of 19141052. As real-world datasets, Cityscapes [11] comprises 2,975 images for training and 500 images for validation, with resolution of 20481024. BDD100K [64] includes 7,000 and 1,000 images for training and validation, each at 12801024 resolution. Mapillary [37] consists of 18,000 training and 2,000 validation images, with varying resolutions across the dataset. We also include the clear-toadverse-weather generalization in the supplement. Network Architecture To make comprehensive evaluation of the proposed MFuser, we employ the VFM of DINOv2 [38], and VLMs including CLIP [45], EVA02CLIP [54], SIGLIP [65]. For the segmentation decoder, we follow tqdm [40] which modifies standard Mask2Former decoder by replacing the randomly initialized object queries with the enhanced class embeddings. Thus, the text object queries are set to 19 to match the number of classes. Implementation Details We keep the parameters of the VFM and VLM frozen and only train the MVFuser, MTEnhancer and the segmentation decoder. We use the same training configuration on all VLM alternatives and both generalization setups. We also apply prompt tuning for the text encoder, similar to [40]. All experiments are conducted with the input size of 512512, batch size of 2 and learning rate of 1e-4. Following [40, 55], AdamW optimizer is employed with linear warm-up over twarm = 1.5k iterations, followed by linear decay. Standard augmentations for segmentation tasks are applied, including random scaling, random cropping, random flipping, and color jittering. All experiments are conducted on one 24GB RTX A5000. 5.2. Comparison with State-of-The-Art Methods We compare our MFuser with existing DGSS methods on two setups: synthetic-to-real (G{C, B, M}) and realto-real (C{B, M}). Three VLMs are involved together with DINOv2, namely CLIP, EVA02-CLIP, and SIGLIP, all of Large types. We mainly compare with recent foundation model-based approaches, including CLOUDS [2], VLTseg [25], Rein [55], SET [63], and tqdm [40]. Several conventional methods are also involved. We provide results on Synthia [49] and ACDC [51] in the supplement. Synthetic-to-Real Generalization Tab. 1 compares the performance of the proposed MFuser with existing stateof-the-art DGSS methods under the synthetic-to-real setup. For each combination of the VFM and VLMs, we consistently outperform the existing methods on all benchmarks by large margin. In particular, our MFuser with the EVA02-CLIP model improves the GB benchmark by 1.49 mIoU. On average, we achieve 2.15 mIoU better than the state-of-the-art. Our proposed MFuser remains excellent performance using different VFM and VLM combinations, showing the versatility of our framework. To better understand how the proposed MFuser improves the feature generalization, Fig. 6 shows the qualitative comparison with the most recent methods, Rein [55] and tqdm [40]. Our method identifies fine-grained differences more effectively. Real-to-Real Generalization As shown in Tab. 2, we compare the performance of MFuser with existing stateof-the-art DGSS methods under the real-to-real setting. MFuser largely surpasses the existing methods with all three VLMs. Specifically, we improve the CB benchmark by 0.74 mIoU, and the CM benchmark by 1.7 mIoU. An overall improvement of 1.43 mIoU is achieved. 5.3. In-Depth Analysis Efficiency Analysis MVFuser is more efficient than self - attention-based adapters, which have quadratic complexity in modeling inter-patch relationships. To evaluate this, we replace MVFuser with 3 self-attention-based adapters while keeping all other components intact: self-attn(concat.): attn(q, k, v=concat(FVFM, FVLM)); self-attn(separate): {attn(q=FVFM, k, v=FVLM), attn(q=FVLM, k, v=FVFM)}. Performance comparison (mIoU in %) under the Table 1. synthetic-to-real setting (G{C, B, M}). DINOv2 [38] is used as the VFM for all MFuser variants, showing only the applied VLMs. Our method is marked in gray . The best and second-best results are highlighted in bold and underlined, respectively. Method Backbone synthetic-to-real GC GB GM Avg. SAN-SAW [43] WildNet [29] SHADE [66] TLDR [27] FAMix [14] RN101 RN101 RN101 RN101 RN101 SHADE [67] MiT-B5 IBAFormer [53] MiT-B5 CLIP-B VLTSeg [25] CLOUDS [2] VLTSeg [25] Rein [55] Rein [55] SET [63] tqdm [40] MFuser MFuser MFuser ConvNeXt-L EVA02-L EVA02-L DINOv2-L DINOv2-L EVA02-L CLIP-L SIGLIP-L EVA02-L 45.33 45.79 46.66 47.58 49.47 53.27 56.34 47.50 60.20 65.60 65.30 66.40 68.06 68.88 71.24 71.10 70.19 41.18 41.73 43.66 44.88 46.40 48.19 49.76 45.70 57.40 58.40 60.50 60.40 61.64 59.18 61.08 61.19 63. 40.77 47.08 45.50 48.80 51.97 54.99 58.26 54.30 67.00 66.50 64.90 66.10 67.68 70.10 71.14 71.71 71.28 42.43 44.87 45.27 47.09 49.28 52.15 54.79 49.17 61.50 63.50 63.60 64.30 65.79 66.05 67.82 68.00 68. Table 2. Performance comparison (mIoU in %) under the real-toreal setting (C{B, M}). DINOv2 [38] is used as the VFM for all MFuser variants, showing only the applied VLMs. Our method is marked in gray . The best and second-best results are highlighted in bold and underlined, respectively."
        },
        {
            "title": "Backbone",
            "content": "SAN-SAW [43] RN101 RN101 WildNet [29] RN101 SHADE [66] HGFormer [13] VLTSeg [25] Rein [55] Rein [55] SET [63] tqdm [40] MFuser MFuser MFuser Swin-L EVA02-L EVA02-L DINOv2-L DINOv2-L EVA02-L SIGLIP-L CLIP-L EVA02-L real-to-real Avg. 54.73 47.01 50.95 61.50 64.40 64.10 65.00 65.07 64.72 65.44 65.58 65.81 61.27 50.94 60.67 72.10 76.40 69.50 72.30 75.67 76.15 77.97 78.10 77.93 58.00 48.98 55.81 66.80 70.40 66.80 68.65 70.37 70.44 71.71 71.84 71. Table 3. Efficiency analysis. The experiments are conducted with DINOv2 and EVA02-CLIP models under the G{C, B, M} settings. The best results are highlighted in bold. Params. (M) FLOPs (G) self-attn (concat.) self-attn (separate) bi-deform-attn MVFuser 4.20 8.40 3.35 1.67 98.64 71.08 34.65 17.21 B Avg. 70.24 62.31 71.11 67.89 69.68 61.91 70.85 67.48 69.46 61.17 70.11 66.91 70.19 63.13 71.28 68.20 road sidew. build. wall fence pole tr. light tr. sign veget. terrain sky person rider car truck bus train m.bike bike n/a. Figure 3. Qualitative results on unseen target domains under the G{C, B, M} setting. MFuser is compared with Rein [55] and tqdm [40]. bi-deform-attn applies self-attn(concat.) using bidirectional deformable self-attention from Deformable DETR [72]. Tab. 3 summarizes efficiency and results, with parameters and FLOPs per adapter (using DeepSpeed package, batch size=2). MVFuser achieves the best while significantly reducing parameters and FLOPs. Table 4. Ablation studies on the vision feature fusion under the G{C, B, M} setting. DINOv2 and EVA02-CLIP are applied as the VFM and the VLM, respectively. w.o finetune: directly concatenate features of the two encoders; Conv: utilize convolution layers for fusion; Cross-Attention: implement cross-attention in [55] for fusion. The best results are highlighted in bold. Foundation Model Ensemble It is natural to consider ensembling multiple foundation models to enhance performance. To rigorously assess the effectiveness of the proposed MFuser, we address the following questions: 1) Is simply combining multi-encoder features sufficient to achieve the desired results? 2) Can any parameter-efficient fine-tuning method alone achieve comparable results? To answer the first question, we replaced the MVFuser with simple concatenation of features from the VFM and VLM visual encoders. We also evaluated using only the VFM or VLM visual features independently. As shown in Tab. 4, merely concatenating the features from both encoders does not yield satisfactory results and even performs worse than using only VFM or VLM features alone. This occurs because the frozen VFM features are not aligned with the text queries when both are input into the decoder. Additionally, the alignment between VLM visual features and text queries is compromised when the VLM features are mixed with the VFM features. Furthermore, fully fine-tuning both encoders is challenging. For example, fully fine-tuning the EVA02-CLIP visual encoder alone requires 480GB A100 GPUs for 20 hours, as reported in [40], which imposes significant computational burdenlet alone the cost of fine-tuning two encoders simultaneously. Alternatively, our MFuser keeps the original VFM and VLM parameters fixed and introduces an additional fusion block, MVFuser, which acts as bridge between the two foundation models. By optimizing only the MVFuser, we not only adapt the features of both encoders Fusion Choice M VFM-only VLM-only w.o Fintune Convolution Cross-Attention Sep. MVFuser MVFuser 67.68 68.26 66.96 69.28 69.67 69.57 70.19 60.82 60.02 58.88 61.45 60.52 62.88 63. 66.89 70.18 68.25 69.78 70.43 70.59 71.28 Avg. 65.13 66.15 64.70 66.83 66.87 67.68 68.20 to be more effective but also facilitate interactions between them. Consequently, our method provides more efficient and effective approach for promoting DGSS with foundation models, achieving the best performance with only 15 hours of training on single 24GB GPU. Fig. 4 shows that our proposed MVFuser significantly improves the localization and robustness of the features. To answer the second question, we implement two alternative adapters to fine-tune the two encoders, based on convolution and attention mechanisms, respectively. For the convolution-based adapter, we first reshape the 1D patch sequence into 2D feature map and then employ an architecture similar to the spatial branch of the MVFuser, replacing 1D convolutions with 2D convolutions. The attentionbased adapter reimplements Rein [55] to jointly fine-tune both encoders using single set of learnable tokens through cross-attention. We do not include self-attention-based adapter due to its quadratic computational cost with respect to the number of tokens, which makes it impractical. As shown in Table 4, our Mamba-based MVFuser Table 5. Ablation studies on the used foundation models. VFM + VLM: only the visual encoder is used when VLM serve as the VFM. The experiments are conducted under the G{C, B, M} setting. The best results are highlighted in bold. VFM + VLM SIGLIP + EVA02 CLIP + EVA02 DINOv2 + CLIP DINOv2 + SIGLIP DINOv2 + EVA02 68.48 68.78 71.24 71.10 70. 60.98 61.17 61.08 61.19 63.13 69.26 70.21 71.14 71.71 71.28 Avg. 66.24 66.72 67.82 68.00 68.20 Table 6. Ablation studies on the text embeddings enhancement. Experiments use DINOv2 and EVA02-CLIP under the G{C, B, M} settings. The best results are highlighted in bold. Enhancement Choice M w.o. Enhance w.o. Hybrid Cross-Attention MTEnhancer 69.57 69.62 69.88 70.19 60.83 61.90 61.26 63. 70.32 70.67 70.78 71.28 Avg. 66.91 67.40 67.31 68.20 Text Queries Enhancement Solely using class names to obtain text embeddings for each class may not adequately adapt to diverse image types. Encoding image-specific information with text embeddings has been common practice. In this section, we evaluate the effectiveness of the proposed MTEnhancer under the G{C, B, M} setting using DINOv2 and EVA02-CLIP. As demonstrated in Tab. 6, the advantages provided by MTEnhancer are evident. Notably, the hybrid architecture that incorporates self-attention with the conditional Mamba proves to be effective. Furthermore, MTEnhancer outperforms the approach of utilizing cross-attention to encode visual priors. 6. Conclusions In this work, we proposed MFuser, novel fusion framework designed to integrate VFMs and VLMs for DGSS. By leveraging the complementary strengths of VFMs and VLMs, MFuser addresses the challenges of increased patch tokens through efficient, scalable fusion with linear complexity. The framework incorporates two key components: MVFuser, which jointly fine-tunes VFMs and VLMs to enhance feature interaction, and MTEnhancer, which refines text embeddings using image priors for better alignment and robustness. Extensive experimental results demonstrate that MFuser achieves precise feature localization and robust text alignment while outperforming state-of-the-art DGSS methods across various benchmarks. The study underscores the potential of combining VFMs and VLMs to achieve superior generalization capabilities in semantic segmentation tasks, and highlights MFusers effectiveness in advancing DGSS by improving generalization to unseen domains without adding significant computational overhead. Figure 4. PCA visualization of features from DINOv2 and EVA02-CLIP, illustrating how MVFuser-based adaptation refines their distributions before and after tuning. significantly outperforms both the convolution-based and attention-based adapters. This is understandable, as the convolution-based adapter captures only local information, while cross-attention struggles to model token dependencies. Conversely, the Mamba-based MVFuser efficiently captures sequential dynamics with linear complexity. In our implementation of MVFuser, VFM features are concatenated before VLM visual features, aiming to enhance VLM features through Mambas sequential modeling. To evaluate this, we implemented separate MVFuser for DINOv2 and EVA02-CLIP, disentangling their connection. It can be observed from Tab. 4 that this leads to performance drops, demonstrating the effectiveness of feature interaction. We provide more insights into MVFusers effectiveness in the supplement. Foundation Model Choices It remains uncertain whether the performance gain arises from the complementary effects between the VFM and the VLM, or if any two foundation models could achieve similar results. Our method is based on the premise that, while both VFMs and VLMs demonstrate strong robustness, they possess distinct properties due to their different training principles. Consequently, MFuser leverages these differences to complementarily enhance the models generalization capabilities. To verify this, we conduct experiments using two VLMs, where the additional VLM serves as the VFM by utilizing only its visual encoder. Two combinations are tested: SIGLIP + EVA02-CLIP and CLIP + EVA02-CLIP with EVA02-CLIP functioning as the VLM while SIGLIP or CLIP acts as the VFM. Evaluation is conducted under the G{C, B, M} setting, and results are presented in Tab. 5. Both combinations show slight performance improvements over the VLM-only in Tab. 4, yet they fall significantly short of any VFM + VLM pairing. This suggests that the complementary effects between VFMs and VLMs are much more significant than those observed among VLMs alone. Additional evaluations on other VFMs beyond DINOv2 are provided in the supplement. Mamba as Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Evaluate on Additional VFMs Besides DINOv2 in the main text, we additionally evaluate VFMs, BEiT2 [44] and iBOT [69]. Both of them are of the Large size. EVA02-CLIP is utilized as the VLM. As shown in Tab. 7, they also improve the performance of solely using VLM. Table 7. Ablation studies on more VFMs under the G{C, B, M} setting. EVA02-CLIP is utilized as the VLM by default. BEiT2 [44] and iBOT [69] are evaluated as VFMs, respectively. Both are of Large types. M VLM-only + BEiT2-L + iBOT-L 68.26 69.60 69.37 60.02 60.19 60.76 70.18 70.39 70.53 Avg. 66.15 66.73 66. 8. Evaluate on SYNTHIA Benchmarks We compare the performance of the proposed MFuser with existing state-of-the-art DGSS methods under the Synthia{C, B, M} (as shown in Tab. 8), GSynthia and CSynthia (as shown in Tab. 9) settings. MFuser achieves the best performance on all settings. 9. Evaluate on ACDC Benchmarks We compare the performance of the proposed MFuser with existing state-of-the-art DGSS methods under the clear-toadverse-weather setting. Models are trained on Cityscapes and tested on ACDC which is composed of four domains, namely fog, night, rain and snow. As shown in Tab. 10, we consistently outperform the existing methods by large margin. Particularly, we surpass SET on rain by 3.79 mIoU. Table 8. Performance comparison (mIoU in %) under the synthetic-to-real setting (S{C, B, M}). Note that we implement DINOv2 [38] as the VFM and EVA02-CLIP [16] as the VLM. Our method is marked in gray . The best and second-best results are highlighted in bold and underlined, respectively. Method Backbone synthetic-to-real SC SB SM Avg. RN101 SAN-SAW [43] RN101 TLDR [27] IBAFormer [53] MiT-B5 Rein [55] SET [63] MFuser DINOv2-L DINOv2-L EVA02-L 40.87 42.60 50.92 48.59 49.65 54.17 35.98 35.46 44.66 44.42 45.45 46. 37.26 37.46 50.58 48.64 49.45 53.22 38.04 38.51 48.72 47.22 48.18 51.35 Table 9. Performance comparison (mIoU in %) under GS and CS. Note that we implement DINOv2 [38] as the VFM and EVA02-CLIP [16] as the VLM. Our method is marked in gray . The best and second-best results are highlighted in bold and underlined, respectively."
        },
        {
            "title": "Backbone",
            "content": "GSynthia CSynthia Rein [55] DINOv2-L DINOv2-L SET [63] EVA02-L tqdm [40] EVA02-L MFuser 48.86 50.01 53.32 54.04 48.56 49.61 50.62 54.13 Table 10. Performance comparison (mIoU in %) on CityscapesACDC. Note that we implement DINOv2 [38] as the VFM and EVA02-CLIP [16] as the VLM. Our method is marked in gray . The best and second-best results are highlighted in bold and underlined, respectively. Method Backbone clear-to-adverse-weather Fog Night Rain Snow Avg. IBN [41] IW [42] ISW [10] ISSA [32] CMFormer [3] Rein [55] SET [63] tqdm [40] MFuser RN50 RN50 RN50 MiT-B5 Swin-L DINOv2-L DINOv2-L EVA02-L EVA02-L 63.80 62.40 64.30 67.50 77.80 79.48 80.06 81.28 82.33 21.20 21.80 24.30 33.20 33.70 55.92 57.29 54.80 57. 50.40 52.40 56.00 55.90 67.60 72.45 74.80 72.92 78.59 49.60 47.60 49.80 53.20 64.30 70.57 73.69 72.41 74.93 46.25 46.05 48.60 52.45 60.85 69.61 71.46 70.35 73.45 10. Ablation on the Number of MVFusers We evaluate the effect of the number of MVFusers utilized for feature fusion. To do so, MVFuser is inserted after every blocks. As shown in Tab. 11, more MVFusers generally improve performance. Table 11. Ablation studies on the number of MVFusers under the G{C, B, M} setting. Note that we implement DINOv2 [38] as the VFM and EVA02-CLIP [16] as the VLM. 8 4 2 1 69.20 68.02 70.49 70. 61.85 61.69 62.71 63.13 69.24 69.96 70.78 71.28 Avg. 66.76 66.56 67.99 68.20 11. More Qualitative Results road sidew. build. wall fence pole tr. light tr. sign veget. terrain sky person rider car truck bus train m.bike bike n/a. Figure 5. Qualitative results on unseen target domains under the GM setting. MFuser is compared with Rein [55] and tqdm [40]. road sidew. build. wall fence pole tr. light tr. sign veget. terrain sky person rider car truck bus train m.bike bike n/a. Figure 6. Qualitative results on unseen target domains under the GB setting. MFuser is compared with Rein [55] and tqdm [40]."
        },
        {
            "title": "References",
            "content": "[1] Yihao Ai, Yifei Qi, Bo Wang, Yu Cheng, Xinchao Wang, and Robby Tan. Domain-adaptive 2d human pose estimation via dual teachers in extremely low-light conditions. In European Conference on Computer Vision, pages 221239. Springer, 2024. 1 [2] Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, and Stephane Lathuili`ere. Collaborating foundation models for domain generalized semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31083119, 2024. 3, 6 [3] Qi Bi, Shaodi You, and Theo Gevers. Learning contentenhanced mask transformer for domain generalized urbanscene segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 819827, 2024. 1 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 3 [5] Prithvijit Chattopadhyay, Kartik Sarangmath, Vivek Vijaykumar, and Judy Hoffman. Pasta: Proportional amplitude spectrum training augmentation for syn-to-real domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1928819300, 2023. 2 [6] Tingting Chen, Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan, and Robby Tan. Dual-rain: Video rain removal using assertive and gentle teachers. In European Conference on Computer Vision, pages 127143. Springer, 2024. [7] Zitan Chen, Zhuang Qi, Xiao Cao, Xiangxian Li, Xiangxu Meng, and Lei Meng. Class-level structural relation modeling and smoothing for visual representation learning. In Proceedings of the 31st ACM International Conference on Multimedia, pages 29642972, 2023. 1 [8] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. 2, 3, 5 [9] Junhyeong Cho, Gilhyun Nam, Sungyeon Kim, Hunmin Yang, and Suha Kwak. Promptstyler: Prompt-driven In style generation for source-free domain generalization. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1570215712, 2023. 2 [10] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1158011590, 2021. 1 [11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes In for semantic urban scene understanding. dataset Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. 5 [12] Anurag Das, Xinting Hu, Li Jiang, and Bernt Schiele. Mtaclip: Language-guided semantic segmentation with maskIn European Conference on Computer text alignment. Vision, pages 3956. Springer, 2024. 3 [13] Jian Ding, Nan Xue, Gui-Song Xia, Bernt Schiele, and Dengxin Dai. Hgformer: Hierarchical grouping transIn former for domain generalized semantic segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1541315423, 2023. 3, 6 [14] Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Perez, and Raoul de Charette. simple recipe for languageguided domain generalized segmentation. arXiv preprint arXiv:2311.17922, 2023. 6 [15] Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Perez, and Raoul de Charette. simple recipe for languageguided domain generalized segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2342823437, 2024. 2 [16] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: visual representation for neon genesis. Image and Vision Computing, page 105171, 2024. 2, 3, [17] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 2, 3 [18] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. 3 [19] Ali Hatamizadeh and Jan Kautz. Mambavision: hyarXiv preprint brid mamba-transformer vision backbone. arXiv:2407.08083, 2024. 3 [20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. [21] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Hrda: Context-aware high-resolution domain-adaptive semantic segmentation. In European Conference on Computer Vision, pages 372391. Springer, 2022. 3 [22] Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyond standardization towards efficient whitening. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 48744883, 2019. 2 [23] Wei Huang, Chang Chen, Yong Li, Jiacheng Li, Cheng Li, Fenglong Song, Youliang Yan, and Zhiwei Xiong. Style projected clustering for domain generalized semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30613071, 2023. 1 [24] Zeyi Huang, Andy Zhou, Zijian Ling, Mu Cai, Haohan Wang, and Yong Jae Lee. sentence speaks thousand images: Domain generalization through distilling clip with language guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11685 11695, 2023. 2 [25] Christoph Hummer, Manuel Schwonberg, Liangwei Zhong, Hu Cao, Alois Knoll, and Hanno Gottschalk. Vltseg: Simple transfer of clip-based vision-language representations for domain generalized semantic segmentation. arXiv preprint arXiv:2312.02021, 2023. [26] Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, and Kwanghoon Sohn. Pin the memory: Learning to generalize semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43504360, 2022. 2 [27] Sunghwan Kim, Dae-hwan Kim, and Hoseong Kim. Texture learning domain randomization for domain generalized segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 677687, 2023. 6, 1 [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3 [29] Suhyeon Lee, Hongje Seong, Seongwon Lee, and Euntai Kim. Wildnet: Learning domain generalized semantic segmentation from the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99369946, 2022. 6 [30] Qinqian Lei, Bo Wang, and Robby Tan. Ez-hoi: Vlm adaptation via guided prompt learning for zero-shot hoi detection. Advances in Neural Information Processing Systems, 37:5583155857, 2024. 3 [31] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023. 3 [32] Yumeng Li, Dan Zhang, Margret Keuper, and Anna Khoreva. Intra-source style augmentation for improved domain generalization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 509 519, 2023. [33] Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan, and Robby Tan. Nighthaze: Nighttime image dehazing via self-prior learning. arXiv preprint arXiv:2403.07408, 2024. 1 [34] Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan, Nightrain: NightShunli Zhang, and Robby Tan. time video deraining via adaptive-rain-removal and adaptiveIn Proceedings of the AAAI Conference on correction. Artificial Intelligence, pages 33783385, 2024. 1 [35] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3 [36] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. 3 [37] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages 4990 4999, 2017. 5 [38] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 3, 5, 6, [39] Byeonghyun Pak, Byeongju Woo, Sunghwan Kim, Daehwan Kim, and Hoseong Kim. Textual query-driven mask transformer for domain generalized segmentation. arXiv preprint arXiv:2407.09033, 2024. 2, 3, 5 [40] Byeonghyun Pak, Byeongju Woo, Sunghwan Kim, Daehwan Kim, and Hoseong Kim. Textual query-driven mask transformer for domain generalized segmentation. In European Conference on Computer Vision, pages 3754. Springer, 2025. 2, 3, 5, 6, 7, 1 [41] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In Proceedings of the european conference on computer vision (ECCV), pages 464479, 2018. 2, 1 [42] Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, and Ping Luo. Switchable whitening for deep representation learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 18631871, 2019. 1 [43] Duo Peng, Yinjie Lei, Munawar Hayat, Yulan Guo, and Wen Li. Semantic-aware domain generalized segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 25942605, 2022. 1, 2, 6 [44] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 1 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3, 5 [46] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1808218091, 2022. [47] Stephan Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 102118. Springer, 2016. 5 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [49] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio Lopez. The synthia dataset: large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32343243, 2016. 6 [50] Jiacheng Ruan and Suncheng Xiang. Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491, 2024. 3 [51] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. ACDC: The Adverse Conditions Dataset with Correspondences for semantic driving scene understanding. In ICCV, 2021. 6 [52] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. 3 [53] Qiyu Sun, Huilin Chen, Meng Zheng, Ziyan Wu, Michael Felsberg, and Yang Tang. Ibaformer: Intra-batch attention transformer for domain generalized semantic segmentation. arXiv preprint arXiv:2309.06282, 2023. 6, [54] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 3, 5 [55] Zhixiang Wei, Lin Chen, Yi Jin, Xiaoxiao Ma, Tianle Liu, Pengyang Ling, Ben Wang, Huaian Chen, and Jinjin Zheng. Stronger fewer & superior: Harnessing vision foundation models for domain generalized semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2861928630, 2024. 2, 3, 6, 7, 1 [56] Yihang Wu, Xiao Cao, Kaixin Li, Zitan Chen, Haonan Wang, Lei Meng, and Zhiyong Huang. Towards better text-toimage generation alignment via attention modulation. arXiv preprint arXiv:2404.13899, 2024. 3 [57] Qi Xu, Liang Yao, Zhengkai Jiang, Guannan Jiang, Wenqing Chu, Wenhui Han, Wei Zhang, Chengjie Wang, and Ying Tai. Dirl: Domain-invariant representation learning for generalizable semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2884 2892, 2022. 2 [58] Weilong Yan, Robby T. Tan, Bing Zeng, and Shuaicheng Liu. Deep homography mixture for single image rolling shutter correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 98689877, 2023. 1 [59] Xin Yang, Michael Bi Mi, Yuan Yuan, Xin Wang, and Robby Tan. Object detection in foggy scenes by embedding depth and reconstruction into domain adaptation. In Proceedings of the Asian Conference on Computer Vision, pages 10931108, 2022. [60] Xin Yang, Wending Yan, Yuan Yuan, Michael Bi Mi, and Robby Tan. Semantic segmentation in multiple adverse weather conditions with domain knowledge retention. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 65586566, 2024. [61] Xin Yang, Yan Wending, Michael Bi Mi, Yuan Yuan, and Robby Tan. End-to-end video semantic segmentation in adverse weather using fusion blocks and temporal-spatial teacher-student learning. Advances in Neural Information Processing Systems, 37:141000141020, 2025. 1 [62] Jong Chul Ye, Yujin Oh, et al. Otseg: Multi-prompt sinkhorn attention for zero-shot semantic segmentation. In The 18th European Conference on Computer Vision, ECCV 2024. European Computer Vision Association (ECVA), 2024. 2, 3, 5 [63] Jingjun Yi, Qi Bi, Hao Zheng, Haolan Zhan, Wei Ji, Yawen Huang, Yuexiang Li, and Yefeng Zheng. Learning spectraldecomposited tokens for domain generalized semantic segIn Proceedings of the 32nd ACM International mentation. Conference on Multimedia, pages 81598168, 2024. 2, 3, 6, 1 [64] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26362645, 2020. 5 [65] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 3, 5 [66] Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, and Gim Hee Lee. Style-hallucinated dual consistency learning for domain generalized semantic segmentation. In European conference on computer vision, pages 535552. Springer, 2022. 1, [67] Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, and Gim Hee Lee. Style-hallucinated dual consistency learning: unified framework for visual domain generalization. IJCV, 2023. 6 [68] Zhun Zhong, Yuyang Zhao, Gim Hee Lee, and Nicu Sebe. Adversarial style augmentation for domain generalized urban-scene segmentation. Advances in neural information processing systems, 35:338350, 2022. 1 [69] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021. 3, 1 [70] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1117511185, 2023. 2, 3, 5 [71] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. 2, 3 [72] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transarXiv preprint formers for end-to-end object detection. arXiv:2010.04159, 2020."
        }
    ],
    "affiliations": [
        "ASUS Intelligent Cloud Services",
        "National University of Singapore"
    ]
}