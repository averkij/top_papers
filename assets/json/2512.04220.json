{
    "paper_title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "authors": [
        "Wenlong Deng",
        "Yushu Li",
        "Boying Gong",
        "Yi Ren",
        "Christos Thrampoulidis",
        "Xiaoxiao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM."
        },
        {
            "title": "Start",
            "content": "ON GRPO COLLAPSE IN SEARCH-R1: LIKELIHOOD-DISPLACEMENT DEATH SPIRAL"
        },
        {
            "title": "THE LAZY",
            "content": "Wenlong Deng1,2,,, Yushu Li1,, Boying Gong3,,Yi Ren1, Christos Thrampoulidis1, Xiaoxiao Li1,2 1University of British Columbia, 2Vector Institute, 3UC Berkeley Project Leader, Equal Contribution, Corresponding author"
        },
        {
            "title": "ABSTRACT",
            "content": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1 (Jin et al., 2025), offers fast convergence and value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients and ultimately causing collapse. We empirically characterize this process across models on SearchR1-style, search-integrated question answering task, revealing consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose lightweight likelihood-preserving regularization LLDS for GRPO that activates only when trajectorys likelihood decreases, and regularize only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as fundamental bottleneck in GRPO-based TIRL and provide practical path toward stable, scalable training of tool-integrated LLM."
        },
        {
            "title": "INTRODUCTION",
            "content": "5 2 0 2 3 ] . [ 1 0 2 2 4 0 . 2 1 5 2 : r Figure 1: Comparative performance of LLDS and baseline methods on benchmark datasets. All baselines are built upon Qwen2.5-7B-Instruct. See Sec. 6.1 for details. Large language models (LLMs) increasingly leverage external tools, such as search engines (Jin et al., 2025) and code-execution environments (Feng et al., 2025; Zeng et al., 2025), to augment their reasoning capabilities. 1 This tool-integrated reasoning (TIR) paradigm has driven recent progress across factual question answering (Jin et al., 2025), image-based reasoning (Jiang et al., 2025), and mathematical problem solving (Feng et al., 2025; Jiang et al., 2025). By enabling models to iteratively query tools, verify intermediate steps, and refine their thoughts, tool calls substantially elevate reasoning quality (Qin et al., 2023). These advances naturally motivate the use of reinforcement learning (RL) to train LLMs to plan, interact with tools, and master multi-step decision making, as instantiated by recent GRPO-based frameworks such as Search-R1 (Jin et al., 2025). However, extending TIR to the RL setting introduces severe and persistent training instability. In particular, GRPO training in Search-R1-style Jin et al. (2025), search-integrated QA pipelines frequently exhibit sudden reward drops and catastrophic collapse (Jin et al., 2025; Sun et al., 2025). These failures are especially pronounced in multi-turn settings (Xue et al., 2025), where tool feedback becomes part of the models conditioning context, creating long, highly entangled trajectories. Although prior work has observed these failures, the underlying mechanisms remain poorly understood. In this work, we firstly identify Lazy Likelihood Displacement (LLD) (Deng et al., 2025; Razin et al., 2024; Ren & Sutherland, 2024), the stagnation or reduction of likelihood for both correct and incorrect responses during GRPO optimization (Deng et al., 2025), as the fundamental but previously overlooked source of collapse in Search-R1style tool-integrated reinforcement learning (TIRL). Using search-integrated question answering as case study (see Fig. 2), we show that LLD emerges early and persistently: even as rewards increase, the likelihood of correct responses enters monotonic decline. This behavior appears across model scales, indicating that LLD is structural failure mode of GRPO-based TIRL rather than configuration-specific artifact. We further demonstrate that LLD drives self-reinforcing LLD death spiral, where the resulting low-confidence regime amplifies negative-gradient influence from incorrect trajectories, accelerating likelihood decay, triggering entropy spikes, inflating likelihood ratios, and ultimately causing the large-gradient instability that leads to collapse. To counteract this failure mode, we propose simple yet effective likelihood-preserving regularization LLDS that prevents harmful likelihood reductions. Our method integrates seamlessly with GRPO and introduces two layers of selectivity: (i) response-level gating, which activates the regularization only when trajectorys overall likelihood decreases, and (ii) token-level selectivity, which penalizes only the tokens responsible for the decrease. This fine-grained design directly mitigates LLD while minimally interfering with GRPOs optimization behavior. By preventing unintentional downward likelihood drift, LLDS keeps optimization away from the unstable high-gradient regime and maintains healthy training dynamics. Empirically, this translates into stable training, suppressed gradient explosion, and consistent gains across seven open-domain and multi-hop QA benchmarks. Our main contributions are threefold: We conduct the first systematic investigation of LLD in GRPO-driven TIRL and show that it arises with striking frequency, consistently progressing through characteristic failure trajectory: steady likelihood decay, followed by gradient amplification and entropy explosion, and ultimately collapse. We propose lightweight likelihood-preserving regularization LLDS that selectively regularize likelihood reductions and resolved the collapse issue in GRPO training. By stabilizing training, we realize significant performance gains across seven QA benchmarks (see Fig. 1), demonstrating more robust and reliable approach to RL-driven TIR."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Tool-Integrated Reasoning and Agentic LLMs. Tool use has emerged as powerful paradigm for equipping LLMs with adaptive reasoning capabilities. Early approaches relied on prompt-based orchestration (Lu et al., 2023; Shen et al., 2023) or multi-agent delegation frameworks to invoke tools without explicit training. Instruction-tuned models (Gou et al., 2023; Qin et al., 2023) later introduced structured tool-calling behaviors via supervised learning, but these systems remained largely static and were constrained to single-turn interactions. More recent work demonstrates that reinforcement learning can substantially enhance tool integration by enabling models to learn tool-usage policies through environmental feedback and task success. Notable systems such as RETool (Feng et al., 2025), VERL-Tool (Jiang et al., 2025), and agentic LLM frameworks (Mai et al., 2025) support multi-step reasoning through dynamic tool use, self-verification, and error correction. This transition from static instruction-following to feedback-driven optimization has proven effective across range of domains, including mathematical problem solving with code execution (Xue et al., 2025), open-domain question answering with retrieval (Jin et al., 2025), SQL generation from natural language (Jiang et al., 2025), and multi-modal visual reasoning (Gao et al., 2024). 2 Figure 2: We illustrate the likelihood displacement in tool-integrated RL training. The steady-decay phase (60-120) emerges when the reward begins to increase only gradually. In the subsequent acceleration phase (after step 120), the likelihood of correct responses drops sharply, accompanied by sudden surge in gradient magnitude (red star), leading to gradient explosion. zoomed-in view of the acceleration region further highlights this effect, showing clearer likelihood displacement, where the gradient accelerates rapidly while the reward starts to decline. Training Collapse in Tool-Integrated GRPO. GRPO (Guo et al., 2025) has gained popularity in reinforcement learning due to its value-free, outcome-driven formulation. However, applying GRPO to train LLMs for multi-turn tool-integrated reasoning remains highly unstable (Jin et al., 2025). When initialized from base models using only verifiable rewards (Zero RL), GRPO-trained models often experience catastrophic failure, marked by abrupt reward collapse (Xue et al., 2025; Mai et al., 2025; Jin et al., 2025). In contrast, Proximal Policy Optimization (PPO) (Schulman et al., 2017) tends to exhibit more stable behavior under comparable settings (Jin et al., 2025). These collapse dynamics have been consistently observed across various systems, including Search-R1 (Jin et al., 2025), SimpleTIR (Xue et al., 2025), and ZeroSearch (Sun et al., 2025). While prior work has largely focused on empirical observations of failure, only Xue et al. (2025) attempt to explain the phenomenon, attributing it to low-likelihood incorrect responses, which inflate importance weights and destabilize learning. Yet this explanation does not fully account for the structural causes of such low-likelihood trajectories. Notably, we observe that reward degradation often precedes the final collapse, suggesting deeper optimization issues. In this work, we identify Lazy Likelihood Displacement (LLD) as the core mechanism underlying GRPOs failure mode in multi-turn TIRL."
        },
        {
            "title": "3 PRILIMINARY",
            "content": "TIRL enables an LLM to interact with the external tools. We denote the query by x, the tool feedback by o, and the models action by y. For given query x, the model autoregressively generates an action at turn according to πθ(yt x, y0, o0, y1, . . . , ot1) , where yt is produced based on the accumulated context, and ot1 denotes the tool feedback returned at turn 1 if the previous action yt1 invoked tool (e.g., issuing search query). If no tool is invoked, ot1 is an empty string. The multi-turn setting therefore corresponds to trajectories that involve multiple tool calls and, consequently, multiple rounds of tool feedback. However, tool feedback ot is inherently out-of-distribution (OOD) for the pretrained language model, since such feedback originates from external environments rather than the models own generative distribution. Directly forcing the model to fit these feedback tokens introduces large traininginference mismatch and may cause the model to memorize environment-specific feedback. Therefore, previous works commonly mask out feedback tokens during supervised training to stabilize optimization and avoid overfitting to external content (Jin et al., 2025). 3 Tool-Integrated GRPO with Feedback Mask. The GRPO loss, introduced in DeepSeek-Math (Shao et al., 2024) and DeepSeek-R1 (Guo et al., 2025), enhances policy optimization by refining how reward and loss signals are computed. Unlike Proximal Policy Optimization (PPO) (Schulman et al., 2017), GRPO removes the need for value function estimation and instead uses group-relative reward normalization for optimization. Specifically, for queryanswer pair (x, a), the policy πθ samples responses: {(yi,0, oi,0, . . . , yi,t, oi,t, . . . , oi,Ti1, yi,Ti)}G where Ti denotes the number of tool calls, and thus determines the number of multi-turn interactions in the i-th rollout. Each action yi,t consists of yi,t tokens, and we denote by yi,t,<k the prefix consisting of its first 1 tokens. Let ri denote the reward assigned to the i-th response. The advantage for the t-th action of i-th response is defined via group-level normalization: ri µ σ = 1, . . . , ˆyi,t, ˆAi,t,k := i=1 , , (cid:113) i=1] and σ = where µ = (cid:98)E[{ri}G i=1] are the empirical mean and standard deviation of rewards within the group. Each token of the same trajectory shares the same normalized advantage. The tool-integrated GRPO objective with feedback mask is given by: JGRPO(θ) = (cid:100)Var[{ri}G (x,a)D {yi,0,...,yi,t,oi,t,yi,Ti }(πθold ,T ) (cid:34) 1 (cid:80)Ti t=1 ˆyi,t (cid:80)G i=1 (cid:88) Ti(cid:88) ˆyi,t (cid:88) i=1 t= k=1 (cid:16) γi,t,k(θ) ˆAi,t,k, ˆAi,t,k clip(cid:0)γi,t,k(θ), 1 ε, 1 + ε(cid:1)(cid:17) min (cid:35) , (1) ε is the = where πθ(yi,t,kx,...,yi,t1,oi,t1,yi,t,<k) πθold (yi,t,kx,...,yi,t1,oi,t1,yi,t,<k) . Although feedback tokens are masked out during loss computation, they still influence the context of subsequent token predictions. parameter, likelihood clipping γi,k(θ) defined ratio the as is"
        },
        {
            "title": "4 LAZY LIKELIHOOD DISPLACEMENT IN TOOL-INTEGRATED GRPO",
            "content": "Recent work (Deng et al., 2025) introduces Lazy Likelihood Displacement (LLD) in GRPO for text-based, non-tool settings, showing that the likelihood of correct responses often decreases or improves only marginally during optimization. In this section, we extend LLD to the tool-integrated RL regime and define: Definition 4.1 ( Tool-Lazy Likelihood Displacement) Let πθold and πθfin denote the initial and finetuned policies obtained before and after optimizing preference-learning objective (e.g., Eq. (1)) over dataset D, with (θfin) < (θold). Consider tool-integrated trajectory consisting of alternating actions and feedback, (y0, o0, y1, o1, . . . , yT ), where only the actions {yt}T t=0 are used in likelihood computation (feedback is masked). For each response action yt, define its log-likelihood change as t(x, yt) := ln πθfin(yt x, y<t, o<t) ln πθold(yt x, y<t, o<t). We say that Lazy Likelihood Displacement (LLD) occurs for the action if t(x, yt) ϵ, where ϵ is small or non-positive constant. And LLD occurs for the whole response if (cid:88) t= t(x, yt) ϵ, (2) Thus, LLD captures the failure mode where one or more response actions exhibit negligible or even negative improvement in likelihood under the optimized policy. To better understand the origin of LLD, we highlight how negative gradients suppress the likelihood of correct actions. The following informal theorem (the formal version and proof see Sec. A) restates Theorem 4.4 from Deng et al. (2025) in the setting of tool-integrated RL: Theorem 4.2 (Informal: Trajectory-Level LLD in Tool-Integrated GRPO) In tool-integrated GRPO, the likelihood of an otherwise correct response can decrease when (i) incorrect responses of low likelihood and (ii) incorrect responses whose embeddings closely resemble the correct one induce large negative gradients that dominate the positive updates. These forces jointly lead to trajectory-level Lazy Likelihood Displacement. In the setting of tool-integrated GRPO, we observed with striking frequency that the likelihood of correct responses fails to improve. This indicates an especially acute form of LLD (ϵ 0), causing progressive decay in the models overall output likelihood. This compounding decay is distinctive failure mode of Search-R1 style tool-integrated RL."
        },
        {
            "title": "5 LAZY LIKELIHOOD DISPLACEMENT IN TOOL-INTEGRATED GRPO",
            "content": "In this section, we illustrate the prevalence of LLD in tool-integrated GRPO training and show how the LLD (ϵ 0) can catastrophically drive the model into training collapse."
        },
        {
            "title": "5.1 LIKELIHOOD DYNAMIC",
            "content": "To illustrate the prevalence and progression of LLD during training, we visualize the likelihood-displacement trajectory in tool-integrated RL. As shown in Fig. 2, the evolution unfolds in three characteristic phases. Phase (early stagnation): the likelihood of correct responses remains nearly unchanged even as the reward increases, showing the initial emergence of LLD. The later two phases correspond to the ϵ 0 regime, where the likelihood of correct responses strictly decays rather than improves. Phase II (steady-decay, steps 60120): the likelihood exhibits slow but consistent downward drift, while the reward grows only gradually and the gradient norm stays stableindicating persistent LLD. Phase III (acceleration, after step 120): the likelihood begins to collapse sharply, coinciding with rapid surge in gradient magnitude (marked by the red star), which triggers gradient explosion and ultimately leads to training collapse. The zoomed-in view on the right highlights this transition: likelihood keeps decreasing while gradients accelerate upward, and the reward curve begins to fall. Although the final collapse is caused by exploding gradients, the likelihood decay is present throughout training, introducing an accumulating instability that becomes dramatically amplified during the acceleration phase (see Appendix Sec. B.1 for more discussion). 5.2 LAZY LIKELIHOOD DISPLACEMENT DEATH SPIRAL In Fig. 2, we observe an accelerated decline in response likelihood as training progresses. In this section, we first simplify notation by writing the feedback-masked trajectory as ˆy = (y0, y1, . . . , yT ). We then formally characterize this accelerated likelihood decay as Lazy Likelihood-Displacement (LLD) death spiral. Definition 5.1 (LLD Death Spiral) Consider tuple (x, ˆy+) and policy update from πθt to πθt+1. We say the system enters an LLD Death Spiral if the policy evolution exhibits the following self-reinforcing progression: LLDt = low = LLDt+1, where LLDt denotes lazy likelihood displacement at time with decreasing likelihood (ϵt 0), and low denotes low-confidence trajectories whose likelihood decreases under πθt. The transition LLDt = low reflects that degraded likelihood leads to increasingly diffuse predictions, while the transition low = LLDt+1 occurs because low-confidence incorrect responses often contain actions similar to those in correct responses, inducing larger value in Eq. (9) and thus stronger negative-gradient influence. When these effects compound across iterations, the likelihood decay accelerates, creating self-perpetuating collapse that we refer to as the LLD Death Spiral. ϵt+1 < ϵt 0 Then we demonstrate the LLD Death Spiral through accelerated entropy explosion and accelerated per-sample LLD. Accelerated Per-Sample LLD. We conduct controlled experiment on the searching-based question answering NQ (Kwiatkowski et al., 2019) dataset to examine how the negative gradient of GRPO affects the log-likelihood of correct responses. Using Qwen2.5-3B-Ins (Yang et al., 2024), we generate 8 rollouts per question and retain only those examples that contain mixture of correct and incorrect responses, discarding cases where all responses are uniformly correct or incorrect. To isolate the per-sample learning dynamics, we reinitialize the model parameters θ for each individual question, apply single GRPO update to obtain θ, and then measure the average log-likelihood change of the correct responses: (x) := 1 + + (cid:88) i=1 (cid:2)ln πθ( ˆy+ x) ln πθ( ˆy+ x)(cid:3) , (3) 5 Figure 3: Effect of likelihood displacement across different training iterations for the Qwen2.5-3B-Instruct model. Results are computed on the first 50 samples of the training set, discarding cases where all responses are uniformly correct or uniformly incorrect. Bars below zero (orange) indicate samples whose correct responses likelihood decreases after training. for question with + correct responses. We present the results in Fig. 3. In the early stage of training (iteration 50), only mild LLD is observed: most questions show negligible reductions in the likelihood of correct responses, likely because correct and incorrect trajectories remain structurally dissimilar. However, as training progresses, the effect becomes more pronounced. Increasing structural similarity between incorrect and correct responses leads to broader likelihood degradation, as shown by the rising orange curve. As training enters the acceleration phase (iterations 120140), the trend intensifies into an LD death spiral, with over half of the samples at iteration 140 showing substantial drops in the likelihood of correct responses. This collapse is driven by the extremely low likelihood of incorrect responses, which amplifies negative-gradient contributions and destabilizes learning. Accelerated Entropy Explosion. Fig. 4 visualizes the evolution of entropy, response length, and valid-search ratio during training for Qwen2.5-3B-Ins (a) and Qwen2.5-3B-Base (b). Across both models, mean token entropy initially increases slowly, mirroring the early slow-LLD regime in Fig. 2, but later transitions into phase of sharply accelerated growth that aligns with the acceleration phase of the LLD trajectory. This steepening entropy slope is clear demonstration of the death spiral: as low-confidence responses accumulate, the model assigns increasingly diffuse, low-probability distributions to its tokens, which further strengthens LD and drives the system toward instability. Crucially, this entropy acceleration occurs while response length and valid-search counts remain nearly unchanged, confirming that the rising entropy and the corresponding likelihood decay is not caused by trajectory length or tool-query frequency, but is instead direct manifestation of the LD effect itself. Figure 4: We illustrate how entropy, response length, and valid-search ratio evolve during training. For both Qwen2.5-3B-Instruct (a) and Qwen2.5-3B-Base (b), entropy exhibits accerlerrated upward trend prior to collapse indicating strong LD issue. Meanwhile, the response length and valid-search times remain stable in the early stages but later begin to fluctuate markedly and eventually drop sharply. 5.3 WATCH FOR CORRECT ACTIONS EMBEDDED WITHIN INCORRECT RESPONSES. Observing tool-integrated GRPO is substantially more susceptible to LLD than non-tool settings (Deng et al., 2025). This section examines LLD at the granularity of response actions, where each action is defined by the segments between consecutive tool calls. Our analysis on Qwen2.5-3B-Ins (Yang et al., 2024) reveals striking pattern unique to tool-integrated GRPO: correct actions frequently appear within otherwise incorrect responses, and this intermixed structure disrupts the likelihood estimation process, amplifying LLD and destabilizing training. Frequent correct actions in Incorrect responses. In our experiments, we found that the model typically uses its first response action to generate search query, and the correctness of this initial action increases steadily as training progresses. To be specific, we measure whether the retrieved documents produced by an incorrect response match those retrieved by correct response; if so, we treat the first action as correct. The accuracy of this initial action across different training stages is shown in Fig. 5. As indicated by the dashed green line, accuracy is low in the early stages, reflecting the models initial acquisition of search capability, but rises markedly over time. By step 140, roughly 60% of incorrect responses begin with correct search query, highlighting the high structural similarity shared by first-action actions. Faster likelihood decay of the first-action response. As illustrated in Fig. 5, we further observe that the likelihood of the first action (light blue) declines substantially more than that of the second action (blue). Early in training, the first-action likelihood is higher, largely because the second action must condition on out-of-distribution feedback. However, as training progresses, both actions exhibit likelihood decay, with the first action eventually dropping below the second around step 110, where the first-action correctness rate is roughly 50%, indicating strong similarity between correct and incorrect trajectories. After step 120, this decay accelerates sharply, marking the onset of the LLD death spiral, where low-likelihood incorrect partial signals further amplify degradation. Figure 5: Evolution of token log-likelihood (measured before vs. after feedback; left axis) and the observation-match ratio for wrong answers (right axis). With training, both likelihoods drop while the overlap between tool observations in incorrect and correct trajectories increases, suggesting many incorrect responses begin with correct search, which skews likelihood estimates and contributes to LLD. Nonsensical first-action. As LLD intensifies and the likelihood of the first action collapses, the model begins producing nonsensical outputs. As illustrated in Fig. 15 in the appendix, it eventually generates random, meaningless tokens, an effect driven by the severely reduced response likelihood, which causes sampling to select arbitrary words. Although full collapse may not yet have occurred at this stage, the model is already effectively unusable, as it can no longer produce meaningful responses. As result, it becomes essential to reduce the unintended penalization of correct actions within incorrect responses. more detailed discussion is provided in Sec. 7. 5.4 LLD REGULARIZATION To address Lazy Likelihood Displacement (LLD), we introduce family of likelihood-preserving regularizers that prevent the model from unintentionally reducing the likelihood of responses during GRPO training. Specifically, for the given preserving responses yi Ypre, we compare its token-level likelihood under the old policy (previous step) and the fine-tuned policy. Our base regularizer penalizes only the tokens whose likelihood decreases after an update. LLD: Token-Level Likelihood Preservation. LLD applies token-level penalty: within each response yi, only likelihood-reducing tokens contribute to the loss: LLLD = 1 (cid:80) yiYpre yi (cid:88) (cid:88) yiYpre yiyi max (0, ln πθold (yix, y<i) ln πθ(yiy<i)) (cid:125) (cid:123)(cid:122) (cid:124) Likelihood-reducing tokens (4) This token-level selectivity ensures that the model is penalized only for genuinely harmful likelihood reductions, without interfering with updates that improve the response overall. However, some responses may still be globally improving even if few individual tokens decrease; penalizing such cases may introduce overly strong constraints. LLDS: Response-Level Gating. To avoid unnecessary penalties on globally improving responses, LLDS introduces response-level gating mechanism: the penalty activates only when the total likelihood of the response decreases. The LLDS loss is: 7 LLLDS = (cid:80) yiYpre (cid:88) yi yiYpre (cid:34) (cid:88) yiyi 1 (cid:124) (ln πθold (yix, y<i) ln πθ(yiy<i)) > 0 (cid:35) (cid:123)(cid:122) Activated only when sum > 0 (cid:125) (cid:88) yiyi max (0, ln πθold (yix, y<i) ln πθ(yiy<i)) (cid:125) (cid:123)(cid:122) (cid:124) Likelihood-reducing tokens (5) This structure preserves normal GRPO learning while directly suppressing responses that suffer from LLD. LLDS-MA: Masking Answer Tokens. To further encourage multi-step reasoning and tool usage, we mask out final answer tokens from the regularization term. LLDS-MA penalizes likelihood reductions only on reasoning and tool-interaction tokens, excluding the answer span yi,Ans: (cid:88) yiyi/ yi,Ans (cid:124) max (0, ln πθold (yix, y<i) ln πθ(yiy<i)) (cid:123)(cid:122) Mask Answer Likelihood-reducing tokens (cid:125) Finally, We integrate the regularization into the GRPO objective as Ltotal = LGRPO + λLLLDS(MA) (6) (7) where λ is the regularization weight. We use LLDS as the default variant and switch to LLDS-MA when stronger encouragement of tool usage is desired. For the preserving set [y]pre, we include all responses with non-negative advantages ˆA 0, ensuring that correct responses ( ˆA > 0) and untrained responses ( ˆA = 0) do not suffer likelihood reduction. The effect of λ is examined empirically in Fig. 8."
        },
        {
            "title": "6 EXPERIMENTS AND ANALYSIS",
            "content": "We evaluate the empirical effectiveness of LLLDS(MA) through comprehensive experiments. Experimental settings. For training, we follow the setup in Jin et al. (2025) and conduct experiments using two model families: Qwen-2.5-3B and Qwen-2.5-7B, each in both Base and Instruct variants (Yang et al., 2024). We consider two training configurations. (1) NQ-Only (single-hop): the model is trained solely on the single-hop Natural Questions (NQ) dataset (Kwiatkowski et al., 2019). (2) NQ+Hotpot (single-hop+multihop): the model is trained on merged corpus combining NQ with HotpotQA (Yang et al., 2018), providing broader coverage of both open-domain and multi-hop reasoning. For the retrieval dataset, we use the 2018 Wikipedia dump (Karpukhin et al., 2020) as the knowledge corpus and employ E5 (Wang et al., 2022) as the dense retriever. To ensure fair comparison across retrieval-augmented baselines, we fix the number of retrieved passages to three, following the configuration in Jin et al. (2025). Unless otherwise specified, we use the same optimization hyperparameters as Search-R1 (Jin et al., 2025), with the only modification being reduced maximum turn limit: from 4 in Search-R1 to 2 for NQ-only training and 3 for the NQ+Hotpot setting, improving training efficiency. Unless stated otherwise, the regularization weight is fixed at λ = 0.1. Evaluation settings. The Evaluation is performed on the validation or test sets of seven datasets, categorized as follows: (1) General Question Answering: NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2022). (2) Multi-Hop Question Answering: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), Musique (Trivedi et al., 2022), and Bamboogle (Press et al., 2022). We adopt exact match (EM) as the primary evaluation metric, consistent with the protocol in Jin et al. (2025). Such diversity ensures thorough examination of search engine-integrated LLM performance across range of realistic reasoning scenarios. 6.1 EXPERIMENTAL RESULTS We evaluate our proposed LLD-mitigation strategy on seven open-domain and multi-hop QA benchmarks using two model families: Qwen2.5-3B (Base/Instruct) and Qwen2.5-7B (Base/Instruct). Tables 1 and 2 summarize the EM performance across all settings. Since vanilla GRPO training often collapses and drives the reward to zero, we use the results from Search-R1 Jin et al. (2025), corresponding to the best checkpoint before collapse, as the GRPO baseline. Results on Qwen2.5-3B. As shown in Tab. 1, GRPO attains an average score of 0.303 in the NQ-only setting. Incorporating LLD raises performance to 0.321 (+5.9%), and LLDS provides further improvement to 0. 8 Methods General QA NQ TriviaQA PopQA Gen-Avg Multi-Hop QA HotpotQA 2Wiki Musique Bamboogle MH-Avg Avg Direct Inference 0.106 CoT 0.023 IRCoT 0.111 Search-o1 0.238 RAG 0.348 SFT 0.249 R1-base 0.226 R1-instruct 0.210 Rejection Sampling 0.294 Search-R1-PPO-Base 0.406 Search-R1-PPO-Ins 0.341 0.440 0.462 0.478 0.421 0.479 0.486 NQ-Only Search-R1-GRPO +LLD +LLDS NQ+hotpot Search-R1-GRPO +LLDS +LLDS-MA Search-R1-GRPO NQ-Only +LLDS +LLDS-MA NQ+hotpot +LLDS 0.288 0.032 0.312 0.472 0.544 0.292 0.455 0.449 0.488 0.587 0.545 0.582 0.609 0.605 0.583 0.634 0.632 0.108 0.005 0.200 0.262 0.387 0.104 0.173 0.171 0.332 0.435 0.378 0.413 0.464 0.449 0.413 0.462 0. 0.167 0.020 0.208 0.324 0.426 0.215 0.285 0.277 0.371 0.476 0.421 0.149 0.021 0.164 0.221 0.255 0.186 0.201 0.208 0.240 0.284 0.324 Qwen2.5-3b-Base 0.478 0.512 0.511 0.472 0.525 0.527 0.265 0.286 0. 0.297 0.350 0.444 Qwen2.5-3b-Ins 0.244 0.021 0.171 0.218 0.226 0.248 0.268 0.275 0.233 0.273 0.319 0.244 0.248 0.250 0.274 0.308 0.430 0.020 0.002 0.067 0.054 0.047 0.044 0.055 0.060 0.059 0.049 0. 0.061 0.063 0.062 0.066 0.086 0.183 0.397 0.565 0.391 0. 0.331 0.310 0.124 0.490 0.464 0.610 0.616 0.450 0. 0.517 0.513 0.294 0.363 0.193 0.320 0.074 0.131 0.462 0. 0.460 0.515 0.432 0.383 0.180 0.024 0.000 0.240 0.320 0.080 0.112 0.224 0.192 0.210 0.088 0. 0.113 0.113 0.129 0.128 0.201 0.371 0.232 0.121 0.315 0.395 0.109 0.011 0.161 0.203 0.152 0.147 0.187 0.184 0.186 0.174 0. 0.171 0.178 0.182 0.191 0.236 0.357 0.249 0.171 0.282 0.134 0.015 0.181 0.255 0.270 0.176 0.229 0.224 0.265 0.303 0.325 0.303 0.321 (+5.9%) 0.323 (+6.6%) 0.312 0.360 (+15.4%) 0.430 (+37.8%) 0.336 0.319 0.381 (+13.4%) 0.347 0.419 (+24.7%) Table 1: Results on General QA and Multi-Hop QA datasets for Qwen2.5-3b-Base/Instruct. denotes results from (Jin et al., 2025) using NQ+hotpot training. denotes open-domain datasets and denotes retrieval-augmented datasets. (+6.6%). In the more challenging NQ+Hotpot setting, LLDS increases the vanilla GRPO score from 0.312 to 0.360, 15.4% relative gain. Notably, since LLDS tends to limit the base model to invoking search only once, we apply the multi-aspect variant LLDS-MA, which delivers the strongest performance with an average score of 0.430, representing substantial +37.8% improvement ratio over vanilla GRPO. similar trend is observed for Qwen2.5-3B-Instruct: LLDS-MA improves performance to 0.381 under NQ-only, and LLDS achieves 0.419 in the NQ+Hotpot setting (+24.7%). Results on Qwen2.5-7B As shown in Tab. 2, for Qwen2.5-7B-Base, applying LLDS yields substantial improvement, reaching an average score of 0.417 when trained on NQ only, representing 19.1% relative gain over the GRPO baseline trained on NQ+Hotpot. When LLDS is applied under the NQ+Hotpot training setting, performance increases from 0.350 to 0.462, corresponding to an even larger 32.0% improvement. LLDS also achieves the strongest results across nearly all individual datasets, indicating enhanced evidence retrieval and multi-step reasoning capability. For Qwen2.5-7B-Instruct, we observe similar pattern. Under the NQ+Hotpot training setting, LLDS raises the average score from 0.396 to 0.469 (improved 18.4%). Notably, LLDS achieves the best performance on every Multi-Hop QA benchmark, including 0.473 on 2Wiki, 0.443 on Musique, and 0.516 on Bamboogle. These results highlight LLDS(-MA) as an effective method for stabilizing GRPO training and substantially enhancing multi-turn reasoning. 6.2 ABLATION STUDY AND ANALYSIS Impact of response-level gating. Applying response-level gating reduces the strength of the regularization applied during training, leading to measurable improvements on multi-hop QA tasks. As shown in Tab. 1, although the average performance increases modestly by 0.2%, the method yields notable 1.6% gain on the Bamboogle dataset, highlighting its effectiveness in enhancing multi-hop reasoning. NQ vs. NQ+Hotpot. Models trained only on NQ achieve strong performance on open-domain, single-hop QA but show limited ability to generalize to multi-hop settings. Moving to NQ+Hotpot consistently improves multi-step reasoning: for example, on Qwen2.5-3B-Base, the GRPO baseline increases from 0.303 (NQ-only) to 0.312 (NQ+Hotpot), and LLDS increases from 0.323 to 0.360 under the same shift. similar trend appears for Qwen2.5-7B-Base, where LLDS rises from 0.417 (NQ-only) to 0.462 (NQ+Hotpot). These gains show 9 Methods General QA NQ TriviaQA PopQA Gen-Avg Multi-Hop QA HotpotQA 2Wiki Musique Bamboogle MH-Avg Avg Qwen2.5-7b-Base/Instruct Direct Inference 0.134 CoT 0.048 IRCoT 0.224 Search-o1 0.151 RAG 0.349 SFT 0.318 R1-base 0.297 R1-instruct 0.270 Rejection Sampling 0.360 Search-R1-PPO-Base 0.480 Search-R1-PPO-Ins 0.393 0.408 0.185 0.478 0.443 0.585 0.354 0.539 0.537 0.592 0.638 0.610 0.140 0.054 0.301 0.131 0.392 0.121 0.199 0.199 0.380 0.457 0.397 0.227 0.096 0.334 0.242 0.442 0.264 0.345 0.335 0.444 0.525 0.467 0.183 0.092 0.133 0.187 0.299 0.217 0.242 0.237 0.331 0.433 0. 0.250 0.111 0.149 0.176 0.235 0.259 0.273 0.292 0.296 0.382 0.414 0.031 0.022 0.072 0.062 0.058 0.066 0.083 0.072 0.123 0.196 0.146 Qwen2.5-7b-Base Search-R1 (GRPO) NQ-Only +LLDS NQ+hotpot +LLDS Search-R1 (GRPO) NQ-Only +LLDS NQ+hotpot +LLDS 0. 0.560 0.388 0.448 0.326 0.297 0. 0.514 0.666 0.470 0.550 0.403 0. 0.151 0.504 0.666 0.471 0.547 0. 0.475 0.209 Qwen2.5-7b-Ins 0.429 0.623 0. 0.493 0.386 0.346 0.162 0.517 0. 0.460 0.541 0.401 0.298 0.158 0. 0.662 0.464 0.541 0.473 0.443 0. 0.120 0.232 0.224 0.296 0.208 0.112 0.203 0.293 0.355 0.432 0.368 0.360 0.347 0.444 0.400 0. 0.516 0.146 0.114 0.145 0.180 0.200 0.164 0.200 0.224 0.276 0.361 0.325 0.181 0.106 0.239 0.206 0.304 0.207 0.262 0.271 0.348 0.431 0.385 0.277 0.350 0. 0.417 (+19.1%) 0.398 0.462 (+32.0%) 0.324 0.396 0. 0.413 (+4.3%) 0.428 0.469 (+18.4%) Table 2: Performance comparison on General QA and Multi-Hop QA datasets for Qwen2.5-7b-Base/Instruct. that adding multi-hop HotpotQA, whose questions require multi-turn reasoning to answer correctly and receive reward, encourages the model to retrieve, integrate, and reason over multiple pieces of evidence more effectively than training on NQ alone. Impact of masking answer (MA). We apply masking answer (MA) specifically to model variants where vanilla GRPO and LLDS degenerate into issuing only single search call, limiting their ability to collect sufficient evidence for multi-hop reasoning. MA disables LLDS regularization on the final answer tokens, encouraging the model to perform additional search steps or intermediate reasoning. As shown in Tab. 1, adding MA on top of LLDS yields substantial improvements for these one-searchlimited models. LLDSMA increases the Qwen2.5-3B-Base average score in the NQ+Hotpot setting from 0.360 (LLDS) to 0.430, and similarly improves the NQ-only Instruct variant from 0.319 to 0.381. These gains demonstrate that MA effectively encourages deeper, multi-step reasoning in scenarios where the underlying GRPO policy underutilizes search actions. More detailed analysis see Sec. B.3. 6.3 EFFECT OF LLDS ON TRAINING STABILITY ACROSS MODELS To verify the universality of the training collapse issue in GRPO and the robustness of our proposed solution, we extended our experiments across different model scales and alignment stages. Fig. 6 illustrates the training reward curves for Qwen-2.5 3B and 7B models, covering both Base and Instruct versions. As observed in the blue curves, the baseline GRPO consistently suffers from catastrophic collapse within the first 300 steps, regardless of the model size (3B vs. 7B) or type (Base vs. Instruct). This confirms that the instability is an inherent characteristic of the algorithm rather than an artifact of specific model configuration. In contrast, the integration of LLDS (red curves) effectively mitigates this issue. In all four scenarios, the models trained with LLDS maintain steady upward trajectory in rewards, successfully bypassing the collapse points that affect the baseline. This demonstrates that LLDS serves as generalized and effective regularizer for stabilizing GRPO training."
        },
        {
            "title": "7 MORE DISCUSSION AND GUIDELINE",
            "content": "Based on our analysis of Lazy Likelihood Displacement (LLD) and the emergence of the LLD Death Spiral, we consolidate several practical guidelines for stabilizing tool-integrated GRPO. Each recommendation follows directly from the core failure mechanisms identified in our study. Understand why tool-integrated GRPO is uniquely vulnerable to LLD. Unlike free-form RL on text, tool-augmented agents introduce structural conditions that magnify likelihood drift. First, tool calls inject 10 (a) Qwen-2.5-3B-Base (b) Qwen-2.5-3B-Instruct (c) Qwen-2.5-7B-Base (d) Qwen-2.5-7B-Instruct Figure 6: Comparisons of training reward curves between baseline GRPO (blue) and GRPO + LLDS (red). The baseline consistently collapses (reward drops to zero) across all model sizes, while LLDS stabilizes training and sustains high rewards. inherently out-of-distribution tokenssuch as search results, API outputs, or error messagesthat differ sharply from pretrained language distributions. These OOD segments raise token-level uncertainty and make GRPOs relative updates more volatile, accelerating the onset of likelihood displacement. Second, toolbased reasoning unfolds across multiple stages, with early stages (e.g., query formulation) stabilizing more quickly than later ones (e.g., tool-result interpretation). Because GRPO applies single scalar reward to all tokens, early-stage tokens that share nearly identical prefixes across correct and incorrect trajectories receive conflicting gradient signals. This rewardtoken misalignment disproportionately harms stable prefixes and amplifies LLD. Recognizing these structural challenges is essential for diagnosing and mitigating instability in tool-integrated RL systems. Closely monitor likelihood dynamicsreward alone is insufficient. central insight of our analysis is that likelihood degradation begins long before any visible drop in reward. Across models, reward continues to rise throughout both the early and steady-decay phases, even as the likelihood of correct responses is already drifting downward. Because GRPO updates are governed by likelihood ratios, such early degradation silently amplifies gradients and initiates instability. Monitoring only the reward therefore obscures the onset of LLD. In practice, reliable early-warning signals come from tracking action-level and trajectory-level log-likelihood, inspecting entropy trajectories, which surge sharply once likelihood begins to collapse, and watching for sudden spikes in likelihood ratios that indicate gradient amplification. Visualization of likelihood or entropy curves provides the earliest and most trustworthy indication that training is approaching the unstable regime."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this work, we firstly identified Lazy Likelihood Displacement as the central cause of instability in GRPObased, Search-R1style tool-integrated RL. We showed that LLD emerges early, occurs frequently, and reliably drives training into self-reinforcing LLD Death Spiral characterized by monotonic likelihood decay, entropy spikes, gradient inflation, and eventual collapse. Our analysis provides the first systematic evidence that GRPO is uniquely vulnerable to likelihood drift in tool-integrated settings. To counteract this failure mode, we proposed LLDS, simple and effective likelihood-preserving regularizer that activates only when likelihood decreases and targets only the offending tokens. Empirically, LLDS stabilizes optimization and yields consistent gains across seven QA benchmarks, with improvements of up to +37.8% on Qwen2.5-3B and +32.0% on Qwen2.5-7B. These findings highlight likelihood control as key mechanism for reliable RL-based tool use and offer promising foundation for more robust and scalable training methods in future agentic LLMs. 11 Acknowledgments: This work was partially funded by the NSERC Discovery Grant RGPIN-2021-03677, Alliance Grant ALLRP 581098-22, the Natural Science and Engineering Research Council of Canada (NSERC), the Canada CIFAR AI Chairs program, the Canada Research Chair program, an IITP grant funded by MSIT, and the Digital Research Alliance of Canada."
        },
        {
            "title": "REFERENCES",
            "content": "Wenlong Deng, Yi Ren, Muchen Li, Danica Sutherland, Xiaoxiao Li, and Christos Thrampoulidis. On the effect of negative gradient in group relative deep reinforcement optimization. Advances in Neural Information Processing Systems, 2025. Y. Feng et al. Retool: Reinforcement-tuned tool-using llms. arXiv preprint, 2025. Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, and Qing Li. Multi-modal agent tuning: Building vlm-driven agent for efficient tool usage. arXiv preprint arXiv:2412.15606, 2024. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pp. 67696781, 2020. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36:4344743478, 2023. H. Mai et al. Agentic llms: Learning multi-step tool-use with feedback planning. arXiv preprint, 2025. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 2022. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. 12 Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi Chen, Sanjeev Arora, and Boris Hanin. Unintentional unalignment: Likelihood displacement in direct preference optimization. arXiv preprint arXiv:2410.08847, 2024. Yi Ren and Danica Sutherland. Learning dynamics of llm finetuning. arXiv preprint arXiv:2407.10490, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. Y. Sun et al. Zerosearch: Tool-integrated reasoning without human feedback. arXiv preprint, 2025. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10: 539554, 2022. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, arXiv preprint and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv:2212.03533, 2022. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: Endto-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025."
        },
        {
            "title": "A THEOREM AND PROOF",
            "content": "In this section, we first give the formal version of theorem 4.2: Theorem A.1 (Trajectory-Level LLD in Tool-Integrated GRPO) Consider tool-integrated trajectory with only the response actions {yt}T t=0 contribute to the likelihood, and tool-feedback tokens ot are masked during likelihood computation but remain in context. Let πθ(s) denote the evolving policy at training time s. action-level likelihood change. For the t-th action of the i-th correct response, denoted y+ instantaneous log-likelihood change i,t, its becomes increasingly lazy or even negative as the following quantity increases: ds ln πθ(s) (cid:0)y+ i,t x, y+ i,<t, o+ i,<t (cid:1) Gi,t(s) = y+ i,t (cid:88) (cid:88) Tj (cid:88) j,t (cid:88) k=0 j= t=0 k=1 α (i,t,k),(j,t,k) (cid:10)hx,y+ i,<t,o+ i,<t,y+ i,t,<k (cid:123)(cid:122) impact of negative gradients , hx,y j,<t ,o j,<t ,y j,t ,<k (cid:11) (cid:125) y+ i,t (cid:88) + (cid:88) Ti (cid:88) y+ ,t (cid:88) k=0 i=1 t=0 k= α+ (i,t,k),(i,t,k) (cid:10)hx,y+ i,<t,o+ i,<t,y+ i,t,<k , hx,y+ i,<t ,o+ ,<t ,y+ i,t ,<k (cid:11). (8) (cid:124) p+ where α (i,t,k),(j,t,k) and α+ Trajectory-level likelihood change. Summing over all actions yields (i,t,k),(i,t,k) denote token-wise prediction-error similarity weights. ds ln πθ(s)(y0:T x) = (cid:88) ˆy+ (cid:88) t=0 k= ds ln πθ(s)( ˆy+ t,k), which becomes lazy or negative whenever (cid:88) ˆy+ (cid:88) t=0 k=1 Gt,k(s) is large. As the theorem indicates, two core factors inflate this negative-gradient effect: 1. Low likelihood of incorrect responses: Negative responses that the model assigns low probability to yield larger prediction-error weights α t,k; t,k , magnifying their influence. In such cases, the model interprets these low-likelihood errors as severe mistakes, causing their gradients to receive disproportionately large scaling. 2. Embedding similarity: When incorrect responses are similar to correct ones, their prefix representations exhibit large inner products, amplifying the negative contribution. This high representational overlap means the model struggles to disentangle correct from incorrect continuations, causing negative examples to push gradients in harmful directions and make the model unconfident. A.1 PROOF OF THEOREM A.1 Setup and masking. Fix query and correct response index i, and consider the feedback-masked trajectory ˆy+ = (y+ i,t}Ti where only the action tokens in {y+ t=0 contribute to the loss. Tool feedback is excluded from the GRPO objective but remains in the conditioning context. We study the log-likelihood change of each action y+ i,t under the evolving policy πθ(s): i,1, . . . , y+ i,Ti i,0, y+ i,0, o+ i,1, o+ ), ds ln πθ(s) (cid:0)y+ i,t x, y+ i,<t, o+ i,<t (cid:1) , 14 and then aggregate over to obtain the trajectory-level result. Reduction to standard GRPO at the action level. Conditioned on the prefix (x, y+ action y+ i,t is generated autoregressively as i,<t, o+ i,<t), the t-th πθ(s) (cid:0)y+ i,t x, y+ i,<t, o+ i,<t (cid:1) = y+ i,t (cid:89) k= (cid:16) πθ(s) y+ i,t,k x, y+ i,<t, o+ i,<t, y+ i,t,<k (cid:17) . Since feedback tokens are only masked in the loss, but still appear in the context, the GRPO objective for tool-integrated training (with feedback masking) has exactly the same functional form as the standard GRPO objective applied to the sequence of action tokens. Thus, each pair (cid:0)question, response(cid:1) = (cid:0)x, y+ i,t (cid:1), i,<t, o+ with context (y+ i,<t), can be treated as single generation in the sense of the non-tool GRPO analysis. Therefore, we can directly invoke the GWHES theorem of Deng et al. (2025) (Theorem 4.4), adapted to the conditional distribution πθ(s)(y+ i,<t), and obtain the following action-level result. i,t x, y+ i,<t, o+ Theorem A.2 (Action-level) For any x, any time 0, and any correct response y+ of its t-th action, , the likelihood change i,<t, o+ becomes lazier (smaller in magnitude, and potentially negative) as ln πθ(s)(y+ i,t x, y+ i,<t), ds Gi,t(s) = y+ i,t (cid:88) (cid:88) Tj (cid:88) j,t (cid:88) k=0 j=1 t=0 k=1 α (i,t,k),(j,t,k) (cid:10)hx,y+ i,<t,o+ i,<t,y+ i,t,<k (cid:123)(cid:122) impact of negative gradients , hx,y j,<t ,o j,<t ,y j,t,<k (cid:11) (cid:125) y+ i,t (cid:88) + (cid:88) Ti (cid:88) y+ i,t (cid:88) k=0 i= t=0 k=1 α+ (i,t,k),(i,t,k) (cid:10)hx,y+ i,<t,o+ i,<t,y+ i,t,<k , hx,y+ ,<t ,o+ ,<t ,y+ ,t,<k (cid:11). (9) (cid:124) p+ increases, where α (i,t,k),(j,t,k) = and (cid:68) ey+ i,t,k πθ(s)( x, y+ i,<t, o+ i,<t, y+ i,t,<k), ey j,t ,k πθ(s)( x, j,<t, j,<t, j,t,<k) (cid:69) i,<t, o+ α+ (i,t,k),(i,t,k) = i,t,k are token-level prediction-error similarity weights. πθ(s)( x, y+ (cid:68) ey+ i,<t, y+ i,t,<k), ey+ ,t,k πθ(s)( x, y+ i,<t, o+ i,<t , y+ i,t,<k) (cid:69) Trajectory level as sum over actions. The tool-integrated trajectory likelihood factorizes over actions: πθ(s)(y0:T x) = (cid:89) t=0 πθ(s)(yt x, y<t, o<t) . Taking logarithms and differentiating with respect to gives ds ln πθ(s)(y0:T x) = (cid:88) t=0 ds ln πθ(s)(yt x, y<t, o<t) = (cid:88) ˆy+ (cid:88) t=0 k=1 ds ln πθ(s)( ˆy+ t,k ), where again denotes the masked context including the feedback tokens. By the action-level result, each term in the sum becomes lazy or negative as Gt,k(s) grows, and hence the full trajectory-level derivative becomes lazy or negative whenever is large. This establishes the trajectory-level statement in Theorem A.1. (cid:88) ˆy+ (cid:88) t= k=1 Gt,k(s)"
        },
        {
            "title": "B ADDITIONAL ANALYSIS",
            "content": "B.1 TRAINING INSTABILITY PRIOR TO COLLAPSE While the final collapse is indeed triggered by large gradients from negative responses, substantial instability and noticeable performance decline had already emerged earlier in training. As shown in Fig. 7, the brown vertical line marks the first sharp drop in model reward (green curve), during which the likelihood of correct responses also decreases significantly, even though the gradient norm remains relatively small (around 2). As training progresses, the LD issue intensifies and the low confidence responses lead to gradient norm steadily grows, ultimately causing the model to collapse. These observations suggest that instability is present well before the collapse point and therefore warrants close attention. B."
        },
        {
            "title": "IMPACT OF REGULARIZATION STRENGTH",
            "content": "To examine the effect of regularization strength, we ablate λ 0, 0.01, 0.1 and plot the corresponding training reward dynamics in Fig. 8. As shown, without regularization (λ = 0), the model collapses around step 200. small regularization value (λ = 0.01, orange curve) delays but does not prevent collapse, which occurs around step 220. In contrast, stronger regularization (λ = 0.1) stabilizes training entirely, allowing the model to continue smoothly without collapse. We truncate the plot at step 250 for clarity, although training can proceed well beyond this point. Figure 7: Demonstration of unstability prior collapse. B.3 UNLOCKING MULTI-STEP REASONING VIA ANSWER MASKING To further explore the potential of LLDS in promoting complex reasoning behaviors, we conducted an ablation study on the Qwen-2.5-3B-Base model, focusing on the regularization scope. Specifically, we mask out the final answer tokens from the calculation of the LLLDS term (denoted as \"Mask Answer\"). It is important to note that Qwen-2.5-3B-Base inherently lacks the capability for multi-turn tool calling. As illustrated in Fig. 9, this limitation is evident in the standard training setups: the baseline GRPO (blue curve) rapidly suffers from model collapse, with the number of valid searches dropping to zero. While the standard GRPO + LLDS (red curve) successfully stabilizes the training and prevents collapse, it fails to elicit any multi-step behavior, with the search frequency remaining stagnant at exactly 1.0. Figure 8: Impact of regularization strength on Qwen2.5-7B-base. However, the \"GRPO + LLDS-MA\" variant (green curve) demonstrates distinct emergent behavior. By removing the regularization constraint on the final answer tokens, we observe significant increase in the number of valid searches per question, rising well above 2.0. This indicates that relaxing the penalty on the answer tokens effectively unlocks the models latent potential to engage in multi-step reasoning and utilize external toolscapabilities that standard methods failed to activate in the base model. B.4 QUALITATIVE EXAMPLES OF TOOL-INTEGRATED REASONING To further illustrate the behavioral characteristics of our trained models, we present qualitative reasoning traces generated by the Qwen2.5-7B family. These examples highlight how models trained with our LLD 16 Figure 9: Evolution of the number of valid searches per question on Qwen-2.5-3B-Base. The base model inherently lacks multi-turn tool calling capabilities. While the baseline (blue) collapses and standard LLDS (red) stabilizes at single search step, LLDS-MA (green) successfully unlocks the models potential for multi-step reasoning, driving the number of valid searches significantly above 1.0. regularization strategy exhibit strong multi-step planning, controlled tool-use, and stable multi-turn reasoning throughout the trajectory. Unlike standard GRPO modelswhich often suffer from premature collapse, our method enables the model to maintain coherent reasoning structure across search, verification, and final answer generation. As shown in Figures 10 and 11, the model not only performs step-by-step decomposition and self-verification but also executes consecutive search calls when needed, integrates retrieved evidence effectively, and produces correct, concise final answer. These qualitative behaviors align with our quantitative findings: stabilizing likelihood dynamics prevents the LLD Death Spiral, allowing the RL policy to leverage deeper tool-integrated reasoning without sacrificing robustness."
        },
        {
            "title": "C CASE STUDIES OF LIKELIHOOD DISPLACEMENT",
            "content": "To better understand how Lazy Likelihood Displacement manifests in practice, we zoom into two representative training samples (corresponding to the most severely affected two samples in Fig. 3 (Step 140)). For each question, we compare correct trajectory with an incorrect one generated by the SEARCH-R1 policy. These paired examples make the abstract mechanisms in our analysis concrete. Case 1: Embedding similarity under group-relative updates. Fig. 12 and Fig. 13 show two rollouts for the question Who won the NRL grand final in 2015?. Both trajectories use nearly identical <think> plans, issue semantically equivalent search queries, and retrieve identical <information> snippets. The only difference lies in the final answer token: the correct trajectory outputs the full entity North Queensland Cowboys, whereas the incorrect one truncates the name to North Queensland. Because GRPO assigns single scalar reward to the entire trajectory, these two highly similar prefixes receive sharply different updates: the incorrect rollout pushes negative gradients onto tokens that are almost identical in embedding space to those of the correct rollout. This illustrates how small semantic deviations at the end of trajectory can, through high prefix similarity, induce strong likelihood displacement on otherwise correct actions. Case 2: Low-likelihood, longer incorrect trajectories. Fig. 14 and Fig. 15 present rollouts for Who is the main character in green eggs and ham?. In the incorrect case, the model begins with long, lowlikelihood <think> segment. Because the likelihood of early tokens is extremely small, sampling drifts into semantically meaningless regions of the distribution, producing an extended sequence of nonsensical text. This also leads the model to violate the tool protocol, which subsequently triggers corrective feedback in the <information> channel. Although the model eventually issues valid search, it still produces an incorrect answer (The first-person narrator). This trajectory is both longer and lower-likelihood than its correct counterpart, causing GRPO to assign it large negative prediction-error weights and accumulate many negative-gradient summands across tokens. In contrast, the correct trajectory remains short and highconfidence, performing single search followed by the correct answer Sam-I-Am. Together, these examples concretely demonstrate how low-likelihood, overlong incorrect responses can dominate gradients and drive the LLD Death Spiral, even when the environment occasionally nudges the model back toward valid tool use. 17 Question: Who was the screenwriter for Three Times? Ground Truth: Hou Hsiao-hsien SEARCH-R1: <think> To answer this question, need to find out who the screenwriter for the movie Three Times is. will conduct search to gather this information. </think> <search> who was the screenwriter for Three Times </search> <information> Doc 1 (Title: Paddy Chayefsky) are at the Wisconsin Historical Society and the New York Public Library for the Performing Arts, Billy Rose Theatre Division ... Doc 2 (Title: Paddy Chayefsky) Paddy Chayefsky Sidney Aaron \"Paddy\" Chayefsky (January 29, 1923 August 1, 1981) was an American playwright ... Doc 3 (Title: Three Times) Three Times (Chinese: Zuìhˇao de shíguang; lit. Best of Times) is 2005 Taiwanese film directed by Hou Hsiao-hsien ... </information> <think> From the search results, found that the director of Three Times is Hou Hsiao-hsien. However, need to find the screenwriter. Since the search results did not provide this information, will conduct another search. </think> <search> screenwriter of Three Times </search> <information> Doc 1 (Title: Three Times) Three Times (Chinese: Zuìhˇao de shíguang; lit. Best of Times) is 2005 Taiwanese film directed by Hou Hsiao-hsien ... Doc 2 (Title: Eric Mendelsohn) Eric Mendelsohn (born November 1, 1964) is an American film director and screenwriter. Two of his films have been screened ... Doc 3 (Title: Paddy Chayefsky) are at the Wisconsin Historical Society and the New York Public Library for the Performing Arts, Billy Rose Theatre Division ... </information> <think> From the search results, found that the director of Three Times is Hou Hsiao-hsien. However, need to find the screenwriter. Since the search results did not provide this information, will infer that the screenwriter is Hou Hsiao-hsien, as he is the director of the film. </think> <answer> Hou Hsiao-hsien </answer> Figure 10: Qwen2.5-7B (successful): SEARCH-R1 conduct multi-step reasoning, search, with self-verification and finally answer the question. 18 Question: Which was founded first University of California Santa Barbara or Hamdard University? Ground Truth: University of California, Santa Barbara SEARCH-R1: <think> To answer this question, need to find out which university was founded first. Ill conduct search to gather this information. </think> <search> Which was founded first, University of California, Santa Barbara or Hamdard University </search> <information> Doc 1(Title: \"University of California, Santa Barbara\") University of California, Santa Barbara The University of California, Santa Barbara ... Doc 2(Title: \"History of the University of California, Santa Barbara\") History of the University of California, Santa Barbara The University of California, Santa Barbara (UCSB)... Doc 3(Title: \"History of the University of California, Santa Barbara\") the chief executive was changed to Chancellor. UCSBs first Provost was thus Clarence L.... </information> <think> From the search results, found that the University of California, Santa Barbara traces its roots back to 1891. Now, need to search for the founding date of Hamdard University. </think> <search> Hamdard University founding date </search> <information> Doc 1(Title: \"Hamdard University\") century. The university includes eight faculties, nine research institutes, three teaching hospitals and three affiliated engineering institutes. ... Doc 2(Title: \"Hamdard University\") Hamdard University Hamdard University (Urdu: ) () is private research university with campuses in Karachi ... Doc 3(Title: \"Hamdard University\") electrical engineer Atta-ur-Rehman Memon, and Zillur Rahman (who has been serving as visiting professor of medicine since 1997) ... </information> <think> From the search results, found that Hamdard University was founded in 1991. Now, can compare the founding dates of the two universities. The University of California, Santa Barbara was founded in 1891, while Hamdard University was founded in 1991. Therefore, the University of California, Santa Barbara was founded first. </think> <answer> University of California, Santa Barbara </answer> Figure 11: Qwen2.5-7B (successful): SEARCH-R1 conduct multi-step reasoning, search and finally answer the question. Question: Who won the NRL grand final in 2015? Ground Truth: North Queensland Cowboys SEARCH-R1: <think> To direct to the original accessible information efficiently, let me consult the query through search. </think> <search> NRL grand final 2015 winner </search> <information> Doc 1(Title: \"2015 NRL Grand Final\") 2015 NRL Grand Final The 2015 NRL Grand Final was the conclusive and premiership-deciding game of the 2015 NRL season and was played on Sunday 4 October at Sydneys ANZ Stadium betwee ... Doc 2(Title: \"2015 NRL Grand Final\") grand finalists, North Queensland and Brisbane, as well as minor premiers the Sydney Roosters, qualified for the second World Club Series, played in the 2016 pre-season against clubs ... Doc 3(Title: \"2015 NRL Grand Final\") was the first time an NRL Grand Final had been decided in Golden Point. At the post-match ceremony, Johnathan Thurston received the Clive Churchill medal as best on ground. This was ... </information> <answer> North Queensland Cowboys </answer> Figure 12: Case 1 (correct). SEARCH-R1 produces compact reasoning trace and the fully correct entity name for the NRL grand final question. 20 Question: Who won the NRL grand final in 2015? Ground Truth: North Queensland Cowboys SEARCH-R1: <think> need hunting and insight into history of NRL Grand Final 2015. Bespoken search seem to be fitting solution here which Ill commence. </think> <search> who won nrl grand final 2015 </search> <information> Doc 1(Title: \"2015 NRL Grand Final\") 2015 NRL Grand Final The 2015 NRL Grand Final was the conclusive and premiership-deciding game of the 2015 NRL season and was played on Sunday 4 October at Sydneys ANZ Stadium betwee ... Doc 2(Title: \"2015 NRL Grand Final\") grand finalists, North Queensland and Brisbane, as well as minor premiers the Sydney Roosters, qualified for the second World Club Series, played in the 2016 pre-season against clubs ... Doc 3(Title: \"2015 NRL Grand Final\") title for the St George Illawarra Dragons in 2010. The North Queensland Cowboys reached the premiership deciding game for the second time in their history since their inception into ... </information> <answer> North Queensland </answer> Figure 13: Case 1 (incorrect). For the same question, SEARCH-R1 follows an almost identical search and evidence pattern but outputs truncated answer. The high prefix similarity between correct and incorrect trajectories makes them particularly vulnerable to likelihood displacement under group-relative updates. 21 Question: Who is the main character in green eggs and ham? Ground Truth: Sam-I-am SEARCH-R1: <think> Im worried do not know tale specifically - therefore to ascertain what the main character in Green Eggs and Ham would be, shall seek this information thusly. </think> <search> who is the main character in Green Eggs and Ham </search> <information> Doc 1(Title: \"Green Eggs and Ham\") Green Eggs and Ham Green Eggs and Ham is childrens book by Dr. Seuss, first published on August 12, 1960. As of 2016, the book has sold 8 million copies worldwide. The story has appe ... Doc 2(Title: \"Green Eggs and Ham (TV series)\") Green Eggs and Ham (TV series) Green Eggs and Ham is an upcoming American animated television series from Warner Bros. Animation, Very Good Production, Stern Talking To, ... Doc 3(Title: \"Green Eggs and Ham (TV series)\") Sam-I-Am and his friend go on cross-country trip that would test the limits of their friendship. As they learn to try new things, they find out what adventure brings. Gree ... </information> <answer> Sam-I-Am </answer> Figure 14: Case 2 (correct). stable SEARCH-R1 trajectory for the same question: the model uses short, coherent <think> step, performs single search, and outputs the correct answer Sam-I-Am. Question: Who is the main character in green eggs and ham? Ground Truth: Sam-I-am SEARCH-R1: <think> In order to resolve who the main character within the text \"Green Eggs and Ham\", can infer from reading this poetic curiously imaginative nature of tales associated with Dr. Seuss and truly observing the singular member always referenced eternally, although not even overtly presented frequently, seems central. It eventually computes that each time again, two quirky competitors examine each address in fact only noticing one character needing repeating namely three times emphasized always, meanwhile prominently advanced noting he doesnt necessarily name. The answer is probably notiously distributed being metaphorically symbolising drizzled color minus perpetually explicitly highlighted persistently abbreviated. Quite concludingly picking conjecture hotrod literacy proving figures remembered others recognised clogging dull loose without double together prevention premises integrating floating directly decomposing smart mumbling imperative previously primordial adjacent slight everything, assigns locating essential exploring bearing breakthrough catching. Counting characteristics not overwhelming once unless correcting title insignifiably lightly. Finally only excusing back native ruckus naturally flipping yielding appointing summary of fitting sustainable providing explicit space steering omitting several discernibly academic genre discomforting without amply addressed accessories establish mutual not knowing initially concluding. Rightly summing up rudimentarily realizing computing Boolean cutely delayed indeed persuading predictable binding nutrients distinguishing vocabularily interrelational strategy cognizant broad range indicating integrating equally locating essence wholly sectioning open mainly umbrage confirming linguistically certainty personistically figuring hyperlinks meeting melodic benders incentives predictive spanning reunite nearest that strictly wholly consistent factual reformaturing calculating empirically traversing parameters meeting aesthetic ... <information> My previous action is invalid. If want to search, should put the query between <search> and If want to give the final answer, should put the answer between <answer> and </search>. </answer>. Let me try again. </information> <search> who is the main character in green eggs and ham </search> <information> Doc 1(Title: \"Green Eggs and Ham\") Green Eggs and Ham Green Eggs and Ham is childrens book by Dr. Seuss, first published on August 12, 1960. As of 2016, the book has sold 8 million copies worldwide. The story has appe ... Doc 2(Title: \"Green Eggs and Ham (TV series)\") Green Eggs and Ham (TV series) Green Eggs and Ham is an upcoming American animated television series from Warner Bros. Animation, Very Good Production, Stern Talking To, ... Doc 3(Title: \"Green Eggs and Ham (TV series)\") Sam-I-Am and his friend go on cross-country trip that would test the limits of their friendship. As they learn to try new things, they find out what adventure brings. Gree ... </information> <answer> The first-person narrator </answer> Figure 15: Case 2 (incorrect). SEARCH-R1 generates long, low-likelihood and partially nonsensical <think> segment, violates the tool protocol, receives corrective feedback, and ultimately outputs an incorrect answer. The low likelihood and extended length of this trajectory make its negative gradients particularly dominant."
        }
    ],
    "affiliations": [
        "UC Berkeley",
        "University of British Columbia",
        "Vector Institute"
    ]
}