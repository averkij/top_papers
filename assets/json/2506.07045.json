{
    "paper_title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs",
    "authors": [
        "Yikun Ji",
        "Hong Yan",
        "Jun Lan",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Qi Fan",
        "Liqing Zhang",
        "Jianfu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing human-understandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AI-generated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct a dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing a foundation for human-aligned visual-textual grounded reasoning. We then finetune MLLMs through a multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 5 4 0 7 0 . 6 0 5 2 : r Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs Yikun Ji Shanghai Jiao Tong University Hong Yan Ant Group Jun Lan Ant Group Huijia Zhu Ant Group Weiqiang Wang Ant Group Qi Fan Shanghai Jiao Tong University Liqing Zhang Shanghai Jiao Tong University Jianfu Zhang Shanghai Jiao Tong University {da-kun,c.sis}@sjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing humanunderstandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AIgenerated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing foundation for humanaligned visual-textual grounded reasoning. We then finetune MLLMs through multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods."
        },
        {
            "title": "Introduction",
            "content": "The past decade has witnessed rapid progress in text-to-image generation, evolving from Generative Adversarial Networks to Diffusion Models, which are now capable of producing images nearly indistinguishable from real photographs [16, 38]. These advances have led to an explosion of highly realistic AI-generated content, raising pressing concerns about misinformation, authenticity, and trust in digital media. Most existing detection methods cast this task as binary classification problem, leveraging convolutional neural networks or vision transformers [62, 32, 37]. However, binary authenticity labels offer limited insight into why an image is classified as AI-generated. In realworld applications, especially those involving legal, journalistic, or ethical implications, explainable detection is essential. An effective detection system should not only identify whether an image is AI-generated but also pinpoint the specific visual cues or logical inconsistencies that betray its synthetic origin. Such explainability promotes user trust, supports verification workflows, and enables more informed decision-making. Preprint. Under review. Figure 1: An overview of our method. The rise of Multi-modal Large Language Models (MLLMs) has enabled cross-modal inference, allowing models to generate human-readable reasoning about AI-generated images [8, 39, 50]. Recent efforts [27, 67] have advanced interpretable textual explanations using MLLMs. However, without visual grounding, it remains uncertain whether these rationales accurately reflect image content or derive from hallucinations. To enhance alignment and reduce hallucinations, fine-grained visual-textual supervision, such as region-level bounding boxes and detailed captions, is crucial. Yet, the scarcity of such datasets presents key obstacle to developing trustworthy and interpretable MLLM-based image detection. In this paper, we introduce the FakeXplained dataset, comprising high-quality AI-generated images, along with an MLLM fine-tuning pipeline that achieves state-of-the-art detection accuracy and grounding capability. Our approach provides comprehensive rationales for fake image detection that demonstrate performance comparable to human annotators, inaugurating new paradigm in human-interpretable AI-generated image detection. Our major contributions are threefold: FakeXplained dataset: We curate human-annotated dataset of 8,772 AI-generated images sourced from diverse range of state-of-the-art generative models, annotating them with bounding boxes highlighting visual anomalies or illogical details, along with concise captions explaining the nature of each flaw. To the best of our knowledge, FakeXplained is the first dataset to provide localized, textual explanations specifically for AI-generated image detection. Two-stage MLLM fine-tuning: We fine-tune Qwen-2.5-VL (32B) on the FakeXplained dataset to build an end-to-end system capable of both detecting and explaining AI-generated images. Given an input image, the model produces (i) binary authenticity decision (real or AI-generated), and (ii) for fake images, set of predicted bounding boxes paired with natural language justifications for each identified region. After fine-tuning, the model achieves state-of-the-art performance across both classification and localization metrics. Grounded reasoning: Beyond quantitative improvements, we demonstrate that fine-tuning on FakeXplained enables MLLMs to perform fine-grained visual reasoning and clearly articulate their observations. The model is capable of answering not only is it fake? but also where and why does this image look fake?, producing interpretable, grounded explanations aligned with human judgment."
        },
        {
            "title": "2 Related works",
            "content": "Detection of AI-generated and manipulated images. Detecting AI-generated images has gained prominence with the improving fidelity of synthetic images from GANs [16, 14], autoregressive 2 transformers [58], diffusion-based models [44, 13, 52, 21, 18, 47] and DiTs [38, 7]. Early research focused on artifact detection in spatial and frequency domains, targeting inconsistencies like upsampling artifacts [15], texture synthesis mismatches [28], or limited high-frequency decay [12] to expose fakes, even imperceptible ones. Beyond artifact-based approaches, deep learning methods such as ResNets and Vision Transformers trained on real and synthetic data [62, 55, 37, 4] leverage strong feature extraction to learn discriminative patterns. However, generalization to unseen models remains challenging DefakeByReals, and many approaches are vulnerable to domain-specific biases such as JPEG compression artifacts [17]. As generation techniques evolve, artifact-based cues alone become increasingly unreliable. complementary research direction focuses on model explainability, as most detectors offer only binary classification without indicating how or where synthetic cues are found. Recent efforts towards fine-grained or localized detection include measuring reconstruction errors [43], using multi-branch systems for multi-level labels [1], computing local intrinsic dimensionalities [29], or integrating text-image contrastive learning [54, 51, 48]. Benchmarks such as FakeBench [27] and A-Bench [67] aim to promote human-understandable textual explanations through visual question answering (VQA)-style interface. However, without corresponding visual grounding, these explanations remain difficult to verify or trust. It is often unclear whether the textual rationale refers to any actual region in the image, and users are unable to determine whether the explanation is factually correct. This raises concerns about the factual alignment, interpretability, and credibility of such explanations. Training & fine-tuning reasoning-capable MLLMs Enhancing the reasoning capabilities of MLLMs is crucial for tasks requiring nuanced understanding. Initial strategies involved converting images into formalized textual representations to enable structured, language-driven reasoning [66]. Subsequent research has focused on instilling deeper cognitive abilities, including self-verification, self-correction, developing slow-thinking capabilities [59], and managing reasoning depth to address phenomena like overthinking [64]. Efforts also explore capturing human-like insight, or the aha moment, even in compact models [69], and constructing high-quality multi-modal Chain-of-Thought (CoT) datasets [22] to guide reasoning processes. Reinforcement Learning (RL) has become pivotal in these advancements, with many sophisticated reasoning developments relying on RL methodologies. Fine-tuning methods such as Generative Reward Policy Optimization (GRPO) [49], introduced for MLLMs in work such as R1-V [8], have spurred significant interest in RL-based multi-modal reasoning. RL, particularly when combined with structured reward functionsfor instance, using Intersection over Union (IoU) for tasks involving image grounding [50]markedly improves multi-modal alignment, visual reasoning, and humaninterpretable decision-making. This has enabled progress across diverse vision-language applications, from generating interpretable analyses of medical images [36] and enhancing text-to-image generation quality [24], to endowing smaller MLLMs with robust reasoning capacities through innovative RL frameworks [39]. These approaches demonstrate RLs capability of advancing model performance in complex vision-language tasks."
        },
        {
            "title": "3 FakeXplained dataset",
            "content": "To address the hallucination problem and improve alignment between visual perception and human reasoning, we aim to train MLLMs not only to detect AI-generated images but also to articulate why they are fake in reliable and human-understandable manner. This necessitates dataset that supports both visual grounding and textual reasoning. We introduce the FakeXplained dataset, designed for interpretable and trustworthy detection of AI-generated images. It consists of high-quality synthetic images paired with fine-grained, humanaligned annotations that indicate the underlying flaws and artifacts responsible for their detection as fake. 3.1 Image generation To ensure data diversity, we generated images across 1,000 ImageNet categories using 28 different text-to-image generation models, as shown in Table 2. All generated images underwent manual quality screening. Each image was independently rated by three volunteers, and the top-rated 8,772 AI-generated images were selected for subsequent annotation. 3.2 Image annotation To support interpretable reasoning and help models understand what constitutes an AI-generated image, we provide detailed annotations for synthetic images. Real images are not annotated because they lack synthesis flaws. Fake regions and explanations. Precise regional annotations and corresponding textual descriptions are essential for visual grounding and interpretability. We recruited 23 trained annotators to label the high-quality AI-generated images selected from the previous stage. Their primary task was to identify and describe all regions within each image that exhibited signs of being fake. Prior to annotation, all participants underwent standardized training focused on identifying visual cues of AI-generated content. The training emphasized the identification of fake regions, which are defined as areas within an image that either violate common sense or exhibit noticeable AI-generated artifacts. Examples of common sense violations include anomalies such as flamingo with three legs or bird feathers with metallic appearance. Common AIGC artifacts include repetitive patterns on blanket or blurred or illegible text. Annotators were also introduced to structured annotation rubric to ensure consistency and alignment with the datasets objectives. Each annotation consists of one or more fake regions, where each region is represented by tuple (Ri, Ti), where Ri denotes rectangular bounding box encapsulating the region, and Ti provides textual description of the identified anomaly or artifact. On average, an annotated image in the dataset contains 5.42 such (Ri, Ti) pairs, which serve as the foundation for grounding and reasoning in downstream model training. Image-level tagging. In addition to region-level annotation, annotators were asked to tag images based on broader perceptual attributes. These attributes include texture quality, overall realism, correctness of attributes (e.g., whether depicted objects adhere to expected characteristics), recognizability of objects, and the presence of other significant defects not explicitly listed (e.g., the occurrence of multiple sub-images within single image). These tags Ci are mutually independent, allowing annotators to assign zero or multiple tags to each image as appropriate. This tagging framework allows the dataset to capture holistic image quality assessments, particularly in cases where visually realistic AI-generated images may lack distinct localized flaws. Quality control. To ensure the reliability of the annotations, we implemented quality control protocol involving both manual inspection and algorithmic validation. subset of annotations was compared against reference set of fake region annotations curated by the research team. Given the inherently subjective nature of visual interpretation, we adopted tolerant validation criterion to accommodate diverse perspectives among annotators. Specifically, minimum Intersection over Union (IoU) threshold of 20.0% was applied for bounding box overlap, and an accuracy threshold of 33.3% was used for image-level taggings. These metrics were assessed on validation set comprising 5% of the annotated images. The IoU metric is used to assess the spatial agreement between annotated and reference bounding boxes. Let Rv represent the rectangular bounding box annotated by volunteer, and Rr represent the corresponding reference bounding box from the reference set. The Intersection over Union (IoU) is computed as: Rv IoU(Rv, Rr) = Rv Rr Rr , Rr Rv denotes the area of the intersection between Rv and Rr, and where denotes the area of their union. The IoU value ranges from 0 to 1, with higher values indicating stronger alignment. This quality control procedure ensures baseline level of annotation fidelity while preserving the diversity of human interpretations. The resulting dataset, enriched with both region-level annotations (R, ) and image-level tags C, offers robust foundation for analyzing the semantic inconsistencies and perceptual flaws of AI-generated images. Rv Rr"
        },
        {
            "title": "4 Methodologies",
            "content": "4.1 Overview of the fine-tuning strategy Our goal is to fully leverage the constructed dataset to train MLLMs to not only determine whether an image is AI-generated, but also to localize the relevant regions and explain the rationale behind its prediction. The training pipeline follows progressive optimization paradigm [49], beginning with supervised fine-tuning (SFT) [35] to establish formatting compliance and enable basic reasoning abilities, followed by Reinforcement Learning from Human Feedback (RLHF) implemented via progressive Group Relative Policy Optimization (GRPO). Before training, each images annotations are reformatted into an end-to-end dialogue between user and an assistant, using prompt structure designed for bounding-box-aware fine-tuning. Inspired by recent studies in reasoning-augmented language models [23, 19], we adopt structured output format that separates intermediate reasoning from final predictions. Region-level annotations (Ri, Ti) are enclosed within <think> markers, image-level tags Ci within <tag> markers, and the final verdict is wrapped in <verdict> markers. 4.2 Cold start with supervised fine-tuning The cold start phase of fine-tuning uses SFT to establish stable foundation before proceeding to RL. During this phase, all linear layers of the vision encoder, projector, and language model components in the MLLM are fine-tuned based on the supervision signals from the data. This initial fine-tuning is crucial for stabilizing the model prior to full-scale reinforcement learning training, preventing instabilities that might arise from pure RL-based updates [19]. The SFT process focuses on teaching the model to produce coherent reasoning patterns with clear structure. The training emphasizes the consistent use of the designated marker format with <think>, <tag>, and <verdict> fields, ensuring clarity and robustness in the models reasoning outputs. This structured Chain-of-Thought format reduces errors and improves explainability, providing solid foundation for subsequent GRPO stages that will refine the models performance on specific metrics. 4.3 Design of reward functions Reward design is critical component of RLHF, guiding the MLLMs to learn not only how to detect fake images, but also how to localize relevant regions and provide coherent reasoning. Let denote the textual output of the MLLM. We define three core reward functions for this purpose. Classification accuracy (Label). To ensure the model produces the correct verdict, we extract the classification decision from within the verdict marker and compare it with the ground-truth label: RC(o) = (cid:26)1, if (o) = y, 1, o.w. where (o) is regex match for the verdict, and is the ground truth label of whether the image is real or generated. Grounding accuracy (Relaxed IoU). To reward alignment between model-predicted and humanannotated regions, we use relaxed version of the Intersection over Union (IoU). This accounts for slight discrepancies between human annotations while still rewarding correct grounding: RG(o) = IoUη = min (cid:18) R(o) 1, η R(o) Ry Ry (cid:19) , where R(o) is the region extraction function that parses textual output to bounding boxes, Ry is the annotated region, and η is relaxing constant. This relaxation reward ensures full credit to the model when the regions annotated by models are in good correlation with human-annotated ones. Output format validity (Format). To ensure the model understands the structural requirements of the task, we introduce format reward that encourages outputs conforming to the expected syntax. valid output must include correctly structured <think>, <tag> and <verdict> markers, as well 5 Table 1: Weights for different GRPO stages used in the training pipeline. GRPO Stages Base IoU1.1 RG Label Format RC RF α 0 β -0.5 1.0 +1 / +2 / 1.5 2 1.5 +2 / 1 +1 / γ -1 2.0 0.5 / 0.5 / 1 Figure 2: Human preference matrix. Figure 3: Accuracy, IoU metric (upper), loss and reward curves (lower) of the model during the training process. as bounding boxes and captions that are syntactically well-formed and can be parsed using regular expressions. Formally, the reward is defined as: RF (o) = (cid:26)1, if (o), R(o), (o), C(o) are all parsable, 1, o.w. where (o) and C(o) extracts the caption of regions and image-level tags from o. 4.4 RLHF stages with group relative policy optimization After SFT, the model will continue to go through three different GRPO stages with variably-weighted reward functions. GRPO progressively aligns the MLLM with our objectives of interpretable and reliable fake image detection by combining structured supervision from the dataset with targeted = rbase + ωGRG + ωCRC + ωF RF , reward signals. The final reward function can be written as where rbase is stage-dependent base reward that ensures stable training dynamics across stages. (rbase, ωG, ωC, ωR) varies with stage; their exact values are provided in Table 1. It is worth noting that for RF (Format), we may apply asymmetric weighting for positive and negative outcomes (e.g., +2/1), allowing finer control over the reinforcement signal strength. RC (Label) and Stage α. The first stage prioritizes correct formatting by assigning it higher reward weight. It maintains balanced weighting for IoU and label accuracy (using +1/-1 rewards). This phase aims to solidify the models ability to adhere to the required output structure established during SFT. Stage β. This phase shifts focus towards improving the models core detection and localization performance. The reward weighting for IoU is increased (1.5x), and the reward for correct labeling is doubled. This stage penalizes wrong predictions and wrong formats, focusing on improving the detection capabilities of the model. Stage γ. This stage further refines the localization accuracy by increasing the IoU reward weight (2.0x). The rewards for labeling and formatting are adjusted again (both 0.5/-1) to maintain balance. In this stage, giving correct prediction of label and format will lead to reward of 0, which consequently challenges the MLLMs visual grounding capabilities to identify the actual artifacts present in the images. 6 Table 2: Experimental result for current AI-generated image detectors and our MLLM-based method across different image generation methods. Ours ObjectFormer [60] SegFormer [65] NPR [32] DMD. [10] ComFor. [37] AfPr. [4] AEROB. [43] DIRE [63] Generators Acc. IoU Acc. DALLE 2 [41] DALLE 3 [33] DDIM [52] DDPM [21] FLUX.1-dev [26] FLUX.1-schnell GLIDE [31] Midjourney v4 [30] Midjourney v5 SD 1.4 [45] SD 1.5 SD 2.1 [44] SD 3.5 Large [13] SD 3.5 Large Turbo VQDM [18] Diffusion BigGAN [2] GALIP [56] VQGAN [14] StyleGAN-XL [25] GAN PixArtAlpha [7] PixArtDelta [6] PixArtSigma [5] DiT [38] DiT VAR [57] Infinity [20] MaskGIT [3] LlamaGen [53] Others Real Images [11] 0.986 0.991 0.974 0.979 0.988 0.972 0.970 0.990 0.992 0.968 0.975 0.980 0.991 0.993 0.973 0.983 0.965 0.882 0.967 0.960 0.955 0.987 0.984 0.989 0.978 0.983 0.976 0.974 0.972 0.980 0.978 0.985 0.360 0.365 0.345 0.350 0.362 0.343 0.340 0.364 0.366 0.338 0.347 0.352 0.365 0.368 0.342 0. 0.335 0.353 0.337 0.330 0.337 0.357 0.354 0.360 0.349 0.354 0.346 0.344 0.342 0.351 0.348 - 0.957 0.949 0.954 0.951 0.958 0.953 0.950 0.956 0.959 0.952 0.955 0.951 0.954 0.957 0.953 0.954 0.950 0.941 0.954 0.951 0. 0.956 0.953 0.957 0.952 0.954 0.954 0.951 0.955 0.953 0.953 0.956 IoU 0.251 0.258 0.285 0.293 0.299 0.287 0.289 0.296 0.273 0.286 0.294 0.291 0.294 0.312 0.288 0.287 0.280 0.279 0.282 0.278 0. 0.295 0.292 0.296 0.290 0.293 0.287 0.289 0.288 0.429 0.369 - Overall 0.981 0. 0.954 0.299 Acc. 0.942 0.950 0.945 0.947 0.940 0.943 0.946 0.949 0.941 0.944 0.948 0.942 0.945 0.950 0.943 0.945 0.941 0.941 0.943 0.940 0.941 0.947 0.943 0.949 0.942 0. 0.945 0.941 0.948 0.944 0.944 0.946 0.945 IoU 0.285 0.292 0.280 0.288 0.295 0.283 0.286 0.294 0.297 0.282 0.290 0.287 0.293 0.296 0.284 0.290 0.278 0.289 0.280 0.275 0. 0.291 0.289 0.293 0.287 0.289 0.283 0.286 0.284 0.289 0.287 - 0.289 Acc. 0.907 0.912 0.917 0.903 0.922 0.926 0.913 0.908 0.902 0.921 0.916 0.911 0.904 0.906 0.927 0. 0.918 0.882 0.907 0.914 0.912 0.908 0.921 0.919 0.913 0.914 0.928 0.914 0.909 0.923 0.920 0.918 0.914 Acc. 0.934 0.942 0.928 0.931 0.937 0.929 0.935 0.939 0.943 0.970 0.949 0.938 0.944 0.947 0.932 0.941 0.892 0.882 0.889 0.884 0.890 0.912 0.909 0.915 0.907 0.910 0.893 0.897 0.895 0.899 0.898 0.903 0. Acc. 0.877 0.872 0.879 0.876 0.874 0.882 0.873 0.869 0.871 0.880 0.875 0.872 0.870 0.868 0.877 0.874 0.903 0.941 0.908 0.980 0.916 0.891 0.893 0.889 0.896 0.893 0.901 0.899 0.904 0.897 0.898 0. 0.882 Acc. 0.892 0.907 0.915 0.898 0.779 0.805 0.661 0.878 0.851 0.852 0.866 0.881 0.934 0.927 0.938 0.864 0.933 0.353 0.921 0.928 0.866 0.934 0.939 0.924 0.928 0.931 0.934 0.938 0.923 0.929 0. 0.934 0.887 Acc. 0.823 0.821 0.839 0.836 0.843 0.827 0.822 0.814 0.718 0.951 0.966 0.833 0.830 0.837 0.932 0.842 0.861 0.706 0.932 0.939 0.860 0.927 0.922 0.931 0.938 0. 0.927 0.924 0.933 0.938 0.933 0.854 0.873 Acc. 0.916 0.923 0.912 0.917 0.919 0.913 0.922 0.925 0.927 0.909 0.915 0.918 0.924 0.928 0.914 0.920 0.887 0.882 0.885 0.879 0. 0.903 0.899 0.905 0.897 0.900 0.889 0.883 0.886 0.892 0.889 0.896 0.911 Table 3: The accuracy of different detection methods on unseen image categories. All enlisted methods are trained on FakeXplained. Sources Ours ObjectFormer SegFormer NPR DMD ComFor AfPr AEROBLADE DIRE OpenAI 4o Preference [42] FaceForensics++ [46] 0.803 0.879 0.513 0.598 0.538 0. 0.790 0.861 0.735 0.562 0.636 0.429 0.597 0.746 0.458 0.681 0.793 0."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental setup We adopt Qwen2.5-VL-32B-Instruct [40, 61] as our base MLLM due to its balance between model size and performance, as well as its strong pre-trained grounding capabilities. We trained our MLLMs on 8x NVIDIA A100 GPUs. All baseline methods are trained on one NVIDIA A100 GPU. The SFT period runs for three epochs, during which the training loss converges; each GRPO stage lasts one epoch. More experimental details are provided in the supplementary materials. For baseline comparisons, we use the same training data as the MLLM setup. We adapt SegFormer [65] and ObjectFormer [60] to multitask setting using our dataset. Bounding box annotations are converted into binary masks for segmentation supervision, with real/synthetic classification employed as parallel classification task. For classification-only methods, including NPR [32], DMD [10], ComFor [37], AfPr [4], and DIRE [63], only image-level labels are used during training. 5.2 Overall performance To ensure robustness and mitigate dataset bias, all models are evaluated using four-fold crossvalidation. During training, the detection model is exposed to 75% of images from the FakeXplained dataset along with an equal number of real samples. Evaluation is conducted on the remaining 25% of synthetic images, again paired with the same number of real images. We report both classification accuracy and localization performance using the IoU metric. Note that IoU is computed only on synthetic images, as real images do not include annotated fake regions. Figure 4: sample user query and the corresponding model output (visualized). Effects of different training stages To analyze the impact of each training stage, we report both accuracy and IoU metrics across the entire training process in Figure 3. Without GRPO, SFT alone yields only marginal improvements over the base model. As the training progresses, each GRPO stage strategically adjusts the reward weights to highlight different aspects of model behavior. Notably, GRPOβ introduces significant boost in both accuracy and IoU, owing to the increased emphasis on classification (ωC) and grounding (ωG) rewards. The model reaches its best overall performance after completing GRPOγ, demonstrating the cumulative benefit of the progressive reward design. Human preference evaluation. While IoU and classification accuracy provide objective metrics for detection performance, they do not fully capture the qualitative aspects of region-caption alignment. In fact, the model may, in some instances, generate annotations that surpass those of the original human annotators. To comprehensively assess the quality and relevance of the generated explanations, we conducted human preference study involving an independent group of evaluators. In this study, participants were shown pairs of outputs for the same image, each with different bounding box annotations and associated captions. No tags or metadata were provided. Evaluators were asked to choose the annotation that demonstrated better alignment between the region and caption, as well as higher overall annotation quality. If no clear preference emerged, neutral option was available. After removing neutral responses, we obtained 1,525 valid preference votes. The comparative results are shown in Figure 2. The Humans category represents annotations from the FakeXplained dataset. When compared directly, human annotations were preferred in 52.9% of cases, indicating that our model achieves near-parity with human annotators in producing region-grounded explanations, demonstrating the effectiveness of our framework in generating high-quality and explainable visualtextual reasoning. Qualitative examples. Figure 4 presents two representative examples of the models output. In the upper example, the MLLM successfully identifies anomalous visual attributes, accurately localizes the flawed regions, and generates coherent explanation, concluding with correct verdict that the image is AI-generated. In the lower example, the model finds no obvious errors or artifacts in the image, reaching verdict that the image is real. Additional qualitative examples will be provided in the supplementary materials. Comparing to other methods. Quantitative results are presented in Table 2. Our best-performing model achieves an overall classification accuracy of 98.1%, demonstrating strong robustness and consistent performance across different image generators. For localization, the model achieves an IoU score of 37.8%, outperforming all segmentation-based baselines, including ObjectFormer [60] and SegFormer [65]. This highlights the models ability to identify fake regions that are more consistent with human annotations. 8 Table 4: Comparisons of model performance when trained with different configurations. Metric Best No-FT Partial Data Training Strategy no-bbox no-caption no-tags label-only SFT GRPOα GRPOβ GRPOγ Acc. IoU 0.981 0.378 0.734 - 0.956 - 0.958 0.261 0.977 0. 0.935 - 0.893 0.043 0.954 0.309 0.966 0.296 0.947 0.401 Out-of-distribution (OoD) evaluation. We also evaluated the models on two OoD datasets, FaceForensics++ [46] and images generated by ChatGPT-4o [34, 42]. As shown in Table 3, our model consistently outperforms all other methods across both datasets, demonstrating superior generalization to unseen generation domains. 5.3 Ablation studies Each variant in Table 4 isolates one training component to assess its individual contribution to overall model performance. Without FakeXplained/fine-tuning. Without any fine-tuning on FakeXplained (No-FT), Qwen2.5-VL-32B-Instruct achieves modest accuracy of 73.4%, indicating inadequate task-specific performance in its original state. The SFT stage alone improves the accuracy to 89.3%, and incorporating RLHF further improves it to 98.1%. These results demonstrate the substantial performance gains enabled by our two-stage fine-tuning pipeline and underscore the necessity of the FakeXplained dataset for enabling explainable and reliable fake image detection. Partial data. Our dataset consists of three essential components: image-level tags, region-level annotations with bounding boxes and captions, and binary real/fake labels. We conduct ablations by removing each component in turn to evaluate its impact. All ablation variants are trained using our complete two-stage pipeline (SFT + RLHF). If component is missing, it is excluded from both the SFT phase and the corresponding reward terms in RLHF. Specifically, when bounding boxes are unavailable, the RG reward is disabled, while other rewards remain active. Using only binary labels (label-only) results in the lowest accuracy among partial data variants at 93.5%. This highlights the importance of structured reasoning information (i.e., tags, regions, and captions) in improving detection accuracy. Nonetheless, this score is still considerably higher than the no-fine-tuning baseline (73.4%) and also surpasses the best image-only detector DMD (92.8%), showcasing the strong adaptability of MLLMs with two-stage fine-tuning for this task. When bounding box annotations and the corresponding captions are removed (no-bbox), the accuracy dropped by 2.5%. IoU is not computed in this setting. This performance degradation demonstrates the value of grounding supervision during training, which helps the model focus attention on semantically meaningful regions and enhances its decision-making process. Removing captions (no-caption) results in slight accuracy drop similar to the no-bbox variant, but causes significant decrease in IoU to 26.1%. This indicates that textual descriptions play crucial role in supporting accurate region-level localization. Captions also help the model articulate why region is fake, thus improving both interpretability and grounding precision. Excluding tags (no-tags) yields marginal drop in both the classification accuracy and the IoU. Tags capture perceptual attributes of the images, and their removal slightly degrades the models holistic understanding of image quality, though the impact is less severe than removing bounding boxes or captions. Training strategies. We evaluate the importance of dynamic reward weighting by comparing our progressive GRPO strategy against fixed-weight configurations (GRPOα, GRPOβ, and GRPOγ) with the same total number of training steps. As shown in the corresponding columns, all fixed-weight variants underperform compared to our full progressive GRPO pipeline. Notably, GRPOγ produces higher IoU than our best model due to its specialized reward design, but this comes at the expense of classification accuracy. Figure 2 also shows that these models are inferior to the best model, receiving no more than 30% of non-neutral votes when compared to human-annotated data. 9 The complete pipeline delivers the most balanced performance across all evaluation criteria, demonstrating that dynamic weighting of rewards is essential for simultaneously optimizing output formatting, classification accuracy, localization precision, and region-caption correlation."
        },
        {
            "title": "6 Conclusion",
            "content": "Our research presents an explainable AI-generated image detection approach utilizing MLLMs that transcends binary classification by providing human-interpretable explanations alongside accurate detection results. Through novel progressive GRPO paradigm, the system achieves superior performance metrics (98.1% accuracy, 37.8% IoU, sound human preference) compared to conventional methods. The work addresses the critical need for transparent detection systems that augment human judgment in an era of advancing generative technologies, establishing foundation for explainable visual media authentication that articulates the rationale behind algorithmic decisions."
        },
        {
            "title": "A Additional experimental details",
            "content": "A.1 Chat templates Annotated images are preprocessed into conversational formats before being used to train MLLMs. chat template specifies how the input components are organized to generate trainable conversation Ti. Formally: T(image, regions, captions, tags, is_real) Ti, To address potential overfitting, we developed nine distinct chat templates: three for real images and six for generated images, randomly selected during pre-processing. Figure 5 illustrates sample conversations derived from single annotated image using different templates. Although system prompts and user queries vary, the model responses do not change significantly in order to maintain consistent structure, ensuring the output parser remains independent of the user query. The impact of the number of chat templates is discussed in Appendix E.3. A.2 Training details Two-stage training. We use ms-swift [68] for fine-tuning Qwen-2.5-VL models. In the LoRA SFT stage, we noticed that freezing either the projector or the vision encoder leads to marginal improvement over the base model without training. To achieve optimal SFT performance, both modules must be fine-tuned jointly. After the SFT stage, we use GRPO instead of PPO. As noted in [49], GRPO obviates the need for additional value function approximation as in PPO, and instead uses the average reward of multiple sampled outputs. For each query q, GRPO samples outputs from the old policy model πθold, and uses the relative advantage to optimize the MLLM, making it particularly well-suited for multi-modal reasoning tasks where absolute reward calibration is challenging. We set the initial learning rate to 104 for the SFT stage and 105 for the RLHF stage. As shown in Figure 3 of the main paper, despite few loss spikes occurring at random steps during the SFT training process, the whole SFT process is relatively stable. Reward signals fluctuated early in each GRPO stage but quickly converged as the model adapted, confirming the effectiveness of our reward design and training strategy. o1, o2, . . . , oG} { Computational resources. The full training procedure took 41.0 hours on 8x NVIDIA A100 (80G) GPUs, among which 16.5 hours (40.2%) were spent on the SFT stage. At inference time, the end-to-end pipeline that takes an image as input, giving verdict and grounding information (if the image is deemed AI-generated) takes an average of 7.8 seconds on 2x NVIDIA A100 (80G) GPUs."
        },
        {
            "title": "B Annotation details of FakeXplained",
            "content": "Annotator qualification and training. All recruited annotators have prior experience in photography, have seen AI-generated images before, possess fundamental photographic literacy understanding, and are familiar with related concepts such as saturation, shadow, perspective, and noise. Instructions on fake regions. The rule for annotating fake regions is, if through observation of the selected regions of interest, humans should be able to clearly determine that the image is not an authentic photograph. Fake regions primarily show objects that do not follow the natural physics laws, or contradict common sense. Common image generation artifacts are also encouraged to be annotated. After selecting local area in the image, it is necessary to describe the reason for identifying it as generated image. The descriptive sentence must start with noun, followed by one or several adjective phrases or short clauses, and must exclusively describe content that appears in the region. 11 Figure 5: An example showing two different chat templates branched from one annotation entry. Definition of tags. We refer to the most prominent depicted object in the non-background portion of the image as the image subject. There are exactly five different tags that annotators can attach to an image. Their definitions are listed below. Perspective errors: Indicates that the image has an unnatural viewing angle, or errors in perspective, vanishing points. Incorrect occlusion and shadow errors do not constitute perspective errors, but can be considered as fake regions instead. Artistic styles: If the overall image presents any artistic style, including but not limited to oil painting, ink painting, or manga style, then select the Artistic Style tag. If only certain part of the image contains content in an artistic style, this tag should not be selected. Unknown objects: Indicates that the subject of the image does not exist in the world, or is obviously unreasonable. There may be unusual insects and furniture with strong design elements. Judgment should be based on intuition; unfamiliar or rare subjects do not necessarily indicate unreasonable or non-existent objects. Structure/attribute errors: Indicates that the subject of the image has structure that is inconsistent with common knowledge, or has attributes inconsistent with common knowledge. Examples include green flower petals, pink elephants, bent iron spoon handles, humans with more than two legs, and asymmetrical shapes. For erroneous attributes that only occupy small portion of the image subject, such as an incorrect number of fingers on human hand, fake regions should be marked as well. Texture errors: If obvious texture errors appear in the image, this tag needs to be selected. For example, the texture of the entire image is blurry, or portion of an object has repetitive, tilted, or distorted texture. Unreadable text does not qualify as texture error and should be labeled as fake region instead. If Artistic Style has already been marked, this tag is usually omitted. Other anomalies: If there are very obvious global errors in the image that do not belong to any of the above categories, check this item. This tag can also be marked even if other tags have already been chosen."
        },
        {
            "title": "C More examples",
            "content": "Figure 6 presents more annotated AI-generated images from FakeXplained. The left column displays the human annotations of FakeXplained. The right column shows the inference results of our best 12 Table 5: Comparative performance analysis under compression artifacts, spatial transformations, and resolution changes. Degradation & Metric SegF. [65] NPR [32] DMD. [10] ComFor. [37] AfPr. [4] AEROB. [43] DIRE [63] Ours ObjectF. [60] JPEG Compression (80% Quality) 0.978 Acc. IoU 0.350 JPEG Compression (30% Quality) 0.973 Acc. IoU 0.322 Random Cropping Downsampling (0.5x) Original Images 0.981 Acc. IoU 0.289 0.981 Acc. IoU 0.370 0.981 Acc. IoU 0.352 0.940 0.284 0.926 0.267 0.943 0. 0.929 0.259 0.954 0.299 0.927 0.231 0.915 0.198 0.934 0.176 0.931 0. 0.945 0.289 0.820 - 0.781 - 0.903 - 0.899 - 0.914 - 0.908 - 0.897 - 0.915 - 0.912 - 0.928 - 0.840 - 0.784 - 0.829 - 0.853 - 0.882 - 0.871 - 0.856 - 0.879 - 0.875 - 0.887 - 0.842 - 0.814 - 0.858 - 0.841 - 0.873 - 0.884 - 0.879 - 0.891 - 0.894 - 0.911 - model. The center bar indicates the proportion of human preference votes from our user study. Note that since the neutral option was allowed, although the third annotated image received 46.2% of the votes, the human annotator is still rated higher than our model response. Our model demonstrates the ability to generate clearer, more descriptive captions for fake regions and reliably identifies content that contradicts common sense. For instance, in the lock-and-keyhole example (row 5), the model successfully detects that the key is not inserted into the correct keyhole. In the volcano example (row 2), in addition to identifying the broken mountain body as in the human annotation, the model also detects subtle issue: the disconnection of the lava flow, highlighting its fine-grained visual reasoning capabilities."
        },
        {
            "title": "D Robustness against image perturbations",
            "content": "To evaluate the practical applicability of our approach, we conduct comprehensive robustness evaluation under common image degradations that are frequently encountered in real-world scenarios. Table 5 presents comparative performance analysis across three perturbation categories: JPEG compression, random cropping, and downsampling. Our method demonstrates exceptional resilience to JPEG compression artifacts, achieving low performance degradations of merely 0.3% and 0.8% from the uncompressed baseline, significantly outperforming current state-of-the-art methods. all of which experience at least 3% degradation. Notably, SegFormer and ObjectFormer show more stability than image-only classification models, indicating that grounding enhances robustness, although they still fall short of our method. For downsampling, we scaled the input images to 50% of their original width and height. In random cropping and downsampling experiments, our approach maintains the original accuracy of 0.981, indicating robust performance across different resolution scales. Meanwhile, downsampling does not severely affect the IoU score, which suggests that our grounded reasoning approach effectively captures semantic-level artifacts that remain detectable even at reduced resolutions, unlike methods that may rely on pixel-level features more susceptible to resolution changes. Since random cropping modifies the overall image layout, this action can remove certain fake regions from an image entirely, leading to lower IoU across all methods. Interestingly, we observe slight increase in IoU after downsampling. We hypothesize that this is because our grounding model focuses on the dominant artifact region, which remains visible at lower resolutions, while noisy fine details are suppressed, leading to more precise and focused localization. Overall, the consistent performance across perturbation types demonstrates that our model captures underlying semantic artifacts in AI-generated content, enabling robust detection even under challenging image conditions."
        },
        {
            "title": "E Additional ablation studies",
            "content": "E.1 Out-of-distribution performance We evaluate generalization capabilities of the ablation models in Section 5.3  (Table 4)  of the main paper on two OoD datasets: 4o [42] and FaceForensics++ [46]. Table 6 shows that our complete pipeline achieves accuracies of 0.803 and 0.879 respectively, compared to 0.412 and 0.565 for the base model without fine-tuning. 13 Figure 6: More annotation examples from FakeXplained and model response visualized from the fine-tuned MLLM. The ratio in the center shows the human preference score. 14 Table 6: Out-of-distribution performance evaluation across different datasets when trained with various configurations mentioned in the paper. Dataset Best No-FT Partial Data Training Strategy no-bbox no-caption no-tags label-only SFT GRPOα GRPOβ GRPOγ 4o [42] FF++ [46] 0.803 0. 0.412 0.565 0.765 0.834 0.780 0.833 0.797 0.831 0.456 0.680 0.587 0. 0.756 0.828 0.788 0.831 0.758 0.829 Among partial data ablations, the label-only configuration performs poorly, achieving accuracies similar to the non-fine-tuned baseline (45.6% vs 41.2% on 4o), confirming that spatial grounding is essential for generalization. The SFT stage alone yields moderate performance (0.587 on 4o, 0.678 on FF++). Subsequent GRPO stages improve results, with GRPOβ performing best among the variants. This result is consistent with findings discussed in our main paper, as the RLHF stages give more performance boost than the SFT stage. The consistent improvements across both datasets suggest our approach learns generalizable features for AI-generated content detection rather than dataset-specific patterns. E.2 Disabling LoRA We employ LoRA during training to reduce computational cost and memory usage. While fullparameter fine-tuning is technically possible, our results show that it does not improve accuracy or IoU. In fact, performance slightly degrades on the FakeXplained test set, likely due to the limited amount of annotated data (see No-LoRA). This suggests that LoRA provides more efficient and suitable training strategy under current data constraints. With significantly more training data, full fine-tuning may yield better results. E.3 Number of chat templates When some of these templates are absent from the training data, the detection accuracy does not change significantly, while the localization capabilities decline slightly. However, when evaluating on the OoD dataset, larger performance gap is found between models trained with fewer chat templates, with 3.3% accuracy drop for the 3+2 group and 2.7% for the 1+1 group on the FaceForensics++ [46] dataset, indicating that generalization capabilities can be improved with more chat templates. E.4 Model size variants We evaluate the 3B and 7B variants of Qwen-2.5-VL and observe that they behave quite differently from the 32B model. When fine-tuning these lighter variants, both models show marginal performance improvement with their corresponding base models. These results confirm that small models lack the capacity to support the complex visual reasoning and grounding required for this task, reinforcing the necessity of large-scale models. E.5 Other base models While InternVL-2.5 [9] also supports visual grounding, when trained using the same pipeline, InternVL-2.5 still suffers from hallucination issues, producing non-negligible proportion (13.7%) of responses that cannot be parsed. This occurs primarily due to the absence of grounding information in the responses, despite fine-tuning and carefully designed user prompts intended to guide the model toward providing such information. Limitations & future works Despite promising results, our approach still has limitations. The Qwen-2.5-VL-32B-Instruct model incurs substantial computational costs, which may limit deployment in resource-constrained environments. Our evaluation does not sufficiently cover domain-specific or real-world image types, such as medical, industrial, or artistic imagery. Future work should enhance robustness through 15 Table 7: More results from different training parameters, number of chat templates and models variants. Dataset Metric Best No-LoRA Chat Templates Model Size InternVL 2.5 FakeXplained 4o [42] FF++ [46] Acc. IoU Acc. Acc. 0.981 0.378 0.803 0.879 0.976 0. 0.806 0.854 1+1 0.978 0.374 0.786 0.858 3+2 3B (No-FT) 3B 7B (No-FT) 7B 8B (No-FT) 8B 26B (No-FT) 26B 0.982 0.369 0.795 0.846 0.710 - 0.407 0.488 0.715 0. 0.625 0.574 0.725 - 0.434 0.595 0.823 0.137 0.690 0.678 0.525 - 0.295 0.410 0.762 0.121 0.385 0.391 0.603 - 0.317 0.412 0.805 0. 0.581 0.639 domain-adaptive fine-tuning, data augmentation, and pre-processing pipelines that account for quality degradation and layout variations."
        },
        {
            "title": "G Broader impact",
            "content": "While our system improves interpretability in detecting AI-generated content, it may also introduce risks. The detailed explanations of detection rationale could inadvertently assist malicious adversaries in developing more sophisticated evasion techniques, potentially contributing to an adversarial arms race. The deployment of such systems without careful consideration could lead to over-censorship of legitimate content, particularly affecting artists and creators who use AI tools ethically. To mitigate these risks, we recommend responsible deployment frameworks, ongoing monitoring for bias and fairness, and collaborative development with stakeholders to ensure the technology serves the public interest while preserving legitimate creative expression."
        },
        {
            "title": "References",
            "content": "[1] Xiuli Bi, Bo Liu, Fan Yang, Bin Xiao, Weisheng Li, Gao Huang, and Pamela C. Cosman. Detecting generated images by real images only, 2023. [2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2018. [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [4] You-Ming Chang, Chen Yeh, Wei-Chen Chiu, and Ning Yu. Antifakeprompt: Prompt-tuned vision-language models are fake image detectors. arXiv preprint arXiv:2310.17419, 2023. [5] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [6] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [8] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025. Accessed: 2025-02-02. [9] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [10] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa In IEEE Verdoliva. On the detection of synthetic images generated by diffusion models. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2023. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. [12] Tarik Dzanic, Karan Shah, and Freddie D. Witherden. Fourier spectrum discrepancies in deep network generated images. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. [14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 17 [15] Joel Frank, Thorsten Eisenhofer, Lea Schönherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. Leveraging frequency analysis for deep fake image recognition. In Proceedings of the 37th International Conference on Machine Learning, ICML20. JMLR.org, 2020. [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Y. Bengio. Generative adversarial networks. Advances in Neural Information Processing Systems, 3, 06 2014. [17] Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, and Janis Keuper. Fake or jpeg? revealing common biases in generated image detection datasets. ArXiv, abs/2403.17608, 2024. [18] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1069610706, 2022. [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [20] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. [21] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. ArXiv, abs/2006.11239, 2020. [22] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [23] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [24] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, PhengAnn Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [25] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 43964405, 2018. [26] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [27] Yixuan Li, Xuelin Liu, Xiaoyang Wang, Shiqi Wang, and Weisi Lin. Fakebench: Uncover the achilles heels of fake images with large multimodal models. ArXiv, abs/2404.13306, 2024. [28] Zhengzhe Liu, Xiaojuan Qi, and Philip H.S. Torr. Global texture enhancement for fake face detection in the wild. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 80578066, 2020. [29] Peter Lorenz, Ricard Durall, and Janis Keuper. Detecting images generated by deep diffusion models using their local intrinsic dimensionality. 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pages 448459, 2023. [30] Midjourney. Midjourney, 2023. [31] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [32] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2448024489, 2023. 18 [33] OpenAI. Dalle 3 system card, 2023. [34] OpenAI. Introducing 4o image generation, Mar 2025. [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [36] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634, 2025. [37] Jeongsoo Park and Andrew Owens. Community forensics: Using thousands of generators to train fake image detectors, 2024. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [39] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [40] Qwen Team. Qwen2.5-vl, January 2025. [41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [42] Rapidata. Rapidata openai 4o preference, Mar 2025. [43] Jonas Ricker, Denis Lukovnikov, and Asja Fischer. Aeroblade: Training-free detection of latent diffusion images using autoencoder reconstruction error. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 91309140, 2024. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. [46] Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. FaceForensics++: Learning to detect manipulated facial images. In International Conference on Computer Vision (ICCV), 2019. [47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [48] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradientbased localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 618626, 2017. [49] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. 19 [50] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [51] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312.6034, 2013. [52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ArXiv, abs/2010.02502, 2020. [53] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [54] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, page 33193328. JMLR.org, 2017. [55] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. ArXiv, abs/1905.11946, 2019. [56] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clips for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1421414223, 2023. [57] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [58] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [59] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [60] Junke Wang, Zuxuan Wu, Jingjing Chen, Xintong Han, Abhinav Shrivastava, Ser-Nam Lim, and Yu-Gang Jiang. Objectformer for image manipulation detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. [61] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnngenerated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86958704, 2020. [63] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2244522455, 2023. [64] Wenyi Xiao, Leilei Gan, Weilong Dai, Wanggui He, Ziwei Huang, Haoyuan Li, Fangxun Shu, Zhelun Yu, Peng Zhang, Hao Jiang, et al. Fast-slow thinking for large vision-language model reasoning. arXiv preprint arXiv:2504.18458, 2025. [65] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In Neural Information Processing Systems (NeurIPS), 2021. 20 [66] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [67] Zicheng Zhang, Haoning Wu, Chunyi Li, Yingjie Zhou, Wei Sun, Xiongkuo Min, Zijian Chen, Xiaohong Liu, Weisi Lin, and Guangtao Zhai. A-bench: Are lmms masters at evaluating ai-generated images? arXiv preprint arXiv:2406.03070, 2024. [68] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, et al. Swift: scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2973329735, 2025. [69] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Shanghai Jiao Tong University"
    ]
}