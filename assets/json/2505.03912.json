{
    "paper_title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation",
    "authors": [
        "Can Cui",
        "Pengxiang Ding",
        "Wenxuan Song",
        "Shuanghao Bai",
        "Xinyang Tong",
        "Zirui Ge",
        "Runze Suo",
        "Wanqi Zhou",
        "Yang Liu",
        "Bofang Jia",
        "Han Zhao",
        "Siteng Huang",
        "Donglin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 2 1 9 3 0 . 5 0 5 2 : r OPENHELIX: Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation Can Cui1, Pengxiang Ding12*, Wenxuan Song4, Shuanghao Bai3, Xinyang Tong1, Zirui Ge2, Runze Suo1, Wanqi Zhou3, Yang Liu1, Bofang Jia1, Hangyu Liu12, Mingyang Sun12, Han Zhao12, Siteng Huang1, Donglin Wang1 1Westlake University 2Zhejiang University 3Xian Jiaotong University 4HKUST(GZ)"
        },
        {
            "title": "Abstract",
            "content": "Dual-system VLA (Vision-Language-Action) architectures have become hot topic in embodied intelligence research, but there is lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/. 1. Short Survey 1.1. Definition of VLAs Traditional policy learning has primarily focused on training novel behaviors from scratch using lightweight models. These models demonstrate high sensitivity to environmental perturbations (both visual and textual) and exhibit limited generalization capabilities. However, with the emergence of large language models (LLMs) and vision-language models (MLLMs), this landscape is undergoing significant transformation. These models, trained on Internet-scale data with vast parameter spaces, have demonstrated exceptional proficiency in text generation and visual comprehension, consequently generating substantial interest in their application to robotics policy training. In this context, RT-2 introduced the pioneering concept of vision-language-action models (VLAs), which co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks. VLAs have demonstrated remarkable improvements in generalizing to novel objects and se- * Project Lead. Email: dingpx2015@gmail.com mantically diverse instructions while exhibiting range of emergent capabilities. Furthermore, VLAs possess the potential to revolutionize robotic skill acquisition methodologies, as they serve as powerful foundation models that can be directly fine-tuned to adapt to domain-specific robotic applications. Subsequently, research on VLAs has proliferated rapidly, offering highly promising approach to addressing the challenges associated with robotic deployment. 1.2. Limitation of VLAs Directly applying VLAs in domain-specific or real-world scenarios remains challenging due to the following limitations: (1) The large and cumbersome model size of VLAs makes achieving efficient real-time performance difficult. For instance, RT-2 demonstrated that their 55B model operates at 1 to 3 Hz, while the 5B model runs at around 5 Hz under experimental conditions. In contrast, traditional lightweight models, such as BC-Transformer, operate at much higher speeds (around 50 Hz). (2) Pre-training is resource-intensive, and end-to-end fine-tuning of pre-trained VLAs on embodied data is also challenging due to domain shift and catastrophic forgetting. Leveraging existing MLLMs and VLAs for practical applications, while retaining their remarkable capabilities in multimodal understanding, reasoning, and generation, and ensuring fast inference for coherent actions, remains challenge that needs to be addressed. 1.3. Definition of Dual-System VLAs As result, the Dual-System VLAs were introduced. LCB [19] pioneered the adoption of the Dual-System VLA structure, while DP-VLA is the first to incorporate dualprocess theory to provide personified explanation for the rationale underlying this architecture. Dual-process theory [7, 10, 16, 21] conceptualizes human cognition as operating through two distinct systems: 1. System 1 is fast, automatic, intuitive, and unconscious. It operates effortlessly and relies on heuristics to make judgments and decisions. System 1 is responsible for our immediate reactions, such as making simple or routine choices. 1 Table 1. Methods Comparisons of Dual-System VLAs. Here, L, R, P, D, T, and PC represent different modalities: Language, RGB, Proprioception, Depth, Tactile, and Point Cloud, respectively. FT denotes fine-tuning. Pretrain and Scratch denote fine-tuning pre-trained policy head and training policy head from scratch, respectively. Method System 2 Latent Rep. System1 Model Input Training Policy Head Sensory Training LCB [19] DP-VLA [9] HiRT [24] Robodual [5] DexVLA [22] Helix LLaVA-7B OpenVLA-7B InstructBLIP-7B OpenVLA-7B Qwen2-VL-2B N/A L+R L+R L+R L+R L+R L+ + Lang(<ACT>) Vis+Lang Lora FT Frozen Lora FT MaxPooling(Vis+Lang) Lora FT Lora FT N/A Action+Lang Lang N/A 3D Diffusion Actor [11] Transformer RT-1 [4] DiT ScaledDP [25] Transformer R+P+PC R+P R+D+T+P R+P R+P Pretrain Scratch Scratch Scratch Scratch N/A System 1 often leads to biases and errors because it relies on mental shortcuts, such as heuristics, which can be effective in some situations but also result in systematic mistakes. In the robotics domain, this system closely resembles traditional lightweight policy networks, which are efficient but often task-specific. 2. System 2 is slow, deliberate, effortful, and conscious. It involves reasoning, logic, and careful evaluation of evidence. System 2 is engaged when performing cognitively demanding tasks, such as solving complex problems or making thoughtful decisions. System 2, while generally more accurate, requires greater cognitive resources and is also prone to errors when cognitive load is high or attention is limited. In robotics, this system is analogous to large-scale models like MLLMs and VLAs, which are computationally heavy but offer superior generalization capabilities. 3. Although the two systems operate in parallel, they update information at different frequencies. The slower System 2-like component updates less frequently and is responsible for making more deliberate decisions based on high-level representations. In contrast, the faster System 1-like component updates at higher frequency to rapidly generate the low-level actions required for real-time robotic control. Notably, the information from the slow system is subject to temporal delay. This architecture addresses the aforementioned challenge by simultaneously enabling efficient real-time inference while preserving the multimodal reasoning capabilities of large models. 1.4. Current Dual-System VLAs description and an RGB observation, LLaVA generates textual action description along with an <ACT> token. The <ACT> token, derived from the final layer, serves as highlevel latent goal. System 1 is pre-trained 3D Diffusion Actor that takes the RGB image, point cloud, and <ACT> token as input to generate actions. System 2 is fine-tuned using LoRA, while System 1 is fine-tuned in standard manner. DP-VLA introduces dual-process theory to justify the rationale behind the dual-system architecture. It presents more generalizable design choice, where System 2 is not limited to MLLMs, but can also be VLAs that are pre-trained on robot data. In experiments, DP-VLA adopts OpenVLA as System 2 and uses its encoder to extract latent representations from language instructions and RGB observations to guide System 1. System 1 is implemented using Transformer architecture, which encodes RGB images and proprioceptive inputs into actions. System 2 is kept frozen, while System 1 is trained from scratch. HiRT adopts InstructBLIP as System 2 and utilizes the finallayer representations obtained from encoding both language instructions and RGB observations. These representations are processed with MAP pooling to produce MLLM latent features that guide System 1. System 1 uses an EfficientNetB3 backbone combined with MAP block to encode RGB inputs into actions. System 2 is fine-tuned using LoRA, while System 1 is trained from scratch. We introduce recent Dual-System VLA approaches below, with comparative analysis of their distinctive features summarized in Table 1. It is important to note that for synchronous inference to occur, System1 must incorporate real-time perception inputs (such as RGB images). According to this criterion, approaches like Ï€0 [3], GR00TN1 [2], and similar methodologies cannot be properly classified within the dual-system framework as they lack this essential characteristic. LCB adopts LLaVA as its System 2. Given high-level task Robodual adopts OpenVLA as System 2 and extracts latent representations from language instructions and RGB observations. It uses both the task latent derived from the instruction and the final action latent as guidance signals. System 1 encodes RGB, depth, tactile, and proprioceptive inputs using ViT-based encoder, and employs Perceiver Resampler to distill key features. DiT model then generates actions by conditioning on the distilled features, the task latent, and noisy action input. System 2 is fine-tuned using LoRA, while System 1 is trained from scratch. 2 Figure 1. Key Design of Dual-System VLAs. It mainly includes: MMLM Selection, Policy Selection, Latent Feature Representation Selection, MLLM Training Strategy, Policy Training Strategy, Dual-System Integration Strategy, and Dual-System Asynchronous Strategy. 1.5. Key Design of Dual-System VLAs The key question lies in how to design the architecture of these two systems and structure the information flow from the slower system to the faster one in way that preserves the strengths of the System 2-like component while effectively guiding the System 1-like component to execute robotic actions. Achieving this delicate balance is essential for building robotic systems that are both highly performant and generalizable. As shown in Figure 1, to achieve this objective, several core issues need to be addressed: 1. MLLM Selection. For different VLA scenarios, requirements of MLLMs vary. For building model suitable for robotic scenarios, the MLLM model should be selected appropriately. For example, Flower [18]s foundation model has strong capabilities in spatial awareness/low-level vision, therefore achieving current SOTA across various tasks; MiniVLA [1] chose Qwen-VL 0.25B as its foundation model to reduce model inference costs and burden. Therefore, in an era of rapidly evolving MLLMs, we should clarify what kind of MLLM model is lightweight enough yet sufficient to complete robotic tasks, which is problem that needs to be addressed. Furthermore, whether MLLM pre-trained on robotic data is necessary remains an unresolved question. Training on extensive robotic datasets not only reduces the domain gap, but also, by exposing the model to more language instructions, makes its performance exceptionally robust on language instruction following tasks, as demonstrated in experiments with Robodual [5]. 2. Policy Selection. The choice of small models is relatively less controversial, with the current general consensus being that models based on DiT structure and Flow Matching structure can both meet current needs. However, with the introduction of new policy models such as CARP [8], Dense Policy [20], and other new architectures, downstream small models may also see new designs. Additionally, like Robodual [5], whether downstream small models need more modal information, and which modal information is essential for system1, is also potential question. 3. Latent Feature Representation Selection. For the selection of latent tokens, this is the most complex aspect of dualsystem tasks that urgently needs research. Previous methods have shown significant differences in their approaches. We need to consider not only dual-system work but also single-system work such as [2, 3, 13] For DP-VLA [9], they directly chose the last layer hidden embedding of the MLLM large model. Meanwhile, GR00T-N1 [2] selected hidden embeddings from middle layers, considering that middlelayer features might contain more visual information and could reduce inference time. Taking this further, Roboflamnigo [13] and HiRT [24] used maxpooling of the last layer language features and visual features as downstream conditions. Beyond directly utilizing MLLM hidden embeddings, some models (e.g., LCB [19]) additionally introduced the concept of <ACT> tokens, hoping to bridge upstream and downstream through fine-tuning special token, which showed promising results. The above two approaches were further developed in Robodual [5], which adopted multiple <ACT> tokens while also incorporating last-layer language features as latent feature representations. Of course, beyond the robotics domain, there are more ingenious works utilizing hidden states, such as Metaquery [17] and LEGO [12], which employed more sophisticated methods for latent feature selection. In summary, the selection of latent feature 3 representations will be an important research focus for dualsystem models, exploring more suitable latent features for downstream action generation models. 4. MLLM Training Strategy. Regarding how to train MLLMs, the main consideration is examining whether we can maintain the models generalization capabilities without loss while also ensuring good integration with downstream tasks. Currently, the main approaches include frozen and fine-tuning methods, but exploring whether there are better fine-tuning techniques remains valuable direction for research. 5. Policy Training Strategy. Regarding how to train the Policy, the main consideration is whether to reduce the models training cost. If we can take pre-trained policy and finetune it, this could greatly reduce the overall training time. Of course, if we train from scratch, whether the different optimization objectives would make model convergence difficult is also an unknown factor that needs to be explored. 6. Dual-System Integration Strategy. Regarding Integration strategies, the main focus is how to embed latent information as condition into downstream models. In LCB [19], the authors demonstrated using CLIP loss to constrain upstream latent features to be similar to the original text CLIP embedding to connect upstream and downstream components. However, this approach clearly limits the model to only handle cases it was trained on downstream, negating the purpose of introducing the generalization capabilities of MLLM models. Additionally, when introducing new embedding, differences in dimensions between upstream and downstream models are inevitable, making it common to add projector between them. However, how to train this projector requires careful consideration. In subsequent experiments, when the downstream policy is pre-trained one, it becomes critically important to pre-align the projector without training the MLLM. If both are unfrozen and trained simultaneously, the model training will collapse. Therefore, the Dual-System Integration Strategy is crucial aspect. 7. Dual-System Asynchronous Strategy. Lastly, there are asynchronous strategies for dual-system models. LCB [19], HiRT [24], and Robodual [5] employ different asynchronous approaches, with LCB [19] being the most naive, using synchronous training but asynchronous testing. Theoretically, differences in inference frequency between upstream and downstream components could affect final performance. However, this is not entirely accurate - if the upstream features being provided arent effective to begin with, perhaps asynchronous inference between upper and lower layers is merely pseudo-requirement. Therefore, more experiments are needed to verify this. 2. Empirical Evaluations According to the above survey, it is evident that current dualsystem models exhibit substantial variation across multiple Figure 2. Three Different Evaluation Environments. dimensions, including the choice of base vision-language model (MLLM), downstream policy architecture, and latent selection mechanisms [5, 9, 19, 22, 24]. These discrepancies highlight the urgent need for systematic and fair comparison, in order to assess the rationale behind different design choices and to establish reference framework for future model development. In this work, we standardize experimental conditions 1, 2, 3, and 7 to ensure consistency, and focus our evaluation primarily on conditions 4, 5, and 6. These conditions involve widely applicable techniques that are largely independent of the specific choices made in conditions 1, 2, 3, and 7. Through this controlled comparison, we aim to offer insights that may inspire and guide future research in this domain. We plan to extend our evaluation to cover additional conditions in future work. All updates and ongoing developments will be made available in our official GitHub repository: https://github.com/OpenHelix-robot/OpenHelix/. 2.1. Experiment setup Model Selection. To ensure consistency with LCB [19], we adopt LLaVA1.0 [14] as the visual language model (MLLM) throughout this paper. To eliminate discrepancies caused by differing policy architectures, all subsequent experiments utilize 3DDA [11] as the unified downstream policy. The integration of latent representations is implemented in accordance with the methodology introduced in LCB [19]. In line with LCB [19], we employ synchronous training and asynchronous testing for experiments involving asynchronous settings. Dataset Processing. Unlike LCB [19], which constructs chat-like response before the <ACT> token, we directly concatenate an <ACT> token after the instruction. This approach was adopted because we have not yet implemented this functionality. Additionally, we observed that even without implementing this feature, performance remains satisfactory. We plan to address the processing of chat-like data in future work. Environment. For comparison with models that are not 4 Table 2. Evaluation of single system in CALVIN-D environment. Model RF 3DDA Success rate (%) Static Left Forward Diagonal Circle 100 82 0 84 0 46 0 0 80 Table 3. Comparison of different training strategies for the low-level policy in standard CALVIN environment. MLLM Task completed in row (%) 1 4 2 3 Avg. Len Fine-tuning From-scratch 96 89 83 68 49 58 42 48 34 3.53 2.85 open-source but have published results, we selected the same environment as they used. To maintain consistency with LCB [19] and RoboDual [5], we selected the CALVIN environment as our core comparative simulation environment. Real-world environment experiments will be supplemented later. Standard Evaluation. Following prior works, we primarily validate our effectiveness in the ABC-D scenario. To ensure rapid assessment of experiments, we used the first 100 evaluations from the standard 1000 evaluations, improving the testing efficiency for ablation experiments. In the final evaluation Table 8, we extend the evaluations to full set of 1,000 to provide more comprehensive experimental results. Harder Evaluation. As is shown in Figure 2, in the standard evaluation test scenario, objects are static, and the given language instructions are standard. However, the dual system should inherently combine the language generalization capabilities of large models with the advantages of small models high-frequency characteristics for dynamic scenarios. Therefore, we conducted additional validation in two scenarios. 1. CALVIN-E: For language instruction generalization, we used Enriched language instructions for testing. 2. CALVIN-D: For dynamic scenario testing, in grasping tasks, we made objects move in four different ways within the environment to examine the models robustness in dynamic scenarios. 2.2. Why not single system? Preliminary. In fact, the definition of dual systems has always been ambiguous, but since we established the CALVIND experiment, we found that previous single-system work (e.g., Roboflmanigo [13]) would directly fail under such testing, therefore subsequent experiments were not conducted on single systems. Setup. The specific experimental configuration involved test5 ing models trained on the standard ABC dataset on CALVIND for 100 trials. The \"Static\" condition represents scenarios where standard objects do not move, while \"Left,\" \"Forward,\" \"Diagonal,\" and \"Circle\" represent four different object movement patterns. The specific results are shown in Table 2. Analysis. We discovered that the results of the RF [13] model on CALVIN-D were quite surprising, as it completely failed to complete the corresponding tasks in dynamic scenarios. The primary reason for the observed performance is that, during the testing phase, the RF method requires processing the previous six image frames to obtain the corresponding latent representations for LSTM-based action inference. While the latent representations typically remain stable during training, they become variable in the testing phase as result of object movement within dynamic scenarios. This discrepancy between training and testing conditions leads to significant drop in performance, resulting in consistently zero success rate in dynamic environments. Nevertheless, we can also observe that the RF model using MLLM demonstrates extremely high performance on simple tasks, showing much greater robustness than the smaller 3DDA model. This highlights the significance of using MLLM as the \"brain\" of the system. Discussion. Of course, we acknowledge that this conclusion may not be completely rigorous, as further testing on Ï€0 [3], GR00TN1 [2] has not yet been conducted. These additional experiments will be included in future work. 2.3. Training strategy of dual system For dual-system models, the main training strategy consists of three parts: how to train the low-level policy, how to train the high-level MLLM, and how to connect the two. The following experiments will be divided into these three components. 2.3.1. Policy Training Strategy. Preliminary. For LCB, the downstream low-level policy uses pre-trained 3DDA, while HiRT employs the RT-1 structure and trains from scratch. Robodual uses its own designed downstream policy. Setting aside the differences in configurations, there are two paradigms for policy training: training from scratch and fine-tuning from pre-trained models. Setup. For fair comparison, the large model configuration follows the LCB structure: LLaVA1.0 backbone, connected with the <ACT> token, all using CLIP Loss to align the <ACT> token with downstream instructions. The only difference is that the downstream policy uses either pre-trained 3DDA policy or policy trained from scratch. The specific results are shown in Table 4. Analysis. In Table 3, we discover that using pre-trained policy can improve performance while reducing overall training time. Therefore, subsequent experiments are all based on fine-tuning from pre-trained policy model. Figure 3. Three Different MLLM Training Strategy. Table 4. Comparison of different training strategies for the high-level MLLM in CALVIN environment. Benchmark MLLM Integration of MLLM and Policy Policy Task completed in row (%) 1 2 4 3 Avg. Len"
        },
        {
            "title": "CALVIN",
            "content": "Frozen Frozen Fine-tuning Fine-tuning CLIP Loss w/o CLIP Loss CLIP Loss w/o CLIP Loss Fine-tuning Fine-tuning Fine-tuning Fine-tuning 94 90 96 88 80 74 83 72 64 61 68 51 54 58 46 41 40 48 30 3.30 3.33 3.53 3.13 2.3.2. MLLM Training Strategy. Preliminary. For LCB, HiRT, and Robodual, the upstream large models all underwent fine-tuning. Although GR00TN1 [2] doesnt fall within the scope of dual systems, it achieved excellent results by adopting frozen paradigm for training. Therefore, we conducted experiments on both approaches. Setup. For fair comparison, the large model configuration follows the LCB structure: LLaVA1.0 backbone, connected with the <ACT> token, all using CLIP Loss to align the <ACT> token with downstream instructions. The downstream policy adopts the fine-tuning paradigm throughout. During the connection process between the MLLM and the policy model, we also introduced whether to include CLIP loss as variable. Analysis. For scenarios where the MLLM is frozen, adding or omitting the CLIP loss does not significantly affect performance. This is because the CLIP loss itself is meant to adjust the unchanged MLLMs output to accommodate the downstream small models input, resulting in minimal performance differences. However, when the MLLM requires fine-tuning, the impact of CLIP loss becomes highly significant. Without the constraint of CLIP loss, its easy to disrupt the small models already-trained attention mechanisms between conditioning and other perceptual inputs, potentially leading to performance degradation. Intuitive hypothesis. Although the introduction of CLIP loss makes the overall model performance functional, this approach essentially compromises the large models inherent generalization capabilities. Is there way to keep the large model parameters frozen while still ensuring that the large model can be updated together with the downstream components? Further setup. As shown in Figure 3, we only changed the training method of the MLLM Specifically, we adopted prompt tuning. We added new <ACT> token to the large models vocabulary and only trained the lm-head layer while keeping all other model parameters fixed. This approach essentially trains an additional token in the vocabulary that only relates to downstream tasks, without altering the MLLM models inherent generalization capabilities. Therefore, theoretically, it can better ensure the connection between the dual systems. Next, we use experiments to verify this hypothesis in Table 5. Further analysis. For the prompt tuning paradigm, while performance in the standard Calvin testing environment is comparable to other training paradigms, there are significant differences in experiments validating language generalization. Similarly, under the premise of having CLIP loss, the generalization capability of prompt tuning results far exceeds that of fine-tuning and frozen approaches. Moreover, without CLIP loss supervision, generalization actually improves somewhat, which fully demonstrates that the prompt tuning paradigm trains the large model with minimal dependence on altering the large models generalization capabilities. 2.3.3. Dual-System Integration Strategy. Preliminary. 6 Table 5. Further MLLM training experiments in CALVIN and CALVIN-E environment. Benchmark MLLM Integration of MLLM and Policy Policy Task completed in row (%) 1 3 4 5 CALVIN CALVIN-E Prompt-tuning Prompt-tuning Prompt-tuning Prompt-tuning Fine-tuning Frozen CLIP Loss w/o CLIP Loss CLIP Loss w/o CLIP Loss CLIP Loss CLIP Loss Fine-tuning Fine-tuning Fine-tuning Fine-tuning Fine-tuning Fine-tuning 94 81 72 76 72 78 77 54 55 49 37 62 67 41 40 30 21 52 27 26 15 11 42 47 15 20 4 5 Avg. Len 3.28 3.45 2.09 2.13 1.74 1. Table 6. Performance of different projector initialization in CALVIN environment. Here, Pre-alignment refers to training the projector prior to training the MLLM. Benchmark Pre-alignment MLLM Integration of MLLM and Policy Policy Task completed in row (%) 1 4 2"
        },
        {
            "title": "CALVIN",
            "content": "Frozen Fine-tuning Prompt-tuning Frozen Fine-tuning Prompt-tuning CLIP Loss CLIP Loss w/o CLIP Loss CLIP Loss CLIP Loss w/o CLIP Loss Fine-tuning Fine-tuning Fine-tuning Fine-tuning Fine-tuning Fine-tuning 94 96 94 0 0 0 80 83 77 0 0 64 68 67 0 0 0 51 58 60 0 0 0 41 48 47 0 0 0 Based on the experiments above, our conclusion is that using pre-trained policy and fine-tuning the MLLM with prompt tuning yields the best results. However, this still involves the process of how to connect the components, as the semantic gap between upstream and downstream can be substantial. Therefore, we primarily conduct the following ablation analysis. Setup. To connect the upstream and downstream components, an MLP projector is needed. We implemented two approaches here: First, directly unfreezing both upstream and downstream models and jointly training them together with the MLP projector. Second, initially freezing the upstream large model while training the MLP projector and downstream small model, then unfreezing the upstream large model for joint training. The main difference between these two approaches is whether there is separate MLP projector training process. The result is in Table 6. Analysis. We found that without prior projector prealignment, connecting upstream and downstream models based on frozen, fine-tuning and prompt tuning approaches directly fails. This demonstrates the importance of Projector pre-alignment in the connection process. Of course, if we adopt train-from-scratch approach for the downstream policy, two-stage process is not required. However, as shown in Table 2, training from scratch produces inferior results. 2.4. Testing strategy of dual system Preliminary. key essence of dual-system models is the need to implement asynchronous control between upper and lower layers. In LCB, the authors did not specifically handle asynchronous operations, instead using synchronous training followed by asynchronous inference. In HiRT, the authors adopted an additional buffer to introduce asynchronous operations during training as well. For Robodual, they utilized real-time replacement of the upper layers coarse actions with actions inferred by the lower layer to perform asynchronous operations. Here, we primarily validated the first approach, with the latter two paradigms to be updated subsequently. Setup. We evaluated different asynchronous steps from 1 to 60 on CALVIN-D. The step refers to the inference steps of action policy during single MLLM inference step. The longest environmental steps of the 3DDA are 60. Analysis. We observe surprising conclusion in Figure 4: regardless of the number of steps between the large models inferences, the performance changes are quite similar. Moreover, even in dynamic scenarios, the experimental results are consistent. Intuitive hypothesis. This result indicates that the current MLLM is not sensitive to changes in the current environment, which is counterintuitive. Therefore, we need to clarify exactly what information is being transmitted from the upper layers latent vector to the lower layer. Further setup. To explore the underlying reasons, we mapped the latent embeddings of action tokens into semantic space and calculated the similarity of different words to analyze what these action tokens from MLLM convey. The experiment involves dynamic scenarios where blue block consistently moves to the left. The result is in Figure 5. Further analysis. We have the following conclusions: 7 Figure 4. Evaluations on hierarchical inference. We evaluate the performance of the dual system on the CALVIN benchmark, with inference steps set to 1 and 60, respectively.Steps\" refers to the inference steps of action policy during single MLLM inference step. The longest environmental steps of the action policy [11] are 60, which means MLLM only inference once and represents the most typical asynchronous scenarios. Figure 5. Evaluation on the shortcoming of existing dual systems. From top to bottom, the first row displays the input to the MLLM. The second row visualizes special scenario where, at environment step 3, the blue block is manually shifted to the left. In the third row, we present the top 10 words that are semantically closest to the latent embedding. The bottom row illustrates the probability distribution of spatial words associated with the latent embedding. 1. As to the similarity with spatial words at different time steps, we observe that regardless of whether the robotic arm moves left or right, the probability of right\" is consistently higher than that of left,\" while the probabilities of different spatial prepositions remain almost unchanged over time. This indicates that the action token has learned semantic feature that remains constant and is unrelated to changes in the environment. The higher probability of right\" compared to left\" may be due to right\" carrying more semantic information; for example, right\" can also imply correctness, contributing to its consistently high probability. 2. As to Top 10 similar words at different time steps, we observe that the latent embedding primarily encodes the target object, spatial relations, and action semantics from the instruction, along with some noise. It means that the latent embedding mainly summarizes the textual instruction and 8 Table 7. Comparison of the using method of MLLM. Benchmark Type of MLLM Auxiliary tasks Policy Task completed in row (%) 1 4 2 5 Avg. Len CALVIN MLLM (Prompt Tuning) LLM (Prompt Tuning) MLLM (Prompt Tuning) Fine-tuning Fine-tuning Fine-tuning 94 77 77 48 92 67 26 76 60 16 72 47 10 63 3.45 1.77 4.01 is largely insensitive to changes in visual information. In other words, the current training method does not effectively leverage the visual reasoning capabilities of the MLLM. Instead, the MLLM merely transmits the semantics of the instructions to the low-level policy. 2.5. Whether the MLLM of dual system is enough? Preliminary. Based on the experimental analysis above, the information currently transmitted through the latent token is insufficient for the downstream model to effectively complete tasks. Therefore, in this section, we aim to explore better ways to utilize upstream information. Setup. The experiments are based on the above conclusions, with the downstream model using fine-tuning, adopting two-stage projector training approach, and the upstream large model utilizing prompt tuning training paradigm. However, three variants were created for how to use MLLM: 1. Standard MLLM; 2. Removing visual information from MLLM, treating it purely as an LLM; 3. Introducing an auxiliary loss, allowing the generated latent token to connect to an additional head layer to infer action-related information(position or rotation). The result is in Table 7. Analysis. From the experimental results, it can be seen that using only LLM produces results far inferior to MLLM, which demonstrates the inherent function of MLLM and shows it hasnt degraded to simply functioning as an LLM. When we have additional auxiliary tasks, we can see significant increase in the success rate of tasks. This is mainly because the additional auxiliary tasks force the model to capture more visual information in order to accomplish them, thus compelling the model to pay attention to tasks that purely MLLM approach would not focus on. 3. Simple yet Effective Dual System VLA Based on the above analysis, we employ prompt-tuning to adapt the output of the large model rather than directly finetuning the MLLM itself. Additionally, we introduce an auxiliary task to exploit MLLMs visual reasoning capabilities fully. This approach results in more robust latent embedding that effectively integrates visual and textual information. 9 Figure 6. Overview of our proposed Dual System VLA. 3.1. Architecture Network. Our system comprises two main components: pre-trained MLLM fÏ• and pre-trained policy Ï€Î¸, with parameters Ï• and Î¸, respectively. The MLLM includes text-only large language model and vision encoder, which projects images into the embedding space of the language model, allowing for multi-modal understanding of textual and visual inputs. The pre-trained policy consists of vision encoder and transformer-based diffusion model. Using multiple cross-attention layers, the diffusion model incorporates lot of conditioning information, such as 3D scene representations, proprioception information, and condition/instruction tokens from the high-level model. In this work, we leverage LLaVA [27] as the high-level MLLM and 3D Diffuser Actor as the low-level pre-trained diffusion policy. Notably, we use linear layer to replace the 3D Diffuser Actors text encoder, aligning the dimension of the latent embedding output by the large model with the input dimension of the low-level policy. Input and Output. The whole system is designed in the format to mimic demonstration trajectories {l, (o1, a1), (o2, a2), ...}, where = {wi Rd}N i=1 represents task-specific language instruction of length with an input dimension d, and ot and at denote the visual observation and corresponding robot action at each timestep t. The input observation ot consists of two RGBD images from different viewpoints. The output action at defines the end-effectors pose, which is decomposed into 3D location, rotation, and gripper state (open/close): at = {al {0, 1}}. The MLLM fÏ• R6, ag R3, ar Figure 7. Detailed framework. (a) The high-level MLLM (left) takes third-view RGB o, task instruction l, and learnable token <ACT> as input. After processing through the Large Language Model (LLM), we extract the feature embedding from the final layer of the <ACT> token as the latent goal for the low-level policy. To fully leverage the MLLMs multimodal reasoning capability, we propose an auxiliary task, using MLPs to predict the action (location al, rotation ar, open/close ag) based on this feature embedding z<ACT>, ensuring it encapsulates both visual and textual information. (b) The low-level policy (right) receives the latent goal from the high-level MLLM, combines it with 3D scene tokens and proprioception token c, and iteratively predicts action noise Ïµ to produce an accurate action trajectory Ï„ and gripper state ag. Notably, our approach keep all parameters of the MLLM frozen and fine-tune the learnable prompt to adjust the MLLMs output, significantly reducing training costs compared to previous methods. processes language instruction and the third-view RGB image t, outputting the latent embedding zt for low-level policy. The low-level pre-trained policy Ï€Î¸ takes as input the noisy trajectory Ï„ , diffusion step i, and the conditioning information from the environment observation ot, the latent embedding zt, and proprioception ct of timestep t, predicting the action trajectory Ï„t = (al t:t+T ) and binary states ag t:t+T at each timestep t, over temporal horizon . t:t+T , ar 3.2. Training Prompt Tuning. In order to avoid the degradation of MLLM, we introduce one learnable token <ACT> Rd at end of language instruction l. The new instruction is defined as = {l, <ACT>}. During training, all parameters of MLLM are frozen; we only update the embedding of learnable token <ACT>. Multimodal Reasoning Learning. As we discussed in section 3.3, we know that these previous methods do not fully utilize MLLMs visual reasoning capability. Specifically, they align the output of the large MLLM model with the output from the text encoder of CLIP. Using purely textual information to supervise the fine-tuning of the MLLM can lead to the degradation of multimodal reasoning capability. Therefore, we designed an auxiliary task to leverage the multimodal reasoning capability of the MLLM fully. This task is very simple and requires no additional data preparation process. The output embedding z<ACT> t, l) from the learnable prompt token is passed through linear layers to predict the action trajectories Ï„t and gripper actions ag . Through supervised training on this task, we ensure that the large model has to utilize visual input information and that the latent embedding contains blend of multimodal information. The loss function is defined as follows: = fÏ•(o Llm(<ACT>) = BCE(MLP(f + Ï‰1 MLP(f + Ï‰2 MLP(f Ï•(o Ï•(o Ï•(o t, l)), ag t, l)) al t, l)) ar t:t+T ) t:t+T t:t+T , (1) where Ï‰1, Ï‰2 are hyperparameters to balance the effect of each loss item, and MLP represents linear layer. To reconstruct the sequence of 3D locations and 3D rotations, we apply the L1loss. Additionally, we supervise the end-effector opening using binary cross-entropy loss (BCE). Diffusion Learning. Following the previous diffusion-based approach [6, 11, 23], we train our model using the action denoising objective. During training, we randomly sample time step and diffusion step i, adding noise Ïµ = (Ïµl, Ïµr) 10 Table 8. Results on CALVIN ABC-D: We report both success rates and average task completion length (out of 5 tasks) per evaluation sequence. MLLM (PT) denotes our proposed prompt tuning method for MLLM training. Policy(P) indicates loading from pretrained policy model. Asy(10) represents inference with 10-step time delay. AUX denotes the additionally introduced auxiliary tasks. Type Method CALVIN CALVIN-E Only Policy MLLM (PT) + Policy(P) MLLM (PT) + AUX + Policy(P) + Asy(10) MLLM (PT) + AUX + Policy(P) + Asy(60) Only Policy MLLM (PT) + Policy(P) MLLM (PT) + AUX + Policy(P) + Asy(10) MLLM (PT) + AUX + Policy(P) + Asy(60) 1 92.2 92.2 93.3 92. 65.2 71.3 78.9 78.1 Task completed in row (%) 3 4 2 78.7 79.2 81.8 79.7 39.1 44.9 57.1 56. 63.9 65.0 67.9 67.5 20.3 28.4 40.2 38.9 51.2 52.9 56.6 57.3 11.7 17.5 29.5 27.0 5 41.2 40.9 46.0 46. 6.1 10.3 20.2 19.5 Avg. Len. 3.27 3.30 3.45 3.44 1.42 1.72 2.26 2.20 to ground-truth trajectory Ï„ 0 . The objective is defined as: 3.3. Results , ct, Ï„ , ct, Ï„ , ct, Ï„ , i), ag , i) Ïµl , i) Ïµr Î¸ (ot, z<ACT> Î¸(ot, z<ACT> Î¸(ot, z<ACT> Lpolicy(Î¸, <ACT>) = BCE(Ï€g + Ï‰3 Ïµl + Ï‰4 Ïµr t:t+T ) t:t+T t:t+T , (2) where Ï‰3, Ï‰4 are also hyperparameters to balance loss items. Please refer to [1] for the details of the loss function. Two stage training. We adopt two-stage training approach to train our proposed dual system. In the first stage, to initially align the embedding produced by the MLLM with the feature space of the pre-trained policy, we freeze the parameters of the large model and the low-level policy, training only the prompt and projection layers. In the second stage, we keep the large model frozen and unfreeze the low-level policy, fine-tuning it together with the prompt and projection. The objectives in both stages remain unchanged. The only difference between the two stages is whether the low-level policy is frozen. In summary, our loss function includes two components and can be defined as follows: Ltotal = Llm + Lpolicy (3) Implementation Details. We use LLaVA-7B[15] and 3D diffuser Actor[11] as the high-level MLLM and low-level policy models, respectively. We select the checkpoint of 3D diffuser Actor at 65,000 iterations as the pre-trained parameters. During training, the first stage (pre-alignment) is conducted for 2,000 iterations, and the second stage continues until 100,000 iterations. The projection is linear layer that reduces the output dimension of the large model from 4096 to 512. We manually add an <ACT>token to LLavas tokenizer and freeze all parameters of the MLLM, fine-tuning only the newly added token embedding. The remaining experimental and training settings are consistent with 3D diffuser Actor; please refer to [11] for details. 11 From Table 8, we can draw conclusions similar to those from the previous empirical study: 1. The crucial role of integrating the upper and lower layers of the model lies in improving performance in language generalization scenarios. 2. Additional auxiliary tasks are very helpful for enhancing both standard task performance and generalization performance, mainly because they improve the models action capability. 3. Asynchronous inference has little impact on the inference performance of the general task model; even if the model only performs asynchronous inference once (Asy (60)), the final performance remains largely unchanged. 4. Discussion & Limitation We first acknowledge that there is still long way to go before we achieve full open-source reproduction of Helix. 1. Deploying on real robots. 2. Achieving sufficiently fast downstream policy execution. 3. Successfully running on physical robots. 4. Deploying on humanoid robots. 5. Realizing collaboration between humanoid robots. There is indeed much work to be done before all of the above goals are achieved. However, this technical report is only our initial version. We are committed to continuously updating this project to fulfill the open-source objectives for all the tasks mentioned above. In addition, we maintain an open attitude toward some of the claims in this article that have not yet been fully verified. We hope that more researchers will join our team, or that more people will support us, so that we can accomplish this meaningful work for the entire community. Since some of the organization was done rather hastily, if any corrections are needed, all authors are welcome to contact me at any time."
        },
        {
            "title": "References",
            "content": "[1] Belkhale and Sadigh. Minivla: better vla with smaller footprint. 2024. 3 [2] Johan Bjorck, Fernando CastaÃ±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. 2, 3, 5, 6 [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 2, 3, 5 [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023. 2 [5] Qingwen Bu, Hongyang Li, Li Chen, Jisong Cai, Jia Zeng, Heming Cui, Maoqing Yao, and Yu Qiao. Towards synergistic, generalized, and efficient dual-system for robotic manipulation. arXiv preprint arXiv:2410.08001, 2024. 2, 3, 4, 5 [6] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. [7] Jonathan St BT Evans. Dual-processing accounts of reasoning, judgment, and social cognition. Annu. Rev. Psychol., 59 (1):255278, 2008. 1 [8] Zhefei Gong, Pengxiang Ding, Shangke Lyu, Siteng Huang, Mingyang Sun, Wei Zhao, Zhaoxin Fan, and Donglin Wang. Carp: Visuomotor policy learning via coarse-to-fine autoregressive prediction. arXiv preprint arXiv:2412.06782, 2024. 3 [9] ByungOk Han, Jaehong Kim, and Jinhyeok Jang. dual process vla: Efficient robotic manipulation leveraging vlm. In Conference on Robot Learning (CoRL), 2024. 2, 3, 4 [10] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011. 1 [11] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. arXiv preprint arXiv:2402.10885, 2024. 2, 4, 8, 10, 11 [12] Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James Rehg, and Miao Liu. Lego: earning ego centric action frame generation via visual instruction tuning. In 12 European Conference on Computer Vision, pages 135155. Springer, 2024. 3 [13] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023. 3, 5 [14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 4 [15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 11 [16] Wim De Neys. Dual processing in reasoning: Two systems but one reasoner. Psychological science, 17(5):428433, 2006. [17] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 3 [18] Moritz Reuss, Hongyi Zhou, Marcel RÃ¼hle, Ã–mer ErdinÃ§ Yagmurlu, Fabian Otto, and Rudolf Lioutikov. Flower: Democratizing generalist robot policies with efficient visionlanguage-action flow policies. In 7th Robot Learning Workshop: Towards Robots with Human-Level Abilities. 3 [19] Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From llms to actions: Latent codes as bridges in hierarchical robot control. arXiv preprint arXiv:2405.04798, 2024. 1, 2, 3, 4, 5 [20] Yue Su, Xinyu Zhan, Hongjie Fang, Han Xue, Hao-Shu Fang, Yong-Lu Li, Cewu Lu, and Lixin Yang. Dense policy: Bidirectional autoregressive learning of actions. arXiv preprint arXiv:2503.13217, 2025. 3 [21] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. science, 185 (4157):11241131, 1974. 1 [22] Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. 2, 4 [23] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. In ICRA 2024 Workshop on 3D Visual Representations for Robot Manipulation, 2024. [24] Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. Hirt: Enhancing robotic control with hierarchical robot transformers. arXiv preprint arXiv:2410.05273, 2024. 2, 3, 4 [25] Minjie Zhu, Yichen Zhu, Jinming Li, Junjie Wen, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, et al. Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation. arXiv preprint arXiv:2409.14411, 2024."
        }
    ],
    "affiliations": [
        "HKUST(GZ)",
        "Westlake University",
        "Xian Jiaotong University",
        "Zhejiang University"
    ]
}