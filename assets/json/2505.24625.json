{
    "paper_title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors",
    "authors": [
        "Duo Zheng",
        "Shijia Huang",
        "Yanyang Li",
        "Liwei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 2 6 4 2 . 5 0 5 2 : r Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang The Chinese University of Hong Kong https://lavi-lab.github.io/VG-LLM"
        },
        {
            "title": "Abstract",
            "content": "Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Birds-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations."
        },
        {
            "title": "1\nRapid advancements and impressive performance of Multimodal Large Language Models (MLLMs)\n[2, 24, 26, 29, 33, 40, 41, 44] have driven their applications in various fields, such as 3D scene\nunderstanding [11, 20, 21, 22, 36, 61, 66], vision-language-action models [3, 25, 37, 48], embodied\nnavigation [58, 62, 64], and learning 3D knowledge from videos [36, 58, 61].",
            "content": "Efforts [36, 42, 61] have been made to improve the 3D spatial understanding capability of MLLMs by considering scenes as video sequences. For example, Video-3D LLM [42, 61] injects 3D coordinates into visual features at patch level to improve 3D perception. GPT4Scene [36] leverages BEV maps [28] rendered from reconstructed 3D point clouds for global awareness. However, shared limitation of these approaches is their dependence on dense 3D data input (e.g., depth maps and point maps), which are often hard to acquire in certain real-world scenarios. Although estimating 3D attributes directly from images is possible [39, 43], it can introduce estimation errors and therefore degrade the performance, thus restricting their practical applicability. This naturally leads to the question: Can MLLMs understand the 3D world directly from videos without any explicit 3D data input? Recent research [51] has shown that MLLMs face difficulties in understanding 3D geometry from encoded visual representations. This issue arises because these MLLMs process video frames as separate tokens through visual encoder, which fails to capture crucial 3D geometric information, such as correspondences across frames [30]. Consequently, the MLLM backbone has to infer the 3D structure from the visual tokens to comprehend spatial relationships, which is both challenging denotes equal contribution Corresponding Author and resource-intensive. This process often requires extensive supervision [6, 15, 57] and meticulous design to prevent issues such as catastrophic forgetting during fine-tuning [55]. These challenges highlight the critical need for methods that can incorporate 3D geometry priors into MLLMs. In this work, we propose Video-3D Geometry LLM (VG LLM), novel framework designed to explicitly integrate 3D visual geometry priors into MLLMs. To achieve this, we introduce 3D visual geometry encoder that enriches input visual sequences with additional geometric information. Specifically, input images are processed by both conventional visual encoder and the newly integrated 3D visual geometry encoder. The features extracted by these encoders are fused at the patch level and subsequently passed to the MLLM backbone. As the 3D visual geometry encoder is pre-trained on tasks such as point map prediction on pairs or sequences of images [39, 43, 45], it embeds strong 3D perception prior knowledge and is able to capture correspondences across frames. By doing so, VG LLM can effectively incorporate 3D geometry priors into the model and become more robust to viewpoint transformations, significantly improving its spatial reasoning abilities. Extensive experiments have been conducted on various 3D scene understanding and spatial reasoning tasks, where the model accepts video input. These 3D scene understanding tasks include 3D visual grounding [7], 3D dense captioning [8], and 3D video object detection [46]. For spatial reasoning tasks, we evaluate our model on VSI-Bench [51], CV-Bench [41], and BLINK [19]. The experiments show that our fine-tuned 4B model outperforms larger spatial-enhanced models by substantial margin. The results uncover several interesting findings: (1) Without explicit dense 3D inputs, our approach outperforms many leading 3D input-based models, underscoring its effective 3D geometric understanding. (2) By implicitly modeling inter-frame correspondences within the visual representation, our model has learned strong egocentric-allocentric transformation capabilities, leading to significant improvements of Recall by 19.3 and F1 by 8.3 on 3D video object detection. (3) On tasks that require complex spatial reasoning skills, i.e., VSI-Bench [51], our 4B model attains an impressive average score of 46.1%, surpassing even the best proprietary model, Gemini-1.5-Pro [40]. This highlights the great utility of 3D geometry modeling in broader scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Large Language Models (MLLMs). MLLMs [2, 24, 26, 29, 33, 40, 41, 44] have achieved significant progress in 2D image and video understanding. However, recent findings [31, 51] indicate critical limitation: current MLLMs still struggle with complex visual spatial reasoning tasks. To address this challenge, some research efforts [2, 41, 44] have focused on enhancing the models ability to perceive and process spatial relationships through improved model representation. For example, Cambrian-1 [41] integrates self-supervised 2D visual representations with semantically rich features, aiming to provide more comprehensive understanding of visual content that could benefit spatial reasoning. These methods focus on enriching the representation of each individual image, neglecting the 3D geometry information inherent in the continuous frames of video. In contrast, our approach integrates 3D geometry priors from the video into the MLLM. 3D Large Language Models. Recent efforts [11, 14, 18, 20, 36, 61] have focused on enabling MLLMs to better understand 3D scenes. Previous work develops comprehensive 3D scene representations by using different types of 3D data. These include point cloud features [11, 14, 50], lifted multi-view image features [18, 20, 66], treating multi-view images as video sequences [36, 61]. The most closely related approaches are Video-3D LLM [61], which integrates positional information into the visual representation, and GPT4Scene [36], which constructs BEV maps through 3D reconstruction. In contrast to these methods, our model does not require any explicit dense 3D inputs. Spatial Reasoning. Some work [5, 6, 15] has enhanced spatial understanding through large-scale synthesized VQA datasets for improved depth estimation. However, these methods primarily focus on static images, overlooking the dynamic and relational aspects inherent in complex spatial reasoning scenarios. To address this limitation, the Visual-Spatial Intelligence Benchmark (VSI-Bench) [51] was introduced, explicitly evaluating relational reasoning and egocentric-allocentric transformation abilities. Recognizing the scarcity of data for such complex scenarios, subsequent works like SAT [38] and SPAR [57] have proposed data generation pipelines to synthesize spatial QA datasets for supervised fine-tuning. In contrast, our model elicits 3D geometry information from the video and injects it into the model architecture, improving its spatial reasoning capabilties. 2 Figure 1: The architecture of our VG LLM. The 3D visual geometry encoder processes sequence of images to produce globally geometry-aware visual features, while the 2D visual encoder extracts vividly shows that the latent 3D semantic-aware visual features from each individual image. geometry tokens are able to recover the 3D scene if with dense prediction head [43]."
        },
        {
            "title": "3 Method",
            "content": "We aim to model the 3D visual geometry in MLLMs without relying on explicit dense 3D input. To achieve this, as illustrated in Figure 1, we introduce 3D visual geometry encoder to extract rich visual-geometric features from multiple images and feed them into the MLLM backbone. Section 3.1 outlines the architecture design of our model, and Section 3.2 details the training process. 2p 2p c. p }n 3.1 Architecture Preliminary. Given sequence of RGB images {Ii}n i=1 and natural language question Q, conventional MLLM utilizes 2D visual encoder to encode these images into image tokens c first, where Ii Rhw3 and is the patch size. Then an MLLM backbone accepts {T i=1 as its input and outputs the response. In this work, we choose Qwen2.5-VL [2] as the MLLM backbone. Note that Qwen2.5-VL additionally compresses the image tokens to reduce the computational cost. Qwen2.5-VL groups spatially adjacent 2 2 patches into single image token, resulting in smaller set of image token input to MLLM backbone, 3D Visual Geometry Encoder. To model 3D geometric information like inter-frame correspondences within input frames, we employ 3D visual geometry encoder to extract such information. from This 3D visual geometry encoder produces 3D visual geometry features all input images {Ii}n i=1 jointly. 3D visual geometry models [45, 39, 43] are natural candidates for the 3D visual geometry encoder, as they are trained to capture inter-frame correspondences and reconstruct 3D scenes without relying on additional 3D priors. These 3D visual geometry models comprise three key components: an encoder for per-image feature extraction, fusion decoder for cross-frame interaction, and task-specific prediction heads for 3D attributes. Since we focus on feature extraction, which embeds 3D geometry prior information, rather than directly outputting 3D attributes, we leverage the encoder and the fusion decoder as our 3D visual geometry encoder. Specifically, we choose VGGT [43] to extract 3D visual geometry features given its superior performance in 3D tasks. Visual Feature Fusion. We fuse both the image tokens {T features {T into augmented visual features merging strategy in Qwen2.5-VL, where we concatenate spatially adjacent 2 2 features in pass them to two-layer MLP to output single feature in To this end, the final visual features {T question into the MLLM backbone to produce the response. i=1 and the 3D visual geometry i=1 before passing them into the MLLM backbone. We first transform each 2p , and then generate the geometryi aligns with the spatial and 2p c, which has the identical shape of + i=1 are concatenated with the text embeddings of the . The transformation of }n i h = i }n }n . i 3 3.2 Training Our VG LLM offers versatile framework designed to integrate 3D vision priors into MLLMs for various 3D tasks. In this work, we demonstrate the application of VG LLM to 3D scene understanding and spatial reasoning tasks. 3.2.1 Applying VG LLM to 3D Scene Understanding To validate the efficacy of the model in understanding and reasoning about 3D scenes, we adapt it to several 3D scene understanding tasks, i.e., 3D visual grounding, 3D dense captioning, and 3D video object detection. In contrast to previous work on understanding 3D scenes, our model relies solely on RGB images as input during both training and inference, without any 3D scene data as prerequisites. For 3D dense captioning, while we utilize pre-detected 3D proposals as the input, our model itself operates solely on RGB images. We directly learn all 3D scene understanding tasks through text generation, which obviates the need for task-specific heads while maintaining simple next-token prediction objective during training. The unified text generation objective allows us to mix all tasks into combined dataset for multi-task training. The detailed data format can be found in the appendix. Coordinate System and Representation. Since we do not utilize any ground truth information of the 3D scene, we follow VGGT [43] to employ the coordinate system of the first frame as the base coordinate system. All coordinates are transformed into this base coordinate system (except for 3D visual grounding, which represents the bounding box in each frames coordinate system). All the numbers are expressed in plain text with 2 decimal places. 3D Visual Grounding. We follow previous work [57] to formulate the 3D visual grounding task as 3D video grounding problem, which involves locating the frame index where the object appears and its 3D bounding box in the corresponding frames coordinates in one feed-forward pass. Unlike SPAR [57], which generates axis-aligned boxes, our method directly predicts 3D-oriented bounding boxes, making it more suitable for real-world scenarios. Specifically, given the 3D scene represented by frames {I1, I2, , In}, and natural language query Q, our model is designed to output both the frame index and the bounding box in the form (x, y, z, w, h, d, ψ, θ, ϕ), where (x, y, z) is the center coordinate, (w, h, d) is the object size, and (ψ, θ, ϕ) is the rotation angles. 3D Dense Captioning. We follow the recent work [61, 66] that decomposes the 3D dense caption task into two phases, i.e., detecting 3D object proposals with an off-the-shelf detector and generating object descriptions based on object coordinates. Specifically, given the 3D scene {I1, I2, , In}, we prompt our model to describe the object located at (x, y, z) in detail, where (x, y, z) is the box center of the object to be described. This setup ensures fair comparison with prior work and effectively evaluates the models ability to understand positional and spatial relationships. 3D Video Object Detection. To investigate the capability of handling egocentric-allocentric transformation, we set up 3D video object detection task based on the ScanNet [16]. Unlike previous benchmarks that focus on monocular or multi-view detection [46], our task requires models to detect all objects throughout the video in unified coordinate system, without relying on explicit camera parameters or depth information. Since some objects arent visible in the first frame, the model must track changes between frames, estimate camera movement, and convert object locations from global view to the cameras perspective. Specifically, given continuous video frames {I1, I2, , In}, the model is tasked to detect all objects {(b1, c1), , (bm, cm)} that appear in this video, where each bi in the form (x, y, z, w, h, d, ψ, θ, ϕ) represents its bounding box in the unified coordinate system and ci is its category. 3.2.2 Instruction Tuning for Enhanced Spatial Reasoning. Previous work [56, 65] has highlighted that existing pre-training and instruction tuning datasets often lack sufficient spatial-related phrases in their annotations, consequently hindering the spatial reasoning capabilities of MLLMs. To overcome this limitation, we leverage the SPAR-7M [57] dataset for instruction tuning. SPAR-7M is comprehensive spatial reasoning dataset curated from three richly annotated 3D datasets: ScanNet [16], ScanNet++ [52], and Structure3D [63]. It encompasses 33 diverse tasks, ranging from fundamental perception to mid-level viewpoint transformation and high-level scene imagination. As some work has shown that post-training on specific datasets can sometimes degrade the original performance on general benchmarks, we also incorporate visual 4 instruction tuning dataset, the LLaVA-Video-178Ks LLaVA-Hound split [60], into our training pipeline to preserve the generalization capability."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we first present the implementation details of our model, followed by an evaluation of our models performance on 3D scene understanding tasks in Section 4.1, including 3D visual grounding, 3D dense captioning, and 3D multi-view detection. Next, we provide comprehensive comparison with state-of-the-art methods on spatial reasoning benchmarks and generic multimodal benchmarks in Section 4.2. 3D scene understanding emphasizes 3D perception, while spatial reasoning focuses on interpreting and reasoning about spatial relationships in video. We train two models for 3D scene understanding and spatial reasoning tasks separately for fair comparison. Finally, we conduct ablation studies to reveal the effectiveness of integrating 3D geometry into MLLMs in Section 4.3. Implementation Details. Our model is built upon Qwen2.5-VL-3B [2], integrated with VGGT-1B [43] as the 3D geometry encoder. We trained our model for one epoch on mixed dataset, detailed as followed in this section, and employed Adam with batch size of 16 and warmup ratio of 0.03. During the warmup phase, the learning rate was gradually increased to 1e-5 before linearly decaying to 0. In each training step, batch was randomly sampled from single source from the mixed dataset. Please refer to the appendix for more details. 4.1 3D Scene Understanding Tasks 4.1.1 Setting Datasets and Benchmarks. To evaluate the versatility of our model across diverse 3D scene understanding tasks, we employ multi-task learning approach on combination of datasets. 3D Visual Grounding. We leverage the ScanRefer [7] dataset, which provides 36,665 object descriptions paired with axis-aligned bounding boxes across 562 indoor scans. We follow SPAR [57] to reformulate 3D visual grounding as 3D spatial-temporal video grounding, aiming to locate the target objects bounding box in camera coordinates along with its corresponding frame index. To determine the relevant appearance frame, we utilize the visible object annotations from EmbodiedScan [46]. We match the target 3D bounding box with those in EmbodiedScan based on their IoU and subsequently pick the optimal frame by comparing the 2D projection areas. 3D Dense Captioning. We utilize the Scan2Cap benchmark [8] for 3D dense captioning, which requires generating descriptive captions for all objects within scene. Following prior work [22, 61, 66], we use Mask3D-detected object proposals extracted by LEO [22] and task our model with generating captions conditioned on their center coordinates. To better leverage the visual geometry, we transform all object center coordinates to the coordinates of the first captured frame. 3D Video Object Detection. For 3D video object detection, we curated dataset from EmbodiedScan [46] consisting of consecutive frames and their corresponding visible object annotations in indoor scenes. Each sample comprises four consecutive frames sampled at 1 FPS, with all associated object instances transformed to the coordinate system of the initial frame. Consistent with EmbodiedScan, we divide the data into 958 training and 243 evaluation scenes. Within each scene, we randomly select 150/10 samples for training/evaluation. Comparison Baselines. For the tasks of 3D visual grounding and dense captioning, our evaluation includes comparisons with both task-specific expert models and general-purpose 3D models. Specifically, for expert models, we compare our method against ScanRefer [7], MVT [23], and ViL3DRel [10] on ScanRefer [7]. On the Scan2Cap dataset [8], we include Scan2Cap [8], 3DJCG [4], D3Net [9], and Vote2Cap-DETR [12] for comparison. Furthermore, we compare our approach with the generalist models, including Chat-3D v2 [21], Grounded 3D-LLM [14], LL3DA [11],LEO [22], LLaVA-3D [66], and Video-3D LLM [61]. For 3D video object detection, we compare our method with baseline (Qwen2.5-VL-3B [2]) that does not incorporate 3D geometry information. 4.1.2 Results on 3D Visual Grounding We follow SPAR [57] to perform the refinement process by matching the predicted bounding box with the pre-detected object proposals. 5 Acc@0. Acc@0.5 Model Model ScanRefer [7] MVT [23] ViL3DRel [10] 3D-LLM [20] Chat-3D v2 [21] Grounded 3D-LLM [14] ChatScene [21] LLaVA-3D [66] Video-3D LLM [61] SPAR [57] VG LLM (Ours) 3D Scene Input 37.3 40.8 47.9 30.3 35.9 47.9 55.5 54.1 58.1 24.3 33.3 37.7 - 30.4 44.1 50.2 42.4 51.7 48.8 (31.9) 51.0 (34.1) 43.1 (12.4) 45.1 (10.1) Scan2Cap [8] 3DJCG [4] D3Net [9] Vote2Cap-DETR [12] LL3DA [11] Chat-3D-v2 [21] Grounded 3D-LLM [14] LEO [22] Chat-Scene [21] LLaVA-3D [66] Video-3D LLM [61] VG LLM (Ours) 3D Scene Input C@0.5 B-4@0.5 M@0.5 R@0.5 39.1 49.5 62.6 61.8 65.2 63.9 70.2 72.4 77.1 79.2 80.0 74. 23.3 51.0 35.7 34.5 36.8 31.8 35.0 38.2 36.5 41.1 40.2 38.7 22.0 24.2 25.7 26.2 26.0 27.9 - 30.2 28.5 28.0 44.8 50.8 53.9 54.4 55. 58.1 - 63.4 61.7 61.0 Table 1: The quantitative results on ScanRefer. The content in () indicates results without proposal refinement. Table 2: The performance on Scan2Cap. VG LLM does not use the ground truth 3D scene information. Figure 2: Visualization of 3D visual grounding results. The ground truth and prediction are masked in blue and green, respectively. The predicted boxes are directly generated by our model without the refinement process. VG LLM shows excellent visual grounding performance without 3D information. We show the performance in ScanRefer in Table 1. Our model obtains an Accuracy of 51.0 at an IoU of 0.25, outperforming SPARs accuracy of 48.8 by 2.2%. Even without the proposal refinement process, our model also delivers an Accuracy of 34.1, which surpasses SPARs 31.9 by 2.2%. This demonstrates that 3D visual grounding can be effectively approached in video grounding manner. Visualization for 3D visual grounding. Figure 2 presents visualization of the 3D visual grounding results. To visualize the predicted and ground truth bounding boxes, we transform their bounding boxes to the respective image coordinate system. Our method effectively handles spatial relationships between objects (e.g., away, opposite, next to), accurately identifying the corresponding frame index and generating the 3D-oriented bounding box. 4.1.3 Results on 3D Dense Captioning Pure-vision solution works surprisingly well for 3D dense captioning. We present the performance on Scan2Cap in Table 2. Despite not utilizing 3D camera parameters or explicit depth information, our model achieves competitive results, surpassing the previous state-of-the-art LEO by 1.7 in CIDEr. This indicates that the 3D geometric information implicitly encoded within the visual features extracted by VGGT is sufficiently informative for indoor scene understanding, indicating the potential of pure-vision solutions for 3D dense captioning. 4.1.4 Results on 3D Video Object Detection As shown in Table 3, we follow the monocular detection of EmbodiedScan [46] to evaluate the performance for categories that are common in daily life. For evaluation, we report the per-class F1 score as well as the average precision, recall, and F1 score. VG LLM significantly outperforms the baselines. Qwen2.5-VL [2], when fine-tuned on detection data, achieves decent performance on common object categories. Furthermore, incorporating visual geometry [43] yields substantial improvement across all evaluation metrics. Notably, the average recall in the 4-frame setting exhibits dramatic increase of 14.1, rising from 32.1 to 46.2. This improvement can be attributed to the models enhanced ability to understand the egocentric-allocentric Model Frames chair cabinet table bin couch bed bathtub toilet 20 Common Classes AP25 AR25 Qwen2.5-VL-3B + Visual Geometry (VG LLM) Improvement Qwen2.5-VL-3B + Visual Geometry (VG LLM) Improvement 4 33.3 48.9 +15.6 29.6 42.8 +13.2 8.7 14.7 +6.0 8.7 14.4 +5.7 17.6 27.2 56.9 27.2 33.7 64.0 +6.5 +9.6 +8.3 +7. 39.0 47.3 15.4 22.8 53.5 22.1 63.1 32.5 +10.4 +7.4 +17.5 +9.6 29.6 47.1 41.0 42.5 +1.5 41.3 48.6 +7. 58.5 66.7 +8.2 32.1 19.4 22.8 46.2 +3.4 +14.1 27.1 17.0 52.2 46.4 22.6 63.1 +10.9 +5.6 +19.3 23.6 29.3 +5.7 20.5 28.8 +8.3 Table 3: The results on 3D video detection. The reported object categories follow the monocular detection in EmbodiedScan, and we report the average F1 score per category. Figure 3: Visualization of 3D video object detection results. transformation between frames through the utilization of visual geometry. Consequently, the model can retrieve more objects that were not visible in the first frame. Figure 2 shows the visualization on 3D visual grounding. Integrating 3D geometry enhances the models robustness to the number of frames. Table 3 shows that our method, although trained on 4-frame sequences, maintains strong performance when evaluated on longer sequences during inference. In contrast, the baselines performance drops noticeably, highlighting the greater robustness of our approach to variations in the number of frames. 4.2 Spatial Reasoning Benchmarks Model Proprietary Models (API) GPT-4o Gemini-1.5-Flash Gemini-1.5-Pro Open-source Models Avg. 34.0 42.1 45.4 InternVL2-8B 34.6 InternVL2-40B 36.0 LongVILA-8B 21.6 VILA-1.5-40B 31.2 LongVA-7B 29.2 LLaVA-NeXT-Video-72B 40.9 LLaVA-OneVision-72B 40.2 Spatial-Enhanced Models SAT-LLaVA-Video-7B - VG LLM-4B (Ours) SPAR-8B 41.1 46.1 Obj.Count Abs.Dist. Obj.Size Numerical Answer RoomSize Rel.Dist. Rel.Dir. RoutePlan Appr.Order Multiple-Choice Answer 46.2 49.8 56. 23.1 34.9 29.1 22.4 38.0 48.9 43.5 - - 66.4 5.3 30.8 30.9 28.7 26.9 9.1 24.8 16.6 22.8 23.9 - - 36.6 43.8 53.5 64. 48.2 46.5 16.7 48.7 38.9 57.4 57.6 - - 55.2 38.2 54.4 43.6 39.8 31.8 0.0 22.7 22.2 35.3 37.5 47.3 - 56.3 37.0 37.7 51. 36.7 42.1 29.6 40.5 33.1 42.4 42.5 41.1 - 40.8 41.3 41.0 46.3 30.7 32.2 30.7 25.7 43.3 36.7 39.9 37.1 - 43.4 31.5 31.5 36. 29.9 34.0 32.5 31.5 25.4 35.0 32.5 36.1 - 30.4 28.5 37.8 34.6 39.6 39.6 25.5 32.9 15.7 48.6 44.6 40.4 - 39.5 Table 4: The comparison with state-of-the-art models on VSI-Bench. Spatial-Enhanced Models are models that are specialized for spatial reasoning. 4.2.1 Setting Datasets and Benchmarks. To fully leverage the 3D knowledge inherent in the 3D vision geometry encoder, we train our model on dataset sampled from SPAR-7M [57] and the LLaVA-Hound split of the LLaVA-Video-178K [60]. In our work, we sample only 234K and 63K data points from 7 SPAR-7M and the LLaVA-Hound split of LLaVA-Video-178K, respectively, which constitute 3% and 25% of the original datasets. We first evaluate our method on video spatial reasoning benchmark, VSI-Bench [51], which evaluates the egocentric-allocentric transformation and relational reasoning capabilities of MLLMs. Then we test our model on two image spatial-related benchmarks i.e., CV-Bench [41] and BLINK (spatial subset) [19]. CV-Bench [41] assesses 2D understanding through spatial relationships and object counting, while its 3D evaluation focuses on depth ordering and relative distance perception. The spatial subset of BLINK [19] includes three tasks requiring 3D spatial perception abilities, namely relative depth, spatial reasoning, and multi-view reasoning. Comparison baselines. We compare our model with state-of-the-art proprietary and open-source MLLMs (e.g., GPT-4o, Gemini-1.5-Pro, LLaVA-NeXT-Video) [1, 24, 26, 27, 33, 35, 40, 41, 47, 60]. We also include two spatial-enhanced MLLMs, SAT-LLaVA-Video-7B [38] and SPAR-8B [57], which are fine-tuned on datasets targeted for spatial reasoning. 4.2.2 Results on Spatial Reasoning Tasks Model 2D (%) 3D. (%) Avg. (%) Proprietary Models (API) GPT-4V [35] GPT-4o [24] Gemini-1.5-Flash [40] Gemini-1.5-Pro [40] Open-source Models Mini-Gemini-HD-34B [27] LLaVA-NeXT-34B [26] Cambrian-1-34B [41] SAT-LLaVA-Video-7B [38] SPAR-8B [57] VG LLM-4B (Ours) 64.3 74.8 70.9 77.1 71.5 73.0 74.0 73.0 72.3 72. 73.8 83.0 71.8 77.6 79.2 74.8 79.7 83.8 89.1 91.3 69.1 78.9 71.4 77.3 75.4 73.9 76.9 78.4 80.7 82.1 Model Depth (%) Spatial (%) Multi-View (%) Avg. (%) Proprietary Models (API) GPT-4V [35] GPT-4o [24] Claude-3-OPUS [1] Open-source Models CogVLM [47] LLaVA-v1.5-13B-xtuner [33] Yi-VL-34B [53] LLaVA-v1.6-34B [33] SAT-LLaVA-Video-7B [38] VG LLM-4B (Ours) 60.0 74.2 47. 52.4 54.0 50.0 67.7 66.1 79.8 72.7 69.2 58.0 63.6 69.9 71.3 74.8 73.4 71.3 55.6 59.4 56.4 54.1 44.4 44.4 62.4 48.1 54.1 62.8 67.6 54. 56.7 56.1 55.2 68.3 62.6 68.4 Table 5: The comparison with state-ofthe-art methods on CV-Bench. Table 6: The comparison with the SOTA methods on the validation split of BLINK (spatial subsets). Our method achieves state-of-the-art performance on VSI-Bench. We present the detailed comparison with the top-tier models in Table 4. As shown in the table, our model achieves stateof-the-art performance with an average score of 46.1 among all compared models, even surpassing the best-performing proprietary model, Gemini-1.5 Pro. Remarkably, our model secures the highest score in counting and room size estimation, reaching 66.4% and 56.3%, respectively. This highlights the models strong capability in understanding and interpreting scene configurations. Our model generalizes well across distinct data sources. To investigate the generalization ability of our method, we also evaluate our approach on the spatial reasoning benchmarks with different data sources. CV-Bench is constructed by repurposing the traditional CV datasets (i.e., ADE20K, COCO, and Omni3D) to vision-centric MLLM benchmark. As illustrated in Table 5, our method achieves the highest accuracy on 3D tasks at 91.3%. BLINK (spatial subset) comprises wide range of image sources, including Wild6D [54], VSR [32], and Depth in the Wild [13]. Moreover, we present the results on BLINK in Table 6. Our 4B model achieves the highest score on relative depth at 79.8, surpassing GPT-4o by 5.6. These results demonstrate our model can also generalize well across the spatial reasoning benchmarks with out-of-the-domain data sources. small amount of training data suffices for strong performance. Unlike SPAR [57], which relies on massive 2 million samples for pre-training, our approach demonstrates that leveraging just small amount of data (3% of SPAR-7M) can achieve strong performance by effectively capturing the embedded visual geometry. This finding highlights the importance of explicit modeling 3D visual geometry in MLLMs. Furthermore, reducing the amount of fine-tuning data helps mitigate distribution drift from the original MLLM, thereby enhancing its generalization capability. 4.2.3 Results on Generic Multimodal Benchmarks Video-MMEw/o sub. [17] Video-MMEw sub. [17] BLINK [19] TempCompassMC [34] NextQAMC [49] Qwen2.5-VL-3B VG LLM-4B 60.1 57.3 60.9 59.8 46.6 50.4 62.2 63. 77.3 74.2 Table 7: Comparison of model performance on generic multimodal benchmarks. Enhancing spatial understanding incurs negligible loss on general multimodal performance. Our model integrates visual geometric information and is fine-tuned on spatial reasoning datasets to bolster its spatial understanding capabilities. As presented in Table 7, these enhancements slightly 8 compromise VG LLMs performance on general multimodal benchmarks when compared to the baseline model, Qwen2.5-VL-3B. VG LLM even achieves improvements on BLNK (+4.0) and TempCompassMC (+0.6), highlighting that augmenting spatial reasoning can not only preserve but also enhance performance on specific multimodal tasks. 4.3 Ablation Study Spatial Signal None Pred Camera Info. Pred Depth Info. Pred (Depth + Camera) Info. Pred Point Info. 3D Visual Geometry ScanRefer Scan2Cap Acc@0.25 47.4 (29.3) 49.1 (32.0) 49.2 (32.1) 49.2 (31.6) 48.3 (31.0) 51.0 (34.1) Acc@0.5 42.0 (7.8) 43.6 (9.3) 43.6 (10.1) 43.6 (9.5) 42.8 (9.2) 45.1 (10.1) CIDEr@0.5 BLEU-4@0.5 59.8 62.5 62.0 61.3 65. 74.1 36.8 37.6 37.4 37.3 36.7 38.7 3D Video Detection AR25 32.1 AP25 19. F125 23.6 20.2 19.2 18.9 19.9 22.8 34.0 32.6 33.5 34.6 46.2 24.7 23.6 23.5 24. 29.3 Table 8: Ablation study of the effects of 3D visual geometry modeling. All models are fine-tuned using the same training data. To demonstrate that the improvement in spatial reasoning and 3D scene understanding in MLLMs is genuinely driven by visual geometry modeling, we conducted an ablation study, as shown in Table 8. Besides the visual geometry feature adopted in our model, VGGT can also directly predict the camera poses (Pred Camera Info.), the depth maps (Pred Depth Info.), and the point maps (Pred Point Info.) of each image frame. For these predicted spatial signals, we adopt two-layer MLP to transform them and add to the corresponding 2D visual token. All models are trained and evaluated following the datasets and benchmarks in Sec. 4.1. Incorporating extra spatial signals enhances 3D scene understanding. After fine-tuning on 3D downstream tasks, the baseline model Qwen2.5-VL shows enhanced 3D scene understanding. However, it continues to struggle with accurately localizing and describing 3D objects, as well as learning spatial transformations across different frames. As illustrated in Table 8, the integration of additional spatial signals significantly boosts performance across various tasks. Notably, even the inclusion of simple predicted camera poses elevates the Acc@0.25 score on ScanRefer from 29.3 to 32.0 and the CIDEr@0.5 score on Scan2Cap from 59.8 to 62.5. These results indicate that current MLLMs still lack adequate spatial modeling in both data curation and architectural design. Visual geometry features are more helpful than predicted spatial information. Although the results predicted directly by VGGT can enhance the spatial understanding ability of Qwen2.5-VL, these Camera/Depth/Point signals are relatively sparse and may contain errors. In contrast, directly using the visual geometry features provided by VGGT not only incorporates these spatial signals simultaneously but also reduces the impact of noise on performance, thereby achieving better results. VSI-Bench CV-Bench Abs. Dist. Room Size Obj. Size Route Plan Depth Distance Qwen2.5-VL-3B + finetune + Visual Geometry (VG LLM) 19.7 31.9 36.6 25.4 47.8 56.3 17.2 54.3 55.2 29.4 27.8 30. 74.5 85.3 93.7 66.0 87.8 89.8 Table 9: Ablation study on spatial reasoning tasks. Visual geometry improves spatial reasoning, specifically in measurement estimation. We also ablate the effect of finetuning and visual geometry in Table 9. The figure illustrates that fine-tuning on our mixed dataset leads to substantial performance gain in measurement estimation (e.g., absolute distance, room size). After incorporating the visual geometry into the model architecture, it further improves absolute distance and room size by 4.7% and 8.5%, respectively."
        },
        {
            "title": "5 Conclusion",
            "content": "We present novel framework to enhance MLLMs 3D spatial understanding capability, which incorporates 3D visual geometry encoder to provide latent 3D geometric information given only video inputs. While being straightforward, our extensive experiments show that our 4B model can outperform larger spatial-enhanced models on various 3D scene understanding tasks and spatial reasoning benchmarks, without relying on any explicit 3D scene input."
        },
        {
            "title": "References",
            "content": "[1] Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family, 2024. 8 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 1, 2, 3, 5, 6 [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. RT-2: vision-languageaction models transfer web knowledge to robotic control. In CoRL, 2023. 1 [4] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: unified framework for joint dense captioning and visual grounding on 3d point clouds. In CVPR, 2022. 5, [5] Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. arXiv:2406.13642, 2024. 2 [6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Dorsa Sadigh, Leonidas J. Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 2 [7] Dave Zhenyu Chen, Angel X. Chang, and Matthias Nießner. Scanrefer: 3d object localization in RGB-D scans using natural language. In ECCV, 2020. 2, 5, 6 [8] Dave Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X. Chang. Scan2cap: Contextaware dense captioning in RGB-D scans. In CVPR, 2021. 2, 5, [9] Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and Angel X. Chang. D3net: unified speaker-listener architecture for 3d dense captioning and visual grounding. In ECCV, 2022. 5, 6 [10] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. In NeurIPS, 2022. 5, 6 [11] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. LL3DA: visual interactive instruction tuning for omni-3d understanding, reasoning, and planning. In CVPR, 2024. 1, 2, 5, 6 [12] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with vote2cap-detr. In CVPR, 2023. 5, 6 [13] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in the wild. In NeurIPS, 2016. 8 [14] Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Ruiyuan Lyu, Runsen Xu, Dahua Lin, and Jiangmiao Pang. Grounded 3d-llm with referent tokens. arXiv:2405.10370, 2024. 2, 5, 6 [15] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. 2 [16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas A. Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 4 [17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv:2405.21075, 2024. 8, 1 [18] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual reasoning. In WACV, 2025. 2 10 [19] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. BLINK: multimodal large language models can see but not perceive. In ECCV, 2024. 2, 8, 1 [20] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. In NeurIPS, 2023. 1, 2, 6 [21] Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. Chat-scene: Bridging 3d scene and large language models with object identifiers. In NeurIPS, 2024. 1, 5, 6 [22] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. In ICML, 2024. 1, 5, [23] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view transformer for 3d visual grounding. In CVPR, 2022. 5, 6 [24] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv:2410.21276, 2024. 1, 2, 8 [25] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. In CoRL, 2024. 1 [26] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv:2408.03326, 2024. 1, 2, 8 [27] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814, 2024. [28] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In ECCV, 2022. 1 [29] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: on pre-training for visual language models. In CVPR, 2024. 1, 2 [30] Benlin Liu, Yuhao Dong, Yiqin Wang, Zixian Ma, Yansong Tang, Luming Tang, Yongming Rao, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondences boost spatial-temporal reasoning in multimodal language model, 2024. 1 [31] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondence elicit 3d spacetime understanding in multimodal language model. arXiv:2408.00754, 2024. 2 [32] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. 8 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 2, 8 [34] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? In ACL, 2024. 8, 1 [35] OpenAI. GPT-4 technical report. arXiv:2303.08774, 2023. 8 [36] Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3d scenes from videos with vision-language models. arXiv:2501.01428, 2025. 1, 11 [37] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv:2501.15830, 2025. 1 [38] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Dynamic spatial aptitude training for multimodal language models, 2024. 2, 8 [39] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander G. Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv:2412.06974, 2024. 1, 2, 3 [40] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024. 1, 2, 8 [41] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 1, 2, [42] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness, 2025. 1 [43] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotný. VGGT: visual geometry grounded transformer. arXiv:2503.11651, 2025. 1, 2, 3, 4, 5, 6 [44] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv:2409.12191, 2024. 1, 2 [45] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jérôme Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 2, [46] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, et al. Embodiedscan: holistic multi-modal 3d perception suite towards embodied AI. In CVPR, 2024. 2, 4, 5, 6, 1 [47] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. In NeurIPS, 2024. 8 [48] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In ICLR, 2024. 1 [49] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In CVPR, 2021. 8 [50] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. In ECCV, 2024. 2 [51] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv:2412.14171, 2024. 1, 2, 8 [52] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, 2023. 4 [53] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01.ai. arXiv:2403.04652, 2024. 8 [54] Yanjie Ze and Xiaolong Wang. Category-level 6d object pose estimation in the wild: semi-supervised learning approach and new dataset. In NeurIPS, 2022. 8 [55] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv:2309.10313, 2023. 2 [56] Huanyu Zhang, Chengzu Li, Wenshan Wu, Shaoguang Mao, Yan xia, Ivan Vulic, Zhang Zhang, Liang Wang, Tieniu Tan, and Furu Wei. call for new recipes to enhance spatial reasoning in mllms, 2025. 4 [57] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, YuJie Yuan, Xinyue Cai, Guowei Huang, et al. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv:2503.22976, 2025. 2, 4, 5, 6, 7, 8 [58] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang. Navid: Video-based VLM plans the next step for vision-and-language navigation. In RSS, 2024. [59] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. 1 [60] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv:2410.02713, 2024. 5, 7, 8 [61] Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d LLM: learning position-aware video representation for 3d scene understanding. In CVPR, 2025. 1, 2, 4, 5, 6 [62] Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. Towards learning generalist model for embodied navigation. In CVPR, 2024. 1 [63] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: large photo-realistic dataset for structured 3d modeling. In ECCV, 2020. 4 [64] Gengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation with large language models. In AAAI, 2024. 1 [65] Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Eric Xin Wang, and Achuta Kadambi. Vlm4d: Towards spatiotemporal awareness in vision language models. 2025. [66] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv:2409.18125, 2024. 1, 2, 4, 5,"
        },
        {
            "title": "A Experimental Setting and Details",
            "content": "A.1 Training Details. Our model is built upon Qwen2.5-VL-3B [2], with 3D geometry encoder initialized from VGGT-1B [43]. During training, the MLLMs visual encoder, the integrated 3D geometry encoder, and the multimodal connector are frozen, while the MLLM backbone remains unfrozen with learning rate of 5e-6. All experiments were conducted on 8 H100 80G GPUs. The training took 8 hours for 3D scene understanding and 12 hours for spatial reasoning instruction tuning. A.2 Evaluation Details. We utilize the LMMs-Eval [59] to evaluate our method. LMMs-Eval offers flexible functionalities that support the customization of specific tasks and model interfaces, thereby standardizing the evaluation process of MLLMs. For inference, we employed greedy sampling to generate model outputs. Spatial Reasoning and General Multi-modal Benchmarks. For benchmarks already integrated into LMMs-Eval, e.g., VSI-Bench [51], Video-MME [17], and TempCompass [34], we utilize their original default configurations. For benchmarks not yet implemented in LMMs-Eval, i.e., BLINK [19] and CV-Bench [41], we customize the task configurations and metrics according to their official implementations. For each input video in video-based benchmarks, we uniformly sample 32 frames from the entire duration. 3D Visual Grounding. For 3D visual grounding, the model is tasked with locating the frame index where the target object appears and the oriented bounding box within its coordinate system. Given predicted frame index and bounding box, we first transform the bounding box to the world coordinate system. Then, the ground truth bounding box is extended to an oriented bounding box by appending (0, 0, 0) to represent its orientation. Finally, we calculate the Intersection-over-Union (IoU) between the predicted and ground truth bounding boxes. In the proposal refinement process, we utilize the Mask3D-detected object proposals extracted by LEO [22], and select the proposal that has the highest IoU with the predicted box. 3D Dense Captioning. For 3D dense captioning, we follow previous work [66, 61] to decompose the task into two stages, i.e., detecting all object proposals and generating object descriptions based on object center coordinates. Specifically, we first leverage the Mask3D-detected object proposals extracted from LEO [22] to obtain the boxs center coordinates within the coordinate system of the reference frame. This can be achieved by multiplying the coordinates by the extrinsic matrix [46]. Then, the model is asked to generate descriptions based on the coordinates through greedy sampling. Lastly, we calculate the CIDEr, BLEU, Rouge, and METEOR scores utilizing the COCO caption evaluation toolkit3. 3D Video Object Detection. For each predicted bounding box, we find its best match among the unused ground truth boxes of the same category through greedy matching. Specifically, we calculate the IoU with all unused ground truth boxes of that category and select the one with the highest IoU if it exceeds threshold (e.g., 0.25). Once ground truth is matched, it is marked as used and cannot be matched again. Finally, for each category, the precision, recall, and F1 score are calculated based on the number of true positives, false positives, and false negatives. Additionally, we provide the prompt for 3D scene understanding tasks in Table 1."
        },
        {
            "title": "B Data preparation",
            "content": "All datasets used in this research are publicly available, and we will provide the details for the data preparation in this section. SPAR-7M. We follow the official codebase4 to mix data and draw visual markers on the input images. Since the navigation type contains images of varying lengths, we discard annotations of this type for simplicity. 3https://github.com/tylin/coco-caption 4https://github.com/fudan-zvg/spar 1 LLaVA-Video-178K (LLaVA-Hound Split). For each input video, we sample frames with sampling rate of 2 FPS, while constraining the total frame count between 4 and 8 through adaptive sampling. 3D Scene Understanding. For ScanRefer, we uniformly sample 24 frames for each scene, while for Scan2Cap, we uniformly sample 16 frames per scene. For 3D video object detection, each entry in the training set contains 4 consecutive frames sampled at 1 FPS. 3D Visual Grounding XUser: Frame-0: <image>Frame-1:<image>Frame-2: <image>Frame-3: <image> Localize the first clear frame in the video showing the object described in the text. Text: There is beige wooden bookshelf. placed next to another bookshelf. Output JSON dictionary with the frame index in \"frame\" and its 3D bounding box in \"box_3d\" in the frames coordinates. XAgent: ```json {\"frame\": 12, \"bbox_3d\": [-0.63, -0.83, 2.43, 3.0, 0.59, 2.35, -2.32, 1.18, 3.05]} ``` 3D Dense Captioning XUser: <image><image><image><image> Carefully watch the video and describe the object located at [-0.89, -0.74, 2.45] in detail. XAgent: white cabinet in the corner of the room. in the direction from the door and from the inside. it will be on the left, there is small brown table on the left side of the cabinet and smaller table on the right side of the cabinet. 3D Video Object Detection XUser: <image><image><image><image> Detect the 3D bounding boxes in the camera coordinate system of the first frame. Output json list where each entry contains the object name in \"label\" and its 3D bounding box in \"box_3d\". The 3D bounding box format should be [x_center, y_center, z_center, x_size, y_size, z_size, yaw, pitch, rolll]. XAgent: ```json [ {\"label\": \"bag\", \"bbox_3d\": [0.0, -0.3, 1.0, 0.26, 0.26, 0.15, 1.67, 0.96, -2.98]}, ] ``` Table 1: The prompt for 3D scene understanding tasks. For 3D visual grounding and 3D video object detection, the output should be in JSON format, while for 3D dense captioning, it should be natural language description. Detailed Results on 3D Video Object Detection Detailed quantitative results for our 3D video object detection are presented in Table 2. As illustrated in the table, our method significantly outperforms the baseline that does not utilize visual geometry across most categories. Nevertheless, we observe that detecting small objects such as pillows, lamps, and backpacks remains challenging."
        },
        {
            "title": "D More Visualization",
            "content": "Figure 1 shows more visualization results for 3D visual grounding. The first three examples are positive cases where our model successfully locates the 3D bounding boxes for different categories like chair, cabinet, and document organizer. While the model correctly identifies these objects, we notice that it still struggles with accurately predicting their orientation. The last two examples are negative cases. While the predictions might appear correct in the 2D projection, the models inaccurate depth estimation causes some discrepancies in 3D space. 2 Qwen2.5-VL-3B + Visual Geometry (VG LLM) 4 Frames Qwen2.5-VL-3B + Visual Geometry (VG LLM) Qwen2.5-VL-3B + Visual Geometry (VG LLM) 6 Frames Qwen2.5-VL-3B + Visual Geometry (VG LLM) 0 0 8.7 14.7 6.6 14.9 bed 56.9 64. desk 23.0 26.2 pillow cabinet chair 33.3 48.9 stand 16.7 34.5 table lamp 2.7 27.2 4.9 33.7 couch 39.0 47.3 bathtub ottoman dresser bin toilet refrigerator stove microwave monitor computer 24.6 29.7 couch 29.6 47.1 bathtub ottoman dresser bin toilet refrigerator stove microwave monitor computer 22.5 42. 17.6 58.5 27.2 66.7 table lamp 3.5 22.1 5.4 32.5 7.8 14.8 backpack 7.4 12.0 backpack 4.9 6.0 9.4 15.4 stand 14.0 28.0 41.0 42.5 chair 29.6 42.8 55.3 50.6 desk 24.2 27. 17.4 24.9 bed 53.5 63.1 15.4 52.2 22.8 63.1 pillow cabinet 8.7 14.4 5.7 11.4 2.4 10. 41.3 48.6 13.1 22.7 45.0 46.8 17.4 25.9 21.2 18.7 2.6 9. 0 0 Table 2: The detailed results on 3D video object detection. We report the F1 score for all categories. We provide more visualization results on 3D video object detection in Figure 2. From this figure, we observe that while the baseline performs decently in detecting objects in given video, incorporating 3D geometry significantly improves both detection precision and recall. For instance, in the first example, the baselines prediction for the desk mismatches the ground truth, whereas our approach improves the prediction box of the desk. 3 Figure 1: Visualization of 3D visual grounding. The first three examples are positive, whereas the last two are negative cases. Figure 2: Visualization of 3D video object detection results."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong"
    ]
}