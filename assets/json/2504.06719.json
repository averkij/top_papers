{
    "paper_title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding",
    "authors": [
        "Pedro Hermosilla",
        "Christian Stippel",
        "Leon Sick"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 9 1 7 6 0 . 4 0 5 2 : r Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding"
        },
        {
            "title": "Leon Sick\nUlm University",
            "content": "Figure 1. Self-Supervised Feature Visualization using PCA. We reduce the point features obtained with our self-supervised model to three dimensions using PCA and visualize them as colors. Features learned by our model are semantic-aware, which is visible from the color separation: Similar objects result in similar features, such as the sofas in the first figure or the chairs in the last one, while different objects result in different features, such as the counter and the tables in the second image or the crib and the curtains in the third one."
        },
        {
            "title": "Abstract",
            "content": "Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as weight initialization step for task-specific fine-tuning, limiting their utility for generalpurpose feature extraction. This paper addresses this shortcoming by proposing robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multiresolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in linear probing setup. In particular, our model is trained natively in 3D with novel self-supervised approach based on Masked Scene Modeling objective, which reconstructs deep features of masked patches in bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by large margin. The model and training code can be found at our Github repository. 1. Introduction 2D self-supervised models, such as DINOv2 [28], have become an integral part of modern computer vision. These models are typically pre-trained on large unlabeled datasets, providing task-agnostic features that can be used off-theshelf to solve any computer vision task without the need for fine-tuning, making them particularly useful in scenarios with limited data [13]. In contrast to 2D computer vision, the field of 3D scene understanding lacks comparable models. Instead, it relies on consolidating features from 2D foundation models into 3D [26, 30, 40] or using them in 2D-3D knowledge distillation setup [8, 55, 59]. Although this is still an emerging field, self-supervised methods for processing 3D scenes have started to gain traction, with several approaches emerging in recent years [19, 42, 43, 45, 46, 50]. Some of these methods incorporate Masked Image Modelling (MIM) objectives [17, 28, 48, 58] in their 3D scene-based frameworks [43, 50], where the model is tasked with reconstructing the input scene from partial view. In the 2D domain, such learning objectives have been shown to lead to semantically rich features better suited for dense prediction tasks [28], achieving unprecedented performance for off-the-shelf feature evaluations. Unfortunately, self-supervised learning on 3D scenes has so far failed to exhibit such semantic properties off-theshelf, and it is used as weight initialization step before fine-tuning the model in downstream task. We believe this is due to two main limitations: (i) The lack of systematic protocol specific to 3D scene understanding to evaluate the representations learned by such models, and (ii) the lack of an effective 3D-scene specific masked prediction objective that takes into account the hierarchical nature of these models. In this paper, we aim to address these limitations: (i) First, we advocate that, to advance the field, we necessitate an evaluation protocol to directly evaluate the quality of the representations learned by self-supervised methods tailored explicitly to 3D scenes. Models designed for 3D scene understanding have hierarchical nature, usually following UNet [35] design. Naively using the output of the last layer of such hierarchical models for off-theshelf feature evaluation might not reflect the underlying semantic capabilities of the self-supervised model. In supervised setup, such models discard unnecessary information for the downstream task during decoding, whereas in self-supervised setup, this information may be relevant for producing task-agnostic features. To address this, we use tri-linear interpolation to upsample the feature maps of each decoder level and combine them to create final taskagnostic feature map. The resulting set of features retains the hierarchical information otherwise lost during decoding and can then be used for off-the-shelf feature evaluation in linear probing or nearest-neighbor setup. This evaluation reflects the effectiveness of representations learned by selfsupervised models better than fine-tuning protocol does, since the quality of the features is not masked by further optimization on the downstream task. In pilot study, we demonstrate that our hierarchical evaluation approach more effectively reveals the off-the-shelf feature capabilities of self-supervised methods. Furthermore, this study reveals significant performance gap between supervised and selfsupervised training, highlighting the necessity for framework better suited for 3D scenes. (ii) To address this gap, we propose hierarchical selfsupervised framework based on the MIM objective specifically designed for models that process 3D scenes. We argue that the failure of existing MIM approaches to learn semantically relevant features from self-supervised learning alone is rooted in their design choices: Some methods mask the input features of the 3D points [43], letting the model infer those from the geometry, simplifying the masking objective. Additionally, some methods use the reconstruction of input features [43, 50] as training objective instead of deep features, which leads to reduced semantic information [2]. Lastly, existing methods do not consider the hierarchical nature of their models when designing the reconstruction loss [43]. With our proposed approach, Masked Scene Modeling, we aim to overcome these limitations by making several crucial design choices to better learn the semantic properties of the 3D scene: We perform bottom-up hierarchical masking approach, where the encoder receives masked sparse voxelization of the scene. During decoding, the masked patches are included, and the model reconstructs the deep features of these patches obtained from teacher model. This design allows for hierarchical reconstruction of the scene without information leakage from the geometric cues present in sparse representations. Moreover, inferring deep features leads to fast learning process with richer semantic features thanks to the use of abstract prediction targets [2]. Our extensive evaluation demonstrates that our self-supervised features can be used off-the-shelf to solve several tasks, achieving competitive performance, for the first time, when compared to supervised methods and significantly outperforming other existing self-supervised methods for 3D scenes. 2. Related Work Self-supervised methods for 3D scene understanding. In 3D self-supervised learning, there are two main lines of work: methods focused on pretext tasks designed for shapes representing single objects [14, 29, 38, 51, 52, 54], and methods with self-supervised objectives designed for large 3D scenes composed of multiple objects [7, 19, 42, 43, 47]. While single object pretext objectives are able to perform well on object-centric tasks such as classification or segmentation [29, 51, 52, 54], as shown experimentally by Xie et al. [47], they fall behind on complex 3D scene understanding tasks. Xie et al. [47] proposed one of the first scene-centric self-supervised methods, which employed contrastive learning objective [6] at the point level. This work was later improved by Hou et al. [19] by partitioning the space around the points and using those to select meaningful negative examples for contrastive learning loss. Chen et al. [7] further extended the same idea to work with object trajectories within scene. Contrastive learning was also used by Zhang et al. [57] and Huang et al. [20] at scene level using momentum encoder as target. Recently, Wang et al. [42] have also suggested using an over-segmentation step to group points and prototype clustering step to improve contrastive learning. However, in recent works, following the trends in 2D vision, the paradigm has shifted, and several methods have suggested using MIM as pretext task [43, 50]. Wu et al. [43] combines per-point contrastive objective with color and normal reconstruction, while Xu et al. [50] aims at reconstructing neighboring point coordi2 derstanding, hierarchical decoder reduces the feature dimensionality at each level while increasing the spatial resolution. the final layer In supervised learning, is composed of small number of features that coninformation to solve the downstream tain the relevant task since the model can learn them from deeper levels and discard unnecessary information along the process. In self-supervised learning, on the other hand, where the model should generate general features to solve various tasks, evaluating only the features of the last layer in the decoder might limit the information available and might discard valuable information within deeper levels. Therefore, in this paper, we suggest using concatenation of the output features of each level in hierarchical thus obtaindecoder, ing features with information at different scales. In particular, we propose to use trilinear interpolation, as shown in Figure 3, to obtain distinct features that better reflect the semantic capabilities for each point in space. These features can then be used off-the-shelf to solve downstream tasks. Figure 3. Hierarchical features Pilot study. We conducted pilot study to validate the assumption that our hierarchical features are better suited for evaluating self-supervised models. We collected pretrained models from recent self-supervised methods [19, 43, 45] that employed the same sparse convolution architecture and evaluated their linear probing performance on the downstream task of semantic segmentation on the ScanNet dataset [11]. In particular, we compare two feature extraction approaches: Naively using only features from the last layer of the decoder or extracting hierarchical features using tri-linear interpolation. Figure 2 shows that only using the features of the last layer does not fully capture the semantic capabilities of the models, resulting in poor segmentation performance. However, when the hierarchical features of all layers are used for linear probing, the models ability to produce semantic features is much better captured, since the performance significantly increases. From this study, we arrived at two main conclusions. First, we confirmed our assumption that deeper layers of self-supervised models still contain relevant information lost during hierarchical decoding and can assist in solving downstream task. Second, the gap between supervised and self-supervised models is still large, limiting the application of existing self-supervised strategies in practice. This highlights the necessity of new self-supervised approaches for 3D scene understanding that consider the hierarchical Figure 2. Pilot study. Our hierarchical features uncover better performance in all self-supervised models. Moreover, our study shows that existing approaches exhibit large performance gap between supervised and self-supervised training. nates at different scales. Despite these advancements, the reconstruction objectives in these works are limited to local features such as color or normals or to reconstructing neighboring point coordinates from sparse representations, which leads to features with lower semantic capabilities [2]. In contrast, this work advocates for deep abstract feature reconstruction of large masked areas in hierarchical bottomup manner, making the self-supervised model obtain semantically richer features at different scales. Validation of self-supervised models. Early work used pre-trained models as weight initialization for downstream tasks and measured the increase in performance [41]. Current state-of-the-art models for images, on the other hand, evaluate the feature space directly by freezing the pretrained model and using Nearest Neighbor (NN) [4, 28, 58] and linear probing protocols to solve classifications tasks [4, 6, 28, 58]. Although these protocols can evaluate the representation learning capabilities of the models better than simple fine-tuning, these are not commonly used in scenecentric 3D self-supervised learning. Methods that use selfsupervised learning from 3D scenes are usually evaluated by fine-tuning protocol on different 3D scene understanding tasks [19, 43, 47, 50]. Recent work [50] used linear probing setup in their evaluation; however, this approach was not been the primary metric used to measure performance and they only relied on features of the last layer of hierarchical models. In this work, we propose an evaluation protocol that uses hierarchical features and better reflects the quality of the learned representations. 3. Feature Evaluation Protocol In this section, first, we describe the proposed evaluation protocol designed to measure the quality of the representations learned by self-supervised hierarchical models, followed by pilot study on existing self-supervised methods. Hierarchical feature extraction. In UNet [35]-like architecture, like the one commonly used in 3D scene unnature of the models used. Based on these observations, in the following section, we describe our novel framework that uses self-supervision at different levels to better capture semantic relations in its features and can achieve supervisedlevel performance when hierarchical features are used offthe-shelf to solve various downstream tasks. 4. Masked Scene Modeling This section introduces our self-supervised framework, named Masked Scene Modeling, designed based on the findings of our pilot study in Section 3 and tailored explicitly for 3D scene understanding. First, Section 4.1 presents the main components of our self-supervised framework. Then, in Section 4.2 we describe in detail the hierarchical reconstruction objective at the core of our method. Figure 4 presents an illustration of the proposed framework. 4.1. Self-Supervised Training The main self-supervised objective of our framework is: From masked partial view of scene, the model is tasked to reconstruct deep features given by teacher model that has access to view of the whole scene. This objective not only forces the model to learn view-invariant features but also makes the model acquire deep understanding of the scenes composition. Our framework has five main components: view generation, feature encoding, feature decoding, reconstruction objective, and teacher model. View generation. Our framework receives as input 3D scene represented as pointcloud, P. First, two different data augmentations are applied to and the resulting scenes are then voxelized into V1 and V2, where only occupied voxels are stored in memory. Then, V1 and V2 are further cropped to obtain partial view of each scene, C1 and C2. From each crop, we then randomly mask certain areas resulting in two sets of voxels, unmasked, Cv1 and Cv2, and masked voxels, Cm1 and Cm2. These cropped views serve as input to our student model whilst the full voxelized views are given to the teacher model. Feature encoding. The proposed framework assumes hierarchical model composed of an encoder Ψ and decoder γ. Our framework, first, encodes the unmasked voxels Cv1 and Cv2 using Ψ, resulting in set of voxel features v1 = Ψ(Cv1) and e v2 = Ψ(Cv2). Feature decoding. Before decoding the features v1 and v2 of unmasked voxels with γ, we incorporate the features from masked voxels, m1 and m2, by assigning them learnable token, . This process results in the feature maps 1 = m2. The combined features are then processed with the decoder γ, which generm1 and 2 = v1 v2 ates the decoded features for each partial view, and 2 = γ(F 2 ). 1 = γ(F 1 ) Reconstruction objective. The self-supervised objective in our framework is the reconstruction of deep features of the masked voxels. Therefore, from the decoded features 1 and 2 , we select those belonging to masked voxels, m1 and m2, and process them with predictor model, Φ, implemented as small Multi Layer Perceptron (MLP). This results in the predicted features m1) and m2 = Φ(F m2). The target features used for supervision are obtained by processing the full scene views with teacher encoder and decoder, ˆF 2 = ˆγ( ˆΨ(V2)). This objective enforces the model to infer semantic knowledge of the full scene from only cropped and masked scene. In addition, to obtain view-invariant features, we perform cross-reconstruction between views, resulting in the following reconstruction loss: 1 = ˆγ( ˆΨ(V1)) and ˆF m1 = Φ(F = m1 ˆF m2 + m2 ˆF m1 (1) Teacher model. Following common practices of selfsupervised methods for images [16, 28, 58], we use as our teacher model with the same architecture as the student, but whose parameters are updated as the Exponential Moving Average (EMA) of the parameters of the student. The slow update of the teacher parameters reduces feature variation during training, making the self-distillation process more robust [16]. This makes the model learn rich semantic features and avoids common problem of self-supervised methods, mode collapse, where the model learns to predict always the same feature vector independently of the input. 4.2. Hierarchical Reconstruction One of the key insights from our pilot study in Section 3 is that all levels in the hierarchical model carry relevant information that can be used in downstream task. Therefore, in this paper, we suggest performing the reconstruction at each level in the hierarchical model to learn features at different scales. Figure 5 illustrates this process in detail. , ..., eL When our encoder processes scene, we receive feature map of the unmasked voxels for each level, = (F e1 ), where is the number of levels in the hierarchical encoder. These features are then combined with the features of masked voxels in each level by assigning different learnable token in each level, = (T 1, ..., L), resulting in the final encoder features, = (F e1, ..., eL). The decoder then starts processing the combined features of the last layer and generates the decoded features for the last layer, dL. From these features, we compute the predicted features for the masked voxels at this level using the predictor network, ΦL. Features dL are then upsampled 4 Figure 4. Overview. Our method receives as input 3D scene represented as pointcloud, (a). The scene is voxelized into two different views, (b), and then further cropped and masked, (c). The student model first encodes the cropped views and then adds the masked voxels with learnable token, (d). The decoder processes the cropped views and reconstructs deep features of the masked tokens, (e). The loss is computed in cross-view manner where the target features, (f), are obtained from teacher model updated with EMA. levels using features from neighboring voxels, making the self-supervised task easier and, therefore, as we show in our ablation studies, leading to lower performance. Masking. One of the key components in our framework is the masking of random voxels. In order to avoid information leakage between levels, we perform consistent masking between levels in the hierarchy, i.e. the same areas are masked at different voxel resolutions. Therefore, to mask voxels in the lowest levels in the hierarchy, where each voxel covers large area of the scene, we fix the mask patch size to the voxel resolution of this level, resulting in large independent masked areas. Recent work on MIM [49] has shown that this strategy leads to more stable performance for different masking ratios. 5. Main Results In this section, we present extensive experiments where we evaluate the representations learned by our self-supervised model on common 3D scene understanding tasks. For detailed description, additional experiments, and ablation studies, we refer the reader to the supplementary material. 5.1. Experimental Setup Baselines. We compare our self-supervised model to recent self-supervised models for 3D scenes trained exclusively with 3D data for which code and pre-trained weights were available at the time of the submission. These cover training objectives based on contrastive learning, masked point modeling, and clustering-based approaches: CSC [19]. This CVPR 2021 work proposed contrastive learning approach with carefully designed sampling strategy of positive and negative points. MSC [43]. This work was presented at CVPR 2023 and proposed contrastive learning approach combined with masking and reconstructing point colors and normals. Figure 5. Hierarchical reconstruction. The masked voxelization is processed by our hierarchical encoder. The decoder processes the encoded features in bottom-up manner by first including the masked voxels with learnable token. Each level is used in the loss computation before the decoded features are upscaled and combined with the skip connection from the previous level. and combined with the features of the encoder at the previous level, eL1, using skip connection. The process is repeated using different predictor networks at each level, Φ = (Φ1, ..., ΦL), until we reach the initial voxel resolution. Then, the predicted features at each level are supervised with the features of our teacher model: = (cid:88) l=0 pl m1 ˆF dl m2 + pl m2 ˆF dl m1 (2) Bottom-up vs Top-down reconstruction. The presented hierarchical reconstruction performs reconstruction in bottom-up manner, including the masked token only on the decoder while completely removing masked tokens from the encoder. Another possible approach could be to include such learnable tokens in the encoder, similar to Wu et al. [43]. Unfortunately, this will allow deeper levels to infer geometric information of the masked regions from previous 5 MM3D [50]. This work was also presented at CVPR 2023 and it suggested masking strategy and hierarchical reconstruction of point coordinates combined with self-distillation in non-masked regions. OESSL [45]. This CVPR 2024 work used clustering approach to perform data augmentation of plausible objects in the scene combined with contrastive learning. For fair evaluation, we downloaded the weights of all the models trained by the authors and used the resulting features without any fine-tuning. Additionally, we trained our model with the best-performing baseline, MSC. We also provide two models trained from scratch in supervised fashion on the downstream tasks for comparison. Pre-Training. We follow our baselines [19, 43, 45, 50] and pre-train our model only on the ScanNet dataset [11] with masking ratio of 0.4. We train our model for 1800 epochs on computer with 4 A6000 GPUs for 3 days. Model. Our model uses UNet architecture [35] with 2 ResNet blocks in each level of the encoder and decoder using sparse convolutions [12]. In the last two levels of encoder and decoder, similar to Stable Diffusion models [33], we also incorporate two Multi-Head Attention (MHA) blocks with serialization strategy as in PTv3 [44]. However, we remove the xCPE layers and use window attention instead of block attention. We refer to this architecture as Hybrid UNet (HUNet). 5.2. Semantic Segmentation Semantic segmentation in 3D scenes aims to predict the class of each point in the scene from closed set of classes. Successfully solving such task indicates that the features used contain semantically rich information. Datasets. We evaluate all models on three different datasets, ScanNet [11], ScanNet200 [36], and S3DIS [1]. All datasets are composed of dense 3D scans of indoor scenes with objects from 20, 200, and 13 different classes, respectively. We follow the standard splits for ScanNet and ScanNet200, reporting mean Intersection over Union (mIoU) performance on the validation set and reporting performance on the Area5 for the S3DIS dataset. For the evaluation, we use two protocols. In the first one, NN, the class of each point in the validation set is predicted by searching the point in the training set with the most similar feature and using its class as the predictor. Since the scenes are composed of large number of points, to reduce the time of the NN search, we group points in super-points similar to Rozenberszki et al. [37], and perform the similarity search at the super-point level. The second evaluation protocol, Linear, trains linear layer on top of the off-the-shelf features. Supervised MM3D [50] CSC [19] OESSL [45] MSC [43] Ours MM3D [50] CSC [19] OESSL [45] MSC [43] Ours r i BB SR-UNet HUNet PT SR-UNet SR-UNet SR-UNet HUNet HUNet PT SR-UNet SR-UNet SR-UNet HUNet HUNet ScN 72.2 77.0 19.3 24.1 31.0 31.3 39.9 65.7 26.7 27.3 35.4 37.3 58. 68.7 ScN200 S3DIS 25.0 35.4 3.9 4.6 6.6 7.1 11.0 22. 4.6 5.6 9.1 10.2 20.3 26.8 68.2 71.3 24.6 32.0 34.0 35.2 39.7 45.7 36.6 31.1 35.8 41.7 57. 59.5 Table 1. Semantic Segmentation. Performance of different selfsupervised models on the task of semantic segmentation (mIoU). Results. Tbl. 1 shows the result of our experiments. From the NN protocol, we can see that our features are able to achieve much better performance than existing models, outperforming them by more than +30 points in ScanNet, +15 points on ScanNet200, and +10 points on S3DIS. When we compare our model with the same architecture trained with MSC, we can see that it surpasses it by large margin, obtaining improvements of +25, +11, and +6. For the Linear evaluation protocol, we can see similar improvements. We outperform existing models by +30, +16, and +18 points. When we compare our architecture trained with MSC, we can still see large improvements in the performance of +10, +6, and +2. Lastly, Tbl. 1 also shows that, when compared to supervised methods trained from scratch, existing selfsupervised models achieve significantly lower performance on all datasets. On the other hand, our model can achieve competitive performance, and even surpass, models trained from scratch, further underlying the semantic relations captured in our off-the-shelf features. 5.3. Instance Segmentation The task of instance segmentation is more challenging since it requires the prediction of the semantic class of each point in the scene and the mask of each independent object instance. Successfully solving such task will indicate that the model is not only aware of the semantics of the scene but also contains object-aware features. Datasets. As our datasets, we use again ScanNet [11], ScanNet200 [36], and S3DIS [1]. We evaluate the performance of the models on those datasets with mean Average Precission (mAP) with threshold of 0.5. Our evaluation 6 Supervised MM3D [50] CSC [19] OESSL [45] MSC [43] Ours n BB SR-UNet HUNet PT SR-UNet SR-UNet SR-UNet HUNet HUNet ScN 56.9 65. 4.3 3.5 13.6 10.1 24.5 44.4 ScN200 S3DIS 24.5 32.8 0.4 0.1 2.5 1.6 3. 8.8 59.3 58.4 7.7 14.8 16.5 16.6 18.2 23.2 Supervised MM3D [50] CSC [19] OESSL [45] MSC [43] Ours . - r BB SR-UNet HUNet PT SR-UNet SR-UNet SR-UNet HUNet HUNet 78.8 65.8 66.9 73.8 77.1 84.4 87. 35.9 32.1 32.6 35.6 37.4 42.9 44.3 44.3 40.4 39.1 43.0 45.0 51.0 55.7 Table 2. Instance Segmentation. Performance of self-supervised models on the task of instance segmentation (mAP@50). Table 3. 3D Visual Grounding. Accuracy of different selfsupervised models on the task of 3D visual grounding. protocol uses linear layer on top of the frozen features to predict the semantic class and single layer MLP to predict the displacement vector in the PointGroup algorithm [21]. Results. Tbl. 2 presents the results of this experiment. We can see that most of the competing methods struggle to solve this challenging task. Our model, on the other hand, is able to outperform all models by more than +30, +7, and +6 points on ScanNet, ScanNet200, and S3DIS respectively. When compared to the HUNet trained with MSC, our method still maintains similar gains, and at the same time reduces the gap between supervised and selfsupervised methods. With this, we show that our learned features not only represent semantic information effectively but also capture object-level properties. 5.4. 3D Visual Grounding The task of 3D visual grounding places high importance on object-level reasoning, where the model has to locate an object in the scene from text description. This task can be divided into two subtasks: object detection and object discrimination, where the model selects the appropriate instance based on the text description. Since object detection capabilities were evaluated in the previous experiments, we follow [26] and only evaluate the object discriminator task by using ground truth boxes of all objects in the scene. Datasets. We use the ScanRefer [5] dataset, which provides text descriptions of objects from different 3D scenes. We report accuracy on the three evaluation sets with different difficulty levels, Unique, Multiple, and Overall. As our evaluation protocol, following [26], we use small model composed of selfand cross-attention layers between the object features (obtained by averaging voxel features inside the bounding boxes) and the text embeddings. BB 1% 5% 10% 20% 100% Supervised MM3D [50] CSC [19] OESSL [45] MSC [43] SR-UNet 26.1 19.7 HUNet 47.8 36.4 PT 9."
        },
        {
            "title": "13.0\nSR-UNet 12.9 16.2\nSR-UNet 15.9 20.8\nSR-UNet 17.6 21.6\n22.0 30.6\nHUNet",
            "content": "56.7 52.5 14.1 18.7 23.7 25.0 33.0 62.9 67.1 15.7 19.9 25.9 26.8 35."
        },
        {
            "title": "Ours",
            "content": "HUNet 35.2 53.6 59.4 60.8 MM3D [50] CSC [19] OESSL [45] MSC [43] PT 16.4 22.6 SR-UNet 17.1 21.2 SR-UNet 20.4 28.5 SR-UNet 21.9 31.4 30.8 44.3 HUNet 25.5 26.0 32.7 34.4 50.7 26.6 27.3 34.4 36.2 54."
        },
        {
            "title": "Ours",
            "content": "HUNet 35.1 54.5 61.5 63.5 a L 72.2 77.0 19.3 24.1 31.0 31.3 39.9 65.7 26.7 27.3 35.4 37.3 58.2 68.7 Table 4. Efficiency benchmark. Semantic segmentation performance with limited number of scenes in the training set. tain moderate accuracy and, in some cases, even surpass the supervised model. However, our model exhibits significantly improved performance, outperforming them all by large margins of +10, +7, and +10 on the validation sets Unique, Multiple, and Overall. Our Hybrid model trained with MSC is also able to provide an improvement over existing models but still falls behind our proposed selfsupervised objective. Unfortunately, training HUNet from scratch leads to unstable training, being unable to converge. 5.5. Limited annotations In this task, we evaluate the performance of the models under different numbers of annotations on the task of semantic segmentation. This highlights the utility of self-supervised methods when data for the downstream task is scarce. Results. We present our results in Tbl. 3, which shows that existing self-supervised models are able to achieve cerDatasets. We use the benchmarks proposed by Hou et al. [19], in which two protocols are used for evaluation. In the first one, the number of annotated scenes for training is Supervised MM3D [50] CSC [19] OESSL [45] MSC [43] BB 20 100 200 Full SR-UNet HUNet PT SR-UNet SR-UNet SR-UNet HUNet 41.9 62. 10.9 15.3 20.0 20.3 25.4 53.9 68.9 11.8 17.1 22.4 22.4 27.6 62.2 73.1 12.9 18.4 24.4 23.6 30.0 65.5 73. 13.5 19.6 24.9 25.1 32.0 72.2 77.0 19.3 24.1 31.0 31.3 39.9 Ours HUNet 55. 60.3 61.3 62.7 65.7 MM3D [50] CSC [19] OESSL [45] MSC [43] PT SR-UNet SR-UNet SR-UNet HUNet 20.6 23.6 30.1 31.7 51.9 24.2 25.6 32.2 34.1 55.3 25.5 26.5 33.2 35.1 56.3 25.9 26.5 33.7 35.6 56.8 26.7 27.3 35.4 37.3 58. Ours HUNet 62.9 65.9 67.3 68. 68.7 a L Table 5. Efficiency benchmark. Semantic segmentation performance with limited number of annotated points per scene. reduced to 1 %, 5 %, 10 %, and 20 % of the total scenes in ScanNet [11]. In the second one, the number of annotated points per scene is reduced to 20, 50, 100, and 200. Results. Tbl. 4 presents the results when the number of scenes available for training is reduced. The results show that our model is able to surpass, not only all other selfsupervised methods but also all supervised methods when the number of training scenes is reduced to 10 % of the original set using both evaluation protocols. When the number of annotated points is reduced, Tbl. 5 shows that our model is also able to outperform all other self-supervised methods. When compared to supervised methods, our model is able to outperform the SR-UNet model in all benchmarks and achieves performance similar to that of the HUNet model. 5.6. Comparison to 2D Foundation Models common practice in 3D understanding, due to the lack of general 3D models, is to lift features from pre-trained 2D foundation models into 3D [26, 30, 40]. Therefore, in this experiment, we compare the performance of our self-supervised model to different 2D foundation models as done in Lexicon3D [26]. We compare our model in the tasks of semantic segmentation and 3D visual grounding. Results. Tbl. 6 presents the results of this experiment. While 2D foundation models, can show impressive performance despite the domain gap, our 3D-native selfsupervised model is able to outperform them all in all experiments, showing that representations learned natively in 3D better capture the 3D-specific properties of the scene. Sem. Seg. 3D VG BB SR-UNet HUNet ViT ViT ViT HUNet HUNet ScN 72.2 77.0 62.8 47.5 3.4 42. 68.7 78.8 35.9 87.0 43.4 88.1 41.2 86.5 41.6 86.4 41.9 44.3 52.0 50.4 50.4 50.6 87.1 44.3 52. Supervised 2 DINOv2 [28] LSeg [23] CLIP [31] SD [34] Ours Table 6. 2D Foundation models. Comparison to 2D foundation models on semantic segmentation and 3D visual grounding. Figure 6. Qualitative results. Feature visualization of off-theshelf features of our method and the baselines. Our learned features align with semantic classes better than existing methods. 5.7. Qualitative evaluation Following [32], we use PCA to reduce the point features to three dimensions and visualize them as point colors. Fig. 6 presents this visualization for all baselines compared to our model, where our learned features align with semantic classes better than existing methods. 6. Conclusions In this paper, we have introduced an evaluation protocol for self-supervised models tailored to 3D scenes that better reflects the capabilities of the representations learned by these models. Moreover, we have introduced the first selfsupervised model for 3D scene understanding that shows task-agnostic features capable of achieving supervised-like performance on several downstream tasks. Our model not only outperforms all 3D self-supervised models tested, but also achieves better performance than 2D foundation models tasked to solve 3D problems, underlying the need for further 3D-native self-supervised representation learning approaches. In the future, we would like to overcome the main limitation of our method, the reduced amount of data used for training, by consolidating large dataset. 8 Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Qualitative Results B. Additional Experiments . . . . B.1. Fine-Tuning . . . B.2. Object-Centric Self-Supervised Methods . . B.3. 2D-3D Knowledge Distillation Methods . . . . . . . . . . . . . . . . . . . . C. Ablation Studies C.1. Masking . . C.2. Hierarchical Supervision . . . C.3. Masking Strategy . . C.4. Model Architecture . . . C.5. Masking Ratio . . . . C.6. Layer Importance . . . C.7. Scaling Properties . . C.8. NN Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. Detailed experimental setup . . D.1. Model architecture . D.2. Experiment hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 9 9 9 11 11 11 11 11 12 12 12 12 13 13 13 A. Additional Qualitative Results Fig. 7 presents additional feature visualization of our selfsupervised model for different 3D scenes. We follow [32] and use PCA to reduce the point features to three dimensions and visualize them as point colors. Results show that semantically similar objects result in similar features for all scenes. B. Additional Experiments B.1. Fine-Tuning Although not the main focus of this work, we also present results where our self-supervised model is used as weight initialization step for fine-tuning on the downstream task. In Tbl. 7, we present results on the semantic segmentation task on the three datasets used in our main experiments. Our self-supervised model provides significant improvement over supervised models trained from scratch and outperforms all existing self-supervised models. B.2. Object-Centric Self-Supervised Methods line of important Another research focuses on selfsupervised models pre-trained specifically on object-centric datasets. While these models present strong performance in object-centric tasks, such as shape classification or shape n n SR-UNet + PC [47] + CSC [19] + MSC [43] + GC [42] HUNet + MSC [43] + Ours ScN 72.2 74.1 73.8 75.3 75.7 77.0 78.2 78. ScN200 S3DIS 25.0 26.2 26.4 28.8 30.0 35.4 34.9 35.7 68.2 70.3 72.2 72.0 71.3 72.1 73. Table 7. Fine-tuning. Performance of different pre-trained methods after fine-tuning on the semantic segmentation task. segmentation, those models are not well suited for dense predictions usually required in 3D scene understanding, such as semantic segmentation of large indoor scenes. However, due to the nature of these object-centric models, they are usually also evaluated on the 3D scene understanding task of object detection, where models need to predict the bounding box of objects instead of dense per-point instance segmentation maps. Therefore, we use our self-supervised model as the 3D backbone in an object detection framework to compare our model with such methods. Dataset. In this experiment, we use the ScanNet dataset [11], and we report mAP with Intersection over Union (IoU) thresholds of 0.5 and 0.25. We use our model as the 3D backbone of the 3DETR [27] object detection framework, and we evaluate our self-supervised model with two different protocols. First, we obtain off-the-shelf features by freezing the 3D backbone while we train the remaining components of the 3DETR [27] framework using our general-purpose features as input. In the second protocol, we also fine-tune all the parameters of the 3D backbone using our self-supervised model as weight initialization. Baselines. We compare our model to several state-ofthe-art self-supervised models pre-trained on object-centric datasets and then fine-tuned on the object detection task. These object-centric models use transformer-based architectures trained with different MIM objectives. While Point-Bert [52], Point-MAE [29], and MaskPoint [24] use non-hierarchical architecture, Point-M2AE [54] use hierarchical model with bottom-up masking approach. How9 Figure 7. Qualitative results. Feature visualization of off-the-shelf features of our method and the baselines. Our learned features align with semantic classes better than existing methods. 10 mAP@25 mAP@50 C. Ablation Studies 3DETR [27] + Point-Bert [52] + Point-MAE [29] + MaskPoint [24] + Point-M2AE [54] + Ours (Lin.) + Ours (FT) 62.1 61.0 63.4 63.4 66.3 65.6 71.3 37.9 38.3 40.6 40.6 48.3 40.2 52.2 Table 8. Object detection. Comparison of our off-the-shelf features to fine-tuning object-centric self-supervised methods. In this section, we describe the ablation studies conducted to validate our design choices. For all our experiments, we report linear probing performance on the task of semantic segmentation on ScanNet. Unless otherwise stated, due to the large training times of the self-supervise stage, we perform our ablation studies on smaller model that takes as input coarser voxelization of the scene, 4 cm voxels, and we train our models for 800 epochs instead of 1800. For more details of the experimental setup and model used, we refer the reader to Sec. D. Obj. Det. Sem. Seg. C.1. Masking mAP@25 mAP@50 Bridge3D [8] SAM-MAE [9] Ours (Lin.) Ours (FT) 65.3 68.2 65.6 71.3 44.2 48.4 40.2 52.2 ScN 73.9 75.4 68.7 78.5 S3DIS 70.2 71.8 59.5 73.2 Table 9. 2D-3D KD. Comparison to methods that rely on knowledge distillation from 2D foundation models. ever, all models reconstruct the point coordinates from the last layer in the model. Results. Tbl. 8 presents the results of our experiments. Our off-the-shelf features, Lin. on Tbl. 8, present competitive performance, outperforming most existing objectcentric self-supervised methods. When we further fine-tune our model on the downstream task, FT on Tbl. 8, we outperform all models by large margin. These results are in line with the results presented by Xie et al. [47] and highlight the need for scene-centric self-supervised methods. B.3. 2D-3D Knowledge Distillation Methods Since general models for 3D scene understanding are not available, recent works have proposed distilling knowledge from 2D foundation models. While Bridge3D [8] combines several 2D foundation models for knowledge distillation into non-hierarchical 3D transformer architecture, SAM-MAE [9] uses SAM [22] to mask objects in 3D space and MIM objective to train the same model architecture. We compare our self-supervised model to these models finetuned on object detection and semantic segmentation tasks. Result. Tbl. 9 presents the results of this experiment. While our linear probing setup is not able to achieve the same performance as the baselines, when fine-tuned, our model can outperform them in all experiments. In this experiment, we evaluate the importance of our Masked Scene Modeling objective. We train model with our full framework and the same model without our masking strategy. In this version of our framework, the crops given to the student model are not masked, and the full crop is processed by the model. Then, the training objective is the prediction of deep features from the teacher model, which has access to full view of the scene with different data augmentation. This objective is similar to the self-distillation objective used in MM3D [50]. Tbl. 10 (a) presents the results of this experiment. We can see that the proposed Masked Scene Modeling objective is essential for learning semantically relevant features, leading to an improvement of more than +16 points. C.2. Hierarchical Supervision In this experiment, we measure the importance of the hierarchical reconstruction objective. We compare our full framework with model trained with supervision only on the last layer of the decoder, common practice in existing self-supervised approaches for 3D scenes [19, 43, 47]. Tbl. 10 (b) shows that supervising only the last layer leads to gap in performance of more than +6. This experiment aligns with the findings of our pilot study and highlights the importance of hierarchical supervision when training hierarchical architectures. C.3. Masking Strategy We also compare our bottom-up masking strategy with traditional top-down approach, similar to the one used in MSC [43]. In this approach, instead of incorporating the masked patches in the decoder, we add them in the encoder with the corresponding learnable token. We can see that in Tbl. 10 (c), even though top-down approach can lead to relatively good features, our bottom-up approach leads to semantically richer features with more than +4 points of improvement on the downstream task."
        },
        {
            "title": "No Mask\nMask",
            "content": "50.7 66."
        },
        {
            "title": "Last\nAll",
            "content": "60.5 66.8 top-down bottom-up 62.4 66.8 (a) Masking. Patch supervision with vs without masking. (b) Supervision. Layers in the hierarchy used in the loss. (c) Mask strategy. Masking hierarchy top-down vs bottom-up."
        },
        {
            "title": "SparseConv\nMHA\nHUNet",
            "content": "61.8 52.3 66.8 (d) Model. Types of model used. Table 10. Ablation studies. Evaluation of the different components of our framework on the task of semantic segmentation on ScanNet. Figure 8. Masking ratio. Linear probing performance for different masking ratios. C.4. Model Architecture We also evaluate the effect of the model architecture used. We trained two additional models, one only based on Sparse convolutions without MHA blocks, and another one with MHA instead of ResNet blocks as in Ptv3 [44]. Tbl. 10 (d) indicates that the model using only sparse convolutions provides lower performance than our hybrid architecture. Moreover, the model with only MHA layers significantly reduces the performance on the downstream task. This is due to the additional constraints of such models, where lower learning rate is necessary to avoid unstable training. Although we believe that an exhaustive hyperparameter search could lead to an improvement of such models, our hybrid model architecture is robust to higher learning rates and, therefore, easier to train. C.5. Masking Ratio Additionally, we measure the influence of the masking ratio on the final performance of the model. We evaluated range of ratios from 20 % to 70 % with intervals of 10 % and plot the results in Fig. 8. The results show that the framework is relatively robust to the masking ratio used, achieving similar performance for ratios between 30 % and 60 %, with the highest value obtained at 40 %. However, smaller ratios, such as 20 %, or too high, such as 70 %, lead to significant drop in performance. C.6. Layer Importance To expand our pilot study, we further evaluate the importance of the different layers on the performance of our final model. First, we evaluate the linear probing abilities when only one layer is used as input. Then, we evaluate the effect of using all layers except one for the same linear probing setup. Tbl. 11 present the results of this experiment. Results show that, for all layers, using the output of one layer"
        },
        {
            "title": "Remove",
            "content": "1 2 3"
        },
        {
            "title": "All",
            "content": "28.3 43.5 54.8 62.8 62.4 67.1 67.0 67.0 66.6 64.4 68.7 Table 11. Layer importance. Comparison of the performance on the linear probing setup when only one layer is used (Alone) or when all layers except one are used (Remove). Figure 9. Scalability experiments. Evaluate the performance of the model under reduced data used for pre-training and reduced number of epochs. alone (Alone in Tbl. 11) leads to lower performance than using concatenation of all of them. Moreover, results also show that using all layers except one (Remove in Tbl. 11) also leads to degradation in performance in all cases. This experiment shows the importance of all layers, indicating that each layer provides complementary information. Additionally, we also evaluate different methods of combining such features. We compare the concatenation of features used in all of our experiments (68.7 mIoU), to setup where the features are aggregated with sum operator (68.7 mIoU) and to setup where the features are aggregated with learned weighted sum (68.8 mIoU). Our results show that there is no significant difference between these methods. C.7. Scaling Properties Moreover, we evaluate the scaling abilities of our framework w.r.t. the data used for pre-training and the number of epochs. For this setup, we use our full model and configuration as in the main experiments in the paper. Fig. 9 presents the results of these experiments. Results show that more data and longer pre-training yield significant improve-"
        },
        {
            "title": "Config",
            "content": "Voxel size Norm layers Downsample Upsample Serialization Block bias Att. drop Drop path Activation func. FF layer FF ratio Enc. channels Enc. ResNet Enc. MHA Enc. MHA Window Enc. MHA # Heads Dec. channels Dec. ResNet Dec. MHA Dec. MHA Window Enc. MHA # Heads"
        },
        {
            "title": "Value",
            "content": "2 cm RMSNorm [53] Strided SparseConv Transpose SparseConv + TZ + + TH [44] False 0.1 0.4 GELU [18] GEGLU [39] 4 [32, 64, 128, 256, 384] [2, 2, 2, 2, 2] [0, 0, 0, 2, 2] [0, 0, 0, 1024, 1024] [0, 0, 0, 32, 48] [64, 96, 128, 256, 384] [2, 2, 2, 2, 2] [0, 0, 0, 2, 2] [0, 0, 0, 1024, 1024] [0, 0, 0, 32, 48] Voxel size Norm layers Downsample Upsample Serialization Block bias Att. drop Drop path Activation func. FF layer FF Ratio Enc. channels Enc. ResNet Enc. MHA Enc. MHA Window Enc. MHA # Heads Dec. channels Dec. ResNet Dec. MHA Dec. MHA Window Enc. MHA # Heads 4 cm RMSNorm [53] Strided SparseConv Transpose SparseConv + TZ + + TH [44] False 0.1 0.4 GELU [18] GEGLU [39] 4 [64, 128, 256, 384] [2, 2, 2, 2] [0, 0, 2, 2] [0, 0, 1024, 1024] [0, 0, 32, 48] [96, 128, 256, 384] [2, 2, 2, 2] [0, 0, 2, 2] [0, 0, 1024, 1024] [0, 0, 32, 48] Table 12. Model configuration. Table 13. Model configuration for ablation studies. ments for linear probing on semantic segmentation. This highlights the importance of additional data and training in self-supervised objectives and paves the road for future improvements of our method. C.8. NN Robustness Lastly, we evaluate the robustness of the NN evaluation protocol w.r.t. the distance metric used to compare features. We compare the L2 distance used in all our experiments (65.7 mIoU), to the L1 distance (66.4 mIoU) and to the cosine distance (66.0 mIoU). Although other distance metrics yield slightly better performance, the experiment indicates that the evaluation protocol is robust to the distance metric chosen for evaluation. D. Detailed experimental setup D.1. Model architecture We designed Hybrid UNet architecture (HUnet) combining standard ResNet blocks [15] with serialization transformer layers as in PTv3 [44]. However, contrary to PTv3 [44], we use sliding-window attention as in LongFormer [3] since this eliminates the need for padding and makes the receptive field adaptive. Moreover, we do not include xCPE [44] in such layers since the ResNet blocks can act as conditional positional encoding. Furthermore, following the design of Stable Diffusion [33], we only included the MHA layers in the lowest resolution levels of the model, making the model faster and more stable to different learning rates. Tbl. 12 presents detailed description of the different components of our architecture, such as channels per level, number of layers per level, or activation function used. We also provide the configuration of the model used for the ablation studies in Tbl. 13. For these experiments, we used smaller model with one level less in the encoder and decoder, which takes bigger voxels of 4 cm as input. D.2. Experiment hyperparameters Self-supervised training. We build our self-supervised framework on top of the codebase Pointcept [10]. The hyperparameters used for training our self-supervised model are described in Tbl. 14. As data augmentation, we use the default augmentations for indoor semantic segmentation of PTv3 [44]. We only increase the number of points per crop as described in Tbl. 14. Linear probing - Semantic and Instance segmentation. We use the codebase Pointcept [10] for our linear probing experiments in the downstream tasks of semantic and instance segmentation. The hyperparameters used in these experiments are described in Tbl. 15 and Tbl. 16. For data augmentation, we use the default configuration of PTv3 [44]."
        },
        {
            "title": "Config",
            "content": "Optimizer Betas Weight decay Learning rate LR Scheduler Batch size Epochs Warmup epochs Crop size Mask Ratio Teacher mom."
        },
        {
            "title": "Value",
            "content": "AdamW [25] (0.9, 0.95) 0.05 0.0015 Cosine 12 1800 60 240000 0.4 0.996 1.0 Table 14. Self-supervised training configuration."
        },
        {
            "title": "ScanNet",
            "content": "ScanNet200 S3DIS"
        },
        {
            "title": "Optimizer\nBetas\nWeight decay\nLearning rate\nLR Scheduler\nBatch size\nEpochs\nWarmup epochs\nCrop size",
            "content": "AdamW [25] (0.9, 0.95) 0.01 0.01 Cosine 8 200 2 120000 200 2 120000 100 1 200000 Table 15. Linear probing config. for semantic segmentation."
        },
        {
            "title": "ScanNet",
            "content": "ScanNet200 S3DIS"
        },
        {
            "title": "Optimizer\nMomentum\nWeight decay\nLearning rate\nLR Scheduler\nBatch size\nEpochs\nCrop size",
            "content": "12 200 SGD 0.9 0.0001 0.1 PolyR 12 200 8 100 200000 Table 16. Linear probing config. for instance segmentation. Coss-Attention - Visual grounding. Given 3D point cloud with associated features, 3D ground truth bounding boxes of objects, and text description, the model is tasked to select the object that matches the text description. We encode the text with the CLIP text encoder [31] and use the attention head of Zhand et al. [56] composed of selfand cross-attention layers. The cross-attention layers combine the text CLIP embeddings and object fea-"
        },
        {
            "title": "Value",
            "content": "AdamW [25] (0.9, 0.95) 0.00001 0.0005 Cosine 12 10 1 Table 17. Visual grounding configuration."
        },
        {
            "title": "Config",
            "content": "Optimizer Betas Weight decay Learning rate LR Scheduler Batch size Epochs Warmup epochs Clip gradients # queries # points"
        },
        {
            "title": "Value",
            "content": "AdamW [25] (0.9, 0.95) 0.1 1e-6 Cosine 24 1080 9 0.1 256 2048 Table 18. Object detection configuration. tures (obtained from aggregating point features inside object bounding boxes). The output of the model is probability per object. Then, we train the model using crossentropy loss, since the task can be formulated as classification problem where the object matching the text description should have the highest probability. We use the codebase of Multi3DRefer [56] and the hyperparameters used in these experiments are described in Tbl. 17. For data augmentation, we use the default configuration of PTv3 [44] for the task of instance segmentation. Object detection. In these experiments, we use the object detection framework 3DETR [27]. For the linear probing and fine-tuning experiments, we use the same configuration described in Tbl. 18. For data augmentation, we use the default configuration of 3DETR [27]. Fine-tuning - Semantic segmentation. For fine-tuning on the task of semantic segmentation, we use different configuration than the one used in our linear probing experiments. The hyperparameters of these experiments are described in Tbl. 19."
        },
        {
            "title": "ScanNet",
            "content": "ScanNet200 S3DIS"
        },
        {
            "title": "Optimizer\nBetas\nWeight decay\nLearning rate\nLR Scheduler\nBatch size\nEpochs\nWarmup epochs\nCrop size",
            "content": "AdamW [25] (0.9, 0.95) 0.05 0.001 Cosine 48 200 2 120000 48 200 2 120000 32 500 20 100000 Table 19. Fine-tuning config. for semantic segmentation. Masked Scene Context. For training our model with the baseline MSC [43], we use different hyperparameters than the ones recommended by the authors. Our model trained with the default parameters leads to subpar performance, obtaining less than 20 mIoU on the task of linear probing for semantic segmentation on ScanNet. Therefore, we modified the number of training epochs to 1800 instead of 600 and the optimizer from SGD to AdamW [25]. These small changes lead to an increase in performance, as reported in the main experiments of this paper. [1] I. Armeni, S. Sax, A. Zamir, and S. Savarese. Joint 2d-3dsemantic data for indoor scene understanding, 2017. 6 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, [3] Iz Beltagy, Matthew Peters, and Arman Cohan. LongarXiv preprint former: The long-document transformer. arXiv:2004.05150, 2020. 13 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 3 [5] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pages 202221. Springer, 2020. 7 [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PMLR, 2020. 2, 3 [7] Yujin Chen, Matthias Niessner, and Angela Dai. 4dcontrast: Contrastive learning with dynamic correspondences for 3d scene understanding. In European Conference on Computer Vision (ECCV), 2022. [8] Zhimin Chen and Bing Li. Bridging the domain gap: Selfsupervised 3d scene understanding with foundation models. Conference on Neural Information Processing Systems (NeurIPS), 2023. 1, 11 [9] Zhimin Chen, Liang Yang, Yingwei Li, Longlong Jing, and Bing Li. SAM-guided masked token prediction for 3d scene In Conference on Neural Information Prounderstanding. cessing Systems (NeurIPS), 2024. 11 [10] Pointcept Contributors. Pointcept: codebase for point cloud perception research. https://github.com/ Pointcept/Pointcept, 2023. 13 [11] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 3, 6, 8, 9 [12] Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse Proceedings of the IEEE/CVF convolutional networks. Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 6 [13] Jie Gui, Tuo Chen, Jing Zhang, Qiong Cao, Zhenan Sun, Hao Luo, and Dacheng Tao. survey on self-supervised learning: Algorithms, applications, and future trends. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [14] Kaveh Hassani and Mike Haley. Unsupervised multi-task In Proceedings of the feature learning on point clouds. 15 IEEE/CVF International Conference on Computer Vision, pages 81608171, 2019. 2 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2016. 13 [16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 4 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1 [18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 13 [19] Ji Hou, Benjamin Graham, Matthias Niesner, and Saining Xie. Exploring data-efficient 3d scene understanding In IEEE / CVF Conferwith contrastive scene contexts. ence on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021. 1, 2, 3, 5, 6, 7, 8, 9, 11 [20] Siyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu. Spatio-temporal self-supervised representation learning for In Proceedings of the IEEE/CVF Inter3d point clouds. national Conference on Computer Vision, pages 65356545, 2021. 2 [21] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, ChiWing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 7 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 11 [23] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segIn International Conference on Learning Repmentation. resentations, 2022. [24] Haotian Liu, Mu Cai, and Yong Jae Lee. Masked discrimination for self-supervised learning on point clouds. Proceedings of the European Conference on Computer Vision (ECCV), 2022. 9, 11 [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. 14, 15 [26] Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liang-Yan Gui, and Yu-Xiong Wang. Lexicon3d: Probing visual foundation models for complex 3d scene understanding. In Advances in Neural Information Processing Systems, 2024. 1, 7, 8 [27] Ishan Misra, Rohit Girdhar, and Armand Joulin. An EndIn to-End Transformer Model for 3D Object Detection. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 9, 11, 14 [28] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1, 2, 3, 4, 8 [29] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In European conference on computer vision, pages 604621. Springer, 2022. 2, 9, [30] Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 8 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning vision. (ICML), 2021. 8, 14 [32] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation In Proceedings of model reduce all domains into one. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1249012500, 2024. 8, 9 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 6, 13 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 8 [35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 2, 3, 6 [36] David Rozenberszki, Or Litany, and Angela Dai. Languagegrounded indoor 3d semantic segmentation in the wild. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. [37] David Rozenberszki, Or Litany, and Angela Dai. Unscene3d: Unsupervised 3d instance segmentation for indoor scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6 [38] Jonathan Sauder and Bjarne Sievers. Self-supervised deep learning on point clouds by reconstructing space. Advances in Neural Information Processing Systems, 32, 2019. 2 [39] Noam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 13 [40] Ayca Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenMask3D: Open-Vocabulary 3D Instance Segmentation. 16 [52] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1931319322, 2022. 2, 9, [53] Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 13 [54] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Pointm2ae: Multi-scale masked autoencoders for hierarchical point cloud pre-training. Advances in Neural Information Processing Systems (NeurIPS), 2022. 2, 9, 11 [55] Xiaoshuai Zhang, Zhicheng Wang, Howard Zhou, Soham Ghosh, Danushen Gnanapragasam, Varun Jampani, Hao Su, and Leonidas Guibas. Condense: Consistent 2d/3d pretraining for dense and sparse features from multi-view images. In European Conference on Computer Vision (ECCV), 2024. 1 [56] Yiming Zhang, ZeMing Gong, and Angel Chang. Multi3drefer: Grounding text description to multiple 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 14 [57] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra. Self-supervised pretraining of 3d features on any point-cloud. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1025210263, 2021. 2 [58] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. International Conference on Learning Representations (ICLR), 2022. 1, 3, [59] Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chunhua Shen, Yu Qiao, and Wanli Ouyang. Ponderv2: Pave the way for 3d foundation model with universal pre-training paradigm. arXiv preprint arXiv:2310.08586, 2023. 1 In Advances in Neural Information Processing Systems (NeurIPS), 2023. 1, 8 [41] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In International Conference on Machine Learning (ICML), 2008. 3 [42] Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, Hengshuang Zhao, and Jiaya Jia. Groupcontrast: Semantic-aware self-supervised representation learning for 3d understanding. In IEEE / CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 2, 9 [43] Xiaoyang Wu, Xin Wen, Xihui Liu, and Hengshuang Zhao. Masked scene contrast: scalable framework for unsupervised 3d representation learning. In IEEE / CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 3, 5, 6, 7, 8, 9, 11, 15 [44] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 6, 12, 13, 14 [45] Yanhao Wu, Tong Zhang, Wei Ke, Congpei Qiu, Sabine Susstrunk, and Mathieu Salzmann. Mitigating object dependencies: Improving point cloud self-supervised learning In IEEE / CVF Conference on through object exchange. Computer Vision and Pattern Recognition (CVPR), 2024. 1, 3, 6, 7, 8 [46] Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pretraining for 3d point cloud understanding. In European Conference on Computer Vision (ECCV), 2020. [47] Saining Xie, Jiatao Gu, Demi Guo, Charles Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised preIn Computer training for 3d point cloud understanding. VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 574591. Springer, 2020. 2, 3, 9, 11 [48] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 96539663, 2022. 1 [49] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 5 [50] Mingye Xu, Mutian Xu, Tong He, Wanli Ouyang, Yali Wang, Xiaoguang Han, and Yu Qiao. Mm-3dscene: 3d scene understanding by customizing masked modeling with informative-preserved reconstruction and self-distilled consistency. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 3, 6, 7, 8, 11 [51] Siming Yan, Yuqi Yang, Yu-Xiao Guo, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Qixing Huang. 3d feature prediction for masked-autoencoder-based point cloud preIn International Conference on Learning Repretraining. sentations (ICLR), 2024."
        }
    ],
    "affiliations": []
}