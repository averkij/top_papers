{
    "paper_title": "VidTok: A Versatile and Open-Source Video Tokenizer",
    "authors": [
        "Anni Tang",
        "Tianyu He",
        "Junliang Guo",
        "Xinle Cheng",
        "Li Song",
        "Jiang Bian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 1 6 0 3 1 . 2 1 4 2 : r a"
        },
        {
            "title": "VidTok",
            "content": "VIDTOK VERSATILE AND OPEN-SOURCE VIDEO TOKENIZER Anni Tang1,2, Tianyu He,1, Junliang Guo1, Xinle Cheng3, Li Song2, Jiang Bian1 1Microsoft Research, 2Shanghai Jiao Tong University, 3Peking University https://github.com/microsoft/VidTok"
        },
        {
            "title": "ABSTRACT",
            "content": "Encoding video content into compact latent tokens has become fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings. Figure 1: Illustration of the quantitative comparison of discrete and continuous tokenization performance across our VidTok model and state-of-the-art methods, evaluated using four metrics: PSNR, SSIM, LPIPS, and FVD. All performance metrics are obtained through experiments conducted under consistent evaluation protocol to ensure fairness and comparability. Larger chart areas correspond to better performance across all metrics."
        },
        {
            "title": "INTRODUCTION",
            "content": "Visual generation and understanding have emerged as prominent research areas, driven by the capacity of visual data to offer immersive experiences (Ho et al., 2022b; Singer et al., 2023; Ho et al., 2022a; Yu et al., 2023; Kondratyuk et al., 2024; Yang et al., 2024c; Bai et al., 2024; Zhu et al., 2024), convey rich semantic information (Li et al., 2023; Zhang et al., 2024b; Liu et al., 2024), and function as an interface for models to Project lead."
        },
        {
            "title": "VidTok",
            "content": "interact with the physical world (Yang et al., 2024b;a; Zhang et al., 2024a; Chen et al., 2024b). However, the high degree of redundancy inherent in pixel-level representations (Sullivan et al., 2012) has led to shift in modern methodologies. These approaches often employ visual tokenization techniques (Rombach et al., 2022; OpenAI, 2024; Kondratyuk et al., 2024; Wu et al., 2024; Chameleon, 2024), transforming raw visual data into compact latent tokens, which serve as more efficient basis for tasks involving generation and understanding. The adoption of visual tokenization has catalyzed extensive research on image tokenizers (Rombach et al., 2022; Zheng et al., 2022; Patil et al., 2024), resulting in the development of several open-source tokenizers that serve as widely used tools to advance and streamline image-related research (CompVis; HuggingFace; TencentARC). However, comparable resources and tools remain largely absent in the domain of video. While it is possible to treat each frame of video as an independent image and compress it using an image tokenizer, this approach overlooks temporal redundancies and consistency, resulting in latent tokens that are temporally redundant and potentially inconsistent across frames. Recent efforts have sought to address this gap by introducing video tokenizers that incorporate temporal modeling. However, these approaches often fail to account for diverse use cases and exhibit limitations in performance. For instance, Yang et al. (2024c) exclusively offers tokenizers with continuous tokens, while Kondratyuk et al. (2024) demonstrates the effectiveness of discrete tokens but remains unavailable as an open-source tool. In this work, we introduce VidTok, versatile and state-ofthe-art video tokenizer designed to support both continuous and discrete tokenizations effectively. Our approach follows common architecture as illustrated in Fig. 2, and incorporates several key advancements over existing solutions: Figure 2: An overview of video tokenizers. Model architecture. We handle spatial and temporal sampling separately, reducing computational complexity without sacrificing reconstruction quality. Specifically, we employ 2D convolutions in spatial up/downsampling modules and adopt an AlphaBlender operator in temporal up/downsampling modules, while the remaining parts still utilize 3D convolutions to perform information fusion. Advanced quantization techniques. To address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ) (Van Den Oord et al., 2017), we propose the use of Finite Scalar Quantization (FSQ) in discrete video tokenization. By optimizing the implicit codebook directly, this approach substantially improves discrete tokenizers. Improved training strategies. To improve training efficiency, we employ two-stage training strategy: initially pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Furthermore, we observe that utilizing training data with reduced frame rates effectively improves the models ability to represent motion dynamics. Building upon the aforementioned advancements, we train VidTok on large-scale video dataset and evaluate its performance on widely used benchmarks such as MCL-JCV (Wang et al., 2016) and web video evaluation set. Experimental results reveal that VidTok outperforms previous models in both discrete and continuous tokenization, achieving superior results across all evaluated metrics, including PSNR, SSIM, LPIPS, and FVD."
        },
        {
            "title": "2.1 DISCRETE VIDEO TOKENIZATION",
            "content": "Discrete tokenization maps input images to latent space and quantizes the latent representations using codebook of vectors by identifying the nearest codebook vector. Compared to continuous tokens, discrete tokens offer the advantage of mitigating error accumulation during the autoregressive generation process. Building on the foundation of discrete image tokenization (Van Den Oord et al., 2017), discrete video tokenization extends this approach to video data (Yan et al., 2021; Yu et al., 2024; Wang et al., 2024; NVIDIA). It incorporates temporal modeling to effectively manage the temporal redundancies inherent in video sequences."
        },
        {
            "title": "VidTok",
            "content": "VideoGPT (Yan et al., 2021) leverages VQ-VAE (Van Den Oord et al., 2017) to learn downsampled discrete latent representations of raw video data through the use of 3D convolutions and axial self-attention. Subsequently, GPT-like architecture is employed to autoregressively model these discrete latents, utilizing spatio-temporal position encodings. This approach produces video samples that are competitive with state-of-the-art GAN-based models for video generation. MAGVIT-v2 (Yu et al., 2024) observes that the generation performance initially improves but then deteriorates for larger vocabulary in VQ-VAE, and decreasing the code embedding dimension when increasing the vocabulary size facilitates learning over the distribution of large vocabulary (Yu et al., 2022). Building on this insight, MAGVIT-v2 reduces the embedding dimension of the VQ-VAE codebook to zero and introduces Lookup-Free Quantization (LFQ), which eliminates the embedding lookup process. This approach improves both reconstruction and generation quality in language models as vocabulary size increases. As concurrent work, Cosmos-Tokenizer (NVIDIA) utilizes Finite Scalar Quantization (FSQ) (Mentzer et al., 2024) to achieve discrete tokenization, where each dimension is quantized to small, fixed set of values. In this work, we integrate several key advancements, including FSQ, to develop state-of-the-art discrete video tokenizer. The proposed tokenizer is designed to facilitate wide range of applications in video analysis, generation, and modeling, fostering further innovation in the field."
        },
        {
            "title": "2.2 CONTINUOUS VIDEO TOKENIZATION",
            "content": "Compared to discrete tokenization, continuous tokenization (Zhao et al., 2024; hpcaitech; Chen et al., 2024a; Yang et al., 2024c; NVIDIA) generally offers higher reconstruction fidelity (Rombach et al., 2022). It is typically employed in conjunction with continuous space modeling techniques, such as diffusion models (Ho et al., 2020), to enhance the quality and smoothness of generated outputs. For example, Latent Video Diffusion Models (LVDMs) (Blattmann et al., 2023; Guo et al., 2024; Yang et al., 2024c; OpenAI, 2024) efficiently and effectively generate video content by compressing visual data into continuous latent representation first and then operating on it with denoising techniques. notable example of this approach is OpenAIs Sora (OpenAI, 2024), which serves as representative work in this domain. CV-VAE (Zhao et al., 2024) introduces continuous video tokenizer designed to achieve spatio-temporal compression of videos, with latent space that aligns with the latent space of existing image VAEs (Rombach et al., 2022) through its proposed latent space regularization method. Open-Sora (hpcaitech) and Open-SoraPlan (PKU-YuanGroup; Chen et al., 2024a) are two open-source projects aimed at reproducing OpenAIs Sora. Both projects offer continuous video tokenizers that effectively perform spatial and temporal compression. CogVideoX (Yang et al., 2024c) introduces continuous tokenizer that preserves greater amount of information by maintaining larger number of latent channels, resulting in enhanced reconstruction fidelity. More recently, Cosmos-Tokenizer (NVIDIA) also provides continuous video tokenizers with various compression ratios. The proposed VidTok builds upon the publicly available models mentioned above by incorporating several key advancements, with the goal of establishing foundational tokenizer for video-related research."
        },
        {
            "title": "3 VIDTOK",
            "content": "In this section, we first introduce the general structure of the video tokenizer with detailed notations. From Sec. 3.2 to Sec. 3.4, we introduce the improved model architecture, the advanced quantization technique, and the improved training strategy respectively."
        },
        {
            "title": "3.1 OVERVIEW OF VIDEO TOKENIZER",
            "content": "To enhance efficiency, existing approaches for video generation and understanding often utilize video tokenizers (e.g., 3D VAEs (Kingma & Welling, 2014)) to convert raw visual data into compact latent tokens. As illustrated in Fig. 2, these methods typically involve an encoder that compresses video data into compact latent tokens across both spatial and temporal dimensions, followed by decoder that reconstructs the tokens back into pixel space. Depending on the scenario, latent tokens can be either continuous (Zhao et al., 2024; Yang et al., 2024c; OpenAI, 2024) or discrete (Yu et al., 2024; Wang et al., 2024; NVIDIA), and the model architecture may be designed to operate in causal (Yu et al., 2024) or non-causal (Blattmann et al., 2023) manner. To enhance the models capacity for generating novel data samples and to mitigate overfitting to the training dataset, it is essential to apply appropriate regularization within the latent space (Kingma & Welling, 2014; Van Den Oord et al., 2017)."
        },
        {
            "title": "VidTok",
            "content": "Figure 3: The improved model architecture. In the context of causal setting, consider an input with dimensions = 17 256 256. Assuming temporal compression factor of 4 and spatial compression factor of 8, the intermediate latent representation is reduced to dimensions = 5 32 32. Let and denote the encoder and the decoder of the video tokenizer respectively, denote the regularizer applied in the latent space, rt and rs denote the temporal and spatial compression ratios respectively. video containing frames is denoted as = {x1, x2, ..., xN } RN 3HW , where = rt n, = rs and = rs w. The workflow can be formulated as: = R(E(X)), ˆX = D(Z) (1) where Rnchw denotes the compressed latent representation and ˆX RN 3HW denotes the reconstructed video. In causal scenarios, the first frame is typically treated as an independent image for compression, enabling the visual tokenizer to function as both an image and video tokenizer (Yu et al., 2024). At this point, video R(N +1)3HW , which contains + 1 frames, is compressed into R(n+1)chw. Specifically, the first frame is compressed solely in the spatial dimension, while the subsequent frames undergo compression in both temporal and spatial dimensions."
        },
        {
            "title": "3.2 MODEL ARCHITECTURE",
            "content": "In the existing literature, it is widely acknowledged that fully 3D architectures offer superior reconstruction quality, albeit at high computational cost (Chen et al., 2024a). However, in this work, we demonstrate that substituting portion of these 3D convolutions with combination of 2D and 1D convolutionseffectively decoupling spatial and temporal samplingcan achieve comparable reconstruction quality while significantly reducing computational demands. The detailed network architecture is illustrated in Fig. 3. As shown, 2D convolutions are employed for spatial upsampling and downsampling modules, while an AlphaBlender operator is utilized in the temporal upsampling and downsampling modules. The remaining components, including the input/output layers and bottleneck layers, leverage 3D convolutions to facilitate information fusion. The specific structures of the temporal upsampling and downsampling modules are depicted on the right side of Fig. 3. Additionally, layer normalization (Lei Ba et al., 2016) is incorporated throughout the architecture to enhance stability and performance. Experimental results, as summarized in Tab. 2, validate the effectiveness of the proposed architectural design. AlphaBlender operator. Given parameter α within the range [0, 1], the AlphaBlender operator performs the following operation to input x1 and input x2: = α x1 + (1 α) x2 (2) where is the result after blending, and α can be either learnable or given hyperparameter (PKU-YuanGroup). In this work, we adopt pre-defined α = Sigmoid(0.2)."
        },
        {
            "title": "VidTok",
            "content": "Figure 4: Left: Vector Quantization (VQ) employed in Vector Quantised-Variational AutoEncoder (VQVAE) (Van Den Oord et al., 2017). Right: Finite Scalar Quantization (FSQ) (Mentzer et al., 2024) utilized in our model. In causal cases, all 3D and 1D convolutions are configured to operate causally, ensuring that each frame has access only to historical information from preceding frames. For given video R(N +1)3HW , the first frame is duplicated rt 1 times and inserted before the original first frame. After completing the full workflow, the process yields (n + 1) rt frames. By discarding the first rt 1 frames, the reconstructed video ˆX R(nrt+1)3HW is obtained."
        },
        {
            "title": "3.3 FINITE SCALAR QUANTIZATION",
            "content": "Variational AutoEncoders (VAEs) (Kingma & Welling, 2014) are class of generative models that map each data point, such as an image, from complex dataset into continuous distribution within latent space, rather than assigning it to single deterministic point. Conversely, the decoder performs the inverse operation, mapping representations from the latent space back to the original input space. However, due to the increasing demand for discrete latent variables, Vector Quantised-Variational AutoEncoder (VQ-VAE) (Van Den Oord et al., 2017) were introduced. Unlike standard VAEs, VQ-VAEs map inputs to finite set of vectors (i.e., codebook), through process known as vector quantization. This approach represents each input by the closest vector in the codebook, which is learned during training. By combining the generative capabilities of VAEs with the advantages of discrete representations, VQ-VAEs provide robust framework for various machine learning applications, including data compression, representation learning, and generative modeling. In this work, we employ Finite Scalar Quantization (FSQ) (Mentzer et al., 2024) to generate discrete tokens. The central principle of FSQ is that each scalar entry in the latent representation is independently quantized to the nearest pre-defined scalar value through rounding. In contrast to Vector Quantization (VQ), FSQ eliminates the need for codebook learning, thereby improving training stability (Mentzer et al., 2024; Yu et al., 2024). The approach can be described as follows: Given vector = (z1, z2, ..., zd) with channels, each channel zi is mapped to value in finite set of pre-defined values, resulting in quantized representation ˆz, which is one of Ld possible vectors. An example is shown in Fig. 4, where d=3 and L=5, representing an implicit codebook with size Ld = 125. Notably, when is set to 2, each zi can take one of two possible values, yielding binary latents. This mechanism corresponds to the Lookup-Free Quantization (LFQ) method proposed in MAGVIT-v2 (Yu et al., 2024). The experiments in Sec. 4.3.2 show that FSQ has significant advantages in codebook utilization, reconstruction quality and training stability, functioning as an advanced quantization technique that effectively improves discrete tokenizers. 3."
        },
        {
            "title": "IMPROVED TRAINING STRATEGIES",
            "content": "Training video tokenizers is often computationally intensive, requiring substantial resources (e.g., 3, 072 GPU hours for 256 256 resolution videos). This necessitates the development of efficient strategies to reduce computational costs while maintaining model performance. In this work, we implement two-stage training approach to address this challenge: the full model is initially pre-trained on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Specifically, the model is first trained from scratch using videos at 128 128 resolution. In the second stage, the decoder is fine-tuned using videos at 256 256 resolution. The experimental results presented in Tab. 4 demonstrate that the proposed two-stage training strategy achieves performance comparable to training the model from scratch on 256 256 resolution videos, while substantially"
        },
        {
            "title": "VidTok",
            "content": "reducing computational costscutting training time by half, from 3, 072 GPU hours to 1, 536 GPU hours. Furthermore, since the encoder remains unchanged, the fine-tuned model retains compatibility with the latent space of the pre-fine-tuned model. This ensures that the model can adapt efficiently to novel domains without impacting the integrity of models trained on the same latent space. Moreover, as the video tokenizer is designed to model the motion dynamics of input videos, it is essential to efficiently represent these dynamics within the model. In this study, we empirically observe that training with data at reduced frame rates significantly enhances the models capability to capture and represent motion dynamics. This finding is substantiated through the experimental results presented in Tab. 4 and Fig. 6, which illustrate the improved reconstruction quality achieved with lower frame rate training data."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "This section verifies the proposed VidTok through comparative experiments with existing state-of-the-art video tokenizers (Yu et al., 2024; Wang et al., 2024; NVIDIA; Zhao et al., 2024; hpcaitech; PKU-YuanGroup; Yang et al., 2024c) and comprehensive ablation studies. Fig. 1 provides several radar charts for quick comparison."
        },
        {
            "title": "4.1 EXPERIMENTAL SETTING",
            "content": "Dataset and metrics. For training, we utilize self-collected video dataset, divided into two subsets based on video quality: (1) Training Set 1, comprising approximately 10 million low-resolution videos (e.g., 480p); and (2) Training Set 2, consisting of approximately 6 million high-resolution videos (e.g., 1080p). All videos in the dataset are natural videos characterized by diverse lighting conditions, motion patterns, and scenarios. For evaluation, we follow the protocol of MAGVIT-v2 (Yu et al., 2024) and use two benchmark datasets: the MCL-JCV dataset (Wang et al., 2016) and the validation set of web video dataset. Evaluation videos are resized to 256 256 with frame rate of 30 FPS. The video reconstruction performance of the models is assessed using four widely-used metrics: Peak Signalto-Noise Ratio (PSNR) (Hore & Ziou, 2010), Structural Similarity Index Measure (SSIM) (Wang et al., 2004), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018) and Frechet Video Distance (FVD) (Unterthiner et al., 2018). Implementation details. We implement video tokenizers with various settings, including both causal and non-causal cases, continuous and discrete latents, and different video compression ratios. All models are trained with four loss terms: reconstruction term, perceptual term, an adversarial term and regularization term. The first three terms follow the practice in Latent Diffusion Models (Rombach et al., 2022). For the regularization term, we use KL loss (Kingma & Welling, 2014) in continuous tokenizers, and entropy penalty and commitment losses in discrete tokenizers (Yu et al., 2024). In the first training stage, Training Set 1 is resized to resolution of 128 128 and used for initial model training. We train VidTok for 50, 000 steps with batch size 16. In the second stage, Training Set 2 is resized to 256 256 and employed for fine-tuning. We fine-tune the decoder for another 30, 000 steps with batch size 8. The frame rate of the training data is maintained at 3 frames per second (FPS) during both stages. We use Adam optimizer (Kingma & Ba, 2015) with constant learning rate of 1 105. The training is conducted on 8 NVIDIA 40G A100 GPUs with PyTorch (Paszke et al., 2019). Baselines. We compare our method with the following state-of-the-art solutions: (1) MAGVIT-v2 (Yu et al., 2024): discrete video tokenizer which maps videos to discrete latent space using the LFQ representation; (2) OmniTokenizer (Wang et al., 2024): discrete video tokenizer using VQ as the discrete representation; (3) CV-VAE (Zhao et al., 2024): continuous video tokenizer with latent space that aligns with the latent space of existing image VAEs; (4) Open-Sora-v1.2 (hpcaitech): an open-source project aimed at reproducing OpenAIs Sora which offers continuous video tokenizer; (5) Open-Sora-Plan-v1.2 (PKU-YuanGroup): another open-source project aimed at reproducing OpenAIs Sora; (6) CogVideoX (Yang et al., 2024c): continuous tokenizer that preserves greater amount of information by maintaining larger number of latent channels; (7) Cosmos-Tokenizer (NVIDIA): suite of continuous and discrete video tokenizers with various compression ratios. We conduct thorough experiments in Sec. 4.2, with aligned settings for all methods to guarantee fairness in comparison."
        },
        {
            "title": "VidTok",
            "content": "Table 1: Quantitative comparison with the state-of-the-art video tokenizers. All evaluated models are causal and have video compression ratio of 4 8 8. The input resolution for most models is 17 256 256, except for MAGVIT-v2, which is evaluated on 17 360 640 as reported in the original study. The sample rate of testing data is 30 FPS. We highlight the best and the second-best numbers in bold and underline respectively. Method Regularizer Param. MCL-JCV WebVid-Val PSNR SSIM LPIPS FVD PSNR SSIM LPIPS FVD MAGVIT-v2 OmniTokenizer Cosmos-DV Ours-FSQ Ours-FSQ - LFQ - 262, 144 26.18 VQ - 8, 192 51M 26.93 FSQ - 64, 000 101M 28.07 FSQ - 32, 768 157M 29.16 FSQ - 262, 144 157M 29.82 KL - 4chn CV-VAE KL - 4chn Open-Sora-v1.2 Open-Sora-Plan-v1.2 KL - 4chn KL - 4chn Ours-KL CogVideoX Cosmos-CV Ours-KL KL - 16chn AE - 16chn KL - 16chn 182M 28.56 393M 29.44 239M 29.07 157M 29.64 206M 33.76 101M 31.27 157M 35.04 - 0.841 0.743 0.854 0.867 0.823 0.766 0.839 0.852 0.930 0.817 0. 0.104 0.165 0.212 0.117 0.106 0.163 0.164 0.131 0.114 0.076 0.149 0.047 - 232.7 227.7 196.9 160.1 334.2 350.7 201.7 194.2 93.2 153.7 78. - 26.26 29.39 31.04 31.76 30.79 31.02 30.85 31.53 36.22 33.04 37.53 - 0.883 0.741 0.883 0.896 0.863 0.764 0.869 0.878 0.952 0.818 0. - 0.112 0.170 0.089 0.080 0.116 0.137 0.101 0.087 0.049 0.107 0.032 - 48.46 57.97 45.34 38.17 70.39 112.34 44.76 36.88 15.30 23.85 9."
        },
        {
            "title": "4.2 COMPARISON WITH BASELINES",
            "content": "To evaluate the advancements achieved by VidTok, we compare its performance against state-of-the-art models across various scenarios, encompassing both discrete and continuous tokenization approaches. The comprehensive comparison results are presented in Tab. 1. All performance metrics reported in the table, except for those of MAGVIT-v2 (Yu et al., 2024), are obtained through our own experiments conducted under an identical evaluation protocol to ensure consistency and fairness. For MAGVIT-v2, as the model is not publicly accessible, we reference the results reported in their original publication. It is important to note that these results were obtained on resolution of 17 360 640, differing from the 17 256 256 resolution used for the other models in our comparison. Compared to existing discrete tokenziers (Yu et al., 2024; Wang et al., 2024; NVIDIA), VidTok demonstrates significantly superior reconstruction performance, even when utilizing smaller codebook size (e.g., 32, 768). This highlights the effectiveness of our approach in discrete tokenization. In the context of continuous tokenization, VidTok achieves comprehensive improvements across all evaluation metrics, regardless of whether the latent representation comprises 4 or 16 channels. Notably, these advancements are achieved even with smaller model size, surpassing the performance of state-of-the-art methods (Zhao et al., 2024; hpcaitech; PKU-YuanGroup; Yang et al., 2024c; NVIDIA). These results underscore the effectiveness of VidTok in both discrete and continuous tokenization tasks. We present the corresponding visual reconstruction results in Fig. 5 for qualitative comparison. From these visual results, our method exhibits distinct advantage in detail reconstruction fidelity and subjective viewing experience."
        },
        {
            "title": "4.3 ABLATION EXPERIMENTS",
            "content": "We conduct comprehensive ablation experiments to validate the superiority of the proposed model architecture, the advanced quantization technique and the improved training strategies. All ablation experiments are conducted with video compression ratio of 4 8 8 and an input size of 17 256 256, evaluated on MCL-JCV (Wang et al., 2016)."
        },
        {
            "title": "4.3.1 ABLATION ON THE MODEL ARCHITECTURE",
            "content": "To evaluate the effectiveness of our proposed model architecture, we compare it with three alternative variants in terms of computational complexity and reconstruction quality. (1) Variant 1 employs fully 3D architecture, integrating spatial and temporal sampling using 3D convolutions. (2) Variant 2 separates spatial and temporal"
        },
        {
            "title": "VidTok",
            "content": "Figure 5: Qualitative comparison with the state-of-the-art video tokenizers."
        },
        {
            "title": "VidTok",
            "content": "Table 2: Ablation study on the model architecture. Variant 1: fully 3D architecture. Variant 2: w/o AlphaBlender. Variant 3: w/o 3D architecture. We use KL - 4chn as regularizer for all settings. Method Variant 1 Variant 2 Variant 3 Ours Param. 245M 142M 126M 157M FLOPs 16.98 7.17 10.18 10.35 PSNR SSIM LPIPS FVD 29.39 29.36 29.26 29. 0.847 0.846 0.846 0.852 0.117 0.119 0.120 0.114 176.9 185.7 200.6 194.2 Table 3: Analysis of the impact of discrete techniques on model performance. R.L. denotes Regularization Loss, while U.R. represents Utilization Rate. Regularizer w/ R.L. PSNR SSIM LPIPS FVD U.R. VQ - 262,144 VQ - 262, LFQ - 262,144 LFQ - 262,144 FSQ - 262,144 FSQ - 262,144 FSQ - 32,768 FSQ - 4,096 - 23. 23.91 28.04 29.75 29.82 29.16 28.36 - 0.657 0.688 0.833 0.866 0.867 0.854 0.832 - 0. 0.251 0.133 0.109 0.106 0.117 0.133 - 960.5 619.8 208.1 167.5 160.1 196.9 218.1 - 0.2% 4.2% 99.9% 99.8% 99.8% 100.0% 100.0% sampling, but does not incorporate the AlphaBlender operator for temporal sampling. (3) Variant 3 replaces all 3D convolutions with 2D convolutions. The experimental results, summarized in Tab. 2, provide insights into the trade-offs between model performance and computational efficiency. The results indicate that employing fully 3D architecture (Variant 1) results in high computational complexity and model size. By modifying the architecture to replace 3D convolutions in the spatio-temporal sampling modules with combination of 2D and 1D convolutions (Variant 2), we achieve significant reduction in computational load without notable degradation in reconstruction quality. Building upon Variant 2, the introduction of the AlphaBlender operator for temporal sampling yields substantial improvements across most metrics, albeit with slight increase in computational cost. Furthermore, replacing all 3D convolutions with 2D convolutions (Variant 3) leads to marked decline in reconstruction performance, underscoring the importance of retaining 3D convolutions for effective spatio-temporal representation. Overall, the findings in Tab. 2 highlight the efficacy of the proposed architecture, which strikes balance between computational efficiency and reconstruction performance."
        },
        {
            "title": "4.3.2 ABLATION ON THE DISCRETE TECHNIQUES",
            "content": "In Tab. 3, we present comparison of various quantization methods, including VQ (Van Den Oord et al., 2017), LFQ (Yu et al., 2024), and FSQ (Mentzer et al., 2024). Additionally, we analyze the impact of the regularization loss term on the performance of discrete tokenizers. The results highlight several key observations. Traditional VQ suffers from common challenges, such as training instability and codebook collapse, which lead to extremely low codebook utilization and suboptimal In contrast, LFQ and FSQ achieve nearly 100% codebook utilization by directly reconstruction quality. optimizing an implicit codebook, resulting in significantly enhanced tokenizer performance. Furthermore, FSQ outperforms LFQs binary quantization by achieving better reconstruction fidelity, suggesting reduced information loss during the quantization process. The effects of regularization loss vary across quantization methods. For conventional VQ, the absence of regularization loss leads to model collapse and convergence failure. In the case of LFQ, while the model remains capable of convergence without regularization, it experiences marked decline in codebook utilization and reconstruction performance. FSQ, on the other hand, demonstrates superior training stability, with its performance remaining largely unaffected even in the absence of the regularization loss term."
        },
        {
            "title": "VidTok",
            "content": "Table 4: Ablation study on the proposed training strategy. To ensure fair comparison, both stages use training data from Training Set 1. Across all configurations, the regularizer KL - 4chn is employed. The training computational cost, measured in GPU hours, is evaluated using NVIDIA A100 GPUs. Sample Rate First Stage Second Stage Fix Enc. PSNR SSIM LPIPS FVD GPU Hours 3 FPS 3 FPS 3 FPS 3 FPS 8 FPS 256 256 128 128 128 128 128 128 128 128 - - 256 256 256 256 256 256 - - 29.19 29.02 29.15 29.21 29.02 0.843 0.838 0.842 0.843 0.839 0.127 0.130 0.127 0.125 0.126 174.9 221.7 203.2 189.8 219.2 3,072 960 1,728 1,536 1, Figure 6: The influence of different sample rates on model performance during training. The second row presents the test results obtained using training data with sample rate of 8 FPS, while the third row shows the test results using training data with sample rate of 3 FPS. The results demonstrate that employing training data with reduced frame rates enhances the models capacity to effectively capture motion dynamics. In summary, FSQ emerges as highly effective quantization technique, offering significant advantages in codebook utilization, reconstruction quality, and training stability. These attributes position FSQ as an advanced method for enhancing the performance of discrete tokenizers."
        },
        {
            "title": "4.3.3 ABLATION ON THE TRAINING STRATEGIES",
            "content": "As detailed in Sec. 3.4, we employ two-stage training strategy: pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. To evaluate the efficiency and effectiveness of this approach, we conduct an ablation study, with results summarized in Tab. 4. In the first row, training the full model on high-resolution videos directly from scratch requires 3,072 GPU hours. In contrast, the results in the fourth row demonstrate that the proposed two-stage training strategystarting with low-resolution data and then fine-tuning on high-resolution datareduces training time by half (from 3,072 to 1,536 GPU hours) while achieving comparable reconstruction quality. comparison between the third and fourth rows reveals that fine-tuning only the decoder during the second stage produces similar performance to fine-tuning the entire model, with lower computational cost. This approach also ensures that the low-resolution and high-resolution models share unified latent space due to the fixed encoder, enabling latent models trained in this shared space to be reused across resolutions and domains. Additionally, the last row examines the impact of varying the sampling rate during training. Qualitative results, presented in Fig. 6, indicate that using training data with reduced frame rates enhances the models ability to represent motion dynamics effectively."
        },
        {
            "title": "4.4 MODEL SUMMARY",
            "content": "We provide comprehensive summary of model performance in Tab. 5, covering both continuous and discrete tokenization, various video compression ratios, and causal versus non-causal scenarios."
        },
        {
            "title": "VidTok",
            "content": "Table 5: Model summary. We offer suite of models with diverse configurations, encompassing both continuous and discrete tokenization, various video compression ratios (VCR), and options for causal and non-causal scenarios. These configurations are designed to address the distinct requirements of various downstream tasks. Regularizer Causal Input Size VCR Latent Size Param. PSNR SSIM LPIPS FVD KL - 4chn KL - 4chn KL - 4chn KL - 4chn KL - 8chn KL - 16chn KL - 8chn KL - 4chn FSQ - 4,096 FSQ - 32,768 FSQ - 262,144 FSQ - 262,144 FSQ - 262,144 FSQ - 262,144 4 8 8 17 256 256 5 32 32 17 256 256 4 16 16 5 16 16 16 256 256 4 32 32 16 256 256 4 16 16 4 16 16 4 8 8 17 256 256 17 256 256 17 256 256 17 256 256 4 8 8 4 8 8 2 8 8 4 4 5 32 32 5 32 32 9 32 32 5 64 64 4 8 8 4 8 8 4 8 8 5 32 32 17 256 256 5 32 32 17 256 256 17 256 256 5 32 32 17 256 256 4 16 16 5 16 16 16 256 256 4 32 32 16 256 256 4 16 16 4 16 16 4 8 8 157M 29.64 199M 25.05 158M 30.60 199M 26.06 157M 31.83 157M 35.04 149M 33.86 155M 34. 157M 28.36 157M 29.16 157M 29.82 199M 25.38 157M 30.78 199M 26.37 0.852 0.711 0.876 0.751 0.897 0.942 0.928 0.941 0.832 0.854 0.867 0.738 0.889 0.772 0.114 0.228 0.098 0.190 0.083 0.047 0.057 0. 0.133 0.117 0.106 0.206 0.091 0.171 194.2 549.1 157.9 423.2 109.3 78.9 80.7 87.2 218.1 196.9 160.1 430.1 132.1 357.0 From Tab. 5, it is evident that as the video compression ratio increases, reconstruction performance deteriorates. Non-causal models generally outperform causal ones, as they are able to capture more extensive temporal information, which aids in the high-fidelity reconstruction of fine details. In the continuous case, increasing the number of channels in the latent representation allows for the retention of more information, leading to better reconstruction performance. Similarly, in the discrete case, larger codebook size usually means smaller quantization errors, preserving more accurate information and thus achieving better reconstruction fidelity. comparison between the continuous and discrete cases reveals that FSQ with codebook size of 262, 144 achieves reconstruction performance comparable to KL - 4chn. Additionally, we compare the performance across different settings: 1) KL - 16chn, with video compression ratio of 4 8 8; 2) KL - 8chn, with video compression ratio is 2 8 8; 3) KL - 4chn, with video compression ratio is 4 4 4. Our analysis indicates that when the latent space contains the same amount of data, allocating it to the channel dimension tends to result in relatively better reconstruction performance. All models and source code associated with this work are publicly available at https://github.com/ microsoft/VidTok. We aspire for this contribution to serve as foundation for and inspire further advancements in this research domain."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we present VidTok, versatile and open-source video tokenizer that achieves state-of-the-art performance in both continuous and discrete tokenization. By converting raw visual data into compact latent tokens, VidTok provides an efficient foundation for tasks related to visual generation and understanding. Through the incorporation of advancements in model architecture, discrete representation, and training strategies, VidTok surpasses existing methods, demonstrating notable improvements across several performance metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation protocols. Additionally, we conduct extensive ablation experiments to thoroughly investigate the performance characteristics of the video tokenizer. We hope this work will inspire future research in this area."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We extend our gratitude to all individuals who contributed their insights and efforts to this project, including Chong Luo, Ruoyu Feng, and Zhipeng Huang for their valuable discussions, and Qi Dai for his guidance on the open-source process."
        },
        {
            "title": "REFERENCES",
            "content": "Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. Uniedit: unified tuning-free framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Chameleon. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinghua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for improving latent video diffusion model. arXiv preprint arXiv:2409.01199, 2024a. Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, and Jiang Bian. Igor: Image-goal representations are the atomic control units for foundation models in embodied ai. arXiv preprint arXiv:2411.00785, 2024b. CompVis. latent-diffusion. https://github.com/CompVis/latent-diffusion. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In International Conference on Learning Representations, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022b. Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pp. 23662369. IEEE, 2010. hpcaitech. Open-sora. https://github.com/hpcaitech/Open-Sora. HuggingFace. open-muse. https://github.com/huggingface/open-muse. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In International Conference on Machine Learning, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. In International Conference on Learning Representations, 2024."
        },
        {
            "title": "VidTok",
            "content": "NVIDIA. Cosmos-tokenizer. https://github.com/NVIDIA/Cosmos-Tokenizer. OpenAI. Sora. https://openai.com/index/sora/, 2024. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction. arXiv preprint arXiv:2401.01808, 2024. PKU-YuanGroup. Open-sora-plan. https://github.com/PKU-YuanGroup/Open-Sora-Plan. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In International Conference on Learning Representations, 2023. Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on circuits and systems for video technology, 22(12):16491668, 2012. TencentARC. Open-magvit2. https://github.com/TencentARC/Open-MAGVIT2. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in neural information processing systems, volume 30, 2017. Haiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang, Ioannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. Mcl-jcv: jnd-based h. 264/avc video quality assessment dataset. In 2016 IEEE international conference on image processing (ICIP), pp. 15091513. IEEE, 2016. Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In International Conference on Learning Representations, 2024a. Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Position: Video as the new language for real-world decision making. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 5646556484. PMLR, 2127 Jul 2024b. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024c."
        },
        {
            "title": "VidTok",
            "content": "Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023. Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. In International Conference on Learning Representations, 2024. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, and Jiang Bian. Video in-context learning. arXiv preprint arXiv:2407.07356, 2024a. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024b. Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan. Cv-vae: compatible video vae for latent generative video models. arXiv preprint arXiv:2405.20279, 2024. Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022. Hanxin Zhu, Tianyu He, Anni Tang, Junliang Guo, Zhibo Chen, and Jiang Bian. Compositional 3d-aware video generation with llm director. In Advances in neural information processing systems, 2024."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Peking University",
        "Shanghai Jiao Tong University"
    ]
}