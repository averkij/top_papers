{
    "paper_title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
    "authors": [
        "Wenqi Dong",
        "Bangbang Yang",
        "Zesong Yang",
        "Yuan Li",
        "Tao Hu",
        "Hujun Bao",
        "Yuewen Ma",
        "Zhaopeng Cui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scene-level 3D generation represents a critical frontier in multimedia and computer graphics, yet existing approaches either suffer from limited object categories or lack editing flexibility for interactive applications. In this paper, we present HiScene, a novel hierarchical framework that bridges the gap between 2D image generation and 3D object generation and delivers high-fidelity scenes with compositional identities and aesthetic scene content. Our key insight is treating scenes as hierarchical \"objects\" under isometric views, where a room functions as a complex object that can be further decomposed into manipulatable items. This hierarchical approach enables us to generate 3D content that aligns with 2D representations while maintaining compositional structure. To ensure completeness and spatial alignment of each decomposed instance, we develop a video-diffusion-based amodal completion technique that effectively handles occlusions and shadows between objects, and introduce shape prior injection to ensure spatial coherence within the scene. Experimental results demonstrate that our method produces more natural object arrangements and complete object instances suitable for interactive applications, while maintaining physical plausibility and alignment with user inputs."
        },
        {
            "title": "Start",
            "content": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation Wenqi Dong1,2* Tao Hu2 Bangbang Yang2* Zesong Yang1,2 Yuan Li1 Yuewen Ma Zhaopeng Cui1 Hujun Bao1 1Zhejiang University 2ByteDance 5 2 0 2 7 1 ] . [ 1 2 7 0 3 1 . 4 0 5 2 : r Figure 1. HiScene allows users to generate scene-level 3D assets with natural layout and appealing looking, while delivering compositional items for versatile applications such as interactive editing and simulation."
        },
        {
            "title": "Abstract",
            "content": "Scene-level 3D generation represents critical frontier in multimedia and computer graphics, yet existing approaches either suffer from limited object categories or lack editing flexibility for interactive applications. In this paper, we present HiScene, novel hierarchical framework that bridges the gap between 2D image generation and 3D object generation and delivers high-fidelity scenes with compositional identities and aesthetic scene content. Our key insight is treating scenes as hierarchical objects under isometric views, where room functions as complex object that can be further decomposed into manipulatable items. This hierarchical approach enables us to generate 3D content that aligns with 2D representations while maintaining compositional structure. To ensure completeness and spatial alignment of each decomposed instance, we develop video-diffusion-based amodal completion technique that effectively handles occlusions and shadows between objects, and introduce shape prior injection to ensure *Authors contributed equally. Corresponding author. Work done during an internship at PICO, ByteDance. spatial coherence within the scene. Experimental results demonstrate that our method produces more natural object arrangements and complete object instances suitable for interactive applications, while maintaining physical plausibility and alignment with user inputs. More details at https://zju3dv.github.io/hiscene/. 1. Introduction Recently, we have witnessed remarkable breakthroughs of generative techniques in both 2D and 3D content creation, which empowers users to create stunning images and intricate 3D objects with simple text prompts. However, extending such generation capabilities to scene-level 3D contentfor instance, generating complete 3D room models with coherent geometry and appearance guided by userprovided text or imagesremains significant challenge. Recent approaches typically rely on large language models (LLMs) with handcrafted rules to generate 3D scene layouts and place objects accordingly, yet these results often lack realism (e.g., exhibiting constrained object diversity and simplistic arrangements) due to LLMs limited spatial understanding capabilities [66]. Others attempt to lift 2D images into 3D scenes using depth-based mesh deformation [13, 16]. While these approaches offer category-free generation capabilities, they typically produce scenes as inseparable wholes, limiting interactive applications such as scene editing, object manipulation, and data curation for robotic semantic understanding. Based on these observations, we believe an ideal scene generation method should have the following properties: 1) Realistic layout & assets: it should generate scenes with natural object arrangements and diverse content beyond just few simple objects from limited categories. The scenes should reflect real-world spatial relationships and object interactions. 2) Compositional & complete instances: for interactive applications, each object in the scene should be complete, intact 3D entity that can be individually manipulated, edited, or replaced without disrupting the entire scene. 3) Spatial alignment & plausibility: the generated scene should faithfully represent the users text or image prompt while maintaining physical coherence and plausibility in the 3D space (e.g., without distorted shapes, unrealistic proportions, or floating placement). In this paper, we propose novel hierarchical scene generation framework, named HiScene. Rather than determining how scenes are built in 3D space with handcrafted rules, we leverage the complementary knowledge embedded in image generation models about how scenes should appear with aesthetic appeal and reasonable layout, and instantiate the concrete 3D representation that aligns with the image in top-down manner. Our key insight is that we can treat scene as hierarchical level objects under the isometric view. From the generators perspective, room can be seen as complex object itself, while each individual item within the room can also be separately generated and manipulated. By leveraging this hierarchical approach, HiScene bridges the gap between object-level and scene-level generation, producing complete scenes that benefit from pre-trained objectcentric generation priors while maintaining compositional structure. While the hierarchical scene generation approach is technically plausible, we identify several challenges in creating high-fidelity and compositional 3D scenes that we address in this paper. Hierarchical Scene Parsing. To bootstrap the hierarchical scene generation, we first initialize the entire scene with pre-trained 3D generation model [70] from the given isometric view image. Once obtaining the complete 3D Gaussian representation [23] of the scene, key challenge is to accurately isolate individual objects from the scene structure. To address this, we implement hierarchical scene parsing approach based on analysis by synthesis to identify distinct objects and prepare them for subsequent generative refinement. Specifically, we render multi-view images and perform 3D identity segmentation using 2D segmentation priors enhanced with contrastive learning techniques [78]. Then, for each identified object, we render object-centric circular views and carefully identify occlusion regions, enabling us to understand the spatial relationships between objects and more effectively reconstruct complete object identities in the following steps. Instance Refinement with Video-diffusion-based Amodal Completion. key challenge during identity refinement is that the rendered instance views often exhibit significant occlusions. Despite advances in 3D object generation, reconstructing complete objects from occluded views remains ill-posed, while directly applying standard inpainting methods often produces implausible results due to limited object understanding (see Fig. 7). Moreover, the target instance might also include ambient shadow caused by the foreground occluder, which cannot be addressed with conventional inpainting frameworks. To tackle this problem, we reformulate the instance refinement as 2D amodal completion and 3D regeneration task, and propose novel video-diffusionbased completion framework to handle it. Specifically, our approach treats the amodal completion process as temporal transition video effect, where occlusions gradually dissolve to reveal the complete object. To enable this capability, we construct specialized dataset for training video diffusion model to perform such completion transitions. The temporal nature of our video-based completion effectively handles challenging cases including occlusion shadow removal, outperforming static image-based inpainting or completion methods by preserving structural coherence and producing more plausible results even in complex scenarios. Spatial Aligned Generation. Even with advanced 3D generation models to refine each segmented identity, ensuring spatial alignment between the refined object and its original placement is non-trivial due to the unposed nature of compressed latent [70]. As result, naıvely applying refinement with amodally completed object views might produce objects with variant shapes, making them incompatible with the original scene layout. To address this issue, we propose shape prior injection mechanism that conditions the refinement stage of each identity. Specifically, we first extract geometric shape prior from view-aligned generation method [72], and use this aligned shape prior as the latent initialization for our refinement pipeline rather than starting from random noise. This approach significantly reduces geometric ambiguity during refinement and ensures proper spatial alignment between the generated objects and the original scene context. The contribution of the paper can be summarized as follows. 1) We propose HiScene, novel scene-level generation method that produces high-fidelity 3D scenes with compositional identities, natural scene arrangement and diverse content. HiScene bridges the gap between 2D and 3D generation by leveraging isometric views, enabling effective hierarchical scene-level generation with pre-trained 3D object generation models. 2) We develop an analysis-by-synthesis 2 approach for scene parsing with zero-shot 3D semantic segmentation, and introduce video-diffusion-based amodal completion method that effectively handles occlusions and ambient shadows during instance refinement. 3) We design spatial alignment mechanism with shape prior injection that ensures refined objects maintain proper geometric alignment with the original scene context, ensuring the object coherence and physical plausibility of the compositional 3D scene. 4) Extensive experiments demonstrate the effectiveness of our approach, demonstrating the superior performance of video-diffusion-based amodal completion in handling complex occlusions and shadows, and the high-quality scene decomposition and object refinement across various challenging scenarios. 2. Related Work 2.1. 3D Object Generation Motivated by the recent advances of diffusion techniques in 2D image generation, numerous works [9, 34, 36, 46, 53, 61, 65, 67, 77] are being made to apply diffusion models to 3D object generation. DreamFusion [53] first attempts to distill 2D gradient priors from the denoising process by employing score distillation sampling loss (SDS loss). Follow-up methods aim to enhance both the quality [9, 36, 46, 65, 67] and efficiency [34, 61, 77] of this method. Zero123 [41] constructs paired viewpoint data on the large-scale dataset Objaverse [15] and fine-tunes the 2D latent diffusion model to achieve arbitrary novel view synthesis and 3D generation under single-image condition. Later works [42, 43, 59, 60, 64] address the issue of cross view inconsistency in Zero123 by synchronously generating multiview images. To enhance 3D object generation efficiency, Large Reconstruction Model (LRM) [17] employ transformer-based architecture that directly reconstructs 3D objects from single-view image through feed-forward inference. Some methods [31, 40, 62, 71, 72, 74] incorporated multi-view information as input, significantly improving generation quality. Recent native 3D generation model [11, 28, 32, 70, 85, 87, 90, 91] such as Shape2Vecset [85], CLAY [87], and TRELLIS [70] adopt decoupled strategy, dividing the process into geometric structure generation and texture generation stages. These methods train generative model directly on 3D data rather than traditional multi-view data, substantially improving the geometric accuracy and consistency of generated results. Our method use existing native 3D generation model, treating scenes as hierarchical level of objects to ultimately achieve interactive scene generation. monocular depth estimators [4, 6] to iteratively generate textured 3D meshes or Gaussian Splatting scenes by inpainting from randomly sampled camera angles. However, these methods typically produce coupled scenes where object instances are difficult to isolate. Some methods [35, 48, 50, 63, 75, 81, 82] first generate 3D scene layouts, then obtain individual objects through retrieval or generation methods, placing them within the scene according to the generated layout. BlockFusion [69] and its follow-up methods [8, 45] adopt native 3D scene generation strategy, dividing the scene into multiple blocks and expanding these blocks into complete scene during generation. However, these two types of methods are often limited to specific categories from training data. Recent methods [29, 33, 88, 93] explore interactive general scene generation using Large Language Models (LLMs) to construct 3D layouts, while generating individual objects through Score Distillation Sampling-based optimization. However, LLMs still lack sufficient spatial understanding capabilities, making it difficult to generate complex and physically plausible 3D layout structures. By contrast, our method adopts top-down hierarchical generation that ensures global layout and appearance coherence while maintaining the separability of individual objects. 2.3. Image Inpainting and Amodal Completion Image Inpainting and Amodal Completion are classic problems in computer vision. Image inpainting aims to restore masked regions in images with reasonable and natural content, requiring specification of the inpainting area. Traditional methods [5, 14, 39, 51, 89] offen relied on auxiliary hand-engineered features with often poor results. Recent approaches [2, 3, 10, 19, 20, 38] utilize diffusion models for text-guided completion, typically regenerating content in masked regions while preserving the rest of the image. In contrast, Amodal Completion aims to generate the complete form of an object from its visible view. Traditional methods [18, 21, 22, 37, 54, 57, 83] focused on generating complete segmentation masks or predicting bounding boxes, while recent zero-shot diffusion-based method [49, 73] further recover the complete content. We adopt Amodal Completion rather than Image Inpainting because Image Inpainting requires manually specified inpainting regions, whereas in Amodal Completion, the observed regions can be automatically obtained through existing segmentation networks, which is crucial for our automated generation of candidate images for each object during the hierarchical scene parsing stage. 3. Method 2.2. Text-to-3D Scene Generation Text-conditioned 3D scene generation has advanced rapidly in recent years. Text2Room [16] and follow-up works [13, 79, 80, 86, 92] leverage latent diffusion models [58] and We present HiScene, novel hierarchical framework for generating compositional 3D scenes with intact and manipulatable objects. As illustrated in Fig. 2, we first initialize 3D Gaussian Splatting scene from generated isometric view. 3 Figure 2. Overview of HiScene. Our hierarchical framework generates 3D scenes with compositional identities through three main stages. First, we create 3D scene from generated isometric view. Next, we perform scene parsing to obtain precise object segmentation, followed by multi-view rendering and detailed occlusion analysis for each identified instance. Finally, we apply our video-diffusion-based amodal completion to generate complete views of each instance, which serve as guidance for regenerating intact objects with proper spatial alignment in the scene. The resulting 3D scene features fully compositional identities, facilitating user-directed modifications like interactive scene editing. revealing multiple faces of objects simultaneously while minimizing occlusion between scene elements. 3) Sceneas-object: The unified representation of scenes in isometric view allows the entire scene to be treated as cohesive entity, enabling direct generation with object-centric generative models. Native 3D Generation Model. We leverage native 3D generation models TRELLIS [70] to bootstrap hierarchical scene generation. TRELLIS introduces unified Structured LATent (SLAT) representation to characterize 3D asset O, as: = {(zi, pi)}L i=1, zi RC , pi {0, 1, . . . , 1}3, (1) TRELLIS employs two-stage generation pipeline. In the first stage, it generates the sparse structure {pi}L i=1 of by first using transformer model GS to produce lowresolution feature grid RDDDCS, followed by latent feature decoder DS to obtain dense binary 3D grid {0, 1}N N . The grid is then converted to the set of 3D coordinates {pi}L i=1. In the second stage, TRELLIS uses another transformer model GL to generate the corresponding structure features {zi}L i=1 for these coordinates. The complete SLAT representation is then processed through specialized decoders (DNeRF, DMesh, or DGS) to produce the final 3D asset in various formats (NeRF, meshes, or 3DGS). Figure 3. Comparison of perspective view and isometric view of living room scene. Zoom in for more details. We then perform hierarchical scene parsing with semantic segmentation to identify distinct objects and obtain each object multi-view rendering and occlusion analysis. Finally, we conduct video-diffusion-based amodal completion to address object occlusion and generate intact, spatially-aligned objects that enable interactive scene manipulation. 3.1. Preliminary Isometric View. In computer graphics, isometric view is an orthographic projection method used to render 3D scenes into images. Isometric view offers three key advantages for scene-level generation: 1) Distortion-free: Unlike perspective projection, isometric view maintains consistent proportions without perspective distortion, ensuring accurate object representation. 2) Minimal-occlusion: As shown in Figure 3, isometric view captures scenes from an elevated angle, 3.2. Hierarchical scene parsing We define an interactive scene = {{Oi, Ci}N i=1} containing multiple separable complete objects {Oi}N i=1 represented by 3DGS, with their configurations Ci = {pi, ri, si, li}, where each configuration includes position pi R3, rotation ri SO(3), scaling si R3, and semantic label li. To obtain this, as shown in Figure 2, we first perform hierarchical scene parsing. Scene Initialization Given users text prompt , we first generate isometric view candidate images by prepending fixed prefix Isometric view of to the . Then, for the selected scene image Iscene, we obtain the initial scene representation S0 through TRELLIS. We adopt 3D Gaussian Splatting as our scene representation method. 3D Semantic Segmentation. We employ OmniSeg3DGS [78], contrastive learning-based semantic segmentation method for 3DGS. Specifically, after obtaining the initial representation S0, we render multi-view images = {Ij}NV j=1 from predefined viewpoints {Vj}NV j=1 , where NV is the number of scene views. Unlike OmniSeg3D-GS, which aims to achieve omniversal segmentation, our goal is to obtain separated objects {Oi}N i=1. Therefore, instead of using SAM [25], we employ EntitySeg [55], an off-the-shelf entity segmentation network, to generate class-agnostic instancelevel 2D segmentation masks = {Mj}M j=1 for the multiview images. Object-Centric Multiview Rendering. As demonstrated in Figure 2, due to the occlusion between objects in 3D scenes, after 3D segmentation, some objects (such as tables, croissants, coffee cups, etc.) typically appear incomplete. To further recover complete objects, we apply the following method to each object Oi in the scene: First, in the objects local coordinate system, we render multiview images {I j=1 , where NV is the number of object views. These rendered images will be served as candidate images for further object-centric instance regeneration. Occlusion Analysis. After obtaining candidate images {I j=1 for each object Oi, we need to evaluate whether Oi is occluded by other objects in these images. For each candidate image j, we employ an advanced Vision Language Model (VLM) for occlusion detection. If occlusion is detected, we calculate the occlusion ratio of the target object. When this ratio exceeds threshold τ , we apply Amodal Completion to recover the visual information in the occluded regions, thereby enhancing the quality and completeness of the candidate images. For more details, please refer to the supplementary materials. j=1 from predefined viewpoints {V }NV j}NV j}NV 3.3. Amodal Completion Task Definition. As discussed in the Section 2.3, unlike image inpainting, amodal completion task aims to recover 5 Figure 4. We present an data curation example of amodal completion, including original image (a), occluded input image (b), visible mask (c), and the linear blended video (d). We also present shadow-aware data examples (e). the complete and plausible form of the object when provided with an input image and visible mask (i.e., objects visible part as shown in Figure 4 (c)). Unlike existing method Pix2gestalt[49] that fine-tune image generation models, we define this task as temporal transition video effect, where occlusions gradually dissolve to reveal the complete object. Video models, trained on large-scale high-quality data, can learn temporal changes in the real world, thus possessing stronger prior knowledge that helps more accurately infer the complete form of occluded parts. Dataset curation. During object completion, apart from filling occluded parts, we need to remove notable visual artifacts caused by occlusion, such as shadows. Objects directly segmented from the SA-1B [24] dataset cannot meet the requirements for constructing data with shadow effect. To this end, we constructed large-scale dataset of objects with shadow occlusions using synthetic data. By filtering the Objaverse dataset [15], we obtained 181K high-quality 3D objects. For each object, we utilized rigid body simulation to naturally place objects on the ground, ensuring realistic shadow effects. Additionally, we configured random lighting setups and employed the path-tracing renderer to generate 468K synthetic images. Integrating the data from Pix2gestalt, we ultimately built training set comprising 1.32 million image pairs. Then, we convert image pair into the required video data. As shown in Figure 4 (d), we adopt linear blending approach to put foreground object over the complete objects in image planes. As shown in Figure 2, we employ the Image-to-Video model Stable Video Diffusion [7]. Specifically, the visible part Ivis is encoded through VAE and used as the first Figure 5. An illustration of Spatial Aligned Generation. We use sparse-view LRM to initialize spatial aligned shape prior (voxel latent), and inject this prior by initializing voxel noises upon it during native 3D generation, thus ensuring regenerated assets adhering the original scene. frame input, concatenated with the downsampled visible region mask along the feature dimension. The whole image is processed through CLIP to extract features, which are then injected into the model via cross-attention. During training, we add noise ϵ (0, I) to the original data x0 to obtain xt = 1 αtϵ, and then use the network to predict the noise ϵθ(xt, t), minimizing the following loss function: αtxt1 + Ldm = Ex,ϵN (0,1),t (cid:2)ϵ ϵθ(xt, t, I, Ivis, )2(cid:3) , (2) Once the Amodal completion model is trained, we apply it to object candidate images with occlusion, which recovers intact image inputs for Spatial Aligned Generation stage. 3.4. Spatial Aligned Generation After obtaining objects occlusion-free views, we aim to regenerate each object to achieve intact instances while preserving their original scale and poses. However, directly applying regeneration using native 3D generative models [70] FNative often results in canonical objects that lose alignment with the original scene context. To address this limitation, we propose injecting spatially aligned shape priors derived from multi-view large reconstruction models FLRM [72] (refer to it as LRM for clarity) into the native 3D generation process. As shown in Figure 5, for each incomplete objects Oi, we first use FLRM to reconstruct an initial geometric structure Sinit from the input observation views and their corresponding camera parameters [ri, pi], and obtain an explicit voxel representation through voxelization. Subsequently, we employ the encoder ES from the TRELLIS structure generation 6 stage to compress into low-dimensional latent feature Simplicit. This coarse 3D structure representation serves as guidance for the subsequent refinement process. Specifically, we use Simplicit as the voxel latent initialization and add noise corresponding to an intermediate timestep of the rectified flow model (typically choosing [0.2, 0.4]), rather than starting generation from pure random noise. The generator GS then produces an optimized structure (cid:98)S. Under the guidance of Simplicit, the finally generated (cid:98)S not only preserves the configuration of the initial geometry but also significantly enhances geometric details and texture quality. 4. Experiments In this section, we first demonstrate methods capabilities of text to interactive 3D scene generation in Sec. 4.1. Next, we evaluate the effectiveness of our method in amodal completion task in Sec. 4.2. Finally, we conduct ablation studies to analyze the different components within our framework in Sec. 4.3. 4.1. Interactive Scene Generation We compare our method with two state-of-the-art decoupable scene generation methods: GALA3D [93] and DreamScene [29]. GALA3D employs large language models for generating initial layouts, integrates layout-guided Gaussian representation and adaptive geometry control, and utilizes compositional optimization mechanism. DreamScene introduces Formation Pattern Sampling (FPS) to balance semantic information and shape consistency, and three-stage camera Table 1. We perform quantitative evaluation and user studies on the Interactive Scene Generation task. Table 2. Comparisons with zero-shot methods."
        },
        {
            "title": "Method",
            "content": "Ours GALA3D [93] DreamScene [29] Aesthe. Score [47] 5.46 ImageReward [68] -0.28 CLIP Score% [56] 26.07 Matching Degree 2.90 Overall Quality 2.76 4.74 -1.67 23.50 1.76 1.75 4.71 -0.73 21. 1.40 1.73 sampling strategy to improve the quality of scene generation. However, both methods require predefined 3D layouts as input, which presents significant barrier for novice users who may find creating reasonable layouts challenging. Large language models often make errors in layout generation as well. As shown in Figure 6, our method addresses these limitations by providing more intuitive and user-friendly approach to 3D scene generation without requiring explicit layout specifications. Qualitative Comparison In Figure 6, we present both complete generated scenes and individual objects. As shown, scenes and objects generated by GALA3D and DreamScene exhibit artifacts. The layouts produced by these methods often violate physical constraints and common sense spatial relationships. Additionally, individual objects frequently suffer from oversaturation and the Janus problem. In contrast, our method generates complex yet plausible scenes with individual objects of significantly higher quality compared to the other approaches. Quantitative Analysis To quantitatively evaluate our method, we employ CLIP Score to assess text-scene alignment, and use ImageReward and Aesthetic Score to evaluate the overall generation quality. As shown in Table 1, our method achieves the best overall performance. DreamScenes more severe multi-face object issues negatively impact its overall scores. Our approach demonstrates superior performance across all metrics, confirming the effectiveness of our layout-free scene generation paradigm. User Study We also conducted user study to compare our method against existing approaches. The evaluation focused on two aspects: text-scene alignment and overall quality. We collected 12 different scenes and asked 20 users to rate them on scale from 1 to 3, with higher scores indicating better results. As shown in Table1, our method achieved the highest ratings, confirming the superior performance of our approach from human perception perspective. 4.2. Amodal Completion We assess the performance of amodal segmentation with existing zero-shot methods. Following [49, 84], we evaluate segmentations on Amodal COCO (COCO-A) [94] and Amodal Berkeley Segmentation (BSDS-A) [44] datasets us7 Zero-shot Method COCO-A BSDS-A SAM [25] SD-XL Inpainting [52] Pix2gestalt [49] Ours 60.27 70.08 82.59 83.84 60.20 66.57 79.59 79.80 ing mean intersection-over-union (mIoU). The COCO-A dataset offers 13,000 amodal annotations across 2,500 images, while the BSDS-A dataset includes 650 objects from 200 images. For both datasets, we evaluate methods that take an image and (modal) mask of the visible portion of an object as input, and produce an amodal mask representing the full extent of the object. We use the same method as in pix2gestalt [49] to convert the amodal completions into semantic masks. We compared our approach with the state-of-the-art method pix2gestalt and two other zero-shot methods, as shown in Table 2. Our method achieves state-of-the-art performance on both datasets, demonstrating that our video model-based approach more effectively recovers occluded objects, resulting in superior segmentation outcomes. We also conducted qualitative experiments on everyday scenes, as illustrated in Figure 7. Our method successfully recovers occluded objects and effectively removes shadows caused by occlusions. While pix2gestalt can reconstruct reasonable shapes, it often produces darkened textures in shadowed regions, likely due to the absence of shadow considerations in its training data. SD-XL Inpainting tends to be influenced by mask boundaries, frequently generating completions that conform to the mask but are semantically unreasonable. 4.3. Ablation Studies Image vs. video model in amodal completion. To qualitatively compare the capabilities of image and video models, we trained both types of models using data constructed by Pix2gestalt on the SA-1B dataset [25]. We evaluated the overall quality of the generated completions using Aesthetic Score, Q-Align IAA, and IQA metrics, and measured textimage alignment using CLIP Score. As shown in Table 3, under the same data settings, the video model outperforms the image model across all metrics. We attribute this superior performance to the video models powerful prior knowledge of object continuity and temporal consistency, which enables it to better understand and complete occluded objects with more coherent and realistic results. Shadow-aware completion for object generation. As illustrated in Figure 8, our observation of the chair is incomplete due to occlusions. To demonstrate the importance of proper amodal completion, we first conducted an ablation experiment where we attempted object generation without amodal Figure 6. We compare the Interactive Scene 3D generation with GALA3D and DreamScene. completion. Since object generation models are trained on complete observations, when presented with partial inputs, the model generates incorrect geometric structures based on the incomplete mask contours, often resulting in black textures in the missing regions. Similarly, when amodal completion is performed but shadow artifacts remain, the generated results still exhibit the aforementioned black geometric errors. Our shadow-aware amodal completion method effectively addresses these issues by properly handling both occlusions and shadows, resulting in geometrically accurate and visually coherent object reconstructions. Spatial Alignment. We finally evaluate the effectiveness of our spatial aligned generation by comparing it with two alternatives: direct native 3D generation (FNative only) and standalone LRM generation (FLRM only). As shown in Figure 9, without spatial alignment, native 3D generation produces Table 3. We evaluated the effectiveness of the video model. Datasets & Method COCO-A BSDS-A I2I I2V I2I I2V Aesthe. Score [47] Q-Align IAA% [68] Q-Align IQA% [68] CLIP Score% [56] 4.30 4.16 23.48 12.65 45.80 35.43 20.27 20.78 4.38 4.17 14.59 23.46 35.06 44.42 20.63 21.20 objects with incorrect orientation and positioning relative to the ground-truth instance, while LRM generation alone results in compromised appearance fidelity. By leveraging LRMs spatial alignment capabilities as shape prior for native 3D generation, our approach achieves precise scale Figure 7. In-the-wild Amodal Completion and Segmentation. Figure 8. We analyzed the necessity of shadow-aware amodal completion. Figure 9. We analyze the effectiveness of Spatial Aligned Generation. and pose matching with the original scene while preserving rich appearance details and visual quality. 5. Conclusion In this paper, we have presented HiScene, novel hierarchical framework for generating compositional 3D scenes. By treating scenes as hierarchical compositions of objects under isometric views, we enable effective scene-level synthesis using pretrained object generation models. To ensure completeness of each object identities, we use video-diffusion9 based amodal completion and spatial alignment to aid the regeneration, ensuring spatial coherence within the scene. Limitations and future works. Despite our advances, scenes generated by HiScene have textures with baked lighting, lacking PBR materials for modern rendering pipelines. Future work will focus on training the generative model to generate scenes that support PBR textures."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 14 [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. 3 [3] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42(4): 111, 2023. 3 [4] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Irondepth: Iterative refinement of single-view depth using surface normal and its uncertainty. arXiv preprint arXiv:2210.03676, 2022. 3 [5] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 417424, 2000. 3 [6] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 3 [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 5, 16 [8] Alexey Bokhovkin, Quan Meng, Shubham Tulsiani, and Angela Dai. Scenefactor: Factored latent 3d diffusion for controllable 3d scene generation. arXiv preprint arXiv:2412.01801, 2024. [9] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 3 [10] Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024. 3 [11] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling high-quality 3d asset generation via primitive diffusion. arXiv preprint arXiv:2409.12957, 2024. 3 [12] Seokhun Choi, Hyeonseop Song, Jaechul Kim, Taehyeong Kim, and Hoseok Do. Click-gaussian: Interactive segmentation to any 3d gaussians. In European Conference on Computer Vision, pages 289305. Springer, 2024. 14 [13] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 2, 3 [14] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Region filling and object removal by exemplar-based image inpainting. IEEE Transactions on image processing, 13(9): 12001212, 2004. [15] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 3, 5, 15 [16] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 79097920, 2023. 2, 3 [17] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 3 [18] Cheng-Yen Hsieh, Tarasha Khurana, Achal Dave, and Deva Ramanan. Tracking any object amodally. CoRR, 2023. 3 [19] Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, and Houqiang Li. Smarteraser: Remove anything from images using maskedregion guidance. arXiv preprint arXiv:2501.08279, 2025. 3 [20] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pages 150168. Springer, 2024. 3 [21] Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jitendra Malik. Amodal completion and size constancy in natural scenes. In Proceedings of the IEEE international conference on computer vision, pages 127135, 2015. [22] Lei Ke, Yu-Wing Tai, and Chi-Keung Tang. Deep occlusionaware instance segmentation with overlapping bilayers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 40194028, 2021. 3 [23] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 5, 15 [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 5, 7 [26] Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, and Timo Ropinski. Open3dsg: Open-vocabulary 3d scene graphs from point clouds with queryable objects and open-set relationships. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1418314193, 2024. 14 10 [27] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. [28] Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, and Chen Change Loy. Gaussiananything: Interactive point cloud latent diffusion for 3d generation. arXiv preprint arXiv:2411.08033, 2024. 3 [29] Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Peng Yuan Zhou. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation In European Conference on Computer pattern sampling. Vision, pages 214230. Springer, 2024. 3, 6, 7 [30] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. arXiv preprint arXiv:2005.04966, 2020. 14 [31] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 3 [32] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 3 [33] Xiao-Lei Li, Haodong Li, Hao-Xiang Chen, Tai-Jiang Mu, and Shi-Min Hu. Discene: Object decoupling and interaction modeling for complex scene generation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 3 [34] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65176526, 2024. [35] Chenguo Lin and Yadong Mu. Instructscene: Instructiondriven 3d indoor scene synthesis with semantic graph prior. arXiv preprint arXiv:2402.04717, 2024. 3 [36] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, MingYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution textIn Proceedings of the IEEE/CVF to-3d content creation. conference on computer vision and pattern recognition, pages 300309, 2023. 3 [37] Huan Ling, David Acuna, Karsten Kreis, Seung Wook Kim, and Sanja Fidler. Variational amodal object completion. Advances in Neural Information Processing Systems, 33:16246 16257, 2020. 3 [38] Anji Liu, Mathias Niepert, and Guy Van den Broeck. Image inpainting via tractable steering of diffusion models. arXiv preprint arXiv:2401.03349, 2023. 3 [39] Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong Han, and Jing Liao. Pd-gan: Probabilistic diverse gan for In Proceedings of the IEEE/CVF conimage inpainting. ference on computer vision and pattern recognition, pages 93719381, 2021. [40] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1007210083, 2024. 3 [41] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9298 9309, 2023. 3 [42] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 3 [43] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. 3 [44] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings eighth IEEE international conference on computer vision. ICCV 2001, pages 416423. IEEE, 2001. 7 [45] Quan Meng, Lei Li, Matthias Nießner, and Angela Dai. Lt3sd: Latent trees for 3d scene diffusion. arXiv preprint arXiv:2409.08215, 2024. [46] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1266312673, 2023. 3 [47] Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: large-scale database for aesthetic visual analysis. In 2012 IEEE conference on computer vision and pattern recognition, pages 24082415. IEEE, 2012. 7, 8 [48] Basak Melis Ocal, Maxim Tatarchenko, Sezer Karaoglu, and Theo Gevers. Sceneteller: Language-to-3d scene generation. In European Conference on Computer Vision, pages 362378. Springer, 2024. 3 [49] Ege Ozguroglu, Ruoshi Liu, Dıdac Surıs, Dian Chen, Achal Dave, Pavel Tokmakov, and Carl Vondrick. pix2gestalt: In 2024 Amodal segmentation by synthesizing wholes. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 39313940. IEEE Computer Society, 2024. 3, 5, 7, 15, 16 [50] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:1201312026, 2021. 3 [51] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Generating diverse structure for image inpainting with hierarchical vq-vae. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1077510784, 2021. [52] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. 11 Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 7 [53] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [54] Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia. In ProAmodal instance segmentation with kins dataset. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30143023, 2019. 3 [55] Lu Qi, Jason Kuen, Weidong Guo, Tiancheng Shen, Jiuxiang Gu, Jiaya Jia, Zhe Lin, and Ming-Hsuan Yang. High-quality entity segmentation. arXiv preprint arXiv:2211.05776, 2022. 5, 14 [56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 7, [57] Dinesh Reddy, Robert Tamburo, and Srinivasa Narasimhan. Walt: Watch and learn 2d amodal representation from time-lapse imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93569366, 2022. 3 [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [59] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 3 [60] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 [61] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3 [62] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. [63] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nießner. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2050720518, 2024. 3 [64] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. 3 [65] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1261912629, 2023. 3 [66] Yian Wang, Xiaowen Qiu, Jiageng Liu, Zhehuan Chen, Jiting Cai, Yufei Wang, Tsun-Hsuan Johnson Wang, Zhou Xian, and Chuang Gan. Architect: Generating vivid and interactive 3d scenes with hierarchical 2d inpainting. Advances in Neural Information Processing Systems, 37:6757567603, 2024. 1 [67] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. 3 [68] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 7, [69] Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, et al. Blockfusion: Expandable 3d scene generation using latent tri-plane extrapolation. ACM Transactions on Graphics (TOG), 43(4):117, 2024. 3 [70] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 2, 3, 4, 6 [71] Chao Xu, Ang Li, Linghao Chen, Yulin Liu, Ruoxi Shi, Hao Su, and Minghua Liu. Sparp: Fast 3d object reconstruction and pose estimation from sparse views. In European Conference on Computer Vision, pages 143163. Springer, 2024. 3 [72] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2, 3, 6 [73] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Amodal completion via progressive mixed context diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90999109, 2024. 3 [74] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. In European Conference on Computer Vision, pages 120. Springer, 2024. 3 [75] Haitao Yang, Zaiwei Zhang, Siming Yan, Haibin Huang, Chongyang Ma, Yi Zheng, Chandrajit Bajaj, and Qixing Huang. Scene synthesis via uncertainty-driven attribute synchronization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 56305640, 2021. 3 [76] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. In European Conference on Computer Vision, pages 162179. Springer, 2024. [77] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings 12 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67966807, 2024. 3 tion via co-modulated generative adversarial networks. arXiv preprint arXiv:2103.10428, 2021. 3 [90] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems, 36:7396973982, 2023. 3 [91] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. [92] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. In European Conference on Computer Vision, pages 324342. Springer, 2024. 3 [93] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided generative gaussian splatting. arXiv preprint arXiv:2402.07207, 2024. 3, 6, 7 [94] Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr Dollar. Semantic amodal segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 14641472, 2017. 7 [78] Haiyang Ying, Yixuan Yin, Jinzhi Zhang, Fan Wang, Tao Yu, Ruqi Huang, and Lu Fang. Omniseg3d: Omniversal 3d segmentation via hierarchical contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2061220622, 2024. 2, 5, 14 [79] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d arXiv preprint scene generation from single image. arXiv:2406.09394, 2024. 3 [80] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. 3 [81] Guangyao Zhai, Evin Pınar Ornek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, and Benjamin Busam. Commonscenes: Generating commonsense 3d indoor scenes with scene graph diffusion. Advances in Neural Information Processing Systems, 36:3002630038, 2023. [82] Guangyao Zhai, Evin Pınar Ornek, Dave Zhenyu Chen, Ruotong Liao, Yan Di, Nassir Navab, Federico Tombari, and Benjamin Busam. Echoscene: Indoor scene generation via information echo over scene graph diffusion. In European Conference on Computer Vision, pages 167184. Springer, 2024. 3 [83] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. Amodal ground truth and completion in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2800328013, 2024. 3 [84] Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, and Chen Change Loy. Self-supervised scene de-occlusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37843792, 2020. 7 [85] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. 3 [86] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 30(12):77497762, 2024. 3 [87] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating highquality 3d assets. ACM Transactions on Graphics (TOG), 43 (4):120, 2024. 3 [88] Qihang Zhang, Chaoyang Wang, Aliaksandr Siarohin, Peiye Zhuang, Yinghao Xu, Ceyuan Yang, Dahua Lin, Bolei Zhou, Sergey Tulyakov, and Hsin-Ying Lee. Towards text-guided In Proceedings of the IEEE/CVF 3d scene composition. Conference on Computer Vision and Pattern Recognition, pages 68296838, 2024. [89] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric Chang, and Yan Xu. Large scale image comple13 HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we describe more training details of Hierarchical Scene Parsing in Sec. and Amodal Completion in Sec. B. We provide more details on Spatial Aligned Generation in Sec. and Sec. D. We provide detailed explanation of our user study in Sec. E. We also conducted Runtime Evaluation in Sec. F. More qualitative results can be found in our supplementary video. Source code and dataset will be released upon the acceptance of this paper. A. Hierarchical Scene Parsing Details A.1. Predefined Viewpoints collection of multi-view images {Ii}M i=1 with their corresponding 2D semantic masks {Mi}M i=1. OmniSeg3D-GS lifts class-agnostic 2D segmentation to 3D space through contrastive learning. To obtain complete 3D instance segmentation results, we employ EntitySeg [55] for 2D segmentation. First, each Gaussian primitive in the initialized scene S0 is assigned an optimizable feature vector hg R16. The optimization phase follows an iterative approach, randomly selecting an image in each iteration and sampling points from it, while determining the patch id for each point. Subsequently, differentiable rendering techniques are used to compute the render features {fi}(i [1, ]) for each sampled point. Within the contrastive learning framework, points sharing the same patch id are treated as positive samples, while the remaining points are considered negative samples. To enhance computational efficiency and ensure stable convergence, OmniSeg3D-GS applies contrastive clustering method [30]. For set of point features sharing the same patch id i, defined as cluster {f i}, with mean feature representation i. The contrastive clustering loss is defined as: Figure 10. We provide examples of predefined viewpoints. In the 3D Semantic Segmentation stage, we render multiview images of the initial scene S0 from predefined viewpoints . As shown in the Figure 10, for scenes of type (a), we position camera viewpoints on sphere with fixed radius, with azimuth angles uniformly sampled at 8 points within [5, 95] and elevation angles uniformly sampled at 5 points within [10, 90]. For scenes of type (b), the azimuth angles span [0, 360). The camera viewpoint set can be represented as = {(r, θi, ϕj) θi Θ, ϕj Φ}, where = 2 is the sphere radius, and Θ and Φ are the sampling sets for azimuth and elevation angles, respectively. Through experimental validation, we found that the specific configuration and number of rendering viewpoints do not significantly affect 3D segmentation performance. As shown in Figure 2 of the main paper, during the objectcentric novel view synthesis stage, we fix the elevation angle at ϕ = 30 and uniformly select 16 viewpoints with azimuth angles θ [0, 360). All cameras are directed toward the center of the target object. A.2. 3D Semantic Segmentation Existing methods [12, 26, 76, 78] support 3DGS semantic segmentation, and we adopt OmniSeg3D-GS [78]. Given 14 LCC = 1 Np Np (cid:88) {f i} (cid:88) i=1 j=1 log f i/ϕi) exp(f k=1 exp(f (cid:80)Np k/ϕk) , (3) where represents the render feature with point index and patch id i. Np represents the total number of patch ids, and ϕi is the temperature parameter for the cluster, used to balance cluster size and variance, calculated as: ϕi = (cid:80)ni i2 j=1 ni log(ni + α) , ni = {f i} (4) where α = 10 is smoothing parameter that prevents small clusters from producing excessively large ϕi values. For more details, please refer to the original paper. A.3. Occlusion Analysis To achieve better generation results, the image condition for LRM and TREELIS models should ideally include complete multi-view observations of the target object. In the Predefined Viewpoints setting, we render 16 candidate images around the object with 360-degree view. We uniformly divide the 360-degree view into 4 regions = {R1, R2, R3, R4}, with each region Ri containing 4 images. For each region Ri, we randomly select one image and use vision-language model (VLM) [1] to determine whether the object is occluded. Figure 11 shows an example prompt used for occlusion analysis. When occlusion is detected in an image, we calculate the occlusion ratio ρ, defined as: ρ = Aother Aother + Atarget (5) where Aother represents the area of other objects and Atarget represents the area of the target object. When the occlusion ratio ρ < 0.4, we apply amodal completion model to process the image. Otherwise, we discard the image and randomly select another image from the same region for evaluation. If all images in region Ri fail to meet the requirements, we proceed with only the qualifying images from other regions for subsequent processing. B. Amodal Completion Implementation Details B.1. Dataset Preparation Our large-scale amodal completion dataset is derived from two sources. Part of the data comes from previous work [49], which is processed and annotated based on the SA-1B [24] dataset. Readers can refer to the original paper for more details. Additionally, to effectively handle visual effects (such as shadows) produced by occluding objects in real-world scenarios, we synthesized additional training data containing natural shadow effects. High-quality Shadow Data Synthesis Based on Objaverse. Objaverse [15], as large-scale diverse dataset containing over 800K 3D assets, provided us with rich 3D model resources. However, there are numerous low-quality models in this dataset, which often have overly simple geometric structures or missing textures. Therefore, we first strictly filtered Objaverse, ultimately obtaining approximately 181K high-quality 3D models for subsequent processing. To generate realistic shadow effects, we used Blender* Cycles rendering engine for high-quality rendering. In realworld scenarios, objects are typically placed on the ground or other supporting surfaces rather than floating in space, which is crucial for shadow formation. Simple coordinate normalization cannot solve the problem of object-ground contact, so we adopted physics simulation approach: first normalizing the 3D models to the [1, 1]3 spatial range, then adding Rigid Body Constraints to the object, and simulating the natural process of objects falling to the ground under gravity. In Figure 12, we present an example. We set the simulation time to 200 seconds and used the objects posture in the final stable state for rendering to ensure natural contact between the object and the ground. Lighting Setup. To simulate diverse real-world lighting conditions, we primarily used two types of light sources in Blender during the rendering process: Sun Light and Area Light. Sun Light simulates parallel light rays produced by distant light sources, with controllable lighting effects *https://www.blender.org Figure 11. We provide an example of using VLM to determine whether the target object is occluded. through parameters such as Strength and Angle; Area Light simulates light emitted from plane, achieving different lighting effects by adjusting parameters such as Size and Power. As outlined in Algorithm1, for each rendering, we randomly selected one of these light source types and randomly set relevant parameters to enhance data diversity. After rendering, we adopted method similar to previous 15 Figure 12. (a) Visualization of the Rigid Body Dynamics (RBD) process; (b) and (c) illustrate the shadow effects under different lighting setup; (d) demonstrates that incorrect shadow effects when object are not pre-processed with RBD. Algorithm 1 Randomized Lighting Setup Ensure: Scene with randomized lighting configuration 1: Sample U(0, 1) 2: if > 0.2 then 3: 4: 5: 6: 7: /* Area Light Setup */ Sample radius U(4.0, 6.0) Sample energy U(800, 1200) Sample size U(0.8, 1.2) Sample elevation angle θ U(40, 89.9) Sample azimuth angle ϕ U(0, 360) Add area light L(R, E, S, θ, ϕ) /* Sun Light Setup */ Sample sun angle α U(0.1, 0.5) Add primary sun light with angle α and energy 5.0 for {1, 2, 3} do Add sun light with angle α and energy 3.0 Rotate light by {90, 180, 270}[i 1] around x-axis 8: 9: 10: else 11: 12: 13: 14: 15: 16: 17: 18: end if end for Algorithm 2 Occlusion-to-Visibility Transition Generation Require: Mv: visible mask, Mw: whole mask, Io: target RGB image, : interpolation frames Ensure: X: video data 1: Md Mv Mw {Difference mask} 2: Iv Io Mw {Visible RGB} 3: Initialize R(N +1)chw {Output tensor} 4: X0 normalize(resize(Io, h)) {First frame} 5: for {0, 1, ..., 1} do 6: 7: Mi Md αi {Weighted mask} 8: 9: Xi+1 normalize(resize(Ii, h)) 10: end for 11: return Ii blend(Iv, Iw, Mi) {Blended image} 1 {Blending coefficient} αi = 1 method [49], overlaying 3D object images with shadows onto complete scene images from SA-1B to form image pairs containing occlusion relationships. Through the above processing, we ultimately constructed large-scale amodal completion dataset containing approximately 1.32 million data pairs, of which about 468K data pairs include natural shadow effects, providing rich training resources for the model to learn to process complex occlusion scenarios. As shown in Figure 12, the shadow effects under different lighting are illustrated. B.2. Network Architecture In Section 4, we proposed amodal completion network based on Stable Video Diffusion [7] (SVD) . This section elaborates on its network architecture design. Stable Video Diffusionis an image-conditioned video generation network. Trained on massive video datasets, given an input image I, it can generate temporally consistent video from I. The SVD architecture consists of three core components: video encoder E, decoder D, and UNet diffusion model Φ. To fully leverage the prior knowledge embedded in the pre-trained model, we optimize the network structure following the principle of minimal modification. Specifically, the input image is first compressed through the encoder to obtain an image embedding representation. Simultaneously, the object observation mask undergoes downsampling to maintain consistent resolution with the image embedding. In the original SVD architecture, the image embedding is concatenated with latent noise; in our improved version, we incorporate the mask into the concatenation process, providing additional information to better guide the video generation process. The image is also processed through CLIP to extract image features, which are injected via crossattention. B.3. Training Details In the model training process, we initialized our model with the pre-trained weights from SVD [7]. To accommodate the additional mask information, we expanded the input Algorithm 3 Spatial Aligned Generation Require: Observation views with camera parameters [ri, pi], timestep range [tmin, tmax] Ensure: Generated 3D structure (cid:98)S 1: /* Initial Structure Generation */ 2: Generate initial geometric structure Sinit FLRM(I, [ri, pi]) 3: Obtain explicit voxel representation through voxelization of Sinit 4: Compress into latent feature Simplicit ES(V ) 5: /* Shape Prior Injection */ 6: Select intermediate timestep [tmin, tmax] {Typically [0.2, 0.4]} 7: Initialize xt Simplicit + noise(t) {Add noise corresponding to timestep t} 8: Set number of discretization steps 9: /* Rectified Flow Sampling Process */ 10: for =INT(N 1) down to 0 do Compute current time ti, step size 11: Predict velocity vθ(xti, ti) using trained model GS 12: Update sample xi1 = xi vθ(xi, ti) 13: 14: end for 15: Set generated structure (cid:98)S x0 16: return (cid:98)S channels of thn illustration of the use diffusion model Φ from 8 to 9. In our experiments, video data was processed at resolution of 512 512 with 8 frames per sequence. Training was conducted using fp16 precision with learning rate of 1 105 and incorporated 500-step warmup phase. All parameters of Φ were fine-tuned during this process. We used batch size of 8 and trained on 8 Nvidia H20 GPUs, with the entire training process taking approximately 3 days. C. Spatial Aligned Generation Here we provide an algorithm 3 for spatial aligned generation. D. Background Structure Generation For each scene, we regenerate the background by modifying the original scene prompt to begin with background of..., then integrate the resulting background structure back into the resulting scene. E. User Study Our user study evaluates the generated 3D scenes from two dimensions: image-text matching degree and overall scene quality. The matching degree assesses how well the generated 3D scene aligns with the input text description, while the overall quality comprehensively considers factors such as scene quality, reasonableness of object layout, and the quality of individual object models. As shown in Figure 13, we designed an intuitive user study interface where participants can quantitatively score the generated results. The scoring standard ranges from 1 to 3 points, with higher scores indicating better quality. Through this approach, we can qualitatively analyze the practical effectiveness of the generated results. Figure 13. An illustration of the user study interface. F. Runtime Evalution As shown in Figure 2 of the main paper, given text prompt, HiScene first utilizes FLUX [27] to generate candidate images within 4 seconds, followed by TRELLIS to complete scene initialization in 5 seconds. Next, HiScene renders multi-view images and performs 2D semantic segmentation in 5 seconds. In the 3D Gaussian semantic segmentation stage, we optimize 5000 steps, taking approximately 2 minutes. After obtaining the initial 3D GS representation of objects, the system spends less than 1 second rendering object candidate images and completes occlusion analysis in 30 seconds. For each candidate image with occlusion, amodal completion requires 6 seconds. As illustrated in Figure 5 of the main paper, during the Spatial Alignment stage for each object, we first utilize Sparse view LRM to obtain initial geometric structures in 2 seconds, followed by voxelization in less than 1 second, and generate low-resolution feature grid using ES in 1 second. Finally, the system takes 3 seconds for structure generation and structure latents generation. Overall, HiScene processes complete scene in approximately 12 minutes. In contrast, methods relying on SDS Loss optimization such as GALA3D and DreamScene require significantly more time. GALA3D needs 2 hours to generate scene, while DreamScene requires more than 1 hour. This comparison clearly demonstrates HiScenes significant advantage in efficiently generating interactive 3D scenes. 17 18 Figure 14. We show examples of our synthetic dataset. Figure 15. More examples of generated scenes. All prompts have fixed prefix Isometric view of . 19 Figure 16. More examples of amodal completion."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Zhejiang University"
    ]
}