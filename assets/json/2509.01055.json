{
    "paper_title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
    "authors": [
        "Dongfu Jiang",
        "Yi Lu",
        "Zhuofeng Li",
        "Zhiheng Lyu",
        "Ping Nie",
        "Haozhe Wang",
        "Alex Su",
        "Hui Chen",
        "Kai Zou",
        "Chao Du",
        "Tianyu Pang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool."
        },
        {
            "title": "Start",
            "content": "Work in Progress VERLTOOL: TOWARDS HOLISTIC AGENTIC REINFORCEMENT LEARNING WITH TOOL USE Dongfu Jiang, Yi Lu, ς Zhuofeng Li, Zhiheng Lyu, ϱPing Nie, ϑHaozhe Wang, Alex Su, εHui Chen, φKai Zou, Chao Du, Tianyu Pang, Wenhu Chen University of Waterloo, Sea AI Lab, University of Toronto, ς Shanghai University, ϑHKUST, εNational University of Singapore, φNetMind.AI, ϱIndependent {dongfu.jiang@uwaterloo.ca, wenhuchen@uwaterloo.ca}"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to singleturn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multiturn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VERLTOOL, unified and modular framework that addresses these limitations through systematic design principles. VERLTOOL provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2 speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multiturn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool. 5 2 0 2 1 ] . [ 1 5 5 0 1 0 . 9 0 5 2 : r Figure 1: Overview of the VERLTOOL, modularized and efficient framework for the Agentic Reinforcement Learning with Tool Use (ARLT) training paradigm, where the RL workflow and tool execution are fully disaggregated for both efficiency and extensibility. Equal Contribution. Dongfu Jiang is the project leader. Corresponding Author. 1 Work in Progress"
        },
        {
            "title": "INTRODUCTION",
            "content": "We shape our tools, and thereafter our tools shape us. Marshall McLuhan Large language models (LLMs) such as OpenAIs O-series (Jaech et al., 2024) and DEEPSEEKR1 (Guo et al., 2025) have recently achieved striking advances, surpassing top human performers in challenging domains like mathematics (AIME) and programming (LIVECODEBENCH (Jain et al., 2024), CODEFORCES (Quan et al., 2025)). central driver of this progress is the paradigm of reinforcement learning with verifiable rewards (RLVR), which strengthens long-context reasoning during training. Through RLVR, LLMs exhibit emergent cognitive behaviors such as reflection, backtracking, and multi-step reasoning. Yet these systems remain constrained in fundamental way: they are unable to interact with the external world. Current LLM reasoning unfolds in closed, single-turn setting without environmental feedback, often leading to brittle behaviors such as overthinking (Chen et al., 2024a) or hallucination (Yao et al., 2025). Conceptually, these models resemble brain in vat, locked into self-contained simulations without grounding in interactive or physical reality. To overcome this isolation, parallel line of work has explored augmenting LLMs with the ability to use tools. Systems such as TOOLFORMER (Schick et al., 2023) and OPENHANDS (Wang et al., 2024b) extend models with supervised training on synthetic tool-use data, enabling practical interaction with code interpreters, search engines, or APIs. However, these approaches primarily rely on imitation learning. They lack the agentic autonomy needed to learn directly from feedback and to adaptively refine their behavior in open-ended environments. Recent research begins to bridge this gap by combining tool use with RLVR, giving rise to new paradigm we term ARLTAgentic Reinforcement Learning with Tool use. In ARLT, LLMs can actively engage with external tools such as code execution environments (Li et al., 2025c), search engines (Jin et al., 2025), image manipulators (Su et al., 2025), and domain-specific APIs (Feng et al., 2025). This interaction transforms training into multi-turn, feedback-rich process that not only improves efficiency and reduces token usage but also fosters more robust agentic behaviors. However, enabling ARLT poses significant challenges from systems perspective. First, rollout efficiency becomes critical: multi-tool trajectories unfold asynchronously, with different tools producing results at varying speeds, demanding scalable asynchronous execution. Second, tool management remains fragmented: existing RLAT codebases are often tailored to specific tools, making it difficult to extend or reproduce results. Finally, multimodal support is still underdeveloped: while most RL frameworks focus narrowly on text, emerging multimodal reasoning agents (e.g., PIXELREASONER (Su et al., 2025)) require handling tool outputs that include images, videos or other structured modalities in unified design. These barriers have slowed community progress, limiting reproducibility, extensibility, and algorithmic innovation. To address them, we introduce VERLTOOL: an open-source, user-friendly, and efficient framework built on top of VERL (Sheng et al., 2024), designed explicitly for ARLT that supports both text and multimodal training. Unlike prior systems, VERLTOOL enables multi-turn, stateful agentic training with tool use through four key contributions: Upstream Alignment. VERLTOOL inherits VERL as submodule, ensuring compatibility with upstream updates. This modular separation between RL training and agentic interaction simplifies maintenance and accelerates framework evolution. Unified Tool Management. We introduce dedicated tool server with standardized interaction APIs, supporting diverse tools such as code execution, search, SQL/tabular reasoning, and vision utilities. Adding new tool requires only lightweight Python definition file, streamlining extensibility for both training and evaluation. Asynchronous Rollouts. By interacting with tool servers on trajectory-by-trajectory basis rather than synchronously batch by batch, VERLTOOL eliminates idle waiting time. This design yields over 2 speedup during rollout execution. Diverse ARLT Tasks. We have implemented and tested VERLTOOL on six ARLT tasks, including Math, Search Retrieval, SQL, Visual Reasoning, Web Browsing, and SWE-Bench, achieving competitive performance with previous baselines while trained in unified framework. We also present common findings in the agentic RL setting across these tasks. Work in Progress In summary, VERLTOOL provides principled, extensible, and efficient framework for ARLT, bridging the gap between isolated LLM reasoning and interactive agentic intelligence. By combining upstream-aligned RL infrastructure, unified tool integration, asynchronous execution, and diverse tasks, it paves the way for scalable research and practical deployment of LLMs as tool-using agents."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 TOOL-INTEGRATED REASONING Augmenting Large Language Models (LLMs) with external tools has become prominent approach to address limitations in parametric reasoning and enable more complex task solving (Shen, 2024). Early approaches focused on prompt-based integration, where systems like HUGGINGGPT (Shen et al., 2023), CHAMELEON (Lu et al., 2023), and MULTITOOL-COT (Inaba et al., 2023) used tool manuals, demonstrations, or structured Chain-of-Thought templates to orchestrate sequential tool invocations. While these methods offer plug-and-play convenience, they face challenges when adapting to complex, multi-step tasks due to their reliance on static prompting strategies. complementary line of work explores instruction tuning, where models are explicitly trained to recognize tool formats and generate appropriate function calls. Representative systems include TOOLFORMER (Schick et al., 2023), which uses bootstrapped annotations to teach tool usage patterns, GPT4TOOLS (Yang et al., 2023), which distills tool-use traces from more capable models, and LIMO (Ye et al., 2025), which demonstrates that targeted examples can elicit extended reasoning chains. However, these supervised approaches primarily provide static guidance and lack mechanisms for dynamic error correction based on tool execution feedback. In contrast, reinforcement learning approaches enable models to develop adaptive tool-calling strategies through interaction-based training. Our work builds on this direction by employing GRPO training to enhance models capabilities for self-reflection and iterative refinement in response to tool feedback. 2.2 REINFORCEMENT LEARNING FOR AGENTIC TOOL USE The integration of reinforcement learning with tool use has emerged as powerful paradigm for developing adaptive LLM agents. Early tool-calling approaches relied on prompt-based orchestration (Yao et al., 2022; Lu et al., 2023; Shen et al., 2023), building on Chain-of-Thought reasoning (Wei et al., 2022) and multi-agent frameworks for training-free tool invocation. While instruction-tuned models (Schick et al., 2023; Kong et al., 2023; Gou et al., 2023) learned structured calling patterns through supervised learning, they remained largely static and limited to single-turn interactions. Recent work has demonstrated the advantages of reinforcement learning for tool use, enabling models to optimize their tool-calling policies based on execution outcomes and environmental feedback (Li et al., 2025c; Feng et al., 2025; Moshkov et al., 2025). This paradigm, which we refer to as Agentic Reinforcement Learning with Tool use (ARLT), extends beyond single-turn verification to support long-horizon, multi-turn interactions. Key characteristics of ARLT include: (1) credit assignment across sequential tool calls, (2) explicit handling of observation tokens from tool responses, and (3) integration with robust, failure-aware execution environments (Plaat et al., 2025; Ke et al., 2025). This shift from static instruction following to dynamic, feedback-driven learning has shown effectiveness across diverse domains, including mathematical reasoning with code execution, information retrieval, natural language to SQL generation, and visual reasoning tasks. These applications require agents to iteratively probe environments, adapt to tool feedback, and refine their strategiescapabilities that are difficult to achieve through purely supervised approaches. 2.3 AGENTIC RL TRAINING FRAMEWORKS The success of Reinforcement Learning from Verifier Rewards (RLVR) has motivated the development of various frameworks to support scalable RL training for language models. Established synchronous frameworks include OPENRLHF (Hu et al., 2024) and VERL (Sheng et al., 2024), 3 Work in Progress Table 1: Tool support comparison across different frameworks (up to update until August 23, 2025). RAGEN and ROLL focus on the puzzle environments like bandit, which we did not list here. Framework 3 (cid:219) Python FAISS Search Executor _ (cid:128) Bash Web Search Terminal Executor ı SQL º Image Processing OPENRLHF (Hu et al., 2024) VERL (Sheng et al., 2024) ROLL (Wang et al., 2025a) RAGEN (Wang et al., 2025b) SLIME (THUDM, 2024) AREAL (Fu et al., 2025) SKYRL (Cao et al., 2025) VERLTOOL(ours) which employ Ray-based distributed computing to manage training workflows. Additionally, fully asynchronous frameworks such as AREAL (Fu et al., 2025), ROLL (Wang et al., 2025a), and SLIME (THUDM, 2024) have emerged to address scalability challenges. As shown in Table 1, existing frameworks exhibit varying degrees of tool support. Traditional RL frameworks like OPENRLHF and VERL provide basic support for search and code execution tools but lack comprehensive multi-modal capabilities. ROLL focuses primarily on core RL training without extensive tool integration, while AREAL supports search functionality but has limited executor capabilities. SKYRL (Cao et al., 2025) offers broader tool support, including bash terminals and SQL executors, but requires complex containerized environments that introduce deployment overhead. The limited tool coverage in existing frameworks has led to the development of domainspecific systems (e.g., SEARCH-R1, PIXELREASONER, and TOOLRL) as task-specific extensions. However, these implementations typically feature hard-coded tool integrations that limit their extensibility and adaptability to new domains. As evident from Table 1, there remains need for frameworks that provide comprehensive, extensible support for diverse tool types while maintaining ease of deployment and development."
        },
        {
            "title": "3 VERLTOOL FRAMEWORK",
            "content": "In this section, we formulate the conceptual foundation of the ARLT paradigm, starting from the original RLVR setting. We then elaborate on how VERLTOOL serves as practical implementation on the server side for Agentic Reinforcement Learning with Tool use (ARLT). 3.1 PRELIMINARIES Reinforcement Learning with Verifiable Reward (RLVR). RLVR optimizes the language model using predefined verifiable reward via the following objective: max πθ ExD,yπθ(x) [Rϕ(x, y)] β DKL [πθ(y x) πref(y x)] , (1) where πθ denotes the policy LLM, πref is the reference LLM, Rϕ is the verifiable reward function, and DKL is the KL divergence. The input is drawn from the dataset D, and is the corresponding single-turn output. typical verifiable reward function is defined as: Rϕ(x, y) = (cid:26)1 1 if match(y, yg) otherwise (2) where yg is the ground-truth answer and match(, ) {1, 0} is verification function that determines whether the generated answer matches yg. This function can be implemented using either rule-based approaches (Wang et al., 2024b) or model-based verifiers (Ma et al., 2025). GRPO (Shao et al., 2024) is widely adopted RL algorithm designed to optimize the objective in Equation 1. In the single-turn RL case, the trajectory is simply the LLM generation τ = {y}. The 4 Work in Progress GRPO objective is given by: JGRPO(θ) = 1 (cid:88) i=1 1 τi τi (cid:88) t= (cid:104) ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:105) , min (3) where ri,t(θ) is the token-level importance ratio and ˆAi,t is the normalized advantage across all tokens: ri,t(θ) = πθ(τi,(t)τi,<t) πold(τi,(t)τi,<t) , ˆAi,t = Rϕ(x, y) mean({Rϕ(τ1), . . . , Rϕ(τG)}) std({Rϕ(τ1), . . . , Rϕ(τG)}) . (4) 3.2 AGENTIC RL TRAINING WITH TOOL USE In the agentic RL setting, rollouts are multi-turn instead of single-turn, and the agent can ARLT. interact with tools to receive external observations during the reasoning process. Thus, the trajectory can be written as τ = {a0, o0, . . . , an1, on1, an}, where ai denotes the LLM-generated action tokens and oi denotes the observation tokens returned by tool call. Here, is the total number of interaction steps. To determine whether an action ai invokes specific tool, we assume that each ai (for 0 < n) ends with stop token Sk, where Sk is the predefined set of stop tokens for tool Tk T. For example, SCI = {output, </python>} for code interpreter tool, or Ssearch = {</search>} for search tool. The complete set of stop tokens is the union over all invoked tools: = (cid:83)T Sk. The introduction of observation tokens oi makes ARLT fundamentally different from the agentic RL defined in RAGEN (Wang et al., 2025b), where the agent only receives scalar rewards through environmental interaction. Moreover, the observation tokens are off-policy with respect to the current LLM πθ being optimized, which can destabilize training (Jin et al., 2025). Therefore, these tokens are typically masked out during policy optimization. Let Tj be the token index of the first token in action segment aj, then GRPO loss for ARLT becomes: k=1 JGRPO-ARLT(θ) = 1 (cid:88) i=1 1 j=0 aj (cid:80)n (cid:88) Tj +aj (cid:88) (cid:104) min j=0 t=Tj ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:105) , (5) 3.3 FRAMEWORK DESIGN Table 2: Currently supported tools Tools 3 Code Interpreter Description Related works Execute Python code ToRL (Li et al., 2025c) (cid:219) Faiss Search Vector similarity search for documents Search-R1 (Jin et al., 2025) (cid:128) Web Search API Real-time web search and retrieval SimpleDeepSearch (Sun et al., 2025) º Image Processing Image resize, video frame selection PixelReasoner (Su et al., 2025) _ Bash Terminal Execute shell commands R2E-Gym (Jain et al., 2025) ı SQL Executor (cid:148) MCP Interface Database queries and data management SkySQL (Liu et al., 2025) Model Context Protocol for external tool ToolRL (Feng et al., 2025) Challenges. Building general RL training framework that supports various tools is inherently challenging due to the additional overhead introduced by tool interactions. Prior ARLT works like SEARCH-R1 (Jin et al., 2025) and PIXEL-REASONER (Su et al., 2025) typically implement custom code to support specific tool interaction scenarios, as VERL and OPENRLHF do not support well for tool use at that time. However, these ad-hoc practices face several limitations that hinder broader community adoption and extensibility. Work in Progress Math-TIR SQL DeepSearch Turns Sync (s) Async (s) Speed Up () 4 87 66 1.32 5 111 91 1.22 5 193 98 1. Table 3: Performance comparison of Synchronous vs Asynchronous approaches. Experiments conducted on 8 H100 GPUs. Figure 2: Visualization of the Async Rollout pipeline design. First, most existing works are designed around single tool and tightly couple the tool interaction logic with the core RL training loop, making it difficult for developers to extend or substitute tools (Li et al., 2025c). This fragmentation increases the development burden for researchers seeking to experiment with novel tools or multi-tool scenarios. Second, these systems often rely on synchronous rollout mechanisms that process trajectories batch by batch (Li et al., 2025c; Jin et al., 2025). In such settings, the tool interaction phase is triggered only after all actions ai in batch have been generated, resulting in idle bubbles and inefficient utilization of computational resources, especially on GPUs. To address these issues, we propose VERLTOOL, general-purpose ARLT framework designed to support various tools as modular plugins via unified API. Our goal is to minimize the integration overhead for community developers and provide more efficient and extensible infrastructure for training LLMs with tool-use capabilities. Overview. As shown in Figure 1, VERLTOOL adopts modular and decoupled architecture consisting of two main components: the Verl Workflow and the Tool Server, connected via unified API. This separation enables independent management of RL training and tool execution while preserving efficient communication between them. The Verl Workflow handles all reinforcement learning activities, including multi-turn rollouts and actor updates. The LLM actor interacts with the external environment by generating sequence of actions {a0, a1, . . . }, each potentially triggering tool interaction. Once an action is identified as tool-invoking (via matching predefined stop token), it is sent to the Tool Server along with auxiliary metadata. The observation oi returned by the tool is then appended to the rollout, enabling observation-aware agent behavior and reward computation. Asynchronous Rollout Design. key feature of VERLTOOL is its support for fully asynchronous rollouts, which avoids the inefficiency of traditional synchronous batch-based frameworks. In such setting, tool calls are processed only after the entire batch has completed generating their respective actions ai, resulting in idle \"bubbles\" in GPU and CPU utilization. In contrast, VERLTOOL enables each trajectory to interact with the tool server independently and immediately upon finishing its action generation, as shown in Fig. 2. This design ensures that tool execution latency does not block the entire batch, significantly improving throughput and system utilization in large-scale distributed settings. As shown in Figure 3, the actor and environment evolve concurrently, achieving near 2 times speedup for the rollout stage. Parallel Tool Server Backend. To support high-throughput and scalable execution of tool interactions, VERLTOOL offers two types of parallel execution backends within the Tool Server: Multi-threading: For small-scale or lightweight tool calls, VERLTOOL employs Pythons ThreadPoolExecutor to parallelize calls across multiple worker threads. Ray-based Asynchronous Execution: To deal with resource-intensive tools for better resource management, VERLTOOL optionally supports integration with Ray (Moritz et al., 2017), enabling distributed and fault-tolerant tool execution across machines or GPU nodes. This design provides robust scalability for long-horizon or computationally intensive tools. Work in Progress Figure 3: Example of code design for adding new tool in VERLTOOL via the plugin interface. Modular Tool-as-Plugin Design. As illustrated in Figure 3, VERLTOOL adopts modular plugin system that cleanly abstracts tool interaction as an interface between the LLM actor and its external environment. Each tool is implemented as subclass of unified BaseTool, enabling seamless registration and extensibility. During rollouts, the actors action ai is parsed by parse_action to determine whether it invokes tool; valid calls are routed to the appropriate module, which retrieves the trajectory state via load_env. The tool then executes its conduct_action, returning the observation oi, validity flag, and termination flag for next action generation. This design maintains per-trajectory environments through lightweight state dictionaries, updated via update_env and cleared at the end of an episode with delete_env. By decoupling tool logic from the training workflow, developers can add new tools with minimal overhead, while the framework dynamically manages their execution across threads or distributed workers. The result is scalable and extensible interface that supports multi-turn, multi-tool interactions without inter-tool interference, forming the backbone of efficient ARLT training. Tokenization. practical challenge in multi-turn agentic RL is how to tokenize tool observations and concatenate them with preceding LLM actions. Two strategies exist: (i) tokenize the action and observation strings separately, then append their token sequences; or (ii) concatenate the raw strings first and tokenize jointly. While they mostly yield the same sequence, discrepancies may arise, as illustrated in Figure 4. For example, the action string </python> and observation n<result> produce consistent tokens under the first strategy (On-Policy), but joint tokenization merges boundary symbols into different token ids (e.g., 29,198 vs. 397), which changed the LLM generated contents (Off-Policy). To avoid such inconsistencies, we adopt the first approach and always keep consistent token list prefix during rollout (using Completion instead of Chat Completion API), ensuring stable alignment in multiple rollouts turns. Figure 4: Tokenization of LLM generated content ...</python> and tool observation n<result>... can produce different token lists using Qwen2.5 tokenizer. Work in Progress"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Figure 5: Training dynamics using VERLTOOL on all 6 tasks. For each task, the corresponding test benchmarks are Spider-Test, NQ, VStar, AIME24, GAIA, and SWE-Verified. All models are trained and evaluated based on VERLTOOL framework. The actual evaluation performance (purple dash) can be higher due to the train-eval settings difference. Number of actions are averaged over all sampled responses in each batch. 4.1 EXPERIMENT SETUP With modular, plug-and-play design, VERLTOOL equips an agent with tools spanning multiple domains and modalities as shown in Table 2. In this section, we show the experiment results in six agentic RL with tool use (ARLT) tasks, including VT-Math  (Table 4)  , VT-Search  (Table 5)  , VTSQL  (Table 6)  , VT-VisualReasoner  (Table 7)  , VT-DeepSearch  (Table 7)  , and VT-SWE  (Table 8)  , demonstrating the compatibility of VERLTOOL with various tools. Please see details of training, evaluation in Appendix A. 4.2 RESULTS Training on VERLTOOL achieves competitive results. Models trained using VERLTOOL consistently match or exceed existing baselines across all six tasks. VT-Math achieves 62.2% average performance on mathematical benchmarks, VT-Search reaches 45.9% on knowledge QA (vs. 35.0% for Search-R1), and VT-SQL matches specialized systems like SkyRL-SQL. VT-VisualReasoner achieves 82.7% on V* Bench while VT-DeepSearch reaches 34.0% on GAIA, demonstrating competitive task-specific performance trained in unified framework instead of separate code bases. Multi-modal tools well-supported. VERLTOOLs modular design seamlessly integrates diverse tool types across modalities within unified API. The framework supports text-based tools (Python, search, SQL), visual processing (image operations, frame selection), and system-level tools (bash, file operations). Visual reasoning experiments demonstrate agents dynamically manipulating images and processing visual information iteratively, enabling complex multi-modal workflows that existing single-modality frameworks cannot support. 8 Work in Progress Table 4: Results on Math-TIR tasks. The best results are indicated in bold and the second-best results are underlined. See experiment details in subsection A.1. Model GSM8K MATH 500 Minerva Math Olympiad Bench AIME24 AMC23 Avg. Qwen2.5-Math-1.5B-Base/Instruct Qwen2.5-Math-1.5B-Instruct 84.9 Qwen-2.5-Math-1.5B-Instruct-TIR 83.7 81.9 SimpleRL-Zoo-1.5B 85.6 ToRL-1.5B 85.4 VT-Math-base (GRPO) 87.4 VT-Math-base (DAPO) 74.2 76.2 70.2 77.8 77.4 78.4 26.8 24.3 20.6 29.8 28.7 28.7 39.0 41.3 33.9 44.0 46.1 45.8 10.0 26.7 20.0 26.7 30.0 30.0 57.5 55.0 55.0 67.5 60.0 62. 48.7 51.2 46.9 55.2 54.6 55.5 Qwen2.5-Math-7B-Base/Instruct Qwen2.5-Math-7B-Instruct Qwen-2.5-Math-7B-Instruct-TIR SimpleRL-Zoo-7B ToRL-7B VT-Math-base (GRPO) VT-Math-base (DAPO) 41.6 16.7 57.3 41.6 30.0 53.3 50.5 30.0 58.2 49.9 43.3 61.1 43.3 50.5 61.7 51.6 62.2 36.7 / represents in-domain/outTable 5: Results of VT-Search on knowledge-QA benchmarks. domain datasets. The best results are indicated in bold and the second-best results are underlined. See experiment details in subsection A.2. 83.0 80.2 82.4 82.2 83.2 82.8 70.0 52.5 62.5 65.0 70.0 75. 37.1 26.8 29.0 33.5 31.6 34.9 95.2 88.8 94.6 92.7 91.8 92.1 Model General QA Multi-Hop QA Avg. NQ TriviaQA PopQA HotpotQA 2wiki Musique Bamboogle Avg. Direct Inference Search-R1-base (GRPO) Search-R1-base (PPO) VT-Search-base (GRPO) VT-Search-base (DAPO) Direct Inference Search-R1-base (GRPO) Search-R1-base (PPO) VT-Search-base (GRPO) VT-Search-base (DAPO) 10.6 42.1 40.6 45.4 45.8 13.4 39.5 48.0 49.3 48. Qwen2.5-3b-Base/Instruct 10.8 41.3 43.5 48.1 46.5 14.9 29.7 28.4 32.4 33.0 Qwen2.5-7b-Base/Instruct 14.0 38.8 45.7 50.2 48.2 18.3 32.6 43.3 44.8 42. 28.8 58.3 58.7 61.6 62.3 40.8 56.0 63.8 66.2 63.4 24.4 27.4 27.3 30.8 31.1 25.0 29.7 38.2 45.3 39.2 2.0 6.6 4.9 7.6 8.2 3.1 12.5 19.6 19.3 18. 2.4 12.8 8.8 15.2 15.2 12.0 36.0 43.2 46.4 38.4 13.4 31.2 30.3 34.4 34.6 18.1 35.0 43.1 45.9 41.2 Table 6: Results of VT-SQL on NL2SQL benchmarks, metrics are reported as pass rate in percentages. The best results are indicated in bold and the second-best results are underlined. See experiment details in subsection A.3 Model Spider (Dev) Spider (Test) Spider (Realistic) Spider-DK Spider-Syn Reasoning without Tool Integration GPT-4o DeepSeekCoder-6.7B-Instruct OpenCoder-8B-Instruct Qwen2.5-Coder-7B-Instruct 70.9 63.2 59.5 73.4 83.2 70.5 68.3 82.2 Tool Integrated Reasoning OmniSQL-7B SkyRL-SQL-7B (GRPO) VT-SQL (Qwen-2.5-Coder-7B-Instruct based) 81.2 83.9 + GRPO 83.9 87.9 85.2 83.4 9 - - - - 63.9 81. 81.3 72.9 60.9 62.6 67.5 - 72.0 71.6 59.6 49.9 46.0 63.1 69.7 73. 73.4 Work in Progress Table 7: Results of VT-VisualReasoner (left) and VT-DeepSearch (right). Best results in bold, second-best underlined. See experiments details in subsection A.4 and subsection A.5 respectively. Model V* Bench Model GAIA HLE Reasoning without Tool Reasoning without Tool GPT-4o Gemini-2.5-Pro Qwen2.5-VL-7B-Instruct Video-R1-7B 62.8 79.2 70.4 51. Tool Integrated Reasoning Visual Sketchpad (GPT-4o) IVM-Enhanced (GPT-4V) Pixel-Reasoner-7B VT-VisualReasoner (Qwen2.5-VL-7B-Instruct Based) 80.4 81.2 84.3 + GRPO-Acc + GRPO-Complex 78.8 82.7 DeepSeek-R1-671B 25.2 17.5 GPT-4o 20.4 Qwen3-8B 8.6 2.6 4.6 Tool Integrated Reasoning (Qwen3-8B) Vanilla RAG Search-o1 WebThinker ReAct VT-DeepSearch (Qwen3-8B Based) 20.4 21.4 22.3 23.3 5.8 6.4 6.6 4.6 + Snippets-Only + QwQ-32B 32.0 34.0 7.8 8.4 Table 8: Results of VT-SWE on SWE-Verified Benchmark. Experiment details in subsection A.6. Model SWE-Verified Score OpenHands Scaffold Qwen3-8B OpenHands-7B-Agent SkyRL-v0 (Cao et al., 2025) Qwen3-8B Based OpenHands-7B-Agent Based R2E Gym Scaffold Qwen3-8B VT-SWE (Qwen3-8B Based) +GRPO 3.6 11. 9.4 14.6 10.4 19.5 Dynamics of Tool Usage across Tasks. Tool usage patterns exhibit substantial variation across different domains, with mathematical tasks typically requiring 1-4 interactions while software engineering tasks may extend to over 100 interactions. Importantly, models do not spontaneously develop effective tool-use capabilities without appropriate reward design and initialization strategies. For instance, without VT-VisualReasoners sophisticated reward mechanism, the frequency of tool actions gradually diminishes to zero within few reinforcement learning (RL) steps. The evolution of tool usage during training demonstrates task-specific characteristics that reflect In VT-SQL settings, the number of actions decreases the underlying utility of tool interactions. rapidly after several dozen RL steps, as the model learns that SQL executors are non-essential for most straightforward queries. Through training, the model develops preference for responses requiring fewer tool calls by gradually memorizing expected execution results, thereby reducing the need for verification through SQL executors. Conversely, in VT-DeepSearch settings, tool usage increases dramatically during training because problem-solving fundamentally depends on search capabilities. Unlike simpler SQL results, the extensive and information-rich content returned by search tools cannot be easily memorized, necessitating increased tool invocations for effective task completion. This divergent behavior underscores how the intrinsic value of tool assistance shapes learning dynamics across different computational domains. Emerging abilities of Agentic RL. Trained models exhibit sophisticated behaviors including selfcorrection, iterative refinement, and strategic tool selection. Mathematical agents verify computations and backtrack from errors, search agents refine queries based on retrieved information, and 10 Work in Progress software agents develop debugging strategies combining code analysis and incremental fixes. These capabilities represent genuine agentic problem-solving that extends beyond simple tool invocation. We present corresponding case studies in Appendix C."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose VERLTOOL, addressing key limitations of Agentic Reinforcement Learning with Tool use (ARLT) models training. Our framework features unified and modular systematic design, providing multi-modal tool management through standardized API designs, while maintaining highefficiency model training featuring asynchronous rollout execution. Our system extends traditional single-turn reinforcement learning with verifiable rewards to ARLT domains, featuring robust system designs and upstream-aligned with VeRL. The framework is extensively examined across six domains featuring diverse tool integrations and modalities. As evidenced by extensive evaluation, agents trained through our framework demonstrated competitive performance compared to specialized systems, while unified under our training infrastructure. We present VERLTOOL as scalable foundational training infrastructure to the RL community and hope our contributions could facilitate the advancement of ARLT research."
        },
        {
            "title": "REFERENCES",
            "content": "Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. ArXiv, abs/2111.00653, 2021. URL https://api. semanticscholar.org/CorpusID:240353884. Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao Wang, Akshay Malik, Graham Neubig, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon agents via reinforcement learning, 2025. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023, 2022. URL https://api.semanticscholar.org/CorpusID: 253801709. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms. ArXiv, abs/2412.21187, 2024a. URL https://api.semanticscholar.org/CorpusID:275133600. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548, 2025. Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, Julian J. McAuley, Linda Ruth Petzold, and William Yang Wang. survey on large language models for critical societal domains: Finance, healthcare, and law. ArXiv, abs/2405.01769, 2024b. URL https://api.semanticscholar.org/CorpusID:269587715. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library. ArXiv, abs/2401.08281, 2024. URL https://api.semanticscholar.org/ CorpusID:267028372. 11 Work in Progress Joshua Epstein and Robert Axtell. Growing artificial societies: social science from the bottom up. Brookings Institution Press, 1996. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms, 2025. URL https://arxiv.org/abs/2504.11536. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https://arxiv.org/ abs/2505.24298. Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John Robert Woodward, Jinxia Xie, and Pengsheng Huang. Towards robustness of text-to-sql models against synonym substitution. ArXiv, abs/2106.01065, 2021a. URL https://api.semanticscholar.org/ CorpusID:235293739. Yujian Gan, Xinyun Chen, and Matthew Purver. Exploring underexplored limitations of crossdomain text-to-sql generalization, 2021b. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation. Proc. VLDB Endow., 17:11321145, 2023a. URL https://api.semanticscholar.org/CorpusID: 261276437. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. ArXiv, abs/2312.10997, 2023b. URL https://api.semanticscholar.org/ CorpusID:266359151. Gemini. Gemini: family of highly capable multimodal models, 2024. URL https://arxiv. org/abs/2312.11805. Nigel Gilbert. Agent-based models. Sage Publications, 2019. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. ArXiv, abs/2504.11456, 2025. URL https://api.semanticscholar.org/CorpusID:277787455. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. 12 Work in Progress Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43:1 55, 2023. URL https://api.semanticscholar.org/ CorpusID:265067168. Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. ArXiv, abs/2201.07207, 2022. URL https://api.semanticscholar.org/CorpusID:246035276. Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao Kurohashi. Multitool-cot: Gpt-3 can use multiple external tools with chain of thought prompting. ArXiv, abs/2305.16896, 2023. URL https://api.semanticscholar.org/CorpusID:258947061. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2egym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. ArXiv, URL https://api.semanticscholar.org/CorpusID: abs/2504.07164, 2025. 277667306. Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. ArXiv, abs/2503.00223, 2025. URL https: //api.semanticscholar.org/CorpusID:276742133. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. ArXiv, URL https://api.semanticscholar.org/CorpusID: abs/2305.06983, 2023. 258615731. Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Ö. Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag. ArXiv, abs/2410.05983, 2024. URL https: //api.semanticscholar.org/CorpusID:273229050. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. ArXiv, URL https://api.semanticscholar.org/CorpusID: abs/2503.09516, 2025. 276937772. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answerArXiv, abs/2004.04906, 2020. URL https://api.semanticscholar.org/ ing. CorpusID:215737187. Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, PeiFeng Wang, Silvio Savarese, Caiming Xiong, and Shafiq Joty. survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems. Trans. Mach. Learn. Res., 2025, 2025. URL https://api.semanticscholar.org/ CorpusID:277781085. 13 Work in Progress Geunwoo Kim, Pierre Baldi, and Stephen Marcus McAleer. Language models can solve computer tasks. ArXiv, abs/2303.17491, 2023. URL https://api.semanticscholar.org/ CorpusID:257834038. Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems. ArXiv, abs/2311.11315, 2023. URL https://api.semanticscholar.org/ CorpusID:265294410. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. ArXiv, abs/2005.11401, 2020. URL https://api.semanticscholar.org/CorpusID:218869575. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2:1 28, 2024. URL https: //api.semanticscholar.org/CorpusID:267938784. Haoyang Li, Shang Wu, Xiaokang Zhang, Xinmei Huang, Jing Zhang, Fuxin Jiang, Shuai Wang, Tieying Zhang, Jianjun Chen, Rui Shi, Hong Chen, and Cuiping Li. Omnisql: Synthesizing high-quality text-to-sql data at scale. ArXiv, abs/2503.02240, 2025a. URL https://api. semanticscholar.org/CorpusID:276774742. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graphaware layers for text-to-sql parsing. ArXiv, abs/2301.07507, 2023. URL https://api. semanticscholar.org/CorpusID:255998567. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating super-human reasoning for web agent. ArXiv, abs/2507.02592, 2025b. URL https: //api.semanticscholar.org/CorpusID:280078605. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl, 2025c. URL https: //arxiv.org/abs/2503.23383. Shu Liu, Sumanth Hegde, Shiyi Cao, Alan Zhu, Dacheng Li, Tyler Griggs, Eric Tang, Akshay Malik, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-sql: Matching gpt-4o and o4-mini on text2sql with multi-turn rl, 2025. Notion Blog. Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yu xin Zhang, Ju Fan, Guoliang Li, Nan Tang, and Yuyu Luo. survey of text-to-sql in the era of llms: Where are we, and IEEE Transactions on Knowledge and Data Engineering, 2024. URL where are we going? https://api.semanticscholar.org/CorpusID:271843296. 14 Work in Progress Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. ArXiv, abs/2304.09842, 2023. URL https://api.semanticscholar. org/CorpusID:258212542. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. ArXiv, abs/2505.14652, 2025. URL https:// api.semanticscholar.org/CorpusID:278768680. Xinji Mai, Haotian Xu, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 7, 2022. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: In The Twelfth International Conference on Learning benchmark for general ai assistants. Representations, 2023. Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: distributed framework for emerging ai applications. ArXiv, abs/1712.05889, 2017. URL https://api.semanticscholar. org/CorpusID:34552495. Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset, 2025. URL https://arxiv. org/abs/2504.16891. OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. Simone Papicchio, Simone Rossi, Luca Cagliero, and Paolo Papotti. Think2sql: Reinforce llm reasoning capabilities for text2sql. ArXiv, abs/2504.15077, 2025. URL https://api. semanticscholar.org/CorpusID:277955819. C.A.I. Peng, Xi Yang, Aokun Chen, Kaleb E. Smith, Nima M. Pournejatian, Anthony Costa, Cheryl Martin, Mona G. Flores, Ying Zhang, Tanja Magoc, Gloria P. Lipori, Duane A. Mitchell, Naykky M. Singh Ospina, Mustafa Mamon Ahmed, William R. Hogan, Elizabeth A. Shenkman, Yi Guo, Jiang Bian, and Yonghui Wu. study of generative large language model for medical research and healthcare. NPJ Digital Medicine, 6, 2023. URL https://api. semanticscholar.org/CorpusID:258841310. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Aske Plaat, Max J. van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, and Kees Joost Batenburg. Agentic large language models, survey. ArXiv, abs/2503.23037, 2025. URL https://api.semanticscholar.org/CorpusID:277451794. Mohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini, Amin Saberi, and Sercan Ö. Arik. Reasoning-sql: Reinforcement learning with sql tailored partial rewards for reasoning-enhanced text-to-sql. ArXiv, abs/2503.23157, 2025. URL https://api.semanticscholar.org/CorpusID:277452634. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. 15 Work in Progress Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Jirong Wen. Tool learning with large language models: survey. ArXiv, abs/2405.17935, 2024. URL https://api.semanticscholar.org/CorpusID:270067624. Shanghaoran Quan, Jiaxin Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, and Junyang Lin. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. ArXiv, abs/2501.01257, 2025. URL https://api.semanticscholar.org/CorpusID:275212089. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. ArXiv, abs/2305.18290, 2023. URL https://api.semanticscholar.org/ CorpusID:258959321. Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents. ArXiv, abs/2308.03427, 2023. URL https://api. semanticscholar.org/CorpusID:265381326. Stuart Russell and Peter Norvig. Artificial intelligence: modern approach. pearson, 2016. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023. URL https://api.semanticscholar.org/ CorpusID:256697342. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017. URL https://api. semanticscholar.org/CorpusID:28695052. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. URL https://api.semanticscholar. org/CorpusID:267412607. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yue Ting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. ArXiv, abs/2303.17580, 2023. URL https://api.semanticscholar.org/CorpusID:257833781. Zhuocheng Shen. Llm with tools: survey. ArXiv, abs/2409.18807, 2024. URL https://api. semanticscholar.org/CorpusID:272968969. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2503.05592. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. ArXiv, abs/2505.15966, 2025. URL https://api.semanticscholar.org/CorpusID:278789415. 16 Work in Progress Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, et al. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834, 2025. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. THUDM. slime: llm post-training framework aiming at scaling rl. https://github.com/ THUDM/slime, 2024. Software framework for LLM post-training with reinforcement learning scaling. H. Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. ArXiv, URL https://api.semanticscholar.org/CorpusID: abs/2212.10509, 2022a. 254877499. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022b. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. In Annual Meeting of the Association for Computational Linguistics, 2019. URL https://api.semanticscholar. org/CorpusID:207863446. Hongru Wang, Yujia Qin, Yankai Lin, Jeff Z. Pan, and Kam-Fai Wong. Empowering large lanIn Proceedings of the 47th Internaguage models: Tool learning for real-world interaction. tional ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, pp. 29832986, New York, NY, USA, 2024a. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.3661381. URL https://doi.org/10.1145/ 3626772.3661381. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning, 2023. URL https://arxiv.org/abs/2310.03731. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pretraining. ArXiv, abs/2212.03533, 2022. URL https://api.semanticscholar.org/ CorpusID:254366618. Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for largescale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025a. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for ai software developers as generalist agents. In International Conference on Learning Representations, 2024b. URL https://api.semanticscholar.org/CorpusID:271404773. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Fei-Fei Li, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding selfevolution in llm agents via multi-turn reinforcement learning. ArXiv, abs/2504.20073, 2025b. URL https://api.semanticscholar.org/CorpusID:278170861. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language modArXiv, abs/2201.11903, 2022. URL https://api.semanticscholar.org/ els. CorpusID:246411621. Work in Progress Michael Wooldridge. Intelligent agents. Multiagent systems: modern approach to distributed artificial intelligence, 1:2773, 1999. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. ArXiv, abs/2305.18752, 2023. URL https://api.semanticscholar.org/CorpusID:258967184. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395. Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. Are reasoning models more prone to hallucination? ArXiv, abs/2505.23646, 2025. URL https://api.semanticscholar.org/CorpusID:278996592. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. ArXiv, abs/2502.03387, 2025. URL https://api.semanticscholar.org/ CorpusID:276116748. Tao Yu, Rui Zhang, Kai-Chou Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-tosql task. ArXiv, abs/1809.08887, 2018. URL https://api.semanticscholar.org/ CorpusID:52815560. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning, 2023. URL https://arxiv.org/abs/2309.05653. Bohan Zhai, Canwen Xu, Yuxiong He, and Zhewei Yao. Excot: Optimizing reasoning for text-to-sql with execution feedback. In Annual Meeting of the Association for Computational Linguistics, 2025. URL https://api.semanticscholar.org/CorpusID:277321668. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Sirens song in the ai ocean: survey on hallucination in large language models. ArXiv, abs/2309.01219, 2023. URL https://api.semanticscholar.org/ CorpusID:261530162. 18 Work in Progress"
        },
        {
            "title": "APPENDIX",
            "content": "A Detailed Experiment Setup A.1 Mathematical Reasoning with Python Executor (VT-Math) . . . . . . . . . . . . . A.2 Knowledge QA with Search Retriever (VT-Search) . . . . . . . . . . . . . . . . . A.3 Multi-Turn SQL Query Generation (VT-SQL) . . . . . . . . . . . . . . . . . . . . A.4 Visual Reasoning with Image Operations (VT-VisualReasoner) . . . . . . . . . . . A.5 Agentic Web Search (VT-DeeepSearch) . . . . . . . . . . . . . . . . . . . . . . . A.6 Software Engineering Benchmark (VT-SWE) . . . . . . . . . . . . . . . . . . . . A.7 Training and Evaluation Configurations . . . . . . . . . . . . . . . . . . . . . . . More related works B.1 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 From Tool Integration to Agentic LLMs . . . . . . . . . . . . . . . . . . . . . . . B.3 Reinforcement Learning with Verifiable Reward (RLVR) . . . . . . . . . . . . . . B.4 Agentic Reinforcement Learning with Tool Use (ALRT) . . . . . . . . . . . . . . Case Study 20 20 20 21 21 21 24 24 24 24 25 19 Work in Progress"
        },
        {
            "title": "A DETAILED EXPERIMENT SETUP",
            "content": "We evaluate VERLTOOL across six diverse domains to demonstrate its effectiveness in toolaugmented reasoning. Each task domain presents unique challenges and requires different tool integration strategies, allowing us to comprehensively assess the frameworks adaptability and performance. A.1 MATHEMATICAL REASONING WITH PYTHON EXECUTOR (VT-MATH) Mathematical reasoning tasks often involve complex computations that are prone to numerical errors when performed purely through natural language reasoning. To address this limitation, we integrate Python code interpreter tool that enables agents to execute mathematical calculations reliably and verify intermediate results. We train mathematical-coding agent that issues Python snippets to sandboxed interpreter and processes execution traces. We use DeepMath (He et al., 2025) as our training dataset. The reward function combines answer accuracy with tool usage incentives: Racc(x, y) = (cid:26) 1 if match(y, yg) otherwise , Rtool(x, y) = (cid:26)0 0.25 if match(y, yg) otherwise (6) where the final reward is Rmath = Racc(x, y) + Rtool(x, y). This design encourages the model to explore Python executor usage for problem-solving while maintaining accuracy focus. We evaluate on multiple mathematical benchmarks: MATH-500 (Hendrycks et al., 2021), OLYMPIAD (He et al., 2024), MINERVA (Lewkowycz et al., 2022), GSM8K (Cobbe et al., 2021), AMC, AIME24, and AIME25, using MATH-EVALUATION-HARNESS1 for standardized eval. A.2 KNOWLEDGE QA WITH SEARCH RETRIEVER (VT-SEARCH) Question answering tasks often require access to external knowledge beyond the models parametric memory, particularly for factual queries and multi-hop reasoning. We integrate FAISS-based search retriever tool that enables agents to query local knowledge base and extract relevant information for answering complex questions. Following prior work (Jin et al., 2025; Song et al., 2025), we integrate an E5 retriever (Wang et al., 2022) and index the 2018 Wikipedia dump (Karpukhin et al., 2020). The agent alternates between search operations and reasoning steps to construct comprehensive answers. For this task, we apply accuracy as the primary reward: Rsearch(x, y) = (cid:26)1 1 if match(y, yg) otherwise (7) We evaluate using Exact Match scores on General Q&A benchmarks (NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al., 2022)) and Multi-hop Q&A benchmarks (HotpotQA (Yang et al., 2018), 2Wiki (Ho et al., 2020), MuSiQue (Trivedi et al., 2022b), Bamboogle (Press et al., 2022)). A.3 MULTI-TURN SQL QUERY GENERATION (VT-SQL) Natural language-to-SQL (NL2SQL) conversion requires understanding database schemas and translating natural language queries into executable SQL commands. This task benefits from tool integration as it allows iterative query refinement based on execution feedback and error correction. We assess SQL Executor adaptability using the SkyRL-SQL training set (Liu et al., 2025) with Qwen2.5-7B-Instruct as the base model. The agent translates natural language questions into executable SQL given schema hints and tool-calling instructions. 1https://github.com/ZubinGou/math-evaluation-harness Work in Progress The reward function focuses solely on execution accuracy: Rsql(x, y) = (cid:26)1 1 if match(y, yg) otherwise (8) Following standard conventions, we evaluate execution accuracy (EX) on SPIDER-1.0 (Yu et al., 2018) (Dev and Test splits), SPIDER-DK (Gan et al., 2021b), and SPIDER-SYN (Gan et al., 2021a). A.4 VISUAL REASONING WITH IMAGE OPERATIONS (VT-VISUALREASONER) Traditional visual reasoning tasks are conducted primarily in the text modality, where models lack the ability to dynamically process images as actions. To address this limitation, we implement image operation tools that enable agents to zoom into specific image regions, select key frames, and perform other visual manipulations to enhance reasoning over dense visual information, following the Pixel-Reasoner (Su et al., 2025) approach. The reward design incorporates both accuracy-oriented and compositional complexity measures: where rcuriosity(x, y) = max(H RaPR(x), 0) 1PR(y) Rvisualreasoner(x, y) = r(x, y) + α rcuriosity(x, y) + β rpenalty(y), (9) (10) (11) where RaPR(x) denotes the ratio of responses that invoke tool calls and nvo(y) denotes the number of actions that response invokes. Hyperparameters are set as = 0.3, = 1, α = 0.5 and β = 0.05. We train two variants using accuracy-only reward and the original complexity-driven reward, denoted as GRPO-acc and GRPO-complex respectively. rpenalty(y) = min(N nvo(y), 0) We use the official training dataset from Pixel-Reasoner and evaluate primarily on V-Star (Wu & Xie, 2024), which assesses MLLM visual search capabilities. A.5 AGENTIC WEB SEARCH (VT-DEEEPSEARCH) Open-web question answering requires real-time information retrieval and multi-step reasoning over diverse web sources. GAIA (Mialon et al., 2023) and HLE (Phan et al., 2025) are representative benchmarks testing these capabilities. We implement Web Search tool using Google Search API through SERPER with caching, enabling agents to perform dynamic information gathering and synthesis from online sources. We apply both accuracy and tool-usage rewards to encourage effective search behavior: Rdeepsearch(x, y) = Racc(x, y) + Rtool(x, y), where Rtool(x, y) = (cid:26)0.1, 0, if tool is called if no tool call (12) We use 1K mixed training examples from SimpleDeepSearcher (Sun et al., 2025) and Web-Sailor (Li et al., 2025b), following the setting in Dong et al. (2025). Starting from Qwen3-8B, we evaluate on GAIA and HLE (text-only) benchmarks. We retrieve top-k URLs for each query and use returned snippets as content during RL training. For evaluation, we employ two settings: Snippet-Only aligns with training conditions using only snippet content, while QwQ-32B uses browser agent to summarize raw content from retrieved URLs. A.6 SOFTWARE ENGINEERING BENCHMARK (VT-SWE) Software engineering tasks require code understanding, localization, debugging, and modification capabilities that benefit from iterative execution and testing. We integrate bash terminal and code execution tools to enable agents to interact with software development environments effectively. We build on the R2E-Gym scaffold (Jain et al., 2025) and its training dataset R2E-Lite, using Qwen3-8B in no-think mode as the base model. The reward function is defined strictly by task completion accuracy: an agent must terminate normally and pass all verification tests to receive reward of 1; otherwise, the reward is 0: Rswe(x, y) = (cid:26)1 if execution terminates successfully and all tests pass otherwise (13) 21 Work in Progress We evaluate performance on the SWE-Verified benchmark, measuring the agents ability to resolve software engineering tasks and pass verification tests. For training and evaluation, we maintain cluster of eight servers (each with 64 CPU cores and 200 GB memory), orchestrating sandbox services via Kubernetes. Each task runs inside the official Docker image provided by R2E-Lite, initialized with 1 CPU and 2 GB memory, elastically scalable up to 2 CPUs and 4 GB memory. We observed that the main bottleneck lies in disk I/O during Docker initialization. To stabilize training, we therefore allocate more CPU and memory resources than minimally required. The modular architecture of VerlTool, which separates training from environment services, allows us to scale sandbox environments efficiently. Each environment interaction is given 90-second timeout, reward computation has 300-second timeout, and the maximum time per trajectory is capped at 20 minutes. Any trajectory that times out, encounters an exception, or exceeds the length limit is assigned reward of 0, and its gradients are masked during updates. A.7 TRAINING AND EVALUATION CONFIGURATIONS Table 9 summarizes the detailed configurations for each task during training and evaluation. Due to configuration differences across tasks, there may be gaps between validation curves and final downstream evaluation performance, as illustrated in Figure 5. The RL training parameters vary across tasks to accommodate different complexity levels and interaction patterns. Math-TIR and Pixel-Reasoner use smaller batch sizes due to computational constraints, while Search-R1 employs larger batch sizes for stable retrieval learning. The agentic tool use parameters reflect task-specific requirements: Math-TIR typically requires single-turn interactions, while SWE tasks may require up to 100 interaction turns for complex debugging scenarios. Evaluation parameters are configured to balance comprehensive assessment with computational efficiency. Temperature settings range from 0.0 for deterministic tasks like SQL generation to 0.6 for creative tasks requiring exploration. Maximum turn limits reflect task complexity, with simple QA tasks limited to 2-5 turns while software engineering tasks allow up to 100 turns for thorough problem resolution. Work in Progress Table 9: Training and evaluation configurations across all six tasks. Tasks VT-Math VT-Search VT-SQL VT-VisualReasoner VT-DeepSearch VT-SWE Rollout BS Samples Gradient BS Temperature Top Learning Rate Val Temperature Val Top Max Turns MTRL Max Prompt Length Max Response Length Max Action Length Max Observation Length 128 16 128 1.0 1.0 1e-6 0.0 1.0 1 1024 3072 2048 Action Stop Tokens output RL Training Parameters 512 16 64 1.0 1.0 1e-6 0.0 1.0 256 5 256 0.6 0.95 1e-6 0.0 0.95 Agentic Tool Use Parameters 2 4096 4096 2048 1024 </search>, </answer> 5 4096 4096 2048 1024 128 8 128 1.0 1.0 1e-6 0.0 1.0 3 16384 16384 2048 8192 </sql> </tool_call> 128 16 128 1.0 1.0 1e-6 0.0 1.0 5 2048 8196 8196 4096 </python>, </search> 32 8 32 1.0 1.0 2e-6 0.0 1.0 100 10240 22528 10240 10240 </function> Temperature Top Max Turns Max Prompt Length Max Response Length Max Action Length Max Observation Length 0.6 0.95 4 1024 3072 2048 Evaluation Parameters 0.0 1.0 2 4096 4096 2048 1024 0.0 1.0 5 4096 4096 2048 1024 0.0 1.0 5 16384 16384 4096 8192 0.6 0.95 10 2048 32768 16483 4096 1.0 1.0 100 - 40960 - - 23 Work in Progress"
        },
        {
            "title": "B MORE RELATED WORKS",
            "content": "B.1 ORGANIZATION In this section, we first establish the definition of Agentic Large Language Models (Agentic LLMs). Then prior tool integration efforts in LLMs and the shift from single-turn, prompt-driven tool-calling to instruction tuning/RL-based multi-turn agentic interaction are reviewed. Further, we introduce various representative domain-specific tasks that are proven to be benefited from developing corresponding tool-use oriented agents. We then distinguish Reinforcement Learning with Verifiable Rewards (RLVR) from Reinforcement Learning via Agentic Tool-Use (RLAT). Finally, we survey existing systems for training RL-based tool-using agents and position our work: VERLTOOL. B.2 FROM TOOL INTEGRATION TO AGENTIC LLMS Large language models (LLMs) demonstrated exceptional flexibility and generality with model parameter and training-data scaling (Team et al., 2024; Qwen et al., 2025; OpenAI, 2025). With recent research advancements dramatically enhanced their capability in reasoning, information retrieval and instruction-following (Ke et al., 2025), current research trends have shifted from developing simple LLM-based tool-callers to empowering LLMs with versatile interaction capability to facilitate its agentic actions in the world (Plaat et al., 2025). Agentic LLMs. As one of the central concepts in artificial intelligence (Russell & Norvig, 2016), Decision-making, identifying environmental changes, communication, and acting on ones goal or will are generally defined as common traits of Agency (Epstein & Axtell, 1996; Wooldridge, 1999; Gilbert, 2019). Following well-established conventions, we denote Agentic LLMs as models that reason, act through tools, and interact over one or multiple turns, maintaining state and revising plans according to environmental observations, framing agent behavior beyond passive text-generation and motivating the possession of improved planning and tool-use capabilities. Agentic Tool-calling Acquisition. As one fundamental capability, tool-calling has been widely studied as one of the most effective ways of empowering Agentic LLMs with the capability to act in exposed environments. Early attempts involve prompt-based orchestration, which structures function calls in training-free manner with specific instructions and tool-calling schemas (Yao et al., 2022; Lu et al., 2023), exploiting LLMs instruction-following capability through Chain-ofThought technique (Wei et al., 2022) and explicit task decomposition (Kim et al., 2023; Huang et al., 2022) or multi-agent orchestration (Shen et al., 2023; Ruan et al., 2023). Instruction-tuning based tool callers learn function-call schemas and appropriate tool-call choices through supervised traces (Schick et al., 2023; Kong et al., 2023; Gou et al., 2023), improving problem-solving capabilities while suffering from lack of generality and remaining largely single-turn (Qu et al., 2024). Reinforcement-Learning-based acquisition further enhances tool-using generality and multi-turn behaviors (Li et al., 2025c; Feng et al., 2025) through problem-solving outcomes and tool-feedback, facilitating environment exploration, self-reflection, error-corrections, and enhanced performance on reasoning-intensive tasks through informative tool responses. (Moshkov et al., 2025). B.3 REINFORCEMENT LEARNING WITH VERIFIABLE REWARD (RLVR) Conventional RLHF pipelines, such as DPO (Rafailov et al., 2023) and PPO (Schulman et al., 2017), optimize answer-level quality and require reward models with substantial sizes. With the introduction of GRPO in DEEPSEEKMATH (Shao et al., 2024), as critic-free variant of PPO, it enables stabilized RL training on long reasoning chains and enables the integration of multi-turn tool responses and verifiable rewards in reinforcement learning. Existing works have explored the potential of extending GRPO training through the inclusion of rule-based verifiable rewards such as format-based rewards and exact-match based comparisons, as well as tool-calling responses. They demonstrated significant success in developing expert agents in multiple domains with tool-calling and self-reflection capabilities. By integrating GRPO and verifiable rewards, advancements have been witnessed in wide range of domain-specific tasks. TORL (Li et al., 2025c) integrates Python Code Interpreters into the GRPO training of solving mathematical tasks, surpassing RL baselines which does not have tool-calling 24 Work in Progress integration on multiple mathematical datasets. TOOLRL (Qian et al., 2025) explored and analyzed the impact of reward design and tool choice on the effect of tool-integrated GRPO training, revealed the significance of tool-use reward design in boosting LLMs tool calling and generalization capability, as well as achieving stable GRPO training. RETOOL (Feng et al., 2025) integrated tool-calling in PPO training, resulting in Python tool-enabled agent with strong mathematical problem-solving capability. EXCOT (Zhai et al., 2025) and THINK2SQL (Papicchio et al., 2025) demonstrated preliminary success of utilizing GRPO and the response of SQL executors to enhance the base models performance on natural language-to-SQL (NL2SQL) tasks through comprehensive reward design and challenging training problem-filtering. By integrating Faiss (Douze et al., 2024)-based local retrievers and API-based online search services, SEARCH-R1 (Jin et al., 2025) and R1-SEARCHER (Song et al., 2025) equip base models with enhanced search tool-calling and retrieval capability, achieving superior performance across multiple retrieval-centric benchmarks. B.4 AGENTIC REINFORCEMENT LEARNING WITH TOOL USE (ALRT) Despite RLVR optimizing policies behaviors using rule-based verifiable checks with the inclusion of single-turn tool-calling, current RLVR-based agentic models limited capability in making real multi-turn, long-horizon interactions with tools and perform effective self-reflection and dynamic plan revising remains to be the gap to the generalist Agent-LLM. The inclusion of training the model over dynamic, long-term interactions while exposing it to intermediate tool responses could shape the agentic models subsequent actions, pushing beyond single-turn verification toward longhorizon, interaction-centric RL training. Therefore, we use Reinforcement Learning via Agentic Tool use (RLAT) to denote reinforcement learning on dynamic, multi-turn trajectories in which tool responses are treated as environmental observations that condition future actions. RLAT therefore (i) assigns credit across tool calls rather than only at the final answer, (ii) handles observation tokens explicitly (e.g., masking non-model tokens during GRPO optimization), and (iii) relies on asynchronous, failure-aware executors for adapting potentially slow, stochastic, or error-prone (Plaat et al., 2025; Ke et al., 2025) tool calls. In contrast to RLVR, RLAT targets exploration, re-planning, and recovery from tool failures, and is naturally suited to settings where the problem-solving requires probing the environment before coming up with correct solutions. In this work, our framework is mainly evaluated on the following domain-specific tasks. Mathematical Interactive Coding. Tool-integrated reasoning was first introduced to tackle computationally intensive mathematical problems by combining natural language reasoning with programming strategies (Chen et al., 2022; Yue et al., 2023; Jin et al., 2025; Song et al., 2025; Wang et al., 2024a; Chen et al., 2022). Building on this idea, (Wang et al., 2023) proposed an iterative method that couples textual reasoning with code execution to cross-check the answers, improving the accuracy. More recently, (Chen et al., 2025) incorporated code execution into reasoning through supervised fine-tuning on curated code-integrated CoT data. Yet this method is limited by its dependence on specific data distributions and cannot learn adaptive tool-use strategiessuch as when and how to call toolsvia reinforcement learning. To solve this, concurrent work, including ToRL (Li et al., 2025c) and ZeroTIR (Mai et al., 2025), applies ZeroRL to train agents for mathematical code interpreter use. Agentic Search and Retrieval. Large language models (Gemini, 2024; OpenAI, 2025) possess huge amount of intrinsic knowledge while struggling at domain-specific, knowledge-centric tasks (Chen et al., 2024b; Peng et al., 2023) and suffer from hallucination (Zhang et al., 2023; Huang et al., 2023). common approach to mitigate this issue is by integrating search engines into the LLMs. Predominant approaches of search engine integration often fall within two categories: Retrieval Augmented Generation (RAG)-based (Lewis et al., 2020; Gao et al., 2023b) and Tool-calling based retrieval (Schick et al., 2023). As RAG relies on separate retriever to extract documents in single turn without the interaction of LLM, it faces challenges of retrieving irrelevant information or returning less useful context (Jin et al., 2024; Jiang et al., 2023). Conversely, Tool-calling-based retrieval enhances LLMs capability of calling the search retriever as tool either through prompting (Yao et al., 2022; Trivedi et al., 2022a), fine-tuning (Schick et al., 2023), or through reinforcement learning (Jin et al., 2025; Song et al., 2025; Jiang et al., 2025), while enhancing searching agents with multi-turn tool-calling-based retrieval capability remains under-explored. Work in Progress Natural Language to SQL. Natural language-to-SQL (NL2SQL) refers to the task of generating database-specific query codes for extracting data of interest. Early efforts typically involve developing expert models with an encoder-decoder structure for achieving this goal, where the encoder fuses the database schema and the natural-language query, leaving the answer generation to the decoder module (Wang et al., 2019; Cai et al., 2021; Li et al., 2023; Liu et al., 2024). With the advances of sequence-to-sequence models, especially large language models, the NL2SQL tasks have seen rapid transfer from simple sequence-to-sequence modeling into developing tabular understanding agents utilizing pre-trained large language models. Existing works mostly focus on prompt enhancement (Gao et al., 2023a), large-scale data synthesis and model finetuning (Li et al., 2024; 2025a), while some recent works explored the realm of Reinforcement Learning-driven NL2SQL agents (Papicchio et al., 2025; Pourreza et al., 2025) and NL2SQL agents featuring multi-round Tool Calling capabilities. (Liu et al., 2025) 26 Work in Progress"
        },
        {
            "title": "C CASE STUDY",
            "content": "In this section, we present case studies of representative tasks, featuring corresponding agents outputs to illustrate Multi-Turn Tool Calling. The queries are randomly extracted from the underlying benchmarks. For readability, tag markers are colorized according to their functional categories: 1. Internal reasoning tags, <think>...</think> are colored in purple. 2. Tool-calling tags are formatted as <python>/<sql>/<search> . 3. Observation/Tool Response tags are colored as <observation>/<information>/<result> . 4. The final output tags are formulated as <answer>/<solution> . We specifically chose the following four representative tasks for demonstration: Mathematical Reasoning with Python Executor, Knowledge Q&A with Search Retriever, Multi-turn SQL Generation, and Agentic Web Search. For detailed explanation of tag definitions, please refer to Tab.9 and corresponding settings in Appendixs Sec. A. For Agentic Web Search, Multi-turn SQL Generation, and Knowledge Q&A with Retriever, the text-heavy responses from the tools are truncated or simplified. 27 Work in Progress Table 10: Example answer demonstration of VT-MATH Example from DeepMath Dataset Question: Determine the function given that it has power series representation centered at 0, converging within the interval (1, 1), and there exists sequence of distinct points (xn) approaching 0 such that (xn) = 0 for all N. What is ? Labeled Answer: Model Output by VT-Math GRPO Lets consider the problem. The function has power series representation centered at 0 that converges within the interval (1, 1). This means that (x) can be written as: (x) = (cid:80) n=0 anxn where the series converges for < 1. We are given that there exists sequence of distinct points (xn) approaching 0 such that (xn) = 0 for all N. This implies that: (xn) = (cid:80) = 0 Since the series converges at xn and xn 0, we can conclude that each an = 0 for all 1 because if any ak = 0 for some 1, then the series would not converge to 0 as xn 0. Therefore, the only possible term is the constant term a0. This means we can write: n=0 anxn Given that (0) = a0 = 0 (since (xn) (0) = 0 as xn 0), we have: (x) = a0 (x) = Therefore, the function is the zero function. Lets put this understanding into more formal version: python from sympy import symbols, Function, Eq, solve # Define the variable and the function = symbols(x) = Function(f)(x) # Define the power series representation centered at 0 # Since we are given that there exists sequence (x_n) approaching 0 such that f(x_n) = 0 for all # It implies that the function is the zero function f_zero = 0 # Print the zero function print(f_zero) output result 0 The power series representation of the function converging within the interval (1, 1), given that there exists sequence of distinct points (xn) approaching 0 such that (xn) = 0 for all N, results in the function being the zero function. Therefore, the function is 0 . 28 Work in Progress Table 11: Example answer demonstration of VT-SEARCH Example from HotpotQA Dataset Question: What is the birth name of Nadeem Siddiques favorite boxer? Labeled Answer: Walker Smith Jr. Model Output by VT-Search (GRPO) <think> To answer this question, need to find out Nadeem Siddiques favorite boxer and then determine the birth name of that boxer. </think> <search> Who is Nadeem Siddiques favorite boxer? </search> <information> Doc 1(Title: \"Nadeem Siddique\") Nadeem Siddique Siddique was born in Bradford, Yorkshire, United Kingdom. He supports his local football team Bradford City and his favorite boxer is Sugar Ray Robinson... Doc 2(Title: \"Nadeem Siddique\") in 2005 for the people of Palestine and Gaza whereby he raised 352,000. He stated Me and Junior have been doing loads of things for charities and with our popularity in Bradford we always raise quite few quid... Doc 3(Title: \"Nadeem Siddique\") 3 fights and 3 wins and is on the verge of major championship fight. Siddique is the founder of Make the Weight gym; the opening of the gym was attended by boxer Mike Tyson in whose... </information> From the search results, found that Nadeem Siddiques favorite boxer is Sugar Ray Robinson. <search> What is the birth name of Sugar Ray Robinson? </search> <information> Doc 1(Title: \"Sugar Ray Robinson\") Robinson was born Walker Smith Jr. in Ailey, Georgia, to Walker Smith Sr. and Leila Hurst. Robinson was the youngest of three children... Doc 2(Title: \"Sugar Ray Robinson\") disease. He died in Los Angeles at the age of 67 and was interred in the Inglewood Park Cemetery, Inglewood, California. Robinson married Marjorie Joseph in 1938; the marriage was annulled the same year. Their son, Ronnie Smith... Doc 3(Title: \"Sugar Ray Robinson\") Jackie Gleason, Nat King Cole, Joe Louis, and Lena Horne among others. Robinson was known as flamboyant personality outside the ring. He combined striking good looks with charisma and flair for the dramatic... </information> From the search results, found that the birth name of Sugar Ray Robinson is Walker Smith Jr. <answer> Walker Smith Jr. </answer> 29 Work in Progress Table 12: Example answer demonstration (SQL reasoning) Example (StudentPet DB) Question: What are the ids of the students who do not own cats as pets? Labeled Answer: SELECT stuid FROM student EXCEPT SELECT T1.stuid FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid = = T2.petid WHERE T3.pettype T2.stuid JOIN pets AS T3 ON T3.petid = cat Model Output (Agentic SQL) <think> need to find the student IDs of students who do not own cats as pets. To achieve this, will first identify all students who own cats and then exclude them from the list of all students. </think> <sql> SELECT StuID FROM Has_Pet WHERE PetID IN (SELECT PetID FROM Pets WHERE PetType = cat); </sql> <observation> 0 1001 <reminder>You have 5 turns left to complete the task.</reminder> </observation> <think> have the list of student IDs who own cats. Now, will use NOT IN clause to exclude these IDs from the list of all students. </think> <sql> SELECT StuID FROM Student WHERE StuID NOT IN (SELECT StuID FROM Has_Pet WHERE PetID IN (SELECT PetID FROM Pets WHERE PetType = cat)); </sql> <observation> 0 1002 1003 ... <reminder>You have 4 turns left to complete the task.</reminder> </observation> <think> have the list of student IDs who do not own cats as pets. Since the question asks for the ids of the students, can now output the final SQL as the solution. </think> <solution> SELECT StuID FROM Student WHERE StuID NOT IN (SELECT StuID FROM Has_Pet WHERE PetID IN (SELECT PetID FROM Pets WHERE PetType = cat)); </solution> 30 Work in Progress Table 13: Example answer demonstration of VT-DEEPSEARCH Example from Gaia Dataset Question: Find the value of to the nearest tenth: Lx = (d/dx * (A * x-squared)) + 4-thousandnninety-7 minus Where is the last two digits of the year of the Venezuelan Declaration of Independence, is the number of colors in the TikTok logo as of July 2023, excluding black and white, and is the height of the average woman in the Philippines according to July 2023 Business Insider article, rounded to the nearest whole centimeter. Labeled Answer: 563.9 Model Output by VT-DeepSearcher (GRPO) <think> Okay, lets try to figure out this problem step by step. First, need to understand what each part of the equation means and find the values for L, A, and C. Then can plug them into the equation to solve for x. The equation given is Lx = (d/dx * (A * x-squared)) + 4-thousandnninety-7 minus C. Wait, the wording here is bit confusing. Let me parse it again. Maybe its multiplied by equals the derivative of (A times squared) plus 4097 minus C. Or maybe theres typo? Let me check the original question again. The equation is written as: Lx = (d/dx * (A * x-squared)) + 4-thousandnninety-7 minus C. Hmm, the \"4thousandnninety-7\" part might be way to write 4097. So maybe its 4097. So the equation is Lx equals the derivative of squared plus 4097 minus C. <search> Venezuelan Declaration of Independence year </search> <result> The Venezuelan Declaration of Independence occurred on **July 5, 1811**. The last two digits of this year are **11**, which corresponds to L... </result> So = 11. <search> TikTok logo colors 2023 </search> <result> The TikTok logo as of 2023 features **two colors excluding black and white** : pink (#FE2C55) and aqua/cyan (#25F4EE). Thus = 2... </result> So = 2. <search> average height of women in Philippines 2023 Business Insider </search> <result> The average height of women in the Philippines is **149.6 cm**, rounded to **150 cm**. Thus = 150... </result> So = 150. Now the equation is: Lx = (d/dx(Ax2)) + 4097 C. Derivative of Ax2 is 2Ax = 4x. Substituting = 11, = 2, = 150: 11x = 4x + 4097 150 = 4x + 3947. Simplify: 7x = 3947 = 3947/7. <python> 3947 / 7 </python> <result> 563.9 </result> <answer> The final answer is 563.9 . </answer> </think>"
        }
    ],
    "affiliations": [
        "HKUST",
        "Independent",
        "National University of Singapore",
        "NetMind.AI",
        "Sea AI Lab",
        "Shanghai University",
        "University of Toronto",
        "University of Waterloo"
    ]
}