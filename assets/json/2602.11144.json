{
    "paper_title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
    "authors": [
        "Ruichuan An",
        "Sihan Yang",
        "Ziyu Guo",
        "Wei Dai",
        "Zijun Shen",
        "Haodong Li",
        "Renrui Zhang",
        "Xinyu Wei",
        "Guopeng Li",
        "Wenshan Wu",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\\textbf{GENIUS}$ ($\\textbf{GEN}$ Fluid $\\textbf{I}$ntelligence Eval$\\textbf{U}$ation $\\textbf{S}$uite). We formalize $\\textit{GFI}$ as a synthesis of three primitives. These include $\\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\\textbf{GENIUS}$ establishes a rigorous standard for $\\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$."
        },
        {
            "title": "Start",
            "content": "GENIUS: Generative Fluid Intelligence Evaluation Suite Ruichuan An * 1 Sihan Yang * 1 Ziyu Guo 2 Wei Dai 1 Zijun Shen 1 Haodong Li 3 Renrui Zhang 2 Xinyu Wei 4 Guopeng Li 3 Wenshan Wu 5 Wentao Zhang 1 6 2 0 2 1 1 ] . [ 1 4 4 1 1 1 . 2 0 6 2 : r Abstract Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GENerative Fluid Intelligence EvalUation Suite). We formalize GFI as synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose training-free attention intervention strategy. Ultimately, GENIUS establishes rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS. 1. Introduction Unified Multimodal Models (UMMs) have witnessed remarkable progress recently (Team, 2024; Chen et al., 2025; Xie et al., 2024), delivering impressive results across diverse *Equal contribution Project leader Corresponding author 1Peking University 2CUHK 3StepFun 4PolyU 5MSRA. Correspondence to: Wentao Zhang <wentao.zhang@pku.edu.cn>. Preprint. February 12, 2026. 1 tasks (An et al., 2025; Li et al., 2025a; Jiang* et al., 2025). Benefiting from the fusion of understanding, UMMs are capable of processing complex, interleaved contexts and exhibiting extensive world knowledge to reshape the generative paradigm. Consequently, they are widely regarded as milestone on the path toward Artificial General Intelligence (AGI). However, this rapid advancement invites natural question: How far are current UMMs from achieving true general intelligence regrading visual generation? To investigate this problem, drawing upon existing literature (Cattell, 1963; Schipolowski et al., 2014; Kent, 2017), we deconstruct General Intelligence in visual generation into two primary components: Crystallized Intelligence (CI) and Fluid Intelligence (FI). Current development and evaluation focus of UMMs mainly targets CI (i.e., the capacity for memorization and retrieval of pre-trained knowledge). For instance, models ability to generate flawless cat often stems from exposure to billions of instances during training, followed by probabilistic reproduction during inference. However, this trend has severely masked critical but long-ignoring deficiency concerning FI of visual generation skills, termed Generative Fluid Intelligence (GFI), (i.e., the ability to perform inducing, reasoning and ad-hoc adaptation in novel scenarios). As shown in Fig. 1, the Simple Constraint task requires the model to identify ad-hoc rules (e.g., abstract symbol denotes rain) and apply them to the visual output, instead of just retrieving static concepts. Despite its critical importance, research along this direction remains limited (shown in Tab. 1): First, formal definition is absent. This theoretical void impedes the foundational guidance, which is necessary for steering UMMs toward general intelligence. Second, benchmarks are inadequate. Current evaluations predominantly assess model memorization and retrieval, failing to disentangle static knowledge to probe the true bounds of general intelligence. Third, systematic analyses are lacking. The lack of investigations into the failure modes leaves critical questions of why models fail and how to improve unanswered. To bridge these gaps, we introduce GENIUS (GENerative Fluid Intelligence EvalUation Suite), the first framework dedicated to the systematic evaluation of GFI. Drawing GENIUS: Generative Fluid Intelligence Evaluation Suite Figure 1. An overview of GENIUS benchmark. It is hierarchically structured into three dimensions, five tasks, and diverse sub-tasks. from the Cattell-Horn-Carroll (CHC) theory (Schneider & McGrew, 2012), we distill three core primitives of FI: (I) Inductive Inference, (II) Abstract Dynamic Reasoning and (III) Adaptive Inhibition. We materialize these theoretical concepts into three corresponding dimensions within GENIUS. To ensure fine-grained assessment, we further construct five novel and well-designed tasks that specify concrete capabilities within each dimension. For each task, we employ hybrid evaluation comprising three metrics: (I) Rule Compliance, which challenges the models precision in following ad-hoc rules; (II) Visual Consistency, which assesses the stability of generated attributes under logical constraints; and (III) Aesthetic Quality, which demands that the model maintains fundamental aesthetic standards. Through manual curation, our suite features tasks well designed by multimodal experts. Unlike traditional benchmarks that prioritize static world knowledge, generation quality or safety, we ensure that every sample presents dynamic and novel rule, strictly decoupling static knowledge to offer pure quantification of the models GFI capabilities. With GENIUS, we systematically evaluate 12 representative open-source and proprietary models. To ensure evaluation robustness, we provide manually annotated hints for each test case, which have undergone at least three rounds of cross-validation to support unbiased hybrid evaluation. Overall, our results reveal clear gaps between current stateof-the-art (SOTA) models and general intelligence. Surprisingly, pre-planning and post-reflection yield marginal gains. These findings expose under-explored deficiencies in current generative models, highlighting the urgent need to advance fluid intelligence in the next generation of UMMs. Building on these findings, we move beyond evaluation to investigate the underlying mechanisms of failure. Taking Bagel (Deng et al., 2025) as an example, by visualizing the attention distribution of it, it surprisingly shows irregular noise and spikes across the multimodal context. This indicates that the model struggles to accurately focus on critical new rules in the context. Inspired by the theoretical perspective of In-Context Learning (ICL) as Implicit FineTuning (Dherin et al., 2025), we demonstrate that ICL process is mathematically equivalent to update specific model parameters while generation. Then, we offer possible view: the imbalanced attention distribution results in insufficient guidance during implicit gradient descent, which causes the 2 GENIUS: Generative Fluid Intelligence Evaluation Suite Table 1. Comparison of representative benchmarks. * denotes understanding tasks. indicates partial satisfaction (e.g., For Manual Curated/Annotation, it implies combination of human curation and automatic methods). GENIUS pioneers Fluid Intelligence evaluation, featuring multi-image input, multimodal interleaved context, hybrid metrics, and pure manually curated and annotated testcases. Benchmark # Samples Multi-Image Input Fluid Intelligence Multimodal Interleaved Context GenEval (Ghosh et al., 2023) WISE (Niu et al., 2025) RISE (Zhao et al., 2025) DPG-Bench (Hu et al., 2024) DreamBooth (Ruiz et al., 2023) UnifyBench (An et al., 2025) Tiif-Bench (Wei et al., 2025) OpenING* (Zhou et al., 2025) UniEval* (Li et al., 2025d) MME-Unify* (Xie et al., 2025b) RealUnify* (Shi et al., 2025) ROVER (Liang et al., 2025) WEAVE (Chow et al., 2025) GENIUS (Ours) 2.2k 1.0k 360 1.0k 3.0k 0.1k 5.0k 5.4k 4.2k 4.1k 1.0k 1.3k 0.1k 0.5k Task Dimension Implicit Pattern Induction Explicit Constraint Execution Contextual Knowledge Adaptation Hybrid Evaluation Manual Curated Annotation gradient direction vague or stochastic, failing to overcome the inertia of pre-trained priors. Based on this, we design training-free mechanism as strong baseline. The results show consistent performance gains across all tasks, which not only validates the effectiveness of our method but also corroborates the rationality of our theoretical framework. Execute Ad-hoc Constraints (Abstract Dynamic Reasoning): Executing logical reasoning within the bound of ad-hoc defined visual or symbolic constraints. Adapt based on Contextual Knowledge (Adaptive Inhibition): Adjusting based on contextual cues, even when necessitating deviation from established common sense. In summary, our core contributions are as follows: We formally define Generative Fluid Intelligence (GFI), filling theoretical void to provide foundational guidance for steering UMMs toward general intelligence. We introduce GENIUS, the first benchmark designed to purely quantify GFI. It features 510 expert-curated samples spanning three dimensions and five tasks, supported by robust hybrid evaluation protocol. We systematically evaluate 12 representative open-source and proprietary models. Results reveal significant deficits in SOTA models, underscoring the clear performance gap. We trace GFI failures via theoretical and empirical analysis. Then, we propose training-free strategy that boosts performance, effectively activating the models GFI. 2. GENIUS 2.1. Benchmark Overview Grounded in the Cattell-Horn-Carroll (CHC) theory (Schneider & McGrew, 2012), General Intelligence is defined not merely by Crystallized Intelligence but more fundamentally by Fluid Intelligence, which is the capacity to induce, reason, and adapt in novel scenarios, independent or even contrary to prior knowledge. Drawing from this, we formally define Generative Fluid Intelligence (GFI) for visual generation as the synthesis of three core primitives, where humans effortlessly demonstrate these capabilities through: Induce Implicit Pattern (Inductive Inference): Distilling implicit patterns and intrinsic attributes from observations. However, these capabilities still present significant challenges for current UMMs. To objectively assess model performance in these areas and pinpoint limitations, we introduce GENIUS (GENerative Fluid Intelligence EvalUation Suite), the first benchmark dedicated to evaluate Generative Fluid Intelligence. As illustrated in Fig. 5, the suite comprises total of 510 expert-curated samples spanning 20 diverse sub-tasks, structurally distributed across three core dimensions: 86 for Implicit Pattern Induction, 213 for Adhoc Constraint Execution, and 211 for Contextual Knowledge Adaptation. Unlike previous benchmarks that prioritize static world knowledge, generation quality or safety, GENIUS is constructed to strictly exclude prior knowledge. It systematically evaluates models across (I) Inductive Inference, (II) Abstract Ad-hoc Reasoning, and (III) Adaptive Inhibition, as aforementioned, purely quantifying their aptitude for solving novel problems. 2.2. Benchmark Construction For each category of GENIUS, we curate diverse set of high-quality, expert-designed test cases. Each instance comprises multi-modal interleaved context and removal of any single modality from context makes the task unsolvable. Implicit Pattern Induction: This dimension mainly contains novel task: Implicit Pattern Generation, which assesses the capacity to deduce unstated visual preferences from context and apply them for generation. As shown in Fig. 1, the interleaved input presents images of varying styles alongside specific user preferences. During testing, 3 GENIUS: Generative Fluid Intelligence Evaluation Suite the model is required to induce the target stylistic pattern based on these preferences and manifest it in the generated output. This task necessitates the integration of both modalities: relying solely on images would cause the model to blindly conflate visual features, while relying solely on text would leave the stylistic preference undefined, causing the model to collapse into its pre-trained distribution. Ad-hoc Constraint Execution: significant ability of FI is to perform dynamic reasoning under newly defined, ad-hoc rules. To systematically assess this, we construct two complementary tasks: (I) Visual Constraint Generation (II) Symbolic Constraint Generation, where novel meanings are assigned to symbols or images within the context. Crucially, we deliberately select elements that are devoid of pre-existing semantic associations, such as an image patch (e.g., defining plain blue square as an operation to remove the specifc object) or mathematical symbol (e.g., defining function as an instruction to make the object melt). This design reflects the models capability to solve novel problems by reasoning abstractly. The absence of either visual or textual modalities would fundamentally compromise the establishment of these ad-hoc rules, making it impossible to link the element to its new definition. Contextual Knowledge Adaptation: model possessing FI must exhibit flexible adaptation rather than rigid adherence to pretrained knowledge. We introduce two novel tasks to evaluate this: (I) Prior-Conflicting Generation: The model must reason based on newly defined common sense presented in the context (e.g., object weight is determined by color), even if it contradicts established facts. (II) MultiSemantic Generation: This task requires the model to discern whether to interpret concept literally or metaphorically (e.g., distinguishing green hand as novice vs. green skin) based on the specific multimodal contexts. In both tasks, missing any modality prevents the precise definition of new knowledge or the clarification of semantics, causing the model to fail in dynamic adaptation. 2.3. Evaluation Metric Evaluating the ability of GFI remains challenging task. Recent studies (Gu et al., 2024) have shown that large multimodal models (LMMs) exhibit strong visual reasoning and alignment capabilities, making them suitable as evaluators. Following this, we construct robust pipeline comprising three orthogonal metrics. We utilize the frontier LMM (Gemini-3-Pro (Google DeepMind, 2025)) as the judge, employing structured prompts (Detailed in the Appendix A.2) and hybrid evaluation strategy that incorporates manuallycurated hints to ensure rigorous quantification. All metrics are assigned score of 0 (fail), 1 (partial), or 2 (perfect). Rule Compliance This metric measures the models accuracy in executing strict ad-hoc rules. Since GFI tasks involve precise constraints (e.g., specific symbolic, layouts or palette), relying on the LMMs unguided interpretation is unreliable. We therefore adopt hybrid protocol that grounds automated evaluation in human-verified truth. For each sample, we provide manually curated eval-hint serving as the gold standard. The LMM evaluator strictly compares the output against the specific nouns, adjectives and spatial predicates defined in this eval-hint. Visual Consistency For some GFI tasks, the visual identity of original objects must remain unchanged (e.g., specific characters or objects). This metric assesses the models ability to preserve context during dynamic reasoning. We provide specific hybrid eval-hints for each sample that identify the key visual elements from the reference images. The evaluator verifies the stability of these elements in the generated image, ensuring the model does not hallucinate or discard critical context while following instructions. Aesthetic Quality Generative outputs must maintain physical coherence even when processing novel or conflicting semantic inputs. We employ specific prompt to evaluate aesthetic quality, focusing on anatomical logic, lighting, and the presence of AI artifacts (e.g., distorted limbs). This metric ensures that the models adaptation to new rules does not come at the cost of basic visual realism. 3. Experiment We conduct comprehensive evaluation of 12 representative open-source and proprietary models. The open-source model comprises Qwen-Image-Edit-2511 (Wu et al., 2025b), GLM-Image (Zhipu AI Team, 2026), FLUX.2-dev (Labs, 2025), NextStep-1 (Team et al., 2025), Emu3.5-Image (Cui et al., 2025) and Bagel (Deng et al., 2025). The proprietary category includes leading commercial models: Nano Banana (Google, 2025a) and its Pro variant (Google, 2025b), SeeDream series (4.0 & 4.5) (Seedream et al., 2025) and GPT-Image (OpenAI Team, 2025). As outlined in Sec. 2.3, we employ Gemini-3-Pro (Google DeepMind, 2025) as the evaluator. To mitigate stochastic variance and ensure robustness, we report the final scores as the average of three independent runs for each sample. The quantitative results are shown in Tab. 2. Given the diversity of multimodal input formats, we adopt interleaved inputs for models capable of processing them, while utilizing decoupled format for those that do not. Further ablation studies concerning interleaved formats are in the Appendix D.1. 3.1. Main Results Generative Fluid Intelligence (GFI) remains significant bottleneck for current models. Our results reveal stark reality: even the state-of-the-art proprietary model, Nano Banana Pro, achieves an overall score of only 57.19, falling 4 GENIUS: Generative Fluid Intelligence Evaluation Suite Table 2. Main Results. We evaluate models across dimensions. The Overall column represents weighted score across all tasks, calculated using ratio of RC:VC:AQ = 6:3.5:0.5. Ours is implemented on Bagel. The best and second best performances are highlighted. Method Interleaved Overall Implicit Pattern Symbolic Constraint Visual Constraint Prior-Conflicting Multi-Semantic Implicit Pattern Induction Ad-hoc Constraint Execution Contextual Knowledge Adaptation Nano Banana Pro Nano Banana GPT-Image SeeDream 4.0 SeeDream 4.5 Qwen-Image GLM-Image FLUX.2-dev NextStep-1 Emu3.5-Image Omini-Gen2 Bagel Ours RC VC AQ RC VC AQ RC VC AQ RC VC AQ RC VC AQ Proprietary Models 57. 50.66 47.15 21.26 52.84 30.58 24. 34.39 10.44 36.67 27.87 26.74 66. 56.47 58.14 12.05 70.00 36.18 32. 34.30 10.74 41.86 29.07 26.74 44. 39.04 41.92 0.70 59.59 27.69 19. 27.70 0.40 35.81 26.35 27.03 96. 94.12 93.60 96.39 97.06 71.05 93. 88.95 25.12 83.72 76.16 84.30 71. 50.00 92.11 76.67 66.67 96.67 52. 41.38 90.59 35.45 60.46 51.91 90. 68.33 79.17 93.33 35.50 39.47 91. 30.28 58.82 32.82 93.79 49.17 62. 92.50 43.50 33.33 90.00 28.64 21. 3.44 84.64 40.00 4.17 76.67 30. 10.34 82.67 30.73 62.91 41.09 94. 58.33 62.50 86.67 40.10 41.38 92. 35.00 Open-Source Models 36.18 27.69 71.05 26. 45.83 55.83 27.72 20.69 71.78 25. 22.37 21.15 87.50 27.50 12.50 70. 20.30 15.52 71.29 17.73 35.76 31. 87.09 39.17 50.00 59.17 25.25 30. 84.16 29.82 11.33 2.54 21.67 21. 4.20 29.17 15.49 7.55 28.71 12. 34.97 39.31 86.93 24.17 29.17 42. 26.24 37.93 82.18 32.87 25.33 30. 77.96 11.67 41.67 52.50 23.76 34. 69.80 19.27 29.61 16.03 76.32 22. 12.50 49.17 22.28 17.24 74.75 33. 32.92 39.54 44.92 66.71 36.54 26. 67.11 30.45 35.11 47.84 23.67 36. 57.78 34.22 - - - - - - - - - - - - - 95.00 93.12 85. 80.00 86.82 69.55 70.91 79.82 20. 75.46 63.76 53.67 52.75 short of passing grade. Meanwhile, representative opensource models like Bagel fall significantly behind, scoring mere 26.74. These tasks demand ad-hoc reasoning and dynamic adaptation to novel rules, which are less directly grounded by the models pre-trained parametric knowledge. Together, these quantitative deficits suggest that while current UMMs have acquired robust capabilities for crystallized reproduction, they remain fundamentally distant from the fluid adaptability required for general-purpose generation. Current models fail to effectively arbitrate the conflict between pre-trained priors and the given context. As shown in Tab. 2, this deficiency is most pronounced in the Contextual Knowledge Adaptation dimension, where performance consistently drops below other task categories. When ad-hoc instructions explicitly contradict world knowledge (e.g., counter-intuitive physical laws or remapped semantics), models exhibit strong cognitive inertia, frequently defaulting to their pre-trained priors. This suggests that existing architectures lack robust mechanism to inhibit intrinsic priors, failing to dynamically adapt to the context. Aesthetic fidelity masks deep logical deficiencies. Our hybrid metric analysis uncovers pervasive illusion of competence: models consistently maintain high Aesthetic Quality scores, yet their performance on Rule Compliance lags substantially behind. This discrepancy suggests that previous model optimization has disproportionately focused on surface-level visual plausibility at the expense of deep context interpretation and logical adherence. By exposing this, GENIUS signals necessary paradigm shift for next generation of models: moving beyond merely generating beautiful pixels to achieving profound context comprehension and ensuring logically correct visual synthesis. 3.2. Discussion and Analysis Pre-planing and post-reflection yield marginal gains. We investigated various inference-time enhancement strategies to potentially mitigate performance deficits. Taking Nano Banana Pro and Bagel as examples, we implemented preplanning (activating reasoning mode) and post-reflection (an iterative process where initial generations are evaluated and re-fed as context for refinement). However, as illustrated in Fig. 2(a), empirical results across both Nano Banana Pro and Bagel indicate that these strategies yield only marginal gains. This suggests that current architectures struggle to effectively leverage explicit reasoning for generation. Context comprehension is the key to solve GFI problems. To isolate the source of failure, we introduced humancurated hints to guide the generation process. Specifically, we employed progressive intervention strategy: initially utilizing text-only hints, and subsequently constructing multimodal hints to ensure information completeness, thereby explicitly guiding the models generation. The results are illustrated in Fig. 2(a). This intervention resulted in substantial performance improvements; however, the degree of improvements varied significantly: Nano Banana Pro exhibited much more boost compared to Bagel. This observation highlights that accurate context comprehension acts as critical factor in solving GFI tasks. Meanwhile, solving GFI problems requires not only accurate context comprehension to decode ad-hoc rules but also robust intrinsic model capabilities to execute them, implying that comprehension aids cannot fully compensate for weaker GENIUS: Generative Fluid Intelligence Evaluation Suite Figure 2. Diagnostic analysis and metric validation. (a) Performance comparison across different context settings. (b) Analysis of the gap between context comprehension (VQA) and generation capabilities. (c) Correlation analysis validating the LMM-as-a-Judge metric. base models generative limitations. Generative failure primarily stems from an execution gap rather than comprehension deficits. To investigate the root cause, we reformulated the generative tasks into comprehension-oriented Visual Question Answering (VQA) probes. Specifically, we structured these probes as multiplechoice questions that query the model regarding the expected visual appearance of the target image. We utilized our expert hints for Rule Compliance as the ground truth answers, while simultaneously constructing three distractors for each sample to facilitate evaluation. The results are shown in Fig. 2(b). Empirical results reveal significant disparity: models frequently demonstrate an accurate understanding of the contexts intent but fail to translate this into compliant visual outputs. This suggests that the models current cognitive processing of the context, while sufficient for discriminative understanding tasks, lacks the granularity required for generative reconstruction. We hypothesize this stems from two factors: first, the high information density of interleaved contexts, where fine-grained visual nuances (e.g., specific textures) are difficult to fully capture and articulate through limited modalities; and second, structural inefficiency in current UMM architectures, where rich semantic understanding from the encoder is not effectively propagated to the generative decoder, resulting in knowbut-cannot-draw phenomenon. We further discuss how to enhance this critical contextual comprehension in Sec. 4.1. 3.3. Validity of LMM-as-a-Judge To verify the reliability of using LMMs as judge, we conducted an analysis of the correlation between LMM-based automated scoring and human expert judgment. We performed study by randomly and uniformly sampling 100 output images across various dimensions from two representative models: Nano Banana Pro and Bagel. Five human experts were invited to independently rate these samples, adhering to the same metrics used by the LMM evaluator to compare the consistency between human and LMM scoring. As shown in Fig. 2(c), the Pearson correlation between human expert ratings and LMM-based scores demonstrates high degree of alignment. Our analysis reveals exceptionally strong global consistency across all samples: the Pearson correlation coefficient (r) reaches 0.9630 for NanoBanana Pro and 0.9659 for Bagel. Such high linear correlation indicates that the LMM evaluator accurately captures the underlying logic of human judgment in image generation tasks. Furthermore, dimension-specific analysis shows that the Mean Absolute Error (MAE) remains consistently low across multiple metrics, ranging from 0.06 to 0.11. Relative to the 02 scoring scale, these errors are quite small, further validating the robustness of the evaluation framework across different models and task dimensions. In conclusion, the LMM-as-a-Judge framework serves as reliable and effective alternative to human evaluation. To further ensure the reproducibility and cross-model robustness, we extended our validation to include the open-source Qwen2.5-VL-72B (Bai et al., 2025) as the judge. Empirical results shown in the Appendix indicate that while Qwen2.5-VL-72B tends to assign systematically lower absolute scores compared to Gemini-3-Pro, suggesting stricter evaluation criterion. The relative performance trends and model rankings remain identical. This consistency across proprietary and open-source evaluators confirms that the observed performance gaps are intrinsic to the models being tested rather than artifacts of specific judge, thereby reinforcing the reliability and generalizability of the results. 4. Potential Solution Evaluation on GENIUS reveals clear gap between current SOTA models and general intelligence. To diagnose the potential causes of this deficit, we conduct comprehensive analysis from both theoretical and empirical perspectives, GENIUS: Generative Fluid Intelligence Evaluation Suite Figure 3. Visualization of attention scores (range [0, 1]). Left: Existing models. Right: Ours. Figure 4. Method overview. Guided by the theoretical insight that attention magnitude dictates gradient norms (a), we implement three-stage pipeline (b) to explicitly suppress noise tokens and rectify the implicit optimization direction. focusing on the widely applicable Bagel framework. 4.1. Experimental Observation To investigate the underlying mechanism of failure, we visualized the attention distribution over the entire context, using the image tokens generated during the process as the query. Surprisingly, as shown in the left part of Fig. 3, we found the attention distribution is unreasonable: it exhibits irregular noise and stochastic spikes across the multimodal context. This indicates the model struggles to precisely capture pivotal ad-hoc rules from the context. Instead of pinpointing the critical definition, the attention is spread out indiscriminately across the input. As result, the model fails to extract the specific signal needed for adaptation and simply falls back to its pre-trained priors. 4.2. Theoretical Analysis To explain this phenomenon, we adopt the theoretical perspective of In-Context Learning (ICL) as Implicit FineTuning from (Dherin et al., 2025; Ahn et al., 2023; Dai et al., 2023; von Oswald et al., 2023). we perform derivation on Bagel, which adopts Mixture-of-Transformer architecture. Since GENIUS targets the generative task, we redefine the function A(u, g) from (Dherin et al., 2025), based on the Bagel model, where denotes the network layer component responsible for context processing, represents the encoding of context and instructions, and denotes the encoding of intermediate noisy tokens of images. We suppose in the t-th step and the l-th Decoder blocks we have g(t,l+1) = L(l) Up,b(u(l), g(t,l)), where Up is projection layer in the decoder block, is the bias of Down layer, and represents the l-th blocks forward propagation. And then we can formalize the relationship between and the (Up, b): Theorem 4.1. The layer update satisfies following property: LUp+Up, b+b(u, g) = LUp,b(u, g) where the bias perturbation is defined as: = A(u, g) A(u, g) (1) (2) and the upsampling operator perturbation be defined as: Up = Up (δA) (A(u, g)) (A(u, g)) , (x) = RMS(x) (3) And the normalized attention difference is given by: δA = (A(u, g)) (A(u, g)) (4) According to Thm. 4.1, we can demonstrate that in multimodal generation, the ICL process is mathematically equivalent to updating specific model parameters. This successfully extends the conclusions of (Dherin et al., 2025): 7 GENIUS: Generative Fluid Intelligence Evaluation Suite We first formalize the vector notations for clarity: let = (u1, . . . , un), and u(i) = (u1, . . . , ui). Then we hope: LUpi,bi(u(i), g) = LUp,b(u, g) = 1, 2, . . . , (5) From this objective, we derive expressions for Upi and bi: Upi = Up + Upi = Up + Up (δAi) (A(g)) (A(g))2 bi = + bi = + A(u(i), g) A(g) where the attention difference term δAi is defined as: δAi = A(u(i), g) A(g). (6) (7) (8) Based on the above derivations, we present the key theorem for iterative parameter updates: Theorem 4.2. For the (i + 1)-th iteration, Up and follow the gradient descent update rules below: Distillation phase, leveraging the semantic reasoning capability of Bagel, we prompt the model to distill task-critical visual cues into set of region-specific keywords (the prompt is detailed in the Appendix F.1). Subsequently, during Relevance Mapping, we compute semantic relevance map by evaluating the alignment between these keywords and the visual context tokens, where serves as proxy for the tokens contribution to the effective gradient signal. Finally, via Bias Injection, we inject spatial bias F(S) directly into the attention logits: Attention = softmax (cid:18) + λ F(S) (cid:19) (10) This formulation ensures tokens with high relevance are emphasized while noise is suppressed. The detailed mathematical formulation is provided in the Appendix F.2. By rectifying the attention landscape, we re-weight the implicit gradient updates, deterministically steering the optimization trajectory to overcome pre-trained priors. (cid:40) Upi+1 = Upi hUpLi(Upi), bi+1 = bi (cid:0)tr (cid:0)δ (cid:1)(cid:1) bi 4.4. Experimental Results (9) (cid:17) (cid:16)ˆδi where = 1/ (A(g))2 denotes the learning rate. Up(cid:1) is loss function, among which Li(Up) = tr (cid:0) (A(g)), ˆδi = (A(u(i), g)) = Up (A(u(i+1), g)), and δi = A(u(i), g) A(u(i+1), g). Combining empirical observations with theoretical analysis, we offer hypothesis for the deficit in GFI: The imbalanced attention distribution results in lack of guidance during implicit gradient descent. Consequently, the descent direction becomes stochastic, failing to overcome the pre-trained priors. Full proof of both theorems is in the Appendix G. As visualized in Fig. 3 (Right), our mechanism successfully rectifies the originally disordered attention landscape into sharpened distribution with distinct peaks focused on critical tokens. Quantitative results in Tab. 2 further demonstrate consistent performance gains across nearly all dimensions compared to the baseline Bagel (e.g., boosting the Overall score of 6.18%). This validates that deterministically steering the implicit gradient trajectory effectively activates the models latent GFI without requiring parameter updates. Consequently, this mechanism establishes strong baseline, offering simple paradigm for improving GFI capabilities. 5. Conclusion 4.3. Attention Adjustment Mechanism Guided by Thm. 4.2, we recognize that the magnitude of attention assigned to the context directly dictates the norm of the implicit gradient update. The irregular attention distribution within the context images, as previously observed in Sec. 4.1, implies that irrelevant noise tokens currently contribute significant, erroneous gradient components, thereby diverting the optimization trajectory away from the optimal path. To counteract this, we propose training-free adjustment mechanism to recalibrate the update direction. By explicitly suppressing the attention weights of noise tokens, we mathematically dampen their corresponding gradient norms (i.e., Upnoise 0), ensuring the implicit fine-tuning is driven solely by critical context signals. Specifically, we implement this mechanism through threestage pipeline shown in Fig. 4(b). First, in the Keyword In this paper, we introduced GENIUS, the first benchmark dedicated to systematically quantifying Generative Fluid Intelligence (GFI). By grounding in the Cattell-Horn-Carroll (CHC) theory, we formalized GFI into three core dimensions, including Implicit Pattern Induction, Ad-hoc Constraint Execution, and Contextual Knowledge Adaptation, providing rigorous standard for assessing model capability in novel, reasoning-intensive scenarios. Through systematic evaluation of 12 representative open-source and proprietary models, we reveal stark reality: even state-of-the-art models like Nano Banana Pro fall short of passing grade, while open-source models exhibit significant performance deficits. Our analysis exposes critical execution gap, where models struggle to arbitrate conflicts between pre-trained priors and ad-hoc context, often prioritizing aesthetic fidelity over logical rule compliance. Furthermore, we partially trace these failures to attention mechanism defects during inferGENIUS: Generative Fluid Intelligence Evaluation Suite ence and propose training-free adjustment strategy that effectively activates latent GFI capabilities. We hope that GENIUS will serve as pivotal testbed for future research, guiding the evolution of next-generation models from crystallized memorization toward true general intelligence."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents benchmark and theoretical framework aimed at advancing the evaluation of Fluid Intelligence in generative models. By distinguishing Generative Fluid Intelligence (GFI) from standard crystallized knowledge retrieval, our work intends to shift the community focus toward developing systems that possess true adaptability and logic-grounded control. Our contributions align with the goal of creating more robust AI systems by highlighting the illusion of competence, phenomenon where aesthetic quality masks logical deficiencies. This focus encourages transparent evaluation and prevents the deployment of models that appear capable but fail in critical, rule-bound scenarios. Furthermore, improved GFI capabilities may contribute to versatile creative tools and scientific visualization assistants that can accurately follow complex, ad-hoc instructions without hallucination. We do not foresee any unique negative societal consequences beyond those already recognized in the broader field of generative AI."
        },
        {
            "title": "References",
            "content": "Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. Transformers learn to implement preconditioned gradient descent for in-context learning, 2023. URL https:// arxiv.org/abs/2306.00297. An, R., Yang, S., Zhang, R., Shen, Z., Lu, M., Dai, G., Liang, H., Guo, Z., Yan, S., Luo, Y., et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Barak, T. and Loewenstein, Y. Investigating learningindependent abstract reasoning in artificial neural networks. arXiv e-prints, pp. arXiv2407, 2024. Cattell, R. B. Theory of fluid and crystallized intelligence: critical experiment. Journal of educational psychology, 54(1):1, 1963. Chen, X., Wu, Z., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., and Ruan, C. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Chollet, F. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. Chow, W., Pan, J., Liang, Y., Zhou, M., Song, X., Jia, L., Zhang, S., Tang, S., Li, J., Zhang, F., et al. Weave: Unleashing and benchmarking the in-context interleaved comprehension and generation. arXiv preprint arXiv:2511.11434, 2025. Cui, Y., Chen, H., Deng, H., Huang, X., Li, X., Liu, J., Liu, Y., Luo, Z., Wang, J., Wang, W., Wang, Y., Wang, C., Zhang, F., Zhao, Y., Pan, T., Li, X., Hao, Z., Ma, W., Chen, Z., Ao, Y., Huang, T., Wang, Z., and Wang, X. Emu3.5: Native multimodal models are world learners, 2025. URL https://arxiv.org/abs/2510. 26583. Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers, 2023. URL https://arxiv.org/abs/2212.10559. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Dherin, B., Munn, M., Mazzawi, H., Wunder, M., and Gonzalvo, J. Learning without training: The implicit dynamics of in-context learning, 2025. URL https: //arxiv.org/abs/2507.16003. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. our"
        },
        {
            "title": "Introducing Gemini",
            "content": "Google. age, state-of-the-art https://developers.googleblog.com/ introducing-gemini-2-5-flash-image/, 2025a."
        },
        {
            "title": "2.5 Flash\nimage",
            "content": "Immodel. Google. Introducing Nano Banana Pro. https: //blog.google/innovation-and-ai/ products/nano-banana-pro/, 2025b. Google DeepMind. Gemini 3 Pro. https://deepmind. google/models/gemini/pro/, 2025. Gu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y., Ma, S., Liu, H., et al. survey on llm-as-ajudge. The Innovation, 2024. 9 GENIUS: Generative Fluid Intelligence Evaluation Suite Guo, Z., Zhang, R., Li, H., Zhang, M., Chen, X., Wang, S., Feng, Y., Pei, P., and Heng, P.-A. Thinking-whilegenerating: Interleaving textual reasoning throughout visual generation. arXiv preprint arXiv:2511.16671, 2025. Li, Y., Wang, H., Zhang, Q., Xiao, B., Hu, C., Wang, H., and Li, X. Unieval: Unified holistic evaluation for unified multimodal understanding and generation. arXiv preprint arXiv:2505.10483, 2025d. Guo*, Z., Zhang*, R., Tong*, C., Zhao*, Z., Gao, P., Li, H., and Heng, P.-A. Can we generate images with cot? lets verify and reinforce image generation step by step. CVPR 2025, 2025. Hu, X., Wang, R., Fang, Y., Fu, B., Cheng, P., and Yu, G. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Jaeggi, S. M., Buschkuehl, M., Jonides, J., and Perrig, W. J. Improving fluid intelligence with training on working memory. Proceedings of the National Academy of Sciences, 105(19):68296833, 2008. Jiang*, D., Guo*, Z., Zhang*, R., Zong, Z., Li, H., Zhuo, L., Yan, S., Heng, P.-A., and Li, H. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. Jin, W., Niu, Y., Liao, J., Duan, C., Li, A., Gao, S., and Liu, X. Srum: Fine-grained self-rewarding for unified multimodal models. arXiv preprint arXiv:2510.12784, 2025. Kent, P. Fluid intelligence: brief history. Applied Neuropsychology: Child, 6(3):193203, 2017. Koh, J. Y., Fried, D., and Salakhutdinov, R. R. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36:21487 21506, 2023. Labs, B. F. FLUX.2: Frontier Visual Intelligence. https: //bfl.ai/blog/flux-2, 2025. Li, C., Wu, W., Zhang, H., Xia, Y., Mao, S., Dong, L., Vulic, I., and Wei, F. Imagine while reasoning in space: arXiv preprint Multimodal visualization-of-thought. arXiv:2501.07542, 2025a. Li, J., Zhang, D., Wang, X., Hao, Z., Lei, J., Tan, Q., Zhou, C., Liu, W., Yang, Y., Xiong, X., et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 415423, 2025b. Li, Q., Ye, Z., Feng, X., Zhong, W., Qin, L., Chen, R., Li, B., Jiang, K., Wang, Y., Liu, T., et al. Cai: Captionsensitive attention intervention for mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2506.23590, 2025c. Li, Y., Yang, J., Li, B., and Tang, R. Cama: Enhancing multimodal in-context learning with context-aware modulated attention. arXiv preprint arXiv:2505.17097, 2025e. Liang, Y., Chow, W., Li, F., Ma, Z., Wang, X., Mao, J., Chen, J., Gu, J., Wang, Y., and Huang, F. Rover: Benchmarking reciprocal cross-modal reasoning for omnimodal generation. arXiv preprint arXiv:2511.01163, 2025. Niu, Y., Ning, M., Zheng, M., Jin, W., Lin, B., Jin, P., Liao, J., Feng, C., Ning, K., Zhu, B., et al. Wise: world knowledge-informed semantic evaluation for textto-image generation. arXiv preprint arXiv:2503.07265, 2025. OpenAI Team. Here. new-chatgpt-images-is-here/, 2025. is New ChatGPT Images https://openai.com/index/ Qin, J., Wu, J., Chen, W., Ren, Y., Li, H., Wu, H., Xiao, X., Wang, R., and Wen, S. Diffusiongpt: Llmdriven text-to-image generation system. arXiv preprint arXiv:2401.10061, 2024. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. Schipolowski, S., Wilhelm, O., and Schroeders, U. On the nature of crystallized intelligence: The relationship between verbal ability and factual knowledge. Intelligence, 46:156168, 2014. Schneider, W. J. and McGrew, K. S. The cattell-horn-carroll model of intelligence. 2012. Seedream, T., Chen, Y., Gao, Y., Gong, L., Guo, M., Guo, Q., Guo, Z., Hou, X., Huang, W., Huang, Y., et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Shi, Y., Dong, Y., Ding, Y., Wang, Y., Zhu, X., Zhou, S., Liu, W., Tian, H., Wang, R., Wang, H., et al. Realunify: Do unified models truly benefit from unification? comprehensive benchmark. arXiv preprint arXiv:2509.24897, 2025. Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 10 GENIUS: Generative Fluid Intelligence Evaluation Suite Xie, J., Darrell, T., Zettlemoyer, L., and Wang, X. Reconstruction alignment improves unified multimodal models. arXiv preprint arXiv:2509.07295, 2025a. Xie, W., Zhang, Y.-F., Fu, C., Shi, Y., Nie, B., Chen, H., Zhang, Z., Wang, L., and Tan, T. Mme-unify: comprehensive benchmark for unified multimodal understanding and generation models. arXiv preprint arXiv:2504.03641, 2025b. Zhang, D., Lei, J., Li, J., Wang, X., Liu, Y., Yang, Z., Li, J., Wang, W., Yang, S., Wu, J., et al. Critic-v: Vlm critics help catch vlm errors in multimodal reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 90509061, 2025. Zhao, S., Hao, S., Zi, B., Xu, H., and Wong, K.-Y. K. Bridging different language models and generative vision models for text-to-image generation. In European Conference on Computer Vision, pp. 7086. Springer, 2024. Zhao, X., Zhang, P., Tang, K., Zhu, X., Li, H., Chai, W., Zhang, Z., Xia, R., Zhai, G., Yan, J., et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. Zhipu AI Team. https://z.ai/blog/glm-image, January 2026. Zhou, P., Peng, X., Song, J., Li, C., Xu, Z., Yang, Y., Guo, Z., Zhang, H., Lin, Y., He, Y., et al. Opening: comprehensive benchmark for judging open-ended interleaved image-text generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 5666, 2025. Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Wang, Y., Rao, Y., Liu, J., Huang, T., and Wang, X. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Team, N., Han, C., Li, G., Wu, J., Sun, Q., Cai, Y., Peng, Y., Ge, Z., Zhou, D., Tang, H., Zhou, H., Liu, K., Huang, A., Wang, B., Miao, C., Sun, D., Yu, E., Yin, F., Yu, G., Nie, H., Lv, H., Hu, H., Wang, J., Zhou, J., Sun, J., Tan, K., An, K., Lin, K., Zhao, L., Chen, M., Xing, P., Wang, R., Liu, S., Xia, S., You, T., Ji, W., Zeng, X., Han, X., Zhang, X., Wei, Y., Xu, Y., Jiang, Y., Wang, Y., Zhou, Y., Han, Y., Meng, Z., Jiao, B., Jiang, D., Zhang, X., and Zhu, Y. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025. von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent, 2023. URL https://arxiv.org/abs/2212.07677. Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Wei, X., Zhang, J., Wang, Z., Wei, H., Guo, Z., and Zhang, L. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025a. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., ming Yin, S., Bai, S., Xu, X., Chen, Y., Chen, Y., Tang, Z., Zhang, Z., Wang, Z., Yang, A., Yu, B., Cheng, C., Liu, D., Li, D., Zhang, H., Meng, H., Wei, H., Ni, J., Chen, K., Cao, K., Peng, L., Qu, L., Wu, M., Wang, P., Yu, S., Wen, T., Feng, W., Xu, X., Wang, Y., Zhang, Y., Zhu, Y., Wu, Y., Cai, Y., and Liu, Z. Qwen-image technical report, 2025b. URL https://arxiv.org/abs/2508.02324. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. GENIUS: Generative Fluid Intelligence Evaluation Suite A. Benchmark Details A.1. Data Statistics Figure 5. Data composition pie chart. GENIUS comprises 3 dimensions, 5 tasks, and 20 sub-tasks. A.2. Evaluation Prompt As illustrated in the previously presented prompt templates, we developed systematic evaluation framework using Large Multimodal Models (LMMs) to assess three key dimensions of generative quality: Rule Compliance (RC): For each GENIUS sample, an audit of textual-visual alignment is conducted. This process rigorously verifies nouns, adjectives, and spatial constraints to ensure 100% compliance with specific modification requests. Details of the prompt template are provided in Fig. 8. Visual Consistency (VC): For each GENIUS sample, Visual Consistency may be evaluated multiple times or not at all, depending on how many reference images (objects in the image) need to remain visually consistent. Since we have observed that many open-source models directly copy reference images to cheat (e.g., Bagel, GLM-Image, etc.), dedicated antiplagiarism screening is conducted prior to the Visual Consistency audit. The LMM first performs pixel-level identity check; if the target image is found to be an exact pixel-for-pixel duplicate of the reference without any generative modifications, the consistency score is automatically set to 0. Details of the prompt template are provided in Fig. 9. Aesthetic Quality (AQ): Assesses visual logic, rendering clarity, and realism. It rewards commercial-grade outputs while penalizing structural collapses or AI hallucinations. Details of the prompt template are provided in Fig. 10. B. Detailed Qualitative Examples and Model Outputs To provide more granular view of the GENIUS benchmark, we present comprehensive qualitative examples for each sub-task. For every data sample, we showcase complete data instance that includes: (1) the full input content (comprising both context and instruction); (2) the specific evaluation hints utilized for assessing Rule Compliance (RC) and Visual Consistency (VC); and (3) the corresponding generated outputs from six representative models: Nano Banana Pro (Google, 2025b), Nano Banana (Google, 2025a), SeeDream4.5 (Seedream et al., 2025), FLUX.2-dev (Labs, 2025), Bagel (Deng et al., 2025) and ours. These detailed comparisons, which highlight the capabilities and failure modes of different architectures, are illustrated in Fig. 11 and Fig. 12. C. Evaluation using Qwen2.5-VL-72B as Judge To further validate the robustness of our evaluation framework, we employed Qwen2.5-VL-72B (Bai et al., 2025) as the judge model to assess GENIUS benchmark. The results are summarized in Tab. 3. 12 GENIUS: Generative Fluid Intelligence Evaluation Suite Table 3. Benchmark Results by Qwen2.5-VL-72B.The Overall column represents the weighted score across all tasks, calculated using metric ratio of RC:VC:AQ = 6:3.5:0.5. The best and second best performances are highlighted. Method Interleaved Overall Implicit Pattern Symbolic Constraint Visual Constraint Prior-Conflicting Multi-Semantic Implicit Pattern Induction Ad-hoc Constraint Execution Contextual Knowledge Adaptation Nano Banana Pro Nano Banana GPT-Image SeeDream 4. SeeDream 4.5 Qwen-Image GLM-Image FLUX.2-dev NextStep-1 Emu3.5-Image Omini-Gen2 Bagel Ours RC VC AQ RC VC AQ RC VC AQ RC VC AQ RC VC AQ Proprietary Models 48.35 42.88 40.94 17. 44.17 25.67 17.45 27.37 9.90 28. 21.12 18.97 62.21 52.72 53.15 8. 64.79 29.81 23.23 33.40 0.38 43. 24.42 14.53 37.84 41.89 41.27 1. 40.32 17.24 17.22 17.57 20.02 26. 18.24 20.27 89.53 80.81 90.35 92. 89.65 79.65 80.23 85.47 11.98 80. 81.40 80.23 69.93 34.35 82.68 74. 54.17 88.33 28.22 42.24 89.11 27. 58.52 35.50 80.72 66.95 45.83 89. 22.24 28.45 85.64 23.00 59.15 29. 82.35 42.50 33.33 69.17 27.72 35. 89.60 17.73 19.93 5.73 76.47 37. 8.33 76.67 11.39 8.62 85.64 22. 62.11 25.08 80.39 60.83 39.50 89. 26.80 40.66 84.16 26.18 Open-Source Models 31. 24.66 74.18 20.83 25.00 59.17 12. 30.17 78.71 15.82 15.99 21.81 71. 23.33 18.67 69.17 6.44 15.93 79. 10.09 33.32 27.36 77.06 30.33 37. 63.75 12.87 30.38 80.69 19.27 1. 15.22 20.54 3.19 19.32 14.11 6. 20.21 10.08 12.28 33.66 26.72 81. 20.83 37.50 45.00 10.89 24.14 84. 20.45 20.59 22.90 82.03 8.33 20. 62.50 11.39 31.90 82.67 8.18 16. 14.89 80.72 16.67 25.00 57.50 6. 16.38 82.67 22.73 23.91 26.45 30. 70.54 22.01 20.24 75.53 25.27 26. 46.37 7.89 27.55 75.92 22.35 - - - - - - - - - - - - - 67.27 68.64 58.18 68.18 62.27 68. 48.64 63.18 13.57 62.73 58.64 60. 47.24 As illustrated, utilizing Qwen2.5-VL-72B as the evaluator results in universal decrease in the Overall Scores across all tested models. This suggests that Qwen2.5-VL-72B may impose stricter standard for rule and visual compliance compared to the primary evaluator. Crucially, despite the shift in absolute scores, the relative performance trends and the ranking order of the models remain largely consistent. This consistency reinforces the reliability of GENIUS benchmark, demonstrating that the observed performance gaps are intrinsic to the models themselves rather than artifacts of specific judge. D. Additional Experiments and Analysis D.1. Ablation on Interleaved Format In the context of the GENIUS Benchmark, multimodal interleaved data can be presented in various input formats. Since models exhibit varying degrees of compatibility with these formats, we investigate the impact of input structure on performance by defining three distinct paradigms, as illustrated in Fig. 6(a). First, in the Edit Mode, the visual and textual modalities are decoupled. Images are provided separately (e.g., appended at the end or beginning) and are referenced within the text using placeholders like image i. Second, the Interleaved Mode corresponds to the standard setting used in our main experiments. Here, images are interleaved with text but are inserted at the boundaries of complete semantic units (typically at the end of sentence), preserving the syntactic integrity of the text strings. Third, the Fine-Grained Interleaved Mode inserts images precisely at their point of reference, even within sentence. In this mode, visual tokens act as intrinsic parts of the syntax and can interrupt the textual flow, requiring the model to handle fine-grained multimodal dependencies. We conducted evaluations on the Nano Banana series and Bagel, as they are among the few models capable of supporting all three formats. The Overall scores are reported in Fig. 6(b). The results indicate that performance trends vary across models, likely due to differences in model architecture. Notably, we observe significant performance gap between Edit Mode and the two interleaved modes (Interleaved and Fine-Grained), while the disparity between the two interleaved formats is relatively marginal. This variability suggests that current multimodal models possess limited robustness regarding input formatting, exhibiting strong sensitivity to how visual information is integrated with text. D.2. Discussion on the Composition of Input To verify the necessity of contextual information for high-fidelity generation, we conducted an ablation study on the Nano Banana Pro model by removing the context component and relying solely on the final instruction. The comparative Rule Compliance scores across different tasks are reported in Fig. 6(c). As observed, removing context leads to precipitous decline in performance across the board, underscoring its indispensable role. Specifically, the Implicit Pattern Generation, 13 GENIUS: Generative Fluid Intelligence Evaluation Suite Figure 6. Additional Experiments and Analysis. This figure presents the definition of three input formats (a) and their corresponding performance impact (b), followed by an ablation study assessing the importance of contextual information in instruction following (c). Symbolic Constraint Generation, and Visual Constraint Generation tasks suffer the most severe degradation. This is anticipated, as these tasks require the model to inductively reason or extract specific visual-textual mappings defined solely within the context; without these definitions, the model lacks the necessary premises to execute the instruction. Similarly, the Prior-Conflicting Generation task exhibits significant drop, as the model inevitably reverts to its pre-trained priors in the absence of an explicit counter-factual context to override them. Interestingly, the decline in the Multi-Semantic Generation task is less pronounced. This relative stability can be attributed to the tasks inherent difficulty (resulting in lower baseline performance) and the probability that the model might fortuitously align with the target semantics even without disambiguating context. Nevertheless, the consistent performance gap confirms that context is not merely supplementary but critical foundation for accurate generation in complex scenarios. E. Related Work Fluid Intelligence Originating from the Cattell-Horn-Carroll (CHC) theory of cognitive abilities (Schneider & McGrew, 2012), general intelligence is structurally divided into Crystallized Intelligence (Gc) and Fluid Intelligence (Gf ). While Gc relies on the utilization of accumulated knowledge, Gf represents the innate capacity to solve novel problems through inductive reasoning and dynamic reasoning, independent of prior knowledge, which is often considered as more indicative of general intelligence (Jaeggi et al., 2008; Chollet, 2019; Barak & Loewenstein, 2024). In the field of understanding, evaluating Gf has traditionally focused on logical reasoning and abstract pattern completion. Prominent benchmarks such as the ARC Bench (Chollet, 2019) assess models ability to induce rules from few-shot examples and generalize to new scenarios. However, these evaluations are predominantly discriminative or symbolic, targeting problem-solving in restricted domains (e.g., grid worlds). In the context of Unified Multimodal Models (UMMs), current assessments remain largely confined to Gc, testing the models capability on static world knowledge. Unified Multimodal Models (UMMs) Recent years have witnessed paradigm shift from modular composition towards native fusion in multimodal models. Early approaches primarily bridged pre-trained Large Language Models (Qin et al., 2024; Esser et al., 2024; Zhao et al., 2024; Li et al., 2025b; Zhang et al., 2025) with diffusion decoders to enable visual synthesis capabilities (Koh et al., 2023). However, the latest wave of UMMs, represented by Chameleon (Team, 2024), Show-o (Xie et al., 2024; Guo* et al., 2025) and Emu Series (Sun et al., 2023; 2024; Wang et al., 2024), marks fundamental departure by discretizing visual signals into discrete tokens. Janus (Wu et al., 2025a) and its improvements (Guo et al., 2025; Jiang* et al., 2025) claims that understanding and generation require distinct information, employing different tokenizers for each task. Among these architectures, Bagel (Deng et al., 2025) and its improvements (Xie et al., 2025a; Jin et al., 2025) have demonstrated remarkable versatility in both understanding and generation tasks, becoming one of the most representative open-sourced models. Despite these architectural breakthroughs, current research predominantly evaluates these models on task-specific benchmarks (e.g., VQA or standard T2I generation), leaving their intrinsic Generative Fluid Intelligence (i.e., the capacity to reason and adaptively generate under novel, ad-hoc constraints) largely unexplored. Generative Evaluation of UMMs With the rapid progress of UMMs, numerous benchmarks (Ghosh et al., 2023; Zhao 14 GENIUS: Generative Fluid Intelligence Evaluation Suite et al., 2025; Wei et al., 2025; Li et al., 2025d; Chow et al., 2025) have been proposed to assess their capabilities, yet most remain confined to traditional evaluation paradigms. Early benchmarks like GenEval (Ghosh et al., 2023), WISE (Niu et al., 2025), and DPG-Bench (Hu et al., 2024) primarily focus on single-image generation tasks, assessing static world knowledge or basic text-to-image alignment without involving complex, interleaved contexts. OpenIng (Zhou et al., 2025) focuses on in-context visual generation, while it primarily targeting interleaved text-and-image generation and Crystallized Intelligence. While more recent suites such as MME-Unify (Xie et al., 2025b), RealUnify (Shi et al., 2025), and ROVER (Liang et al., 2025) have begun to incorporate multi-image inputs, they predominantly target Crystallized Intelligence, evaluating the models ability to recall pre-trained information rather than adapting novel rules. Crucially, none of the existing benchmarks systematically evaluate Generative Fluid Intelligence (GFI). As shown in Table 1, current methods lack comprehensive coverage across key GFI dimensions, including Implicit Pattern Induction, Explicit Constraint Execution, and Contextual Knowledge Adaptation. Furthermore, many rely heavily on synthetic data or purely LLM-as-Judge that fail to capture failure cases. In contrast, GENIUS fills this critical void by being the first benchmark to feature fully multimodal interleaved context, purely manually curated annotations, and hybrid evaluation protocol to quantify FI in generative scenarios. F. Details of Method F.1. Prompt Template for Keyword Generation To extract task-critical visual cues, we employ following prompt to guide Bagel in identifying key regions within the context images. The template is shown in Fig. 7. The generated keywords are subsequently used to compute the relevance map. F.2. Mathematical Formulation of Attention Modulation In this section, we detail the implementation of the Bias Injection stage. Our modulation strategy is mathematically inspired by (Li et al., 2025e;c), adapted to our keyword-based relevance scoring. The modulation is applied selectively to subset of decoder layers Lselected and generation steps Tselected. For targeted head in layer Lselected at step Tselected, let Al,h RN denote the original attention logits (before Softmax). Let RN be the relevance score vector computed in the Relevance Mapping stage. To enforce the models focus on critical signals, we inject dynamic bias term into the attention mechanism. The modulated attention logits ˆAl,h are computed as follows: ˆAl,h(i, j) = Al,h(i, j) + λ F(Sj) (11) where denotes the query token index, denotes the key token index, and λ is scalar hyperparameter controlling the modulation intensity. The function F() maps the raw relevance scores to bipolar bias distribution: F(Sj) = Sj µS σS + ϵ (12) Here, µS and σS are the mean and standard deviation of the relevance scores across the current context window. The final attention weights are obtained via the standard Softmax operation: Attention(Q, K, ) = softmax (cid:33) (cid:32) ˆA V (13) This formulation ensures that the gradient norm contribution from noise tokens is effectively dampened by the exponential suppression of the Softmax function. 15 GENIUS: Generative Fluid Intelligence Evaluation Suite G. Theorem Part G.1. Exact Definition of Let xt denote the noisy intermediate variable at certain time step. Let C1 represent the context and instruction (text modality), and C2 represent the image modality. Then for + 1-th step, we have: cause Bagel uses MoE architecture, the intermediate variables are defined as: xt+1 = (u1, u2, gt) u1 = Und Encoder(C1 C2), u2 = Gen Encoder(C2), gt = Gen Encoder(xt). (14) (15) (16) (17) For the (l + 1)-th Decoder layer, Bagel employs Pre-Layer Normalization (Pre-LN) structure. Let = (u1u2) where denotes matrix concatenation operation. We then obtain the detailed update rule of the decoder layer: (cid:16) (cid:17)(cid:17) (cid:16) g(t,l+1) = A(u(l), g(t,l)) + Up (A(u(l), g(t,l)) + = L(l) Up,b(u(l), g(t,l)) (18) where the initial bias is set as binitial = 0, RMS denotes the Root Mean Square operation, and Up denotes the Up layer in the decoder block. The core attention function A(u, g) is formulated as: A(u, g) = MoE attn ((U1 U2), G) + U1 = Und (RMSNorm(u1)) U2 = Gen (RMSNorm(u2)) = Gen (RMSNorm(g)) where MoE attn (U, G) = Softmax (cid:32) Gquery (Ukey Gkey) dattn (cid:33) (Uvalue Gvalue) , (19) (20) (21) (22) (23) where = (Uquery, Ukey, Uvalue) and = (Gquery, Gkey, Gvalue) denote the query/key/value components of and respectively, dattn is the dimension of attention heads. Und (Understanding) and Gen (Generation) denote the Q, K, and projection layers for different experts in the MoE architecture. G.2. Proof of Thm. 4.1 Our goal is to prove: According to the definition of L, we have: LUp+Up, b+b(u, g) = LUp,b(u, g) LUp+Up,b+b(u, g) = A(u, g) + ((Up + Up)( A(u, g) RM S(A(u, g)) )) + + Cause: Up = Up (δA) (A(u, g)) (A(u, g))2 , = A(u, g) A(u, g) We proceed to expand Equation (25): LUp+Up,b+b(u, g) = A(u, g) + (UpN (A(u, g)) + Up(δA)) + where Thus, we complete the theorems proof. = A(u, g) + (UpA(u, g))) + = LUp,b(u, g) δA = (A(u, g)) (A(u, g)) 16 (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) GENIUS: Generative Fluid Intelligence Evaluation Suite G.3. Proof of Thm. 4.2 Our goal is to prove the following conclusion: (cid:40) Upi+1 = Upi hUpLi(Upi), bi+1 = bi (cid:0)tr (cid:0)δ (cid:1)(cid:1) bi For Up, we first expand the update rule directly and obtain: Upi+1 Upi = Upi+1 Upi = = Up (δAi+1) (N (A(g))) (A(g))2 Up (δAi) (N (A(g))) (A(g))2 Up (δAi+1 δAi) (N (A(g))) (A(g))2 According to the main text, if we define: 1 (A(g))2 = = Up (δAi δAi+1) (N (A(g))) Li(Up) = Up and notice the following trivial property: then we can conclude that: For b, we have: Up trace (cid:0) Up(cid:1) = Upi+1 = Upi hUpLi(Upi) bi+1 bi = bi+1 bi = A(u(i+1), g) A(u(i), g) so according to property Equation (34): Thus, we complete the theorems proof. bi+1 = bi (cid:0)tr (cid:0)δ bi (cid:1)(cid:1) GENIUS: Generative Fluid Intelligence Evaluation Suite Prompt Template for Keyword Generation You are an EXPERT IMAGE GENERATION PLANNER. Your goal is to parse multimodal instructions and map every provided image to its specific role in the generation process. Task: Analyze the users Instruction and the list of image num provided images. Determine precisely what information needs to be extracted or retained from each image. (cid:22) Focus Definition Rules: 1. TARGET CANVAS / BASE IMAGE ˆ Target Canvas / Base Image: If an image serves as the foundation to be edited, reshaped, or modified (e.g., \"change the background of this image\", \"add hat to this person\"), the value must be \"all\". ˆ Feature Extraction: If only specific part is needed (e.g., \"swap face\", \"use this shirt\", \"holding this object\"), output the specific noun (e.g., \"face\", \"shirt\", \"cup\"). ˆ Style/Attribute Reference: If the image provides abstract attributes (e.g., \"use this lighting\", \"copy this art style\", \"follow this pose\"), output the attribute name (e.g., \"lighting\", \"art style\", \"pose\"). ˆ Irrelevant:"
        },
        {
            "title": "If an image is mentioned but contributes no visual content to the",
            "content": "result, output empty string \"\". Output Format: Return ONLY strictly valid JSON object: { \"<image X>\": \"focus string\" } (cid:17) Few-Shot Examples: \"Transfer the style of <image 1> to the car in <image 2>, but make sure Example 1: Input: the background matches <image 3>.\" Output: \"<image 1>\": \"<image 2>\": \"<image 3>\": } \"art style\", \"car\", \"background\" { \"Take <image 1> and remove the person from it.\" Example 2: Input: Output: \"<image 1>\": } { \"all\" _ Current Input Context: (cid:143) Images Count: {image num} (cid:143) Instruction: \"\"\" {content} \"\"\" Figure 7. Prompt Template for Keyword Generation. 18 GENIUS: Generative Fluid Intelligence Evaluation Suite Rule Compliance Evaluation Your task is to act as PRECISION VISUAL AUDITOR in Zero-Tolerance Fidelity Mode. Mission Statement: Rigorously evaluate the alignment between the HINT and the MODEL OUTPUT IMAGE. This mode is You must prioritize Technical Precision over Perceptual Plasticity. designed for high-stakes instruction following where close enough\" is considered failure. (cid:22) Scoring Hierarchy: 1. SCORE 2 [PERFECT EXECUTION] Standard: The image is flawless visual manifestation of the HINT. Every explicit and implicit constraint must be met with 100% accuracy. Identity and Nouns: All requested subjects are present, anatomically/structurally correct, and positioned exactly as described. Adjective/Attribute Fidelity: Every single descriptor (color, texture, material, style, state, quantity) is rendered without deviation. Strict Detail Check: If the hint specifies three buttons\" and there are two If the hint says crimson\" and the output is bright or four, it is NOT 2. red,\" it is NOT 2. Rule: Award only if there is ZERO discrepancy between text and pixels. 2. SCORE 1 [PARTIAL COMPLETION / ACCURACY DRIFT] (cid:3) Standard: The core subject/action is present, but the execution fails on specific details, modifiers, or secondary constraints. (cid:3) Minor Omissions:A secondary adjective is ignored (e.g., vintage\" style is missing, but the object is there). (cid:3) Count/Scale Errors: (cid:3) Color Drift: Wrong number of objects or incorrect relative sizing."
        },
        {
            "title": "The color is in the correct family but lacks the specific shade",
            "content": "or intensity requested. (cid:3) Rule: Award 1 if the viewer can tell what was intended, but the fine print\" of the instruction was neglected. 3. SCORE 0 [FAILURE / GROSS ERROR] Standard: Subject Error: Total failure to execute the primary intent. The wrong object was added, the target was removed, or the primary subject is unrecognizable. Non-Action:"
        },
        {
            "title": "No change was made to the image despite a request for",
            "content": "modification. Semantic Inversion/Incoherence: The model did the opposite of the hint or produced visual hallucination/glitch. Rule: Award 0 if the primary goal of the HINT is unfulfilled. (cid:223) Evaluation Steps: 1. SCAN: List every Noun, Adjective, Count, and Spatial relation in the HINT. 2. VERIFY PRIMARY: Is the core action/subject present? (If No 0). 3. VERIFY EXHAUSTIVE: Check every item from Step 1. Any error 1. 4. FINAL SCORE: Award 2 ONLY if Step 3 yields perfect match. ı Evaluation Data: HINT: {{hint}} TARGET: {{output image tag}} Output Format: Return ONLY the following line: Rule Compliance: Figure 8. Prompt Template for Rule Compliance Evaluation. 19 GENIUS: Generative Fluid Intelligence Evaluation Suite Visual Consistency Evaluation Your task is to act as VISUAL FIDELITY ANALYST in Strict Evaluation Mode. Mission Statement: Your mission is to evaluate the consistency between the REFERENCE IMAGE and the TARGET based on the HINT. Your goal is to distinguish between Great Work\" (2), Acceptable Effort\" (1), and Total Failure\" (0). (cid:22) Scoring Hierarchy: 1. SCORE 2 [HIGH FIDELITY (SUCCESSFUL)] Standard: The TARGET is high-quality implementation of the HINT. The core subject remains stable and the image feels professional. Strong Identity: The main subject (person, object, or scene) is clearly the same as in the Reference. Smooth Transformation: The changes requested by the HINT are integrated. Minor Tolerance: Small shifts in color, slight facial softening, or minor background variations are PERFECTLY ACCEPTABLE. Rule: If the image is good, and follows the HINT, give it 2. 2. SCORE 1 [RECOGNIZABLE DERIVATION] (cid:3) Standard: The TARGET is clearly related to the Reference, even if the execution is imperfect. the idea right\" but lose some detail. This is the catch-all category for images that get (cid:3) Recognizable Link:"
        },
        {
            "title": "You can still tell it is based on the same subject or",
            "content": "concept, even if the face looks bit different or background have shifted. (cid:3) Moderate Drift: The HINT was attempted, but the model may have simplified original details or introduced AI blurring/messiness. (cid:3) High Tolerance: Even if the image has lost some Visual DNA,\" as long as it isnt completely different subject, it stays in this category. (cid:3) Rule: If the model tried to follow the HINT and the result is okay\" or recognizable,\" award 1. 3. SCORE 0 [TOTAL FAILURE] Standard:"
        },
        {
            "title": "Total failure to execute the primary intent or maintain subject",
            "content": "connection. Subject Swap: Ignored Instruction: The model provided generic image that ignores both the The model generated completely different person or object. Reference and the HINT entirely. Broken Output: Rule: The image is corrupted, unidentifiable mess of pixels. Do NOT award 0 if there is any link to the Reference Image. (cid:223) Evaluation Steps: 1. RELATION: Is the Target even remotely related to the Reference subject? (If No Score 0). 2. QUALITY & FIDELITY: Does the Target look stable, professional, and closely follow the HINT without distracting errors? (If Yes Score 2). 3. DRIFT ASSESSMENT: For everything else in between (minor drift, detail loss, background shifts, but same subject) Score 1. ı Evaluation Data: REFERENCE IMAGE: {{reference image}} HINT: {{hint}} TARGET: {{output image tag}} Output Format: Return ONLY the following line: Visual Consistency: Figure 9. Prompt Template for Visual Consistency Evaluation. 20 GENIUS: Generative Fluid Intelligence Evaluation Suite Aesthetic Quality Evaluation Your task is to act as PROFESSIONAL VISUAL ARTS CURATOR in High-Visual Harmony Mode. Mission Statement: Your mission is to categorize images into Masterpiece Level (2), Standard Work (1), and Technical Failure (0). harmony and logical consistency, prioritizing Visual Cohesion and professional merit."
        },
        {
            "title": "You must reward images that achieve high visual",
            "content": "(cid:22) Scoring Hierarchy: 1. SCORE 2 [EXCEPTIONAL / MASTERPIECE LEVEL] Standard: High-end, commercial quality."
        },
        {
            "title": "The image is indistinguishable from",
            "content": "professional work. Structural Logic:"
        },
        {
            "title": "All objects and characters follow the laws of physics and",
            "content": "anatomy. Rendering Clarity: Textures look intentional and sharp; lighting creates convincing sense of depth. Visual Appeal:"
        },
        {
            "title": "The overall composition is professional and free of distracting",
            "content": "AI hallucinations. Tolerance Clause: Award 2 even if there is tiny, non-distracting flaw, provided the overall impact is professional. 2. SCORE 1 [STANDARD / THE GOOD EFFORT ZONE] (cid:3) Standard: The DEFAULT category. Visually acceptable and logically sound but contains visible \"AI-isms.\" (cid:3) Visible Flaws: Slightly soft hands, plastic skin, or shadows that do not perfectly align with the light source. (cid:3) Minor Perspective Issues:"
        },
        {
            "title": "Background elements that are slightly tilted or out",
            "content": "of scale. (cid:3) Rule: Use this as the safety score for any image that is \"pretty good\" but not perfect. 3. SCORE 0 [FAILED / TECHNICAL FAILURE] Standard: Logical Collapse: Images that are visually nonsensical or structurally broken. Extra limbs that are gross and distracting, or faces that have lost basic human structure. Extreme Noise: Rule: Grain or artifacts so heavy they obscure the main subject. Only award 0 if the image is unusable."
        },
        {
            "title": "If the parts are mostly in the",
            "content": "right place, do NOT give 0. (cid:223) Evaluation Steps: 1. SCAN: Identify the visual logic, realism, and presence of any AI-generated artifacts. 2. VERIFY LOGIC: Does the image maintain basic structural integrity? (If No 0). 3. ASSESS QUALITY: Check for professional rendering, sharp textures, and lighting depth. 4. FINAL SCORE: Award 2 for commercial quality, 1 for standard AI output, and 0 for unusable failure. ı Evaluation Data: METRIC: Aesthetic Quality (Visual Logic and Realism) TARGET: {{output image tag}} Output Format: Return ONLY the following line: Aesthetic Quality: Figure 10. Prompt Template for Aesthetic Quality Evaluation. GENIUS: Generative Fluid Intelligence Evaluation Suite Figure 11. Detailed Qualitative Examples and Model Outputs. (1/2) 22 GENIUS: Generative Fluid Intelligence Evaluation Suite Figure 12. Detailed Qualitative Examples and Model Outputs. (2/2)"
        }
    ],
    "affiliations": [
        "CUHK",
        "MSRA",
        "Peking University",
        "PolyU",
        "StepFun"
    ]
}