{
    "paper_title": "Mitigating Overthinking through Reasoning Shaping",
    "authors": [
        "Feifan Song",
        "Shaohang Wei",
        "Bofei Gao",
        "Yejie Wang",
        "Wen Luo",
        "Wei Li",
        "Linli Yao",
        "Weimin Xiong",
        "Liang Chen",
        "Tianyu Liu",
        "Houfeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost. Prior designs of penalization in RLVR manage to reduce token consumption while often harming model performance, which arises from the oversimplicity of token-level supervision. In this paper, we argue that the granularity of supervision plays a crucial role in balancing efficiency and accuracy, and propose Group Relative Segment Penalization (GRSP), a step-level method to regularize reasoning. Since preliminary analyses show that reasoning segments are strongly correlated with token consumption and model performance, we design a length-aware weighting mechanism across segment clusters. Extensive experiments demonstrate that GRSP achieves superior token efficiency without heavily compromising accuracy, especially the advantages with harder problems. Moreover, GRSP stabilizes RL training and scales effectively across model sizes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 5 3 5 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Mitigating Overthinking through Reasoning Shaping",
            "content": "Feifan Song1,2, Shaohang Wei1, Bofei Gao1, Yejie Wang2, Wen Luo1, Wei Li1 Linli Yao1, Weimin Xiong1, Liang Chen1, Tianyu Liu1*, Houfeng Wang1 1State Key Laboratory of Multimedia Information Processing School of Computer Science, Peking University 2Moonshot AI songff@stu.pku.edu.cn; wanghf@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost. Prior designs of penalization in RLVR manage to reduce token consumption while often harming model performance, which arises from the oversimplicity of tokenlevel supervision. In this paper, we argue that the granularity of supervision plays crucial role in balancing efficiency and accuracy, and propose Group Relative Segment Penalization (GRSP), step-level method to regularize reasoning. Since preliminary analyses show that reasoning segments are strongly correlated with token consumption and model performance, we design length-aware weighting mechanism across segment clusters. Extensive experiments demonstrate that GRSP achieves superior token efficiency without heavily compromising accuracy, especially the advantages with harder problems. Moreover, GRSP stabilizes RL training and scales effectively across model sizes."
        },
        {
            "title": "Introduction",
            "content": "Test-time Scaling with RLVR has greatly accelerated the development and adoption of Large Reasoning Models (LRMs) (Team et al., 2025; Guo et al., 2025a; Yu et al., 2025; Zheng et al., 2025). During inference, LRMs typically exhibit distinct behavior from normal post-trained LLMs: they first produce reasoning trajectory before generating the final response. Unlike conventional Chain-ofThoughts (Wei et al., 2022), such trajectories often involve exploration and self-reflection over multiple possible solution paths, and gradually reach the final answer. However, this pattern can also lead LRMs to overthink, or repeatedly explore and * Project lead. Corresponding author. 1 revise their paths, resulting in excessively long decoding sequences and huge computational costs. Recent works treat this issue from the algorithmic perspective, introducing additional supervision on token efficiency within RLVR. For example, Aggarwal and Welleck (2025) penalizes samples whose output length exceeds that of the reference responses. However, while such methods effectively reduce token consumption, they also compromise the benefits of Test-time Scaling, leading to significant degradation in LRM performance. Compressing the reasoning content essentially means removing redundant parts of the thought process. For humans, it first requires the ability to be aware of the redundant content before making decision. When each token is considered as the candidate to remove, as illustrated in Figure 1(a), it is difficult to identify which specific tokens are unnecessary, since most tokens cannot be directly associated with the sparse verifier reward and recognized as high-value tokens. On the other hand, arbitrarily removing tokens is not feasible either, as they are still essential to preserve the readability and coherence of the reasoning. This motivates us to reconsider the granularity of supervision for balancing computational cost and task performance, and we find intermediate steps/segments can be more natural unit. As shown in Figure 1(b), humans can more easily identify redundant step than an individual token, since each step typically carries semantically coherent piece of thought. Building on this insight, we propose to supervise the reasoning process at the segment level, enabling stable control of LRM reasoning behavior. In this work, we introduce Group Relative Segment Penalization (GRSP), novel method that balances computational efficiency and task performance by operating at the reasoning-step granularity. As foundation, our observations on opensource LRMs reveal that the quantity of segments is positively correlated with token consumption, Figure 1: Illustration of redundancy detection. (a) Identifying redundant tokens is challenging, as most of them are weakly correlated with the golden answer; (b) Identifying redundant steps is much easier for its clearer meaning. while they are easier to assess for redundancy, making them more reasonable target for penalization. Further preliminary analyses indicate statistical relationship between the performance of LRMs and the distribution of segment lengths: stronger models tend to exhibit more balanced segment-length distributions. It suggests chance to mitigate performance degradation by applying length-aware penalties: penalizing segment counts within each length cluster and assigning decreasing weights for longer segments. We conduct extensive experiments comparing GRSP with baselines and demonstrate its advantages in both token efficiency and accuracy. Notably, as task difficulty increases, together with longer reasoning, the advantages of GRSP become even clearer. We also analyze the impact of the weighting mechanism. Although it appears to contradict the intuition of encouraging shorter segments to minimize token count, our results show that it ultimately saves the cost and stabilizes RL training. Moreover, we investigate the effect of different segmentation strategies and evaluate the scalability of our approach across model sizes. Our work can be summarized into three aspects: (1) We first address the granularity of supervision in mitigating overthinking, proposing step-level supervision, and conducting preliminary analyses that uncover correlated features. (2) We propose GRSP, which employs lengthaware weighting across segment clusters to control behavior in reasoning, mitigating performance degradation. (3) We perform comprehensive experiments to verify the broad effectiveness of segment-level penalization and weighting, and analyze its scalability on model size, offering insights for future research."
        },
        {
            "title": "2 Preliminary",
            "content": "Large reasoning models (LRMs) improve downstream accuracy by prefixing thinking block to the final answer. This pattern has been further scaled by recent RL work, such as Kimi-k1.5 (Team et al., 2025) and DeepSeek-R1 (Guo et al., 2025a). Given the prompt and corresponding responses {yy = yt πθold}, the goal is to maximizes the expected verifiable reward, πθ = arg max ExD,yπθold where π is the policy and is the accuracy signal obtained from deterministic verifier. Prevalent algorithms include Reinforce (x)R(x, y) (1) =ExD,Y πθold 1 yi (cid:88) yi (2) (cid:88) 1 yiY tx, yi <t (cid:2)Ai,t πθ (cid:0)yi and GRPO (Shao et al., 2024), which adds an importance-sampling ratio (cid:1)(cid:3) r(x, yt, y<t) = πθ(ytx, y<t) πθold(ytx, y<t) (3) and clip operator inherited from PPO (Schulman et al., 2017). It also replaces the token-level advantage Ai,t with sequence-level score Ai R(x, yi) yj [R(x, yj)] Ai = std[R(x, yj)] (4) Despite its effectiveness, RLVR produces unnecessarily long thinking content. common remedy is to augment with token-length penalty = ({yt}) (5) where penalises the total number of generated tokens. It shortens the output but also degrades accuracy. Figure 2: The overall workflow of GRSP. Model # Tokens # Segments DeepSeek-R1 (Guo et al., 2025a) QwQ-32B (Yang et al., 2024) DS-Qwen-Distill-32B DS-Qwen-Distill-14B 8544.70 12003.31 7464.53 6539. 135.75 216.52 105.51 94.73 Table 1: Statistics of open-source LRMs on token consumption and reasoning segment production."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we present GRSP, drop-in replacement for the token-level penalty in Eq 5 in RL algorithms. We first introduce the core segment-count penalty ( 3.1), then extend it with length-group weights based on our observations of reasoning cases ( 3.2). Finally, we describe our segmentation methods ( 3.3). The overall workflow of GRSP is illustrated in Figure 2. 3.1 Penalization on Segments Mitigating overthinking essentially means compressing the thinking content. As shown in Figure 1, identifying redundant tokens is often ambiguous. However, distinct steps, i.e., trajectory segments within the reasoning process, can serve as natural higher-level unit, as also adopted by Guo et al. (2025b). Moreover, our investigation of several open-source LRMs reveals clear positive correlation between the quantity of segments and total token consumption (see Table 1). Therefore, by decomposing the thinking content into segments {si}, we can indirectly reduce token usage by discouraging unnecessary steps. To be specific, for single prompt and corresponding response group = {yj} sampled from denote the trajectory segments in reπθold, let si sponse yj. We then compute the z-score of the segment count inside the group for each yj , and treat its negative value as the penalty, which requires no task-specific threshold: (yj) = {si}j EyY [{si}] std[{si}] (6) 3.2 Group Relative Penalization Simply changing the granularity of penalization from tokens to trajectory segments does not solve the problem in 2, because the performance gain of an LRM over an ordinary post-trained model comes directly from generated-token scaling, and any mechanism that shortens the reasoning consequently risks sharp drop in performance. To be specific in the RL setting, continued improvement on downstream performance is usually obtained by stable training or more optimization steps (Yu et al., 2025; Zheng et al., 2025). However, introducing length penalty can destabilise it: there is phenomenon that once the response length exceeds certain point, the penalty dominates and gradually the training collapses with task accuracy drifting downward. Hence, how to discourage overlength thinking while keeping the training process stable becomes the central question. We investigate the above LRMs again for statistical signatures that correlate with these two requirements. We further hypothesize that model performance is correlated with the extent of training, i.e., stronger models tend to undergo more training, which in turn suggests greater training stability. In detail, using the RL problems in 4.1, we roll out batch of responses for each model. For these responses, we segment the thinking content and group the segments into five clusters by length, where segments longer than 300 tokens are excluded, as they rarely occur. Since this experiment is related to model performance, we apply the above procedure separately to passed and failed 3 Figure 3: The ratio of segment counts across each cluster (correct vs. wrong). Longer segments are generally more prevalent in correct cases, and stronger models (a, b) exhibit flatter slopes compared to weaker ones (c, d). cases, and compute the average number of segments in each cluster for the two sides. Intuitively, we find that failed cases tend to contain more segments across most clusters. We then calculate, for each length cluster, the ratio of the average number of segments from the passed side to that from the failed side. The results are shown in Figure 3. This experiment leads to several findings: (1) It is general trend that failed cases contain more segments than passed ones. (2) Errors of LRMs in problem solving can be associated with the presence of relatively shorter segments in the thinking content. In detail, the difference between passed and failed cases is larger in shorter-length clusters (with lower ratios in Figure 3), while the difference decreases in longerlength clusters (with higher ratios). It is shared pattern for all examined LRMs. (3) potential link between model performance (+ training stability) and more balanced distribution of segments can exist. Stronger models show smaller variation in this ratio across clusters, which is reflected in smaller slope of the linear fit. Based on these observations, we propose stabilizing RL training by explicitly shaping the distribution of segments through segment-level penalties, which we term Reasoning Shaping. Concretely, following the procedure above, we first split the thinking content into segments and cluster them by length. For each cluster k, we compute relative penalty using z-score normalization: k(yj) = sk EyY [sk ] std[sk ] (7) The overall penalty is then obtained by weighting across clusters: (yj) = (cid:88) wkP k(yj) (8) Following the findings above, we penalize short segments more heavily, while applying weaker penalties to longer ones. To achieve this, we assign descending weights from shorter to longer clusters. The final reward is then given by: = + αP (yj) (9) 3.3 Segmentation We design two mechanisms for segmentation. The first is keyword-based matching, similar to Guo et al. (2025b). We collect list of keywords from typical reasoning cases to identify potential segment boundaries. It is used by default for conciseness and high computational efficiency, while its applicability is limited to specific languages. The second mechanism is token log-probability matching. Following observations in Li et al. (2024); Song et al. (2025); Wang et al. (2025a), segment boundaries often correspond to local minima in token log-probabilities. This is because segment transitions usually admit more candidate continuations, leading to lower confidence at the beginning of new segment. Based on it, we locate boundaries by identifying these local minima. We implement and test it in 4.6."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup The training pipeline consists of two post-training stages: supervised fine-tuning (SFT) followed by RLVR. For SFT, problems are collected from NuminaMATH (LI et al., 2024), while completions of reasoning and response are O1-mini patterned examples, following the prompt-engineering procedure of Gao et al. (2025b). We utilize these data to warm up the base LLM and to establish strong initialization for subsequent RL. For the RL stage, we use more challenging problems sampled from Omni-MATH and AIME, which tend to induce 4 Model / Method MATH 500 AIMO Prize Acc. Avg Len. Acc. Avg Len. Acc. Omni-MATH 500 Avg Len. Open-source Models 96.60 DeepSeek-R1 QwQ-32B 98.00 DS-Qwen-Distill-32B 96.40 DS-Qwen-Distill-14B 93.80 Qwen-2.5-14B-it 54.40 Qwen-2.5-14B-it 84.80 2428 4260 2594 2538 488 1460 91.25 95.00 82.50 80.00 10.00 53. 4704 9222 6650 6504 1091 2579 70.80 71.20 61.80 63.60 16.60 39.80 Reinforce + LCPO + O1-Pruner + GRSP GRPO + LCPO + O1-Pruner + GRSP Training Qwen-2.5-14B-it 87.20 86.40 85.20 85.40 85.20 86.00 85.00 86.20 1887 2222 1738 2128 2131 1919 1706 2054 53.75 57.50 60.00 55.00 60.00 52.50 61.25 60. 3568 3771 3416 3215 2648 3597 3299 2826 44.20 42.40 40.40 45.60 45.40 43.40 40.60 43.80 7456 12125 8997 9091 918 2159 5131 7994 5226 5315 5855 5497 4897 Overall Acc. Avg Len. 84.26 85.37 79.35 78.80 33.61 61.67 64.81 63.89 62.59 64.72 64.91 63.80 62.69 64.63 4924 8268 5859 6075 811 3513 5009 3477 3477 3643 3866 3579 3427 Table 2: Results of different models/methods across different benchmarks. The higher accuracy (Acc.) and lower average length of responses (Avg Len.) are expected. Models labeled by have been SFT-tuned. longer reasoning trajectories than NuminaMATH, and therefore better expose test-time scaling effects. The data volume and implementation detail are summarized in Appendix and B. Moreover, we set Qwen-2.5-14B-it as the starting checkpoint, and mark it with after SFT. During RL, models are trained with Reinforce and GRPO, with the addition of length penalty. 4.2 Evaluation For evaluation, we construct three test sets with increasing difficulty: 500 problems from MATH 500, 10 challenging problems from AIMO Prize-1, and 500 difficult problems from Omni-MATH 500, which enables us to compute general scores and analyze how task difficulty influences model behavior and token efficiency. Hence, we report two metrics: task performance measured by accuracy and token efficiency measured by the average number of decoding tokens per example. We compare GRSP with two baselines: (1) LCPO (Aggarwal and Welleck, 2025) uses the ratio between the generated response length and the reference one as weighting factor for the verifiable reward, reducing the sparsity of reward distribution and introducing direct correlation between reward and token usage. (2) O1-Pruner (Luo et al., 2025) computes ratio factor similar to LCPO, while adding it as an auxiliary reward term to the verifiable reward. 4.3 Main Results In this section, we report results on the above benchmarks. We first evaluate four open-source LRMs introduced in 3.1. Although all of them exhibit strong performance, there remains clear gap between DeepSeek-R1/QwQ-32B and the two distill models. Notably, with 671B parameters, DeepSeek-R1 demonstrates remarkable token efficiency, suggesting that stronger base capability can contribute to more concise reasoning. In addition, the fine-tuned Qwen-2.5-14B-it achieves substantial improvement over Qwen-2.5-14B-it, highlighting the effectiveness of test-time scaling. Overall, GRSP achieves the best scores on both task performance and token efficiency under both GRPO and REINFORCE frameworks. Notably, on the most challenging Omni-MATH 500, GRSP achieves the most significant reduction in token usage while maintaining the highest accuracy among all baselines, even matching or surpassing the naive RL version (Reinforce + GRSP). Although LCPO also performs competitively, it fails to deliver noticeable gains in token efficiency. We further analyze how token length correlates with task difficulty. For all methods, token overFigure 4: Comparison of two weighting strategies. (a) Accuracy over training steps; (b) Average response length over training steps; (c) Average segment length over training steps. head increases as problems become more challenging, indicating that models rely on length scaling to tackle complex reasoning tasks. RL accentuates this trend, as the token growth on Omni-MATH is much larger than on easier benchmarks, and GRSP primarily reduces token usage on such problems, where overthinking is most likely to occur, while preserving task performance. We also observe distinct segmentation patterns across different methods, which may strongly correlate with their results. Using the keyword-based segmentation on all models trained with Reinforce, we find that GRSP produces an average of 21.07 segments, notably fewer than 26.66 from the model trained without additional penalty. This confirms that GRSP effectively regulates the number of reasoning segments and thus reduces token overhead. In contrast, the two baselines yield substantially more segments of 51.97 and 142.12, respectively, and the largest part is short segments, as the ratio of segments from the cluster of = 1 is 79.17% and 91.36% for O1-pruner and LCPO, while that for GRSP is 62.61%. We attribute this to the ambiguity in supervising token length: it disturbs the optimization on verifiable reward, making the model resort to rapidly iterating short reasoning steps to maintain performance. Such settings not only lead to lower accuracy but also have larger likelihood of overthinking, as extremely long outputs are observed in LCPO. We provide further analyses of this pattern in 4.4. 4.4 Ablation on the Weighting Mechanism In this section, we investigate the effect of the proposed weighting mechanism across segment clusters. By default, the penalty weight decreases with the cluster index k, that is, longer segments receive smaller penalties (Descending), which encourages potentially deeper reasoning within each segment while discouraging the overproduction of short segments. As contrastive setting, we reverse the order to make the weights increase with k, defined as wk = (k1)0.05+1 (Ascending). We train both configurations under REINFORCE on Qwen-2.514B-it and track the evolution of accuracy (Acc), average token quantity of responses (Avg Len), and average segment length (Avg Seg Len), smoothed and visualized in Figure 4. The results reveal that test-time scaling emerges under both settings: as training progresses, the average token quantity grows alongside task performance. However, the Ascending configuration exhibits much steeper rise in both metrics, reaching peak earlier but soon suffering sharp accuracy collapse, indicating training instability. In contrast, the Descending configuration shows steadier improvement, with accuracy increasing more smoothly over time. However, after reaching the peak length, the Ascending model fails to compress thinking content effectively. Considering its high weight on the short segment clusters, it resembles the degenerate feature observed in LCPO, where the model excessively expands reasoning without meaningful gains. The segment-level patterns in Figure 4 (c) further support this observation. At the beginning, RLVR generally drives models to produce shorter segments to activate length scaling, and both configurations behave similarly. However, as training continues, the Ascending weights push the model to rapidly over-optimize for shorter segments, which is turning point that coincides with its accuracy collapse. Conversely, the Descending configuration gradually shifts toward generating longer segments, 6 quires fewer tokens than its 14B counterpart. This naturally raises the question of how model capacity interacts with GRSP during RL training. To investigate it, we conduct experiments on three models from the Qwen-2.5 family: 7B-it, 14B-it, and 32B-it. Each model is first warmed up with the same SFT dataset, followed by RL training under two settings: standard Reinforce and Reinforce + GRSP, with all other hyperparameters held constant. We still track accuracy and average response length to test how the effect of GRSP varies across model scales (Figure 5). Our results reveal two clear trends: (1) Larger models are inherently more efficient and accurate even in RL, where the accuracy improves steadily with model size, while token consumption decreases under the same training setup. (2) GRSP consistently improves efficiency across all scales, with minimal impact on accuracy. While accuracy remains nearly unchanged, the reduction in token usage can grow. In particular, the 32B model can complete tasks with significantly fewer tokens compared to smaller models, reflecting its greater capacity to leverage reasoning compression. 4.6 Confidence-based Segmentation In 3.3, we introduce another segmentation strategy based on the models confidence distribution. Unlike keyword matching, it does not rely on manually collected keywords yet achieves similarly strong performance. Transitions between reasoning segments often correspond to local minima in the models log probabilities. However, low logprob values can also occur in other cases, such as generating digits or single characters. Hence, after identifying positions where the smoothed logprob falls below threshold γ, we further filter them based on token length and whether they correspond to local minima. Figure 6 (a) illustrates an example where local minimum coincides with the start of new reasoning segment. We conduct experiments using the confidencebased segmentation under the Reinforce+GRSP framework on Qwen-2.5-14B, and compare it with the proposed keyword-based segmentation in terms of accuracy and response length. As shown in Figure 6 (b, c), the two curves exhibit similar trends. Notably, the confidence-based segmentation shows risefallrise pattern in both accuracy and length, whereas the keyword-based segmentation displays steady decline in length without recovery. These Figure 5: Effect of GRSP across models of varying capacities, comparing changes in accuracy and average response length. i.e., increasing the proportion of long segments. Interestingly, this adjustment leads to an overall shorter average response length, suggesting that the model learns more efficient reasoning strategy: thinking more thoroughly within each segment, thus requiring fewer total steps to reach the correct answer. These findings also shed light on the dynamics between the verifiable reward and the length penalty. During the early stage, the verifiable reward dominates, and both configurations exhibit similar trends across all three metrics. In the midstage, both encounter transition point where accuracy begins to drop bit. In the near time, length decreases, indicating shift in optimization focus from purely correctness to brevity, which in turn risks destabilizing training. The Descending configuration, however, escapes this collapse by increasing the proportion of long segments, allowing accuracy and efficiency to improve jointly. This confirms that accuracy and length optimization are not zero-sum game; with proper pattern, GRSP achieves stable balance between concise reasoning and task effectiveness. Hence, we conclude that descending weighting can be more stable and interpretable optimization signal. 4.5 Scaling of Model Capacity Results on open-source LRMs from Table 2 suggest that the capacity of base models plays critical role in task performance and token efficiency. For example, DeepSeek-R1, the largest model in our evaluation, achieves near the top accuracy and token efficiency, while DS-Qwen-Distill-32B also re7 Figure 6: (a) Illustration of the log-probability trend around segmentation point; (b) Comparison of accuracy between keyword-based and confidence-based segmentation; (c) Comparison of average response length. patterns are consistent with our findings in 4.4: the verifiable signal and the length penalty occasionally compete, leading to temporary degradation in one metric that later recovers, instead of collapsing entirely. Ultimately, the confidence-based segmentation achieves higher accuracylength pair (64.91, 3415) than the keyword-based segmentation (64.72, 3477), confirming the effectiveness of this design."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Test-time Scaling of LLMs Test-time scaling has proved effective at boosting LLM performance on complex question answering, mathematical problem solving, and code generation (Wu et al., 2024; Wang et al., 2022; Wei et al., 2022; Guo et al., 2025a). One line of such work is Monte Carlo Tree Search or tree-structured prompting (Wu et al., 2024; Yao et al., 2023). However, it involves trivial human-crafted engineering that hinders scaling up. Another line is Reinforcement Learning with Verifiable Reward (RLVR) (Team et al., 2025; Guo et al., 2025a; Muennighoff et al., 2025; Ye et al., 2025) which encourages exploration of long and complex reasoning paths via simple binary signals provided by verifiers. 5.2 Efficient Long Chain-of-Thought LLMs Despite the remarkable effectiveness of the long reasoning patterns, they suffers from substantial computational overhead, especially for challenging user inputs where the model tends to repeatedly deliberate, named overthinking. To mitigate such phenomena, recent work aims at shortening reasoning trajectories to reduce computation, while preserving task performance, e.g., accuracy (Sui et al., 2025). Lightweight approaches include designing control prompt, such as indicating token budget or specifying reasoning granularity (Han et al., 2025; Xu et al., 2025; Aytes et al., 2025), or intervening in the decoding process (Liu et al., 2025; Liao et al., 2025; Ding et al., 2025; Fu et al., 2024; Huang et al., 2025; Taubenfeld et al., 2025; Wang et al., 2025b). However, they do not essentially modify the models reasoning patterns, which may limit their robustness and ultimate performance. fundamental solution is to introduce supervision on reasoning efficiency during fine-tuning (Xia et al., 2025; Zhang et al., 2025; Arora and Zanette, 2025; Aggarwal and Welleck, 2025; Luo et al., 2025; Team et al., 2025), especially in RLVR settings where both positive and negative rewards can effectively guide model behavior. Nevertheless, existing studies mostly impose penalties on tokens, while the correlation between token-level penalties and the final verifiable rewards remains weak, leading to limited compression effectiveness or even degradation in accuracy."
        },
        {
            "title": "6 Conclusion",
            "content": "To mitigate the overthinking of Large Reasoning Models (LRMs), we propose Group Relative Segment Penalization (GRSP), step-level method that regularizes reasoning in RLVR. By supervising at the granularity of reasoning segments and applying length-aware weighting, GRSP effectively mitigates performance degradation while still increasing efficiency. We conduct extensive experiments to verify that GRSP has the advantages in the above two aspects, while also improving training stability and scaling across model sizes. We hope our study could inspire future research on the granularity selection of supervision during the design of RLVR algorithms for LRMs."
        },
        {
            "title": "Limitations",
            "content": "We also experimented with warm-up data constructed from other patterns, such as those derived from DeepSeek-R1. However, we found that such data tends to make the model generate overly long responses even before reinforcement learning begins. As result, it becomes difficult to observe clear scaling effects on response length. To further validate the compression of token usage, it would be ideal to continue training from our current checkpoint using reinforcement learning. Nevertheless, this requires substantial computational resources beyond our current budget. Despite these limitations, our experiments demonstrate the robustness of the proposed method and its potential for continued efficient scaling. We hope future work will further explore these directions."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks arXiv preprint learning. with reinforcement arXiv:2503.04697. Daman Arora and Andrea Zanette. 2025. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463. Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang. 2025. Sketch-of-thought: Efficient LLM reasoning with adaptive cognitive-inspired sketching. CoRR, abs/2503.05179. Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, and 1 others. 2025. Dynamic parallel tree search for efficient llm reasoning. arXiv preprint arXiv:2502.16235. Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Yonghao Zhuang, Yian Ma, Aurick Qiao, Tajana Rosing, Ion Stoica, and 1 others. 2024. Efficiently scaling llm reasoning with certaindex. arXiv preprint arXiv:2412.20993. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. 2025a. Omni-MATH: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations. Bofei Gao, Yejie Wang, Yibo Miao, Ruoyu Wu, Feifan Song, Longhui Yu, Tianyu Liu, and Baobao Chang. 2025b. Towards better initial policy model for scalable long-cot reinforcement learning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 76527665. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025a. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, and Shuang Qiu. 2025b. Segment policy optimization: Effective segment-level credit assignment in rl for large language models. arXiv preprint arXiv:2505.23564. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2025. Tokenbudget-aware LLM reasoning. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 2484224855. Association for Computational Linguistics. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. 2025. Efficient testarXiv preprint time scaling via self-calibration. arXiv:2503.00031. Bolian Li, Yifan Wang, Anamika Lochab, Ananth Grama, and Ruqi Zhang. 2024. Cascade reward sampling for efficient decoding-time alignment. arXiv preprint arXiv:2406.16306. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numinamath. [https://huggingface.co/ AI-MO/NuminaMath-1.5](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. 2025. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu, Jason Klein Liu, Chonghan Liu, Zefan Cai, Yunhui Xia, Li Zhao, Jiang Bian, and 1 others. 2025. Adaptivestep: Automatically dividing reasoning step through model confidence. arXiv preprint arXiv:2502.13943. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025. O1-pruner: Lengthharmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570. 9 Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. CoRR, abs/2502.18600. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. 2025. Lightthinker: ThinkarXiv preprint ing step-by-step compression. arXiv:2502.15589. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, and 1 others. 2025. Group sequence policy optimization. arXiv preprint arXiv:2507.18071. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Feifan Song, Shaohang Wei, Wen Luo, Yuxuan Fan, Tianyu Liu, Guoyin Wang, and Houfeng Wang. 2025. Well begun is half done: Low-resource preference alignment by weak-to-strong decoding. arXiv preprint arXiv:2506.07434. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, and 1 others. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, and Gal Yona. 2025. Confidence improves self-consistency in llms. arXiv preprint arXiv:2502.06233. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, and 1 others. 2025a. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. 2025b. Sampling-efficient test-time scaling: Selfestimating the best-of-n sampling in early decoding. arXiv preprint arXiv:2503.01422. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837."
        },
        {
            "title": "A Statistics of Utilized Data",
            "content": "Please refer to Table 3. Stage Data Source # SFT NuminaMath-1.5 (LI et al., 2024) RL Eval AIME Omni-MATH (Gao et al., 2025a) MATH 500 (Lightman et al., 2024) AIMO Prize 1 Omni-MATH 500 (Gao et al., 2025a) 800 2400 500 10 * 8 Table 3: Statistics of data utilized for SFT, RL, and evaluation, respectively."
        },
        {
            "title": "B Implementation Details",
            "content": "We set the maximum sequence length to 30K to accommodate long reasoning trajectories. Each RL training run consists of 150 steps, with rollout performed every 3 steps. The learning rate is set to 2e-6. We randomly sample 1024 problems for each rollout iteration, and for each problem, we conduct 10 rollouts per iteration with temperature as 1.0, while temperature for evaluation is 0.6. Dynamic sampling (Yu et al., 2025) is applied, so the actual batch size varies across iterations. For GRSP, the descending weights for each length cluster wk are determined by wk = (K k) 0.05 + 1, where the maximum cluster index is 5. The balancing coefficient α is set to 5e-3 for keyword-based segmentation and 2.5e-3 for confidence-based segmentation. We report the results with the highest task performance."
        },
        {
            "title": "C GRPO Ablation on the Weighting Mechanism",
            "content": "Figure 7 presents the ablation results of the weighting mechanism on GRPO. similar trend to Reinforce in Figure 4 can be observed, where the Ascending weighting improves accuracy more rapidly while the Descending weighting achieves stronger compression and maintains high accuracy. In addition, we find that GRPO training is generally more stable than Reinforce across all methods, so the Ascending setting does not collapse in the middle stage and excessively encourages short segments as in Reinforce training. Nevertheless, the Descending weighting still produces longer reasoning segments as expected. Figure 7: Comparison of two weighting strategies for GRPO. (a) Accuracy over training steps; (b) Average response length over training steps; (c) Average segment length over training steps. Confidence-based Segmentation on GRPO Figure 8 illustrates the effect of confidence-based segmentation on GRPO, compared to the default keyword-based segmentation. It acquires higher accuracy in Figure 8 (a). In fact, it reaches more surprising performance of (64.81, 3405) for accuracy and average response length than (64.63, 3427) 11 Figure 8: GRPO results. (a) Comparison of accuracy between keyword-based and confidence-based segmentation; (b) Comparison of average response length. of GRPO + GRSP + keyword-based segmentation. Note that around the last 20 steps actually witness significant drop in response length, enabling the model to be both powerful and efficient in reasoning, which cannot be shown in Figure 8 (b) due to the curve smoothness."
        },
        {
            "title": "E Prompt Template",
            "content": "<im_start>system You are helpful assistant. You should first try long-text process of thinking and reflection to handle the problem in the mind before each responding to the user. The thinking process are enclosed within <think>n and </think>nn tags, respectively, i.e., <think> [thinking process here]</think> [final answer here]. <im_end> <im_start>user The problem text Please reason carefully step by step, reflect thoroughly, and put the final answer in boxed{{}}. <im_end> Figure 9: The prompt template used for SFT and RL."
        }
    ],
    "affiliations": [
        "Moonshot AI",
        "State Key Laboratory of Multimedia Information Processing School of Computer Science, Peking University"
    ]
}