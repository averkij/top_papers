{
    "paper_title": "Robust Reward Modeling via Causal Rubrics",
    "authors": [
        "Pragya Srivastava",
        "Harman Singh",
        "Rahul Madhavan",
        "Gandharv Patil",
        "Sravanti Addepalli",
        "Arun Suggala",
        "Rengarajan Aravamudhan",
        "Soumya Sharma",
        "Anirban Laha",
        "Aravindan Raghuveer",
        "Karthikeyan Shanmugam",
        "Doina Precup"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome (Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM. Empirically, Crome significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 7 0 5 6 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Robust Reward Modeling via Causal Rubrics",
            "content": "Pragya Srivastava1*, Harman Singh1*, Rahul Madhavan1*, Gandharv Patil2,3, Sravanti Addepalli1, Arun Suggala1, Rengarajan Aravamudhan1, Soumya Sharma1, Anirban Laha1, Aravindan Raghuveer1, Karthikeyan Shanmugam1, Doina Precup1,3 1Google DeepMind, 2McGill University, 3MILA - Quebec AI Institute, *Equal Contribution 2025-6-23 Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome (Causally Robust Reward Modeling), novel framework grounded in an explicit causal model designed to mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM. Empirically, rom significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained in Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k. 1. Introduction Aligning Large Language Models (LLMs) with human preferences is paramount for their safe and effective deployment, with Reinforcement Learning from Human Feedback (RLHF) and its reliance on reward models (RMs) being the dominant paradigm (Bai et al., 2022a; Christiano et al., 2017; Ouyang et al., 2022; Rafailov et al., 2024; Schulman et al., 2017; Shao et al., 2024). The fidelity of these RMs is critical, as flaws directly propagate to the aligned policy (Casper et al., 2023). However, standard RM training faces significant challenge: reward hacking (Gao et al., 2023; Skalse et al., 2022). RMs often learn to assign high scores based on superficial or spurious attributessuch as response length (Singhal et al., 2023), specific formatting patterns (Zhang et al., 2024), or stylistic quirksbecause these features are statistically correlated with preferred responses in the training data. This occurs because standard training objectives do not explicitly require the RM to disentangle the true causal drivers of response quality (e.g., factuality, relevance) from these spurious correlates, leading to brittle RMs and misaligned policies (Eisenstein et al., 2023; Shen et al., 2023). Recent efforts for RM robustness have explored various avenues. Some focus on consistency checks against meaning-preserving transformations (Wu et al., 2025), while others employ data augmentations, such as using non-contextual or query-independent comparisons to reduce spuriousness (Liu et al., 2024). Corresponding author(s): {pragyahere, hrman, rahulmadhavan, araghuveer, karthikeyanvs, doinap}@google.com 2025 Google DeepMind. All rights reserved Robust Reward Modeling via Causal Rubrics Attribute-based evaluation, often leveraging LLMs to dynamically generate assessment criteria (Gupta et al., 2025), aims for more grounded reward signals. Other works investigate specific regularization techniques against known biases like length or sycophancy (Wang et al., 2025), or explore methods for causal effect estimation like RATE (Reber et al., 2024). Despite these advances, significant limitations persist. Many approaches target only prespecified spurious factors, potentially missing unknown correlates, or lack the fine-grained control needed to truly isolate causal quality drivers from confounding spurious features within responses. Augmentation strategies can be coarse (Liu et al., 2024), and evaluationfocused methods (Gupta et al., 2025; Reber et al., 2024) may not directly equip the RM with mechanisms for robust training against wide array of spurious variations through targeted counterfactual learning. There is thus clear need for framework that systematically leverages causal understanding of preference formation to train RMs that are both sensitive to causal quality attributes and demonstrably invariant to diverse spurious cues. Figure 1 The Crome Data Augmentation and Training Pipeline. From an original QA pair (Q, A1, A2), an oracle LLM identifies Causal Attributes (CA). This guides counterfactual generation, producing degraded A1, and upgraded A2 responses. These form the set of Causal Augmentations which teach the model sensitivity to relevant attributes. Next, we generate Irrelevant Query Neutrals by flipping the question on both the newly generated causal contrastive pairs and the original answer pairs, which reduces reliance on spurious correlates. After verification and filtration, the combined dataset (Originals, Causals, Neutrals) trains the RM, enhancing its robustness. Motivated by this, we aim to address the following question in this paper: How do we train reward models to be robust against reward hacking, particularly when a) the specific spurious attributes that an RM may exploit are not known, and b) only the stable or invariant causal attributes found in ground truth/human preferences can be accessed? To address this question, we propose rom (Causally Robust Reward Modeling), novel framework grounded in an explicit causal model of answer generation (Figure 2). rom teaches the RM to differentiate genuine quality drivers from superficial cues by augmenting the preference dataset with targeted, LLM-generated counterfactual examples. It creates two key types of synthetic training pairs: (1) Causal Augmentations, which introduce changes along specific causal attributes (e.g., factuality) to enforce sensitivity to true quality shifts, and (2) Neutral Augmentations, using both (i) the causally augmented data as well as (ii) the original preference pairs, to enforce invariance along spurious attributes (e.g., style) using tie-labels. Training on this enriched dataset with modified loss (Section 4) guides the RM towards causal faithfulness. Our evaluations show rom significantly improves robustness, boosting RewardBench accuracy by up to 4.5%, with substantial gains in Safety and Reasoning. We list the key contributions in this work below. 2 Robust Reward Modeling via Causal Rubrics 1. Spurious-Unaware Causal Framework. We propose causal framework for training reward models (Sec. 3) that requires intervention only on LLM-identified causal quality rubrics, eliminating the need for prior specification of or intervention on any of the spurious attributes. 2. Targeted Counterfactual Augmentations along Causal Attributes. We propose to train reward models on the available preference data and the proposed data augmentations (Sec. 4) along LLMidentified causal attributes: 1) Causal Augmentations create minimal pairs isolating specific causal dimensions for precise sensitivity. 2) Neutral Augmentations create variations in spurious features (preserving causal content) with tie-labels for invariance. Notably, we do not assume any explicit knowledge of spurious factors nor do we perturb them directly to create these augmentations. We show that interventions along causal rubrics alone is primarily sufficient to mitigate sensitivity to much larger set of spurious correlates. 3. State-of-the-Art RM Robustness. rom significantly outperforms baselines on RewardBench (Sec. 6), improving average accuracy by up to 5.4% (Safety +13.18%, Reasoning +7.19%)  (Table 2)  , and shows superior robustness on reWordBench (Figures 5). 4. Improved BoN results. Best-of-N selection using Crome-RM shows consistent gains across different values of when compared to baselines on the popular RewardBench, WildGuardTest and GSM8K benchmarks. This highlights the robustness of rom in the presence of rare (or long tailed) spurious factors as well, which typically appear at large values of N. 2. Related Works Our work on causally robust reward modeling, Crome, addresses the challenge of reward hacking in the context of aligning Large Language Models (LLMs) via Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022a; Ouyang et al., 2022). Standard RLHF relies on reward model (RM), typically trained on pairwise preferences using Bradley-Terry (Bradley and Terry, 1952) or pairwise ranking approaches (Liu et al., 2025; Qin et al., 2023). critical limitation of learned RMs is reward hacking (Gao et al., 2023; Skalse et al., 2022), where the RM assigns high scores based on spurious attributes (e.g., verbosity (Singhal et al., 2023), formatting (Zhang et al., 2024), sycophancy (Denison et al., 2024)) that are correlated with, but do not cause, true response quality. This leads to misaligned policies that exploit these spurious cues (Shen et al., 2023). Various mitigation strategies exist, including architectural modifications like Odin (Chen et al., 2024), policy-level adjustments (Park et al., 2024), and data-centric methods involving ensembles (Ramé et al., 2024) or consistency checks (Shen et al., 2023). Recent causal-inspired approaches include using MMD regularization against pre-specified spurious factors (Wang et al., 2025) or estimating the causal effects of given attribute of response using corrected rewrites (Reber et al., 2024). Our approach falls into the data-centric category, using synthetic data augmentation guided by principles of causal inference (Pearl, 2009; Peters et al., 2017). While prior work has used LLMs for causal reasoning (Kiciman et al., 2023) or counterfactual data augmentation in NLP (Kaushik et al., 2019), and related methods like RRM (Liu et al., 2024), ReWordBench (Wu et al., 2025) target RM robustness, Crome is distinct in its explicit use of causal graph framework (Section 3.2) which guides the answer generation and the reward labeling process. We leverage LLMs to generate targeted causal (attribute-specific upgrade/degradation) and neutral (spurious-varying, causally-equivalent) counterfactual examples. By training on this augmented data, Crome aims to systematically disentangle causal attributes (𝐶) from spurious ones (𝑆𝑃), learning reward function that is inherently more robust and aligned with the true drivers of quality, as detailed in Section 4. We provide longer version of related work in Appendix B. 3 Robust Reward Modeling via Causal Rubrics Path/ Relationship Interpretation Summary (Q, C(A)) SP(A) C(A) SP(A) C(A) Ground-truth reward determined by query and causal attributes C(A); stable relationship. Query and unknown spurious attributes SP(A) are correlated/confounded by unstable exogenous factors. Query determines relevant causal attributes C(A). Bidirectional (potentially complex) relationship between spurious SP(A) and causal C(A) attributes. Figure 2 Conceptual Causal Graph for Reward Modeling. is the query. Answer (A) has causal attributes C(A) and spurious attributes SP(A). dim(𝐶( 𝐴)) dim(𝑆𝑃( 𝐴)) 𝐴. SP(A) is unknown. Ground-truth reward depends only on C(A) and (R SP(A)C(A), Q). Augmentations heighten ˆR𝜃s sensitivity to C(A) (approximated by oracle LLM). 3. Causal Framework for Reward Modeling We aim to develop reward model that accurately assesses the quality of an answer provided in response to query Q. Our approach is grounded in causal framework designed to distinguish genuine quality drivers from spurious correlates often present in preference data. This involves understanding the answer generation process and strategically augmenting training data with approximated counterfactual examples. 3.1. Reward Model and Pairwise Preferences We train reward model (RM), denoted ˆR𝜃(Q, A), to assign scalar quality score to an answer for query Q. This RM is typically optimized on dataset preferences pairs Dpref = {(Q(𝑖) , (𝑖) 𝑖=1. Given pair of answers (A1, A2), the probability of A1 being preferred over A2 is commonly modeled using the Bradley-Terry framework (Bradley and Terry, 1952): 𝑤 , (𝑖) )}𝑁 𝑙 P(A1 A2Q; 𝜃) = 𝜎(ˆ𝑠𝜃(Q, A1) ˆ𝑠𝜃(Q, A2)) = exp(ˆ𝑠𝜃(Q, A1)) exp(ˆ𝑠𝜃(Q, A1)) + exp(ˆ𝑠𝜃(Q, A2)) (1) where ˆ𝑠𝜃(Q, A) represents the underlying scalar score (or logit) assigned by the model to answer for query Q. 1 The parameters 𝜃 are learned by minimizing the negative log-likelihood of preferences. 3.2. Causal Model of Answer Generation We propose causal model (Figure 2) for answer generation and quality perception. For query-answer pair (Q, A), we distinguish two attribute types: Causal Attributes C(A) = {C1, . . . , Cℓ}: Fundamental quality dimensions (e.g., factuality, relevance) genuinely determining quality relative to Q. Spurious Attributes SP(A) = {SP1, . . . , SP𝑘}: Other features (e.g., length, formatting) correlated with preferences or in Dpref, but not intrinsically determining quality. SP(A) can be high-dimensional and unknown. 1The score ˆ𝑠𝜃 (Q, A) can be the direct output of reward head or, in some pairwise preference models, ˆ𝑠𝜃 (Q, A1) ˆ𝑠𝜃 (Q, A2) might be directly modeled as the logit of preferring A1 over A2. 4 Robust Reward Modeling via Causal Rubrics Category Strategy Generation Pair Example Causal Augmentation (Dcausal) - Enhancing Sensitivity to ( A(𝐶 𝑗upgraded) , A) or Causal (A, A(𝐶 𝑗degraded) ) Attribute Upgradation/Degradation Neutral Augmentation (Dneutral) - Enforcing Invariance to SP Neutral Pairing with Irrelevant Queries (B1, B2) with new Qirrelevant s.t. C(B1Qirrelevant) C(B2Qirrelevant) 0 Assigned Label Training Objective (P𝜃) 1 (tie) 0.5 Table 1 Summary of Crom es synthetic data augmentation strategies using LLM-approximated counterfactuals. A(𝐶 𝑗target) signifies an LLM-generated counterfactual of with its 𝑗-th causal attribute 𝐶 𝑗 modified. The ground-truth reward R(Q, A) is assumed to be solely function of causal attributes: R(Q, A) = 𝑓 (Q, C(A)). This implies conditional independence: SP(A)Q, C(A). We explicitly assume the following stability property: If the entire process of answer generation and reward labeling were repeated (e.g., with different labeler or answer generator), the relationship (Q, C(A)) determining the reward is stable/invariant. In contrast, correlations involving SP(A) (e.g., SP(A) C(A) or SP(A) Q) can arise from various, potentially unstable or unknown exogenous factors, and thus these correlations may vary across such repetitions. The primary challenge is that standard reward models ˆR𝜃 may inadvertently learn high sensitivity to these unstable correlations with SP(A) (due to its unknown, high-dimensional nature). Our goal is to train ˆR𝜃 such that its dependence on is primarily mediated through the identified, stable causal attributes C(A), ensuring robustness to unspecified SP(A). 3.3. Approximating Counterfactuals for Attribute Intervention To instill causal sensitivity and spurious invariance in ˆR𝜃, rom leverages counterfactual reasoning about how answer quality changes if specific attributes were altered. For an answer with attributes (C(A), SP(A)), an ideal counterfactual, 𝐴(𝐶 𝑗𝑐 𝑗 ) (𝑢), would manifest if only its 𝑗-th causal attribute 𝐶 𝑗 were set to 𝑐 , considering its causal effects on other features, while all other exogenous factors 𝑢 (that 𝑗 produced the factual answer 𝑎) remained constant. Formally, 𝑃𝑈 ( 𝐴(𝐶 𝑗𝑐 As generating such ideal textual counterfactuals is intractable, Crome employs Large Language Models (LLMs) to produce approximations. These LLM-generated answers, denoted A(𝐶 𝑗target) , are rewrites of an original answer A, prompted to modify 𝐶 𝑗 (e.g., to degraded state, lowering reward) while aiming for minimal changes to other attributes. Remark 1. For brevity, we denote these LLM approximations as A(𝐶 𝑗𝑐) , dropping the explicit 𝑢 conditioning, assuming the generation approximates such sample. While imperfect, these approximations provide the targeted variations crucial for our data augmentation. 𝑗 ) (𝑈) 𝐴(𝑈) = 𝑎). 3.4. Augmented Training Data for Causal Disentanglement We augment the original preference dataset Dpref with synthetically generated examples Daug designed to enforce specific causal properties on ˆR𝜃. This augmented dataset Daug comprises two principal categories: Causal Augmentation Pairs (Dcausal) and Neutral Augmentation Pairs (Dneutral), summarized in Table 1. Robust Reward Modeling via Causal Rubrics Figure 3 Visualizing Cromes core augmentation strategies (detailed in Appendix G). (Top) Causal Augmentation: For given query, we use an LLM-driven counterfactual generation process to alter specific causal attribute, yielding Answer 2. Some spurious attributes may co-vary. The RM is trained with preference (e.g., 𝐴1 𝐴2 if 𝐴2 is degradation), teaching causal sensitivity. (Bottom) Irrelevant Query Neutral: The same answer pair (𝐴1, 𝐴2) is re-contextualized with new, irrelevant question. Their original causal attributes become effectively spurious or irrelevant (greyed-out bar). The RM is trained with tie-label (𝐴1 𝐴2), teaching invariance to the attribute differences when no true causal signal for the current query exists. This illustrates how IQN provides invariance to those spurious attributes that change with (like length of response changing with clarity of response). similar invariance is imposed using the ( 𝐴1, 𝐴2) pairs from the original dataset to provide robustness to general spurious attributes (SP) that do not change with C. 3.4.1. Causal Augmentation Pairs Cromes strategy causal pairs Dcausal focus on isolating the impact of important causal attributes. Attribute Upgradation and Degradation. For an original answer (from Dpref) and specific causal attribute 𝑗, we generate LLM-approximated counterfactuals. If is of lower quality regarding 𝑗, we create an upgraded version A(C 𝑗upgraded) . The pair ( A(C 𝑗upgraded) , A) is added to Dcausal with label A(C 𝑗upgraded) post-verification. Conversely, if is of higher quality on 𝑗, we generate degraded version A(C 𝑗degraded) . The pair (A, A(C 𝑗degraded) ) is added to Dcausal with label A(C 𝑗degraded) . These pairs collectively teach ˆR𝜃 sensitivity to changes along individual causal dimensions. 3.4.2. Neutral Augmentation Pairs Neutral Augmentation Pairs, Dneutral (with tie-labels) teach invariance to SP(A) when C(A) is held constant/ is irrelevant. Irrelevant Query Neutrals (IQN) We pair two answers, B1, B2 (from Dpref Dcausal), with new, unrelated query Qirrelevant. This makes their causal attributes w.r.t. Qirrelevant (i.e., C(B1Qirrelevant), C(B2Qirrelevant)) minimal. The pair (B1, B2) under Qirrelevant receives tie-label, training the RM to disregard spurious differences when causal relevance is absent. Their causal distinction becomes moot, isolating spurious variations under Qirrelevant. Presenting these as tied responses to the reward model enforces invariance to such spurious attributes. We provide various other techniques tested for spurious suppression in Section 6.3. The rationale for rom es specific choices are discussed in Appendix along with different neutral augmentation strategies we tried out. We provide the prompts for generating neutrals in Section K. 6 Robust Reward Modeling via Causal Rubrics Figure 4 The Crome data augmentation pipeline. Original preference data (Dpref) is used as basis to generate: (1) Causal Augmentations (Dcausal) by performing Attribute Upgradation and Degradation on specific attributes to enforce sensitivity to genuine quality drivers, and (2) Neutral Augmentations (Dneutral) via Irrelevant Query Neutrals (with tie-labels) to teach spurious feature invariance. After optional filtering, the reward model is trained on the combined original and augmented dataset. 4. Methodology: Training Robust Reward Model The Crome framework trains robust reward models using causally-motivated data augmentation strategy, outlined in Figure 4. This involves two main phases: (1) generating attribute-aware counterfactual data based on our causal model (Section 3), and (2) training the reward model ˆR𝜃 with specialized loss on the combined data. 4.1. Attribute-Aware Counterfactual Data Generation This phase prepares the augmented dataset Daug = Dcausal Dneutral required for robust training, involving three conceptual steps: Step 1: Attribute Identification. As prerequisite, we identify the Principal Causal Components = (C1, . . . , Cℓ) relevant to the task, leveraging the causal framework from Section 3.2. This typically involves LLM prompting and refinement (Details in Appendix H.1). Step 2: Counterfactual Generation. Using the identified attributes C, we generate synthetic data pairs via LLM-approximated counterfactuals, as defined in Section 3.3. Following the strategies summarized in Table 1 and detailed conceptually in Section 3.4, we create: Causal Augmentation Pairs (Dcausal): Examples enforcing sensitivity to individual causal attributes 𝑗 via Attribute Upgradation and Degradation, with standard preference labels (). Neutral Augmentation Pairs (Dneutral): Examples enforcing invariance to spurious attributes SP while ensuring is irrelevant or holding causal content constant. These are generated via Irrelevant Query Neutrals and Causally Aligned Neutrals respectively. These receive tie labels (). LLM prompts are in Appendix K. This yields the raw Daug. 3. Data Filtering. Daug is filtered to Daug_filtered by retaining pairs where baseline RM (trained on Dpref) is uncertain or incorrect, focusing training on informative examples (details: Section 6, Appendix H.3). This yields the final training datasets Dpref and Daug_filtered. Robust Reward Modeling via Causal Rubrics 4.2. Robust Reward Model Training Given the original data Dpref and the filtered augmented data Daug_filtered, the final rom reward model ˆR𝜃 is trained by minimizing composite loss function (𝜃) over the combined dataset = Dpref Daug_filtered: (𝜃) = log(cid:2)𝜎(Δ𝑤𝑙)(cid:3) 𝜆 (cid:18) 1 2 (cid:2)log 𝜎(Δ12) + log 𝜎(Δ12)(cid:3) (cid:19) (2) (Q,y𝑤,y𝑙 ) Dpref Dcausal (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) Preference Loss (Causal Sensitivity) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (Q,A1,A2, 𝑦=tie) Dneutral (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) Neutral Tie Loss (Spurious Invariance) (cid:125) where Δ𝑤𝑙 = ˆR𝜃(Q, A𝑤) ˆR𝜃(Q, A𝑙) and Δ12 = ˆR𝜃(Q, A1) ˆR𝜃(Q, A2). The first term (Preference Loss) trains sensitivity to causal quality using Dpref and Dcausal. The second term (Neutral Tie Loss, weighted by 𝜆 0) trains invariance to spurious features using Dneutral by encouraging Δ12 0 for tie-labeled pairs. For our current set of experiments we keep 𝜆 = 1. This optimization guides ˆR𝜃 to be sensitive to causal attributes while remaining robust to variations in spurious attributes SP. We demonstrate rom es effectiveness in mitigating reward hacking and improving downstream policy performance in Section 6. 5. Theoretical Analysis We provide theoretical analysis, detailed in Appendix I, to formalize how Cromes causal augmentation isolates true reward drivers from spurious correlates. Under an idealized model, we show that training on data with targeted interventions on causal attributes enables the learned reward model to accurately identify causal reward determinants, even in the presence of numerous, unspecified spurious features. Intuition and Analytical Approach When only specific causal attribute is intervened to vary, and all other causal attributes are fixed to their factual versions, and spurious factors are ancestral to all causal attributes, then the reward model is forced to learn the true impact of that causal attribute in an approximate sense. To formalize this, we consider setting where: (1) Causal attributes C(A) and spurious attributes SP(A) are modeled as boolean variables. (2) True reward is sparse quadratic polynomial of C(A) only. (3) The learned ˆR𝜃 can be denser quadratic polynomial including SP(A) and C(A)SP(A) terms. (4) Spurious attributes SP(A) are not descendants of causal attributes C(A). (5) Causal augmentation is an ideal counterfactual that (given same exogenous factors leading to the answer) intervenes one 𝐶𝑖 𝐶𝑖, leaving other 𝐶 𝑗 intervened to be their factual versions. We frame learning the coefficients of as an ℓ1-constrained linear regression (Lasso) on features derived from attribute differences between an augmented answer 𝐴aug and its original 𝐴. The key insight is that the feature matrix from such augmented pairs exhibits properties conducive to sparse recovery, such as low column coherence or satisfying Restricted Isometry Property (RIP) variant. Specifcally, compared to the original training set, the augmented one has much lower RIP. 8 Robust Reward Modeling via Causal Rubrics Method PairPM BT Average Chat Chat-Hard Safety Reasoning Average Chat Chat-Hard Safety Reasoning 81.22 82.54 87.84 97.90 97. 97.54 63.64 71.05 72.30 77.48 74. 87.14 85.88 87.27 94.39 79.14 83. 85.46 97.26 97.21 96.28 58.85 69.15 65.83 69. 73.13 84.05 91.17 94.35 95.70 ΔCrome - RRM +5.30 +0.42 +1.25 +12.44 +7.12 +2.00 -0.93 -3.32 +10.92 +1.35 78.18 82.04 83.15 97.21 97. 96.37 52.85 64.80 61.73 73.99 75.27 82. 88.68 90.86 92.26 72.73 78.20 80. 97.21 98.04 96.93 46.27 59.65 58.66 68.04 72. 78.92 79.39 82.66 88.71 ΔCrome - RRM +1.11 -0.84 -3.07 +6.96 +1.40 +2.61 -1.11 -0.99 +6.49 +6.05 53.75 66.23 70.69 92. 94.13 92.18 33.33 43.75 50.00 42.03 47. 55.14 46.74 79.38 85.42 65.52 66. 72.45 94.27 94.97 92.74 38.27 49.34 53. 50.20 50.07 60.00 79.34 73.42 83. ΔCrome - RRM +4.46 -1.95 +6.25 +7.50 +6.04 +5.50 -2.23 +4.28 +9.93 +10.03 Vanilla RM RRM Crome - 9 - 2 - e RRM Vanilla RM 7 - 5 . 2 Q Crome RRM Vanilla RM 2 - 2 - e Crom Table 2 Performance Comparison of Pairwise Preference Model and Bradley-Terry Reward Model on RewardBench trained using various base models. See Appendix Section C.1 for variance in results. 5.1. Main Theoretical Result (Informal) This structure leads to the following result (formalized as Theorem 2 in Appendix I): Theorem 1 (Informal Statement). Under the idealized model assumptions, ℓ1-constrained regression on 𝑚 causally augmented examples recovers the true causal reward coefficients with an ℓ2-error 𝜃 ˆ𝜃2 that scales (ignoring constants and terms related to imperfect sparsity recovery) roughly where is the top 𝑂(𝑘) coefficients in the 𝑅 true reward model. This as 𝑂 𝜃N𝑐 1( 1 log(𝑘+ℓ) 𝑚 𝑘 + (cid:18) (cid:19) ) highlights primary dependence on the number of causal attributes 𝑘 and samples 𝑚, and only weak, logarithmic dependence on the spurious attribute dimension ℓ. Implications: This theorem suggests that rom es causal augmentation, by promoting favorable properties (like RIP or low incoherence) in the effective design matrix, guides the reward model towards genuine causal drivers. Further, the error vector has ℓ2 norm is linear in the causal dimension 𝑘 in the worst case and zero in the best case where 𝑅 has sparser dependence on the causal factors. If it was the preference training dataset, the error could be proportional to 𝜃1 (which is 𝑂(𝑘2)). 6. Experiments Our experiments are designed to address the following research questions: RQ1: RM Performance and Robustness: How does Crome perform on standard preference prediction tasks and how robust is it against spurious correlations(Table 2, Figure 5)? RQ2: Best-of-N Alignment: Does the robustness achieved by rom lead to favorable results in Best-of-N setup as well, when compared to strong baselines (Figures 7, 8, Table 3)? RQ3: Neutral Augmentations: How effective are the different neutrals augmentation strategies in enforcing invariance to unknown spurious correlates (Figures 9, 10)? 9 Robust Reward Modeling via Causal Rubrics 6.1. Experimental Settings Crome and baseline reward models (Vanilla RM, RRM (Liu et al., 2024)) are trained on the UltraFeedback dataset (Cui et al., 2023), with counterfactuals generated using Gemini 2.0 Flash. We evaluate performance on RewardBench (Lambert et al., 2024) and robustness on reWordBench (Wu et al., 2025) 2. Experiments utilize diverse base LLMs (Gemma-2-9B-IT, Qwen2.5-7B, Gemma-2-2B) for both Pairwise Preference (PairPM) and Bradley-Terry (BT) reward models. Downstream alignment impact is assessed via Best-of-N selection on tasks including RewardBench, GSM8K, and WildGuardTest. Comprehensive details on datasets, model specifics, augmentation procedures, filtering, training hyperparameters, and all experimental configurations are provided in Appendix E. 6.2. Experimental Results addressing Research Questions (RQ1-3): Figure 5 Robustness of rom on reWordBench. Comparing RM, RRM and rom by measuring ranking accuracy on diverse set of meaning preserving transformations in reWordBench. Various transformations such as paraphrasing, addition of irrelevant text or code, comments etc, test the sensitivity of models to spuriousness. Robust training of Crome leads to robustness to spuriousness and increased sensitivity to causal attributes. On RewardBench  (Table 2)  , rom consistently improves ranking accuracy over RRM across diverse base models and reward modeling techniques (PairPM, BT). These improvements are particularly notable on the challenging Safety (up to 13.18%) and Reasoning (up to 7.19%). rom also demonstrates superior robustness on reWordBench, which tests for robustness of RMs against meaning-preserving transformations (Figure 5). These results show Cromes robustness to inputs having spurious punctations, paraphrasing, irrelevant text, code or comments as tested by various reWordBench transformations. With Gemma-2-9B-IT, rom in the PairPM setting shows an aggregate accuracy gain of up to 9.1% and is superior on (21/23) transformations. Key Takeaway: rom improves RM performance on standard benchmarks while significantly improving performance and mitigating ranking accuracy drops on diverse transformed inputs, without ever being explicitly trained on such spurious transformations. 2Since reWordBench has not been released, we follow the paper and communicated with the authors to reproduce it, see Appendix Section 10 Robust Reward Modeling via Causal Rubrics BoN for Robust LLM Alignment Across Chat, Reasoning, and Safety Following the method used by Wu et al. (2025), we perform best-of-n selection using rom across RewardBench categories, which consists of datasets such as AlpacaEval. Across all values of 𝑁, Crome provided significant improvements over baselines in head-to-head comparison. Key Takeaway: Cromes emphasis on causal attributes enhances its discriminative power in Best-of-N selection, leading to more consistent identification of superior responses. 4 8"
        },
        {
            "title": "Crome",
            "content": "RM"
        },
        {
            "title": "RRM Ties",
            "content": "28.08 34.32 39.93 44.79 13.85 58.07 17.24 48.43 20.54 39.53 21.88 33.33 28.03 34.36 41.14 45.46 14.13 57.84 17.19 48.45 20.40 38.46 22.01 32.53 Table 3 Win rates for Crome compared with RM and RRM on RewardBench. We follow Wu et al. (2025) and take all 2985 prompts from RewardBench and get BoN responses from Gemma-2-9B-IT model using rom e, RM or RRM as the reward models. Following this, we separately compare responses generated by rom with RM and RRM, using GPT-4 as judge. Figure 6 Percentage improvement in ranking accuracy between RewardBench and reWordBench. Here we show the average ranking accuracy across reWordBench transformations of Crome and baselines on reWordBench and RewardBench as done in Wu et al. (2025), as well as the percentage drop in ranking accuracy on reWordBench compared to RewardBench. We show that Cromes ranking accuracy percentage drop going from RewardBench to reWordBench is the lowest compared to baselines. Ranking Accuracy Percentage Improvements: We measure the percentage drop in response ranking accuracy between RewardBench and reWordBench scores (following the macro-avg metric used in Wu et al. (2025)). rom exhibits smaller ranking accuracy percentage drop from RewardBench to reWordBench (In case of PairPM: 19.78% vs. RRMs 21.54%. See Figure 6 for the results on BT and PairPM settings. Key Takeaway: Assuming sufficient concentration of spurious elements in the prompt as well as the 𝑁 responses, Crome is better at selecting the best response based on causal attributes only. For e.g., in safety, harmful prompts and responses may be spuriously disguised as benign. 11 Robust Reward Modeling via Causal Rubrics Causal Attributes help in detecting jailbreaks For Gemma-2-9B-IT as the solution generation model, BoN with rom shows significant improvements on safety as measured on WildGuardTest. In particular the attack success ratio (ASR) on harmful prompts is much lower compared to models aligned with RM and RRM and this gap increases with N. This improved ASR comes at at similar refusal-to-answer rate on benign prompts. Figure 7 Best-of-N results: ASR reduction on WildGuardTest. Key Takeaway: rom es causal augmentations achieve superior trade-off between safety and over-refusals, because its contrastive pairs delineate the decision boundary for harmful content more faithfully. This leads to safer content, while avoiding excessive refusals on benign prompts. Disentangling Content related features from stylistic (spurious) ones helps in reasoning For Gemma-2-9B-IT as the solution generation model on GSM8K, rom shows consistent gap over baseslines across different values of 𝑁. Non robust reward models may focus on stylistic details. Good looking, detailed but wrong reasoning steps may mis-guide non-robust RMs into giving higher score to the response. Figure 8 Best-of-N Reasoning evaluation on GSM8K. Key Takeaway: Reasoning correctness is dependent on focusing on correctness over stylistic features. Our training ensures Crome is good at capturing content-features over other attributes. 6.3. Neutral Ablations Along with IQN, we tested several methods for enforcing spurious invariance: Causally Aligned Neutrals (CAN). Given preference pair ( 𝐴𝑤, 𝐴ℓ) where ( 𝐴𝑤 𝐴ℓ), we rewrite 𝐴ℓ into 𝐴ℓ such that the causal content of 𝐴ℓ aligns with 𝐴𝑤 (𝐶( 𝐴𝑤) 𝐶( 𝐴𝑙)), but due to the rewrite from 𝐴ℓ, the spurious attributes of 𝐴ℓ remain. By assigning tie-label to this pair during training, we force the model to learn invariance to the spurious differences. While this method is sound theoretically, the approximation of 𝐶( 𝐴𝑤) by 𝐶( 𝐴𝑙) is not perfect. Furthermore, some spurious attributes 𝑆𝑃( 𝐴𝑙) 𝑆𝑃( 𝐴𝑙) vary when we move causal attributes. Invariance to these attributes 𝑆𝑃( 𝐴𝑙) is not captured by CAN. Paraphrase Neutral (PARA). Given an answer 𝐴 to query 𝑄, we rewrite 𝐴 to an approximate 𝐴 using an LLM, such that spurious features vary, but causal features do not. Unlike CAN which provides structured rewrites, PARA is simpler method for rewriting equivalent answers (neutrals). This idea is common in literature (For example, see Wu et al. (2025)). Yet the central issue here is that 𝐶( 𝐴) may inadvertently vary during rewrite (due to the 𝑆𝑃 𝐶 causation in Fig 2). Furthermore, the SP variations introduced through paraphrasing are not reflective of the complex downstream distributions. Other Combinations. We provide two more variations for completeness (i) causal only augmentations, with no neutrals (C) (ii) Both IQN and CAN neutrals sampled equally (IQN+CAN). 12 Robust Reward Modeling via Causal Rubrics Neutrals help in spurious suppression Neutral augmentations significantly improve robustness compared to causal-only training (Figures 10 and 9). All neutral variants outperform the causalonly rom e-C model. Among them, rom eIQN achieves the best overall performance on RewardBench, with gain of +5.4% over the RRM baseline. Meanwhile, rom e-CAN achieves the best performance on reWordBench, with gain of +12.5%. Figure 9 Average performance on RewardBench and reWordBench for Crome trained with different neutral augmentation strategies. Key Takeaway: Explicit suppression of spurious correlates via neutral augmentations mitigates reward hacking by learning invariant reward signals, thereby improving downstream performance. Figure 10 Evaluations of neutral augmentation variants on the different subsets of RewardBench. The rom variants include: rom e-C (only causals), rom e-IQN (causals + irrelevant query neutrals), rom e-PARA (causals + paraphrased neutrals), rom e-CAN (causals + causally-aligned neutrals), and Crome-IQN+CAN (causals + irrelevant query neutrals + causally-aligned neutrals). On the especially challenging Chat-Hard subset, rom e-IQN performs best. See Appendix Section for more details. Prompts for obtaining these neutrals is given in Appendix K. Key Takeaway: combination of well-designed augmentation strategies, e.g. causal upgradations and degradations, along with IQN produces the most robust and generalizable reward models. Discussion on Neutrals: Our Figure 2 suggests that interventions along spurious attributes can confound causal attributes in myriad ways. Firstly, there could be causal attributes, which upon intervention can lead to spurious attribute change (𝐶 𝐴 𝑆𝑃). Secondly, if spurious attributes change, this can lead to change in Causal Attributes (𝑆𝑃 𝐶 𝐴). Due to such confounding factors, an intervention free solution, such as IQN, turns out to be clever way to provide invariance to spuriousness. IQN provides invariance to those spurious factors that change with causal changes (See Fig. 3), as well as natural spurious variations when irrelevant questions are paired with answers corresponding to different question. Ablations and Additional Results: See Appendix Section where we show that Crome exhibits stable and significant improvements in robustness with low variance across different training runs. We also show that using open-weights models as the oracle LLM, such as Gemma-3-27B-IT, rom exhibits significant improvements in robustness. Additionally, we also show performance of Crome and baselines on in-distribution and out-of-distribution examples, showing superior effective robustness achieved by Crom e. 13 Robust Reward Modeling via Causal Rubrics Method Vanilla RM RRM Crome reWordBench RewardBench Average Average Chat Chat-Hard Safety Reasoning 59.97 64. 67.90 80.61 82.53 85.15 98.18 96.93 97. 63.38 72.04 68.75 76.08 73.78 84.80 87. 83.51 91.13 Table 4 RM Performance with Gemma-3-27B-IT as oracle. Results on RewardBench and REwardBench with Gemma-2-9B-IT as base model and Gemma-3-27B-IT as oracle LLM used for attribute extraction and counterfactual augmentations. Results are in PairPM setting. Robustness to Oracle LLM Choice To test our robustness to the choice of oracle LLM, we provide experimental results using Gemma-3-27B-IT to perform attribute extraction and augmentations following which we train rom on the augmented data. Table 4 shows that rom outperforms the baselines by up to 2.5% on RewardBench and 3.2% on reWordBench. In Figure 11, our results indicate an improvement in 18/23 transformations of reWordBench. This shows that our method is performant even with weaker oracle LLM. This potentially indicates that the strength of Crome lies in its causal method, and goes beyond simply leveraging the knowledge of the oracle model. Figure 11 Robustness with Gemma-3-27B-IT as oracle LLM Comparing of RM, RRM and Crome on reWordBench. Here all reward models are Gemma-2-9B-IT based, in the PairPM setting. 7. Discussion, Conclusion and Future Work In this paper, we proposed Crome, causal framework to mitigate reward hacking during the training of reward models. Crome systematically disentangles causal from spurious attributes through two targeted synthetic data augmentation strategies: (1) Causal Augmentations to enforce sensitivity to genuine quality drivers, and (2) Neutral Augmentations to enforce invariance to spurious features. Notably, rom does not assume access to types of spurious attributes that might effect RMs. Across multiple base models and reward modeling techniques (PairPM, BT), rom consistently outperforms strong baselines on the RewardBench benchmark. Furthermore, rom shows superior robustness on the reWordBench benchmark, which specifically tests for vulnerabilities to spurious correlations. We also achieve consistent improvements in downstream Best-of-N setups. Future Work. Our training method, centered on dataset curation, paves the way for new research directions in synthetic data research. compelling application is in synthetic data generation for base model training, where the use and verification of causal attributes could prove particularly fruitful. 14 Robust Reward Modeling via Causal Rubrics 8. Acknowledgments We thank Prateek Jain, Praneeth Nethrapalli, Rishi Saket, Partha Talukdar, Vihari Piratla, Darshan Singh and Saisuresh Krishnakumaran for providing feedback on this work. The authors would like to thank Manish Gupta for his valuable support and guidance."
        },
        {
            "title": "References",
            "content": "M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. L. Chen, C. Zhu, D. Soselia, J. Chen, T. Zhou, T. Goldstein, H. Huang, M. Shoeybi, and B. Catanzaro. Odin: Disentangled reward mitigates hacking in rlhf. arXiv preprint arXiv:2402.07319, 2024. H. Chi, H. Li, W. Yang, F. Liu, L. Lan, X. Ren, T. Liu, and B. Han. Unveiling causal reasoning in large language models: Reality or mirage? Advances in Neural Information Processing Systems, 37:96640 96670, 2024. P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. T. Coste, U. Anwar, R. Kirk, and D. Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023. 15 Robust Reward Modeling via Causal Rubrics G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. C. Denison, M. MacDiarmid, F. Barez, D. Duvenaud, S. Kravec, S. Marks, N. Schiefer, R. Soklaski, A. Tamkin, J. Kaplan, et al. Sycophancy to subterfuge: Investigating reward-tampering in large language models. arXiv preprint arXiv:2406.10162, 2024. J. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. DAmour, D. Dvijotham, A. Fisch, K. Heller, S. Pfohl, D. Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023. K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. A. Feder, N. Oved, U. Shalit, and R. Reichart. Causalm: Causal model explanation through counterfactual language models. Computational Linguistics, 47(2):333386, 2021. A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-Doughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, et al. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. Transactions of the Association for Computational Linguistics, 10:11381158, 2022. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. T. Gupta, S. Shandilya, X. Zhang, R. Madhavan, S. Ghosh, C. Bansal, H. Yao, and S. Rajmohan. Carmo: Dynamic criteria generation for context-aware reward modelling, 2025. URL https://arxiv.org/ abs/2410.21545. S. Han, K. Rao, A. Ettinger, L. Jiang, B. Y. Lin, N. Lambert, Y. Choi, and N. Dziri. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms, 2024. URL https://arxiv. org/abs/2406.18495. J. Hong, N. Lee, and J. Thorne. Orpo: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1117011189, 2024. D. Kaushik, E. Hovy, and Z. C. Lipton. Learning the difference that makes difference with counterfactuallyaugmented data. arXiv preprint arXiv:1909.12434, 2019. M. Khalifa, R. Agarwal, L. Logeswaran, J. Kim, H. Peng, M. Lee, H. Lee, and L. Wang. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. E. Kiciman, R. Ness, A. Sharma, and C. Tan. Causal reasoning and large language models: Opening new frontier for causality. Transactions on Machine Learning Research, 2023. N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. W. Li and Y. Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287, 2024. 16 Robust Reward Modeling via Causal Rubrics T. Liu, W. Xiong, J. Ren, L. Chen, J. Wu, R. Joshi, Y. Gao, J. Shen, Z. Qin, T. Yu, et al. Rrm: Robust reward model training mitigates reward hacking. arXiv preprint arXiv:2409.13156, 2024. Y. Liu, Z. Yao, R. Min, Y. Cao, L. Hou, and J. Li. Pairwise rm: Perform best-of-n sampling with knockout tournament. arXiv preprint arXiv:2501.13007, 2025. S. Long, A. Piché, V. Zantedeschi, T. Schuster, and A. Drouin. Causal discovery with language models as imperfect experts. arXiv preprint arXiv:2307.02390, 2023. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. X. Lou, D. Yan, W. Shen, Y. Yan, J. Xie, and J. Zhang. Uncertainty-aware reward model: Teaching reward models to know what is unknown. arXiv preprint arXiv:2410.00847, 2024. Y. Meng, M. Xia, and D. Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. A. Mishra, G. Nayak, S. Bhattacharya, T. Kumar, A. Shah, and M. Foltin. Llm-guided counterfactual data generation for fairer ai. In Companion Proceedings of the ACM Web Conference 2024, pages 15381545, 2024. S. Negahban, B. Yu, M. J. Wainwright, and P. Ravikumar. unified framework for high-dimensional analysis of 𝑚-estimators with decomposable regularizers. Advances in neural information processing systems, 22, 2009. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. A. Pace, J. Mallinson, E. Malmi, S. Krause, and A. Severyn. West-of-n: Synthetic preference generation for improved reward modeling. arXiv preprint arXiv:2401.12086, 2024. A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models, 2022. URL https://arxiv.org/abs/2201.03544. R. Park, R. Rafailov, S. Ermon, and C. Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024. J. Pearl. Causality. Cambridge university press, 2009. J. Peters, D. Janzing, and B. Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Y. Qiang, S. Nandi, N. Mehrabi, G. V. Steeg, A. Kumar, A. Rumshisky, and A. Galstyan. Prompt perturbation consistency learning for robust language models. arXiv preprint arXiv:2402.15833, 2024. Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, L. Yan, J. Shen, T. Liu, J. Liu, D. Metzler, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023. Robust Reward Modeling via Causal Rubrics R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. A. Ramé, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and J. Ferret. Warm: On the benefits of weight averaged reward models. arXiv preprint arXiv:2401.12187, 2024. S. Ravfogel, A. Svete, V. Snæbjarnarson, and R. Cotterell. Gumbel counterfactual generation from language models, 2025. URL https://arxiv.org/abs/2411.07180. D. Reber, S. Richardson, T. Nief, C. Garbacea, and V. Veitch. Rate: Score reward models with imperfect rewrites of rewrites. arXiv preprint arXiv:2410.11348, 2024. B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612634, 2021. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. J. Shen, R. Xu, Y. Jun, Z. Qin, T. Liu, C. Yang, Y. Liang, S. Baumgartner, and M. Bendersky. Boosting reward model with preference-conditional multi-aspect synthetic data generation. arXiv preprint arXiv:2407.16008, 2024. L. Shen, S. Chen, L. Song, L. Jin, B. Peng, H. Mi, D. Khashabi, and D. Yu. The trickle-down impact of reward (in-) consistency on rlhf. arXiv preprint arXiv:2309.16155, 2023. P. Singhal, T. Goyal, J. Xu, and G. Durrett. long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023. J. Skalse, N. Howe, D. Krasheninnikov, and D. Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 30083021, 2020. G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. R. Tu, C. Ma, and C. Zhang. Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. arXiv preprint arXiv:2301.13819, 2023. L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. C. Wang, Z. Zhao, Y. Jiang, Z. Chen, C. Zhu, Y. Chen, J. Liu, L. Zhang, X. Fan, H. Ma, et al. Beyond reward hacking: Causal rewards for large language model alignment. arXiv preprint arXiv:2501.09620, 2025. 18 Robust Reward Modeling via Causal Rubrics H. Wang, W. Xiong, T. Xie, H. Zhao, and T. Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024. Z. Wu, M. Yasunaga, A. Cohen, Y. Kim, A. Celikyilmaz, and M. Ghazvininejad. rewordbench: Benchmarking and improving the robustness of reward models with transformed inputs. arXiv preprint arXiv:2503.11751, 2025. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. R. Yang, X. Pan, F. Luo, S. Qiu, H. Zhong, D. Yu, and J. Chen. Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment. arXiv preprint arXiv:2402.10207, 2024b. X. Zhang, W. Xiong, L. Chen, T. Zhou, H. Huang, and T. Zhang. From lists to emojis: How format bias affects model alignment. arXiv preprint arXiv:2409.11704, 2024. J. Zhao, R. Liu, K. Zhang, Z. Zhou, J. Gao, D. Li, J. Lyu, Z. Qian, B. Qi, X. Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. X. Zhu, C. Tan, P. Chen, R. Sennrich, Y. Zhang, and H. Hu. Charm: Calibrating reward models with chatbot arena scores. arXiv preprint arXiv:2504.10045, 2025. Robust Reward Modeling via Causal Rubrics"
        },
        {
            "title": "Supplementary Material",
            "content": "These supplementary materials provide additional details, derivations, and experimental results for our paper. The appendix is organized as follows: Section discusses potential limitations of this work. Section provides broader overview of recent related literature. This is an expanded version of the literature covered in the main paper. Section provides some additional set of results. This is an expanded version of the results covered in the main paper. Section provides the detailed steps we took to reproduce the reWordBench benchmark, as proposed in Wu et al. (2025). Section provides detailed overview of our experimental setup. Section provides detailed walk through of how our causal model extends to prior method. We revisit prior works in light of our causal model. It extends on the shorter version provided in Section 3. Section provides walkthrough of the causal details of the core data augmentation strategies. Section provides detailed walk through of the method used to train the reward model. It extends on the shorter version provided in Section 4. Section provides detailed analysis of the theory relating to Reward Hacking and how our proposed method mitigates it. Section presents qualitative example of augmented data created from original data using which is used to train Crome. Section presents lists of prompt templates that we use to query our models for generating the data. Section presents qualitative view common failure modes or biases commonly observed in reward models. A. Limitations and Future Work While rom demonstrates significant improvements, we acknowledge certain limitations which also suggest avenues for future research: Idealized Assumptions in Theoretical Analysis: Our theoretical justification (Section 5, Appendix I) relies on simplifying assumptions such as boolean attributes, quadratic reward models, and perfect counterfactual interventions. These idealizations, necessary for analytical tractability, mean our formal guarantees are indicative of Cromes potential mechanism rather than absolute predictions of real-world performance, where the complexities of LLM behavior and data are greater. Scalability and Cost of Data Augmentation: The generation of targeted causal and neutral augmentations, while effective, involves multiple LLM inference calls per original data point. Although filtering helps optimize the final dataset size, the initial augmentation phase can be computationally intensive and potentially costly for extremely large-scale applications. Future work could explore more sample-efficient augmentation strategies or methods to distill the benefits of augmentation into smaller datasets. 20 Robust Reward Modeling via Causal Rubrics Generalization to Highly Novel Spurious Correlations: Crome is designed to be robust against unspecified spurious correlations by focusing on causal signals and diverse neutral examples. However, its ability to generalize to entirely novel types of spuriousness, drastically different from any patterns implicitly covered or contrasted during augmentation, remains an empirical question. The breadth and nature of the neutral augmentations play role here, and continuous adaptation or more abstract invariance learning might be needed for extreme out-of-distribution spuriousness. Fidelity of LLM-Generated Counterfactuals: The efficacy of Crome is linked to the quality of the LLM-generated counterfactuals. While current LLMs are powerful, ensuring perfect attribute isolation in causal augmentations or complete causal content preservation in neutral pairs is challenging. Imperfections in these LLM-approximated interventions can introduce noise. While our empirical results show strong benefits, further research into enhancing the precision and verifiability of LLM-driven textual counterfactual generation could yield additional improvements. Future research could focus on extending the theoretical framework to encompass more realistic settings, developing more cost-effective and adaptive augmentation techniques, and further exploring the boundaries of generalization against emergent spurious correlations. B. Extended Related Works Our work on Crome, framework for causally robust reward modeling, intersects with and builds upon several key areas of research: the alignment of Large Language Models (LLMs) via human feedback, techniques for reward model training, the persistent challenge of reward hacking, the application of causal inference principles to machine learning, and data augmentation strategies for enhancing model robustness. LLM Alignment and RLHF. The dominant paradigm for steering LLM behavior towards desired attributes like helpfulness, honesty, and harmlessness is Reinforcement Learning from Human Feedback (RLHF) (Askell et al., 2021; Bai et al., 2022a; Christiano et al., 2017; Ouyang et al., 2022; Stiennon et al., 2020). The standard RLHF process involves training reward model (RM) on human preferences (typically pairwise comparisons) and subsequently using this RM as reward signal to fine-tune the LLM policy via RL algorithms such as PPO (Schulman et al., 2017). The quality, calibration, and robustness of the RM are paramount, as flaws in the RM directly impact the alignment outcome (Casper et al., 2023). While alternative alignment algorithms like Direct Preference Optimization (DPO) (Rafailov et al., 2024) and its extensions (e.g., IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024), SimPO (Meng et al., 2024)) bypass explicit RM training by directly optimizing the policy on preference data, they still implicitly rely on the preference information learnable from the data, making the problem of distinguishing true quality from spurious correlates equally relevant. Reward Modeling Techniques. Learning accurate reward models from preference data remains central challenge. Methodologies include Bradley-Terry style pointwise models that learn scalar score 𝑟(𝑥, 𝑦) (Bai et al., 2022a; Bradley and Terry, 1952; Ouyang et al., 2022), and pairwise ranking models that directly predict preference probabilities, often implemented within the LLM architecture itself (PairPM) (Liu et al., 2025; Qin et al., 2023). Other approaches explore Q-function based rewards (Li and Li, 2024) or process supervision (Khalifa et al., 2025). Significant effort focuses on improving specific RM Robust Reward Modeling via Causal Rubrics properties like calibration (Zhao et al., 2023; Zhu et al., 2025), training efficiency (Tunstall et al., 2023), uncertainty quantification (Lou et al., 2024), interpretability through multi-aspect rewards (Wang et al., 2024; Yang et al., 2024b), and scalability via reasoning or chain-of-thought mechanisms (Zhao et al., 2025). Our work complements these efforts by focusing specifically on enhancing the causal robustness of the learned reward function ˆ𝑅 against spurious attributes. Reward Hacking and Spurious Correlations. Learned reward models are notoriously susceptible to reward hacking or over-optimization (Gao et al., 2023; Pan et al., 2022; Skalse et al., 2022). Because RMs are trained on finite, potentially biased data, they often learn to associate high rewards with superficial or spurious features that are merely correlated with desirable responses in the training set. Common examples include excessive length or verbosity (Singhal et al., 2023), specific formatting patterns like lists or markdown (Zhang et al., 2024), adherence to stylistic conventions like politeness, or even sycophantic agreement with user views (Denison et al., 2024). Policies optimized against such RMs learn to exploit these spurious cues, leading to outputs that maximize the predicted reward but fail to align with genuine human preferences or task goals (Shen et al., 2023). Approaches to Mitigating Reward Hacking. Various strategies have been proposed to address reward hacking. Model-centric approaches include using ensembles of RMs to average out idiosyncratic biases (Coste et al., 2023; Eisenstein et al., 2023; Ramé et al., 2024), incorporating explicit calibration methods (Zhao et al., 2023), or designing architectures that factorize reward components, such as ODINs disentanglement of quality and length (Chen et al., 2024). Policy-optimization techniques might involve adding explicit penalties for spurious features (e.g., length penalties (Park et al., 2024)) or using specific regularization methods during fine-tuning. Data-centric approaches aim to improve the training data or process itself. Examples include iterative re-labeling or refinement (Bai et al., 2022b), performing consistency checks across related prompts (Shen et al., 2023), or augmenting the dataset with synthetic examples designed to improve robustness (Pace et al., 2024; Shen et al., 2024). Our work, Crome, falls firmly in this data-centric category. It is closely related to RRM (Liu et al., 2024), which also uses data augmentation (non-contextual and query-independent pairs) for robustness. However, Crome is distinct in its use of an explicit causal framework and its generation of targeted, attribute-specific counterfactuals to disentangle causal from spurious factors. Causal Inference in Machine Learning. Causal inference provides formal tools, such as Structural Causal Models (SCMs) and DAGs (Pearl, 2009; Peters et al., 2017), for reasoning about cause-effect relationships, confounding, and counterfactuals. Applying causal principles in machine learning aims to build models that are more robust, fair, and interpretable by focusing on underlying causal mechanisms rather than potentially brittle statistical correlations (Schölkopf et al., 2021). Techniques like Invariant Risk Minimization (IRM) seek models that perform well across different environments by relying on invariant (presumably causal) predictors (Arjovsky et al., 2019). Our work adopts this causal perspective, framing spurious attributes as non-causal factors whose influence on the learned reward model should be minimized. Causality in LLMs and NLP. The intersection of causality and LLMs is rapidly evolving. Research includes probing the innate causal reasoning abilities of LLMs (Chi et al., 2024; Kiciman et al., 2023), 22 Robust Reward Modeling via Causal Rubrics leveraging LLMs as tools for automating parts of the causal discovery or analysis pipeline (Long et al., 2023; Tu et al., 2023), and applying causal methods to enhance NLP tasks. For instance, counterfactual reasoning and data augmentation have been used to improve robustness against biases in text classification (Feder et al., 2021; Kaushik et al., 2019) and assess fairness (Feder et al., 2022). rom uniquely employs predefined causal graph to structure the generation of counterfactual data specifically for training robust RM, using LLMs as the generation engine. Data Augmentation for Robustness. Data augmentation is cornerstone technique for improving model generalization. Beyond traditional NLP methods like synonym replacement or back-translation (Wu et al., 2025), more recent approaches leverage LLMs for sophisticated augmentations, including paraphrasing, style transfer, generating adversarial examples (Qiang et al., 2024), or creating counterfactuals (Feder et al., 2021; Mishra et al., 2024). Counterfactual generation, often using LLMs as rewriters, is also central to evaluation methods like RATE (Reber et al., 2024), which uses rewrites of rewrites to estimate causal effects robustly. Methods based on sampling, like Gumbel temperature sampling, have also been explored for counterfactual generation (Ravfogel et al., 2025). In the specific context of reward modeling, data augmentation aims to enhance robustness against spurious correlations; examples include the non-contextual and query-independent pairs used by RRM (Liu et al., 2024) or consistency checks via paraphrased inputs as explored in ReWordBench (Wu et al., 2025). Furthermore, generating entirely synthetic preference pairs (Pace et al., 2024; Shen et al., 2024) represents another data-centric approach to improving reward models. Counterfactual data augmentation, particularly generating minimally different pairs to isolate specific features (Kaushik et al., 2019), is highly relevant to disentangling causal factors. Our work, Crome, operationalizes this concept within an explicit causal framework, generating targeted \"causal\" (attribute-isolating) and neutral (spurious-varying) pairs via LLM rewriting to enforce specific invariance and sensitivity properties in the trained RM. Positioning of rom e. rom integrates insights from causal inference and data augmentation to address the critical problem of reward hacking in LLM alignment. While related works like RRM (Liu et al., 2024) use data augmentation for robustness and rom is distinguished by its explicit grounding in causal graph model of answer attributes. It systematically generates attribute-specific counterfactual and neutral examples via guided LLM prompting to directly train the RM to distinguish causal quality drivers (𝐶) from spurious correlates (𝑆𝑃). This allows Crome to potentially handle wider range of spurious attributes beyond commonly studied ones like length, aiming for more principled and generalizable form of robustness. We provide the methodology and empirical validation (Section 6) demonstrating that this causally-informed data augmentation leads to more robust reward models and better downstream policy alignment compared to standard baselines. Robust Reward Modeling via Causal Rubrics C. Additional Results Our main findings presented in this section are as follows: Stable and Significant Performance Gains: rom consistently outperforms baseline reward models (Vanilla RM and RRM) on RewardBench across multiple independent training runs, with small standard deviations indicating stable performance. The improvements, particularly on reWordBench transformations, are substantial and typically exceed multiple standard deviations of the baselines, underscoring their statistical significance (Sec. C.1, C.2). Strong Out-of-Distribution Generalization: rom exhibits strong generalization from indistribution (UltraFeedback validation) to out-of-distribution benchmarks (RewardBench, reWordBench). Notably, it often achieves the highest OOD accuracy (e.g., +7.02% over RRM on reWordBench PairPM) while having similar ID accuracy, suggesting its augmentations teach more generalizable preference representations (Sec. C.3). C.1. Variance in Performance on RewardBench To assess the stability of our findings, we conducted three independent training runs for reward models built upon the Gemma-2-9B-IT base model. Table 5 for PairPM and BT reports the mean accuracy and standard deviation on RewardBench categories. The standard deviations for average RewardBench accuracies are consistently small across all methods (e.g., 0.09 on average for rom e-PairPM, 0.12 on average for RRM-PairPM), indicating stable performance. While there is some variation in specific sub-categories, Cromes average performance advantage over baselines remains robust. Method PairPM BT Average Chat Chat-Hard Safety Reasoning Average Chat Chat-Hard Safety Reasoning Vanilla RM RRM 81.22 0.56 82.54 0.12 Crome 87.84 0.09 97.90 0.48 97.12 0.21 97.54 0.21 63.64 0.28 71.05 0.87 77.48 1.21 74.70 0.98 85.88 1.34 87.27 0.21 79.14 0.68 83.46 0. 72.30 0.39 87.14 0.16 94.39 0.21 85.46 0.27 97.26 0.40 97.21 0.28 96.28 0.32 58.85 1.14 69.15 0.54 65.83 0.81 69.30 3.61 73.13 0.61 91.17 1.17 94.35 0. 84.05 1.10 95.70 0.52 ΔCrome - RRM +5.30 +0.42 +1.25 +12.44 +7.12 +2.00 -0.93 -3.32 +10.92 +1.35 - 9 - 2 - e Table 5 Mean Accuracy and Standard Deviation across 3 different training runs of Gemma-2-9B-IT based Reward Models in both PairPM and Bradley-Terry Reward Model settings. Results on RewardBench. Remark 2. Note that main paper Table 2 has mean of the three training runs considered in these variance experiments. For Gemma-2-2B and Qwen2.5-7B based reward models we only run single training runs. C.2. Variance in Performance on reWordBench For reWordBench, we plot mean performance numbers and error bars showing std. deviation in Figures 12 and 13. Here we depict mean accuracies with error bars representing standard deviations. Across most transformations, the error bars are relatively small, particularly for the average performance over all transformations. The observed improvements of rom compared to RRM and Vanilla RM are substantial and typically exceed multiple standard deviations of the respective models, suggesting that these gains are statistically significant. Robust Reward Modeling via Causal Rubrics Figure 12 Standard deviation error-bars for absolute robustness comparison of RM, RRM and rom in the Bradley-Terry setup, for reward models built over Gemma-2-9B-IT. Mean values and std deviation plotted are for 3 independent training runs. Figure 13 Standard deviation error-bars for absolute robustness comparison of RM, RRM and rom in the PairPM setup, for reward models built over Gemma-2-9B-IT. Mean values and std deviation plotted are for 3 independent training runs. PairPM Model Ultrafeedback (ID) reWordBench Accuracy (OOD) RewardBench Accuracy (OOD) Chat Chat-Hard Safety Reasoning Avg RM RRM Ours 74.55 75.20 74.02 59.97 64.68 72.71 97.90 97.12 97.54 63.64 71.05 72.30 77.48 74.70 87.14 85.88 87.27 94. 81.22 82.54 87.84 Bradley Terry Model Ultrafeedback (ID) reWordBench Accuracy (OOD) RewardBench Accuracy (OOD) Chat Chat-Hard Safety Reasoning Avg RM RRM Ours 74.60 74.75 74.00 61.48 65.69 69. 97.26 97.21 96.28 58.85 69.15 65.83 69.30 73.13 84.05 91.17 94.35 95.70 79.14 83.46 85.46 Table 6 Comparison of In-Distribution (UltraFeedback-Val) and Out-of-Distribution (RewardBench, reWordBench) Accuracy (%) for Gemma-2-9B-IT RMs C.3. Effective Robustness of Crome and Baselines We evaluate the generalization capabilities of the trained reward models by comparing their performance on in-distribution (ID) data (UltraFeedback validation split) against out-of-distribution (OOD) benchmarks 25 Robust Reward Modeling via Causal Rubrics (RewardBench, reWordBench). Table 6 presents these results for models based on Gemma-2-9B-IT. Crome demonstrates strong OOD performance, particularly on reWordBench. For instance, in the PairPM setup, Crome achieves the highest reWordBench accuracy (72.71%), while having similar ID accuracy, suggesting that its learned robustness translates well to challenging, unseen transformations. Similarly, for Bradley Terry models, rom shows the best reWordBench accuracy (69.81%) and similar ID accuracies compared to baselines. Overall, these results indicate that Cromes augmentations effectively teach more generalizable representations of preferences. C.4. Extended Results on Safety Prompts from WildGuardTest To complement the Best-of-N (BoN) safety results in Figure 7 (Sec. 6.2), we provide the complete Attack Success Rate (ASR) on harmful prompts and Refusal to Answer (RTA) on benign prompts in Table 7. We note that lower numbers are better for both ASR as well as RTA. Significantly, the results indicate that without too much regression on RTA (< 0.5% decrease), we show consistent gains in ASR (%) numbers and these gains increase as becomes larger. For instance, at N=32, rom reduces ASR to 39.39%, compared to 42.11% for RM and 41.70% for RRM. In practice, reward models are used to detect jailbreak attacks, and hence our model performance indicates favorable trade-off as the reward model detects harmful content (resisting jail-break attempts) while maintaining utility (low refusal-to-answer rate). RM RRM Ours ASR (%) RTA (%) ASR (%) RTA (%) ASR (%) RTA (%) 2 4 8 16 32.76 36.13 38.49 39.33 42.11 7.39 6.97 6.29 6.27 5.80 32.47 35.88 38.24 39.33 41.70 7.39 7.18 6.10 5.89 6.30 32.18 34.63 36.42 36.71 39.39 7.58 7.46 6.97 6.39 6. Table 7 Comparison of Attack Success Rate (ASR) on harmful prompts and Refusal to Answer (RTA) on benign prompts for rom compared to baselines (RM, RRM) in the Best-of-N setup for varying N. Lower values are considered better for both metrics. C.5. Additional Results on reWordBench We provide additional results on reWordBench in this section. See Figures 14 to 18 for reWordBench results on various base models over which we build our Reward Models, such as Gemma-2-9B-IT, Gemma-2-2B and Qwen2.5-7B, across Bradley-Terry and pairwise-preference Reward Models. D. reWordBench Reproduction The primary motivation reWordBench is the observation that contemporary reward modelskey components of RLHF systemsoften latch onto superficial formatting cues or benign artifacts in their training data, leading to dramatic drops in pairwise-preference accuracy under minor, semantically neutral edits. To diagnose and quantify this brittleness in systematic way, Wu et al. (2025) introduce reWordBench, new benchmark built by applying 28 carefully designed, meaning-preserving transformations to the Robust Reward Modeling via Causal Rubrics Figure 14 Absolute Robustness Comparison of RM, RRM and Crome in the Bradley-Terry RM setup, for reward models built over Gemma-2-2B-IT. Figure 15 Absolute Robustness Comparison of RM, RRM and rom in the PairPM setup, for reward models built over Gemma-2-2B-IT. Figure 16 Absolute Robustness Comparison of RM, RRM and rom in the PairPM setup, for reward models built over Qwen2.5-7B. Figure 17 Absolute Robustness Comparison of RM, RRM and Crome in the Bradley-Terry RM setup, for reward models built over Gemma-2-9B-IT. Robust Reward Modeling via Causal Rubrics Figure 18 Absolute Robustness Comparison of RM, RRM and Crome in the Bradley-Terry RM setup, for reward models built over Qwen2.5-7B. original RewardBench instances. The authors organize these edits into three overarching families each targeting different potential failure modes of reward models. Together, transformations systematically stress-test reward models invariance to innocuous changes, revealing large accuracy drops even under minor edits and motivating the need for robust-training methods. Since the original dataset is not publicly available, on authors suggestion we reconstructed the data independently following the instructions in the original paper. Paraphrasing and back-translation transformations are generated using foundation models or translation tools for which we use OpenAI API, specifically the \"gpt-4o-2024-08-06\" model. For generating back-transcription transformations we use the \"gpt-4o-transcribe\" and \"gpt-4o-mini-tts\" models available on the OpenAI API. Here are some details of the transformations in reWordBench: 1. Controlled Transformations: These are template-based edits that guarantee semantic equivalence by construction. They include: a. Add Quotes: Surrounding the entire prompt and responses with fixed number of quotation marks. b. Punctuation Spaces: Inserting spaces around each punctuation mark. c. Twitter Handle/URL: Appending randomly generated (harmless) Twitter handle or URL to the text. d. StressTest: Repeating semantically vacuous conjunctions (e.g. and true is true or and false is not true) to the end of the text. e. Ignore Above/Below: Injecting the response before or after the prompt with an explicit instruction to ignore it. f. Rot-N Encoding: Applying simple character-shift ciphers (Rot-13 or Rot-2) to the prompt text while leaving responses in plain form. 28 Robust Reward Modeling via Causal Rubrics 2. Naturalistic Transformations: These simulate the kinds of noise and variation that occur in the wild and may not perfectly preserve meaning, but reflect realistic robustness challenges: a. Paraphrase: Rewriting prompt and response via strong LLM (Llama-3-70B-instruct) under paraphrasing instruction. b. Back-translation: Translating English Spanish English for several rounds using OPUS-MT, accepting only those with high semantic similarity. c. Back-transcription: Converting text to audio and back using TTS model (fairseq S2) and an ASR model (Whisper-base). d. Homoglyph Substitution: Replacing Latin characters with visually identical Unicode glyphs (e.g. Cyrillic for Latin e). e. Character-level Edits: Randomly swapping, inserting, deleting, or substituting characters at rates reflecting real-world typos (including QWERTY-adjacent substitutions). f. Word Deletion: Omitting randomly chosen word from prompt and response, subject to similarity filter. 3. Domain-Targeted Transformations: These focus on specialized subsets of RewardBenchcode, mathematics, and safety promptswhere specific artifacts may bias reward models: a. Code Minification: Automatically renaming variables, removing whitespace, and otherwise minifying Python snippets without changing functionality. b. Add Comment: Inserting # bad annotations after each line of chosen responses (and optionally # good after rejected ones). c. Append Other Code: Concatenating the losing snippet after the winning one (and vice versa), taking advantage of Pythons return-ended semantics. d. Swap Format: Exchanging the usual answer formats (e.g. LaTeX vs. markdown # Answer) in arithmetic problems. e. Jailbreak Prompts: Prepending known jailbreak instructions (from the ChatGPT-JailbreakPrompts dataset) to safety-critical queries to see if the RM prefers harmful completions. Robust Reward Modeling via Causal Rubrics E. Experimental Setup Details This appendix provides supplementary details to the experimental settings outlined in Section 6.1 of the main paper. E.1. Best-of-N Experimental Methodology Algorithm 1 Best-of-𝑁 Selection with Pairwise Preference Model 1: Input: Query 𝑄; responses = ( 𝐴1, . . . , 𝐴𝑁) with 𝑁 1 2: Input: Pairwise model ˆR𝜃 : (𝑄, 𝐴𝑖, 𝐴 𝑗) {1, 2} The output {1, 2} from the Pairwise preference model indicates if the first answer is better or the second, given the query. 3: Output: Selected best response 𝐴best 4: 𝐴best 𝐴1 5: for 𝑖 2 to 𝑁 do 𝐴cand 𝐴𝑖 6: if ˆR𝜃(𝑄, 𝐴best, 𝐴cand) = 2 then 7: 8: 𝐴best 𝐴cand 9: end if 10: end for 11: return 𝐴best For all our Best-of-N results using PairPM models, we follow simple procedure to find the best response out of 𝑁 responses generated by base LLM. In particular, PairPM models take responses 2 at time, and provide the better response for the given query. Given 𝑁 response = ( 𝐴1, . . . , 𝐴𝑁) with 𝑁 1, in randomly shuffled order, we sequentially compare responses 2 at time (starting from 𝐴1 and 𝐴2) using the PairPM reward model and keep track of the best response. At each iteration, the best response is compared to the next response in the list and the best response is updated. The best response after 𝑁 1 iterations is taken as the selected response. The algorithm for this procedure is given in Algorithm 1. E.2. Experimental setting for Calculating Win Rates on RewardBench Prompts To show the performance of rom on general purpose datasets, we follow reWordBench (Wu et al., 2025) and use all 2985 prompts from RewardBench (Lambert et al., 2024). We use Gemma-2-9B-IT as the base model and sample responses for each prompt in this set. Following this, we use the PairPM reward models (RM, RRM and Crome) to select the best response among the responses, as described in supplementary Section E.1. We use GPT-4 as judge to compare Cromes responses with baselines RM and RRM. E.3. WildGuardTest and GSM8K experimental settings For both WildGuardTest results (main paper Figure 7 as well as supplementary Table 7), as well as GSM8K results (main paper Figure 8), we use Gemma-2-9B-IT as the base model and sample responses from it. Following this, we use the PairPM reward models (RM, RRM and Crome) to select the best response 30 Robust Reward Modeling via Causal Rubrics among the responses, as described in supplementary Section E.1. For WildGuarTest, for obtaining results given the final responses, we use the WildGuard model Han et al. (2024) to obtain annotations for prompt-harmfulness, response-harmfulness, response-refusal, is-parsing-error, as described in the WildGuard repository3. Using these annotations, we obtain ASR and RTA for rom and baselines. E.4. Datasets and Augmentation For human preference data (Dpref) we use Ultrafeedback (Cui et al., 2023), which furnishes approximately 60,000 preference pairs across diverse domains. The data augmentation process, central to Crome (Section 4), employs Gemini 2.0 Flash. This LLM is first used to identify ℓ = 5 principal causal attributes relevant to response quality. Subsequently, Gemini 2.0 Flash generates (a) causal upgrade/degradation pairs targeting these attributes (Dcausal), and (b) neutral pairs (Dneutral). The raw augmented data, Daug, undergoes filtering step. This involves applying model-based confidence filter, using baseline RM (trained solely on Dpref) with threshold of 𝜏 = 0.2. This filtering focuses the training on more informative examples. The amplification process involves initially generating approximately 10x data from causal augmentations (5 attributes, 2 versions per original response) and 1x data from neutral augmentations, followed by verification and the confidence-based filtering. The final training dataset = Dpref Daug_filtered typically contains about 3.5 times the number of examples in the original Dpref, similar to RRM (Liu et al., 2024). E.5. Models and Training Reward Models (RMs): We instantiate RMs using Qwen2.5-7B (Yang et al., 2024a) and Gemma-2-9B-IT, Gemma-2-2B (Team et al., 2024) as base transformer architectures. Our RM variant, Crom e-PairPM, processes inputs formatted as Q, A, and predicts preference token (A or B) via cross-entropy loss. An alternative variant, Crome-BT, implements the Bradley-Terry model by deriving scalar scores for each answer. Policy Models: For downstream alignment tasks, we use the Best-of-N setup where we generate responses using Gemma-2-9B-IT and use rom as well as baseline reward models to select the best candidate response. Training Hyperparameters: All models are trained in PyTorch with the Hugging Face Transformers library. For RM training, following Liu et al. (2024), we use the AdamW optimizer (Loshchilov and Hutter, 2017) for 1 epoch, with learning rate of 1𝑒6, global batch size of 256, and cosine learning rate schedule. We use warmup ratio of 0.03. For training all models, we use 8 NVIDIA A100 80GB GPUs. RM training runs require time between 10-16 hours for 2B to 9B mdoels we consider. 3https://github.com/allenai/wildguard 31 Robust Reward Modeling via Causal Rubrics E.6. Baselines and Evaluation Baselines: Our full Crome approach is compared against two primary baselines: 1. Base RM, trained solely on the original Dpref. 2. The RRM Baseline (Liu et al., 2024), which employs distinct augmentation strategy using noncontextual examples and responses from different queries, not specifically aligned with identified causal or spurious attributes. Evaluation Benchmarks: RM quality is assessed by accuracy on RewardBench (Lambert et al., 2024) (overall and per category: Chat, Chat-Hard, Safety, Reasoning) and robustness on Re-word Bench (Wu et al., 2025). BoN Policy performance is evaluated using RewardBench, WildGuardTest (Han et al., 2024), GSM8K (Cobbe et al., 2021). F. Causal Model and Augmentation Details This appendix provides further details on the causal framework underpinning rom and discusses various data augmentation strategies in the context of robust reward modeling. F.1. Elaboration on the Causal Model The causal graph presented in Figure 2 (Section 3.2) models the generation of an answer and the formation of its attributes. The query influences the generators latent intent I. This intent, along with unobserved generator-specific confounders (e.g., inherent stylistic preferences, verbosity tendencies, pre-existing biases), leads to the textual answer A. The answer then manifests both causal attributes C(A) (e.g., factuality, relevance) and spurious attributes SP(A) (e.g., length, specific formatting, politeness). The true, idealized reward is assumed to be function only of and C(A). The challenge in training reward model ˆR𝜃 arises because SP(A) can become correlated with in the training data. This correlation can occur if influences both the choice of spurious features and the aspects that contribute to causal quality, or simply because certain spurious features happen to co-occur with preferred answers in Dpref. Without explicit guidance, ˆR𝜃 may learn to rely on these spurious correlations, leading to reward hacking. rom es data augmentation strategy aims to provide this explicit guidance by generating new answer pairs that help ˆR𝜃 disentangle C(A) from SP(A). F.2. Crom es Causal Augmentation: Attribute Isolation rom es primary strategy for enhancing sensitivity to causal attributes involves Attribute Upgradation/Degradation. This generates pairs ( A(𝐶 𝑗upgraded/degraded) , A) or (A, A(𝐶 𝑗upgraded/degraded) ) by prompting an LLM to modify an original answer (from Dpref) along single causal attribute 𝐶 𝑗 while attempting to keep other attributes constant. This provides targeted signal about the marginal contribution of 𝐶 𝑗. F.2.1. Comparison with Relevance Contrast Augmentation An alternative strategy, Relevance Contrast Augmentation (used in RRM-style approaches (Liu et al., 2024), termed non-contextuals therein), involves pairing relevant answer A1 (for query Q) with an irrelevant 32 Robust Reward Modeling via Causal Rubrics answer B2 (e.g., an answer to different query, so 𝐶(𝐵2 Q) 0), labeled A1 B2. While Relevance Contrast establishes baseline understanding of relevance, Cromes Attribute Isolation offers: Specificity and Nuance: It directly teaches about individual causal attributes (𝐶 𝑗), enabling the RM to learn compositional understanding of quality and distinguish between relevant answers differing subtly in one dimension. Data Efficiency for Complex Attributes: Focusing changes along one attribute creates diverse, targeted examples for each quality facet. rom es attribute-specific counterfactuals thus provide richer, more disentangled signal than broad relevance contrasts alone. F.3. Neutral Augmentation Strategies Neutral augmentations aim to make the reward model invariant to spurious attributes when causal content is held constant or is irrelevant. F.3.1. Common Spurious Perturbation Methods (Not primary Crome strategy) Several methods focus on general spurious perturbations: 1. Direct Spurious Feature Perturbation (e.g., Paraphrasing, Formatting Changes): This involves taking an answer and generating A(𝑆𝑃𝑠𝑝 ) by applying meaning-preserving transformations (e.g., paraphrasing) intended to alter only SP(A) while preserving C(A). The pair (A, A(𝑆𝑃𝑠𝑝 ) ) is labeled as tie. This is central to benchmarks like reWordBench (Wu et al., 2025). 2. Rewrites of Rewrites (e.g., RATE (Reber et al., 2024)): RATE uses sequential rewrites for robust causal effect estimation. Adapted for augmentation, multiple causally-equivalent rewrites of an answer could form neutral pairs. Challenges with these General Methods: Unknown/Unspecified Spurious Features: Its hard to priori identify and target all spurious features an RM might exploit. Preserving Causal Content: Ensuring \"spurious\" perturbations dont inadvertently alter causal meaning is difficult. F.3.2. Neutral Augmentation Strategies developed in this work We developed the following two strategies for neutral augmentation. 1. Irrelevant Query Neutrals (IQN): rom generates these neutral pairs efficiently by leveraging its existing pool of answers (original or causally augmented). Given two answers, B1 and B2, that were 33 Robust Reward Modeling via Causal Rubrics generated or selected for specific query Qorig, Crome creates neutral pair by associating them with new, unrelated query Qirrelevant. For this Qirrelevant, both B1 and B2 are now contextually irrelevant; their causal attribute scores 𝐶(𝐵1Qirrelevant) and 𝐶(𝐵2Qirrelevant) are effectively zero (or very low). Despite potentially different spurious attributes 𝑆𝑃(𝐵1) and 𝑆𝑃(𝐵2), the pair (B1, B2) is presented to the reward model with query Qirrelevant and labeled as tie. This teaches the RM that when answers are equally and maximally irrelevant to the current query, their differing spurious features should not induce preference. 2. Causally-Aligned Neutrals (CAN): This method directly leverages the original preference pairs or the outputs of causal augmentation. Given an original preference pair from Dpref, say (A1, A2) where A1 A2, we generate A(𝐶𝐶 ( 𝐴1 ) ) by rewriting A2 to match the causal attribute profile of A1, while instructing the LLM to retain the spurious characteristics 𝑆𝑃( 𝐴2) of the original A2. The pair (A1, A(𝐶𝐶 ( 𝐴1 ) ) ) is then labeled as tie. symmetric pair can also be generated. 2 2 Similarly, if we have an answer and its causally degraded version A(𝐶 𝑗degraded) (from Dcausal), we can attempt to reconstruct the degraded version by prompting an LLM to restore 𝐶 𝑗 to its state in A, while aiming to preserve the spurious features of A(𝐶 𝑗degraded) . If successful, this reconstructed version, reconstr, would form neutral pair (A, reconstr) labeled as tie. The core idea is to teach invariance to the spurious differences that remain after causal attributes have been aligned or restored. Moreover, applying CAN to counterfactually generated data from Dcausal helps mitigate imperfections in oracle rewritesan issue highlighted in the RATE paper (Reber et al., 2024), which notes that LLM edits often unintentionally modify \"off-target attributes\" (e.g., introducing formality, removing HTML tags). CAN thereby enhances robustness on two fronts: (1) disentangling spurious correlations in original data, and (2) neutralizing new biases introduced during causal augmentation. This helps in enhancing models robustness against confounding signals in the data. While this method is sound theoretically, we qualitatively find that the approximation of 𝐶( 𝐴𝑤) by 𝐶( 𝐴𝑙) is not perfect. Furthermore, some spurious attributes 𝑆𝑃( 𝐴𝑙) 𝑆𝑃( 𝐴𝑙) vary when we move causal attributes. Invariance to these attributes 𝑆𝑃( 𝐴𝑙) is not captured by CAN. For these reasons, we encourage future work for improving this neutral augmentation strategy. G. Detailed Mechanistic View of Augmentation Strategies This appendix section provides more granular, node-based representation (Figure 19) to elaborate on the hypothesized attribute interactions and the counterfactual generation process. This detailed view aims to offer causal understanding that complements the main paper. Figure 19 aims to provide deeper, causal understanding of the causal perturbation process through which we obtain our causal upgradations and degradations. We term the spurious attributes that move when causal attributes are intervened upon as 𝑆𝑃2( 𝐴) 𝑆𝑃( 𝐴) for any answer 𝐴. Part 1: Causal Augmentation (Attribute Upgradation/Degradation). We first generate counterfactual Answer 2 from an original Answer 1 (for query Q) via an LLM-driven \"Counterfactual Generation Process.\" This process intervenes to modify specific causal attribute 𝐶 𝑗 within Answer 1s causal profile 34 Robust Reward Modeling via Causal Rubrics Figure 19 Detailed mechanistic diagram of Cromes Causal Attribute Upgradation and Degradation, illustrating attribute components and transformations. This causal diagram indicates that on changing causals some spurious features also can get dragged along (we call these 𝑆𝑃2). Hence separating these is very hard. This illustrates the need for neutral augmentation strategy that provides invariance to 𝑆𝑃2 attributes. C(A1) to target state 𝐶, resulting in C(A2). We aim to keep spurious attributes fixed by asking for minimal perturbation. Therefore attributes SP1(A1) are ideally preserved. Yet, SP2(A1) (which may co-vary with C(A1)) might transition to SP2(A2) SP2(A1). The goals of this transformation are to ensure 𝐴2 reflects the intended causal change. The RM is then trained on the pair ( 𝐴1, 𝐴2) with preference label reflecting the upgrade/degradation, teaching sensitivity to isolated causal attribute modifications. Part 2: Neutral Augmentation (via Irrelevant Query). As illustrated in Figure 19, we need spurious invariance to 𝑆𝑃2 which are hard to disentangle as well. This illustrates the need for an intervention free method for neutral augmentation like IQN. When we present an answer pair (𝐴1, 𝐴2) from Dpref Dcausal, re-contextualized with new, unrelated query Qirrelevant, we teach the model invariance to (𝑆𝑃1, 𝑆𝑃2). This is because, the primary differences between 𝐴1 and 𝐴2 in this new context are their spurious attributes (SP1,SP2). Note that the causal difference between 𝐴1 and 𝐴2 in Dpref Dcausal in presence of irrelevant query is now spurious, and hence there need not be any sensitivity to it. H. Detailed Crome Methodology This appendix provides the detailed implementation steps for the rom framework introduced in Section 4, covering attribute identification, counterfactual data generation, filtering, and the specific training objective. 35 Robust Reward Modeling via Causal Rubrics H.1. Step 1: Attribute Identification The foundation involves identifying the attributes that genuinely determine answer quality versus those merely correlated with it, as defined in Section 3.2. For query and example answers (y𝑤, y𝑙) from Dpref, we define: Causal attributes = (C1, . . . , Cℓ) (e.g., factuality) and Spurious attributes SP = (SP1, . . . , SP𝑘) (e.g., verbosity). Automated Attribute Extraction. We employ an LLM prompted with and example responses (see Appendix for prompt). The primary output is the set of attributes C. Refinement and Verification. The LLM-generated list is reviewed for coherence and consistency in this verification phase. The verification prompts are provided in Appendix K. H.2. Step 2: Generating Counterfactual Augmented Data Using identified attributes C, we generate Daug via LLM-approximated counterfactuals (Section 3.3). Causal Augmentation (Dcausal). Pairs (A, A) are generated to differ primarily along single causal attribute 𝑗. We use LLM prompts (Appendix K) for upgradation (generating an improved from ground-truth rejected answer A) and degradation (generating degraded from ground-truth selected answer A), aiming to keep other attributes constant. Pairs are labeled accordingly. Neutral Augmentation (Dneutral). Notice that when we causally augment an answer in Dcausal, we might in-advertantly move spurious correlates (as illustrated in Figure 3). Furthermore, even in our dataset, there could be systematic effect where spurious attributes highly correlate with the better (or worse) answer. In such cases, we need to create dataset of equivalent pairs, with tie label to teach the model invariance to spurious correlates. Our primary technique is irrelevant query neutrals (IQN). Here, the idea is that given new query, the causal attribute becomes irrelevant. Essentially, for the new query, the causal attributes are spurious. Hence, by taking any two answers for given query, and labeling them tie, given an irrelevant query, the reward model learns invariance to these features. For example, if the reward model has spuriously learnt that bullet points in an answer should be rewarded, our tie labels teach them that bullet points should be rewarded only if the content of the answer is relevant to the query. Similarly, by creating such pairs with our own causally augmented data in Dcausal, we teach the model invariance to the spurious pairs that move when the causal attributes (CA) are perturbed. H.3. Step 3: Filtering Augmented Data The raw Daug is then filtered to Daug_filtered. Model-based Confidence Filtering. Using baseline ˆRbase, we calculate 𝑝 = Pbase(B A) for each augmented pair (A, B) with target label 𝑦. We retain the pair only if 𝑝 𝕀( 𝑦 = A) 0.5 𝕀( 𝑦 = tie) > 𝜏. 36 Robust Reward Modeling via Causal Rubrics We use threshold 𝜏 = 0.2, focusing training on examples where the baseline is uncertain or incorrect (Liu et al., 2024). Quality Verification. Further checks (e.g., automated fluency scoring) verify pair validity. The result is Daug_filtered. H.4. Step 4: Training the Robust Reward Model The final model ˆR𝜃 is trained on = Dpref Daug_filtered by minimizing the composite loss: (𝜃) = log 𝜎( ˆR𝜃(Q, y𝑤) ˆR𝜃(Q, y𝑙)) (Q,y𝑤,y𝑙 ) Dpref Dcausal 𝜆 (Q,A1,A2,𝑦=tie) Dneutral Ltie(𝜃; Q, A1, A2) (3) where Ltie is defined as in Eq. 2. The hyperparameter 𝜆 0 weights the neutral tie loss and is tuned on validation set (Section 6). I. Theoretical Analysis In this section, we provide formal justification for why the Crome training framework, specifically the composite loss function operating on causally augmented data, mitigates spurious reward hacking. We demonstrate that the optimization objective inherently discourages the reward model from relying on spurious correlations, guiding it towards the true causal drivers of quality. I.1. Formal Setup We adopt the notation and causal framework established in Section 3. Our analysis considers query Q, an answer with corresponding Principal Causal Components C(A) and spurious attributes SP(A). The idealized ground-truth reward is R(Q, A) = 𝑓 (Q, C(A)), and the learned reward model is denoted ˆR𝜃(Q, A). The model parameters 𝜃 are optimized by minimizing the composite loss function (𝜃) = Lpref (𝜃) + 𝜆Ltie(𝜃) (Eq. 2) over the training dataset = Dpref Daug_filtered, which combines original preferences Dpref with filtered causal Dcausal and neutral Dneutral augmentations. For theoretical analysis, Lpref and Ltie represent expectations over the respective data distributions: Lpref (𝜃) = 𝔼(Q,y𝑤,y𝑙 )Dpref Dcausal Ltie(𝜃) = 𝔼(Q,A1,A2,𝑦=tie)Dneutral where Δ12 = ˆR𝜃(Q, A1) ˆR𝜃(Q, A2). (cid:104)log 𝜎( ˆR𝜃(Q, y𝑤) ˆR𝜃(Q, y𝑙)) (cid:20) (cid:105) (log 𝜎(Δ12) + log 𝜎(Δ12)) 1 2 I.2. Justification under the Boolean variable causal model for attributes Assumption 1. Assume that: (cid:21) Robust Reward Modeling via Causal Rubrics 1. Causal attributes {𝐶𝑖 (𝑄, 𝐴)}𝑘 and spurious attributes {𝑆 𝑗 ( 𝐴)}ℓ 𝑗=1 are all boolean variables taking 𝑖= values in {+1, 1} 2. All spurious variables are non-descendants of all causal variables. 3. Reward function is trying to fit quadratic polynomial in causal and spurious attributes, i.e. ˆ𝑅 = 𝑖 𝛼𝑖𝐶𝑖 (𝑄, 𝐴) + 𝑗 𝛽 𝑗𝑆 𝑗 ( 𝐴) + 𝑖𝑖 𝛼𝑖,𝑖𝐶𝑖 (𝑄, 𝐴)𝐶𝑖 (𝑄, 𝐴)+ 𝑗 𝑗 𝛽 𝑗, 𝑗𝑆 𝑗 ( 𝐴)𝑆 𝑗 ( 𝐴) + 𝛾𝑖, 𝑗𝐶𝑖 (𝑄, 𝐴)𝑆 𝑗 ( 𝐴). 𝑖 𝑗 (4) 4. Assume that the true reward function is sparse quadratic polynomial depend on only the causal attributes. 𝑅 = 𝜃𝑖𝐶𝑖 (𝑄, 𝐴) + 𝑖 𝑖𝑖 𝜃𝑖,𝑖𝐶𝑖 (𝑄, 𝐴)𝐶𝑖 (𝑄, 𝐴) (5) Here, 𝜃0 𝑠 << 𝑘2 and 𝜃𝑖 and 𝜃𝑖,𝑖 variables form the vector 𝜃. All other coefficients for other features that involves the spurious variables are set to 0 in 𝜃. Let be the support set of the true coefficient. From the reward modeling objective, we try to fit model Δ( ˆ𝑅) to target which is the difference between true rewards to two answers 𝐴1 and 𝐴2 for the same question, i.e. 𝑅(𝑄, 𝐴1) 𝑅(𝑄, 𝐴2). From the assumption in 4, this is equivalent to fitting linear model with coefficients 𝛼𝑖, 𝛼𝑖,𝑖 , 𝛽 𝑗, 𝛽 𝑗, 𝑗 , 𝛾𝑖, 𝑗 and differences in features (across the two answers), i.e. 𝐶𝑖 (𝑄, 𝐴1) 𝐶𝑖 (𝑄, 𝐴2), 𝑆 𝑗 ( 𝐴1) 𝑆 𝑗 ( 𝐴2), 𝑆 𝑗 ( 𝐴1)𝑆 𝑗 ( 𝐴1) 𝑆 𝑗 ( 𝐴2)𝑆 𝑗 ( 𝐴2), 𝐶𝑖 (𝑄, 𝐴1)𝐶𝑖 (𝑄, 𝐴1) 𝐶𝑖 (𝑄, 𝐴2)𝐶𝑖 (𝑄, 𝐴2), 𝐶𝑖 (𝑄, 𝐴1)𝑆 𝑗 ( 𝐴1) 𝐶𝑖 (𝑄, 𝐴2)𝑆 𝑗 ( 𝐴2) respectively. To simplify notation, we drop the reference to 𝐴1, 𝐴2 and 𝑄 and call 𝐶𝑖 (𝑄, 𝐴1) 𝐶𝑖 (𝑄, 𝐴2) as Δ𝐶𝑖. Similarly, we use Δ𝑆 𝑗, Δ𝐶𝑖,𝑖 , Δ𝑆 𝑗, 𝑗 and Δ(𝐶𝑖𝑆 𝑗). The dependence of these features on the 𝐴1, 𝐴2 and 𝑄 are understood. Let 𝐹𝑞,𝑎1,𝑎2 {+1, 1}𝑘+ℓ+𝑘ℓ+( 𝑘 {Δ𝐶𝑖}, {Δ𝑆 𝑗}, {Δ𝐶𝑖,𝑖 }, {Δ𝑆 𝑗, 𝑗 }, {Δ(𝐶𝑖𝑆 𝑗)} stacked row wise for the triplet 𝑞, 𝑎1, 𝑎2. 2) be the boolean vector with features 2)+( ℓ Consider two types of triplets, one drawn from the natural distribution of the preference training dataset 𝐷pref and the others drawn from augmented distribution 𝐷aug. Let us assume for the sake of the theoretical results to follow, that we upgrade/degrade answer 𝑎2 to 𝑎𝑎𝑢𝑔 by changing only one causal factor at time while all the other causal factors are fixed to their factual version and all things remaining the same to form 𝐷aug. The degradation aspect only serves to reinforce the phenomenon we seek to show formally below. 1 Assumption 2. (Model for Counterfactual Generation) We assume that: 1. 𝑎𝑎𝑢𝑔 is formed by generating 𝐶𝑖 (𝑄, 𝐴) and 𝑆 𝑗 ( 𝐴) following an counterfactual generation where the 1 following set of intervention is made 𝐶𝑖 (𝑄, 𝐴) 𝐶𝑖 (𝑄, 𝐴), 𝐶 𝑗 (𝑄, 𝐴) 𝐶 𝑗 (𝑄, 𝐴), 𝑗 𝑖 which propagates to potential descendants of variable 𝐶𝑖 and not affecting 𝑆 𝑗 (due to no 𝑆 𝑗 being descendant of 𝐶 𝑗) with all other factors remaining as in answer 𝑎2. 2. Let us assume that we have 𝑚 augmentations where triplet is randomly sampled from the training preference data distribution Dpref and then augmented using the above counterfactual with randomly chosen causal attribute negated. 38 Robust Reward Modeling via Causal Rubrics Remark There are the main assumptions - 1) 𝑆 𝑗 being non-descendant of 𝐶𝑖, 2) Reward model is quadratic sparse boolean model (The treatment could be extended to boolean polynomials of higher degree too with lot more algebraic technical work). Theorem 2. Let the feature matrix of the counterfactually augmented triplets, that is formed by stacking row wise, be denoted F. Consider the following ℓ1 constrained regression problem: feature vectors 𝐹𝑞,𝑎aug ,𝑎2 1 ˆ𝜃 = arg min b1 𝑠.𝑡.F𝑏 = Δ𝑅 (6) Here, Δ𝑅 is vector of the difference in the true reward between the reward applied to the augmented answer and the non-augmented one across augmented triplets. Let be the top 𝑐2𝑘 non zero entries of vector by magnitude. Then, we have: Δ𝜃2 = 𝜃 ˆ𝜃2 𝑐3𝜃IN 1 (cid:18) 8 log(𝑘+ℓ) 𝑚 4 𝑘 + (cid:19) w.h.p. Remark: If the true sparsity 𝑠 < 𝑐2𝑘, then it ensures perfect recovery since = . Since 𝑠 < 𝑘2, and if every coefficient is 𝑂(1), the bound becomes 𝑂(𝑘) which is independent of the spurious dimension. Proof. Under the model assumptions 1 and assumptions on counterfactual generation 2, we seek to show that when restricted to feature set Δ𝐶𝑖, Δ𝐶𝑖,𝑖 , Δ𝐶𝑖𝑆 𝑗 has smaller incoherence (by multiplicative factor of 𝑘) than an feature matrix made of i.i.d triplets sampled from the preference distribution. This accommodates recovering the 𝑠 = 𝑂(𝑘) sparse solutions exactly and in the general case, the error in coefficient estimation is 𝑂(𝑘) independent of spurious dimension ℓ. First, we show that features Δ(𝑆 𝑗, 𝑗) = 0, Δ(𝑆 𝑗) = 0 for the augmented triplets. This is because all 𝑆 𝑗 variables are ancestors to 𝐶𝑖 variables. Therefore, counterfactual intervention on the answer 𝑎2 leaves the two spurious attribute sets (for the original and its counterfactual) unchanged. Intervention fixed all causal variables to the factual ones (but fixed through intervention) and intervenes on variable to change. There are many types of correlation between non zero features because of this. We consider them one by one: 1) Δ𝐶𝑖 = 0 if is 𝐶𝑖 is not intervened. This occurs with probability 1 1/𝑘. 2) Δ𝐶𝑖Δ𝐶 𝑗 = 0 with probability 1 2/𝑘. 3) Δ𝐶𝑖,𝑖 Δ𝐶 𝑗, 𝑗 = 0 if all 𝑖, 𝑖, 𝑗, 𝑗 are distinct indices. 4) Δ𝐶𝑖, 𝑗Δ𝐶 𝑗,𝑘 = 0, with probability 1 1/𝑘. 5) Δ𝐶𝑖, 𝑗Δ𝐶𝑖𝑆 𝑗 = 0 with probability 1 1/𝑘. 6) Δ𝐶𝑖,𝑖 Δ𝐶 𝑗𝑆𝑘 = 0 always if all four indices not equal. 7) Δ𝐶𝑖Δ𝐶 𝑗𝑆𝑘 = 0 always. 8) Δ𝐶𝑖Δ𝐶𝑖𝑆𝑘 = 0 with probability 1-1/k. If any of the these products is non zero, conditioned on that event, they equal the correlation on the preference training dataset (every correlation between features is bounded by at most 4). Therefore, expected pairwise correlation amongst two features for randomly chosen augmented triple is at most 4/𝑘. Given every augmented triple is obtained by counterfactual generation applied to an i.i.d sample from preference dataset, there is deviation of at most 8 log(𝑘+ℓ) with probability 1 1 𝑚 (𝑘+𝑙) 4 . Therefore, 1 𝑚 F𝑇 4 𝑘 + 8 log(𝑘 + ℓ) 𝑚 𝑤. 𝑝. 1 (𝑘 + ℓ) 4 (7) 39 Robust Reward Modeling via Causal Rubrics This means that the data matrix is incoherent with high probability. We now follow standard Lasso analysis. Recall the cone condition Negahban et al. (2009): For subset of indices that have non zero values in 𝜃, Δ𝜃𝑁 𝑐 1 Δ𝜃𝑁 1 + 2𝜃𝑁 𝑐 1. This implies: Δ𝜃1 2𝑁 Δ𝜃2 + 2𝜃𝑁 𝑐 1. We have the following chain: 0 = 1 𝑚 𝐴Δ𝜃2 Δ𝜃 1 𝑚 2 Δ𝜃2 1 (cid:16)8N Δ𝜃2 2 F𝑇 2 8𝜃N𝑐 2 1 Δ𝜃 (8) (9) (cid:17) 1 𝑚 F𝑇 Let us set 𝑚 such that 1 𝑚 < 8 log(𝑘+ℓ) 𝑘 parameters in 9, we get: Δ𝜃2 = 𝜃 ˆ𝜃2 4 J. Qualitative Example Walkthrough . Let 𝑁 < 𝑘/80, 𝑐2 < 1/80 in the theorem. Substituting these 2𝜃IN 1 (cid:18) 8 log(𝑘+ℓ) 𝑚 4 𝑘 + (cid:19) In this section, we show qualitative examples of augmented data created from original data using which is used to train Crome. User Prompt Question: Can you provide table showing two potential causes for global warming? Chosen Response Sure, here is table showing two potential causes for global warming: Potential Causes Greenhouse Gas Emissions Land Use Changes Description Release of gases such as carbon dioxide, methane, and nitrous oxide from human activities like burning fossil fuels, deforestation, and agriculture. These gases trap heat in the atmosphere, causing the planet to warm. Alteration of the earths surface for human activities like urbanization, agriculture, and forestry can result in changes in albedo, water cycling, and energy balance, leading to warming effects. For example, when forests are cleared, the land reflects less sunlight back into space, which leads to more warming. It is important to note that global warming is complex issue with multiple causes. The two potential causes listed above are just few of the many factors that contribute to climate change. 40 Robust Reward Modeling via Causal Rubrics Rejected Response Potential Causes Greenhouse Gas Emissions Land Use Changes Description Release of gases such as carbon dioxide, methane, and nitrous oxide from human activities like burning fossil fuels, deforestation, and agriculture. Alteration of the earths surface for human activities like urbanization, agriculture, and forestry can result in changes in albedo, water cycling, and energy balance, leading to warming effects. See Table 1 for additional information. Attributes and their Relative Importance Scores Accuracy 0.3 Completeness 0.25 Clarity 0.2 Format 0.15 Conciseness 0. 41 Robust Reward Modeling via Causal Rubrics Causal Attributes with Elements: 1 { \" causal_elements \" : 2 \" Accuracy \" : [ 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 { } , { } , { } , { } , { } \" element \" : \" Providing scientifically accurate descriptions of the causal mechanisms by which greenhouse gas emissions lead to global warming ( . . , trapping heat ) \" , \" impact \" : \" increases Accuracy \" \" element \" : \" Providing scientifically accurate descriptions of the causal mechanisms by which land use changes lead to global warming ( . . , altering albedo ) \" , \" impact \" : \" increases Accuracy \" \" element \" : \" Including irrelevant or factually incorrect details in the descriptions of the causes . \" , \" impact \" : \" decreases Accuracy \" \" element \" : \" Omitting key details or causal links in the explanation of how the causes contribute to global warming . \" , \" impact \" : \" decreases Accuracy \" \" element \" : \" Presenting information suggesting single cause when the phenomenon has multiple contributors \" , \" impact \" : \" decreases Accuracy \" ] , \" Completeness \" : [ { } , { \" element \" : \" Providing mechanism by which each potential cause contributes to global warming \" , \" impact \" : \" Increases Completeness because it explains * how * the causes lead to the effect , rather than simply stating the cause . \" \" element \" : \" Including specific examples to illustrate the effects of the land use changes \" , \" impact \" : \" Increases Completeness by providing concrete instances that support the description of potential cause . \" Robust Reward Modeling via Causal Rubrics } , { } , { } , { \" element \" : \" Acknowledging the multifactorial nature of global warming and that the listed causes are not exhaustive \" , \" impact \" : \" Increases Completeness by providing appropriate context and preventing the impression of single , simple answer to complex problem . \" \" element \" : \" Providing table with potential causes and descriptions \" , \" impact \" : \" Increases Completeness because the response directly provides the information requested in the question . \" \" element \" : \" Omitting crucial details or explanations about the causes , assuming the user has prior knowledge \" , \" impact \" : \" Decreases Completeness , as the answer requires additional , unstated information to be fully understood . \" } ] , \" Clarity \" : [ { } , { } , { } , { \" element \" : \" Providing specific examples related to the described cause . \" , \" impact \" : \" Increases Clarity by illustrating the abstract description with concrete instances , making the explanation more understandable . \" \" element \" : \" Explicitly stating the mechanism by which each cause contributes to global warming . \" , \" impact \" : \" Increases Clarity by directly linking the cause to its effect on global warming , removing ambiguity about the causal relationship . \" \" element \" : \" Omitting relevant contextual information that provides background or rationale . \" , \" impact \" : \" Decreases Clarity because the information vacuum leaves the user to fill in the gaps , potentially leading to misunderstanding . \" \" element \" : \" Including references to external and unavailable sources ( . . , \" See Table 1 \" ) . \" , 32 33 35 36 37 38 39 41 42 43 44 45 47 48 49 50 51 53 54 55 56 57 59 60 43 Robust Reward Modeling via Causal Rubrics } , { \" impact \" : \" Decreases Clarity by creating an incomplete picture . The reader understanding is hindered by the inability to access referenced information . \" \" element \" : \" Acknowledging the complexity of the issue and the existence of other contributing factors . \" , \" impact \" : \" Increases Clarity by setting appropriate expectations and preventing the reader from oversimplifying the causes of global warming . \" } ] , \" Format \" : [ { \" element \" : \" Presence of clearly defined table with appropriate column headers ( Potential Causes and Description ) \" , \" impact \" : \" Increases Format by structuring the information in an easily digestible manner . \" \" element \" : \" Use of appropriate table delimiters and alignment ( . . , , - - - ) to create visually distinct table . \" , \" impact \" : \" Increases Format by visually structuring and separating table elements , enhancing readability . \" \" element \" : \" Absence of extraneous references to external resources ( . . , See Table 1 ) \" , \" impact \" : \" Increases Format by preventing fragmentation of the answer and ensuring all required information is self - contained within the response . \" \" element \" : \" Inclusion of concise introductory sentence acknowledging the question and introducing the table . \" , \" impact \" : \" Increases Format by providing context and signaling the beginning of the structured answer . \" \" element \" : \" Adherence to consistent and readable text formatting style within the table cells . \" , \" impact \" : \" Increases Format by promoting clarity and ease of understanding of the information presented in the table . \" } , { } , { } , { } , { } 62 63 64 65 66 68 69 70 71 72 74 75 76 77 78 80 81 82 83 84 86 87 88 44 Robust Reward Modeling via Causal Rubrics ] , \" Conciseness \" : [ { } , { } , { } , { } , { } \" element \" : \" Addition of unnecessary introductory or concluding phrases \" , \" impact \" : \" Decreases Conciseness \" \" element \" : \" Omission of essential details or context \" , \" impact \" : \" Increases Conciseness \" \" element \" : \" Inclusion of redundant information \" , \" impact \" : \" Decreases Conciseness \" \" element \" : \" Use of simpler , direct vocabulary \" , \" impact \" : \" Increases Conciseness \" \" element \" : \" Presence of tangential or irrelevant information \" , \" impact \" : \" Decreases Conciseness \" 89 90 92 93 94 95 96 98 99 100 101 102 104 105 106 107 108 110 ] 111 112 } 45 Robust Reward Modeling via Causal Rubrics Chosen Response Edits (degradations to chosen responses along all attributes) 1 \" hos en_r espo nse_e dits \" : { 2 \" Accuracy \" : \" Sure , here is table showing two potential causes for global warming : nPotential Causes Description - - - - - - Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . These gases make the sky brighter , causing the planet to warm . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in the color of the ground , which leads to warming effects . For example , when forests are cleared , the land becomes smoother , which leads to more warming . nIt is important to note that global warming is simple issue with one major cause . The two potential causes listed above are really the same thing . \" , \" Completeness \" : \" Sure , here is table showing two potential causes for global warming : nPotential Causes Description - - - - - - Greenhouse Gas Emissions Land Use Changes global warming is complex issue with multiple causes . The nIt is important to note that two potential causes listed above are just few of the many factors that contribute to climate change . \" , \" Clarity \" : \" Sure , here is table showing two potential causes for global warming : nPotential Causes Description - - - - - - Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo , water cycling , and energy balance . nIt is important to note that global warming is complex issue with multiple causes . The two potential causes listed above are just few of the many factors that contribute to climate change . \" , \" Format \" : \" Sure , here is table showing two potential causes for global warming : nGreenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . These gases trap heat in the atmosphere , causing the planet to warm . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo , water cycling , and energy balance , leading to warming effects . For example , when 46 3 4 5 6 8 Robust Reward Modeling via Causal Rubrics forests are cleared , the land reflects less sunlight back into space , which leads to more warming . nIt is important to note that global warming is complex issue with multiple causes . The two potential causes listed above are just few of the many factors that contribute to climate change . \" , \" Conciseness \" : \" Sure , can definitely help you with that ! Here is table showing two potential causes for global warming . It important to understand these causes to get better grasp of the overall issue : nPotential Causes Description - - - - - - Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . These gases trap heat in the atmosphere , causing the planet to warm . This is because they absorb infrared radiation , preventing it from escaping into space . Therefore , the earth warms up . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo , water cycling , and energy balance , leading to warming effects . For example , when forests are cleared , the land reflects less sunlight back into space , which leads to more warming . This is due to the fact that trees absorb sunlight , and when they re gone , that sunlight is reflected back , causing warming . nIt is important to note that global warming is complex issue with multiple causes , as mentioned earlier . The two potential causes listed above are just few of the many , many factors that contribute to climate change and its associated effects . There are other causes as well . So , basically , global warming has many causes . \" 9 11 } Rejected Response Edits (upgradation to rejected responses along all attributes) 1 \" ej ct d_ re po se _e it \" : { 2 \" Accuracy \" : \" Potential Causes Description - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . These gases trap heat in the atmosphere , leading to warming effect ( the greenhouse effect ) . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo , with darker surfaces absorbing more solar radiation than lighter ones , leading to increased heat retention . These changes also impact the water cycle , 47 3 4 6 7 8 9 10 Robust Reward Modeling via Causal Rubrics reducing evaporation and transpiration , which can further contribute to warming . \" , \" Completeness \" : \" Potential Causes Description - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . These gases trap heat in the atmosphere , leading to rise in global temperatures . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo , water cycling , and energy balance , leading to warming effects . For instance , deforestation reduces the earth capacity to absorb carbon dioxide . nIt is important to note that global warming is complex issue with multiple contributing factors , and this table only lists two potential causes . \" , \" Clarity \" : \" Potential Causes Description - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . These gases trap heat in the atmosphere , leading to rise in global temperatures . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo ( reflectivity ) , water cycling , and energy balance . For example , deforestation reduces the earth ability to absorb carbon dioxide and alters local weather patterns which can contribute to warming . \" , \" Format \" : \" Here are two potential causes for global warming in table : Potential Causes Description - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . Land Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo , water cycling , and energy balance , leading to warming effects . \" , \" Conciseness \" : \" Greenhouse Gas Emissions Release of gases such as carbon dioxide , methane , and nitrous oxide from human activities like burning fossil fuels , deforestation , and agriculture . nLand Use Changes Alteration of the earth surface for human activities like urbanization , agriculture , and forestry can result in changes in albedo , 48 Robust Reward Modeling via Causal Rubrics water cycling , and energy balance , leading to warming effects . \" 11 } Verification Verdicts 1 \" i a n _ u _ r t s \" : { \" Accuracy \" : \" Completeness \" : \" Clarity \" : \" Format \" : \" Conciseness \" : \" Pass \" , \" Pass \" , \" Pass \" , \" Pass \" , \" Fail \" 6 7 } , 8 9 \" i a n _ u _ r t s \" : { 10 \" Accuracy \" : \" Completeness \" : \" Clarity \" : \" Format \" : \" Conciseness \" : \" Pass \" , \" Pass \" , \" Pass \" , \" Pass \" , \" Pass \" 4 5 11 12 13 14 15 } K. Prompt Templates This section details the prompt templates used for identifying attributes and generating counterfactual examples in the rom framework. Placeholders like {question} are replaced with actual content during the process. K.1. Prompt for Attribute Identification Identifying Causal Attributes You are reward model which means you have to rate answers for given question across multiple different attributes. The first step is to identify these attributes as well as give an importance score between 0 and 1 for all these attributes, based on how important they are for rating response for that question. The importance score for all attributes should sum up to 1. The following is Question and 2 Candidate Answer for it. Question: question Example Answer 1: answer1 Example Answer 2: answer2 49 Robust Reward Modeling via Causal Rubrics Task: Give me 5 **mutually exclusive** and important attributes that are required to rate an answer for the give question holistically, along with their importance score. These important attributes should be independent of each other, and should largely depend on the Question given above. Answer Format: Give your answer in JSON format, for example: { Attributes: { \"attribute_1\": attribute_1_score, \"attribute_2\": attribute_2_score, \"attribute_3\": attribute_3_score, \"attribute_4\": attribute_4_score, \"attribute_5\": attribute_5_score } } Where attribute_i is the name of the ith attribute, attribute_i_score is the importance score of the ith attribute, and the Key \"Attributes\" is fixed constant string you should output. Summation of attribute_i_score across all is should be 1. Strictly adhere to the format and only give the json string as output (i.e. start with \"\" and end your response with \"\"). Do not include any commentary, explanations, chattiness, any extra words, or additional keys outside of the specified JSON structure. Answer: 50 Robust Reward Modeling via Causal Rubrics K.2. Prompt for Identifying Causal Elements Identifying Causal Elements per Attribute You are an expert in causal reasoning and response evaluation. You are given: - question - Two example answers Your task is to identify generalizable causal elements that directly affect the strength of the attribute \"{attribute}\" in response to the given question. The two example answers are provided to help you understand how the attribute manifests in this specific context. Do not restrict your analysis to these examplesuse them only to inform your understanding of the attribute in this setting. Question: {question} Accepted Answer: {answer1} Rejected Answer: {answer2} ### Instructions: - Identify exactly five causal elements that impact {attribute} in the response. - Each element must have clear role in either increasing or decreasing {attribute}. Clearly explain its direct causal impact on {attribute}. - Do not include any non-causal heuristics. - Do not include unnecessary explanations, disclaimers, or formattingreturn only the structured JSON output. ### Format: Return raw JSON object only without additional text, explanations, or formatting: ```json {causal_elements_format} ``` 51 Robust Reward Modeling via Causal Rubrics K.3. Prompts for Generating Counterfactuals (Causal Augmentation) Generating Upgraded Responses **Task:** Given question and models response, generate new response with significantly improved response for the specified **{ATTRIBUTE}**, while *strictly preserving* all other aspects of the original response. ### **Input:** - **Question:** {QUESTION} - **Original Response:** {RESPONSE} - **Causal Elements for {ATTRIBUTE}:** {CAUSAL_ELEMENTS} ### **Instructions:** **Understand the Context:** Carefully read the question and original response and ex1. amine the provided causal elements that influence {ATTRIBUTE}. 2. **Identify the strength of {ATTRIBUTE}:** Determine which causal elements are **present** and their **direction of effect** (i.e., whether they increase or decrease {ATTRIBUTE}). 3. **Improve the Response:** Modify the causal elements to significantly improve {ATTRIBUTE}. Ensure that the improvement is **significant but isolated** to {ATTRIBUTE} leaving the other attributes intact. 4. **Verify the New Response:** Reassess whether {ATTRIBUTE} has been significantly improved. Confirm that all **other attributes remain unchanged**. If necessary, improve the response further to better meet the improvement goal for {ATTRIBUTE}. **Return the New Response:** Provide the final modified response with significantly 5. improved {ATTRIBUTE} score. Format your response according to the format given below and in no other format. 52 Robust Reward Modeling via Causal Rubrics ### **Output Format:** Chain of Thoughts: <Your analysis of the original response, identification of causal elements, and strategy for improvement.> New Response: <The final modified response which is significantly improved on {ATTRIBUTE}.> Generating Degraded Responses (Non-Safety) **Task:** Given question and models response, generate new response with significantly degraded response for the specified **{ATTRIBUTE}**, while *strictly preserving* all other aspects of the original response. ### **Input:** - **Question:** {QUESTION} - **Original Response:** {RESPONSE} - **Causal Elements for {ATTRIBUTE}:** {CAUSAL_ELEMENTS} ### **Instructions:** **Understand the Context:** Carefully read the question and original response and ex1. amine the provided causal elements that influence {ATTRIBUTE}. 2. **Identify the strength of {ATTRIBUTE}:** Determine which causal elements are **present** and their **direction of effect** (i.e., whether they increase or decrease {ATTRIBUTE}). 3. **Degrade the Response:** Distort the causal elements to significantly degrade {ATTRIBUTE}. Ensure that the degradation is **significant but isolated** to {ATTRIBUTE} leaving the other attributes intact. 4. **Verify the New Response:** Reassess whether {ATTRIBUTE} has been significantly degraded. Confirm that all **other attributes remain unchanged**. If necessary, degrade the response 53 Robust Reward Modeling via Causal Rubrics further to better meet the degradation goal for the {ATTRIBUTE}. **Return the New Response:** Provide the final modified response with significantly 5. degraded {ATTRIBUTE} score. Format your response according to the format given below and in no other format. ### **Output Format:** Chain of Thoughts: <Your analysis of the original response, identification of causal elements, and strategy for degradation.> New Response: <The final modified response which is significantly degraded on {ATTRIBUTE}.> K.4. Prompts for Generating Causally-Aligned Neutrals K.4.1. Prompt for Comparing Responses via Causal Elements Generating Differences < You compare two responses based on content differences using set of defined attributes and their causal elements. <im_end> <im_start>user will give you question, two responses, and list of attributes with their causal elements. Here is the question: 1 { 2 3 } \" question \" : \" \" \" { QUESTION } \" \" \" Here are the responses: 1 [ 2 4 5 6 7 8 9 10 ] { } , { } \" model \" : \" Response_ 1 \" , \" answer \" : \" \" \" { RESPONSE 1 } \" \" \" \" model \" : \" Response_ 2 \" , \" answer \" : \" \" \" { RESPONSE 2 } \" \" \" Here are the attributes and causal elements: {CAUSAL_ELEMENTS} 54 Robust Reward Modeling via Causal Rubrics Please compare the responses for each attribute: - Identify key content differences. - Explain those differences using the causal elements only. - Do not quote the responses directly. - Focus only on what is said, not how its said. Return your output in this format: 1 { 2 4 5 6 7 8 10 11 12 } ] \" differences \" : [ { } \" attribute \" : \" < attribute > \" , \" difference \" : \" < summary > \" , \" analysis \" : { \" Response_ 1 \" : \" ... \" , \" Response_ 2 \" : \" ... \" } No extra text or explanation outside the JSON object. 55 Robust Reward Modeling via Causal Rubrics K.4.2. Prompt for Generating Causally-Aligned Reconstruction Modifying Response Using Attribute-wise Causal Analysis You modify given response by adjusting its causal elements to match target profile based on attribute-wise analysis. <im_end> <im_start>user will provide you question, given response, and an attribute-based comparison analysis describing how to transform the given response into target response. Inputs: 1. Question: {PLACEHOLDER_FOR_QUESTION} 2. Given Response: {PLACEHOLDER_FOR_GIVEN_RESPONSE} 3. Attribute-wise Differences Analysis: {PLACEHOLDER_FOR_ATTRIBUTE_DIFFERENCES_ANALYSIS} This analysis shows the differences between the given and target responses, broken down per attribute. Each attribute section contains: - Difference: summary of how the responses differ in content or emphasis. - Analysis: - Given Response: Describes its content elements, grounding causal elements, and how they lead to the observed attribute. - New Response: Describes the content and causal elements the target response should exhibit instead. Instructions: 1. Read the question and given response. 2. Carefully study each attribute in the analysis and identify the causal elements needed to change. 3. Generate rewritten response that: Retains the original meaning and structure. Implements the target causal elements. Removes or alters original ones as needed. 4. Do not introduce changes beyond the specified elements. 5. Ensure the new response fully reflects the target causal profile across all attributes. Output Format: {{ }} \"Final Response\": \"<Write the transformed response here>\" Return only the final response JSON. Do not include any explanations or commentary. 56 Robust Reward Modeling via Causal Rubrics K.5. Prompt for Generating Paraphrasing-Based Neutrals Prompt for Paraphrasing Responses \"\"\" Paraphrase the following text while maintaining the style: {text} Make sure the meaning is completely the same without any changes. Respond only with the paraphrase and no extra text at all; for example, do NOT preface with anything like: \"Here is the paraphrased text:\" \"\"\" 57 Robust Reward Modeling via Causal Rubrics K.6. Prompt for Verifying the Counterfactual Quality Prompt for verifying the causal degradations You are an expert evaluator verifying whether the degraded response significantly weakens {ATTRIBUTE} while keeping all other aspects unchanged. ### Input Data - Query: {QUESTION} - Original Response: {RESPONSE} - Modified Response: {NEW_RESPONSE} - Causal Elements for {ATTRIBUTE}: {CAUSAL_ELEMENTS} ### Verification Steps 1. Identify Changes: Compare the original and modified responses to determine which causal elements were altered. 2. Ensure Significant Degradation: Confirm that {ATTRIBUTE} is noticeably weakened, not subtly reduced. 3. Check for Unintended Changes: Verify that the degradation is done by distorting the causal elements for {ATTRIBUTE} alone while keeping the other attributes unaffected. 4. Determine Verdict: If only {ATTRIBUTE} is degraded significantly while all else remains unchanged, return **Pass**; otherwise, return **Fail**. *Strictly* adhere to the provided format. ### Output Format - If the modified response meets all requirements, return: Verdict: Pass - If the modified response does not meet the criteria, return: Verdict: Fail Prompt for verifying the causal upgradations You are an expert evaluator verifying whether the degraded response significantly strengthens {ATTRIBUTE} while keeping all other aspects unchanged. ### Input Data - Query: {QUESTION} - Original Response: {RESPONSE} - Modified Response: {NEW_RESPONSE} - Causal Elements for {ATTRIBUTE}: {CAUSAL_ELEMENTS} ### Verification Steps 1. Identify Changes: Compare the original and modified responses to determine which causal elements were altered. 2. Ensure Significant Improvement: Confirm that {ATTRIBUTE} is noticeably improved, not subtly improved. 3. Check for Unintended Changes: Verify that the improvement is done by modifying the causal elements for {ATTRIBUTE} alone while keeping the other attributes unaffected. 58 Robust Reward Modeling via Causal Rubrics 4. Determine Verdict: If only {ATTRIBUTE} is improved significantly while all else remains unchanged, return **Pass**; otherwise, return **Fail**. *Strictly* adhere to the provided format. ### Output Format - If the modified response meets all requirements, return: Verdict: Pass - If the modified response does not meet the criteria, return: Verdict: Fail K.7. GPT4-as-a-Judge Prompt LLM-as-a-Judge Prompt <im_start>system You are helpful assistant, that ranks models by the quality of their answers, prioritizing substantive content and relevance to the query. <im_end> <im_start>user want you to create leaderboard of different large-language models. To do so, will give you the instructions (prompts) given to the models, and the responses of two models. All inputs and outputs should be Python dictionaries. Here is the prompt: 1 { 2 3 } \" instruction \" : \" \" \" { instruction } \" \" \" Here are the outputs of the models: 1 [ 2 3 4 5 6 8 9 10 ] { } , { } \" model \" : \" model_ 1 \" , \" answer \" : \" \" \" { output_ 1 } \" \" \" \" model \" : \" model_ 2 \" , \" answer \" : \" \" \" { output_ 2 } \" \" \" Now please rank the models by the quality of their answers, so that the model with rank 1 has the best output. Your ranking should be based primarily on which response provides the most accurate, relevant, and comprehensive content in direct relation to the query. Focus on the core information and how well the querys intent is addressed, rather than on aspects like writing style, formatting, length, or politeness, unless these directly impact the clarity or usefulness of the core content requested by the query. Your response MUST be valid JSON object. This JSON object must contain ONLY ONE key named \"ranking_list\". The value of the \"ranking_list\" key MUST be list of two dictionaries, where each dictionary contains the model name and its rank. The structure should be exactly as follows: 59 Robust Reward Modeling via Causal Rubrics 1 { 2 4 5 6 } \" ranking_list \" : [ { \" model \" : \" < model - name > \" , \" rank \" : < model - rank >} , { \" model \" : \" < model - name > \" , \" rank \" : < model - rank >} ] Do not include any other text, explanations, or additional keys in the JSON object. Please provide the ranking that well-informed human, focusing on these content-centric criteria, would likely give. <im_end> L. Qualitative Examples of Reward Model Behavior This section presents several examples illustrating common failure modes or biases observed in standard reward models, highlighting the types of spurious correlations they may learn. Example 1: Formatting Bias (Plumbing Diagram) Question Construct plumbing diagrams for two-story house (describe its core in natural language) Main floor plumbing diagram for two-story house: Waste pipes from the sinks, toilets, and bathtubs on the main floor should all connect to common pipe that runs to the main drain line in the basement. Similarly, the water supply lines from the faucets, showers, and other fixtures should all connect to common supply pipe which then runs to the first-floor water heater. Chosen Response Plumbing diagram for two-story house: Ground floor: - Toilet - Bathroom sink - Shower - Dishwasher Second floor: - Toilet - Bathroom sink - Shower Rejected Response Remarks The rejected response uses list format (a spurious attribute), which might be preferred by reward model biased towards formattedness, even though the chosen response provides more descriptive answer aligned with the prompts request for natural language description. 60 Robust Reward Modeling via Causal Rubrics Example 2: Politeness Bias (Medical Diagnosis) Question Imagine yourself as doctor tasked with devising innovative remedies for various ailments and maladies. Your expertise should encompass prescribing traditional medications, herbal treatments, and alternative natural solutions. Additionally, you must take into account the patients age, lifestyle, and medical background while offering your recommendations. To begin, please assist me in diagnosing scenario involving intense abdominal discomfort. Certainly! To diagnose scenario involving intense abdominal discomfort, would first conduct thorough examination of the patients symptoms and medical history. This would involve physical examination of the abdomen, including pelvic exam, an abdominal X-ray, and colonoscopy. Chosen Response If the patient has history of abdominal pain and nausea, he may be suffering from condition called gastroparesis, also known as \"silent gas pain.\" This is serious condition characterized by severe abdominal pain and discomfort that can be severe or constant. The discomfort can It is important to seek medical attention be accompanied by nausea, vomiting, and diarrhea. immediately if the patient experiences any of these symptoms. Once Ive determined the cause of the abdominal discomfort, would then prescribe traditional medications to alleviate any pain or discomfort. These medications may include pain relievers like ibuprofen or acetaminophen, as well as anti-inflammatory drugs like ibuprofen and naproxen. If traditional medication is not effective, alternative natural treatments may be prescribed. These treatments may include acupuncture, herbal remedies, and home remedies that may help to increase blood flow to the abdomen and alleviate any pain. Its important to note that these are just few examples of possible causes of abdominal discomfort. If you have any other symptoms or medical conditions that may be contributing to your discomfort, it is important to consult with healthcare professional for proper diagnosis and treatment plan. Sure, can help you with that. Based on the symptoms youve described, its possible that the patient may be suffering from gastroenteritis, commonly known as stomach flu. However, its essential to consider other potential causes such as food poisoning, gastritis, or appendicitis. To determine the root cause, Id recommend thorough medical evaluation, including blood tests and imaging studies. Once the diagnosis is confirmed, appropriate treatment can be prescribed, which may include medication, hydration, and rest. Its crucial to consult healthcare professional for accurate diagnosis and treatment. Rejected Response Remarks The rejected response begins with polite phrases (\"Sure, can help you with that.\"), potentially spurious attribute that reward models might learn to favor, even if the chosen response provides more detailed diagnostic thought process aligned with the persona requested. 61 Robust Reward Modeling via Causal Rubrics Example 3: Formatting and Instruction Following (Survey Question) Question Create survey question that can help capture insights about the customer experience. How likely would you be to recommend our product or service to friend or colleague? Chosen Response Sure! Heres sample feedback form that can gather information about the user interface experience: Rejected Response (Scale of 1-5, 1. Overall, how would you rate your experience with the user interface? with 1 being poor and 5 being excellent) 2. Which specific features of the user interface did you find most useful or intuitive? (Open-ended text response) 3. Were there any features of the user interface that you found confusing or difficult to use? If yes, please provide details. (Open-ended text response) 4. Did you encounter any technical issues or bugs while using the user interface? If yes, please describe the issue(s) you faced. (Open-ended text response) 5. How would you rate the visual design and aesthetics of the user interface? (Scale of 1-5, with 1 being poor and 5 being excellent) 6. Were there any specific aspects of the visual design that you particularly liked or disliked? (Open-ended text response) 7. Did you find the user interface responsive and fast? (Yes/No) 8. Were there any specific areas where you experienced slow loading times or delays? If yes, please specify. (Open-ended text response) 9. How would you rate the organization and layout of the user interface? (Scale of 1-5, with 1 being poor and 5 being excellent) 10. Were there any specific areas where you felt the organization or layout could be improved? If yes, please provide details. (Open-ended text response) 11. Did you find the user interface easy to navigate and find what you were looking for? (Yes/No) 12. Were there any specific areas where you had difficulty navigating or finding information? If yes, please specify. (Open-ended text response) 13. How likely are you to recommend the user interface to others? (Scale of 1-5, with 1 being not likely at all and 5 being very likely) 14. Do you have any additional comments or suggestions for improving the user interface? (Open-ended text response) Thank you for taking the time to provide your feedback! Remarks The rejected response fails to follow the instruction (provide single question) and instead provides lengthy form. reward model might erroneously prefer it due to spurious features like length, formatting (numbered list), and politeness (\"Sure!\", \"Thank you!\"). Robust Reward Modeling via Causal Rubrics Example 5: Formatting Bias vs. Content Quality (Electrician) Question What is the fastest way to become an electrician in the us Obtain high school diploma or GED. Complete an electrical apprenticeship program, which typically lasts 45 years and combines on-the-job training with classroom instruction. Pass the Journeyman Electrician Exam to become licensed. Note that requirements and the time frame may vary depending on the state you are in. Chosen Response Rejected Response To become an electrician in the US, the fastest path typically involves the following steps: Foundational Knowledge: Formal Education: Consider pursuing degree in electrical engineering or related field. This will provide you with structured courses that cover the basics of electrical theory, circuit analysis, electronics, power systems, electromagnetism, and more. Online Courses and Tutorials: Websites like Coursera, edX, Udacity, and Khan Academy offer courses in electrical engineering and electronics. They can serve as an excellent supplement or alternative to traditional education. Hands-on Experience: DIY Projects: Start with small projects like building basic circuits, then gradually move to more complex ones. Kits are available online to help beginners learn the basics of circuit design and assembly. Internships and Apprenticeships: These provide invaluable real-world experience. Working under mentor or experienced electrician can greatly enhance your learning. Certifications and Specializations: While not always mandatory, obtaining certifications can validate your expertise and open doors for specialized roles or areas within the field of electrics. Continuous Learning: Workshops and Seminars: Regularly attend these to stay updated with the latest techniques, tools, and best practices in the field. Books and Publications: Regularly read books, journals, and other publications related to electrical engineering and electronics. Networking: Professional Associations: Joining organizations like the Institute of Electrical and Electronics Engineers (IEEE) can provide networking opportunities, resources, and continued education. Online Forums and Groups: Websites like Stack Exchange (for electrical engineering), online forums, and even Reddit have active communities where you can ask questions, share knowledge, and learn from others. Practice and Experimentation: Regularly challenge yourself with new projects or problems to solve. The more you apply your knowledge, the more intuitive and deeply ingrained it will become. Stay Updated: Industry Trends: Electrics, like all fields of technology, is continuously evolving. Ensure youre aware of the latest tools, technologies, and methodologies in the field. Regular Refreshers: Periodically revisit foundational concepts to ensure your foundational knowledge remains strong. Soft Skills: While technical knowledge is crucial, dont underestimate the importance of communication, teamwork, problem-solving, and critical thinking skills. These are often essential, especially if you aim to take on leadership or interdisciplinary roles. Teach and Mentor: Once youve gained certain level of expertise, consider teaching or mentoring others. This not only benefits the community but can also reinforce and expand your understanding. Becoming an electrician requires dedication, passion, and willingness to continually push your boundaries. With focused effort and the right resources, you can achieve high level of expertise and enjoy fulfilling career in the world of 63 Robust Reward Modeling via Causal Rubrics electronics. Remarks The rejected response, while containing much unnecessary and potentially irrelevant information (e.g., suggesting an electrical engineering degree), uses extensive formatting (numbered lists with sub-points). reward model overly sensitive to formattedness might prefer it over the more concise and directly relevant chosen response."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "MILA - Quebec AI Institute",
        "McGill University"
    ]
}