{
    "paper_title": "Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks",
    "authors": [
        "Jiun-Cheng Jiang",
        "Morris Yu-Chao Huang",
        "Tianlong Chen",
        "Hsi-Sheng Goan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Variational quantum circuits (VQCs) are central to quantum machine learning, while recent progress in Kolmogorov-Arnold networks (KANs) highlights the power of learnable activation functions. We unify these directions by introducing quantum variational activation functions (QVAFs), realized through single-qubit data re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We show that DARUAN with trainable weights in data pre-processing possesses an exponentially growing frequency spectrum with data repetitions, enabling an exponential reduction in parameter size compared with Fourier-based activations without loss of expressivity. Embedding DARUAN into KANs yields quantum-inspired KANs (QKANs), which retain the interpretability of KANs while improving their parameter efficiency, expressivity, and generalization. We further introduce two novel techniques to enhance scalability, feasibility and computational efficiency, such as layer extension and hybrid QKANs (HQKANs) as drop-in replacements of multi-layer perceptrons (MLPs) for feed-forward networks in large-scale models. We provide theoretical analysis and extensive experiments on function regression, image classification, and autoregressive generative language modeling, demonstrating the efficiency and scalability of QKANs. DARUANs and QKANs offer a promising direction for advancing quantum machine learning on both noisy intermediate-scale quantum (NISQ) hardware and classical quantum simulators."
        },
        {
            "title": "Start",
            "content": "Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Jiun-Cheng Jiang Morris Yu-Chao Huang Tianlong Chen Hsi-Sheng Goan jcjiang@phys.ntu.edu.tw, {morris, tianlong}@cs.unc.edu, goan@phys.ntu.edu.tw September 18, 2025 Abstract Variational quantum circuits (VQCs) are central to quantum machine learning, while recent progress in Kolmogorov-Arnold networks (KANs) highlights the power of learnable activation functions. We unify these directions by introducing quantum variational activation functions (QVAFs), realized through single-qubit data re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We show that DARUAN with trainable weights in data pre-processing possesses an exponentially growing frequency spectrum with data repetitions, enabling an exponential reduction in parameter size compared with Fourier-based activations without loss of expressivity. Embedding DARUAN into KANs yields quantum-inspired KANs (QKANs), which retain the interpretability of KANs while improving their parameter efficiency, expressivity, and generalization. We further introduce two novel techniques to enhance scalability, feasibility and computational efficiency, such as layer extension and hybrid QKANs (HQKANs) as drop-in replacements of multi-layer perceptrons (MLPs) for feed-forward networks in large-scale models. We provide theoretical analysis and extensive experiments on function regression, image classification, and autoregressive generative language modeling, demonstrating the efficiency and scalability of QKANs. DARUANs and QKANs offer promising direction for advancing quantum machine learning on both noisy intermediate-scale quantum (NISQ) hardware and classical quantum simulators. Code available at: https://github.com/Jim137/qkan 5 2 0 2 7 1 ] - u [ 1 6 2 0 4 1 . 9 0 5 2 : r Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei 106319, Taiwan Center for Quantum Science and Engineering, National Taiwan University, Taipei 106319, Taiwan Physics Division, National Center for Theoretical Sciences, National Taiwan University, Taipei 106319, Taiwan Department of Computer Science, University of North Carolina at Chapel Hill, NC 27514, USA"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Results 2.1 The Quantum Variational Activation Function . . . . . . . . . . . . . . . . . 2.2 QKAN architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Theoretical Analysis of QKAN . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Discussion 4 Methods 4.1 Kolmogorov-Arnold Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Data Re-Uploading Circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Knowledge Distillation from QKANs to KANs . . . . . . . . . . . . . . . . . 4.4 Distributed Training of QKANs . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 QKAN Implementation Methods . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Numerical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Theoretical Backgrounds A.1 Proof of Theorem 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Proof of Theorem 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Numerical Results Details B.1 Complementary Results of Figure 5 . . . . . . . . . . . . . . . . . . . . . . . B.2 Visualization of QKAN Activations on Noisy Function Regression . . . . . . Experimental Details C.1 Software Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Software and System Information . . . . . . . . . . . . . . . . . . . . . . . . 2 4 4 6 7 9 17 19 19 20 20 21 22 23 34 34 34 40 40 42 42 42 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Figure 1: Schematic overview of the proposed quantum machine learning models. (a) quantum neural network (QNN) serves as variational activation function (VAF), where an input is processed by quantum circuit to yield the output ϕ(x). (b) Specifically, we implement VAFs within perceptron using single-qubit data re-uploading circuit with trainable data pre-processing weights wℓ in the ℓ-th encoding block S(wℓx). The ℓ-th parameterized unitary is (ℓ) = (ℓ)(θℓ), where θℓ is the set of trainable parameters in (ℓ). The aggregated input = (cid:80) αixi is repeatedly uploaded into each data encoding block, and the measurement outcome of the parameterized circuit defines the activation function. (c) We further incorporate data re-uploading activations into the structure of Kolmogorov-Arnold networks (KANs), treating each edges activation as the output of distinct quantum circuit. Post-activation values are summed according to predefined pattern to yield subsequent layer outputs, ultimately resulting in quantum-inspired Kolmogorov-Arnold network (QKAN)."
        },
        {
            "title": "1 Introduction",
            "content": "Quantum computing (QC) and quantum machine learning (QML) represent rapidly evolving interdisciplinary research frontiers that leverage quantum mechanical principles to perform computation [Biamonte et al., 2017, Dunjko et al., 2016, Chen and Liang, 2025]. In QML, central theme involves encoding classical data into high-dimensional Hilbert spaces using quantum states, harnessing the advantages of superposition, coherence, and entanglement to enable efficient learning from complex datasets [Biamonte et al., 2017, Ciliberto et al., 2018, Schuld and Killoran, 2019, Phillipson, 2020, Meyer et al., 2023, Liu et al., 2024a, Devadas and T, 2025]. Among the primary models in QML are variational quantum circuits (VQCs) which 2 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks serve as quantum analogues of classical neural networks [Farhi and Neven, 2018, Chen et al., 2020]. VQCs encode input data via structured ansatz and optimize parameterized gates using classical algorithms [Schuld et al., 2021, Pérez-Salinas et al., 2020]. VQCs are the foundation of the hybrid quantum-classical machine learning paradigms [Mari et al., 2020] and widely regarded as one of the most promising routes toward demonstrating quantum advantage in near-term applications [Jerbi et al., 2023]. In parallel, advances in classical machine learning have spotlighted the use of learnable variational activation functions (VAFs), particularly through the development of KolmogorovArnold networks (KANs) [Liu et al., 2024c]. Inspired by the Kolmogorov-Arnold representation theorem (KART), KANs extend the concept of classical multilayer perceptrons (MLPs) by replacing fixed nonlinearities with learnable VAFs at each edge and performing summation at each node. This approach has yielded improvements in both predictive accuracy and interpretability and has shown promising results across broad range of tasks including computer vision [Li et al., 2024a, Abd Elaziz et al., 2024, Xingyi Yang, 2025] and time series forecasting [Vaca-Rubio et al., 2024, Xu et al., 2024b, Genet and Inzirillo, 2024]. More researches have explored VAF implementations in KAN using Chebyshev polynomials [SS et al., 2024], wavelets [Bozorgasl and Chen, 2024, Seydi, 2024b], Fourier series [Xu et al., 2024a], radial basis functions [Li, 2024], and other function bases [Howard et al., 2024, Ta, 2024, Aghaei, 2024, Seydi, 2024a, Xingyi Yang, 2025]. Liu et al. [2024b] further enriched the expressivity of KANs by incorporating multiplicative interactions at the nodes. Recent studies have established that VQCs can approximate any analytic function [Mitarai et al., 2018], and under certain conditions, even arbitrary continuous functions [Schuld et al., 2021, Yu et al., 2022, 2024b]. Nevertheless, much of the QML literature has focused on data encoding strategies rather than function approximation. Notably, the ability of VQCs to learn even single-variable function (x) already exceeds classical capabilities, motivating new approaches that leverage this expressive potential [Wach et al., 2023]. In this work, we propose to use VQCs not as standalone learners but as VAFs within classical or hybrid architectures, introducing novel framework termed quantum variational activation functions (QVAFs). As single-qubit data re-uploading circuit itself is classically efficiently simulable and powerful enough to learn an univariate function [Pérez-Salinas et al., 2020, Schuld et al., 2021, Yu et al., 2022], we instantiate QVAFs using single-qubit circuits and name this model DatA Re-Uploading ActivatioN (DARUAN). The term daruan is derived from Chinese traditional plucked string instrument renowned for its deep and coherent, mellow tone. To demonstrate the broader applicability of this concept, we further extend DARUAN to Quantum-inspired Kolmogorov-Arnold Networks (QKANs), where DARUAN modules serve as quantum-inspired VAFs within the KAN framework. schematic illustration of the QVAF, DARUAN, and QKAN architectures is provided in Figure 1 respectively. We theoretically analyze the frequency spectrum, m-norm approximation error and parameter estimation of single-qubit data re-uploading circuits in QKANs with and without trainable data pre-processing weights. We demonstrate that when trainable weights are 3 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks incorporated into the data pre-processing phase, we can achieve an exponential reduction in parameter size compared to the classical Fourier-series-based KAN. We empirically validate our proposed models on variety of tasks, including regression, classification, and generative modeling. While QKANs and KANs show promising results, we notice that the number of parameters increases quadratically with the input and output dimensions, challenge inherent to both architecture. To mitigate this issue, we present an adaptive algorithm, hybrid QKAN (HQKAN), that dynamically reduce parameter counts, thereby improving scalability. While QKANs and DARUANs utilize only single qubit in each activation, this approach enables efficient simulation on classical computers. By leveraging the concept of distributed machine learning on GPUs, our models can effectively handle large-scale training tasks, including the training of large language models (LLMs). Our results show that QKANs, with appropriate architectural refinements, match or surpass the performance of classical KANs and MLPs while using significantly fewer parameters and computational resources. By integrating quantum circuit designs into VAFs, the QVAF and DARUAN frameworks offer compelling avenue toward practical, resource-efficient quantum machine learning."
        },
        {
            "title": "2.1 The Quantum Variational Activation Function",
            "content": "The concept of VAFs has recently gained attention in classical machine learning, where the activation functions within neural networks are no longer fixed but instead treated as trainable parameters θ Rd. ϕθ : R, ϕθ(x). (cid:55) (2.1) This approach enhances networks expressivity by allowing it to learn the most suitable nonlinear transformations from data, leading to improvements in accuracy, convergence, and generalization performance [Molina et al., 2019, Apicella et al., 2021, Liu et al., 2024c]. Parametric ReLU (PReLU)[He et al., 2015] and Swish[Ramachandran et al., 2017] are two representative examples, where slope or gating parameters are learned during training. More flexible formulations include adaptive piecewise linear (APL) units [Agostinelli et al., 2015], kernel activation functions [Scardapane et al., 2019], and spline-based activations [Liu et al., 2024c], all of which treat activation function as learnable entities. Inspired by this classical paradigm, we propose quantum analogue: the QVAF. In this framework, the role of an activation function is replaced with variational quantum circuit, which processes the input and produce nonlinear transformation derived from quantum measurement. In general, QVAF is defined as: ϕθ(x) = 0 (x; θ) (x; θ) , 0 4 (2.2) Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks where the input is trainable unitary circuit, the expectation value yields bounded nonlinear function. is encoded via data re-uploading or angle encoding into qubits, (x; θ) 1, and is an Hermitian observable with the norm The expressive power of QVAFs stems from their ability to generate highly nonlinear, tunable transformations even in low-depth, single-qubit circuits [Schuld et al., 2021, Mitarai et al., 2018]. Moreover, since these activations are inherently smooth and bounded, they are particularly well-suited for stable training. Prior work has shown that VQCs are capable of approximating any analytic function [Mitarai et al., 2018] and even arbitrary continuous functions under certain conditions [Schuld et al., 2021]. This positions QVAFs as universal approximators, analogous to classical VAFs but with access to quantum-enhanced feature spaces. Recent efforts such as variational quantum splines [Inajetovic et al., 2023] and quantuminspired activation circuits in hybrid convolutional networks [Li et al., 2024b] have demonstrated the potential of empirical viability of QVAFs on both synthetic and real-world data. To implement QVAFs in practice and facilitate their integration into layered network architectures, we introduce DARUAN leveraging the data re-uploading circuit framework [PérezSalinas et al., 2020] to construct scalable quantum-inspired activation layer with multiple repetitions and trainable pre-processing weights in single qubit. Each block consists of data encoding alternated with trainable unitaries, forming variational circuit capable of approximating smooth periodic and non-periodic functions. The output is obtained via measurement of Pauli observable (typically σz in computational basis) and is used as the nonlinear transformation applied to the neuron output. In Figure 1(b), DARUAN acts as VAF in perceptron where the classical data is re-uploaded multiple times to data re-uploading variational quantum circuit and readout the expectation value of the data re-uploading circuit as final output. Importantly, DARUAN supports architectural flexibility through concept we term layer extension, which progressively increases the number of re-uploading repetitions, where we discuss the details in Section 4. In our latter part of the experiments, this design allows the model to scale its expressivity on demand while preserving previously learned features. Layer extension addresses the challenge of optimizing deep quantum circuits, which are notoriously difficult to optimize [McClean et al., 2018]. The simplicity of DARUAN, which relies solely on single-qubit circuits, makes it well-suited for implementation on current noisy intermediate-scale quantum (NISQ) devices. State-of-the-art trapped-ion platforms have achieved single-qubit gate error rates at the 107 level [Smith et al., 2025], while superconducting architectures have reached 105 [Rower et al., 2024], ensuring that DARUAN is experimentally feasible on current NISQ hardware. At the same time, our method is highly efficient to simulate on modern GPUs and multi-node high-performance computing (HPC) clusters, enabling large-scale benchmarking and seamless integration into GPT architectures. This dual capability underscores both near-term hardware implementability and practical scalability in classical simulation, while preserving competitive expressive power across both 5 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks settings. Furthermore, hybrid models leveraging QVAFs benefit from form of exponential compression: QVAF can implicitly encode large number of frequency modes. This suggests that QVAFs serve not only as activation functions but also as compact feature extractors within hybrid quantum-classical architectures. QVAFs open promising path toward expressive, trainable nonlinearities in QML. They provide both theoretical richness and practical flexibility, enabling efficient approximation capabilities in low-resource quantum settings."
        },
        {
            "title": "2.2 QKAN architecture",
            "content": "Building upon the insights from KANs [Liu et al., 2024c] and the expressive power of DARUAN, we introduce the QKANs. In this architecture, each activation function traditionally implemented via B-spline interpolation in KANs is replaced by DARUAN, single-qubit data re-uploading variational quantum circuit, yielding compact and trainable nonlinearity. The central idea of QKANs is to harness the mathematical nature of Fourier-like expansion properties of data re-uploading quantum circuits, which approximate target functions via tunable superpositions of frequency components [Schuld et al., 2021]. These circuits serve as QVAFs, enabling QKANs to learn highly expressive mappings using significantly fewer parameters than classical VAFs. While KANs approximate activation functions through B-spline bases with grid size G, QKANs estimate analogous Fourier coefficients through parameter-efficient quantum circuit with relative small number of data re-uploading repetitions r. Each QKAN layer is constructed from collection of single-qubit DARUANs organized in feedforward structure. For layer with nl input nodes and nl+1 output nodes, the layer is defined as: ϕl,j,i Φl = } { ϕl,j,i(xl,i) = nl(cid:88) xl+1,j = i=1 , = 1, 2, . . . , nl, = 1, 2, . . . , nl+1; ; 0 (xl,i, θl,j,i) (xl,i, θl,j,i) 0 ϕl,j,i(xl,i), (2.3) (2.4) (2.5) where i, are indexes of input and output node respectively, (x; θ) denotes the data is the Pauli observable re-uploading unitary circuit with trainable parameters θ, and measured to obtain the circuit output. The final model is obtained by composing these layers: where the output is bounded within [ values. = QKAN(x) = (ΦL ΦL1 Φ Φ1)(x), (2.6) nL1, nL1]nL due to the nature of quantum expectation QKANs offer both theoretical and practical advantages in terms of approximation capacity and parameter efficiency. From complexity perspective, the number of parameters required 6 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks for QKAN with depth L, width , and repetition count scales as (N 2Lr). In contrast, (N 2LG) parameters, where is the number of spline grid points. As KANs demand detailed in Theorem 2.2, the number of Fourier modes grows exponentially with r, and therefore, small suffices to yield significant reduction in parameters compared to classical grid-based and Fourier-based approaches. Consequently, QKANs inherit the shallow architecture and structured interpretability of KANs while achieving parameter efficiency and enhanced expressivity through quantuminspired VAF, DARUAN. As such, they form promising and scalable approach for realizing compact, data-efficient, and interpretable models in QML."
        },
        {
            "title": "2.3 Theoretical Analysis of QKAN",
            "content": "We analyze the architectural design of QKAN and establish its quantum advantages over classical KAN. First, we present an approximation theory for KART (Theorem 2.1) based on Fourier series, extending Theorem 2.1 of Liu et al. [2024c] from the case of B-spline basis functions. We then investigate the frequency spectrum, m-norm approximation error, and parameter estimation for the Fourier series representation of single-qubit data re-uploading circuits within QKAN, as formalized in Theorem 2.2. Theorem 2.1 (Approximation Theory with Fourier Series, modified from Theorem 2.1 in Liu et al. [2024c]). Let = (x1, x2, . . . , xn). Suppose that function (x) admits representation: (x) = (ΦL ΦL1 Φ2 Φ1)(x), (2.7) where each Φl,j,i is (k + 1)-times continuously differentiable. Then, there exists constant depending on and its representation such that we have the following approximation bound in terms of the highest frequency in the Fourier series: there exist trigonometric polynomial approximations ΦK k, we have the bound: l,i,j such that for any integer with (cid:13) (cid:13)f (ΦK ΦK L1 1 )(x)(cid:13) ΦK (cid:13)Cm CK (k+1m), (2.8) where the m-norm approximation error measures the magnitude of derivatives up to order m: Cm = max βm sup x[0,1]n (cid:12)Dβg(x)(cid:12) (cid:12) (cid:12) , (2.9) and Dβ denotes the partial derivative of order β. The proof is provided in Section A.1. Remark 1. In Theorem 2.1, by changing the B-splines in Theorem 2.1 of Liu et al. [2024c] to Fourier series with the highest frequency K, we derive the approximation error bound. This shows the approximation error in the m-norm decays at the rate (K (k+1m)) as the number of Fourier modes 2K increases. The approximation accuracy is controlled by the grid size in B-splines or the highest frequency in the Fourier series. 7 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Theorem 2.2 (Spectrum expansion and approximation error of QKAN with linear layer). 1, consider two Fix an integer single-qubit data re-uploading circuits: k+1[0, 1]. For any depth 0 and target function (A) Baseline (Un-weighted). (x) = (r+1)(cid:2)S(x) (r)(cid:3) S(x) = eixH, (cid:2)S(x) (1)(cid:3), where (ℓ) = (ℓ)(θℓ) is the ℓ-th parameterized unitary with θℓ being the set of trainable parameters and = 1 2σj is the Hermitian generator and σj (B) With classical linear layer. Let ω = (w1, . . . , wr) . } )r and set σx, σy, σz (0, { Uω(x) = (r+1) 1 (cid:89) (cid:2)S(wℓx) (ℓ)(cid:3), S(wℓx) = eiwℓxH. ℓ=r Define the model outputs fA(x) = fB(x) = (x) 0 ω(x) 0 0 (x) Uω(x) , 0 . (a) Baseline spectrum. fA is trigonometric polynomial fA(x) = (cid:80)r α=r cαeiαx, hence the number of distinct non-zero frequencies is = 2r. ΩA (b) Linear-layer expansion. fB has spectrum ΩB = (cid:40) (cid:88) ℓ=1 mℓwℓ (cid:12) (cid:12) (cid:12) mℓ 1, 0, 1 { (cid:41) . } The number of distinct non-zero frequency satisfies ΩB (3r 1). (c) m-norm approximation error. For 0 fA fB Cm Cm Cf (k+1m) Cf (k+1m) , KA := r, (cid:88) , KB := ℓ=1 wℓ. (d) Parameter efficiency vs. Fourier-series-based KAN. classical Fourier-seriesbased KAN requires = Θ(cid:0)ε1/(k+1m)(cid:1) parameters to reach error ε in norm. By choosing the geometric weights wℓ = 2ℓ1, we have KB = 2r 1 and fB Cm Cf 2r(k+1m). 8 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Solving Cf 2r(k+1m) = ε gives = (cid:24) log2(Cf /ε) + 1 (cid:25) = Θ(cid:0) log 1 ε (cid:1), so the total number of trainable parameters is Θ(log 1 compared to Fourier-series-based KAN. ε ), an exponential reduction The proof is provided in Section A.2. In Theorem 2.2(a), we show that the maximum spectrum size of an unweighted data re-uploading circuit grows only linearly with r. To overcome this limitation, DARUAN and QKAN introduce trainable weights that exponentially expand and reshape the dynamically controllable frequency spectrum, as formalized in Theorem 2.2(b) and also employed in prior works [Zhao et al., 2024, Yu et al., 2022]. Together with Theorem 2.1, these results establish Fourier-series-based approximation theory, where the error bound is determined by the maximum frequency KB. By exploiting the exponentially large Fourier spectrum, we demonstrate the expressive power of QKAN, as stated in Theorem 2.2(c) and (d). central challenge, however, is the exponential vanishing-gradient problem [McClean et al., 2018] in large quantum models. QKAN addresses this by enabling dynamical frequency spectrum expansion during training through the parameter r. This stands in contrast to classical KANs, where functional granularity is adjusted by the grid size and refined through grid extension. In QKANs, the frequency increases exponentially during training with learnable circuit depth, enabling gradual fine-tuned global feature through layer extension. Therefore, we employ the layer extension strategy, introduced earlier and elaborated in the Section 4, as practical solution to mitigate vanishing gradients while preserving expressive power."
        },
        {
            "title": "2.4 Numerical Results",
            "content": "In this section, we assess the versatility and effectiveness of QKANs by simulating various tasks, including regression, classification, and generative modeling. Overall, our experiments demonstrate that QKAN consistently outperforms classical KAN and MLP baselines across tasks. In regression benchmarks, QKAN achieves up to an order-of-magnitude reduction in approximation error with significantly fewer parameters. For classification tasks, QKANs and HQKANs consistently surpass classical KANs and MLP baselines in both top-1 and top-5 accuracy, while HQKANs achieves this with substantially fewer parameters. In generative modeling, HQKAN integrated into GPT-2 obtains lower perplexity and reduced training time and computational resources compared to MLP counterparts. These results establish QKAN as scalable and hardware-efficient alternative to classical architectures, with strong potential for near-term and large-scale quantum-classical hybrid applications. Unless otherwise specified, the following results are obtained on personally available NVIDIA RTX 4090 GPU. Detailed setups for each task are provided in the Section 4. 9 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Figure 2: Function fitting with noise using QKAN and KAN. The target function is (x) = J0(20x), fitted with noisy data. Both QKAN and KAN use shape of [1, 1]. The QKAN prediction exhibits smoother behavior compared to KAN which tends to overfit local noise features. We begin with regression task focused on function fitting. Previous studies have demonstrated that KANs outperform MLPs in various regression problems, including multivariate function fitting and solving partial differential equations (PDEs) [Liu et al., 2024c,b]. In particular, KANs have shown strong performance in symbolic expression recovery [Yu et al., 2024a, Liu et al., 2024b]. To investigate the robustness of QKAN and its potential for real-world applications, we prepare regression dataset with added noise in both training and testing labels. We start with heuristic function fitting, where the hidden layers and hidden nodes are manually specified. As simple example, we consider fitting the function (x) = J0(20x), where J0 is Bessel function that J0(x) = sin (x)/x, using QKAN and KAN with heuristic QKAN (KAN) shape [1, 1]. The training label is subject to Gaussian error with standard deviation of 0.1. The results are shown in Figure 2, where QKAN yields smoother approximation to the noisy data, while KAN captures more localized features, which in this case correspond to the noise. To further assess model performance, we conduct more complex heuristic function fitting experiments using the QKAN (KAN) shapes suggested in Liu et al. [2024c]. We report the average of the best performance across five random seeds in Table 1, and summarize the best test loss and parameter sizes for QKANs and KANs in Table 2. As shown in Table 1, QKAN consistently outperforms both KAN and MLP baselines in the majority of cases, achieving the lowest test RMSE on 7 out of the 10 benchmark equations. This trend demonstrates that QVAF in QKAN effectively captures complex patterns in noisy regression tasks. In the remaining three cases, QKAN still ranks second, with performance closely matching or slightly trailing that of the best-performing baseline. 10 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Table 1: Heuristic noisy function regression. We select 10 functions in Feynman dataset [Udrescu and Tegmark, 2020, Udrescu et al., 2020] and each function is represented by dimensionless formula with its variables. Moreover, we randomly sample the data with 10 % noise level and 1000 training, 1000 testing data points for each function. Each function has inputs within the range of [0, 1]. The shapes of QKAN/KAN are represented by the number of hidden nodes in each layer. The performance is measured by the root mean square error (RMSE) of the predicted outputs compared to the test data. The best performance is highlighted in bold while the second is labeled with underline. The results show that QKAN outperforms KAN and MLP in most cases, demonstrating the effectiveness of the QKAN architecture in noisy function regression tasks. Feynman eq. Dimensionless formula Variables QKAN/KAN shape QKAN KAN MLP I.12.11 I.29.16 I.40.1 I.50.26 II.2.42 II.6.15a II.35.18 II.36.38 III.10.19 III.17. (cid:112)1 + a2 1 + sin θ 2a cos (θ1 n0ea cos + α cos2 (a 1)b 4π ca2 + b2 n0 exp (a)+exp (a) + αb 1 + a2 + b2 β(1 + α cos θ) θ2) a, θ a, θ1, θ2 n0, a, α a, a, b, n0, a, b, α a, α, β, θ [2, 2, 1] [3, 2, 3, 1] [2, 2, 1, 1, 1, 2, 1] [2, 2, 3, 1] [2, 2, 1] [3, 2, 1, 1] [2, 1, 1] [3, 2, 1] [2, 1, 1] [3, 3, 1] 1.1911 1.4452 1.0391 1.1680 2.4054 3.1332 2.1154 7.3153 1.2415 6. 101 101 101 101 102 103 102 102 101 102 1.210 1.4658 3.8206 1.1858 2.4416 3.0759 2.1275 8.0143 1.2443 7.0658 101 101 102 101 102 103 102 102 101 102 1.3877 3.1924 7.9577 1.1351 4.3203 7.9275 1.1086 1.6865 1.4845 3. 101 101 101 101 101 103 101 101 101 101 The results suggest that QKANs expressive feature mappings are especially well-suited for capturing high-frequency or compositional structures, where classical models tend to underperform. Table 2 further reveals that QKAN models not only deliver superior or comparable performance but also achieve this with significantly fewer parameters. On average, QKAN uses about 30% fewer parameters than KAN while maintaining or improving generalization accuracy. This parameter efficiency is especially advantageous in scenarios with limited model capacity or deployment constraints. However, in most scenarios the underlying functional form is unknown priori. To evaluate model robustness under such uncertainty, we extend our study to diverse suite of 66 symbolic expressions drawn from the Feynman dataset [Udrescu and Tegmark, 2020, Udrescu et al., 2020], each input normalized to the unit hypercube and output subject to 10% additive noise. For each equation, we optimize QKAN, KAN and MLP architectures over hidden-layer depths as described in Method Section, and record the lowest test RMSE attained across five random seeds. Figure 5 displays these best-case losses on logarithmic scale, with the total number of trainable parameters annotated for both QKANs and KANs. Notably, QKAN achieves the lowest RMSE on over 80% of the benchmark equations, despite employing on average 30% fewer parameters than classical KAN. In the minority of cases where KAN marginally outperforms, the QKAN remains competitive, often within the same order of magnitude, 11 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Table 2: Comparison of QKAN and KAN in heuristic noisy function regression. Continue from Table 1, we report the number of parameters and RMSE loss of QKAN and KAN models. The best performance is highlighted in bold. The results show that QKAN outperforms KAN while requiring 30 % fewer parameters in average. Feynman QKAN KAN equation RMSE loss # Params RMSE loss # Params I.12.11 I.29.16 I.40.1 I.50.26 II.2.42 II.6.15a II.35.18 II.36.38 III.10.19 III.17. 1.1911 101 1.4452 101 101 1.0391 1.1680 101 2.4054 102 103 3.1332 2.1154 102 7.3153 102 1.2415 101 6.8907 102 96 240 192 208 96 144 48 128 48 192 101 1.210 101 1.4658 3.8206 102 101 1.1858 102 2.4416 3.0759 103 102 2.1275 102 8.0143 101 1.2443 102 7.0658 135 336 272 292 135 202 68 179 68 268 while retaining its parameter-efficiency advantage. By contrast, standard MLPs exhibit rapidly deteriorating generalization as equation complexity increases, underscoring the critical role of structured feature embeddings in noisy symbolic regression. These results substantiate the dual strengths of QKAN: QVAF and DARUAN not only enhance expressivity in the absence of exact analytic priors but also deliver substantial parameter savings. Consequently, QKAN represents compelling approach for data-driven discovery and modeling in scientific applications where both accuracy and resource constraints are paramount. The quantum-enhanced architecture provides both improved generalization and model compactness, highlighting its potential for broader applications in data-driven scientific modeling. To evaluate the expressive power and scalability of QKANs beyond function regression, we investigate their performance on image classification tasks. We consider three standard benchmarksMNIST [Deng, 2012], CIFAR-10, and CIFAR-100 [Krizhevsky and Hinton, 2009]and use hybrid architecture where convolutional neural network (CNN) is followed by fully connected network (FCN). In our setup, the FCN is instantiated using either an MLP, KAN, or QKAN, enabling direct comparison across model families with identical convolutional backbones. Table 3 reports the top-1 and top-5 classification accuracies together with the parameter counts of the FCN components. Top-1 accuracy measures the proportion of test samples for which the models single most confident prediction coincides with the true label, whereas top-5 accuracy considers prediction correct if the ground-truth label is contained within 12 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Table 3: Performance of different models on MNIST, CIFAR-10, and CIFAR-100 datasets. The top-1, top-5 test accuracy and the parameter size of each model. The second column indicates the parameter size of CNN shared by all models. Model Training dataset (CNN) CNN+MLP CNN+KAN(G=10) CNN+QKAN(r=3) CNN+HQKAN CNN Top-1 Top-5 HQKAN KAN # Params Accuracy (%) # Params Accuracy (%) # Params Accuracy (%) # Params Accuracy (%) # Params Top-1 Top-5 Top-1 Top-5 Top-1 TopQKAN MLP MNIST CIFAR-10 CIFAR-100 1,084 56,320 56,320 97.9 71.4 39.8 100.0 97.8 69. 850 41,802 86,948 97.7 68.4 40.6 100.0 97.4 70.4 1,500 39,900 384,000 98.0 68.8 41.2 100.0 97.0 70. 800 21,280 204,800 95.9 71.6 39.9 99.7 97.9 70.6 222 14,370 32,636 the top five highest probabilities. On MNIST, all models achieve high accuracy, with CNN+QKAN attaining the highest top-1 accuracy of 98.0% and perfect top-5 accuracy using only 800 parameters in the FCN. In contrast, CNN+MLP and CNN+KAN require significantly more parameters (850 and 1,500, respectively) to achieve comparable accuracy. This suggests that QKAN can achieve strong performance with minimal parameter overhead, even on simple tasks. The advantage of QKAN becomes more evident on CIFAR-10, where CNN+QKAN achieves the comparable top-1 and top-5 accuracies (68.8% and 97.0%, respectively) while requiring nearly half the parameters of CNN+KAN (21,280 vs. 39,900). CNN+MLP, despite having similar parameter count to KAN, doesnt outperform in both metrics. This highlights QKANs superior parameter efficiency and generalization capacity. For the more challenging CIFAR-100 dataset, CNN+QKAN achieves the highest top-1 accuracy (41.2%), outperforming CNN+KAN and CNN+MLP while using fewer parameters than CNN+KAN. Notably, CNN+KAN and CNN+QKAN require substantial number of parameters due to their linear scaling with output size, 384k and 205k respectively, posing limitations for practical deployment. To address the scalability constraints of QKAN and KAN on high-dimensional tasks, we introduce Hybrid QKAN (HQKAN), which incorporates two additional fully connected layers to form an autoencoder-like structure around compact QKAN core. By compressing features into small latent space, HQKAN leverages QKANs expressivity while significantly reducing parameter count. As demonstrated in Table 3, HQKAN achieves competitive accuracy with an order-ofmagnitude reduction in parameters. For example, on CIFAR-100, HQKAN attains top-5 accuracy of 70.6% using only 32,636 parameters, compared to 86,948 and 384,000 required by MLP and KAN models, respectively. On CIFAR-10, HQKAN not only outperforms both MLP and KAN in top-1 and top-5 accuracy but does so with the fewest parameters (14,370). This result underscores the practicality of HQKAN for memoryand compute-constrained environments. Together, these results validate the efficacy and versatility of QKANs and HQKANs across classification tasks of varying complexity. They demonstrate that QVAFs, particularly in the single-qubit data re-uploading circuit, DARUAN, provide an expressive, scalable, and Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Figure 3: GPT-2 model trained on the WebText dataset [Radford et al., 2019], incorporating HQKAN and MLP layers. To further reduce parameter count and improve efficiency, we adopt the hybrid QKAN (HQKAN) strategy, where the input and output dimensions fed into QKAN are compressed via fully connected layers, forming an autoencoder-like bottleneck. This compression enables QKAN to operate effectively in low-dimensional latent space with reduced overhead. Parameter sizes are indicated in square brackets. HQKAN achieves better perplexity performance than MLP while using only one-third of its parameters and the same training time, demonstrating the effectiveness of QKAN-based architectures for generative modeling on classical hardware. hardware-efficient approach to deep learning. We further evaluate the generative modeling capability of QKANs by integrating them into autoregressive language models, specifically the GPT-2 architecture. For preliminary testing, we utilize an open-source kan-gpt module [Ganesh, 2024]. In this approach, we replace all the linear layers within the transformer blocks with HQKANs we introduced earlier in classification tasks to address the scaling problem in KANs architecture. Subsequently, we pretrain the resulting models on the WebText dataset [Radford et al., 2019]. Figure 3 presents the perplexity curves during training of GPT-2 models equipped with MLP and HQKAN modules. Despite operating in reduced-dimensional latent space, HQKAN consistently achieves superior convergence and reduced final perplexity compared to the MLP baseline. This is achieved while taking only one-third of the parameters and 30% less memory during training time. To complement the generative modeling results presented in the main text, we provide detailed runtime and memory benchmarks for GPT-2 models incorporating QKAN-based components. In particular, we aim to assess the scalability of these models under large-batch training regimes, as large batch sizes have been shown to significantly improve performance in LLMs with scaling laws [Brown et al., 2020, Shuai et al., 2024]. Table 4 summarizes the performance of various GPT-2 configurations on the WebText 14 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks dataset. The top section of the table (above the second double midrule) evaluates models using the kan-gpt framework, where the linear layers in GPT-2 are replaced by HQKANs. Models marked with an asterisk () correspond to those visualized in Figure 3 of the main text. We measure the iteration time and memory consumption for each methods. The lower section of the table reports results for the KANsformer architecture [Xie et al., 2024], which integrates HQKAN modules directly into the feedforward layers of transformer blocks, with flash attention [Dao et al., 2022]. Flash attention significantly reduces memory usage and improves speed. We report performance at various batch sizes to demonstrate the scalability and hardware compatibility of QKAN-based architectures under realistic training regimes. Table 4 highlights the runtime and memory efficiency of HQKAN-based GPT-2 models. On single RTX 4090, HQKAN achieves comparable speed to standard MLPs (60 ms vs. 63 ms per iteration), while reducing memory consumption by over 35% (4.1 GB vs. 6.5 GB). V100S GPUs at batch size 48, HQKAN maintains efficiency with only When scaled to 4 marginal overhead (4,648 ms vs. 4,536 ms), despite threefold reduction in model size (40M vs. 124M parameters). Nevertheless, the device memory consumption and training time reveal scaling limitations for the kan-gpt method, which restricts its practicality for very large models. To address this issue, we turn to KANsformer architecture with flash attention, and the results are shown in the lower section of Table 4. This architecture provides further memory savings and speed improvements, enabling scalability to larger training regimes. At large batch sizes (e.g., 320), HQKANsformer reduces device memory from 656 GB to 592 GB compared to MLP, while also lowering per-iteration time (6,400 ms vs. 5,760 ms on total H200, HQKANsformer completes iterations with H100 times). Even at batch size 800 on 16 both 10% less memory and 10% less time, demonstrating its efficiency at scale. In summary, HQKANsformer effectively overcomes the scalability bottleneck of the kan-gpt approach, making it practical solution for training foundation models at large scale. Overall, the results demonstrate that HQKAN can serve as an effective, efficient, and scalable plug-and-play replacement for classical fully connected layers in generative transformer architectures. Its parameter efficiency and faster convergence open the door to practical quantum-inspired modeling in large-scale natural language processing tasks. Additionally, to investigate the compatibility between quantum-inspired and classical KAN, we present knowledge distillation method to transfer learned parameters from trained QKAN to corresponding KAN (see Section 4.3 for details). This process enables QKANs to serve as pretraining mechanism for KANs, potentially accelerating convergence and improving generalization in the classical regime. We illustrate this approach with toy function, (x, y) = sin(ex + y2), nonlinear expression with compositional structure. As shown in Figure 4, the QKAN is first trained close to convergence. Learned parameters are then converted into B-spline basis representations and transferred into structurally matched KAN. Owing to spline approximation errors during the conversion, the KAN continues training for limited number of epochs to fine-tune 15 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Table 4: Performance comparison of GPT-2 models on the WebText dataset [Radford et al., 2019]. This table presents the runtime and memory usage for various GPT-2 configurations equipped with QKAN-based modules. The top section (above the second double midrule) corresponds to models using the kan-gpt framework introduced in the main text, where all linear layers are replaced by HQKAN. The models marked with an asterisk () are those displayed in Figure 3. Results include training speed (ms per iteration) and memory consumption. The lower section (below the second double midrule) reports large-scale results using the KANsformer [Xie et al., 2024] architecture with flash attention [Dao et al., 2022], which integrates HQKAN directly into the feed-forward network of transformer block instead of all linear layers. We benchmark across batch sizes and hardware configurations, including singleand multi-GPU setups with NVIDIA RTX 4090, V100S, H100, and H200 GPUs. These configurations demonstrate the scalability and practical feasibility of HQKAN-based models in both singleand distributed-training regimes. Method # Params Batch size Time/iter [GPU MLP-based GPT w/o flash atten. HQKAN-gpt w/o flash atten. MLP-based GPT w/ flash atten. 124 41 124 HQKANsformer w/ flash atten. 67 *1 48 *1 48 1 10 48 320 800 1 10 48 320 800 63 [RTX 4090 4,536 [V100S 60 [RTX 4090 4,648 [V100S 40 [RTX 4090 252 [RTX 4090 3,884 [V100S 6,400 [H100 14,784 [H200 39.5 [RTX 4090 240 [RTX 4090 3,540 [V100S 5,760 [H100 13,232 [H200 16 ms] Device memory ms] ms] ms] ms] ms] ms] ms] ms] ms] ms] ms] ms] ms] ms] 6.5 GB 111 GB 4.1 GB 120 GB 4.7 GB 17.3 GB 81.2 GB 656 GB 1,340 GB 3.8 GB 15.6 GB 73.6 GB 592 GB 1,224 GB Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Figure 4: Knowledge distillation from QKAN to KAN. We consider the regression task (x, y) = sin(ex + y2). QKAN is first trained for 500 epochs, after which its learned variational parameters are converted into B-spline coefficients and transferred into KAN of matching architecture. Due to approximation errors during the conversion, small loss shift is observed, which is corrected through continued training. The resulting QKAN-initialized KAN achieves 70% reduction in test loss compared to KAN trained from scratch, demonstrating the effectiveness of QKAN as pretraining strategy for classical KAN. the inherited parameters. This transfer approach yields substantial performance gain: the transferred KAN achieves 70% lower test loss compared to baseline KAN trained from scratch under identical conditions. These results suggest that QKAN can serve as powerful initialization strategy for KAN, effectively bootstrapping their learning via QVAFs."
        },
        {
            "title": "3 Discussion",
            "content": "In this work, we introduce QKANs, novel framework that integrates QVAFs into the classical KAN architecture. The core of this innovation lies in the design of QVAFs based on single-qubit data re-uploading circuits with trainable data pre-processing weights, which form the fundamental building blocks of our proposed DARUAN. DARUANs enable QKANs to approximate complex nonlinear mappings using significantly fewer parameters than traditional methods. QVAFs offer compelling perspective on variational quantum circuits, not only as data encoders or feature maps, but also as expressive, learnable nonlinearities embedded directly within neural architectures. By systematically organizing DARUAN into KAN layers, QKANs achieve an significant increase in functional expressivity per parameter with repetition, all while maintaining compatibility with the current hardware constraints of the NISQ era. 17 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Our architecture not only preserves the structured interpretability of classical KANs but also enhances it with the power of quantum-inspired DARUAN. Empirical evaluations across regression, classification, and generative modeling tasks consistently show that QKANs outperform or rival both MLPs and traditional KANs, despite using fewer parameters and reduced training overhead. To improve scalability and adaptability, we further introduced layer extension and HQKANs, allowing QKANs to flexibly accommodate various task complexities. These extensions broaden the models applicability without compromising its foundational design. Though, due to the scope of this paper, which mainly focuses on the effectiveness of QKAN over classical KAN and MLP, we only benchmark QKAN with well-established models, instead of SOTA models on any particular task. Our results still verify that QKANs on par with MLP can serve as versatile backbone models across the spectrum of classical architecturesranging from simple to complexand consistently outperform classical KAN and MLP in terms of parameter efficiency with comparable or even better performance. Importantly, QKANs are constructed from modular components that are readily implementable on current quantum devices, particularly leveraging single-qubit operations on NISQ devices [Smith et al., 2025, Rower et al., 2024]. The possibility of transferring trained QKAN parameters to classical KANs enables seamless hybridization of quantum and classical pipelines, offering practical benefits even before full-scale quantum hardware becomes widely accessible. During the final stage of our manuscript revision, we became aware of several recent studies that also explore similar idea on QVAFs in KANs [Werner et al., 2025, Wakaura et al., 2025b,a]. These works propose alternative VQC ansatz to serve as activation functions within the KAN framework, aligning with our formulation of QVAFs. However, key distinction lies in the quantum resource requirements of these approaches. The aforementioned studies utilize multi-qubit circuits for their VQCs, which presents three major scalability challenges. First, the current limitations of quantum hardware, as well as the computational cost of classical quantum simulators, restrict the practical deployment of models with large qubit counts. Second, and more fundamentally, scaling up multi-qubit VQCs often leads to the emergence of barren plateaus, regions in the optimization landscape where the gradient vanishes exponentially with system size, making the training of large-scale quantum neural networks exceedingly difficult [McClean et al., 2018]. Not to mention the fidelity, or error rate, of the two-qubit gate on NISQ devices is still challenging to realize deep circuits [Preskill, 2018, Singh et al., 2024, Smith et al., 2025], which leads to inscalability of the models. These limitations significantly hinder the scalability and practicality of multi-qubit QVAF-based KANs. Third, existing models still depend on access to real quantum hardware when scaling up, both during training and inference. Such real devices remain scarce and must typically be accessed through remote servers, introducing latency that is particularly detrimental for real-time tasks. In contrast, our proposed architecture, QKAN, employs only single-qubit data re-uploading circuits for its VAFs. These circuits are lightweight and efficient enough to be simulated on classical quantum simulators, thereby removing the reliance on real quantum 18 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks devices during inference. This design not only enables efficient implementation on near-term quantum devices but also mitigates the risk of barren plateaus, providing more scalable and robust path toward quantum-enhanced machine learning. In summary, QKANs represent significant step toward practical quantum-enhanced learning by integrating QVAFs and KANs within scalable and computationally efficient framework. Crucially, this quantum-inspired design is already executable on classical quantum simulator and demonstrates readiness for large-scale deployment. Our work thus establishes QKANs as bridge between interpretable classical architectures and future quantum acceleration, offering blueprint for next-generation machine learning models."
        },
        {
            "title": "4 Methods",
            "content": "This section provides detailed overview of the core methods proposed and employed in this work."
        },
        {
            "title": "4.1 Kolmogorov-Arnold Networks",
            "content": "KANs are designed to approximate multivariate functions using compositions of univariate functions and addition, establishing themselves as universal approximators with high interpretability and efficiency. Kolmogorov-Arnold representation theorem. KART [Kolmogorov, 1957] states that any multivariate continuous function (x1, . . . , xN ) can be represented as finite composition of univariate functions and addition: (x) = 2N +1 (cid:88) Φq (cid:32) (cid:88) q=1 p=1 (cid:33) ϕq,p(xp) , (4.1) and Φq : where ϕq,p : [0, 1] are continuous functions. While KART provides theoretical guarantees of universal approximation, the original functions can be non-smooth or hard to learn in practice [Girosi and Poggio, 1989, Poggio et al., 2020, Liu et al., 2024c], hence the need for parameterized and trainable alternatives. Formalism of Kolmogorov-Arnold networks. Liu et al. [2024c] introduced KANs as practical realization of the KART, generalizing it to deep and wide architectures. Each activation in KANs is modeled as learnable VAF parameterized by B-splines, which are piecewise polynomial functions capable of approximating any continuous function with arbitrary precision [Liu et al., 2024c, Douzette, 2017]. Formally, KAN layer maps the output of the l-th layer to the (l + 1)-th layer via: nl(cid:88) xl+1,j = ϕl,j,i(xl,i), (4.2) i=1 19 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks where ϕl,j,i is the learnable univariate VAF connecting input node to output node j. This can be expressed in matrix notation as: xl+1 = Φl(xl), Φl = ) ) ϕl,1,1( ϕl,2,1( ... ϕl,nl+1,1( ) ) ϕl,1,2( ϕl,2,2( ... ) ϕl,nl+1,2( ) (4.3) (4.4) ) . ) ) ϕl,1,nl( ϕl,2,nl( ... ϕl,nl+1,nl( . . . KAN with depth L, width , spline order k, and grid intervals requires (N 2LG) parameters [Liu et al., 2024c]. This is comparable to MLPs with (N 2L(G + 2L) ( , but KANs typically require significantly smaller k)) parameters for depth and width width , resulting in improved efficiency and generalization. N However, the computational cost of evaluating B-spline bases and rescaling grids can become computational bottleneck in practice [Li, 2024]. This motivates our exploration of quantum-inspired methods to replace spline-based activations with more efficient quantum circuit approximators."
        },
        {
            "title": "4.2 Data Re-Uploading Circuits",
            "content": "To address the computational challenges associated with B-spline activations in classical KANs, we leverage the expressive power of VQCs, particularly through data re-uploading circuits [Pérez-Salinas et al., 2020]. Data re-uploading circuits encode classical inputs via repeated parameterized embeddings into quantum states, alternating with trainable unitaries. In general, for an implementation with repetitions with n-dimensional input Rn, the circuit can be expressed as: (x, θ) = (r+1)S(x)W (r) S(x)W (1), (4.5) where each (ℓ) is trainable unitary and S(x) is the data-encoding operation. common Rn [Zhao et al., 2024], where instantiation is S(x denotes Hadamard product. In our QVAF task, the input data is limited to single ω + β) with trainable parameters ω, β dimension, which allows us to reduce the circuit to (x, θ) = (r+1)S(wrx + br)W (r) S(w1x + b1)W (1). (4.6)"
        },
        {
            "title": "4.3 Knowledge Distillation from QKANs to KANs",
            "content": "One of the key advantages of our quantum-inspired KANs is their ability to transfer learned VAFs to classical KANs after training, enabling deployment on classical hardware. Since each DARUAN unit approximates an univariate function, we can estimate its functional form and reparameterize it using classical spline or Fourier bases. The transfer procedure involves: 20 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks 1. Forward evaluation on quantum machine. After training, each DARUAN activation is evaluated across discretized input domain to sample its function output. 2. Classical or quantum coefficient estimation. Using the sampled outputs, we fit spline coefficients either classically or using quantum linear solvers such as the HarrowHassidimLloyd (HHL) algorithm [Harrow et al., 2009]. 3. VAF replacement. The estimated coefficients are used to define classical B-spline or Fourier basis functions that replace the DARUAN activations in the classical KAN. This strategy enables training on quantum hardware and inference on classical machines, facilitating hybrid deployment. Additionally, for Fourier-based replacements, coefficients can be estimated using either the discrete Fourier transform (DFT) or quantum Fourier transform (QFT), enabling flexible and efficient post-training adaptation."
        },
        {
            "title": "4.4 Distributed Training of QKANs",
            "content": "The architecture of QKANs is naturally suited for distributed quantum machine learning, due to the independence of each DARUAN activation. Each activation is implemented as single-qubit data re-uploading circuit, requiring no entanglement between qubits. For QKAN layer with input nodes and output nodes, n) independent qubits are required to compute the layer outputs in parallel. To total of (m process mini-batches of size b, we consider two distributed strategies: Synchronous parallelism: Execute (b batches in parallel. n) qubits simultaneously to process all ii Asynchronous parallelism: Run quantum systems independently, each with (m qubits, and perform asynchronous gradient updates. n) These distributed approaches are compatible with current quantum cloud infrastructures, which often impose qubit count limitations per device. As such, QKANs can be deployed on multiple quantum machines with classical communication links for parameter synchronization. Moreover, it is compatible to quantum federated learning (QFL) [Chen and Yoo, 2021], each quantum machine can independently compute gradients for its local data batch, and the results can be aggregated to update the global model parameters. This architecture aligns well with the emerging paradigm of quantum-centric supercomputing and enables quantum-accelerated learning via parallelized quantum computation, similar to tensor parallelism in classical deep learning frameworks. This also enables us to perform classical simulations efficiently on both personal computers (PCs) and HPCs, particularly for multi-node clusters that can handle large-scale training tasks, such as training LLMs with our QKAN. 21 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks"
        },
        {
            "title": "4.5 QKAN Implementation Methods",
            "content": "In this section, we will provide detailed overview of the implementation for QKAN. Layer extension. Analogous to the grid extension strategy used in KANs to refine approximation accuracy [Liu et al., 2024c], QKANs leverage technique termed layer extension, wherein the number of data re-uploading repetitions is progressively increased. This extension enriches the models frequency spectrum and improves its capacity to approximate complex univariate functions. Practically, layer extension for data re-uploading circuit can be interpreted as transition from shallower, pre-trained model to deeper one. Specifically, the learned parameters from the original model are retained, while the newly added parameterized unitaries in the extended layers are initialized to identity (i.e., their corresponding parameters are set to zero). In this configuration, the new encoding blocks S(x) do not contribute to the output, as the new parameterized unitaries act trivially on the quantum state. And it is easy to see that the results are remained the same after layer extension. As training proceeds, these newly introduced layers begin to adjust, allowing the extended DARUAN to fine-tune the overall function approximation without disrupting the performance already achieved by the shallower model. This approach provides systematic and stable method for incrementally increasing model expressivity while preserving convergence behavior. σz , is confined to the interval [ Post-activation process. The output of QKAN layer is intrinsically bounded due to the nature of quantum measurement. Specifically, the expectation value of single-qubit observable, such as 1, 1]. Consequently, for l-th QKAN layer receiving nl inputs and producing nl+1 outputs, the output domain is constrained to nl, nl]nl+1, assuming summation over independent channels. While this bounded output range is beneficial for stability, it can be limiting when modeling functions that require outputs beyond this interval. Increasing the number of hidden units to expand the range is not always practical due to parameter overhead and architectural constraints. [ To overcome this limitation, we explore two post-activation strategies: First, lightweight output mapping network, such as shallow MLP, can be appended to the QKAN. Given the universal approximation capabilities of MLPs, such model can flexibly rescale or reshape the QKAN output to meet task-specific requirements. This hybrid setup preserves the advantages of QKAN, including reduced parameter count and quantum-inspired expressivity, while enabling output adaptation for broader applications. Second, we incorporate learnable scaling and bias parameters directly into the QKAN output. These parameters act multiplicatively and additively on the expectation values produced by the quantum circuits, allowing direct rescaling without architectural expansion. This approach is both efficient and seamlessly integrates with the QKAN framework. Importantly, it keeps the compatibility of parameter transfer mechanisms from QKAN to classical 22 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks KAN, preserving interoperability between quantum and classical VAFs. Together, these methods ensure that QKANs remain scalable and adaptable across variety of tasks. Moreover, they highlight QKANs flexibility as modular component that can be combined with classical models to enhance performance or meet specific output constraints. These strategies enhance the practical use of QKAN and pave the way for integrating QVAFs with conventional learning pipelines. Base activation function. To further enhance training stability and optimization in QKAN, we incorporate base activation function, inspired by similar mechanism in the original KAN framework [Liu et al., 2024c]. This base activation acts analogously to residual connection, providing direct and smooth signal path during learning. The output with the residual activation function is defined as: ϕ(x) = wbb(x) + wd (x, θ) where wb and wd are learnable weights, and b(x) denotes the base activation, chosen here as the SiLU function, i.e., b(x) = sigmoid(x). (x, θ) 0 0 , (4.7) This residual pathway ensures that even if in the early stages of training, when VAFs and QVAFs may be poorly initialized, the model still has access to smooth nonlinear function. As result, the learning landscape is improved, and optimization becomes more robust. This method is particularly beneficial in deeper QKANs or settings with limited data, where maintaining gradient flow is critical."
        },
        {
            "title": "4.6 Numerical Methods",
            "content": "Function regression with heuristic architectures. Ten multivariate equations are drawn from the Feynman dataset [Udrescu and Tegmark, 2020, Udrescu et al., 2020]. For 1, 1)d are sampled uniformly and noisy targets are generated each function , inputs xi as ( yi = (xi) + ϵ, ϵ (0, 0.1 µf ) , where µf is the mean value of (x) over the input domain. Each dataset comprises 1,000 training and 1,000 test points. Three model classes are compared: i. KAN. Hidden-layer shapes are adopted from the best settings reported in Table 2 of Liu et al. [2024c]. The grid number is extended over 5, 10, . . . , { . } ii. QKAN. The same layer counts and widths as in the corresponding KANs are employed. Data re-uploading repetition number is extended over 3, 6, . . . , { . } iii. MLP. fixed hidden-layer width of 5 is used, with depths chosen from 3, 5, 10 { } . 23 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks All parameters are initialized at random and optimized via L-BFGS [Liu and Nocedal, 1989] for 200 epochs. For each combination of architecture and extension hyperparameter, five independent runs are performed; the lowest test RMSE is reported in Tables 1 and 2. Function regression with empirical architectures. To further assess performance without prior knowledge of layer shapes, broader benchmark of 66 equations is selected from the Feynman dataset (see Appendix Table 5). Datasets are constructed as above, with inputs restricted to avoid singularities and 10% additive noise. Model families and hyperparameter sweeps are defined as follows: i. KAN. Five hidden-layer depths number { 5, 10, 15, 20, 25 . } 2, 3, 4, 5, 6 } { are tested with fixed width 5, and grid ii. QKAN. Depths match those of KAN, with width 5; data re-uploading repetition number 3, 6, 9, 12, 15 . } { iii. MLP. Width is fixed to 5, and depths vary over 2, 3, 4, 5, 6 { . } Training is performed as above. For each hyperparameter combination, five random seeds are used and the best test RMSE is recorded. Results are presented in Figure 5. Image classification. We evaluate the classification performance of QKAN, KAN, and MLP modules on three datasets: MNIST [Deng, 2012], CIFAR-10, and CIFAR-100 [Krizhevsky and Hinton, 2009]. MNIST consists of 28 28 grayscale digits, while CIFAR-10 and CIFAR32 RGB natural images. All datasets are normalized such that pixel 100 consist of 32 values are centered at 0.5 with standard deviation of 0.5. For each dataset, we construct CNN backbone comprising either two (for MNIST) or three layers (for CIFAR-10 and CIFAR-100), each consisting of 2D convolutional layer, followed by ReLU activation and 2D max-pooling layer. The resulting feature maps are then flattened and passed to FCN, instantiated as either an MLP, KAN, or QKAN. All models are trained for 100 epochs using the Adam optimizer [Kingma and Ba, 2014] with learning rate of 103 and batch size of 1000. The baseline CNN+MLP uses fixed hidden width of 5 and serves as standard reference. For QKAN, we sweep over multiple CNN output sizes and QKAN configurations, fixing the re-uploading repetition number = 3 and selecting the best performing combination. KAN models are then configured to match the chosen CNN features and evaluated with fixed grid number of = 10. The total parameter count of the convolutional backbone is reported separately from the FCN to clearly illustrate the contribution of the representation module. We conduct all evaluations using the same initialization and training setup for consistency. In addition to standard QKAN architectures, we design parameter-efficient variant referred to as HQKAN. HQKAN introduces two auxiliary fully connected layers before and after the QKAN block, respectively, functioning as feature compressor and expander. This 24 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks design mimics an autoencoder structure, wherein the high-dimensional feature vector is downscaled to latent representation whose size is logarithmic in the original dimension. The compressed features are processed by the QKAN layer, which benefits from its high expressivity even in small latent spaces, before being upscaled again to match the required output dimension. During training, the latent dimension is chosen based on the logarithm of the original input size and output size to QKAN. All other training configurations, including optimizer, learning rate, and batch size, are kept consistent with those used for the main QKAN and KAN experiments. This setup enables direct comparisons in both performance and model compactness. Natural language generation. To assess the generative modeling capabilities of QKANs and their variants, we utilize the open-source kan-gpt module [Ganesh, 2024], which replaces all linear layers in the GPT architecture with either QKAN modules. For the implementation of QKANsformer, we modified an open-source nanoGPT [Karpathy, 2022] project and customized the feed-forward network to utilize HQKANs. Our experiments are conducted on the WebText dataset [Radford et al., 2019], with GPT-2 as the backbone model. The GPT-2 architecture consists of 12 transformer layers, each comprising 12 attention heads, with an embedding dimension of 768. All models are evaluated using perplexity as the primary metric. Training is performed over 2000 epochs using the AdamW optimizer [Loshchilov and Hutter, 2017] with batch size of 1, learning rate of 5 103, and momentum parameters β1 = 0.9, β2 = 0.95. weight decay of 0.1 is applied to all matrix multiplication weights in both linear and QKAN-based layers. For large-scale batch size, models are benchmarked using multi-GPU clusters setups, including 4V100S (1 node), 32H100 (4 nodes), and 16H200 (2 nodes) configurations. GPUs in each node are interconnected in pairs using NVLink bridges or NVSwitch technologies, facilitating efficient data transfer within nodes. In the multi-node training setup, each node is interconnected via InfiniBand (IB), enabling high-throughput and low-latency communication essential for efficient distributed learning. Transferring VAFs from QKAN to KAN. We design two-phase training strategy: (1) QKAN is trained on target regression task for 500 epochs, and (2) its learned activation parameters are mapped to classical B-spline basis and loaded into KAN of equivalent depth and width. Due to mismatch between the QKANs Fourier-based activations and KANs B-spline basis, small loss offset appears upon transfer. To mitigate this, the KAN continues training from the transferred parameters for an additional 1000 epochs. For comparison, we train KAN from scratch under the same architecture and with fixed total training epochs. The grid size is set to 5 in both models, while data re-uploading repetition number is set to 3 for QKAN. We evaluate mean squared error over held-out test 25 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks set of 1,000 samples drawn from the input domain (x, y) using the L-BFGS optimizer with identical learning settings across all models. 1, 1]2. Training is performed ["
        },
        {
            "title": "Data and Code Availability",
            "content": "The data and code used in this study, implemented using PyTorch [Ansel et al., 2024], is available at: https://github.com/Jim137/qkan [Jiang, 2025]."
        },
        {
            "title": "Acknowledgements",
            "content": "J.-C. Jiang would like to thank Qian-Rui Lee, Damien Jian, Chen-Yu Liu and Dr. Samuel Yen-Chi Chen for helpful discussions and comments. J.-C. Jiang also thanks the National Center for High-Performance Computing (NCHC), National Institutes of Applied Research (NIAR), Taiwan, for providing computational and storage resources supported by the National Science and Technology Council (NSTC), Taiwan, under Grants No. NSTC 114-2119-M-007013. H.-S. Goan acknowledges support from the NSTC, Taiwan, under Grants No. NSTC 113-2112-M-002-022-MY3, No. NSTC 113-2119-M-002-021, No. 114-2119-M-002-018, No. NSTC 114-2119-M-002-017-MY3, from the US Air Force Office of Scientific Research under Award Number FA2386-23-1-4052 and from the National Taiwan University under Grants No. NTU-CC-114L8950, No. NTU-CC114L895004 and No. NTU-CC-114L8517. H.-S. Goan is also grateful for the support of the Center for Advanced Computing and Imaging in Biomedicine (NTU-114L900702) through the Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education (MOE), Taiwan, the support of Taiwan Semiconductor Research Institute (TSRI) through the Joint Developed Project (JDP) and the support from the Physics Division, National Center for Theoretical Sciences, Taiwan."
        },
        {
            "title": "Author Contribution",
            "content": "The project was conceived by J.-C.J. The theoretical aspects of this work were developed by Y.C.H. The numerical experiments were conducted by J.-C.J. The project is supervised by H.-S.G. All authors contributed to technical discussions and writing of the manuscript."
        },
        {
            "title": "Conflict of Interest",
            "content": "The authors declare no competing interests. 26 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks"
        },
        {
            "title": "References",
            "content": "Mohamed Abd Elaziz, Ibrahim Ahmed Fares, and Ahmad O. Aseeri. Ckan: Convolutional kolmogorovarnold networks model for intrusion detection in iot environment. IEEE Access, 12:134837134851, 2024. doi: 10.1109/ACCESS.2024.3462297. 3 Alireza Afzal Aghaei. fKAN: Fractional kolmogorov-arnold networks with trainable jacobi basis functions, 2024. URL https://arxiv.org/abs/2406.07456. 3 Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks, 2015. URL https://arxiv.org/abs/1412.6830. 4 Jason Ansel et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS 24, page 929947, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703850. doi: 10.1145/3620665.3640366. 26, 42 Andrea Apicella, Francesco Donnarumma, Francesco Isgrò, and Roberto Prevete. survey on modern trainable activation functions. Neural Networks, 138:1432, June 2021. ISSN 0893-6080. doi: 10.1016/j.neunet.2021.01.026. URL http://dx.doi.org/10.1016/j.neunet. 2021.01.026. Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549(7671):195202, September 2017. ISSN 1476-4687. doi: 10.1038/nature23474. URL http://dx.doi.org/10.1038/nature23474. 2 Zavareh Bozorgasl and Hao Chen. Wav-kan: Wavelet kolmogorov-arnold networks, 2024. URL https://arxiv.org/abs/2405.12832. 3 Tom B. Brown et al. Language models are few-shot learners, 2020. URL https://arxiv.org/ abs/2005.14165. 14 Samuel Yen-Chi Chen and Zhiding Liang. Introduction to quantum machine learning and quantum architecture search, 2025. URL https://arxiv.org/abs/2504.16131. 2 Samuel Yen-Chi Chen and Shinjae Yoo. Federated quantum machine learning. Entropy, 23(4), 2021. ISSN 1099-4300. doi: 10.3390/e23040460. URL https://www.mdpi.com/ 1099-4300/23/4/460. 21 Samuel Yen-Chi Chen, Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Xiaoli Ma, and HsiSheng Goan. Variational quantum circuits for deep reinforcement learning. IEEE Access, 8:141007141024, 2020. doi: 10.1109/ACCESS.2020.3010470. 3 27 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Carlo Ciliberto et al. Quantum machine learning: classical perspective. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474(2209): 20170551, January 2018. ISSN 1471-2946. doi: 10.1098/rspa.2017.0551. URL http: //dx.doi.org/10.1098/rspa.2017.0551. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv. org/abs/2205.14135. 15, 16 Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6):141142, 2012. doi: 10.1109/ MSP.2012.2211477. 12, 24 Raghavendra Devadas and Sowmya T. Quantum machine learning: comprehensive review of integrating ai with quantum computing for computational advancements. MethodsX, 14:103318, 2025. ISSN 2215-0161. doi: https://doi.org/10.1016/j.mex.2025.103318. URL https://www.sciencedirect.com/science/article/pii/S2215016125001645. 2 Andre Sevaldsen Douzette. B-splines in machine learning. Masters thesis, 2017. 19 Vedran Dunjko, Jacob M. Taylor, and Hans J. Briegel. Quantum-enhanced machine learning. Phys. Rev. Lett., 117:130501, Sep 2016. doi: 10.1103/PhysRevLett.117.130501. URL https://link.aps.org/doi/10.1103/PhysRevLett.117.130501. 2 Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term processors, 2018. URL https://arxiv.org/abs/1802.06002. 3 Aditya Nalgunda Ganesh. Kan-gpt: The pytorch implementation of generative pre-trained transformers (gpts) using kolmogorov-arnold networks (kans) for language modeling, May 2024. URL https://github.com/AdityaNG/kan-gpt/. Release 1.0.0, 9th May 2024. 14, 25 Remi Genet and Hugo Inzirillo. Tkan: Temporal kolmogorov-arnold networks, 2024. URL https://arxiv.org/abs/2405.07344. 3 Federico Girosi and Tomaso Poggio. Representation properties of networks: Kolmogorovs theorem is irrelevant. Neural Comput., 1(4):465469, December 1989. ISSN 0899-7667. doi: 10.1162/neco.1989.1.4.465. URL https://doi.org/10.1162/neco.1989.1.4.465. 19 Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of equations. Physical Review Letters, 103(15), October 2009. ISSN 1079-7114. doi: 10.1103/physrevlett.103.150502. URL http://dx.doi.org/10.1103/PhysRevLett.103.150502. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International 28 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Conference on Computer Vision (ICCV), pages 10261034, 2015. doi: 10.1109/ICCV.2015. 123. 4 Amanda A. Howard, Bruno Jacob, Sarah H. Murphy, Alexander Heinlein, and Panos Stinis. Finite basis kolmogorov-arnold networks: domain decomposition for data-driven and physics-informed problems, 2024. URL https://arxiv.org/abs/2406.19662. 3 Matteo Antonio Inajetovic, Filippo Orazi, Antonio Macaluso, Stefano Lodi, and Claudio Sartori. Enabling non-linear quantum operations through variational quantum splines. In Jiří Mikyška, Clélia de Mulatier, Maciej Paszynski, Valeria V. Krzhizhanovskaya, Jack J. Dongarra, and Peter M.A. Sloot, editors, Computational Science ICCS 2023, pages 177192, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-36030-5. Sofiene Jerbi, Lukas J. Fiderer, Hendrik Poulsen Nautrup, Jonas M. Kübler, Hans J. Briegel, and Vedran Dunjko. Quantum machine learning beyond kernel methods. Nature Communications, 14(1), January 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-36159-y. URL http://dx.doi.org/10.1038/s41467-023-36159-y. 3 Jiun-Cheng Jiang. Quantum-inspired kolmogorov-arnold network, 2025. URL https://github. com/Jim137/qkan. 26 Andrej Karpathy. nanogpt, 2022. URL https://github.com/karpathy/nanoGPT. 25 Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL https://api.semanticscholar.org/CorpusID:6628106. Andrei Nikolaevich Kolmogorov. On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition. In Doklady Akademii Nauk, volume 114, pages 953956. Russian Academy of Sciences, 1957. 19 Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. URL https: //www.cs.toronto.edu/kriz/learning-features-2009-TR.pdf. 12, 24 Chenxin Li et al. U-kan makes strong backbone for medical image segmentation and generation, 2024a. URL https://arxiv.org/abs/2406.02918. 3 Shaozhi Li, Sabbir Salek, Yao Wang, and Mashrur Chowdhury. Quantum-inspired activation functions and quantum chebyshev-polynomial network, 2024b. URL https: //arxiv.org/abs/2404.05901. 5 Ziyao Li. Kolmogorov-arnold networks are radial basis function networks, 2024. 3, Chen-Yu Liu et al. Quantum-train: Rethinking hybrid quantum-classical machine learning in the model compression perspective, 2024a. URL https://arxiv.org/abs/2405.11304. 2 29 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503528, 1989. URL https://api.semanticscholar. org/CorpusID:5681609. 24 Ziming Liu, Pingchuan Ma, Yixuan Wang, Wojciech Matusik, and Max Tegmark. Kan 2.0: Kolmogorov-arnold networks meet science, 2024b. URL https://arxiv.org/abs/2408.10205. 3, 10 Ziming Liu et al. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756, 2024c. URL https://arxiv.org/abs/2404.19756. 3, 4, 6, 7, 10, 19, 20, 22, 23, 34 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. URL https://api.semanticscholar.org/ CorpusID:53592270. 25 Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, and Nathan Killoran. Transfer learning in hybrid classical-quantum neural networks. Quantum, 4:340, October 2020. ISSN 2521-327X. doi: 10.22331/q-2020-10-09-340. URL http://dx.doi.org/10.22331/ q-2020-10-09-340. 3 Jarrod McClean, Sergio Boixo, Vadim Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren plateaus in quantum neural network training landscapes. Nature communications, 9(1):4812, 2018. 5, 9, 18 Johannes Jakob Meyer et al. Exploiting symmetry in variational quantum machine learning. PRX Quantum, 4(1), March 2023. ISSN 2691-3399. doi: 10.1103/prxquantum.4.010328. URL http://dx.doi.org/10.1103/PRXQuantum.4.010328. 2 K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum circuit learning. Physical Review A, 98(3), September 2018. ISSN 2469-9934. doi: 10.1103/physreva.98.032309. URL http://dx.doi.org/10.1103/PhysRevA.98.032309. 3, Alejandro Molina, Patrick Schramowski, and Kristian Kersting. Padé activation units: End-to-end learning of flexible activation functions in deep networks. In International Conference on Learning Representations, 2019. 4 NVIDIA, Péter Vingelmann, and Frank H.P. Fitzek. Cuda, release: 10.2.89, 2020. URL https://developer.nvidia.com/cuda-toolkit. 42 Frank Phillipson. Quantum machine learning: Benefits and practical examples. In QANSWER, pages 5156, 2020. 2 Tomaso Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks. Proceedings of the National Academy of Sciences, 117(48):3003930045, 2020. 19 30 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks John Preskill. Quantum Computing in the NISQ era and beyond. Quantum, 2:79, August 2018. ISSN 2521-327X. doi: 10.22331/q-2018-08-06-79. URL https://doi.org/10.22331/ q-2018-08-06-79. 18 Adrián Pérez-Salinas, Alba Cervera-Lierta, Elies Gil-Fuster, and José I. Latorre. Data re-uploading for universal quantum classifier. Quantum, 4:226, February 2020. ISSN 2521327X. doi: 10.22331/q-2020-02-06-226. URL http://dx.doi.org/10.22331/q-2020-02-06-226. 3, 5, 20 Alec Radford, Jeff Wu, and Jong Wook Kim. gpt-2-output-dataset. https://github.com/ openai/gpt-2-output-dataset, 2019. 14, 16, 25 Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017. URL https://arxiv.org/abs/1710.05941. 4 David A. Rower et al. Suppressing counter-rotating errors for fast single-qubit gates with fluxonium. PRX Quantum, 5:040342, Dec 2024. doi: 10.1103/PRXQuantum.5.040342. URL https://link.aps.org/doi/10.1103/PRXQuantum.5.040342. 5, 18 Simone Scardapane, Steven Van Vaerenbergh, Simone Totaro, and Aurelio Uncini. Kafnets: Kernel-based non-parametric activation functions for neural networks. Neural Networks, 110:1932, 2019. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2018.11.002. URL https://www.sciencedirect.com/science/article/pii/S0893608018303174. 4 Maria Schuld and Nathan Killoran. Quantum machine learning in feature hilbert spaces. Phys. Rev. Lett., 122:040504, Feb 2019. doi: 10.1103/PhysRevLett.122.040504. URL https://link.aps.org/doi/10.1103/PhysRevLett.122.040504. Maria Schuld, Ryan Sweke, and Johannes Jakob Meyer. Effect of data encoding on the expressive power of variational quantum-machine-learning models. Physical Review A, 103(3), March 2021. ISSN 2469-9934. doi: 10.1103/physreva.103.032430. URL http: //dx.doi.org/10.1103/PhysRevA.103.032430. 3, 5, 6, 36 Seyd Teymoor Seydi. Exploring the potential of polynomial basis functions in kolmogorovarnold networks: comparative study of different groups of polynomials, 2024a. URL https://arxiv.org/abs/2406.02583. 3 Seyd Teymoor Seydi. Unveiling the power of wavelets: wavelet-based kolmogorov-arnold network for hyperspectral image classification, 2024b. URL https://arxiv.org/abs/2406. 07869. 3 Xian Shuai, Yiding Wang, Yimeng Wu, Xin Jiang, and Xiaozhe Ren. Scaling law for language models training considering batch size, 2024. URL https://arxiv.org/abs/2412.01505. 14 31 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Akhil Pratap Singh et al. Experimental demonstration of high-fidelity virtual two-qubit ISSN 2643-1564. doi: 10.1103/ gate. Physical Review Research, 6(1), March 2024. physrevresearch.6.013235. URL http://dx.doi.org/10.1103/PhysRevResearch.6.013235. 18 M. C. Smith, A. D. Leu, K. Miyanishi, M. F. Gely, and D. M. Lucas. Single-qubit gates with errors at the 107 level. Phys. Rev. Lett., 134:230601, Jun 2025. doi: 10.1103/42w2-6ccy. URL https://link.aps.org/doi/10.1103/42w2-6ccy. 5, 18 Sidharth SS, Keerthana AR, Gokul R, and Anas KP. Chebyshev polynomial-based kolmogorov-arnold networks: An efficient architecture for nonlinear function approximation, 2024. URL https://arxiv.org/abs/2405.07200. 3 Hoang-Thang Ta. Bsrbf-kan: combination of b-splines and radial basis functions in kolmogorov-arnold networks, 2024. URL https://arxiv.org/abs/2406.11173. 3 Silviu-Marian Udrescu and Max Tegmark. AI Feynman: Physics-Inspired Method for Symbolic Regression. Sci. Adv., 6(16):eaay2631, 2020. doi: 10.1126/sciadv.aay2631. 11, Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark. Ai feynman 2.0: pareto-optimal symbolic regression exploiting graph modularity. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. 11, 23 Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, and Màrius Caus. Kolmogorov-arnold networks (kans) for time series analysis, 2024. URL https://arxiv.org/abs/2405.08790. 3 Noah L. Wach, Manuel S. Rudolph, Fred Jendrzejewski, and Sebastian Schmitt. Data re-uploading with single qudit. Quantum Machine Intelligence, 5(2), August 2023. ISSN 2524-4914. doi: 10.1007/s42484-023-00125-0. URL http://dx.doi.org/10.1007/ s42484-023-00125-0. 3 Hikaru Wakaura, Rahmat Mulyawan, and Andriyan B. Suksmono. Adaptive variational quantum kolmogorov-arnold network, 2025a. URL https://arxiv.org/abs/2503.21336. 18 Hikaru Wakaura, Rahmat Mulyawan, and Andriyan B. Suksmono. Enhanced variational quantum kolmogorov-arnold network, 2025b. URL https://arxiv.org/abs/2503.22604. 18 Yannick Werner, Akash Malemath, Mengxi Liu, Vitor Fortes Rey, Nikolaos Palaiodimopoulos, Paul Lukowicz, and Maximilian Kiefer-Emmanouilidis. Qukan: quantum circuit born machine approach to quantum kolmogorov arnold networks, 2025. URL https://arxiv.org/ abs/2506.22340. Xinke Xie, Yang Lu, Chong-Yung Chi, Wei Chen, Bo Ai, and Dusit Niyato. Kansformer for scalable beamforming, 2024. URL https://arxiv.org/abs/2410.20690. 15, 16 32 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Xinchao Wang Xingyi Yang. Kolmogorov-arnold transformer. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= BCeock53nt. 3 Jinfeng Xu et al. Fourierkan-gcf: Fourier kolmogorov-arnold network an effective and efficient feature transformation for graph collaborative filtering, 2024a. URL https: //arxiv.org/abs/2406.01034. Kunpeng Xu, Lifei Chen, and Shengrui Wang. Kolmogorov-arnold networks for time series: Bridging predictive power and interpretability, 2024b. URL https://arxiv.org/abs/2406. 02496. 3 Runpeng Yu, Weihao Yu, and Xinchao Wang. Kan or mlp: fairer comparison, 2024a. URL https://arxiv.org/abs/2407.16674. 10 Zhan Yu, Hongshun Yao, Mujin Li, and Xin Wang. Power and limitations of single-qubit native quantum neural networks, 2022. URL https://arxiv.org/abs/2205.07848. 3, 9 Zhan Yu et al. Non-asymptotic approximation error bounds of parameterized quantum circuits, 2024b. URL https://arxiv.org/abs/2310.07528. Jiaming Zhao, Wenbo Qiao, Peng Zhang, and Hui Gao. Quantum implicit neural representations. arXiv preprint arXiv:2406.03873, 2024. 9, 20 33 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks"
        },
        {
            "title": "A Supplementary Theoretical Backgrounds",
            "content": "A.1 Proof of Theorem 2.1 Proof of Theorem 2.1. Our proof closely follows Liu et al. [2024c]. We replace the B-splines with Fourier series with highest frequency K. By the classical Fourier approximation theory Φl are (k + 1)-times continuously differentiable functions, we and the fact that the ϕl,j,i such that for any know that there exist trigonometric polynomial approximations ϕK 0 l,j,i ΦK k: (cid:13) (cid:13)(ϕl,j,i Φl1 Φ1)(x) (ϕK l,j,i Φl1 Φ1)(x)(cid:13) (cid:13)Cm CK (k+1m), (A.1) with constant independent of K. We fix these Fourier approximations. Therefore, we have that the residual Rl defined via: Rl = (ΦK ΦK Φl Φl1 (ΦK l+1 ΦK l+1 ΦK Φl1 Φ1)(x) Φ1)(x) satisfies: Rl Cm CK (k+1m), (A.2) (A.3) with constant independent of K. The total approximation error is the summation of residuals. (ΦK ΦK L1 ΦK 1 )(x) = RL + RL1 + + R1, (A.4) Therefore, we have (cid:13) (cid:13)f (ΦK 1 )(x)(cid:13) ΦK (cid:13)Cm (cid:88) l=1 Rl Cm CLK (k+1m) . (A.5) Since is fixed, we can absorb it into the constant C, yielding (cid:13) (cid:13)f (ΦK 1 )(x)(cid:13) ΦK (cid:13)Cm CK (k+1m) . (A.6) This completes the proof. A.2 Proof of Theorem 2.2 We restate the Theorem 2.2 from the main paper for completeness and provide the detailed proof as follows. 34 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Theorem 2.2 (Spectrum expansion and approximation error of QKAN with linear layer). 1, consider two Fix an integer single-qubit data re-uploading circuits: k+1[0, 1]. For any depth 0 and target function (A) Baseline (Un-weighted). (x) = (r+1)(cid:2)S(x) (r)(cid:3) S(x) = eixH, (cid:2)S(x) (1)(cid:3), where (ℓ) = (ℓ)(θℓ) is the ℓ-th parameterized unitary with θℓ being the set of trainable parameters and = 1 2σj is the Hermitian generator and σj (B) With classical linear layer. Let ω = (w1, . . . , wr) . } )r and set σx, σy, σz (0, { Uω(x) = (r+1) 1 (cid:89) (cid:2)S(wℓx) (ℓ)(cid:3), S(wℓx) = eiwℓxH. ℓ=r Define the model outputs fA(x) = fB(x) = (x) 0 ω(x) 0 U (x) Uω(x) , 0 . (a) Baseline spectrum. fA is trigonometric polynomial fA(x) = (cid:80)r α=r cαeiαx, hence the number of distinct non-zero frequencies is = 2r. ΩA (b) Linear-layer expansion. fB has spectrum ΩB = (cid:40) (cid:88) ℓ= mℓwℓ (cid:12) (cid:12) (cid:12) mℓ 1, 0, 1 { (cid:41) . } The number of distinct non-zero frequency satisfies 1 ΩB (3r 1). (c) m-norm approximation error. For 0 fA fB Cm Cm Cf (k+1m) Cf (k+1m) , KA := r, (cid:88) , KB := ℓ=1 wℓ. (d) Parameter efficiency vs. Fourier-series-based KAN. classical Fourier-seriesbased KAN requires = Θ(cid:0)ε1/(k+1m)(cid:1) parameters to reach error ε in norm. By choosing the geometric weights wℓ = 2ℓ1, we have KB = 2r 1 and fB Cm Cf 2r(k+1m). 35 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Solving Cf 2r(k+1m) = ε gives = (cid:25) (cid:24) log2(Cf /ε) + 1 = Θ(cid:0) log ε (cid:1), so the total number of trainable parameters is Θ(log 1 compared to Fourier-series-based KAN. ε ), an exponential reduction Proof of Theorem 2.2. The proof of (a) closely follows Schuld et al. [2021], specialized to the single-qubit case. Consider univariate quantum model fA(x) defined as the expectation value of an with respect to state prepared via parameterized quantum circuit (x): observable fA(x) = (x) is the initial state, and the circuit (x) = (r+1)S(x)W (r) where (2)S(x)W (1) consists of layers, each composed of data encoding gate S(x) = eixH and an arbitrary unitary operation . (x) 0 0 0 , (A.7) For single qubit we have S(x) = ei 2 σj , (A.8) { σx, σy, σz . Noting that we can always do singular value decomposition to an where σj Hermitian operator, namely the generator Hamiltonian has = σj 2 having Hs 2 = σz σz eigenvalues 2 , and V, terms can . So the data encoding gates become S(x) = eix be absorbed into the parameterized unitaries terms = . } 1 2 Next, we expand the quantum state (x) in terms of the eigenvalues of H. Denote multi-indices = (j1, j2, . . . , jr) 1, 2 and define the sums of eigenvalues 0 { } Λj = λj1 + λj2 + λ1 = 1 2, λ2 = + λjr, 1 2. (A.9) The components of the state are [U (x) ]i = 0 (cid:88) j{1,2}r eiΛj (r+1) ijr (2) j2j1 (1) j11. Similarly, the adjoint operation gives fA(x) = (x) = [U (x) 0 ]. Substituting into (A.7) yields (cid:88) ei(ΛkΛj )x ak,j, k,j{1,2}r where ak,j are coefficients involving the unitaries (ℓ) and the measurement observable ak,j = (cid:88) i,i (cid:0)W (1) 1k1 (cid:1)(cid:0)W (2) k1k (cid:1) (cid:0)W (r+1) kri (cid:1) iiW (r+1) ijr (2) j2j1 (1) j11. : Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks We group terms with the same frequency ω = Λk Ωfreq = ω = Λk Λj { k, Thus, the quantum model fA(x) can be rewritten as fA(x) = (cid:88) cωeiωx, ωΩfreq where the coefficients cω are given by cω = (cid:88) ak,j. k,j{1,2}r ΛkΛj =ω Λj and define the frequency spectrum 1, 2 . } } { (A.10) (A.11) Because each Λj is sum of eigenvalues, the differences ω = Λk of Ω: Λj are integer multiples ΩA = { = r, (r 1), . . . , . } (A.12) Accounting for the zero frequency separately, the maximum number of distinct non-zero frequencies is therefore This completes the proof of (a). Secondly, to prove (b), each encoding gate is now having weighting parameter w, = 2r. ΩA (A.13) resulting S(wx) = eiw 2 σj , = σj 2 . Now replace wx by wℓx in the ℓ-th layer. The ℓ-th layers eigenphase is multiplied by wℓ, so computational-basis term accumulates Λj(W ) = (cid:88) ℓ= sℓ 1 2 wℓ, sℓ +1, 1, . } { The contribution. The set of all possible frequency differences is therefore choice arises when two basis states coincide in that layer, giving no frequency 0 } { ΩB = (cid:40) (cid:88) ℓ= mℓwℓ (cid:12) (cid:12) (cid:12) mℓ 1, 0, 1 { (cid:41) . } The number of non-zero sums in ΩB is at most (3r 1) and the maximum frequency is KB = (cid:88) ℓ=1 wℓ. 37 (A.14) Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks If wℓ = 1, then ΩB = (b). { r, . . . , } , recovering = ΩB ΩA = 2r, and completing the proof of For (c), let K, then for ω k+1([0, 1]) have Fourier series g(x) = (cid:80) k, ωZ cωeiωx. If we truncate to where Cg depends only on and k. gK Cm Cg (k+1m), (A.15) To proof (A.15), we repeated integration by parts, of the truncation error is cω g(k+1) ωk+ . The m-th derivative (cid:88) ω>K cω ω g(k+1) nm(k+1) (cid:88) n>K g(k+1) + 1 (k+1m). (A.16) (A.17) Applying this to with = KA (baseline) or = KB (linear layer) proves (c). Finally, for Fourier-series-based KAN, the highest frequency required to achieve error ε in norm can be obtained by setting the right-hand-side of (A.6) to ϵ, resulting in = Θ(ε1/(k+1m)), hence Θ(K) parameters. In classical Fourier-series-based KANs, we use integer Fourier series to construct for practical purposes. However, data re-uploading circuits with trainable data pre-processing weights can result in fractional Fourier components. To achieve the same approximation error as the integer Fourier series of classical KANs, simple strategy is to set wℓ = 2ℓ1 to obtain integer Fourier series. Under this choice, the maximum frequency given by (A.14) is which exceeds that of Fourier-series-based KAN counterpart. Moreover, by applying (c), we obtain the error bound KB = 2r 1, Setting Cf 2r(k+1m) = ε gives fB Cm Cf 2r(k+1m). = (cid:25) (cid:24) log2(Cf /ε) + 1 = Θ(cid:0) log 1 ε (cid:1). Since the parameter count is proportional to r, this is Θ(log 1 ε ) parameters exponentially fewer than Fourier-series-based KANs Θ(ε1/(k+1m)), which proves (d). This completes the proof. 38 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks i d c o R t r a t p w , t e e F . t fi t f o c p : 5 g d i s r r m p r u t c i b n a n d l h . y e i s e e t f % 0 8 o R t e t i n N t w s l r . t h w h e h , K f . a t s i A h e a r f m w , i q Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks"
        },
        {
            "title": "B Numerical Results Details",
            "content": "B.1 Complementary Results of Figure 5 In addition to the summary presented in Figure 5, we provide the complete set in Table 5. This table includes the full list of 66 equations used in the regression benchmark, which were not explicitly shown in the main text. For each equation, we report the RMSE loss and the number of trainable parameters for both QKAN and classical KAN models. These results further illustrate that QKANs can achieve comparable or superior approximation accuracy while maintaining significantly fewer parameters. Table 5: Detailed regression results corresponding to Figure 5. This table presents comparison between QKAN and classical KAN across 66 symbolic regression expressions. For each expression, we report the root mean square error (RMSE) loss and the total number of trainable parameters. Bold values highlight the better-performing model in each row (lower RMSE). The results demonstrate that QKAN consistently achieves comparable or superior approximation accuracy with significantly fewer parameters, validating the expressive efficiency of DARUAN. Idx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Equation Expression QKAN KAN RMSE Loss # Params RMSE Loss # Params (cid:1) (cid:1) ℏω Nnµ Ef q2 0.5m (cid:0)u2 + v2 + w2(cid:1) sin (θ) x1y1 + x2y2 + x3y3 mrv sin θ 0.25mx2 (cid:0)ω2 + ω2 0 pd cos θ/4πϵr2 ℏω3/π2c2 (exp (ℏω/T kb) 1) (cid:0)Bv sin θ + Ef 2U (1 cos (dk)) qrv/2 (x + y) sin (exp(2y)) 3pd sin θ cos θ/4πϵr3 Ef αϵn/(αn/3 + 1) 0.5ksx2 0.5E2 ϵ E2 cϵ gmz kbµ E2 ϵ I0 sin2 (nθ/2)/sin2 (θ/2) 2 exp(θ2/2)/2 4I0 sin2 (α/2) sin2 (N δ/2)/α2 sin2 (δ/2) n0/ (exp(Bµ/T kb) + exp(Bµ/T kb)) /(2σ + 2) l/2πc2ϵr ℏω/(exp(ℏω/T kb) 1) (H 2c2 (1 2α) + c4kf /a2 )/8πG π BµM cos θ 3.652 1004 3.652 1004 3.652 1004 1.023 1003 1.274 1003 1.476 1003 1.715 1003 2.818 1003 2.915 1003 3.800 1003 4.455 1003 5.404 1003 6.219 1003 6.419 1003 7.082 1003 7.388 1003 7.729 1003 7.729 1003 8.531 1003 1.249 1002 1.249 1002 1.546 1002 1.598 1002 1.611 1002 1.637 1002 1.655 1002 1.681 1002 1.810 1002 1.872 1002 1.952 1002 2.023 1002 255 255 255 850 765 595 1,275 1,275 425 1,785 935 340 340 255 425 425 255 255 340 340 340 255 340 17 1,275 935 1,105 425 1,275 595 340 3.619 1004 3.568 1004 3.662 1004 6.779 1004 8.490 1004 8.556 1004 1.244 1003 2.933 1003 2.683 1003 3.898 1003 3.377 1002 5.442 1003 6.503 1003 6.693 1003 7.648 1003 7.572 1003 7.942 1003 7.937 1003 8.722 1003 1.293 1002 1.294 1002 1.586 1002 1.650 1002 1.611 1002 1.676 1002 1.872 1002 1.700 1002 1.903 1002 1.920 1002 7.809 1003 2.084 1002 336 336 336 556 1,001 776 1,111 556 556 1,221 1,221 2,111 1,556 336 556 2,221 336 891 2,111 1,556 2,111 1,446 446 23 1,666 2,331 891 1,111 2,221 776 446 Continued on next page 40 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Table 5 continued from previous page Equation Expression QKAN KAN RMSE Loss # Params RMSE Loss # Params Idx 32 33 34 35 36 37 38 39 40 41 42 43 45 46 47 48 49 50 51 Ef pd cos θ asin (n sin θ2) µn tanh (Bµ/T kb) ℏn (cid:0)1 α2(cid:1)/(α cos (θ1 θ2) + 1) E/(E (1 cos θ)/c2m + 1) kbn log (V2/V1) 1.5V pF n0 exp(gmx/T kb) Bµ (χ + 1) 2 + c2kf /a2 (cid:113) + B2 + B2 µ 1/(n/d2 + 1/d1) (cid:113) /8πG B2 (cid:16) (cid:17) 3 c4m2 + c2 (Aq + p)2 Veq + 1/(exp(ℏω/T kb) 1) β (α cos θ + 1) (cid:0)α cos2 (ωt) + cos (ωt)(cid:1) x1 (m1r1 + m2r2)/(m1 + m2) (cid:112)γp/ρ (cid:113) 2.023 1002 2.219 1002 2.254 1002 2.372 1002 2.603 1002 3.062 1002 3.509 1002 3.558 1002 3.716 1002 3.761 1002 4.767 1002 4.778 1002 5.355 1002 5.753 1002 6.836 1002 6.930 1002 6.938 1002 7.342 1002 7.662 1002 8.846 1002 (cid:113) 1 2x1x2 cos (θ1 θ2) + x2 x2 2 (x1 + x2)2 + (y1 + y2)2 αn/(αn/3 + 1) + 1 58 kGm (cid:16)(cid:113) I1I2 cos (δ) I1 + I2 + 2 52 53 54 55 56 57 kbv/A (γ 1) sin2 (Et/ℏ) 3pdz(cid:112)x2 + y2/4πϵr5 1.181 1001 1.252 1001 1.896 1001 2.172 1001 3.291 1001 3.467 1001 3.777 1001 3.895 1001 4.017 1001 60 61 Gm1m2/((x1 + x2)2 + (y1 + y2)2 + (z1 + z2)2) 4.331 1001 4.681 1001 62 6.304 1001 63 6.937 1001 64 8.487 1001 65 9.339 1001 2EL2/k2 2 exp((θ θ1)2/2σ2)/2 8πGρ/3 c2kf /a2 pF /(γ 1) hq/4πm (m2ω2x2 (αy/x + 1) + p2)/2m Gm + 1 cos (θ1 θ2) + 1 σ2 2 exp(θ2/2σ2)/2 q/ d2 2dr cos α + r2 /L σ2 (cid:113) 59 (cid:17) π π 340 255 935 255 425 850 935 255 1,020 340 510 850 340 595 850 340 425 425 340 1,275 850 255 340 425 340 595 1,190 510 153 340 765 595 1,530 850 2.082 1002 2.270 1002 2.473 1002 2.427 1002 2.709 1002 3.198 1002 3.597 1002 3.645 1002 4.005 1002 3.845 1002 4.834 1002 4.992 1002 5.372 1002 6.745 1002 6.983 1002 7.042 1002 7.132 1002 7.578 1002 7.965 1002 8.931 1002 1.215 1001 1.265 1001 1.920 1001 2.192 1001 1.853 1001 3.517 1001 3.887 1001 4.078 1001 4.050 1001 4.295 1001 4.655 1001 6.766 1001 6.810 1001 9.130 1001 1.731 10+ 446 1,446 1,776 1,446 556 1,111 666 891 1,331 1,001 666 556 446 776 1,666 1,556 2,221 1,666 446 1,111 1,666 891 1,001 1,666 2,111 2,441 1, 1,556 666 2,771 1,556 1,556 1,886 891 1,111 B.2 Visualization of QKAN Activations on Noisy Function Regression To further analyze the expressive power and interpretability of QKANs, we visualize the learned activation functions from selected regression tasks involving noisy symbolic expressions. This follows similar methodology to KANs, where each activation function is learnable one-dimensional function, enabling insight into the internal structure of the model. Figure 6 presents the per-node activation functions learned by QKANs that achieved the lowest RMSE for representative equations drawn from the benchmark in Table 1. These visualizations demonstrate that QKANs are capable of learning smooth, structured nonlinearities even in the presence of input noise, highlighting their robustness and alignment 41 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks with symbolic structure."
        },
        {
            "title": "C Experimental Details",
            "content": "C.1 Software Implementation To enable efficient numerical simulation of QKANs on CUDA-enabled devices [NVIDIA et al., 2020], we build our implementation using PyTorch [Ansel et al., 2024], extending its tensor operations with custom routines for quantum state evolution. The quantum state is represented as complex-valued tensor with shape (B, N, M, 2), where denotes the batch size, is the number of post-nodes, is the number of pre-nodes, and the final dimension encodes the amplitudes of the 2-level quantum system (i.e., single qubit). Quantum gates are encoded as complex tensors with shape (N, M, 2, 2), where the last 2 matrix structure of single-qubit unitary gate, and the two dimensions represent the 2 first two dimensions index the interaction between input and output nodes. We adopt the Pauli-Z operator as the observable for measurement. The data encoding blocks and trainable unitaries in the data re-uploading ansatz are implemented as parameterized single-qubit rotation gates. To initialize the quantum state, Hadamard gate is applied to place the qubit into an equal superposition of basis states, which empirically improves the stability and convergence of training. C.2 Software and System Information For full reproducibility, Table 6 summarizes the software versions, dependencies, and hardware system specifications used in our experiments. 42 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Figure 6: Visualization of learned QKAN activations for heuristic noisy regression equations. This figure illustrates the per-node activation functions learned by QKANs, similar to the interpretability offered by KANs. We display the QKAN models that achieved the lowest RMSE for subset of representative symbolic regression equations, as listed in Table 1. The transparency of each node is proportional to output range divided by input range; darker therefore denote stronger connections. Each sub-panel corresponds to distinct equation, demonstrating that QKANs can learn smooth and structured nonlinear transformations despite the presence of noise. 43 Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks Table 6: Software and System Information. Software Version matplotlib wandb tqdm h5py numpy scikit_learn setuptools sympy pandas requests transformers PennyLane PennyLane_Lightning torch torchaudio torchvision pykan RTX 4090 Personal Computer Parameter Python version Python compiler Python build OS CPUs CPUs Memory (GB) GPUs GPUs Memory (GB) 3.6.2 0.16.6 4.66.2 3.11.0 1.24.4 1.1.3 65.5.0 1.11.1 2.0.3 2.31.0 4.40.1 0.37.0 0.37.0 2.4.0 2.4.0 2.4.0 0.0.5 Value 3.11.5 GCC 11.2.0 main, Sep 11 2023 13:54:46 Linux 1 64 1 (NVIDIA GeForce RTX 4090) 24 Mon Sep 16 08:47:24 2024 UTC Tesla V100S Single Node Cluster Parameter Value Python version Python compiler Python build OS GPUs GPUs Memory (GB) 3.11.11 GCC 11.2.0 main, Dec 11 2024, 16:28:39 Linux 4 (Tesla V100S-PCIE-32GB) 128 Fri Mar 29 18:32:15 2025 UTC NVIDIA H100 GPUs & NVIDIA H200 GPUs Cluster Parameter Value Python version Python compiler Python build OS Scheduling System GPUs on H100 node GPUs Memory (GB) on H100 node GPUs on H200 node GPUs Memory (GB) on H200 node Cross-node Interconnector 3.11.13 GCC 11.2.0 main, Jun 5 2025, 13:12:00 Linux Slurm Workload Manager 8 (NVIDIA H100 PCIe) 640 8 (NVIDIA H200 PCIe) 1128 8 InfiniBand NDR 400G network ports Wed Jul 2 23:10:24 2025 UTC"
        }
    ],
    "affiliations": [
        "Center for Quantum Science and Engineering, National Taiwan University",
        "Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei 106319, Taiwan"
    ]
}