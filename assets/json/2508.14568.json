{
    "paper_title": "Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell",
    "authors": [
        "Wouter Legiest",
        "Jan-Pieter D'Anvers",
        "Bojan Spasic",
        "Nam-Luc Tran",
        "Ingrid Verbauwhede"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a novel approach to calculating the Levenshtein (edit) distance within the framework of Fully Homomorphic Encryption (FHE), specifically targeting third-generation schemes like TFHE. Edit distance computations are essential in applications across finance and genomics, such as DNA sequence alignment. We introduce an optimised algorithm that significantly reduces the cost of edit distance calculations called Leuvenshtein. This algorithm specifically reduces the number of programmable bootstraps (PBS) needed per cell of the calculation, lowering it from approximately 94 operations -- required by the conventional Wagner-Fisher algorithm -- to just 1. Additionally, we propose an efficient method for performing equality checks on characters, reducing ASCII character comparisons to only 2 PBS operations. Finally, we explore the potential for further performance improvements by utilising preprocessing when one of the input strings is unencrypted. Our Leuvenshtein achieves up to $278\\times$ faster performance compared to the best available TFHE implementation and up to $39\\times$ faster than an optimised implementation of the Wagner-Fisher algorithm. Moreover, when offline preprocessing is possible due to the presence of one unencrypted input on the server side, an additional $3\\times$ speedup can be achieved."
        },
        {
            "title": "Start",
            "content": "Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell Wouter Legiest1, Jan-Pieter DAnvers1, Bojan Spasic2, Nam-Luc Tran2 and Ingrid Verbauwhede1 1 COSIC, KU Leuven firstname.lastname@esat.kuleuven.be 2 Society for Worldwide Interbank Financial Telecommunication (Swift) Abstract. This paper presents novel approach to calculating the Levenshtein (edit) distance within the framework of Fully Homomorphic Encryption (FHE), specifically targeting third-generation schemes like TFHE. Edit distance computations are essential in applications across finance and genomics, such as DNA sequence alignment. We introduce an optimised algorithm that significantly reduces the cost of edit distance calculations called Leuvenshtein. This algorithm specifically reduces the number of programmable bootstraps (PBS) needed per cell of the calculation, lowering it from approximately 94 operations required by the conventional Wagner-Fisher algorithm to just 1. Additionally, we propose an efficient method for performing equality checks on characters, reducing ASCII character comparisons to only 2 PBS operations. Finally, we explore the potential for further performance improvements by utilising preprocessing when one of the input strings is unencrypted. Our Leuvenshtein achieves up to 278 faster performance compared to the best available TFHE implementation and up to 39 faster than an optimised implementation of the Wagner-Fisher algorithm. Moreover, when offline preprocessing is possible due to the presence of one unencrypted input on the server side, an additional 3 speedup can be achieved."
        },
        {
            "title": "Introduction",
            "content": "The past 20 years have seen major evolution of the global financial system. Financial crises, geopolitical events and economic growth have deeply impacted the direction that banking regulations have taken. One of the major policy shifts is in the direction of increasing transparency and sharing information between financial institutions. For instance, the G20 set targets for cross-border payments [FSB21] formulating objectives for enhancing cost, speed, financial inclusion and transparency in an effort to guarantee efficiency and seamlessness of an interconnected financial system. In line with the PSD2 directive [Cou15], which initiated the open-banking initiative in Europe, the EU has recently proposed the Financial Data Access framework which will grant consumers and SMEs to authorise third parties to access their data held by financial institutions. Information sharing among financial institutions is seen as paramount in the fight against financial 5 2 0 2 0 2 ] . [ 1 8 6 5 4 1 . 8 0 5 2 : r 2 Legiest et al. crime and money laundering [MA17], and is also expected to drive GDP gains of major economies [EC22]. In the previously mentioned initiatives, success can only be achieved if trust is built among all the actors. Trust can only be built if security and privacy are guaranteed in the exchange of information. While the directives are clear, the means to achieve successful implementation are left to the actors proposing the services and products. Considering the recent legislative proposal to make Euro payments instant [EC22], there is an obligation for payment providers to verify the match between the bank account number and the name of the beneficiary provided by the payer, as well as to alert the payer of possible mistakes or suspected fraud before the payment is made. In such applications, string similarity calculations are ubiquitous to provide robustness against spelling errors [AQA21]. One example is the edit distance, which calculates the minimum number of edits between two given strings. Recently, technologies enabling computation on encrypted data, namely fully homomorphic encryption (FHE) have become more practical. Informally, FHE is an encryption scheme that enables data owner to securely outsource computation on their data to an untrusted processing party, whereby the processing party computes over encrypted data and stays oblivious of the data and the computed result. The utility of FHE comes at performance price, which can sometimes be prohibitive for time-critical applications. However, the recent advances in software [CJL+20] and hardware [GBP+23, vDTV23] implementation of the underlying FHE algorithms show promising performance results, encouraging the practitioners to start including FHE in production. FHE schemes fall into two main categories: second-generation schemes like BGV [BGV14], BFV [FV12], and CKKS [CKKS17] support parallel computations on batched ciphertexts but have larger ciphertexts and slower bootstrapping. Third-generation schemes like TFHE [CGGI20] prioritise speed with smaller ciphertexts and faster bootstrapping, though they work on small, individual messages and require bootstrapping for nearly every operation. TFHE also allows any function to be applied for free during bootstrapping, making it ideal for fast, logic-based encrypted computations. In the context of string matching for financial applications, FHE could pose as an important enabler [Max21]. Instead of physically sharing their customer information, parties could compute the desired outcome of the matching operation avoiding data sharing in clear altogether. The institution sending the payment would encrypt the transaction data using suitable FHE scheme, and send it to third party which would compute the desired matching score in the encrypted domain and return the encrypted result. This result (and any intermediate variable) can only be decrypted by the payer institution. In the process, according to the principles of FHE, the third party provably does not learn anything about the transaction data, and the institution sending the payment does not learn anything about the customers of the institution receiving the payment. Another interesting application of approximate string matching is in secure and privacy-preserving DNA analysis. Approximate string matching is essential Title Suppressed Due to Excessive Length 3 in DNA analysis, where genetic sequences must be matched while allowing for slight discrepancies due to mutations. This flexibility is vital for detecting similar sequences that may vary because of natural mutations or sequencing errors. Cheon et al. [CKL15] provided the first edit distance algorithm in the context of somewhat homomorphic encryption. They develop both equality check and min functions and use this to build up edit distance calculation. They also give thorough analysis of the homomorphic depth of their solution. Their methods were later generalised by Vanegas et al. [VCA23], elaborated for an MPC context. Later, Aziz et al. [AAM17], Asharov et al. [AHLR18] and Zheng et al. [ZLS+19] proposed an approximation of the edit distance for genome analysis for fully homomorphic encryption. All the above techniques are based on second-generation FHE schemes and are built around arithmetic ciphertexts. Edit distance calculations for third-generation FHE schemes are less wellresearched. Recently, ZAMA showed an edit distance calculation for TFHE as demonstration of the concrete compiler [Zam22a]. This demonstrator is based on high-level code (i.e., Python) that is transformed to TFHE by the concrete compiler, and the implementation uses recursive definition of the edit distance."
        },
        {
            "title": "1.1 Our contribution",
            "content": "In this paper, we develop new edit distance algorithm for third-generation FHE schemes. We develop new algorithm adapted to TFHE, which we call Leuvenshtein, and use this to show that the properties of the programmable bootstrapping play very well with edit distance calculations. The main ideas of our implementation are: 1. Small representations: We use differential values that represent the differences between intermediate results, instead of working on the intermediate results themselves. This reduces the size of the intermediate variables, leading to smaller representation and more efficient calculations, significantly reducing the PBS costs. The size of our intermediate representations is small enough to fit in one ciphertext encoding 4 bits. 2. Re-using the programmable bootstrap: In each iteration of the Leuvenshtein algorithm we have to produce two output values (i.e., the horizontal and vertical differential values). We show that one can rewrite the equations so that both output values can be computed with the same non-linear parts, which as result means that they only differ by (cheap) addition. As the main cost is in the non-linear part, which needs to be done using costly programmable bootstrapping, this technique reduces the calculation cost by roughly half. 3. Non-linear calculation in only one lookup: During our calculation we have to compute the minimum of three inputs. common strategy would be to do two bivariate lookups that each take two inputs. To enable the calculation of the non-linear part in only 1 programmable bootstrap, we propose denser packing of the inputs. The input to our non-linear part has total of 3 3 2 = 18 values, while our 4-bit programmable bootstrap only 4 Legiest et al. allows 16-value function. By adapting the non-linear function to start and end with zeros, we enable larger effective lookup that can accommodate the full 18 values, saving another factor two in PBS. Our resulting Leuvenshtein algorithm3 requires 94 less PBS compared to textbook Wagner-Fischer implementation, and 16 compared to bitsliced implementation (i.e., from Myers [Mye99]), excluding the equality calculations. Our second contribution is an optimised equality check implementation that uses significantly fewer programmable bootstraps (PBS). This method allows us to encode characters more optimally, reducing the number of ciphertexts required by half. More specifically, using our method, we are able to do an equality test on 7-bit ASCII strings in 2 PBS, instead of the standard 5 PBS as would be used by standard equality check (as for example implemented in TFHE-rs). third contribution looks at preprocessing to reduce the (online) running cost. As our improved edit distance calculation only requires 1 PBS per edit distance, the main cost of the algorithm sits in the equality calculation. In case one of the input strings is unencrypted, we show that one can do precalculation where each encrypted string is compared to each letter of the alphabet and all the results are stored in lookup table. During the edit distance calculation one then only has to perform an (unencrypted) lookup to select the relevant equality value. This technique is useful when the encrypted string is known in advance, or when the alphabet is small compared to the string lengths. Combining these contributions, our implementation of the Levenshtein distance for ASCII inputs achieves speedup of up to 278 over the best available implementation, and factor 40 over our own state-of-the-art Wagner-Fisher implementation. In case of one unencrypted input, in some instances further 3 speedup is possible due to our improved preprocessing."
        },
        {
            "title": "2 Preliminaries",
            "content": "In this section, we will first introduce the TFHE homomorphic encryption scheme. Then, we will define edit distances and specifically the Levenshtein distance, including the most relevant algorithms to calculate it."
        },
        {
            "title": "2.1 Notation",
            "content": "For the rest of this work, we will compare two strings a1..m, b1..n, with lengths of and characters, respectively. All characters of the strings come from an alphabet Σ, and with Σ we denote the number of characters in the alphabet. The ith character of string will be denoted as ai. When the alphabet size is larger than the plaintext size, characters are represented through multiple symbols that are encrypted individually, denoted with a(j) for the jth part of the ith character of string a. For example, an 8-bit character can be split into two 4-bit symbols, a(1) and a(2). 3 Rust implementation is available on https://github.com/KULeuvenCOSIC/leuvenshtein and on https://zenodo.org/records/15638825 Title Suppressed Due to Excessive Length"
        },
        {
            "title": "2.2 TFHE",
            "content": "Fully homomorphic encryption (FHE) enables computations to be carried out on encrypted data. This paper will focus on FHE schemes with programmable bootstrapping, specifically the Torus Fully Homomorphic Encryption (TFHE) scheme [CGGI20]. We will provide high-level introduction to TFHE; for more details, we refer to [CGGI20, Joy22]. In TFHE, ciphertext typically holds 1-4 bits of plaintext while allowing for linear operations such as addition, subtraction, and multiplication with small unencrypted value at relatively low cost. However, these operations increase the noise in the ciphertexts. Once certain number of operations have been performed, noise reduction procedure called bootstrapping becomes necessary. Bootstrapping resets the noise, enabling further computations, but it is significantly more costly than linear operations. It is possible to chain ciphertexts together to encrypt larger integers. One key advantage of TFHE is its capability to apply any lookup table (LUT) to the ciphertext without incurring any cost during bootstrapping. This process, known as programmable bootstrapping (PBS), facilitates executing highly nonlinear functions on encrypted data. An example of such LUT is shown in Table 1 on the following page. To illustrate, consider two 2-bit encrypted messages, and y, both encrypted in 4-bit plaintext space (Figure 1 on the next page). To compare them, we first compute + (y 2) (x + 4 y), resulting in 4-bit value. The two least significant bits represent x, and the two most significant bits represent y. Then, we use lookup table to check if the first 2 bits of the input equal the last 2 bits. In more detail, the plaintext space is typically divided into message bits (the least significant bits of the plaintext space), carry bits (in the middle), and padding bit (the most significant bit). The message bits represent plaintext values after encryption or bootstrapping. In contrast, the carry bits, initially zero, are filled after linear operations are performed (e.g. the + (y 2) operation as described above). The padding bit is typically kept at zero to simplify the application of LUT during programmable bootstrapping. In more advanced scenarios, one can make an abstraction of the plaintext and carry space and use the entire plaintext space without the message-carry division. However, in this scenario, one should generally ensure that the padding bit remains zero to ensure proper LUT lookups during programmable bootstrapping. More specifically, we can create the corresponding Lookup Table (LUT) for any arbitrary function as long as the padding bit is 0. Due to the nature of TFHE calculations, when the padding bit is 1, the lookup result will be the negative of the corresponding input with the padding bit 0. For example, if using 5-bit plaintext space to evaluate function through LUT, and we want to use the entire 5-bit, we must consider that for input values = 25/2 = 16 < < 25 = 32, the function will become (x) = (x 16). This property is due to the 6 Legiest et al. Fig. 1: Subdivision of 5-bit plaintext in 2-bit message and carry space for TFHE ciphertext Table 1: LUT Table for function (x) = 4 in 5-bit plaintext space. The right side can not be chosen, as it is the negative (mod 16) of the left side. Output Output Output Output 0 (0 0000) 1 (0 0001) 2 (0 0010) 3 (0 0011) 4 (0 0100) 5 (0 0101) 6 (0 0110) 7 (0 0111) 12 13 14 15 0 1 2 3 8 (0 1000) 9 (0 1001) 10 (0 1010) 11 (0 1011) 12 (0 1100) 13 (0 1101) 14 (0 1110) 15 (0 1111) 4 5 6 7 8 9 10 11 16 (1 0000) 17 (1 0001) 18 (1 0010) 19 (1 0011) 20 (1 0100) 21 (1 0101) 22 (1 0110) 23 (1 0111) 4 3 2 1 0 15 14 13 24 (1 1000) 25 (1 1001) 26 (1 1010) 27 (1 1011) 28 (1 1100) 29 (1 1101) 30 (1 1110) 31 (1 1111) 12 11 10 9 8 7 6 5 negacyclic nature of the polynomials and is inherent in any current FHE scheme with PBS. Another critical aspect of FHE is that data-dependent branching (e.g., if, while statements) cannot be used due to its confidential nature. To evaluate branch in FHE, all possible outcomes must be calculated. This means that ideally, programs need to be rewritten to avoid if statements, and if statements cannot be used to skip irrelevant parts of the execution. We will revisit this topic in our discussion of edit distance calculation algorithms. In the following sections, we will use parameter set with plaintext size of 5 bits. In this set, the lowest 4 bits can be freely assigned, while the most significant bit is utilised for padding. This parameter set is commonly used in practice."
        },
        {
            "title": "2.3 Edit distance",
            "content": "The edit distance is metric used to measure the similarity between two strings by calculating the number of edit operations needed to transform one string into another. It differs from the Hamming distance, which only considers the similarity of corresponding characters. For example, the Hamming distance between abcdex and xabcde is six, while the edit distance is two (one insertion and one deletion). The edit distance exists in various variants where each variant allows different set of operations: the first two operations, insertion and deletion, correTitle Suppressed Due to Excessive Length 7 spond to the addition or removal of character. The third operation is substitution, which involves replacing one character with another. The fourth operation is transposition, where two adjacent characters swap places. In this work we will focus on the Levenshtein distance, which considers the first three operations. There are various versions of the edit distance. For example, costs can be assigned to each operation, allowing different weights to be applied. When all operations have uniform unit cost, it is referred to as simple edit distance. If non-unit costs are used, it is called general edit distance. In some cases, the goal is to find the exact value below specific limit, and once that limit is exceeded, the exact value becomes unimportant. An approximated edit distance can be used in such scenarios."
        },
        {
            "title": "The popular Levenshtein distance is often interchangeably used with edit",
            "content": "distance. These metrics are popular tools in (financial) fraud detection, DNA sequence comparison, calculating distances between matrix sequences [PHRL19, SAE+08, BDL+19], and spell checkers. Calculating the edit distance The Levenshtein distance was originally obtained using recursive definition. This definition was later converted to an executable algorithm, the Wagner-Fischer algorithm, using dynamic programming. Since then, more efficient variations have been proposed, improving time and space complexity. For comprehensive overview of this plaintext algorithm, refer to the work of Navarro [Nav01]. Advanced algorithms, such as those based on the Four Russians Method [MP80], Suffix trees [Knu73], or filtering [Ukk92], are not suitable for implementation in FHE due to their data-dependent assumptions or alphabet-specific data representations. Additionally, algorithms based on nondeterministic finite automaton (NFA) [Ukk85b] will have the same complexity as the Wagner-Fischer algorithm in the encrypted domain. The representation of the automaton will have the same form as the d-matrix. Therefore, specific FHE-friendly optimisations are needed to speed up the calculations of the edit distance further. Wagner-Fischer The Wagner-Fischer algorithm [Vin68, WF74] uses dynamic programming to create distance matrix (or d-matrix). Each element in the matrix represents the edit distance of the corresponding substrings up to that point in the matrix. For instance, D[i, j] = ed(a1..i, b1..j) corresponds to the edit distance of the first characters of string and the first characters of string b. Specifically, in the simple edit distance case, each value of the d-matrix is determined by the following equation: D[i, j] = (cid:40) D[i 1, 1] if ai = bj 1 + min(D[i 1, j], D[i, 1], D[i 1, 1]) otherwise. (1) To calculate the next value, the algorithm uses three previously computed values. These dependencies make the algorithm difficult to parallelise. The original definition has time and memory complexity of O(n2). simple optimisation Legiest et al. is to reduce the space complexity to O(n) by only storing some columns of the d-matrix. Examples of the d-matrix are given in Figure 2. Since its definition, many variations have been proposed to optimise the calculation. They mostly rely on skipping parts of the calculations based on the alphabet or data-dependent intermediate values [Mye86, Ukk85a]. It is impractical to port these optimisations to the FHE domain, as we do not know the value of intermediate variables and can thus not do any data-dependent optimisations. d 0 1 2 3 4 5 6 1 1 2 3 4 5 6 2 2 2 3 4 5 6 3 3 3 3 4 5 6 4 4 4 4 3 4 5 5 5 5 5 4 3 4 6 6 6 6 5 4 3 c 0 1 2 3 4 1 1 1 2 3 2 2 2 1 2 3 3 3 2 1 4 3 4 3 2 Fig. 2: d-matrix of the (Simple) Edit distances of d(monday, friday) = 3 and d(abcx, xabc) = Myers An alternative approach to Wagner-Fischer was proposed by Myers [Mye99]. This approach targets modern CPUs by rewriting the algorithm in terms of bits and optimising it for this lead. The main idea is to store the differential values (or differences between adjacent horizontal and vertical cells) in the d-matrix instead of absolute distances. In simple edit distance, each neighbouring value in the d-matrix can differ by at most one (see Eq. 1). The Myers algorithm takes advantage of this by only representing the horizontal and vertical differences between neighbouring cells in the d-matrix (Figure 3 on the next page). This allows Boolean logic to compute the distance, making it possible to better utilise hardware parallelisation, particularly on CPUs. Note that from these horizontal and vertical differences, it is straightforward to reconstruct any value in the d-matrix by choosing path from the start to the targeted cell and summing the horizontal and vertical differences along this path."
        },
        {
            "title": "The core of the algorithm is to store only the neighbour differences using",
            "content": "ternary values {1, 0, 1}, both in the vertical and horizontal direction: v[i, j] = D[i, j] D[i 1, j], h[i, j] = D[i, j] D[i, 1]. (2) We can use the following equations to directly calculate the and values. When we examine cell (i, j), we can define vout and hout as the values v[i, j] and h[i, j]. These are the values we aim to compute in this cell. The equations above can then be rephrased in terms of the equality eq Title Suppressed Due to Excessive Length 9 1 1 2 2 2 1 2 3 3 2 2 0 2 3 S 1 1 1 1 +1+1 +1+1 +1+1 00 00 00 1 1 1 1 +1+1 +1+ -1-1 00 -1-1 +1+1 -1-1 +1+1 +1+1 +1+1 00 00 -100 1 1 1 1 +1+ +1+1 +1+1 1 1 1 Fig. 3: Edit distance calculation of d(KID, SIT) = 2 through the WagnerFisher algorithm (left), where the absolute distances are calculated; and the Myers algorithm (right), which calculates the relative distances. between the two relevant characters ai and bj, and the previous values of and which we denote as vin and hin. An overview of the input and output variables is given in Figure 4. Fig. 4: Myers cell [Mye99] We can transform Equation 2 to calculate the outputs vout and hout of single d-matrix cell, in function of the inputs eq, vin, hin: vout = min hout = min 1, vin + 1 hin, 1 eq hin 1, 1 + hin vin, 1 eq vin , . 10 Legiest et al. Building on this concept, it is possible to transform the entire Wagner-Fischer algorithm to use only Boolean operations and additions. For instance, they define two Boolean values to represent the ternary nature of the delta elements. By leveraging bit CPU architecture, the algorithm achieves time complexity of O(m/wn). However, the Myers algorithm does not necessarily translate well to TFHE environment, where operations can be performed on multi-bit values. detailed overview of the Myers algorithm is provided in [Mye99]."
        },
        {
            "title": "3 Encrypted Levenshtein",
            "content": "Edit distance calculation generally involves two phases: equality checking and the main algorithm. The equality checking phase determines character equality between the input strings. The main algorithm then uses these equalities to compute the d-matrix (or differentials in the Myers approach) and determine the edit distance. This section will focus on the main algorithm, while section 4 will focus on the equality checking phase."
        },
        {
            "title": "3.1 Main algorithm",
            "content": "This section will develop new edit distance algorithm suited for the encrypted domain. Our algorithm improves the Myers approach detailed in section 2.3 in the FHE case. As reminder, the main idea of this approach is to calculate the differential values between two nodes in the edit distance calculation, as given in Figure 4. The Myers approach is designed for CPU-optimised operations, such as Boolean operations and additions, by using bitslicing. This makes operations very efficient on CPU, but they do not necessarily translate to efficient operations in the TFHE domain. Our approach focuses on optimising for FHE in two steps: First, our approach uses small multivalue operands (typically in the range {1, 0, 1}) instead of binary values in Myers. For example, we represent the trinary hin and vin operators in one operand instead of splitting into binary positive and negative parts, as done by Myers. This limits the number of inputs and outputs that need to be handled and is efficient due to the native multivalue operations in TFHE. Secondly, we rewrite the cell equations to allow the calculations in only one bootstrap. This means that vin, hin, and eq are given as inputs to the PBS, and vout, hout are extracted as the outputs. To achieve this, we have to optimise the PBS to perform lookup that is relevant for both the outputs vout and hout. We then show that we can still perform this lookup in 1 PBS by carefully manipulating the PBS function. Combining the PBS calculations Using the Myers approach, our algorithm calculates two output values vout and hout for each cell, as explained in Figure 4. Title Suppressed Due to Excessive Length 11 The formulas to compute vout, hout are shown below: vout = min hout = min 1, vin + 1 hin, 1 eq hin 1, 1 + hin vin, 1 eq vin , . The first optimisation is to rewrite the equations to have similar non-linear operation. We can rewrite both equations to: vout = min(eq, vin, hin) + (1 hin), hout = min(eq, vin, hin) + (1 vin). In this form, we can focus on min(eq, vin, hin) in the PBS, and perform the (1 hin) operations using linear computations without bootstrapping at low cost. This optimisation reduces the calculation cost with approximately factor 2. (3) Extended lookups standard approach to calculate the min function in Equation 3 would be to do two bivariate lookups, which would first combine two inputs into the key (key = vin + 4 hin), after which PBS with relevant lookup table is performed on this key to calculate the function min(vin, hin). In the second phase, similar min function is performed between the result of the first min function and eq. This approach requires two PBS lookups for each cell of the Leuvenshtein calculation. In this section we will reduce this further to one PBS per cell in the standard Levensthein case, by combining the calculation of both min functions. In this case, we have vin, hin [1, 0, 1] and eq [0, 1]. By combining the inputs to the min functions in more dense way (e.g. vin + 3 hin + 9 eq) one could reduce the input size. However, even in the best case, this entails 332 = 18entry lookup table for the min operation, while we only have 16-entry lookup table available. It is important to note the negacyclic nature of the TFHE-PBS lookup. In TFHE with 4-bit message (and carry), one can construct any lookup table of 16 values (values 0 to 15). Lookups at values 16 to 31 will result in the negated value of the corresponding value at position 16. This means that if we can place zero lookup at position 0, and the value at position 16 is also 0, we can essentially extend the lookup table with one extra value. We can extend this as long as the ith and (i + 16)th values are both 0. When we rewrite the formulas for vout and hout as: vout = {1 + min(eq, vin, hin)} hin, hout = {1 + min(eq, vin, hin)} vin. 12 Legiest et al. The function between brackets returns mostly zeros during the programmable bootstrap. We will denote this value with , or Mij, denoting the value in the cell at location i, j. Combining this with the following key for the PBS lookup: (vin + 1) + 3 (1 + hin) + 9 eq (4) results in lookup table of 18 values, which starts and ends with two zeros. This allows us to fit this lookup table into 16-value TFHE lookup table, as detailed in Table 2. Table 2: LUT Table for function LUTmin : 1 + min(eq, vin, hin), with = (vin + 1) + 3 (1 + hin) + 9 eq in 5-bit plaintext space. Note that the right side is the negative of the left side (mod 16) due to the negacyclic nature of the LUT. Output Output Output Output 0 (0 0000) 1 (0 0001) 2 (0 0010) 3 (0 0011) 4 (0 0100) 5 (0 0101) 6 (0 0110) 7 (0 0111) 0 0 0 0 1 1 0 1 8 (0 1000) 9 (0 1001) 10 (0 1010) 11 (0 1011) 12 (0 1100) 13 (0 1101) 14 (0 1110) 15 (0 1111) 1 0 0 0 0 0 0 0 16 (1 0000) 17 (1 0001) 18 (1 0010) 19 (1 0011) 20 (1 0100) 21 (1 0101) 22 (1 0110) 23 (1 0111) 0 0 0 0 15 15 0 15 24 (1 1000) 25 (1 1001) 26 (1 1010) 27 (1 1011) 28 (1 1100) 29 (1 1101) 30 (1 1110) 31 (1 1111) 15 0 0 0 0 0 0 0 Limiting the noise growth Our approach yields the same result as the corresponding plaintext algorithms. However, since the computation is performed under FHE, there remains small probability of error. This probability is typically kept extremely low for cryptographic security purposes (for instance, below 264) by carefully choosing parameters that control noise growth. In this subsection, we demonstrate that, with few minor adaptations, our algorithm remains within the predefined noise bounds, thereby preserving the intended low failure probability. This failure probability is sufficiently low that it should not be noticeable by end-users. In the previous discussion, we simplified the operations in two cases: costly non-linear bootstraps and cheap linear operations that do not require bootstrap. In reality, there is maximum number of linear operations that can be performed before bootstrap is needed. In our case the calculation of the key can exceed this threshold when not taken into account. We will denote the variance of the noise of one ciphertext with ϵ2 BS (i.e., the noise of ciphertext at bootstrap time when it has not been combined with another ciphertext). When adding independent ciphertexts, the noise will be Title Suppressed Due to Excessive Length 13 equivalent to ϵ2 in noise equivalent of k2 ϵ2 addition and multiplication of stochastic variables. BS, while the multiplication of ciphertext with results BS. These come from standard equations for the For the key calculation in Equation 4, one thus has noise equivalent of: key = ϵ2 ϵ2 vin + 9 ϵ2 hin + 81 ϵ2 eq9 . Note that for now, we assume independence between the variables, which we will come back to later in this section. first trick to reduce the error is to include the factor 9 at the eq term: instead of calculating enc(eq) during the equality checking phase, we adapt the bootstrap to calculate enc(9 eq). Moreover, this gives us small speedup since scalar multiplication is avoided. We will denote the value 9 eq with eq9. This reduces the noise of the key to: key = ϵ2 ϵ vin + 9 ϵ2 hin + ϵ2 eq9 = ϵ vin + 9 ϵ2 hin + ϵ2 BS. second thing to notice is that vin and hin themselves are calculated recursively. Denoting with vi,j the vout of cell (i, j) (and similarly for hout) and Mi,j = D[i, j] D[i 1, 1], we get the following formulas: vin =vi,j1 =Mi,j1 hi1,j1 =Mi,j1 Mi1,j1 + vi1,j2 Mi2,j2 Mi1,j1 Mi1,j1 + Mi1,j2 Mi2,j2 =Mi,j1 Mi1,j1 Mi2,j2 + Mi2,j2 Mi1,j1 Mi2,j2 + Mi1,j1 + Mi2,j1 Mi2,j2 hin =Mi1,j Mi1,j1 All are calculated using different inputs, and thus, their respective noise can be considered independent. However, the formulas of vin and hin have the same negative terms (in bold in the equation above), which increases the noise in the key: key = vin + 3 hin + eq9 + Cte = (Mi,j1 4 Mi1,j1 + 3 Mi1,j)+ (Mi1,j2 4 Mi2,j2 + 3 Mi2,j1)+ + eq9 + Cte or: key = NH ϵ2 ϵ2 BS + 16 NM ϵ2 BS + 9 NL ϵ2 BS + ϵ2 BS with NH , NM and NL the number of respective terms (i.e., NH is the number of Mi,j1-like terms, NM the number of 4 Mi1,j1-like terms and NL the number of 3 Mi1,j-like terms). 14 Legiest et al. Fig. 5: The relative value of the noise in each cell of the calculations. The red line indicates refresh of the noise is needed using bootstrap operation. This is for parameter set that can handle up to 25 additions. The noise can be easily reduced by changing the equation of the key as: (1 vin) + 3 (1 + hin) + 9 eq, (5) where vin has been negated. This changes the LUT of the bootstrap in Table 2, but the general techniques developed are still valid. As the constant near the Mi1,j1 Mi1,j1 Mi1,j1 term is now 2 instead of 4, we have an equivalent noise of: key = NH ϵ2 ϵ2 BS + 4 NM ϵ2 BS + 9 NL ϵ2 BS + ϵ2 BS. We now have improved noise equations. To make sure our noise does not surpass the threshold, we have to refresh the noise in the vin and vout terms just before the noise threshold is exceeded. Figure 5 depicts the noise in the ciphertexts, where the red line indicates where refreshing is needed. This example is for parameter set that allows maximum of 25 additions of ciphertexts before bootstrap is needed. In practice, the standard parameter sets used in TFHE-rs can typically handle more than 4 000 additions before bootstrap is needed, according to our calculations. As such, only in edit distance calculations with very large words does one have to take these refreshes into account. We have experimentally verified this calculation by running the bootstrap on increasingly noisy ciphertexts and testing when an error occurs."
        },
        {
            "title": "3.2 Skipping irrelevant cells",
            "content": "When calculating the d-matrix (or equivalently, the horizontal and vertical differences and v), certain cells do not influence the final result and thus do not need to be computed. This can be understood as follows: Title Suppressed Due to Excessive Length 15 Maximum Levenshtein Distance: The maximum possible value of the Levenshtein distance is bounded. In the worst case, it is max(m, n), meaning that every character in one string needs to be substituted to match the other string. Shortest Path Analogy: Computing the Levenshtein distance is analogous to finding the shortest path through the d-matrix, where the cost of each move is defined by Equation 1. Horizontal and vertical steps always have cost of 1, while diagonal steps depend on the value of eq. Path Cost Bounds: The minimal cost of traversing from the top-left to the bottom-right corner of the d-matrix is + (i.e., vertical steps and horizontal steps). This value exceeds the maximum possible Levenshtein distance, which implies that not all cells are relevant to the computation. These observations reveal that many cells in the d-matrix are unnecessary for determining the final distance. Without loss of generality, consider the case where = n. According to Ukkonen [Ukk85a], if cell that is steps away from the main diagonal is on the shortest path, the path has minimum path cost of 2k (consisting of at least horizontal and vertical steps). Thus, cells located more than k/2 steps from the diagonal can be excluded from computation without affecting the accuracy of the result. In more extreme situation, one can compute approximate Levenshtein distances, where the result is accurate up to Levenshtein distance of ℓ, but for larger Levenshtein distances, the output might be wrong (i.e., the output might be larger than expected). In this case, one only has to compute cells that are within ℓ/2 of the diagonal. This is useful when you want to determine if two strings are approximately equal. Concretely, when = n, we do not need to calculate all the m2 cells, but we can reduce this to + 2 ℓ (cid:88) i= (m i) = (2ℓ + 1) ℓ2 ℓ. Furthermore, this modification precludes the calculation of the edit distance value from the final row. Consequently, the final score is determined by summing all and elements along the diagonal."
        },
        {
            "title": "3.3 The resulting algorithm",
            "content": "Combining all of the above approaches leads to Algorithm 1 on the following page. This algorithm has as inputs matrix eq9 and parameter ℓ. Matrix eq9 will contain the equality information for each character pair. That is, element [i, j] will contain 9 if xi = yj and otherwise will contain 0. Parameter ℓ will denote the approximation level. By assigning ℓ = max(m, n)/2, the exact edit distance will be calculated. To make fair comparison, we ran our WF and Myers implementations and counted the number of PBS performed. The WF and Myers algorithm would need on average, approximately 94 PBS and 16 PBS to calculate cell, when 16 Legiest et al. Algorithm 1 Leuvenshtein 1: function Edit Distance(eq9, ℓ) Setup 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: OneMatrix[0..m, 0..n] OneMatrix[0..m, 0..n] LUTmin[key] Table 2 Main Algorithm for 1 to do for 1 to do if j ℓ then key (1 v[i, 1]) + 3 (1 + h[i 1, j]) + eq9[i, j] min PBS(key, LUTmin) v[i, j] min h[i 1, j] h[i, j] min v[i, 1] i=1 h[i, i] + Σn i=0v[i + 1, i] return Σm+1 Table 3: Overview of the PBS load of the different algorithms for single cell, using ASCII encoding. The exact number of programmable bootstrap needed for the WF and Myers algorithm is estimated here, as it depends on the exact scenario. More information can be found in section 6 Algorithm Levenshtein Improvement factor WF Myers Ours 94 16 1 1 5.87 94 using ASCII encoding. While the size of the other algorithms depends on the character encoding and length of the input strings, our algorithm is independent of these. The cost for our algorithm will remain constant, requiring only one PBS."
        },
        {
            "title": "4 Equality checking",
            "content": "In the previous section, we described an algorithm that only uses 1 PBS per cell to calculate the Levenshtein distance. For each cell, one also has to calculate the equality between the corresponding characters of the input strings. This typically costs more than 1 PBS per cell, and thus the equality calculation is the most expensive operation of the full Levenshtein algorithm in our case. In this section, we improve the equality calculation in two ways: we introduce technique that allows doubling the number of plaintext bits in one PBS equality operation, and then we propose new technique to more efficiently look at larger symbols, notably 7-bit ASCII symbols. Title Suppressed Due to Excessive Length"
        },
        {
            "title": "4.1 Doubling the equality PBS size",
            "content": "The standard approach to equality checking involves dividing the binary representation of the input letters into chunks of 2-bit and then pairwise comparing the corresponding bits. This method is also implemented in the software library TFHE-rs [Zam22b]. For instance, when comparing 2-bit with 2-bit y, one computes the key + 4 and then performs PBS that maps = to 1 and all other values to 0. Using the standard parameter size (2-bit plaintext, 2-bit carry), this method can only handle inputs of at most 2 bits. We present new method that can handle 4-bit symbols. To compare two (4-bit) chunks and y, we subtract both values, resulting in variable with value between 15 and 15, and value of 0 if and only if both chunks are the same. As before, this results in more values than the normal 16-value PBS lookup. By choosing the following lookup table for the PBS: LUTeq = (cid:40) 1 if (x y) = 0 0 else, (6) we can have 31-value lookup due to the negacyclic property of the lookup and the abundance of 0 values. Note that in our Levenshtein calculation, we sometimes want to calculate eq9, for which we adapt the LUT to: LUTeq9 = (cid:40) 9 if (x y) = 0 0 else. (7) Thus, our new method of equality checking can handle symbols of double size. For typical large integers where the integer is divided into ciphertexts that each contains 2-bit chunks, two 2-bit equality checks can be combined into one by calculating = 4 x(2) + x(1) (and similarly for y), thus halving the PBS cost. This also means that an equality check for larger inputs can be done at half of the PBS cost using our method."
        },
        {
            "title": "4.2 Equality check for large-sized symbols",
            "content": "Larger characters are typically divided into smaller symbols of size t, usually 2 to 4 bits. sub-equality operation is performed for each pair of chunks, after which the results of these sub-equality operations are combined to produce the final equality result. The TFHE-rs library provides two-step algorithm for calculating equality. First, corresponding chunks are compared using the method + (y 2), as described earlier. This will produce sub-equality that will compare 2-bit or character data. The ciphertext will contain one if the two parts are equal, zero in the other case. In the second step, the sub-equality results are summed together, in triangular way, to find the overall equality. All of the sub-equalities are divided into groups of maximally 1 elements. All sub-equalities in group are summed together and PBS is applied to the sum to check if the sum reaches its maximum 18 Legiest et al. possible value, i.e. 1. The results are now again grouped into maximally 1 elements, summed together and used in PBS. This process is repeated until single ciphertext is obtained, representing the result of the equality check. If all of the sub-equalities of step one consist of one, the final equality result will also depict one. Algorithm 2 outlines this approach for characters of at most 30-bit. For example, when comparing two ASCII characters (7-bit), each character is encoded into four ciphertexts. After computing the sub-equalities, the four outcomes are summed, and PBS is used to verify if the total sum equals 4."
        },
        {
            "title": "This method can be further improved using a hybrid approach of our custom",
            "content": "equality subcheck and the TFHE-rs combination phase. In this hybrid approach, characters are encoded into consecutive 4-bit chunks. In the first part, subcomponents are computed using our subtraction method. In the second part, the TFHE-rs aggregation step is used to combine the subcomponents efficiently. This reduces the cost of calculation by roughly half. Algorithm 2 TFHE-rs compare method for characters with maximum size of 30 bit. Require: Two encoded characters and Require: x, are split into encrypted symbols, < 16, each symbol has = 2p size plaintext Equal Part 1: for 0 to do 2: 3: z0 x(i) + 4 y(i) eq(i) PBS(z0; LUTeq) Merging Part 4: Acc 0 5: for 0 to do Acc Acc + eq(i) 6: 7: Acc PBS(Acc, LUTmax) 8: return Acc"
        },
        {
            "title": "4.3 Equality check for medium-sized symbols (e.g., ASCII)",
            "content": "While this combination method of TFHE-rs is efficient for large input symbols, we propose better method for medium-sized inputs. Specifically, our method outperforms state-of-the-art for 5to 16-bit inputs, specifically for ASCII inputs. For this explanation, we will assume 7-bit input characters and 4-bit combined message and carry space. The first step in our method is to decide on the representation of our characters. We will encode our characters using 4-bit and 3-bit symbol, each encrypted in one ciphertext. We will then perform the equality check between the 4-bit symbols as explained in the previous section. The result is single bit Title Suppressed Due to Excessive Length 19 eq denoting equality, which is combined with the 3-bit symbol in the following way: 2 (x(2) y(2) ) + (1 eq(1)). The result of this linear computation is 0 if and only if both the 4-bit symbols and the 3-bit symbols are the same. Thus, we can perform the same PBS lookup as before on this result to calculate the equality of the characters. The formal representation is given in Algorithm 3. Algorithm 3 Our equality check for 7-bit characters, split into 4 and 3-bit Require: Two ASCII encoded characters and Require: x, are split into 4-bit symbols (x(1), y(1)) and 3-bit symbols (x(2), y(2)) 1: z0 x(1) y(1) 2: eq(1) PBS(z0; LUTeq) 3: z1 2 (x(2) y(2)) + (1 eq(1)) 4: eq PBS(z1; LUTeq) 5: return eq first 4 bits of the character last 3 bits of the character The result is that we can perform an ASCII equality check with two PBS, while other methods would need five lookups when using state-of-the-art 2-bit equality techniques or three lookups when using our 4-bit equality technique."
        },
        {
            "title": "4.4 Conclusion",
            "content": "To calculate the equality between two characters, there are three options: TFHErs, our own, or the combined approach. All three approaches demonstrate different PBS behaviour. Figure 6 denotes the PBS loads as function of the input size. From this analysis, we can see that our method is the best for characters of up to 16-bit, which is ideal for an ASCII usecase. For larger characters, the combined method performs better. Furthermore, our method reduces the memory footprint of the encrypted character by up to factor of two."
        },
        {
            "title": "5 Preprocessing",
            "content": "Even with reduced cost for the equality check as discussed in the previous section, the equality checking is typically still more expensive than the main algorithm. More specifically the main algorithm uses 1 PBS per cell, while the equality calculation costs PBS calculations per cell with constant depending on the input characters. In this section we will show that in the specific case that one of the strings is unencrypted, we can perform preprocessing to speed up the equality calculations, making their total cost linear in the input size (i.e. m, for the size of the encrypted input). 20 Legiest et al. Fig. 6: Comparison of the PBS count for the three equality calculation techniques. The idea of this preprocessing is to precompute the output of the equality for each possible character of the alphabet for the encrypted input (which has cost Σ m), similar to technique proposed by Myers [Mye99] to allow efficient bitsliced implementations. During the equality check one then can perform simple lookup using the unencrypted input character to find the result of the equality, which does not require any PBS. More specifically, during the preprocessing phase, we check the equality of each character in the encrypted string with every possible character in the alphabet. The results are then stored in an encrypted table and when the edit distance is calculated, we can use the unencrypted data to obtain the encrypted equality information. For instance, if the word abbey is the encrypted string and only encodes lowercase letters, we would store 5 26 elements, as shown in Table 4. Note that it is also possible to store eq9 = enc(9) and can therefore be used directly in the key calculation. The preprocessing step has cost of Σ m, where is the length of the encrypted string and Σ is the size of the character set. This is specifically useful in the case of matching DNA sequences, we would only need to store the 4 characters. Moreover, if during encryption we know we will only deal with specific subset of characters Σ, we can reduce the cost to by constructing table only for the characters in S. As an example, this could be the case with names where characters like and might not occur in the unencrypted query string. standard edit distance calculation requires mn equality operations. Therefore, if Σ < (or < n), this optimisation becomes advantageous. For example, with full ASCII, the method becomes more efficient for > 128. For Title Suppressed Due to Excessive Length 21 Table 4: Preprocessed storage of the word abbey using only lowercase letters. character 1 2 3 4 c ... ... enc(1) enc(0) enc(0) enc(0) enc(0) enc(0) enc(1) enc(1) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(0) enc(1) enc(0) ... enc(0) enc(0) enc(0) enc(0) enc(1) ... ... ... ... ... ... ... ... ... lowercase letters, it applies when > 26, and for DNA sequences, it becomes beneficial when > 4. This approach is particularly useful when matching plaintext string against database. In the case where the database is unencrypted and the query is encrypted, one needs to do the query only one time and the result can be used for multiple lookups. If the database is encrypted and the query is not encrypted, one can preprocessing each encrypted string in the database in an offline preparation phase. This means that future plaintext equality checks can be done without the need for additional PBS. Once new plaintext string needs to be matched, one only needs to perform lookup in the table."
        },
        {
            "title": "6 Results",
            "content": "In this section, we will compare our Leuvenshtein algorithm to the state-of-theart in FHE edit distance calculations. challenge in analysing the efficiency of our improvements is that there is only one implementation of the Levenshtein distance available in TFHE, which is an implementation that is used as demonstrator for the concrete compiler [Zam22a]. To have more comparison points, we implemented standard versions of the Wagner-Fischer and Myers algorithms using the TFHE-rs library. Both algorithms rely on standard 2-bit message and 2-bit carry ciphertexts as fundamental building blocks. In our experiments, we used ASCII encoding to encrypt each character. The Wagner-Fischer and Myers algorithms are implemented using the state-of-the-art equality check techniques as available in the TFHE-rs library. Our algorithm uses both our improved Levenshtein (algorithm 1) and our improved equality check (algorithm 3). complete overview of the algorithm is given in Algorithm 4 on page 32. We discern four versions of our algorithm in our experiments: Leuvenshtein Exact: Full calculation of all the cells in the edit distance algorithm. 22 Legiest et al. Leuvenshtein Exact Skipping: Calculation of all the cells that are relevant for the end result, but skipping irrelevant cells as discussed in subsection 3.2. ℓ = n/4 ℓ = n/4: Calculation of the approximate edit disLeuvenshtein Approx. ℓ = n/4 tance that is accurate for distances lower than m/4 as discussed in subsection 3.2. To simplify the discussion we assume = for the approximate results. ℓ = 10 ℓ = 10: Calculation of the approximate edit disLeuvenshtein Approx. ℓ = 10 tance that is accurate for distances lower than 10 as discussed in subsection 3.2."
        },
        {
            "title": "6.1 Counting the bootstraps",
            "content": "In this section, we analyse the theoretical cost of the algorithms, focusing on the number of bootstraps required. Due to the insertion of auxiliary PBS operations for message-carry maintenance in the TFHE-rs library, determining the exact number of PBS used in theory is not trivial. We experimentally determined the average number of PBS per cell in the WF and Myers algorithms by running the program and counting the PBS. For the Leuvenshtein algorithm, we derive first-order estimate based on the bootstraps incurred by non-linear operations within each cell, excluding incidental bookkeeping bootstraps introduced for noise management. Table 5: Overview of the PBS load of the different algorithm to match an nelement with an m-element string. The preprocessing and Levenshtein column denote the operations per cell, the last column is first-order approximation of the total cost over all cells. Algorithm WF Myers Leuvenshtein Exact Leuvenshtein Exact Skipping Leuvenshtein Approx. ℓ = m/4 Leuvenshtein Approx. ℓ = 10 Preprocessing Levenshtein Total 5 5 2 2 2 2 28 13 1 1 1 1 33 mn 18 mn 3 mn 2.25 mn 6 ℓ 60 From Table 5 one can see that our algorithm significantly reduces the number of required bootstraps to compute cell in the encrypted domain. Firstly, we need only 2.5 fewer PBS for equality calculations. Secondly, we achieve reduction of 94 and 16 less PBS compared to the Wagner-Fischer and Myers algorithms, respectively. Overall, for the full distance calculation, our best exact method demonstrates 44 reduction in the number of PBS required over the Wagner-Fischer algorithm and an 9.33 improvement over the Myers algorithm. Title Suppressed Due to Excessive Length 23 6."
        },
        {
            "title": "Implementation results",
            "content": "We implemented the Wagner-Fischer, Myers, and our custom algorithm using Rust v1.80.0, TFHE-rs v0.7.2 [Zam22b] and concrete compiler v2.8.1 [Zam22a] on an Ubuntu 22.04 system. All experiments were conducted on dual AMD EPYC 9174F 16-Core Processor (for total of 64 threads). The parameter set used for the Wagner-Fischer and Myers implementations are 2-bit message and 2-bit carry ciphertext which is the most standard choice., For the Leuvenshtein implementation, we opted not to use the carry space, instead employing 4-bit message 0-bit carry parameter set. This implementation choice makes the implementation easier, but has little effect on the performance. All timing results represent the full calculation of the edit distance, including both preprocessing and the main algorithm. Each experiment was conducted using the sequential implementation of the shortint API. We will discuss parallelisation options for these algorithms in next section. Table 6: Latency results in seconds of the calculation of the edit distance, using ASCII encoding, in the encrypted domain. = 8 = 100 = 256 [CKL15]a [Zam22a] WF Myers Lvs Exact Lvs Exact Skip Lvs Apx. ℓ = 4 Lvs Apx. ℓ = 10 Seq. 27.54 241.10 77.59 17.81 2.83 2.28 1.50 Seq. Seq. 1 3.1 14 85 106 161 12h 36m 3h 24m 38m 12s 7m 19s 5m 35s 3m 19s 1m 26s 1 3.7 20 103 135 228 527 6d 21hb 22h 19m 4h 7m 48m 23s 35m 41s 20m 52s 3m 50s 1 7 40 205 278 475 2588 Using the DGHV scheme, an 80-bit security level and other hardware. Extrapolated based on 24300 cell calculations. The results in this table demonstrate that our Leuvenshtein algorithm significantly outperforms state-of-the-art TFHE algorithms. Specifically, we achieve up to 278 speedup over the best available algorithm, and 39 speedup over our Wagner-Fisher implementation. While we have included the numbers for edit distance for second generation FHE, these results are run with different hardware and lower security settings, and are therefore not suitable for fair comparison. Additionally, skipping irrelevant cells as discussed in subsection 3.2 is an efficient way to optimise the algorithm by 25% for exact calculations, and limiting the accuracy for large edit distances can give another significant efficiency 24 Legiest et al. improvement. These results clearly show that our algorithm consistently outperforms existing approaches across all scenarios. Parallelism: Previous results were achieved using the sequential TFHE-rs parameter set. Greater efficiency can be attained by developing parallel implementations. While comprehensive investigation into parallelisation is left for future work, we will provide some preliminary insights and suggestions here. In general one can think of three levels at which to parallelise: Parallelisation over operations: Some operations require multiple PBS operations that can be performed in parallel. This is the case for for example additions or min function over larger integers (as used in Wagner-Fisher) or Boolean operations on multiple inputs at the same time (as used in Myers). TFHE-rs has parallel implementation available for these operations. This strategy can not be used for our method, as we only have 1 PBS per cell of the alogrithm. Parallelisation over cells: The edit distance algorithm allows for parallelisation across multiple cells that are independent of one another. Specifically, cells equidistant from the first cell can be computed simultaneously. This approach enables efficient parallel computation for the majority of the cells, with only the first and last cells experiencing limited parallelisation. For large string sizes, this strategy achieves near-complete parallelisation, significantly enhancing execution efficiency. The implementation of this parallelisation method is deferred to future work. Batch inputs matching: In case of multiple edit distance calculations that need to be performed at the same time, one can easily parallelise the calculation by performing all lookups in parallel. This is especially relevant to parallellise the workload for database lookup scenarios. Table 7 shows comparison of the algorithms under parallel computations. Execution using the TFHE-rs parallel API shows only modest impact on execution time (approximately 2.4), highlighting the need to explore alternative sources of parallelism. In contrast, our batched inputs significantly accelerate computation, achieving near-optimal speedup of 32, which aligns with full core utilisation of our CPU. This demonstrates that batching is an effective strategy for maximising parallelism and fully utilising server resources. However, for single lookups, alternative parallelisation approaches, such as parallelisation over cells, will need to be explored. In certain cases, such as the Myers approach, the optimal 32 speed-up is not fully realised. This is due to memory management factors, including cache saturation and reliance on RAM, which could be addressed with careful memory handling. Preprocessing: In section 5 we discussed preprocessing when one of the strings is not encrypted. Table 8 compares execution of the algorithm with and without Title Suppressed Due to Excessive Length 25 Table 7: Latency in seconds of parallelised implementation using 64 threads each calculating one Levenshtein distance. The first results of WF and Myers in the table only use the parallel TFHE-rs API, the following results use both the parallel API and batched inputs. Relative speedups compare with sequential results from Table 6. WF (TFHE-rs parallel ops.) Myers (TFHE-rs parallel ops.) WF (batch inputs) Myers (batch inputs) Lvs Exact Lvs Exact Skip Lvs Exact ℓ = 4 Lvs Exact ℓ = = 8 = 100 = 256 Lat. 32.28 12.00 2.60 0. 0.15 0.14 0.11 - Lat. Lat. 2.4 1.5 1h 29m 2.4 2.5 15m 28s 2.4 9h 8m 1h 41m 2.4 29.8 6m 30s 19.5 2m 45s 31.4 42m 52s 13.9 18m 30s 18.9 14.7 16.3 11.3 13.6 6.88 3.39 - 29.9 1m 35 29.6 1m 12 28.9 43.7 25.4 10.4 31.2 13.4 30.5 29.7 28.6 22.1 Table 8: Latency results for the case of one unencrypted string, both the latency of the building up the preprocessing table and main algorithm are given. All results in seconds. The relative speedup is given for the main calculation. = 8 = 100 = 256 pre main pre main pre main Lvs Exact - No Prep. Lvs Exact Lvs Exact Skip - No Prep. Lvs Exact Skip Lvs Exact ℓ = 4 - No Prep. Lvs Exact ℓ = 4 Lvs Exact ℓ = 10 - No Prep. Lvs Exact ℓ = 10 2.83 0.93 2.28 0.75 1.5 0.49 - 29. - 29.5 - 29.6 - - 1 3 369 - 1 3 367 - 1 3 368 - 370 439 146 335 199 65 86 29 - 1 3 942 - 1 3 - 1 3 946 2903 950 2141 717 1252 420 - 1 3 941 230 77 1 3 1 3 1 3 1 3 preprocessing, based on the scenarios outlined above. All of the calculations are done using our Leuvenshtein algorithm. From this table we can see that preprocessing reduces the computation with factor 3, as can be expected from the fact that character equality costs 2 PBS per cell while the Levenshtein calculation costs 1 PBS per cell. Note that the preprocessing numbers can be improved more if some ASCII characters are not used. In general, one can discern three scenarios where preprocessing is useful: 26 Legiest et al. Single Lookup: In this scenario, speedup is achievable when the alphabet size is smaller than the string length (i.e., Σ < n), as discussed in section 5. From the table, one can observe this effect for = 256, where the total cost of preprocessing plus the main algorithm is lower than the cost without preprocessing. Encrypted query against large unencrypted database: In this scenario, preprocessing needs to be performed only once. After that, only the main algorithm is executed. For large datasets, this approach clearly demonstrates speedup approaching factor of 3. Unencrypted query against an encrypted database: Here, the database can undergo one-time preprocessing step, either in plaintext or in the encrypted domain. Following this, only the main computation cost is incurred, resulting in similar speedup of 3."
        },
        {
            "title": "6.3 Comparison with MPC implementations",
            "content": "In Multi-Party Computation (MPC) settings, trade-off exists between computation time and bandwidth. Both FHE and MPC enable privacy-preserving computations, but directly comparing their specific security guarantees is challenging due to their differing threat models and trust assumptions. MPC categorises adversarial models into two main types: passive MPC, also called the honest-but-curious model, which operates under the assumption that compromised parties will adhere to the protocol while trying to infer confidential information. On the other hand, active MPC is designed to withstand adversaries who may deviate from the protocol in arbitrary ways to compromise correctness or obtain sensitive data. Vanegas et al. [VCA23] implemented edit distance calculation in an MPC setting, optimising their implementation for DNA strings (i.e., 2-bit characters). They considered an passive and active security model with LAN network delays for both Garbled Circuit (GC) and secret-sharing schemes operating over Z2k domain. Table 9: Evaluation of the edit distance calculations using LAN setting MPC and FHE solutions for matching 210-length DNA string. All tested on an c6a.4xlarge AWS EC2 instance. Situation Security Algorithm Lat. [s] Data sent [MB] Passive Yaos GC Semi2k 2.7 103.0 MPC [VCA23] Active FHE - BMR-MASCOT 9, 034.0 SPDZ 368.5 2k Lvs Exact Skip 1820.8 345.8 113.1 2.09 106 14,893.8 1.38 Title Suppressed Due to Excessive Length 27 For fair comparison, we evaluated our algorithm using DNA strings of 210 characters in length on the same AWS EC2 instance of type c6a.4xlarge4. Specifically, only one PBS was utilised for the equality calculation, leaving the remainder of the algorithm unchanged. In addition, our solution easily generalises to arbitrary character sets and is not restricted to domain-specific inputs. As shown in Table 9, compared to the passive MPC case, our solution is 674 slower but requires 251 less data to be sent. Compared to the active case, our solution is only 4.94 slower but requires 10 792 less data. In other experiments, Vanegas et al. [VCA23] calculate the edit distance for DNA strings of length 1020. All implementations considered (plaintext, FHE, and MPC) exhibit quadratic cost growth as the input string lengths increase. This trend is evident in our results for string lengths between 100 and 256 and is expected to hold for even larger input sizes. Consequently, the cost scaling between FHE and MPC remains independent of input length, meaning that the ratio of their computational costs remains approximately constant, regardless of how long the input strings are. It is also worth noting that the approximate algorithm demonstrates significantly better performance due to its linear complexity. However, since this linear behaviour is consistent across all implementations (plaintext, FHE, and MPC), it does not affect the relative cost comparison between them."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduces novel method for efficiently computing the edit distance on encrypted data within the TFHE framework. Our first contribution demonstrates how to streamline edit distance calculations by employing compact ternary representation, reusing programmable bootstrapping (PBS) results, and computing three-input minimum function in single lookup. The resulting Leuvenshtein algorithm achieves 94 reduction in the number of PBS operations compared to traditional approaches. Our second contribution enhances equality checks, particularly for mediumsized inputs. For ASCII encoding, we reduced the lookup cost from 5 PBS operations in the state-of-the-art to just 2 PBS operations. Finally, we introduced preprocessing stage that precalculates equality checks when one of the inputs is unencrypted, enabling an additional speedup. For ASCII inputs, this approach achieves up to 3 improvement. Our implementation results demonstrate that the Leuvenshtein algorithm delivers speedups of up to 278 over the best available implementation, underscoring its efficiency. These optimisations significantly advance the practicality of encrypted edit distance computations, reducing computational overhead and enhancing scalability for real-world applications. 4 https://aws.amazon.com/ec2/instance-types/c6a/ Legiest et al."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by the Horizon 2020 ERC Advanced Grant (101020005 Belfort) and the CyberSecurity Research Flanders with reference number VOEWICS02. Wouter Legiest is funded by FWO (Research Foundation Flanders) as Strategic Basic (SB) PhD fellow (project number 1S57125N)."
        },
        {
            "title": "References",
            "content": "[AAM17] Md Momin Al Aziz, Dima Alhadidi, and Noman Mohammed. Secure approximation of edit distance on genomic data. BMC Med. Genomics, 10(S2), July 2017. [AHLR18] Gilad Asharov, Shai Halevi, Yehuda Lindell, and Tal Rabin. Privacypreserving search of similar patients in genomic data. Proceedings on Privacy Enhancing Technologies, 2018(4):104124, October 2018. [AQA21] Mohannad Alkhalili, Mahmoud Qutqut, and Fadi Almasalha. Investigation of applying machine learning for watch-list filtering in anti-money laundering. iEEE Access, 9:1848118496, 2021. [BDL+19] Jasper Beernaerts, Ellen Debever, Matthieu Lenoir, Bernard De Baets, and Nico Van de Weghe. method based on the levenshtein distance metric for the comparison of multiple movement patterns described by matrix sequences of different length. Expert Systems with Applications, 115:373385, 2019. [BGV14] Zvika Brakerski, Craig Gentry, and Vinod Vaikuntanathan. (leveled) fully homomorphic encryption without bootstrapping. ACM Trans. Comput. Theory, 6(3):13:113:36, 2014. [CGGI20] Ilaria Chillotti, Nicolas Gama, Mariya Georgieva, and Malika Izabachène. TFHE: fast fully homomorphic encryption over the torus. J. Cryptol., 33(1):3491, 2020. [CJL+20] Ilaria Chillotti, Marc Joye, Damien Ligier, Jean-Baptiste Orfila, and Samuel Tap. Concrete: Concrete operates on ciphertexts rapidly by extending tfhe. In WAHC 2020-8th Workshop on Encrypted Computing & Applied Homomorphic Cryptography, 2020. [CKKS17] Jung Hee Cheon, Andrey Kim, Miran Kim, and Yong Soo Song. HoIn momorphic encryption for arithmetic of approximate numbers. Tsuyoshi Takagi and Thomas Peyrin, editors, Advances in Cryptology - ASIACRYPT 2017 - 23rd International Conference on the Theory and Applications of Cryptology and Information Security, Hong Kong, China, December 3-7, 2017, Proceedings, Part I, volume 10624 of Lecture Notes in Computer Science, pages 409437. Springer, 2017. [CKL15] Jung Hee Cheon, Miran Kim, and Kristin E. Lauter. Homomorphic computation of edit distance. In Financial Cryptography Workshops, volume 8976 of Lecture Notes in Computer Science, pages 194212. Springer, 2015. [Cou15] Council of European Union. Consolidated text: Directive (eu) 2015/2366 of the european parliament and of the council of 25 november 2015 on payment services in the internal market, amending directives 2002/65/ec, 2009/110/ec and 2013/36/eu and regulation (eu) no 1093/2010, and repealing directive 2007/64/ec (text with eea relevance). OJ, 337:35, 2015. 30 Legiest et al. [EC22] EC. Legislative proposal on instant payments. European Commission, 2022. [FSB21] FSB. G20 targets for enhancing cross-border payments. Financial Stability Board, Bank for International Settlements, 2021. [FV12] Junfeng Fan and Frederik Vercauteren. Somewhat practical fully IACR Cryptol. ePrint Arch., page 144, homomorphic encryption. 2012. [GBP+23] Robin Geelen, Michiel Van Beirendonck, Hilder V. L. Pereira, Brian Huffman, Tynan McAuley, Ben Selfridge, Daniel Wagner, Georgios D. Dimou, Ingrid Verbauwhede, Frederik Vercauteren, and David W. Archer. BASALISC: programmable hardware accelerator for BGV fully homomorphic encryption. IACR Trans. Cryptogr. Hardw. Embed. Syst., 2023(4):3257, 2023. [Joy22] Marc Joye. SoK: Fully homomorphic encryption over the [discretized] torus. IACR Transactions on Cryptographic Hardware and Embedded Systems, 2022(4):661692, 2022. [Knu73] Donald E. Knuth. The Art of Computer Programming, Volume III: Sorting and Searching. Addison-Wesley, 1973. [MA17] Nick Maxwell and David Artingstall. The role of financial information-sharing partnerships in the disruption of crime. Royal United Services Institute for Defence and Security Studies, FFIS, 2017. [Max21] Nick Maxwell. Innovation and discussion paper: Case studies of the use of privacy preserving analysis to tackle financial crime. Royal United Services Institute for Defence and Security Studies, FFIS, 2021. [MP80] William J. Masek and Mike Paterson. faster algorithm computing string edit distances. J. Comput. Syst. Sci., 20(1):1831, 1980. [Mye86] Eugene W. Myers. An O(ND) difference algorithm and its variations. Algorithmica, 1(2):251266, 1986. [Mye99] Gene Myers. fast bit-vector algorithm for approximate string matching based on dynamic programming. J. ACM, 46(3):395415, 1999. [Nav01] Gonzalo Navarro. guided tour to approximate string matching. ACM Comput. Surv., 33(1):3188, 2001. [PHRL19] Alexander Payne, Nadine Holmes, Vardhman K. Rakyan, and Matthew Loose. Bulkvis: graphical viewer for oxford nanopore bulk FAST5 files. Bioinform., 35(13):21932198, 2019. [SAE+08] Zhan Su, Byung-Ryul Ahn, Ki-Yol Eom, Min-Koo Kang, Jin-Pyung Kim, and Moon-Kyun Kim. Plagiarism detection using the levenshtein distance and smith-waterman algorithm. In 2008 3rd International Conference on Innovative Computing Information and Control, pages 569569, 2008. [Ukk85a] Esko Ukkonen. Algorithms for approximate string matching. Inf. Control., 64(1-3):100118, 1985. Title Suppressed Due to Excessive Length 31 [Ukk85b] Esko Ukkonen. Finding approximate patterns in strings. J. Algorithms, 6(1):132137, 1985. [Ukk92] Esko Ukkonen. Approximate string matching with q-grams and maximal matches. Theor. Comput. Sci., 92(1):191211, 1992. [VCA23] Hernán Vanegas, Daniel Cabarcas, and Diego F. Aranha. Privacypreserving edit distance computation using secret-sharing two-party computation. In LATINCRYPT, volume 14168 of Lecture Notes in Computer Science, pages 6786. Springer, 2023. [vDTV23] Michiel van Beirendonck, Jan-Pieter DAnvers, Furkan Turan, and Ingrid Verbauwhede. FPT: fixed-point accelerator for torus fully homomorphic encryption. In ACM CCS 2023: 30th Conference on Computer and Communications Security, pages 741755. ACM Press, November 2023. [Vin68] T. K. Vintsyuk. Speech discrimination by dynamic programming. Cybernetics, 4:5257, 1968. [WF74] Robert A. Wagner and Michael J. Fischer. The string-to-string correction problem. J. ACM, 21(1):168173, 1974. [Zam22a] Zama. Concrete: TFHE Compiler that converts python programs into FHE equivalent, 2022. https://github.com/zama-ai/concrete. [Zam22b] Zama. TFHE-rs: Pure Rust Implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data, 2022. https://github.com/zama-ai/tfhe-rs. [ZLS+19] Yandong Zheng, Rongxing Lu, Jun Shao, Yonggang Zhang, and Hui Zhu. Efficient and privacy-preserving edit distance query over encrypted genomic data. In 2019 11th International Conference on Wireless Communications and Signal Processing (WCSP), pages 1 6, 2019. Legiest et al."
        },
        {
            "title": "8 Our complete algorithm",
            "content": "Algorithm 4 Complete ASCII-based Encrypted Levenshtein 1: function Edit Distance(x1..m, y1..n, ℓ) Setup 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: OneMatrix[0..m, 0..n] OneMatrix[0..m, 0..n] LUT-eq9 [9, 0, 0, . . . ] LUT-eq [1, 0, 0, . . . ] LUT-min[key] Table 2 for 1 to do v[i, 0] 1 for 1 to do h[0, j] Main Algorithm for 1 to do for 1 to do if j < ℓ then y(1) z1 x(1) eq1 PBS(z1; LUT-eq) eq1 1 eq1 z2 x(2) y(2) z2 2 z2 + eq1 eq9 PBS(z2; LUT-eq9) Hin h[i 1, j] + 1 Vin v[i, 1] + 1 key (1 v[i, 1]) + 3 (1 + h[i 1, j]) + eq9[i, j] min PBS(key, LUT-min) v[i, j] min h[i 1, j] h[i, j] min v[i, 1] return + Σn i=0h[m, i]"
        }
    ],
    "affiliations": [
        "COSIC, KU Leuven",
        "Society for Worldwide Interbank Financial Telecommunication (Swift)"
    ]
}