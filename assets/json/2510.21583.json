{
    "paper_title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation",
    "authors": [
        "Yifu Luo",
        "Penghui Du",
        "Bo Li",
        "Sinan Du",
        "Tiantian Zhang",
        "Yongzhe Chang",
        "Kai Wu",
        "Kun Gai",
        "Xueqian Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods."
        },
        {
            "title": "Start",
            "content": "SAMPLE BY STEP, OPTIMIZE BY CHUNK: CHUNKLEVEL GRPO FOR TEXT-TO-IMAGE GENERATION , Penghui Du2 Yifu Luo1, 2 Kai Wu2 (cid:66), Kun Gai2, Xueqian Wang1 (cid:66) 1Tsinghua University, 2Kolors Team, Kuaishou Technology , Bo Li2 , Sinan Du1,2, Tiantian Zhang1, Yongzhe Chang1, 5 2 0 O 4 2 ] . [ 1 3 8 5 1 2 . 0 1 5 2 : r Figure 1: Chunk-GRPO significantly improves image quality, particularly in structure, lighting, and fine-grained details, demonstrating the superiority of chunk-level optimization."
        },
        {
            "title": "ABSTRACT",
            "content": "Group Relative Policy Optimization (GRPO) has shown strong potential for flowmatching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent chunks that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods. Equal Contibution. Project Lead. (cid:66)Corresponding Authors. Work done during internship in Kolors Team, Kuaishou Technology."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning (RL)(Sutton et al., 1998; Schulman et al., 2017) has recently found success beyond traditional domains, particularly in the reasoning of Large Language Models (LLMs)(Jaech et al., 2024; Guo et al., 2025). Inspired by these advances, recent works(Xue et al., 2025; Liu et al., 2025b; Wang & Yu, 2025) have explored applying RL to text-to-image (T2I) generation for aligning specific preferences. In this context, Group Relative Policy Optimization (GRPO)(Shao et al., 2024; Guo et al., 2025) has emerged as promising approach for flow-matching-based T2I generation (Lipman et al., 2022; Liu et al., 2023; Esser et al., 2024). GRPO-based methods typically sample group of images from the same prompt, evaluate them using reward models, convert the rewards into group relative advantages, and then assign these advantages equally across all timesteps during optimization. While effective, this uniform assignment introduces two key limitations: (1) inaccurate advantage attribution, and (2) disregard for the temporal dynamics of generation. We first illustrate the former in Figure 2, and discuss temporal dynamics later. Consider two generation trajectories from the same prompt in Figure 2, each consisting of three timesteps. Although the final advantage correctly favors the better trajectory (Trajectory1), assigning this same advantage uniformly across all timesteps incorrectly assumes that every step in Trajectory1 is superior to its counterpart in Trajectory2. However, at timestep = 1 Trajectory2 is clearly better than Trajectory1, despite Trajectory1 achieving the higher overall reward. Figure 2: While Trajectory1 has the greater final reward (advantage), its = 1 timestep is worse than that in Trajectory2. However, GRPO assigns the final advantages equally across all timesteps. To address this, we draw inspiration from action chunking (Zhao et al., 2023; Li et al., 2025b) in robotics, which predicts sequences of consecutive actions jointly rather than treating each step independently. In similar spirit, we propose to group consecutive timesteps into chunks, and optimize at the chunk level rather than the step level. This alleviates the issue of inaccurate advantage attribution, as we analyze in detail in Section 4.1. Related ideas have been explored in LLMs as Group Sequence Policy Optimization (GSPO) (Zheng et al., 2025), where an entire token sequence is treated as single unit (analogous to viewing the whole trajectory as one chunk). However, our preliminary studies reveal that different chunk settings (e.g. how many consecutive timesteps for chunk) have substantial impact on performance. We argue that this is due to the overlooking of temporal dynamics of flow matching generation, which we proposed before. Different from LLMs, flow matching exhibits distinct temporal dynamics: each timestep operates under different noise conditions and contributes differently to the final image. Specifically, following (Wimbauer et al., 2024; Liu et al., 2025a) , we analyze the relative L1 distance of intermediate latents. As shown in Figure 3, the results reveal clear, prompt-invariant dynamic patterns that naturally segment the trajectory into meaningful chunks. These observations suggest that chunks should not be arbitrary but guided by the inherent temporal dynamics, with timesteps that are dynamically correlated optimized together. Based on these, we propose Chunk-GRPO, novel chunk-level RL approach for flow-matchingbased T2I generation. As demonstrated in Figure 4, our key innovation is grouping timesteps into chunks that reflect temporal dynamics, and optimizing them as units with principled chunk-level importance ratio. Furthermore, motivated by the varying contributions of different chunks, we design an optional weighted sampling strategy to further boost Chunk-GRPOs performance. Our contributions can be summarized as follows: We are the first to introduce the chunk-level RL optimization for T2I generation. We pinpoint that chunk-level optimization alleviates the inaccurate advantage attribution and mitigates the neglect of temporal dynamics from GRPO-based approaches."
        },
        {
            "title": "Preprint",
            "content": "We propose Chunk-GRPO, novel chunk-level approach for flow-matching-based T2I generation, which integrates chunk-level optimization with temporal-dynamic-guided chunking. An optional weighted sampling strategy is introduced to push Chunk-GRPO further. Extensive experiments demonstrate that Chunk-GRPO achieves superior performance on preference alignment and standard T2I benchmarks, highlighting the effectiveness of chunk-level optimization."
        },
        {
            "title": "2.1 ACTION CHUNK",
            "content": "Action chunk (Zhao et al., 2023; Lai et al., 2022) has been widely applied to robotics Chi et al. (2023). This approach mitigates compounding error and non-Markovian noise in human demonstrations by jointly predicting sequence of future actions rather than single step. By shortening the effective control horizon, action chunking enables smoother and more stable rollouts. Recently, it has also proven effective in vision-language-action (VLA) models (Black et al., 2024a; Intelligence et al.) and in RL (Li et al., 2025b). These successes suggest that chunking stabilizes long-horizon prediction, accelerates value propagation, and more effectively leverages non-Markovian behavior. 2.2 REINFORCEMENT LEARNING FOR DIFFUSION-BASED IMAGE GENERATION Diffusion models (Ho et al., 2020; Rombach et al., 2022; Podell et al., 2023; Labs et al., 2025; Wu et al., 2025) have become one of the dominant paradigms for T2I generation. Early works (Xu et al., 2023; Black et al., 2024b; Fan et al., 2023) introduced RL into diffusion models through policy gradient optimization. Preference-based methods (Wallace et al., 2024; Sun et al., 2025a;c;d;e) were later developed, achieving competitive alignment without explicit reward modeling. More recently, GRPO (Shao et al., 2024; Sun et al., 2025b) has attracted attention as an efficient alternative. Dance-GRPO (Xue et al., 2025) and Flow-GRPO (Liu et al., 2025b) pioneered the use of GRPO for T2I generation, unifying diffusion and flow matching through an SDE-based reformulation. MixGRPO (Li et al., 2025a) further improved efficiency via mixed ODESDE paradigm. TempFlow-GRPO (He et al., 2025) introduced temporal-aware weighting across denoising steps. Pref-GRPO (Wang et al., 2025) identified the issue of illusory advantage and reformulated the optimization objective as pairwise preference fitting. BranchGRPO (Li et al., 2025c) restructured the rollout process into branching tree, amortizing computation across shared prefixes. In contrast to these works, our approach explicitly addresses two key issues in GRPO-based T2I generation: (1) inaccurate advantage attribution, and (2) neglect of temporal dynamics. By introducing chunk-level optimization guided by the inherent temporal structure of flow matching, we enhance GRPO from the perspective of optimization granularity."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "3.1 FLOW MATCHING Suppose that x0 X0 is data sample from the true distribution, and x1 X1 is noise sample. Following (Liu et al., 2023), the intermediate noised samples xt can be expressed as: xt = (1 t)x0 + tx1, (1) where [0, 1] denotes the noise level. Then, flow matching aims to directly regress the estimated velocity field ˆvθ(xt, t) by minimizing the objective function (Lipman et al., 2022): LFM(θ) = Et,x0X0,x1X1 [v ˆvθ(xt, t)2 2], where = x1 x0 represents the target velocity field. Furthermore, deterministic Ordinary Differential Equation (ODE) is utilized to model the forward process of flow matching: (2) dxt = ˆvθ(xt, t)dt. (3)"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: The prompt-invariant temporal dynamics of flow matching."
        },
        {
            "title": "3.2 GRPO ON FLOW MATCHING",
            "content": "As an RL algorithm, GRPO (Guo et al., 2025; Shao et al., 2024) effectively eliminates the need for an additional critic model by estimating the baseline through group-wise relative rewards. In line with the settings of DDPO (Black et al., 2024b), GRPO is also applied in flow matching. Given group of images {xi i=1 generated from the same prompt c, the advantage corresponding to the i-th sample is formulated as: 0}G 0, c) mean({r(xj 0, c)}G always keeps the same for any timestep t. For simplicity, we neglect the subscript Notice that Ai and denote it as Ai. The policy is updated by maximizing the following GRPO objective: 0, c)}G j=1) std({r(xj j=1) r(xi = Ai (4) . J(θ) = Ec,{xi}G (cid:34) 1 1 i=1 (cid:88) (cid:88) i=1 t=1 (cid:0)min (cid:0)ri (θ) Ai, clip (cid:0)ri (θ) , 1 ϵ, 1 + ϵ(cid:1) Ai(cid:1) βDKL (πθπref )(cid:1) (cid:35) , (5) Where ri denotes the importance ratio: pθ(xi pold(xi Furthermore, to meet the exploration requirement of RL, Flow-GRPO (Liu et al., 2025b) and DanceGRPO (Xue et al., 2025) introduce stochasticity into flow matching by transforming the deterministic ODE into an equivalent Stochastic Differential Equation (SDE): t1xi t1xi t, c) t, c) ri t(θ) = (6) . dxt = (cid:0)vθ(xt, t) + σ2 2t (xt + (1 t)vθ(xt, t))(cid:1)dt + σtdwt, (7) where dwt represents the increments of the Wiener process and σt controls the stochasticity."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we begin by introducing chunk-level optimization for GRPO and show why it improves upon standard step-level GRPO in Section 4.1. Next, we describe how to set chunks using the inherent temporal dynamics of flow matching in Section 4.2. Finally, we present our proposed Chunk-GRPO along with an optional weighted sampling strategy in Section 4.3. 4.1 CHUNK-LEVEL OPTIMIZATION FOR GRPO Recall the example in Figure 2. With standard step-level GRPO loss in Equation (5), the optimization object for timesteps = 1 and = 2 in the two trajectories is: J(θ) = 1 1 2 (cid:88) 2 (cid:88) i=1 t=1 (cid:0)min (cid:0)ri (θ) Ai, clip (cid:0)ri (θ) , 1 ϵ, 1 + ϵ(cid:1) Ai(cid:1) βDKL (πθπref )(cid:1) . (8)"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The overall framework of Chunk-GRPO. Chunk-GRPO integrates chunk-level optimization with temporal-dynamic-guided chunking, based on the grounded defined chunk-level importance ratio r. It also introduces an optional weighted sampling strategy, assigning sampling weight for each chunk. As discussed in Section 1, this uniform stepwise assignment introduces inaccurate advantage attribution. To alleviate this, the first principle of chunk-level optimization is to group consecutive timesteps into chunks and optimize each chunk as unit. In the example case, the optimization then becomes: J(θ) = 1 2 (cid:88) i= (cid:0)min (cid:0)ri (θ) Ai, clip (cid:0)ri (θ) , 1 ϵ, 1 + ϵ(cid:1) Ai(cid:1) βDKL (πθπref )(cid:1) , (9) where the importance ratio is redefined over the chunk likelihood: ri(θ) = (cid:32) 2 (cid:89) t= pθ pold (cid:0)xi (cid:0)xi t1xi t1xi t, c(cid:1) t, c(cid:1) (cid:33) 1 . (10) The key underlying proposition is as follows: Proposition 1. When advantage attribution is inaccurate at individual timesteps, optimizing them jointly within chunk yields better performance than optimizing them independently as steps, especially when the chunk size is small(e.g. chunk size of 5). mathematical analysis is provided in Section A. With this insight, we formally define chunk-level optimization for GRPO as follows: Given an image generation trajectory: (xT , xT 1, , x2, x1, x0)i, (11) we split it into different chunks : {ch1, ch2, , chK}i = {(xT , , xT cs1+1), (xT cs1 , , xT cs1cs2+1), , ( , x1)}i, (cid:88) j=1 csi = T, (12) where csj denotes the chunk size of the j-th chunk chj. The chunk-level optimization objective is then: J(θ) = Ec,{xi}G i=1 1 1 (cid:88) (cid:88) i=1 j=1 (cid:0)min (cid:0)ri (θ) Ai, clip (cid:0)ri (θ) , 1 ϵ, 1 + ϵ(cid:1) Ai(cid:1) βDKL (πθπref )(cid:1) , (13) we neglect xi 0 because there is no more transition into xi"
        },
        {
            "title": "Preprint",
            "content": "where we redefine the importance ratio ri j(θ) based on chunk likelihood: ri j(θ) = (cid:89) tchj pθ pold (cid:0)xi (cid:0)xi t1xi t1xi t, c(cid:1) t, c(cid:1) 1 csj . (14) Thus, optimization shifts from step-level to chunk-level, alleviating the issue of inaccurate advantage attribution. Notably, setting = 1 will group the whole trajectory into single chunk, and the optimization further shifts to sequence-level similar to GSPO (Zheng et al., 2025). Conversely, setting = will force csj = 1, and the optimization reverts to standard step-level GRPO. The central question then becomes: given the many possible chunk configurations (chj and csj), how should chunks be determined?"
        },
        {
            "title": "4.2 CHUNK WITH TEMPORAL DYNAMICS",
            "content": "Before diving into the deeper analysis, we first designed toy experiment, where all chunks are fixed with an equal chunk size cs1 = cs2 = csk. As shown in Figure 5, performance varies with chunk size, indicating that chunk design is non-trivial. We attribute this to the inherent temporal dynamics of flow matching. Unlike LLMs, flow matching consists of time-dependent dynamics in the generation process, where different timesteps contribute unequally to image quality. To better understand this, following (Wimbauer et al., 2024; Liu et al., 2025a), we illustrate the relative L1 distance L1rel(x, t) throughout the generation process: Figure 5: Performance varies with different chunk sizes. The TD refers to temporal dynamics. L1rel(x, t) = xt xt11 xt1 . (15) As shown in Figure 3, L1rel(x, t) exhibits prompt-invariant yet timestep-dependent patterns. large L1rel(x, t) indicates rapid latent changes, while small value indicates that adjacent latents are similar to each other. From this observation, we argue that: Timesteps with similar dynamics should be grouped into the same chunk, while timesteps with different dynamics should be separated into different chunks. Fortunately, the prompt-invariant dynamic patterns of L1rel(x, t) naturally segment the trajectory into meaningful chunks, yielding temporal-dynamic-guided chunks. Thus, we can set chunks based on the relative L1 distance, aligning the optimization process with the intrinsic temporal structure of flow matching. 4.3 CHUNK-GRPO We now present Chunk-GRPO, which integrates chunk-level optimization with temporal-dynamicguided chunking. Specifically, given an image generation trajectory, we first compute the relative L1 distance and set chunks like Equation (12) according to the dynamic profile. This yields the chunk numbers and chunk sizes csj. The optimization then follows the chunk-level object in Equation (13). The whole framework is shown in Figure 4. In practice, we observe that the choice of and csj is closely tied to the total number of sampling steps . practical strategy, which we adopt in our experiment, is to precompute chunk boundaries based on observed dynamics and keep them fixed throughout training. detailed discussion is provided in Section 5.1 and Section B.1."
        },
        {
            "title": "Preprint",
            "content": "Furthermore, we propose an optional weighted sampling strategy to further enhance Chunk-GRPO. Following Dance-GRPO (Xue et al., 2025), we select only fraction of chunks (e.g. with fraction 0.5) per update; but instead of uniform sampling, we assign sampling weight for each chunk: (cid:80) w(chj) = 1 csj 1 L1rel (x, t) tchj t=1 L1rel (x, t) (cid:80)T . (16) From Figure 3, this strategy biases the sampling process toward high-noise regions, and the motivation primarily stems from our ablation studies on training specific chunks. However, although this strategy improves certain aspects of Chunk-GRPO, its overall effects on image quality remain nuanced, as discussed in Section 5.3."
        },
        {
            "title": "5.1 EXPERIMENT SETUP",
            "content": "Training Settings We adopt Dance-GRPO (Xue et al., 2025) as the baseline and conduct experiments with FLUX.1 Dev (Labs, 2024) as our base model. HPDv2.1 (Wu et al., 2023) serves as the dataset, while HPSv3 (Ma et al., 2025) is used as the primary reward model. In ablation studies Section 5.3, we additionally validate our approach with Pick Score (Kirstain et al., 2023) and Clip (Radford et al., 2021) as the reward model. For the chunk setting, we use {csj}4 j=1 = [2, 3, 4, 7] with total sampling steps = 17 , fixed throughout training. Further explanation of chunk configuration and additional training details are provided in Section B. Evaluation Details We evaluate both preference alignment and standard T2I benchmarks. For preference alignment, we use HPSv3 (Ma et al., 2025) and ImageReward (Xu et al., 2023) as in-domain and out-of-domain evaluation metrics, respectively, on the HPDv2.1 (Wu et al., 2023) test set. For the standard T2I benchmark, we report results on WISE (Niu et al., 2025), using its rewritten version due to its improved generalization. We also report results on GenEval (Ghosh et al., 2023) in ablation studies Section 5.3. All evaluations adopt hybrid inference from (Li et al., 2025a), which has proven effective in mitigating reward hacking. More details are provided in Section B.3. 5.2 MAIN RESULTS Model Table 1: Results on Preference Alignment Table 1 presents results on preference alignment, and Table 2 shows WISE Chunk-GRPO benchmark results. consistently outperforms both the base model and Dance-GRPO, confirming the effectiveness of chunklevel optimization. Qualitative comparisons in Figure 1, Figure 7, and Figure 8 further highlight ChunkGRPOs in image improvements quality. Chunk-GRPO generates outputs that align more closely with human aesthetic preferences, exhibiting stronger lighting contrast, more vivid colors, and finer details. Chunk-GRPO w/o ws Chunk-GRPO w/ ws 1 The ws refers to the weighted sampling strategy. ImageReward 15.236 15.373 Dance-GRPO 1.147 1.149 HPSv 13.804 15.080 1.086 1.141 Flux For preference alignment, our approach achieves additional gains of up to 23% over the baseline across both in-domain and out-of-domain evaluations. On WISE, our approach achieves the strongest overall performance. Notably, the weighted sampling strategy enhances preference alignment but has mixed effects on WISE, phenomenon we further analyze in Section 5.3. 5.3 ABLATION STUDY Chunk Setting. We first vary chunk settings under different total sampling steps, excluding the weighted sampling strategy to isolate chunk setting effects. Results in Table 3 show that chunk-level optimization consistently outperforms standard step-level GRPO. We neglect the last timestep following Dance-GRPO, as the last step does not introduce stochasticity."
        },
        {
            "title": "Preprint",
            "content": "Model"
        },
        {
            "title": "Flux",
            "content": "Dance-GRPO Table 2: Results on WISE Cultural Time Space Biology Physics Chemistry Overall 0.75 0.82 0. 0.75 0.76 0.78 0.69 0.66 0.68 0. 0.71 0.69 0.69 0.65 0.68 0.64 0.68 0. 0.73 0.75 0.76 0.73 Chunk-GRPO w/o ws Chunk-GRPO w/ ws 1 The ws refers to the weighted sampling strategy. 2 We use the rewritten version of WISE. 0.82 0.80 0.76 0. 0.77 0.76 Table 3: Ablation Results of Chunk Setting Sampling Timesteps Chunk Setting HPSv3 Model"
        },
        {
            "title": "DanceGRPO",
            "content": "Chunk-GRPO w/o td - 17 25 17 17 17 17 25 25 25 25 - - - [2, 2, , 2] [4, 4, 4, 4] [8, 8] [16] [2, 2, , 2] [4, 4, , 4] [12, 12] [24] [2, 3, 4, 7] 13.804 15.080 15.015 15.115 15.078 15.173 15.142 15.057 15.136 15.111 15.100 15. Chunk-GRPO w/ td 17 1 The td refers to the temporal dynamics. Moreover, temporal-dynamics-guided chunking outperforms that of fixed chunk size, underscoring the importance of aligning the optimization process with the intrinsic temporal structure of flow matching. Training on Specific Chunks. We next train Chunk-GRPO on individual chunks only. Note that we also remove the weighted sampling strategy here. Results in Figure 6 show that high-noise chunks (e.g., ch1) yield larger improvements than low-noise chunks (e.g., ch4), but also suffer from training instability (e.g. after 60 steps). This observation motivated our weighted sampling strategy in Equation (16), which adaptively emphasizes highnoise chunks. Weighted Sampling Strategy. As shown in Table 1 and Table 2, the optional weighted sampling strategy improves preference alignment but slightly reduces WISE performance. Careful qualitative analysis reveals trade-off: while the strategy accelerates preference optimization, it can destabilize image structure in high-noise regions, occasionally leading to semantic collapse. failure example is shown in Figure 9. Although all methods struggle with this challenging prompt (e.g. Dance-GRPO misses the attribute sleeveless), the weighted sampling strategy further alters the overall image structure, producing the worst case by omitting the entire item black loafers and only partially showing capris). This demonstrates the complex effects of the strategy. Figure 6: The results of training specific chunks."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Additional visualization comparison between the FLUX, DanceGRPO, Chunk-GRPO w/o temporal dynamics, Chunk-GRPO w/ temporal dynamics and Chunk-GRPO w/ weighted sampling."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Additional visualization comparison between the FLUX, DanceGRPO, Chunk-GRPO w/o temporal dynamics, Chunk-GRPO w/ temporal dynamics and Chunk-GRPO w/ weighted sampling."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: failure case of the weighted sampling strategy. The strategy wrongly changes the image structure in the high-noise region, leading to the worst variant."
        },
        {
            "title": "Flux",
            "content": "Model Reward Models. Finally, we test Chunk-GRPOs robustness under different reward models. We first replace Hpsv3 with Pick Score (Shukor et al., 2025) as our reward model. Results in Table 4 confirm that Chunk-GRPO consistently outperforms standard steplevel GRPO regardless of the reward model, validating its generality. DanceGRPO Table 4: Ablation on Different Reward Models Pick Score HPSv3 Image Reward 22.643 23.427 13.804 14.612 1.086 1. 1.222 1.233 Chunk-GRPO w/o ws Chunk-GRPO w/ ws 1 The ws refers to the weighted sampling strategy. 23.442 23.476 14.810 14.913 Since both HPSv3 and PickScore are reward models primarily designed for preference alignment, we further validate our approach using Clip (Radford et al., 2021), which, while not preference alignment model, is well recognized for its ability to capture high-level semantics. We evaluate this on GenEval (Ghosh et al., 2023), benchmark that mainly tests instruction-following capability. Results in Table 5 demonstrate that Chunk-GRPO also outperforms standard step-level GRPO, demonstrating its broader generalization and robustness beyond preference alignment tasks. It is worth noting that the weighted sampling strategy results in decline in GenEvals semantic performance, which further corroborates our previous analysis. Table 5: Results on GenEval Single Obj. Two Obj. Counting Colors Position Color Attr. Overall Model Flux Dance-GRPO 0.99 1. 0.83 0.86 Chunk-GRPO w/o ws Chunk-GRPO w/ ws 1 The ws refers to the weighted sampling strategy. 0.85 0.82 0.99 0.98 0. 0.71 0.75 0.73 0.75 0.78 0.81 0.76 0. 0.22 0.21 0.27 0.44 0.46 0.51 0.48 0. 0.67 0.69 0."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for flowmatching-based T2I generation. By leveraging the temporal dynamics of flow matching, ChunkGRPO groups consecutive timesteps into chunks and optimizes at the chunk level, achieving consistent improvements over standard step-level GRPO. We further introduce an optional weighted sampling strategy to push Chunk-GRPO further. Despite its strong performance, several limitations remain. First, exploring how to combine heterogeneous rewards across different chunks (e.g., employing different reward models for highvs. low-noise regions) could unlock further improvements. Second, our chunk segmentation is fixed throughout training. Developing self-adaptive or dynamic chunking strategies that adjust to training signals would be an important next step."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported in part by the Natural Science Foundation of Shenzhen (No. JCYJ20230807111604008, No. JCYJ20240813112007010), the Natural Science Foundation of Guangdong Province (No. 2024A1515010003), National Key Research and Development Program of China (No. 2022YFB4701400) and Cross-disciplinary Fund for Research and Innovation of Tsinghua SIGS (No. JC2024002)."
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. pi 0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024a. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024b. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, pp. 02783649241273668, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0. 5: vision-language-action model with open-world generalization, 2025. URL https://arxiv. org/abs/2504.16054, 1(2):3. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024."
        },
        {
            "title": "Preprint",
            "content": "Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Lucy Lai, Ann ZX Huang, and Samuel Gershman. Action chunking as conditional policy compression. 2022. Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025a. Qiyang Li, Zhiyuan Zhou, and Sergey Levine. Reinforcement learning with action chunking. arXiv preprint arXiv:2507.07969, 2025b. Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, and Shanghang Zhang. Branchgrpo: Stable and efficient grpo with structured branching in diffusion models. arXiv preprint arXiv:2509.06040, 2025c. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 7353 7363, 2025a. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025b. Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. arXiv preprint arXiv:2508.03789, 2025. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
        },
        {
            "title": "Preprint",
            "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Haoyuan Sun, Bin Liang, Bo Xia, Jiaqi Wu, Yifei Zhao, Kai Qin, Yongzhe Chang, and Xueqian Wang. Diffusion-rainbowpa: Improvements integrated preference alignment for diffusion-based text-to-image generation. Transactions on Machine Learning Research, 2025a. Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, and Xueqian Wang. Reinforcement fine-tuning powers reasoning capability of multimodal large language models. arXiv preprint arXiv:2505.18536, 2025b. Haoyuan Sun, Bo Xia, Yongzhe Chang, and Xueqian Wang. Generalizing alignment paradigm of text-to-image generation with preferences through f-divergence minimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2764427652, 2025c. Haoyuan Sun, Bo Xia, Yifei Zhao, Yongzhe Chang, and Xueqian Wang. Identical human preference alignment paradigm for text-to-image models. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025d. Haoyuan Sun, Bo Xia, Yifei Zhao, Yongzhe Chang, and Xueqian Wang. Positive enhanced preference alignment for text-to-image models. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025e. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Feng Wang and Zihao Yu. Coefficients-preserving sampling for reinforcement learning with flow matching. arXiv preprint arXiv:2509.05952, 2025. Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for stable text-to-image reinforcement learning. arXiv preprint arXiv:2508.20751, 2025. Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: AcceleratIn Proceedings of the IEEE/CVF Conference on ing diffusion models through block caching. Computer Vision and Pattern Recognition, pp. 62116220, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025."
        },
        {
            "title": "Preprint",
            "content": "Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025."
        },
        {
            "title": "A MATHEMATICAL ANALYSIS",
            "content": "Here we Here we provide mathematical analysis for Proposition 1. For simplicity, we assume that there are timesteps with inaccurate advantage attribution between two trajectory segments: (xT , xT 1, , x2, x1, x0)1, (xT , xT 1, , x2, x1, x0)2, (17) where 1 . We denote Ta and Tia as the sets of timesteps with accurate and inaccurate advantage attribution, respectively, and: Ta Tia = , Ta Tia = {1, 2, , }. Let Ai and Aj as the advantage of the two trajectories. Without loss of generality, we assume: We denote ˆAi as the ground-truth advantage. Then for each timestep t: A1 = 1, A2 = 1. = 1, ˆA1 ˆA1 = A1 = t = 1, ˆA2 = A2 ˆA2 = 1, = A2 Ta, = 1, Tia. The expected ground-truth loss object can thus be expressed as: ˆJ(θ) = 2 (cid:88) (cid:88) i= t=1 min (cid:16) (θ) ˆAi, clip (cid:0)ri ri (θ) , 1 ϵ, 1 + ϵ(cid:1) ˆAi (cid:17) . (18) (19) (20) (21) Here we omit constant factors such as 1 ratio ri is defined in Equation (6), reproduced here for clarity:: 2 , 1 , and KL KL regularization. The step-level importance ri t(θ) = pθ(xi pold(xi t1xi t1xi t, c) t, c) . (22) Substituting Equation (20) into eq. (21), we obtain: ˆJ(θ) = (cid:88) tTa (cid:2)min (cid:0)r1 (θ) , clip (cid:0)r1 (θ) , 1 ϵ, 1 + ϵ(cid:1)(cid:1) + min (cid:0)r2 (θ) , clip (cid:0)r2 (θ) , 1 ϵ, 1 + ϵ(cid:1)(cid:1)(cid:3) (cid:88) + tTia (cid:2)min (cid:0)r2 (θ) , clip (cid:0)r2 (θ) , 1 ϵ, 1 + ϵ(cid:1)(cid:1) + min (cid:0)r (θ) , clip (cid:0)r1 (θ) , 1 ϵ, 1 + ϵ(cid:1)(cid:1)(cid:3) . Since the clipping operation only affects timesteps where the importance ratio lies outside the trust region (Schulman et al., 2015), and such cases are rare under small policy updates, we approximate the gradient of Equation (23) by the gradient of following expression: (23) ˆJ(θ) = (cid:88) tTa (cid:0)r1 (θ) r2 (θ)(cid:1) + (cid:88) tTia (cid:0)r2 (θ) r1 (θ)(cid:1) . (24) Similarly, the step-level GRPO loss has gradient approximated to the gradient of following: we neglect x0 because there is no more transition into"
        },
        {
            "title": "Preprint",
            "content": "J(θ)GRP = (cid:88) tTa (cid:0)r1 (θ) r2 (θ)(cid:1) + (cid:0)r1 (θ) r2 (θ)(cid:1) . (cid:88) tTia (25) We now analyze chunk-level optimization. For simplicity, we treat each trajectory in Equation (17) as single chunk. Following Equation (12), we have: {ch1}i = {(xT , , x1)}i, = 1, 2, cs1 1 = cs2 1 = T, (26) The reason is that if trajectories are split into smaller chunks, each chunk can be viewed as complete trajectory as in Equation (17). For convenience, we rewrite the chunk-level importance ratio from Equation (14) as: si j(θ) = (cid:89) tchj The chunk-level objective then becomes: pθ pθold (cid:0)xi t1xi (cid:0)xi t1xi t, c(cid:1) t, c(cid:1) 1 csj . J(θ)chunk = 2 (cid:88) i= min (cid:0)si 1 (θ) Ai, clip (cid:0)si 1 (θ) , 1 ϵ, 1 + ϵ(cid:1) Ai(cid:1) . (27) (28) Similarly, the gradient of J(θ)chunk can be approximated by the gradient of following expression:: where J(θ)chunk = s1 1 s2 1, si 1(θ) = (cid:32) (cid:89) tch1 pθ pθold (cid:0)xi t1xi (cid:0)xi t1xi (cid:33) 1 cs1 t, c(cid:1) t, c(cid:1) (cid:33) 1 = = (cid:32) (cid:89) t=1 (cid:32) (cid:89) t=1 pθ pθold t, c(cid:1) t, c(cid:1) (cid:0)xi t1xi (cid:0)xi t1xi (cid:33) 1 ri (θ) , = 1, 2. (29) (30) In Proximal Policy Optimization (PPO) (Schulman et al., 2017) and GRPO-based methods, the importance ratio ri (θ) remains close to 1 due to trust-region constraints Schulman et al. (2015; 2017). We therefore set: where ϵi is minimal term. Substituting into Equation (24) and Equation (25): (θ) = 1 + ϵi ri t, ˆJ(θ) = (cid:88) tTa (cid:0)ϵ1 ϵ2 (cid:1) + (cid:88) (cid:0)ϵ2 ϵ1 (cid:1) tTia J(θ)GRP = (cid:88) tTa (cid:0)ϵ1 ϵ2 (cid:1) + (cid:88) (cid:0)ϵ1 ϵ2 (cid:1) tTia = (cid:88) t=1 (cid:0)ϵ1 ϵ2 (cid:1) . 17 (31) (32) (33)"
        },
        {
            "title": "Preprint",
            "content": "For the chunk-level ratio in Equation (30), applying the logarithm and Taylor expansion gives: si 1(θ) = = (cid:33) 1 ri (θ) (cid:33) 1 (cid:0)1 + ϵi (cid:1) (cid:32) (cid:89) t=1 (cid:32) (cid:89) t=1 = 1 +"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) 1 ϵi t. Thus the chunk-level objective reduces to: J(θ)chunk = s1 1 s2 1 (cid:32) = 1 + 1 (cid:33) (cid:32) 1 + (cid:88) ϵ1 1 (cid:33) 1 (cid:88) ϵ2 1 (cid:88) t=1 (cid:0)ϵ1 ϵ2 (cid:1) J(θ)GRP O. = = 1 1 (34) (35) This shows that chunk-level optimization yields smoothed version of the step-level GRPO obˆJ(θ), jective. More formally, by comparing the squared distances between coefficient vector of J(θ)GRP O, and J(θ)chunk, we find: ˆJ(θ) J(θ)GRP O2 2 = 2m (1 (1))2 ˆJ(θ) J(θ)chunk 2 = ˆJ(θ) 1 = ˆJ(θ)2 + = 2T + 2T 2 = 2T 4 + = 8m. J(θ)GRP O2 2 1 2 J(θ)GRP O2 2 2 (T 2m) 8m + 2 , 2 ˆJ(θ) J(θ)GRP (36) (37) Where denotes the number of inaccurately attributed timesteps, which we mentioned in the beginning of this section. We want Equation (37) to be smaller than Equation (36), i.e., ˆJ(θ) J(θ)GRP O2 2 ˆJ(θ) J(θ)chunk2 2 0 Solving yields: ˆJ(θ) J(θ)GRP O2 2 ˆJ(θ) J(θ)chunk2 2 0 8m 2T + 4 8m + 2 2T 2 (4m + 8)T + (8m + 2) 0 2 (2m + 4)T + (4m + 1) 0 (cid:112) m2 + 3 + 2 + (cid:112) m2 + 3 + 2 (38) (39)"
        },
        {
            "title": "Preprint",
            "content": "Since 1 , the first inequality always holds. As both and are positive integers, we obtain: (cid:26) 5, 2m + 2, if = 1 if 2. (40) Note that here cs1 = , and the whole trajectory is treated as single chunk. When the chunk size cs 5, Equation (38) always holds, meaning that the chunk-level objective J(θ)chunk is closer to the ground-truth object ˆJ(θ) than J(θ)GRP O. For larger chunks, Equation (38) still holds when 2 2 . The insights of this solution are: For small chunks (e.g. csj = 5), chunk-level optimization always outperforms step-level GRPO. For large chunk sizes, it also holds when roughly half of the timesteps suffer from inaccurate advantage attribution. From Equation (35), chunk-level optimization consistently provides smoother gradients than step-level GRPO.."
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "B.1 CHUNK CONFIGURATION In practice, the default Chunk-GRPO segments the image generation trajectory into = 4 chunks with csj 4 j=1 = 2, 3, 4, 7 under = 17 timesteps. The rationale is as follows: Following Figure 3, We set the first chunk as cs1 = 2. For the last chunk, we first conduct pre-observation: we compute the relative L1 distance in Equation (15) again, but with Dance-GRPO-trained model instead of the base model. As shown in Figure 10, RL alters the relative L1 distance primarily in the latter half of timesteps. Based on this, we set ch4 = 7. For ch2 = 3 and ch3 = 4, we base the segmentation on the second derivative of the L1 curve. This configuration also satisfies the requirement in Proposition 1, which recommends keeping chunk size small (e.g. 5). We emphasize that this segmentation is not guaranteed to be the only optimal choice. Exploring adaptive chunk configurations under different is an interesting direction for future work. B.2 TRAINING DETAILS All experiments were conducted on 8 Nvidia H800 GPUs. The hyperparameters are summarized in table 6. B.3 EVALUATION DETAILS We set = 50 during evaluation. Following (Li et al., 2025a), the first 30 steps are sampled with the trained model, while the remaining 20 steps are sampled with the base model. This hybrid inference strategy and corresponding settings, also used in (Li et al., 2025a), have proven effective in mitigating reward hacking. Figure 10: The relative L1 distance comparison, before and after the training of Dance-GRPO. We neglect the last timestep following Dance-GRPO, as the last step does not introduce stochasticity."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Hyperparameter Settings Parameter Value Parameter Learning rate Train batch size SP batch size Resolution Eta Grad. accum. steps Clip range Timestep fraction 1 105 2 2 720 720 0.7 12 5 105 0. Weight decay SP size Max grad norm Sampling steps Num. generations Shift (branch offset) Training steps Value 1 104 1 0.01 17 12 3"
        }
    ],
    "affiliations": [
        "Kolors Team, Kuaishou Technology",
        "Tsinghua University"
    ]
}