{
    "paper_title": "MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework",
    "authors": [
        "Ailing Yu",
        "Lan Yao",
        "Jingnan Liu",
        "Zhe Chen",
        "Jiajun Yin",
        "Yuan Wang",
        "Xinhao Liao",
        "Zhiling Ye",
        "Ji Li",
        "Yun Yue",
        "Hansong Xiao",
        "Hualei Zhou",
        "Chunxiao Guo",
        "Peng Wei",
        "Junwei Liu",
        "Jinjie Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts. We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions. Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains."
        },
        {
            "title": "Start",
            "content": "MedResearcher-R1: Expert-Level Medical Deep Researcher via Knowledge-Informed Trajectory Synthesis Framework Ailing Yu1 Yuan Wang1 Xinhao Liao1 Lan Yao2* Jingnan Liu1 Zhe Chen1 Jiajun Yin Zhiling Ye1 Ji Li1 Yun Yue1 Junwei Liu1 Jinjie Gu1 Hansong Xiao1 Hualei Zhou1 Chunxiao Guo1 Peng Wei1 1Ant Group 2Harbin Institute of Technology (cid:135) Code and Dataset: AQ-MedAI/MedResearcher-R1 Abstract Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts. We present medical deep research agent that addresses these challenges through two core innovations. First, we develop novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions. Through two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains. Code and datasets will be released to facilitate further research. 5 2 0 2 1 ] . [ 3 0 8 8 4 1 . 8 0 5 2 : r Figure 1: Overall performance of MedResearcher-R1 across three benchmarks. On MedBrowseComp, our MedResearcher-R1-32B achieves state-of-the-art performance with 27.5/50 correct answers, surpassing o3-deepresearch (25.5/50), Gemini-2.5-Pro-deepresearch (25.0/50), and significantly outperforming search-only approaches (o3-search: 19.0/50, Gemini-2.5-Pro-search: 14.0/50). On general deep research tasks, we achieve competitive results on GAIA (53.4 vs. WebSailor-32Bs 53.2) and xBench (54.0 vs. WebSailor-32Bs 53.3). *Work done during an internship at Ant Group. 1 preprint"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Language Models (LLMs) have catalyzed widespread adoption of LLM-based agents across diverse domains, including software engineering [Wang et al., 2024, Jimenez et al., 2023] and deep research systems [Xu and Peng, 2025]. These agents exhibit impressive capabilities in processing environmental observations, maintaining context across multiple interactions, and executing complex multi-step reasoning tasks. However, the medical domain presents unique challenges that current general-purpose deep research agents fail to address adequately. The recently introduced MedBrowseComp benchmark [Chen et al., 2025b] reveals this critical gap: even OpenAIs o3-deepresearch, the leading proprietary deep research system, achieves only 25.5% accuracy on complex medical queries requiring multi-hop reasoning across medical knowledge sources. We identify two fundamental limitations that contribute to this performance gap: (1) general-purpose agents lack the dense, specialized medical knowledge required for accurate clinical reasoning, and (2) they rely on generic retrieval tools that fail to capture the nuanced relationships in medical information. The core challenge lies in what we term the sparse medical knowledge problem. Medical research often requires connecting rare diseases, emerging treatments, and specialized clinical findings through non-obvious pathwaysconnections that exist in specialized medical literature but remain inaccessible to general search tools. While existing medical AI systems have made progress in structured tasks like diagnosis, they primarily focus on common medical scenarios with well-established reasoning patterns. These systems fail to develop the capability for exploratory medical research that characterizes expert clinicians: simultaneously pursuing multiple hypotheses, synthesizing evidence from disparate sources, and identifying subtle connections between rare medical entities. To address these limitations, we propose comprehensive approach that fundamentally rethinks how medical agents should be trained. Our key insight is that effective medical reasoning requires exposure to genuinely complex medical scenarios during training rather than simplified approximations. We achieve this through three interconnected innovations: First, we develop novel data synthesis framework that generates training examples of exceptional complexity through systematic pipeline: We begin by extracting medical entities from over 30 million PubMed abstracts, then apply frequency analysis to identify candidates with occurrence rates below 106 in medical corpora. Through LLM-assisted evaluation, we filter these candidates to select genuinely rare yet clinically significant entities, avoiding both trivial typos and overly common conditions. Around these carefully selected rare medical entities, we construct knowledge graphs to extract the longest reasoning chains for multi-hop question generation. This approach creates questions that mirror real medical research challenges and cannot be answered through simple retrieval but require systematic exploration and synthesis across multiple medical information sources. Figure 2: Comparison of medical reasoning agents. MedResearcher-R1 resolves the Valsartan identification case that defeats general-purpose agents, demonstrating the strength of specialized medical database access and evidence-based reasoning integration. 2 preprint Second, we introduce proprietary medical domain tools that address retrieval gaps in general systems. As illustrated in Figure 2, while general agents often fail when encountering medical-specific queries, particularly those involving rare diseases or complex chemical compounds, MedResearcher-R1 can iteratively invoke specialized medical tools alongside general-purpose tools to ensure accurate information retrieval. Unlike conventional search engines that rely on general web crawling, our custom-built private medical retrieval engine directly accesses authoritative medical databases, including FDA databases, official prescription data, clinical trial registries, and peer-reviewed medical publications. The comparison in Figure 2 demonstrates how MedResearcher-R1 dynamically switches between general and medical-specific tools, enabling systematic validation of the complete evidence chain: from corporate merger information to heart failure drug development, to chemical composition and mechanism, ultimately ensuring clinical accuracy while avoiding the reasoning errors that plague general-only approaches. The system employs medical ontology-aware ranking to prioritize clinical authority and relevance over general web popularity metrics, effectively combining the breadth of general-purpose search with the precision of domain-specific medical expertise. Third, we implement training methodology specifically designed for medical domains. Unlike recent work advocating pure reinforcement learning approaches, we find that medical tasks require what we call knowledge-anchored learning: initial supervised fine-tuning on high-quality medical trajectories proves highly effective for learning tool usage patterns and significantly improves final performance. Our Masked Trajectory Guidance (MTG) technique provides structural scaffolding while preventing memorization, forcing models to develop genuine medical reasoning capabilities rather than pattern matching. Our experimental results validate this approach. The trained model, MedResearcher-R1, achieves score of 27.5/50 on MedBrowseComp, establishing new state-of-the-art and substantially outperforming both the Qwen2.5-32B baseline and existing deep research systems. Notably, our medical specialization does not compromise general capabilities: on general agent benchmarks (GAIA: 53.4, xBench: 54), MedResearcher-R1maintains competitive performance comparable to OpenAI o4-mini. Please refer to Figure 1 for an overview. These results challenge the prevailing assumption that domain-specific agents require sacrificing general capabilities. Instead, we demonstrate that the rigorous reasoning demanded by medical tasksprecise terminology, careful evidence evaluation, and systematic hypothesis testingprovides superior training signal for developing robust agent capabilities. The dense knowledge structures and complex reasoning patterns learned from medical domains transfer effectively to general tasks, suggesting that specialized training can enhance rather than limit agent versatility. This work contributes to the rapidly evolving field of medical AI by demonstrating that achieving medical deep research capabilities requires fundamental innovations beyond applying general agents to medical tasks. Through careful design of training data, specialized tools, and learning algorithms tailored to medical reasoning, we show that it is possible to develop agents that approach expert-level medical research capabilities. We release our code, datasets, and trained models to facilitate further research in this critical area."
        },
        {
            "title": "2 MedResearcher-R1: Medical Deep Research Agent Framework",
            "content": "2.1 Problem Definition We formalize the medical deep research task as sequential decision-making problem where an agent must navigate complex medical knowledge sources to answer multi-hop queries that characterize the sparse medical knowledge problem identified in Section 1. Given medical question Q, the agent operates with heterogeneous toolset = Tgeneral Tmedical, where Tgeneral = {tg m} comprises general-purpose tools (web search, document analysis) 1 , . . . , tm and Tmedical = {tm } contains our proprietary medical domain tools that directly access authoritative medical databases. 1, . . . , tg The agent maintains an evolving state st = (ct, kt, ht) at timestep t, where: ct C: dialogue context encoding the current query and response history kt K: accumulated medical knowledge from retrieved sources, structured as knowledge graph ht H: reasoning history tracking explored knowledge paths and hypothesis evolution This state representation enables tracking of multi-hop reasoning chains essential for connecting rare medical entities through non-obvious pathways. At each timestep, the agent selects an action according to learned policy: at πθ(a st, , q) where πθ is trained through our knowledge-anchored learning approach to dynamically switch between general and medical-specific tools based on query requirements. 3 preprint 2.2 Agent Architecture Our framework directly addresses the two fundamental limitations of general-purpose agents: insufficient medical knowledge density and reliance on generic retrieval tools that fail to capture nuanced medical relationships. Reasoning-Acting Paradigm. Following the REACT framework [Yao et al., 2023a], our agent operates via iterative reasonactobserve cycles, augmented with medical-specific enhancements that enable exploratory medical research capabilities. At each step, the policy generates: Thought: medical reasoning trace that identifies information gaps, formulates hypotheses, and determines whether general or specialized tools are needed Action: tool invocation with parameters optimized for medical information extraction, prioritizing authoritative sources over general web content Observation: Structured medical knowledge validated against clinical evidence and incorporated into the agents evolving state This process continues iteratively, with the agent pursuing multiple hypotheses simultaneously until synthesizing comprehensive answer. Complex multi-hop questions typically require 4-5 tool interactions, mirroring the systematic exploration patterns of expert clinicians. General-Purpose Tools. Our agent retains access to standard tools for breadth of coverage: (1) WebSearch: Standard web retrieval for general medical information, recent developments, and corporate/organizational data (e.g., pharmaceutical company mergers as shown in Figure 2). (2) DocumentRead: Extraction and synthesis from retrieved documents using high-capacity LLM backbones (e.g., Qwen2.5-72B [Bai et al., 2024]), particularly for processing lengthy clinical reports or research papers. Medical-Specific Tool Suite. core innovation of our architecture is the integration of proprietary medical domain tools that address the unique challenges of clinical research and bridge the gap between general retrieval and specialized medical reasoning. Our medical-specific tool suite includes: (1) PrivateMedicalRetriever: This module aggregates evidence directly from authoritative clinical resources, including FDA databases, clinical trial registries, and PubMed publications. Each candidate document is scored for query by weighted linear combination of semantic relevance and clinical authority: Score(d, q) = λ Rel(d, q) + (1 λ) Auth(d), where Rel(d, q) represents the semantic similarity to the query (computed via embedding cosine similarity), and Auth(d) reflects the clinical authority (combining impact factor and guideline status). The hyperparameter λ (0 λ 1) balances the importance between relevance and authority; in all experiments, we set λ = 0.4 to favor reliable and clinically significant evidence. (2) ClinicalReasoningEngine: Designed for evidence-based differential diagnosis, this tool applies Bayesian inference to systematically evaluate multiple hypotheses. Given observed symptoms s, candidate diagnoses Dj, and patient context c, the posterior for each diagnosis is computed as: (Dj s, c) = (cid:81)n i=1 (si Dj, c) (Dj c) (cid:81)n i=1 (si Dk, c) (Dk c) (cid:80)m k=1 where conditional probabilities are derived from clinical literature and iteratively updated based on newly retrieved evidence. Dynamic Tool Selection Strategy. As illustrated in Figure 2, our agent dynamically switches between general and medical-specific tools to ensure complete evidence chains. The tool selection is governed by learned policy that evaluates query complexity: (t st, q) = (cid:26)σ(wT σ(wT mϕ(st, q)) ϕ(st, q)) if Tmedical if Tgeneral preprint where ϕ(st, q) extracts features including entity rarity, required reasoning hops, and presence of medical terminology, wm and wg are learned weight vectors, and σ() is the sigmoid function. The policy learns to prioritize medical tools when encountering rare diseases or complex chemical compounds while leveraging general tools for contextual information. Together, PrivateMedicalRetriever and ClinicalReasoningEngine comprise the medical-specific tool suite, enabling the agent to retrieve, interpret, and reason over specialized clinical evidence far beyond the reach of general-purpose tools."
        },
        {
            "title": "3 KISA: Knowledge-Informed Trajectory Synthesis Approach",
            "content": "To address the critical challenge of training data scarcity for medical deep research agents, we propose KnowledgeInformed Trajectory Synthesis Approach (KISA) that generates complex, multi-hop medical reasoning trajectories. Our framework directly tackles the limitations of general-purpose agents by creating training data that emphasizes: (1) rare medical entity connections requiring dense domain knowledge, and (2) effective utilization of medical-specific retrieval tools. 3.1 Agentic Dataset Construction Our dataset construction pipeline consists of three interconnected components designed to generate genuinely complex medical queries that robustly stress-test agent capabilities: 3.1.1 Entity-Centric Knowledge Graph Construction We construct medical knowledge graphs specifically optimized for generating complex reasoning chains. Unlike traditional approaches that focus on common concepts, we prioritize rare medical entities Eseed with frequency below threshold τrare = 106 in general medical corpora. Focusing on rare entities ensures that generated questions require deep medical knowledge, as opposed to surface-level information obtainable through general search. The graph expansion follows an iterative process: ei+1 (cid:26)Uniform(N (ei)) with probability 0.5 Discover(Enewei) with probability 0.5 where (ei) denotes the set of neighbors of ei and Discover() identifies novel entities via our private medical retrieval engine, ensuring that new connections are both medically valid and challenging. Each relation is augmented with additional contextual information: = esubj, p, eobj, ttemporal, lspatial, cclinical where cclinical encodes the clinical context (e.g., disease stage, patient demographics), ttemporal captures temporal aspects, and lspatial denotes spatial context. This enriched representation improves multi-hop reasoning accuracy by 12.3% compared to standard triplets. 3.1.2 Multi-Hop Question Generation via Longest-Path Extraction Our key innovation lies in extracting longest chains from subgraphs to generate maximally complex queries. For each rare entity subgraph Gsub, we compute the longest valid reasoning path: = arg max pP(Gsub) Length(p) s.t. MedicallyValid(p) where P(Gsub) is the set of all paths in Gsub. This longest-path strategy ensures that questions require multiple reasoning hops (average 4.2 per trajectory), rather than being answerable via simple lookups. These paths are subsequently transformed into natural language questions that require sequential tool invocations to reconstruct the complete reasoning chain. 5 preprint Figure 3: Knowledge graph-based question generation pipeline: extracting longest chains from rare entity subgraphs to create complex multi-hop medical queries. 3.1.3 Quality Control and Difficulty Calibration To ensure that generated questions remain challenging for current systems, we implement adaptive difficulty calibration. Each question is evaluated against OpenAI-o3 deepresearch and GPT-4. If either model achieves > 50% accuracy, the question is automatically regenerated with increased complexity: = (cid:26)q Regenerate(q, complexity + 1) if max(AccO3(q), AccGPT4(q)) < 0.5 otherwise This approach ensures our dataset remains challenging even for state-of-the-art systems, directly addressing the 25.5% performance ceiling previously observed in MedBrowseComp. 3.2 Trajectory Synthesis with Medical Tool Integration 3.2.1 Masked Trajectory Guidance (MTG) To generate high-quality training trajectories that effectively utilize our medical-specific tools, we introduce Masked Trajectory Guidance(MTG). Given reasoning graph path = {(e1, r1, e2), . . . , (en1, rn1, en)} extracted from the knowledge graph, we create structural scaffold by masking the entities: This masking process serves two main purposes: Tmasked = {([MASK], ri, [MASK])}n1 i=1 Tool selection learning: Encourages the model to determine when medical-specific retrieval tools are required versus when general search suffices. Prevention of shortcuts: Prevents answer memorization while maintaining the underlying reasoning process. Figure 4: Masked Trajectory Guidance: structural scaffold that enables reasoning without shortcut learning by masking entities. 6 preprint 3.2.2 Hybrid Strategy for Tool Diversity To promote robust and diverse tool usage, we adopt hybrid data strategy: Dtrain = α Dguided + (1 α) Dexploration, where α = 0.7 balances structured learning with exploration. The exploration trajectories naturally cultivate three key behaviors: Medical tool prioritization: 78% begin with private medical retriever for rare entities Tool switching: 42% demonstrate adaptive switching between general and medical tools Error recovery: 34% include explicit correction using alternative tools"
        },
        {
            "title": "4 Large-scale Agent Training",
            "content": "4.1 Cold Start with Supervised Fine-Tuning We initiate agent training via supervised fine-tuning (SFT) on large collection of synthetic agentic dialogues = {(x(i), y(i))}N i=1. Here, x(i) denotes the input context, and y(i) the ideal next-action sequence (thought, tool call, etc.) for each example. The objective is to maximize the likelihood of generating the correct trajectory conditioned on context and prior agent history: LSFT(θ) = 1 (cid:88) y(i) (cid:88) i=1 k=1 log pθ(y(i) x(i), y(i) <k). To promote agent robustness and generalization, we incorporate several key augmentations during fine-tuning: Tool failure simulation (5% corruption rate): Randomly corrupts tool output to encourage contingency planning and robust error recovery in downstream trajectories. Intermediate thought supervision: Teaches the agent to articulate explicit reasoning prior to every tool invocation, improving interpretability and decision traceability. Multi-task sampling: Diversifies training batches across medical domains (diagnosis, treatment, guidelines, rare diseases), supporting broad generalization and transfer. Optimization proceeds using the AdamW optimizer (learning rate λ = 0.01), with cosine annealing schedule (ηmax = 3 107), for 3 epochs on 8 H800 GPUs. This ensures rapid exploration of diverse trajectories and convergence to well-calibrated agent policies. 4.2 Reinforcement Learning After supervised warm-starting, we refine the agent via reinforcement learning using Grouped Regularized Policy Optimization (GRPO), optimizing agentic trajectories relative to task-specific composite rewards: rt = α rtask + β rexpert γ refficiency, where rtask measures answer accuracy, rexpert reflects preference according to GPT-4-based expert model, and refficiency penalizes excessive or redundant tool usage. The weighting coefficients α, β, and γ are set to 1.0, 0.2, and 0.1, respectively. Reward Modeling: The reward function is broken down as follows: rtask: The primary component of the reward function, measuring answer accuracy directly and calculating the task completion score per query. rexpert: Derived from the GPT-4 preference model, this term refines the models responses to align with expert knowledge. refficiency: Penalizes unnecessary tool usage, including repeated calls to the same tool without added value, excessive tool usage after an answer is found, and using irrelevant tools for task. The efficiency penalty is evaluated using both rule-based systems and an LLM-judge to classify unnecessary usage. 7 preprint (cid:1)(cid:3), where rG(x) GRPO Objective: The GRPO objective optimizes: LGRPO = E(x,y)D is the group-level baseline, computed as the average reward from responses in the same batch. This group normalization stabilizes the gradient estimates. (cid:2)log πθ(yx) (cid:0)r(x, y) rG(x) Additional Modifications: KL Regularization: We remove KL-regularization from the training pipeline, as it may hinder performance improvements, especially during multi-stage training. This aligns with literature showing benefits of omitting KL loss for model generalization[He et al., 2025]. Task Complexity: Task complexity increases progressively via curriculum learning, monitored by the average pass rate on tasks. This ensures the model is challenged appropriately without overwhelming it early in training."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate MedResearcher-R1 across both domain-specific and general-purpose benchmarks to assess its effectiveness in complex medical research tasks and its generalization capabilities beyond the medical domain. 5.1 Benchmarks MedBrowseComp [Chen et al., 2025b] is recently proposed benchmark specifically designed to evaluate the capabilities of LLM-based agents in retrieving and synthesizing medical evidence from multiple web sources. This benchmark presents agents with open-ended clinical questions that necessitate multi-step reasoning, strategic information gathering, and effective utilization of web browsing APIs to construct comprehensive medical assessments. GAIA [Shinn et al., 2023] (General AI Assistant) is comprehensive evaluation framework that tests real-world assistant capabilities through complex, multi-modal tasks requiring tool use, web search, and multi-step reasoning. The benchmark emphasizes tasks that are conceptually simple for humans but challenging for AI systems, focusing on fundamental skills like reading comprehension, logical reasoning, and the ability to use tools effectively in realistic scenarios. XBench-DeepSearch [Chen et al., 2025a] is an extensive multi-domain agent evaluation suite that systematically assesses tool-use capabilities across diverse open-domain tasks. The benchmark encompasses broad spectrum of scenarios, including fact-checking, comparative analysis, web browsing-based reasoning, and complex information synthesis tasks, providing comprehensive evaluation of the ability of LLM-based agents to navigate and utilize various tools in real-world problem-solving contexts. 5.2 Main Results As shown in Table 1, our tool-augmented agent achieves new state-of-the-art performance on the MedBrowseComp benchmark, achieving pass@1 score of 27.5/50 and outperforming both previous best agents and the Qwen2.5-32B baseline. The supervised fine-tuning (SFT) stage already delivers notable gains, while subsequent reinforcement learning further improves decision quality and tool orchestration efficiency. Notably, despite being primarily trained for medical domains, our agent demonstrates strong generalization to opendomain tasks shown in Table 2. On GAIA and XBench-deepsearch, our system shows competitive helpfulness scores, demonstrating the versatility of tool-based training paradigms. Table 1: Performance Comparison on MedBrowseComp Benchmarks (number correct out of 50) Model o3 search gemini2.5pro deepsearch o3 deepresearch claude-cua MedResearcher-R1-32B MedBrowseComp 19.0 24.5 25. 18.0 27.5 5.3 Qualitative Analysis To understand the underlying factors driving performance improvements, we conducted an in-depth analysis of the training data patterns and their impact on agent behavior. Our investigation reveals that training data following 8 preprint Table 2: Performance Comparison on Xbench-DeepSearch and GAIA Benchmarks Model Paradigm Xbench-DeepSearch GAIA Qwen-2.5-32B Qwen-2.5-72B GPT-4o GPT-4.1 QwQ-32B o4-mini DeepSeek-R1 Qwen-2.5-32B WebDancer-32B QwQ-32B WebSailor-7B WebSailor-32B WebSailor-72B MedResearcher-R1-32B(Ours) Direct Direct Direct Direct Direct Direct Direct Search-o1 ReAct Search-o1 ReAct ReAct ReAct ReAct 8.7 12.7 18.0 17.0 10.7 22.3 32.7 3.7 38.7 25.0 34.3 53.3 55.0 54. 13.6 14.6 17.5 22.3 22.3 33.3 16.5 28.2 40.7 39.8 37.9 53.2 55.4 53.4 the paradigm of iterative search-verification-synthesis yields the most significant improvements in deep research capabilities. Figure 5 illustrates representative example where our agent demonstrates superior research depth through systematic evidence gathering. The agent executes 4-step strategy: (1) initial broad search to identify relevant sources, (2) verification of information consistency across multiple authoritative medical databases, (3) targeted follow-up queries to resolve ambiguities, and (4) comprehensive synthesis of validated findings. This methodical approachcharacterized by multiple verification cycles ensuring answer uniqueness before final synthesiscontrasts sharply with baseline agents that exhibit premature convergence or suboptimal tool utilization patterns. Analysis of successful trajectories reveals that the critical differentiator lies in the searchverifynsynthesize pattern, where represents multiple verification iterations. Training instances exhibiting this pattern show 34.2% higher success Figure 5: Case study demonstrating the search-verify-synthesize paradigm: The agent performs multiple verification rounds across information sources, ensuring information consistency before synthesis. Baseline agents (shown in gray) terminate prematurely after initial search, while our approach (blue) continues until achieving high confidence through cross-validation. 9 preprint rates in complex multi-hop reasoning tasks compared to single-verification approaches. The iterative verification ensures answer uniqueness and factual grounding, particularly crucial for domains requiring high accuracy, such as medical diagnosis. These findings demonstrate that tool-augmented agent training effectiveness is fundamentally linked to the structural patterns in training data, with iterative verification serving as the key mechanism for developing robust deep research capabilities that generalize across diverse tool-reasoning environments."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 General Deep Research Methods Recent advances in agent-based deep research and autonomous information gathering frameworks have emerged in two main paradigms: Multi-agent planning architectures and Agent Reinforcement Learning systems. Multi-agent planning architectures decompose the research process into semantically distinct roles, with different agents focusing on subtasks such as retrieval, reasoning, synthesis, or evaluation. These agents collaborate through modular pipelines or structured communication protocols. CAMEL [Li et al., 2023] introduces communication-driven multi-agent framework in which agents communicate using natural language to solve complex planning and reasoning tasks. This framework emphasizes inter-agent communication to achieve policy coordination and emergent behavior. Since 2025, many large-scale reasoning models (LRMs) have extended deep research capabilities, such as OpenAI O3, Perplexity Deep Research, and Kimi K2. For example, Anthropics multi-agent research system proposes master agent that dynamically spawns specialized child agents to perform web search, document reading, and synthesis. This architecture excels at complex, long-term research tasks through parallel execution and implicit memory sharing. There are also many open-source projects implementing deep learning with multi-agent mechanisms, such as Deerflow. Deep learning systems implemented using multi-agent approaches have significant advantages: they are more interpretable and easier to scale through parallelization. However, due to the lack of targeted reasoning training, planning based solely on prompts and characters can cause errors to propagate across multiple agents and is unable to handle tasks requiring high-level reasoning. In contrast to modular designs, Agent RL approaches train single or semi-autonomous agents through reward-guided interaction with research environment (typically web browsing or open-domain question answering). These agents typically learn to autonomously search, click, read, and synthesize using offline data and are then fine-tuned through post-training. The ReAct agent [Yao et al., 2023b] was originally proposed as prompting strategy but has been further optimized using RLHF to enforce optimal reasoning paths. Reinforcement learning techniques enable agents to optimize tool usage and avoid hallucinations during long-term interactions. WebArena [Zhou et al., 2024] provides high-fidelity web interaction environment for training reinforcement learning agents to perform multi-hop reasoning and agentic data collection via real browser APIs, enabling realistic, feedback-driven learning. WebSailor [Li et al., 2025] enables superhuman web research in high-uncertainty QA environments. It uses synthetic task construction, RFT-based cold starts, and DUPO (Repeated Sampling Policy Optimization) reinforcement learning fine-tuning to build robust agents for tool-augmented web tasks. Search-R1 [Jin et al., 2025] trains LLMs to interleave reasoning and search via unified RL, while S3 [Jiang et al., 2025] decouples search from generation and reaches comparable accuracy with 70 fewer samples. Compared to deep research systems based on multi-role agents, agent reinforcement learning offers the advantage of internalizing the models problem-solving capabilities through learned behaviors, enabling better generalization to unknown tasks and adaptability to complex environments such as web browsing. However, while general-purpose web agents excel in open-domain environments, their architecture systematically ignores the importance and time constraints inherent to evidence provenance in healthcare. The lack of healthcarespecific components (e.g., de-identification engines, clinical-level evidence graders, and medication compliance audits) severely limits their clinical utility. 6.2 Medical RAG Systems The domain-specific Retrieval-Augmented Generation (RAG) architecture has made significant contribution to the field of medical clinical AI through systematic innovations in evidence integration. MedRAG [Zhao et al., 2025b] establishes paradigm for evidence-based generation by enabling immutable corpus retrieval from PubMed snapshots and proprietary databases. Deeprare [Zhao et al., 2025a] MedRAGs real-time evidence assimilation, which continuously synchronizes with evolving medical knowledge through live CDC/WHO data streams and dynamically weights it (F1 score +14.3%), directly addresses the knowledge obsolescence issue inherent in systems like DeepRare. SurgRAW [Low et al., 2025] pioneered the integration of real-time surgical video retrieval with 10 preprint reinforcement learning, enabling intraoperative decision support with an instrument recognition accuracy of 90.2%. Federated ClinicalCamel [Toma et al., 2023] addresses data fragmentation through cross-institutional knowledge distillation while maintaining privacy compliance (AUROC of 0.92 across 12 hospitals). Despite these advances, current medical RAG systems still suffer from fundamental limitations. First, knowledge obsolescence remains critical issue, as module updates in systems like DeepRare [Zhao et al., 2025a] require manual orchestration, resulting in curation delays that can reduce retrieval relevance by months. In addition, evidence misalignment manifests as semantic drift, which is particularly evident in KBLaMs plug-in architecture. Module updates lead to cumulative embedding misalignment (MRR drops by 18.4% after 5 iterations) [Wang et al., 2025]. 6.3 Medical Multi-Role Systems Recent advances in agent-based architectures reveal paradigm shift through endogenous integration of retrievalreasoning-verification loops, particularly evident in the emergence of Agentic RAG frameworks and multimodal knowledge integration. These systems demonstrate three core innovations that redefine clinical decision support: Dynamic Knowledge Internalization through self-updating graphs that eliminate external dependencies enables continuous synchronization with evolving medical knowledge. SeaKRs [Yao et al., 2024] self-aware retrieval introduces temporal grounding mechanisms that dynamically adjust knowledge weights based on publication recency and evidence grade, while Med-PaLMs [Tu et al., 2023] visual-linguistic separation processes radiology images and genomic data via dedicated pathways while maintaining diagnostic coherence. These approaches reduce knowledge latency from days to minutes compared to traditional RAG systems. Preference-Aligned Reinforcement Learning frameworks like MedicalGPT v2.4s GRPO (Group Relative Policy Optimization) achieve 98.7% agreement with clinician panels in oncology decisions [Xu, 2023]. Unified Cognitive Architectures collapse retrieval-reasoning-verification into integrated pipelines, exemplified by Microsofts MAI-DxO [Nori et al., 2025] with five collaborative agents achieving 85.5% diagnostic accuracy - quadruple average clinician performance. Regulatory compliance is maintained through Med-Geminis [Saab et al., 2024] 3-stage pipeline combining temporal grounding, clinician-verified SFT, and multiobjective RLHF. Despite these advancements, critical limitations persist in the reasoning capabilities of current medical multi-role agent systems - fundamental gap compared to deep reasoning methods in medical research. First, multi-step clinical reasoning remains constrained by shallow inference depth: while systems like AgentClinic [Schmidgall et al., 2025] demonstrate 42.9% diagnostic accuracy in sequential decision-making, this drops significantly when tasks require > 5 reasoning steps (27.3% at 7 steps). Second, causal reasoning deficits manifest in treatment planning scenarios, where agents struggle to model long-term outcome dependencies (e.g., chemotherapy sequencing effects) compared to human specialists (F1-score gap of 19.4% in NCCN guideline adherence) [?]. Third, adaptive reasoning limitations emerge in dynamic clinical environments - systems like MAI-DxO show 34% performance degradation when handling real-time patient deterioration scenarios requiring protocol switching. These challenges highlight the urgent need for next-generation architectures that bridge the reasoning depth and adaptability gap between multi-role agents and human medical experts."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we address the challenge of complex, evidence-based medical research by introducing new agent development framework centered on the KISA data generation approach. KISA systematically produces challenging, multi-hop medical questionanswer pairs with corresponding reasoning trajectories, grounded in rare entity mining and knowledge graph-based reasoning chains. This ensures that agents are exposed to the intricate, compositional problems characteristic of real-world medical research. Built on this rich dataset and equipped with comprehensive training pipelineincluding supervised fine-tuning, trajectory masking, and reinforcement learning with specialized medical toolsour agent, MedResearcher-R1 achieves state-of-the-art pass@1 accuracy on MedBrowseComp (27.5%) and demonstrates robust performance on general agent benchmarks. These findings show that MedResearcher-R1 is capable of solving complex medical questions that demand systematic exploration and nuanced evidence synthesis, highlighting its effectiveness as next-generation deep research agent in the medical domain."
        },
        {
            "title": "8 Future Work",
            "content": "Building on the foundation of this study, we identify several concrete directions for advancing deep medical research agents: 11 preprint Multi-modal Tool Integration: Extend the current framework to support multi-modal medical tools such as radiology image viewers, pathology slide analyzers, genomic data sources, and electronic health records. Such integration would enable agents to process and synthesize diverse data types, aligning more closely with real-world clinical workflows. Human-Expert Collaboration: Incorporate human-in-the-loop feedback from medical professionals to guide agent behavior. Developing interfaces for expert evaluation and annotation can improve reasoning quality, tool usage, and the clinical relevance of agent outputs. Safety and Reliability: Systematically study model safety and reliability for open deployment, focusing on robust hallucination detection, uncertainty estimation, and the implementation of fail-safe mechanisms suitable for highstakes medical scenarios. Advanced Medical Reasoning Benchmarks: Construct comprehensive benchmark for complex multi-hop reasoning across medical domainscovering pharmacology, diagnostics, epidemiology, genetics, surgical planning, and therapy. This would set higher standard for evaluating agents ability to orchestrate tools and synthesize evidence in challenging scenarios. Our framework paves the way for more aligned and reliable agent-based systems in specialized domains like healthcare. By releasing our codebase, datasets, and trained models, we seek to foster collaborative progress and rigorous evaluation, moving toward trustworthy AI companions that can augment medical research and support improved patient outcomes."
        },
        {
            "title": "References",
            "content": "Jiaming Bai, Jin Qiu, Jing Liu, et al. Qwen2: Scaling open language models with decoupled attention and comprehensive alignment, 2024. Available at https://huggingface.co/Qwen/Qwen2-72B. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651, 2025a. Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, and Danielle Bitterman. Medbrowsecomp: Benchmarking medical deep research and computer use. arXiv preprint arXiv:2505.14963, 2025b. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, and Jiawei Han. s3: You dont need that much data to train search agent via rl. arXiv preprint arXiv:2505.14146, 2025. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B. Mazomenos, and Yueming Jin. Surgraw: Multi-agent workflow with chain-of-thought reasoning for surgical intelligence, 2025. URL https: //arxiv.org/abs/2503.10265. Harsha Nori, Mayank Daswani, Christopher Kelly, Scott Lundberg, Marco Tulio Ribeiro, Marc Wilson, Xiaoxuan Liu, Viknesh Sounderajah, Jonathan Carlson, Matthew Lungren, Bay Gross, Peter Hames, Mustafa Suleyman, Dominic King, and Eric Horvitz. Sequential diagnosis with language models, 2025. URL https://arxiv.org/abs/2506. 22405. 12 preprint Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, Mike Schaekermann, Aishwarya Kamath, Yong Cheng, David G. T. Barrett, Cathy Cheung, Basil Mustafa, Anil Palepu, Daniel McDuff, Le Hou, Tomer Golany, Luyang Liu, Jean baptiste Alayrac, Neil Houlsby, Nenad Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh Azizi, Kimberly Kanada, SiWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He, Ben Caine, Albert Webson, Natasha Latysheva, Melvin Johnson, Philip Mansfield, Jian Lu, Ehud Rivlin, Jesper Anderson, Bradley Green, Renee Wong, Jonathan Krause, Jonathon Shlens, Ewa Dominowska, S. M. Ali Eslami, Katherine Chou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean, Demis Hassabis, Yossi Matias, Dale Webster, Joelle Barral, Greg Corrado, Christopher Semturs, S. Sara Mahdavi, Juraj Gottweis, Alan Karthikesalingam, and Vivek Natarajan. Capabilities of gemini models in medicine, 2024. URL https://arxiv.org/abs/2404.18416. Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. Agentclinic: multimodal agent benchmark to evaluate ai in simulated clinical environments, 2025. URL https://arxiv.org/ abs/2405.07960. Noah Shinn, Heng Zhu, Alex Chen, Xinyu Li, et al. Gaia: benchmark for general-purpose web agents. arXiv preprint arXiv:2307.12030, 2023. Augustin Toma, Patrick R. Lawler, Jimmy Ba, Rahul G. Krishnan, Barry B. Rubin, and Bo Wang. Clinical camel: An open expert-level medical language model with dialogue-based knowledge encoding, 2023. URL https: //arxiv.org/abs/2305.12031. Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, Basil Mustafa, Aakanksha Chowdhery, Yun Liu, Simon Kornblith, David Fleet, Philip Mansfield, Sushant Prakash, Renee Wong, Sunny Virmani, Christopher Semturs, Sara Mahdavi, Bradley Green, Ewa Dominowska, Blaise Aguera Arcas, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Karan Singhal, Pete Florence, Alan Karthikesalingam, and Vivek Natarajan. Towards generalist biomedical ai, 2023. URL https://arxiv.org/abs/2307.14334. Xi Wang, Taketomo Isazawa, Liana Mikaelyan, and James Hensman. Kblam: Knowledge base augmented language model, 2025. URL https://arxiv.org/abs/2410.10450. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Ming Xu. Medicalgpt: Training medical gpt model. https://github.com/shibing624/MedicalGPT, 2023. Renjun Xu and Jingwen Peng. comprehensive survey of deep research: Systems, methodologies, and applications. arXiv preprint arXiv:2506.12594, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023b. URL https://arxiv.org/abs/2210.03629. Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li. Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation, 2024. URL https://arxiv.org/ abs/2406.19215. Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, et al. An agentic system for rare disease diagnosis with traceable reasoning. arXiv preprint arXiv:2506.20430, 2025a. Xuejiao Zhao, Siyan Liu, Su-Yin Yang, and Chunyan Miao. Medrag: Enhancing retrieval-augmented generation with knowledge graph-elicited reasoning for healthcare copilot, 2025b. URL https://arxiv.org/abs/2502.04413. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024. URL https://arxiv.org/abs/2307.13854."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Harbin Institute of Technology"
    ]
}