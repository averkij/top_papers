{
    "paper_title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
    "authors": [
        "Jinghao Zhang",
        "Naishan Zheng",
        "Ruilin Li",
        "Dongzhou Cheng",
        "Zheming Liang",
        "Feng Zhao",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals."
        },
        {
            "title": "Start",
            "content": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment RLFR: EXTENDING REINFORCEMENT LEARNING FOR LLMS WITH FLOW ENVIRONMENT Jinghao Zhang 1,2 Naishan Zheng 1,3 Ruilin Li 2,4 Dongzhou Cheng 2,5 Zheming Liang 1,2 1University of Science and Technology of China 3ByteDance 2Shanghai Innovation Institute 5Southeast University 4Wuhan University Jiaqi Wang 2 Feng Zhao"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as reward signal. RLFR first demonstrates that well-established flow field can be sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting promising paradigm for reward shaping with auxiliary signals. 5 2 0 2 1 1 ] . [ 1 1 0 2 0 1 . 0 1 5 2 : r Figure 1: Overall performance on language (left) and multimodal (right) reasoning benchmarks. By introducing flow reward from latent space, RLFR shows consistent progress over RLVR with binary verification and entropy based shaping method Cheng et al. (2025) from logit space, highlighting the expressive latent space is much underexplored for reward signal collection. Corresponding authors. 1 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Figure 2: Policy optimized with RLVR prone to overlook potential valuable explorations in reasoning trajectories. To beyond binary verification, auxiliary signals are used for reward shaping of process tokens, involving token entropy and likelihood collected from logit space, where self-policy rewarding risks are non-negligible. Alternatively, we show that the latent space is much underexplored yet highly expressive and well established flow field can be sound environment for yielding flow reward from velocity deviations and extending RLVR with latent reward utilization."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in improving reasoning abilities of Large Language Models (LLMs) underscore the substantial promise of Reinforcement Learning with Verifiable Rewards (RLVR) Lambert et al. (2024); Jaech et al. (2024); DeepSeek-AI et al. (2025). By incentivizing the optimization of LLMs with outcome verification, there are far less susceptible to reward hacking. However, the binary verification prone to overlook potential valuable policy explorations in reasoning trajectories in cases that the answers are difficult to derive with part of correct trajectory Hammoud et al. (2025), thus provides an intolerant reward signal with decreased exploratory behavior Cui et al. (2025). more natural way to address these issues is to provide step-by-step process rewards along reasoning trajectories with Process Reward Models (PRMs) Zhao et al. (2025); Liu et al. (2025d); Yang et al. (2025a), however, the heavy annotation costs of intermediate steps pose significant bottleneck for scalability at time, and the misalignment between PRMs training corpora and online reasoning trajectories further introduce reward gaps Ye et al. (2025). Alternatively, the value model in PPO framework Schulman et al. (2017); Yue et al. (2025); Yuan et al. (2025) offers promising strategy, but the effective credit assignment over the binary outcome reward are still underexplored. To encouraging policy exploration, recent works leverage auxiliary signals for reward shaping Ng et al. (1999) beyond binary outcome verification, involving model likelihood Damani et al. (2025); Li et al. (2025b); He et al. (2025) and token entropy Cheng et al. (2025); Wang et al. (2025a) collected from logit space. While the confidence may serve as an appropriate indicator for examining policy states, it may not be well-suited for constituting reward signals for optimization Wang et al. (2025b); Cui et al. (2025). As the self-policy rewarding may cause LLM over exploits its own confidence estimates rather than learning genuinely improved reasoning strategies, where the potential hacking risks are non-negligible, and may undermine prolong RL training Liu et al. (2025b;c). In this work, we propose RLFR, that offering novel framework on shaping RLVR with flow rewards for regarding reasoning trajectory. We aspire to explore whether the broader latent space of LLMs encompass productive signals for reward utilization with reliable stability. RLFR first constructs flow fields Lipman et al. (2022); Liu et al. (2022) of model latents from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as reward signal. While the larger deviations are penalized as drifting away from the reference distribution formed by flow, and smaller deviations are encouraged. The flow fields are online updated alongside the policy optimization, with rejection sampling data filtered by desired metrics. We also formally show that the evidence lower bound of log-likelihood is constituted by negative velocity deviation, thus establishing the connection between velocity deviation and probability likelihood with inverse correlation under reference distribution. Particularly, RLFR first demonstrates that well-established flow field can be sound environment for reward signal collection, yielding stable performance gains throughout RL training with no sign RLFR: Extending Reinforcement Learning for LLMs with Flow Environment of degeneration. And we also highlight that the expressive latent space are highly underexplored as substrate for reward design, complementing prior auxiliary signals from logit space. Moreover, RLFR provides natural way to leverage expert reasoning trajectories from off-policy data into the constitution of reward signals through reference flow fields. Experiments on both language and multimodal reasoning benchmarks across Qwen and Llama models demonstrate the reliability of flow rewards with consistent progress over baseline RLVR methods. The contributions of this work can be summarized as follows: (1) We propose RLFR, promising reward shaping framework with flow rewards derived from LLMs latents, extending RLVR with latent rewards utilization. (2) We demonstrate that well-established flow field can be sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. (3) Both off-policy expert data and on-policy rejection sampling data are introduced for constituting reward signals as flow reference . (4) Comprehensive experiments on diverse reasoning benchmarks across both language and multimodal models demonstrate the reliability of our framework. All the codes, data, and model weights are released to foster future research in this area."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS Reinforcement Learning with Verifiable Rewards (RLVR) Lambert et al. (2024) constitutes general post-training paradigm where model response can be deterministically verified. Let πθ be an LLM parameterized by θ, that receives prompt and generates token sequence = (a1, ..., aK) as response. binary verifier then assigns scaler reward ro(q, a) {0, 1} to each prompt-response pair, where ro underlines its outcome nature. The goal of RL is to maximize the expected reward: (θ) = EqD,aπθ(q)[ro(q, a)]. Here, is dataset of prompts with corresponding ground-truth answer. Despite robustness against reward hacking, the coarse granularity outcome rewards make RLVR prone to overlooking the potential valuable policy exploration in reasoning trajectories in cases of derived incorrect answers. (1) RLVR Algorithms. Group Relative Policy Optimization (GRPO) Shao et al. (2024) as widely used reinforcement learning algorithm that simplfies the Proximal Policy Optimization (PPO) Schulman et al. (2017) by discarding the value model for baseline advantage estimation. While sampling group of response {ai}G i=1 per prompt and using their average reward as baseline, and the clipped surrogate objective is preserved as PPO, leading to the following maximization objective: JGRPO(θ) = EqD, {ai}G i=1πθ(q) 1 (cid:88) i=1 1 ai ai (cid:88) t=1 (cid:16) min ρi,k (cid:98)Ai, clip (ρi,k, 1 ε, 1 + ε) (cid:98)Ai (cid:17) , (2) where ρi,k = πθ(ai,kq,ai,<k) πθold (ai,kq,ai,<k) is the importance sampling ratio between the current and old policy models, and the advantage (cid:98)Ai,o is shared among all tokens within response ai and is computed as (cid:98)Ai,o = ri,o mean({ri,o}G std({ri,o}G i=1) i=1) . (3) We denote ri,o = ro(q, ai) for simplicity and clear comparison with later introduced dense rewards. Reward Shaping. As common technique in reinforcement learning for accelerating and stabilizing policy optimization, reward shaping transforms explicit environment-based rewards into proxy reward function, which typically involves operations like clipping or shifting, etc., Wang et al. (2024c), and may also incorporate auxiliary signals, such as response length or token entropy to steer the model toward desired behavior more effectively Arora & Zanette (2025); Cheng et al. (2025). 2.2 FLOW MATCHING Flow Matching (FM) Lipman et al. (2022); Liu et al. (2022) defines generative process that learns continuous-time velocity field, transporting samples from simple prior distribution pinit (e.g., Gaussian) into the target data distribution pdata. With data pairs ( x0, x1) sampled from pinit pdata, 3 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Figure 3: Distribution of trajectory tokens in LLM reasoning. (a) Distribution of trajectory tokens in latent space (up) and logit space (down). We perform 256 rollouts for prompt randomly sampled from MATH Hendrycks et al. (2021). The latent distribution show progressively expressive signals on tail trajectory tokens, as continuously interacting with preceding tokens for context compression. In contrast, neither the logit distribution nor the (b) & (c) textual clouds of reasoning trajectories reveal any distinguishable signals, highlighting the potential of latent space for reward utilization. the forward process is given by the linear interpolation: xt = (1t) x0+t x1, and the neural network vϕ is trained to predict the target velocity field = x1 x0 by minimizing the flow matching loss: (4) We denote as latent signal and distinguish it from tokens. The vϕ characterizes the pdata with the flow field through accurate velocity prediction, where the outsider can be notably identified. LFM( xt; ϕ) = EtU [0,1], x0pinit, x1pdata (cid:2)vϕ( xt, t) ( x1 x0)2(cid:3) ."
        },
        {
            "title": "3 RLFR",
            "content": "In this section, we first show that the latent space is highly expressive for reward utilization, and then introduce the flow reward formed by velocity deviation, where the connection with probability likelihood are established. We then introduce RLFR, that extends RLVR with flow rewards for process tokens in advantage estimation, while the flow conditions are involved to introduce context dependency. The flow fields are online updated with rejection sampling data during policy optimization. 3.1 ANALYZING LATENT SPACE IN REASONING TRAJECTORY Previous studies have shown the effectiveness of signals from logit space in shaping binary verification reward Damani et al. (2025); Cheng et al. (2025); Wang et al. (2025b); Yu et al. (2025b), while the broader latent space is much underexplored. In light of this, we further analysis the latents of reasoning trajectory tokens to evaluate the reliability of latent space for reward utilization. We use Qwen2.5-7B Yang et al. (2024) to generate 256 rollouts for prompt randomly sampled from MATH Hendrycks et al. (2021) with 0.7 temperature for decoding, and ultimately get 105 response tokens within the same subject content along with their corresponding latents. The latents are extracted from layers = (τ0L, ...τN L) for probing, where denotes the total number of model layers, and τi [0, 1] specifies the percentile positions for extraction. We show the case of τi = 0.5 in Figure 3, while additional results are provided in Appendix and show the similar tendency. The latents of tail tokens in reasoning trajectory show progressively pronounced signals when distinguished by answer correction. We observe that compared to head tokens in reasoning trajectory where basically no identifiable signals are emerged, the latents of tail tokens are much more expressive as continuously interacting with all preceding tokens for context compression. However, neither the logit space likelihood nor the textual clouds exhibit any noticeable signals. Subset of latents in reasoning trajectory with incorrect answer exhibit close to those in correct trajectories, while RLVR with outcome verification penalizes them. We discern that penalization should be applied to latents with substantial deviations from high-quality trajectory latents, while similar latents with minor deviations should be treated tolerantly. RLFR: Extending Reinforcement Learning for LLMs with Flow Environment"
        },
        {
            "title": "3.2 FLOW REWARD FROM VELOCITY DEVIATION",
            "content": "Encouraged by the expressive latent space, sound yet efficient metric is required to collect emerging signals for reward utilization, and we consider flow matching as underlying framework for which typically used in continuous modeling with established velocity field. Inspired by Li et al. (2025c), we notice that instead of using predicted velocity to reverse the forward process for data distribution generation, the accuracy of velocity prediction can serve as sensible metric to evaluate whether current samples are within the data distribution formed by flow. And we rewrite the Eq. 4 as FM( ak; t, τ ) = vϕ( ak,t, t) ( ak,1 ϵ)2, where t, τ U[0, 1], ϵ (0, I), (5) where ak denotes latents of token ak, and ak,t is the linear interpolation between ak and ϵ. The flow network vϕ is first pre-trained on latents of high-quality data to establish the reference distribution for offline start, and then frozen for flow reward calculation that evaluates the velocity deviation of the current sample ak under the reference flow field. Rϕ Debiasing the Timestep Impacting. The flow network vϕ provides velocity drifts toward reference distribution through the whole reverse process, which underscores the challenge on timestep priority in deviation evaluation for flow reward. Considering the connection between velocity prediction and score function Gao et al. (2024); Liu et al. (2025a), we present the score given by vϕ as: ak,t log pvϕ( ak,t) = ak,t 1 detailed proof is provided in Appendix C.1. Instead of relying on global-consistent direction provided by velocity prediction, the score function provides more accurate drift direction from local distributional gradients, which is more practical for deviation evaluation, and we have vϕ( ak,t, t). 1 (6) + Rϕ CF ( ak; , L) = EtT ,τ = EtT ,τ (cid:2) ak,t log pvϕ( ak,t) ak,t log pu( ak,t)2(cid:3) (cid:21) (cid:20) 1 ( ak; t, τ ) Rϕ , (7) where Rϕ CF is used for calculating flow reward with timestep collection by debiasing weighting. We suggest that the velocity deviation serves as surrogate for score deviation, while the coefficient emphasizes the timestep priority through the reverse process. We provide ablations in Sec. 4.3 that using flow reward at different timesteps for RL, where the larger timesteps with less noises are favorable in constituting reward signal, which is consistent with suggestion given by Eq. 7. Theoretical Analysis with Likelihood. Comparing to probability likelihood under reference distribution, velocity deviations show directional drifts error toward reference distribution. These two paradigms appear as different profile for distribution evaluation, and we further clarify their underlying relationship as: log pvϕ( ak) C( ak) λEt,τ [0,1] (cid:104) Rϕ FM( ak; t, τ ) (cid:105) , (8) where log pvϕ( ak) is the log-likelihood under distribution parameterized by vϕ, λ > 0 is constant, and C( ak) is the sundry term. We show that the evidence lower bound of log-likelihood is constituted by negative velocity deviation under reference distribution, indicated that the two paradigms are inversely correlated, and the minimal Rϕ FM( ak; t, τ ) with respect to given ak corresponds to the maximal evidence lower bound (ELBO) on the log-likelihood log pvϕ( ak). We calibrate the sign of flow rewards in RLFR, and provide the detailed proof of Eq. 8 in Appendix C.2. 3.3 EXTENDING RLVR WITH FLOW REWARD Velocity-Based Advantage Shaping. The main idea behind RLFR is to leverage the expressive latent space of LLMs with flow reward and thus extend RLVR with latent rewards utilization. Instead of sharing common advantage within response a, we shape advantage term for each token ak with flow returns, yielding by the accumulation of decayed flow rewards. While the advantage shaping makes it more flexible for different RLVR algorithms, without considering the specific advantage estimation methods. We have: ˆAk = γskrvϕ + ˆAo, (9) (cid:88) s=k 5 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Algorithm 1: Reinforcement Learning with Flow Rewards (RLFR) Inputs: Online data Q{q, a}, offline data Dof {q, z, a}, reference data buffer B, initial flow model vϕ, validated batch size κ, layer collection = (l1, ..., lN ), response quality metric() Offline Start: Extract policy latents from layer collection on Dof and construct the Perform flow training on Online Optimization: Initialize reference data buffer while Training do of with Eq. 4 where the loss is only calculated on response tokens of Generate rollouts for batch data from Optimize the policy with RL algorithms such as Eq. 2 with advantage estimated for each token by Eq. 9 Rejection-Sampling(G, metric) ; // Recommend metrics: correctness, entropy while > κ do Optimize flow vϕ on batch data from with policy latents in using Eq. 4 B.pop(batch data) end while end while return policy, flow vϕ and end training I[ˆrvϕ = β ˆrvϕ rvϕ CF ( ak); , L}a > η], where ˆrvϕ = minmax({Rϕ (10) where I[] is the indicator function and return 1 if condition is true and 0 otherwise, where we discard noisy fluctuations in flow rewards and preserve only substantial deviations above η. We perform the minmax-norm within the single sequence to regularize the numerical values between [1, 1]. and are the collections of timesteps and layer percentiles used to calculate the velocity deviations, and the latents are detached from the computational graph for stopping backpropagation. Practically, we incorporate the latents of subsequent token ˆak+1 to serve as conditions for assisting velocity prediction in flow reward, that further establishes context dependence with enlarged interactive space, and more ablations are provide in Sec 4.3. The flow reward provides stable examination on model latents that quantify their velocity deviation from flow pre-trained on off-policy high-quality data. k=1), Updated Rewards with Rejection-Sampling. As the policy are progressing during optimization alongside with their latents Huan et al. (2025), yielding flow rewards from frozen vϕ pre-trained on offline start data introduces inherent distribution gap. Therefore, we update flow by Eq. 4 throughout the policy optimization with online rejection-samping data, where the filtered metrics are flexible to direct the constitution of reference distribution for flow reward calculation. We provide detailed framework in Algorithm 1, and we empirically found that the correctness is still the most effective metric, where more ablations are provided in Sec. 4.3 for comparison."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Training Data. We conduct experiments for both language and multimodal models for evaluation. In language settings, we use openr1 Hugging Face (2025) as offline start data for flow pretraining, which contains 93k carefully curated mathematical reasoning problems. The reinforcement learning of RLFR is performed on MATH Hendrycks et al. (2021), which includes diverse reasoning-intensive problems spanning algebra, geometry, number theory, and combinatorics. In multimodal settings, we filter the math subset from MMPR Wang et al. (2024b) as offline data for flow pretraining, which consists 115k multimodal mathematical reasoning problems. Subsequently, the reinforcement learning is conducted on the MMK12 Meng et al. (2025a), which includes mathematics, physics, and general science with multimodal contexts. Evaluation. We assess the language reasoning performance using suite of standard mathematical reasoning benchmarks, including: AIME24/25 1, AMC23 2, MATH500 Hendrycks et al. (2021), and OlympiadBench He et al. (2024). We report Pass@1 metric with rollout temperature of 0 and 1https://huggingface.co/datasets/AI-MO/aimo-validation-aime 2https://huggingface.co/datasets/AI-MO/aimo-validation-amc RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Table 1: Overall performance on language reasoning benchmarks. Pass@32 and Pass@1 metrics are reported with zero-shot evaluation. means the model trained in our setting for evaluation. Model AIME25 AIME24 AMC MATH500 OlympiadBench Pass@32 Pass@1 Pass@32 Pass@ Pass@32 Pass@1 Pass@32 Pass@1 Pass@32 Pass@ Qwen2.5-Math-1.5B RLVR RLFR 26.7 23.3 30.0+6.7 Qwen2.5-Math-7B Qwen2.5-Math-7B-Inst Oat-Zero 30.0 36.6 30.0 RLVR Entropy Adv. RLFR Llama3.1-8B Llama3.1-8B-Inst RLVR RLFR 30.0 26.6 33.3+3.3 0.0 16.7 6.7 13.3+6.6 0.0 3.3 6.7+3.4 3.3 10.0 6.7 10.0 6.6 10.0+0.0 0.0 3.3 0.0 6.7+6. 43.3 46.7 50.0+3.3 56.6 46.6 56.6 56.6 53.3 56.6+0.0 0.0 26.7 6.7 16.7+10.0 13.3 16.7 13.3-3. 16.6 13.3 30.0 26.7 20.0 30.0+3.3 0.0 10.0 0.0 6.7+6.7 79.5 81.9 83.1+1.2 80.7 79.5 81.9 80.7 75.9 83.1+2. 14.4 69.8 25.3 27.7+2.4 32.5 41.4 44.6+3.2 37.3 50.6 55.4 57.8 54.2 59.0+1.2 0.0 21.7 7.2 12.0+4.8 90.2 91.8 92.4+0. 94.2 - - 92.0 93.0 92.6+0.6 28.2 - 46.0 46.4+0.4 31.8 71.0 72.0+1.0 50.8 79.8 79.6 79.2 75.6 80.6+1. 0.6 48.0 18.0 18.8+0.8 61.9 62.7 63.1+0.6 61.2 - - 64.8 61.2 66.1+1.3 5.9 - 18.6 19.3+0.7 22.8 33.5 35.7+2. 17.2 41.2 42.6 40.1 36.4 41.7+1.6 0.0 14.7 4.0 11.3+7.3 Table 2: Overall performance on multimodal reasoning benchmarks. Logic Avg. denotes average of LogicVista and VisuLogic. Math Avg. denotes average of other four math benchmarks. MathVista MathVision MathVerse WeMath LogicVista VisuLogic Math Avg. Logic Avg. Avg. Qwen2.5VL-3B-Inst RLVR RLFR Qwen2.5VL-7B-Inst R1-OneVision-7B OpenVLThinker-7B MM-Eureka-7B RLVR Entropy Adv. RLFR 62.0 65.9 67.7+1.8 65.6 63.7 64.5 73. 73.1 71.1 73.4+0.3 21.1 24.1 29.6+5.5 23.4 22.4 24.3 27.9 27.6 26.3 31.6+4.0 33.7 42.2 42.6+0.4 40.7 55.2 56.9+1. 47.3 45.2 46.1 51.9 53.4 52.9 50.3 58.7 48.0 52.1 52.0+4.0 64.6 63.1 66.1+1.5 38.9 41.1 42.3+1.2 47.8 38.9 38.7 46. 48.3 44.5 48.3+0.0 26.8 26.4 25.4-1.0 27.1 18.3 10.6 25.5 24.8 25.7 26.7+1.9 39.4 46.8 49.2+2.4 32.9 33.7 33.8+0. 37.2 42.5 44.1+1.6 47.4 46.1 46.3 53.0 37.5 28.6 24.6 36.2 44.1 40.2 39.1 47.4 53.3 53.2 55.8+2.4 36.5 35.1 37.5+1. 47.7 47.1 49.7+1.9 Pass@32 with temperature of 0.7 for decoding, under maximum response length of 8192 tokens. For multimodal reasoning benchmarks, we include MathVista (test mini) Lu et al. (2023), MathVerse (test mini) Zhang et al. (2024), MathVision (test) Wang et al. (2024a), WeMath Qiao et al. (2024), and logic benchmarks including LogicVista Xiao et al. (2024) and VisuLogic Xu et al. (2025). We adopt greedy decoding with temperature of 0 and report Pass@1 metric for evaluation. Implementation Details. We adopt RL algorithm as GRPO in our experiments. In flow pretraining, we use training batch size of 128 with 104 learning rate and warmup ratio of 0.1, where the LLM backbone is frozen. We empirically set the percentiles of layer collection as {0.25, 0.5, 0.75} throughout the model, where the layer position embedding are added. The flow network comprises 4 layers for 3B models and 6 layers for 7B/8B models. In reinforcement finetuning, we exclude both KL divergence loss and entropy loss, and use training batch size of 128 with policy learning rate of 106 and flow learning rate of 104, where the maximal response length is set for 4096. We use the threshold η of 0.6 to discard noisy fluctuations, the validated batch size κ of 32 for flow update, while the discount factor γ and coefficient β are set as 1 and 0.01. The timestep collection for yielding flow reward is set for {0.8}, and we use the temperature of 1 for response rollout. 4.2 MAIN RESULTS The main experimental results in Table 1 and Table 2 demonstrate that RLFR consistently outperforms baselines across both language and multimodal reasoning benchmarks. The baselines including basic RLVR with binary verification, existing approaches Liu et al. (2025e); Yang et al. (2025b); Deng et al. (2025); Meng et al. (2025b) and entropy-based advantage shaping method Cheng et al. (2025), which serves as strong baseline for logit-space comparison. Table 1 reports results on language reasoning benchmarks using Qwen and Llama base models across 1.5B and 7B/8B size, where RLFR shows consistent improvement, surpassing basic RLVR by 1.5% average score on Qwen2.5-Math-7B and by 5.3% average score on Llama3.1-8B, while achieving superior performance compared to entropy-based shaping method in logit space. Table 2 presents results on multimodal reasoning benchmarks, where RLFR achieves compelling improvements on challenging benchmarks like MathVision and MathVerse, and show steady generalization on out-of-domain logic benchmarks. The performance gains across both language and multimodal reasoning benchRLFR: Extending Reinforcement Learning for LLMs with Flow Environment marks on different model families show that the flow rewards derived from latent space reliably advance the performance with binary verification, while competitive to logit space shaping. Steps Type Frequently Reward Tokens 286 + - + - frac theta +-={} sqrt cdot matrix times abs pi math symbol sin line product set angle... To output of need equation problem value these many how determine import break maximum... frac sqrt area times cdot average triangle sin equality outer second integers length sum... To problem output determine understand python frac denote Given integers use properties... Table 3: Case study of reward tokens in training progress. + means positive flow reward and - means negative flow reward. Figure 4: Textual cloud of offline start dataset."
        },
        {
            "title": "4.3 ABLATIONS",
            "content": "We conduct ablations on the framework of RLFR and the variant of flow reward. The ablations are conducted on multimodal settings with Qwen2.5VL-3B, where Pass@1 metric is reported. Effect of online rejection sampling and corresponding metrics. We present the ablations of RLFR framework in Table 4, where both offline start of flow network and online updated with rejection sampling show steady contribution to the final performance. Additionally, noisy fluctuations in flow reward appear to be detrimental and the filtering for substantial deviations is essential for stable RL training. In Table 5, we present the effects of metric choices in rejection sampling that incentivize the desired reference distribution of the flow, including outcome correctness by binary verification, average trajectory entropy above 50th percentile, and their composition, and the correctness is still validated to be the most effective metric on filtering the reference data. Table 4: Ablation experimental results of model trained under incomplete RLFR framework. Method RLFR w/o offline start w/o rejection sampling w/o fluctuation filtering. Math Avg. Logic Avg. 49.2 45.6-3.6 46.8-2.4 47.6-1.6 33.8 31.3-2.5 32.4-1.4 32.6-1.2 Table 5: Ablation results of metrics used in rejection sampling that contribute to the online updated reference distribution for flow reward. Metric Math Avg. Logic Avg. Outcome correctness Trajectory entropy Correctness+Entropy 49.2 47.9 46.7 33.8 33.9 33.1 Effect of timesteps debiasing in flow reward. The flow reward derived from velocity deviations delivers several impact factors, including timesteps used for velocity prediction and conditions in flow network, and we provide their effect on reward quality in Figure 5 and Table 6. The results in Figure 5 provide strong evidence for timesteps debiasing method suggested in Sec. 3.2, where the larger timesteps with less noisy are preferred for flow reward, as their more reliable velocity prediction, and when using collection of timesteps such as = {0.2, 0.4, 0.6, 0.8} for calculating velocity deviation, the debiasing weighting further shows superiority by 1.7% average score improvement compared to equal average. Figure 5: Results on different timesteps for flow reward and debiasing effect. Effect of flow conditions in velocity prediction. We provide results with different velocity prediction conditions for constructing flow reward in Table 6, including latents from identity token, previous token, and post token, where the post token condition exhibits apparent advantage compared to other two paradigms, validating that using larger auxiliary space to support velocity prediction is beneficial for precedent reward quality. Table 6: Ablations results of auxiliary conditions in flow network for velocity prediction. Position Math Avg. Logic Avg. Identity Previous Post ak ak1 ak+1 34.0 32.6 33. 48.9 45.8 49.2 Condition 8 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment"
        },
        {
            "title": "4.4 ANALYSIS",
            "content": "We further analysis the reward behaviors derived from velocity deviations during training progress to better understand what is being encouraged. In Table 3, we found that: (1) Contrast to previous entropy-based method that encouraging tokens with logical connection function to dominate reasoning directions Wang et al. (2025b); Cheng et al. (2025), the flow rewards prefer tokens that practically execute the question, and depress tokens with empty content such as connection tokens. We attribute this to that the high entropy tokens typically correspond to ambiguity hidden states that prepared for large set of candidate tokens, which makes it hard to predict in flow field. (2) The positive reward tokens are initially related to offline start data as shown in Figure 4, such as sqrt, angle, and progressively updated with policy. (3) Despite the large portion of general words in pretrained dataset, the flow yields limited reward on these tokens rather than completely matching. Additionally, some tokens receive either positive reward or negative reward in cases, e.g., frac, indicating that the flow reward is capable of relying on efficient context dependence compressed within the hidden states, rather than individual token-level denotation for context comprehending. Takeaways Flow rewards prefer tokens that practically execute the question, and depress tokens with empty content such as connection tokens. High entropy in logit space makes larger velocity deviations in latent space, attributing to the ambiguity hidden states correspond to large set of candidate tokens. Flow rewards rely on efficient context dependence compressed within the hidden states, rather than individual token-level denotation for context comprehending."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Reinforcement learning beyond binary verification. Reinforcement learning with binary verifiable reward has recently demonstrates promising effectiveness in advancing reasoning abilities of Large Language Models Yu et al. (2025a); Hu et al. (2025); DeepSeek-AI et al. (2025). Despite robustness to reward hacking, the binary verification largely restricts the potential valuable exploration in reasoning trajectory. Indicated by the policy entropy Cui et al. (2025); Wang et al. (2025b), recent practices leverage variety of metrics derived from model likelihood either to shape the reward signal Cheng et al. (2025); Damani et al. (2025); He et al. (2025); Li et al. (2025b), or to serve as indicators for identifying tokens with different optimization Wang et al. (2025a;b); Fu et al. (2025). However, this work highlights the latent space could be an expressive substrate for reliable reward collection, complementing prior methods that primarily focus on logit space. Additionally, recent work also adopts the pass@k training Chen et al. (2025) that tolerates incorrect answer with potential valuable trajectory, which is orthogonal to this work and also promising direction. Flow Matching in Reinforcement Learning. As the most effective continuous modeling framework, flow matching Lipman et al. (2022); Liu et al. (2022) is especially expert at generating highdimensional signals, and has achieved remarkable success across wide range of domains. Building on recent progress of RLVR, series of works have been proposed to furthe advance the generation quality in respective areas, including visual generation Liu et al. (2025a); Li et al. (2025a); Xue et al. (2025) and robotics Pfrommer et al. (2025); McAllister et al. (2025). These works leverage the flow as the policy model for optimization, which is distinct from RLFR that uses flow as environment for reward collection, and concentrates on velocity deviation metrics rather than reverse the process."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we analysis the auxiliary signals for reward shaping of RLVR from the perspective of latent space, and show that the latent space is highly expressive yet underexplored, complementing prior methods that focus closely on logit space. In light of this, RLFR offers novel framework on shaping RLVR with flow reward,mwhere the flow field of model latent are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the deviations of policy latents within it are quantified to serve as reward signal, extending RLVR for latent reward uti9 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment lization. RLFR first demonstrates that well-established flow field can be sound environment for reward signal collection, yielding steady performance improvements across both language and multimodal benchmarks, highlighting the potential of latent substrate for reward design. Additionally, RLFR naturally leverages expert reasoning trajectories from off-policy data into the constitution of reward signal, instead of relying on self-confidence. Future directions involve scaling the flow environment to release the latent potential, and the prospect of latent signals for test-time scaling."
        },
        {
            "title": "REFERENCES",
            "content": "Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, and Dongbin Zhao. Srft: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025. Ruiqi Gao, Emiel Hoogeboom, Jonathan Heek, Valentin De Bortoli, Kevin Murphy, and Tim Salimans. Diffusion meets flow matching: Two sides of the same coin. 2024. URL https://diffusionflow. github. io, 2024. Hasan Abed Al Kader Hammoud, Hani Itani, and Bernard Ghanem. Beyond the last answer: Your reasoning trace uncovers more than you think. arXiv preprint arXiv:2504.20708, 2025. Andre He, Daniel Fried, and Sean Welleck. Rewarding the unlikely: Lifting grpo beyond distribution sharpening. arXiv preprint arXiv:2506.02355, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning. arXiv preprint arXiv:2507.00432, 2025. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36:6548465516, 2023. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. 11 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025a. Yi-Chen Li, Tian Xu, Yang Yu, Xuqin Zhang, Xiong-Hui Chen, Zhongxiang Ling, Ningjing Chao, Lei Yuan, and Zhi-Hua Zhou. Generalist reward models: Found inside large language models. arXiv preprint arXiv:2506.23235, 2025b. Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, and Ping Luo. Aligning latent spaces with flow priors. arXiv preprint arXiv:2506.05240, 2025c. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507, 2025b. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025c. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025d. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025e. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053, 2025. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rulebased reinforcement learning. arXiv preprint arXiv:2503.07365, 2025a. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025b. Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pp. 278287. Citeseer, 1999. Samuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073, 2025. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 12 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415 1428, 2021. Jiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, and Guorui Zhou. Stabilizing knowledge, promoting reasoning: Dual-token constraints for rlvr. arXiv preprint arXiv:2507.15778, 2025a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024a. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024b. Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex DAmour, Sanmi Koyejo, and Victor Veitch. Transforming and combining rewards for aligning large language models. In Proceedings of the 41st International Conference on Machine Learning, ICML24, 2024c. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv:2412.15115, 2024. Wenkai Yang, Jingwen Chen, Yankai Lin, and Ji-Rong Wen. Deepcritic: Deliberate critique with large language models. arXiv preprint arXiv:2505.00662, 2025a. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025b. Chenlu Ye, Zhou Yu, Ziji Zhang, Hao Chen, Narayanan Sadagopan, Jing Huang, Tong Zhang, and Anurag Beniwal. Beyond correctness: Harmonizing process and outcome rewards through rl training. arXiv preprint arXiv:2509.03403, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025b. 13 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. Whats behind ppos collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. 14 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment"
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 SETTINGS Experiments are conducted on 8 H20 GPUs. We use AdamW optimizer and sample 8 rollouts per prompt with (0.2, 0.28) clip range in policy loss for training. The prompt template is shown in Tab. 7, where we adopt bbox template for language training and tag template for multimodal training. In evaluation, we deploy Qwen2.5-7B-Instruct model server for answer extraction and judge, and adopt DeepSeek v3.1 for more complex benchmarks, such as MathVision and MathVerse. RLFR training prompt Bbox <im_start>system Please reason step by step, and put your final answer within boxed{}. <im_end> <im_start>user {{question}}<im_end> <im_start>assistant RLFR training prompt Tag <im_start>system You should first thinks about the reasoning process in the mind and then provides the user with the answer. Your answer must be in latex format and wrapped in $...$. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> Since $1+1=2$, so the answer is $2$. </think> <answer> $2$ </answer>, which means your output should start with <think> and end with </answer>. <im_end> <im_start>user {{question}}<im_end> <im_start>assistant Table 7: Training prompt for RLFR. A.2 TRAINING LOGS We monitor the training dynamics of RLVR and RLFR in Fig. 6 for comparison. During training, the flow reward derived from latent space steadily improves the reasoning performance and accelerates the policy optimization, validating the reliability of the latent signals and their underexplored expressiveness. The policy entropy of RLFR also stabili zed at slightly higher level during the training plateau compared to RLVR, underscoring the effectiveness of velocity deviation as the dense reward for encouraging exploration. While the response length shows healthy behavior with steady increases and adjustment, while no sign of degeneration are observed. Figure 6: Training logs of RLVR and RLFR on Qwen2.5VL-3B."
        },
        {
            "title": "B EXTENDED ANALYSIS OF LATENT SPACE",
            "content": "Sec. 3.1 analysis the latent space of Qwen2.5-Base-7B at layer percentile of 0.5. We further show that the broad latent space exhibits the similar tendency throughout the LLM. In Fig. 7, we provide 15 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Figure 7: Distribution of reasoning trajectory tokens in latent space across different layer percentiles of the Qwen2.5-Base-7B, which consistently show expressive signals on tail trajectory tokens, highlighting the broader potential of latent space for reward signal collection. the latent distributions of reasoning trajectory tokens at the {25, 50, 75, 100} layer percentiles, and found that there is no evidence of contradiction and specialization across layer positions, instead, the latent space exhibits coherent and consistent signals for trajectory quality identification. In practice, we exclude the 100th percentile in training, as the last hidden states are heavily modulated by the lm head for logit prediction, and we therefore rely on intermediate percentiles for reward collection."
        },
        {
            "title": "C THEORETICAL ANALYSIS",
            "content": "C.1 DERIVATION OF SCORE FUNCTION FOR VELOCITY Here, we provide the derivations for Eq. 6 that establish the connection between velocity prediction and score function. Proof. We consider the linear interpolation xt = αtx0 + βtx1, where x0 and x1 are independent. We denote derivatives αt = the data x1, the noisy variable xt is Gaussian: x0 (0, I), x1 pdata, dt αt, βt = dt βt. Conditioned on with conditional score pt(xt x1) (cid:0)βtx1, α2 I(cid:1), log p(xt x1) = xt βtx1 α2 . By Fishers identity, the marginal score given by st(x) = log pt(x) = E[ log p(xt x1) xt = x] = 1 α2 (cid:16) (cid:17) βt E[x1 xt = x] . Rearranging yields E[x1 xt = x] = (cid:16) 1 βt + α2 st(x) (cid:17) . Considering dt xt = αtx0 + βtx1, thus the velocity field is (cid:12) (cid:105) (cid:104) vt(x) = (cid:12) (cid:12) xt = αtx0 + βtx1 = αtE[x0 xt = x] + βE[x1 xt = x] . 16 (11) (12) (13) (14) RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Since x0 = (xt βtx1)/αt, we have E[x0 xt = x] = αt βt αt E[x1 xt = x]. Substituting the Eq. 15 and Eq. 13, the velocity field is given by vt(x) = = αt αt βt βt + (cid:17) αt (cid:16) βt αtβt (cid:32) βt βt αt αt + α2 E[x1 xt = x] (cid:33) st(x). For linear schedule, where αt = 1 and βt = t, we have"
        },
        {
            "title": "The score function is given by",
            "content": "vt(x) = 1 + 1 st(x). st(x) = 1 + 1 vt(x). (15) (16) (17) (18) Eq. 17 and Eq. 18 show the exact equivalence between the score function and velocity field under the linear interpolation schedule. C.2 PROOF OF VARIATIONAL LOWER BOUND Here, we provide the proof of Eq. 8 that establish the connection between the velocity deviations which measured by flow matching objective, and the likelihood given by evidence lower bound (ELBO) under target distribution. We indicate that the connection between ELBO and diffusion objective has been shown by previous works Kingma et al. (2021); Song et al. (2021); Kingma & Gao (2023), and we further extend it to velocity field. Proof. For sample that need to be evaluated for velocity deviation in reference flow distribution and consider the linear interpolation yt = αtx0 + βty, with x0 pinit. Let st(yt) = log pvϕ(yt) be the score of yt. Recall the flow matching objective in Eq. 5, we take LFM(y; ϕ) = (cid:90) 1 0 Eyqt (cid:20) 1 2 vϕ(y, t) uy 2 (cid:21) dt. By continuity equation, the density and velocity of flow defined by vϕ(yt, t) satisfies tpvϕ(yt) + (cid:0)pvϕ(yt) vϕ(yt, t)(cid:1) = 0. (19) Given log pvϕ(yt) = pvϕ(yt)/pvϕ(yt), we have log pvϕ(yt) = vϕ(yt, t) vϕ(yt, t) log pvϕ(yt). (20) Considering dt yt = vϕ(yt, t), and substituting the Eq. 20, the total derivative of log pvϕ(yt) is dt log pvϕ(yt) = log pvϕ(yt) + log pvϕ(yt) dt yt = vϕ(yt, t) vϕ(yt, t) log pvϕ(yt) + log pvϕ(yt) = vϕ(yt, t). dt yt (21) 17 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Therefore, integrating over [0, 1] yields the standard change-of-variables formula log pvϕ(y) = log pinit(x0) (cid:90) 1 0 vϕ(yt, t) dt = Ex0pinit (cid:2) log pinit(x0)(cid:3) (cid:90) 0 Eypdata (cid:2) vϕ(yt, t)(cid:3) dt = C0(x) (cid:90) 1 Eypdata (cid:2)vϕ( yt, t), log pdata( yt )(cid:3) dt (22) where , denotes inner product, and we derive the last step from the Steins identity. Considering a, = a, 1 2 b2 + 2 g2 1 2 b2, we thus substitute = vϕ(y, t), = uy , and = log pdata(y), yielding vϕ, log pdata = vϕ, uy 2 uy 2 + 1 2 log pdata2 1 2 log pdata uy 2. We simplify vϕ(yt, t) as vϕ and pdata(yt) as pdata for clarity, and have (cid:90) 1 0 Epdata (cid:2)vϕ, log pdata(cid:3) dt = (cid:90) 1 (cid:104) Epdata vϕ, uy (cid:105) dt + B(y), (23) where B(y) depends only on the fixed bridge (pdata, uy ), and have B(y) = (cid:90) 1 0 (cid:104) Epdata 1 2 uy 2 + 1 2 log pdata2 1 2 log pdata uy 2(cid:105) dt. (24) Considering the FenchelYoung inequality vϕ, uy 1 2λ vϕ2 λ uy 2, λ > 0, where we substitute into Eq. 23, and havex (cid:90) 1 Epdata (cid:2)vϕ, log qdata(cid:3) dt = (cid:90) 1 0 (cid:90) 1 (cid:104) Epdata (cid:104) Epdata 1 2λ 1 2λ vϕ2 dt + B(y) 2(cid:105) uy λ 2 2(cid:105) vϕ λuy dt + B(y) (25) For the special case λ = 1, this simplifies to the equivalence given by Eq 8. Therefore, combining with Eq. 22, we obtain log pvϕ(y) C(y) λ (cid:90) 1 Eypdata (cid:104) 1 2 vϕ(yy, t) uy 2(cid:105) dt, which establish the connection between the velocity and likelihood under target distribution."
        },
        {
            "title": "D CASE STUDY",
            "content": "Question and Ground Truth Question: Jen enters lottery by picking 4 distinct numbers from = {1, 2, 3, , 9, 10}. 4 numbers are randomly chosen from S. She wins prize if at least two of her numbers were 2 of the randomly chosen numbers, and wins the grand prize if all four of her numbers were the randomly chosen numbers. The probability of her winning the grand prize given that she won prize is Ground Truth: 116 where and are relatively prime positive integers. Find + n. 18 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Response from RLVR Baseline To solve this problem, we need to calculate the probability of Jen winning the grand prize given that she won prize. We can break this down into several steps: 1. Calculate the total number of ways to choose 4 numbers from the set ( = {1, 2, 3, . . . , 10} ). 2. Calculate the number of ways Jen can win prize (i.e., at least 2 of her numbers are among the 4 chosen numbers). 3. Calculate the number of ways Jen can win the grand prize (i.e., all 4 of her numbers are among the 4 chosen numbers). 4. Use the conditional probability formula to find the probability of winning the grand prize given that she won prize. 4 (cid:1) ). Lets go through these steps one by one. 1. The total number of ways to choose 4 numbers from 10 is given by the combination formula ( (cid:0)10 2. The number of ways Jen can win prize is the sum of the number of ways she can have exactly 2, 3, or 4 of her numbers among the 4 chosen numbers. This can be calculated as: (cid:1) ) - Number of ways to have exactly 2 of her numbers among the 4 chosen numbers: ( (cid:0)4 - Number of ways to have exactly 3 of her numbers among the 4 chosen numbers: ( (cid:0)4 (cid:1) ) - Number of ways to have exactly 4 of her numbers among the 4 chosen numbers: ( (cid:0)4 (cid:1) ) 3. The number of ways Jen can win the grand prize is simply the number of ways to choose 4 numbers from her 4 chosen numbers, which is ( (cid:0)4 4. The probability of winning the grand prize given that she won prize is the ratio of the number of ways to win the grand prize to the number of ways to win prize. (cid:1) (cid:0)6 (cid:1) (cid:0)6 (cid:1) (cid:0)6 (cid:1) = 1 ). (cid:1) (cid:0)6 0 1 3 4 2 2 4 Lets calculate this using Python. import math # Total number of ways to choose 4 numbers from 10 total_ways = math.comb(10, 4) # Number of ways to win prize ways_to_win_prize = (math.comb(4, 2) * math.comb(6, 2) + math.comb(4, 3) * math.comb(6, 1) + math.comb(4, 4) * math.comb(6, 0)) # Number of ways to win the grand prize ways_to_win_grand_prize = math.comb(4, 4) * math.comb(6, 0) # Probability of winning the grand prize given that she # won prize probability = ways_to_win_grand_prize / ways_to_win_prize from fractions import Fraction fraction = Fraction(ways_to_win_grand_prize, ways_to_win_prize) m, = fraction.numerator, fraction.denominator result = + print(result) output 47 The probability of Jen winning the grand prize given that she won prize is ( 1 Therefore, the values of (m) and (n) are 1 and 47, respectively. The sum (m + n) is (1 + 47 = 48). 47 ). So, the final answer is ( 48 ). 19 RLFR: Extending Reinforcement Learning for LLMs with Flow Environment Response from RLFR 4 10! 4!(104)! = 210 To solve this problem, we need to calculate the probability of Jen winning the grand prize given that she won prize. Lets break down the problem step-by-step: (cid:1) = 1. Total number of ways to choose 4 numbers from 10: (cid:0)10 2. Number of ways Jen can win prize: - Jen wins prize if at least 2 of her numbers are among the 4 chosen numbers. - We need to calculate the number of ways she can have 2, 3, or 4 of her numbers among the 4 chosen numbers. - Number of ways to have exactly 2 of her numbers among the 4 chosen numbers: (cid:0)4 15 = 90 - Number of ways to have exactly 3 of her numbers among the 4 chosen numbers: (cid:0)4 6 = 24 - Number of ways to have exactly 4 of her numbers among the 4 chosen numbers: (cid:0)4 1 = 1 - Total number of ways Jen can win prize: 90 + 24 + 1 = 115 3. Number of ways Jen can win the grand prize: - Jen wins the grand prize if all 4 of her numbers are among the 4 chosen numbers. - There is only 1 way for this to happen: (cid:0)4 4. Number of ways to win the grand prize Number of ways to win prize Probability of winning the grand prize given that she won prize: = 1 115 (cid:1) (cid:0)6 (cid:1) (cid:0)6 (cid:1) (cid:0)6 (cid:1) = (cid:1) = 6 (cid:1) = 1 (cid:1) = 1 3 2 4 1 4 2 1 115 , so = 1 and = 115. Since 1 and 115 are relatively prime, the final 5. Final answer: The probability is answer is: + = 1 + 115 = 116 The final answer is: 116 ]"
        }
    ],
    "affiliations": [
        "ByteDance",
        "Shanghai Innovation Institute",
        "Southeast University",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}