{
    "paper_title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
    "authors": [
        "Zihan Yang",
        "Shuyuan Tu",
        "Licheng Zhang",
        "Qi Dai",
        "Yu-Gang Jiang",
        "Zuxuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 4 1 0 9 0 . 2 0 6 2 : r ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation Zihan Yang1,*, Shuyuan Tu1,*, Licheng Zhang1, Qi Dai2, Yu-Gang Jiang1, Zuxuan Wu1 1Fudan University, 2Microsoft Research Asia"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves 40 speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively. Website: https://github.com/pnotp/ArcFlow"
        },
        {
            "title": "Introduction",
            "content": "Diffusion and flow matching models have emerged as the dominant paradigms for high-fidelity visual generation [1, 15, 20, 2935]. Despite their impressive capabilities, they rely on iterative differential equation solvers, typically requiring 40 to 100 denoising steps to traverse the trajectory from noise to data, making them impractical for real-time applications. Therefore, accelerating sampling without compromising quality remains critical challenge. To address this issue, recent research has explored various paradigms to distill pre-trained teacher model into few-step student generator. Different methods range from progressive distillation [23, 25] to consistencybased approaches [22, 28], and distribution matching [7, 26, 39] that employ adversarial or divergence losses to align distributions. Equal Contribution. 1 Figure 1 Comparisons between images generated by ArcFlow and other state-of-the-art distillation methods based on Qwen-Image-20B, demonstrating the power of ArcFlow for few-step high-fidelity generation while maintaining remarkable parameter efficiency. However, their essence still lies in approximating the trajectory from the teacher generation process (40 100 steps), whose tangent directions vary over multiple timesteps, via linear shortcut under very few steps (2 4 steps). This enforces the students to implicitly learn such tangent variation with linear trajectories, leading to geometric mismatch. In light of this, we propose ArcFlow, few-step distillation framework that introduces explicit non-linear flow trajectories via velocity parameterization to approximate the flow trajectories from pre-trained teacher model. Since the flow trajectory is equivalent to how the velocity evolves across timesteps, we utilize the notion of momentum in physics [13] to describe this evolution, where the overall trajectory is determined only by the initial velocity and momentum factor. Consequently, we parameterize the velocity field as weighted mixture of continuous momentum processes. By harnessing the continuity of this momentum process over adjacent timesteps, the model can extrapolate coherent velocities through the parameterization, thus efficiently constructing the non-linear trajectories based on the predicted velocity shifts. Notably, our parameterization admits closed-form analytical solution to the flow ODE [27], enabling direct computation of the terminal state in single forward pass. This ensures the predicted velocity evolution is applied accurately across timesteps within the interval, rather than being approximated by linear discrete update, thereby ensuring high-precision flow distillation. By tackling the geometric mismatch, ArcFlow ensures that the trajectory of the student naturally aligns with the teachers inherent tangent variation. This alignment fundamentally simplifies the distillation task, enabling parameter-efficient training. Unlike prior methods requiring full-model training, ArcFlow achieves state-of-the-art results by fine-tuning only lightweight LoRA adapters and the output head. 2 As illustrated in figure 1, with only 2 NFEs, ArcFlow achieves high-fidelity generation comparable to the teacher Qwen-Image-20B, surpassing the 2-step generation quality of pi-Flow [4] and TwinFlow [7] while utilizing very few trainable parameters. Furthermore, the convergence analysis (figure 2) highlights that the training of ArcFlow yields significantly faster convergence and superior stability, validating the effectiveness of the alignment with non-linear trajectory that efficiently eliminates the geometric optimization bottleneck. Our main contributions are as follows: (1) We propose ArcFlow, the first distillation framework to explicitly construct non-linear flow trajectory to approximate the teacher trajectory. We parameterize the velocity as continuous momentum mixture, whose analytic solution for trajectory integration ensures high-precision alignment with the teacher. (2) We introduce an analytic trajectory solver for ArcFlow, which enables an efficient objective for distillation. It simplifies the training process, enabling parameterefficient adaptation and fast convergence. (3) Evaluations on benchmark datasets demonstrate superior robustness of ArcFlow, achieving SOTA across diverse backbones. It achieves 40 inference speedup over the teacher and at most 4 faster training convergence than prior methods, while fine-tuning only less than 5% of the original parameters. Figure 2 Comparison of FID scores across training iterations for different methods. ArcFlow achieves superior convergence speed."
        },
        {
            "title": "2 Related Work",
            "content": "Text-to-image generation Diffusion [15] and flow matching models [20] have emerged as the standard for high-resolution visual synthesis. Recent scaling efforts, such as Stable Diffusion 3 [9], FLUX [17, 18], and Qwen-Image [37], leverage Transformer to achieve exceptional performance. However, these models fundamentally rely on integrating probability flow ODEs via iterative numerical solvers. They necessitates 40 to 100 function evaluations (NFEs), creating significant latency bottleneck that hinders real-time deployment and necessitates acceleration. Few-step Image Generation Accelerating the inference of diffusion models has become critical topic, aiming to achieve high-fidelity synthesis with few function evaluations (NFEs). To this end, knowledge distillation has emerged as dominant paradigm, where student model is trained to approximate the complex sampling trajectory of pre-trained teacher. One line of work focuses on trajectory simplification, such as Progressive Distillation [23, 25] and Rectified Flow [21], attempting to reduce NFEs by iteratively straightening the flow. However, they struggle to eliminate discretization errors in the few-step regime. Consistency Models [22, 28] map points directly to the data via self-consistency constraints, but they often require computationally expensive Jacobian-vector product calculations to maintain convergence stability [11]. To further push limits to 1-4 steps, VSD [36] and DMD [39] introduce discriminator-based losses, and TwinFlow [7] uses self-adversarial objective. While these improve visual sharpness, the reliance on adversarial objectives and unstable training leads to mode collapse and high memory overhead. Recent attempts [4, 5] approximate evolution of velocities via Gaussian mixtures. However, their probabilistic approximations lack precision at lower NFEs (2 steps). By contrast, ArcFlow utilizes an analytic momentum solver to achieve precise, stable, and parameter-efficient distillation."
        },
        {
            "title": "3 Method",
            "content": "Pre-trained diffusion models follow PF-ODE integration trajectories with constantly varying tangents [27], whereas existing distillation methods [7, 28, 39] approximate them using linear shortcuts, resulting in geometric mismatch. 3 Figure 3 ArcFlow Framework. (a) The forward pipeline of ArcFlow. Given an input ğ‘¥ğ‘¡ , the condition ğ‘ and timestep ğ‘¡, DiT backbone with three projection heads predicts the parameters ğ‘£, ğœ”, ğ›¾ across ğ¾ dynamic modes, which respectively denote the mode-specific velocities, momentum factors, and the gating probabilities used to reconstruct the teacher velocity field. (b) comparison of flow trajectories produced by the multi-step teacher model, the few-step linear student model, and our ArcFlow. In light of this, we propose ArcFlow, text-to-image distillation framework that utilizes the notion of momentum process [13] to construct non-linear flow trajectories across long timestep intervals. In this section, we first formalize the momentum-based parameterization (section 3.1), derive the analytical trajectory integration solver (section 3.2), and detail the trajectory distillation strategy to train ArcFlow (section 3.3). The condition variable (text prompt) is omitted from subsequent descriptions for brevity. Since ArcFlow mainly relies on learning from pre-trained teacher, we define the velocity field from the frozen teacher as ground truth for the subsequent analysis."
        },
        {
            "title": "3.1 Momentum Parameterization of Probability Flow",
            "content": "The Probability Flow ODE framework [27] reveals that the diffusion process follows continuous trajectory, where the denoising velocities are strongly correlated between adjacent timesteps. However, standard numerical solvers (e.g., Euler method [2]) appoximate the integration process by taking discrete steps independently without considering their associations across timesteps. Thus, we argue that the standard multi-step sampling, which re-evaluates the network repeatedly to traverse this smooth trajectory, suffers from severe redundancy. To explicitly exploit this inherent continuous evolution of the velocity field across timesteps, we introduce the notion of momentum in physics [13], to parameterize such properties. Specifically, let (xğ‘¡ , ğ‘¡) denote the velocity field at timestep ğ‘¡ [0, 1]. The relationship between velocities at adjacent timesteps should follow momentum transmission law parameterized by factor ğ›¾. It implies that the velocity transfer as (xğ‘¡ , ğ‘¡) = (xğ‘¡+Î”ğ‘¡ , ğ‘¡ + Î”ğ‘¡) ğ›¾Î”ğ‘¡ from ğ‘¡ + Î”ğ‘¡ to ğ‘¡. Recursively apply the above formula from starting timestep ğ‘¡ğ‘  to any ending timestep ğ‘¡ [0, ğ‘¡ğ‘ ), the velocity evolution is derived as follows: ğ”¼ (cid:2)v (xğ‘¡ , ğ‘¡) (cid:0)xğ‘¡ğ‘  , ğ‘¡ğ‘  (cid:1) , ğ›¾(cid:3) = (cid:0)xğ‘¡ğ‘  , ğ‘¡ğ‘  (cid:1) ğ›¾ğ‘¡ğ‘  ğ‘¡ , (1) where ğ›¾ â„+. Based on Eq. (1), given the initial velocity (xğ‘¡ğ‘  , ğ‘¡ğ‘ ) , (xğ‘¡ , ğ‘¡) at any timestep ğ‘¡ [0, ğ‘¡ğ‘ ) can be extrapolated directly. Thus, our momentum parameterization allows flow matching to analytically predict the velocity at every timestep after only single NFE, reaching xğ‘¡ directly. While momentum helps approximate velocity evolution, single momentum factor ğ›¾ is insufficient to capture the hierarchical frequency dynamics in image generation. Empirical studies [8] show that different frequency components evolve at distinct rates during denoising, implying that the corresponding velocity field (xğ‘¡ , ğ‘¡) inherently consists of multiple evolution modes with different decay rates. To model such dynamics, as shown in figure 3(a), we formulate (xğ‘¡ , ğ‘¡) as probabilistic mixture of ğ¾ distinct momentum modes. Specifically, we decompose the velocity field into different modes indexed by ğ‘§ [1, ..., ğ¾] , and then derive the overall velocity field vğœƒ (xğ‘¡ , ğ‘¡), optimized by parameter ğœƒ: vğœƒ (xğ‘¡ , ğ‘¡) = ğ”¼ğ‘§ğ‘ğœƒ(ğ‘§xğ‘¡ ) [v (xğ‘¡ , ğ‘¡ ğ‘§)] = (cid:213)ğ¾ ğ‘˜=1 ğ‘ğœƒ (ğ‘§ = ğ‘˜xğ‘¡ ) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) (cid:124) ğœ‹ğ‘˜ (xğ‘¡ ) vğ‘˜ (xğ‘¡ ) ğ›¾ğ‘˜ (xğ‘¡ )1ğ‘¡ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) Mode-specific Dynamics (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) , (2) where ğœ‹ğ‘˜(xğ‘¡) [0, 1] refers to the gating probability predicted by the parameter ğœƒ, subject to (cid:205) ğœ‹ğ‘˜ = 1. vğ‘˜(xğ‘¡) â„ğ· and ğ›¾ğ‘˜(xğ‘¡) â„+ are the predicted basic velocity and momentum factor for the ğ‘˜-th mode. Consequently, ArcFlow divides the trajectory into several mode-specific sub-trajectories, enabling each to be learned in more targeted manner, thereby improving overall learning efficiency. To further prove the rationality of our parameterization, we introduce theorem showing that Eq. (2) with ğ¾ dynamic modes theoretically admits parameter setting that perfectly fits the sampled trajectory at ğ‘ ğ¾ distinct timesteps. Theorem 1. Consider the velocity field predicted by ArcFlow at any sampled latent and timestep ğ‘¡, parameterized as vğœƒ(y, ğ‘¡) = (cid:205)ğ¾ ğ‘˜=1 ğœ‹ğ‘˜(y) vğ‘˜(y) ğ›¾ğ‘˜(y)1ğ‘¡ according to Eq. (2). Let u(y, ğ‘¡) denote the ground-truth velocity field, observed at ğ‘ distinct timesteps ğ’¯ = {ğ‘¡1, . . . , ğ‘¡ğ‘ } (0, 1]. If the number of modes satisfies ğ¾ ğ‘, then there exists parameter configuration ğœƒ = {ğœ‹ğ‘˜ , vğ‘˜ , ğ›¾ğ‘˜}ğ¾ ğ‘˜=1: vğœƒ(y, ğ‘¡ğ‘›) = (y, ğ‘¡ğ‘›), ğ‘¡ğ‘› ğ’¯ , (3) We prove the theorem in section E.2. The theoretical result validates that the momentum-based parameterization is capable of approximating the ground-truth velocity field in non-linear way, ensuring high-precision distillation."
        },
        {
            "title": "3.2 Analytic ODE Solvers",
            "content": "As described above, ArcFlow parameterizes the velocity field as mixture of momentum modes (Eq. (2)), which is mathematically equivalent to linear combination of exponential time factors. This structure admits closed-form integration over arbitrary timestep intervals, allowing accurate latent updates with very few steps. Concretely, for sampling step from timestep ğ‘¡ğ‘  to ğ‘¡ğ‘’ (ğ‘¡ğ‘  > ğ‘¡ğ‘’ ), we define the Analytic Transition Operator Î¦ as the latent displacement Î”xğ‘¡ğ‘  ğ‘¡ğ‘’ induced by the velocity field vğœƒ(xğ‘¡ , ğ‘¡). By analytically integrating this velocity based on Eq. (2) across timesteps, Î¦(xğ‘¡ğ‘  , ğ‘¡ğ‘  , ğ‘¡ğ‘’ ; ğœƒ) admits the following closed-form expression: Î¦(xğ‘¡ğ‘  , ğ‘¡ğ‘  , ğ‘¡ğ‘’ ; ğœƒ) Î”xğ‘¡ğ‘  ğ‘¡ğ‘’ = ğ¾ (cid:213) ğ‘˜= ğœ‹ğ‘˜ (xğ‘¡ğ‘  ) vğ‘˜ (xğ‘¡ğ‘  ) ğ’ (cid:0)ğ›¾ğ‘˜ (xğ‘¡ğ‘  ), ğ‘¡ğ‘  , ğ‘¡ğ‘’ (cid:1) . where the Momentum Integral Coefficient ğ’ () is defined as ğ’ (ğ›¾, ğ‘¡ğ‘  , ğ‘¡ğ‘’ ) = ğ›¾1ğ‘¡ğ‘’ ğ›¾1ğ‘¡ğ‘  ln ğ›¾ , ğ›¾ 1, ğ‘¡ğ‘  ğ‘¡ğ‘’ , ğ›¾ = 1, (4) (5) The full derivation of Eq. (4) and Eq. (5) is provided in section E.1. Crucially, the coefficient ğ’ (ğ›¾, ğ‘¡ğ‘  , ğ‘¡ğ‘’ ) smoothly reduces to the linear form ğ‘¡ğ‘  ğ‘¡ğ‘’ as ğ›¾ 1 (see section E.1). This ensures numerical stability of our solver at the singularity, showing that our parameterization seamlessly bridges non-linear dynamics (ğ›¾ 1) and the linear flow regime (ğ›¾ = 1). 5 Consequently, for any arbitrary step from ğ‘¡ğ‘  to ğ‘¡ğ‘’ , the next latent is given explicitly by xğ‘¡ğ‘’ = xğ‘¡ğ‘  Î¦(xğ‘¡ğ‘  , ğ‘¡ğ‘  , ğ‘¡ğ‘’ ; ğœƒ), allowing direct integration to target states. As shown in figure 3(b), while previous few-step students rely on very few straight sub-lines to fit multi-step teacher trajectory whose tangent directions rapidly changing, leading to poor approximation, ArcFlow trajectory from our analytic solver can naturally inherit the non-linearity and better align with the teachers overall trajectory."
        },
        {
            "title": "3.3 Flow Distillation with Analytic Solvers",
            "content": "Since ArcFlow naturally aligns with the teacher trajectory, we propose practical flow distillation strategy based on pre-trained teacher model. As directly synthesizing the trajectory is infeasible within the flow matching framework which only predicts the velocity field, we reconstruct the teachers trajectory by aligning its tangent direction (instantaneous velocity) at every timestep. For ArcFlow, this tangent is analytically derived via Eq. (2), so the trajectory alignment reduces to velocity-matching objective: minimizing the discrepancy between student and teacher instantaneous velocities at the sampled (xğ‘¡ , ğ‘¡) pairs. As shown in algorithm 1, we propose flow distillation method. For each timestep interval [ğ‘¡dst, ğ‘¡src], we train ArcFlow within this interval by iterating two steps as follows. Mixed Latent Integration. To enable the student to learn the teachers velocity field over the whole interval [ğ‘¡dst, ğ‘¡src], we sample ğ‘› intermediate timesteps {ğ‘¡1, . . . , ğ‘¡ğ‘›} and construct the corresponding latents {xğ‘¡ğ‘– }. Let ğ‘¡src = ğ‘¡0 and ğ‘¡dst = ğ‘¡ğ‘›+1, each target latent xğ‘¡ğ‘–+1 is then sequentially obtained by integrating over each sub-interval [ğ‘¡ğ‘– , ğ‘¡ğ‘–+1]. Within each [ğ‘¡ğ‘– , ğ‘¡ğ‘–+1], we apply mixed integration curriculum as training progresses: early training mainly follows teacher guidance to keep latents on the teacher manifold, while the student progressively takes over to gain the self-correction ability on its own generated latents. Concretely, for each sub-interval [ğ‘¡ğ‘– , ğ‘¡ğ‘–+1], we introduce switching timestep ğ‘¡mix = ğ‘¡ğ‘– (1 ğœ†)(ğ‘¡ğ‘– ğ‘¡ğ‘–+1), where ğœ† is gradually increased from 0 to 1 during training. Starting from xğ‘¡ğ‘– , the teacher integrates the latent from ğ‘¡ğ‘– to ğ‘¡mix, after which the student completes the integration to ğ‘¡ğ‘–+1. This sequential handoff yields: xğ‘¡ğ‘–+1 = xğ‘¡ğ‘– + ğ‘¡ğ‘– ğ‘¡mix u(xğ‘¡ğ‘– , ğ‘¡ğ‘–) ğ‘‘ğ‘¡ + ğ‘¡mix ğ‘¡ğ‘–+1 v(xğ‘¡ , ğ‘¡; Î˜) ğ‘‘ğ‘¡. (6) Here, u(xğ‘¡ğ‘– , ğ‘¡ğ‘–) is the instantaneous velocity predicted by the teacher at (xğ‘¡ğ‘– , ğ‘¡ğ‘–); v(xğ‘¡ , ğ‘¡; Î˜) represents the velocity derived from momentum parameters Î˜ predicted by ArcFlow at ğ‘¡0. Implementation details of the mixed integration are provided in the section C.1. With every target latent state xğ‘¡ğ‘– obtained, we detach it from the computation graph and use it as the anchor for the velocity alignment step detailed below. Instantaneous Velocity Matching. At each xğ‘¡ğ‘– , we proceed to align the velocity field predicted by the student with that of the teacher. We compute the instantaneous velocity v(xğ‘¡ğ‘– , ğ‘¡ğ‘–; Î˜) predicted by the student parameter Î˜ via Eq. (2), and obtain the target velocity u(xğ‘¡ğ‘– , ğ‘¡ğ‘–) by evaluating the teacher network. The optimization objective: â„’distill = ğ”¼ğ‘¡ğ‘– ,xğ‘¡ğ‘– 2(cid:105) (cid:104)(cid:13) (cid:13)v(xğ‘¡ğ‘– , ğ‘¡ğ‘–; Î˜) u(xğ‘¡ğ‘– , ğ‘¡ğ‘–)(cid:13) (cid:13) , (7) Enforcing this loss ensures that the students overall continuous trajectory adheres to the teachers complex trajectory. ArcFlow further simplifies this distillation process due to our momentum parameterization. As the momentum parameterization naturally inherits the non-linearity, matching the instantaneous velocity with very few timesteps (ğ‘› = 2 4) is sufficient for ArcFlow to learn the velocity field of the teacher, leading to high-precision restoration of the teacher trajectory and fast training process. Moreover, reduced distillation difficulty results in requiring fewer trainable parameters. While linear methods force the student to override the teachers priors to fit linear rectification, which requires invasive full-parameter finetuning of large pre-trained models, ArcFlow naturally adapts to the non-linear trajectory. Empirically, we find that training only LoRA adapters on few layers and the output projection head is sufficient for convergence, which proves our assumption that ArcFlow enables efficient alignment with the teacher trajectory. 6 Table 1 Quantitative comparisons on Geneval, DPG-Bench and OneIG-Bench. means the results are cited from pi-Flow [4] and TwinFlow [7]. The NFE of Qwen-Image-20B is recorded as 50 2 since it uses CFG [14]. Model NFE Geneval DPG-Bench FLUX.1-dev [17] SenseFlow (FLUX) [10] Pi-Flow (GM-FLUX) [4] ArcFlow-FLUX (Ours) 50 2 2 2 Qwen-Image-20B [37] 50 Qwen-Image-Lightning [24] pi-Flow (GM-Qwen) [4] TwinFlow (Qwen) [7] ArcFlow-Qwen (Ours) 2 2 2 2 0.66 0.60 0.58 0.65 0.87 0.85 0.83 0. 0.85 84.16 79.86 82.36 84.29 88.32 88.42 86.45 87.01 88. OneIG-Bench Alignment Text Diversity Style Reasoning 0.790 0.743 0. 0.798 0.880 0.875 0.837 0.862 0.877 0.556 0.230 0.141 0.368 0.888 0.879 0.634 0.825 0.853 0.238 0.139 0.216 0.210 0.194 0.098 0.176 0. 0.182 0.307 0.341 0.332 0.350 0.427 0.415 0.382 0.364 0. 0.257 0.212 0.212 0.224 0.306 0.292 0.259 0.267 0.289 Table 2 Quantitative comparisons on Align5000. FIDs and pFIDs are calculated against 50-step teacher generations. NFE FID pFID CLIP Model FLUX.1-dev SenseFlow (FLUX) Pi-Flow (GM-FLUX) ArcFlow-FLUX (Ours) 2 2 2 - - 27.55 32.62 16.83 9.25 37.84 11. Qwen-Image-20B 50 2 - - Qwen-Image-Lightning pi-Flow (GM-Qwen) TwinFlow (Qwen) ArcFlow-Qwen (Ours) 2 2 2 16.86 20.07 16.77 12.40 11.32 12.42 4.34 3.78 0.312 0.311 0. 0.315 0.325 0.320 0.323 0.320 0."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "We apply our distillation framework to two text-to-image models: Qwen-Image-20B [37] and FLUX.1-dev [17]. As parameter-efficient strategy, we freeze the vast majority of the backbone and train only 256-rank LoRA adapters injected into the feed-forward layers along with the final output projection head to accommodate the momentum parameter predictions. We train ArcFlow on large-scale prompt dataset (2.3 million samples) introduced by pi-Flow [4]. We provide more training details in section D. We conduct evaluation on 1024 1024 image generation from three distinct benchmarks: (1) Geneval [12] (complex object combination), (2) DPG-Bench [16] (dense and long prompts), (3) OneIG-Bench [3] (complex prompts from distinct aspects). We additionally collect another dataset with 5,000 prompts, referred to the Align5000, composed of 3,200 prompts from HPSv2 prompt set [38] and 1,800 prompts randomly sampled from the COCO 2014 validation set [19]. This combination covers both diverse artistic styles (HPSv2) and natural image distributions (COCO), enabling more comprehensive evaluation of teacher alignment and distributional fidelity. Regarding metrics, the FIDs and patch FIDs (pFIDs) are computed against the 50-step teacher model to evaluate students alignment with the teacher, and the CLIP similarity score measures the prompt alignment ability. In the patch FID metric, patch size is set to 64, and stride is set to 128."
        },
        {
            "title": "4.2 Comparison Study\nQuantitative Results. We compare with recent few-step generative models distilled from the same teacher.\nFor FLUX.1-dev [17], we compare against: SenseFlow [10], which uses DMD; pi-Flow (GM-FLUX) [4],\nwhich approximates the linear step with policy. For Qwen-Image-20B [37], we compare with: Qwen-Image-",
            "content": "7 Figure 4 Qualitative comparisons with methods distilled on Qwen-Image-20B (2NFE). Every column contains two images which are generated from the same batch of initial noise. ArcFlow generates diverse samples that better align with teacher than competitors. Lightning [24] based on VSD; TwinFlow [7] based on self-adversarial loss; pi-Flow (GM-Qwen). All models are set to NFE=2. We observe that ArcFlow consistently outperforms or remains competitive with state-of-the-art few-step models across the three distinct benchmarks, demonstrating robust alignment with complex instructions. Specifically, while adversarial-based methods (e.g., Qwen-Image-Lightning) suffer from mode collapse (losing diversity to improve semantic alignment), ArcFlow achieves substantial +85.7% improvement in Diversity on OneIG-Bench, proving that our parameterization effectively preserves the teachers pre-trained priors and the generative diveristy. Furthermore, as shown in Table 2, ArcFlow achieves the lowest FID and pFID across both backbones, indicating that our method ensures significantly more precise alignment with the teachers generation compared to other linear shortcut baselines. Notably, although Qwen-Image-Lightning achieves competitive prompt-following scores, its inferior FID highlights trade-off where perceptual optimization compromises trajectory fidelity, whereas ArcFlow maintains high fidelity to the original distribution (see Figure 5 Qualitative comparisons with Qwen-Image-Lightning. Our ArcFlow exhibits visibly clearer details. Table 3 Momentum factor ğ›¾. Table 4 (ğ‘ğ‘£ , ğ‘ğ›¾). ğ¾ set to 16. Table 5 Numbers of momentum modes ğ¾. ğ›¾ settings FID ğ›¾ 1 ğ›¾ fixed ğ›¾ learnable 17.06 14.77 14.56 (ğ‘ğ‘£ , ğ‘ğ›¾) FID (ğ¾, 1) (1, ğ¾) (ğ¾, ğ¾) 15.08 14.97 14.56 ğ¾ 8 16 32 FID pFID 12.54 12.4 12.39 4.17 3.78 3.69 section for detailed discussion). Qualitative Results. We conduct qualitative comparison between ArcFlow and prior state-of-the-art methods by generating images from the same batch of initialized noise and comparing them with the teacher outputs, as shown in figure 4. Linear distillation methods, including TwinFlow and Qwen-Image-Lightning, exhibit clear mode collapse and quality degradation, often producing nearly identical samples. Moreover, TwinFlow suffers from degraded visual aesthetics (the 3rd column), while Qwen-Image-Lightning shows blurred textures (background in the 3rd column) and structural artifacts (bent or duplicated swords in the 2nd column). These failures reveal fundamental limitation of linear-step distillation methods in comprehensively approximating the teacher trajectory. In contrast, at the same batch of initialized noise, ArcFlow consistently preserves both high visual quality and generation diversity, producing results that are more closely aligned with the teacher. This proves its superiority in both high-quality generation and high-precision approximation of the teacher, ensuring generation diversity. Although Qwen-Image-Lightning achieves competitive quantitative performance in the benchmarks, further zoomed-in comparisons in figure 5 show that ArcFlow yields noticeably finer and more coherent details. We attribute this discrepancy to the training objective of Qwen-Image-Lightning, which may sacrifice fine-grained visual fidelity for better semantic alignment objective, underscoring the inherent challenge of linear-step trajectory approximation. More discussion is provided in section B. Convergence Speed and Stability. To validate the convergence speed and training stability of ArcFlow, we respectively distill ArcFlow, pi-Flow and TwinFlow based on Qwen-Image-20B. For pi-Flow and TwinFlow, we conduct training as guided in their official codebase. We utilize the same training dataset as depicted in section 4.1, and train models with batch size of 16. We use the FID of Align5000 as the evaluation metric to measure the alignment between students and the teacher, and we evaluate the model at an iteration interval of 500 training steps. As shown in figure 2, ArcFlow converges significantly faster and with more stable FID reduction compared to other models. This validates that ArcFlow can efficiently leverage the pre-trained teacher weights, requiring only minor adaptation to reach near-optimal alignment. In contrast, TwinFlow, which uses full-parameter training, must override the teachers pre-trained weights due to the geometric mismatch, leading to high-error initial parameter state and slow convergence. Notably, ArcFlow surpasses the FID of Qwen-Image-Lightning after only 1,000 training steps, demonstrating its efficiency in distillation training. We further visualize this convergence process comparison in figure 8 and provide detailed analysis in section F.2, which demonstrates the ArcFlows superiority in inheriting and adapting to the pre-trained teacher knowledge, leading to efficient high-precision distillation."
        },
        {
            "title": "4.3 Ablation Study\nImpact of Momentum Dynamics ğ›¾. We investigate the necessity of the momentum factor ğ›¾ in approximating\ntrajectory tangent variation. All experiments are conducted on the Align5000 prompt set with 1,500 training\nsteps. As shown in table 3, setting ğ›¾ â‰¡ 1 removes the explicit momentum factor from the parameterization.\nIn this setting, the model must rely solely on the velocity mixture to approximate the trajectory evolution,\nwhich forces the predicted velocity to implicitly compensate for the overall dynamics information, resulting in\nsuboptimal alignment and inferior FID scores. Then, introducing fixed momentum factors brings non-linearity\ninto the trajectory and yields consistent improvements, demonstrating the benefit of explicit employment\nof non-linear trajectories. Furthermore, making ğ›¾ learnable leads to the best performance, suggesting",
            "content": "9 Figure 6 Ablation qualitative results on core settings. (a) Comparison of different momentum settings ğ›¾. (b) Comparison of different mixture configurations (ğ‘ğ‘£ , ğ‘ğ›¾). that adaptive momentum factors better capture the varying trajectory behaviors across different samples and timesteps. figure 6(a) highlights the importance of adaptively employing momentum for precise teacherstudent alignment. Decoupling Velocity and Momentum Mixtures. Given the necessity of adaptive non-linearity, we further examine how each mixture component should be parameterized. We denote the configuration as (ğ‘ğ‘£ , ğ‘ğ›¾), representing the number of independent basic velocities and momentum factors used across the ğ¾ mixture modes. We compare our default (ğ‘ğ‘£ , ğ‘ğ›¾) = (ğ¾, ğ¾) against two restricted variants: (ğ¾, 1), which forces diverse velocity directions to evolve under unified motion pattern, and (1, ğ¾), which restricts diverse momentum dynamics to start from the same basic velocities. We train 1,500 steps for each experiment. table 4 and figure 6(b) show that neither restricted setting is competitive with the performance of (ğ¾, ğ¾). This confirms that decoupling velocity and momentum clarifies the optimization task, while constraining either factor forces the remaining parameters to implicitly compensate for the missing dynamics, creating an overloaded and ambiguous learning target. Scalability of Mixture Components ğ¾. We study the effect of the mixture size ğ¾ by evaluating ArcFlow with ğ¾ {8, 16, 32}, while keeping all other settings fixed. As shown in table 5, increasing ğ¾ generally improves performance, indicating that richer mixture enhances the models ability to capture non-linear trajectory tangent variation. While ğ¾ = 32 achieves slightly better FID and pFID than ğ¾ = 16, the improvement is marginal. Considering the diminishing returns, the increased parameter, and computational cost, we adopt ğ¾ = 16 as the default configuration, which offers favorable trade-off between expressiveness and efficiency in the few-step distillation regime."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we proposed ArcFlow, few-step distillation framework that explicitly employs non-linear trajectories to approximate the complex dynamics of pre-trained diffusion teachers. By parameterizing the velocity field as mixture of continuous momentum processes, ArcFlow admits closed-form analytic solver and enables accurate trajectory integration. We further introduced flow distillation strategy to align the 10 students analytical trajectory with the teacher. Benefiting from its intrinsic non-linearity, ArcFlow ensures high-precision alignment with the teacher. Moreover, it avoids unstable adversarial objectives and invasive full-parameter training, leading to faster convergence and more efficient distillation. Extensive experiments demonstrated that ArcFlow consistently achieves superior generation quality with fewer trainable parameters compared to linear baselines. We believe ArcFlow highlights the importance of respecting the underlying flow dynamics for efficient generative inference."
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to advance the efficiency of image generation models by enabling high-quality few-step inference through improved distillation techniques. Such progress can facilitate broader accessibility and deployment of generative models in practical applications, including creative tools, simulation, and content generation. At the same time, as with prior work on image generation, our method could potentially be misused for generating misleading or harmful visual content. These concerns are not unique to our approach and are inherent to the broader class of generative image models. We emphasize that responsible deployment, including appropriate content moderation, usage policies, and the development of reliable AI-generated content detection mechanisms, remains important. We believe that the technical contributions of this work primarily improve inference efficiency and fidelity, without introducing new ethical risks beyond those already present in existing diffusion-based image generation systems."
        },
        {
            "title": "References",
            "content": "[1] Michael S. Albergo and Eric Vanden-EÄ³nden. Building normalizing flows with stochastic interpolants, 2023. URL https://arxiv.org/abs/2209.15571. [2] John Butcher. Numerical methods for ordinary differential equations. John Wiley & Sons, 2016. [3] Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation, 2025. URL https: //arxiv.org/abs/2506.07977. [4] Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, and Sai Bi. pi-flow: Policy-based few-step generation via imitation distillation, 2025. URL https://arxiv.org/abs/2510.14974. [5] Hansheng Chen, Kai Zhang, Hao Tan, Zexiang Xu, Fujun Luan, Leonidas Guibas, Gordon Wetzstein, and Sai Bi. Gaussian mixture flow matching models. In ICML, 2025. [6] E. W. Cheney. Introduction to Approximation Theory. McGraw-Hill, 1966. [7] Zhenglin Cheng, Peng Sun, Jianguo Li, and Tao Lin. Twinflow: Realizing one-step generation on large models with self-adversarial flows, 2025. URL https://arxiv.org/abs/2512.05150. [8] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prioritized training of diffusion models, 2022. URL https://arxiv.org/abs/2204.00227. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv.org/abs/2403.03206. [10] Xingtong Ge, Xin Zhang, Tongda Xu, Yi Zhang, Xinjie Zhang, Yan Wang, and Jun Zhang. Senseflow: Scaling distribution matching for flow-based text-to-image distillation, 2025. URL https://arxiv.org/abs/2506.00523. [11] Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling, 2025. URL https://arxiv.org/abs/2505.13447. [12] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/2310.11513. [13] Herbert Goldstein, Charles Poole, and John Safko. Classical Mechanics. Addison-Wesley, 3rd edition, 2002. [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv.org/abs/2207. 12598. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv. org/abs/2006.11239. [16] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. [17] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [18] Black Forest Labs. FLUX.2: Frontier Visual Intelligence. https://bfl.ai/blog/flux-2, 2025. [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr DollÃ¡r. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. [20] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. [21] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. URL https://arxiv.org/abs/2209.03003. [22] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023. URL https://arxiv.org/abs/2310.04378. [23] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models, 2023. URL https://arxiv.org/abs/2210.03142. [24] ModelTC. Qwen-image-lightning. https://github.com/ModelTC/Qwen-Image-Lightning. GitHub repository, accessed 2026-01-23. [25] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022. URL https://arxiv.org/abs/2202.00512. [26] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023. URL https://arxiv.org/abs/2311.17042. [27] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. URL https://arxiv.org/abs/2011.13456. [28] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. URL https://arxiv. org/abs/2303.01469. [29] Shuyuan Tu, Qi Dai, Zuxuan Wu, Zhi-Qi Cheng, Han Hu, and Yu-Gang Jiang. Implicit temporal modeling with learnable alignment for video recognition. In Proceedings of the ieee/cvf international conference on computer vision, pages 1993619947, 2023. [30] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78827891, 2024. [31] Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motionfollower: Editing video motion via lightweight score-guided diffusion. arXiv preprint arXiv:2405.20325, 2024. [32] Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableavatar: Infinite-length audio-driven avatar video generation. arXiv preprint arXiv:2508.08248, 2025. 12 [33] Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, and Zuxuan Wu. Flashportrait: 6x faster infinite portrait animation with adaptive latent prediction. arXiv preprint arXiv:2512.16900, 2025. [34] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2109621106, 2025. [35] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableanimator++: Overcoming pose misalignment and face distortion for human image animation. arXiv preprint arXiv:2507.15064, 2025. [36] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation, 2023. URL https://arxiv.org/abs/2305. 16213. [37] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/2508.02324. [38] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [39] Tianwei Yin, MichaÃ«l Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation, 2024. URL https://arxiv.org/abs/2311.18828. 13 NFE , . . . , 1} Algorithm 1 Flow Distillation for ArcFlow Require: NFE, Teacher ğºğœ“, Student ğºğœ™, Ratio ğœ† 1: Sample start timestep ğ‘¡src { 1 2: Initialize xğ‘¡src 3: Sample timesteps {ğ‘¡1, . . . , ğ‘¡ğ¾} [ğ‘¡src 1 4: Î˜ ğºğœ™(xsrc, ğ‘¡src); 5: for ğ‘¡ {ğ‘¡1, . . . , ğ‘¡ğ¾} do 6: xğ‘¡ MixedIntegration(xsrc, ğ‘¡src, ğ‘¡; Î˜, ğœ†) Ë†xğ‘¡ stopgrad(xğ‘¡) 7: vstu v( Ë†xğ‘¡ , ğ‘¡; Î˜) 8: ğºğœ“( Ë†xğ‘¡ , ğ‘¡) 9: 10: â„’ â„’ + vstu u2 11: end for 12: Update ğœ™ using ğœ™â„’ NFE , ğ‘¡src]"
        },
        {
            "title": "A Preliminaries",
            "content": "In this section, we first introduce the Flow Matching framework and the Probability Flow ODE (PF-ODE). We then discuss the numerical simulation of this ODE, highlighting the difference between multi-step integration solvers and few-step solvers via distillation. Flow Matching and Probability Flow ODE. Let ğ‘(x0) denote the data distribution and ğ‘(x1) ğ’© (0, I) the noise distribution. Flow Matching [20] defines probability trajectory that iteratively transforms ğ‘(x1) to ğ‘(x0) over timesteps ğ‘¡ [0, 1]. This transformation is driven by the probability flow ODE, which is defined as: ğ‘‘xğ‘¡ ğ‘‘ğ‘¡ = (xğ‘¡ , ğ‘¡), (8) Integrating this velocity field u() over timestep ğ‘¡ [0, 1] forms the flow trajectories from noise x1 to data x0, which we term as trajectory integration. To construct this velocity field, we use the Conditional Flow Matching (CFM) [20] formulation. We define linear trajectory between data sample x0 and noise x1 to derive the latent xğ‘¡ at any timestep ğ‘¡: u(xğ‘¡x0, x1) = ğ‘‘ ğ‘‘ğ‘¡ xğ‘¡ = x1 x0, (9) Therefore, the practical objective is to train flow-matching model to approximate the marginal velocity field uğ‘¡() via predicted velocity field vğœƒ(x, ğ‘¡) parameterized by ğœƒ, through the minimization of the expectation over these conditional flow trajectories: â„’FM(ğœƒ) = ğ”¼ğ‘¡,ğ‘(x0),ğ‘(x1) (cid:2)vğœƒ(xğ‘¡ , ğ‘¡) u(xğ‘¡x0, x1)2(cid:3) , (10) = vğœƒ(xğ‘¡ , ğ‘¡), enabling deterministic data generation. Once optimized, numerical solvers are employed to integrate the learned PF-ODE ğ‘£ğœƒ, formulated as ğ‘‘xğ‘¡ ğ‘‘ğ‘¡ ODE Sampling and Distillation. Inference requires integrating the PF-ODE in timesteps from ğ‘¡ = 1 to ğ‘¡ = 0, yielding the sample Ë†x0 = x1 + 0 ğ‘£ğœƒ(xğœ, ğœ)ğ‘‘ğœ. Since this integral is analytically intractable, numerical solvers (e.g., Euler or Heun [2]) are employed to approximate the integration by accumulating discrete velocities in multiple timesteps. To minimize the discretization error, this approximation typically involves 40 100 14 function evaluations (NFEs). With step size Î”ğ‘¡, an Euler solver iteratively updates ğ‘¥ğ‘¡ with the formula as follows: xğ‘¡Î”ğ‘¡ = xğ‘¡ ğ‘£ğœƒ(xğ‘¡ , ğ‘¡) Î”ğ‘¡, However, this iterative process imposes significant computational bottleneck. To mitigate this, knowledge distillation is widely adopted, where the student ğ‘£ğœ™ is trained to emulate the behavior of the teacher ğ‘£ğœƒ across multiple timesteps, allowing it to traverse the trajectory with fewer NFEs. The standard distillation objective is typically formulated as regression problem, where the student minimizes the discrepancy between its predicted velocity field and target signal yğ‘¡ derived from the teacher: (11) â„’KD(ğœ™) = ğ”¼ğ‘¡,xğ‘¡ (cid:2)ğ‘£ğœ™(xğ‘¡ , ğ‘¡) yğ‘¡(xğ‘¡ , ğ‘£ğœƒ)2(cid:3) , (12) where yğ‘¡ refers to the supervision target from the teacher, which varies depending on the specific method. Discussions on Qwen-Image-Lightning Although Qwen-Image-Lightning achieves competitive performance on prompt-alignment benchmarks, In its inferior FID and pFID indicate clear degradation in generation quality at the distribution level. particular, lower FID typically reflects distorted local statistics and weakened high-frequency details, such as blurred textures and over-smoothed structures, which are not captured by prompt-alignment metrics that primarily emphasize semantic correctness and global visual saliency. As result, this indicates that although Qwen-Image-Lightning can generate images that remain semantically consistent with the prompt, it deviate noticeably from the teachers generation distribution in fine-grained structure and detail. Moreover, this discrepancy reveals fundamental difference in distillation efficiency and objective. QwenImage-Lightning focuses on achieving perceptual and semantic alignment under limited steps, but does not explicitly enforce high-precision alignment with the teachers underlying generation trajectory, leading to partial loss of the teachers original generative prior. As shown in figure 10, Qwen-Image-Lightning exihibits unstable performance in our test cases (especially in the first column), proving our assumption that it sacrifices the overall distribution correctness for higher semantic alignment. In contrast, ArcFlow directly distills the teachers non-linear velocity field and preserves its trajectory non-linearity through momentum-based parameterization. This design allows ArcFlow to rapidly converge to high-precision teacher alignment with minimal training cost, thereby maintaining both strong prompt-following ability and high distributional fidelity in the few-step regime."
        },
        {
            "title": "C Additional Technical Details",
            "content": "C.1 Mixed trajectory Integration This section provides the implementation details of the mixed latent integration strategy described in the main paper. Teacher Integration. For the teacher phase within each sub-interval [ğ‘¡ğ‘– , ğ‘¡mix], we use the instantaneous velocity prediction u(xğ‘¡ğ‘– , ğ‘¡ğ‘–) as constant velocity. Since each sub-interval is sufficiently small, this approximation significantly reduces computational cost without affecting training stability. Student Integration via Analytic Transition. The student velocity field v(xğ‘¡ , ğ‘¡; Î˜) is induced by the momentum parameters Î˜ predicted by ArcFlow at ğ‘¡0. As ArcFlow defines continuous velocity field, we apply the analytic transition operator Î¦ derived in Eq. (4) to compute the integration from ğ‘¡mix to ğ‘¡ğ‘–+1: ğ‘¡mix ğ‘¡ğ‘–+ v(xğ‘¡ , ğ‘¡; Î˜) ğ‘‘ğ‘¡ = ğ‘¡0 ğ‘¡ğ‘–+1 v(xğ‘¡ , ğ‘¡; Î˜) ğ‘‘ğ‘¡ ğ‘¡0 ğ‘¡mix v(xğ‘¡ , ğ‘¡; Î˜) ğ‘‘ğ‘¡ = Î¦(xğ‘¡0 , ğ‘¡0, ğ‘¡ğ‘–+1; Î˜) Î¦(xğ‘¡0 , ğ‘¡0, ğ‘¡mix; Î˜). (13) 15 This formulation allows efficient and exact integration of the student dynamics over arbitrary time intervals without numerical solvers. C.2 Momentum Factor Related Setting. Log-parameterization of the Momentum Factor ğ›¾. According to the notion of momentum, the momentum factor ğ›¾ is required to be strictly positive. However, directly regressing ğ›¾ introduces an implicit positivity constraint, which may lead to optimization difficulties and numerical instability during training, especially when the predicted values approach zero. To address this, we parameterize the momentum factor in the logarithmic space. Specifically, in training, the momentum factor projection head is designed to predict log ğ›¾ instead of ğ›¾, and the actual momentum factor is recovered by exponentiation. This reparameterization naturally enforces the positivity constraint while providing smoother gradients and more stable optimization behavior in practice. Projection Layer Initialization. To ensure that the model captures diverse range of trajectory dynamics across timesteps, we initialize the momentum factors {ğ›¾ğ‘˜}ğ¾ as geometric progression spanning the interval [0.4, 5.0]. This range allows the mixture to cover both decelerating regimes (ğ›¾ < 1) and accelerating regimes (ğ›¾ > 1), enabling flexible modeling of complex flow trajectories. In implementation, we first construct the geometric sequence in the ğ›¾ space and then convert it to the logarithmic domain. Since the momentum factor projection layer is linear layer, we initialize its weight matrix to zeros and assign the corresponding log ğ›¾ğ‘˜ values to the bias vector. As result, at the beginning of training, the predicted momentum factors exactly match the predefined geometric progression, providing stable and interpretable initialization. ğ‘˜=1 Crucially, we explicitly constrain one specific mode to be fixed at ğ›¾ = 1. This design introduces linear inductive bias, serving as stable anchor that allows the model to naturally fall back to standard linear flow matching when the velocity evolution is negligible. Learning Rate for the Momentum Factor Projection Layer. Since the momentum factor projection layer predicts log ğ›¾ rather than ğ›¾, as shown in Eq. (2), updates to this layer lead to exponential changes in the effective velocity field. Consequently, using the same learning rate as other network components may cause unstable updates. To improve numerical stability during training, we apply reduced learning rate to this projection layer, specifically setting it to 0.1 that of all other trainable layers. This targeted adjustment effectively stabilizes optimization while preserving sufficient learning capacity for adapting the momentum factors. Training Details and Hyperparameters setting. We freeze the teacher backbone and only train LoRA adapters together with extended output heads for predicting velocities, momentum factors, and gating probabilities. For Qwen-Image-20B, we insert rank-256 LoRA adapters into small subset of modules, including the image MLP, timestep embedding layers, and the text MLP blocks of the transformer. Specifically, LoRA is applied to the projection layers of the image MLP, both linear layers of the timestep embedder, and the text MLPs across all transformer blocks, while all remaining parameters are kept frozen. For FLUX.1-dev, we apply rank-256 LoRA adapters to the projection and feed-forward modules that dominate the models conditional and feature transformation capacity. Specifically, LoRA is inserted into the MLP projection layers, the output projection head, the feed-forward networks in both the main and context branches, as well as the timestep embedding layers. All other parameters of the teacher backbone are kept frozen. We conduct our experiment on 96 H100 GPUs. All models are trained with BF16 mixed precision. We detail our other training configurations in table 6. 16 Table 6 Detailed training configurations for distillation on Qwen-Image-20B and FLUX.1-dev. Configuration ArcFlow-Qwen ArcFlow-FLUX Method-specific Settings Num of momentum modes ğ¾ ğ›¾ initialization range Num of intermediate timesteps Trained NFEs Mixed Trajectory Guidance Steps Training Details Batch Size Total Training Steps Optimizer Settings Optimizer Learning rate Learning rate for ğ›¾ Weight Decay (ğ›½1, ğ›½2) 16 [0.5, 4.0] 4 2 384 7500 AdamW 1ğ‘’4 1ğ‘’5 0 (0.9, 0.95) 16 [0.5, 4.0] 4 2 2000 384 8000 AdamW 1ğ‘’4 1ğ‘’5 0 (0.9, 0.95)"
        },
        {
            "title": "E Theoretical Analysis",
            "content": "E.1 Implementation and Derivation of the Analytic Transition Operator Î¦ This supplement provides the step-by-step derivation of Eq. (4) and the limiting behavior ğ›¾ 1. Expansion of the velocity mixture. Assume the ArcFlow velocity at base state xğ‘¡ğ‘  is expressed as vğœƒ(xğ‘¡ğ‘  , ğ‘¡) = ğ¾ (cid:213) ğ‘˜=1 ğœ‹ğ‘˜(xğ‘¡ğ‘  ) vğ‘˜(xğ‘¡ğ‘  ) ğ›¾ğ‘˜(xğ‘¡ğ‘  ) 1ğ‘¡ , where all mode-dependent quantities ğœ‹ğ‘˜ , vğ‘˜ , ğ›¾ğ‘˜ are evaluated at xğ‘¡ğ‘  and considered constant w.r.t. integration variable ğ‘¡. the"
        },
        {
            "title": "Then",
            "content": "vğœƒ(xğ‘¡ğ‘  , ğ‘¡) ğ‘‘ğ‘¡ Î¦(xğ‘¡ğ‘  , ğ‘¡ğ‘  , ğ‘¡ğ‘’ ; ğœƒ) = = ğ‘¡ğ‘  ğ‘¡ğ‘’ ğ¾ (cid:213) ğ‘˜= ğœ‹ğ‘˜(xğ‘¡ğ‘  ) vğ‘˜(xğ‘¡ğ‘  ) ğ‘¡ğ‘  ğ‘¡ğ‘’ ğ›¾ğ‘˜(xğ‘¡ğ‘  ) 1ğ‘¡ ğ‘‘ğ‘¡,"
        },
        {
            "title": "Define the scalar integral",
            "content": "For ğ›¾ 1 we compute ğ¼(ğ›¾; ğ‘¡ğ‘  , ğ‘¡ğ‘’ ) ğ‘¡ğ‘  ğ‘¡ğ‘’ ğ›¾ 1ğ‘¡ ğ‘‘ğ‘¡. ğ¼(ğ›¾; ğ‘¡ğ‘  , ğ‘¡ğ‘’ ) = ğ‘¡ğ‘  ğ‘’(1ğ‘¡) ln ğ›¾ ğ‘‘ğ‘¡ = ğ‘¡ğ‘  ğ‘¡ğ‘’ ğ‘’ln ğ›¾ ğ‘’ğ‘¡ ln ğ›¾ ğ‘‘ğ‘¡ ğ‘’ğ‘¡ ln ğ›¾ ğ‘‘ğ‘¡ = ğ›¾ ğ‘’ğ‘¡ ln ğ›¾ ln ğ›¾ (cid:12) (cid:12) (cid:12) ğ‘¡ğ‘  ğ‘¡ğ‘’ ğ‘¡ğ‘’ ğ‘¡ğ‘  = ğ›¾ ğ‘¡ğ‘’ ğ›¾1ğ‘¡ğ‘’ ğ›¾1ğ‘¡ğ‘  ln ğ›¾ , = 17 (14) (15) This yields Eq. (5) for ğ›¾ 1. Singular case ğ›¾ = 1 and continuity. When ğ›¾ = 1 the integrand equals 1, hence ğ¼(1; ğ‘¡ğ‘  , ğ‘¡ğ‘’ ) = ğ‘¡ğ‘  ğ‘¡ğ‘’ . We also show the limit limğ›¾1 ğ¼(ğ›¾; ğ‘¡ğ‘  , ğ‘¡ğ‘’ ) = ğ‘¡ğ‘  ğ‘¡ğ‘’ to prove continuity. Set ğ›¾ = ğ‘’ â„ with â„ 0. Then As â„ 0, apply Taylor expansion (or equivalently LHÃ´pitals rule via â„): ğ¼(ğ‘’ â„; ğ‘¡ğ‘  , ğ‘¡ğ‘’ ) = ğ‘’ â„(1ğ‘¡ğ‘’ ) ğ‘’ â„(1ğ‘¡ğ‘  ) â„ , ğ‘’ â„(1ğ‘¡ğ‘’ ) ğ‘’ â„(1ğ‘¡ğ‘  ) â„ lim â„0 = (1 ğ‘¡ğ‘’ ) (1 ğ‘¡ğ‘ ) = ğ‘¡ğ‘  ğ‘¡ğ‘’ , Thus the coefficient is continuous at ğ›¾ = 1, and the analytic expression recovers the linear dynamic mode. Full analytic operator. Combining Eq.(14) and Eq.(15) yields Î¦(xğ‘¡ğ‘  , ğ‘¡ğ‘  , ğ‘¡ğ‘’ ; ğœƒ) = ğ¾ (cid:213) ğ‘˜=1 ğœ‹ğ‘˜(xğ‘¡ğ‘  ) vğ‘˜(xğ‘¡ğ‘  ) ğ’ (ğ›¾ğ‘˜(xğ‘¡ğ‘  ), ğ‘¡ğ‘  , ğ‘¡ğ‘’ ), with ğ’ () as in Eq. (5). Numerical remarks. For numerical stability when ğ›¾ is very close to 1, we branch to the second case (ğ‘¡ğ‘  ğ‘¡ğ‘’ ) when ln ğ›¾ < ğœ– (ğœ– = 106). E.2 Proof of Theorem 1 In this section, we provide the proof for Theorem 1. We demonstrate that the momentum parameterization in ArcFlow can accurately approximate any ground truth trajectory at ğ‘ discrete timesteps by using only ğ¾ = ğ‘ momentum modes. E.2.1 Problem Reformulation ğ‘› = u(y, ğ‘¡ğ‘›) â„ğ· represent the Let {ğ‘¡1, . . . , ğ‘¡ğ‘ } denote the set of sampled distinct timesteps, and let corresponding ground truth velocities for any latent state sampled from the data manifold. Our objective is to demonstrate that there exists parameter set ğœƒ = {ğœ‹ğ‘˜ , vğ‘˜ , ğ›¾ğ‘˜}ğ¾ that exactly satisfies the conditions: ğ‘˜=1 ğ¾ (cid:213) ğ‘˜=1 ğœ‹ğ‘˜vğ‘˜ ğ›¾1ğ‘¡ğ‘› ğ‘˜ ğ‘› , ğ‘› {1, . . . , ğ‘}, = (16) where ğ¾ = ğ‘. Directly solving Eq. (16) is complicated by the bilinear coupling between ğœ‹ğ‘˜ and vğ‘˜. Therefore, we introduce the composite parameter wğ‘˜ ğœ‹ğ‘˜vğ‘˜. The independence of the ğ· dimensions allows us to decouple the problem into ğ· identical scalar equations. Thus, we focus on single scalar dimension, letting ğ‘¤ğ‘˜ and ğ‘¢ ğ‘› denote the scalar components of wğ‘˜ and ğ‘›, respectively. Since our goal is to establish the existence of at least one feasible parameter set ğœƒ, we may fix subset of the parameters without loss of generality. Specifically, we fix the momentum factors Î“ = {ğ›¾1, . . . , ğ›¾ğ‘ } to be arbitrary distinct positive real values. With Î“ fixed, the exponential terms become known constants, and the problem of finding {ğœ‹ğ‘˜ , vğ‘˜} reduces to solving for the composite weights ğ‘¤ğ‘˜. This reduction holds because any valid solution for ğ‘¤ğ‘˜ guarantees the existence of ğœ‹ğ‘˜ and vğ‘˜. Consequently, the problem can be formulated as the following linear system: Mc = b, (17) where = [ğ‘¤1, . . . , ğ‘¤ğ¾] â„ğ¾ and = [ğ‘¢ , . . . , ğ‘¢ ğ‘ ] â„ğ‘ . The matrix â„ğ‘ğ¾ is the basis matrix 18 determined by the fixed ğ›¾ğ‘˜: ğ‘€ğ‘›ğ‘˜ = ğ›¾1ğ‘¡ğ‘› ğ‘˜ , (18) Establishing the existence of ğœƒ for an arbitrary ground truth is equivalent to guaranteeing that the linear system Mc = is solvable for any b. This condition holds if and only if the basis matrix is non-singular (invertible). Thus, the proof reduces to demonstrating the invertibility of M. E.2.2 Proving Invertibility via Chebyshev Systems The solvability of the linear system relies on the non-singularity of the basis matrix M. To establish this, we frame the problem within the theory of Chebyshev Systems. Definition 1 (Chebyshev System). Let { ğ‘“1, . . . , ğ‘“ğ‘ } be set of continuous functions defined on an interval â„ . This set constitutes Chebyshev System if every non-trivial linear combination ğ¹(ğ‘¡) = (cid:205)ğ‘ ğ‘˜=1 ğ‘ğ‘˜ ğ‘“ğ‘˜(ğ‘¡) (where coefficients ğ‘ğ‘˜ â„ are not all simultaneously zero) possesses at most ğ‘ 1 distinct zeros in â„ . The significance of this definition lies in the Haar Condition, which directly links the zero-counting property of functions to the determinant of their basis matrix: Lemma 1 (Haar Condition [6]). If the set { ğ‘“1, . . . , ğ‘“ğ‘ } forms Chebyshev System on â„ , then for any set of distinct sampling points {ğ‘¡1, . . . , ğ‘¡ğ‘ } â„ , the resulting matrix Î¦ with entries Î¦ğ‘›ğ‘˜ = ğ‘“ğ‘˜(ğ‘¡ğ‘›) is non-singular. To apply Lemma 1 to our specific problem, we must demonstrate that our proposed momentum dynamics functions satisfy the definition of Chebyshev System. Proposition 1. The set of functions {ğ›¾1ğ‘¡ Chebyshev System on â„. ğ‘˜ }ğ‘ ğ‘˜=1, parameterized by distinct momentum factors ğ›¾ğ‘˜ â„+, forms Proof. Let ğ¹(ğ‘¡) be an arbitrary non-trivial linear combination of the basis functions with coefficients ğ‘ğ‘˜ â„. We can rewrite the expression as generalized polynomial of exponentials: ğ¹ğ‘ (ğ‘¡) = ğ‘ (cid:213) ğ‘˜=1 ğ‘ğ‘˜ ğ›¾1ğ‘¡ ğ‘˜ = ğ‘ (cid:213) (ğ‘ğ‘˜ ğ›¾ğ‘˜)ğ‘’(ln ğ›¾ğ‘˜ )ğ‘¡ = ğ‘˜=1 ğ‘ (cid:213) ğ‘˜=1 ğ›¼ğ‘˜ ğ‘’ğœ†ğ‘˜ ğ‘¡ , Here, we define the new coefficients ğ›¼ğ‘˜ ğ‘ğ‘˜ ğ›¾ğ‘˜ (which remain non-zero if ğ‘ğ‘˜ are non-zero) and the distinct exponents ğœ†ğ‘˜ ln ğ›¾ğ‘˜. We prove that ğ¹ğ‘ (ğ‘¡) has at most ğ‘ 1 zeros by induction on ğ‘. Base case (ğ‘ = 1): ğ¹1(ğ‘¡) = ğ›¼1ğ‘’ğœ†1ğ‘¡. Since exponentials are strictly positive and ğ›¼1 0, ğ¹1(ğ‘¡) has no zeros. Inductive step: Assume that any linear combination of ğ‘ 1 exponentials ğ¹ğ‘1(ğ‘¡) has at most ğ‘ 2 distinct zeros. Suppose, for contradiction, that ğ¹ğ‘ (ğ‘¡) has ğ‘ distinct zeros. Define the auxiliary function ğºğ‘ (ğ‘¡) = ğ‘’ğœ†1ğ‘¡ ğ¹ğ‘ (ğ‘¡), which shares the same zeros as ğ¹ğ‘ (ğ‘¡). Its derivative is: ğº ğ‘ (ğ‘¡) = (cid:32) ğ‘‘ ğ‘‘ğ‘¡ ğ›¼1 + ğ‘ (cid:213) ğ‘˜=2 (cid:33) ğ›¼ğ‘˜ ğ‘’(ğœ†ğ‘˜ ğœ†1)ğ‘¡ = ğ‘ (cid:213) ğ‘˜= ğ›¼ğ‘˜(ğœ†ğ‘˜ ğœ†1)ğ‘’(ğœ†ğ‘˜ ğœ†1)ğ‘¡ . Note that ğº ğ‘ hypothesis, ğº ğ‘ its derivative ğº ğ‘ ğ‘ distinct zeros. (ğ‘¡) is linear combination of ğ‘ 1 exponentials with distinct exponents ğœ†ğ‘˜ ğœ†1. By the induction (ğ‘¡) can have at most ğ‘ 2 zeros. However, by Rolles Theorem, if ğºğ‘ (ğ‘¡) has ğ‘ distinct zeros, (ğ‘¡) must have at least ğ‘ 1 distinct zeros. This contradiction implies that ğ¹ğ‘ (ğ‘¡) cannot have Conclusion. Since Proposition 1 confirms that our basis functions form Chebyshev System, Lemma 1 ensures that the matrix is invertible for any set of distinct timesteps. This guarantees the existence of solution vector c. Translating this mathematical result back to our original objective, the solvability of implies that for any ground truth velocities ğ‘›, we can explicitly construct parameter set ğœƒ (e.g., by setting 19 Figure 7 Visualization of the ablation effect on the adoption of mixed trajectory integration for training. Here, (a) is generated from distilling Qwen-Image-20B, and (b) is generated from distilling FLUX.1-dev. Table 7 Quantitative ablation results on the adoption of mixed trajectory integration for training."
        },
        {
            "title": "Method",
            "content": "ArcFlow-Qwen w/o Mixed Trajectory Integration w/ Mixed Trajectory Integration ArcFlow-FLUX w/o Mixed Trajectory Integration w/ Mixed Trajectory Integration FID 14.04 13.52 19.17 18.21 ğœ‹ğ‘˜ = 1 and vğ‘˜ from c) that satisfies Eq. (16) exactly. This completes the proof of Theorem 1, theoretically validating that the proposed momentum parameterization possesses sufficient expressivity to perfectly align with arbitrary trajectory dynamics on the data manifold."
        },
        {
            "title": "F More Experiment Results",
            "content": "F.1 Ablations on Mixed Trajectory Integration To further validate the effectiveness of the proposed mixed trajectory integration strategy during training, we conduct ablation studies by training ArcFlow models with and without mixed trajectory integration on both Qwen-Image-20B and FLUX.1-dev. In all settings, models are trained for 3,000 iterations with batch size of 16, and evaluated using teacher-alignment FID on the Align5000 dataset. As shown in table 7, adopting mixed trajectory integration consistently improves FID across both backbones, indicating more accurate alignment with the teacher distribution. We further provide qualitative comparisons in figure 7. Models trained with mixed trajectory integration produce images with richer local details and sharper structures, benefiting from learning the velocity field while staying closer to the teacher trajectory in early training. In contrast, models trained without this strategy, while preserving comparable global structure, exhibit smoother and less detailed results, suggesting that the student is more prone to learning inaccurate velocity estimates at early stages, which leads to slower and less stable convergence. 20 Figure 8 Convergence visualization across different student methods based on Qwen-Image-20B. F.2 Convergence Visualization To further validate ArcFlows superior convergence speed and stability, we conducted visualization experiments. We reuse the same training checkpoint as in the convergence analysis of section 4.2 to ensure fair comparison. figure 8 shows our visualization result generated from one single prompt. As shown in figure 8, ArcFlow already exhibits coherent global structure after only 0.5K training iterations, with most stochastic artifacts and irregular noise largely suppressed. At this early stage, the generated images mainly suffer from mild over-smoothing, rather than structural corruption, indicating that the model has already entered meaningful generative regime. This behavior suggests that ArcFlow can immediately benefit from its natural compatibility with the pre-trained teacher weights, enabling effective adaptation without requiring extensive retraining. As training proceeds, the visual quality of ArcFlow improves in stable and monotonic manner. By 3K iterations, the generated images reach level where no obvious visual defects can be identified by human inspection, demonstrating both fast convergence and strong training stability. In comparison, pi-Flow preserves reasonable global structure at early iterations; however, it consistently struggles with residual noise artifacts throughout the training process. Even with increased iterations, the presence of scattered noise prevents clear improvement in perceptual quality, resulting in noticeably slower and less stable convergence. Moreover, TwinFlow exhibits the weakest convergence behavior. As linear distillation method, its parameterization conflicts with the teachers inherently complex trajectory, which prevents effective reuse of the teachers pre-trained weights at initialization. Consequently, TwinFlow is forced to re-learn viable representation from high-error state, leading to significantly slower convergence and inferior visual quality during early and intermediate training stages. F."
        },
        {
            "title": "Inference time",
            "content": "To quantitatively validate our generation acceleration, we measure the inference time of our method and other few-step baselines by running each model five times on the same prompt and reporting the average. Table 8 summarizes the inference time of different student models evaluated at resolution of 1024 1024, with all methods generating images using 2 NFEs. We observe that Qwen-Image-Lightning exhibits the longest inference time, which can be attributed to its use of multiple LoRA adapters, introducing additional low-rank computations during inference. In contrast, TwinFlow and SenseFlow achieve the lowest inference time, as they are trained via full-parameter finetuning, where the adapted weights directly overwrite the original parameters and incur no additional computational overhead at inference time. Our methods, ArcFlow-Qwen 21 Table 8 Average inference time of different models on the same prompt and 1024 1024 resolution, all generating with 2 NFEs. Qwen-Image Students Inference Time (s)"
        },
        {
            "title": "FLUX Students",
            "content": "Inference Time (s) Qwen-Image-Lightning TwinFlow pi-Flow (GMQwen) ArcFlow-Qwen (Ours) 1.718 1.372 1.440 1.411 SenseFlow (FLUX) pi-Flow (GMFLUX) ArcFlow-FLUX (Ours) 1.432 1.470 1.466 Figure 9 One failure case of ArcFlow, as 1-NFE inference produces blurry results. and ArcFlow-FLUX, fall between these two extremes. This indicates that the additional parameters introduced by our finetuning strategy incur only negligible increase in floating-point operations, resulting in inference times comparable to fully finetuned baselines. Overall, the results demonstrate that our approach achieves favorable balance between generation quality and inference efficiency, without sacrificing the low-latency advantage crucial for few-step image generation."
        },
        {
            "title": "G Limitations and Future Work",
            "content": "figure 9 illustrates representative limitation of our method. Specifically, when forced to degenerate to the extreme setting of single-step inference (1 NFE), ArcFlow exhibits severe degradation in generation quality and fails to produce meaningful results. We attribute this limitation to the difficulty of accurately modeling the momentum factor ğ›¾ under 1 NFE regime, where ğ›¾ becomes highly sensitive and challenging to predict without sufficient modeling capacity. potential direction to address this issue is to design deeper or more expressive network layers dedicated to modeling ğ›¾. In addition, we plan to validate the effectiveness of our method across models with diverse parameter scales. This part is left as future work."
        },
        {
            "title": "H Additional Qualitative Results",
            "content": "H.1 Comparison of Few-step Students on Qwen-Image-20B We provide additional comparison results of student models that are based on Qwen-Image-20B in figure 10. H.2 Comparison of Few-step Students on FLUX.1-dev We provide additional comparison results of student models that are based on FLUX.1-dev in figure 11. H.3 More High-Resolution Visualizations In this section, we present additional qualitative examples generated by ArcFlow to further demonstrate its generative performance. All prompts are randomly sampled, and the results are shown directly without any manual selection or filtering. Visualization results are shown in figure 12, figure 13, figure 14. 22 Figure 10 Additional qualitative comparisons between different student models distilled on Qwen-Image-20B. Note that results in each column are generated from the same batch of initial noise. 23 Figure 11 Qualitative comparisons between different student models distilled on FLUX.1-dev. 24 Figure 12 Visualization of ArcFlow-Qwen (NFE=2). Each image is of 1024 1024 resolution. 25 Figure 13 Visualization of ArcFlow-Qwen (NFE=2). Each image is of 1024 1024 resolution. 26 Figure 14 Visualization of ArcFlow-FLUX (NFE=2). Each image is of 1024 1024 resolution."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Microsoft Research Asia"
    ]
}