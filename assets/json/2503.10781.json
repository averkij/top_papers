{
    "paper_title": "Large-scale Pre-training for Grounded Video Caption Generation",
    "authors": [
        "Evangelos Kazakos",
        "Cordelia Schmid",
        "Josef Sivic"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce a new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset compared to a number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 1 8 7 0 1 . 3 0 5 2 : r Large-scale Pre-training for Grounded Video Caption Generation Evangelos Kazakos1, Cordelia Schmid2, Josef Sivic1 1Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague 2Inria, Ecole normale superieure, CNRS, PSL Research University https://ekazakos.github.io/grounded_video_caption_generation/ Figure 1. Output of our GROunded Video caption gEneration (GROVE) model on an instructional video. The model outputs video-level caption (bottom) with key noun phrases in the caption coloured and localised (grounded) in the video by temporally consistent bounding boxes (top). Note how the objects are consistently annotated (with the same color) despite changes in scale and viewpoint and how the person is marked as occluded (orange box not present) in frames 1 and 4 when the person (or their hand) is not visible."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We propose novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct large-scale pretraining dataset, named HowToGround1M. We also introduce Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatiotemporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-ofthe-art results on the proposed iGround dataset compared to number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model. We seek to generate grounded captions for given input video. As illustrated in Figure 1, the task is challenging as it entails both (i) generating the natural language caption for the video and (ii) predicting temporally dense bounding boxes for multiple noun phrases from the caption. Compared to grounded captioning in still images [5, 19, 26, 41, 44], the task in the video domain has the additional difficulty that objects might disappear in some frames because of occlusions and we need to produce temporally dense and consistent bounding boxes across the frames of the input video. This problem is important as spatio-temporal grounding of individual objects with natural language descriptions on large scale is one of the key steps to advance areas such as human-robot interaction and embodied perception [16, 21, 25, 30, 47]. Despite the advances on the grounded video caption generation problem [20, 40, 45] one of the key limiting factors hindering further progress is the lack of suitable large-scale videos datasets with captions densely grounded with multiple spatio-temporal boxes in the video. Existing datasets are restricted to localising single spatio-temporal tube for each short textual description [4, 33, 34, 43], have limited temporal consistency as they provide bounding boxes for only few sparsely sampled frames per video [9, 12, 45], or are limited to specific domain such as egocentric videos [7, 9, 18]. 1 Dataset Annot. type VidSTG [43] HC-STVG [34] ActivityNet-Entities [45] HowToGround1M (Ours) Automatic iGround (Ours) Manual Manual Manual Manual Multiple frames Multi-object grounding Num. videos 36.2K 10.1K 37.4K 1M 2K Num. instances 9.9M 1.5M 93.6K 80.1M 236.9K Table 1. Comparison of our two datasets iGround and HowToGround1M with state-of-the-art video grounding datasets. In this work, we address this key limitation by the following three contributions. First, to address the issue of limited training data, we introduce large-scale automatic annotation method leveraging an existing model for grounded still-image captioning together with an LLM to summarize frame-level captions into video-level captions. The LLM is also tasked to perform temporally consistent bounding box annotation, associating frame-level phrases that correspond to objects with video-level phrases, resulting in video-level caption grounded with multiple object tubes with consistent natural language labels. We apply this approach to videos from the HowTo100M [22] dataset, which results in new large-scale pre-training dataset, namely HowToGround1M, of more than 1M videos for this problem. This automatic annotation method is coupled with proposed GROunded Video Caption gEneration model, called GROVE. The key technical contributions of this model include: (i) spatio-temporal adapters, which enable efficient modeling of spatio-temporal information in video; (ii) bounding box decoder that outputs temporally coherent bounding boxes in video and (iii) temporal objectness head that explicitly models objects that temporary leave the frame or are occluded. We pre-train the GROVE model on the proposed large-scale automatically annotated HowToGround1M dataset. Second, we introduce new manually annotated dataset for the grounded caption generation task, which we name iGround. The dataset contains 3500 videos and more than 230,000 annotated object bounding boxes. We split this dataset into train/val/test sets (2000/500/1000, respectively). This allows us to measure progress on this challenging problem, as well as to fine-tune our model on small-scale but high-quality data. Third, our results demonstrate that our GROVE model achieves state-of-the-art performance on the proposed iGround dataset as well as on the VidSTG [43] and ActivityNet-Entities [45] datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model. 2. Related Work Image-based grounded data generation. Recent efforts in multi-modal learning have focused on grounding text to images and comprehending referring expres2 sions [5, 19, 26, 28, 41, 44]. Given the scale of these models, large-scale training data are essential, yet manual annotation is prohibitively expensive. To address this, methods typically leverage pre-trained models for automatic annotation. Several approaches rely on LLM-driven data generation [5, 41], where GPT-4 [24] is used to create grounded dialogue datasets. Some methods pair instruction-tuned captions with bounding boxes [41], while others generate QA pairs from existing bounding boxes and captions [5]. Other methods leverage NLP techniques, such as extracting noun chunks via Part-of-Speech tagging and aligning them with bounding boxes using pre-trained grounding model [26]. More complex multi-stage annotation pipelines integrate multiple pretrained models to generate large-scale pseudolabeled datasets [28]. We build on this line of work but extend it to the video domain. Datasets for spatio-temporal grounding in video. The existing datasets [4, 11, 33, 34, 43, 45] for spatio-temporal video grounding are relatively small scale as they rely on manual annotation, which is tedious and time consuming. Typically, the existing datasets also focus on localizing single spatio-temporal tube for each short textual description [4, 33, 34, 43], which can be limiting factor in instructional videos where multiple objects are often manipulated. In contrast, both our automatically annotated HowToGround1M dataset as well as our manually-labelled iGround dataset contain multiple spatio-temporal bounding boxes grounding multiple objects described in the caption. We compare our HowToGround1M dataset as well as our iGround dataset with exisiting video grounding datasets in Table 1. ActivityNetEntities [45] is closest to ours in that it provides grounded captions multiple objects per frame for annotated noun phrases in the caption. Nevertheless, the authors annotate single frame per object in the video segment while both our automatically annotated HowToGround1M as well as our manually annotated iGround dataset have densely annotated frames per video segment. Moreover, HowToGround1M has the largest number of videos across all datasets and the largest number of annotated instances. More statistics and analysis of our proposed datasets is provided in the supplementary material. Spatio-temporal grounding in video. Spatio-temporal grounding [4, 10, 13, 17, 3234, 38, 39, 42, 43] aims to predict single spatio-temporal tube enclosing an event described in natural language query. Early approaches relied on object detection features [34, 43]. [43] introduced spatio-temporal graph encoder, while [34] and [39] adapted transformers for multi-modal grounding. Some works employed contrastive learning on large-scale instructional videos, leveraging weak supervision from HowTo100M [4, 33]. Others explored architectural innovations, such as twostream models for appearance and motion [17] or contextguided decoding for object-centric grounding [10]. However, Figure 2. method for automatic annotation of spatio-temporally grounded captions. In the first stage (left), we apply still-image grounded caption generation model on individual video frames producing temporally inconsistent outputs. In the second stage (middle), the captions from individual frames are aggregated into single video-level caption describing the most salient actions/objects in the video. Third (right), individual frame-level phrases and bounding boxes are associated over time into temporally consistent and dense labelling of object bounding boxes over the video. these methods lack text generation component, as they assume the query is given, and they cannot generate multiple spatio-temporal tubes for multiple objects. Our approach addresses this issue with the GROVE model, novel automatic annotation method, and manually-labeled dataset for finetuning and evaluation. Closest to our work is [45], which separately performs captioning and grounding. In contrast, our GROVE model jointly (i) captions, (ii) tags groundable noun phrases, and (iii) grounds them with temporally dense bounding boxes while handling occlusions. Experimental results demonstrate that we outperform [45] by margin. 3. Large-scale generation of grounded captions In this section, we first introduce our automatic annotation method for generating large-scale dataset for grounded video caption generation (Sec. 3.1). We then describe HowToGround1M (Sec. 3.2), our dataset created by applying this method to videos from HowTo100M [22, 31]. 3.1. Automatic annotation method We describe our method for generating an automatically annotated dataset for grounded video caption generation. Given an unlabelled dataset of videos depicting humans interacting with objects and/or other humans, the goal of the method is to generate both video-level captions describing what is happening in the video and temporally dense and consistent bounding boxes grounded to the noun phrases from the caption that describe the main objects in the video. We leverage foundation LMMs and LLMs as they have been pretrained on large-scale datasets and provide rich source of information. Our method consists of three steps, as shown in Figure 2: i) frame-wise grounded caption generation, ii) video-level caption aggregation and iii) temporally consistent annotation of objects. We describe these steps next. Stage 1: Frame-wise grounded caption generation. We begin by generating grounded captions for individual video frames, producing text descriptions alongside bounding boxes for noun phrases referring to objects. As no video models exist for this task, we use image-based models in frame-by-frame manner. We adopt GLaMM [28], which excels in image-based grounded captioning. Since GLaMM outputs segmentation masks, we convert them to bounding boxes. An example output is shown in Figure 2 (left). In the experiments, we also compare an alternative approach that combines frame-level captioning [37] with open-vocabulary object detection [23] to obtain candidate captions and bounding boxes in individual frames. Stage 2: Video-level caption aggregation. The previous stage provides outputs for individual frames, which are, however, not temporally consistent. We address this issue in Stage 2. From the frame-level captions, we generate videolevel caption that highlights the most salient actions and objects while ensuring consistency in noun phrase annotations across frames. We achieve this by prompting Llama2 [35], as described next. Since the grounded captioning model produces lengthy captions with extraneous details, we first extract Subject-Verb-Object (SVO) triplets and relevant adpositional phrases using Part-of-Speech (POS) tagging. These structured triplets serve as input to the LLM, encapsulating subjects, actions, and relevant objects. To enhance accuracy, we perform in-context learning by providing example pairs of frame-level SVO triplets and their corresponding video-level captions (see supplementary material for details). Given new set of SVO triplets, the LLM generates videolevel caption and tags noun phrases corresponding to objects of interest within <p></p> tags. Figure 2 (middle) illustrates this process. In experiments, we compare the above approach with providing the LLM with full captions from Stage 1 instead of extracting Subject-Verb-Object triplets from the caption to assess the impact of additional context. Stage 3: Temporally consistent bounding box annotation. While Stage 2 ensures consistent video-level caption, noun phrase annotations remain inconsistent across frames due to the use of an image-based model. To resolve this, we 3 introduce temporal labeling of objects, ensuring that bounding boxes corresponding to the same object are consistently labeled throughout the video using the video-level noun phrases. These consistently labeled bounding boxes form video object tracks. We formulate this as text classification task and prompt the LLM with in-context learning. The input consists of frame-level noun phrases to be classified and the video-level noun phrases serving as class labels. The supplementary material provides details on the prompt. Figure 2 (right) illustrates this process. An alternative approach is to use visual tracker, such as [14], for associating bounding boxes over time, which we compare against in Sec. 6.2. Upon completing all three stages, we obtain videos with automatically generated captions, grounded bounding boxes, and temporally coherent labels aligned with the noun phrases in the caption. Additional details are provided in the supplementary material. 3.2. The HowToGround1M dataset We apply our automatic annotation method to Internet instructional videos from the HowTo100M dataset [22]. We choose HowTo100M due to its diversity of actions, scenes, objects and lighting conditions. We apply our approach to clips obtained using time stamps from the HowToCaption [31] version of the dataset. These clips contain meaningful events in the video identified based on LLM analysis of the associated narrations. In detail, we randomly sample 1M video clips from HowTo100M videos using start/end timestamps from HowToCaption. The videos from HowTo100M have variable frame rates usually ranging in 25-30 fps, and we process the videos at 5fps. The majority of the clips are 8 seconds long with spatial resolution of 455256 pixels. We run our automatic annotation method on this set of data to obtain our HowToGround1M pre-training dataset. The resulting dataset contains 1M videos, with 1M captions containing 3.2M noun phrases. The captions contain 18.6k unique terms and 142k unique noun phrases. Overall, the dataset contains 43.6M annotated frames with 80.1M bounding boxes. This dataset is used to pre-train the GROVE model, described next. 4. The GROVE Model We introduce GROunded Video caption gEneration model, called GROVE, see Figure 3. The input to the model is video clip with frames (left) and the output is natural language caption (right) together with spatio-temporal bounding boxes localizing the individual noun phrases in the video together with objectness score indicating the presence / absence of the object in specific frame (top). We build on the GLaMM [28] model, which is state-of-the-art method for still image-based grounded caption generation, and extend it to the video domain. The key technical components enabling grounded caption generation in videos are (shown in dashed red rectangles in Figure 3): i) the spatiotemporal adapters with pooling which enable modelling temporal information efficiently; ii) the bounding box decoder which allows re-using large-scale pretrained decoder weights [28]; and iii) the temporal objectness head for modelling objects that temporary leave the frame or are occluded. Details are given below. We assess the importance of these components in Section 6. Additional details of the GROVE model and the training procedure are in the supplementary material. Spatio-temporal adapters and pooling. The visual information is encoded using the Global Video Encoder Ve(), which represents the video globally for captioning, and the Grounding Video Encoder Vg(), which represents the finegrained details for grounding. We build these encoders by adapting the respective pre-trained image encoders [28]. We achieve this by interleaving spatio-temporal adapter layers (denoted as a()) between the image-based encoder layers. To stabilise training, we add residual connections and introduce learnable parameter that is multiplied by the adapters output and starts from 0 at the beginning of training [1]. By doing so, at the beginning of training the adapters output is effectively cancelled out and the network observes only the original encoders output. As training progresses, the learnable parameter is tuned and the network automatically adjusts the contribution of the adapter based on the gradients of the loss. In detail, the adapter layer performs a(o) = + tanh(α) (o), where is the output of the preceding encoder layer, α is the tunable parameter that is initialised to 0 and passes through tanh activation and () is the adapter layer. As feeding the full video tokens oe to the LLM is computationally prohibitive, we introduce spatio-temporal pooling function after the output of Ve(), i.e., op = p(oe). Bounding box decoder and prediction head. We adapt the pre-trained mask decoder [15, 28] for bounding box decoding. We focus on bounding boxes (rather than full pixel-level masks) as they are easier and cheaper to manually annotate yet provide good localization accuracy for compact objects, which are the main focus of this work. We transform the mask decoder to bounding box decoder by using the embedded detection tokens as queries, and the visual features of the Grounding Video Encoder as keys/values, resulting in an output that has same length as the detection tokens, allowing us to predict bounding box for each detection token that corresponds to noun phrase in the caption. Importantly, while Vg() performs video processing, we apply the crossattention in frame-wise fashion to predict objects at each frame of the input video. We employ bounding box prediction head on the output of the bounding box decoder, od. It is an MLP that predicts bounding box coordinates for the embedded detection tokens at each frame: pbb = hbb(od), where pbb RT Nd4 are the bounding box predictions 4 Figure 3. An overview of our GROVE model for grounded video caption generation. Dashed red rectangles outline the key technical contributions enabling grounded caption generation in video and include: (i) spatio-temporal adapters; (ii) the bounding box decoder and (iii) the temporal objectness head. and hbb() is the bounding box head. Temporal objectness head. As discussed previously, one major challenge for videos is that objects might disappear and reappear in different frames of the video. To address this, we introduce temporal objectness head. Different than objectness predictions in image-based object detection, the purpose of this head is to predict whether an object is visible or not at given frame of video: ptobj = htobj(od), where ptobj RT Nd1 are the temporal objectness scores and htobj() is the temporal objectness head. During inference, we threshold ptobj and for each frame we select only the bounding boxes for which the temporal objectness scores pass the threshold. should match exactly short phrase/word from the caption. More information about the annotation procedure including the exact annotation guidelines for the raters and the mechanisms for assuring the consistency and the overall quality of the rater annotations, can be found in the supplementary material. 6. Experiments In this section we introduce evaluation datasets and metrics, compare the proposed approach with the state of the art on three benchmark datasets, analyze the effect of the pretraining dataset size and ablate the key components of the proposed model and automatic annotation procedure. 5. Manually annotated iGround dataset 6.1. Datasets and evaluation metrics For the iGround dataset we select 3500 clips from the HowTo100M dataset [22] by sampling interesting videos that typically include dynamic events or actions that are clear and distinguishable. In those events/actions, people usually interact with objects. We make sure that the 3500 clips do not overlap with those in the HowToGround1M dataset described in section 3. We split the set into 2000/500/1000 video clips to form the train/val/test sets, respectively. The video annotation itself consists of 3 steps. The first step entails watching the video and providing natural language description of what is happening in the video and the objects that are being manipulated. Note, that we are interested in the active objects, i.e. objects that humans interact with, rather than densely describing all objects in the scene. In the second step, bounding boxes are annotated for all visible instances of humans/objects mentioned in the caption that has been provided in the previous step. Finally, each bounding box is annotated with short phrase or word that We evaluate the proposed approach on three datasets, the newly introduced grounded video caption generation dataset iGround as well as the established VidSTG [43] and ActivityNets-Entities [45] video grounding benchmarks. iGround. We build on the metrics for grounding captions in still images [28] and adapt them to our task in videos. These include METEOR [2] and CIDEr [36] for the quality of the captions, AP50 for the grounding accuracy, and recall [28] that combines (i) IoU between ground truth (GT) and predicted bounding boxes as well as (ii) the similarity of embeddings of GT and predicted noun phrases that correspond to bounding boxes. The aim of the recall metric is to assess the rate of positive predictions. prediction is considered positive if both the bounding box IoU and the noun phrase similarity are above certain threshold. We propose video-level evaluation setting for AP50 and recall for the grounded video caption generation task where the metrics are calculated per video and averaged across videos. 5 Method METEOR CIDER AP50 Recall 6.2. Comparison with the state of the art a. GLaMM [28] e b. GROVE - PT (Ours) c. GROVE - PT+FT (Ours) A d. Automatic annotation e. GROVE - PT (Ours) f. GROVE - FT (Ours) g. GROVE - PT+FT (Ours) 11.9 14.5 21.7 13.8 14.5 21.0 21.7 29.9 49.9 85.4 40.0 49.9 81.7 85.4 20.8 26.7 31. 27.1 34.2 15.7 40.8 19.3 23.0 25.4 20.4 25.4 17.4 28.6 Table 2. Grounded video caption generation on manuallyannotated iGround test set. Pre-training on our new large-scale HowToGround1M dataset followed by finetuning on manuallyannotated iGround training data (PT+FT) clearly outperforms pretraining only (PT) and finetuning only (FT) as well as the GLaMM baseline [28] (a.) and directly applying automatic annotation (d.). We show center frame (Center) and all frame (All) evaluation. Method STVGBert [32] TubeDETR [39] STCAT [13] DenseVOC [46] GROVE FT (Ours) GROVE PT+FT (Ours) msIoU 47.3 59.0 61.7 61.9 61.3 62.9 Table 3. State-of-the-art comparison of spatial grounding on the VidSTG [43] test set. All models use ground truth temporal localization. Large-scale pretraining (PT+FT) results in an improvement over fine-tuning only (FT) for our model GROVE. Method F1all F1all per sent F1loc F1loc per sent GVD [45] GROVE FT (Ours) GROVE PT+FT (Ours) 07.10 09.51 13.57 17.30 21.15 24.23 23.80 30.96 43. 59.20 68.79 76.99 Table 4. Results on the validation set of ActivityNet-Entities [45]. Large-scale pretraining (PT+FT) results in an improvement over fine-tuning only (FT) for our model GROVE. ActivityNet-Entities. We follow [45] and report F1all, F1all per sent, F1loc, and F1loc per sent. In F1all, region prediction is considered correct if the associated noun phrase is both correctly predicted (exact match) and correctly localised (IoU > 0.5). F1loc considers only localisation accuracy ignoring errors in the generated noun phrases. In these metrics, accuracies are averaged across noun categories while in F1all per sent and F1loc per sent accuracies are averaged across sentences. VidSTG. We follow [13, 32, 39, 46] and assume that the videos are temporally segmented to the events of interest (using the available start/end timestamps) and report msIoU, defined as the average IoU across frames, between the predicted and ground truth bounding boxes for the target event. Implementation details. All implementation details including architectural choices of the GROVE model as well as training and inference details can be found in the supp. mat. iGround. The results on our human-annotated iGround test set are shown in Table 2. We compare the results of the proposed GROVE model with our automatic annotation method (described in Section 3) and (still image) GLaMM [28]. The automatic annotation method is natural fit for baseline for this task as it performs image-based grounded captioning followed by video-level aggregation without any training. Comparing with [28] aims to assess the benefits of our method in comparison to still-image grounding. Since [28] runs per frame, it does not provide video-level caption and noun phrases; these differ across frames (see Sec. 3). Thus, we use predictions of [28] for the center frame of each video. We call this the Center set-up in Table 2. The All set-up considers video-level caption and all frames of the video for bounding box localisation. GROVE, pre-trained on our automatically annotated HowToGround1M dataset (b.), significantly outperforms the [28] baseline (a.) with an improvement of 20 points in CIDEr and 5.9 points in AP50. This demonstrates that temporal context is crucial for this task. Results further improve with finetuning (c., PT+FT). In the all-frame evaluation, the GROVE model pretrained on the automatically annotated HowToGround1M dataset (e.) provides significant performance improvements over directly obtaining predictions on the test set using the automatic annotation method (d.). This indicates that the GROVE model can correct (smooth out) some of the noise in the automatic annotations during the large-scale training. Finally, we obtain further major performance improvements by fine-tuning GROVE on our manually-annotated iGround dataset, showing that large-scale pre-training with automatic labels followed by fine-tuning on small-scale but highly accurate dataset (PT+FT, c. and g.) is good recipe for this task. ActivityNet-Entities and VidSTG. We adapt the GROVE model for the spatio-temporal video grounding task on the ActivityNet-Entities and VidSTG datasets. The details are in the supplementary material. Tables 3 and 4 show the comparison of GROVE with published results on ActivityNetEntities and VidSTG, respectively. On VidSTG, GROVE achieves the best performance despite not being designed for this task. On ActivityNet-Entities, GROVE significantly outperforms the GVD baseline, demonstrating its effectiveness even with sparse annotations. Qualitative Results. We show qualitative results of the GROVE model on several example videos in Figures 1 and 4. More qualitative results are in the supplementary material. 6.3. Effects of pre-training dataset size We study the scaling behaviour of the GROVE model by varying the size of the pre-training data. Specifically, we pre-train GROVE on 1k, 10k, 100k and 1M videos sampled from our HowToGround1M dataset. The performance is 6 Figure 4. Qualitative examples showing predictions of our GROVE model. Please note that GROVE is able to: (i) produce video-level natural language captions describing the main action in the video; (ii) ground multiple objects; and (iii) produce spatio-temporally consistent bounding box predictions. Please note that the second row shows an example of model prediction that is partly incorrect as the blue box, while temporally consistent, does not depict yarn. More qualitative results including video results are in the supp. mat. Figure 5. Results after pre-training (PT) vs. after fine-tuning and pre-training (PT+FT) as function of the pre-training dataset size. Results are reported on the iGround validation set. measured on the iGround validation set, where we report CIDEr, AP50 and recall. The results are shown in Figure 5. For each metric, we report the performance of the pre-trained model for different amounts of pre-training data, but also the performance of the fine-tuned model when initialised from the pre-trained models and fine-tuned on the manually annotated iGround training set. The results show that as we scale the pre-training dataset, both the pre-trained and the fine-tuned models continue to improve consistently across all metrics. This finding is important, as it verifies that obtaining automatic annotations at large scale is beneficial for pretraining. It also signifies that fine-tuning using small-scale but high-quality data is most efficient when the model is pre-trained at the largest scale. Benefits of pre-training. The benefits of large-scale pretraining on the HowToGround1M dataset are clearly demonstrated on three different tasks and datasets, as shown in Table 2 (iGround), Table 3 (VidSTG) and Table 4 (ActivityNet-Entities), where the model pre-trained on the HowToGround1M dataset and finetuned on each specific dataset reaches state-of-the-art results. 6.4. Ablation analysis Here we provide the ablation of the key components of the GROVE model as well as evaluate variants of the automatic annotation method. Model ablations. We perform ablations of the key components of our GROVE model. We report results on the iGround validation set after pre-training with fine-tuning, which achieves the best results. For the ablations, pretraining is performed on 50k random subset of the HowToGround1M dataset to keep the ablation costs manageable. In Table 5, we ablate the spatio-temporal adapters as well as investigate the importance of training (unfreezing) the bound7 Unfreeze AD METEOR CIDEr AP50 Recall 19.4 19.4 18.8 88.5 84.2 79.0 37.9 31.7 30.1 26.0 23.4 23. Table 5. Ablation of spatio-temporal adapters (AD) and unfreezing the bounding box decoder and projection layers (unfreeze). We report results on the iGround validation set. Figure 6. Benefits of temporal objectness. AP50 (left) and recall (right) of our model for different temporal objectness thresholds. Results are reported on the iGround validation set. Auto. annotation METEOR CIDER AP50 Recall Average Proposed Alt. Stage 1 Alt. Stage 2 Alt. Stage 3 12.3 12.4 11.9 12. 31.7 36.8 28.7 31.7 26.9 20.1 27.8 23.2 19.3 14.5 17.6 16.0 22.5 20.9 21.5 20.8 Table 6. Comparison of our automatic annotation approach vs. its alternatives: Alt. Stage 1 is based on GIT [37], Llama3 [8] and OWLv2 [23]. In Alt. Stage 2, we use the full caption instead of Subject-Verb-Object triplets. Alt. Stage 3 is based on CoTracker3 [14] for tracking the objects of interest. Results are reported on the iGround validation set. ing box decoder and projection layers (VL and LQ in Fig. 3). To mitigate overfitting, we keep the visual backbones and the LLM frozen. However, we train the embedding and output layers of the LLM to accommodate the modified vocabulary with special tokens. Next, we assess the importance of the temporal objectness head. Figure 6 shows the performance of the GROVE model in AP50 and recall when varying the threshold for the temporal objectness from 0.0 (completely removing the temporal objectness head) to 0.5. This threshold regulates the sensitivity of the model to detecting whether an object leaves the frame (or is occluded). These results demonstrate that by increasing the temporal objectness threshold we obtain substantial benefits in AP50 while sacrificing only negligible amount of recall. This demonstrates the importance of modelling objects that temporally leave the frame. Ablations of automatic annotation. We replace each stage 8 of our automatic annotation method with an alternative. In Stage 1, we replace the still-image model [28] with an alternative still-image grounded caption generation method. This approach leverages GIT [37] for frame-level captioning, Llama3 [8] for extracting noun phrases from the caption, and OWLv2 [23] for their bounding box localisation within each frame. To ablate Stage 2, we provide the LLM with full captions from Stage 1 instead of extracting Subject-Verb-Object triplets from the caption to assess the impact of additional context. To ablate Stage 3, we incorporate CoTracker3 [14], SOTA visual point tracking method to provide temporal association of bounding boxes across frames. Using 5 uniformly sampled frames and their bounding box predictions from Stage 1, we track objects in between with CoTracker3 and associate the resulting tracks with noun phrases from the caption. Results are reported in Table 6 where we compare the alternative automatic annotation methods on the iGround validation set. The alternative Stage 1 performs better in captioning due to GITs superior performance but performs noticeably worse for grounding. This is because our Stage 1 still-image grounding model [28] is explicitly trained for grounding, unlike GIT, Llama3, and OWLv2, which may underperform due to various factorssuch as Llama3 extracting non-groundable noun phrases or OWLv2 missing objects. Compared to our proposed method, the alternative Stage 2 underperforms across all metrics except AP50. This is because our Stage 1 grounding model [28] produces long captions, which include distractor (non-interacted) objects, leading to small drop in precision (AP50). The alternative Stage 3 underperforms in grounding due to tracker drift caused by abrupt viewpoint changes. Overall, on average (the last column in Table 6) our proposed method achieves the best performance. 7. Conclusion In this work, we introduced large-scale automatic annotation method to generate densely grounded captions in video, which we used to construct the large-scale HowToGround1M video dataset. Building on this, we developed GROVE, new model for grounded video captioning, which we pre-trained on this large-scale data. To enable rigorous evaluation, we introduced iGround, dataset with high-quality manual annotations, which also serves as fine-tuning resource. Our experiments demonstrate the effectiveness of our pre-training strategy, the importance of scaling the pre-training dataset as well as the benefits of fine-tuning on smaller, high-quality data. Our approach not only sets the state of the art on the new manually annotated iGround dataset but also outperforms existing methods on VidSTG and ActivityNet-Entities benchmarks. We believe this work provides strong foundation for future research in grounded video captioning and highlights the importance of large-scale pre-training combined with smaller-scale but high-quality supervision. Acknowledgments. This work was supported by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90140). This research also received the support of projects EXA4MIND and ERC FRONTIER, funded by the European Unions Horizon Europe Research and Innovation Programme, under Grant Agreements 101092944 and 101097822. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 4 [2] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with In Proceedings of the ACL Workshop human judgments. on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, 2005. 5 [3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. 15 [4] Brian Chen, Nina Shvetsova, Andrew Rouditchenko, Daniel Kondermann, Samuel Thomas, Shih-Fu Chang, Rogerio Feris, James Glass, and Hilde Kuehne. What, when, and where? self-supervised spatio-temporal grounding in untrimmed multi-action videos from narrated instructions. arXiv preprint arXiv:2303.16990, 2024. 1, 2 [5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 1, 2 [6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 15 [7] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen. Epic-kitchens visor benchmark: Video segmentations and object relations. In Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2022. [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 8 [9] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, and et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1 [10] Xin Gu, Heng Fan, Yan Huang, Tiejian Luo, and Libo Zhang. Context-guided spatio-temporal video grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [11] De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, and Juan Carlos Niebles. Finding it: Weaklysupervised reference-aware visual grounding in instructional videos. In Proceedings of the the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [12] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatiotemporal scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1 [13] Yang Jin, Yongzhi Li, Zehuan Yuan, and Yadong Mu. Embracing consistency: one-stage approach for spatio-temporal video grounding. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 2, 6 [14] N. Karaev, I. Makarov, J. Wang, N. Neverova, A. Vedaldi, and C. Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. 4, [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, arXiv preprint and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 4, 15 [16] Zongmian Li, Jiri Sedlar, Justin Carpentier, Ivan Laptev, Nicolas Mansard, and Josef Sivic. Estimating 3d motion and forces of human-object interactions from internet videos. International Journal of Computer Vision (IJCV), 130(2):363383, 2022. 1 [17] Zihang Lin, Chaolei Tan, Jian-Fang Hu, Zhi Jin, Tiancai Ye, and Wei-Shi Zheng. Collaborative static and dynamic vision-language streams for spatio-temporal video grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [18] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: 4d egocentric dataset for category-level humanobject interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1 [19] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. arXiv preprint arXiv:2404.13013, 2024. 1, 2 [20] Chih-Yao Ma, Yannis Kalantidis, Ghassan AlRegib, Peter Vajda, Marcus Rohrbach, and Zsolt Kira. Learning to generate grounded visual captions without localization supervision. In Proceedings of the European Conference on Computer Vision (ECCV), page 353370, 2020. [21] Robert McCarthy, Daniel CH Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas Thuruthel, 9 and Zhibin Li. Towards generalist robot learning from internet video: survey. arXiv preprint arXiv:2404.19664, 2024. 1 [22] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, and Josef Sivic. HowTo100M: Learning Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 2, 3, 4, 5 Ivan Laptev, [23] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. arXiv preprint arXiv:2306.09683, 2024. 3, [24] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2024. 2 [25] Austin Patel, Andrew Wang, Ilija Radosavovic, and Jitendra Malik. Learning to imitate object interactions from internet videos. arXiv preprint arXiv:2211.13225, 2022. 1 [26] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 1, 2 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), 2021. 15 [28] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. GLaMM: Pixel grounding large multimodal model. The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 4, 5, 6, 8, 15 [29] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: metric and loss for bounding box regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 15 [30] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2018. 1 [31] Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, and Hilde Kuehne. Howtocaption: Prompting llms to transform video annotations at scale. arXiv preprint arXiv:2310.04900, 2023. 3, [32] Rui Su, Qian Yu, and Dong Xu. STVGBert: visuallinguistic transformer based framework for spatio-temporal In Proceedings of the IEEE/CVF Intervideo grounding. national Conference on Computer Vision (ICCV), 2021. 2, 6 [33] Reuben Tan, Bryan A. Plummer, Kate Saenko, Hailin Jin, and Bryan Russell. Look at what im doing: Self-supervised spatial grounding of narrations in instructional videos. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 1, 2 [34] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatiotemporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology, 32(12):82388249, 2021. 1, 2 [35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [36] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 5 [37] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 3, 8 [38] Syed Talal Wasim, Muzammal Naseer, Salman Khan, MingHsuan Yang, and Fahad Shahbaz Khan. Videogrounding-dino: Towards open-vocabulary spatio-temporal video grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [39] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. TubeDETR: Spatio-temporal video grounding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 6 [40] Mihai Zanfir, Elisabeta Marinoiu, and Cristian Sminchisescu. Spatio-temporal attention models for grounded video captioning. In Proceedings of the European Conference on Computer Vision (ECCV), 2016. 1 [41] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, and Jianwei Yang. Llava-grounding: Grounded visual chat with large multimodal models. arXiv preprint arXiv:2312.02949, 2023. 1, 2 [42] Zhu Zhang, Zhou Zhao, Zhijie Lin, Baoxing Huai, and Jing Yuan. Object-aware multi-branch relation networks for spatiotemporal video grounding. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2020. 2 [43] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, 2, 5, 6, 15 [44] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. BuboGPT: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023. 1, 2 [45] Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason Corso, and Marcus Rohrbach. Grounded video description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 2, 3, 5, 6, 15 [46] X. Zhou, A. Arnab, C. Sun, and C. Schmid. Dense video object captioning from disjoint supervision. arXiv preprint arXiv:2306.11729, 2023. [47] Kateryna Zorina, Justin Carpentier, Josef Sivic, and Vladimır Petrık. Learning to manipulate tools by aligning simulation to 10 video demonstration. IEEE Robotics and Automation Letters, 7(1):438445, 2021."
        },
        {
            "title": "Supplementary material",
            "content": "A. Additional qualitative results Figures 8 and 9 show qualitative results of our GROVE model (Section 4 in the main paper), pre-trained on the HowToGround1M dataset and finetuned on the iGround training set (2000 examples). The results are shown on the iGround test set. In the figures captions we discuss some of the benefits of our model. Additional qualitative results showcasing the predictions of our approach overlaid over the input videos are shown in the supplementary video. B. Dataset Statistics Table 7 reports the statistics of both the HowToGround1M pre-training dataset and the iGround manually annotated set. Word clouds of the natural language descriptions from those datasets are shown in Figure 7."
        },
        {
            "title": "Statistic",
            "content": "HowToGround1M iGround Avg num frames per video Avg duration (seconds) Avg num instances per video Total num instances Avg box width height Avg tube length (frames) Avg caption length (words) 44.6 7.9 80.1 80,092,775 243.7 172.6 6.4 12.1 40.1 8.0 118.1 421,588 174.9 135.5 29.0 15.4 Table 7. Statistics of HowToGround1M and iGround datasets. C. Details of the GROVE model Model architecture. Figure 3 in the main paper shows the different components of our approach. The Global Video Encoder, Ve(), outputs video features, oe, which are pooled spatio-temporally, resulting in the video prompts. These are projected to language embedding space with L(). The LLM, LM(), ingests multimodal prompt consisting of video and language tokens. The LLM is prompted to generate caption for the video by tagging the noun phrases that correspond to objects and appending them with detection tokens (shown with red and green in the LLMs generated caption in Figure 3 The LLMs output hidden states that correspond to the generated caption are projected to queries (using LQ()). The queries corresponding to the detection tokens are fed to the bounding box decoder D(). The Grounding Video Encoder, Vg(), outputs fine-grained video features, which are also fed to the decoder. The decoder performs cross-attention frame-wise between the queries and the outputs of Vg(), og, which are used as keys/values. Finally, the prediction heads output bounding box predictions and temporal objectness scores for each object at each frame. This objectness score is used to predict the presence/absence of the object in each video frame and is of 12 (a) (b) Figure 7. Word cloud for (a) HowToGround1M dataset and (b) iGround dataset. major importance for the grounded video caption generation task. Details about the visual backbones Ve() and Vg() as well as details about the LLM LM() including the format of its multimodal inputs and its vocabulary are given next. Projection layers. We project the outputs of the Global Video Encoder and the output hidden states of the LLM with MLPs, op = L(op) and oq = LQ(ol), where L() projects the visual features to an embedded language space, while LQ() projects the LLMs hidden states to queries. op is the LLMs visual input while oq is input to the bounding box decoder that is described next. Backbones. GROVE consists of two video encoders and multimodal LLM as its main backbones. The Global Video Encoder Ve(), takes as input video RT H1W 1 and produces an output oe RT H1 , where is the patch size of the underlying visual transformer. Its purpose is to provide holistic representation of the video that will be ingested by the LLM. The Grounding Video Encoder Vg(), takes as input video RT H2W 2, where 2 > 1 and H2 > H1. It produces og RT H2 . og is used to ground phrases from the caption to the visual content, which is performed by the bounding box decoder that is described later. The input video to the Grounding Video Encoder is of larger spatial resolution than that of the Global Video Encoder for enhanced localisation capability. the LLM LM() takes as input multimodal Finally, 1 2 Figure 8. Qualitative results of our GROVE model on the (unseen) iGround test set. The colour-coded sentence fragments are spatiotemporally localised in the video with the bounding boxes colour coded with the same colour. The results demonstrate that: (i) our model can localise even small objects such as pen or tooth brush; (ii) objects are consistently labelled across frames despite changes of viewpoint or scale; (iii) the model focuses on the human and the interacted objects; (iv) the model can successfully ground multiple objects in the video. Additional results are shown in the supplementary video. 13 Figure 9. Additional qualitative results of our GROVE model on the (unseen) iGround test set. The colour-coded sentence fragments are spatio-temporally localised in the video with the bounding boxes colour coded with the same colour. In addition to the models properties discussed in Fig. 8, GROVE is capable of predicting whether an object is present in certain frame via the temporal objectness head; in the second example there are no bounding box predictions for the hand in the first three frames while in the fourth example there are no predictions for the hand and the screwdriver in the second and fifth frame. Additional results are shown in the supplementary video."
        },
        {
            "title": "Please respond with interleaved",
            "content": "sequence RLD and produces an output ol of the same size. Its input is of the form The <video> provides an overview of the video. Could you please give me description of the video? bounding boxes for the corresponding parts of the answer. <video> is replaced by the output of Ve(), and therefore the LLM ingests mixed language and visual tokens. We also augment the LLMs vocabulary with detection token <DET>, prompting the model to generate responses with <DET> tokens by the phrases that correspond to objects to be detected in the video. Loss function. Our loss function is combination of language modelling loss and losses relevant to video object detection. The language modelling loss is Cross-Entropy loss applied on ol. For object detection, we follow DETR [3] and use gIoU loss [29] and an L1 loss applied on pbb. Different than [3], the losses are applied per frame and summed over frames. Moreover, the losses are applied only to the objects that appear in the frame (rather than each object in the caption) using the ground-truth temporal objectness scores. The representation that we use for the bounding boxes is [x,y,w,h] and their coordinates are normalised with the dimensions of the video. Finally, we employ binary cross-entropy loss on ptobj. Our loss is, hence, defined as: LLM = CE(ol) LgIoU = gIoU (pbb, gtbb) LL1 = L1(pbb, gtbb) Ltobj = BCE(ptobj, gttobj) = λLM LLM + λgIoU LgIoU + λL1 LL1 + λtobj Ltobj, (1) (2) (3) (4) (5) (6) where gtbb are the ground truth boxes and gttobj are the ground truth objectness scores and λ are the weights for the losses. Training/inference. We realise the Global Video Encoder Ve() with CLIP-L [27] model with an input of 336336 and patch size of 14. The Grounding Video Encoder Vg() is instantiated with SAM [15] encoder and the bounding box decoder D() is SAM-based decoder, the same as in GLaMM [28]. The LLM LM() is Vicuna-7B model [6]. During training we keep Ve(), Vg() and LM() frozen. Vg() originally takes as input 1024 1024 images. As this is too large to fit in memory for videos, we instead use 512512 video spatial resolution, while we interpolate the positional encodings of Vg() and fine-tune them. Adapters are 3D spatiotemporal convolutional layers with kernel of size 3 3 3 and stride of 1. We apply adapters to every 3 layers of Ve() and to all global attention layers of Vg(). The bounding box head hbb is an MLP with two FC layers and ReLU activation function in between, while the temporal objectness head htobj is linear layer. Both prediction heads employ sigmoid activation function. We apply threshold of 0.5 to the temporal objectness scores. Both the adapters and the prediction heads are randomly initialised. We use = 8 frames for the videos during both training and testing. During training we perform random sparse sampling of frames by splitting the video in 8 segments and randomly drawing frame from each segment while during testing we pick the centre frame of each segment. We train GROVE for 20 epochs using batch size of 128. We use learning rate of 5 105 with warmup for the first 100 training steps and linearly decay the learning rate for the rest of training. We do not apply any weight decay or spatial data augmentation. We use λLM = 1,λgIoU = λL1 = λtobj = 2. For VidSTG [43] and ActivityNet-Entities [45], we do not use the temporal objectness head. That is because in VidSTG the spatio-temporal tubes are continuous within the segments boundaries, while ActivityNet-Entities provides annotations for single frame per object and in the rest of the frames the objects might still be present but without annotation, and thus should not be modelled as absent. As the task in VidSTG entails predicting the spatio-temporal bounding boxes given short description, we provide the short descriptions as input to our GROVE model during both training and inference in teacher-forcing setup. D. Details of the automatic annotation method Multiple people in the video. Our automatic annotation method can handle multiple subjects in video as long as one of the two following conditions are met: a) the subjects are described with distinct language, e.g. man with green jumper and man with blue shirt, or b) the subjects are within Subject-Verb-Object relationship even when described with the same terms, e.g. (person, dances, with, person) which would produce person dances with another person. If neither conditions are met, the caption aggregation (Stage 2) may merge the two subjects into one. Association of verbs and objects is naturally performed through the Subject-Verb-Object triplets. For example, given two relationships: (man, cuts, onions) and (woman, stirs, food, in, pot). The LLM-based caption aggregation step (Stage 2) has sufficient information to associate the man with the action of cutting the onions and the woman with stirring the food. Additional details of Stage 3. We provide additional details of the procedure of Stage 3 using the example from Figure 2 in the main paper, right. The object in the womans hands is described as green beverage and glass of green liquid across different frames. Stage 2 has provided the videolevel noun phrases woman and beverage. Stage 3 is formulated as classification problem where each one of 15 green beverage and glass of green liquid are the inputs to be classified in one of the classes {a woman, beverage, } and thus associated with the right bounding box. The class represents the None class, i.e. when an input does not belong to any of the known classes and it is useful for noisy inputs. E. Protocol for human annotations In Figure 10, we describe the annotation guidelines for annotating the training/validation/test sets of the GROVE dataset. The annotation criteria have been extensively discussed with the annotation provider and the annotators have been trained based on those criteria prior to commencing the annotation process. We have also performed pilot annotation project with the annotation provider on 10 video clips with several rounds of careful checking and feedback. Moreover, the annotation provider performed regular quality reviews on the annotations to ensure that the annotation criteria have been met. F. Prompts for automatic curation of spatiotemporally grounded captions The full prompt for the Stage 2 (Video-level caption aggregation) of our automatic annotation approach (Section 3 in the main paper) is shown in Figure 11 and the full prompt for Stage 3 (Temporally consistent bounding box annotation) in Figure 12. 16 Annotation Guidelines: 1. Video Selection: You will be provided with larger set of videos than needed. Your first task is to select clips that are considered interesting based on criteria that will be discussed further. An interesting video typically includes dynamic events or actions that are clear and distinguishable despite the low video quality. In those events/actions people usually interact with objects, e.g. man is cutting an onion using knife. Non-interesting events are typically static, e.g. person simply standing/sitting and talking. Non-interesting events are also events with ambiguous actions taking place, i.e. generic/abstract actions that cannot be described concisely or actions for which the annotator is unsure about what is happening in the video. 2. Video Annotation: For each selected video clip, write concise, one-sentence description of the main event taking place in the clip. If the action is too complex, use at most two sentences for describing it, but prioritise one-sentence descriptions. Focus only on the objects that humans interact with rather than describing densely every object in the scene. To enrich the language descriptions, also describe properties of objects such as color, shape, etc, e.g. blue cup or red onion. It is not strictly necessary to always describe the objects property but only when deemed important by the annotator. When you are unsure about the object being used, you can simply describe it as object. If object is unknown but the category of the object is known, please describe the object using its category, e.g. food. When there are two or more humans in the scene, use one of their characteristics to distinguish them, e.g. the woman in the red shirt standing next to the woman in the green shirt is putting strawberry on cocktail glass. If there are multiple actions happening consecutively, describe all of them and their associated objects. E.g. person is doing action-1 using object-1, then doing action-2 with an object-2. As shown in the example, you can use then for connecting temporally adjacent actions. Provide bounding boxes for humans/different objects mentioned in your description. These bounding boxes should be applied to all frames where the objects are visible. Label each bounding box with short phrase directly from your sentence description (e.g., brown dog, persons hands). It is not necessary that each object appears in each frame of the video. For example, person might be using tool, then leaving it down and using another tool. In this case, you would annotate with bounding boxes the first tool for the first half of the video and the second tool for the second half. Another common case is that objects or the person might disappear and then reappear. In this case, again all instances of the object must be annotated, so you should be careful about objects leaving the scene as they might enter the scene again later. If there are many small objects, e.g. mushrooms in pan, use single bounding box labelled as mushrooms. There are cases where two or more bounding boxes are needed for objects of the same type: a) one bounding box for each human hand when both are used to perform an action, b) one bounding box for each tool/container/appliance etc of the same type that the human is using, e.g. when they are placing food in two dishes, or pouring the content of shaker in two cocktail glasses. Descriptions: Must be accurate and written in fluent English. Suitable for either native speakers or highly proficient English speakers. Bounding Boxes: Ensure that bounding boxes accurately encompass the objects for the entirety of their visibility within the clip. The bounding boxes should be consistent and smooth across frames, maintaining size and position as closely as possible given the movement of the object and video quality. An exception is when there are abrupt viewpoint changes of the camera, which might result in objects abruptly changing position and size across neighbouring frames. Figure 10. Annotation guidelines for the manually annotated iGround dataset. 17 System Instructions Generate dynamic, video-level description based on frame-level inputs. The inputs include actions performed in individual frames in the form of Subject-Verb-Object (SVO) triplets along with prepositions and prepositional objects. The SVO triplets describe how actions are performed and how objects interact. Your output should be concise narrative in 1 sentence, focusing on the most salient actions depicted across the frames. Enclose the exact text of relevant objects within <p></p> tags. Input format: [[subject: subject_text, verb: action_text, object: object_text, prepositions_objects: [(preposition, prepositional_object)],],] Output format: Python dictionary with key CAPTION, and as value dynamic description of the video content. Infer motion from static descriptions. E.g. image shows person holding spoon and bowl implies person is stirring food in bowl. Enclose the human and the most frequent object that is used to perform the action within <p></p> tags. If there is no human, enclose the two most frequent objects within <p></p> tags. User Input 1 SVO: [[image, shows, cup], [bowl, is]], [[person, holding, spoon], [spoon, is, bowl], [[image, shows, spoon, (inside, bowl)]], [[person, seen], [person, holding, spoon], [spoon, used], [spoon, stir, food, (in, bowl)]], [[person, holding, spoon], [spoon, is, bowl]], [[person, holding, spoon], [spoon, is, bowl]], [[person, holding, spoon], [spoon, is, bowl]], [image, shows, spoon, (in, bowl)]], [[image, shows, bottle], [bottle, positioned, (beside, bowl)]], [[image, shows, bottle], [bottle, positioned, (beside, cup)]], [[image, shows, bottle], [image, placed, (on, counter)], [bottle, positioned, (beside, bowl)]]] Assistant Response 1 {CAPTION: <p>A person</p> is stirring <p>food in bowl</p> using spoon} User Input 2 SVO: [[hand, using, cutting board]], [[woman, using, cutting board], [woman, make, craft project]], [[child, using, craft cutter], [child, cut, object]], [[child, using, craft cutter], [child, cut, paper]], [[woman, using, craft cutter], [woman, cut, object]], [[woman, using, scissors pair], [woman, cut, piece, (of, paper)]], [[hand, using, scissors pair], [hand, cut, piece, (of, paper)]], [[woman, using, scissors pair], [woman, cut, piece, (of, paper)]], [[woman, using, craft cutter], [woman, cut, object]], [[woman, using, craft cutter], [woman, cut, plate]]] Assistant Response 2 {CAPTION: <p>A woman</p> is cutting <p>an object</p> using craft cutter} New User Input SVO: {input_svo} Figure 11. The full prompt for Stage 2 (Video-level caption aggregation) of our automatic annotation approach (Section 3 in the main paper). 18 System Instructions You are tasked with classifying humans and objects to set of given categories. Input format: Human/Object (string), set of categories (lists of strings). Output format: Python dictionary with key CATEGORY, and as value the predicted category of the human/object. Use None if the human/object doesnt belong to any of the categories. DO NEVER classify human as the object category and vice versa. User Input 1 Input: person Categories: [a woman, her hair] Assistant Response 1 {CATEGORY: woman} User Input Input: table Categories: [a person, bowl] Assistant Response 2 {CATEGORY: None} User Input 3 Input: piece of food on plate Categories: [a woman, meal] Assistant Response {CATEGORY: meal} User Input 4 Input: hand Categories: [a person, food on plate] Assistant Response 4 {CATEGORY: person} User Input Input: man in white shirt and black apron is also present Categories: [a person, food] Assistant Response 5 {CATEGORY: person} New User Input Input: {input_object} Categories: {input_categories} Figure 12. The full prompt for Stage 3 (Temporally consistent bounding box annotation) of our automatic annotation approach (Section 3 in the main paper)."
        }
    ],
    "affiliations": [
        "Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague",
        "Inria, Ecole normale superieure, CNRS, PSL Research University"
    ]
}