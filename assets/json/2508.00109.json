{
    "paper_title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality",
    "authors": [
        "Mingda Chen",
        "Yang Li",
        "Xilun Chen",
        "Adina Williams",
        "Gargi Ghosh",
        "Scott Yih"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 9 0 1 0 0 . 8 0 5 2 : r FACTORY: Challenging Human-Verified Prompt Set for Long-Form Factuality Mingda Chen, Yang Li, Xilun Chen, Adina Williams, Gargi Ghosh, Scott Yih"
        },
        {
            "title": "FAIR at Meta",
            "content": "Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, large-scale, human-verified prompt set. Developed using model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts. Date: August 4, 2025 Correspondence: Mingda Chen at mingdachen@meta.com Scott Yih at scottyih@meta.com Dataset: https://huggingface.co/datasets/facebook/FACTORY"
        },
        {
            "title": "1 Introduction",
            "content": "Long-form factuality aims to assess models abilities to produce comprehensive and accurate responses to relatively short prompts. Existing benchmark prompt sets are typically generated automatically, often missing crucial human verification for quality assurance. While considerable research has been devoted to improving models factuality (Zhang et al., 2024; Xie et al., 2024; Chen et al., 2024), there is notable gap in developing reliable benchmarks. To bridge this gap, we introduce FACTORY1, large-scale, human-verified, and challenging prompt set. We employ model-in-the-loop approach to ensure quality and address the complexities of evaluating long-form generation. Starting with seed topics from Wikipedia, we expand each topic into diverse set of prompts using large language models (LLMs). We then apply the model-in-the-loop method to filter out simpler prompts, maintaining high level of difficulty. Human annotators further refine the prompts to ensure they are fact-seeking, answerable, unambiguous, not time-sensitive, and safe. To push the boundaries of long-form factuality evaluation, we identify hard split of FACTORY that presents significant challenges to current state-of-the-art LLMs, with their outputs containing approximately 40% of claims for which humans cannot find supportive information online. In our experiments, we benchmark 6 state-of-the-art LLMs on FACTORY and two existing benchmarks. The human evaluations consistently show that state-of-the-art LLMs achieve around 90% factual precision on existing benchmarks. However, FACTORY proves more challenging, as the factual precision of the SOTA LLMs are only approximately 60% on its hard subset. These findings indicate that FACTORY presents genuine challenges to current state-of-the-art LLMs. We then conduct detailed analysis to identify the characteristics of FACTORY and the quality issues in prior benchmarks. We discover that prior benchmarks suffer from issues such as answerability, hallucinations, and time-sensitivity, likely due to their lack of human verification, which makes their evaluation results less trustworthy. In contrast, FACTORY is human-verified and features longer, detailed prompts that seek more 1Framework for Assessing Contextual Truth in Open Book Long-Form Factuality Figure 1 Diagram illustrating our data generation pipeline. To tackle the challenges of long-form evaluation, we employ model-in-the-loop approach, enabling rapid and comprehensive assessments of prompt quality. specific information, thus posing greater challenges to LLMs by requiring them to reason across long-tailed facts. We quantify these observations by breaking down our longer prompts into shorter, generic prompts that request basic details about the proper nouns involved. Ideally, LLMs equipped with all the necessary knowledge to solve prompts in our dataset should achieve perfect results on these decomposed prompts. Interestingly, we find that even the basic details of the proper nouns pose challenge for LLMs, indicating that they also lack the necessary knowledge to solve our benchmark."
        },
        {
            "title": "2 Related Work",
            "content": "Factuality evaluation has traditionally been focused on short-form question answering (Joshi et al., 2017; Kwiatkowski et al., 2019; Lin et al., 2022; Vu et al., 2023; Wei et al., 2024a). Recently, there has been increasing interest in long-form factuality tasks. Lee et al. (2022) adapted the FEVER fact verification dataset (Thorne et al., 2018) into prompts, prompting LLMs to generate continuations. They evaluated the factuality of these continuations based on the presence of named entities. Manakul et al. (2023) used templates to form prompts, asking LLMs to generate articles for Wikipedia entities and checking factuality through LLMs as well. In similar setup, Min et al. (2023) proposed using external knowledge sources during evaluation, though their focus was primarily on Wikipedia biographies. Recent long-form factuality benchmarks have begun exploring more diverse topics. Bang et al. (2025) and Ravichander et al. (2025) compiled benchmark from existing datasets and automatically created prompts to investigate different types of hallucinations. Unlike most prompt sets used in prior work, FACTORY is human annotated and contains more diverse topics. Perhaps the most related works to ours are LongFact (Wei et al., 2024b), where LLMs are prompted to generate questions based on list of predefined topics, and FactBench (Bayat et al., 2024), where existing datasets are automatically filtered to construct the dataset. Unlike these works, which contain only automatically constructed prompts and where state-of-the-art LLMs can already achieve saturated performance on most prompts, FACTORY is human-verified, challenging prompt set."
        },
        {
            "title": "3 FACTORY",
            "content": "One method for curating set of diverse and challenging prompts is to have human editors directly interact with the model and revise the prompts iteratively until the model generates an incorrect response. While effective for short-form QA tasks like SimpleQA (Wei et al., 2024a), this method is impractical for prompts requiring long, factual responses due to the laborious process of verifying each claim. As result, we adopt model-in-the-loop approach (Nie et al., 2020; Kiela et al., 2021) and create pipeline to automatically identify prompts that are challenging to state-of-the-art models and then have human editors to verify and revise those candidate prompts. We primarily utilize Llama models2 throughout the pipeline, unless otherwise specified. As shown in Figure 1, our prompt construction process, at high level, involves generating candidate questions, automatically filtering out less challenging ones, and then having humans revise them to ensure quality. While 2Llama-3.1-70B and Llama-4-Maverick-Instruct 2 Figure 2 Distributions of prompt categories in FACTORY. Human Verified Scale Difficulty LongFact (Wei et al., 2024b) FactBench (Bayat et al., 2024) FACTORY 2k 1k 10k Easy Medium Hard Table 1 Comparison between FACTORY and previous long-form factuality benchmarks. This comparison highlights that FACTORY is large-scale, human-verified, and presents greater challenge than earlier datasets. the filtering and revising steps could be repeated multiple times for improved quality, we perform them only once as the resulting benchmark is already challenging for state-of-the-art LLMs to solve. To create diverse set of prompts, it is essential to curate varied collection of seed prompts before they undergo human revision. We begin with Wikipedia because it covers broad range of topics and provides publicly available knowledge on the subjects. Specifically, we use the entire set of Wikipedia titles3 as queries to retrieve relevant information from MassiveDS (Shao et al., 2024), and then prompt LLMs to generate questions. With these generated questions, we first use LLMs to respond to them. Prompts that LLMs find unanswerable, given the retrieved passages from MassiveDS, are filtered out at this stage. We then employ VeriScore (Song et al., 2024) to fact-check each models response,4 filtering out questions that LLMs can answer with high factual precision.5 This approach ensures that our dataset remains challenging to current state-of-the-art LLMs. The remaining questions are then sent to human annotators for revision. For human annotations, annotators are instructed to make necessary edits or even reject prompts to ensure that each question meets the following criteria: (a) it is fact-seeking (rather than, for example, asking hypothetical questions or seeking help with creative writing); (b) it is self-contained and does not need additional context to disambiguate the question; (c) it can be answered using public, trustworthy online information; (d) expected responses should not vary over time; (e) it does not seek to elicit unsafe responses. Additionally, annotators are instructed to provide URLs where they can find minimal relevant information to answer the prompts.6 Annotators are instructed to reject prompts if complete rewrite is necessary during the revision process. total of 39 human annotators participated in the annotation process, spending approximately 5 minutes on each prompt revision task. Ultimately, around 20% of the prompts were rejected or edited according to different error categories, leaving us 10,156 prompts. To demonstrate the topic diversity of these prompts, we present the distribution of topic categories in Figure 2 (see Appendix for details on the prompt used for topic classification). Additionally, the distribution of the types of human edits performed is provided in 3We use the 2021 Wikipedia dump from Izacard et al. (2023), which consists of approximately 33 million entries. 4The standard version of VeriScore relies on online search engines, which can be costly. To enable scalable usage, we adapt the prompting versions of the claim verifier from VeriScore for use with MassiveDS (Shao et al., 2024), while still using its finetuned claim extractors. 5We retain prompts for which the Llama models achieve less than 60% factual precision. 6Although annotators are not expected to find information that exhaustively covers every aspect of the prompt, this exercise has proven helpful in improving annotation quality. 3 Figure 3 Factual precision as evaluated by human annotators on 100 sentences per model for each benchmark. All the models are retrieval-augmented. Appendix C. To identify the most challenging subset of prompts, we apply further filtering to select those where subset of our benchmarked state-of-the-art LLMs7 achieves around 50% factual precision. This subset, referred to as the hard split, consists of 421 prompts."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We benchmark 6 state-of-the-art LLMs, including Claude 3.7 Sonnet8, Gemini 2.5 Pro9, DeepSeek V310, GPT-4o11, Qwen312 and Llama 4 Maverick13 on LongFact (Wei et al., 2024b), FactBench (Bayat et al., 2024) and FACTORY. Additionally, we apply retrieval augmentation to these models by retrieving the top 20 most relevant passages from MassiveDS,14 using Contriever (Izacard et al., 2022) as the embedding model and input prompts as the queries. For all models and datasets, we use the prompt In the response, provide as many specific details and examples as possible (such as names of people, numbers, events, locations, dates, times, etc.) and generate up to maximum of 1,024 steps, similar to Chen et al. (2024). For LongFact, we use the subtset of 250 prompts provided by Wei et al. (2024b). For FactBench, our preliminary experiments indicate that SOTA LLMs have achieved saturated performance (over 90% precision) on the non-hard splits. Therefore, we only report model performance on the hard split, which consists of 532 prompts. We use human evaluations to verify the factual precision for the benchmarks. We do so by randomly sampling 100 sentences from each models output. These sentences, along with the automatically extracted claims, are presented to human annotators. Annotators have the ability to edit the claims before making their judgments, ensuring that the claims accurately reflect the content of the sentences. They then review and judge the claims (using online search engines) according to the same three labels used in VeriScore (supported, unsupported or inconclusive). This annotation task employed 25 annotators and each sentence took approximately 12 minutes."
        },
        {
            "title": "Prompt",
            "content": "1. Who is Emilia Chico? 2. Can you provide an overview of the International Monetary Fund? 3. What was the role of the United States in the Treaty of Tordesillas?"
        },
        {
            "title": "FactBench Hard",
            "content": "1. What are the strongest countries in Civilization 5? 2. The latest 50 kernel versions and release times of Linux 3. What might happen as consequence if the VAT on public transport services was reduced e.g. from 25% to 10%?"
        },
        {
            "title": "Subjective\nTime Sensitive\nHyperthetical",
            "content": "Explain the legal framework established by the Protection from Eviction Act 1977 in the United Kingdom regarding tenant rights. What challenges did Benni McCarthy encounter during his initial seasons as head coach of Cape Town City FC, and what were the teams performances in league and cup competitions? Figure 4 Top: Examples illustrating potential quality issues of existing factuality benchmarks. Bottom Right: Average prompt lengths. Compared to LongFact and FactBench, FACTORY has significantly longer and more detailed prompts. Bottom Left: Example prompts from FACTORY."
        },
        {
            "title": "4.2 Benchmarking Results",
            "content": "We present benchmarking results in Figure 3. As expected, most state-of-the-art LLMs achieve over 90% factual precision on LongFact, suggesting that their performance is approaching saturation. Additionally, FactBench Hard slightly reduces the performance of LLMs to approximately 85%. It is important to note that since FactBench and LongFact are automatically constructed, the lower factual precision could partly be due to the limited quality of the prompt set (see Section 4.3 for more discussions on the quality issues). In contrast, FACTORY presents greater challenge than both FactBench Hard and LongFact, further decreasing LLM performance to about 75%, while also being much larger in scale. With FACTORY Hard, the performance of state-of-the-art LLMs drops to around 60%. In addition, we do not observe significant differences in the length of model outputs across various benchmarks (see Appendix for more details), suggesting that the lower factual precision on FACTORY does not come from longer model generations."
        },
        {
            "title": "4.3 Analysis",
            "content": "What makes FACTORY different from LongFact and FactBench? We explore specific examples that distinguish FACTORY from other existing long-form factuality benchmarks. Firstly, we emphasize that FACTORY addresses several issues found in previous benchmarks, such as answerability, difficulty, and the querying of time-sensitive information. Figure 4 presents examples illustrating these issues. For instance, the first question from LongFact is unanswerable because Emilia Chico is not well-known figure. Conversely, the second question is too easy, as there is Wikipedia page about the International Monetary Fund. Retrieval-augmented LLMs could simply return the introductory paragraph from that page to provide an accurate response. The 7We used Claude and GPT models. 8claude-3-7-sonnet-20250219 9gemini-2.5-pro-preview-05-06 10https://huggingface.co/deepseek-ai/DeepSeek-V3-0324 11gpt-4o-2024-06-01 12https://huggingface.co/Qwen/Qwen3-235B-A22B 13https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 14We use 9 domains, namely: DPR Wiki (Karpukhin et al., 2020), Math (Welleck et al., 2021; Paster et al., 2023), Pes2o (Lo et al., 2020; Soldaini and Lo, 2023), PubMed (of Medicine, 2023), RPJ Books (Weber et al., 2024), RPJ arXiv (Weber et al., 2024), RPJ C4 (Weber et al., 2024), RPJ GitHub (Weber et al., 2024), RPJ StackExchange (Weber et al., 2024). 15Suppose responses of the same length make similar number of claims and the number of facts to be included about each prompt is constant. lower factual precision indicates lower recall when the length of the response remains the same. 5 Figure 5 Diagram illustrating the evaluation pipeline of atomic prompts. The atomic prompts are automatically generated by instructing language models to build generic questions using proper nouns in the original prompts. These atomic prompts are then used independently to assess the language models knowledge of specific subjects. Figure 6 Comparison of factual precision when evaluated on atomic prompts versus original prompts from the FACTORY Hard dataset. last question contains hallucinations. The phrase United States of America was coined in 1776, the same year the US declared its independence. Therefore, it could not have been involved in the Treaty of Tordesillas, which was signed in 1494. Examining examples from FactBench, the first prompt seeks the LLMs subjective opinion on the strongest countries in game. While seeking subjective opinions can be acceptable if there is public consensus, it rarely is the case and often leads to challenges in evaluating factuality, as finding supporting evidence can be difficult. With supporting information being absent, model outputs will be considered inconclusive, leading to lower factual precision without making the prompt set actually more challenging. Therefore, when constructing FACTORY, we explicitly instruct human annotators to edit or reject prompts that seek subjective advice. To assess the prevalence of this issue, we calculated the fraction of prompts containing words related to subjective opinions, such as best and top. Our findings revealed that 16% of FactBenchs hard prompts seek subjective opinions. This suggests that the difficulty of these hard prompts may partly stem from their inherent subjectivity. The second example is time-sensitive, as it requests the latest information. The final example is hypothetical, making it unsuitable for evaluation under the factuality setting. In contrast, as shown in Figure 4, prompts in FACTORY are significantly longer, seek more specific details, and are thus more challenging to existing LLMs. Why do LLMs fail on FACTORY? To determine whether the challenges in FACTORY arise from the long, detailed style of prompts (which necessitates reasoning capabilities across different facts) or the long-tailed knowledge required, we conducted additional experiments. We transformed the detailed prompts into more generic ones for each proper noun mentioned in the prompt, creating what we call atomic prompts, similar to the question styles in LongFact and FactBench. Ideally, LLMs equipped with all the necessary knowledge to solve prompts in our dataset should achieve perfect results on these atomic prompts. The evaluation process is demonstrated in Figure 5. This process was applied to FACTORY Hard, and we randomly sampled 100 prompts for evaluation using VeriScore (Song et al., 2024).16 On average, each prompt was broken down into 16When using VeriScore, we employ their finetuned claim extractor in combination with the GPT-4o claim verifier, as this setup performs better than other combinations. 6 two prompts. We evaluated GPT-4o and Claude 3.7 Sonnet on these prompts without using RAG and showed the results in Figure 6. Interestingly, while making the prompts more atomic helps in making them more solvable, they remain relatively challenging compared to current long-form factuality benchmarks. Notably, factual precision of the responses to FACTORY prompts is substantially lower as more specific information is needed. Such prompts requires models to reason across various facts, thereby increasing the difficulty of the task. These findings suggest that FACTORY is challenging to solve due to both its long-tailed knowledge and the reasoning capabilities required."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we addressed the limitations of existing long-form factuality benchmarks by introducing FACTORY, human-verified prompt set designed to challenge state-of-the-art LLMs. Our findings demonstrate that FACTORY significantly reduces factual precision compared to existing benchmarks, underscoring its difficulty and the need for improved model capabilities. Our analysis highlights the importance of human verification in benchmark development and identifies key issues in prior benchmarks, such as answerability and hallucinations. By requiring models to reason across detailed, specific information, FACTORY sets new standard for evaluating long-form factuality, paving the way for future advancements in model development and evaluation."
        },
        {
            "title": "References",
            "content": "Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. Hallulens: Llm hallucination benchmark. arXiv preprint arXiv:2504.17550, 2025. Farima Fatahi Bayat, Lechen Zhang, Sheza Munir, and Lu Wang. Factbench: dynamic benchmark for in-the-wild language model factuality evaluation. arXiv preprint arXiv:2410.22257, 2024. Mingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Sun, Luke Zettlemoyer, Gargi Ghosh, and Wen-tau Yih. Improving factuality with explicit working memory. arXiv preprint arXiv:2412.18069, 2024. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. https://openreview.net/forum?id=jKN1pXi7b0. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143, 2023. http://jmlr.org/papers/v24/23-0037.html. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. https://aclanthology.org/P17-1147/. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.emnlp-main.550. https://aclanthology.org/2020.emnlp-main.550/. Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 41104124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. https://aclanthology.org/2021.naacl-main.324/. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, MingWei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. https://aclanthology.org/Q19-1026/. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. Factuality enhanced language models for open-ended text generation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3458634599. Curran Associates, Inc., 2022. https://proceedings.neurips.cc/paper_files/paper/2022/file/ df438caa36714f69277daa92d608dd63-Paper-Conference.pdf. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. https://aclanthology.org/ 2022.acl-long.229/. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49694983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. https://aclanthology.org/2020.acl-main.447/. Potsawee Manakul, Adian Liusie, and Mark Gales. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 8 Conference on Empirical Methods in Natural Language Processing, pages 90049017, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.557. https://aclanthology.org/2023. emnlp-main.557/. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.741. https://aclanthology.org/2023.emnlp-main.741/. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 48854901, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.441. https://aclanthology.org/2020.acl-main.441/. National Library of Medicine. Pubmed baseline 2023 repository, 2023. https://lhncbc.nlm.nih.gov/ii/information/ MBR.html. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023. Abhilasha Ravichander, Shrusti Ghela, David Wadden, and Yejin Choi. Halogen: Fantastic llm hallucinations and where to find them. arXiv preprint arXiv:2501.08292, 2025. Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 9126091299. Curran Associates, Inc., 2024. https://proceedings.neurips.cc/paper_files/paper/ 2024/file/a5d8aba27dfef4e849e8cb03fb87a954-Paper-Conference.pdf. Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o. Yixiao Song, Yekyung Kim, and Mohit Iyyer. VeriScore: Evaluating the factuality of verifiable claims in long-form text generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 94479474, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.552. https://aclanthology.org/2024.findings-emnlp.552/. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: large-scale dataset for fact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. https://aclanthology.org/N18-1074/. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214, 2023. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Padataset for training large language models. quet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 116462116492. Curran Associates, Inc., 2024. https://proceedings.neurips.cc/paper_files/paper/2024/ file/d34497330b1fd6530f7afd86d0df9f76-Paper-Datasets_and_Benchmarks_Track.pdf. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024a. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8075680827. Curran Associates, Inc., 2024b. https://proceedings.neurips.cc/paper_ files/paper/2024/file/937ae0e83eb08d2cb8627fe1def8c751-Paper-Conference.pdf. Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. https://openreview.net/forum?id=Jvxa8adr3iY. Yiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, Han Fang, Carolyn Rose, et al. Improving model factuality with fine-grained critique-based evaluator. arXiv preprint arXiv:2410.18359, 2024. Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. Selfalignment for factuality: Mitigating hallucinations in LLMs via self-evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19461965, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.107. https://aclanthology.org/2024.acl-long.107/."
        },
        {
            "title": "Prompt for Generating Initial Questions",
            "content": "Instructions: 1. Given list of documents, you are tasked to ask few generic, fact-seeking question about \"{object}\" that are challenging yet still answerable. 2. Ensure the question prompts long-form responses of approximately 1024 tokens and avoiding seeking subjective opinions or predictions about future trends. 3. Avoid asking time-sensitive questions; instead of using terms like \"now\", \"today\", or \"current,\" specify the exact date. Dates are not needed for events that have already occurred. 4. Eliminate any ambiguity in the question, especially regarding the names of events or individuals, as multiple entities may share the same name. 5. Make sure the question is self-contained and does not reference the text provided in these instructions and the relevant documents, or in prior generated questions. 6. Avoid asking \"why\" and \"how\" questions. 7. Avoid asking questions that require knowledge beyond 2019. 8. Avoid including too many details in the question as it may leaks relevant information. 9. Instead of providing the exact date, simply mention the year an event took place. 10. Try your best to make sure that the questions can be answered using the documents, and it's better to draw information from multiple documents instead of depending on just one. However, if the provided documents lack relevant information, simply reply with \"No relevant information!\". 11. Avoid asking questions about the impact, influence, implication, or significance of certain events; instead, ask questions like those seeking details of an event, the features of an object, or the biography of person. 12. Please prepend \"Question: \" to the start of each question that is asked. EXAMPLES: Question: What is the Stanford Prison Experiment? Question: Explain the use of the Atomic Bomb on during World War II. Question: What is the Facebook's data privacy scandal in 2018? Question: Tell me about the Thalidomide drug scandal in the 1950s and 1960s. Question: What unfolded during the Oklahoma City bombing in 1995? The examples above are provided to help you understand the high-level requirements of the task. You do not need to reuse the exact phrases or follow the same style of the example questions. Documents: {docs}"
        },
        {
            "title": "B Model Output Lengths",
            "content": "We report the average number of sentences and the average number of claims (produced by VeriScore) in model outputs across different benchmarks, shown in Figure 7 and Figure 8, respectively. Overall, except for LongFact, models generate outputs of similar length across the various benchmarks."
        },
        {
            "title": "C Human Edits in Prompt Annotations",
            "content": "We present the distribution of human edit types made during prompt annotation in Figure 9."
        },
        {
            "title": "Prompt for Generating Responses",
            "content": "Instructions: 1. Please respond to the following question. 2. In the response, provide as many specific details and examples as possible (such as names of people, numbers, events, locations, dates, times, etc.) Question: {question} Prompt for Generating Retrieval-Augmented Responses Instructions: 1. Please respond to the following question based on the provided documents. 2. In the response, provide as many specific details and examples as possible (such as names of people, numbers, events, locations, dates, times, etc.) Question: {question} Relevant Documents: {docs}"
        },
        {
            "title": "Prompt for Categorizing Text",
            "content": "Given short text, determine the single most appropriate category from the following options: TV Shows, Music, Sports, Geography, Culture, Art, Politics, Science & Technology, Video games, History, Medical, Legal, Equipment, Biology & Nature, and Other. When generating the category, use new line and append \"Category: \" to the begining of the line. Below are the defitions of each category and the text. Category Definition: - TV Shows: Assign prompts related to television series, including specific shows, characters, or networks. - Music: Use this category for prompts about songs, artists, albums, genres, or music events. - Sports: Categorize prompts that involve athletic activities, teams, players, or sporting events. - Geography: This category is for prompts about locations, countries, cities, landmarks, or geographical features. - Art: Assign prompts related to visual arts, including painting, sculpture, photography, or art movements. - Politics: This category should cover political ideologies, political parties, elections, governance, policy-making, and political figures. It involves the processes and activities associated with government and decision-making at various levels. - Legal: This category should focus on the judicial system, laws, legal processes, court cases, and the roles of legal professionals. It deals with the application and interpretation of laws and legal principles. - Science & Technology: Categorize prompts that involve scientific concepts, discoveries, technological advancements, or related figures. - Video Games: This category is for prompts about video games, gaming consoles, developers, or gaming culture. - History: Assign prompts related to historical events, figures, periods, or artifacts. - Medical: Use this category for prompts about health, medicine, diseases, treatments, or medical professionals. - Equipment: This category includes devices and machinery used in various applications, such as cameras, vehicles, consumer electronics, industrial machinery, and tools. - Culture: This cateogry encompasses customs, traditions, social behaviors, and shared values of different groups and societies, including cultural practices, languages, cuisine, fashion, and cultural heritage. - Biology & Nature: This category covers prompts related to living organisms, their environments, and ecological interactions. It includes topics on species, ecosystems, conservation, and biodiversity. - Other: Use this category for prompts that do not fit into any of the specified categories. Text: {text}"
        },
        {
            "title": "Prompt for Converting Prompts into Atomic Prompts",
            "content": "Please extract the proper nouns from the following question and create questions that inquire about each specific proper noun. The questions should be concise and general, such as \"Tell me about Computer Science.\" Ensure that you ask exactly one question for each proper noun found in the original question. The questions should be clear and comprehensible on their own, without any ambiguity. Begin each question with \"Question: \". \"{question}\" 13 Figure 7 Average numbers of claims (produced by VeriScore) for model outputs. Figure 8 Average numbers of sentences for model outputs. Figure 9 Distributions of the types of human edits."
        }
    ],
    "affiliations": [
        "Meta"
    ]
}