{
    "paper_title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning",
    "authors": [
        "Zimu Lu",
        "Houxing Ren",
        "Yunqiao Yang",
        "Ke Wang",
        "Zhuofan Zong",
        "Junting Pan",
        "Mingjie Zhan",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 4 4 6 2 2 . 9 0 5 2 : r WEBGEN-AGENT: ENHANCING INTERACTIVE WEBSITE GENERATION WITH MULTI-LEVEL FEEDBACK AND STEP-LEVEL REINFORCEMENT LEARNING Zimu Lu1, Houxing Ren1, Yunqiao Yang1, Ke Wang1, Zhuofan Zong1 Junting Pan1, Mingjie Zhan1, Hongsheng Li1,2 1Multimedia Laboratory (MMLab), The Chinese University of Hong Kong, 2Ace Robotics luzimu@link.cuhk.edu.hk zhanmingjie@sensetime.com hsli@ee.cuhk.edu.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUIagent testing of the websites are generated by visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce Step-GRPO with Screenshot and GUI-agent Feedback to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide dense and reliable process supervision signal, which effectively improves the models website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our StepGRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7. We release the WebGen-Agent workflow code, along with the training code, data, and model weights at https://github.com/mnluzimu/WebGen-Agent."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent studies on code agents have shown great advancements in repository-level code-generation tasks, such as fixing GitHub issues (Yang et al., 2024b) and implementing new features (Miserendino et al., 2025). However, for tasks like website code generation, which depend heavily on visual aesthetics and the fluency of user interactions, current code-agent systems fail to fully capture the actual quality of the generated codebase, because they mostly rely on simple code-execution feedback. This limitation can lead to various rendering and functional problems in the generated web applications, such as misaligned components, disharmonious coloring, unresponsive buttons, and broken links. To enable the code agent to effectively handle such tasks, we introduce WebGen-Agent, codegeneration system that generates websites from natural-language instructions that specify appearEqual contribution Corresponding author 1 ance and functional requirements, thus offering highly automated website-development process. To ensure that the generated websites meet both functional requirements and aesthetic standards, we leverage both execution feedback and visual feedback to refine the project. Specifically, we leverage visual language model (VLM) to assess the visual appeal and aesthetic quality of the current website, and graphical user interface (GUI) agent to evaluate the correctness and intended functionality of the websites codebase, thereby gathering accurate information and providing targeted suggestions. By iteratively applying this feedback and editing the codebase, WebGen-Agent builds websites with appealing designs and smooth interactive functionality. As shown in Fig. 1, WebGen-Agent adopts an iterative, multi-step paradigm in which each step consists of three actions: code generation, code execution, and feedback gathering. The agent begins each step by creating and editing files in the codebase in manner similar to Bolt.diy (stackblitz labs, 2024). During code execution, dependencies are installed, and the website service is started. If execution emits errors, the errors are returned to the agent, which starts the next step to fix them. If five consecutive erroneous steps occur, the agent backtracks to previous non-erroneous step. In the feedback-gathering process, screenshot of the landing page of the website is captured first. VLM then provides description and an appearance score based on the screenshot. If the screenshot has room for improvement, the model supplies suggestions, which are implemented in the subsequent step to explicitly refine the websites visual aesthetics. Otherwise, GUI-agent session is initiated to explore the website, which evaluates the functional requirements and generates corresponding feedback. If the testing is successful, the task is complete; otherwise, suggestions for fixing the website are generated, and the agent can edit the codebase in the next step. At the end of the task trajectory, the best step is selected on the basis of the screenshot and GUI-agent scores, and the codebase is restored to the state of that step. Based on the pipeline, various models achieve better performance on WebGen-Bench (Lu et al., 2025b), consistently outperforming other code agents. Remarkably, Claude-3.5-Sonnet improves its accuracy from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming Bolt.diy. To equip code agents with enhanced reasoning abilities, we further propose Step-GRPO with Screenshot and GUI-agent Feedback. Given an instruction, multiple WebGen-Agent trajectories are generated. Each step in an agent trajectory is accompanied by screenshot score and GUI-agent testing score, and an accurate and reliable step-level reward can be computed by summing these two scores. This dual supervision of website appearance and functionality effectively optimizes the model to generate high-quality website codebases, providing stepwise, process-level guidance for the agent trajectory. Training Qwen2.5-Coder-7B-Instruct model with this StepGRPO approach increases the accuracy from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7 on WebGen-Bench, greatly improving both the functionality and the appearance of the generated websites. We name the trained family of models WebGenAgent-LM. Our contributions include: We propose WebGen-Agent, code-agent system that leverages screenshots and GUI-agent testing to provide feedback signals and iteratively improve the quality of generated websites. We introduce Step-GRPO with Screenshot and GUI-agent Feedback, which uses screenshots and GUI-agent scores as step-level supervision in the GRPO training process, significantly improving the performance of smaller open-source models. Extensive experiments demonstrate the effectiveness of the proposed system. The system increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming Bolt.diy. Our training approach also increases the accuracy of Qwen2.5Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7."
        },
        {
            "title": "2 METHOD",
            "content": "In this section, we first introduce WebGen-Agent, novel website generation system that leverages screenshots and GUI-agent testing as reliable feedback to iteratively refine both the appearance and functionalities of the generated website with coding LLM. Building on the reliable visual scores produced by WebGen-Agent, we then propose Step-GRPO with Screenshot and GUI-agent Feedback, method that uses these scores to provide process supervision during GRPO training. This system significantly enhances language models ability to generate high-quality websites. 2 Figure 1: Iterative website generation with screenshotand GUI-agent-based feedback. backtracking and best-step-selection mechanism is applied on the basis of the screenshot and GUI-agent testing scores."
        },
        {
            "title": "2.1 WEBGEN-AGENT WORKFLOW",
            "content": "The WebGen-Agent workflow consists of multiple steps, with each step including code generation, code execution, and feedback gathering. As shown in Fig. 1, the agent trajectory starts from website generation instruction (I), denoted as = [I], and an empty codebase C0. The instruction is created by concatenating system prompt similar to that of Bolt.diy (stackblitz labs, 2024) with the user-provided website-generation request. coding LLM acting as the engine of the agent generates code C1 to edit the codebase, resulting in C1. Then, the dependencies of the codebase are installed, and the website service is started. The code execution output is denoted as O1, which contains both stdout and stderr. If the dependency installation or service initialization fails, the output message O1 is returned to the agent as feedback, so that the agent can fix the error in the next step. If no error occurs, screenshot of the website is captured and presented to VLM (e.g. Qwen2.5-VL32B; Bai et al. (2025)), which is requested to provide description of the screenshot and, if needed, suggestions to improve the websites appearance. The prompt for acquiring screenshot feedback is provided in Fig. 4 of Appendix C. score of the website appearance based on the screenshot is also generated and, together with the description and suggestions, composes the screenshot feedback. The feedback can be denoted as: Fshot = (cid:10)Description, Scoreshot, Suggestionsshot (cid:11) (1) Fshot is used to reflects the integrity and aesthetics of the websites appearance. Here, separate VLM is used besides the coding LLM to make the system more cost-effective, as we observe that relatively small open-source VLM is sufficient for the task, while the code generation requires an LLM with strong coding abilities. We use Qwen2.5-VL-32B-Instruct as the VLM in our experiments unless stated otherwise. The code execution and screenshot feedback are appended to the agent trajectory, resulting in = [I, C1, O1, Fshot,1]. Then, the agent judges whether the websites appearance is satisfactory based on the trajectory. If it is unsatisfactory, the agent continues to generate code C2 to improve the websites appearance. Otherwise, the agent initiates GUI-agent testing session, generating an instruction for the GUI-agent to explore various website functionalities specified in the instruction I, resulting in GUI-agent testing trajectory. The prompt used to generate the GUI-agent instructions is shown in Fig. 6 of Appendix C. It instructs the model to produce GUI-agent instruction that comprehensively checks all website-development requirements and includes one-shot example. As shown in Tab. 6 of Appendix F, manual inspection indicates that 98.3% of the sampled instructions achieve high coverage of the requirements. Based on the GUI-agent testing result, the LLM acting as the engine of the agent judges whether the testing is successful and provides score, denoted as Scoregui. The prompt for acquiring the GUI-agent testing feedback is provided in Fig. 7 of Appendix C. If the testing result is unsatisfactory, suggestions are also made to improve the functionality. Thus, the GUI-agent testing feedback can be denoted as: Fgui = (cid:10)Scoregui, Suggestionsgui (cid:11) (2) is also appended to the trajectory, resulting in = [I, C1, O1, Fshot,1, Fgui,1] = Fgui [I, C1, O1, F1]. Here, F1 denotes [Fshot,1, Fgui,1]. In this way, WebGen-Agent continues to improve the appearance and functionality of the website, resulting in trajectory , denoted as = [I, C1, O1, F1, C2, O2, F2, . . . , CK, OK, FK]. 3 Figure 2: Step-GRPO with Screenshot and GUI-agent Feedback. Multiple WebGen-Agent trajectories are produced, and the reward for each step is computed by summing the screenshot score and the GUI-agent score. The process ends when the website passes the GUI-agent testing, or the maximum iteration number is reached. During the iterations, at step {1, 2, . . . }, the codebase state Ci, the edit Ci, together with the Scoreshot,i and Scoregui,i, are stored in memory list. If five consecutive steps contain code execution errors, backtracking mechanism is triggered, and the agent trajectory and the codebase are returned to the state at the best previous step. The best previous step is selected by first choosing the steps with the highest Scoregui, and then among these steps, the ones with the highest Scoreshot are chosen. If there are still more than one chosen step, then the latest one among them is selected. Considering that later code edits might not always improve the previous codebase, at the end of the agent workflow, the best step among all the steps is selected in the same way as mentioned above. more detailed algorithmic presentation can be found in Appendix and example trajectories are presented in Appendix D."
        },
        {
            "title": "2.2 STEP-GRPO WITH SCREENSHOT AND GUI-AGENT FEEDBACK",
            "content": "While using strong proprietary models as the engine LLM in WebGen-Agent can produce high performance, the agent workflow would be more cost-efficient if smaller open-source models of 7B8B parameters can be used instead. However, current small open-source language models still lag behind proprietary models in website code generation. Therefore, we introduce Step-GRPO with Screenshot and GUI-agent Feedback, leveraging the Scoreshot and Scoregui inherently produced in the WebGen-Agent workflow to train them with step-level process supervision in GRPO training. Before the GRPO-based training, we first perform light supervised fine-tuning (SFT) using approximately 700 WebGen-Agent trajectories generated by DeepSeek-V3, training for one epoch to serve as warm start. Then, Step-GRPO is performed on the fine-tuned model. The Step-GRPO training objective can be written as: JGRP O(θ) = [qP (Q),{oi}G i=1πθold (Oq)] (cid:26) 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 min (cid:20) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) ˆAi,t, clip (cid:18) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:21)(cid:27) , ˆAi,t (3) Here, denotes the website generation instruction, and {oi}G i=1 denotes the group of trajectories generated from the instruction q. We remove the KL loss to encourage the model to more freely adapt its behavior to the reward signals (Qian et al., 2025). oi can be denoted as [C1, O1, F1, . . . , CKi, OKi, FKi]. Different from the naive GRPO, which sets the advantages on all tokens in trajectory to the same value, the Step-GRPO sets advantages on tokens in different steps to different values. In our work, the GRPO loss is only applied to the model outputs C1, C2, . . . , CK. We denote the reward of all tokens in the j-th step of o(i) as r(i) , which is computed by summing the Scoreshot and Scoregui of that step, generated in the WebGen-Agent workflow: = Score(i) r(i) shot,j + Score(i) gui,j (4) The rewards for all steps in the trajectories sampled from can be written as = {{r(1) }}. The advantage for step of the i-th trajectory is com- }, . . . , {r(G) , , r(G) KG 1 , , r(1) K1 r(i) mean(R) std(R) . ˆA(i) denotes the advantage puted by standardizing its immediate reward: ˆA(i) = 4 of o(i) at the j-th step. We do not accumulate normalized rewards from future steps as in Shao et al. (2024), because in the website-generation task Scoreshot and Scoregui directly reflect the quality of the website at the current step, which is more appropriate for representing the desirability of the current code. The Step-GRPO training process is illustrated in Fig. 2. This Step-GPPO method, with screenshot and GUI-agent feedback, incorporates accurate step-level supervision and effectively helps the model learn to generate websites with an appealing appearance and smooth functionality."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In this section, we first present the performance of WebGen-Agent on WebGen-Bench using variety of proprietary and open-source LLMs, as well as models trained using Step-GRPO with Screenshot and GUI-agent Feedback. Then, we conduct comprehensive ablation studies on the design choices in the WebGen-Agent workflow and the Step-GRPO training process."
        },
        {
            "title": "3.1 MAIN RESULTS",
            "content": "Benchmark Dataset and Baselines. We evaluate WebGen-Agent using WebGen-Bench (Lu et al., 2025b), benchmark containing 101 website-generation instructions in natural language and 647 GUI-agent test cases, covering wide range of web applications. Following Lu et al. (2025b), we use Qwen2.5-VL-32B-Instruct (Bai et al., 2025) in functional testing and GPT-4o (Hurst et al., 2024) in appearance evaluation. We compare WebGen-Agent with three other popular code agents: OpenHands (Wang et al., 2024), Aider (Aider-AI, 2024), and Bolt.diy (stackblitz labs, 2024). We present the results of OpenHands and Aider in combination with DeepSeek-V3 (Liu et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024), and DeepSeek-R1 (Guo et al., 2025a), as well as the results of Bolt.diy with DeepSeek-V3 (Liu et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024), DeepSeekR1 (Guo et al., 2025a), GPT-4o (Hurst et al., 2024), o3-mini (OpenAI, 2025b), Qwen2.5-Coder32B (Hui et al., 2024), Qwen2.5-72B-Instruct (Yang et al., 2024a), WebGen-LM-7B, WebGen-LM14B, and WebGen-LM-32B (Lu et al., 2025b). The values are taken from (Lu et al., 2025b). Models and WebGen-Agent Inference Settings. We evaluate WebGen-Agent using wide range of proprietary and open-source models as coding LLMs. The proprietary models we tested include Claude-3.5-Sonnet (Anthropic, 2024), DeepSeek-R1 (Guo et al., 2025a), DeepSeek-V3 (Liu et al., 2024), o3 (OpenAI, 2025a), Claude-4-Sonnet (Anthropic, 2025), Gemini-2.5-Pro (Comanici et al., 2025), and Qwen3-Coder-480B-A35B-Instruct (Yang et al., 2025a). The smaller open-source models we tested include Qwen2.5-Coder-32B-Instruct (Hui et al., 2024), Qwen3-Coder-30B-A3BInstruct (Yang et al., 2025a), Qwen2.5-72B-Instruct (Yang et al., 2024a), Qwen2.5-Coder-7BInstruct (Hui et al., 2024), and Qwen3-8B (Yang et al., 2025a), as well as 7B and 8B WebGenAgentLM models trained with SFT and Step-GRPO. The maximum number of iterations is set to 20, and the model temperature is set to 0.5. We use Qwen2.5-VL-32B-Instruct as the feedback VLM for screenshot and GUI-agent testing in all the experiments. Analysis of the maximum iteration number is presented in Appendix H. Training Settings. We first fine-tune Qwen2.5-Coder-7B-Instruct and Qwen3-8B on approximately seven hundred WebGen-Agent trajectories collected from DeepSeek-V3 for one epoch with learning rate of 4e-5 and batch size of 32. This results in the models WebGenAgent-LM-7B-SFT and WebGenAgent-LM-8B-SFT, which serve as warm start for Step-GRPO. We then train these SFT models using Step-GRPO on five hundred website generation instructions randomly sampled from WebGen-Instruct for one epoch, resulting in the final models WebGenAgent-LM-7B-StepGRPO and WebGenAgent-LM-8B-Step-GRPO. The learning rate is set to 1e-6 with batch size of 16. For each instruction, we sample 5 outputs. Ambiguous or underspecified instructions are manually filtered out. We observe that this relatively small number of high-quality instructions is sufficient for Step-GRPO training, likely due to the reliable step-level feedback from screenshots and the GUI agent. Training on more samples is costly and does not yield noticeable gains. Results. The WebGen-Agent test results are presented in Tab. 1. Based on the results, we make the following observations: (1) WebGen-Agent demonstrates superior performance across various proprietary models compared to other code agent systems. On Claude-3.5-Sonnet, DeepSeek-R1, and DeepSeek-V3, WebGen-Agent significantly outperforms OpenHands, Aider, and Bolt.diy when 5 Table 1: The performance of WebGen-Agent with various proprietary and open-source models on WebGen-Bench (Lu et al., 2025b), compared with other code agent systems. The highest Accuracy and Appearance Score are highlighted in bold. Test Name Yes Partial No Start Failed Accuracy Appearance Score Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 GPT-4o o3-mini Qwen2.5-Coder-32B-Inst. Qwen2.5-72B-Inst. WebGen-LM-7B WebGen-LM-14B WebGen-LM-32B Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 o3 Gemini-2.5-Pro Claude-4-Sonnet Qwen3-Coder-480B-A35B-Inst. Qwen2.5-Coder-32B-Inst. Qwen3-Coder-30B-A3B-Inst. Qwen2.5-72B-Inst. OpenHands 8.3 3.4 3.2 Aider 5.9 8.7 3.1 Bolt.diy 7.6 6.2 4.5 4.8 3.4 2.6 3.6 7.1 8.7 8. 18.1 8.5 7.4 19.9 23.3 12.5 22.6 24.7 18.5 10.4 17.9 8.2 12.1 24.9 25.0 34.2 WebGen-Agent Proprietary Models 45.6 40.2 46.1 45.7 44.5 48.8 50. 12.7 12.4 13.1 11.9 12.7 15.3 15.3 58.6 60.4 73.9 42.0 44.5 54.3 64.1 64.3 73.9 64.5 40.0 81.8 80.7 68.0 66.3 57.8 40.6 45.9 40.6 41.6 39.4 33.4 34.2 Open-Source Models (30B72B) 26.7 45.7 29.1 10.5 14.1 13.8 60.3 40.2 57.2 Open-Source Models (7B8B) Qwen2.5-Coder-7B-Inst. WebGenAgent-LM-7B-SFT WebGenAgent-LM-7B-Step-GRPO Qwen3-8B WebGenAgent-LM-8B-SFT WebGenAgent-LM-8B-Step-GRPO 10.0 33.8 40.2 29.5 32.8 37.4 4.8 10.2 10.5 9.1 11.6 12.1 60.9 56.0 49.3 61.4 55.6 50. 15.0 27.7 15.5 32.1 23.5 30.1 5.7 4.8 3.1 20.4 38.6 7.4 3.7 0.0 0.0 0.0 1.1 1.5 0.2 0.8 3.4 2.5 0.0 2.5 0.0 0.0 24.3 0.0 0. 0.0 0.0 0.0 22.3 10.2 9.0 22.9 27.7 14.1 26.4 27.8 20.8 12.8 19.6 9.5 13.8 28.4 29.4 38.2 51.9 46.4 52.6 51.7 50.9 56.5 58.2 32.0 52.8 35. 12.4 38.9 45.4 34.1 38.6 43.4 2.6 1.4 1.5 1.9 2.7 1.3 3.0 2.5 2.0 1.5 1.6 1.1 1.4 2.5 2.5 2.8 3.9 3.8 3.8 3.5 3.8 4.1 4. 3.3 4.0 3.4 1.6 3.4 3.7 3.2 3.4 3.6 using the same model. Across all seven proprietary models from five different providers, WebGenAgent achieves consistently high performance, demonstrating the generalizability of the method. Qwen3-Coder-480B-A35B-Instruct achieves the highest accuracy of 58.2% and an appearance score of 4.3. (2) With 30B72B sized open-source models, WebGen-Agent also achieves high performance. On Qwen2.5-Coder-32B-Instruct and Qwen2.5-72B-Instruct, WebGen-Agent outperforms the previous state-of-the-art, Bolt.diy, by 22.5% and 22.1% in accuracy, and by 2.2 and 2.0 in appearance scores, respectively. Qwen3-Coder-30B-A3B-Instruct achieves the best performance among (3) Step-GRPO with 30B72B models, with 52.8% accuracy and an appearance score of 4.0. 6 Table 2: Ablation study on the WebGen-Agent workflow. The configuration starts from executiononly and incrementally adds capabilities. Test Name Yes Partial No Execution-only Screenshot Screenshot+GUI-agent Screenshot+GUI-agent+Backtrack Screenshot+GUI-agent+Backtrack+Select-best 39.7 41.3 43.0 45.6 46.1 12.4 10.7 13.9 11.1 13. 43.3 45.9 41.3 43.1 40.6 Start Failed 4.6 2.2 1.9 0.2 0.2 Accuracy Appearance Score 3.0 3.6 3.4 3.7 3. 45.9 46.6 49.9 51.2 52.6 Table 3: Training strategy ablation on the Qwen2.5-Coder-7B-Instruct model. The configuration starts from the raw model and successively introduces supervised fine-tuning (SFT) and various reinforcement-learning variants. Test Name Yes Partial No No Additional Training SFT for 1 Epoch SFT for 2 Epochs Naive Outcome GRPO Step-GRPO w/ Cumulative Advantage Step-GRPO w/ Screenshot Reward Only Step-GRPO w/ GUI-agent Reward Only Step-GRPO w/ Screenshot+GUI-agent (ours) 10.0 33.8 32. 38.0 32.6 34.9 34.8 40.2 4.8 10.2 14.2 9.0 12. 10.5 11.3 10.5 60.9 56.0 53.5 53.0 55.2 53.9 53. 49.3 Start Failed 24.3 0.0 0.2 0.0 0.0 0.6 0. 0.0 Accuracy Appearance Score 12.4 38.9 39.3 42.5 38. 40.2 40.4 45.4 1.6 3.4 3.4 3.5 3.5 3.5 3. 3.7 Screenshot and GUI-agent Feedback significantly improves the performance of Qwen2.5-Coder7B-Instruct and Qwen3-8B. For Qwen2.5-Coder-7B-Instruct, SFT improves accuracy from 12.4% to 38.9% and the appearance score from 1.6 to 3.4; Step-GRPO further improves accuracy from 38.9% to 45.4% and the appearance score from 3.4 to 3.7. For Qwen3-8B, SFT improves accuracy from 34.1% to 38.6% and the appearance score from 3.2 to 3.4; Step-GRPO further improves accuracy from 38.6% to 43.4% and the appearance score from 3.4 to 3.6. Qualitative analysis of SFT and Step-GRPOs effect in improving the performance is presented in Appendix I. These results demonstrate the effectiveness of our training method in improving both the functionality and appearance of the generated websites. Categorical results are presented in Tab. 7 of Appendix G."
        },
        {
            "title": "3.2 ABLATION STUDIES",
            "content": "Analysis of the WebGen-Agent Workflow. We analyze various design choices in the WebGenAgent workflow in Tab. 2. We incrementally add the designs, starting from using only the code execution response messages (Execution-only), then gradually adding screenshot feedback Fshot (Screenshot), GUI-agent testing feedback Fgui (Screenshot+GUI-agent), the backtracking mechanism (Screenshot+GUI-agent+Backtrack), and finally the select-best mechanism (Screenshot+GUI-agent+Backtrack+Select-best), which makes up the full WebGen-Agent workflow. As shown in Tab. 2, each of these designs yields notable gains in accuracy and appearance. The GUI-agent testing contributes the largest accuracy gain of 3.3%, showing its effectiveness in guiding the functionality of the generated websites. The addition of screenshot feedback greatly improves the appearance score, raising it from 3.0 to 3.6, demonstrating its effect in enhancing website appearance. Adding GUI-agent testing slightly impairs the appearance score, likely because modifying the codebase for functional fulfillment sometimes damages the website appearance or causes errors. This negative effect is mitigated by the addition of the backtracking and select-best mechanisms. Qualitative analysis of the effect of screenshot and GUI-agent feedback is provided in Appendix J. Also, as shown in Tab. 6 of Appendix F, manual inspection indicates that 98.3% of the sampled instructions achieve high coverage of the requirements. 7 Figure 3: Comparison of the average file count and average line count among the original, SFT, and Step-GRPO models for Qwen2.5-Coder-7B-Instruct and Qwen3-8B. Table 4: Impact of the feedback VLM on the performance of WebGen-Agent on WebGen-Bench. The highest Accuracy and Appearance Score are highlighted in bold. Coding LLM Feedback VLM Yes Partial No Start Failed Accuracy Appearance Qwen2.5-VL-32B-Inst. Qwen2.5-VL-32B-Inst. DeepSeek-V3 DeepSeek-V3 GPT-4o Qwen2.5-VL-32B-Inst. 4.5 46.4 46.1 2.2 11.4 13.1 78.8 42.0 40.6 14.5 0.2 0.2 5.6 52.1 52.6 Score 1.3 3.6 3.8 Analysis of Step-GRPO with Screenshot and GUI-agent Feedback. We analyze the design choices in the Step-GRPO with Screenshot and GUI-agent Feedback training process in Tab. 3. The first line shows the result of the Qwen2.5-Coder-7B-Instruct model with no additional training. The analysis based on Tab. 3 is as follows: (1) The second and third lines present SFT training for one epoch and two epochs, showing that training for two epochs does not notably improve performance compared to training for only one epoch. Therefore, we train for only one epoch in the SFT stage. (2) The fourth and fifth lines show the results of using naive outcome GRPO and StepGRPO with cumulative advantage. The rewards in these two variants are the same as in our final design (Scoreshot +Scoregui); only the advantage computation method differs. Naive outcome GRPO uses the maximum value of the step-level rewards in trajectory as the outcome reward, setting the advantages to the normalized outcome rewards. Step-GRPO with cumulative advantage calculates the advantage of each token as the sum of the normalized rewards from the subsequent steps, as introduced in Shao et al. (2024). Both GRPO advantage computation variants perform notably worse than our final Step-GRPO setting. (3) The sixth and seventh lines present the results of using only the screenshot scores (Scoreshot) or only the GUI-agent testing scores (Scoregui) as the rewards. Both are lower than using Scoreshot + Scoregui, demonstrating the necessity of incorporating both types of feedback. We also gather statistics on the average file count and average line count for the Original, SFT, and Step-GRPO models, as shown in Fig. 3. For both Qwen2.5-Coder-7B-Instruct and Qwen3-8B, the average file count and average line count consistently increase after SFT and Step-GRPO training. This shows that both the SFT and Step-GRPO stages increase the complexity of the generated websites, which is consistent with their improved performance. Analysis of the Coding LLM and Feedback VLM. We analyze the choice of the coding LLM and feedback VLM in Tab. 4. In our experiments, we use relatively small and inexpensive VLM, Qwen2.5-VL-32B-Instruct, to provide screenshot and GUI-agent testing feedback, while employing strong LLM capable of generating high-quality code, such as DeepSeek-V3, as the coding LLM. As shown in the second row of Tab. 4, replacing Qwen2.5-VL-32B-Instruct with proprietary VLM, GPT-4o, as the feedback VLM does not notably improve the accuracy or the appearance score. This demonstrates that Qwen2.5-VL-32B-Instruct is already sufficient for providing accurate screenshot and GUI-agent testing feedback, while being more cost-effective than proprietary VLMs. As shown in the first row of Tab. 4, replacing DeepSeek-V3 with Qwen2.5-VL-32B-Instruct results in significantly worse performance, indicating that the coding LLM cannot be replaced by smaller open-source VLMs. The design choice of decoupling the coding LLM and feedback VLM ensures that code is generated by strong LLM to maintain quality, while screenshot and GUI-agent testing feedback is handled by smaller open-source VLM for cost efficiency. Further analysis of the ac8 curacy of the screenshot and GUI-agent scores provided by the feedback VLM is included in Tab. 5 of Appendix E, demonstrating the reliability of the scores."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Visual Code Generation. Code generation that is associated with visual effects exists in wide range of application scenarios, such as web page development (Lu et al., 2025b; Xu et al., 2025) and GitHub-issue fixing (Yang et al., 2024d; Guo et al., 2025b). Previous work has proposed various ways to treat visual elements in code generation and other reasoning-intensive tasks (Su et al., 2025), such as generating code to represent images in problem statements (Huang et al., 2025; Wang et al., 2025b) and using natural language to describe images(Zhang et al., 2024b). We also apply natural language descriptions when providing screenshot feedback. More related to our work, line of studies(Guo et al., 2024; Si et al., 2025; Yun et al., 2024; Beltramelli, 2017; Sun et al., 2025; Gui et al., 2025; Laurencon et al., 2024; Wan et al., 2024) explores MLLMs ability to reconstruct singlefile HTML code from webpage screenshots. Other studies benchmark MLLMs performance in implementing interactive elements in existing web projects (Xiao et al., 2025a) or performing web development tasks in pre-defined sequential manner with detailed technical settings (Xiao et al., 2025b; Xu et al., 2025). The web development tasks in these works are often solved in single HTML file (Zhang et al., 2025a) or contain rigid pipelines (Xu et al., 2025), which are more suitable for testing MLLMs rather than code agents for end-to-end, repository-level website development, as proposed in our work. Therefore, we evaluate our agent workflow with WebGen-Bench (Lu et al., 2025b), which measures code agents ability to create multi-file website codebases from scratch and includes diverse website generation instructions. Code Agents. Equipped with various tools and powered by LLMs (Soni et al., 2025; Yao et al., 2023; Zhang et al., 2024a), code agents can perform variety of tasks, such as developing websites (Lu et al., 2025b) and fixing GitHub issues (Jimenez et al., 2024; Yang et al., 2024c). Some code agents specialize in specific field, such as bug fixing (Zhang et al., 2024c) or machine learning (Jiang et al., 2025). Similar to our work, Bolt.diy (stackblitz labs, 2024) specializes in multi-file website generation. Others, such as OpenHands (Wang et al., 2024) and Aider (Aider-AI, 2024), are general-purpose code agents that are not limited to single field, though their performance on specific task might not match that of specialist code agents (Lu et al., 2025b). Our WebGen-Agent is code agent specializing in end-to-end website generation, with screenshot feedback and GUI-agent testing features specifically designed for this task, achieving state-of-the-art performance. Fine-tuning and Reinforcement Learning for Agents. Supervised fine-tuning (Pan et al., 2025; Yang et al., 2025b) and reinforcement learning (Dong et al., 2025; Qian et al., 2025) are two methods widely used to improve the agentic and tool-calling abilities of LLMs. In the field of code agents, various works (Pan et al., 2025; Yang et al., 2025b; Zhang et al., 2025b; Wang et al., 2025a; Ma et al., 2024; Xie et al., 2025; Jain et al., 2025; Guo et al., 2025c; Ma et al., 2025a) leverage supervised fine-tuning combined with software engineering data synthesis and rejection sampling to improve the performance of open-source models. Similar to these works, we also use rejection sampling and supervised fine-tuning in the warm-up stage before the GRPO training. Other works use reinforcement learning with rewards acquired through comparison with the ground truth (Wei et al., 2025a; Ma et al., 2025c; Zhuang et al., 2025), determined by the code execution output (Gehring et al., 2025; Ma et al., 2025b; Golubev et al., 2025), or dependent on task success (Wei et al., 2025b; Lu et al., 2025a; Chen et al., 2025). These works either use outcome supervision, which provides sparse training signals, or require detailed ground truth to provide step supervision, which is rigid and difficult to obtain. In contrast to these methods, our work leverages screenshot and GUI-agent testing scores at each stepwhich are inherent in the WebGen-Agent pipelineto provide accurate step-level supervision in GRPO training."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce WebGen-Agent, code agent that leverages screenshot and GUI-agent testing feedback, combined with backtracking and select-best mechanisms, to iteratively generate websites with appealing appearance and smooth functionality. We also propose Step-GRPO with 9 Screenshot and GUI-agent Feedback, which leverages inherent screenshot and GUI-agent testing scores to provide step-level supervision in the GRPO training process. Testing WebGen-Agent on WebGen-Bench shows significant improvements across wide range of proprietary and open-source LLMs compared to other code agent systems. WebGen-Agent with Qwen3-Coder-480B-A35BInstruct achieves the best performance, with an accuracy of 58.2% and an appearance score of 4.3. Training Qwen2.5-Coder-7B-Instruct and Qwen3-8B first with supervised fine-tuning and then with Step-GRPO with Screenshot and GUI-agent Feedback notably improves accuracies and appearance scores, demonstrating the effectiveness of our training approach."
        },
        {
            "title": "6 REPRODUCIBILITY STATEMENT",
            "content": "To ensure reproducibility, we release the WebGen-Agent workflow code, along with the training code and data for Step-GRPO with Screenshot and GUI-Agent Feedback, as well as the weights of the WebGenAgent-LM models. The complete code base and datasets are provided in the supplementary material accompanying this paper. Details of the agent workflow and of all prompts used to deliver multi-level feedback are presented in Section 2.1, Appendix B, and Appendix C. The training procedure for Step-GRPO with Screenshot and GUI-Agent Feedback is described in Section 2.2 and Section 3.1. We also report manual inspection of the screenshot and GUI-agent scores in Appendix E, and we assess the comprehensiveness of the GUI-Agent testing instructions in Appendix F. Collectively, these resources ensure that our findings are transparent, robust, and independently verifiable."
        },
        {
            "title": "REFERENCES",
            "content": "Aider-AI. Ai pair programming in your terminal, 2024. URL https://github.com/ Aider-AI/aider. Accessed: 2025-04-22. Anthropic. Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic.com/ news/claude-3-5-sonnet. Accessed: 2025-04-22. Anthropic. Claude sonnet 4, 2025. URL https://www.anthropic.com/claude/sonnet. Accessed: 2025-08-11. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Tony Beltramelli. pix2code: Generating code from graphical user interface screenshot, 2017. URL https://arxiv.org/abs/1705.07962. Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, and Chuchu Fan. R1-code-interpreter: Training llms to reason with code via supervised and reinforcement learning, 2025. URL https://arxiv.org/abs/2505.21668. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic reinforced policy optimization, 2025. URL https://arxiv.org/ abs/2507.19849. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025. URL https://arxiv.org/abs/2410.02089. 10 Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, Sergei Skvortsov, and Boris Yangel. Training long-context, multi-turn software engineering agents with reinforcement learning, 2025. URL https://arxiv.org/abs/2508.03501. Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Bohua Chen, Yi Su, Dongping Chen, Siyuan Wu, Xing Zhou, Wenbin Jiang, Hai Jin, and Xiangliang Zhang. Webcode2m: real-world dataset In Proceedings of the ACM on Web Conference for code generation from webpage designs. 2025, WWW 25, pp. 18341845. ACM, April 2025. doi: 10.1145/3696410.3714889. URL http://dx.doi.org/10.1145/3696410.3714889. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Hongcheng Guo, Wei Zhang, Junhao Chen, Yaonan Gu, Jian Yang, Junjia Du, Binyuan Hui, Tianyu Liu, Jianxin Ma, Chang Zhou, and Zhoujun Li. Iw-bench: Evaluating large multimodal models for converting image-to-web, 2024. URL https://arxiv.org/abs/2409.18980. Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, and Zibin Zheng. Omnigirl: multilingual and multimodal benchmark for github issue resolution, 2025b. URL https://arxiv.org/abs/2505.04606. Lianghong Guo, Yanlin Wang, Caihua Li, Pengyu Yang, Jiachi Chen, Wei Tao, Yingtian Zou, Duyu Tang, and Zibin Zheng. Swe-factory: Your automated factory for issue resolution training data and evaluation benchmarks, 2025c. URL https://arxiv.org/abs/2506.10954. Kai Huang, Jian Zhang, Xiaofei Xie, and Chunyang Chen. Seeing is fixing: Cross-modal reasoning with multimodal llms for visual software issue fixing, 2025. URL https://arxiv.org/ abs/2506.16136. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2e-gym: Procedural environments and hybrid verifiers for scaling open-weights swe agents, 2025. URL https://arxiv.org/abs/2504.07164. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide: Ai-driven exploration in the space of code, 2025. URL https://arxiv. org/abs/2502.13138. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770. Hugo Laurencon, Leo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. URL https://arxiv.org/abs/2403.09029. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo:end-to-end policy optimization for gui agents with experience replay, 2025a. URL https://arxiv.org/abs/ 2505.16282. 11 Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Webgen-bench: Evaluating llms on generating interactive and functional websites from scratch, 2025b. URL https://arxiv.org/abs/2505. 03733. Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. Lingma swe-gpt: An open development-process-centric language model for automated software improvement, 2024. URL https://arxiv.org/abs/2411. 00622. Yingwei Ma, Yongbin Li, Yihong Dong, Xue Jiang, Rongyu Cao, Jue Chen, Fei Huang, and Binhua Li. Thinking longer, not larger: Enhancing software engineering agents via scaling test-time compute, 2025a. URL https://arxiv.org/abs/2503.23803. Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, and Bing Xie. Sorft: Issue resolving with subtask-oriented reinforced fine-tuning, 2025b. URL https://arxiv.org/ abs/2502.20127. Zexiong Ma, Chao Peng, Qunhong Zeng, Pengfei Gao, Yanzhen Zou, and Bing Xie. Tool-integrated reinforcement learning for repo deep search, 2025c. URL https://arxiv.org/abs/ 2508.03012. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn 1 million from real-world freelance software engineering?, 2025. URL https: //arxiv.org/abs/2502.12115. OpenAI. Introducing openai o3 and o4-mini, 2025a. URL https://openai.com/index/ introducing-o3-and-o4-mini/. Accessed: 2025-08-11. OpenAI. Openai o3-mini, 2025b. URL https://openai.com/index/openai-o3-mini. Accessed: 2025-04-22. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2025. URL https: //arxiv.org/abs/2412.21139. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, 2025. URL https://arxiv. org/abs/2504.13958. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: Benchmarking multimodal code generation for automated front-end engineering, 2025. URL https://arxiv.org/abs/2403.03163. Aditya Bharat Soni, Boxuan Li, Xingyao Wang, Valerie Chen, and Graham Neubig. Coding agents with multimodal browsing are generalist problem solvers, 2025. URL https://arxiv.org/ abs/2506.03011. stackblitz labs. bolt.diy, 2024. URL https://github.com/stackblitz-labs/bolt. diy. Accessed: 2025-04-22. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, and Yi R. Fung. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers, 2025. URL https://arxiv.org/abs/2506.23918. Haoyu Sun, Huichen Will Wang, Jiawei Gu, Linjie Li, and Yu Cheng. Fullfront: Benchmarking mllms across the full front-end engineering workflow, 2025. URL https://arxiv.org/ abs/2505.17399. 12 Yuxuan Wan, Yi Dong, Jingyu Xiao, Yintong Huo, Wenxuan Wang, and Michael R. Lyu. Mrweb: An exploration of generating multi-page resource-aware web code from ui designs, 2024. URL https://arxiv.org/abs/2412.15310. Haoran Wang, Zhenyu Hou, Yao Wei, Jie Tang, and Yuxiao Dong. Swe-dev: Building software engineering agents with training and inference scaling, 2025a. URL https://arxiv.org/ abs/2506.07636. Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, and Hongsheng Li. Mathcoder-vl: Bridging vision and code for enhanced multimodal mathematical reasoning, 2025b. URL https://arxiv.org/abs/ 2505.10557. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2024. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution, 2025a. URL https://arxiv.org/abs/ 2502.18449. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. Webagent-r1: Training web agents via endto-end multi-turn reinforcement learning, 2025b. URL https://arxiv.org/abs/2505. 16421. Jingyu Xiao, Yuxuan Wan, Yintong Huo, Zixin Wang, Xinyi Xu, Wenxuan Wang, Zhiyao Xu, Yuhang Wang, and Michael R. Lyu. Interaction2code: Benchmarking mllm-based interactive webpage code generation from interactive prototyping, 2025a. URL https://arxiv.org/ abs/2411.03292. Jingyu Xiao, Ming Wang, Man Ho Lam, Yuxuan Wan, Junliang Liu, Yintong Huo, and Michael R. Lyu. Designbench: comprehensive benchmark for mllm-based front-end code generation, 2025b. URL https://arxiv.org/abs/2506.06251. Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution, 2025. URL https: //arxiv.org/abs/2501.05040. Kai Xu, YiWei Mao, XinYi Guan, and ZiLong Feng. Web-bench: llm code benchmark based on web standards and frameworks, 2025. URL https://arxiv.org/abs/2505.07473. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv:2412.15115, 2024a. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Swe-agent: Agent-computer interfaces enable automated In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. PaInformation ProcessURL Inc., 2024b. Narasimhan, and Ofir Press. software engineering. in Neural quet, ing Systems, volume 37, pp. 5052850652. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2024/file/ 5a7c947568c1b1328ccc5230172e1e7c-Paper-Conference.pdf. and C. Zhang (eds.), Advances J. Tomczak, John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering, 2024c. URL https://arxiv.org/abs/2405.15793. 13 John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. Swe-bench multimodal: Do ai systems generalize to visual software domains?, 2024d. URL https://arxiv.org/abs/2410.03859. John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025b. URL https://arxiv.org/abs/2504.21798. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv. org/abs/2210.03629. Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, and Zhiqiang Shen. Web2code: large-scale webpage-to-code dataset and evaluation framework for multimodal llms, 2024. URL https://arxiv.org/abs/2406.20098. Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, and Fengzong Lian. Artifactsbench: Bridging the visual-interactive gap in llm code generation evaluation, 2025a. URL https://arxiv.org/abs/2507.04952. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges, 2024a. URL https: //arxiv.org/abs/2401.07339. Kechi Zhang, Huangzhao Zhang, Ge Li, Jinliang You, Jia Li, Yunfei Zhao, and Zhi Jin. Sealign: Alignment training for software engineering agent, 2025b. URL https://arxiv.org/abs/ 2503.18455. Linhao Zhang, Daoguang Zan, Quanshun Yang, Zhirong Huang, Dong Chen, Bo Shen, Tianyu Liu, Yongshun Gong, Pengjie Huang, Xudong Lu, Guangtai Liang, Lizhen Cui, and Qianxiang Wang. Codev: Issue resolving with visual data, 2024b. URL https://arxiv.org/abs/2412. 17315. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement, 2024c. URL https://arxiv.org/abs/2404.05427. Yuchen Zhuang, Di Jin, Jiaao Chen, Wenqi Shi, Hanrui Wang, and Chao Zhang. Workforceagentr1: Incentivizing reasoning capability in llm-based web agents via reinforcement learning, 2025. URL https://arxiv.org/abs/2505.22942."
        },
        {
            "title": "A LIMITATIONS AND FUTURE WORK",
            "content": "WebGen-Agent is specifically designed to generate websites based on natural language instructions from non-expert users. We do not consider website response speed or complex network conditions when generating and evaluating the websites; these are interesting questions for future work. In the supervised fine-tuning and Step-GRPO experiments, we train only 7Band 8B-parameter models due to limited computing power and GPU memory, as Step-GRPO training would take more than 24 hours on 16 NVIDIA A800 GPUs, and we currently do not have enough GPUs to train larger models. The results on the 7B and 8B models show great potential for our method, and we plan to apply our training approach to 30B72B models in the future. WEBGEN-AGENT ALGORITHM Algorithm 1 demonstrates the WebGen-Agent inference workflow in detail. Algorithms 2 and 3 are two helper functions for Algorithm 1, presented separately for clarity. 14 trajectory: instruction, edit, feedback, . . . archive of step snapshots current codebase restore codebase (cid:10)t, C, , (cid:11) SELECTBESTSTEP(Steps) TRUNCATE(T , t) + 1, consecErr 0 Algorithm 1 WebGen-Agent else + 1 Ct GENERATEEDIT(T ) += Ct APPLYEDIT(C, Ct) EXECUTE(C) if = error then += consecErr consecErr + 1 if consecErr = 5 then Require: Initial instruction I, maximum steps Ensure: Final codebase 1: [ ] 2: Steps 3: 4: 1, consecErr 0 5: while do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: end if 39: 40: end while 41: (cid:10), C, , (cid:11) SELECTBESTSTEP(Steps) 42: return end if img SCREENSHOT(C) (cid:10)desc, suggshot, scoreshot += desc, suggshot goNext AGENTDECISION(T ) if not goNext then end if (cid:10)pass, sugggui, scoregui) GUI AGENT(C(cid:11) += pass, sugggui Steps += (cid:10)t, C, scoreshot, scoregui if pass then break + 1; continue end if continue consecErr 0 + else else (cid:11) (cid:11) VLM JUDGE(img) Algorithm 2 SELECTBESTSTEP Require: Steps = {t, C, scoreshot, scoregui} 1: gmax maxsSteps scoregui 2: Sg {s scoregui = gmax} 3: return arg max sSg scoreshot WEBGEN-AGENT PROMPTS The prompts for acquiring screenshot and GUI-agent testing feedback are presented in Fig. 4, Fig. 5, Fig. 6, and Fig. 7. 15 Prompt: You are given single website screenshot as input. Task 1. Examine the screenshot closely for any rendering or runtime errors (e.g., 404 Not Found, stack traces, missing styles, blank areas). 2. Decide whether the screenshot shows rendering or runtime error. If yes, set is error: true, extract or paraphrase the visible error message(s) into error message, and leave screenshot description empty. If no, set is error: false, leave error message as an empty string (), and write concise but thorough screenshot description that covers: Overall layout (e.g., header/sidebar/footer, grid, flex, single-column). Key UI components (navigation bar, buttons, forms, images, cards, tables, modals, etc.). Color scheme and visual style (dominant colors, light/dark theme, gradients, shadows). Visible content and text (headings, labels, sample data). Notable design details (icons, spacing, font style) that help someone understand what the page looks like). 3. Suggest ways to improve the appearance of the website, for example: Separate incorrectly overlapping components. Adjust layout to avoid large blank areas. Adjust text or background color to avoid text color being too similar to the background color. If no improvement is necessary, leave suggestions as an empty string (); otherwise, briefly list the suggestion(s) in suggestions. 4. Grade the response. Output format (valid JSON) json { \"is_error\": <boolean>, \"error_message\": \"<string>\", \"screenshot_description\": \"<string>\", \"suggestions\": \"<string>\" } Return only this JSON objectno additional commentary, markdown, or code fences. Figure 4: The prompt for generating the description and suggestions based on the website screenshot. 16 Prompt: You are tasked with evaluating the design of webpage. Grade the webpages appearance on scale of 0 to 5 (5 being highest), considering the following criteria: Successful Rendering: Are there any components in the page or is it completely blank? Does the webpage render correctly without visual errors? Are colors, fonts, and components displayed as specified? Content Relevance: Does the design align with the websites purpose and user requirements? Are elements (e.g., search bars, report formats) logically placed and functional? Layout Harmony: Is the arrangement of components (text, images, buttons) balanced, intuitive, and clutter-free? Modernness & Beauty: Does the design follow contemporary trends (e.g., minimalism, responsive layouts)? Are colors, typography, and visual hierarchy aesthetically pleasing? Grading Scale: 0 (Blank Page): The screenshot is completely blank or does not contain any visible content. It may only have background color or display an error message. 1 (Poor): Major rendering issues (e.g., broken layouts, incorrect colors). Content is irrelevant or missing. Layout is chaotic. Design is outdated or visually unappealing. 2 (Below Average): Partial rendering with noticeable errors. Content is partially relevant but poorly organized. Layout lacks consistency. Design is basic or uninspired. 3 (Average): Mostly rendered correctly with minor flaws. Content is relevant but lacks polish. Layout is functional but unremarkable. Design is clean but lacks modern flair. 4 (Good): Rendered well with no major errors. Content is relevant and logically organized. Layout is harmonious and user-friendly. Design is modern and visually appealing. 5 (Excellent): Flawless rendering. Content is highly relevant, intuitive, and tailored to user needs. Layout is polished, responsive, and innovative. Design is cutting-edge, beautiful, and memorable. Task: Review the provided screenshot(s) of the webpage. Provide concise analysis of few sentences and then assign grade (05) based on your analysis. Highlight strengths, weaknesses, and how well the design adheres to the specifications. Your Response Format json { \"analysis\": \"<string>\", \"grade\": <int> } Your Response: Figure 5: Prompt for evaluating the visual quality of webpage and generating an appearance score. 17 Algorithm 3 TRUNCATE Require: Trajectory , step id 1: return prefix of ending just after the edit and feedback of step Based on the original website development instruction, you should identify all the requirements of the website generation and create comprehensive instruction for web-navigation GUI agent to test the generated website. The following is an example of triggering the GUI agent testing based on the original instruction: Example Original instruction: Please implement self-driving tour website that provides self-driving tour products and services. The website should have functionalities for browsing self-driving tour routes, booking self-driving tour hotels, and self-help self-driving tour packages. Users should be able to browse different types of self-driving tour routes, book hotels and packages, and query self-driving club information. The website should also provide search and filtering functions to help users quickly find the self-driving tour products they need. Define background as cream; define components with dark teal. <boltAction type=\"gui agent test\"> Verify cream background and dark-teal buttons. Browse different types of self-driving tour routes, book hotels and packages, and query self-driving club information. Search and filter for self-driving tour products. </boltAction> The following is the original website development instruction: <instruction>{instruction}</instruction> Trigger the GUI agent testing based on the original instruction in way similar to the example. Do not generate additional comments. Figure 6: Prompt for generating GUI-agent testing instruction from the original website specification. EXAMPLES OF WEBGEN-AGENT TRAJECTORIES To demonstrate the WebGen-Agent workflow in straightforward way, we present three example trajectories in Fig. 8, Fig. 9, and Fig. 10. As shown in these examples, WebGen-Agent iteratively improves the appearance and functionality of the generated website based on screenshot and GUIagent testing feedback. ACCURACY OF SCREENSHOT AND GUI-AGENT TESTING SCORES To analyze the accuracy of the screenshot and GUI-agent testing scores given by the feedback VLM in the WebGen-Agent workflow, we evaluated the results of Claude-4-Sonnet, Qwen3-Coder30B-A3B-Instruct, Qwen3-Coder-480B-A35B-Instruct, and DeepSeek-V3 as coding LLMs, with Qwen2.5-VL-32B-Instruct as the feedback VLM, as well as DeepSeek-V3 as the coding LLM and GPT-4o as the feedback VLM. We manually verified the accuracy of the screenshot and GUI-agent testing scores. Human annotators were provided with the score and the screenshot or GUI-agent trajectory at each step and asked to judge whether the score was accurate. If the score was inaccurate, they provided the correct score. The results are presented in Table 5. The accuracies of the screenshot scores across all experiments ranged from 93% to 96%, while the accuracies of the GUI-agent scores ranged from 89% to 93%. The standard errors of the screenshot scores range from 0.20 to 0.26, while the standard errors of the GUI-agent scores range from 0.31 to 0.44. This demonstrates that the scores are highly accurate, supporting the effectiveness of the WebGen-Agent workflow and the Step-GRPO training process. Compared with using Qwen2.5-VL18 Prompt: You are given GUI-agent testing trajectory. The GUI agent testing trajectory: GUI-agent Testing Instruction: {gui instruction} Trajectory: {result} Task 1. Examine the trajectory for any failed actions that indicate problem in the website design. 2. Decide whether the GUI-agent testing trajectory reveals any flaw in the website implementation. If yes, set \"test passed\": If no, true, and leave \"improvement suggestions\" empty. thorough \"improvement suggestions\" that covers the suggested improvements targeting the problems revealed by the testing result. set \"test passed\": and write false, concise but 3. Evaluate the results of the GUI-agent test run and assign one integer grade from 1 to 5: 1: The vast majority of tested functions fail or behave incorrectly. 2: Many functions fail; only few behave as expected. 3: About half of the functions work as expected; success is mixed. 4: Most functions work as expected; only minor issues remain. 5: All tested functions work exactly as expected; no issues observed. Assign the grade to \"grade\". Output format (valid JSON) json { \"test_passed\": <boolean>, \"improvement_suggestions\": \"<string>\", \"grade\": <int> } You can first make short analysis of two or three sentences, then output this JSON object. Figure 7: Prompt for evaluating GUI-agent testing trajectories and providing improvement suggestions. 19 Table 5: Accuracy of the screenshot and GUI-agent scores using human annotation as ground truth. For every experiment we report the accuracy together with the standard error compared to human scores. Score Type Screenshot GUI agent Coding LLM Feedback VLM Accuracy (%) Std. Error Claude-4-Sonnet Qwen3-Coder-30B-A3B-Inst. Qwen3-Coder-480B-A35B-Inst. DeepSeek-V3 DeepSeek-V Claude-4-Sonnet Qwen3-Coder-30B-A3B-Inst. Qwen3-Coder-480B-A35B-Inst. DeepSeek-V3 DeepSeek-V3 Qwen2.5-VL-32B-Inst. Qwen2.5-VL-32B-Inst. Qwen2.5-VL-32B-Inst. Qwen2.5-VL-32B-Inst. GPT-4o Qwen2.5-VL-32B-Inst. Qwen2.5-VL-32B-Inst. Qwen2.5-VL-32B-Inst. Qwen2.5-VL-32B-Inst. GPT-4o 93.6 93.9 95.6 94.8 95.5 90.1 91.4 89.6 91.2 92.2 0.25 0.26 0.20 0.22 0. 0.31 0.44 0.41 0.36 0.33 Table 6: Distribution (%) of human scores regarding the comprehensiveness of the GUI-agent testing instructions and the resulting average score. The definition of the scores are presented in Fig. 11. The scores range from 1 to 5. Model Claude-4-Sonnet DeepSeek-V3 Qwen3-Coder-30B-A3B-Inst. Qwen3-Coder-480B-A35B-Inst. Total 84.2 73.3 75.2 76.2 77.2 4 13.9 24.8 23.8 21.8 21.0 2.0 2.0 1.0 2.0 1.7 2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Avg. Score 4.82 4.71 4.74 4.74 4.75 32B-Instruct, using GPT-4o as the feedback VLM only marginally improved the screenshot score accuracy from 94.8% to 95.5% and the GUI-agent score accuracy from 91.2% to 92.2%. This shows that Qwen2.5-VL-32B-Instruct is sufficient for the task while being significantly more cost-effective. ANALYSIS OF THE COMPREHENSIVENESS OF GUI-AGENT TESTING"
        },
        {
            "title": "INSTRUCTIONS",
            "content": "To analyze the comprehensiveness of the GUI-agent testing instructions generated by the agent, we manually evaluated the instructions from the experiment runs using Claude-4-Sonnet, Qwen3Coder-30B-A3B-Instruct, Qwen3-Coder-480B-A35B-Instruct, and DeepSeek-V3. We graded each GUI-agent instruction on 15 scale, determined by how completely the instruction translates each website requirement into concrete GUI-agent checks. The grading guidelines are presented in Fig. 11. As shown in Tab. 6, 77.2% of the GUI-agent testing instructions across the four models receive score of 5 (Complete, 100% of requirements). Instructions with score of 4 or higher (High, 7590%) account for 98.3% of the total, while only 1.7% receive score of 3 (Moderate, 5075%); none score below 3. These results indicate that the GUI-agent instructions comprehensively cover most of the website requirements."
        },
        {
            "title": "G CATEGORICAL RESULTS",
            "content": "Tab. 7 shows the categorical results of WebGen-Agent with various proprietary and open-source models on WebGen-Bench. As shown in the table, WebGen-Agent consistently achieves superior performance across all instruction and test-case categories compared to other code agent systems. For both the 7B and 8B models, Step-GRPO improves performance in most categories compared to the original instruct model and the SFT model. This demonstrates the effectiveness of the WebGen20 Table 7: Categorical results of WebGen-Agent with various proprietary and open-source models on WebGen-Bench (Lu et al., 2025b), compared with other code agent systems. The highest score of each column is marked in bold. Instruction Categories Test-case Categories Content Presentation User Interaction Data Management Functional Testing DataDisplay Testing DesignValidation Test Name Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 GPT-4o o3-mini Qwen2.5-Coder-32B Qwen2.5-72B-Inst. WebGen-LM-7B WebGen-LM-14B WebGen-LM-32B Claude-3.5-Sonnet DeepSeek-R1 DeepSeek-V3 o3 Claude-4-Sonnet Gemini-2.5-Pro Qwen3-Coder-480B-A35B-Inst. Qwen2.5-Coder-32B-Inst. Qwen3-Coder-30B-A3B-Inst. Qwen2.5-72B-Instruct OpenHands 18.4 8.9 7.3 Aider 21.1 28.6 12.8 Bolt.diy 21.2 20.6 16.6 5.9 17.7 6.9 10.1 23.8 27.8 33.2 32.8 16.4 12. 31.9 39.1 17.8 35.6 43.7 37.1 26.4 28.7 17.5 28.2 27.9 30.2 46.6 WebGen-Agent Proprietary Models 57.8 57.8 58.0 59.2 68.7 60.3 64.7 48.7 44.2 53.2 46.6 51.8 48.2 55. 18.4 5.9 8.4 16.6 13.4 12.5 26.2 24.7 11.2 11.2 13.4 5.9 5.6 38.1 31.6 38.8 51.9 38.1 45.6 53.4 52.5 45.6 55.9 Open-Source Models (30B72B) 35.6 55.2 43. 28.8 54.3 30.4 34.4 47.2 38.8 Open-Source Models (7B8B) Qwen2.5-Coder-7B-Inst. WebGenAgent-LM-7B-SFT WebGenAgent-LM-7B-Step-GRPO Qwen3-8B WebGenAgent-LM-8B-SFT WebGenAgent-LM-8B-Step-GRPO 20.7 53.4 51. 37.4 41.7 52.0 8.6 33.5 41.1 34.3 34.2 38.8 10.9 33.8 47.8 30.0 43.8 43.1 12.4 5.0 3. 14.9 17.6 9.7 17.1 21.1 10.5 4.7 11.4 1.9 5.8 22.0 23.6 29.1 38.5 35.0 40.9 43.7 44.0 37.9 43.2 20.9 39.1 23.0 7.4 23.5 30.7 26.8 26.8 30. 33.9 9.9 8.1 30.1 35.2 19.1 26.3 29.3 28.2 19.6 25.5 14.5 21.0 27.7 26.9 43.0 60.5 53.8 61.0 55.1 69.4 60.2 71.2 32.3 62.1 39.8 15.9 48.4 56. 34.1 43.8 51.1 32.0 25.0 25.0 34.0 44.3 18.4 52.0 44.3 38.1 24.6 33.6 23.0 25.4 47.5 49.2 56.1 76.2 66.8 72.5 68.9 71.7 72.5 79.9 62.3 76.6 66. 21.3 67.6 69.3 54.1 63.1 68.4 Agent workflow and the Step-GRPO training process, which incorporates screenshots and GUIagent feedback. 21 Table 8: Influence of the maximum number of iterations on WebGen-Agent performance. Metric Accuracy (%) Appearance Score Exceed Rate (%) 2 42.4 3.2 100.0 4 47.9 3.6 57.4 50.2 3.7 34.7 8 50.4 3.6 22.8 10 52.0 3.7 15.8 51.9 3.8 13.9 14 51.2 3.8 10.9 16 53.3 3.7 8.9 52.6 3.8 8.9 20 52.6 3.8 7."
        },
        {
            "title": "H ANALYSIS OF MAXIMUM ITERATION NUMBERS",
            "content": "To analyze the effect of the maximum iteration number parameter on the performance of WebGenAgent, we test the accuracy, appearance score, and the percentage of samples that exceed the maximum iteration limit (exceed rate) at different maximum iteration numbers. The coding LLM used is DeepSeek-V3. As shown in Fig. 12 and Tab. 8, the accuracy and appearance score show rising trend as the maximum iteration number increases, while the exceed rate continuously decreases. When the maximum iteration number is between 14 and 20, the accuracy, appearance score, and exceed rate all begin to converge. This is because most samples finish before reaching the iteration limit, as reflected by the exceed rate, and the impact of the maximum iteration number on performance diminishes. QUALITATIVE ANALYSIS OF SUPERVISED FINETUNING AND STEP-GRPO To provide qualitative analysis of the effects of supervised fine-tuning and Step-GRPO with screenshot and GUI-agent feedback, we present examples of websites generated by Qwen2.5-Coder-7BInstruct, WebGenAgent-LM-7B-SFT, and WebGenAgent-LM-7B-Step-GRPO in Figs. 13 and 14. We also include examples of websites generated by Qwen3-8B, WebGenAgent-LM-8B-SFT, and WebGenAgent-LM-8B-Step-GRPO in Figs. 15 and16. As demonstrated in the examples, supervised fine-tuning greatly reduces the models tendency to generate erroneous or malformed websites and improves their ability to follow the appearance requirements specified in the instructions. Step-GRPO further refines the aesthetics and harmony of the generated websites. QUALITATIVE ANALYSIS OF THE WEBGEN-AGENT WORKFLOW To demonstrate how the WebGen-Agent workflow functions, we provide examples of steps in WebGen-Agent trajectories where the agent improves the websites appearance based on screenshot or GUI-agent feedback. As shown in Fig. 17, Fig. 18, Fig. 19, Fig. 20, and Fig. 21, the agent enhances the websites visual appeal by incorporating suggested improvements. Similarly, Fig. 22, Fig. 23, Fig. 24, Fig. 25, and Fig. 26 illustrates how the agent refines the websites functionality based on feedback from the GUI-agent testing process. The steps are simplified due to space constraints."
        },
        {
            "title": "K USAGE OF LARGE LANGUAGE MODELS IN PAPER WRITING",
            "content": "The paper is primarily human-written. However, large language models such as o3 (OpenAI, 2025a) and DeepSeek-V3 (Liu et al., 2024) are used to check for grammar and spelling mistakes. The words and phrases are occasionally polished by LLMs to make the wording more fluent. 22 Figure 8: Example of WebGen-Agent trajectory. 23 Figure 9: Example of WebGen-Agent trajectory. Figure 10: Example of WebGen-Agent trajectory. 25 GUI-agent Instruction Evaluation Guidelines: Score the instruction 1 5, where 5 = best. The dominant criterion is comprehensiveness: how completely the instruction translates every website requirement into concrete GUI-agent checks. Grading Scale: 1 (Minimal, < 25 %): The instruction overlooks most of the stated requirements. 2 (Low, 25 50 %): Only some primary requirements are mentioned; many important items are absent. 3 (Moderate, 50 75 %): Core functionalities are covered, but several secondary features or style rules are skipped. 4 (High, 75 90 %): All major functional requirements plus most visual or secondary ones are included; only few minor details are missing. 5 (Complete, 100 % of requirements): Every requirement is turned into checks. Nothing significant is left out. Figure 11: Grading guidelines for manually evaluating GUI-agent testing instructions (a) Accuracy (%) and Appearance Score as function of the maximum number of iterations. (b) Exceed Rate (%) versus the maximum number of iterations. Figure 12: Effect of the maximum iteration number hyper-parameter on different performance metrics. Figure 13: Screenshots of websites created by Qwen2.5-Coder-7B-Instruct, WebGenAgent-LM-7BSFT, and WebGenAgent-LM-7B-Step-GRPO. 27 Figure 14: Screenshots of websites created by Qwen2.5-Coder-7B-Instruct, WebGenAgent-LM-7BSFT, and WebGenAgent-LM-7B-Step-GRPO. 28 Figure 15: Screenshots of websites created by Qwen3-8B, WebGenAgent-LM-8B-SFT, and WebGenAgent-LM-8B-Step-GRPO. Figure 16: Screenshots of websites created by Qwen3-8B, WebGenAgent-LM-8B-SFT, and WebGenAgent-LM-8B-Step-GRPO. 30 Figure 17: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on screenshot feedback. The step is simplified due to space constraints. 31 Figure 18: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on screenshot feedback. The step is simplified due to space constraints. Figure 19: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on screenshot feedback. The step is simplified due to space constraints. 33 Figure 20: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on screenshot feedback. The step is simplified due to space constraints. 34 Figure 21: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on screenshot feedback. The step is simplified due to space constraints. Figure 22: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on GUI-agent testing feedback. The step is simplified due to space constraints. 36 Figure 23: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on GUI-agent testing feedback. The step is simplified due to space constraints. 37 Figure 24: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on GUI-agent testing feedback. The step is simplified due to space constraints. Figure 25: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on GUI-agent testing feedback. The step is simplified due to space constraints. 39 Figure 26: Example of step in WebGen-Agent trajectory where the agent improves the websites appearance based on GUI-agent testing feedback. The step is simplified due to space constraints."
        }
    ],
    "affiliations": [
        "Ace Robotics",
        "Multimedia Laboratory (MMLab), The Chinese University of Hong Kong"
    ]
}