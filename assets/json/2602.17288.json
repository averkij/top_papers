{
    "paper_title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
    "authors": [
        "Anuj Gupta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models."
        },
        {
            "title": "Start",
            "content": "ARXIV-TO-MODEL: PRACTICAL STUDY OF SCIENTIFIC LM TRAINING 6 2 0 2 9 1 ] . [ 1 8 8 2 7 1 . 2 0 6 2 : r Anuj Gupta Independent Researcher India anuj0456@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present detailed case study of training 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2A100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in data-rich regime (52B pretraining tokens). Rather than proposing novel architecture, this work provides an engineering-grounded, transparent account of training small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models. Keywords Scientific Language Models ArXiv Language Model Training Machine Learning"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models have demonstrated impressive reasoning, mathematical, and scientific capabilities. However, the majority of publicly reported systems rely on large-scale curated corpora, proprietary data mixtures, or undisclosed preprocessing pipelines. In contrast, the practical process of constructing scientific language model directly from raw open-access sources remains sparsely documented. Training domain-specific models from raw scientific corpora introduces unique challenges. ArXiv distributions consist of heterogeneous LaTeX archives, multi-file project structures, custom macros, symbolic-heavy content, and inconsistent metadata. Seemingly minor preprocessing choicessuch as language filtering heuristics or archive validation rulescan significantly impact final dataset yield and training stability. In this paper, we document our experience training 1.36B-parameter scientific language model using raw arXiv sources. Our goal is not to introduce new architecture nor to compete with large instruction-tuned systems, but rather to provide transparent and reproducible account of: End-to-end dataset construction from raw metadata and LaTeX archives Tokenization strategies for formula-dense scientific text Training dynamics under constrained GPU and storage budgets Empirical scaling behavior in data-rich regime ArXiv-to-Model: Practical Study of Scientific LM Training Through 24 iterative runs, we analyze failure modes, optimization instabilities, preprocessing bottlenecks, and hardware utilization patterns. We compare small-data (20GB) and full-data (200GB) regimes to illustrate the impact of dataset scale on convergence behavior. Our contributions are primarily empirical and engineering-focused: 1. reproducible pipeline for constructing scientific corpus from raw arXiv LaTeX sources. 2. Quantitative analysis of dataset yield losses caused by metadata filtering and extraction failures. 3. Practical insights into tokenizer design for mathematical and symbolic-heavy corpora. 4. detailed examination of training stability across 24 experimental runs. We believe that transparent reporting of data engineering, infrastructure tradeoffs, and training dynamics is essential for advancing reproducible scientific language modelingparticularly for researchers operating under limited compute resources."
        },
        {
            "title": "2 Training Data Scaling Considerations",
            "content": "The backbone of this study is 80GB text corpus constructed from arXiv LaTeX sources. The dataset construction pipeline was divided into four major stages: (1) source extraction, (2) metadata filtering, (3) LaTeX normalization and cleaning, and (4) weighted mixture assembly. 2.1 How much data (tokens) do you need? Following the Chinchilla scaling law, the optimal number of training tokens scales approximately linearly with model parameters: where denotes the number of training tokens and the number of model parameters. This compute-optimal regime suggests that under-training (too few tokens) leads to underutilized model capacity, while over-training wastes compute without proportional gains. 20 (1) 2.2 Converting Text Size to Tokens commonly used empirical approximation for clean English text is: 1 billion tokens 34 GB of processed text Using this approximation: 40B tokens 120160 GB 140B tokens 420560 GB Note that scientific LaTeX corpora often exhibit slightly different compression characteristics due to symbolic density and equation-heavy content. 2.3 Practical Data Scale Regimes From empirical training observations and scaling literature, we categorize dataset sizes as follows: <10 GB Too small for pretraining; suitable primarily for fine-tuning. 2050 GB Borderline regime; requires many epochs and risks overfitting. 100300 GB Suitable for training 1.5B parameter models in compute-efficient regime. 2 ArXiv-to-Model: Practical Study of Scientific LM Training 2.4 Metadata Filtering and Normalization Raw arXiv distributions present several structural and quality challenges. Submissions often contain multiple versions, short-form letters, withdrawn manuscripts, and non-English text. Additionally, many papers are organized as multi-file LaTeX projects linked via input or include directives, frequently relying on custom macros or external style files. These factors introduce significant noise into large-scale scientific language model training. To ensure corpus consistency and quality, we implemented multi-stage filtering pipeline with the following constraints: Subject Focus: Restricted to math, cs, hep-th, hep-ph, quant-ph, stat.ML, and stat.TH categories to preserve formal scientific reasoning structure. Temporal Filter: Only papers published after the year 2000 were retained, reducing legacy formatting inconsistencies and outdated stylistic conventions. Withdrawal Removal: Submissions marked as withdrawn in metadata comments were excluded to prevent inclusion of invalidated or retracted work. Volume Filter: Documents with body text under 2,000 characters were removed to ensure exposure to complete scientific arguments rather than fragments or short notes. Language Detection: Automated language identification was applied to the combined title, abstract, and cleaned body text. Due to dense mathematical notation, occasional false negatives were observed; however, this step effectively removed clearly non-English submissions. Archive Validation: Each downloaded source archive was verified for structural integrity before extraction to prevent malformed or corrupted tar files from entering the pipeline. All available .tex sources within validated archives were extracted and concatenated. The cleaning stage removed figures, references, formatting commands, and non-semantic LaTeX artifacts while preserving mathematical expressions and structural environments essential for scientific reasoning. To mitigate redundancy, we applied both exact deduplication via content hashing and near-duplicate detection using similarity-based thresholding. This reduces repeated content arising from versioned submissions or minimally modified re-uploads while retaining substantive revisions. Despite these efforts, LaTeX extraction remains primary source of unavoidable data loss due to malformed sources or unconventional project layouts. 2.5 Sampling Weights and Data Mixture To balance expertise and stylistic diversity, we implemented weighted sampling strategy during mixture assembly. High-quality Gold scientific documents were upsampled to increase exposure to precise terminology, formal proofs, and domain-specific structure. Simultaneously, broader domain papers were retained at lower sampling weights to prevent overfitting to narrow stylistic patterns while preserving generalization capacity. This weighted mixture enables the model to achieve strong technical competence without sacrificing robustness across adjacent scientific subdomains. Source Cleaned arXiv (LaTeX) Volume 80GB Stage Pretraining Weight Relevance 2.0X OpenWebMath 50GB Pretraining 1.0X StackExchange (STEM) 10GB Post-training 1.0X MathInstruct 100MB Post-training 1.0X UltraChat 1.2GB Post-training 1.0X Core scientific reasoning, formal proofs, mathematical structure Informal mathematical intuition and diverse problem formulations Logical question-answer reasoning and structured explanations Supervised problem-solving and step-by-step reasoning Conversational alignment and dialogue formatting Table 1: Training Data Sources, Stages, and Sampling Weights 3 ArXiv-to-Model: Practical Study of Scientific LM Training Pretraining vs Post-Training Strategy. The base model was pretrained exclusively on LaTeX-formatted scientific corpora, primarily cleaned arXiv sources. This ensures that the model internalizes formal mathematical structure, symbolic reasoning, theorem-proof patterns, and domain-specific terminology without contamination from conversational noise. External datasets (e.g., StackExchange, MathInstruct, UltraChat) were introduced only during post-training. Their purpose is not to teach scientific knowledge, but to align the model to instruction-following behavior, conversational formatting, and structured reasoning outputs. This separation allows the model to first develop deep scientific competence before being aligned for interactive usage, minimizing catastrophic interference between formal reasoning and conversational fluency."
        },
        {
            "title": "3 Tokenization",
            "content": "Scientific and mathematical corpora differ fundamentally from general natural language. They contain dense symbolic expressions, structured equations, operator heavy sequences, and domain-specific LaTeX environments. Generic tokenization schemes optimized for web text or conversational data tend to fragment mathematical symbols, oversegment operators, and distort structural boundaries within equations. Such fragmentation negatively impacts compression efficiency, increases sequence length, and weakens the models ability to learn stable representations of formal reasoning patterns. For scientific language modeling, tokenization is therefore not trivial preprocessing step but core architectural design decision. 3.1 Design Objectives The tokenization pipeline was developed with the following objectives: Preserve mathematical expressions and LaTeX structural environments. Reduce unnecessary fragmentation of symbols, operators, and formula blocks. Improve token compression efficiency for equation-heavy documents. Maintain representational consistency across scientific subdomains. 3.2 Methodology We conducted exploratory experiments training custom BPE and SentencePiece tokenizers on curated subsets of the scientific corpus. These experiments aimed to: Preserve common LaTeX commands and operators Reduce fragmentation of symbolic expressions Improve compression efficiency in equation-heavy documents Preliminary trials revealed that tokenizer sampling strategy and vocabulary size significantly influenced symbolic segmentation patterns. However, integrating newly trained tokenizer introduced additional complexity in embedding alignment and model initialization stability. 3.3 Final Tokenizer Selection For the final KiteFish-A1-1.5B model, we adopted LLaMA-compatible SentencePiece tokenizer with vocabulary size of approximately 102,400 tokens. This decision was motivated by: Architectural compatibility with the LLaMA transformer design Stable embedding initialization Reduced risk of token-ID misalignment Consistent convergence across experimental runs Although domain-trained tokenizers remain promising direction, the LLaMA tokenizer demonstrated sufficient robustness for scientific text modeling under current compute constraints. 4 ArXiv-to-Model: Practical Study of Scientific LM Training 3.4 Tokenization Output Statistics After tokenization of approximately 200 GB of curated scientific data, the resulting corpus yielded: 52.18B tokens for scientific pretraining 5B tokens for post-training and alignment data The token density reflects the symbolic compression characteristics of scientific LaTeX, where formula-heavy documents produce distinct segmentation behavior relative to general web corpora. 3.5 Evaluation Metrics Tokenizer evaluation during exploratory experiments considered: Average tokens per document (compression efficiency) Symbol fragmentation patterns Early-stage training stability Future work may systematically compare domain-trained tokenizers against general-purpose tokenizers for symbolic efficiency and long-context reasoning."
        },
        {
            "title": "4 Model Architecture",
            "content": "The model is implemented as dense, decoder-only transformer following the LLaMA architectural framework. The final configuration contains approximately 1.36 billion parameters, including untied input and output embeddings. 4.1 Architectural Specification The architectural configuration is summarized below: Hidden dimension (dmodel): 2048 Transformer layers: 24 Attention heads: 16 Keyvalue heads: 16 (standard multi-head attention) Feed-forward dimension: 5504 Vocabulary size: 102,400 Positional encoding: Rotary Position Embeddings (RoPE, θ = 10,000) Maximum context length: 4096 tokens Activation function: SiLU Normalization: RMSNorm (ϵ = 106) Precision: bfloat16 Unlike embedding-tied configurations, input and output token embeddings are maintained as separate matrices, increasing representational flexibility at the cost of additional parameters. The resulting model contains approximately 1.36B trainable parameters. 4.2 Design Rationale dense transformer architecture was selected over sparse or Mixture-of-Experts (MoE) alternatives for the following reasons: Training Stability: Dense models exhibit more predictable convergence behavior under moderate-scale compute. 5 ArXiv-to-Model: Practical Study of Scientific LM Training Deterministic Compute Per Token: Unlike MoE routing, each token activates all parameters within layer, simplifying optimization dynamics and distributed training. Efficient Multi-GPU Scaling: Dense architectures reduce cross-device communication overhead relative to expert routing strategies. Domain Specialization: Given the high-quality, domain-focused scientific corpus, parameter efficiency was prioritized over sparse capacity scaling. 4.3 Compute Budget and Infrastructure Training was conducted on: 2 NVIDIA A100 GPUs (80GB memory each) Distributed data-parallel setup High-throughput storage-backed dataset streaming Projected compute usage for the primary training phase was approximately 5,0008,000 GPU-hours. Efficiency was maximized using: bfloat16 mixed-precision training Activation checkpointing Optimized data loading pipelines 4.4 Batch Size and Parallelization Strategy To ensure stable optimization and efficient hardware utilization, training employed data parallelism combined with gradient accumulation. Micro-batch size per GPU: 12 sequences Effective global batch size: 5122,048 sequences Gradient accumulation: tuned dynamically to match memory constraints Fully Sharded Data Parallel (FSDP) or ZeRO-style optimization was used to enable scalable memory partitioning across GPUs while maintaining numerical stability. 4.5 Dense vs Sparse Capacity Tradeoffs While Mixture-of-Experts (MoE) architectures increase total parameter capacity without proportional inference cost, they introduce routing complexity, expert imbalance, and higher distributed communication overhead. Given the available compute budget (2A100 GPUs) and the objective of scientific specialization rather than parameterscale maximization, dense transformer was selected for stability, deterministic compute per token, and efficient multi-GPU scaling."
        },
        {
            "title": "5 Training Setup",
            "content": "The training pipeline was designed to balance scientific rigor, hardware constraints, and optimization stability under dual A100 (80GB) GPU setup. 5.1 Curriculum Strategy To stabilize early optimization and improve symbolic adaptation, we employed staged curriculum approach: Stage 1 Textual Warm-up: Initial training focused on abstracts, introductions, and conclusions to establish linguistic fluency before exposing the model to dense symbolic content. Stage 2 Symbolic Integration: Full LaTeX bodies, including theorem environments and mathematical derivations, were introduced to enable structured reasoning adaptation. 6 ArXiv-to-Model: Practical Study of Scientific LM Training Stage 3 Mixed Curriculum: balanced mixture of prose and formula-heavy content ensured robustness across explanatory and symbolic regimes. Although the architecture supports 4096-token context window, training sequences were constructed at 768 tokens to maximize batch throughput and maintain stable memory utilization. 5.2 Training Time and Scaling Considerations Training was conducted on 2NVIDIA A100 (80GB) GPUs using ZeRO Stage 2 optimization and bfloat16 precision. The primary pretraining phase required approximately 5,0008,000 GPU-hours. For 1.36B-parameter model, the Chinchilla scaling law suggests an optimal training token budget of approximately 27B tokens. Our 52.18B-token pretraining corpus therefore places the model in data-rich regime ( 38 tokens per parameter), prioritizing domain robustness over strict compute optimality. This configuration reflects deliberate tradeoff: maximizing scientific coverage and symbolic stability under moderate hardware constraints rather than scaling parameter count alone. 5.3 Optimization Details Training was conducted using: AdamW optimizer with weight decay bfloat16 mixed precision ZeRO Stage 2 memory optimization Gradient checkpointing for activation memory reduction The effective global batch size was scaled via gradient accumulation to maintain stable gradient statistics while fitting within GPU memory limits."
        },
        {
            "title": "6 Training Dynamics and Optimization Analysis",
            "content": "A total of 24 experimental runs were conducted to refine hyperparameters, stabilize optimization, and improve hardware utilization. These runs varied in dataset scale, learning rate, gradient accumulation, and preprocessing configurations. 6.1 Iterative Optimization Across 24 Runs Early experimental runs were intentionally exploratory and frequently unstable. Several runs terminated prematurely due to suboptimal hyperparameters or memory constraints. Notably: Run 24: Trained on reduced 20GB subset to validate pipeline stability. Run 23 and Run 20: Trained on the full 200GB processed corpus. This progression enabled controlled scaling from small-data debugging to full-scale training. 6.2 Small-Data Regime (20GB) Run 24, trained on approximately 20GB of data, exhibits unstable convergence behavior. Training loss decreases initially but oscillates and plateaus at relatively high value (Figure 1). 7 ArXiv-to-Model: Practical Study of Scientific LM Training Figure 1: Training loss for Run 24 (20GB dataset). Loss oscillates and converges slowly, indicating limited data regime. Observation: Small-scale pretraining leads to noisy gradient dynamics and reduced convergence efficiency. The model begins to memorize patterns without sufficient diversity for stable generalization. 6.3 Full-Data Regime (200GB) Run 23 and Run 20 were trained on the full 200GB processed corpus. Figure 2: Training loss for Run 23 (200GB dataset). Loss decreases smoothly with improved stability compared to small-data regime. ArXiv-to-Model: Practical Study of Scientific LM Training Figure 3: Final optimized run (Run 20, 200GB). Training loss exhibits smooth monotonic convergence with long-tail stabilization. Observations: Loss reduction is significantly smoother under full data scale. Gradient noise is reduced relative to the 20GB regime. Convergence exhibits classic transformer long-tail behavior. 6.4 Validation and Overfitting Analysis Validation loss decreased monotonically throughout training and remained closely aligned with training loss. Importantly, no sustained divergence between train and eval curves was observed. Figure 4: Training and validation loss curves for the final run. No widening divergence is observed. The absence of widening traineval gaps suggests that the model did not enter severe overfitting regime within the observed training horizon. Final validation loss corresponds to perplexity of approximately: ArXiv-to-Model: Practical Study of Scientific LM Training exp(1.438) 4.2 indicating strong adaptation to scientific corpora. 6.5 Gradient Stability Gradient norm monitoring revealed: Early warm-up spikes (expected behavior) Rapid stabilization below 1.0 No late-stage explosion or vanishing gradients Figure 5: Gradient norm across training steps. Stability is achieved after early warm-up. This confirms stable optimization under the selected learning rate and batch configuration. 6.6 Hardware Utilization GPU monitoring indicated: Sustained utilization above 95% Stable power consumption ( 300W) No ECC memory errors No persistent I/O stalls Figure 6: GPU utilization and power metrics during final training run. These results indicate efficient pipeline throughput and effective distributed configuration. 10 ArXiv-to-Model: Practical Study of Scientific LM Training 6.7 Best Practices Derived The 24-run optimization cycle yielded several practical insights: Conservative learning rate schedules improve stability in symbolic-heavy corpora. Full-scale data significantly reduces gradient noise. Monitoring gradient norms prevents silent divergence. Storage throughput can bottleneck training before compute. Iterative small-scale debugging (20GB subset) accelerates stabilization prior to full-scale runs. Collectively, these observations reinforce the importance of systematic experimentation and infrastructure-aware optimization when training small scientific language models."
        },
        {
            "title": "7 Evaluation",
            "content": "Model evaluation was conducted primarily using perplexity on held-out scientific validation data. The trained model demonstrates strong familiarity with mathematical notation, LaTeX structures, and formal scientific writing patterns. However, as the base model was trained exclusively on raw scientific corpora, it does not exhibit instruction-following or conversational behavior. This work does not aim to compete with large-scale instruction-tuned systems. Instead, the evaluation focuses on analyzing the capabilities and limitations of small, domain-specialized language model trained from structured scientific data."
        },
        {
            "title": "8 Empirical Observations",
            "content": "Several practical insights emerged during training: Data Yield is Pipeline-Dependent: Effective dataset size was driven more by preprocessing decisions than by raw data availability. Archive validation, LaTeX cleaning, and filtering heuristics significantly impacted usable token volume. Storage as Bottleneck: In early stages, I/O throughput and storage constraints proved more limiting than raw compute capacity. Language Filtering Sensitivity: Applying language detection heuristics too early in the pipeline resulted in the removal of valid scientific documents due to dense symbolic content. Instruction Following Does Not Emerge Naturally: Pretraining on raw scientific corpora does not produce conversational or instruction-following capabilities without explicit post-training alignment. These findings highlight the central role of data engineering and pipeline design in small-scale language model training, often outweighing architectural modifications in practical impact."
        },
        {
            "title": "9 Limitations and Lessons Learned",
            "content": "Despite careful design and systematic engineering, several limitations remain. Compute Constraints. Training was performed on dual A100 (80GB) setup. While sufficient for 1.36B-parameter model, this restricts exploration of larger architectures, extended context training, or aggressive hyperparameter sweeps. The total compute cost (5,0008,000 GPU-hours) highlights the non-trivial resource requirements even for mid-scale language models. Storage and I/O Bottlenecks. Raw arXiv archives, intermediate extraction artifacts, and processed JSONL corpora require substantial disk capacity and high-throughput I/O. In early stages, archive handling and storage bandwidth became more limiting than GPU compute. Preprocessing Sensitivity. LaTeX extraction, archive validation, and metadata filtering significantly influenced final token yield. Small heuristic changes led to large variations in usable data volume. This introduces an unavoidable degree of pipeline-induced bias. 11 ArXiv-to-Model: Practical Study of Scientific LM Training Scaling Regime Tradeoffs. Although trained on 52.18B tokens, the model contains 1.36B parameters, placing it in data-heavy regime relative to compute-optimal scaling. While beneficial for domain specialization, this may reduce marginal efficiency gains compared to larger parameter models trained under strictly optimal token-to-parameter ratios. Context-Length Utilization. The architecture supports 4096-token context window; however, training sequences were limited to 768 tokens to maximize batch throughput. As result, long-context reasoning capacity may not be fully realized. Evaluation Scope. Evaluation primarily relies on perplexity over held-out scientific corpora. This does not directly measure reasoning correctness, theorem validity, or symbolic proof consistency. More structured mathematical benchmarks would provide stronger assessment of formal reasoning capabilities. Domain Bias. The dataset is restricted to selected scientific categories (math, theoretical physics, and statistical learning). While this strengthens specialization, it limits general-domain adaptability. Lack of Instruction Alignment. The base model is not instruction-tuned and therefore unsuitable for direct conversational deployment without additional alignment. Reproducibility Constraints. Large-scale LaTeX preprocessing pipelines, storage requirements, and GPU resource needs may limit exact reproducibility for researchers without similar infrastructure. Collectively, these limitations emphasize that successful small-scale language model training depends as much on infrastructure planning and data engineering rigor as on architectural design."
        },
        {
            "title": "10 Conclusion",
            "content": "This paper presents an end-to-end case study of training 1.36B-parameter scientific language model from raw arXiv LaTeX sources. Beyond architectural configuration, we emphasize the critical role of data filtering, tokenization design, and curriculum strategy in enabling stable domain specialization. Our findings suggest that for small-to-mid-scale models, careful data engineering can rival architectural scaling in impact. Moreover, transparent reporting of preprocessing decisions, tokenization strategies, and compute tradeoffs remains essential for reproducibility in language model research. We hope this work contributes to more realistic and engineering-aware perspective on scientific language modeling, particularly for researchers operating under constrained compute budgets. Future work includes exploring longer-context training, instruction-aligned post-training strategies, and systematic evaluation on formal mathematical reasoning benchmarks."
        },
        {
            "title": "Acknowledgements",
            "content": "The author would like to thank the arXiv project for maintaining the open-access scientific archive used in this work. We also express our gratitude to the creators of the Proof-Pile-2, OpenWebMath, and MathInstruct datasets for their contributions to the open-science community. This work was supported by computational resources utilizing NVIDIA A100 GPUs. 12 ArXiv-to-Model: Practical Study of Scientific LM Training"
        },
        {
            "title": "A Code Repository",
            "content": "The code used for experiments and analysis in this work is publicly available at: https://github.com/kitefishai/ KiteFish-A1-1.5B-Math Difference between Tokenization, Chunking, Vector and Embedding Tokenization Split text into small units called tokens (words or sub-words). Example: love AI tokens Chunking Group tokens into larger pieces so long text can be processed. Example: Paragraph multiple chunks Vector numerical list that represents something. Example: [0.12, 0.87, 3.45] Embedding vector that captures meaning of chunk (sentence, image, etc.). Similar meaning similar vectors Pipeline Text tokenization chunking model embeddings Importance of model_type in config.json In config.json, model_type is not merely documentation. It is used to: 1. Select the correct configuration class \"model_type\": \"llama\" LlamaConfig \"model_type\": \"deepseek\" DeepseekConfig 2. Dispatch the correct model and tokenizer classes AutoModel.from_pretrained() AutoTokenizer.from_pretrained() Internally, Hugging Face maintains mappings such as: MODEL_TYPE_TO_CONFIG = { \"llama\": LlamaConfig, \"deepseek\": DeepseekConfig, } Thus, model_type determines how the configuration is interpreted. Relationship Between model_type and the Tokenizer An important distinction is the following: Tokenizers are not inferred from model weights. They are inferred from the configuration and the tokenizer files. D.1 What model_type Influences for Tokenization"
        },
        {
            "title": "E What Happens When Loading a Tokenizer",
            "content": "When executing: AutoTokenizer.from_pretrained(path) Hugging Face performs the following steps: 1. Load config.json 13 ArXiv-to-Model: Practical Study of Scientific LM Training Aspect Tokenizer class (e.g., LLaMATokenizer) Tokenizer vocabulary BPE vs. SentencePiece Special tokens (BOS, EOS, <s>, </s>) Chat templates Influenced Yes"
        },
        {
            "title": "Often",
            "content": "Table 2: Effect of model_type on tokenizer behavior 2. Read model_type 3. Select tokenizer class 4. Load tokenizer files, such as: tokenizer.json tokenizer.model (SentencePiece) vocab.json, merges.txt, etc. Warning: If the tokenizer files do not match the selected tokenizer class, the tokenizer may fail or behave incorrectly. Example: model_type = \"llama\" with DeepSeek Architecture F.1 Case A: DeepSeek Model with model_type = \"llama\" This configuration is dangerous but subtle. F.1.1 Tokenizer Effects Hugging Face loads the LLaMA tokenizer Uses: SentencePiece LLaMA special tokens LLaMA BOS/EOS behavior If the model was trained using DeepSeek tokenizer: Token IDs no longer align Embedding rows correspond to incorrect tokens Generated output becomes meaningless This failure mode is typically silent. F.1.2 Model Architecture Effects AutoModel attempts to instantiate LLaMA architecture DeepSeek may differ in: Attention implementation Rotary embedding scaling Mixture-of-Experts layers RMSNorm usage Possible outcomes include: Tensor shape mismatches (hard failure) Successful loading with incorrect behavior ArXiv-to-Model: Practical Study of Scientific LM Training F.2 Case B: DeepSeek Weights with LLaMA Tokenizer This configuration leads to the following issues: Problem Token ID mismatch Special token differences Different vocabulary size Different token segmentation Result Nonsensical generation BOS/EOS errors Runtime failures Severe perplexity increase Table 3: Effects of tokenizer mismatch Even single token offset is sufficient to invalidate the model."
        },
        {
            "title": "G Why Some Models Appear to Work Despite Mismatch",
            "content": "In some cases: DeepSeek forks the LLaMA tokenizer Vocabulary is identical Special tokens remain compatible In such scenarios: Mislabeling model_type may appear to work However, it breaks downstream tooling, including: Chat templates LoRA adapters Quantization configurations Future library updates This setup is fragile and unsafe. Correct Handling of DeepSeek vs. LLaMA H.1 Recommended Configuration { } \"model_type\": \"deepseek\", \"architectures\": [\"DeepseekForCausalLM\"], \"vocab_size\": 102400 The tokenizer files must match exactly those used during training. If DeepSeek intentionally uses the LLaMA tokenizer: Keep tokenizer files identical Still use model_type = \"deepseek\""
        }
    ],
    "affiliations": [
        "Independent Researcher"
    ]
}