{
    "paper_title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence",
    "authors": [
        "Amirhosein Ghasemabadi",
        "Keith G. Mills",
        "Baochun Li",
        "Di Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 2 3 0 2 . 5 0 5 2 : r Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence Amirhosein Ghasemabadi ECE Department, University of Alberta ghasemab@ualberta.ca Keith G. Mills ECE Department, University of Alberta kgmills@ualberta.ca Baochun Li ECE Department, University of Toronto bli@eecg.toronto.edu Di Niu ECE Department, University of Alberta dniu@ualberta.ca"
        },
        {
            "title": "Abstract",
            "content": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Bestof-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs lightweight tree search guided solely by intrinsic LLM signalstoken-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B70B parameters), while reducing GPU memory usage by up to 10. Compared to PRM-based methods, GG achieves comparable accuracy with 8 faster inference speeds and 45 lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques. The code is available at https://github.com/Amirhosein-gh98/Guided-by-Gut."
        },
        {
            "title": "Introduction",
            "content": "Enhancing the performance of Large Language Models (LLMs) often requires significant computational resources through model scaling [1, 23, 31] or complex inference strategies [13, 50]. Test-Time Scaling (TTS) techniques like Chainof-Thought (CoT) [36] allocate additional computation during inference. This re-allocation of compute resources provides powerful alternative for boosting LLM reasoning capabilities, as evidenced by models like OpenAIs series [22], DeepSeek R1 [9], and others [41, 30, 4, 20]. Contemporary TTS methods [14, 35, 28] are capable of enhancing LLMs containing 1.5B parameters such that they outperform 70B, 405B parameter or even large closed-source LLMs on Preprint. Under review. Figure 1: We compare the performance and GPU VRAM usage of Guided by Gut (GG; stars) to Bestof-N (BoN; triangles) and Chain-of-Thought (CoT; circles) on several LLMs. GG achieves better accuracy at much lower memory cost (log-scaled). difficult reasoning and mathematical benchmarks [15]. However, TTS is an expensive search process where the total compute cost to generate an answer matches or may even exceed that of larger LLM [47, 18]. For example, Sampling-based methods [35] like Best-of-N (BoN) [6] operate by generating large number of candidate solutions (e.g., potentially hundreds) and then choosing the optimal one from this pool, which requires prohibitively large amounts of LLM inference for complex tasks. In addition, Process Reward Models are auxiliary verification models which guide the TTS process by providing step-by-step correctness feedback [40, 48, 47, 35]. Such verifier-guided techniques can be computationally expensive to train and deploy and suffer from generalizability issues [15, 49, 48]. Thus, regardless of strategy, TTS for small-scale LLMs relies on expensive inference, which severely limits practical application and motivates the need for more cost-effective TTS frameworks. To bridge this gap, we propose Guided by Gut (GG), computationally efficient and scalable TTS framework to enhance LLM reasoning. GG leverages intrinsic signals derived from the LLMs generation process, fine-tuned by reinforcement learning (RL), to enable smaller models to achieve substantially stronger reasoning performance which matches or exceeds results achieved by much larger models (e.g., 70B) and expensive TTS strategies at much lower GPU memory costs, as Figure 1 illustrates. Our detailed contributions are as follows. Token Confidence and Novelty: Instead of relying on costly external verifier models, we utilize intrinsic cues from the LLM output, e.g., token probabilities, which we interpret as confidence scores and measure the novelty of potential reasoning step. This provides lightweight avenue for guiding inference-time search and can be integrated into existing models and algorithms. Reinforced Confidence via RL Fine-tuning: We incorporate RL via Group Relative Policy Optimization (GRPO) into model fine-tuning specifically to improve the reliability of LLM internal confidence estimation, leading to more reliable guidance for our test-time search strategy. Efficient Test-Time Search with Self-Guidance: We introduce tree search algorithm based on Diverse Verifier Tree Search (DVTS) [5] guided by the LLMs intrinsic signals (token probability/confidence, novelty). To achieve efficient TTS, our algorithm is specifically optimized for minimal computational cost during inference. We apply GG to reasoning LLMs from DeepSeek R1 family [9] and Qwen2.5-Math [42] as nonreasoning model and evaluate it on benchmark tasks like AIME24/25 [2], MATH500 [10], and AMC [3]. Experimental results not only demonstrate that GG achieves significant performance improvements over relevant baselines such as BoN and CoT, but also highlight its superior computational efficiency. Specifically, GG enables smaller models (e.g., 1.5B-7B parameters) to outperform much larger counterparts (e.g., 32B and 70B), achieving similar or superior accuracy while using up to 410 less GPU memory. Furthermore, compared to computationally expensive PRM-based approaches, GG achieves comparable accuracy at fraction of the computational cost, leading to 45 lower GPU memory usage and up to 8 faster inference speeds. Furthermore, GG achieves an approximately 50% reduction in KV cache memory usage compared to the BoN strategy, facilitating significantly more efficient and cost-effective deployment of reasoning LLMs."
        },
        {
            "title": "2 Related Work",
            "content": "Test-Time Scaling (TTS). OpenAIs o1 [22] highlighted the critical role of enhanced reasoning for complex tasks like code generation and mathematical problem solving. The subsequent open release of models like DeepSeek-R1 [9] further spurred research into reasoning mechanisms like Chain-ofThought (CoT) [36]. CoT falls under the umbrella of Test-Time Scaling [5, 8], which enhances model performance by strategically allocating additional computational effort during inference, through methods like extended reasoning or search, rather than relying solely on increased model size or longer training. As we illustrate in Figure 2, CoT primarily consists of letting the LLM generate reasoning steps and reach an answer/conclusion in purely autoregressive manner. Sampling methods like Best-of-N (BoN) [6] involve generating multiple independent solutions, from which the optimal one is selected, often with the aid of verifier or through voting mechanisms. Tree-based methods like Beam 2 Figure 2: Comparison of reasoning generation strategies. (1) Standard Chain-of-Thought (CoT) generates single reasoning path autoregressively. (2) Search guided by an external Process Reward Model (PRM) explores multiple candidate steps (st 2, . . . ), using PRM scores to select promising paths. (3) Our proposed Self-Guided Search similarly explores multiple steps but uses intrinsic signals, Confidence (C) and Novelty (N ), derived from the LLM to guide the search at each step without relying on an external PRM. In the example, stands for an independent tree and stands for branch within that tree. Example text best read zoomed-in. 1, st Search [38], Diverse Verifier Tree Search (DVTS) [5], and Monte Carlo Tree Search (MCTS) [39] provide different algorithmic avenues towards selecting optimal multi-step reasoning paths. While these models enable <10B parameter reasoning LLMs to perform at level similar to much larger models, e.g., 70B and above [15], they necessitate multiple rounds of inference to generate large quantities of potential reasoning steps or chains. External Verification. To guide exploration towards the most promising step or reasoning path, many Test-Time Scaling methods require mechanism to quantify the effectiveness of different choices. This role is typically filled by an external verifier [28, 5], such as Process Reward Model (PRM) [15] or an Outcome Reward Model (ORM) [14]. These verifiers are often substantial models themselves, significantly contributing to the overall computational burden of TTS. While various strategies like confidence-weighted voting [29, 24], dynamic early stopping [33], and sample pruning [12] aim to mitigate this overhead, many sophisticated search-based TTS approaches still fundamentally rely on powerful (and thus costly) PRM or similar verifier to effectively guide the search. For instance, even approaches combining model confidence with PRM supervision still ultimately rely on the costly PRM [38]. To address this, GG avoids costly external verifiers for simple, near-zero overhead internal signals, proving this minimalist approach surprisingly effective. Reinforcement Learning for LLM Reasoning. Recent literature highlights Reinforcement Learnings crucial role in advancing Large Language Model reasoning without human intervention [8]. ReFT[19] employs Proximal Policy Optimization (PPO) to enhance the generalizability of LLMs for reasoning. key algorithm, Group Relative Policy Optimization (GRPO) [27], notably eliminates the need for separate value function in PPO. Further research explores various RL training aspects to improve reasoning capabilities [45, 46, 16]. DeepScaleR [18] aims to boost existing reasoning models through additional GRPO fine-tuning with iterative context lengthening."
        },
        {
            "title": "3 Methodology",
            "content": "This section outlines our proposed method, Guided by Gut (GG). We begin by providing essential background on the Test-Time Scaling process. Following this, we elaborate on the self-guided search mechanism and overall strategy. 3.1 Preliminaries Problem Formulation. Given an input prompt or question Q, our objective is to generate logical reasoning chain = [s1, s2, . . . , sT ] leading to correct final answer A, where each step st typically constitutes sentence or short paragraph incrementally building upon previous steps. The overall 3 reasoning process thus follows the pipeline: A, with the reasoning chain explicitly bridging the input question and the final answer through intermediate logical steps. Chain-of-Thought (CoT) Reasoning. Standard Chain-of-Thought (CoT) [36] approaches jointly generate the reasoning chain and final answer via autoregressive language modeling. Formally, given Q, the model sequentially generates each reasoning step conditioned on previously generated steps: (R = s1:T Q) = (cid:89) t=1 (st Q, s1:t1). (1) Each step st thus depends on the input and preceding reasoning steps s1:t1, mirroring standard autoregressive token generation in language modeling. Guiding Search with Reward Models. Single-path autoregressive generation methods can suffer from error accumulation [37, 21]. To mitigate this, tree search methods explore multiple reasoning trajectories simultaneously. These methods typically use an external Process Reward Model (PRM) or Outcome-supervised Reward Model (ORM) for step-wise correctness evaluations. PRM is model that, given an input and previous steps s1:t1, assigns correctness score or reward rt to candidate next steps st: rt = PRM(st Q, s1:t1) or rt = PRM(st Q). (2) Likewise, an ORM is sparse reward model where only the final step receives non-zero reward; rt<T = 0. Thus, these reward models improve logical coherence and accuracy by guiding search algorithms like Beam Search, BoN and DVTS. 3.2 Proposed Method: Self-Guided Search The usage of verifier models like PRMs and ORMs, while effective, introduces computational overhead and generalizability issues [15]. To address these limitations, we propose Guided by Gut (GG), which leverages the intrinsic signals directly obtained from the LLM internal token generation process. This removes the dependency on external evaluation, ensuring minimal computational overhead. Specifically, our approach uses two intrinsic signals to guide reasoning: Confidence C(st) reflects the internal assurance model has with respect to given reasoning step st. We compute confidence directly from token-level probabilities: mt(cid:88) C(st) = log p(st context), (3) 1 mt l=1 where mt is the number of tokens in reasoning step st and context represents previous tokens in st and all prior reasoning steps. Novelty (st) encourages exploration by measuring the dissimilarity of candidate reasoning steps to previously explored paths. Specifically, we calculate novelty as the proportion of new tokens introduced by the candidate step st relative to tokens already explored within the current reasoning context. We formulate reward rt to guide the search process by combining these intrinsic signals as follows: rt = λCC(st) + λN (st), (4) where λN and λC balance exploration and exploitation, respectively. Unlike verifier-guided approaches, our reward is intrinsically computed from LLM prediction statistics, eliminating external dependency. 4 Figure 3: Answer Confidence Distribution Across Training Settings. Each subplot shows the normalized distribution of confdence scores for correct (green) and incorrect (orange) completions across different fine-tuning strategies. The vertical dashed lines mark the mean confidence for correct and wrong completions, respectively. The base model (left) is generally overconfident, with incorrect completions receiving high confidence scores. Fine-tuning with correctness reward (middle) improves accuracy but leaves the confidence distribution largely unchanged. Confidence-based fine-tuning (right) better separates correct from incorrect completions, showing improved calibration. 3.3 Enhancing Confidence via Reinforcement Learning Fine-Tuning significant challenge in using intrinsic statistics is ensuring reliability, as raw model confidence and answer novelty may not accurately reflect correctness. To refine this process, we incorporate Reinforcement Learning fine-tuning phase. Specifically, we utilize Group Relative Policy Optimization (GRPO) [27], memory-efficient variant of Proximal Policy Optimization (PPO) [26] tailored for LLM applications. Let π represent the LLM we want to fine-tune, parameterized either current fine-tuned weights θ, fine-tuned weights from the previous iteration θold or the original reference weights θref . At each iteration GRPO samples group of outputs {oi}G i=1 from πθold , where each output oi represents chain of reasoning steps and an answer oi = [Ri, Ai]. Each output receives reward ri, which we describe below: Confidence-Based Reward. Our RL training enhances confidence accuracy with novel reward function, moving beyond typical correctness-only rewards. Many literature methods use such rewards for the final answer, yielding sparse, binary signals which provide no learning signal when no completion is correct. In contrast, our fine-tuning reward is more comprehensive: it specifically integrates the models internal confidence throughout reasoning steps with final answer correctness. This richer, multifaceted signal is crucial for calibrating model certainty, enabling more reliable self-guided search. To compute it, we first calculate weighted summation of the confidence scores for the last reasoning steps in the reasoning chain Ri, C(Ri): C(Ri) = 1 (cid:80)k l=1 k (cid:88) l=1 c(sT k+l ) (5) Then the RL fine-tuning reward ri is computed based on Ais correctness and the reasoning chain confidence C(Ri), as follows: ri = (cid:26)1 + C(Ri)4 1 10C(Ri)4 if IsCorrect(Ai), otherwise, (6) where IsCorrect(Ai) returns boolean validating the final answer as correct or not. Equation 6 ensures that correct, highly confident answers are rewarded more strongly, whereas incorrect, overconfident answers receive greater penalties, thus promoting precise confidence calibration. The calibration effect is demonstrated in Figure 3. Advantage and Fine-tuning Update. After computing the fine-tuning reward ri for each sampled output oi, we compute the normalized advantage ˆAi as follows: 5 ˆAi = ri mean({rj}G std({rj}G j=1) j=1) . (7) We can then compute the clipped surrogate policy update δi [26] for given output as δi = 1 oi oi (cid:88) l=1 [min(ri ˆAi, clip(ri, 1 ϵ, 1 + ϵ) ˆAi) βDKL(πθπref )], (8) where ϵ controls the magnitude of the update and the Kullback-Leibler divergence term DKL, controlled by β, prevents the fine-tuning weights from moving too far away from the original weights. We then update the LLM by using all clipped surrogates to maximize the GRPO objective. JGRPO(θ) = {oi}G i=1πθold ["
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 δi]. (9) 3.4 Search Strategy We employ Diverse Verifier Tree Search (DVTS) [5], an extension of beam search that splits initial beams into independent subtrees that are expanded greedily using reward model or our proposed intrinsic rewards. DVTS enhances solution diversity and performance. We apply DVTS within the CoT framework by operating at the reasoning step (st) level, identifying step completions via model-specific delimiters. This structure allows the search to evaluate and expand complete logical increments during the reasoning process. Our modified DVTS algorithm runs recursively until it encounters one of the termination conditions we specify: maximum reasoning depth or maximum token length limit is exceeded, the reasoning exhibits signs of text degeneration, e.g., excessive repetition, or in the best case, the LLM arrives at final answer A. To avoid incomplete outputs due to overthinking near the limits, we inject model-specific signal prompting conclusion. For example, with DeepSeek models, appending **Final Answer** effectively elicits the final answer, ensuring usable completions even in complex cases. For further details on DVTS, we provide formal explanation in the supplementary materials due to space constraints."
        },
        {
            "title": "4 Experimental Results",
            "content": "In this section, we validate our Guided by Gut Test-Time Scaling (GG) approach. We first describe our experimental setup, then present our findings. Finally, we ablate the components of our method. 4.1 Confidence Calibration via Reinforcement Learning We perform reinforcement learning fine-tuning phase. Our primary goal during RL fine-tuning is not to maximize raw task accuracy but to enhance the reliability of the intrinsic confidence signals utilized by GG through GRPO, as described in Section 3.3 and in our reward function (Equation 6). For this calibration step, we utilize the LIMO dataset [43]. This dataset contains 817 high-quality examples curated specifically for complex mathematical reasoning, featuring detailed, structured solutions. We fine-tune the DeepSeek-R1-Distill-Qwen-1.5B & 7B models with Low-Rank Adapters (LoRA) [11]. The implementation leverages the TRL [32] library and an adaptation of the open-r1 codebase [8]. Key aspects of the setup include low learning rate of 2.0 106 with cosine scheduler [17], LoRA rank = 128 and α = 128, and GRPO training with = 8 generations per prompt at temperature of 0.6. We perform fine-tuning on two NVIDIA A100 80 GPUs using bfloat16 precision with FlashAttention-2 [7] optimization, using prompt/completion length limits of 768/8096 tokens and batch size of 8 per device with 8 gradient accumulation steps for 3 epochs. The fine-tuning process completes in approximately one day. 6 Table 1: Performance metrics for various models and TTS strategies on the AIME24 and AIME25 benchmarks. Inference Speed and GPU Memory measured using NVIDIA A100 80GB cards. Model TTS Strategy AIME24 Acc. [%] AIME25 Acc. [%] Inference Speed time/question [m] GPU Memory [GB] DeepSeek-R1-Distill-Qwen-1.5B CoT DeepSeek-R1-Distill-Qwen-7B CoT DeepSeek-R1-Distill-Qwen-14B CoT DeepSeek-R1-Distill-Qwen-32B CoT DeepSeek-R1-Distill-Llama-70B CoT CoT DeepSeek-R1-671B CoT OpenAI o1 mini DeepSeek-R1-Distill-Qwen-1.5B BoN(N = 32) DeepSeek-R1-Distill-Qwen-1.5B BoN(N = 64) DeepSeek-R1-Distill-Qwen-1.5B GG (N = 32) DeepSeek-R1-Distill-Qwen-1.5B GG (N = 64) GG (N = 32) DeepSeek-R1-Distill-Qwen-7B GG (N = 64) DeepSeek-R1-Distill-Qwen-7B 26.8 48.1 65.8 66.9 70.0 79.1 63.6 56.7 66.7 66.7 66.7 73.3 76. 21.4 38.6 48.4 51.8 54.1 64.3 36.7 36.7 40.0 46.7 53.3 53.3 0.2 1.0 6.5 11.5 20.0 2.8 5.1 2.7 5.1 10.3 18. 4 18 36 80 180 1536 18 32 11 18 31 45 4.2 Results To demonstrate the efficacy and efficiency of Guided by Gut (GG), we first benchmark our approach using <10B parameter LLMs against several larger Chain-of-Thought (CoT) LLMs, some of which are closed-source like OpenAI o1 mini. We also compare to competitive tree-based TTS methods such as Best-of-N (BoN), robust TTS baseline that outperforms PRM-guided search with R1 models [15], using standardized evaluation settings. As for the evaluation settings, we allocate token budgets based on method requirements: 16k tokens for tree-based TTS methods, i.e., GG and BoN, and 32k tokens for standard CoT models (consistent with original papers) to balance comprehensive comparisons with computational feasibility. We set the maximum reasoning steps to 200 for R1 models. We configure BoN to use = 32 or = 64 samples using majority voting. Likewise, we evaluate our Confidence-Guided DVTS with equivalent compute budgets: using = 32 total paths (from 16 subtrees with = 2 beam width/verifiers) and = 64 total paths (from 32 subtrees with = 2), distinctively employing weighted majority voting based on final answer confidence scores. We evaluate GG on the AIME 2024 & 2025 (AIME24, AIME25) [2, 44] benchmarks, which each consist of 30 problems from the respective American Invitational Mathematics Examinations that emphasize advanced high-school-level reasoning. Table 1 presents our findings. First, we observe that GG delivers competitive performance compared to much larger LLMs employing CoT strategies. Specifically, 1.5B parameter LLM with GG achieves accuracy comparable to 32B CoT model, while 7B GG LLM reaches accuracy levels closer to those of 70B CoT modeland even beats OpenAIs o1 mini on AIME24. Moreover, the results on DeepSeek-R1-Distill-Qwen-7B are equally impressive. Even with moderate sampling budget (N = 32), GG outperforms all CoT-based LLMs with fewer than 100B parameters. Notably, it achieves this while requiring at most one sixth of the VRAM memory and delivering faster inference compared to Distill-Llama-70B, despite producing similar accuracy. Only the DeepSeek-R1-671B model, which has nearly 10 the parameters, achieves higher accuracy on both benchmarksbut at prohibitively high memory cost, requiring nearly 30 more VRAM memory. On the DeepSeek-R1-Distill-Qwen-1.5B model, GG consistently outperforms BoN (at = 32 and = 64) while using almost 50% less GPU memory. For instance, with computational budget of = 32, GG attains 10% and 3.3% performance on AIME24 and AIME25, respectively. Further, for = 64, GG outperforms BoN by 10% on AIME25 (46.7% vs. 36.7%). In summary, Table 1 demonstrates that GG is competitive, self-guided TTS method that allows smaller models to achieve accuracy comparable to larger counterparts while using significantly less memory. Comparison with Process Reward Models. We further compare the self-directed search mechanism of GG to TTS guided by an external verifier, i.e., Process Reward Model (PRM). We 7 Table 2: Performance comparison of PRM-based and non-PRM (GG) scoring strategies using Qwen2.5-Math-1.5B-Instruct as the base LLM with DVTS as the search algorithm. Horizontal line demarcates = 16 from = 32 total path hyperparameter. We denote the external verifier utilized in PRM trials. We vary the total paths for both methods and the external verifier utilized in PRM trials. Maximum token limit of 4048. Higher accuracy and lower inference speed/memory are preferred. Best and second best results in bold and italics, respectively. Scoring Strategy AMC23 Acc. [%] MATH500 Acc. [%] Inference Speed time/question [m] GPU Memory [GB] PRM (math-shepherd-mistral-7B-PRM, = 16) PRM (RLHFlow/Llama3.1-8B-PRM, = 16) Guided by Gut (GG; ours) (N = 16) PRM (math-shepherd-mistral-7B-PRM, = 32) PRM (RLHFlow/Llama3.1-8B-PRM, = 32) Guided by Gut (GG; ours) (N = 32) 58.3 60.0 63.3 60.0 61.7 65.0 79.1 79.0 78.9 81.1 80.7 80.0 0.8 0.9 0. 1.5 1.6 0.2 19 21 4 23 25 5 Table 3: Accuracy and KV Cache memory usage for various models and TTS strategies across mathematical reasoning datasets. Horizontal lines demarcate base model and search budget. Accuracy reported as mean (max) for AIME24/25 and mean performance for MATH500/AMC. Higher is better for accuracy; lower is better for KV Cache memory. AIME24 Acc. [%] MATH500 Acc. [%] KV Cache [GB] AIME25 Acc. [%] AMC Acc. [%] TTS Strategy Model DeepSeek-R1 1.5B [9] DeepSeek-R1 7B [9] CoT 26.8 21.4 BoN (N = 32) 52.2(56.7) 34.4(36.7) GG (N = 32) 58.9(66.7) 37.5(40.0) BoN (N = 64) 57.8(66.7) 36.7(36.7) GG (N = 64) 61.7(66.7) 42.2(46.7) CoT 48.1 38.6 BoN (N = 32) 72.2(73.3) 51.1(53.3) GG (N = 32) 71.7(73.3) 52.5(53.3) BoN (N = 64) 75.5(76.7) 50.0(53.3) GG (N = 64) 76.7(76.7) 51.7(53.3) 83.9 91.7 91. 92.3 92.9 92.8 96.1 96.3 96.1 96.5 68.3 90.0 92. 90.0 93.3 85.5 92.5 94.1 93.7 95.0 0.86 13.7 6. 27.4 13.7 1.7 27.2 13.7 54.4 27.2 conduct this experiment using Qwen2.5-Math-1.5B-Instruct [4] as the base model. This is non-reasoning LLM commonly employed for evaluating PRM-guided TTS in the literature [34, 5, 15]. Specifically, we set limit of 4k tokens and 50 reasoning steps per trial for this experiment, which reflects the shorter non-CoT answers and the overhead of PRM-guided search. We evaluate on two datasets: AMC2023 (AMC23) [51], comprising 40 problems from the American Mathematics Competition testing foundational high-school math skills; and MATH500, diverse set of 500 problems randomly sampled from the full MATH benchmark [14]. Table 2 summarizes our findings from comparing GG, which utilizes simple intrinsic search signals, against external PRM-guided approaches. Our aim is to evaluate whether lightweight, cost-effective signal could achieve competitive reasoning performance. On AMC23, GG consistently outperforms both the math-shepherd-mistral-7B-PRM (by 5.0%) and the RLHFlow/Llama3.1-8B-PRM (by 3%) across both settings. On MATH500, GGs performance is also highly competitive, trailing the PRM accuracies by narrow margin of only 0.1 to 1.1 percentage points. Crucially, GG achieves this strong standing with its significantly more lightweight methodology: it is 8x faster and uses <5GB of VRAM, as it avoids loading an additional large reward model. These results underscore GGs robust performance and efficiency, especially for local deployment scenarios. Additional TTS Results: Accuracy and Efficiency. We further evaluate GG against Best-of-N (BoN) and Chain-of-Thought (CoT) on AIME24, AIME25, MATH500, and AMC using DeepSeekR1 1.5B and 7B models  (Table 3)  . To ensure fair and reliable comparison, particularly for AIME 8 benchmarks known for high variance, all configurations were run four times with different seeds, reporting average and maximum accuracies across different runs. Experimental settings match those in Table 1. Table 3 demonstrates that GG consistently matches or surpasses BoN across most benchmarks and model sizes, particularly in mean accuracy, while BoN occasionally equals GG only in maximum scores. For example, on AIME24 with the 7B model, BoN shows some advantage in specific scenarios, but otherwise, GG maintains strong comparative performance. Crucially, GG achieves these competitive or superior results against BoN while using approximately 50% less KV cache memory, major efficiency gain."
        },
        {
            "title": "5 Ablation Studies",
            "content": "We conduct several ablation studies to verify the efficacy of the different components that constitute GG. Due to space constraints, we focus here on the importance of RL fine-tuning with confidence, providing additional hyperparameter ablations (novelty weight and search) in the appendix. Table 4: Effectiveness of RL fine-tuning onr AIME24: Performance scores with various reward strategies versus no-RL baseline. Best result in bold. Fine-tuning Setting Impact of Confidence-Based RL Fine-tuning. To isolate the effectiveness of our proposed confidencebased reward mechanism within the RL fine-tuning phase (Equation 6), we conduct an ablation study comparing four settings: (1) without RL fine-tuning, (2) with RL fine-tuning using an accuracy reward, (3) with RL fine-tuning using confidence reward (ours), and (4) RL fine-tuning using confidence reward without penalty for incorrect answers. Table 4 summarizes our results which clearly demonstrate the importance of our confidence-based RL fine-tuning. Specifically, the full Confidence Reward method (58.9) substantially outperforms both the no RL fine-tuning baseline (54.5) and the fine-tuning strategy using only Correctness Reward (54.9). Furthermore, removing the negative penalty for incorrect, highly confident answers results in the lowest score (54.0). This underscores the critical role of the penalty term for effective confidence calibration and confirms that the improvements stem directly from the specific reward design aimed at enhancing confidence calibration. No RL Fine-tuning Correctness Reward Only Confidence (No Penalty) Confidence Reward (Ours) 54.5% 54.9% 54.0% 58.9% Score"
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce Guided by Gut (GG), Test-Time Scaling framework enabling smaller Large Language Models (e.g., 1.5B parameters) to surpass significantly larger models (e.g., 32B) in performance while offering faster inference and substantially reduced GPU memory and KV cache usage. GG leverages intrinsic model signalsconfidence and noveltyextracted directly from LLM outputs, further calibrated through reinforcement learning, providing lightweight and effective alternative to costly external verifier-based methods such as Process Reward Models (PRMs). Compared to Best-of-N (BoN) strategies, GG achieves comparable or superior results with approximately 50% less KV cache memory and competitive inference speeds, while delivering performance on par with PRM-guided approaches. Limitations. The intrinsic signals employed in GG, such as confidence scores, do not inherently verify the actual correctness of each reasoning step. There exist cases where the model assigns high confidence to incorrect reasoning steps, hence the importance of our RL fine-tuning phase. Overall, the goal of our method is to provide simple, efficient, and without relying on external verifier signals to guide the search process, rather than guaranteeing the absolute correctness of each step that may occur when using strong PRM. We analyze and discuss representative failure cases in the Appendix to provide deeper insight into these limitations. Broader Impacts. This work proposes an efficient test-time reasoning framework for language models. Our contributions reduce the computational requirement needed to use reasoning models. Experimental results demonstrate that we can combine GG with locally smaller LLM and achieve 9 comparable performance to open-source models that require GPU rack servers or closed-source models behind an API. Therefore, one potential future impact of our work we hope to see is increased usage of locally-deployed reasoning LLMs."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] AI-MO. Aime 2024, 2024. URL https://huggingface.co/datasets/AI-MO/ aimo-validation-aime. [3] AI-MO. Amc 2023, 2024. URL https://huggingface.co/datasets/AI-MO/ aimo-validation-amc. [4] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] E. Beeching, L. Tunstall, and S. Rush. Scaling test-time compute with open models. URL https: //huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute. [6] B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. Ré, and A. Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [7] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [8] H. Face. Open r1: fully open reproduction of deepseek-r1, 2025. [9] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum? id=7Bywt2mQsCe. [11] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [12] X. Huang, L. L. Zhang, K.-T. Cheng, F. Yang, and M. Yang. Fewer is more: Boosting llm reasoning with reinforced context pruning. arXiv preprint arXiv:2312.08901, 2023. [13] Y. Ji, J. Li, H. Ye, K. Wu, J. Xu, L. Mo, and M. Zhang. Test-time computing: from system-1 thinking to system-2 thinking. arXiv preprint arXiv:2501.02497, 2025. [14] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [15] R. Liu, J. Gao, J. Zhao, K. Zhang, X. Li, B. Qi, W. Ouyang, and B. Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. [16] Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [17] I. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?id=Skq89Scxx. [18] M. Luo, S. Tan, Li, R. A. Popa, model by DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog Post. J. Luo, L. E. DeepScaleR: Surpassing O1-preview with 1.5B https://pretty-radio-b75.notion.site/ J. Wong, X. Shi, W. Y. Tang, M. Roongta, C. Cai, and I. Stoica. scaling RL, 2025. URL [19] T. Q. Luong, X. Zhang, Z. Jie, P. Sun, X. Jin, and H. Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024. 10 [20] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [21] S. Mukherjee, A. Chinta, T. Kim, T. A. Sharma, and D. Hakkani-Tür. Premise-augmented reasoning chains improve error identification in math reasoning with llms. arXiv preprint arXiv:2502.02362, 2025. [22] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. [23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [24] A. Razghandi, S. M. H. Hosseini, and M. S. Baghshah. Cer: Confidence enhanced reasoning in llms. arXiv preprint arXiv:2502.14634, 2025. [25] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084. [26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [27] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [28] C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [29] A. Taubenfeld, T. Sheffer, E. Ofek, A. Feder, A. Goldstein, Z. Gekhman, and G. Yona. Confidence improves self-consistency in llms. arXiv preprint arXiv:2502.06233, 2025. [30] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [31] P. Villalobos, A. Ho, J. Sevilla, T. Besiroglu, L. Heim, and M. Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. arXiv preprint arXiv:2211.04325, 2022. [32] L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, S. Huang, K. Rasul, and Q. Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. [33] G. Wan, Y. Wu, J. Chen, and S. Li. Dynamic self-consistency: Leveraging reasoning paths for efficient llm sampling. arXiv preprint arXiv:2408.17017, 2024. [34] J. Wang, M. Fang, Z. Wan, M. Wen, J. Zhu, A. Liu, Z. Gong, Y. Song, L. Chen, L. M. Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024. [35] P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [36] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [37] Y. Wu, Y. Wang, T. Du, S. Jegelka, and Y. Wang. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. [38] Y. Xie, K. Kawaguchi, Y. Zhao, J. X. Zhao, M.-Y. Kan, J. He, and M. Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:4161841650, 2023. [39] Y. Xie, A. Goyal, W. Zheng, M.-Y. Kan, T. P. Lillicrap, K. Kawaguchi, and M. Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. [40] W. Xiong, H. Zhang, N. Jiang, and T. Zhang. An implementation of generative prm, 2024. 11 [41] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [42] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [43] Y. Ye, Z. Huang, Y. Xiao, E. Chern, S. Xia, and P. Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. [44] yentinglin. Aime 2025, 2025. URL https://huggingface.co/datasets/yentinglin/aime_2025. [45] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [46] W. Zeng, Y. Huang, Q. Liu, W. Liu, K. He, Z. Ma, and J. He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [47] Z. Zhang, C. Zheng, Y. Wu, B. Zhang, R. Lin, B. Yu, D. Liu, J. Zhou, and J. Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [48] C. Zheng, Z. Zhang, B. Zhang, R. Lin, K. Lu, B. Yu, D. Liu, J. Zhou, and J. Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. [49] J. Zhong, W. Shen, Y. Li, S. Gao, H. Lu, Y. Chen, Y. Zhang, W. Zhou, J. Gu, and L. Zou. comprehensive survey of reward models: Taxonomy, applications, challenges, and future. arXiv preprint arXiv:2504.12328, 2025. [50] Z. Zhou, X. Ning, K. Hong, T. Fu, J. Xu, S. Li, Y. Lou, L. Wang, Z. Yuan, X. Li, et al. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024. [51] zwhe99. amc23 dataset. https://huggingface.co/datasets/zwhe99/amc23, 2023. Accessed: 2025-05-12. 12 Table 5: Novelty method ablation on AIME24. Best Score bold. Table 6: Novelty weight (λN ) ablation on AIME24 (New Token Counting). Best score bold. Table 7: Beam width (M ) ablation on AIME24. Total paths = 32. Best score bold. Novelty Method Score New Token Counting Cosine Similarity 58.9 58.4 Novelty Weight (λN ) Score Trees (N/M ) Score 0.0 0.5 1.0 57.5 58.9 51.9 2 4 8 16 8 58.9 47.0 44."
        },
        {
            "title": "A Supplementary Materials",
            "content": "We provide additional details and analyses to complement the main paper. Section A.1 includes further ablation studies to dissect the contributions of individual components of our Guided by Gut (GG) framework. Section A.2 provides detailed walkthrough of our Self-Guided Search algorithm. Finally, Section A.3 provides an illustrative example showcasing the step-by-step reasoning process of GG. A.1 Ablation Studies In addition to the confidence-based RL fine-tuning ablation presented in the main paper (Section 5), we ablate the key components of GG. We specifically analyze the impact of the novelty weight (λN ) in our reward formulation and the beam width (M ) within the DVTS search algorithm. Novelty: Method Selection and Weight (λN ) Impact. For the novelty component (st), we consider two methods: The first is cosine similarity using embeddings from sentence transformer. Sentence transformers are models that generate dense vector representations (embeddings) capturing sentence semantics; we specifically employed all-MiniLM-L6-v2 [25]. The second method is to count new words. As shown in Table 5, counting new words performs comparably to cosine similarity but is simpler and computationally lighter. Thus, we selected word counting for (st). Using word counting for novelty, we then investigate the impact of the corresponding weight, λN . Table 6 summarizes this ablation. Setting λN = 0 (no novelty signal) yields score of 57.5. balanced λN = 0.5 achieves the best score of 58.9. Conversely, setting λN = 1 (relying predominantly on novelty) significantly degrades performance to 51.9. This confirms that balancing novelty (for exploration) with confidence is crucial, with confidence being the primary guiding signal. Impact of Beam Width (M ). We examine the effect of the beam width (M ) parameter within our DVTS search strategy, keeping the total path budget fixed at = 32. In DVTS, increasing reduces the number of independent subtrees (N/M ) explored. We compare performance for = 2 (16 subtrees), = 4 (8 subtrees), and = 8 (4 subtrees), with results in Table 7. Increasing while keeping constant entails significantly worse performance: = 2 achieved 58.9, while = 4 dropped to 47.0, and = 8 further decreased to 44.0. This confirms that increasing under fixed budget limits exploration diversity crucial for DVTS, making smaller beam width (M = 2) more effective for the = 32 budget. A.2 Implementation Details of Self-Guided Search To clarify how the search operates, we walk through Algorithm 1 step by step. Our approach builds on Diverse Verifier Tree Search, variant of beam search that partitions the total number of candidate paths into diverse subtrees. Each subtree is then greedily expanded based on confidence-calibrated intrinsic rewards. The algorithm begins with prompt Q, language model πθ, and user-defined hyperparameters such as the total number of paths . In line 2, the LLM is queried with to generate initial reasoning branches, forming the roots of several subtrees. For each subtree, we define maximum search depth and consider candidate next steps at each level. These candidates are scored using the reward in Eq. 4 (line 11), and the highest-scoring step is appended to the current reasoning chain (lines 1213). 13 Algorithm 1: Self-Guided Test-Time Scaling with Confidence-Calibrated DVTS Input: Prompt Q, Model πθ, Beam width , Total paths , Max depth , Token limit τ Output: Final Answer M diverse subtrees from Q; 1 Initialize empty answer set = ; 2 Initialize 3 foreach subtree = 1 to N/M ; 4 do 5 6 7 Initialize path R(j) [ ]; for = 1 to ; do 8 9 10 11 13 14 15 16 17 18 19 20 Generate candidate steps {st foreach candidate st do ; Compute ri = λC C(st) + λN (st); i}M i=1 using model πθ(R(j)); arg maxi ri; Select top-1 step st Append st to R(j); if st then contains final answer token (e.g., boxed{}) ; Extract A(j) and add to set A; break and prune subtree j; if TokenCount(R(j)) > τ or = 1 ; then Append Final Answer to R(j); Traverse each subtree Roll out reasoning steps Score each candidate Answer is complete Force answer near limit 21 Select final answer confidence-weighted vote over A; 22 return The procedure continues recursively until one of the termination criteria (lines 1420) is met: (1) the reasoning chain exceeds depth , (2) the total token budget τ is surpassed, (3) the model exhibits signs of degeneration such as repetitive output, or (4) complete reasoning chain with conclusive final answer is generated. To mitigate premature truncation near token or depth limits, we incorporate model-specific prompts that encourage finalization. For instance, appending \"Final Answer\" is particularly effective with DeepSeek models for reliably triggering conclusive output. Finally, Algorithm 1 aggregates the candidate completions and selects the final answer using confidence-weighted voting scheme defined in Eq. 5. A.3 Search and Self-Guidance Example We analyze particular reasoning trace, distinct from the illustrative example shown in Fig. 2, to demonstrate scenario where an error initially occurred and to highlight the step-by-step operation of the Guided by Gut (GG) framework. Even though intrinsic confidence often serves as reliable guiding signal, the model can still make mistakes, as it lacks mechanism to definitively verify correctness. In the provided example trace, initially, the high-confidence Branch 2 mistakenly computes the sum as 459 pounds, despite confidence score of 0.89 at Step 2 and 0.79 at Step 5. However, the intrinsic confidence eventually leads to self-correction: at Step 7, Branch 2 corrects its previous error, accurately computing the sum as 449 pounds with confidence of 0.82. From this point forward, the correct answer is consistently maintained. This behavior highlights the GG frameworks key strength: it effectively leverages intrinsic signals from the model to guide reasoning decisions with negligible computational overhead, achieving performance comparable to other Test-Time Scaling (TTS) methods like Process Reward Models (PRMs), but crucially without the heavy computational demands typically associated with them."
        },
        {
            "title": "Question and the Reasoning Trace",
            "content": "Question: In the quiet town of Balancing Scales, four friends like to cloak their weights in puzzles. Ivy confides, If you double my weight and then add bakers dozen, youll land exactly on Jaspers number. Jasper counters, Kai is as heavy as the arithmetic mean of Ivy and me. Kai chuckles, That may be so, yet Luna still comes in eight pounds lighter than do. Everyone already knows that Ivy tips the scale at 125 pounds. What is the combined weight of Jasper and Luna, expressed in pounds? Step 2 Branch 1, Confidence Score: 0.88: 2. Jasper and Kais Weight Relationship: Jasper says, \"Kai weighs the average of Ivy and me.\" Let me think about this. If take the average of two numbers, thats their sum divided by two. So, mathematically: Kais weight = Ivys weight+Jaspers weight 2 Plugging in the known values: Kais weight = 125+263 2 Lets compute the numerator first: 125 + 263 = 388 Then divide by 2: 388/2 = 194 So, Kai weighs 194 pounds. Got that down. Branch 2, Confidence Score: 0.89: 2. Jasper and Kais Weight Relationship: Jasper says, \"Kai weighs the average of Ivy and me.\" This means that Kais weight is the average of Ivys and Jaspers weights. So, mathematically: = Ivys weight+Jaspers weight Plugging in the known values: = 125+263 Let me compute the numerator: 125 + 263 = 388 Then divide by 2: 388/2 = 194 So, Kai is 194 pounds. 3. Kai and Lunas Relationship: Kai laughs, Thats true, but Luna still comes in eight pounds lighter than 2 2 am. This tells us that Lunas weight is equal to Kais weight minus 8 pounds. = Since we found as 194 pounds, this gives: = 194 8 = 186 pounds. So, Luna is 186 pounds. 4. We need to find the combined weight of Jasper and Luna: So, thats + L. We already know is 263 and is 186, so adding those together: 263 + 186 = . . . Let me compute that: 200 + 100 = 300 60 + 80 = 140 3 + 6 = 9 Wait, that might not be the right way to break it down. Alternatively, just adding them directly: 263 + 186: * Start with the units place: 3 + 6 = 9 15 * Tens place: 6 + 8 = 14, carryover 1 * Hundreds place: 2 + 1 = 3, plus the carried-over 1 makes So altogether, 459 pounds. Step 3 Branch 1, Confidence Score: 0.5: Let me double-check just in case made any arithmetic mistakes. Branch 2, Confidence Score: 0.66: Wait, let me verify each step again to make sure didnt make an arithmetic error. Step 5 Branch 1, Confidence Score: 0.73: 263 + 186: Starting from the right: 3 + 6 = 9 6 + 8 = 14, carryover 1 2 + 1 + 1(carried) = 4 So, yes, 449. Previously, thought 388/2 was 194, then 194 8 = 186, and then adding 263 + 186. Branch 2, Confidence Score: 0.79: Compute 263 + 186: Units place: 3 + 6 = 9 Tens place: 6 + 8 = 14, write down 4, carryover 1 Hundreds place: 2 + 1 = 3, plus the carried-over 1 makes So total is 459. So why did think it was 449 earlier? Maybe mistake in initial breakdown. Step 6 Branch 1, Confidence Score: 0.6: Wait, no, my breakdown seems correct: 400 + 40 + 9 = 449. Oh! Wait, waitno. 263 + 186: Maybe an error in adding. Lets try adding 263 + 186 step by step. Branch 1, Confidence Score: 0.52: Wait, 263 + 186: think miscalculated the tens and hundreds in my head. Maybe its 459, correct. Step Branch 1, Confidence Score: 0.68: Starting from the right: 3 (units place) + 6 (units) = 9 6 (tens place) + 8 (tens) = 14, carryover 1 2 (hundreds place) + 1 (hundreds) = 3, plus the carried over 1 = 4 So, its 459. So correct. Branch 2, Confidence Score: 0.82: 263 + 186. Break it down: Start with 263. Let me add 100 to it: 263 + 100 = 363 Then add 80: 363 + 80 = 443 Then add 6: 443 + 6 = 449 Step 8 Branch 1, Confidence Score: 0.81: Wait, so if add 263 + 186: 200 + 100 = 300 60 + 80 = 140 3 + 6 = 9 16 Total: 300 + 140 + 9 = 449. Branch 2, Confidence Score: 0.45: Wait earlier, when thought 449, was off by one. No, waitno, actually 263 + 186 = 449 is incorrect. So where is the confusion? Is it an arithmetic error? Step 18 Branch 1, Confidence Score: 0.73: Wait, so earlier, when thought it was 459, that was mistake. Branch 2, Confidence Score: 0.67: Wait, so thats the correct combined weight. So earlier when thought 459, that was miscalculation. So now, the answer is 449 pounds."
        }
    ],
    "affiliations": [
        "ECE Department, University of Alberta",
        "ECE Department, University of Toronto"
    ]
}