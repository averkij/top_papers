{
    "paper_title": "Improve Representation for Imbalanced Regression through Geometric Constraints",
    "authors": [
        "Zijian Dong",
        "Yilei Wu",
        "Chongyao Chen",
        "Yingtian Zou",
        "Yichi Zhang",
        "Juan Helen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 6 7 8 0 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Improve Representation for Imbalanced Regression\nthrough Geometric Constraints",
            "content": "Zijian Dong1,* Yilei Wu1,* Chongyao Chen2,* Yingtian Zou1 Yichi Zhang1 Juan Helen Zhou1, 1National University of Singapore,"
        },
        {
            "title": "2 Duke University",
            "content": "{zijian.dong, yilei.wu}@u.nus.edu, helen.zhou@nus.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classificationbased methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometrybased loss functions. Code is available here. Figure 1. 2D feature space of vanilla baseline and ours from UCI-Airfoil [2]. The vanilla feature space lacks uniformity and is dominated by samples from the Many-shot region. In contrast, our approach achieves more uniform distribution over the feature space, improving the performance, especially in the Medium and Few-shot regions. (For visualization purposes, we curated the dataset to ensure equal partitions across the three regions.) 1. Introduction Imbalanced datasets are ubiquitous across various domains, including image recognition [30], semantic segmentation [31], and regression [25]. Previous studies have demonstrated the significance of uniform or balanced distribution of class representations for effective imbalanced classification [5, 8, 9, 12, 22, 26, 33] and imbalanced semantic segmentation [31]. In classification tasks, these representations typically form distinct clusters. However, in the context of regression, representations are expected to be continuous and ordered [24, 27], rendering the methods used for quantifying and analyzing *Equal contribution Corresponding author uniformity in classification inapplicable. While the issue of deep imbalanced regression (DIR) has received considerable attention, the focus has predominantly been on training unbiased regressors, rather than on the aspect of representation learning [6, 10, 16, 19, 25]. Among the methods that do explore representation learning [24, 27], the emphasis is typically on understanding the relationship between the label space and the feature space (the representations themselves should be continuous and ordered). However, critical aspect that remains under-explored is the interaction between data representations and the entire feature space. Specifically, how these representations distribute within the full scope of the feature space has not been examined."
        },
        {
            "title": "Uniformity in classification refers to how effectively",
            "content": "1 Figure 2. t-SNE visualization [20] of feature comparison. The first row corresponds to the original UCI Airfoil Dataset [2], while the second row corresponds to its curated version, with an additional few-shot region in the middle of the label range. Colored arrows point to the few-shot regions and their corresponding positions in the feature distributions. We evaluate feature distributions using: MSE Loss (Baseline), SRL without uniformity loss (w/o Lenv), SRL without homogeneity loss (w/o Lhomo), and complete SRL (ours). The baseline leads to feature collapse to many-shot regions and inadequate distinction of few-shot samples. In w/o Lenv, features collapse into trivial shape, not fully utilizing the feature space. In w/o Lhomo, features spread out along the trace. Different from the previous ones, our SRL uniformly and smoothly fills the feature space. different clusters or centroids occupy the feature space, essentially partitioning it among various classes. In regression, here we define the term latent trace as the pathway that the representations follow, delineating the transition from the minimum to the maximum label values. In this paper, we aim to evaluate how well latent trace occupies the feature space? To quantify this, we approximate tubular neighborhood around the latent trace and measure its volume relative to the entire feature space. This method gauges the effectiveness of the trace in enveloping the hypersphere, and we call it enveloping loss. This loss ensures that the trace shape fills the surface of the hypersphere to facilitate uniformity. In parallel, it is equally important that the points (i.e., individual data point representations) are evenly distributed along the trace. To address uniform distribution along the trace as well as smoothness, we have developed homogeneity loss. This loss is computed based on the arc length of the trace, allowing us to effectively measure and promote an even and smooth distribution of points. We model the uniformity in regression in two aspects: the induced trace aims to fully occupy the surface of the hypersphere (Enveloping), exhibiting smoothness with representations spaced at uniform intervals (Homogeneity). The two losses we introduce act as geometric constraints on latent trace, implying they should not be applied to set of representations from single mini-batch. This is because single batch likely does not encompass the full range of labels. To address this, we have developed Surrogate-driven Representation Learning (SRL) scheme. It involves averaging representations of the same bins within mini-batch to form centroids and re-filling missing bins by taking corresponding centroids from the previous epoch. This process results in surrogate containing centroids for all bins, enabling the effective application of geometric loss across the complete label range. Furthermore, we introduce Imbalanced Operator Learning (IOL) as new DIR benchmark for training models on imbalanced domain locations in function space mapping. In summary, our main contributions are four-fold: Geometric Losses for Uniformity in deep imbalanced regression (DIR). To the best of our knowledge, this work is the first to study representation learning in DIR. We introduce two novel loss functions, enveloping loss and homogeneity loss, to ensure uniform feature distribution for DIR. SRL Framework. new framework is proposed that incorporates these geometric principles into data representations. Imbalanced Operator Learning (IOL). For the first time, we pioneer the task of operator learning within the realm of deep imbalanced regression, introducing an innovative task: Imbalanced Operator Learning (IOL). Extensive Experiments. The effectiveness of the proposed method is validated through experiments involving real-world regression and operator learning, on five datasets: AgeDBDIR, IMDB-WIKI-DIR, STS-B-DIR, and two newly created DIR benchmarks, UCI-DIR and OL-DIR. 2. Related Work Uniformity in imbalanced classification. Wang and Isola [22] identifies that uniformity is one of the key properties in contrastive representation learning. To promote uniformity in representation space for imbalanced classification, variety of training strategies have been proposed. Kang et al. [8] 2 decouples the training into two-stage training of representation learning and classification. Yin et al. [26] designs transfer learning framework for imbalanced face recognition. Kang et al. [9] combines supervised method and contrastive learning to learn discriminative and balanced feature space. PaCo [5] and TSC [12] learn set of class-wise balanced centers. BCL [33] balances the gradient distribution of negative classes and data distribution in mini-batch. Recent study suggests that sample-level uniform distribution may not effectively address imbalanced classification, advocating for category-level uniformity instead [1, 32]. Though progress has been made in this field, challenges persist in adapting the approach of modeling uniformity from classification to regression. Deep imbalanced regression. With imbalanced regression data, effective learning in continuous label space focuses on modeling the relationships among labels in the feature space [25]. Label Distribution Smoothing (LDS) [25] and DenseLoss [19] apply Gaussian kernel to the observed label density, leading to an estimated label density distribution. Feature distribution smoothing (FDS) [25] generalizes the application of kernel smoothing from label to feature space. Ranksim [6] aims to leverage both local and global dependencies in data by aligning the order of similarities between labels and features. Balanced MSE [16] addresses the issue of imbalance in Mean Squared Error (MSE) calculations, ensuring more balanced distribution of predictions. VIR [23] provides uncertainty for imbalanced regression. ConR [10] regularizes contrastive learning in regression by modeling global and local label similarities in feature space. RNC [27] and SupReMix [24] learn continuous and ordered representation for regression through supervised contrastive learning. How imbalanced regression representations leverage the feature space remains under-explored. 3. Method In the field of representation learning for classification, the concept of uniformity is pivotal for maximizing the use of the feature space [5, 8, 9, 12, 22, 26, 33]. This idea is based on the principle of ensuring that features from different classes are not only distinctly separated but also evenly distributed in the latent space. This uniform distribution of class centroids fosters clear and effective decision boundary, leading to more accurate classification. However, in regression, where we deal with continuous, ordered trace [24, 27] rather than discrete clusters, the concept of uniformity is not only more complex but essentially remains undefined. We draw an analogy to the process of winding yarn around ball. In this analogy, the yarn represents the latent trace, and the ball symbolizes the entirety of the available feature space. Just as the yarn must be evenly distributed across the balls surface to effectively cover it (without any crossing), the latent trace should strive to occupy the hypersphere of the latent space uniformly. This ensures that the model leverages the available feature space to its fullest extent, enhancing the models ability Figure 3. 2D schematic overview of two geometric losses. The arrow indicates the improvement of the loss function. Enveloping loss encourages the representations to fill the latent space, and homogeneity loss encourages the smoothness and even distribution of the representations along the trace. Furthermore, to capture the variability inherent in the data. the latent trace should be smooth and continuous, akin to the even stretching of yarn, rather than loose and disjointed. This smoothness ensures consistent and predictable model behavior, which is crucial for the accurate prediction and interpretation of results. We outline our method in this section. Firstly, we establish the fundamental notations and preliminaries (Section 3.1). Following this, we delve into the concept and definition of our enveloping loss (Section 3.2) and homogeneity loss (Section 3.3). Finally, we present our Surrogate-driven Representation Learning (SRL) framework, which incorporates the geometric constraints from the global image of the representations into the local range (Section 3.4). Refer to Supplementary Material 9 for the pseudo code of our method. 3.1. Preliminaries regression dataset is composed of pairs (xi,yi), where xi represents the input and yi is the corresponding continuous-value target. Denote zi = f(xi) as the feature representation of xi, generated by neural network f(). The feature representation is normalized so that zi = 1 for all i. Suppose the dataset consists of unique bins 1, we define surrogate as set of centroids ck, where each represents distinct bin. These centroids are computed by averaging the representations sharing the same bin and they are normalized to ck=1. Let be path: : [ymin,ymax] (cid:55) Rn with l(y) = 1, such that l(yk)=ck. The path is continuous curve extended from the discrete dataset that lies on submanifold of Rn. 3.2. Enveloping To maximize the use of the feature space, it is crucial for the ordered and continuous trace of regression representations to fill the entire unit hypersphere as much as possible. This is by 1The binning process (only for geometric loss calculations) facilitates surrogate formulation by using discrete centroids to approximate continuous trace. Binning is unnecessary for most DIR datasets since they come pre-binned. The performed regression tasks follow the original dataset settings. 3 analogy with wrapping yarn with certain length around ball (without any crossing), aiming to cover as much surface area as possible. The trace of regression representations lying on submanifold of Rn has negligible hypervolume, which makes it challenging to assess its relationship with the entire hypersphere. To address this challenge, we extend the line into tubular neighborhood. This expansion allows us to introduce the concept of enveloping loss. Our objective with this loss function is to maximize the hypervolume of the tubular neighborhood in proportion to the total hypervolume of the hypersphere. Denote the set of all unit vectors in Rn as U. Given ϵ(0,1), define tubular neighborhood (l,ϵ) of as: (l,ϵ)={zU tz>ϵ for some tIm(l)} (1) where for function : B, the image is defined as Im(f):={f(x),xA}. Then our enveloping loss is defined as: Lenv = vol(T (l,ϵ)) vol(U) (2) where vol() returns the hypervolume of its input in the induced measure from the Euclidean space. In practical scenarios, the trace is composed of discrete representations, which complicates the direct computation of the tubular neighborhoods hypervolume. To navigate this challenge, we propose continuous-to-discrete strategy. We first generate points that are uniformly distributed across the hypersphere. We then determine the fraction of these points that fall within the neighbourhood ϵ. This fraction effectively approximates the proportion of the hypersphere covered by the tubular neighborhood with sufficiently large N. To adapt Lenv to discrete datasets, we re-formalize our optimization objective as: max lim (N) where (N):={pi max {pil(y)}>ϵ,i[N]} (3) (4) assuming for each > 0, we can choose evenly distributed points in U, and denote these points as pi,i [N] = 1,...,N. For numerical application, we take to be sufficiently large number and use the standard Monte-Carlo method [17] to approximate the evenly distributed points. In our implementation, we did not directly define ϵ due to the non-differentiability of the binarization required to determine if pi is within the ϵ-tube. Instead, for each pi, we maximize the cosine similarity between pi and its closest point on the trace. In this way, we relax the step function represented by (4) to its soft version, leading to smooth gradient computation. 4 3.3. Homogeneity While the enveloping loss effectively governs the overall distribution of representations on the hypersphere, it alone may not be entirely adequate, presenting two unresolved issues. 1) The first is distribution along the trace. The enveloping loss predominantly controls the overall shape of representations on the hypersphere, yet it does not guarantee uniform distribution along the trace. This poses notable concern, as it may result in uneven representation density across different trace segments. 2) The second is trace smoothness. The enveloping loss could lead to zigzag pattern of the representations, which should be avoided. Considering age estimation from facial images as an example, the progression of facial features over time is gradual. Consequently, in the corresponding latent space, we would anticipate similar, smooth transition without abrupt changes, underlining the desirability of smoother trace. Interestingly, these two issues can be aptly analogized to winding yarns around ball as well. For the yarn on the ball to be smooth, it should be tightly stretched, rather than being disjointed or loosely arranged. We name the property of trace to be smooth with representations evenly distributed along it as homogeneity. We encourage such homogeneity property, i.e., smoothness of the trace Im(l) and uniform distribution of representations along it, by penalizing the arc length. Formally, the homogeneity loss is defined as: Lhomo = (cid:90) ymax ymin (cid:13) (cid:13) (cid:13) (cid:13) dl(y) dy (cid:13) 2 (cid:13) (cid:13) (cid:13) dy (5) Given different ys which have been ordered, the discrete format for Lhomo is defined as summation of the squared differences between adjacent points: Lhomo = K1 (cid:88) k=1 l(yk+1)l(yk)2 yk+1yk (6) The use of only homogeneity loss might result in trivial solutions like representation convergence to circle or point due to feature collapse (shown in Figure 2). The homogeneity loss should be treated as regularization of the enveloping loss, promoting not only smoothness but also an even distribution of representations along the trace. To quantitatively define the relationship between trace arc length and these desired characteristics, we introduce Theorem 1. It demonstrates that with given Im(l) (lenv is fixed as it does not depend on the parameterization of l), the homogeneity loss is minimized if and only if when representations are uniformly distributed along the trace. Theorem 1. Given an image of l, Lhomo attains its minimum if and only if the representations are uniformly distributed along the trace, i.e., yl(y)=c, where is constant. Refer to Supplementary Material 6 for the proof. Therefore, we formulate our geometric constraints (LG) as combination of enveloping and homogeneity: The surrogate is then generated by re-filling the missing centroids: =C{c YY} (9) We use AdamW [13] with momentum to update parameters θ in f(), to ensure smooth transition of the local shape in the batch-wise representations. At the end of each epoch (0,E] (excluding the first), we use the representations learned during that epoch to form running surrogate ˆSe. Se+1 is formulated from the current epochs surrogate Se and ˆSe with momentum. Se+1 is employed for training in the subsequent epoch. It facilitates gradual transition between epochs, preventing abrupt variations: Se+1 αSe+(1α) ˆSe. We aim for individual representations from the encoder to converge towards their respective centroids that share the same label, while simultaneously distancing them from centroids associated with different labels. To achieve this, we incorporate contrastive loss between the individual representations and the centroids. For each representation zm with ym = y, the centroid cy is considered as the positive, and the other centroids as negatives. The contrastive loss is defined as: Lcon = (cid:88) log m=1 exp(sim(zm,cy)) yYexp(sim(zm,cy)) (cid:80) (10) where sim() is the cosine similarity between two input. The framework is trained end-to-end, the total loss used to update the parameters θ in f() is defined as: Lθ =Lreg+LG+Lcon (11) where Lreg is the mean squared error (MSE) loss. 4. Experiments We perform extensive experiments to validate and analyze the effectiveness of SRL for deep imbalanced regression. Our regression tasks span age estimation from facial images, tabular regression, and text similarity score regression, as well as our newly established task: Imbalanced Operator Learning (IOL). This section begins by detailing the experiment setup (Section 4.1) followed by the main results (Section 4.2). The results of IOL are shown in Section 4.3 followed by the comparison with classification-based methods and hyperparameters analysis (Section 4.4). 4.1. Experiment Setup Datasets. We employ three real-world regression datasets developed by Yang et al. [25], and our curated UCI-DIR from UCI Machine Learning Repository [2], to assess the effectiveness of SRL in deep imbalanced regression. Refer to Supplementary Material 7 for more dataset details. 5 Figure 4. Overview of Surrogate-driven Representation Learning (SRL). (1) Every mini-batch is encoded to the latent space. Some bins may not be present in the current batch. To address this, (2) it takes centroids corresponding to the missing bins from the previous epoch. These stored centroids are used to re-fill the missing bins in the current batch. (3) Average the representations for bins that appear multiple times, creating centroids for these bins. This surrogate, containing representation for the full label range, allows for the effective application of geometric loss across all bins. (4) Loss calculation based on the surrogate. (5) Update the surrogate in memory to ensure enveloping and homogeneity. The training of the first epoch is driven by MSE loss only. LG =λeLenv+λhLhomo (7) where λe and λh are weights for the two geometric losses. In Section 4.4, we further explore the behavior of these two geometric constraints, uncovering new insights into imbalanced regression. 3.4. Surrogate-driven Representation Learning (SRL) Our geometric loss (LG) is calculated on surrogate instead of mini-batch (Figure 4), as the representations from one mini-batch very likely fail to capture the global image of l, due to the randomness of batch sampling. For illustration purposes, here we assume the original dataset has already been binned, as is the case in most DIR datasets [6, 10, 25]. Let ={z1,z2,...,zM } be set of representations from batch with batch-size M, and let = {y1,y2,...,yM } (repetitions of the label values might exist) be the corresponding labels. Define the centroid cy for (cy=1) as: cy = 1 {zm ym =y} (cid:88) zm {zmym=y} (8) Suppose the whole label range is covered by set of unique K}. The centroids for these bins }. bins = {y 2,...,y from the last epoch are denoted as = {c ,...,c 1,y ,c 1 2 Table 1. Results on UCI-DIR (MAE). We report the average MAE of three runs. The best results are in bold. Datasets Shot Airfoil Abalone Real Estate Concrete All Many Med Few All Many Med Few All Many Med Few All Many Med Few 5.66 VANILLA 5.76 LDS + FDS [25] 5.23 RankSim [6] BalancedMSE [16] 5.69 Ordinal Entropy [29] 6.27 5.11 4.45 5.05 4.51 4.85 5.03 6.75 4.57 4.79 7.79 5.09 4.91 5.72 4.33 5.04 7.28 5.37 5.37 8.32 6.77 0.88 0.90 0.98 2.14 2.31 0.33 7.97 2.65 0.35 9.26 3.26 0.37 7.42 2.59 2.66 0.34 9.37 4.01 11.61 0. 0.27 0.33 0.34 0.31 0.29 0.38 0.37 7.29 0.40 0.34 6.88 0.38 0.40 6.71 0.40 0.33 7.03 0.42 0.35 7.12 5.77 6.21 6.00 4.67 5.50 6.92 9.74 6.73 7.59 5.57 9.46 6.37 9.72 6.36 9.31 SRL (ours) 5. 4.83 4.75 5.69 4.16 0.89 2.42 7.19 0. 0.26 0.30 0.29 5.94 5.32 5.80 6.60 Table 2. Results on AgeDB-DIR, the best are in bold. Metrics Shot MAE GM All Many Med Few All Many Med Few 7.67 VANILLA 7.55 LDS + FDS [25] 7.41 RankSim [6] 7.98 BalancedMSE [16] Ordinal Entropy [29] 7.60 7.41 ConR [10] 6.66 7.03 6.49 7.58 6.69 6. 9.30 12.61 4.85 8.46 10.52 4.86 8.73 12.47 4.71 8.65 5.01 9.93 8.87 12.68 4.91 8.81 12.04 4.70 4.17 4.57 4.15 4.83 4.28 4.13 6.51 8.98 5.38 6.75 5.74 8.92 5.46 6.30 6.20 9.29 5.91 8.59 SRL (ours) 7.22 6. 8.28 9.81 4.50 4.12 5.37 6.29 Table 3. Results on IMDB-WIKI-DIR, the best are in bold. Metrics Shot MAE GM All Many Med Few All Many Med Few 8.03 VANILLA 7.73 LDS + FDS [25] 7.72 RankSim [6] 8.43 BalancedMSE [16] Ordinal Entropy [29] 8.01 7.84 ConR [10] 7.16 7.22 6.92 7.84 7.17 7.15 15.48 26.11 4.54 12.98 23.71 4.40 14.52 25.89 4.29 13.35 23.27 4.93 15.15 26.48 4.47 14.36 25.15 4.43 4.14 4.17 3.92 4.68 4.07 4.05 10.84 18.64 15.77 7.87 18.02 9.72 15.51 7.90 10.56 21.11 18.55 9. SRL (ours) 7.69 7.08 12.65 22.78 4.28 4.03 7.28 15. Table 4. Results on STS-B-DIR, the best are in bold. Metrics Shot MSE Pearson correlation All Many Med Few All Many Med Few 0.993 0.963 1.000 1.075 0.742 0.685 0.693 0.793 VANILLA 0.900 0.911 0.881 0.905 0.757 0.698 0.723 0.806 LDS + FDS [25] 0.889 0.907 0.874 0.757 0.763 0.708 0.692 0.842 RankSim [6] 0.909 0.894 1.004 0.809 0.757 0.703 0.685 0.831 BalancedMSE [16] Ordinal Entropy [29] 0.943 0.902 1.161 0.812 0.750 0.702 0.679 0.767 SRL (ours) 0.877 0.886 0.873 0.745 0.765 0.708 0.749 0. AgeDB-DIR [25]: It serves as benchmark for estimating age from facial images, which is derived from the AgeDB dataset [15]. It contains 12,208 images for training, 2,140 images for validation, and 2,140 images for testing. IMDB-WIKI-DIR [25]: It is facial image dataset for age estimation derived from the IMDB-WIKI dataset [18], which consists of face images with the corresponding age. It has 191,509 images for training, 11,022 images for validation, and 11,022 for testing. STS-B-DIR [25]: It is natural language dataset formulated from STS-B dataset [3, 21], consisting of 5,249 training 6 sentence pairs, 1,000 validation pairs, and 1,000 testing pairs. Each sentence pair is labeled with the continuous similarity score. UCI-DIR: To evaluate the performance of SRL on tabular data, we curated UCI Machine Learning Repository [2] to formulate UCI-DIR that includes four regression tasks (Airfoil Self-Noise, Abalone, Concrete Compressive Strength, Real estate valuation). Following the DIR setting [25], we make each regression task consist of an imbalanced training set and balanced validation and test set. Metrics. In line with the established settings in DIR [25], subsets in an imbalanced training set are categorized based on the number of available training samples: many-shot region (bins with > 100 training samples), medium-shot region (bins with 20 to 100 training samples), and few-shot region (bins with < 20 training samples), for the three real-world datasets. For AgeDBDIR and IMDB-WIKI-DIR, each bin represents 1 year. In the case of STS-B-DIR, bins are segmented by 0.1. For UCI-DIR, the bins are segmented by 0.1 to 1 depending on the range of regression targets. Our evaluation metrics include mean absolute error (MAE, the lower the better) and geometric mean (GM, the lower the better) for AgeDB-DIR, IMDB-WIKI-DIR and UCIDIR. For STS-B-DIR, we use mean squared error (MSE, the lower the better) and Pearson correlation (the higher the better). Implementation Details. For age estimation (AgeDB-DIR and IMDB-WIKI-DIR), we follow the settings from Yang et al. [25], which uses ResNet-50 [7] as backbone network. For text similarity regression (STS-B-DIR), we follow the setting from Cer et al. [3], Yang et al. [25] that uses BiLSTM + GloVe word embeddings. For tabular regression (UCI-DIR), we use an MLP with three hidden layers (d-20-30-10-1) following the setting from Cheng et al. [4]. For all baseline methods, results were produced following provided training recipes through publicly available codebase. All experimental results, including ours and baseline methods, were obtained from server with 8 RTX 3090 GPUs. Baselines. We consider both DIR methods [6, 10, 25] and recent techniques proposed for general regression [16, 28, 29], in addition to VANILLA regression (MSE loss). We compare the performance of SRL with all baselines on the above four datasets. Furthermore, as SRL is orthogonal to previous DIR methods, we examine the improvement of them by adding our geometric losses. Figure 5. SRL performance gain compared to VANILLA across age ranges on AgeDB-DIR. The gray histogram in the background shows the distribution of samples across age groups. SRL substantially improves the performance on the medium-shot and few-shot regions (age < 20 and > 70). Table 5. Combine SRL with existing DIR methods (MAE) Datasets Shot AgeDB (MAE) IMDB-WIKI (MAE) All Many Med Few All Many Med Few SRL+LDS+FDS [25] GAINS v.s. LDS+FDS (%) 7.32 6.81 8.14 9.81 7.61 7.03 12.28 21.77 8.19 3.05 3.23 3.89 6.75 1.66 2.64 5.40 SRL+RankSim [6] GAINS v.s. RankSim (%) 7.29 6.57 8.58 10.48 7.67 7.08 12.40 22.85 1.62 -1.23 1.72 16.96 0.65 -1.15 14.61 11.75 SRL+BalancedMSE [16] GAINS v.s. BalancedMSE (%) 9.27 10.69 9.14 0.89 8.19 9.06 7.24 6.77 7.86 9.85 7.74 7.13 12.77 22.04 5.29 4.35 SRL+ConR [10] GAINS v.s. ConR (%) 7.40 6.87 8.08 10.50 7.56 7.01 12.03 21.71 0.14 -5.53 8.39 13.80 3.68 1.96 16.23 13. 4.2. Main Results To show the effectiveness of SRL on DIR, we first benchmark SRL and baselines for tabular regression on our curated UCIDIR with four different regression tasks  (Table 1)  . Moreover, we evaluate our method on established DIR benchmarks [25] including age estimation on AgeDB-DIR and IMDB-WIKI-DIR (Table 2 & 3, and Figure 5), and text similarity regression on STS-B-DIR  (Table 4)  . We evaluate the combination of SRL and previous DIR methods on AgeDB-DIR and IMDB-WIKI-DIR  (Table 5)  . Notably, Table 1 and 4 omit results from ConR [10], as it depends on data augmentation, technique not fully established in the domain of tabular data and natural language. Combine SRL with existing methods. Our SRL approach enhances imbalanced regression by imposing geometric constraints on feature distributions, strategy that is orthogonal to existing methods. To illustrate this, we leverage SRL as regularizing term in conjunction with other methods. The results of this experiment are presented in Table 5. It shows that when SRL is integrated with existing regression methods, there is improvement in performance across different regions for both datasets. This demonstrates the effectiveness and compatibility of SRL as complementary tool in the realm of regression analysis. 4.3. Imbalanced Operator Learning (IOL) We introduce novel task for DIR called Imbalanced Operator Learning (IOL). Traditional operator learning aims to train neural network to model the mapping between function spaces [11, 14]. However, unlike the standard approach of uniformly 7 sampling output locations, in IOL, we intentionally adjust the sampling density within the output functions domain to create regions with few, medium, and many regions (Figure 6). For the linear operator, the model is trained to estimate the integral operator denoted as G: G:u(x)(cid:55)s(x)= (cid:90) 0 u(τ)dτ,x[0,1] (12) where denotes the input function which is sampled from Gaussian random field (GRF), and is the target function. For the nonlinear operator, the model is trained to learn particular stochastic partial differential equation (PDE): (cid:17) (cid:16) eb(x;ω)u(x;ω) div =f(x),x[0,1] (13) where eb(x;ω) is the diffusion efficient and u(x;ω) is the target function. Denote the domain of output function as y. For both linear and non-linear operator learning, we changed the original uniform sampling of to three curated regions: few/medium/many. Afterward, we manually created an imbalanced training set of 10k samples and balanced testing test of 100k samples, namely OL-DIR. In Figure 6, we have schematic overview of Imbalanced Operator Learning (IOL). The network is trained to model an integral operator G. The data provided to the model is ([u,y],G(u)(y)). The input consists of function and sampled ys from the domain of G(u). The target is G(u)(y). We manipulate the distribution density of across its range to formulate few/med./many regions. Here the imbalance comes from the unequal exposure of integral interval to the model training. Refer to Supplementary Material 7.2 for more details. Shown in Table 6, SRL consistently outperforms VANILLA and the state-of-the-art operator learning for the whole label range including all, many-shot, medium-shot, and few-shot regions. The results position SRL as the superior approach for IOL in terms of accuracy and generalizability. 4.4. Further Analysis Quantification of geometric impact. We further quantified the impact of geometric constraints by comparing percentages of get centroids at maximal distances from one another. However, this strategy adversely affects the ordinality and continuity which are essential for regression tasks. As result, such methods often lead to suboptimal performance for imbalanced regression, even worse than any of the regression baselines shown in Figure 1. Balancing of enveloping and homogeneity: Our proposed SRL advocates for two pivotal geometric constraints in feature distribution: enveloping and homogeneity, to effectively address imbalanced regression. These two losses are modulated by their respective coefficients, λe for the enveloping loss and λh for the homogeneity loss. Figure 8 illustrates that the omission of either constraint detrimentally impacts the performance, highlighting the importance of both of them, and it demonstrates that the best performance, as measured by Mean Absolute Error (MAE) on the AgeDB-DIR dataset, is achieved when both coefficients λe and λh are set to 1e1. Ablation studies on choices of N: Table 10 (in Supplementary Material) shows that achieving optimal performance on the AgeDB-DIR and IMDB-WIKI-DIR datasets requires sufficiently large N, as smaller may lead to imprecise calculation of the enveloping loss. Ablation studies on proposed loss component: Table 11 (in Supplementary Material) demonstrates that incorporating homogeneity, enveloping, and contrastive loss term yields superior model performance compared to using each individually. Computational cost: As shown in Table 12 (in Supplementary Material), the computational overhead introduced by the Surrogate-driven Representation Learning (SRL) framework is comparable to that of other imbalanced regression methods. Impact of bin numbers: As shown in Table 13 (in Supplementary Material), while increasing the number of bins generally leads to better model performance, the improvements become marginal beyond certain thresholds. Limitations: Section 11 (in Supplementary Material) examines the limitations of SRL, including its inability to handle higher-dimensional labels. Figure 7. Comparison of the feature distributions among KCL [9], TSC [12] and SRL(ours) on UCI-Airfoil. All methods aim to promote uniformity in feature distribution while KCL and TSC are originally proposed for imbalanced classification. 5. Conclusion As the first work of exploring uniformity in deep imbalanced regression, we introduce two novel loss functions - enveloping 8 Figure 6. Imbalanced Operator Learning. Table 6. Results on OL-DIR. We report the average MAE of ten runs. The best results are bold. Operation Shot"
        },
        {
            "title": "Linear",
            "content": "MAE(103) MSE (104) All Many Med Few All Many Med Few VANILLA [14] Ordinal Entropy [29] 10.07 9.18 SRL (ours) 15.64 11.86 15.45 27.00 13.01 9.33 9.85 9.47 9.26 8.32 5.40 2.00 1.98 2.81 1.53 0. 4.40 1.89 1.72 14.20 3.42 2."
        },
        {
            "title": "Nonlinear",
            "content": "VANILLA [14] 11.64 Ordinal Entropy [29] 12.91 11.25 SRL (ours) 9.89 9.93 9.48 11.02 19.77 9.20 13.07 21.02 13.80 8.60 17.00 9.22 4.33 8.82 7.42 7.53 24.70 11.84 30.12 14.12 6.41 Full results with standard deviation are reported in Supplementary Material 8.10. uniformly sampled points within few-shot regions (a measure of proportion). The results show our method significantly increases few-shot proportion (AgeDB-DIR: 1.98%15.80%, upper bound: 23%; STS-B-DIR: 4.52% 22.39%, upper bound: 38%), leading to improved performance  (Table 7)  . Table 7. Impact of geometric constraints on few-shot proportion. Few-shot"
        },
        {
            "title": "MAE",
            "content": "AgeDB-DIR (1.10% samples, 23% label range): VANILLA LDS + FDS [25] Ours 1.98% 4.95% 15.80% STS-B-DIR (3.49% samples, 38% label range): VANILLA LDS + FDS [25] Ours 4.52% 8.13% 22.39% 12.61 10.52 9.81 1.075 0.905 0."
        },
        {
            "title": "MAE",
            "content": "7.67 7.55 7.22 0.993 0.900 0.745 Compare with methods for long-tailed classification: In Figure 7, we compare the feature distribution of our method with KCL [9] and TSC [12]. This comparison reveals that classification-based approaches like KCL and TSC tend to distribute feature clusters on the hypersphere by positioning the tarIEEE conference on computer vision and pattern recognition, pages 770778, 2016. 6 [8] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In International Conference on Learning Representations, 2019. 1, 2, 3 [9] Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representation learning. In International Conference on Learning Representations, 2020. 1, 3, 8 [10] Mahsa Keramati, Lili Meng, and David Evans. Conr: Contrastive regularizer for deep imbalanced regression. arXiv preprint arXiv:2309.06651, 2023. 1, 3, 5, 6, 7, [11] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):197, 2023. 7 [12] Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe Yang, Rogerio Feris, Piotr Indyk, and Dina Katabi. Targeted supervised contrastive learning for long-tailed recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69186928, 2022. 1, 3, 8 [13] Ilya Loshchilov and Frank Hutter. Decoupled weight decay In International Conference on Learning regularization. Representations, 2018. 5 [14] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature machine intelligence, 3(3):218229, 2021. 7, 8, 1 [15] Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 5159, 2017. 6 [16] Jiawei Ren, Mingyuan Zhang, Cunjun Yu, and Ziwei Liu. Balanced mse for imbalanced visual regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79267935, 2022. 1, 3, 6, 7 [17] Christian Robert, George Casella, and George Casella. Monte Carlo statistical methods. Springer, 1999. 4 [18] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from single image without facial landmarks. International Journal of Computer Vision, 126 (2-4):144157, 2018. 6 [19] Michael Steininger, Konstantin Kobs, Padraig Davidson, Anna Krause, and Andreas Hotho. Density-based weighting for imbalanced regression. Machine Learning, 110:21872211, 2021. 1, 3 [20] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. 2 [21] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. 6 [22] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 99299939. PMLR, 2020. 1, 2, 3 Figure 8. Confusion matrix of MAE on AgeDB-DIR from different values of λh and λe. and homogeneity loss - to encourage the uniform feature distribution of an ordered and continuous trajectory. The two loss functions serve as geometric constraints which are integrated into the data representations through Surrogate-driven Representation Learning (SRL) framework. Furthermore, we set new benchmark in imbalanced regression: Imbalanced Operator Learning (IOL). Extensive experiments on real-world regression and operator learning demonstrate the effectiveness of our geometrically informed approach. We emphasize the significance of uniform data representation and its impact on learning performance in imbalanced regression scenarios, advocating for more balanced and comprehensive utilization of feature spaces in regression models."
        },
        {
            "title": "References",
            "content": "[1] Mido Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, and Nicolas Ballas. The hidden uniform cluster prior In The Eleventh International in self-supervised learning. Conference on Learning Representations, 2022. 3 [2] Arthur Asuncion and David Newman. Uci machine learning repository, 2007. 1, 2, 5, 6 [3] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017. 6 [4] Xin Cheng, Yuzhou Cao, Ximing Li, Bo An, and Lei Feng. arXiv Weakly supervised regression with interval targets. preprint arXiv:2306.10458, 2023. [5] Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya Jia. Parametric contrastive learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 715724, 2021. 1, 3 [6] Yu Gong, Greg Mori, and Fred Tung. Ranksim: Ranking similarity regularization for deep imbalanced regression. In International Conference on Machine Learning, pages 76347649. PMLR, 2022. 1, 3, 5, 6, 7, 4 [7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the 9 [23] Ziyan Wang and Hao Wang. Variational imbalanced regression: Fair uncertainty quantification via probabilistic smoothing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 3 [24] Yilei Wu, Zijian Dong, Chongyao Chen, Wangchunshu Zhou, and Juan Helen Zhou. Mixup your own pairs. arXiv preprint arXiv:2309.16633, 2023. 1, [25] Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. Delving into deep imbalanced regression. In International Conference on Machine Learning, pages 1184211851. PMLR, 2021. 1, 3, 5, 6, 7, 8, 2 [26] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature transfer learning for face recognition with under-represented data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57045713, 2019. 1, 3 [27] Kaiwen Zha, Peng Cao, Jeany Son, Yuzhe Yang, and Dina Katabi. Rank-n-contrast: Learning continuous representations for regression. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 1, 3 [28] Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, and Angela Yao. Improving deep regression with ordinal entropy. In The Eleventh International Conference on Learning Representations, 2022. 6 [29] Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, and Angela Yao. Improving deep regression with ordinal entropy. arXiv preprint arXiv:2301.08915, 2023. 6, 8 [30] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [31] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1955019560, 2023. 1 [32] Zhihan Zhou, Jiangchao Yao, Feng Hong, Ya Zhang, Bo Han, and Yanfeng Wang. Combating representation learning disparity with geometric harmonization. Advances in Neural Information Processing Systems, 36, 2024. 3 [33] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang Jiang. Balanced contrastive learning for long-tailed visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69086917, 2022. 1,"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Proof of Theorem 1. Since (r(t)r(y))2 0, t,y [0,1], we have Proof. Define [0, 1]. reparametrization of the path l(y) is defined by bijective strictly increasing function r(y):[0,1][0,1], denoted as l(y):=(lr)(y). Due to the fact that Im(l)=Im(l), (l,ϵ)=T (l,ϵ)Lenv(l,ϵ)=Lenv(l,ϵ) (14) Denote as the derivative of r. Further we have Lhomo(l)= = = = = (cid:90) 0 (cid:90) 1 0 (cid:90) 1 0 (cid:90) 1 0 (cid:90) 1 0 yl(r(y))2dy rl(r)2r=r(y)r(y)2dy rl(r)2r=r(y)r(y)2dy (15) rl(r)2r(y)dr rl(r)2s(r)dr, where = r1. This separates the dependence of Lhomo on the reparametrization to single weight function s:[0,1]R+."
        },
        {
            "title": "Then we have",
            "content": "Lhomo(l)Lhomo(l)= (cid:90) 1 0 yl(y))2(s(y)1)dy. (16) Now if the original curve is moving at constant speed, i.e., yl(y)=c, where is positive constant. In other words, the data is uniformly distributed. Then Lhomo(l)Lhomo(l)=c2 (cid:90) 1 (s(y)1)dy 0 (cid:18)(cid:90) 1 0 =c s(y)dy1 (cid:19) , which means in this case the loss will increase if (cid:82) 1 1 and decrease otherwise. Since is bijection, we have 0 s(y)dy > (cid:90) 1 s(r)dr = = (cid:90) 1 0 (cid:90) 1 0 s(r(y))r(y)dy r(y)2dy (cid:90) 1 (cid:90) 1 0 (r(y)r(t))2dtdy 0 (cid:90) =2 0 (cid:90) 1 =2 0 (cid:90) 1 0 r(y)2dydt (cid:18)(cid:90) 1 (cid:19)2 r(y)dy 0 r(y)2dy2 0 (cid:90) 0 r(y)2dy 1, where the inequality holds when r(y) is constant, since is bijective, should be the function: r(y) = y. This means l(y) =l(y),y. Therefore, we have (cid:82) 1 0 r(y)2dy > 1, for = l, which means, the loss attains its minimum if and only if the data is uniformly distributed. 7. Datasets 7.1. UCI-DIR We curated UCI-DIR to evaluate the performance of imbalanced regression methods on tabular datasets. Here, we consider four regression tasks from UCI machine learning repository [2] (Airfoil, Concrete, Real Estate and Abaleone). Their input dimensions range from 5 to 8. Following the original DIR setting [25], we curated balanced test set with balanced distribution across the label range and leave the training set naturally imbalanced (Figure 9). We partitioned the label range into three regions based on the occurrence. The threshold for [few-shot/med-shot, med-shot/many-shot] are [10, 40], [5, 15], [3, 10] and [100, 400] for Airfoil, Concrete, Real Estate and Abalone respectively. 7.2. OL-DIR We follow Lu et al. [14] for the basic setting of operator learning. However, we change the original uniform sampling of locations in the domain of the output function to three regions: few, medium, and many regions. For the linear operator defined in Equation (12), the input function is generated from Gaussian Random Field (GRF): uG(0,k(x1,x2)) (cid:18) k(x1,x2)=exp (cid:19) x1x22 2l2 (17) (18) where the length-scale parameter is set to be 0.2. For x, we fix 100 locations to represent the input function u. The locations 1 Table 8. Overview of the six curated datasets used in our experiments Dataset Target type Target range Bin size # Training set # Val. set # Test set IMDB-WIKI AgeDB-DIR STS-B-DIR Age Age Text similarity score 0 186* 0 101 0 5 1 1 0.1 191,509 12,208 5,249 11,022 2,140 1,000 11,022 2,140 1,000 *Note: wrong labels in the original dataset. Figure 9. Overview of training and test set label distribution for UCI-DIR datasets. in the output function ys are manually sampled from the domain of G(u), such that few-shot region: [0.0,0.2] [0.8,1.0]; medium-shot region: [0.2,0.4][0.6,0.8]; many-shot region: [0.4,0.6]. We manually create an imbalanced training set with many/medium/few-shot regions of 10k samples and balanced testing test of 100k samples. For the nonlinear operator defined in Equation (13), the input function is defined as: b(x;ω)GP(b0(x),cov(x1,x2)) b0(x)=0 cov(x1,x2)=σ2exp (cid:18) (cid:19) x1x22 2l (19) (20) (21) where ω is sampled from random space with Dirichlet boundary conditions u(0) = u(1) = 0, f(x) = 10. GP is Gaussian random process. The target locations are sampled in the same way as the linear task."
        },
        {
            "title": "The number and split of the nonlinear operator dataset are",
            "content": "the same as those of the linear one. 7.3. AgeDB-DIR, IMDB-DIR and STS-B-DIR For the real-world datasets (AgeDB-DIR, IMDB-WIKI-DIR and STS-B-DIR), We follow the original train/val./test split from [25]. 7.4. Ethic Statements All datasets used in our experiments are publicly available and do not contain private information. All datasets (AgeDB, IMDB-WIKI, STS-B, and UCI) are accrued without any engagement or interference involving human participants and are devoid of any confidential information. 8. Experiment Detail 8.1. Implementation Detail  (Table 9)  . 8.2. Choices of N. We investigate how varying (the number of uniformly distributed points on hypersphere used to calculate enveloping loss) impacts the performance of our approach on the AgeDBDIR and IMDB-WIKI-DIR datasets  (Table 10)  . To achieve optimal performance, it is crucial to choose sufficiently large N. smaller might fail to cover the entire hypersphere adequately, resulting in an imprecise calculation of enveloping loss. 8.3. Ablation on proposed components. The Table 11 presents the results of an ablation study examining the impact of different loss functions on the model performance. As we mentioned before, the use of only homogeneity loss (Lhomo) could lead to trivial solutions due to feature collapse. Additionally, using only the enveloping loss (Lenv) causes the features to spread out along the trajectory, resulting in suboptimal performance. Through the contrastive loss (Lcon), individual representations could converge towards their corresponding locations on the surrogate. It is evident from the Table 11 that the model incorporating all loss functions outperforms the other configuration. 8.4. Computational cost In this subsection, we compare the time consumption of the Surrogate-driven Representation Learning (SRL) framework with other baseline methods for age estimation and text similarity regression tasks. The reported time consumption, expressed in seconds, represents the average training time per mini-batch update. All experiments were conducted using GTX 3090 GPU. Table 9. Hyper-parameters used in SRL Dataset IMDB-WIKI AgeDB-DIR STS-B-DIR UCI-DIR OL-DIR Temperature (τ) Momentum (α) λe λh Backbone Network (f()) Feature Dim Learning Rate Batch Size 0.1 0.9 2000 1e-1 1e-1 ResNet-50 128 2.5e-4 256 0.1 0.9 2000 1e-1 1e-1 ResNet-50 128 2.5e-4 0.1 0.9 1000 1e-2 1e-4 BiLSTM 128 2.5e-4 16 0.1 0.9 1000 1e-2 1e-2 3layer MLP 128 1e-3 256 0.1 9.9 1000 1e-1 1e-1 3layer MLP 128 1e-3 1000 Table 10. Vary the number of 200 500 1000 2000 4000 10000 AgeDB 7.78 7.55 7.37 7.31 IMDB-WIKI 7.85 7.78 7.72 7.69 7.22 7.69 7.22 7.69 7.22 7. OL-DIR dataset and the real-world AgeDB-DIR dataset, we varied the number of bins across the label space. Note that for AgeDB-DIR, the finest possible bin size is constrained to 1 due to the discrete nature of age labels, while OL-DIR allows for arbitrary bin sizes. The results are presented in Table 13. Table 11. Ablation Studies, best results are bold Table 13. Impact of bin numbers on model performance Lenv Lhomo Lcon MAE GM 10 20 50 100 1000 2000 4000 OL-DIR (MAE 103) 9.92 9.29 9.20 9.18 9.18 AgeDB-DIR (MAE) 7.44 7.38 7.31 7.22 - 9.17 - 9.18 - 8.6. Experiments on UCI-DIR (Table 14, 15, 16, 17) Table 14. Complete results on UCI-DIR for Airfoil (MAE with standard deviation), the best results are bold. Metrics Shot MAE All Many Med Few 5.657(0.324) 5.112(0.207) 5.031(0.445) 6.754(0.423) VANILLA 5.761(0.331) 4.445(0.208) 4.792(0.412) 7.792(0.499) LDS + FDS 5.228(0.335) 4.908(0.786) 5.718(0.712) RankSim BalancedMSE 5.694(0.342) 4.512(0.179) 5.035(0.554) 7.277(0.899) Ordinal Entropy 6.270(0.415) 4.847(0.223) 5.369(0.635) 8.315(0.795) 5.100(0.286) 4.832(0.098) 4.745(0.336) 5.693(0.542) SRL (ours) 5.049(0.92) Table 15. Complete results on UCI-DIR for Abalone (MAE with standard deviation), the best results are bold."
        },
        {
            "title": "Few",
            "content": "4.567(0.211) 0.878(0.152) 2.646(0.349) 7.967(0.344) VANILLA 9.261(0.807) 5.087(0.456) 0.904(0.245) 3.261(0.435) LDS + FDS 7.421(0.966) 4.332(0.403) 0.975(0.067) 2.591(0.516) RankSim BalancedMSE 9.368(0.896) 5.366(0.542) 2.135(0.335) 2.659(0.456) Ordinal Entropy 6.774(0.657) 2.314(0.256) 4.013(0.654) 11.610(1.275) 7.191(0.301) 4.158(0.196) 0.892(0.042) 2.423(0.199) SRL (ours) All Many Med Few All Many Med Few 7.67 7.87 7.52 7.50 7.55 7.22 6.66 7.01 6.63 6.73 6.73 7.38 9.30 12.61 4.85 8.99 12.90 5.12 8.69 12.63 4.85 8.53 11.92 4.81 8.47 12.71 4.79 4.50 8.28 6.64 4.17 4.56 4.27 4.37 4.24 4.12 6.51 8.98 6.11 9.39 5.90 9.48 5.49 8.29 5.68 9.42 5.37 6. Table 12 shows that SRL achieves considerably lower training time compared to the LDS + FDS, while remaining competitive with RankSim, Balanced MSE, and Ordinal Entropy. This demonstrates SRLs ability to handle complex tasks efficiently without introducing substantial computational overhead. Table 12. Average training time per mini-batch update (in seconds) for age estimation (AgeDB-DIR) and text similarity regression (STS-B-DIR) tasks, using GTX 3090 GPU. Method AgeDB-DIR (s) STS-B-DIR (s) VANILLA LDS + FDS RankSim Balanced MSE Ordinal Entropy SRL (Ours) 12.24 38.42 16.86 16.21 17.29 17.10 25.13 44.45 30.04 28.12 29.37 27.35 8.5. Impact of Bin Numbers In our geometric framework, we employ piecewise linear interpolation to approximate the continuous path l. The granularity of this approximation is determined by the number of bins used for discretization, where finer binning naturally leads to smoother interpolation. To empirically analyze the impact of bin numbers (B) on model performance, we conducted extensive experiments across both synthetic and real-world datasets. For the synthetic 3 mance in the original ConR [10] paper, where they report overall MAE of 7.33 in the main result  (Table 2)  and 7.84 in the ablation studies  (Table 8)  , The results in Table 8 are close to our reported result. 8.9. Complete result on STS-B-DIR  (Table 20)  8.10. Complete result on Operator Learning ( Table 21) 9. Pseudo Code (Algorithm 1) for Surrogatedriven Representation Learning (SRL) 10. Broader impacts We introduce novel geometric constraints to the representation learning of imbalanced regression, which we believe will significantly benefit regression tasks across various real-world applications. Currently, we are not aware of any potential negative societal impacts. 11. Limitation and Future Direction In considering the limitations and future directions of our research, its important to acknowledge that our current methodology has not delved into optimizing the feature distribution in scenarios involving regression with higher-dimensional labels. This presents notable area for future exploration. Additionally, investigating methods to effectively handle complex label structures in imbalanced regression scenarios could significantly enhance the applicability and robustness of our proposed techniques. Table 16. Complete results on UCI-DIR for Real Estate (MAE with standard deviation), the best results are bold. Datasets Shot MAE All Many Med Few 0.326(0.003) 0.273(0.005) 0.376(0.003) 0.365(0.012) VANILLA 0.346(0.004) 0.325(0.002) 0.400(0.002) 0.335(0.023) LDS + FDS 0.373(0.008) 0.343(0.004) 0.381(0.008) 0.397(0.032) RankSim BalancedMSE 0.337(0.007) 0.313(0.004) 0.398(0.009) 0.326(0.028) Ordinal Entropy 0.339(0.007) 0.286(0.004) 0.421(0.005) 0.351(0.031) 0.278(0.002) 0.262(0.006) 0.296(0.005) 0.287(0.023) SRL (ours) Table 17. Complete results on UCI-DIR for Concrete (MAE with standard deviation), the best results are bold. Datasets Shot MAE All Many Med Few 7.287(0.364) 5.774(0.289) 6.918(0.346) 9.739(0.487) VANILLA 6.879(0.344) 6.210(0.310) 6.730(0.337) 7.594(0.380) LDS + FDS 6.714(0.336) 5.996(0.300) 5.574(0.279) 9.456(0.473) RankSim 7.033(0.352) 4.670(0.234) 6.368(0.318) 9.722(0.486) BalancedMSE Ordinal Entropy 7.115(0.356) 5.502(0.275) 6.358(0.318) 9.313(0.466) 5.939(0.297) 5.318(0.266) 5.800(0.290) 6.603(0.330) SRL (ours) 8.7. Experiments on AgeDB-DIR Training Details: In Table 18, our primary results on AgeDB-DIR encompasses the replication of all baseline models on an identical server configuration (RTX 3090), adhering to the original codebases and training recipes. We observe performance drop in RankSim [6] and ConR [10] in comparison to the results reported in their respective studies. To ensure fair comparison, we present the mean and standard deviation (in parentheses) of the performances for SRL (ours), RankSim, and ConR, based on three independent runs. We found SRL superiors performance in most categories and all Med-shot and Few-shot metrics. We would like to note that we found self-conflict performance in the original ConR [10] paper, where they report overall MAE of 7.20 in main result  (Table 1)  and 7.48 in the ablation studies  (Table 6)  . The results in Table 6 are closed to our reported result. 8.8. Experiment on IMDB-WIKI-DIR Training Details: In Table 19, our primary results on IMDBWIKI-DIR encompass the replication of all baseline models on an identical server configuration (RTX 3090), adhering to the original codebases and training receipes. We observe performance drop of ConR [10] in comparison to the results reported in their respective studies. To ensure fair comparison, we present the mean and standard deviation (in parentheses) of the performances for SRL (ours) and ConR, based on three independent runs. We found SRL superiors performance in most categories and all Med-shot and Few-shot metrics. We would like to note that we found self-conflict perfor4 Table 18. Complete Results on AgeDB-DIR Metrics Shot VANILLA LDS + FDS RankSim BalancedMSE Ordinal Entropy"
        },
        {
            "title": "ConR",
            "content": "SRL (ours) MAE GM MSE"
        },
        {
            "title": "All\nMany\nMed\nFew",
            "content": "7.67 6.66 9.30 12.61 4.85 4.17 6.51 8.98 100.01 76.67 130.21 237.00 7.55 7.03 8.46 10.52 4.86 4.57 5.38 6.75 97.05 82.68 114.00 185. 7.41(0.03) 6.49(0.01) 8.73(0.05) 12.47(0.09) 4.71(0.03) 4.15(0.02) 5.74(0.04) 8.92(0.08) 94.37(0.10) 72.00(0.09) 121.38(2.15) 230.97(3.22) 7.98 7.58 8.65 9.93 5.01 4.83 5.46 6.30 107.35 95.49 125.55 169. 7.60 6.69 8.87 12.68 4.91 4.28 6.20 9.29 97.28 74.79 122.07 241.13 7.41(0.02) 6.51(0.02) 8.81(0.03) 12.04(0.04) 4.70(0.02) 4.13(0.02) 5.91(0.06) 8.59(0.0) 7.22(0.02) 6.64(0.01) 8.28(0.04) 9.81(0.05) 4.50(0.02) 4.12(0.02) 5.37(0.02) 6.29(0.04) 91.71(0.02) 92.57(0.06) 72.06(0.04) 77.23(0.05) 121.24(1.88) 115.65(1.42) 207.00(3.09) 162.22(2.08) Table 19. Complete Results on IMDB-WIKI-DIR Metrics Shot VANILLA LDS + FDS RankSim BalancedMSE Ordinal Entropy"
        },
        {
            "title": "ConR",
            "content": "SRL (ours) MAE GM MSE"
        },
        {
            "title": "All\nMany\nMed\nFew",
            "content": "8.03 7.16 15.48 26.11 4.54 4.14 10.84 18.64 136.04 105.72 373.07 978.00 7.73 7.22 12.98 23.71 4.40 4.17 7.87 15.77 130.56 106.93 315.92 861. 7.72 6.92 14.52 25.89 4.29 3.92 9.72 18.02 130.95 102.06 351.22 977.82 8.43 7.84 13.35 23.27 4.93 4.68 7.90 15.51 146.19 121.64 343.12 787. 8.01 7.17 15.15 26.48 4.47 4.07 10.56 21.11 137.50 107.62 369.88 976.56 7.84(0.04) 7.15(0.03) 14.36(0.04) 25.15(0.06) 4.43(0.04) 4.05(0.03) 9.91(0.05) 18.55(0.06) 7.69(0.02) 7.08(0.01) 12.65(0.04) 22.78(0.06) 4.28(0.02) 4.03(0.02) 7.28(0.03) 15.25(0.05) 132.41(1.22) 129.97(0.93) 105.29(0.88) 105.83(0.77) 338.30(1.99) 311.17(1.25) 934.12(3.03) 859.81(2.28) Table 20. Complete Results on STS-B-DIR"
        },
        {
            "title": "Metrics",
            "content": "Shot VANILLA LDS + FDS RankSim BalancedMSE Ordinal Entropy SRL (ours) MSE Pearson correlation MAE Spearman correlation"
        },
        {
            "title": "All\nMany\nMed\nFew",
            "content": "0.993 0.963 1.000 1.075 0.742 0.685 0.693 0.793 0.804 0.788 0.865 0.837 0.740 0.650 0.495 0.843 0.909 0.894 1.004 0.809 0.757 0.703 0.685 0. 0.776 0.763 0.839 0.749 0.762 0.677 0.487 0.867 0.943 0.902 1.161 0.812 0.750 0.702 0.679 0.767 0.782 0.756 0.900 0.762 0.755 0.669 0.448 0. 0.877 0.886 0.873 0.745 0.765 0.708 0.749 0.844 0.750 0.748 0.773 0.694 0.769 0.689 0.503 0.879 0.900 0.911 0.881 0.905 0.757 0.698 0.723 0. 0.768 0.772 0.785 0.712 0.760 0.670 0.488 0.819 0.889 0.907 0.874 0.757 0.763 0.708 0.692 0.842 0.765 0.772 0.779 0.699 0.767 0.685 0.495 0. 5 Table 21. Complete results on OL-DIR with standard deviation added, best results are bold."
        },
        {
            "title": "Linear",
            "content": "MAE(103) MSE (104)"
        },
        {
            "title": "Few",
            "content": "VANILLA Ordinal Entropy 10.07(1.22) 9.26(0.98) 8.32(0.66) SRL (ours) 15.64(2.72) 11.86(2.20) 15.45(3.55) 27.00(5.62) 5.40(1.10) 2.81(0.75) 4.40(1.23) 14.20(2.25) 3.42(0.82) 2.67(0.99) 9.85(1.45) 13.01(1.92) 2.00(0.32) 1.53(0.19) 1.89(0.73) 1.98(0.37) 0.98(0.21) 1.72(0.62) 9.47(1.13) 9.33(1.89) 9.18(0.92)"
        },
        {
            "title": "Nonlinear",
            "content": "11.64(1.87) 9.89(1.25) 11.02(2.23) 19.77(2.89) 9.20(1.23) 4.33(0.88) 7.53(1.55) 24.70(1.99) VANILLA Ordinal Entropy 12.91(1.25) 9.93(0.93) 13.07(1.57) 21.02(1.89) 13.80(2.98) 8.82(2.25) 11.84(3.59) 30.12(5.40) 9.22(1.45) 17.00(1.54) 8.60(1.04) 7.42(0.70) 6.41(1.15) 14.12(1.39) SRL (ours) 11.25(1.13) 9.48(0.75) Algorithm 1 Pseudo Code for Surrogate-driven Representation Learning (SRL) Require: Training set ={(xi,yi)}N distributed points U, surrogate S, batch size M. i=1, encoder f, regression function g, total training epochs E, momentum α, set of uniformly 1: for e=0 to do repeat 2: 3: 4: Sample mini-batch {(xm,ym)}M {zm}M if e=0 then m=1 =f({xm}M m=1) m=1 from 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: Update the model with loss L=Lreg({ym}M m=1,g({zm}M m=1)) else m=1 using Equation (8) from and Se using Equation (9) get from {zm}M get Se Update the model with loss L=Lreg({ym}M m=1,g({zm}m m=1))+LG(Se ,U)+Lcon(Se ,{zm}M m=1) end if until iterate over all training samples at current epoch // Update the surrogate get ˆSeby calculate the class center for the current epoch if e=0 then S1 = ˆSe Se+1 =αSe+(1α)ˆSe # Momentum update the surrogate, Equation (9) else 17: 18: end if 19: 20: end for"
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}