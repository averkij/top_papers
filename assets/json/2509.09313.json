{
    "paper_title": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data",
    "authors": [
        "Moritz Mock",
        "Thomas Forrer",
        "Barbara Russo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep learning solutions for vulnerability detection proposed in academic research are not always accessible to developers, and their applicability in industrial settings is rarely addressed. Transferring such technologies from academia to industry presents challenges related to trustworthiness, legacy systems, limited digital literacy, and the gap between academic and industrial expertise. For deep learning in particular, performance and integration into existing workflows are additional concerns. In this work, we first evaluate the performance of CodeBERT for detecting vulnerable functions in industrial and open-source software. We analyse its cross-domain generalisation when fine-tuned on open-source data and tested on industrial data, and vice versa, also exploring strategies for handling class imbalance. Based on these results, we develop AI-DO(Automating vulnerability detection Integration for Developers' Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated recommender system that uses fine-tuned CodeBERT to detect and localise vulnerabilities during code review without disrupting workflows. Finally, we assess the tool's perceived usefulness through a survey with the company's IT professionals. Our results show that models trained on industrial data detect vulnerabilities accurately within the same domain but lose performance on open-source code, while a deep learner fine-tuned on open data, with appropriate undersampling techniques, improves the detection of vulnerabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 3 1 3 9 0 . 9 0 5 2 : r Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data Moritz Mock1, Thomas Forrer2, and Barbara Russo 1 Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy {momock,brusso}@unibz.it 2 R&D Department, WÃ¼rth Phoenix, Bolzano, Italy thomas.forrer@wuerth-phoenix.net Abstract. Deep learning solutions for vulnerability detection proposed in academic research are not always accessible to developers, and their applicability in industrial settings is rarely addressed. Transferring such technologies from academia to industry presents challenges related to trustworthiness, legacy systems, limited digital literacy, and the gap between academic and industrial expertise. For deep learning in particular, performance and integration into existing workflows are additional concerns. In this work, we first evaluate the performance of CodeBERT for detecting vulnerable functions in industrial and open-source software. We analyse its cross-domain generalisation when fine-tuned on open-source data and tested on industrial data, and vice versa, also exploring strategies for handling class imbalance. Based on these results, we develop AI-DO (Automating vulnerability detection ntegration for Developers Operations), Continuous IntegrationContinuous Deployment (CI/CD)-integrated recommender system that uses fine-tuned CodeBERT to detect and localise vulnerabilities during code review without disrupting workflows. Finally, we assess the tools perceived usefulness through survey with the companys IT professionals. Our results show that models trained on industrial data detect vulnerabilities accurately within the same domain but lose performance on open-source code, while deep learner fine-tuned on open data, with appropriate undersampling techniques, improves the detection of vulnerabilities. Keywords: Vulnerability Detection Deep Learner PHP Cross-Domain Technology Evaluation Data Balancing"
        },
        {
            "title": "Introduction",
            "content": "The role of security experts in software development is of paramount importance. Their major task is to review developers code pushed into development pipeline and report back security weaknesses. Often, their work is performed manually as the last task before code delivery. They frequently rely on publicly available open-source information and, at times, use black-box static analysers that are not always integrated into the companys development pipeline, creating custom rules to make the security scanner work for specific application [34]. 2 Mock et al. In this work, we aim to investigate whether deep learner can be leveraged to enhance the automatic detection of vulnerable functions in industrial software and how it can be integrated into companys DevOps environment. To this end, we first evaluated the performance of CodeBERT, deep learner pre-trained on source code on the detection of vulnerable functions in both industrial and open-source software. Then, we examined whether this capability is retained when the model is fine-tuned on open-source data and then tested on industrial data. We further developed tool (AI-DO) to automate the detection of vulnerable functions in an industrial DevOps environment embedding deep learner, it is designed to allow to exchange the deep learner for future adjustments. AI-DO has been developed as recommender system running over the Continuous Integration - Continuous Deployment (CI/CD) pipeline with deep learner pre-trained on open source. It leverages multi-channel monitoring data to automatically detect and localise security weaknesses in code and present them to reviewers in their native environment. Finally, we gathered feedback from the companys IT professionals regarding the tool. To guide the analysis for our case study, we have defined two research questions: RQ1. Can CodeBERT be employed for vulnerability detection in industry source code? This research question aims to explore how deep learning model can be leveraged for the detection of vulnerabilities in industry. Our approach is therefore twofold: first, we compare the performance of pre-trained model finetuned and tested with different balancing strategies on industrial data and open source data. Then, we develop AI-DO, embed it into the industrial DevOps of company and ask the opinion of the companys IT professionals. RQ2. How well does CodeBERT fine-tuned on open source vulnerability data generalise to industrial technology-specific data? This research question investigates the performance of fine-tuned models trained on open data and tested against industry data and vice versa. Furthermore, we explored various strategies for fine-tuning to mitigate data imbalance. Overall, the contribution can be summarized with the following: We performed cross-domain performance evaluation on between datasets collected in industry and open data and made them available. We implemented AI-DO, which supports reviewers in detecting vulnerabilities during the review process; furthermore, we have surveyed developers about the tool, e.g., AI-DO, can support the review process. The paper is structured as follows: Section 2 contains the related work, followed by the methodology in Section 3.Section 4 presents the case study, going over to the Section 5 and 6 which discuss the the threats to validity and conclusion."
        },
        {
            "title": "2 Related Work",
            "content": "We have reviewed the existing literature according to two dimensions: (i) AI models for vulnerability detection, and (ii) cross-domain evaluation of datasets collected in open-source and industry contexts. Cross-Domain Evaluation"
        },
        {
            "title": "2.1 AI Models for Vulnerability Detection",
            "content": "Vaswani et al. [37] introduced the transformer architecture, enabling efficient learning across diverse tasks. Typically, transformer models are pre-trained on general tasks and subsequently fine-tuned for specific applications, such as vulnerability detection in source code. BERT [4], trained on an English corpus, was later extended by RoBERTa [2], which modified the training data, objectives, and parameters, while still focusing on the English language. Building on previous developments, CodeBERT [7] leverages multiple programming languages into its pre-training, using the CodeSearchNet dataset [13]. This domain-specific training improves performance on code-related tasks compared to general-purpose models such as RoBERTa. Fu et al. [9] proposed LineVul, model designed to detect vulnerabilities using the Big-Vul dataset [6], which is derived from CVE-referenced commits. Their approach initially focused on function-level vulnerability detection and was later extended to perform detections at the line level. In their work, Fu et al. employed CodeBERT as the underlying model and processed source code at the function level, treating it as plain text without incorporating any structural or positional information, later their approach was refined for line-level detection. Hin et al. [12] introduced LineVD, deep learning framework, leveraging CodeBERT, that reframes statement-level vulnerability detection as node classification task over program dependency graphs. By combining graph neural networkscapable of capturing controland datadependency structureswith to process raw source tokens, LineVD significantly outperforms prior techniques, achieving over 105% improvement in F1-score on Big-Vul, real-world C/C++ vulnerability dataset."
        },
        {
            "title": "2.2 Cross-Domain Performance Evaluation of Datasets",
            "content": "Evaluating the performance of novel approach is often limited to single dataset, which may have been specifically created for that particular approach. However, assessing tools performance is resourceand time-consuming and using different benchmarks can unveil lack of robustness in the approach. comparison of different Static Application Security Testing (SAST) tools, focusing on their performance in detecting vulnerabilities in Java projects indicating that the combination of multiple SAST tools increases the accuracy of detecting vulnerabilities [17]; however, it is not considered to solely relay on them. In contrast, when evaluating different deep learning approaches on new created dataset, trained and tested, resulted into performance drop of up to 91% [1], indicating that shift in the dataset makes highly specialized deep learning techniques impractical. Despite the growing need to protect industry code from vulnerabilities [5, 25], to the best of our knowledge, no cross-domain evaluation has been performed between open data, which are widely used in academia, and industry data. Therefore, the evaluation conducted in this work represents critical first step in bridging the gap between academic research and industry practices. In 4 Mock et al. Fig. 1: Creation of an annotated dataset by data fusion of two annotation types from static analysers for 40 open source and 8 industry projects. contrast to the existing work, we do not focus solely on open data but investigate the performance of the deep learner, in our case CodeBERT [7], trained on open data and applied on industry data. For the particular use case of the company, as an integration within the review process of source code."
        },
        {
            "title": "3 Methodology",
            "content": "This section illustrates our methodology, starting with the creation of the dataset, going over to the multiple fine-tuning strategies of CodeBERT and cross-validation, concluding with the implementation details."
        },
        {
            "title": "3.1 Dataset creation and annotation",
            "content": "To the best of our knowledge, there are no publicly available datasets of PHP functions annotated for vulnerability using consistent methodology for both open and industrial code. As such, in this section, we present our technique to create and fuse three datasets: one from industry data, i.e., Industry Dataset (ID) and the other two from open source data, the former from popular technologyagnostic open source projects, i.e., Generic Open-source Dataset (GOD), and the latter from open source data originated from the same type of technology of the industrial data, i.e., Technology-similar Open-source Dataset (TOD). For each of the datasets, we applied the same annotation process as illustrated in Fig. 1. To extract the PHP functions, we leveraged TreeSitter [36], which was already employed successfully in recent mining studies [22, 23]. After extracting the functions from each project, we removed duplicates. We used PMD-CPD [26], which detects duplicated code using sliding-window approach and 99% Jaccard index threshold [32]. Following Mock et al. [23], we set the window size to 30, to balance coverage and computational cost. Cross-Domain Evaluation 5 Combining annotations from different tools helps detect greater number of vulnerabilities in source code [24], we performed function-level annotations at the project level and mapped the output to the extracted functions. We leveraged two state-of-the-art static analysersSemGrep [28] and SonarQube [33]which are widely used and accepted in both industry and academia [16, 14]. We reviewed the resulting function annotations and decided not to leverage the labels Info or Minor from SemGrep, and Warning from SonarQube, as indicators of vulnerabilities. The annotation is counted uniquely, i.e., function might have multiple vulnerabilities; however it is counted once. The schema of the datasets consists of two main parts. The first part contains the information that helps to extract function, i.e., the url, commit-id, filename, and position of the function in the original file. The second part contains the annotation per severity level. The data is provided in three distinct CSV files, one for each of the datasets. The datasets were created to meet the quality standards defined by the framework proposed by Croft et al. [3], which specifies five attributes: accuracy, uniqueness, consistency, completeness, and currentness. Accuracy refers to the extent to which the data correctly represents the true value of the intended property. In our context, this means whether each function is accurately labelled as vulnerable or not. We employed Semgrep and SonarQube, two state-of-the-art SAST tools, for the automatic labelling of the functions. Uniqueness is the degree to which there is no duplication in records. In our context, this means whether each function has been unique in the dataset. We leveraged PMD-CPD [26] with 30-token sliding window to detect duplicate code snippets across functions. To remove ducplicates, we also compute the Jaccard similarity of each function pair and removed functions of 99% or higher similarity. Consistency is the degree to which data has attributes that are free from contradiction and are coherent with other data. In our context, this means whether each function have been annotated in consistent manner with the different analysers over the different types of datasets. For this we leveraged the two SAST tools in the same version and setting across annotation of the three datasets. Completeness refers to the extent to which the data associated with an entity contains values for all expected attributes and related instances. In our context, this means that (1) the dataset must include sufficient information for each function to be uniquely identified within the source code and (2) the output of the static analysers can be used to clearly identify function as vulnerable or not. For this, we include the Url, commit-ID, file path, and start/end-position of each functions using TreeSitter [36], light-weighted code parser, such that the function can be traced back to the original location3. The output of the SAST tools is mapped automatically to the associated functions and stored by severity level; the scripts are available in the replication package [21]. Currentness is the degree to which data has attributes that are of the right age. In our context, this means that the data of the open source datasets pertain to the same period of time of industrial dataset. All datasets were created simultaneously leveraging the same SAST tools and configurations, ensuring temporal alignment of vulnerabilities. 3 Due to company restrictions, the Url and commit ID are omitted for the dataset ID 6 Mock et al."
        },
        {
            "title": "3.2 CodeBERT fine-tuning",
            "content": "CodeBERT is pre-trained deep learner on six programming languages developed by Microsoft Research [7]. It is designed specifically to work with both natural and programming language inputs. CodeBERT has been trained on the dataset CodeSearchNet [13]. CodeSearchNet covers six programming languages (Python, Java, JavaScript, PHP, Ruby, and Go) and provides general code understanding but lacks vulnerability information. CodeBERT has better performance than other pre-trained models, e.g., UniXcoder [11], also in different software engineering settings [20]. Fine-tuning refers to the process of further training pre-trained model on specific downstream task, allowing it to adapt to task-specific data and improve performance. During this process, the weights of the models are changed so that the model can perform the desired task [4]. In our case, we performed fine-tuning on the pre-trained model CodeBERT [7]. We have fine-tuned CodeBERT on each of the three datasets for which we set the block size, batch size, and learning rate as 512, 16 and 2e-5 respectively, as suggested in the recent literature [9, 30]. We fine-tuned each model for 10 epochs and performed manual early stopping [27] if the F1 increases more than 0.001 within five epochs. We randomly split each dataset in 80%/10%/10%, for training, validation, and testing respectively. Furthermore, we trained and validated the model using four different data balancing techniques: (NB) no balancing, i.e., the natural distribution of the dataset is taken; (USC) undersampling with the size of the smallest class across all datasets [15], i.e., in our case, the vulnerable instances of ID constitute the smallest class with 4,934 instances, and all other classes have been balanced accordingly; (URSC) undersampling with the size of the relatively smallest class per dataset [9], i.e., in each dataset, the smaller class (the vulnerable class) determines the sample size for the non-vulnerable class; and (WLF) weighted loss function with inverse class frequency [30], i.e., the natural distribution of the dataset is maintained, but during training, the importance of the two classes is differentiated by adjusting the loss functions weights. The loss function quantifies the discrepancy between the models detections and the actual values during training, serving as metric to guide optimization by penalizing inaccurate detections [20]. No balancing was applied to the validation and testing set, i.e., the natural distribution of the datasets were maintained."
        },
        {
            "title": "3.3 Cross-Domain Evaluation",
            "content": "We used precision, recall, and the F1 score to evaluate the performance of CodeBERT in vulnerability detection in the testing portion of the three data sets. We repeat the analysis for each of the balancing techniques. Performance measures are computed as follows: = P + , = P + , F1 = 2 P + . (1) The goal of cross-domain analysis in our context is to evaluate the performance of CodeBERT on industry data after it has been fine-tuned on open source data. Cross-Domain Evaluation Table 1: Development team role and IT experience Role Software Developer Software Engineer Software Engineer in R&D Security Software Engineer Software Architect DevOps Team Leader # Years working in industry 6 2 1 1 2 2 1 1-3 years 12, 15 years 2 years 4 years 5, 11 years 5 years 15 years This analysis helps answer questions such as: RQ1. Can CodeBERT be employed for vulnerability detection in industry source code? and RQ2. How well does CodeBERT fine-tuned on open source vulnerability data generalise to industrial technology-specific data? To investigate these problems, we fine-tune CodeBERT on the two datasets GOD and TOD and analyse its performance on the testing portion of the ID dataset. We then compared CodeBERTs performance on ID with its performance on the same dataset on which it has been fine-tuned, also using different balancing strategies."
        },
        {
            "title": "4 Case Study",
            "content": "The case study was performed at WÃ¼rth Phoenix, an Italian software company. We selected an ERP project written in PHP as it is representative of the companys business. The project has over 2.2 million lines of code, spanning over 74 thousand commits written in period of up to 20 years. The development team consists of 15 members with the roles and IT experience reported in Table 1. The companys internal CI/CD pipeline runs on self-hosted and maintained instance of OpenShift [29]. Before the release of every new feature of the project, five-step workflow is sequentially followed in the pipeline: low-level code design, development, code review, security review, and staging. Low-level code design. When new feature is requested by customer or internally, low-level code design is performed [10, 19]. Multiple developers internally discuss the new feature, including how it can be implemented, where best to place it in the architecture of the current source code, and, if the current code needs to be modified, how to do this with minimal impact while still maintaining high reusability for the future. If needed, the feature is broken down into multiple tasks so that it is easier to handle them within sprint [31]. The security expert participates in Sprint meetings to anticipate potential security risks and ensure security by design. Development. Based on the JIRA issues, developers implement the requested feature. single task can be feature or part of larger feature. At this stage, no reviewers (code or security) are involved. Code Review. In this stage, newly added, changed, or removed code is inspected regarding architectural consequences, maintainability, and general quality of the code. Developers, architects, and software engineers are typically involved. Security Review. In the 8 Mock et al. Table 2: Summary of annotations categorised by severity levels and tools. Dataset # fns GOD TOD ID Total 206,647 238,161 64,333 509,141 # vulnerable fns 48,797 21,579 6,149 76,525 Major 38,249 16,210 3,337 57,796 Sonarqube Critical 16,084 10,195 4,094 30,373 Blocker 2,061 2,361 322 4,744 Semgrep Error 647 122 0 security review, newly added, changed, or removed source code is inspected for potential vulnerabilities, following the same schema as the code review by the security expert. Deployment. Issues resolutions follow pre-defined deployment schedule if they are general features. However, if they are vulnerability patches, they are deployed directly without respecting the general schedule."
        },
        {
            "title": "4.1 Data collection",
            "content": "The dataset GOD contains 209,532 functions of 21 open-source PHP projects selected based on their popularity. The dataset TOD contains 240,876 functions of the top 19 GitHub Enterprise Resource Planning tools (ERP ) written in PHP (forks have been excluded). The dataset ID consists of 65,385 functions of eight ERP projects of the partner company. For each project, we collected all the functions included in the most recent version of projects repository. We then applied our approach and successfully removed 6,136 duplicated functions (1,052 in ID, 2,885 in GOD, and 2,229 in TOD), which were consistently duplicated with the two SAST tools. 76,181 were marked as vulnerable by SonarQube, and an additional 769 by SemGrep, resulting in combined total of 76,525 vulnerable functions. It should be noted that the sum does not add up due to partial overlaps of the different severity levels. Table 2 illustrates the resulting datasets, with further insights into the severity levels from each SAST tool."
        },
        {
            "title": "4.2 Cross-Domain Performance Evaluation",
            "content": "Plots in Fig. 2 illustrate Precision, Recall, and the F1 score of CodeBERT in each of the configurations of this study: the colour of the bars represents the type of dataset chosen for testing. For example, green means that the performance of CodeBERT has been tested on ID. The pattern of the bars represents the balancing strategy. For instance, no pattern means that fine-tuning has been performed without balancing. On the x-axis, the labels indicate the datasets which have been used to fine-tune CodeBERT, i.e., trained and validated on the vulnerability task. Thus, the first green bar in Fig. 2 refers to the F1 score (70.7) of CodeBERT that has been fine-tuned without balancing and tested on ID. In vulnerability detection, the primary objective is to minimize false negatives (FN), since even single undetected vulnerable function can lead to significant damage if exploited [18]. Therefore, in the following analysis, we prioritize recall over precision, as capturing all potential vulnerabilities is considered more Cross-Domain Evaluation 9 Fig. 2: Testing performance of CodeBERT fine-tuned on different datasets and with different balancing strategies. critical than avoiding false alarms. To guide our analysis, we applied threefold approach: (1) analyse the most effective balancing strategy, (2) analyse how fine-tuning on different datasets affects the performance of CodeBERT on unseen data, and (3) restrict our analysis to the performance of CodeBERT on industrial unseen data. To analyse the most effective balancing strategy, we analyse the plots in terms of the four balancing strategies described in Section 3.2. When both fine-tuning and testing are performed with the same dataset, we observe higher F1 score and precision with NB or WLF, whereas any type of undersampling is generally preferred to reduce FNs (increased recall). When fine-tuning is performed with ID and TOD and testing with GOD, we observe higher F1 score and recall with any type of undersampling techniques, whereas NB or WLF are better choices to increase precision. 10 Mock et al. When fine-tuning is performed with ID and GOD and testing with TOD, we observe higher F1 score and precision without balancing (NB), whereas any type of undersampling techniques is preferred to increase recall. When fine-tuning is performed on TOD and GOD and testing is computed on ID, we observe higher F1 score and precision with NB and WLF, whereas any type of undersampling techniques is preferred to increase recall. In summary, when maximizing recall is the primary objective, we recommend applying undersampling techniques. Conversely, for optimizing overall performance as measured by the F1-score, either no balancing or the use of weighted loss function is more appropriate. We then study how fine-tuning on different datasets affects the performance of CodeBERT on unseen data. When CodeBERT has been fine-tuned with TOD, we observe the best performance for all measures is testing with the same dataset. In addition, recall is better when testing with ID than with the dataset GOD. When CodeBERT has been fine-tuned with GOD, we observe the best performance for Precision and F1 score when testing with GOD, whereas testing with ID produces similar - sometimes even better - recall. When CodeBERT has been fine-tuned with ID, we observe the best performance for all measures when testing with ID itself. Testing with open source data produce poor recall. Generally speaking, testing on the same dataset on which CodeBERT has been fine-tuned yields better performance. However, in terms of FNs, we can observe that (1) Fine-tuning on open-source data and testing on industrial data both from the same technology - results in fewer FNs compared to testing on general open-source data. Technology-related open source data does not generalize well to general open source data. (2) Fine-tuning on industrial data and testing on open-source data leads to significant increase of FNs. (3) Fine-tuning on technology-agnostic open source data is worth to detect vulnerabilities of industrial code (similar or better recall). Motivated by the last result, we pose the following question. Thus, when we further restrict our analysis to the performance of CodeBERT on industrial unseen data we notice that: precision and F1 score decrease if CodeBERT has been fine-tuned with any open-source data; fine-tuning with GOD combined with undersampling within the same dataset (URSC) yields higher recall than fine-tuning with ID or TOD using any balancing techniques. In addition, CodeBERT achieves higher recall if finetuned on the GOD dataset rather than on the ID dataset, whether using NB or WLF techniques. In summary, using technology-agnostic open-source data to detect unseen industrial vulnerabilities can result in better control of FNs than using industrial data from the same system. However, FPs are an issue. Cross-Domain Evaluation 11 Fig. 3: Flow of the pipeline integrated into GitHub Action."
        },
        {
            "title": "4.3 AI-DO",
            "content": "In the previous section, we showed that CodeBERT successfully detects vulnerabilities in industrial software. We also observed that CodeBERT can be fine-tuned on technology-agnostic open source data for this task. Thus, we developed AI-DO that leverages an instance of CodeBERT fine-tuned on GOD on the DevOps pipeline of the company illustrated in Section 4. AI-DO is integrated within GitHub Action; furthermore, it leverages git [35] and TreeSitter [36] for the automatic extraction of functions changed or newly added within pull request. Fig. 3 illustrates the process of pushing to the repository, triggering the pipeline, detecting the vulnerabilities, and commenting on the pull request (PR). The first step is divided into five sub-steps critical to the vulnerability detection process. First, git diff identifies the affected file paths and the corresponding modified lines. The repository is checked out with both the destination and source branches. The files impacted by the pull request are extracted using git diff, along with the modified line ranges. Functions from these files are parsed using TreeSitter [36] and filtered based on their overlap with the modified lines to ensure relevance. The fine-tuned instance of CodeBERT is applied to the relevant functions. Functions detected as vulnerable are automatically tagged to support reviewer decision-making. An example setup and the source code are available in the replication package [21]. Finally, we presented AI-DO to the IT professionals of the company and collected their opinions with brief survey. AI-DO hosts deep learner that is fine-tuned offline, ensuring complete nonintrusiveness in the development process. This model can be periodically updated offline with new data to improve accuracy, if necessary. Furthermore, AI-DO is designed to allow the fine-tuned model to be replaced, enhancing its usability and applicability beyond the scope of our work. Developers Perspective We administered brief questionnaire about AI-DO4 to 13 of the 15 IT professionals at the company (two developers were absent). The questionnaire collects information about their review process, their opinion on line-level vulnerability detection tool like AI-DO, and their demographic. The questionnaire alternates closed and open questions. We review their answers by their role as illustrate in Table 1. The security expert has been conducting code reviews focused on security for four years, multiple times per week, identifying vulnerabilities several times each month. However, he 4 Available in the replication package [21]. 12 Mock et al. is only moderately enthusiastic about the tool. This is typical reaction among specialized professionals, who often believe that new tools might eventually put at danger their role in the organization [8]. The two DevOps experts have different levels of experience with vulnerability detection during the review process. Both review code weekly to identify bugs; however, the expert who encounters vulnerabilities more frequentlyapproximately one per monthexpressed greater appreciation for the tool, stating: This tool is really useful to speed up and improve the development process! It could help to spot issues before they go to production. Conversely, the expert who rarely encounters vulnerabilities is the most negative among all participants and suggests that the tool should be enhanced with more extensive set of detection rules. The three software engineers, although they do not normally find many vulnerabilities during their review tasks, greatly appreciate the tool. Two of them have extensive experience in IT and code review. The third is junior engineer whose work focuses more on process quality than on code, as she rarely performs code reviews. The four developers are junior professionals who perform code reviews frequentlymultiple times weekas it is one of their primary tasks. All are moderately satisfied with the tool, although they acknowledge lacking sufficient experience in vulnerability detection. The two software architects, who review code as frequently as the developers, are more enthusiastic about the tool. They both recommend enhancing it to provide more detailed information on the type of vulnerability and to explain the reasoning behind AI-DOs output (i.e., the vulnerable lines of code). The team leader is an experienced professional who performs code reviews sporadically, approximately once per month, also with the specific aim of detecting vulnerabilities. He moderately appreciate the tool, considering it insufficiently precise, as vulnerabilities often span multiple lines and classes. In summary, professionals enthusiasm for security tools largely depends on their experience with vulnerability detection and how well the tool supports their specific needs. While experts who frequently find vulnerabilities see clear benefits, those with less exposure or more specialized roles tend to be more cautious or critical, often concerned about the tools limitations or its potential impact on their expertise. Implementation Details and Replication Package The implementation details can be found in the replication package [21]."
        },
        {
            "title": "4.4 Answers to the Research Questions",
            "content": "RQ1. Can CodeBERT be employed for vulnerability detection in industry source code? Yes, CodeBERT fine-tuned on data of the company performs comparatively well on industrial unseen data with at most 10% decrease of the F1 score with respect to CodeBERT fine-tuned and tested on open source unseen data. Using weighted loss function in fine tuning this gaps further reduces. However, when the focus is on reducing FNs, undersampling techniques can be more efficacious. After introducing AI-DO, which implements our approach in the companys DevOps PHP environment, we observed that professionals enthusiasm for the tool depends on their experience with vulnerability detection Cross-Domain Evaluation 13 and the extent to which it addresses their specific needs. Experts who frequently detect vulnerabilities recognize clear benefits, whereas those with less exposure or highly specialized roles are more cautious as they may see its potential to diminish the value of their expertise. CodeBERT is suitable for vulnerability detection in the industrial domain, achieving performance comparable to its detection capabilities in the opensource domain. Undersampling techniques can help decrease the number of FNs in both domain. Automating vulnerability detection in an industrial DevOps environment characterized by clearly defined roles eventually needs to address the resistance of highly specialized professionals who may perceive their roles as being at risk. RQ2. How well does CodeBERT fine-tuned on open source vulnerability data generalise to industrial technology-specific data? We conducted cross-domain evaluation by fine-tuning CodeBERT on technology-specific and technologyagnostic open-source data (resp. TOD and GOD) and testing it on unseen industrial data (ID). We found that fine-tuning with URSC undersampling on technology-agnostic open-source dataset can provide better control of false negatives in detecting unseen industrial vulnerabilities than fine-tuning industrial data from the same system with any balancing techniques. This may be due to the greater heterogeneity of vulnerability types and coding styles present in the open-source dataset that can improve the models ability to identify real vulnerabilities in industrial systems, reducing false negatives that could otherwise leave security flaws unaddressed. However, this detection capability comes at the cost of increased false positives, which can burden development teams with unnecessary investigations and potentially reduce trust in AI-DO. Fine-tuning CodeBERT with technology-agnostic open-source vulnerability data using appropriate undersampling improves the detection of industrial vulnerabilities. However, this may increase professional workload and affect trust in detection tool."
        },
        {
            "title": "5 Threats to Validity",
            "content": "Construct Validity: Threats to construct validity refer to the extent to which the experimental setting actually reflects the construct under study. We study the portability of fine-tuned pre-trained deep learner for vulnerability detection between open and industry data. Three datasets were collected using the same; differences in sample size and vulnerability count were addressed by balancing and weighting strategies. The deep learner was created in the same way for all the datasets; enabling fair same-domain and cross-domain performance comparison. Internal Validity: Threats in this category are related to internal factors that could have influenced the results. Threats including duplicate code in the datasets, 14 Mock et al. which we mitigated by PMD-CPD, and possible shared code between open and industry datasets from sources like Stack Overflow, which could not be identified. External Validity: Threats in this category concern the generalizability of the results. Results may be specific to our datasets. domain shift was found to reduce the generalizability, as intended in our study design. Results from the survey might be biased in favour of the work, as the first author had prior contact with the participants; replication of the survey would strengthen the findings. PHP was selected as target programming language due to the need of the company for it, which might limit broader generalizability. Conclusion Validity: This aspect regards the relationship between treatment and outcome, i.e., if we can ascertain, with given significance, that the outcome was consequence of the treatment. We applied standard metrics to evaluate model performance to assess the performance of the deep learner."
        },
        {
            "title": "6 Conclusion and Further Work",
            "content": "We evaluated fine-tuned deep learning model for vulnerability detection in an industrial setting. The insights we collected led to the development of AI-DO, which we later evaluated by surveying developers from 13 companies to assess its value. To examine cross-domain applicability, we created three datasets annotated under the same policy: one company projects, and two from open-source projectsone domain-related and one general. Fine-tuning CodeBERT on the three datasets individually, and further assessing different balancing strategies, enabled us to study learning transfer. Results show the approach is feasible and valuable for companies aiming to shift left vulnerability detection, i.e., detecting vulnerable code as early in the development process as possible. Developers were open-minded regarding the tool, AI-DO, but concerns arose regarding trust and localisation of the solution. Models trained on industrial data performed well in-domain but poorly on general open-source code, likely due to differences in vulnerability types and coding standards. Training on technologyagnostic open data with undersampling increases the detection of vulnerabilities in the industrial dataset. Future work includes extending to multiple programming languages, transformer models, and annotation types, leveraging our new multi-annotation dataset [23]. Acknowledgments. Funded by the European UnionNext Generation EU, Mission 4 Component 1 CUP I52B23000570003. The work has been funded by the project CyberSecurity Laboratory no. EFRE1039 under the 2023 EFRE/FESR program. We acknowledge ISCRA for awarding this project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy). We thank WÃ¼rth Phoenix for hosting the first author and providing the data for this work."
        },
        {
            "title": "References",
            "content": "1. Chakraborty, P., Arumugam, K.K., Alfadel, M., Nagappan, M., McIntosh, S.: Revisiting the performance of deep learning-based vulnerability detection on realistic Cross-Domain Evaluation 15 datasets. IEEE Transactions on Software Engineering 50(8), 21632177 (2024). https://doi.org/10.1109/TSE.2024.3423712 2. Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., Carbin, M.: The lottery ticket hypothesis for pre-trained bert networks. In: Advances in Neural Information Processing Systems. vol. 33. Curran Associates, Inc. (2020) 3. Croft, R., Babar, M.A., Kholoosi, M.M.: Data quality for software vulnerability datasets. In: 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). pp. 121133 (2023). https://doi.org/10.1109/ICSE48619.2023.00022 4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training (2019), language understanding transformers for of deep bidirectional https://arxiv.org/abs/1810.04805 5. Dong, C., Li, S., Yang, S., Xiao, Y., Wang, Y., Li, H., Li, Z., Sun, L.: Libvdiff: Library version difference guided oss version identification in binaries. In: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. ICSE 24, Association for Computing Machinery, New York, NY, USA (2024) 6. Fan, J., Li, Y., Wang, S., Nguyen, T.N.: c/c++ code vulnerability dataset with code changes and cve summaries. In: 2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR). pp. 508512 (2020) 7. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al.: Codebert: pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020) 8. Fitzgerald, B., Kesan, J.P., Russo, B., Shaikh, M., Succi, G.: Adopting Open Source Software: Practical Guide. The MIT Press (2011) 9. Fu, M., Tantithamthavorn, C.: Linevul: transformer-based line-level vulnerability prediction. In: Proceedings of the 19th International Conference on Mining Software Repositories. pp. 608620 (2022) 10. Gamma, E.: Design patterns: elements of reusable object-oriented software. Person Education Inc (1995) 11. Guo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., Yin, J.: Unixrepresentation (2022), coder: Unified cross-modal pre-training https://arxiv.org/abs/2203.03850, published in ACL 2022 code for 12. Hin, D., Kan, A., Chen, H., Babar, M.A.: Linevd: statement-level vulnerability detection using graph neural networks. In: Proceedings of the 19th International Conference on Mining Software Repositories. p. 596607. MSR 22, Association for Computing Machinery, New York, NY, USA (2022) 13. Husain, H., Wu, H.H., Gazit, T., Allamanis, M., Brockschmidt, M.: Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019) 14. Improta, C., Tufano, R., Liguori, P., Cotroneo, D., Bavota, G.: Quality in, quality out: Investigating training datas role in ai code generation. In: 2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC) (2025) 15. Le, T.H.M., Ali Babar, M.: Mitigating data imbalance for software vulnerability assessment: Does data augmentation help? In: Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement. p. 119130. ESEM 24, Association for Computing Machinery, New York, NY, USA (2024). https://doi.org/10.1145/3674805.3686674 16. Lenarduzzi, V., Lomio, F., Huttunen, H., Taibi, D.: Are sonarqube rules inducing bugs? In: 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). pp. 501511 (2020) 16 Mock et al. 17. Li, K., Chen, S., Fan, L., Feng, R., Liu, H., Liu, C., Liu, Y., Chen, Y.: Comparison and evaluation on static application security testing (sast) tools for java. In: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. p. 921933. ESEC/FSE 2023, Association for Computing Machinery, New York, NY, USA (2023) 18. Li, Z., et al.: Vuldeepecker: deep learning-based system for vulnerability detection. arXiv preprint arXiv:1801.01681 (2018) 19. McConnell, S.: Code complete. Pearson Education (2004) 20. Mock, M., Borsani, T., Di Fatta, G., Russo, B.: Optimizing deep learning models to address class imbalance in code comment classification. In: 2025 IEEE/ACM International Workshop on Natural Language-Based Software Engineering (NLBSE). pp. 4548 (2025). https://doi.org/10.1109/NLBSE66842.2025.00016 21. Mock, M., Forrer, T., Russo, B.: Cross-evaluation detection based data on https://github.com/CybersecurityLab-unibz/cross_domain_evaluation vulnerability proprietary open and of transformer- (2025), 22. Mock, M., Forrer, T., Russo, B.: Where do developers admit their security-related concerns? In: Agile Processes in Software Engineering and Extreme Programming Workshops. pp. 189195. Springer Nature Switzerland, Cham (2025) 23. Mock, M., Melegati, J., Kretschmann, M., Diaz Ferreyra, N.E., Russo, B.: Madewic: Multiple annotated datasets for exploring weaknesses in code. In: Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. p. 23462349. ASE 24, Association for Computing Machinery (2024) 24. Nguyen-Duc, A., Do, M.V., Luong Hong, Q., Nguyen Khac, K., Nguyen Quang, A.: On the adoption of static analysis for software security assessmenta case study of an open-source e-government project. Computers & Security 111, 102470 (2021) 25. Pan, S., Bao, L., Zhou, J., Hu, X., Xia, X., Li, S.: Unveil the mystery of critical software vulnerabilities. In: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering. p. 138149. FSE 2024, Association for Computing Machinery, New York, NY, USA (2024) 26. Pmd-cpd (2024), https://pmd.github.io/pmd/pmd_userdocs_cpd.html 27. Prechelt, L.: Early stopping-but when? In: Neural Networks: Tricks of the trade, pp. 5569. Springer (2002) 28. r2c: Semgrep (2024), https://semgrep.dev, version 0.73.0 29. Red Hat, I.: OpenShift: Kubernetes Platform for Developing and Running Applications (2011), https://www.openshift.com/, accessed: June 16, 2025 30. Russo, B., Melegati, J., Mock, M.: Leveraging multi-task learning to improve the detection of satd and vulnerability. In: 2025 IEEE/ACM 33rd International Conference on Program Comprehension (ICPC). pp. 0112 (2025) 31. Schwaber, K., Sutherland, J.: The scrum guide. Scrum Alliance 21(1), 138 (2011) 32. Sneath, P.: The application of computers to taxonomy. Microbiology 17(1) (1957) 33. SonarSource S.A: Sonarqube (2024), https://www.sonarqube.org, version 9.4 34. Thomas, T.W., Tabassum, M., Chu, B., Lipford, H.: Security during application development: An application security expert perspective. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. pp. 112 (2018) 35. Torvalds, L., Hamano, J.C.: Git: Distributed Version Control System (2005), https://git-scm.com/, accessed: June 16, 2025 36. Tree-sitter Project: Tree-sitter: An Incremental Parsing System for Programming Tools (2018), https://tree-sitter.github.io/tree-sitter/, accessed: June 16, 2025 37. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in Neural Information Processing Systems (2017)"
        }
    ],
    "affiliations": [
        "Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy",
        "R&D Department, WÃ¼rth Phoenix, Bolzano, Italy"
    ]
}