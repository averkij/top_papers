{
    "paper_title": "HARP: Hesitation-Aware Reframing in Transformer Inference Pass",
    "authors": [
        "Romain Storaï",
        "Seung-won Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to \"off-the-shelf\" Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers a practical solution for enhancing the performance of Transformer-based language models with minimal computational impact."
        },
        {
            "title": "Start",
            "content": "HARP: Hesitation-Aware Reframing in Transformer Inference Pass Romain Storaï and Seung-won Hwang* Computer Science and Engineering, Seoul National University {romsto,seungwonh}@snu.ac.kr 4 2 0 2 0 1 ] . [ 1 2 8 2 7 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, simple modification to off-the-shelf Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers practical solution for enhancing the performance of Transformer-based language models with minimal computational impact."
        },
        {
            "title": "Introduction",
            "content": "Causal language models based on the Transformer architecture (Vaswani et al., 2017) use constant number of layer traversals to generate each new token. While this architecture is beneficial to provide easy parallelization during training (Dehghani et al., 2019), it may not fully leverage the models full potential during inference, where tokens are generated sequentially. Research in adaptive computation (e.g. Graves, 2017; Leviathan et al., 2023; Elhoushi et al., 2024; Leviathan et al., 2024) suggests that inference steps are not equally challenging, with some being harder and others easier. Intuitively, these more challenging tokens would benefit from additional computational resources * Corresponding author. 1 Figure 1: The left side represents the Transformers vanilla forward pass, while the right side illustrates the modified forward pass, HARP, which selectively applies additional computation by reframing inputs when the model hesitates. This improves performance on harder tokens without the need for retraining. to improve accuracy. Unfortunately, the current Transformer architecture treats each token equally regardless of its difficultypotentially leading to imprecision and performance drops. To address this limitation, trend in language models has been to simply scale up models in size (Brown et al., 2020), allowing for more computation. While the difficult tokens benefit from this scaling, it also leads to unnecessary overhead for easier tokens. To tackle this uniform additional computation, Speculative Decoding (Leviathan et al., 2023; Chen et al., 2023) employs bigger model to verify and correct the tokens generated by the smaller model, acting as an external verifier. Considering the smaller model as the original model, the larger model performs additional computations to ensure the quality of the generated tokens. This enables the system to spend more computational resources on complex tokens while preserving efficiency for easier ones. However, this approach requires the involvement of an external model. Similarly, Goyal et al. (2024) add fixed pauses during inference, using pause tokens, to allow for additional computation on harder tokens. While their method improves the generation of harder tokens, it requires full training and finetuning, and it still applies uniform additional computation when simpler tokens do not need the extra time. We draw inspiration from human behaviors to allow models to perform additional computations for harder steps without relying on external models or requiring retraining. Two cognitive effects the (1) hesitation and the (2) framstand out: ing effect. First, hesitation reflects uncertainty in decision-making. Humans tend to pause and reconsider when faced with difficult decisions (Shenhav et al., 2013) such that more effort is spent on complex inputsaligning with the idea of harder tokens during inference. Second, the framing effect indicates that how information is presented can influence judgment and response (Kahneman, 2012). It implies that different representation of In the same input can lead to better outcomes. the following, we will refer to this another-view representation as reframing the inputs. Building on these human-inspired concepts, we introduce Hesitation-Aware Reframed Forward Pass (HARP), plug-and-play modification to the Transformers forward pass. To illustrate, we summarize HARP on the right side of Figure 1. We begin with the standard forward pass, processing the inputs through the embedding layer and subsequent layers to produce initial logits. We then evaluate the hesitation of the model by computing its uncertainty over the logits (detailed in Section 3.2). If the model is not hesitating, we directly output the initial logits. However, when the model is in hesitation state, we reframe the inputs to infuse different representation (explained in Section 3.3). To do so, we perturb the embeddings and perform new forward pass. Finally, we combine the logits from the original and the additional forward pass, producing final output incorporating both perspectives. By selectively reframing inputs, HARP mimics human reconsideration under uncertainty, achieving up to 5.16% performance improvements with minimal additional cost. Our method outperforms beam search (Kasai et al., 2024) across multiple tasks, offering higher accuracy and significantly faster inference. The code1 is publicly available. Our contributions can be summarized as follows: We introduce selection process based on 1https://github.com/romsto/HARP token-level uncertainty to determine when additional computation is beneficial during inference and demonstrate its importance for improving model performance. We evaluate novel approach to reframing inputs at inference time using dropout on the embeddings. We combine these two components to create HARP (Hesitation-Aware Reframed Forward Pass), method that generally applies to any decoding algorithms, and thoroughly evaluate its performance across various tasks, model sizes, and in combination with existing techniques."
        },
        {
            "title": "2.1 Adaptive Computation in Transformers",
            "content": "Adaptive computation in Transformers (ACT) can be categorized into efficiency-focused and performance-focused categories. Efficiencyfocused ACT has been the primary focus of research, aiming at improving efficiency by reducing computation for easier inference steps (Leviathan et al., 2023; Chen et al., 2023; Elhoushi et al., 2024). These approaches often involve using smaller models or skipping layers when processing less challenging tokens, thereby optimizing computational resources and resulting in inference speedups. In contrast, performance-focused ACT, which includes our method HARP, targets the harder inference steps, prioritizing performance gains over efficiency improvements. Our approach shares motivations with the work of Goyal et al. (2024), who allocate more computation by extending the models vocabulary with pause tokens. These prepend tokens to the generation instruct the model to perform additional fixed computations, resulting in higher accuracy. However, while effective, the pause tokens method requires retraining and fine-tuning of the model. Unlike the pause tokens approach or scaled-up models using efficiency-focused ACT, HARP is training-free, model-agnostic, and plug-and-play method that can be applied to any Transformerbased model without the need for retraining, making it an advantageous solution within the performance-focused ACT framework. In parallel to adaptive computation work, some studies have explored methods to enhance reason2 ing capabilities in LLMs (e.g. Zelikman et al., 2022; Zelikman et al., 2024; Hosseini et al., 2024; Andukuri et al., 2024). While these approaches can be seen as incorporating extra computation, their focus diverges from ours, as they improve reasoning through fine-tuning rather than optimizing token-level computation selectively."
        },
        {
            "title": "Modeling",
            "content": "Uncertainty quantification (Abdar et al., 2020) on token-level is not well-explored area. Most of the existing works focus on the evaluation of sequencelevel uncertainty (e.g. Arteaga et al., 2024; Manakul et al., 2023; Kuhn et al., 2023) or on higher level. In contrast, our work focuses on token-level uncertaintythe uncertainty in the probability distribution over the vocabulary for predicting the next token. Luo et al. (2024) introduce ratiobased method to measure uncertainty. While this approach offers an intuitive interpretation of uncertainty, it lacks some theoretical grounding and might fail to capture more subtle hesitation as it relies on only the two highest probabilities of the distribution. Therefore, we use the Shannon Entropy (Shannon, 1948) as our uncertainty estimator. It is an information-theoretic uncertainty measure that captures the amount of information in probability distribution. Entropy represents the expected number of bits required to resolve the uncertainty of prediction. Higher entropy indicates more uncertainty, while lower entropy suggests more confident prediction."
        },
        {
            "title": "2.3 Reframing at Inference Time",
            "content": "Reframing data into different perspectives is wellestablished technique for training machine learning models, particularly through Multi-View Learning (MVL) (Chaudhuri et al., 2009). In MVL, models are trained on multiple representations of the same data, which improves generalization. However, these techniques are restricted to the training phase and are not designed to be applied during inference (Xu et al., 2013), our target. Parallely, NEFTune (Jain et al., 2024) brings promising direction. It introduces noise into embeddings during the training to improve instructionbased fine-tuning. Although NEFTune targets the training phase only, we hypothesize that similar approachinjecting noise into embeddings could be beneficial during inference as well. By adding noise into embeddings at inference time, the model could gain new perspective of the same inputs, potentially improving its ability to handle ambiguous inputs. While NEFTune uses random uniform noise, our work explores different noise approaches, utilizing dropout on the embeddings to induce new representation. As detailed in Appendix A, we find that dropout leads to more consistent improvements."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we present our Hesitation-Aware Reframed Forward Pass (HARP) method. We aim to perform an additional forward step from different perspective when the model encounters uncertainty. We begin by reviewing the standard forward pass of transformers. Then, we introduce the two key components of HARP: quantifying uncertainty during inference and reframing inputs. Finally, we will integrate these components to present the complete algorithm."
        },
        {
            "title": "3.1 Preliminary: Transformer Forward Pass",
            "content": "We recall the forward pass of Transformer model as we will modify its architecture. It processes input tokens to generate logits, representing unnormalized probabilities for predicting each token in the sequence, including the next token. Let = (x1, . . . , xn) be the tokenized input sequence of length n, where each xi is token ID. For simplicity, we denote the embedding layer as emb(), while femb() represents the rest of the model, consisting of serial layers. Each layer includes multi-head self-attention sublayer, fully connected sublayer, and layer normalizations. We deliberately omit the discussion of positional encodings and last-layer projection. First, the embedding layer emb maps each input token to dense vector representation: = emb(x), where Rnd and is the embedding dimension. The embedded inputs are then processed through the rest of the model. Thus, the forward pass can be concisely expressed as: logits = femb(emb(x)) (1) where logits RnV and is the vocabulary size. The resulting logits contains unnormalized predictions for each input position. The last positions logits are used to predict the next token."
        },
        {
            "title": "3.2 Uncertainty Estimation",
            "content": "We want to quantify the models uncertainty for each newly generated token. To do this, we focus on the logits of the last position, which are used to predict the next token. First, the logits are normalized using the SOFTMAX function to obtain probability distribution over the vocabulary . The SOFTMAX ensures that each value in the distribution is between 0 and 1 and that the total sum of probabilities over equals 1, i.e., (cid:80)V i=1 (vi x) = 1, where vi represents token in the vocabulary and (vi x) is the probability of token vi, knowing x, under the distribution P. To measure the uncertainty of the distribution P, we then use Shannon entropy (Shannon, 1948). The entropy SHANNON is defined as: SHANNON(P) = (cid:88) i=1 (vi x) log2 (vi x) (2)"
        },
        {
            "title": "3.3 Reframing Inputs",
            "content": "Our objective is to perturb the embeddings, which represent the models understanding of the data, to present the input sequence to the model from an alternate perspective. We follow the approach of NEFTune (Jain et al., 2024), which injects random noise into the embeddings during training. NEFTune generates this noise by sampling values from uniform distribution in [1, 1], then scaling the noise based on sequence length, embedding dimension, and tunable parameter. To explore alternative perturbation methods, we experimented with various noise strategies. We choose to use dropout among them, as it consistently yields the best results. In Appendix A, we demonstrate that dropout performs better than NEFTunes random uniform noise in this context. Let = emb(x) be the original embeddings of the input sequence x, and let δ be the dropout rate. The reframed embeddings ˆe are then obtained as follows: ˆe = DROPOUT(e, δ) (3) where DROPOUT randomly sets fraction δ of the elements in the embeddings to zero."
        },
        {
            "title": "3.4 Hesitation-Aware Reframed Forward Pass",
            "content": "(HARP) Our method, HARP, adapts the standard Transformer forward pass by adding an additional computation Algorithm 1 HARP: Hesitation-Aware Reframed Forward Pass Step Inputs: input sequence x, embedding layer emb(), rest of the model femb() Hyperparameters: uncertainty threshold θ, dropout rate δ, combination factor β 1: emb(x) logits femb(e) 2: 3: SOFTMAX(logits) 4: If the model is uncertain, we perform one 5: 6: 7: more forward pass but reframed. if SHANNON(P) > θ then ˆe DROPOUT(e, δ) logitsr femb(ˆe) logits β logits + (1 β) logitsr 8: 9: end if 10: return logits step when the model exhibits uncertainty. This procedure is outlined in Algorithm 1, using the same colors as in Figure 1 for clarity. First, we perform the standard Transformer forward pass (presented in Section 3.1). From the input sequence x, we compute the token embeddings and the logits, denoted as logits. Next, we estimate the uncertainty at the current step using Shannon Entropy (detailed in Section 3.2). To achieve this, we normalize the logits and compute the Shannon entropy. If the entropy is below predefined uncertainty threshold θ, the model is considered confident in its generation. In this case, we return the logits from the standard forward pass, and the current generation step ends. However, if the uncertainty exceeds the threshold θ, the model is uncertain. In this case, we initiate second forward pass but with reframed input sequence (explained in Section 3.3). The reframed embeddings, ˆe, are obtained by applying dropout with rate δ to the original embeddings e. We then perform second forward pass using ˆe, which outputs reframed logits, denoted as logitsr. Finally, the original and reframed logits are combined using combination factor β to produce the final output logits. logits = β logits + (1 β) logitsr (4) The combination factor β controls the balance between original and reframed logits. We empirically find that setting β = 0.5 balances the contributions of both passes effectively. 4 The dropout-based perturbation forces the model to consider different representations of the input. By selectively introducing additional computations only when necessary, HARP balances computational efficiency and prediction reliability. Our method introduces three hyperparameters: the uncertainty threshold θ, the dropout rate δ, and the combination factor β. In our experiments, we set these parameters to values identified as optimal during preliminary testing. The optimal choices for these hyperparameters may vary across tasks or models and can benefit from further tuning; we leave studying the impact of these values in more depth as future work."
        },
        {
            "title": "4.1 Models",
            "content": "We consider decoder-only models of size 3.8B, 7B, and 8B as they are the standard in recent times. Particularly, we use LLaMA-3.1 (Dubey et al., 2024), Mistral 7B v0.3 (Jiang et al., 2023) and Phi 3.5 Mini (Abdin et al., 2024) to cover different scales and architectures. We consider only aligned models (denoted as \"Instruct\") for their simplicity of evaluation and greater performance. Aligned models (Christiano et al., 2017) refer to models that are fine-tuned to better follow instructions, using methods such as supervised fine-tuning or reinforcement learning from human feedback (RLHF). All the models are loaded using quantized INT8 precision to fit the GPU and accelerate inference."
        },
        {
            "title": "4.2 Datasets",
            "content": "As our method targets off-the-shelf LLMs, we consider five datasets covering varied downstream tasks and output formats in order to reproduce the variety of missions they can encounter. GSM8K (Cobbe et al., 2021) is famous mathematical reasoning benchmark where the task involves solving grade school-level math word problems. The output is free text, typically detailed solution or numerical answer. CommonSenseQA (CsQA) (Talmor et al., 2019) is multiple-choice question benchmark that tests commonsense reasoning and general understanding abilities of the model. The output is one selected option from five possible choices. LAMBADA (Paperno et al., 2016), language modeling benchmark focused on text understanding and completion, where the task is to predict the final word of passage based on its context. The output is single word. MMLU Pro (Wang et al., 2024)an enhancement of Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) datasetevaluates models on wide range of academic and professional subjects. The task consists of answering multiple-choice questions, with the output being one selected option from up to ten possible choices. CNN/Daily Mail (CNN/DM) (Nallapati et al., 2016) dataset is used for text summarization tasks, where the goal is to generate concise summaries of news articles. The output is free text in the form of summary. Prompt details can be found in Appendix C. All the datasets are evaluated with zero-shot prompt."
        },
        {
            "title": "4.3 Evaluation",
            "content": "Since HARP is generally applicable to any decoding methods, we compare different decoding methods (Shi et al., 2024) both with and without the HARP forward pass modification. We use greedy search decodinga deterministic method that fetches the highest probable tokenand nucleus sampling searcha stochastic one that samples the next token (Shi et al., 2024). In addition, we evaluate beam search decoding (Kasai et al., 2024) without HARP as this decoding method usually results in more accurate results. The hyperparameters used in our experiments are fixed at dropout rate of δ = 0.20 and an uncertainty threshold of θ = 1.0. We set the temperature for nucleus sampling to τ = 0.6 and the top-p to 0.9. In the case of beam search, the number of beams is = 3 with top-k of 5. We chose to apply length-normalization (Wu et al., 2016) as we found it beneficial to tasks such as summarization and reasoning. This length penalty is fixed to α = 0.6 for every datasets, as we seek to compare HARP with modeland task-agnostic methods. We empirically found the value by evaluating model on subset of three datasets (CNN/DM, GSM8K and LAMBADA). When evaluating Chain-of-Thought (Wei et al., 2024) prompting, we prepend Lets think step-by-step. to the generation of the model. We evaluate subset of each dataset with batch size of 1, without caching, on single RTX3090 (24GB) using the same seed for every example. 5 The accuracy is reported for all datasets except CNN/DM, where we evaluate the ROUGE-1 score. Moreover, we record the generation time of each method."
        },
        {
            "title": "5 Results",
            "content": "HARP improves the performance of all tasks. Our method demonstrates consistent performance improvements across all tasks. As shown in Table 1, in the greedy setting, HARP outperforms both the vanilla model and beam search decoding in most scenarios. The improvements are particularly notable for tasks like LAMBADA, with gains of up to 5.16%. When applied to nucleus sampling, HARP outperforms the vanilla model in most cases, especially in math-reasoning tasks like GSM8K. On GSM8K, Goyal et al. (2024) achieved +1.00% improvement (from 7.50% to 8.50% accuracy) with their training and fine-tuning pause token method. In comparison, HARP delivers an even higher improvement of +1.51% on the same dataset, using more performant model and without any retraining. In summary, our HARP allows further improvements as high as +5.16% of high-ended models that already show strong performance on many tasks without requiring any fine-tuning. This demonstrates the potential of HARP to enhance state-ofthe-art models. HARP is model-agnostic. We show that HARP also consistently improves models ranging from 3 to 8 billion parameters, including LLaMA-3.1, Mistral It illustrates its robustv0.3, and Phi 3.5 Mini. ness and wide applicability across diverse language models. HARP works with advanced techniques. As shown in Table 2, advanced techniques, such as Chain-of-Thought (CoT) (Wei et al., 2024) prompting, are further enhanced with HARP. For instance, applying the modified forward pass to CoT prompting with the LLaMA model improves accuracy by 0.79%, 1.16%, and 1.09% on CommonsenseQA, GSM8K, and MMLU Pro, respectively. This shows our method can seamlessly combine with other existing methods, further enhancing model performance. HARP is faster than Beam Search. Figure 2 reveals that despite the additional computation, the inference time of HARP remains close to that of Figure 2: LLaMA 3.1 Instruct (8B) average relative inference time of the original model greedy search (Vanilla), beam search decoding (Beam S.), and our HARP (Ours) using greedy search decoding. Values and other models are detailed in Table 6. the vanilla generation, introducing minimal additional computational cost. The inference time for our method is consistently under twice that of the original model across all tasks. In contrast, beam searchwhile most of the time yielding lower performance than HARPinduces significantly higher inference cost. This is the indicator that our method strikes balance between performance gains and inference time. Beam search shows significant delays, often higher than x2.4 the time required by the greedy search generation, whereas HARP achieves inference times lower than x1.4. Additional results in Table 6 validate this efficiency by comparing other models. For example, the Mistral model increases inference time by only x1.17 to x1.49, while beam search increases it by x2.51 to x3.72. On average, our method results in an x1.25 increase in inference time across all tasks and models, demonstrating its efficiency compared to more computationally intensive approaches like beam search, which takes more than twice as long."
        },
        {
            "title": "6 Analysis",
            "content": "Uncertainty guides the additional computations. To investigate the effectiveness of our uncertaintybased approach (denoted as SHANNON), we compare it with method that unconditionally adds an extra forward step for every token (denoted as wo SHANNON). This comparison helps us under6 Models Methods CsQA GSM8K LAMBADA MMLU Pro CNN/DM Datasets Relative Cost overhead to Vanilla LLaMA-3.1 Instruct (8B) Mistral v0.3 Instruct (7.25B) Phi 3.5 Mini Instruct (3.82B) Vanilla (Greedy) Beam Search Ours (Greedy) 78.79 79.29 80.30 (+1.52) 76.88 76.38 78.39 (+1.51) Vanilla (Nucleus) Ours (Nucleus) 79.80 79.29 (-0.51) 73.00 74.00 (+1.00) Vanilla (Greedy) Beam Search Ours (Greedy) 70.37 70.99 70.99 (+0.62) Vanilla (Nucleus) Ours (Nucleus) 70.74 70.21 (-0.53) Vanilla (Greedy) Beam Search Ours (Greedy) 77.20 77.72 78.24 (+1.04) 43.62 50.53 48.40 (+4.79) 29.38 31.88 (+2.50) 72.50 73.00 73.00 (+0.50) Vanilla (Nucleus) Ours (Nucleus) 77.04 77.55 (+0.51) 71.50 74.50 (+3.00) 30.86 31.35 36.02 (+5.16) 27.44 31.38 (+3.94) 45.15 45.70 49.76 (+4.64) 45.02 48.26 (+3.24) 32.76 33.60 33.44 (+0.68) 31.85 32.99 (+1.14) 46.42 48.21 48.21 (+1.79) 42.55 43.45 (+0.90) 31.76 33.11 31.76 (0.00) 32.44 33.17 34.03 (+1.59) 30.81 32.38 (+1.57) 29.11 28.71 29.57 (+0.47) 31.76 33.79 (+2.03) 28.72 28.72 (0.00) 29.65 33.78 32.75 (+3.10) 28.82 34.53 (+5.71) 26.10 25.79 26.97 (+0.87) 25.30 25.19 (-0.11) x2.79 x1.16 x1.17 x3.07 x1.24 x1.29 x3.18 x1.26 x1. Table 1: Performance comparison of the original model (Vanilla) with various decoding methods: greedy search, nucleus sampling, beam search, and our HARP modified forward pass in both greedy and nucleus sampling settings. Numbers in parentheses indicate performance gain or loss relative to Vanilla for each decoding method. The cost column shows the relative inference time based on Vanillas corresponding decoding method, averaged over five datasets. Reported scores reflect accuracy, except for CNN/DM, where the ROUGE-1 score is reported. Models Methods LLaMA CoT Ours CoT 75.75 (+0.79) CsQA 74.96 Datasets GSM8K MMLU Pro 48.19 49.35 (+1.16) 75.49 76.58 (+1.09) Mistral CoT 70.64 Ours CoT 75.23 (+4.59) 48.00 46.00 (-2.00) 32.37 32.93 (+0.56) Table 2: Accuracy comparison of Chain-of-Thought (CoT) and HARP applied to CoT, using greedy decoding. Numbers in parentheses indicate gain or loss relative to the standard CoT approach. stand whether the improvements come solely from the additional computation or from our uncertaintyselected approach. Table 3 presents the results across multiple tasks and two models. Our analysis reveals that unconditionally adding an extra forward step is not universally beneficial. While it improves performance compared to the vanilla model in most cases, especially in tasks like GSM8K and LAMBADA, it also results in lower or equal performance on tasks like multiple-choice questions (CommonsenseQA and MMLU Pro). The uncertainty-conditional method outperforms the unconditional approach, even though it is sometimes marginal. This suggests that uncertainty is an excellent signal for selecting the steps requiring extra computation. key advantage of the selective approach is its computational efficiency. As shown in the Cost column of Table 3, the uncertainty7 based method requires significantly lower computational overhead than the unconditional method. While additional computation can generally improve performance, our uncertainty-based approach offers more nuanced and efficient solution. It achieves comparable or superior results to unconditional computation while balancing performance gains and computational costs. Uncertainty pinpoints key reasoning steps. We analyze the output generations of HARP by highlighting the tokens that require extra computation in Appendix B. Upon reviewing some examples, particularly in problem-solving tasks, we observe that high-uncertainty states usually appear at the start of each reasoning step. This mirrors human decision-making, where individuals take more time to consider the stages of reasoning to construct valid response than the actual content of the reasoning. Additionally, we note that HARP tends to generate shorter sequences than the original forward pass. Across every task (except ones with nextword prediction), we observe an average reduction of 5.5% in output length when applying HARP. This shortening effect is not caused by promoting the end-of-sequence (EOS) token as the most likely token earlier in uncertain cases. Instead, this effect appears from variations in token selection during earlier stages of generation. HARP results in more Models Methods CsQA GSM8K LAMBADA MMLU Pro CNN/DM Datasets LLaMA-3.1 Instruct (8B) wo SHANNON SHANNON 77.16 (-1.63) 80.30 (+1.52) 78.39 (+1.51) 78.39 (+1.51) 36.08 (+5.22) 36.02 (+5.16) 44.90 (-1.52) 48.21 (+1.79) 32.66 (+0.22) 34.03 (+1.59) Mistral v0.3 Instruct (7.25B) wo SHANNON SHANNON 70.37 (0.00) 70.99 (+0.62) 45.74 (+2.12) 48.40 (+4.79) 49.57 (+4.42) 49.76 (+4.64) 31.76 (0.00) 31.76 (0.00) 28.62 (-0.49) 29.57 (+0.47) Relative Cost x2.31 x1 x1.68 x1 Table 3: Study of the impact of unconditional additional step (wo SHANNON) and HARP uncertainty-based additional step (w SHANNON) using greedy decoding. The cost column denotes the relative inference time with SHANNON averaged over the five datasets. Numbers in parentheses indicate performance gain or loss relative to the original model performance. Models Methods CsQA GSM8K LAMBADA MMLU Pro CNN/DM Datasets Relative Cost LLaMA-3.1 Instruct (8B) Mistral v0.3 Instruct (7.25B) 1-step 2-steps 4-steps 1-step 2-steps 4-steps 80.30 79.84 (-0.46) 79.72 (-0.58) 70.99 70.47 (-0.52) 70.41 (-0.58) 78.39 76.89 (-1.50) 80.39 (+2.00) 48.40 46.40 (-2.00) 45.36 (-3.04) 36.02 35.52 (-0.50) 35.02 (-1.00) 49.76 48.26 (-1.50) 48.15 (-1.61) 48.21 47.21 (-1.00) 43.71 (-4.50) 31.76 31.76 (0.00) 31.76 (0.00) 34.03 33.92 (-0.11) 34.29 (+0.26) 29.57 29.66 (+0.09) 29.83 (+0.26) x1 x1.52 x1.97 x1 x1.11 x1.37 Table 4: Performance comparison using different number of reframing steps across all the datasets. \"x-steps\" indicates that the model can perform up to additional forward passes when uncertainty exceeds the threshold. The cost column denotes the average relative inference time with HARP (1-step). Numbers in parentheses indicate performance gain or loss relative to HARP. concise responses, especially for reasoning-based tasks and summarizing tasks. One additional representation is sufficient for reframing. When facing uncertainty, HARP reframes the inputs only once before pursuing the next generation step. To explore the effect of multiple reframings, we experiment with adding extra forward passes while the uncertainty remains higher than θ (capping it at maximum number of steps). Surprisingly, Table 4 shows that increasing the number of additional steps often leads to decline in performance. This interesting outcome suggests that while single additional representation can provide useful alternative perspective on the inputs, too many representations may be penalizing. In datasets like CommonsenseQA, LAMBADA, and MMLU Pro, we observe consistent drop in accuracy with more reframing steps. Although GSM8K achieves notable +2.00% increase in accuracy using four additional steps with LLaMA, these gains come at the cost of significantly higher computation time, scaling up to x1.97 in the 4-step method. We hypothesize that more than two representations might confuse and distract the model from the original task, decreasing precision. This aligns with cognitive theories, suggesting that while considering reframings enhances problem-solving, too many representations increase cognitive load (Sweller, 1988), reducing performance. In more mathematical way, this behavior may be due to the random noise introduced during reframing and the way logits are combined. While random noise can sometimes be beneficial, it can also be detrimental, and increasing the number of reframings increases the likelihood of harmful noise. Additionally, as the logits from the vanilla and reframed representations are averaged (see Equation 4), introducing more than one reframing step reduces the weight of the vanilla logits, disproportionately favoring the reframed ones, contributing the the higher likelihood of harmful representations."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper presented novel method, HARP, designed to enhance language model inference without requiring retraining or architectural modifications. Our approach uses selection process based on token-level uncertainty to identify when additional computation is advantageous and an innovative method for reframing inputs during inference. We demonstrated that HARP significantly improves performance across various tasks and model sizes 8 by combining these components, achieving accuracy gains up to 5.16%. Importantly, HARP maintains inference efficiency compared to methods like beam search, with only an x1.25 average increase in inference time over the standard model."
        },
        {
            "title": "8 Limitations",
            "content": "Our study has several limitations that need to be addressed in future research. First, due to resource and time constraints, the evaluation of HARP was conducted on limited subset of each dataset. Additionally, the method has yet to be tested on larger language models, particularly those with 70 billion parameters or more. Although we attempt to cover range of tasks, there are other language challenges that our method has not been tested on. Consequently, we cannot definitively assess how HARP generalizes across broader tasks or scales to significantly larger models. Furthermore, our current implementation faces some challenges in inference efficiency. The method could slow down batch processing since uncertain tokens require additional computation. Moreover, performance might be impacted when using HARP with models that employ key-value caching (KVCache). Embedding dropout may temporarily invalidate the KV cache, causing VRAM spike during each reframing step. In the worst-case scenariowhere all tokens are affectedVRAM usage could briefly double. Other approaches could be explored to mitigate this, such as directly perturbing the KVCache to perform reframing or doing layer-specific perturbations. Looking forward, there are promising ideas for future work, in addition to exploring alternative uncertainty measures or perturbation methods. One intriguing direction would be to explore the application of the uncertainty selection mechanism during the fine-tuning process rather than only at inference time. This could involve integrating pause token, as proposed by Goyal et al. (2024), during training to teach the model when to allocate additional computation. Such an approach could enable models to increase computation when needed autonomously. Another direction could involve adapting beam search decoding with hesitation and reframing. Creating new beams when uncertainty is encountered and keeping track of all the branches may enhance the results even more. Finally, combining the proposed method with speculative decoding (Leviathan et al., 2023; Chen et al., 2023) could offer further optimizations, enabling gains in both efficiency and performance."
        },
        {
            "title": "9 Ethics Statement",
            "content": "In this work, we propose modifying the forward pass for improved performance. While we evaluate different models on datasets, we acknowledge that we have not evaluated the impact of our method on safety, including concerns such as toxicity, bias, or content moderation. Our goal is to enhance accuracy, but broader implicationssuch as generating harmful content or sensitive applicationsremain unexplored."
        },
        {
            "title": "Acknowledgements",
            "content": "We are grateful to Jihyuk Kim, Jongho Kim, and Jongyoon Kim for their relecture, comments, and feedback."
        },
        {
            "title": "References",
            "content": "Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. 2020. Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges. Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, and Harkirat Behl et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Chinmaya Andukuri, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah Goodman. 2024. STar-GATE: Teaching language models to ask clarifying questions. In First Conference on Language Modeling. Gabriel Y. Arteaga, Thomas B. Schön, and Nicolas Pielawski. 2024. Hallucination detection in llms: Fast and memory-efficient finetuned models. Preprint, arXiv:2409.02976. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda et al. Askell. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Kamalika Chaudhuri, Sham M. Kakade, Karen Livescu, and Karthik Sridharan. 2009. Multi-view clustering via canonical correlation analysis. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 09, page 129136, New York, NY, USA. Association for Computing Machinery. 9 Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model Preprint, decoding with speculative sampling. arXiv:2302.01318. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 43024310, Red Hook, NY, USA. Curran Associates Inc. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. 2019. Universal transformers. Preprint, arXiv:1807.03819. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, and Angela Fan et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu. 2024. LayerSkip: Enabling early exit inference and self-speculative decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1262212642, Bangkok, Thailand. Association for Computational Linguistics. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. 2024. Think before you speak: Training lanIn The Twelfth guage models with pause tokens. International Conference on Learning Representations. Alex Graves. 2017."
        },
        {
            "title": "Adaptive",
            "content": "time for recurrent neural networks. arXiv:1603.08983. computation Preprint, Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-STar: Training verifiers for self-taught reasoners. In First Conference on Language Modeling. Neel Jain, Ping yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. NEFTune: Noisy embeddings improve instruction finetuning. In The Twelfth International Conference on Learning Representations. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, and Lucile Saulnier et al. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Daniel Kahneman. 2012. Thinking, fast and slow. Penguin, London. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Dragomir Radev, Yejin Choi, and Noah A. Smith. 2024. call for clarity in beam search: How it works and when it stops. In LREC/COLING, pages 7790. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Selective attention improves transformer. 2024. Preprint, arXiv:2410.02703. Ziqin Luo, Haixia Han, Haokun Zhao, Guochao Jiang, Chengyu Du, Tingyun Li, Jiaqing Liang, Deqing Yang, and Yanghua Xiao. 2024. Sed: Self-evaluation decoding enhances large language models for better generation. Preprint, arXiv:2405.16552. Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90049017, Singapore. Association for Computational Linguistics. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çaglar Gulçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence In Proceedings of the 20th RNNs and beyond. SIGNLL Conference on Computational Natural Language Learning, pages 280290, Berlin, Germany. Association for Computational Linguistics. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The LAMBADA dataset: Word prediction requiring broad discourse context. In 10 Chang Xu, Dacheng Tao, and Chao Xu. 2013. survey on multi-view learning. Preprint, arXiv:1304.5634. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. 2024. Quiet-STar: Language models can teach themselves In First Conference on to think before speaking. Language Modeling. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534, Berlin, Germany. Association for Computational Linguistics. Claude Elwood Shannon. 1948. mathematical theory of communication. The Bell System Technical Journal, 27:379423. Amitai Shenhav, Matthew M. Botvinick, and Jonathan D. Cohen. 2013. The expected value of control: An integrative theory of anterior cingulate cortex function. Neuron, 79(2):217240. Funding Information: This work is supported by the C.V. Starr Foundation (A.S.), the National Institute of Mental Health R01MH098815-01 (M.M.B), and the John Templeton Foundation. The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the John Templeton Foundation. Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. 2024. thorough examination of decoding methods in the era of llms. CoRR, abs/2402.06925. John Sweller. 1988. Cognitive load during problem solving: Effects on learning. Cognitive Science, 12(2):257285. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2024. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, and Klaus Macherey et al. 2016. Googles neural machine translation system: Bridging the gap between human and machine translation. Preprint, arXiv:1609.08144."
        },
        {
            "title": "Methods",
            "content": "Datasets CsQA GSM8K LAMBADA MMLU Pro CNN/DM HARP (NEFTuneα=5) HARP (NEFTuneα=10) HARP (DROPOUT) 80.30 80.30 80.30 78.86 78.39 78.39 32.7 38.23 36.02 66.26 66.26 71. 43.03 43.03 48.21 Table 5: Comparison of NEFTune noise (with hyperparameter α in (5, 10)) and dropout (δ = 20%) in the HARP method using LLaMA-3.1 Instruct 8B with greedy decoding. We report accuracy for each dataset, except CNN/DM, for which we report the ROUGE-1 score. Table 5 compares HARP using NEFTune noise and dropout. It reveals that while NEFTune improves performance compared to the vanilla model  (Table 1)  by adding scaled uniform noise to embeddings, dropout offers more consistent enhancements. Specifically, dropout performances better on MMLU Pro and CNN/DM while having close score to NEFTune on other datasets. Moreover, NEFTuneα=5 results in lower score than the vanilla forward pass in the LAMBADA next word prediction task. Dropout is better candidate for infusing different perspective representation into the model during inference time."
        },
        {
            "title": "B Uncertainty in practice",
            "content": "In Figure 3, we present an answer generated with HARP modified forward pass, where we highlight the steps with SHANNON entropy higher than the threshold θ. These uncertain steps happen right before new decision is made. We also notice that, unlike humans, the model does not encounter uncertainty while playing with numbers. In this generation, HARP adjusted the top-1 predictions for five tokens before reframing. The initial top-1 predictions, before reframing, are highlighted in blue in Figure 3. Therefore, orange tokens without blue tokens are the ones where the initial prediction was already the current final token. Prompt: Walter is collecting money for charity. First he collects $500 from his neighbors. Then he collects $1500 from fund he set up online. His lawyer offers to donate three times as much as everyone else donates. Walter is going to tell him about his neighbors and the online fund until his friend gives him $200 as well. How much is Walters lawyer going to contribute? Answer: To find out how much Walters much Walter has collected so lawyer will contribute, we need to far excluding , and calculate find the out the how then multiply it by 3. Walter collected $500 from his neighbors and $1500 from the online fund. Total $ 500 + $1500 = $2000 Then, his friend gives him $200. $2000 + $200 = $2200 alters lawyer will donate three times as much as everyone else donates . So, we multiply $2200 by 3 . $2200 * 3 = $6600 alters lawyer will contribute $6600. Figure 3: Answer to the given prompt generated using HARP. Orange tokens highlight additional forward steps (i.e. tokens where uncertainty is higher than θ). Blue tokens represent the models top-1 predictions prior to reframing. 12 Models Methods CsQA GSM8K LAMBADA MMLU Pro CNN/DM Datasets LLaMA 3.1 Instruct (8B) Mistral v0.3 Instruct (7.25B) Phi 3.5 Mini Instruct (3.82B) Vanilla (Greedy) Beam Search Ours (Greedy) Vanilla (Nucleus) Ours (Nucleus) Vanilla (CoT) Ours (CoT) Vanilla (Greedy) Beam Search Ours (Greedy) 1.09 3.24 (x2.98) 1.11 (x1.02) 1.09 1.12 (x1.02) 22.28 31.97 (x1.43) 13.63 50.72 (x3.72) 16.16 (x1.19) 47.01 113.27 (x2.41) 55.28 (x1.18) 47.28 55.04 (x1.16) 41.92 49.99 (x1.19) 95.85 240.47 (x2.51) 111.94 (x1.17) Vanilla (Nucleus) Ours (Nucleus) 16.19 20.77 (x1.28) 99.08 121.23 (x1.22) 0.80 1.84 (x2.29) 1.11 (x1.39) 0.76 1.11 (x1.46) - - 1.40 4.40 (x3.16) 1.97 (x1.41) 1.36 2.03 (x1.49) Vanilla (CoT) Ours (CoT) 21.95 31.59 (x1.44) 114.47 135.08 (x1.18) - - Vanilla (Greedy) Beam Search Ours (Greedy) 24.64 85.77 (x3.48) 33.57 (x1.36) 54.07 177.82 (x3.29) 57.22 (x1.06) Vanilla (Nucleus) Ours (Nucleus) 26.49 37.07 (x1.40) 56.63 59.75 (x1.06) 0.73 2.06 (x2.84) 0.97 (x1.33) 0.73 0.97 (x1.34) 24.46 79.59 (x3.25) 23.20 (x0.95) 28.13 24.36 (x0.87) 49.75 63.98 (x1.29) 128.39 388.72 (x3.03) 164.37 (x1.28) 125.53 169.19 (x1.35) - - 112.00 319.76 (x2.86) 133.16 (x1.19) 187.57 578.84 (x3.09) 231.45 (x1.23) 110.82 133.02 (x1.20) 198.16 244.85 (x1.24) 129.55 154.16 (x1.19) 93.42 270.75 (x2.90) 113.25 (x1.21) - - 188.01 636.49 (x3.39) 255.51 (x1.36) 94.00 109.94 (x1.17) 188.27 262.50 (x1.39) Table 6: Average inference time in seconds for various models and methods across downstream datasets. Numbers in parentheses indicate the relative inference time compared to each models Vanilla corresponding method. On average, HARP is x1.25, while beam search is x3.01, making it almost 2.5 times faster than beam search. This table complements Figure 2."
        },
        {
            "title": "C Prompts",
            "content": "This section presents the prompt for evaluating models on the different datasets. {. . . } refers to the data extracted from the dataset itself for each example. Multiple-choice Question Prompt General Instructions: Please read the multiple-choice question below carefully and select ONE of the listed options. Please write \"The answer is LETTER\". Question: {question} Options: {options} Figure 4: Multiple-choice Question prompt (CommonsenseQA and MMLU Pro). 13 GSM8K Prompt {question} Figure 5: GSM8K prompt."
        },
        {
            "title": "LAMBADA Prompt",
            "content": "You have to predict the next word of the sentence given the context. Your answer should be single word. Context: {context (minus one sentence)} Sentence to continue: {last sentence (minus one word)} Figure 6: LAMBADA prompt. CNN/DM Prompt Summarize the given article. Do not output something else than the summary. Article: {article} Figure 7: CNN/DailyMail prompt."
        }
    ],
    "affiliations": [
        "Computer Science and Engineering, Seoul National University"
    ]
}