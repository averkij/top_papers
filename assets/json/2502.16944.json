{
    "paper_title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance",
    "authors": [
        "Chenghua Huang",
        "Lu Wang",
        "Fangkai Yang",
        "Pu Zhao",
        "Zhixu Li",
        "Qingwei Lin",
        "Dongmei Zhang",
        "Saravan Rajmohan",
        "Qi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose \\textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \\emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\\% and training time by 35\\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 4 9 6 1 . 2 0 5 2 : r Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Chenghua Huang * Lu Wang Fangkai Yang Pu Zhao Zhixu Li Qingwei Lin Dongmei Zhang Saravan Rajmohan Qi Zhang School of Computer Science, Fudan University Microsoft huangch22@m.fudan.edu.cn, {wlu, fangkaiyang, puzhao, dongmeiz}@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining value model or reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose Decoupled Value Policy Optimization (DVPO), lean framework that replaces traditional reward modeling with pretrained global value model (GVM). The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40% and training time by 35% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching stateof-the-art PPO in performance. 1. Introduction Large Language Models (LLMs) have demonstrated stateof-the-art performance across range of tasks (Achiam et al., 2023; Bubeck et al., 2023), including dialogue generation (Ouyang et al., 2022), summarization (Stiennon et al., 1 2020), and code synthesis (Gao et al., 2023a). 1 To further align these models with human preferences, Reinforcement Learning from Human Feedback (RLHF) has become an essential technique (Christiano et al., 2017; Bai et al., 2022b; Song et al., 2024). RLHF leverages human-labeled feedback to guide model behavior, producing more accurate and user-friendly responses over time (Bai et al., 2022a; Ganguli et al., 2022). Among RLHF algorithms, Proximal Policy Optimization (PPO) is widely adopted due to its balance between stability and performance (Schulman et al., 2017; von Werra et al., 2020; Huang et al., 2024c). Despite its effectiveness, PPO-based RLHF comes with significant challenges. It requires joint training of an actor (policy) and critic (value function), while relying on fixed, pretrained reward model (Ziegler et al., 2019; Ouyang et al., 2022; Wang et al., 2024). This joint optimization introduces dense computational complexity and training instability, as the actor and critic must co-adapt over time (Yao et al., 2023; Huang et al., 2024b). Unlike in traditional reinforcement learning (RL), where continuous environment interaction provides updated rewards, RLHF in LLMs lacks access to ground-truth environment rewards during training. As result, the reward model cannot evolve to reflect changes in the policys behavior, further complicating value estimation and policy optimization (Moskovitz et al., 2023). To mitigate these issues, several works have proposed simplifying the RLHF process by reducing the number of trainable components. Direct Preference Optimization (DPO) bypasses training both reward and value models, optimizing policies directly on preference data (Rafailov et al., 2024; Azar et al., 2024; Zhao et al., 2023; Park et al., 2024). While efficient, DPO lacks the iterative refinement of true reinforcement learning and struggles with distribution shifts (Xu et al., 2024). Other reward-only approaches, such as ReMax (Li et al., 2023) and GRPO (Shao et al., 2024), use final rewards applied uniformly across all tokens but suffer 1Work is done during * an internship at Microsoft. corresponding author. Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance from high variance due to coarse token-level supervision, failing to optimize return-to-go dynamics and resulting in inefficient policy updates (Ahmadian et al., 2024; Gunter et al., 2024; Hu, 2025) loading four models (including the reference model) simultaneously during training. This significantly increases training complexity and computational resource consumption (Yao et al., 2023; Hu et al., 2024). In the context of RLHF without ground-truth environment rewards, we observe that pretraining reward model and pretraining value model become theoretically equivalent. Both models provide static supervisory signals about expected outcomes and cannot improve without additional reward feedback. This insight allows us to simplify the RLHF pipeline by eliminating the need for joint actor-critic training. Instead, we propose Decoupled Value Policy Optimization (DVPO), framework that eliminates joint actorcritic training while preserving fine-grained value guidance. It pretrains global value model (GVM) conditioned on policy trajectories. The GVM predicts token-level return-to-go values and serves as stable guide for policy optimization, reducing both training complexity and instability. Importantly, our method requires no external information beyond the standard RLHF data used in PPO. Once the GVM is trained, policy optimization proceeds via standard RL objective, such as PPO-like update, without requiring further updates to the value model. This design reduces both training complexity and instability while maintaining fine-grained reward supervision. We provide theoretical justification for this approach by proving that, under the constraint of no new reward feedback, pretraining reward model and global value model are functionally interchangeable. Our experiments across multiple benchmarks (MT-Bench (Zheng et al., 2023), Alpaca-Eval (Dubois et al., 2024), Arena-Hard (Li et al., 2024), etc.) demonstrate that the proposed method achieves performance comparable to state-of-the-art RLHF methods while reducing training time and GPU usage by 40% and 35%, respectively. These results highlight the scalability and stability of our approach in large-scale LLM fine-tuning. 2. Related Work Reinforcement Learning in Language Model Optimization. Reinforcement learning has emerged as prevalent method for fine-tuning large language models (LLMs), with Proximal Policy Optimization (Schulman et al., 2017) and its variations (Ramamurthy et al., 2022; Wu et al., 2023) being the most prevalent methods. These techniques largely adhere to the actor-critic paradigm (Sutton, 2018), This approach alternates between training value estimator for the current policy and leveraging it to enhance policy performance. This bilevel process may result in suboptimal policy, as demonstrated by empirical studies (Gao et al., 2023b). Moreover, the alternating optimization of the policy and critic models, along with the use of rewards provided by the reward model as environmental feedback, necessitates Training Efficiency. Many recent studies have sought to mitigate the computational complexity and resource consumption of the reinforcement learning (RL) step in RLHF. Methods such as DPO (Rafailov et al., 2024) and its variants (Meng et al., 2024; Ethayarajh et al., 2024; Hong et al., 2024) bypass reward modeling and the actor-critic learning framework by directly learning from preferences. However, existing research indicates that due to their offline nature, these approaches exhibit performance gap compared to online RL (Xu et al., 2024). Some recent works have proposed reward-only approach to reduce the training cost of the RL phase (Li et al., 2023; Gunter et al., 2024; Shao et al., 2024; Ahmadian et al., 2024). However, this method lacks value estimation and assigns the same reward score to each token, leading to high variance and instability during training (Hu, 2025). Unlike these approaches, our method pre-trains global value model (GVM) and leverages it to guide RL training, providing token-level supervision signals. This not only reduces training resource consumption but also stabilizes the training process, achieving performance comparable to the original PPO. Value-base Inference. Some recent works have attempted to learn value function and use it to guide the decoding phase of LLMs, thereby bypassing the RL optimization stage (Han et al., 2024; Kong et al., 2024; Mao et al., 2024). However, this approach significantly increases inference complexity and raises inference costs. In contrast, we leverage the learned value model to guide RL training, where the pre-trained value model helps the policy model converge more stably (Noukhovitch et al., 2024). 3. Method We propose DVPO for RLHF in which GVM is trained once and then fixed to guide policy updates. This approach removes the need for joint policyvalue training and mitigates the associated computational overhead and instability. As shown in Figure 1, our method comprises two primary stages: (1) Train GVM: Use offline trajectories (states, actions, returns, and policy-specific data) to learn policyconditioned action-value function Qϕ. (2) Decoupled Value Policy Optimization: Freeze Qϕ and optimize policy using standard RL objective (e.g., PPO), taking advantage estimates from the fixed GVM. In what follows, we first define our setting (3.1), then introduce the GVM and its training procedure (3.2), describe how we fix the GVM for stable policy optimization (3.3), and finally present theoretical analysis showing that pre2 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance To transform the sentence-level reward into token-level rewards r(st, at), we apply simplified form of temporal difference (TD) learning. Specifically, we assign reward of 0 to each intermediate step and use the final steps reward as the sentence-level reward value. By this definition, the cumulative return from step onwards is expressed as: Gt = (cid:80)T k=t γktr(sk, ak), which simplifies to Gt = γT tr(x, y) when all intermediate rewards are zero. In RLHF, we rely on offline data D, which contains state action trajectories, returns, and policy behaviors. The dataset is defined as: = (cid:8)(τi, si t, ai t, Gi t)(cid:9)N i=1, where τi represents the i-th policy behavior, typically expressed as sequence of questionanswer pairs, (si t) is stateaction pair sampled under that policy. Note that τi and (si t) may refer to different or identical questionanswer pairs under the same policy. No additional ground-truth rewards are collected during training. t, ai t, ai Figure 1. Overview of Decoupled Value Policy Optimization (DVPO) and PPO in RLHF. DVPO eliminates the need for reward model and decouples policy and value learning during policy optimization. In contrast, PPO requires training reward model before policy optimization. DVPO instead trains global value model using the same offline data as the reward model. During policy training, no additional ground-truth rewards are obtained. training reward model and pretraining global value model yield equivalent information when no new groundtruth rewards are available (3.4). 3.1. Problem Setting We model the sequence generation task in NLP as Markov Decision Process (MDP). The response consists of tokens, denoted by = y<T +1 := [y1, y2, . . . , yT ], where yt and is the vocabulary. We assume y<1 = [ ], indicating an empty prefix. Given prompt and the first 1 tokens y<t, the language model (LM) predicts the probability distribution for the next token as: πθ( [x, y<t]). In this MDP formulation, the state is defined as st = [x, y<t], representing the prompt (i.e., x) and the generated response up to step t. The action is the next generated token at = yt. The ground-truth reward at the sentence level, denoted by r(x, y), is provided by human feedback. 3 3.2. Training Global Value Model (GVM) Our key insight is to learn policy-conditioned action-value function Qϕ(τ, s, a) from offline data, where τ represents sampled trajectory capturing how the policy behaves in unrelated contexts. This conditioning approximates how well particular policyembodied by τ would perform when taking action in state s. Policy Conditioning via Trajectories. Traditional actor critic methods require online adaptation of the value function to the actors evolving behavior. In contrast, we aim for single, global Qϕ that generalizes across different policies, thus avoiding iterative re-learning. Instead of conditioning on explicit policy parameters, we leverage trajectories τ randomly sampled from the policy in question. Each τ is sequence of questionanswer pairs (in LLM tasks) or other interactions that reveal distinct policy traits (e.g., stylistic tendencies, correctness, domain expertise). Formally, we parametrize: Qϕ(τ, s, a) (cid:104) (cid:88) t= γt r(st, at) (cid:12) (cid:12) (cid:12) s0 = s, a0 = a, τ (cid:105) , where τ implicitly determines which policy π( s) we are approximating. In practice, we train the global value model Qϕ(τ, s, a) using Temporal Difference (TD) learning. The target return Gi is estimated based on future rewards and value predictions. Specifically, Gi is computed as: Gt = r(st, at) + γQϕ(τ, st+1, at+1), Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance where r(st, at) is the reward for taking action at in state st, and Qϕ(τ, st+1, at+1) is the predicted value of the next state-action pair. optimized. We adopt clipped PPO objective, which stabilizes policy updates by limiting the impact of large policy changes: The training objective for the global value model is to minimize the TD loss: LGVM(ϕ) = E(τt,st,at,rt,st+1,at+1)D (cid:2)(cid:0)rt+ γQϕ(τ, st+1, at+1) Qϕ(τ, st, at)(cid:1)2(cid:3), (1) This TD-based loss drives Qϕ to iteratively adjust its estimates of the expected return, ensuring that the value function accurately reflects both immediate and future rewards. This approach is particularly suitable for offline RLHF scenarios, as it can handle sequences with deterministic state transitions. Batch Normalization for Value Imbalance. Offline RLHF data may exhibit skewed returns: for instance, predominantly positive if most responses are high-quality. This can push Qϕ to extreme positive values and slow convergence. To stabilize training, we adopt batch normalization on the predicted values. For mini-batch of size B, µB = 1 B (cid:88) t=1 Qϕ(τ, st, at), σ2 = 1 (cid:88) i=1 (cid:0)Qϕ(τ, st, at) µB (cid:1)2 , (2) we then normalize each output: (cid:101)Qϕ(τ, st, at) = Qϕ(τ, st, at) µB σB + ϵ . This procedure (with small ϵ for numerical stability) prevents extreme values from dominating training and improves gradient flow. 3.3. Decoupled-Value Policy Optimization In traditional actorcritic methods, both the policy (actor) and value (critic) are trained simultaneously, which introduces instability due to their interdependence. This issue is exacerbated in offline RLHF settings, where no new environment rewards can be collected to correct misaligned updates. To address these challenges, we propose decoupled-value policy optimization approach: the global value model Qϕ is trained and fixed before policy optimization, decoupling the learning dynamics of the actor and critic. Policy Optimization Objective. Once the global value model Qϕ converges, we fix its parameters and use it to guide policy updates. Let πθ(a s) be the policy to be 4 (cid:104) LPPO(θ) = min(cid:0)rt(θ) ˆAt, clip(cid:0)rt(θ), 1ϵ, 1+ϵ(cid:1) ˆAt , (3) where rt(θ) = πθ(atst) πθold (atst) is the importance sampling ratio, and ϵ is clipping parameter to prevent overly large updates. (cid:1)(cid:105) The advantage function measures the relative quality of an action compared to the expected value of the state. We define the advantage using the fixed, normalized value model: ˆAt = (cid:101)Qϕ(τ, st, at), where (cid:101)Qϕ is the batch-normalized value estimate computed during the GVM training stage (see 3.2). In offline RLHF, this static advantage definition provides reliable signal for policy improvement without requiring dynamic value updates. Our decoupled-value approach offers two key advantages: (1) Stability via Fixed Value: By fixing Qϕ, we eliminate the moving target problem inherent in actorcritic methods, leading to more stable and predictable policy updates. (2) Efficient Offline Use: Since no additional environment rewards can be collected, the static Qϕ provides all necessary supervisory information, enabling efficient reuse of the offline dataset for policy optimization. 3.4. Theoretical Analysis: Equivalence of Pretrained Reward and Global Value Models In an offline RLHF scenario where no new ground-truth rewards are available, pretraining either reward model or GVM provides essentially the same supervisory information for policy optimization. Below, we present rigorous proof of this equivalence and discuss its implications. 3.4.1. PRELIMINARIES Assume we have two possible pretrained models from the same offline dataset. Reward Model, Rϕ(s, a), approximates an unknown ground-truth reward r(s, a). Global Value Model, Qψ(s, a), approximates the action-value function for any given policy π. Here, Qψ(s, a) is global value model conditioned on the trajectory τπ, state s, and action a. However, to simplify the proof, we remove the explicit dependence on τπ by merging it into the definition of the state. Because no new rewards are collected during policy training, neither model can be updated to reflect policy changes. Hence, whichever model is used, the agent has fixed signal to optimize against. We formalize this equivalence next. Theorem 3.1 (Equivalence of Pretrained Reward and GVM). Suppose: Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance 1. Rϕ(s, a) is pretrained reward model whose approximation error to the true reward r(s, a) is at most ϵR in the offline data distribution. 2. Qψ(s, a) is pretrained global value model whose approximation error to Qπ(s, a) is at most ϵQ in the same data distribution. 3. No further ground-truth rewards can be obtained to update either model. acts as static supervisor. If both models are sufficiently accurate within the relevant stateaction distribution, their induced policy gradients differ only by bounded approximation error. The above proof confirm that pretrained reward model Rϕ or pretrained global value model Qψ provides equivalent guidance for offline policy optimization, proving Theorem 3.1. Then any policy gradient method that employs either Rϕ or Qψ as its supervisory signal will yield policy updates differing by at most constant factor dependent on (ϵR, ϵQ). As ϵR, ϵQ 0, the two approaches become equivalent in guiding policy optimization. Proof. Deriving Value Function from Rϕ. Given Rϕ(s, a) and fixed policy π, one can solve the Bellman equation offline to obtain (cid:101)Qϕ(s, a; π): (cid:101)Qϕ(s, a; π) = Rϕ(s, a)+γEsP (cid:104) (cid:105) Eaπ(s)[ (cid:101)Qϕ(s, a; π)] . Under standard coverage assumptions in offline RL, (cid:101)Qϕ converges to an approximation of Qπ, up to an error that depends on ϵR and the data distribution. Deriving Reward Signal from Qψ. Conversely, if Qψ(s, a) approximates Qπ(s, a) for policy π, the (unknown) reward can be approximated by rearranging the Bellman equation: r(s, a) = Qπ(s, a) γEsP,aπ (cid:2)Qπ(s, a)(cid:3). Replacing Qπ with Qψ yields (cid:101)Rψ(s, a) = Qψ(s, a) γEsP,aπ (cid:2)Qψ(s, a)(cid:3). Hence, Qψ can serve as surrogate reward model for policy updates. Equivalence in Policy Gradient. Let θJR be the policy gradient update using Rϕ (converted to value function (cid:101)Qϕ), and θJQ the update using Qψ directly. By standard policy gradient theorems, θJR θJQ κ(ϵR, ϵQ), for some bounded function κ, given that both methods rely on fixed models and the policy remains within the offline datasets support. As ϵR, ϵQ 0, the updates converge to the same directions in parameter space, leading to the same optimal policy. No New Ground-Truth Rewards. Since no fresh environment rewards r(s, a) can be obtained, neither Rϕ nor Qψ can be refined once training begins. Thus, each model 4. Experiment Setup In this section, we conduct extensive experiments to validate the superiority of DVPO. Specifically, based on real-world scenarios of RLHF implementation, we perform preference optimization under two settings: Base and Instruction. 4.1. Training settings For the Base setting, we follow the training pipeline of (Huang et al., 2024a), perform preference optimization according to the following process. First, we select base model for instruction fine-tuning (LLaMA3 8B2 and LLaMA3 3B3(Dubey et al., 2024)) to obtain the SFT model. Then, we initialize the preference optimization process from the SFT model and conduct training on the Ultrafeedback(Cui et al., 2023) dataset. This setting encompasses the standard RLHF process and is highly transparent, as the SFT model is trained on open-source data and has not undergone preference optimization. For the Instruction setting, We select an off-the-shelf instruction-tuned model as the SFT model. To facilitate comparison with related work(Li et al., 2023; Rafailov et al., 2024), we choose the mistral-instruction-v0.24. These models have undergone extensive instruction fine-tuning processes based on the base model, resulting in stronger capabilities compared to the SFT model in the Base setting. However, the instruction fine-tuning process is not transparent. We perform preference optimization on the instruction models. 4.2. Evaluation We evaluate the response quality of the models using the most popular instruction-following benchmarks: MTBench(Zheng et al., 2023), Arena-Hard(Li et al., 2024), and Alpaca-Eval(Dubois et al., 2024). These benchmarks consist of series of open-ended questions and employ LLM as judge approach to score or compare the responses gener2https://huggingface.co/meta-llama/ Meta-Llama-3-8B 3https://huggingface.co/meta-llama/ Llama-3.2-3B 4https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.2 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance ated by the models. Additionally, we compare the win rate on the held-out test set. MT-Bench comprises 80 multi-turn questions. The evaluation is conducted using GPT-4 as an automated judge, which scores the models responses on scale from 1 to 10 based on criteria such as relevance, coherence, and informativeness. Arena-Hard features dataset of 500 prompts curated from Chatbot Arenas live data. The evaluation employs an LLM-as-a-judge framework, where models responses are compared, and win rates are calculated to determine performance. AlpacaEval consists of 805 paired instructions. Evaluations are performed using an LLM-based automatic evaluator, such as GPT-4, which compares the responses of different models to the same prompt and determines winner. To mitigate biases like verbosity, AlpacaEval employs length-controlled win rates. 4.3. Baseline For the Base setting, as previously described, we carried out experiments on the Ultrafeedback dataset. Following the experimental setup of (Huang et al., 2024a), the dataset is divided into three parts: 30% for SFT, 50% for reward learning, and 20% for RL optimization. The value model is trained on the reward learning data and then used to perform preference optimization on the SFT model using the 20% RL optimization data. For the standard PPO process, the learned reward model is used similarly to optimize the SFT model on the 20% RL optimization data. In addition, we compare the results with the original SFT model. For the Instruction setting, we select Mistral-instructionv0.2 as the starting point and perform RL preference optimization on the Ultrafeedback dataset. For this purpose, we utilize the existing reward model, UltraRM5, aligning with the experimental setup of ReMax(Li et al., 2023). We compare our model with other models of similar parameter sizes, including LLaMA3-8B-Instruction6, DPO, ReMax7, etc. 5. Experiment results In this section, we present the main experimental results for Base and Instruction settings, as well as the computational efficiency advantages of DVPO compared to standard RLHF methods. For details on the performance of the GVM, please refer to the Appendix C. 5https://huggingface.co/openbmb/ UltraRM-13b 6https://huggingface.co/meta-llama/ Meta-Llama-3-8B-Instruct 7https://huggingface.co/ziniuli/ Mistral-7B-ReMax-v0.1 5.1. DVPO on Base setting As mentioned earlier, to clearly evaluate the feasibility of DVPO compared to existing methods, we conducted experiments on publicly available datasets. The datasets were split proportionally for supervised fine-tuning (SFT) and reward learning (value learning). Subsequently, the resulting value model (reward model) was used to guide the policy optimization for preference learning. Given the relatively small amount of data at each stage (with the RL phase accounting for approximately 20% of the entire dataset), our focus in the Base setting is on relative improvement rather than absolute performance metrics. The results are presented in Table 1. DVPO demonstrates superior optimization performance. In experiments conducted on LLaMA3-3B and LLaMA38B, DVPO consistently outperforms SFT significantly. Specifically, under the Base setting of LLaMA3-8B, DVPO achieves improvements of 0.2 on MT-Bench, 1.5 on ArenaHard, and 1.74 on Alpaca-Eval compared to the SFT model. The improvements are even more pronounced with LLaMA3-3B, where DVPO achieves 0.51 increase on MTBench, 5-point increase on Arena-Hard, and 4.14-point increase on Alpaca-Eval. Similarly, as shown in Figure 2, on the held-out test set, DVPO achieves higher win rate compared to the SFT model. These results validate the robustness and effectiveness of DVPO. The pre-trained value model can provide sufficiently high performance ceiling. According to the experimental results in the Base setting, the final convergence performance of DVPO is very close to that of standard PPO (and in some cases, slightly exceeds it), indicating that the supervision signals provided by the pre-trained value model can support sufficiently high performance ceiling. As an actor-critic method, PPO trains the policy model and value model simultaneously, with the reward model providing environmental feedback. The value model, as bridge between immediate rewards (sentence-level) and long-term returns, offers finer-grained supervision signals (token-level) compared to immediate rewards alone. In DVPO, the global value model(GVM) is pre-trained on responses from different policies, enabling it to provide supervision feedback to various policies during the RL phase. The experimental results demonstrate that the fine-grained environmental feedback provided by DVPO can achieve performance ceiling comparable to that of the actor-critic framework while significantly reducing training complexity and instability. 5.2. DVPO on Instruction setting To evaluate the effectiveness of DVPO in most RLHF scenarios (optimization starting from pre-aligned model), we selected Mistral-Instruction-v0.2 as the SFT model. An existing UltraRM reward model was used to collect labeled 6 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Table 1. Performance under the Base setting. The suffix 3B indicates that llama3.2-3B-Base was used as the base model, while 8B indicates that llama3-8B-Base was used as the base model. Both PPO and DVPO were initialized from SFT and optimized. Compared to PPO, DVPO achieved the best benchmark scores across models with different parameter sizes. MODEL MTBENCH ARENA HARD ALPACAEVAL2 SFT-8B PPO-8B DVPO-8B SFT-3B PPO-3B DVPO-3B 4.87 4.98 5.01 5.22 5.33 5.73 10.3 11.7 11. 10.4 13.5 15.1 9.79 11.14 11.33 8.19 11.54 12.33 Figure 2. Results of the model on the Ultrafeedback held-out testset. We employed GPT4o as judge to assess the quality of model-generated responses. Performance is measured using the win rate, where Left represents DVPO, and Right represents the baseline model for comparison. data for training the value model, value model also initialized from Mistral-Instruction-v0.2. The trained value model then guided the preference optimization of the SFT model. The results are presented in Table 2. Experimental findings demonstrate that our method exhibits significant advantages over other approaches. Our main findings are as follows: DVPO significantly enhances the performance of instruction models. Compared to the original Mistral-Instructionv0.2, DVPO consistently demonstrates performance improvements across all benchmarks. Specifically, it achieves 0.19 improvement on MT-Bench and increases the win rate relative to GPT-4 by 12.1% on Arena-Hard. Additionally, it enhances the length-controlled win rate by 10.32% on Alpaca-Eval. Furthermore, compared to larger models such as LLaMA3-8B-Instruction, DVPO also exhibits notable advantages, outperforming 4.1% in Arena-Hard and 4.51% in Alpaca-Eval. These results indicate that the pretrained value model can effectively and reliably guide the 7 Table 2. Performance under the Instruction setting. Mistral 7B refers to Mistral-7B-instruction-v0.2, and Llama 8B refers to Llama3-8B-instruction. DVPO outperformed other preference alignment methods. MODEL MTBENCH ARENA HARD ALPACAEVAL2 MISTRAL 7B REMAX DPO DVPO LLAMA3 8B GPT 6.60 6.67 6.30 6.79 6.90 7.93 12.6 21.9 16.3 24.7 20.6 35.0 17.11 20.55 26.80 27.43 22.92 30. Table 3. Computational results under the Base setting. For the 3B model, we conducted experiments using Zero-1. For the 8B model, due to PPO encountering out-of-memory (OOM) issues with the Zero-1 training strategy, we employed Zero-3 instead. Details can be found in the Appendix. We report the GPU memory consumption per GPU (GB), the time required per step (seconds), and the number of steps needed to reach convergence under an 8A100 training environment. MODEL MEMORY/PER GPU TIME/S TRAINING STEP PPO-8B DVPO-8B PPO-3B DVPO-3B 78.96G 60.49G 41.43G 27.50G 70 48 23 13 1250 810 600 optimization of policy models. Finer-grained feedback signals lead to superior performance. Compared to reward-only methods such as ReMax, DVPO demonstrates significant performance advantages on Mt-bench, Arena hard and Alpaca Eval. In ReMax, the feedback signal is at the sentence level, assigning single reward score to the entire sentence and treating all tokens as equally contributing to the overall score. This design prevents the policy model from learning fine-grained preferences during the RL phase, resulting in suboptimal performance. In contrast, DVPO assigns return value to each token, which facilitates more effective training of the policy model. Furthermore, DVPO outperforms direct preference optimize(DPO) approaches. In addition to providing finegrained returns, DVPO retains the on-policy characteristics of the original PPO, enabling larger exploration space and offering higher performance ceiling. 5.3. DVPO Computation Efficiency DVPO improves upon RLHF by leveraging pre-trained value model to provide environmental feedback, resulting Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Figure 3. Learning curve of the policy model during the RL stage under the Base setting. DVPO demonstrates faster and more stable convergence compared to other methods. in enhanced computational efficiency, stability, and convergence speed. Memory Efficiency. As shown in Table 3, the experimental setup for standard PPO and DVPO was kept identical. Under standard training conditions, DVPO achieves approximately 50% reduction in memory usage. For instance, with LLaMA-3B, we trained using Zero-1 on 8 A100 GPUs with batch size of 4. Standard PPO requires 41.43 GB of memory due to the need to simultaneously load 4 models (policy model, critic model, reference model, and reward model), with two models (policy model and critic model) requiring activation. In contrast, DVPO only requires 27.5 GB of memory, as it only loads the policy model, global value model, and reference model, with only the policy model being activated during training. For LLaMA-8B, we used DeepSpeed Zero-3 with batch size of 4. Even under this configuration, DVPO effectively reduces memory usage, enabling support for larger batch sizes during training. Time Efficiency. DVPO accelerates the training process, requiring only half the time per batch compared to standard PPO under the same batch size. This improvement stems from the fact that, during the generation phase, DVPO requires outputs from only three models, whereas standard PPO relies on four. Additionally, during the backpropagation phase, PPO updates both the policy and value models, while DVPO updates only the policy model. Overall, under identical settings (batch size), DVPO achieves 1.76x speedup in training (1.76 23/13). Training Step. We observe that using pre-trained value model to provide token-level fine-grained environmental feedback enables faster model convergence, which aligns with the findings in (Noukhovitch et al., 2024). Specifically, under identical experimental settings (same batch size, experience replay count, etc.), DVPO requires only approximately 3/4 of the interaction steps to converge. As shown in Figure 3, in the LLaMA3-8B experiment, the reward curve of PPO reaches its peak at 1250 steps, whereas DVPO achieves optimal performance around 810 steps. Similarly, in LLaMA3-3B, PPO reaches its peak at 600 steps, while DVPO converges in 450 steps. Since DVPO provides tokenlevel supervision feedback and is pre-trained in advance, it can be considered form of warm-starting, thereby accelerating policy convergence. Policy Training Curve. As shown in Figure 3. In the Llama3 3B setting, DVPO achieves faster convergence and exhibits more stable performance compared to PPO, with smoother increase in reward scores and consistent improvements in value scores throughout training. In the Llama3 8B setting, DVPO also exhibits more stable training curve, indicating better training stability and efficiency relative to PPO. These results substantiate the advantages of DVPO, particularly its capability to achieve faster and more stable convergence across models with varying parameter scales. Furthermore, DVPO demonstrates the ability to converge to stable state more rapidly. In the Llama3 3B experiments, DVPO achieved convergence at approximately 450 steps, whereas PPO required around 600 steps to converge. Similarly, in the Llama3 8B experiments, DVPO converged at around 800 steps, while PPO completed convergence only after 1200 steps. Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance 6. Conclusion We propose Decoupled Value Policy Optimization (DVPO), framework that eliminates joint actor-critic training in RLHF by leveraging pretrained global value model (GVM). Unlike traditional PPO-based RLHF, which suffers from training instability and computational overhead, DVPO stabilizes policy optimization through static, tokenlevel return-to-go estimates. Our theoretical analysis proves the functional equivalence of reward and value models under the constraint of no new reward feedback, justifying the use of fixed GVM for efficient and scalable optimization. Empirical results demonstrate that DVPO achieves comparable performance to state-of-the-art RLHF methods on multiple benchmarks while reducing training time and GPU usage by over 35%. Future work will focus on refining the value models training process to enhance prediction accuracy."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ahmadian, A., Cremer, C., Galle, M., Fadaee, M., Kreutzer, Ustun, A., and Hooker, S. Back J., Pietquin, O., to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback. 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 1076410799. PMLR, 2023a. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023b. Gunter, T., Wang, Z., Wang, C., Pang, R., Narayanan, A., Zhang, A., Zhang, B., Chen, C., Chiu, C.-C., Qiu, D., et al. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. Han, S., Shenfeld, I., Srivastava, A., Kim, Y., and Agrawal, P. Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639, 2024. Hong, J., Lee, N., and Thorne, J. Orpo: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1117011189, 2024. 9 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Hu, J. Reinforce++: simple and efficient approach arXiv preprint for aligning large language models. arXiv:2501.03262, 2025. et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Hu, J., Wu, X., Wang, W., Zhang, D., Cao, Y., et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Park, R., Rafailov, R., Ermon, S., and Finn, C. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024. Huang, C., Fan, Z., Wang, L., Yang, F., Zhao, P., Lin, Z., Lin, Q., Zhang, D., Rajmohan, S., and Zhang, Q. Self-evolved reward learning for llms. arXiv preprint arXiv:2411.00418, 2024a. Huang, S., Liu, T., and Von Werra, L. The implementation details of rlhf with ppo. In The Third Blogpost Track at ICLR 2024, 2024b. Huang, S., Noukhovitch, M., Hosseini, A., Rasul, K., Wang, W., and Tunstall, L. The n+ implementation details of rlhf with ppo: case study on tl; dr summarization. arXiv preprint arXiv:2403.17031, 2024c. Kong, L., Wang, H., Mu, W., Du, Y., Zhuang, Y., Zhou, Y., Song, Y., Zhang, R., Wang, K., and Zhang, C. Aligning large language models with representation editing: control perspective. arXiv preprint arXiv:2406.05954, 2024. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., and Stoica, I. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Li, Z., Xu, T., Zhang, Y., Yu, Y., Sun, R., and Luo, Z.- Q. Remax: simple, effective, and efficient method arXiv preprint for aligning large language models. arXiv:2310.10505, 2023. Mao, X., Li, F.-L., Xu, H., Zhang, W., and Luu, A. T. Dont forget your reward values: Language model alignment via value-based calibration. arXiv preprint arXiv:2402.16030, 2024. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. Moskovitz, T., Singh, A. K., Strouse, D., Sandholm, T., Salakhutdinov, R., Dragan, A. D., and McAleer, S. Confronting reward model overoptimization with constrained rlhf. arXiv preprint arXiv:2310.04373, 2023. Noukhovitch, M., Lavoie, S., Strub, F., and Courville, A. C. Language model alignment with elastic reset. Advances in Neural Information Processing Systems, 36, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi, Y. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Song, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang, H. Preference ranking optimization for human alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1899018998, 2024. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 30083021, 2020. Sutton, R. S. Reinforcement learning: An introduction. Bradford Book, 2018. von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouedec, Q. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Wang, B., Zheng, R., Chen, L., Liu, Y., Dou, S., Huang, C., Shen, W., Jin, S., Zhou, E., Shi, C., et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024. Wu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., and Jiao, J. Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment. arXiv preprint arXiv:2310.00212, 2023. Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Xu, S., Fu, W., Gao, J., Ye, W., Liu, W., Mei, Z., Wang, Is dpo superior to ppo for G., Yu, C., and Wu, Y. llm alignment? comprehensive study. arXiv preprint arXiv:2404.10719, 2024. Yao, Z., Aminabadi, R. Y., Ruwase, O., Rajbhandari, S., Wu, X., Awan, A. A., Rasley, J., Zhang, M., Li, C., Holmes, C., et al. Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint arXiv:2308.01320, 2023. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 11 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance A. Training Details SFT training. We use the following hyperparameters for instruction fine-tuning training. We employ learning rate of 2e-5 with cosine decay, 2 warmup steps, and batch size of 16. We calculate the loss only for the target tokens rather than the full input sequence, and we train for 3 epochs on the training data. we conduct the training on 8 NVIDIA A100 80G GPUs. Reward training. To enable the model to learn the relative ranking among different responses, we use pair-wise loss. We utilize the LoRA method to train the RM on the SFT baseline, with rank of 8, LoRA alpha of 32, and LoRA dropout of 0.1. The task type is sequence classification. We use learning rate of 2e-5 with linear decay and the AdamW optimizer for training over 2 epochs, with batch size of 4. We conduct the training on 8 NVIDIA A100 80G GPUs . PPO training and GVPO training. For PPO training, we use learning rate of 2e-6 and set the generate sample length to 1024. We employ batch size of 8 and mini-batch size of 2, with 4 PPO epochs and 2 gradient accumulation step. The target KL divergence is set to 0.1 and initial KL coefficient is set to 0.2. For fair comparison, GVPO training was conducted using the same hyperparameter settings (e.g., batch size, sequence length, etc.). Global value model training. We initialize the value model from the SFT model. The training is conducted with batch size of 32, sequence length of 1024, and learning rate of 2e-6. We employ the LoRA method to train the value model on the SFT baseline, using rank of 8, LoRA alpha of 32, and LoRA dropout of 0.1. B. GVM case study Figure 4. An example of the supervisory signal provided by Global Value Model (GVM). The GVM is capable of providing token-level feedback. In this example, the GVM assigns lower value to the incorrect response (response2: is an island) and higher value to the critical token not in the correct response (response1: not an island). For the same question, As an island, is Beijing located in Asia?, the value model provides fine-grained supervisory signals for two different responses. The GVM assigns specific values to each token in the responses. These values represent the models assessment of the importance or correctness of each token in the given context. As shown in Figure 4. For Response 1, the critical token not is given higher value (0.2099), highlighting its significance in forming the correct response, not an island. For Response 2, the GVM assigns lower values to incorrect tokens, such as is (-0.6177) and an (-0.4766), indicating their contribution to the incorrect response, is an island. This token-level evaluation demonstrates the GVMs ability to guide learning by penalizing incorrect responses and reinforcing critical tokens in correct responses, thereby enhancing training accuracy and interpretability. C. GVM performance We evaluate the performance of the GVM from multiple perspectives. Specifically, we observe that the GVM assigns higher value scores to good actions and lower value scores to bad actions, as illustrated in Figure 4. We evaluate the global value model(GVM) on held-out test set. For each question, we expect the value model to assign higher values to good responses compared to bad ones. We calculate the accuracy under various metrics (mean, P1, etc.) to assess the models performance. 12 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Table 4 presents the performance metrics of the Global Value Model (GVM) initialized from Mistral-7B-instruction-v0.2. The metrics include the mean value, which indicates the average score assigned by the GVM, and various percentiles (P1, P5, P10, P90, P95, and P99), representing the corresponding value distributions. The accuracy metric evaluates the proportion of cases where the GVM correctly assigns higher value to good response compared to bad response, thereby reflecting the effectiveness of the GVM in providing accurate feedback. As shown in the table, the mean accuracy is 64.51%, with percentile values ranging from 56.02% (P1) to 61.47% (P90), indicating consistent performance across different value ranges. These results demonstrate the GVMs ability to provide reliable token-level evaluations. Table 4. The performance of the GVM initialized from Mistral-7B-instruction-v0.2. Here, mean represents the average value of the predictions, P1 indicates the 1st percentile value, and accuracy reflects the proportion of cases where the value for good response exceeds the value for bad response, which is considered as correct feedback from the global value model. METRIC"
        },
        {
            "title": "ACCURACY",
            "content": "MEAN VALUE P1 VALUE P5 VALUE P10 VALUE P90 VALUE P95 VALUE P99 VALUE 64.51 56.02 58.46 59.83 61.47 60.30 59.94 D. GPT4 evaluation Prompt crucial element of our experimental framework is the evaluation of win rates using GPT-4. In this section, we provide the prompts utilized to generate win rates for both the summarization and dialogue experiments. All experiments were conducted using the gpt-4o-20240806 model. The sequence of responses was randomized for each evaluation to ensure unbiased results. GPT-4 as judge system prompt: Review the users question and the corresponding response using the additive 5-pointscoring system described below. Points are accumulated based on the satisfaction of each criterion: - Add 1 point if the response is relevant and provides some information related to the users inquiry, even if it is incomplete or contains some irrelevant content. - Add another point if the response addresses substantial portion of the users question, but does not completely resolve the query or provide direct answer. - Award third point if the response answers the basic elements of the users question in useful way, regardless of whether it seems to have been written by an Al Assistant or if it has elements typically found in blogs or search results. - Grant fourth point if the response is clearly written from an Al Assistants perspective, addressing the users question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus. - Bestow fifth point for response that is impeccably tailored to the users question by an AI Assistant, without extraneous information, refecting expert knowledge, and demonstrating high-quality, engaging, and insightful answer. After examining the users instruction and the response, please first give the total score. Then provide brief explanation of your total score, up to 100 words. Output with the following format: Score: {total score} Evaluation evidence: {your brief explanation here} Remember to assess from the Al Assistant perspective, utilizing web search knowledge as necessary. To evaluate the response in alignment with this additive scoring model, well systematically attribute points based on the outlined criteria. 13 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance User input prompt: <Question>: <response>: {question} {answer} Input Compare and contrast Harry Potter with Percy Jackson series. Table 5. case study for GPT4 evaluation: LLama3 8B SFT Llama3 8B SFT GPT4-as-Judge Sure, Id be happy to help you compare and contrast the Harry Potter and Percy Jackson series!Both Harry Potter and Percy Jackson are popular fantasy series that have captured the hearts of readers around the world. Here are some key similarities and differences between the two:Similarities:1. Magical elements: Both series feature magical aspect, with Hogwarts School of Witchcraft and Wizardry being the setting for Harry Potter and Camp Half-Blood for Percy Jackson.2. Mythological references: Both series draw heavily on Greek mythology, with Percy Jacksons stories being based on the Labors of Hercules and other Greek myths.3. Relatable protagonists: Both Harry and Percy are relatable and charming protagonists that readers can connect with.Differences:1. Setting: While both series take place in magical worlds, the settings are quite different. Harry Potters stories take place in more traditional school setting, while Percy Jacksons stories take place in summer camp atmosphere.2. Tone: Harry Potter is more lighthearted and comedic series, while Percy Jackson takes darker and more serious tone, exploring themes of mortality and responsibility.3. Plot structure: Harry Potter has more linear plot structure, with each book building on the previous ones events. Percy Jackson, on the other hand, has more episodic structure, with each book focusing on different adventure or challenge for the characters.4. Themes: While both series explore themes of good and evil, Harry Potter focuses more on the importance of friendship and loyalty, while Percy Jackson delves into the consequences of power and responsibility.In conclusion, while both Harry Potter and Percy Jackson share many similarities, their differences in setting, tone, plot structure, and themes offer unique reading experiences for readers. Both series are highly recommended for anyone looking for magical and engaging read! Score: 3 Evaluation evidence: The response provides relevant comparison of the Harry Potter and Percy Jackson series, highlighting key similarities and differences in magical elements, settings, tone, plot structure, and themes. It addresses the users question by outlining basic elements and offering useful comparison. However, the answer contains inaccuracies, such as misrepresenting the tone of both series and the nature of mythological references in Percy Jackson. Additionally, the response could be more precise and direct, as some points are not entirely accurate or comprehensive. Thus, it meets the criteria for basic, useful response but lacks thoroughness and precision for higher score. 14 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Input Compare and contrast Harry Potter with Percy Jackson series. Table 6. case study for GPT4 evaluation: LLama3 8B PPO Llama3 8B PPO GPT4-as-Judge Sure, Id be happy to help you compare and contrast the Harry Potter and Percy Jackson series!Both Harry Potter and Percy Jackson are popular fantasy series that have captured the hearts of readers around the world. Here are some key similarities and differences between the two:Similarities:1. Magical elements: Both series feature magical aspect, with Hogwarts School of Witchcraft and Wizardry being the setting for Harry Potter and Camp Half-Blood being the setting for Percy Jackson.2. Teenage protagonists: The main characters in both series are teenagers, navigating their way through adolescence while also facing epic challenges and learning valuable life lessons.3. Epic adventures: Both series involve the protagonists on thrilling quests, battling mythical creatures, and uncovering secrets that could change the course of history.4. Portal to another world: In both series, there is gateway to another world (Harrys cupboard under the stairs and the portal at Camp Half-Blood) that allows the protagonists access to realm filled with magical creatures and unparalleled adventure.Differences:1. Setting: While both series take place in magical worlds, the settings are quite different. Harry Potter is set in medieval-style world with castles and magical creatures, while Percy Jackson is set in modern-day world with technology and urban landscapes.2. Tone: The tone of the two series is quite different. Harry Potter is lighthearted and comedic, while Percy Jackson is darker and more serious, with themes of mortality and morality.3. Mythological references: While both series draw on mythological references, the specific myths and legends used are different. Harry Potter relies more on European and British myths, while Percy Jackson draws on Greek and Roman mythology.4. Magic systems: The magic systems in the two series are also different. Harry Potter uses spell-based magic system, while Percy Jackson uses more physical, athletic approach to magic.5. Protagonists: The protagonists in the two series are quite different. Harry Potter is boy wizard who discovers his true identity and heritage, while Percy Jackson is demigod who must navigate the complexities of his divine heritage and mortal life.6. Plots: The plots of the two series are distinct. Harry Potter focuses on the battle between good and evil, with Harry as the prophesized hero who must face off against Voldemort. Percy Jackson, on the other hand, is about the struggles of the demigods and the impact of their mortal lives on their divine heritage.In conclusion, both Harry Potter and Percy Jackson are captivating fantasy series that share some similarities, but also differ in many ways. While both offer engaging and entertaining reads, they appeal to different audiences and perspectives. Score: 3 Evaluation evidence: The response provides relevant comparison of the Harry Potter and Percy Jackson series, addressing the users question with mix of similarities and differences. It covers key aspects such as magical elements, protagonists, and settings, offering basic understanding of both series. However, the response contains inaccuracies, particularly in tone descriptions and settings, which detracts from its comprehensiveness and accuracy. While it offers useful comparison, it does not fully address the users question with the depth or precision expected from an AI Assistant, nor does it reflect expert knowledge or exceptional insight. 15 Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Input Compare and contrast Harry Potter with Percy Jackson series. Table 7. case study for GPT4 evaluation: LLama3 8B DVPO Llama3 8B DVPO GPT4-as-Judge Sure, Id be happy to help! Both the Harry Potter and Percy Jackson series are popular and successful book series that have captivated readers worldwide. Here are some key similarities and differences between the two:Similarities:1. Magical elements: Both series feature magical aspects, such as spells, wizards, and fantastical creatures.2. Young protagonists: The main characters in both series are young and face challenges and adventures that come with growing up in magical world.3. Family themes: Both series emphasize the importance of family and the bonds between family members.4. Friendship: The series also highlight the importance of friendship and the power of having loyal and supportive friends.Differences:1. Genre: While both series can be classified as fantasy, Harry Potter is more traditionally fantasy-oriented, while Percy Jackson is urban fantasy with contemporary setting.2. Setting: Harry Potter is set in parallel universe where wizardry and magic are prevalent, while Percy Jackson is set in present-day America and Greece.3. Magic system: The magic systems in each series are different. In Harry Potter, magic is cast using wands and spells, while in Percy Jackson, magic is accessed through the use of magical coins and enchanted items.4. Gods and myths: Harry Potter is based on the idea of witches and wizards living among us, while Percy Jackson is based on Greek mythology and features the gods and goddesses of ancient Greece.5. Tone: Harry Potter is written in more traditional, epic style, while Percy Jackson is written in more contemporary, humorous tone.Ultimately, both series offer engaging, imaginative storytelling that captures the hearts and minds of readers. While they both explore similar themes, their unique settings, magic systems, and tones make them distinct from one another. hope this helps! Let me know if you have any other questions. Score: 5 Evaluation evidence: The response thoroughly addresses the users question by comparing and contrasting the Harry Potter and Percy Jackson series. It identifies key similarities, such as magical elements, young protagonists, and themes of family and friendship. The response also effectively highlights differences in genre, setting, magic systems, mythological influences, and tone. The information is well-organized, relevant, and engaging, demonstrating comprehensive understanding of both series. The response is tailored specifically to the users inquiry, providing insightful and expert knowledge without any extraneous information."
        }
    ],
    "affiliations": [
        "Microsoft",
        "School of Computer Science, Fudan University"
    ]
}