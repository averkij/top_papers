{
    "paper_title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
    "authors": [
        "Nishad Singhi",
        "Hritik Bansal",
        "Arian Hosseini",
        "Aditya Grover",
        "Kai-Wei Chang",
        "Marcus Rohrbach",
        "Anna Rohrbach"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling."
        },
        {
            "title": "Start",
            "content": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Nishad Singhi*1, Hritik Bansal*2, Arian Hosseini*3,4, Aditya Grover2, Kai-Wei Chang2, Marcus Rohrbach1 and Anna Rohrbach1 1TU Darmstadt & hessian.AI, 2University of California Los Angeles, 3Google DeepMind, 4Mila, *Equal Contribution Abstract: Scaling test-time compute has emerged as key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. traditional approach, Self-Consistency (SC), generates multiple solutions to problem and selects the most common answer via majority voting. Another common method involves scoring each solution with reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as next-token prediction task, enabling inference-time scaling along new axis. Specifically, GenRM generates multiple verification chains-ofthought to score each solution. Under limited inference budget, this introduces fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8 the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling. 5 2 0 2 1 ] . [ 1 5 0 0 1 0 . 4 0 5 2 : r Figure 1: Left: The prominent approach is to compare GenRM and Self-Consistency (SC) at fixed number of solutions, suggesting that GenRM is more efficient as it matches SC with fewer solutions. Right: When evaluated under fixed compute budget, including verification costs, SC outperforms GenRM at lower budgets, using up to 8 less compute, while GenRM excels at higher budgets. Each curve corresponds to fixed number of verifications; the number of solutions is doubled at each point along the x-axis. The solutions are generated by Llama-3.1-8BInstruct [14], which also performs verifications after being fine-tuned as GenRM, on the MATH dataset [18]. When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Figure 2: Compute-Matched Analysis. Given fixed inference budget ùêµ, our analysis (b) compares the performance of (a) scaling the number of solutions (ùëÜ = ùêµ) with Self-Consistency vs. (c) generating fewer solutions (ùëÜ = ùêµ/ùëâ ) while relying on verifications (ùëâ ) using Generative Reward Models. 1. Introduction Large Language Models (LLMs) have shown substantial improvements in their reasoning capabilities with increased test-time compute [13, 17, 35] across diverse domains such as math and science. One of the most straightforward and effective ways to scale test-time compute is by generating multiple solution chains-of-thought (CoTs) for given problem and performing majority voting over them, technique known as Self-Consistency (SC) [38]. Alternatively, reward model (aka verifier) can be used to score the solutions and identify the best one. This strategy, commonly referred to as Best-of-N (BoN), has been widely employed to enhance LLM reasoning capabilities [8]. robust reward model can detect errors and discard incorrect solutions, even when they are overrepresented among the generated solutions, making this approach particularly effective. Recent studies [1, 29, 43] have framed verification as next-token prediction task, leading to new class of reward models known as Generative Reward Models (GenRM). These models enable test-time scaling along new axis: generating multiple verification CoTs and aggregating their verdicts to score given solution. Prominently, prior work compares the performance of GenRM (BoN) and SC at fixed number of solutions. For instance, Llama GenRM (BoN) with 32 verification CoTs surpasses SC across different solution counts (Figure 1(a)). This comparison suggests that GenRM is more efficient, achieving the same performance as SC with 4 fewer solutions. However, this conclusion is misleading in practical scenarios where inference compute budgets (FLOPs) are limited, as the current comparisons overlook the significant computational cost of generating many verifications for several candidate solutions of given problem. This gives rise to the question: At fixed inference compute budget, is it more effective to perform SC over many solutions, or use GenRM to select the best solution from smaller set of solutions by scaling the number of verifications? To address this, we present framework to estimate the inference budget for Self-Consistency and GenRMs. Subsequently, we conduct compute-matched analysis, comparing the effectiveness of these test-time scaling strategies under fixed computational budget (3.1). Specifically, we assume an LLM is used for both problemsolving (solution generator) and generative verification (reward model). The generative verification capability can be leveraged through either prompting or task-specific fine-tuning. Consequently, the inference compute comparison between SC and GenRMs is based on the total number of solutions and verifications generated by the LLM, as illustrated in Figure 2. In our experiments, we compare the performance of scaling solutions and verifications with GenRM against simply scaling solutions via SC across various inference compute budgets. Our results indicate that SC outperforms GenRM at lower compute budgets, while GenRM performs better at higher budgets (Figure 1(b)). In particular, GenRM first surpasses SC after 8 compute and requires an additional 128 inference compute to achieve 3.8% performance gain over SC. While prior work realized GenRM in limited settings, we demonstrate the robustness of our findings across various model families (e.g., Llama [14] and Qwen [40]), model sizes (e.g., 7B and 70B), thinking models (e.g., QwQ-32B [32]), and reasoning tasks (e.g., math) (5.1). As the compute budget is scaled and GenRM starts outperforming SC, second challenge emerges: with GenRM, When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning the available compute can be split between generating solutions and verifying them in different ways, leading to varying performance. For example, in Figure 1(b), using 8 verifications (red curve) performs better than 4 verifications (green curve) at budget proportional to 28 FLOPs. This shows key tradeoff in GenRM: sampling too few solutions may lower the chances of generating correct one (low coverage), while performing too few verifications can make it harder to identify the correct solution (low precision). Hence, this raises the question: Under GenRM, how to allocate given compute budget between generating solutions and verifying them to achieve optimal performance? To address this, we derive inference scaling laws for GenRM, which describe how the optimal number of solutions and verifications scales with the total compute budget ( 3.2). We observe that while both need to be scaled in tandem, the solutions should be scaled more rapidly than verifications, by factor of 1.5 2, for optimal performance ( 5.2). Overall, our work provides solid foundation for understanding the trade-offs associated with test-time scaling of solutions and verifications, offering key insights for practitioners seeking to optimize their inference strategies and budgets. 2. Background Repeated sampling. prominent approach for scaling test-time compute is repeated sampling, which involves generating multiple potential solutions from the LLM, ùí¢, and selecting final answer as the prediction. Specifically, we study two common methods for choosing the final answer: (a) Self-Consistency (SC) selects the most common answerdetermined by majority votingas the final answer [38]. This strategy benefits from exploring diverse reasoning pathways and marginalizing over them, increasing the likelihood of correct answers. (b) Best-of-N (BoN) scores each candidate solution independently and selects the highest-scoring one [8]. This method relies on reward model capable of accurately assessing problem-solution pairs for correctness. Generative Verification. Unlike traditional reward models that are discriminative, recent works have developed Generative Reward Models (GenRM) that pose verification as next token prediction task [29, 43]. Concretely, the verifier takes the problem and the step-by-step solution, and provides verification chain-of-thought (CoT) followed by its verdict (e.g., Is this answer correct? Yes/No). This approach enables GenRM to inherit all the advantages of LLM reasoning, most notably test-time scaling, along new axis: verification. In particular, we can sample multiple verification CoTs for each solution, and average over their verdicts to obtain more accurate score.1 GenRM-Base. The simplest form of GenRM involves prompting an instruction-tuned LLM to verify solution step-by-step and predict whether the candidate solution/answer is correct for the given problem. We refer to this as GenRM-Base since it uses an off-the-shelf LLM without specialized fine-tuning to act as GenRM.2 GenRM-Finetuning. [43] train an LLM to perform generative verification via supervised fine-tuning, which we denote as GenRM-FT. We start with solution generator ùí¢, student verifier ùëüstudent which will be fine-tuned to obtain GenRM-FT, and teacher verifier ùëüteach which is used to generate synthetic data to fine-tune ùëüstudent. The fine-tuning data is generated in the following way. We take training dataset, ùíütrain = {(xùëñ, yùëñ, aùëñ)}, consisting of problems xùëñ, ground-truth solutions yùëñ, and ground-truth answers aùëñ. We use ùí¢ to generate ùëÅùë† solutions for each problem in the dataset, where yùëñ,ùëó and aùëñ,ùëó are the ùëóth solution and answer generated by ùí¢ for problem ùëñ. Then, we use the teacher verifier ùëüteach (usually strong model like GPT-4o [22]) to generate ùëÅùë£ synthetic verification rationales for the generated solutions yùëñ,ùëó. The teacher verifier has access to the ground-truth solutions in its prompt, allowing it to generate accurate, high-quality verifications. Every verification generated by the teacher consists of chain-of-thought (CoT) reasoning about the solutions correctness, followed by final verdict (Yes or No). We filter these synthetic verifications, retaining only those whose final verdict matches the ground-truth correctness of the generated answer aùëñ,ùëó. Further, we balance this data to have an equal number of Yes and No verifications, leading to the final fine-tuning dataset ùíüGenRM-FT = {(vCoT, x, y)} consisting of verification rationales vCoT and 1While other sophisticated inference strategies exist (e.g. process-level scoring or tree search [35, 39]), their practical use is constrained by the necessity of high-quality process-level training data and complexities associated with their training. 2This is akin to self-verification in [44] where an LLM verifies its own solutions. When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning final verdicts v. Finally, we fine-tune ùëüstudent on this dataset to obtain GenRM-FT. During inference, we generate verification rationale via the GenRM (-base or -FT) and use the probability of the Yes token as the final score ùëüGenRM(x, y) = ùëùùúÉ(Yesx, y, vCoT).3 Test-time scaling with GenRM. For every problem xùëñ in the test dataset, we generate ùëÜ samples {(yùëñ,ùëó, aùëñ,ùëó)ùëó=ùëÜ ùëó=1 } using the solution generator ùí¢, where yùëñ,ùëó and aùëñ,ùëó refer to the ùëóth solution and answer, respectively. For every problem-solution pair, we generate ùëâ verifications {(vùëñ,ùëó,ùëò, ùëüùëñ,ùëó,ùëò)ùëò=ùëâ ùëò=1 } using the generative verifier, where ùëüùëñ,ùëó,ùëò ùëüGenRM(xùëñ, yùëñ,ùëó) [0, 1] is the verification score. We obtain the final score for the solution yùëñ,ùëó by averaging the verification scores from all ùëâ verifications as ùëüùëñ,ùëó = (1/ùëâ ) ùëâ ùëò=1 ùëüùëñ,ùëó,ùëò. Finally, we pick the solution with the highest score ùëüùëñ,ùëó as the final solution. To sum up, GenRM allows time-test scaling along new dimension by increasing the number of verifications ùëâ in addition to increasing the number of solutions ùëÜ. Thinking Models. An emerging approach to scaling test-time computation is training models to generate longer chains-of-thought (CoTs) involving deep thinking before producing solution. These models, e.g., DeepSeekR1 [10], QwQ-32B [32], are trained using reinforcement learning and can leverage additional test-time compute for self-verification, reflection, and backtracking to arrive at final solution. However, the amount of compute they allocate to find and verify solution in their thought process remains uncontrollable. Moreover, their reasoning mechanisms are not yet well understood in the current literature. Here, we focus on complementary inference strategies, which can also benefit thinking models, rather than sequentially refining single solution. 3. Methodology 3.1. Compute-Matched Analysis of Test-time Scaling Strategies Given the option to allocate fixed inference budget toward either scaling solutions via Self-Consistency or verifying them using GenRMs, it remains unclear which approach is compute-optimal for LLM reasoning. To address this question, we conduct compute-matched analysis of their respective scaling behaviors. We consider an autoregressive LLM with ùëÉ parameters that will perform 2ùëÉ FLOPs per output token during inference [23]. Hence, the number of inference FLOPs for generating ùëá tokens is 2ùëÉ ùëá . Let the number of tokens required for generating solution and verification be ùëáùëÜ and ùëáùëâ , respectively. Following [43], we use the same model for problem-solving and generative verification. For instance, one might use Llama-8B to generate solutions and fine-tuned version of the same model as GenRM-FT (or the same model without fine-tuning as GenRM-Base). Hence, the number of model parameters for the solution generator and verifier is identical, say ùëÉ . Thus, the total inference compute (FLOPs) required to solve reasoning problem with ùëÜ solutions and ùëâ verifications is 2ùëÉ (ùëáùëÜùëÜ + ùëáùëâ ùëÜùëâ ) . Further, we consider ùëáùëâ = ùúÜùëáùëÜ where ùúÜ is the ratio of the number of tokens per verification and solution. In our analysis, we use the formula ùê∂(ùëÜ, ùëâ ) = ùëÜ(1 + ùúÜùëâ ) to measure inference compute for simplicity, as it is proportional to the total inference FLOPs for given LLM. For Self-Consistency (SC), we set the number of verifications to ùëâ = 0. We evaluate SC by sampling ùëÜ solutions and performing majority voting over them, for all ùëÜ ùíÆ = {20, 21, ..., 2ùëÅ }, where 2ùëÅ is the maximum number of solutions. Similarly, we evaluate GenRM by varying the number of solutions ùëÜ ùíÆ and verifications ùëâ ùí± = {20, 21, ..., 2ùëÄ }, where 2ùëÄ is the maximum number of verifications per solution. For every combination ùëÜ, ùëâ {ùíÆ ùí±}, we sample the corresponding number of solutions and verifications, and pick the final answer via Best-of-N. We compare the final answers against the ground-truth to compute success rates (SR), and plot them against the total compute, ùê∂(ùëÜ, ùëâ ). Thus, we compare the performance of GenRM and SC at the same compute budget. 3We find that counting the number of Yes across multiple verifications also works well. 4 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning 3.2. Inference Scaling Laws for GenRM As an emerging paradigm, GenRM lacks formalized inference scaling laws. Scaling test-time compute with GenRM involves scaling two independent axes: solutions and verifications. Further, the same amount of compute can be allocated between these two in different ways, leading to different performance outcomes. Hence, understanding the tradeoff between scaling solutions and verifications is crucial for compute-optimal inference. To address this, we extend the approach from Chinchilla [19] to inference-time scaling. Specifically, we follow these steps: 1. Compute the success rate SRùë†,ùë£ for an increasing number of verifications ùë£, while keeping the number of solutions per problem ùë† constant. 2. Plot [SRùë†,20 , . . . , SRùë†,2ùëÄ ] against the inference compute budget ùê∂ = ùë†(1 + ùúÜùë£), and generate such plots for all values of ùë†. 3. Smooth and interpolate curves to obtain mapping from inference compute to SR. 4. For each budget, determine the optimal number of solutions (ùëÜopt) that maximizes SR (Appendix Fig. 8). This results in trend of ùëÜopt as function of the inference budget ùê∂. 5. Fit power law, ùëÜopt ùê∂ ùëé, to establish relationship between the optimal number of solutions and the budget. 6. Similarly, repeat steps 15 for verifications to compute ùëâopt ùê∂ ùëè. higher value of ùëé (ùëè) indicates that as compute increases, the number of solutions (verifications) must be scaled more rapidly compared to the number of verifications (solutions). If ùëé = ùëè, it implies that solutions and verifications should be scaled at the same rate. 4. Experimental Setup Tasks. We use MATH [18], dataset of high-school competition problems to evaluate mathematical reasoning. Additionally, we utilize the MATH train split to train GenRM-FT models. To evaluate generalization of GenRM-FT to harder math tasks, we use the AIME24 dataset [2], which consists of advanced high-school math problems. Finally, to evaluate reasoning beyond math, we use the GPQA-Diamond dataset [33], which consists of problems pertaining to physics, chemistry, and biology. Following [6], we perform all experiments on subset of 128 problems randomly sampled from the MATH test set. Similarly for GPQA, we perform experiments on subset of 64 problems randomly sampled from the diamond split. Models. Following [1, 6, 29], we perform our experiments with Llama-3.1-8B-Instruct [14]. To ensure coverage across model families and sizes, we also experiment with Qwen-2.5-7B-Instruct [40] and Llama-3.3-70B-Instruct on the MATH dataset. We sample solutions with temperature of 0.7 and with maximum of 1024 tokens. GenRM-FT. We fine-tune Llama-3.1-8B-Instruct and Qwen-2.5-7B-Instruct models to serve as GenRM-FT. To create the GenRM fine-tuning data, we use the corresponding models to generate solutions to problems from the MATH training split. Then, we use stronger model, GPT-4o [22], to generate verification rationales for these solutions. Further details are available in Appendix C. We find that the verifications generated by GPT-4o tend to be lengthier than the solution itself, as it analyzes several steps in the solution before making its verdict (Appendix I). Hence, during inference, we sample up to 2048 tokens from our GenRM-FT models with temperature of 0.7, i.e., ùúÜ = 2 for GenRM-FT. We sample up to 32 verifications for 256 solutions, i.e., ùíÆ = {20, ..., 28} and ùí± = {20, ..., 25}. GenRM-Base. We use Llama-3.3-70B-Instruct to generate solutions and verifications with no fine-tuning, i.e., GenRM-Base, due to its strong instruction-following and reasoning capabilities. We sample solutions and verifications using temperature of 0.7 and maximum length of 1024 tokens, i.e., ùúÜ = 1. We experiment with MATH and GPQA for this model. Evaluation. We measure performance in terms of success rate, the average percentage of problems solved on test set. Following [20, 43], we use Best-of-N to select the final answer with verifier. Unlike SC, Best-of-N can detect rare but correct solutions with an effective verifier [44]. We provide the verifier success rate in Appendix Eq. 1. 5 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning (a) (b) Figure 3: Left: Relative improvement achieved by Llama-3.1-8B-Instruct GenRM-FT (32 verifications) over SC for different difficulty levels in MATH. Hard problems benefit more from GenRM-FT, with up to 30% relative improvement over SC. Right: Comparing GenRM-FT against GenRM-Base, we find that GenRM-FT consistently performs better, requiring much less compute to match the performance of GenRM-Base. This highlights the importance of high-quality verifications. 5. Experiments First, we address the question whether at given budget one should scale both solutions and verifications via GenRM or only scale solutions via Self-Consistency (SC). Hence, in 5.1, we compare the performance of these two approaches across range of computational budgets. Further, using GenRM poses another question, as the same budget can be distributed between solutions and verifications in different ways, leading to different performance outcomes. Hence, we develop inference-time scaling laws for GenRM in 5.2. 5.1. Fixed Budget Comparison between Self-Consistency and GenRM Following prior work [29, 43], we compare the success rates of Self-Consistency (SC) and GenRM-FT (w/ 32 verifications) across different number of solutions using Llama-3.1-8B-Instruct on the MATH dataset in Figure 1(a). We see that GenRM-FT outperforms SC when both have the same number of solutions. Further, GenRM-FT matches the performance of SC with 4 fewer solutions. However, this comparison does not account for the cost of generating verifications, and may give the misleading impression that GenRM is generally more efficient than SC. For fair comparison, we plot the success rates against the total compute used to generate solutions and verifications, and compare SC and GenRM at the same budget in Figure 1(b). Interestingly, we find that SC outperforms GenRMFT at lower compute budgets. Notably, GenRM-FT needs 8 more compute to match the performance of SC. Similar to Brown et al. [6], we find that the performance of SC plateaus at around 128 solutions, and sampling solutions further does not provide additional benefits. Consequently, allocating test-time compute to GenRM-FT only starts to yield benefits beyond certain budget. Finally, we find that GenRM-FT achieves 3.8% improvement over the best performance of SC, but requires 128 more compute to achieve this performance."
        },
        {
            "title": "Takeaway",
            "content": "At lower inference-compute budgets, scaling solutions using Self-Consistency leads to better performance than scaling both solutions and verifications with GenRM. However, at higher budgets, GenRM surpasses Self-Consistency in performance. 6 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Figure 4: (Left) Evaluation of GenRM-FT (Llama-3.1-8B trained on MATH) generalizing to AIME24. GenRMFT provides significant improvements over Self-Consistency (SC) on these harder problems, demonstrating its generalization ability, though it requires substantially more compute to outperform SC. (Right) Comparison of GenRM-Base versus SC for an RL-tuned QwQ-32B model. This confirms previous observations: SC performs better at lower budgets, while GenRM shines at higher budgets.We extrapolate the SC curves (dashed lines) because their performance saturates beyond certain point. Impact of Problem Difficulty. When GenRM outperforms SC at higher inference budgets, we investigate which types of problems benefit the most. In Figure 3a, we analyze the relative improvement achieved by applying Best-of-N with GenRM-FT over Self-Consistency, computed as Improvement = (SRGenRM SRSC)/SRSC. Specifically, we evaluate Llama-3.1-8B-Instruct on two difficulty levels from the MATH dataset: level 1 and level 5, which we denote as MATH (Easy) and MATH (Hard), respectively. Our results indicate that GenRM is particularly advantageous for more challenging problems, yielding up to 30% relative improvement in performance. These findings can inform the choice between GenRM and SC, with GenRM offering greater benefits for harder problems. Impact of Verifier Quality. We also compare the performance of GenRM-FT against GenRM-Base at fixed compute budget in Figure 3b. We find that GenRM-FT consistently outperforms GenRM-Base, requiring up to 16 less compute to reach the same performance. This highlights the benefit of fine-tuning LLMs for verification, especially for smaller models with weaker instruction-following and reasoning capabilities. Additionally, this suggests that as the verification capabilities of LLMs improve, GenRM based methods might become more compute-efficient. More details are available in Appendix E. Easy-To-Hard Generalization. In practice, GenRM-FT may encounter unseen problems with higher difficulty than the ones seen during training [36]. Hence, we extend our analysis to harder dataset (AIME-2024) for GenRM-FT trained on an easier dataset (MATH). In particular, we compare SC and GenRM-FT for Llama-3.1-8B-Instruct in Figure 4(a). Interestingly, we find that our previous observations still hold: SC outperforms GenRM-FT at lower compute budgets, whereas GenRM-FT outperforms SC at higher budgets. For instance, SC achieves its peak performance using 16 less inference compute than GenRM-FT needs to reach the same level. However, GenRM almost doubles the performance of SC, but requires 256 more compute to do so. This highlights that generative verifiers can effectively generalize to much harder reasoning tasks. Trends Across Model Families. To study whether our findings generalize to other model families, we compare GenRM-FT and SC with Qwen-2.5-7B-Instruct on the MATH dataset. Our results in Appendix Figure 7 are consistent with our previous findings: SC outperforms GenRM-FT for most of the lower end of the budget spectrum, achieving its peak performance with 64 less compute than GenRM-FT. However, GenRM-FT shines at higher budgets, achieving an improvement of 5.4%, but by utilizing 512 more compute. 7 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Figure 5: Comparing GenRM-Base with Llama-3.3-70B-Instruct on (a) MATH, and (b) GPQA-Diamond. These results highlight that across model sizes and reasoning domains, GenRM outperforms Self-consistency (SC) only at high compute budgets. We extrapolate SC (horizontal dashed line) as its performance saturates after point. Trends for Thinking Models. We extend our analysis to the emerging class of RL-tuned reasoning models that are capable of deep thinking involving self-reflection (e.g., aha moments) before generating the final solution [10]. We evaluate QwQ-32B [32] on the challenging AIME 2025 [3] dataset as solution generator and verifier (GenRMBase). We provide more inference details in Appendix B. We present the results in Figure 4(b). Interestingly, we observe very similar trends for these models as well. In particular, GenRM requires 4 more compute to match the performance of SC, and achieves 2.5% improvement with 16 more compute. Trends Across Model Sizes. Here, we study whether our findings apply to larger LLMs. To this end, we experiment with Llama-3.3-70B-Instruct as GenRM-Base, which has 9 more parameters than Llama-3-8B. The results in Figure 5(a) are consistent with our previous findings: SC performs better at lower budgets and achieves its peak performance with 4 less compute as compared to GenRM-Base at the same performance. Further, GenRM-Base performs better at higher budgets, using 64 more compute to improve performance by 1.7%. This also highlights that using strong models like Llama-3.3-70B as Generative Reward Models can improve reasoning performance even without fine-tuning them for verification. Trends Across Reasoning Domains. Test-time scaling can benefit reasoning in domains beyond math, such as physics, chemistry, and biology. Hence, we compare SC against GenRM-Base at fixed compute budget on GPQA-Diamond using Llama-3.3-70B-Instruct. Our results in Figure 5(b) show that GenRM-Base can provide boost in reasoning abilities across domains, but only at large budget. For instance, it uses 256 more compute than SC to yield 2.5% improvement on SC. At lower budgets, however, SC performs better. 5.2. Inference Scaling Laws for Generative Reward Models Our experiments so far have shown that GenRM becomes favourable choice as the compute budget increases. However, it is important to strike careful balance between the number of solutions and verifications to achieve optimal performance at given budget. This raises the question: What is the optimal way to allocate given compute budget between generating solutions and verifying them? To address this, we derive inference scaling laws, which describe how the optimal number of solutions and verifications scales with compute budget. We sample up to 128 solutions per problem and up to 128 verifications per solution for the MATH test split using Llama-3.1-8B-Instruct and GenRM-FT. Then, we compute the performance at various values of ùëÜ and ùëâ (Figure 6a) and identify the optimal number of solutions and verifications for given budget. Subsequently, we study When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning how the optimal number of solutions and verifications must be scaled as the budget is increased by fitting power law curves. Our findings in Figure 6b show that the optimal number of solutions scales as ùëÜopt ùê∂ 0.57 while the optimal number of verifications scales as ùëâopt ùê∂ 0.39. The larger exponent associated with ùëÜopt indicates that while both solutions and verifications should be scaled in tandem, solutions should be scaled at faster rate for compute-optimal performance. Further experiments in Appendix show that this finding holds across models. (a) Scaling trends of GenRM at (Left) fixed number of solutions and increasing the number of verifications, and (Right) fixed number of verifications and increasing the number of solutions. (b) The optimal number of (Left) solutions and (Right) verifications for given compute budget. Every point corresponds to compute budget. The plots show that as the budget scales, the optimal number of solutions and verifications follows power law, with the number of solutions increasing more rapidly. Figure 6: Compute-optimal scaling of solutions and verifications in GenRM-FT with Llama-3.1-8B-Instruct on MATH."
        },
        {
            "title": "Takeaway",
            "content": "We derive inference scaling laws for generative reward models along two axes: the number of solutions to verify and the number of verifications per solution. For compute-optimal inference, the number of solutions should scale 1.5 2 faster than the number of verifications. 9 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning 6. Related Work Test-Time Compute Scaling. Leveraging more test-time compute to improve the performance of LLMs has gained lot of popularity. Recent studies have explored various methods to scale test-time compute. widely recognized baseline technique is repeatedly sampling candidate solutions from model to choose the most frequent answer (aka self-consistency or majority-voting) [38]. However, recent studies are pushing beyond this, investigating methods that leverage LLMs to iteratively refine their generated outputs [9, 12, 26]. Reasoning models, such as OpenAI o3 series [31] and DeepSeek R1 [10] have enabled sequential scaling of test-time compute by scaling the length of the generated CoT (rather than parallel scaling by generating multiple shorter candidate solutions). While these long CoTs may implicitly incorporate forms of reflection, verification, or refinement within their extended reasoning sequence, such models and previous studies do not primarily address the compute optimality of their proposed methods, the main focus of our investigation. Verification. Another common method to scale inference-time compute is to score candidate solutions through verification, which can be achieved via several techniques. Traditionally, discriminative models are employed, trained via binary classification [8, 28, 41] or preferences [20, 42]. Generative verifiers frame verification as next-token-prediction task, enabling the use of CoT reasoning and another axis to increase inference-time compute, either with trained verifiers [1, 29, 43] or simply via prompting an off-the-shelf LLM (aka LLM-as-ajudge) [4, 7, 24, 45, 46] or self-verification based sampling [44]. These studies evaluate candidate solutions at outcome level, rather than process level verification [27, 37]. Discriminative verifiers have become less favored due to the inherent challenges in their training and their tendency to exhibit lower performance compared to generative approaches [43]. Existing verification-based scaling studies focus on improving accuracy but ignore the overall cost of adding more verifications. They are not concerned with the compute optimal setting about spending ones budget on generating more candidate solutions or more verifications for existing solutions. Inference Scaling Laws. We can better allocate resources by understanding the characteristics of different inference scaling strategies. [35] study how scaling test-time compute, through search against dense rewards and adaptive response updates impacts reasoning performance, revealing prompt difficultly as key factor. In contrast, our work takes into account the verification budget and studies the trade-off between allocating compute to generate new candidate solutions versus verifying existing ones. [39] investigate compute-optimal scaling, specifically examining the trade-off between model size and generating multiple samples. Consistent with findings in [5, 6, 39], they demonstrate that sampling multiple times from smaller model can outperform larger and stronger model within fixed budget. However, we focus our scaling analysis on the trade-off between generating additional solution candidates and generating verifications for existing solutions. [34] argue that test-time compute scaling without verification is suboptimal which is consistent with our overall findings. 7. Conclusion Generative Reward Models (GenRMs) introduce novel approach to scaling test-time compute through verifications. While prior work demonstrates that scaling both solutions and verifications can surpass Self-Consistency (SC), it often overlooks verification costs. In this study, we investigate whether scaling verifications improves performance under fixed budget. We find that SC outperforms GenRMs at lower budgets, whereas GenRMs excel at higher ones. Our conclusions regarding the compute-optimality of SC and GenRMs across different budgets remain robust across various model families (including thinking models), sizes, and reasoning tasks. Furthermore, we derive inference scaling laws to optimize budget allocation between solutions and verifications in GenRM. Overall, our findings provide practical guidance for compute-efficient scaling to achieve optimal performance."
        },
        {
            "title": "Acknowledgements",
            "content": "Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380. Nishad Singhi is supported by LOEWEStart-Professur (LOEWE/4b//519/05.01.002-(0006)/94). Marcus Rohrbach is supported in part by an 10 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Alexander von Humboldt Professorship in Multimodal Reliable AI, sponsored by Germanys Federal Ministry for Education and Research. We thank Ashima Suvarna, Xueqing Wu, Hector Garcia Rodriguez, Jonas Grebe, and Tobias Wieczorek for their helpful comments on the draft."
        },
        {
            "title": "References",
            "content": "[1] Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critique-outloud reward models. arXiv preprint arXiv:2408.11791, 2024. [2] AoPS. Aime 2024 dataset, 2024. URL https://artofproblemsolving.com/wiki/index.php/2024_ AIME_I,II. [3] AoPS. Aime 2025 dataset, 2025. URL https://artofproblemsolving.com/wiki/index.php/2025_ AIME_I,II. [4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [5] Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, and Mehran Kazemi. Smaller, weaker, yet better: Training LLM reasoners via compute-optimal sampling. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=3OyaXFQuDl. [6] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher R√©, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [7] Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan √ñ. Arik. SETS: leveraging self-verification and self-correction for improved test-time scaling. CoRR, abs/2501.19306, 2025. doi: 10.48550/ARXIV.2501.19306. URL https://doi.org/10.48550/arXiv.2501.19306. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [9] Jonathan Cook, Tim Rockt√§schel, Jakob N. Foerster, Dennis Aumiller, and Alex Wang. Ticking all the boxes: Generated checklists improve LLM evaluation and generation. CoRR, abs/2410.03608, 2024. doi: 10.48550/ARXIV.2410.03608. URL https://doi.org/10.48550/arXiv.2410.03608. [10] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [11] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation. 12 2023. URL https://zenodo.org/records/10256836. [12] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Sx038qxjek. 11 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning [13] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. [14] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [15] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=tEYskw1VY2. [16] Etash Guha, Negin Raoof, Jean Mercat, Ryan Marten, Eric Frankel, Sedrick Keh, Sachin Grover, George Smyrnis, Trung Vu, Jon Saad-Falcon, Caroline Choi, Kushal Arora, Mike Merrill, Yichuan Deng, Ashima Suvarna, Hritik Bansal, Marianna Nezhurina, Yejin Choi, Reinhard Heckel, Seewong Oh, Tatsunori Hashimoto, Jenia Jitsev, Vaishaal Shankar, Alex Dimakis, Mahesh Sathiamoorthy, and Ludwig Schmidt. Evalchemy: Automatic evals for llms, November 2024. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [20] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-STar: Training verifiers for self-taught reasoners. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=stmqBSW2dV. [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [22] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [23] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [24] Seungone Kim, Jamin Shin, Yejin Choi, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=8euJaTveKw. [25] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [26] Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. Evolving deeper LLM thinking. CoRR, abs/2501.09891, 2025. doi: 10.48550/ARXIV.2501.09891. URL https://doi.org/10.48550/arXiv.2501.09891. When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning [27] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [28] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. CoRR, abs/2406.06592, 2024. doi: 10.48550/ARXIV.2406.06592. URL https: //doi.org/10.48550/arXiv.2406.06592. [29] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fr√§nken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. [30] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [31] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. [32] Qwen. Qwq-32b: Embracing the power of reinforcement learning. Technical report, 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. [33] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [34] Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without verification or rl is suboptimal, 2025. URL https://arxiv.org/abs/2502.12118. [35] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [36] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-tohard generalization: Scalable alignment beyond human supervision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=qwgfh2fTtN. [37] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Mathshepherd: Verify and reinforce llms step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 94269439. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.510. URL https: //doi.org/10.18653/v1/2024.acl-long.510. [38] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [39] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. [40] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 13 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning [41] Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning in mathematical reasoning. In Kevin Duh, Helena G√≥mez-Adorno, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 858 875. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-NAACL.55. URL https://doi.org/10.18653/v1/2024.findings-naacl.55. [42] Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels, 2024. URL https://arxiv.org/abs/2412.01981. [43] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=Ccwp4tFEtE. [44] Eric Zhao, Pranjal Awasthi, and Sreenivas Gollapudi. Sample, scrutinize and scale: Effective inference-time search by scaling verification, 2025. URL https://arxiv.org/abs/2502.01839. [45] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [46] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llmas-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html. 14 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning A. Discussion In this work, we conduct compute-matched comparison between self-consistency and generative reward models. Specifically, we analyze performance under fixed inference FLOP budget. relevant direction for future work is to extend this analysis to fixed latency (tokens/sec) setting across diverse hardware (e.g., GPUs/TPUs, different VRAM configurations) and algorithmic approaches (e.g., batching, efficient attention). Additionally, our compute budget calculations focus on the widely used decoder-only autoregressive large language models (LLMs). However, emerging LLM architectures, such as sub-quadratic models [15] and language diffusion models [30], present promising directions for future exploration. key bottleneck is the current lack of competitive reasoning-capable LLMs in these architectures. Finally, we evaluate the Best-of-N strategy with outcome-based verification using generative verifier. Future work could extend compute-matched analyses to alternative inference strategies, such as beam search or lookaround search [35]. B. Inference Details Following [6], we use vLLM [25] with up to 16 A100 GPUs to generate solutions and verifications from LLMs. Math Datasets Following [6], we use 4-shot prompt to generate solutions (Prompt H.1). We always sample with temperature of 0.7 and top_p = 0.95. For solutions, we set the maximum number of tokens to 1024. We sample up to 2048 and 1024 tokens per verification for GenRM-FT and GenRM-Base, respectively. We use the minerva_math function from LMEval [11] to extract the final answer from the solution, and the is_equiv function to check if the answer matches the ground-truth answer. For GenRM-base, we use 2-shot prompt to generate verifications (Prompt H.2). For GenRM-FT, we use Prompt H.3 for both fine-tuning and inference. Thinking Model We evaluate QwQ-32B on the challenging AIME2025 benchmark.4 We use Prompt H.7 [16] and Prompt H.8 to generate solutions and verifications, respectively. Since this model generates much longer reasoning traces than traditional instruction-tuned models, we generate maximum of 32, 768 tokens (ùúÜ = 1) with temperature of 0.7 for both solution and verification tasks. We note that the compute required to generate the thought process within the <think> tokens is included in the solution and verification token budget. GPQA We perform our experiments on subset of 64 randomly sampled problems from the diamond split of GPQA. We use zero-shot prompt to generate solutions (Prompt H.5). We sample with temperature of 0.7 and with the maximum number of tokens set to 1024. For GenRM-Base, we use zero-shot prompt to generate verifications (Prompt H.6) with temperature of 0.7 and maximum of 1024 tokens. C. GenRM Training Details We use the solution generator, e.g., Llama-3.1-8B-Instruct, to generate 4 solutions for problems in the MATH training split (7500 problems), using the prompt described in Appendix B. Then, we provide these solutions to GPT-4o along with the ground-truth solutions, and generate 4 verifications for every solution using Prompt H.4. We filter out the verifications whose verdict doesnt match the ground-truth correctness of the solution. Further, we sample verifications such that the number of correct and incorrect samples is balanced. We use LoRA [21] to fine-tune the same model on this dataset of synthetic verification rationales for 3 epochs. We pick the learning rate from {5ùëí 7, 1ùëí 6, 1ùëí 5} based on accuracy on validation split (10% of training split). 4We observed significant performance drop on AIME2025 in comparison to MATH and AIME2024. This highlights that the scope of improvement is higher with verification in comparison to saturated datasets. 15 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Figure 7: Comparing GenRM-FT against Selfconsistency (SC) with Qwen-2.5-7B-Instruct. SC is better at lower budgets while GenRM outperforms at higher budgets, suggesting that our findings hold across model families. D. Evaluation Details Reliable Estimation of Best-of-N Following [20], we estimate the average success rate by estimating the probability that when sampling ùëò out of ùëÅ (> ùëò) solutions, the one with the highest score is correct, and averaging it over ùêæ repetitions: Best-of-ùëò := 1 (ùëÅ ùëò ) ùëÅ ùëò ùëñ=0 ) (ùëÅ ùëñ 1 ùëò 1 ùõºùëñ (1) where [ùõº0, ùõº1, ..., ùõºùëÅ 1] are the binary correctness scores (0 or 1) for the candidate solutions sorted in decreasing order of their verifier scores. E. Impact of Verifier Quality Our results indicate that using small number of verifications does not lead to competitive performance. In most cases, at least 32 verifications are needed to achieve good results, which significantly increases computational cost. This suggests that individual verifications may be noisy. If verification quality improves, however, then fewer verifications might be required to achieve the same performance. To investigate this, we compare two generative verifiers: (1) strong\" verifier: Llama-3.1-8B-Instruct fine-tuned on GPT-4o verification rationales (GenRM-FT), and (2) weak\" verifier: Llama-3.1-8B-Instruct using two-shot verification prompt (GenRM-base) on the MATH dataset. We use ùúÜ = 1 for GenRM-Base and ùúÜ = 2 for GenRM-FT. Figure 3b shows how these verifiers perform as the number of solutions and verifications increases. For clarity, we only plot the best-performing configurations (number of solutions and verifications) for given compute budget. Our findings reveal that across the board, the strong verifier achieves similar performance with significantly less (up to 16) compute than the weak verifier. Moreover, the strong verifier outperforms the weak verifier overall. These results underscore the importance of high-quality verification rationales. As the verification capabilities of LLMs improve in the future, the compute required for GenRM to achieve strong performance may decrease. 16 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning F. Trends Across Model Families In Figure 7, we present compute-matched comparison between SC and GenRM-FT with Qwen-2.5-7B-Instruct on the MATH dataset. Similar to previous observations, we find that SC outperforms GenRM-FT at lower budgets, whereas GenRM shines at higher budgets. G. Additional Details and Results on Compute-Optimal Scaling Analysis Figure 8: Toy illustration of how we compute ùëÜopt for given budget. Every curve corresponds to fixed number of solutions, with the number of verifications increasing along x-axis. For any budget on the x-axis, we find the curve that has the highest success rate at that budget. For instance, at budget of 26, the curve of ùëÜ = 23 has the highest success rate, hence, ùëÜopt = 23 at this budget. Further, the value of ùëÜopt increases in step-changes, as the number of optimal solutions must be an integer. The figure shows that ùëÜopt = 24 for budgets between 27 and 28. ùëâopt can be computed analogously from plot of fixed verifications and increasing number of solutions. In this section, we derive scaling laws for Qwen-2.5-7B-Instruct and Llama-3.3-70B-Instruct on the MATH dataset. For Qwen-2.5-7B-Instruct, we sample 128 solutions and 128 verifications using GenRM-FT. Our findings in Figure 9 show that as the budget is increased, the optimal number of solutions scales as ùëÜopt ùê∂ 0.75 and ùëâopt ùê∂ 0.32. For Llama-3.3-70B-Instruct, we sample 64 solutions and 64 verifications using GenRM-Base. Figure 10 shows that ùëÜopt ùê∂ 0.69 and ùëâopt ùê∂ 0.43. These results show that as test-time compute is scaled, the number of solutions should be scaled more rapidly as compared to the number of verifications. 17 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Qwen-2-7B w/ GenRM-FT on MATH (a) Scaling trends of GenRM at (Left) fixed number of solutions and increasing the number of verifications, and (Right) fixed number of verifications and increasing the number of solutions. (b) The optimal number of (a) solutions and (b) verifications for given compute budget. Every point corresponds to compute budget. The plots show that as the budget scales, the optimal number of solutions and verifications follows power law, with the number of solutions increasing more rapidly. Figure 9: Compute-optimal scaling of solutions and verifications in GenRM-FT with Qwen-2.5-7B-Instruct on MATH. 18 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Llama-3-70B w/ GenRM-Base on MATH (a) Scaling trends of GenRM at (Left) fixed number of solutions and increasing the number of verifications, and (Right) fixed number of verifications and increasing the number of solutions. (b) The optimal number of (a) solutions and (b) verifications for given compute budget. Every point corresponds to compute budget. The plots show that as the budget scales, the optimal number of solutions and verifications follows power law, with the number of solutions increasing more rapidly. Figure 10: Compute-optimal scaling of solutions and verifications in GenRM-Base with Llama-3.3-70BInstruct on MATH. 19 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning H. Prompts H.1. Generating Solutions to Math Problems 4-shot Prompt for generating solutions to math problems 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 Problem : Find the domain of the expression {5 - }} $ .} $ frac { sqrt {x -2}}{ sqrt Solution : The expressions inside each square root must be non - negative . Therefore , $x -2 ge 0$ , so $x ge2$ , and $5 - ge 0$ , so $x le 5 $ . Also , the denominator cannot be equal to zero , so $5 - >0 $ , which gives $x <5 $ . Therefore , the domain of the expression is $ boxed {[2 ,5) } $ . nFinal Answer : The final answer is $ [2 ,5) $ . hope it is correct . Problem : If $ det mathbf { } = 2 $ and $ det mathbf { } = 12 , $ then find $ det ( mathbf { } mathbf { }) . $ Solution : We have that $ det ( mathbf { } mathbf { }) = ( det mathbf { }) ( det mathbf { }) = (2) (12) = boxed {24}. $ nFinal Answer : The final answer is $24$ . hope it is correct . Problem : Terrell usually lifts two 20 - pound weights 12 times . If he uses two 15 - pound weights instead , how many times must Terrell lift them in order to lift the same total weight ? Solution : If Terrell lifts two 20 - pound weights 12 times , he lifts total of $2 cdot 12 cdot20 =480 $ pounds of weight . If he lifts two 15 - pound weights instead for $n$ times , he will lift Equating total of $2 cdot15 cdot =30 n$ pounds of weight . this to 480 pounds , we can solve for $n$ : begin { align *} n30n &=480 Rightarrow qquad &=480/30= boxed {16} end { align *} nFinal Answer : The final answer is $16$ . hope it is correct . Problem : If the system of equations begin { align *} n6x -4 &= , n6y -9 &= . end { align *} has solution $ (x , ) $ where $x$ and $y$ are both nonzero , nfind $ frac { }{ } , $ assuming $b$ is nonzero . Solution : If we multiply the first equation by $ - frac {3}{2} $ , we obtain 20 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning n$$6y -9 = - frac {3}{2} . $$Since we also know that $6y -9 = b$ , we have n$$ - frac {3}{2} = Rightarrow frac { }{ }= boxed { - frac {2}{3}}. $$ nFinal Answer : The final answer is $ - frac {2}{3} $ . hope it is correct . 24 25 26 27 Problem : < Problem > Solution : H.2. GenRM-Base Few-shot GenRM-Base prompt for math problems 1 3 4 5 6 7 9 10 11 12 13 15 16 17 Problem : Find the domain of the expression {5 - }} $ .} $ frac { sqrt {x -2}}{ sqrt Solution : The expressions inside each square root must be non - negative . Therefore , $x -2 ge 0$ , so $x ge2$ , and $5 - ge 0$ , so $x le 5 $ . Also , the denominator cannot be equal to zero , so $5 - >0 $ , which gives $x <5 $ . Therefore , the domain of the expression is $ boxed {[2 ,5) } $ . nFinal Answer : The final answer is $ [2 ,5) $ . hope it is correct . Problem : If $ det mathbf { } = 2 $ and $ det mathbf { } = 12 , $ then find $ det ( mathbf { } mathbf { }) . $ Solution : We have that $ det ( mathbf { } mathbf { }) = ( det mathbf { }) ( det mathbf { }) = (2) (12) = boxed {24}. $ nFinal Answer : The final answer is $24$ . hope it is correct . Problem : Terrell usually lifts two 20 - pound weights 12 times . If he uses two 15 - pound weights instead , how many times must Terrell lift them in order to lift the same total weight ? Solution : If Terrell lifts two 20 - pound weights 12 times , he lifts If he total of $2 cdot 12 cdot20 =480 $ pounds of weight . lifts two 15 - pound weights instead for $n$ times , he will lift total of $2 cdot15 cdot =30 n$ pounds of weight . Equating this to 480 pounds , we can solve for $n$ : begin { align *} n30n &=480 Rightarrow qquad &=480/30= boxed {16} end { align *} nFinal Answer : The final answer is $16$ . hope it is 21 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning correct . Problem : If the system of equations begin { align *} n6x -4 &= , n6y -9 &= . end { align *} has solution $ (x , ) $ where $x$ and $y$ are both nonzero , nfind $ frac { }{ } , $ assuming $b$ is nonzero . Solution : If we multiply the first equation by $ - frac {3}{2} $ , we obtain n$$6y -9 = - frac {3}{2} . $$Since we also know that $6y -9 = b$ , we have n$$ - frac {3}{2} = Rightarrow frac { }{ }= boxed { - frac {2}{3}}. $$ nFinal Answer : The final answer is $ - frac {2}{3} $ . hope it is correct . Problem : < Problem > Solution : 18 19 20 21 23 24 25 26 27 H.3. GenRM-FT 1 2 3 4 Question : {} Solution : {} Verification : Prompt used for GenRM-FT H.4. Prompt to Generate Synthetic Training Data"
        },
        {
            "title": "Prompt used to generate synthetic verification rationales for training GenRM",
            "content": "You are math teacher . Grade the Solution , verifying correctness step by step . At the end of the Solution verification , when you give your final grade , write it in the form Verification : Is the answer correct ( Yes / No ) ? , where is either Yes or No . Example 1: Question : < Question > 1 3 4 5 6 7 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning Solution : < Solution > Expected Solution : < Ground - truth Solution > Teacher Verification : < Ground - truth verification rationale > Verification : Is the answer correct ( Yes / No ) ? No --- Example 2: Question : < Question > Solution : < Solution > Expected Solution : < Ground - truth Solution > Teacher Verification : < Ground - truth verification rationale > Verification : Is the answer correct ( Yes / No ) ? No -- Now , continue grading the next solution step - by - step as follows . Question : {} Solution : {} Expected Solution : {} Teacher Verification : 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 39 40 41 42 44 45 46 47 48 23 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning H.5. GPQA Solution"
        },
        {
            "title": "Prompt used to generate solutions to GPQA problems",
            "content": "1 2 3 4 5 7 8 9 Think step - by - step to solve the following problem . Only include the letter choice (A , , , or ) as your final response . End your answer with Final Answer : The final answer is . hope it is correct . , where is your final answer . Problem : < Problem > Options : < Options > Answer : H.6. GPQA Verification Prompt used to generate verifications to GPQA solutions with GenRM-Base You are teacher expert in physics , biology , and chemistry . Grade the Solution , verifying correctness step by step . At the end of the Solution verification , when you give your final grade , write it in the form Verification : Is the answer correct ( Yes / No ) ? , where is either Yes or No . Question : {} Solution : {} Teacher Verification : 1 2 4 5 6 7 H.7. Thinking Models Solution Prompt"
        },
        {
            "title": "Prompt used to generate solutions from reasoning models",
            "content": "1 2 Problem : {} Mark your solution with boxed nAnswer : 24 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning H.8. Thinking Models Verification Prompt Prompt used to generate verifications from reasoning models as GenRM-Base 1 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 ** Question :** {} ** Student Solution :** {} # Math Solution Grading Instructions As math teacher , your role is to grade student solutions following these standardized steps : ## Verification Process 1. Perform detailed verification by : - Checking each step sequentially - Verifying all calculations - Validating mathematical properties and rules used - Examining the logical flow between steps - Confirming proper notation and mathematical writing ## Error Documentation 2. For any errors found : - Point out the specific location of the error - Explain why it incorrect - Show the correct approach - Note if the error affected subsequent steps ## Assessment Format 3. End your answer in this format : \" Verification : Is the answer correct ( Yes / No ) ? \". Where is Yes or No . ** Verification :** Alright , let verify the solution step by step . I. Verification Example from GPT-4o Verification Example from GPT-4o 2 Problem : street has 20 houses on each side , for total of 40 houses . The addresses on the south side of the street form an arithmetic sequence , as do the addresses on the north side of the street . On the south side , the addresses are 4 , 10 , 16 , etc . , and on the north side they are 3 , 9 , 15 , etc . sign painter paints house numbers on house for $ $1$ per digit . If he 25 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning paints the appropriate house number once on each of these 40 houses , how many dollars does he collect ? Solution : There are $20$ houses on each side , so the painter will paint total of $40$ numbers . The house numbers on the south side are $4 , 10 , 16 , 22 , 28 , 34 , 40 , 46 , 52 , 58 , 64 , 70 , 76 , 82 , 88 , 94 , 100 , 106 , 112 , 118 $ and the house numbers on the north side are $3 , 9 , 15 , 21 , 27 , 33 , 39 , 45 , 51 , 57 , 63 , 69 , 75 , 81 , 87 , 93 , 99 , 105 , 111 , 117 $ . Among the house numbers on the south side , the number of digits in each number is as follows : - 4 and 40 have 1 digit . - 10 , 22 , 28 , 34 , 46 , 52 , 58 , 64 , 70 , 76 , 82 , 88 , 94 , 100 , 106 , 112 , 118 have 2 digits . - 16 , 26 , 36 , 46 , 56 , 66 , 76 , 86 , 96 have 2 digits . - 58 , 68 , 78 , 88 , 98 have 2 digits . - 64 , 74 , 84 , 94 have 2 digits . - 100 , 110 have 3 digits . Among the house numbers on the north side , the number of digits in each number is as follows : - 3 and 33 and 63 and 93 and 123 have 1 digit . - 9 , 21 , 39 , 51 , 69 , 81 , 99 have 1 digit . - 15 , 27 , 45 , 57 , 75 , 87 , 105 have 2 digits . - 23 , 35 , 47 , 59 , 71 , 83 , 95 , 107 have 2 digits . - 25 , 37 , 49 , 61 , 73 , 85 , 97 have 2 digits . - 29 , 41 , 53 , 65 , 77 , 89 , 101 , 113 have 2 digits . - 43 , 55 , 67 , 79 , 91 , 103 , 115 have 2 digits . - 55 , 67 , 79 , 91 , 103 , 115 have 2 digits . - 119 has 3 digits . Since there are 1 one - digit number , 7 two - digit numbers , and 1 three - digit number among the house numbers on the south side , the total amount of money the painter will collect is $ (1 cdot 1) +(2 cdot 17) +(3 cdot 1) = boxed {39} $ dollars . Similarly , since there are 5 one - digit numbers , 7 two - digit numbers , 1 three - digit number among the house numbers on the north side , the total amount of money the painter will collect is $ (1 cdot 5) +(2 cdot 7) +(3 cdot 1) = boxed {22} $ dollars . Therefore , the total amount of money the painter will collect is 39+22=61 dollars . Final Answer : The final answer is 61. hope it is correct . Verification : Let verify the student solution step by step against the expected solution : --- ### ** Step 1: Problem Understanding ** 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning The problem asks for the total amount of money collected by sign painter to paint house numbers on 40 houses , divided into 20 houses on the north and south sides of the street . Each house has unique address , determined by the arithmetic sequence provided for each side . The painter charges $1 per digit when painting house number . The problem clearly requires us to : 1. Identify the numbers on each side of the street . 2. Group these numbers by the number of digits (1 - digit , 2 - digit , or 3 - digit ) . 3. Calculate the total cost per side of the street , then find the sum for both sides . The expected solution uses the arithmetic sequence formulas and organizes the numbers by digit count . The student solution tries to achieve this goal but deviates significantly in implementation . --- ### ** Step 2: Address Lists ** #### ** South Side :** - Numbers belong to the arithmetic sequence (4 , 10 , 16 , dots , 118) . - Common difference ( = 6) . - Using the ( ) - th term formula ( a_n = + (n -1) ) , the (20^{ text { th }}) term is : [ a_ {20} = 4 + 6(20 -1) = 4 + 6(19) = 4 + 114 = 118. ] The numbers are confirmed to be (4 , 10 , 16 , dots , 118) . #### ** North Side :** - Numbers belong to the arithmetic sequence (3 , 9 , 15 , dots , 117) . - Common difference ( = 6) . - Using the ( ) - th term formula ( a_n = + (n -1) ) , the (20^{ text { th }}) term is : [ a_ {20} = 3 + 6(20 -1) = 3 + 6(19) = 3 + 114 = 117. ] The numbers are confirmed to be (3 , 9 , 15 , dots , 117) . The student constructs these lists correctly in their solution . ** Step 2 is correct .** --- 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 61 62 63 64 65 67 27 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning ### ** Step 3: Counting 1 - digit , 2 - digit , and 3 - digit Numbers ** The student attempts to classify the numbers into groups based on digit count . Let verify the classification for each side based on the expected solution : #### ** South Side :** - **1 - digit numbers **: (4) (1 number ) . - **2 - digit numbers **: From (10) to (94) , inclusive . Using the ( ) - th term formula for the first two - digit ((10) ) and last two - digit ((94) ) : [ 10 = 4 + 6( -1) implies = 2. ] [ 94 = 4 + 6( -1) implies = 16. ] So there are (16 - 2 + 1 = 15) two - digit numbers . - **3 - digit numbers **: From (100) to (118) , inclusive . There are (20 - 16 = 4) three - digit numbers . Thus : - 1 - digit : (1) number , - 2 - digits : (15) numbers , - 3 - digits : (4) numbers . #### ** North Side :** - **1 - digit numbers **: (3 , 9) (2 numbers ) . - **2 - digit numbers **: From (15) to (99) , inclusive . Using the ( ) - th term formula for the first two - digit ((15) ) and last two - digit ((99) ) : [ 15 = 3 + 6( -1) implies = 3. ] [ 99 = 3 + 6( -1) implies = 17. ] So there are (17 - 3 + 1 = 15) two - digit numbers . - **3 - digit numbers **: From (105) to (117) , inclusive . There are (20 - 17 = 3) three - digit numbers . Thus : - 1 - digit : (2) numbers , - 2 - digits : (15) numbers , - 3 - digits : (3) numbers . The student has errors in their count . For example : - They incorrectly list certain numbers as having the wrong number of digits . - They double - count or mislabel numbers . ( For instance , (4) 68 70 71 72 73 74 76 77 78 79 80 82 83 84 85 86 88 89 90 91 92 94 95 96 97 98 100 101 102 103 104 106 107 108 109 110 When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning is listed as having multiple digit counts .) ** Step 3 is incorrect ** due to significant misclassification . --- ### ** Step 4: Cost Calculation ** Using the corrected counts : #### ** South Side :** [ text { Cost } = (1 cdot 1) + (2 cdot 15) + (3 cdot 4) = 1 + 30 + 12 = 43. ] #### ** North Side :** [ text { Cost } = (1 cdot 2) + (2 cdot 15) + (3 cdot 3) = 2 + 30 + 9 = 41. ] #### ** Total Cost :** [ text { Total } = 43 + 41 = 84. ] The student calculation is incorrect . They arrive at total of (61) , which significantly underestimates the total due to their misclassification of numbers . ** Step 4 is incorrect ** due to incorrect cost calculation . --- ### ** Step 5: Final Verification and Grade ** - The student correctly identified the arithmetic sequences . - However , they made significant errors in counting the number of digits for many house numbers on both the north and south sides . These errors carried over into their cost calculation , leading to an incorrect total . ** Verification : Is the answer correct ( Yes / No ) ? No ** 111 112 113 114 116 117 118 119 120 122 123 124 125 126 128 129 130 131 132 134 135 136 137 138 140 141 142 143 144"
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Mila",
        "TU Darmstadt & hessian.AI",
        "University of California Los Angeles"
    ]
}