{
    "paper_title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
    "authors": [
        "Yuwei Guo",
        "Ceyuan Yang",
        "Hao He",
        "Yang Zhao",
        "Meng Wei",
        "Zhenheng Yang",
        "Weilin Huang",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 2 0 7 5 1 . 2 1 5 2 : r End-to-End Training for Autoregressive Video Diffusion via Self-Resampling Yuwei Guo1,, Ceyuan Yang2,, Hao He1, Yang Zhao2, Meng Wei3, Zhenheng Yang3, Weilin Huang2, Dahua Lin1 1The Chinese University of Hong Kong, 2ByteDance Seed, 3ByteDance Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via posttraining, they typically rely on bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training. Date: December 18, 2025 Project Page: https://guoyww.github.io/projects/resampling-forcing/"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in generative video models have demonstrated strong potential for world modeling by approximating physical dynamics and predicting future states conditioned on current observations [2, 7, 28, 36]. Realizing this vision necessitates an autoregressive video generation paradigm that predicts the next frame conditioned on past context, thereby mirroring the strict causal nature of the physical world. Beyond world simulation, such paradigm empowers diverse array of applications, spanning game simulation [1, 62, 78, 85], interactive content creation [42, 54], and temporal reasoning [68]. Despite its conceptual elegance, autoregressive video generation poses significant challenges. The primary hurdle is exposure bias [48, 53]: under teacher forcing, the model conditions on ground truth histories during training, yet must rely on its own generated outputs during inference. This traintest mismatch can induce error accumulation, where small artifacts in model predictions are amplified across the autoregressive rollout, potentially leading to catastrophic video collapse (see figure 1 top). Furthermore, the ever-expanding historical context in autoregressive generation exacerbates attention complexity, posing practical obstacles for both training and inference over long horizons. 1 Figure 1 We introduce Resampling Forcing, an end-to-end, teacher-free training framework for autoregressive video diffusion models. Top: The teacher forcing accumulates errors and leads to video collapse. Middle: Distilled from short bidirectional teacher, Self Forcing suffers from the degraded quality on longer videos. Bottom: Our method offers stable quality by native training on long videos. To mitigate the traintest mismatch, recent works [32, 42] employ post-training strategies aimed at aligning the generated video distribution with real data. For instance, Self Forcing [32] first autoregressively rolls out full videos, subsequently applying distillation or adversarial objectives to enforce distribution matching. By simulating inference during training, they reduce the discrepancy between training and test conditions. However, the reliance on bidirectional teacher or an online discriminator impedes scalable training of autoregressive video models from scratch. bidirectional teacher can also leak future information, compromising the strict temporal causality of the student model. Additionally, extensions to longer sequences typically use simple sliding-window attentions that disregard the varying importance of historical context, which may undermine long-term consistency. In this work, we present Resampling Forcing, an end-to-end training framework for autoregressive video diffusion models. Drawing inspiration from the next-token prediction objective in LLMs, we condition each frame on its clean history and train in parallel via per-frame diffusion loss under causal masking. We posit that, to mitigate error propagation and amplification, the model must be trained for robustness against input perturbations while retaining clean prediction objective. To this end, the core of our method is self-resampling mechanism: the model first induces errors in the history frames, then utilizes this degraded history to condition next-frame prediction. To simulate inference-time model errors, we autoregressively resample the latter segment of each frames denoising trajectory with the online model weights. This process is detached from gradient backpropagation to avoid shortcut learning. In addition, we introduce history routing mechanism that dynamically retrieves the top-k most relevant history frames via parameter-free router, maintaining near-constant attention complexity in long-horizon rollout. Empirical results demonstrate that Resampling Forcing effectively mitigates error accumulation in autoregressive video diffusion models, achieving generation quality comparable to state-of-the-art distilled models. Leveraging native long-video training, our approach outperforms extrapolated baselines in longer video generation. Furthermore, our model exhibits stricter adherence to causal dependencies compared to distillation baselines. We also demonstrate that our history routing mechanism attains sparse context with negligible quality loss, offering viable memory design for long-horizon generation. We anticipate that this work will advance scalable training and long-term memory for future video world models."
        },
        {
            "title": "2 Related Works",
            "content": "Bidirectional Video Generation. Bidirectional video generation refers to non-causal models that synthesize all frames jointly, allowing each frame to attend to both past and future context. Early efforts leveraged 2 GANs [55, 61] or adapted UNet-based text-to-image diffusion models [3, 5, 6, 25]. Inspired by SoRA [7], the field shifted towards 3D autoencoders and scalable Diffusion Transformers (DiT) [49], where all video tokens interact via self-attention, with text conditions injected through MMDiT-style fusion [18, 26, 37] or separate cross-attention layers. State-of-the-art systems include commercial models such as Veo [23], Seedance [21], Kling [38], as well as open-source ones like CogVideoX [74], LTX-Video [27], HunyuanVideo [37], and WAN [64]. Our approach is built with DiT-based video diffusion backbone. Autoregressive Video Generation. Recently, autoregressive video generation [24, 29, 41, 44, 45, 51, 67, 79, 86] has gained significant traction due to its potential in world and game simulation. It generates video sequentially under causal factorization, conditioning each frame on its historical context. pivotal challenge for autoregressive video diffusion is the traintest mismatch that leads to error accumulation [66]. Earlier attempts that directly use teacher forcing suffer from degraded quality as video length increases [20, 31, 84]. To counteract this, prior work injects small noise into history frames to approximate inference-time degradation [62, 67]. Another avenue adopts Diffusion Forcing [10, 11, 57], assigning each frame an independent noise level to enable conditioning at arbitrary noise during autoregressive rollout. Other works explore relaxing strict causality via rolling denoising framework [52, 58, 60, 72]. In this setup, video frames within sliding window maintain non-decreasing noise levels and initiate generation when the preceding frame reaches target timestep. Some works explore the plan-interpolate strategy, by first generating future keyframe and then interpolating intermediate frames [82]. Recently, Self Forcing [32, 42] has emerged as promising post-training solution for train-test alignment. It first autoregressively rolls out the entire video, then computes holistic distribution matching loss. However, its reliance on online discriminators for adversarial loss [22] or pretrained bidirectional teachers for distillation [75, 76, 87, 88] limits scalability and hinders training-from-scratch viability. While sharing the insight of inference simulation, our method uniquely supports end-to-end training without recourse to auxiliary models. Conditioning on Model Predictions. Conditioning the model on its own output is strategy widely adopted to mitigate exposure bias in autoregressive systems. Schedule Sampling [4, 9, 47] in language models exemplifies this approach, replacing portions of the ground truth sequence with model-predicted tokens. In diffusion models, Self-Conditioning [13] improves sample quality by conditioning the current denoising step on previous estimation. For video generation, Stable Video Infinity [39] adopts an error-recycling strategy to reduce drifting in autoregressive long video inference. Efficient Attention for Video Generation. As high-dimensional spatiotemporal signals, video representation requires vast quantity of tokens, making quadratic-cost attention computational bottleneck. To alleviate complexity, numerous works explore efficient attention design for video generation. One line of work employs linear complexity attention [12, 33, 50, 65]. Others exploit the sparsity in the attention score, pruning less activated tokens via defined attention masks [59, 70, 71, 81, 83]. For instance, Radical Attention [40] observes spatial-temporal decay and proposes sparse mask that shrinks importance as temporal distance grows. Recent works also adapt advanced sparse-attention designs from LLMs [46, 80]. For example, MoC [8] and VMoBA [69] integrate Mixture of Block Attention [46] into DiT blocks, where the key and value in attention are dynamically selected via top-k router. Our work integrates similar routing mechanism, specifically tailored for handling long histories in autoregressive video generation."
        },
        {
            "title": "3 Method",
            "content": "We begin by reviewing the background of autoregressive video diffusion models and analyzing the exposure bias issue in section 3.1. We then present our Resampling Forcing algorithm for end-to-end training in section 3.2, and the dynamic history routing for efficient long-horizon attention in section 3.3."
        },
        {
            "title": "3.1 Background",
            "content": "Definition. Autoregressive (AR) video diffusion models factorize video generation into inter-frame autoregression and intra-frame diffusion [30, 56]. Specifically, given condition c, the joint distribution of an -frame video 3 sequence x1:N is expressed as p(x1:N c) = (cid:89) i= p(xix<i, c). (1) To sample from each conditional distribution, the i-th frame xi (denoted as xi by solving the reverse-time ODE starting from Gaussian noise xi 0 at timestep = 0) is synthesized 1 (0, I) at timestep = 1 through xi = xi 1 + (cid:90) 0 1 vθ(xi t, x<i, t, c) dt, (2) which can be calculated via numerical solvers like Euler. Here, the neural network vθ() with parameter θ parameterizes the velocity field dxi t, x<i, t, c). Modern diffusion models typically employ Diffusion Transformer (DiT) [49] architecture, where videos are patchified and processed via attention mechanisms [63]. In practice, it is also common to generate chunk of frames per autoregressive step. For simplicity, we refer to each chunk as frame throughout this paper. t/dt and is conditioned on history frames x<i, i.e., dxi t/dt = vθ(xi Teacher Forcing. To train such sequence models, common approach is teacher forcing, where the model is trained to predict the current frame xi given its ground truth history x<i. In Flow Matching [43], the sample xi at timestep is an interpolation between Gaussian noise ϵi (0, I) and the clean frame xi via = (1 t) xi + ϵi. xi (3) The network vθ() is then trained to regress the t/dt = ϵi xi by minimizing velocity dxi = Ei,t,x,ϵ (cid:2)(ϵi xi) vθ(xi t, x<i, t, c)2 2 (cid:3) . (4) Here, vθ() takes two seperate sequences as inputs: the noisy frames xi as diffusion samples and the noise-free ground truth frames x<i. causal mask restricts each frame to attend only to its clean history, enabling parallel training for all frames (see figure 3 (b,c)). During inference, once frame is generated, its clean features can be cached and reused for subsequent frame generation (KV cache). Therefore, the number of attention queries remains constant, while the keys and values grow as the video becomes longer. Figure 2 Error Accumulation. Top: Models trained with ground truth input add and compound errors autoregressively. Bottom: We train the model on imperfect input with simulated model errors, stabilizing the long-horizon autoregressive generation. The gray circle represents the closest match in the ground truth distribution. Error Accumulation. Under teacher forcing, each frames generation is conditioned on its ground-truth history. However, during inference, model predictions are inevitably imperfect. In other words, model generations always have non-zero discrepancy from the ground truth distribution, which we refer to as model error. The model trained on perfect inputs will propagate and accumulate such errors through the autoregressive loop, leading to quality degradation and eventual failure in long-horizon rollouts. (see figure 2 top)."
        },
        {
            "title": "3.2 Enhancing Error Robustness",
            "content": "As analyzed above, the failure of teacher forcing stems from the distributional mismatch between training and inference inputs, driven by irreducible model errors. With finite model capacity and training samples, eliminating such error is intractable. Instead, we propose training the model to condition on degraded histories while maintaining error-free targets for prediction. This relaxes the model from strict adherence to input conditions and enables correction of input errors. As illustrated at the bottom of figure 2, although the model predictions remain imperfect, errors no longer compound. Instead, the errors are stabilized to near constant level over the autoregressive process. Simulating the Model Error. To train error-robust autoregressive models, we must simulate inference-time model errors on the input conditions. We pursue an end-to-end, teacher-free approach to achieve this, prioritizing 4 Figure 3 Resampling Forcing. (a) To simulate inference-time model error, we add noise on clean videos to sampled timestep ts, then use the online model weights to autoregressively complete the remaining denoising steps. (b) The model is parallel trained with frame-level diffusion loss. (c) sparse causal mask restricts each frame to attend only to its clean history frames. simplicity and scalability. There are two major factors that drive the distribution shift in autoregressive diffusion: (1) intra-frame generation errors that come from imperfect score estimation and discretization, which mainly affect high-frequency details [19, 66]; and (2) inter-frame accumulated errors that propagate through the autoregressive loop [66, 77]. To simulate errors from both aspects, we introduce autoregressive self-resampling on the history condition. To mimic intra-frame errors, we resample the latter denoising trajectory, where high-frequency details are typically synthesized. Specifically, we corrupt ground truth video frame xi to sampled timestep ts (0, 1) via equation (3) to obtain xi . Subsequently, we employ the online model vθ() to complete the remaining ts denoising steps and produce degraded noise-free frame xi that contains model errors. The timestep ts controls how close xi is to its ground truth version xi. To mimic inter-frame error accumulation, we resample each frame autoregressively, conditioning on degraded history frames x<i (see figure 3 (a)), i.e., xi = xi ts + (cid:90) 0 ts vθ(xi t, x<i, t, c) dt. (5) Utilizing online model weights ensures that the error distribution evolves alongside training, thereby compelling the model to continuously learn to correct its current imperfections. Gradients are detached from this process to prevent shortcut learning. In practice, this procedure can be efficiently implemented with KV cache. Sampling Simulation Timestep. The timestep ts in equation (5) governs the trade-off between history faithfulness and error correction flexibility. small ts yields low resampling strength, resulting in degraded sample xi that closely resembles its ground truth xi. This encourages the model to stay faithful to the history frames and risks error accumulation (teacher forcing is limiting case where ts = 0). On the other hand, large ts grants greater flexibility for error correction but raises the chance of content drifting, as the model is permitted to deviate significantly from the historical context. Consequently, the distribution of ts should concentrate density on intermediate values while suppressing extremes. To model this, we chose to sample ts from logit-normal distribution LogitNormal(0, 1) that satisfies the above properties: logit(ts) (0, 1). (6) Generally, stronger models induce fewer errors, allowing for greater emphasis on low resampling strength, and vice versa. To manually bias tss distribution, inspired by [18], we apply timestep shifting with parameter after sampling ts from the standard logit normal distribution via ts ts/ (1 + (s 1) ts) . (7) In implementation, we set < 1 to put more weights on the low-noise part. After resampling, we use the degraded video x1:N as history condition, and use the ground truth video x1:N as the training objective. The complete pseudo code for Resampling Forcing is shown in algorithm 1. Teacher Forcing Warmup. In the initial training phase, the model has not yet converged to the causal architecture and is incapable of generating meaningful content autoregressively. The model errors at this 5 Algorithm 1 Resampling Forcing Require: Video Dataset Require: Shift Parameter Require: Autoregressive Video Diffusion Model vθ() 1: while not converged do 2: 3: 4: 5: ts LogitNormal(0, 1) ts ts/ (1 + (s 1) ts) Sample video and condition (x1:N , c) ts (1 ts) x1:N + ts ϵ, ϵ (0, I) x1:N with gradient disabled do for = 1 to do (cid:90) 0 xi xi ts + vθ(xi t, x<i, t, c) dt end for ts end with Sample training timestep ti Sample ϵi (0, I) ti (1 ti) xi + ti ϵi xi 1 i= (cid:88) (ϵi xi) vθ(xi ti , x<i, ti, c)2 2 Update θ with gradient descent 15: 16: end while 17: return θ 6: 7: 8: 9: 10: 11: 12: 13: 14: sample simulation timestep shift timestep (equation (7)) using numerical solver and KV cache (equation (5)) autoregressive resampling parallel training with causal mask (equation (4)) stage are dominated by random initialization rather than specific intra-frame imperfections or inter-frame accumulation. Therefore, performing history self-resampling can lead to uninformative learning signals and will hinder convergence. We therefore first warm up the model using teacher forcing. Once the model acquires basic autoregressive capabilities (though imperfect), we transition to Resampling Forcing and continue training."
        },
        {
            "title": "3.3 Routing History Context",
            "content": "In autoregressive generation, the number of history frames grows as the video becomes longer. This decelerates generation for subsequent frames, as dense causal attention necessitates attending to the entire historical context. To resolve this, common solution is to restrict the attention receptive field to local sliding window [32, 42, 73]. However, this approach compromises long-term dependency, sacrificing global consistency and exacerbating the drifting problem. To maintain stable attention complexity, we opt to optionally replace the dense causal attention with dynamic routing mechanism, inspired by the advanced sparse attention in LLMs [46, 80]. Specifically, for the query token qi of the i-th frame, rather than attending to the full history, we dynamically retrieve and attend to the top-k most relevant history frames (see figure 4), i.e., Figure 4 History Routing Mechanism. Our routing mechanism dynamically selects the top-k important frames to attend. In this illustration, we show = 2 example, where only the 1st and 3rd frames are selected for the 4th frames query token q4. Attention(qi, K<i, V<i) = Softmax (cid:33) (cid:32) qiK Ω(qi) VΩ(qi), (8) where Ω(qi) is set of selected indices of history frames for query qi. For selection metric, we use the dot 6 product of qi and frame descriptor ϕ(Kj) (for j-th frame), i.e., Ω(qi) = arg max Ω (cid:0)q ϕ(Kj)(cid:1) . (cid:88) jΩ (9) Following [8, 46, 69], we use mean pool as the descriptor transformation ϕ() since it adheres to the attention score computation and is parameter-free. This reduces the per-token attention complexity from linear O(L) to constant O(k) as the number of history frames grows, achieving an attention sparsity of 1 k/L. Notably, while may be set small for high sparsity, the routing mechanism operates in head-wise and token-wise manner, implying that tokens across different attention heads and spatial locations can route to distinct history mixtures, and collectively yield receptive field significantly larger than frames. In implementation, following MoBA [46], we adopt an efficient two-branch attention that fuses an intra-frame pathway and sparse history pathway via global log-sum-exp. In the intra-frame branch, each query attends only to tokens within its own frame. In the history branch, we select up to top-k relevant past frames for each query token. Both branches are implemented efficiently with the flash_attn_varlen_func() interface from FlashAttention [15, 16]. Outputs are combined by aligning their log-sum-exp terms, yielding result equivalent to single softmax over the union of keys."
        },
        {
            "title": "4 Experiments",
            "content": "Model. We build our method upon WAN2.1-1.3B [64] architecture and load its pre-trained weights to speed up convergence. The original model uses bidirectional attention and generates 5 videos (81 frames) in 480 832 resolution. We modify timestep conditioning to support per-frame noise levels, and implement the sparse causal attention in figure 3 (c) with torch.flex_attention(), incurring no additional parameters. Following [14, 32, 73], we use chunk size of 3 latent frames as the autoregressive unit. Training. After switching to causal attention, the model was trained on 5 videos with the teacher forcing objective for 10K steps to warm up, then transition to Resampling Forcing and trained sequentially on 5 and 15 (249 frames) videos, for 15K and 5K steps respectively. We then fine-tune with sparse history routing enabled for 1.5K iterations on 15 videos. The training batch size is 64 and the learning rate for the AdamW optimizer is 5e 5. We set the timestep shifting factor = 0.6 (section 3.2), and = 5 in top-k history routing (section 3.3). For efficiency, we use 1-step Euler solver for history resampling (equation (5)). Inference. We use consistent inference settings to generate all video frames. We use an Euler sampler with 32 steps and timestep shifting factor of 5.0. The classifier-free guidance scale is 5.0 for all frames."
        },
        {
            "title": "4.1 Comparisons",
            "content": "Baselines. We compare our method with recent autoregressive video generation baselines, including SkyReelsV2 [11], MAGI-1 [60], NOVA [17], Pyramid Flow [35], CausVid [77], Self Forcing [32], and concurrent work, LongLive [73]. Notably, SkyReel-V2 operates as clip-level autoregressive model, generating 5 video segments sequentially. MAGI-1 relaxes the strict causal constraint, initiating next-chunk denoising prior to the completion of the current chunks generation. LongLive adopts the same principle as Self Forcing but rolls out longer videos, then takes 5 sub-clips and computes distillation loss with teacher models. Qualitative Comparison. We provide visual qualitative comparison across different methods in figure 5, where all models are prompted to generate 15-second videos. In the upper panel, we compare the visual quality over time, observing that most strict autoregressive models (e.g., Pyramid Flow [35], CausVid [77], and Self Forcing [32]) exhibit error accumulation, manifested as progressive degradation in color, texture, and overall sharpness. Specifically, CausVid drifts towards over-saturation, while Pyramid Flow and Self Forcing display color and texture distortions. By contrast, approaches that relax strict causality (MAGI-1 [60]) or use large autoregressive chunks (SkyReels-V2 [11]) alleviate long-horizon degradation. However, these relaxed settings compromise intrinsic advantages of strict autoregression, such as per-frame interactivity and faithful causal dependency for future-state prediction. 7 Figure 5 Qualitative Comparisons. Top: We compare with representative autoregressive video generation models, showing our methods stable quality on long video generation. Bottom: Compared with LongLive [73] that distilled from short bidirectional teacher, our method exhibits better causality. We use dashed lines to denote the highest liquid level, and red arrows to highlight the liquid level in each frame. Within the strict autoregressive paradigm, our method demonstrates superior robustness in long-term visual quality compared to baselines. In the lower panel, we further compare with concurrent autoregressive model LongLive [73], which first generates long videos and then performs sub-clip distillation using short-horizon teacher. Although LongLive attains strong long-range visual quality, we observe that distillation from short bidirectional teacher fails to ensure strict causality, even with temporally causal student architecture. In the milk pouring example in figure 5, LongLive produces liquid level that rises and then falls despite continuous pouring, which violates physical laws. By contrast, our model maintains strict temporal causality: the liquid level monotonically increases while the source container empties. We attribute this non-causal behavior to two factors. First, the bidirectional teacher is inherently non-causal, allowing future information to influence earlier frames via attention, thereby leaking future context to the student during distillation. Second, sub-clip distillation emphasizes local appearance quality and neglects global causality. Conversely, our training strictly precludes information leakage from the future. Quantitative Comparison. We evaluate methods using the automatic metrics provided by VBench [34]. All models are prompted to generate 15-second videos, which we partition into three segments and evaluate them separately to better assess long-term quality. Results are summarized in table 1. As evidenced in the table, our method maintains comparable visual quality and superior temporal quality on all video lengths to baselines. On longer video lengths, our methods performance also matches the long video distillation baseline LongLive. Given that the distill-based methods (i.e., CausVid [77], Self Forcing [32], and LongLive [73]) necessitate pretrained 14B-parameter bidirectional teacher, our method offers substantial efficiency and practicality for training autoregressive video models. Moreover, the history routing mechanism achieves an attention sparsity 8 Table 1 Quantitative Comparisons. We split the generated 15-second videos into three parts, i.e., 05 s, 510 s, and 1015 s, and separately evaluate the videos with VBench [34]. Method #Param Teacher Model Video Length = 015 Video Length = 510 Video Length = 1015 Temporal Visual Text Temporal Visual Text Temporal Visual Text SkyReels-V2 [11] MAGI-1 [60] NOVA [17] Pyramid Flow [35] CausVid [77] Self Forcing [32] LongLive [73] Ours (75% sparsity) Ours 1.3B 4.5B 0.6B 2.0B 1.3B 1.3B 1.3B 1.3B 1.3B - - - - WAN2.1-14B(5s) WAN2.1-14B(5s) WAN2.1-14B(5s) - - 81.93 87.09 87.58 81.90 89.35 90.03 81.84 90.18 91.20 60.25 59.79 44.42 62.99 65.80 67.12 66.56 63.95 64.72 21.92 26.18 25.47 27.16 23.95 25.02 24. 24.12 25.79 84.63 89.10 88.40 84.45 89.59 84.27 81.72 89.80 90.44 59.71 59.33 35.65 61.27 65.29 66.18 67.05 61.95 64.03 21.55 25.40 20.15 25.65 22.90 24.83 23. 24.19 25.61 87.50 86.66 84.94 84.27 87.14 84.26 84.57 87.03 89.74 58.52 59.03 30.23 57.87 64.90 63.04 67.17 61.01 63.99 21.30 25.11 18.22 25.53 22.81 24.29 24. 23.35 24.39 of 75% while incurring only negligible drop relative to the dense-attention baseline, demonstrating its strong potential for long-horizon generation under constrained compute and memory budgets."
        },
        {
            "title": "4.2 Analytical Studies",
            "content": "In this section, we perform ablation on design components and analyze the model behaviors. Table 2 Error Simulation Strategies. Autoregressive resampling achieves the best quality. In section 3.2, we Error Simulation Strategies. hypothesized that exposing the model to imperfect historical contexts during training mitigates error accumulation, and propose autoregressive selfresampling to simulate model errors. We compare against two alternatives: noise augmentation [62] and parallel resampling. In the first, small Gaussian noise is added to the history frames to improve robustness to inference errors. In the second, all historical frames are resampled in parallel rather than autoregressively. As shown in table 2, the autoregressive resampling strategy achieves the highest quality, followed by parallel resampling and noise augmentation. We attribute this to mismatch between additive noise and the models inference-time error mode, as well as the fact that parallel resampling captures only per-frame degradation while neglecting autoregressive accumulation across time. noise augmentation resampling - parallel resampling - autoregressive Video Length = 015 Visual Simulation Strategies 61.90 62.51 64.25 87.15 88.01 90.46 21.44 24.51 25.26 Temporal Text Simulation Timestep Shifting. We ablate the shifting factor that bias the timestep ts distribution. As defined in equation (7), small concentrates ts in the low-noise region, and large shifts ts toward higher noise. Equivalently, small corresponds to weaker history resampling, encouraging faithfulness to past content, whereas large enforces stronger resampling, promoting content modifications that may induce drift. We observe that model performance is robust to the choice of s; therefore, we adopt extreme values in this ablation to better visualize the impact of the shifting factor. In figure 6, the model trained with small exhibits error accumulation and quality degradation, while very large reduces semantic consistency with history, increasing the risk of initial content drifting. Thus, moderate value for is essential to strike balance between mitigating error accumulation and preventing drift. Figure 6 Comparing Timestep Shifting. moderate shifting scale for resampling timestep ts is necessary to balance between error accumulation and content drifting. Sparse History Strategies. We compare three mechanisms for leveraging historical context in autoregressive generation: dense causal attention, dynamic history routing, and sliding-window attention [32, 42]. We show 9 Figure 7 Sparse History Strategies. We compare dense causal attention, dynamic history routing, and sliding window attention in terms of appearance consistency. qualitative results for dense attention, top-5 and top-1 routing, and sliding-window attention in figure 7. As shown in the figure, routing to the top-5 of 20 history frames yields 75% sparsity with quality comparable to dense attention. Reducing from top-5 to top-1 (95% sparsity) causes only minor quality degradation, demonstrating the robustness of the routing mechanism. We further contrast top-1 routing with sliding window of size 1. Despite equal sparsity, the routing mechanism maintains superior consistency in the fishs appearance. We hypothesize that sliding-window attentions fixed and localized receptive field exacerbates the risk of drift. By contrast, our dynamic routing enables each query token to select diverse historical context combinations, collectively yielding larger effective receptive field that better preserves global consistency. History Routing Frequency. To provide deeper insights into history routing, we experiment with = 1, 3, 5, 7 and visualize the selection frequency of each history frame during the generation of the current frame. As shown in figure 8, the selection frequencies exhibit hybrid sliding-window and attention-sink pattern: the router prioritizes initial frames alongside the most recent frames preceding the target. This effect is most pronounced under extreme sparsity (k = 1) and becomes more distributed as sparsity decreases (k = 1 7), encompassing broader range of intermediate frames. The observation offers empirical support for recent attention designs combining frame sinks with sliding windows for long video rollout [73], which can be viewed as special case of our approach. Our results suggest an alternative path to attention sparsity: replacing fixed heuristic masks with dynamic, content-aware routing mechanism capable of exploring vastly larger space of context combinations. Figure 8 History Routing Frequency. We visualize the beginning 20 frames frequency of being selected when generating the 21st frame. For readability, the maximum bar is truncated and labeled with its exact value."
        },
        {
            "title": "5 Discussions",
            "content": "We presented Resampling Forcing, an end-to-end, teacher-free framework for training autoregressive video diffusion models. Identifying the root cause of error accumulation, we proposed history self-resampling strategy that effectively mitigates this issue, ensuring stable long-horizon generation. Furthermore, we introduced history routing mechanism designed to maintain near-constant attention complexity despite the ever-growing historical context. Experiments demonstrate our methods superior visual quality and robustness under high attention sparsity. Limitations. As diffusion-based approach, our model necessitates iterative denoising steps for inference. Achieving real-time latency may require post-hoc acceleration, such as few-step distillation or improved samplers. Additionally, our training entails processing dual sequence (diffusion samples and clean history), which could be improved with architecture optimizations similar to [42]."
        },
        {
            "title": "References",
            "content": "[1] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37:5875758791, 2024. [2] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aäron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 3: new frontier for world models, 2025. [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [4] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. URL https://openai.com/research/video-generation-models-as-world-simulators. [8] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. [9] Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhari, Huzefa Rangwala, George Karypis, and Rasool Fakoor. Bridging the training-inference gap in llms by leveraging self-generated tokens. arXiv preprint arXiv:2410.14655, 2024. [10] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2025. [11] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [12] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. [13] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022. [14] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-forcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 11 [15] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [16] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [17] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [19] Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, and Sushrut Karmalkar. fourier space perspective on diffusion models. arXiv preprint arXiv:2505.11278, 2025. [20] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, and Long Chen. Ca2-vdm: Efficient autoregressive video diffusion model with causal generation and cache sharing. arXiv preprint arXiv:2411.16375, 2024. [21] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [23] Google DeepMind. Veo, 2025. URL https://deepmind.google/models/veo. [24] Jiatao Gu, Ying Shen, Tianrong Chen, Laurent Dinh, Yuyang Wang, Miguel Angel Bautista, David Berthelot, Josh Susskind, and Shuangfei Zhai. Starflow-v: End-to-end video generative modeling with normalizing flow. arXiv preprint arXiv:2511.20462, 2025. [25] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [26] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. [27] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [28] Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, and Ling Pan. Pre-trained video generative models as world simulators. arXiv preprint arXiv:2502.07825, 2025. [29] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25682577, 2025. [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [31] Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, and Maosong Sun. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. [32] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. [33] Yushi Huang, Xingtong Ge, Ruihao Gong, Chengtao Lv, and Jun Zhang. Linvideo: post-training framework towards (n) attention in efficient video generation. arXiv preprint arXiv:2510.08318, 2025. 12 [34] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [35] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. [36] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. [37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [38] Kuaishou. Kling video model. https://kling.kuaishou.com/en, 2024. [39] Wuyang Li, Wentao Pan, Po-Chien Luan, Yang Gao, and Alexandre Alahi. Stable video infinity: Infinite-length video generation with error recycling. arXiv preprint arXiv:2510.09212, 2025. [40] Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, et al. Radial attention: O(n log n) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025. [41] Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, Lingwei Meng, Xun Guo, Jinyu Li, Hefei Ling, and Furu Wei. Arlon: Boosting diffusion transformers with autoregressive models for long video generation. arXiv preprint arXiv:2410.20502, 2024. [42] Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. [43] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [44] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. [45] Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Infinitystar: Unified spacetime autoregressive modeling for visual generation. arXiv preprint arXiv:2511.04675, 2025. [46] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. [47] Tsvetomila Mihaylova and André FT Martins. Scheduled sampling for transformers. arXiv preprint arXiv:1906.07651, 2019. [48] Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Onal Ertugrul. Elucidating the exposure bias in diffusion models. arXiv preprint arXiv:2308.15321, 2023. [49] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [50] Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. arXiv preprint arXiv:2505.20171, 2025. [51] Sucheng Ren, Chen Chen, Zhenbang Wang, Liangchen Song, Xiangxin Zhu, Alan Yuille, Yinfei Yang, and Jiasen Lu. Autoregressive video generation beyond next frames prediction. arXiv preprint arXiv:2509.24081, 2025. [52] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models, 2024. URL https://arxiv.org/abs/2402.09470. 13 [53] Florian Schmidt. Generalization in generation: closer look at exposure bias. arXiv preprint arXiv:1910.00292, 2019. [54] Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, and Xun Huang. Motionstream: Real-time video generation with interactive motion controls. arXiv preprint arXiv:2511.01266, 2025. [55] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36263636, 2022. [56] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. [57] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. [58] Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, and Jing Liu. Ar-diffusion: Asynchronous video generation with auto-regressive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 73647373, 2025. [59] Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Shunyu Liu, and Dacheng Tao. Vorta: Efficient video diffusion via routing sparse attention. arXiv preprint arXiv:2505.18809, 2025. [60] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. [61] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15261535, 2018. [62] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. [63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [64] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [65] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video generation with linear computational complexity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25782588, 2025. [66] Jing Wang, Fengzhuo Zhang, Xiaoli Li, Vincent YF Tan, Tianyu Pang, Chao Du, Aixin Sun, and Zhuoran Yang. Error analyses of auto-regressive video diffusion models: unified framework. arXiv preprint arXiv:2503.10704, 2025. [67] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-to-video generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73957405, 2024. [68] Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. [69] Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, and Yunhai Tong. Vmoba: Mixture-of-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. [70] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 14 [71] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. [72] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 63226332, 2025. [73] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. [74] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [75] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. [76] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [77] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv e-prints, pages arXiv2412, 2024. [78] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. [79] Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, et al. Lumos-1: On autoregressive video generation from unified model perspective. arXiv preprint arXiv:2507.08801, 2025. [80] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2307823097, 2025. [81] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. [82] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2(3):5, 2025. [83] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025. [84] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. [85] Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, et al. Matrix-game: Interactive world foundation model. arXiv preprint arXiv:2506.18701, 2025. [86] Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, and Nan Duan. Generative pre-trained autoregressive diffusion transformer. arXiv preprint arXiv:2505.07344, 2025. [87] Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, and Hai Huang. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. arXiv preprint arXiv:2410.14919, 2024. [88] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024."
        }
    ],
    "affiliations": [
        "ByteDance",
        "ByteDance Seed",
        "The Chinese University of Hong Kong"
    ]
}