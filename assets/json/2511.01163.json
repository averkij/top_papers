{
    "paper_title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation",
    "authors": [
        "Yongyuan Liang",
        "Wei Chow",
        "Feng Li",
        "Ziqiao Ma",
        "Xiyao Wang",
        "Jiageng Mao",
        "Jiuhai Chen",
        "Jiatao Gu",
        "Yue Wang",
        "Furong Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation."
        },
        {
            "title": "Start",
            "content": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation"
        },
        {
            "title": "Yongyuan Liang",
            "content": ", Wei Chow , Feng Li"
        },
        {
            "title": "Jiuhai Chen",
            "content": ", Jiatao Gu , Yue Wang , Ziqiao Ma , Xiyao Wang , Furong Huang , Jiageng Mao , University of Maryland, College Park, University of Pennsylvania, University of Michigan, University of Southern California, The Hong Kong University of Science and Technology, Authors contributed equally to this work, Advisors contributed equally to this work. Unified multimodal models (UMMs) have emerged as powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1,312 tasks grounded in 1,876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as critical frontier for enabling true omnimodal generation. Homepage: roverbench.github.io Benchmark: cheryyunl/ROVER Release Date: Nov 2, 2025 Contact: cheryunl@umd.edu Code: github.com/cheryyunl/ROVER 5 2 0 2 3 ] . [ 1 3 6 1 1 0 . 1 1 5 2 : r 1. Introduction The development of unified multimodal models (also referred to as omnimodal models) has drawn significant attention to their potential for unified understanding and generation across text and images (Comanici et al., 2025, Hurst et al., 2024, Tong et al., 2024, Deng et al., 2025, Xu et al., 2025b). However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning: textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. On the language side, evaluation focuses on generating text in response to an image and an accompanying question, thereby testing perceptual understanding (Chen et al., 2024, Liu et al., 2024, Yu et al., 2024) and reasoning (Lu et al., 2023, Yue et al., 2024, Wang et al., 2024, Hao et al., 2025, Gao et al., 2025). On the vision side, evaluation centers ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 1: The ROVER benchmark. ROVER evaluates UMMs through reciprocal cross-modal reasoning: ROVER-IG (left) requires generating images with language-augmented reasoning, while ROVER-TG (right) requires generating text answers with visually-augmented reasoning. on generating images conditioned on either instructions or text-image pairs, thereby testing direct image generation (Ghosh et al., 2023, Ma et al., 2024, Niu et al., 2025) or image editing (Kawar et al., 2023, Zhang et al., 2023, Ma et al., 2024, Sheynin et al., 2024, Yu et al., 2025a, Liu et al., 2025b, Wu et al., 2025e). Unlike earlier multimodal systems that specialize in either visual perception or generation, UMMs are designed to reason seamlessly across modalities and produce outputs that span both. This creates pressing need for benchmarks that evaluate their ability to use one modality to guide, verify, or refine outputs in the other. We refer to this capability as reciprocal cross-modal reasoning (Figure 1). To benchmark such capability in current unified multimodal models, we present ROVER, human-annotated and rigorously verified benchmark with over 1,312 tasks grounded in 1,876 images. ROVER targets two complementary settings: (i) verbally-augmented reasoning for visual generation, including 4 conceptual domains (natural science, culture & art, common sense, and logic & math) with high complexity are instantiated across 7 reasoning subtasks: temporal, spatial, causal, synthetic, quantitative, abstract, and mathematical. Each instance provides textual prompt with an initial image and chain of constraints that correct output image must satisfy. (ii) visually-augmented reasoning for verbal generation, including 6 subtask variants spanning 3 problem domains: visual perception, world modeling for robot manipulation & physical dynamics prediction, and logical reasoning for geometry & puzzle solving. Instances interleave turns of text and images, requiring the model to emit visual intermediates that make downstream reasoning auditable. key challenge is that evaluating reciprocal cross-modal reasoning requires assessing both the rationales and the output. Text-only metrics overlook visual fidelity, while image-only metrics cannot verify whether the image reflects valid reasoning. Human evaluation provides accurate judgments but is prohibitively expensive at scale. To address this, we adopt multi-dimensional protocol that combines an automated VLM judge (based on GPT-4.1 (Hurst et al., 2024)) with expert validation on stratified samples. The judge is supplied with rubric cards and reference assets and scores along three reasoning-specific dimensions: (i) the logical coherence of domain-specific reasoning processes, (ii) the alignment of generated outputs with target descriptions or ground-truth answers, and (iii) the consistency between intermediate reasoning steps and the final images or answers. For visual generation tasks, the framework additionally incorporates established image consistency and quality metrics (Hu et al., 2023, Wu et al., 2023, Kirstain et al., 2023, Xu et al., 2023, Brooks et al., 2023). The judge is calibrated with expert explanations, and its agreement with expert evaluations is reported, following recent LLM-as-judge methodologies (Kim et al., 2023, Hu et al., 2023). 2 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Table 1: Summary of Multimodal Reasoning Benchmarks. We compare existing works from aspects including: 1interleave, 2process evaluation, 3vision necessaity, 4multidimensional evaluation, 5hybrid evaluation, and 6whether manual annotations and filtering are applied."
        },
        {
            "title": "Venue",
            "content": "Inter."
        },
        {
            "title": "Process\nEval",
            "content": "Vision Necess. Multi. Hybrid Manual Anno."
        },
        {
            "title": "Eval",
            "content": "#Types ReasonPix2Pix (Jin et al., 2024) arXiv24 ReasonEdit (Huang et al., 2024) CVPR24 MM25 arXiv25 arXiv25 arXiv25 arXiv25 arXiv25 arXiv25 EditWorld (Yang et al., 2024) Reason50K (He et al., 2025) KRIS-Bench (Wu et al., 2025e) RISEBench (Zhao et al., 2025) WorldGenBench (Zhang et al., 2025) Unified-Bench (Yan et al., 2025) MetaQuery (Pan et al., 2025)"
        },
        {
            "title": "Ours",
            "content": "1 1 7 4 7 4 2 1 - 23 Through extensive evaluation of 17 unified multimodal models, our experiments reveal significant gaps in cross-modal reasoning capabilities. Specifically, unified models exhibit: (1) Cross-modal reasoning capabilities largely affect reasoning-dependent visual generation quality, with interleaved image-text generation models significantly outperforming non-interleaved counterparts; critically, strong unimodal models cannot replicate this cross-modal reasoning on ROVER, even when combined; (2) Striking dissociation between physical and symbolic visual reasoning. While unified models excel at generating visual reasoning steps for perceptual and physical world concepts through literal interpretation of visual elements, they fundamentally struggle to reason about visual abstractions as symbolic representations (Hsu et al., 2025), causing poor visual reasoning to degrade downstream performance rather than improving it. These findings reveal fundamental capability limitations in current unified models, underscoring the critical role of reciprocal cross-modal reasoning for holistic omnimodal generation, where independent optimization of constituent modalities proves insufficient. Our main contributions are summarized as follows: We introduce ROVER, the first benchmark that explicitly targets reciprocal cross-modal reasoning for visual generation and interleaved multimodal reasoning. We provide principled task taxonomy and verification-ready instance design with process targets and visual artifacts, together with multi dimensional protocol that scores coherence, alignment, and step to output consistency. We evaluate 17 unified models, uncovering significant limitations in cross-modal reasoning and providing the community with insights on unified model development toward truly omnimodal generation. 2. Related Work Unified Multimodal Models (UMMs). UMMs represent paradigm of architectures designed to seamlessly integrate multimodal comprehension and generation capabilities within singular, cohesive framework. To achieve this unified objective, seminal works (Karypis et al., 1999, Wu et al., 2025b, Chen et al., 2025b) leverage image tokenization strategies, employing autoregressive next-token prediction paradigms to generate visual tokens. Building upon these foundations, Show-o2 (Xie et al., 2025) introduces discrete diffusion scheduling mechanisms to enhance the token prediction process and improve generation fidelity. Subsequent developments, driven by the pursuit of enhanced image synthesis quality, incorporate diffusion-based or flow-matching heads (Lipman et al., 2022) integrated with shared transformer architectures (Deng et al., 2025, Ma et al., 2025, Zhou et al., 2024). Alternative approaches within the UMM paradigm maintain ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 2: Overview of ROVER-IG, the benchmark for evaluating how unified multimodal models generate images under intensive verbal reasoning. The benchmark spans 4 domains (natural science, culture and art, common sense, and logic), instantiated across 7 reasoning subtasks. powerful pretrained backbone in frozen state for reasoning tasks, while routing their intermediate feature representations through learnable query mechanisms to external image generation modules (Pan et al., 2025, Wu et al., 2025d). However, the comprehensive evaluation of synergistic relationships between multimodal understanding, reasoning, and generation in UMMs remains largely unexplored, with existing benchmarks inadequately assessing whether these capabilities exhibit mutual enhancement or coordination deficiencies. Reasoning-Guided Image Generation. With the emergence of UMMs, multimodal reasoning has garnered increasing attention from the research community. However, the majority of existing work remains focused on instruction comprehension, namely leveraging input images to perform instruction translation and subsequently generate corresponding visual outputs (Jin et al., 2024, Huang et al., 2024, Yang et al., 2024, He et al., 2025, Wu et al., 2025e, Yu et al., 2025b). Unified-Bench (Yan et al., 2025) employs iterative image-text generation to measure the degree of unification between comprehension and generation models. RISEBench (Zhao et al., 2025) extends beyond prior work by introducing LMM-as-a-judge to evaluate visual rationality in addition to assessing image consistency, yet remains limited to computing similarity scores against human-provided ground truth. However, these benchmarks lack comprehensive evaluation beyond image consistency, particularly overlooking the intermediate reasoning processes, such as whether the rationale is sound and aligns with the generation outcome. In contrast, ROVER represents the first benchmark to investigate the interplay between reasoning and generation. detailed comparison can be found in Table 1. more detailed discussion about interleaved reasoning can be found in Appendix A. 4 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 3: Overview of ROVER-TG, the benchmark for evaluating visually-augmented reasoning in verbal generation. The benchmark spans 3 scenarios and 6 subtasks: physical world modeling, logical assistance, and visual perception enhancement. 3. ROVER Benchmark 3.1. Verbally-Augmented Reasoning for Visual Generation We introduce ROVER-IG, benchmark designed to evaluate how UMMs generate images when jointly guided not only by visual understanding but also by intensive language reasoning. Taxonomy. It spans 4 domains and 7 reasoning subtasks, each demanding complex text-driven reasoning chains to direct image generation and test models ability to integrate text-augmented reasoning with visual synthesis. Figure 2 provides visual overview of our benchmark taxonomy and representative examples. Domains. We categorize tasks across 4 distinct areas: Nature Science encompasses scientific phenomena, experimental processes, and fundamental laws of nature; Culture & Art includes artistic creation, cultural artifacts, humanities, and aesthetic principles; Common Sense covers everyday scenarios requiring intuitive understanding and practical reasoning; Logic & Math focuses on abstract visual puzzles, mathematical relationships, and general pattern discovery. Reasoning subtasks. We define 5 core reasoning capabilities: Temporal involves sequence prediction, progression analysis, and time-based changes; Spatial requires understanding geometric relationships, perspective changes, and spatial visualization; Causal connects cause-effect relationships and mechanism understanding; Imaginative combines multiple elements through creative integration and novel object generation; Quantitative involves numerical changes, scaling operations, and mathematical relationships. The Logic domain additionally includes two specialized reasoning types: Puzzle for abstract visual pattern discovery and problem solving, and Geometry for geometrical principles applied to visual generation. Data Curation. We curated our dataset through systematic multistage process, beginning with human experts selecting candidate images from large-scale web image datasets. For each selected image, domain experts and large language models collaboratively generated reasoning tasks that require genuine visual understanding and complex reasoning chains. Each task includes 4 key components: the reasoning prompt specifying the required generation results, target descriptions detailing expected visual outcomes, domainspecific keywords identifying relevant concepts that should guide the reasoning process, and optionally target ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation reference images for validation purposes. All generated tasks were subjected to final human verification to confirm the complexity and rationality of the reasoning. Our final dataset comprises 908 visual generation tasks involving 1,009 images, with both single-image and multi-image generation scenarios distributed across all reasoning subtasks and domains. Ideally, the evaluation protocol should cover both the reasoning process and the Evaluation Metrics. resulting outputs. As human evaluation is prohibitively costly at scale, we automated the evaluation following LMM-as-judge. We assess model performance across 5 rubric dimensions designed to capture the effectiveness of reasoning-to-generation workflows. Reasoning Process (RP) evaluates the quality of verbal reasoning through logical structure, domain knowledge application, reasoning type-specific validation, and completeness assessment. Reasoning Visual (RV) measures how well the generated visual output matches target descriptions and demonstrates correct reasoning principles. Reasoning Alignment (Align.) specifically quantifies the consistency between verbal reasoning processes and visual generation outcomes, addressing whether models can effectively translate reasoning into visual results. Visual Consistency (VC) ensures that non-target elements remain unchanged during reasoning-guided generation, validating precise control capabilities. Image Quality (IQ) assesses the technical excellence and visual coherence of generated images, including structural coherence, visual fidelity, and absence of generation artifacts. 3.2. Visually-Augmented Reasoning for Verbal Generation We then introduce ROVER-TG, the benchmark counterpart for evaluating how UMMs generate language responses guided by interleaved reasoning with visually-augmented rationale. Unlike text-only Chain-ofThought, we examine scenarios where models generate intermediate visual representations to facilitate reasoning. This interleaved reasoning paradigm reflects human cognitive patterns that integrate verbal and visual thinking for complex problem solving (Barsalou, 1999). Taxonomy. We focus on 3 scenarios, comprising 6 challenge types and 404 tasks, where visual generation genuinely enhances reasoning beyond text-only rationale, as shown in Figure 3: physical world simulation, logical problem solving with visual aids, and enhanced visual perception through generated representations. World Model. Tasks require models to act as world simulators, predicting intermediate visual states that reflect environment dynamics under given initial conditions and actions. Models must generate these states from robotic actions or physical processes and leverage them for embodied planning, spatial reasoning, and motion prediction. Logic & Math. Tasks involve generating visual aids to solve abstract puzzles and geometry problems, similar to how humans draw auxiliary lines, diagrams, or visual representations to facilitate logical reasoning. Models must create helpful visual elements that make implicit relationships explicit and support step-by-step logical inference processes. Visual Perception. Tasks focus on generating supportive images to improve performance on challenging visual perception problems, including multi-view reasoning and jigsaw puzzles. The generated images in the rationale serve as intermediate representations that reduce hallucinations and improve accuracy in visual understanding tasks. Data Curation. Our dataset compilation draws from diverse sources including robotics datasets, physical simulation videos, logic puzzles, and challenging perception tasks. We establish consistent structure for each task: contextual setup with initial images, progressive reasoning steps, and verified ground truth solutions. Crucially, our curation process ensures that generated visuals serve as active reasoning components rather than decorative elements, thereby fully leveraging omnimodal generation capabilities to tackle complex problem-solving scenarios. 6 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Table 2: Main Results on Verbally-Augmented Visual Generation. We evaluate 13 closedand open-source unified models across four conceptual domains. Verb.-Aug. denotes verbally-augmented reasoning. Performance is measured using three key metrics: Reasoning Process (RP), which assesses the logical quality of the verbal reasoning; Alignment (Align.), which quantifies the consistency between the reasoning process and the generated visual output; and Reasoning Visual (RV), which measures how well the final image reflects the target description. Verb.-Aug. Reasoning for Visual Generation"
        },
        {
            "title": "Nature Science",
            "content": "Culture & Art"
        },
        {
            "title": "Common Sense",
            "content": "Logic & Math"
        },
        {
            "title": "Overall",
            "content": "RP Align. RV RP Align. RV RP Align. RV RP Align. RV RP Align. RV Closed-source Unified Models Nano Banana (Comanici et al., 2025) 64.8 88.8 77.3 68.1 81.9 76.6 61.8 85.0 74.8 78.6 66.1 55.1 67.0 82.3 73.2 Gemini 2.0 Flash (Comanici et al., 2025) 64.1 88.4 68.8 62.8 78.7 71.9 57.8 74.4 66.1 74.5 63.2 42.6 64.8 78.6 62.3 61.7 87.9 71.3 63.4 80.2 72.6 56.3 77.2 65.3 75.4 60.2 45.8 64.2 76.4 63.7 GPT-5 (Hurst et al., 2024) - 35.9 - Open-source Unified Models 58.1 64.2 54.0 53.2 78.0 63.7 50.1 69.4 55.9 57.7 26.2 20.8 54.3 64.4 52.7 BAGEL-Think (Deng et al., 2025) - 40.5 BAGEL (Deng et al., 2025) 29.7 59.7 46.2 31.4 71.6 50.6 28.7 61.0 46.1 77.5 35.5 18.4 37.0 60.3 43.5 Step1X-Edit v1.2 (Liu et al., 2025a) 52.4 68.9 38.2 57.3 69.2 63.9 53.1 64.3 56.3 50.3 23.1 21.5 50.7 56.3 47.4 UniCoT (Qin et al., 2025) 37.8 - BLIP3o-NEXT (Chen et al., 2025a) 33.8 Ovis-U1 (Wang et al., 2025) - 39.2 UniPic2-Metaquery-9B (Wei et al., 2025) - 32.0 - ILLUME+ (Huang et al., 2025) 32.3 - Emu2-Gen (Sun et al., 2023) 32.2 - OmniGen2 (Wu et al., 2025c) 22.5 - 20.5 - 27.1 - 20.1 - 20.3 - 20.2 - 47.5 - 44.3 - 52.7 - 43.2 - 42.6 - 42.3 - 43.3 - 42.1 - 43.2 - 36.9 - 37.4 - 39.2 - 38.2 - 28.6 - 33.8 - 28.1 - 29.1 - 27.4 - 27.1 - 49.2 - 42.0 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Evaluation Metrics. Similarly, we automated the evaluation using VLM judge across 3 rubric dimensions. Interleaved Reasoning Quality (IR) evaluates the plausibility and relevance of intermediate visual representations through physical/logical correctness, task-specific utility, visual coherence, and reasoning completeness. Final Answer Accuracy (Acc.) measures whether the models final reasoning outcome matches the provided ground truth answer across all three scenario types. Reasoning-Answer Alignment (Align.) quantifies how effectively generated images contribute to reaching correct conclusions, examining causal relationships between visual aids and final outputs, reasoning chain coherence, and whether the visual generation process was necessary for successful task completion. 4. Experiments 4.1. Evaluation Setup Models. We evaluate diverse set of models across different categories. For closed-source unified models, we assess three state-of-the-art systems: Gemini 2.5 Flash Image (a.k.a Nano Banana) (Comanici et al., 2025), Gemini 2.0 Flash (Comanici et al., 2025), and GPT-5 (Hurst et al., 2024). For open-source unified models, we evaluate ten representative models including BAGEL-Think and BAGEL (Deng et al., 2025), UniCoT (Qin et al., 2025), Step1X-Edit v1.1/v1.2 (Liu et al., 2025b), BLIP3o-NEXT (Chen et al., 2025a), Ovis-U1 (Wang et al., 2025), UniPic2-Metaquery-9B (Wei et al., 2025), ILLUME+ (Huang et al., 2025), Emu2-Gen (Sheynin et al., 2024), OmniGen2 (Wu et al., 2025c). We also compare against specialized image editing models, including Qwen-Image-Edit (Wu et al., 2025a), FLUX.1 Kontext (Labs et al., 2025), UltraEdit (SD3) (Zhao et al., 2024), VAREedit-8B (Mao et al., 2025). Additionally, we include reasoning language models such as GPT-4.1 (Hurst et al., 2024) to present verbal-only reasoning baselines. All evaluation details are provided in Appendix B. Evaluation Protocol. We employ GPT-4.1 as the automatic judge to assess model outputs across multiple dimensions. All metrics are scored on 5-point scale (1-5) and normalized to 0-100 scale for consistent ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 4: Example outputs on ROVER-IG. Each row corresponds to one reasoning subtask, with the input on the left and outputs from representative unified multimodal models shown across columns. 8 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Table 3: Performance on visually-augmented reasoning. We evaluate 6 leading unified and language models across three problem types, comparing two distinct reasoning modes. Verb. denotes standard verbal reasoning, where the model generates final answer directly from the prompt. Verb.+Vis. denotes visually-augmented reasoning, where the model generates intermediate visual artifacts to support its final answer. We report on the quality of Interleaved Reasoning (IR), Alignment (Align.), and Final Answer Accuracy (Acc.). Verb.+Vis. Reasoning for Verbal Generation"
        },
        {
            "title": "World Model",
            "content": "Logic & Math"
        },
        {
            "title": "Overall",
            "content": "IR Align. Acc. IR Align. Acc IR Align. Acc. IR Align. Acc. Closed-source Unified Models Nano Banana (Comanici et al., 2025) Verb.+Vis. 35.3 62.0 40.6 14.8 61.2 44.9 66.5 56.8 50.0 38.8 60.0 43.6 40.8 Verb. Gemini 2.0 Flash (Comanici et al., 2025) Verb.+Vis. 27.1 46.7 35.6 11.4 47.9 30.4 49.5 46.8 43.0 29.3 47.1 36.3 35.2 Verb. Verb.+Vis. 32.8 61.5 39.2 13.2 58.7 45.6 62.7 54.9 45.5 36.2 60.9 43.4 42.8 Verb. GPT-5 (Hurst et al., 2024) 43.7 - 39.8 - 43.6 - 36.9 - 33.2 - 32.6 - 39.2 - 42.0 - 45.6 - - - - - - - - - - - - - - - - Open-source Unified Models BAGEL-Think (Deng et al., 2025) UniCoT (Liu et al., 2025b) Qwen-2.5-VL-7B (Bai et al., 2025) Reasoning Language Models GPT-4.1 (Liu et al., 2025b) - Verb.+Vis. 22.3 34.7 26.6 10.8 36.9 24.6 31.2 44.3 34.1 21.4 38.6 28.4 Verb. 26.7 Verb.+Vis. 22.1 35.4 26.7 10.6 38.8 21.7 34.2 42.3 34.1 22.3 38.8 27.5 26.7 Verb. 26.5 Verb. 33.5 - 32.9 - 24.6 - 24.2 - 21.7 - 22.4 - 32.2 - 23.1 - 24.9 - - - - - - - - - - - - - - - Verb. - - 37.8 - - 31.8 - - 37.9 - - 35. comparison. For VQA problems in ROVER-TG with objective answers, Acc. denotes exact answer accuracy. 4.2. Verbally-Augmented Reasoning for Visual Generation Cross-modal reasoning capabilities and alignment strongly correlate with visual generation effectiveness. The consistent pattern across all models and dimensions in Table 2. Closed-source models excel in reasoning processes and demonstrate strong alignment performance, which directly contributes to their superior visual generation quality. In contrast, open-source models show notably weaker verbal reasoning during visual generation taskstheir reasoning processes are approximately 38% lower and alignment performance falls about 31% short of closed-source models. This substantial reasoning gap translates into correspondingly diminished visual generation performance that is approximately 39% lower than closed-source models. This finding confirms that cross-modal reasoning capabilities serve as the fundamental driver of visual generation effectiveness on ROVER-IG, with stronger reasoning processes and better alignment consistently enabling superior visual output quality. Models capable of interleaved image-text generation demonstrate superior visual generation performance. Our results reveal significant performance gap between models that support interleaved generation and those limited to single-turn, single-modality outputs. Among the open-source models evaluated, those with interleaved generation capabilities demonstrate markedly superior performance on Reasoning Visual (RV) metricapproximately 38.1% higher than non-interleaved models. This performance advantage suggests that reasoning and generation processes are synergistic, effectively enhancing the models performance in visual expression tasks. Unified models demonstrate absolute advantages over image editing models across visual quality metrics on reasoning-dependent tasks. As shown in Table 4, Unified models substantially outperform specialized image editing models across all visual quality metrics on ROVER-IG. While existing editing models ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 5: Example outputs on ROVER-TG. Each row corresponds to one reasoning scenario, with the input on the left and outputs from representative unified models shown across columns. excel at complex text rendering and precise image editing consistency, they fundamentally lack the internal reasoning capabilities required for our reasoning-dependent visual generation tasks. This performance gap fully demonstrates that ROVER effectively evaluates cross-modal reasoning capabilities essential for visual generation. 4.3. Visually-Augmented Reasoning for Verbal Generation Current unified models exhibit limited capacity in visual reasoning, constraining their ability to leverage visual augmentation for improved performance. The evaluation in Table 3 reveals fundamental limitations in generating meaningful visual reasoning steps, with even the best-performing models achieving only 38.8% average IR quality and open-source models lagging substantially behind. Models with weaker visual reasoning capabilities show minimal or even negative improvements in final accuracy compared to pure text-based reasoning. Flawed visual reasoning proves worse than no visual reasoning at all. Visual reasoning proves effective for physical world tasks but fails systematically on symbolic reasoning. Across all models, visual augmentation consistently improves performance on World Model and Visual Perception tasks, where visual reasoning steps naturally align with physical phenomena. In stark contrast, Logic & Math tasks reveal systematic failures with minimal and unstable improvements, exposing fundamental inability to visually symbolize abstract reasoning. Models struggle to create visual representations that capture symbolic reasoning processes (e.g., auxiliary lines in geometry, Figure 5), as the mapping from symbolic logic to visual form remains poorly developed. 4.4. Further Analyses and Discussions Cross-modal reasoning matters for UMMs. To validate that UMM performs cross-modal reasoning internally and that this mechanism cannot be replicated through external models serving as intermediate 10 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Table 4: Visual performance comparison with image editing models on ROVER-IG benchmark. We evaluate image editing models and unified models, measuring Reasoning Visual (RV), Visual Consistency (VC), and Image Quality (IQ) performance."
        },
        {
            "title": "Nature Science",
            "content": "Culture & Art"
        },
        {
            "title": "Common Sense",
            "content": "Logic & Math RV VC IQ RV VC IQ RV VC IQ RV VC IQ"
        },
        {
            "title": "Overall",
            "content": "Image Editing Models Qwen-Image-Edit (Wu et al., 2025a) FLUX.1 Kontext (Labs et al., 2025) UltraEdit(SD3) (Zhao et al., 2024) VAREdit-8B (Mao et al., 2025) Step1X-Edit v1.1 (Liu et al., 2025a) Step1X-Edit v1.2 (Liu et al., 2025a) 46.7 69.1 89.8 62.5 69.6 95.2 53.1 74.2 94.4 30.4 64.5 87.2 47.1 37.4 61.9 83.5 44.9 64.6 88.8 42.3 62.1 85.0 20.2 50.6 78.2 40.9 27.0 43.6 75.7 45.2 42.6 79.0 27.9 37.3 74.7 25.2 60.1 76.1 34.6 34.6 64.3 75.4 46.5 58.5 78.2 33.6 59.0 75.0 17.4 46.6 57.1 37.5 38.2 75.7 85.5 50.5 62.7 83.8 35.2 67.9 85.3 16.1 61.1 85.9 42.1 46.2 76.8 80.6 50.6 63.0 79.2 46.1 67.2 79.6 18.4 61.1 72.2 57.4 Closed-source Unified Models Nano Banana (Comanici et al., 2025) 77.3 85.7 87.0 76.6 78.4 89.2 74.8 87.1 93.8 55.1 70.3 81.0 79.6 Gemini 2.0 Flash (Comanici et al., 2025) 68.8 72.0 81.1 71.9 65.3 83.2 66.1 76.4 91.2 42.6 68.0 79.3 72.1 71.3 69.9 90.5 72.6 58.8 96.0 65.3 80.9 87.2 45.8 74.9 86.6 74.9 GPT-5 (Hurst et al., 2024) Open-source Unified Models BAGEL-Think (Deng et al., 2025) BAGEL (Deng et al., 2025) 54.0 65.5 78.0 63.7 65.8 71.6 55.9 76.9 80.2 20.8 48.7 76.6 62.9 35.9 53.6 69.9 49.2 50.2 71.9 42.0 59.1 73.0 27.1 59.2 79.8 37.8 Figure 6: Cascade reasoning evaluation across EditWorld and ROVER benchmarks. We compare cascade approaches (FLUX+GPT with GPT-4o prompt refinement) against UMMs. reasoning agents, we conduct comparative analysis between Bagel, FLUX.1 Kontext (Labs et al., 2025) and its GPT-4o-refined variant. Key findings are: (1) UMMs enable superior cross-modal reasoning. The think mechanism consistently improves performance on ROVER, boosting visual consistency by 20.7%. Lower CLIP-I on EditWorld also indicates more substantive edits, underscoring the advantage of UMMs for complex multimodal tasks like ROVER. These results demonstrate that reasoning across modalities cannot fully transfer across different model architectures - unified models must transcend modality boundaries to produce emergent cross-modal insights. ROVER effectively highlights the cross-modal advantages of UMMs. While GPT-4o refinement improves FLUXs CLIP-T score on EditWorld by 1.5%, it simultaneously degrades both visual consistency and quality on ROVER, demonstrating that ROVER provides superior evaluation of cross-modal capabilities in UMMs compared to existing datasets. Do visual reasoning artifacts help? To investigate whether visual reasoning artifacts generated by UMMs can enhance downstream reasoning in VLMs, we conduct controlled study, where visual reasoning outputs from unified models are provided as intermediate steps to assist VLM (Bai et al., 2025) reasoning in Figure 7. Key findings reveal that visual reasoning quality determines its effectiveness for downstream reasoning: (1) 11 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 7: Visual reasoning augmentation evaluation across three problem domains. We compare VLM performance w/ and w/o visual reasoning artifacts from UMMs. UMMs successfully augment VLMs on perceptual tasks. Visual reasoning improves Qwen2.5-VL-7B by +3.5% and +3.8% on physical world modeling and visual perception respectively, where UMMs generate reliable visual intermediates. (2) Low-quality visual reasoning hinders rather than helps. Performance degrades by -1.4% on logical reasoning for VLMs, where UMMs struggle to produce valid abstract visual representations. These results reveal that while UMMs can leverage visual modality to enhance reasoning on concrete, perceptual tasks, they fail to generate meaningful visual abstractions for logic-intensive problems. This underscores core limitation: cross-modal reasoning in UMMs remains constrained by their inability to visually represent abstract concepts, which ROVER effectively exposes. Coherence between reasoning subtasks. Figure 8a reveals uneven performance across reasoning dimensions, with models excelling in temporal, spatial, and causal reasoning while struggling with abstract and mathematical tasks. This pattern indicates that current UMMs better handle concrete, observable phenomena than symbolic reasoning, particularly evident in quantitative tasks where severe counting hallucinations occur. The correlation matrix in Figure 8b shows strong interdependence among physical reasoning types: temporal-spatial, causal-temporal, and synthetic-causal correlations suggest shared mechanisms for processing spatiotemporal relationships. Conversely, abstract reasoning correlates weakly with physical reasoning (0.55 to 0.60) but strongly with mathematical reasoning, indicating it develops as distinct, independent capability from concrete reasoning skills. Reliability of the evaluation protocol. To evaluate the reliability of VLM-as-a-judge scores, we conducted user study with 4 human experts across three models (Nano-banana, GPT-5, BAGEL-Think) with 200 instances per model. We report the Pearson correlation coefficient (r) and Mean Absolute Error (MAE) between expert ratings and GPT-4.1 scores, also compared against Gemini-2.5-Pro evaluations, as shown in Figure 9. The results demonstrate that GPT-4.1 maintains strong alignment with human expert judgments across all evaluation dimensions. Visual-quality-related metrics such as Image Quality show strong humanVLM agreement. Reasoning-related metrics exhibit larger discrepancies due to the inherent hallucination tendencies in VLM when processing complex multimodal reasoning metrics, though these variations remain within acceptable bounds. The modest differences between GPT-4.1 and Gemini-2.5-Pro evaluations suggest reasonable cross-VLM consistency, with limited impact from the choice of VLM evaluator. 12 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation (a) Reasoning subtask performances. Figure 8: Analysis of reasoning capabilities across different models. (b) Reasoning subtask correlation matrix. Figure 9: Evaluation reliability of GPT-4.1 across five assessment dimensions. Left: Pearson correlation coefficients between GPT-4.1 and human experts (green) versus GPT-4.1 and Gemini-2.5-Pro (purple). Right: Mean Absolute Error for the same comparisons. 5. Conclusion We introduce ROVER, the first benchmark for evaluating reciprocal cross-modal reasoning in unified multimodal models. Through systematic evaluation of 17 models across 23 task types, including verbally augmented reasoning for visual generation and visually augmented reasoning for verbal generation, we uncover fundamental capability gaps and performance asymmetries in how current unified models leverage cross-modal reasoning. Our analysis identifies key factors that determine the effectiveness of reasoning in omnimodal generation, providing insights for advancing unified multimodal models in reasoning-dependent visual generation, world modeling, and complex reasoning tasks. We hope ROVER serves the community by informing training paradigms and architectural considerations for future omnimodal model development. 6. Acknowledgment Liang, Wang, and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, DARPA HR001124S0029-AIQ-FP-019, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, National Science Foundation NSF-IIS2147276 FAI, National Science Foundation NAIRR240045, National Science Foundation TRAILS Institute ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation (2229885), Peraton and Open Philanthropy. The USC Geometry, Vision, and Learning Lab acknowledges generous support from Toyota Research Institute, Dolby, Google DeepMind, Capital One, Nvidia, and Qualcomm. Yue Wang is also supported by Powell Research Award and National Science Foundation NSF-CPS-2434460. 14 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation"
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Lawrence Barsalou. Perceptual symbol systems. Behavioral and brain sciences, 22(4):577660, 1999. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, et al. Blip3o-next: Next frontier of native image generation. arXiv preprint arXiv:2510.15857, 2025a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, et al. Do vision-language models have internal world models? towards an atomic evaluation. arXiv preprint arXiv:2506.21876, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1573315744, 2025. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, and Jiangning Zhang. Reasoning to edit: Hypothetical instruction-based image editing with visual reasoning. arXiv preprint arXiv:2507.01908, 2025. 15 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Joy Hsu, Jiayuan Mao, Joshua Tenenbaum, Noah Goodman, and Jiajun Wu. What makes maze look like maze? International Conference on Learning Representations (ICLR), 2025. Yushi Hu et al. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In ICCV, 2023. arXiv:2303.11897. Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, et al. Illume+: Illuminating unified mllm with dual visual tokenization and diffusion refinement. arXiv preprint arXiv:2504.01934, 2025. Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83628371, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Shashank Mohan Jain. Hugging face. In Introduction to transformers for NLP: With the hugging face library and models to solve problems, pages 5167. Springer, 2022. Ying Jin, Pengyang Ling, Xiaoyi Dong, Pan Zhang, Jiaqi Wang, and Dahua Lin. Reasonpix2pix: instruction reasoning dataset for advanced image editing. arXiv preprint arXiv:2405.11190, 2024. George Karypis, Eui-Hong Han, and Vipin Kumar. Chameleon: Hierarchical clustering using dynamic modeling. computer, 32(8):6875, 1999. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. Seungone Kim et al. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023. Yuval Kirstain et al. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas MÃ¼ller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, and Yang Liu. Scaffolding coordinates to promote vision-language coordination in large multi-modal models. arXiv preprint arXiv:2402.12058, 2024. Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025a. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. URL https://arxiv.org/abs/2307.06281. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Zhibin Wang, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, and Rongrong Ji. I2ebench: comprehensive benchmark for instruction-based image editing. Advances in Neural Information Processing Systems, 37:4149441516, 2024. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77397751, 2025. Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, and Tao Mei. Visual autoregressive modeling for instruction-guided image editing. arXiv preprint arXiv:2508.15772, 2025. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision, 2025. URL https://arxiv.org/abs/2508.05606. Neal Roese. Counterfactual thinking. Psychological bulletin, 121(1):133, 1997. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. 17 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. 2023. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Jianshan Zhao, Yang Li, and Qing-Guo Chen. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025. Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024. Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, Chuanxin Tang, Zidong Wang, Yichen Wei, Liang Hu, Boyi Jiang, William Li, Ying He, Yang Liu, Xuchen Song, Eric Li, and Yahui Zhou. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model, 2025. URL https://arxiv.org/abs/2509.04548. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025a. URL https://arxiv.org/abs/2508.02324. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 12966 12977, 2025b. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025c. Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025d. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. URL https://arxiv.org/abs/2306.09341. 18 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, Ming-Hsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025e. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https://proceedings.neurips.cc/paper_ files/paper/2023/file/33646ef0ed554145eab65f6250fab0c9-Paper-Conference.pdf. arXiv:2304.05977. Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan VuliÄ. Visual planning: Lets think only with images. arXiv preprint arXiv:2505.11409, 2025a. Zhiyang Xu, Jiuhai Chen, Zhaojiang Lin, Xichen Pan, Lifu Huang, Tianyi Zhou, Madian Khabsa, Qifan Wang, Di Jin, Michihiro Yasunaga, et al. Pisces: An auto-regressive foundation model for image understanding and generation. arXiv preprint arXiv:2506.10395, 2025b. Zhiyuan Yan, Kaiqing Lin, Zongjian Li, Junyan Ye, Hui Han, Zhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, et al. Can understanding and generation truly benefit togetheror just coexist? arXiv preprint arXiv:2509.09666, 2025. Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785, 2024. Qian Yu et al. Anyedit: Mastering unified high-quality image editing for any idea. In CVPR, 2025a. Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, and Mohit Bansal. Veggie: Instructional editing and reasoning of video concepts with grounded generation. arXiv preprint arXiv:2503.14350, 2025b. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. URL https: //arxiv.org/abs/2308.02490. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. Daoan Zhang, Che Jiang, Ruoshi Xu, Biaoxiang Chen, Zijian Jin, Yutian Lu, Jianguo Zhang, Liang Yong, Jiebo Luo, and Shengda Luo. Worldgenbench: world-knowledge-integrated benchmark for reasoning-driven text-to-image generation. arXiv preprint arXiv:2505.01490, 2025. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. In NeurIPS Datasets and Benchmarks, 2023. 19 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 20 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation A. Extended Related Work Interleaved Reasoning. Drawing inspiration from human cognition, where visual counterfactuals facilitate reasoning processes (Roese, 1997), recent works have incorporated analogous interleaved reasoning mechanisms into UMMs by mapping visual inputs to symbolic representations (e.g., images or bounding boxes) (Wei et al., 2022, Lei et al., 2024). Xu et al. (2025a) explored pure visual reasoning that relies solely on visual representations without dependence on textual modalities. Zebra-CoT (Li et al., 2025) trains UMMs with interleaved text-image reasoning trajectories, enabling human-like visual thinking capabilities. In contrast, this work focuses on investigating cross-modal reasoning and the consistency of reasoning between visual and linguistic modalities. B. Experiment Details B.1. VLM as Judge We employed GPT-4.1 as an automated judge to assess five critical dimensions as mentioned in Section 3. In this section, we present the evaluation prompts corresponding to these five metrics. Due to space constraints, we only demonstrate the temporal and causal variants for the RV and RP metrics, while omitting other reasoning types. These evaluation metrics encompass: (1) Reasoning Process (RP), which evaluates the quality of verbal reasoning through logical structure, domain knowledge application, reasoning typespecific validation, and completeness assessment (Figures 10 and 13); (2) Reasoning Visual (RV), which measures how well the generated visual output aligns with target descriptions and demonstrates correct reasoning principles (Figures 1112 and 1415); (3) Reasoning Alignment (Align.), which quantifies the consistency between verbal reasoning processes and visual generation outcomes, addressing whether models can effectively translate reasoning into visual representations (Figures 1617); (4) Visual Consistency (VC), which ensures that non-target elements remain unchanged during reasoning-guided generation, thereby validating precise control capabilities (Figure 18). B.2. Model Setup"
        },
        {
            "title": "Unified Models",
            "content": "Bagel (Deng et al., 2025) is an open-source multimodal foundation model featuring 7B active parameters (14B total) trained on large-scale interleaved multimodal data. BAGEL demonstrates superior performance compared to current state-of-the-art open-source Vision-Language Models (VLMs) such as Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding benchmarks, while achieving text-to-image generation quality competitive with specialized models such as Stable Diffusion 3. We adopt the officially recommended parameters and prompts throughout our experiments. Specifically, we employ the following system prompts: VLM_THINK_SYSTEM_PROMPT = \"You should first think about the reasoning process in the mind and then provide the user with the answer. The reasoning process is enclosed within <think> </think> tags, i.e. <think> reasoning process here </think> answer here\" GEN_THINK_SYSTEM_PROMPT = \"You should first think about the planning process in the mind and then generate the image. The planning process is enclosed within <think> </think> tags, i.e. <think> planning process here </think> image here\" 21 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation BLIP3o-NEXT (Chen et al., 2025a) is an open-source unified multimodal foundation model with 3B parameters for both image understanding and generation. We adopt the image editing checkpoint (https://huggingface.co/BLIP3o/BLIP3o-NEXT-edit-VAE) and the inference code from the official repository (https://github.com/JiuhaiChen/BLIP3o). Uni-CoT (Qin et al., 2025) is unified chain-of-thought reasoning framework extending Bagel-7B-MoT with 7B active parameters (14B total) and self-reflection mechanism for multimodal reasoning. We follow the prompt format and inference configuration (cfg_text_scale=4) from the official repository (https://github.com/Fr0zenCrane/UniCoT). ILLUME+ (Huang et al., 2025) is 7B unified multimodal model with dual visual tokenization and diffusion-based refinement. We follow the image editing inference code from the official repository (https://github.com/illume-unified-mllm/ILLUME_plus). Emu2-Gen (Sheynin et al., 2024) is generative multimodal model with 37B parameters supporting textto-image generation and image editing through diffusion-based pipeline. We use the official checkpoint (https://huggingface.co/BAAI/Emu2-Gen) for evaluation. UniPic2-Metaquery-9B (Wei et al., 2025) is 9B unified multimodal model built on Qwen2.5-VL-Instruct and SD3.5-Medium using the MetaQuery (Pan et al., 2025) framework. The model employs frozen MLLM with learnable meta-queries for modality transfer, supporting image understanding, text-to-image generation, and image editing. We use the official checkpoint (https://huggingface.co/Skywork/ UniPic2-Metaquery-9B) and inference code (https://github.com/SkyworkAI/UniPic). Ovis-U1 (Wang et al., 2025) is 3B unified multimodal model that integrates multimodal understanding, text-to-image generation, and image editing. We use the official checkpoint (https://huggingface. co/AIDC-AI/Ovis-U1-3B) and image editing test code and settings from https://github.com/ AIDC-AI/Ovis-U1. OmniGen2 (Wu et al., 2025c) is unified multimodal generative model that demonstrates enhanced computational efficiency and modeling capacity. In contrast to its predecessor OmniGen v1, OmniGen2 employs dual-pathway decoding architecture with modality-specific parameters for text and image generation, coupled with decoupled image tokenization mechanism. For experimental evaluation, we utilize fixed temporal offset parameter of 3.0, set the text guidance scale to 5.0 and image guidance scale to 1.5. The negative prompt is configured as \"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar\" All inference procedures employ the default 50-step sampling schedule. Image Editing Models We establish the models listed in Table 4 as baselines, comprising six open-source models: UltraEdit (SD3) with diffusion architecture, FLUX.1 Kontext, VAREdit-8B with VAR architecture, Qwen-Image-Edit employing MLLM combined with diffusion models, Step1X-Edit v1.1, and Step1X-Edit v1.2. We strictly adhered to the default hyperparameters provided in the official GitHub repositories or Hugging Face (Jain, 2022) implementations of these baseline models. In the following descriptions, we enumerate the key parameter configurations: 22 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Qwen-Image-Edit (Wu et al., 2025a): An image editing variant of Qwen-Image that extends the foundational 20B Qwen-Image models distinctive text rendering capabilities to instruction-based image editing tasks, enabling precise textual modifications within images. The architecture incorporates dual-pathway approach where the input image is simultaneously processed through Qwen2.5-VL for semantic understanding and control, and through VAE encoder for visual appearance preservation and manipulation. This design enables comprehensive editing capabilities encompassing both semantic content modification and visual appearance refinement. Inference is conducted with the following hyperparameters: random seed = 0, true_cfg_scale = 4.0, negative_prompt = \"\", and num_inference_steps = 50. FLUX.1 Kontext (Labs et al., 2025): 12 billion parameter rectified flow transformer architecture designed for instruction-guided image editing. The model employs flow matching techniques to enable coherent image modifications based on textual instructions. We utilize guidance_scale = 2.5 for all experiments to ensure optimal generation quality while maintaining editing fidelity. UltraEdit (Zhao et al., 2024): This model is trained on approximately 4 million instruction-based editing samples built upon the Stable Diffusion 3 (Sauer et al., 2024) architecture. It supports both free-form and mask-based input modalities to enhance editing performance. For consistency across all experiments, we exclusively employ its free-form variant. We note that since UltraEdit is trained on the SD3 architecture, its performance metrics may not fully reflect the intrinsic improvements attributable to its specialized editing dataset. We utilize the BleachNick/SD3_UltraEdit_w_mask model variant in free-form editing mode with blank mask initialization. The evaluation is conducted with hyperparameters num_inference_steps=50, image_guidance_scale=1.5, guidance_scale=7.5, and negative_prompt=\"\" to maintain consistency with our experimental protocol. Inference is performed at 512512. VAREdit-8B (Mao et al., 2025): visual autoregressive (VAR) framework for instruction-guided image editing, built upon Infinity (Han et al., 2025). This approach reframes image editing as next-scale prediction problem, achieving precise image modifications through the generation of multi-scale target features. We employ the following hyperparameters: classifier-free guidance scale cfg=3.0, temperature parameter tau=0.1, and random seed seed=42. Step1X-Edit v1.1 (Liu et al., 2025a): Step1X-Edit leverages the image understanding capabilities of multimodal large language models (MLLMs) to parse editing instructions and generate editing tokens, which are subsequently decoded into images using DiT-based network. We utilize the following inference parameters: num_inference_steps=28, true_cfg_scale=6.0, and seed=42. Step1X-Edit v1.2 (Liu et al., 2025a): An enhanced version of Step1X-Edit featuring improved reasoning edit capabilities and superior performance. We employ num_inference_steps=28, true_cfg_scale=4.0, seed=42, enable_thinking_mode=True, and enable_reflection_mode=False. 23 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation B.3. Evaluation Prompt for ROVER-TG"
        },
        {
            "title": "World Model for Embodied Task",
            "content": "SYSTEM_PROMPT = '''You are robotics trajectory planner with visualization capabilities. When given robotics scene, you must: 1. Generate trajectory visualization image: - Overlay 10 waypoint markers on the input scene - Style: Blue circles with white outlines + connecting trajectory lines - Labels: 'traj1', 'traj2', ..., 'traj10' - Reference the example image (Image 2) for exact visualization style 2. Output pixel coordinates based on your visualization: - Format: [[x1, y1], [x2, y2], ..., [x10, y10]] - Coordinate system: (0, 0) at top-left corner Constraints: - Start from current end-effector position - End at task completion position - Generate smooth, collision-free waypoints - All coordinates must be within image boundaries Format your output as: <think> [Your analysis and planning process] </think> Final Answer: [Trajectory coordinates: [[x1, y1], [x2, y2], ..., [x10, y10]]]''' 24 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation"
        },
        {
            "title": "World Model for Physical Task",
            "content": "SYSTEM_PROMPT = '''You are physics simulation AI with image generation capabilities. Given physical scenario, you must: 1. Analyze the initial scene (Image 1): - What objects are present? (cars, balls, liquids, blocks, pulleys, etc.) - What are their initial positions and states? - What forces or motions will be applied? 2. Use Image 2 (if provided): - It may show additional context of the scene 3. Generate physics simulation result image: Simulate what happens when physics is applied and generate the resulting scene: - Apply gravity, momentum, friction, collision dynamics - Simulate the complete physical process (cars moving and colliding, balls falling, liquids flowing, pulleys rotating) - Generate an image showing the final state or outcome after physics simulation - The simulated image should naturally show where objects end up, what gets affected, and what the result is Format your output as: <think> 1. Initial state: [describe the setup] 2. Generate the physics simulation image 3. Analyze the generated image: [what does the simulation show?] 4. Determine answer: [based on the generated image, what is the result?] </think> Final Answer: [exact answer format requested in question]''' 25 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Logic & Math SYSTEM_PROMPT = '''You are helpful AI assistant. You need to think about the given prompt/question and any hints provided, then generate USEFUL VISUAL AIDS based on the hints during your thinking process, and finally answer the question based on your analysis and the generated images. IMPORTANT REQUIREMENTS: 1. You MUST generate images that are USEFUL VISUAL AIDS for solving the problem (e.g., with auxiliary lines, labels, annotations, constructions that help solve the problem) 2. Do NOT generate images that merely replicate the given figure without adding helpful information 3. You MUST provide final answer after your thinking process Enclose your thinking process within <think> </think> tags, generate relevant images during thinking, then provide your final answer. Format your output as: <think> Step 1: Analyze what auxiliary constructions would help solve this problem. Step 2: Generate the visual aid with those constructions. [generate USEFUL images with helpful additions like auxiliary lines, labels, or constructions] Step 3: OBSERVE the generated image carefully and use the visual information to perform your reasoning. Step 4: Based on what you see in the generated image, work through the solution. </think> Final Answer: [your answer based on the generated images and analysis] REMEMBER: The generated images must be USEFUL VISUAL AIDS that add value beyond the original figure.''' 26 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation"
        },
        {
            "title": "Visual Perception for Jigsaw",
            "content": "SYSTEM_PROMPT = '''You are helpful AI assistant solving visual jigsaw puzzles. You need to analyze the puzzle image with gray box covering part of it, then generate completed image by filling in the missing area, and finally select the correct option. IMPORTANT REQUIREMENTS: 1. You MUST first generate completed image by filling in the missing area covered by the gray box 2. Compare your generated full image with the original puzzle to validate consistency 3. Use the generated complete image to determine which option (A, B, C, or D) correctly fills the missing area 4. Do NOT answer before generating the completed image Enclose your thinking process within <think> </think> tags, generate the completed image during thinking, then provide your final answer. Format your output as: <think> Step 1: Analyze what is visible in the puzzle and what patterns/objects might be in the missing area. Step 2: Generate completed image by filling in the missing top part of the puzzle. [Generate the full completed image] Step 3: Carefully observe your generated complete image and compare it with the original puzzle to ensure consistency. Step 4: Compare your generated complete image with each option (A, B, C, D) to find which one matches the missing area in your generated image. </think> Final Answer: [A/B/C/D]''' 27 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Visual Perception for Multi-view Reasoning SYSTEM_PROMPT = '''You are helpful AI assistant analyzing multi-view images to determine camera movement direction. Given two images taken from different camera positions around the same scene, you need to reason about the spatial relationships and determine the camera rotation direction. IMPORTANT REQUIREMENTS: 1. You MUST first generate wider-angle image taken from farther away that includes ALL objects visible in both Image 1 and Image 2 2. This generated image should be like stepping back and using wider lens - showing more of the scene in one frame 3. Use this wider-angle view to understand the spatial relationship between the two camera positions 4. Determine if the camera rotated clockwise (left) or counter-clockwise (right) from Image 1 to Image 2 5. Do NOT answer before generating the wider-angle image Think of it like this: If you step back from the scene and take photo with wider angle lens, you can see all the objects from both viewpoints in one image. Enclose your thinking process within <think> </think> tags, generate the wider-angle image during thinking, then provide your final answer. Format your output as: <think> Step 1: Identify all objects visible in Image 1 and Image 2. Step 2: Generate wider-angle image from farther away that includes all these objects. [Generate the wider-angle image showing the complete scene] Step 3: Use this wider view to understand where the two cameras were positioned. Step 4: Determine the rotation direction: clockwise (left) or counter-clockwise (right)? </think> Final Answer: [left/right]''' ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 10: Prompt used for evaluating the reasoning process of temporal (RP). 29 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 11: Prompt template for evaluating visual-temporal reasoning capabilities (RV). (Continued in Figure 12) ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 12: Prompt template for evaluating visual-temporal reasoning capabilities (RV). (Continued from Figure 11) 31 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 13: Prompt template for evaluating process of causal reasoning capabilities (RP). ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 14: Prompt template for evaluating visual causal reasoning capabilities (RV). (Continued in Figure 15) 33 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 15: Prompt template for evaluating visual causal reasoning capabilities (RV). (Continued from Figure 14) ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 16: Prompt template for evaluating reasoning alignment capabilities (Align.). (Continued in Figure 17) 35 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 17: Prompt template for evaluating reasoning alignment capabilities (Align.). (Continued from Figure 16) ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 18: Prompt template for evaluating visual consistency (VC.). (Continued from Figure 18) 37 ROVER : Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation Figure 19: Prompt template for evaluating image quality (IQ.). (Continued from Figure 19)"
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology",
        "University of Maryland, College Park",
        "University of Michigan",
        "University of Pennsylvania",
        "University of Southern California"
    ]
}