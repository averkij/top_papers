{
    "paper_title": "dKV-Cache: The Cache for Diffusion Language Models",
    "authors": [
        "Xinyin Ma",
        "Runpeng Yu",
        "Gongfan Fang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 1 8 7 5 1 . 5 0 5 2 : r dKV-Cache: The Cache for Diffusion Language Models Xinyin Ma Runpeng Yu Gongfan Fang Xinchao Wang National University of Singapore maxinyin@u.nus.edu, xinchao@nus.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) have been seen as promising competitor for autoregressive language models (ARs). However, diffusion language models have long been constrained by slow inference. core challenge is that their non-autoregressive architecture and bidirectional attention preclude the keyvalue cache that accelerates decoding. We address this bottleneck by proposing KVcache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKVCache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10 speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in training-free manner from current DLMs. The code is available at https://github.com/horseee/dKV-Cache"
        },
        {
            "title": "Introduction",
            "content": "Diffusion language models (DLMs) [3, 25] have recently emerged as an alternative paradigm for text generation, inspired by the success of diffusion models in continuous domains like images [15, 48] and videos [28, 5]. Unlike autoregressive transformers (ARs) [14, 43, 12] that generate text leftto-right at time, diffusion language model produces text by gradually refining sequence of initially noisy tokens or masked tokens into coherent output [24, 29]. Recent advancements in diffusion-based language models have underscored their versatility across both continuous [21] and discrete formulations [40, 3]. In particular, discrete diffusion models have shown competitive performance in language modeling [37], attracting growing interest for their potential to achieve faster decoding than traditional ARs while maintaining comparable generation quality. One notable advantage of diffusion language models is their potential to decode an arbitrary number of tokens in parallel [49], whereas ARs require one forward pass per generated token. This parallel decoding paradigm offers the potential for improved wall-clock inference time and higher throughput. However, despite their theoretical efficiency, current DLMs remain substantially slower than ARs in practice. This inefficiency is primarily due to two factors: the incompatibility of DLMs with the Corresponding author Preprint. Under review. KV-Cache mechanism [33, 13], and the large number of network evaluations for denoising [17]. Specifically, generating sequence of length typically entails denoising steps, each involving full bidirectional attention pass, leading to cubic time complexity of O(L3). In contrast, AR models utilize KV-Cache to reduce per-step complexity to O(L2), achieving much faster inference overall. In this paper, we tackle the challenge of integrating the KV-Cache mechanism into diffusion language models and operate without autoregressive or semi-autoregressive structures [2]. We identify two core reasons that prevent the direct usage of KV-Cache in DLMs. (1) KV-Cache hinges on the assumption that the key and value states of previously generated tokens remain fixed during subsequent decoding steps. This property is preserved in autoregressive models through the use of causal attention mask, which restricts each token to attend only to earlier positions [11]. However, DLMs adopt bidirectional attention mechanism, similar to non-autoregressive models [20], allowing every token to attend to all others in the sequence and making it nontrivial to reuse previously cached keys and values. (2) KV-Cache presupposes fixed and left-to-right sequential decoding, where the position of the next token is deterministic. This assumption enables ARs to compute QKV states selectively only at the current decoding position. However, DLMs break this paradigm by supporting flexible generation orders. At each denoising step, any token position may be selected for update, which is key advantage of DLMs for tasks involving long-range dependencies and holistic planning [51]. To solve this problem, we propose the delayed KV-Cache, dKV-Cache, the KV-Cache for diffusion language models. The core design of our proposed dKV-Cache centers on how to enable caching of key and value states across denoising steps in diffusion language models. key insight motivating this design is that, although DLMs employ bidirectional attention, intuitively incompatible with caching, the representations of key and value are not fundamentally unreusable, but rather require delayed and conditioned reuse. In particular, we observe that the evolution of key and value states is strongly influenced by whether token has been decoded. This behavior motivates delayed caching strategy, wherein only the key and value states of decoded tokens would be cached, delayed from the ARs that caching occurs immediately upon input. We further propose one-step delayed caching mechanism, in which the caching of key/value states is postponed by another denoising step. This intentional delay substantially boosts the performance and also reduces memory overhead for KV storage. Besides the delayed caching, we further propose more aggressive decoding strategy to reduce the computational complexity of diffusion language models from O(L3) to O(L2) by restricting the caching to compact subset of tokens: the delayed tokens and the current to-be-decoded tokens, and the window tokens. Our experiments demonstrate that the proposed method achieves 210 speedup on existing 7Bscale diffusion language models, including LLaDA [37] and Dream [52], across broad range of benchmarks such as general language understanding, code generation, and mathematical problem solving. These efficiency gains come with only minor and often negligible performance degradation, highlighting the practical value of our approach for accelerating DLM inference without training. Furthermore, we demonstrate that dKV-Cache is robust across variations in prefill length, output length, and the number of sampling steps. Contributions. (1) We propose the first KV-Cache mechanism for diffusion language models by leveraging the evolving dynamics of token representations. We introduce delay caching strategy compatible with bidirectional attention. (2) We propose two practical variants of our method: dKVCache-Decode, which enables long-term cache reuse, and dKV-Cache-Greedy, which reduces the per-step time complexity for faster decoding. (3) Extensive experiments on DLMs demonstrate that our approach achieves 210 inference speedup with minimal or negligible performance loss."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion Language Models Diffusion models [24, 41] model the data generation as the inversion of forward-noise process and demonstrated impressive generation quality in image [38], video [5], and audio generation [16]. For diffusion models on language generation, [25, 3] extends DDPM to categorical data and defines the transition matrices for the corruption and denoising. [29] introduces the continuous-time diffusion over the continuous word-embedding space, and [21] closes the quality of generation on par with GPT2 via simplex defined over the vocabulary. Besides the diffusion in the continuous space [47, 53] 2 for discrete distribution, another approach seeks the path in discrete language diffusion models. [22] training BERT to learn the reverse process of discrete diffusion process with an absorbing state in D3PM. SEDD [33] introduces score entropy training, novel loss extending score-matching to discrete data and MDLM [40] shows that simple masked discrete diffusion is competitive to all previous kinds of DLMs. Block diffusion [2] extends the current non-autogressive [20] discrete language diffusion models into semi-autoregressive one [21, 55], making it feasible to generate sequences of arbitrary length. [37, 19] scaling the masked diffusion language models to billions of parameters, achieving performance comparable to leading autoregressive LLMs. 2.2 Cache in Generative Models Cache [45] is small, fast memory that stores frequently accessed data, reducing the time that the CPU needs to fetch data from slower memory. Cache is first introduced in deep neural networks in transformers [44], where the KV-Cache caches previous tokens key and value tensors. KVCache becomes fundamental technique in transformers, and several improved techniques are proposed [18, 32, 26] to reduce the memory consumption of KV-Cache for long-context generation. Besides this, caches are also been explored in diffusion models for image [36, 46]. Those work leverage the temporal similarities between high-level features [35, 9], attention map [54, 31] to achieve faster inference of diffusion generation. This also has been explored in 3D generative modeling [50] and video generation [56, 34]. However, the cache for diffusion language models is less explored, especially the KV-Cache for diffusion language models. [2] explores the kvcache in semi-autoregressive diffusion language models. This requires considering the KV-Cache in training, making twice the forward computation in the training and its form is still constrained in the autoregressive formula. [40] also considers cache, but under strict condition that no new tokens have been calculated."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Preliminary We primarily focus on continuous-time discrete language models, with particular attention to masked diffusion language models [40, 37], which have shown strong scalability to billion-parameter scales with high generation quality. Consider text sequence with tokens x1:L , sampled from the target distribution pdata(x0). Each token is represented by one-hot vector with categories, where is the vocabulary size. The forward process adds noise in the original sequence x0, which, in the discrete diffusion models here, takes the form of masking some of the tokens randomly. The masking process can be controlled by transition matrix t, where each element in this matrix [U t]ij represents the probability transition from token to token at step [3]. The denoising process is discretized into steps, and we define the continuous timestep c(t) = t/T , where {0, 1, . . . , }. We use timestep in the continuous temporal space of the denoising process and step in the discrete space. The forward diffusion can be modelled as: 0 (cid:0)xc(t) x0 (cid:1) = Cat (cid:0)xc(t); = x0U (cid:1) , where = (cid:89) i=1 Then we can get the corrupted x0. The absorbing form for the transition matrix is used here, where each token either remains the same or transfers to the special [MASK] token at the probability of βt. The cumulated transition matrix t, as defined in the masked diffusion models, can be formulated as: (cid:2)U (cid:3) ij = (cid:40) 1 αt 1 αt if if if = = [MASK] = = [MASK] = m, = [MASK] with αt = (cid:89) i= (1 βi) where αt linearly decrease to 0 as approaches T. The reverse process is learned by the models θ, where pθ(xc(t1)xc(t)) is learned to approximate q(xc(t1)xc(t), x0). In pθ(xc(t1)xc(t)), the neural network with parameters θ is optimized to predict the clean tokens x0 given xc(t). 3 Figure 1: Illustration of dKV-Cache. At step t, no prior cache would be activated though the token Dt1 has been decoded. and are delayed to the next step to be reordered and reused. Sampling Process of DLMs. Given the noisy sequence x1:L , which is consisted only with the masked token. Each timestep, the denoising model pθ(xc(t1)xc(t)) would be called, with x0 predict first and then remasking the rest by q(xc(t1)x0, xc(t)). The unmasked tokens would remain unchanged during the later denoising process. Several strategies are used in the remasking stage, e.g., random remasking [3], keeping the most confident ones [7] or by selecting the topk positions with the largest margin between the two most probable values [27]. Formulation of KV-Cache. Since diffusion language models still use the transformer architecture (with or without GQA [1] would not affect our method), we simply grab the formulation of KV-Cache in ARs first. In transformer decoder, each layer would project the current hidden states ht into query-key-value triplet (Qt, Kt, Vt) via learned projection WQ, WK and WV. At step t, only the hidden states of the t-th tokens h[t] would be calculated. The recursive KV-Cache update is appending the new keyvalue pair to the running buffered KV-Cache: zt = softmax Q[t] (cid:16) K[1:t] dk (cid:17) V[1:t] with K[1:t] V[1:t] = concat = concat (cid:16) (cid:16) K[1:t1] t1 V[1:t1] (cid:17) (cid:17) , K[t] , V[t] where zt is the output of the attention head at step and the dot products are scaled down by (1) dk. 3.2 Why KV-Cache Cannot be Used in DLMs? The effectiveness of KV-Cache can be attributed to the reuse of previously computed and states, and the targeted computation only for the current decoding token. We conclude that standard KV-Cache is fundamentally incompatible with diffusion language models for two reasons: Timestep-variant key and value states. In the autoregressive setting, every time step shares single, causally growing set of key and value tensors. K[1:t1] are the same at each step from 1 and later with the help of causal attention mask. By contrast, DLMs employ bidirectional attention mask; consequently, the key and value representations that each token can attend to at timestep m, K[1:t1] , differ from those at timestep n. Put differently, K[1:t1] = V[1:t1] if = m. The bidirectional attention introduced by diffusion language models therefore breaks the global reuse assumption that supports conventional KV-Cache. and V[1:t1] and V[1:t1] = K[1:t1] and V[1:t1] m Non-sequential decoding order. Generation in DLMs does not follow strictly left-toright order. Instead, the model dynamically fills masked positions based on probabilities computed at each denoising step. As result, the positions of decoded tokens are only revealed after the model forward pass, and the subsequent update may target any position in the sequence, rather than progressing sequentially. This uncertainty prevents us from pre-determining which token will require the computation of its hidden states h[i] and its Q[i], K[i] and V[i]. 4 Figure 2: (a) We present heatmap illustrating the pairwise similarities among the key states across different timesteps. Here we use LLaDA with = 128, = 128 and the block size is set to 64. We then compute the Euclidean distance between consecutive steps and + 1 to analyze the dynamics of intermediate representations. (b) We report the average distance measured before and after the decoding of each token. (c) We highlight the top-2 steps exhibiting the largest and smallest changes in the key and value states for each token in their decoded order. Representation Dynamics for tokens in Diffusion Sampling We investigate in DLMs whether and can be reused. We focus on the dynamics of and for each token, and the results are shown in Figure 2. Interestingly, we observe several noteworthy patterns in the dynamics of QKV states: (1) Despite step-to-step differences, the key and value embeddings, and V, exhibit consistently high similarity across timesteps, as shown in Figure 2(a). (2) Once token is decoded, its representation becomes relatively stable in subsequent steps, whereas the representations of still-masked tokens continue to fluctuate significantly. This phenomenon is evident in Figure 2(b), where QKV fluctuations are more pronounced prior to decoding than thereafter. (3) The most substantial changes in and occur at the decoding step of each token and then in the early stages of the denoising process. This is reflected by prominent changes along the diagonal in Figure 2(c), corresponding to the i-th decoding step which decodes the i-th token. These observations provide key insights into the temporal structure of discrete diffusion models and motivate the design of our KV-Cache mechanism for diffusion language modeling. 3.3 Delayed KV-Cache for Masked Diffusion Language Models We first present more general non-sequential KV-Cache formulation that replaces the contiguous slice K[1:t1] used in Eq.1 with an arbitrary-order index set St = {1, . . . , L} and the next token position from the fixed to Dt. The cached keys and values gathered from previous steps are now KSt , which retrieves cached states at the positions specified by indexes in St: zt = softmax QDt (cid:17) (cid:16) KSt{Dt} dk VSt{Dt} (cid:40) with KSt{t} VSt{t} = concat_reorder (cid:0)KSt = concat_reorder (cid:0)VSt t1, KDt t1, VDt (cid:1) (cid:1) (2) where Dt is the denoised token at step t. If we use random sampling for diffusion, then before any inference, we can know the decoding order of the sequence. We here use the notation of Dt, and later would not depend on knowing the decoding order. The operator concat_reorder is proposed to enhance the efficiency of the indexing and gathering of and here. We explain in the appendix about this operator and how it works with ROPE [42] and how it accelerates. We first extend the formulation in Eq.2 by generalizing the query input QDt from the single-token decoding to the multiple and arbitrary token decoding. Specifically, at decoding step t, we construct dynamic set Mt, where Mt denotes the subset of tokens that are not yet finalized during generation. And with the hMt . We delay the caching of each token, not the time it is appended in the input, but rather on the step it is decoded: calcuated, we also can get the corresponding QMt and VMt , KMt t concat_reorder ( KSt t1 (cid:124) (cid:123)(cid:122) (cid:125) Cache from t1 , KDt (cid:124)(cid:123)(cid:122)(cid:125) Calcualte at ) concat_reorder ( KIMt t1 (cid:124) (cid:123)(cid:122) (cid:125) Cache from t1 , KMt (cid:124) (cid:123)(cid:122) (cid:125) Calculate at ) (3) where denotes the set of all tokens involved in the denoising process. This design reflects our core observation in the previous section: only the decoded tokens are eligible for caching, while 5 the remaining masked tokens must be re-encoded at each step. Besides, this method solves the problem that we need to predefine or predict the denoising order, as Mt is explicitly known at each step. Cached keys and values corresponding to Mt are reused across steps, while non-finalized positions are recomputed at each step to ensure correctness under bidirectional attention masking. One-step Delayed Caching. As our analysis in Figure 2(c), the most significant change in and occurs exactly at the step where token transitions from [MASK] to its decoded form. Currently, KDt would be used for caching, as it is no longer in the masked set Mt. However, KDt can differ substantially from KDt t+1, and prematurely reusing it lead to severe performance degradation. To address this, we introduce one-step delayed caching. At timestep t, we use the masking state from the previous step, Mt1, to determine which tokens are cacheable. The method, named dKV-Cache-Decode, is finally formalized as: zt = softmax (cid:32) QMt1 (cid:0)KI dk (cid:1) (cid:33) VI with KI = concat_reorder VI = concat_reorder (cid:16) (cid:16) KIMt1 t1 VIMt1 t1 (cid:17) (cid:17) , KMt1 , VMt1 (4) While this slightly reduces efficiency, we find it to be critical for maintaining accuracy and stability in the proposed dKV-Cache mechanism for diffusion language models. Cache Refreshing Mechanism. While it is possible to apply caching after each token is decoded and reuse it throughout the denoising process, in practice, when the sequence is sufficiently long, occasionally recomputing the cache incurs small computational overhead. To maintain consistency and improve correctness during decoding, we add cache refreshing mechanism. Every steps, the stored cache would be discarded and refreshed. The calculation would revert back to the normal calculation, resulting in an empty set to replace Mt1 for this refresh step in Eq.4. dKV-Cache-Prefill and dKV-Cache-PD. The set Mt1 can be further divided into two subsets: decoded tokens and always-decoded tokens, i.e., prefill tokens. Our experiments show that prefill tokens primarily attend to each other, indicating limited influence from later tokens. Based on this, we adopt special strategy, dKV-Cache-Prefill, that caches prefill tokens without refreshing. This design aligns with the disaggregation of prefill and decoding phases [57] for serving DLMs. Building on this, we have another variant, dKV-Cache-PD, which intermittently refreshes only the newly decoded tokens, while keeping the key and values of prefill tokens without any recomputation. 3.4 dKV-Cache-Greedy: Greedy Formulation of dKV-Cache However, the above method still incurs O(L3) complexity, which is less efficient than the O(L2) complexity of ARs. This is primarily because Mt initially consists of the entire sequence with tokens and only narrows to single token at the end. To improve efficiency, it is essential to decouple Mt for each step from the sequence length L. Instead of the minimally refreshed caches proposed above, we adopt more relaxed cache mechanism to refresh more so as to mitigate the performance degradation caused by stale and V. Building on our earlier observation that token representations undergo significant changes at their decoding step, we define Mt to include only three components: the token at the current step Dt, the token from the previous step D(t 1) (motivated by one-step delayed caching), and local window W(t). For this local window, we extend it to include the token itself and its neighboring tokens within fixed-size window Wt = (cid:8)xi (cid:5)(cid:3) (cid:9), where is the window size. We evaluated local windows centered at both Dt and Dt1, and found that the latter yields better performance. Since the window size Wt is fixed (set to at most 6 in our experiments), this strategy introduces additional computation but retains an overall time complexity of O(L2). (cid:12) (cid:12) (cid:2)Dt (cid:6) (cid:7) , Dt + (cid:4)"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We tested our method under the original evaluation benchmark of LLaDA [37] and Dream [52]. Datasets: We conduct comprehensive evaluations across diverse set of benchmarks that assess 6 Table 1: Benchmark results on LLaDA-8B-Instruct. We use zero-shot evaluation here. Detailed configuration is listed in the Appendix. We set the cache refresh step for dKV-Cache-Decode to be 8 and dKV-Cache-Greedy to be 2. The window size of dKV-Cache-Greedy is listed in the bracket. Remasking MMLU GSM8K Math500 GPQA HumanEval MBPP Base (random) Few-Steps (random) dKV-Cache-Greedy (random) dKV-Cache-Greedy w/ Window (random) Base (confidence) Half-Steps (confidence) dKV-Cache-Decode (confidence) 51.79 30.20 72.25 15.16 27.4 12.00 27.46 11.40 19.88 7.50 21.4 7. 43.19 47.49 (1.67) 65.58 24.08 (1.59) 21.8 19.36 (1.61) 24.78 18.59 (1.63) 15.61 12.50 (1.67) 15.6 12.97 (1.73) 45.77 50.56 (1.57) 67.93 25.47 (1.68) 26.0 20.34 (1.70) 26.79 19.27 (1.69) 15.13 12.31 (1.64) 17.8 12.55 (1.67) 47.70 (4) 45.72 (1.51) 68.23 (4) 24.76 (1.63) 27.0 (4) 19.86 (1.66) 28.35 (4) 18.26 (1.60) 15.37 (4) 12.13 (1.62) 20.4 (2) 12.44 (1.66) 51.11 28.27 77.56 14.31 36.6 11.53 30.80 11.86 39.63 7.08 40.4 7. 51.11 55.80 (1.97) 77.91 28.71 (2.00) 34.2 23.10 (2.00) 27.68 23.88 (2.01) 33.54 14.18 (2.00) 33.8 15.01 (2.00) 51.00 66.52 (2.35) 78.85 27.50 (1.92) 36.8 24.46 (2.12) 28.13 28.73 (2.42) 46.34 13.76 (1.83) 40.4 13.93 (1.86) Table 2: Benchmark results on Dream-Base-7B. We use the few-shot ICL here and the configuration is in Appendix. We set the cache refresh interval for dKV-Cache-Decode and dKV-Cache-PD to 4. dKV-Cache-PD dKV-Cache-Decode dKV-Cache-Prefill Half-Steps Dream-7B GSM8K (8-shot) = MBPP (3-shot) = 512 HumanEval (0-shot) = 512 = 256 = 128 = 512 = = 512 = 256 76.88 15.1 (1.00) 68.08 30.3 (2.00) 68.81 30.3 (2.01) 46.63 60.5 (4.01) 55.8 5.4 (1.00) 45.2 10.8 (2.00) 45.2 10.8 (2.00) 26.2 21.5 (3.98) 57.93 10.3 (1.00) 37.20 20.5 (1.99) 37.20 20.5 (1.99) 18.29 40.9 (3.97) 76.57 31.6 (2.09) 65.35 62.31 (4.13) 53.4 10.4 (1.93) 43.4 20.6 (3.81) 57.32 15.5 (1.50) 31.70 31.1 (3.02) 75.66 53.6 (3.55) 65.96 107.4 (7.11) 55.2 13.6 (2.52) 41.8 27.1 (5.02) 56.10 14.4 (1.40) 33.54 28.7 (2.79) 74.07 50.2 (3.32) 63.31 99.5 (6.6) 51.0 14.5 (2.69) 42.6 28.9 (5.35 ) 59.76 17.4 (1.69) 31.70 34.8 (3.38) general language understanding [23], mathematical reasoning [10, 30, 39], and code generation [8, 4]. Since the multi-choice evaluation based on the token likelihood doesnt require more than one step for inference, we request the models to generate the answer letter and match the generated answer with the ground-truth answer. Evaluation: We follow the prompt in simple-evals2 for LLaDA, making the model reason step by step. On Dream, we follow the evaluation setting of Dream to conduct few-shot in-context learning [6]3. Other implementation details are listed in the Appendix. Baseline: We choose the few-step sampling method (50% steps for Half-Steps and 62.5% steps for Few-Steps) as our baseline and select the number of steps such that their sampling speed is comparable to or slower than ours, and showing that our method can have better performance. Metric. We report the accuracy for performance and token/s for speed. We tested the speed on A6000 (for LLaDA) and H20 (for Dream). Besides this, we use one more metric to show the cache (cid:12) ratio, calculated as: 1 (cid:12)/Ti, where is the total number of decoding steps. Ti denotes the number of tokens processed at step (normally the whole sequence) and cache is the subset of tokens whose KV pairs are reused from cache at timestep i, which is Mi. (cid:12) (cid:12)T cache (cid:80)T i=1 4.2 Performance and Speed with dKV-Cache We begin by addressing the central question: What are the performance trade-offs and speedups introduced by applying dKV-Cache? For LLaDA, we evaluate two variants, dKV-Cache-Greedy and dKV-Cache-Decode, against baselines that accelerate generation by halving or reducing the number of denoising steps. As shown in Table 1, dKV-Cache-Greedy consistently outperforms few-step 2https://github.com/openai/simple-evals 3https://github.com/EleutherAI/lm-evaluation-harness Table 3: Results on long prefill settings. dKV-Cache-Decode uses refresh step of 4; dKV-Cache-Prefill never refreshes. Dream-7B Half-Steps dKV-Cache-Decode dKV-Cache-Prefill MMLU (5-shot) = = 8 = 4 GPQA (5-shot) = 128 = 128 = 64 72.19 9.1 (1.00) 72.21 18.1 (1.99) 36.83 7.4 (1.00) 35.49 14.7 (1.99) 72.21 18.1 (1.99) 71.63 36.1 (3.97) 35.49 14.7 (1.99) 35.27 29.4 (3.97) 71.74 25.2 (2.77) 71.69 49.2 (5.41) 35.71 18.2 (2.46) 34.15 36.8 (4.97) 71.76 57.6 (6.33) 71.71 67.3 (7.40) 35.27 75.40 (10.19 ) 35.27 139.9 (18.91) Figure 3: Effect of onestep delayed caching baselines across most benchmarks, except for HumanEval. Notably, integrating lightweight cache window yields substantial gains with negligible computational overhead. For dKV-Cache-Decode, dKV-Cache-Decode achieves near-lossless performance with high cache ratio and only few refresh steps. Among all strategies, dKV-Cache-Decode delivers the best trade-off, outperforming both dKV-Cache-Greedy and the baselines. Crucially, it maintains accuracy nearly indistinguishable from the full model, demonstrating that KV-Cache can also be applied in diffusion language models without sacrificing performance. Since dKV-Cache-Greedy relies on predefined (e.g., random) decoding order, which results in marked accuracy drop relative to low-confidence remasking, we concentrate our experiments more on dKV-Cache-Decode. We provide several case studies in the Appendix that compare the generated text before and after applying dKV-Cache. The results on Dream are shown in Table 2 and Table 3. There is small difference in the position of the decoded token since Dream is adapted from auto-regressive models and shifts the token position. We provide detailed illustration in the appendix for this. Due to the use of few-shot in-context learning, the model requires long input context, leading to significant overhead from encoding those tokens repeatedly. In this setting, dKV-Cache-Prefill provides substantial speed improvements; for instance, on MMLU and GPQA, it achieves up to 10 acceleration. Across all tested datasets, we observe that dKV-Cache largely outperforms the baseline under different prefilling and decoding lengths. We further evaluate the impact of applying dKV-Cache to few-step diffusion models and observe consistent trends: as the number of diffusion steps increases, our method yields even larger gains over the baseline. For example, on GSM8K with decoding length of 256, the baseline model with 64 steps achieves 46.63 Pass@1 with 4 speedup, whereas dKV-Cache attains 6.6 speedup while significantly improving performance to 63.31 (+16.68). 4.3 Analysis The one-step delay in dKV-Cache. Figure 3 illustrates the impact of applying one-step delay to the cache mechanism. Without the delay, performance remains acceptable at low cache ratios but degrades rapidly as the cache ratio increases, ultimately collapsing to near-zero accuracy. In contrast, introducing one-step delay stabilizes the generation quality, enabling the model to maintain nearly lossless performance even under high cache ratios. Performance on different decoding length, denoising steps and refreshing steps. Figure 4 presents the results of applying dKV-Cache-Decode and dKV-Cache-Greedy under various configurations, including different decoding lengths, refresh intervals, sampling steps, and window sizes. Overall, our method consistently achieves performance comparable to the original model without dKV-Cache, effectively pushing the Pareto front forward. We observe the following findings: (1) Decoding robustness: The performance impact of our method is largely insensitive to the number of decoding steps, indicating strong robustness across varying generation lengths and sampling steps. (2) Enhanced long-form generation: In tasks involving longer outputs (e.g., = 512), our method outperforms the baseline, even improving generation quality from 80.97% to 83.13% on GSM8K and from 39.63% to 46.34% for HumanEval. The results imply potential inefficiency in how bidirectional attention aggregates contextual signals, pointing to redundancy or underutilization in long-context modeling. (3) Effectiveness with only few refreshing: Even with infrequent refreshes (e.g., every 16 steps), the performance degradation remains small. (4) Effectiveness of local windows: 8 Figure 4: dKV-Cache-Decode(left) and dKV-Cache-Greedy(right) on GSM8K with different settings: decoding length L, sampling steps S, refresh intervals and the window size. Figure 5: Speed and memory for dKV-Cache-Decode and dKV-Cache-Greedy. The number (2 and 8) means that in every steps, the cache would be refreshed. Incorporating local window notably enhances the performance of dKV-Cache-Greedy with minimal additional computational cost. Memory and speed analysis. We analyze the speed and memory footprint of dKV-Cache-Decode and dKV-Cache-Greedy across varying decoding and prefill lengths. Our method achieves substantial inference acceleration, ranging from 1.75 to 3.3, while introducing only modest increase in memory usage. Notably, dKV-Cache-Greedy demonstrates greater potential for accelerating inference while dKV-Cache-Decode would be capped. In our main experiments, we observed that setting the refresh interval larger than 2 for dKV-Cache-Greedy may degrade performance. However, under the same refresh interval, dKV-Cache-Greedy consistently achieves higher speedups than dKV-Cache-Decode, highlighting its potential advantage when refresh frequency is relaxed."
        },
        {
            "title": "5 Conclusions and Limitations",
            "content": "In this work, we explore the feasibility of incorporating the caching mechanism into diffusion language models. Specifically, we propose delayed KV-Cache for DLMs, motivated by our empirical observations on the dynamics of the token representations throughout the diffusion process. Building on this insight, we introduce two cache variants, dKV-Cache-Decode and dKV-CacheGreedy, each designed to leverage delayed caching for improved compatibility with diffusion-based generation. Our analysis reveals that introducing delay is crucial for the cache to function effectively in this setting. Extensive experiments demonstrate that our approach largely accelerates inference while maintaining model performance. One primary limitation of this work lies in its focus on algorithmic design in isolation. While our proposed method introduces an effective caching mechanism from purely algorithmic perspective, diffusion language models also exhibit substantial room for improvement at the system level. We believe that future research integrating algorithmic innovations with system-level optimization, such as memory management, parallelism, and hardware-aware execution, could unlock further efficiency gains and performance improvements for DLMs."
        },
        {
            "title": "References",
            "content": "[1] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [2] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [3] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. [9] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: training-free acceleration method tailored for diffusion transformers, 2024. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [11] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. ArXiv, abs/1901.02860, 2019. [12] DeepSeek-AI. Deepseek-v3 technical report. ArXiv, abs/2412.19437, 2024. [13] Justin Deschenaux and Caglar Gulcehre. Promises, outlooks and challenges of diffusion language modeling. arXiv preprint arXiv:2406.11473, 2024. [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, and Ava Spataru et al. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [16] Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. ArXiv, abs/2402.04825, 2024. [17] Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, and Di He. Theoretical benefit and limitation of diffusion language model. ArXiv, abs/2502.09622, 2025. [18] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. In The Twelfth International Conference on Learning Representations, 2024. 10 [19] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. [20] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017. [21] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022. [22] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022. [23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [25] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in neural information processing systems, 34:1245412465, 2021. [26] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37:12701303, 2024. [27] Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. [28] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [29] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. [30] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [31] Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan-Manuel Perez-Rua, and Jürgen Schmidhuber. Faster diffusion via temporal attention decomposition. arXiv preprint arXiv:2404.02747, 2024. [32] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36:5234252364, 2023. [33] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. [34] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee Wong. Fastercache: Training-free video diffusion model acceleration with high quality. arXiv preprint arXiv:2410.19355, 2024. [35] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. Advances in Neural Information Processing Systems, 37:133282133304, 2024. [36] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXiv preprint arXiv:2312.00858, 2023. [37] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [39] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. 11 [40] Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. [41] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [43] Qwen Team. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [45] Maurice Wilkes. Slave memories and dynamic storage allocation. IEEE Transactions on Electronic Computers, (2):270271, 2006. [46] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. arXiv preprint arXiv:2312.03209, 2023. [47] Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36:3995739974, 2023. [48] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. [49] Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, and Arash Vahdat. Energy-based diffusion language models for text generation. In The Thirteenth International Conference on Learning Representations, 2025. [50] Xingyi Yang and Xinchao Wang. Hash3d: Training-free acceleration for 3d generation. arXiv preprint arXiv:2404.06091, 2024. [51] Jiacheng Ye, Zhenyu Wu, Jiahui Gao, Zhiyong Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Implicit search via discrete diffusion: study on chess. ArXiv, abs/2502.19805, 2025. [52] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. [53] Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Mingxuan Wang. Dinoiser: Diffused conditional sequence learning by manipulating noises. arXiv preprint arXiv:2302.10025, 2023. [54] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. Advances in Neural Information Processing Systems, 37:11961219, 2024. [55] Lingxiao Zhao, Xueying Ding, and Leman Akoglu. Pard: Permutation-invariant autoregressive diffusion for graph generation. arXiv preprint arXiv:2402.03687, 2024. [56] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. [57] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving. In USENIX Symposium on Operating Systems Design and Implementation, 2024. Design for concat_reorder concat_reorder is our implementation of dKV-Cache designed to improve the speed of dKV-Cache in diffusion language models. Unlike standard KV-Cache used in autoregressive models, dKV-Cache requires gathering and scattering keys and values from arbitrary positions, introducing indexing operations that are less efficient than the simple concatenation in contiguous space used in ARs. In dKV-Cache, the cache process involves two additional indexing operations: (1) At the cache step: After computing keys and values, we need to gather the corresponding states of cached tokens at non-continuous positions. (2) At the reuse step: to obtain the whole matrices for key and value, we need to scatter these vectors back to their original positions in the sequence. In contrast, KV-Cache in ARs only requires matrix slicing and concatenation, making it significantly more efficient. To minimize the overhead of gathering and scattering, we propose an algorithm similar to that of standard KV-Cache to avoid too many indexing operations. The key idea is to reorder token positions during the forward calculation of the Transformer, placing all cached tokens contiguously on one side (e.g., left) and newly decoded tokens on the other. This allows us to move parts of the indexing operation to the token level (matrices with shape [B, L]) instead of the intermediate states (matrices with shape [B, L, D]): At Step t-1: Gather the cached key KIMt1 and value states VIMt based on the t1 t1 position index Mt1 with one indexing operation. At Step t: Reorder the sequence, making the cached tokens (at position Mt1) at the left, and uncached tokens (at position Mt1) at the right. (cid:16) At Step t: Using concat_reorder for KIMt1 : t1 First, concatenate the cached and current key/value states directly without further gathering/scattering (concat, for getting all and to calculate attention), and reorder the whole KV matrics based on VIMt to get the cached states for the next step (reorder, for obtaining the cache). VIMt1 t1 , KMt1 , VMt1 and for (cid:17) (cid:16) (cid:17) The reorder operation is to know the position mapping from [I Mt1; Mt1] to [I Mt; Mt]. For example, if the unmasked position at 1 is [2, 4, 5] from sequence of 8 tokens, and at step + 1 is [2, 4, 5, 7]. Then [I Mt1; Mt1] would be [2, 4, 5, 0, 1, 3, 6, 7], and [I Mt; Mt] would be [2, 4, 5, 7, 0, 1, 3, 6]. The mapping would be [0, 1, 2, 7, 3, 4, 5, 6], and we only need to get (cid:104) KIMt1 the corresponding entries [0, 1, 2, 7] from VIMt1 t1 ; KMt1 ; VMt1 and (cid:104) (cid:105) (cid:105) . The only remaining thing is that the change of token position would impact the positional encoding. However, this is easy to solve; we can also reorder the positional embedding. Reordering positional embeddings is required only once per model evaluation and can be shared across layers, thus, it would not cost much time. Furthermore, since our method introduces one-step shift in caching, the position of cached tokens at step corresponds to the token positions decoded from step 1. This alignment allows us to track which key and value entries need to be cached without storing the entire key/value matrices, which, to cache which tokens, can only be known after the decoding results at step t. We present the pseudo-algorithm of our approach in Algorithm 1. While it largely improves inference speed over the naive implementation, the concat and reorder operations still introduce some overhead. We believe there is substantial potential for further optimization."
        },
        {
            "title": "B Design for Dream",
            "content": "Dream has different caching strategy with LLaDA. The main reason for this is that Dream is adapted from pre-trained autoregressive models, which would make the position of output align with the probability of the next token, instead of the current token in the traditional setting of masked diffusion models. This would make difference in the caching strategy and we investigate between different designs of those caching strategies: 13 Algorithm 1 Pseudo code for dKV-Cache-Decode. We take step as an example. Require: Sequence x1:L c(t) at step (Simplied as x), position index of masked tokens Mt, cached Key t1 and Value VIMt KIMt1 t1 1: x[Mt1] Mt1: 1 for one-step shift 2: PE (cid:2)PE[I Mt1] ; PE[Mt1](cid:3) Positional embeddings: cached on left, uncached on right 3: QMt , KMt : Calculation in Transformer to get Q, and , VMt1 4: KI Get all and Concat 5: KIMt : The index of Mt in the , VMt (x) (cid:16) KIMt1 t1 , KMt1 , ), VIMt Concat Reorder(VI Reorder(KI VIMt1 t , ) , VI (cid:17) (cid:16) (cid:17) (cid:2)x[I Mt1] ; x[Mt1](cid:3) 6: A(QMt , VI ) 7: Scatter(p, Mt1) , KI 8: Return p, KIMt , VIMt Put the token logits back to the original position Figure 6: Three variants for the caching strategy for diffusion language models adapted from autoregressive language models, which would have shifted output position. Un-Shift, Figure 6(a): We cache the unshifted token representations. Specifically, for the t-th token, we store its key and value as Kt and Vt at position t. Right-Shift, Figure 6(b): Given that the hidden state is highly sensitive to changes in input, we also explore right-shifted variant. Here, for the t-th token, we cache Kt+1 and Vt+1. Un&Right-Sift, Figure 6(c): We introduce stricter variant where caching is conditioned on both input stability and decoding completion. For the t-th token, we cache its features only after its input is fixed and it has been decoded. The one-step shift is still used here. For example, in the right-shift variant, the t-th token is fed into the model at position t+1 in the next step, and we cache its output Kt+1 and Vt+1 then. The results are shown in Table 4, where Un&right-shift would have the best performance, and the right shift would largely harm the model performance. However, we use the Un-Shift in our main experiment, since Un&right-shift is incompatible with the above concat_reorder. Table 4: Comparison between different types of caching strategy for Dream-Base-7B. Un-Shift Right-Shift Un&Right Shift MMLU GSM8K 71.78 76.34 64.60 32.68 71.73 77."
        },
        {
            "title": "C Evaluation Details",
            "content": "C.1 For LLaDA We re-implemented the evaluation of LLaDA on those reported datasets. We generate and extract the final answer instead of comparing the log prob in the multiple-choice question. Thus, the result of MMLU and GPQA is lower than reported since the model sometimes cannot generate the answer in 14 the given format or does not generate the answer. We show the configuration of each experiment in Table 5 Remasking Table 5: Configurations of experiments on LLaDA-Instruct. Base (random / confidence) Configuration dKV-Cache-Greedy (random) Window Size dKV-Cache-Greedy (random) Cache Interval Few-Steps (random) Steps Half-Steps (confidence) Steps dKV-Cache-Decode (confidence) Cache Interval MMLU GSM8K Math500 GPQA L=32, T=32, B=16 L=256, T=256, B=32 L=256, T=256, B=64 L=128, T=128, B=64 HumanEval MBPP L=512, T=512, B=32 L=512, T=512, B=32 T= T=160 T=160 T=80 T=320 T=320 2 2 2 2 2 2 4 4 4 4 2 T=16 T=128 T=128 T=64 T=256 T=256 8 8 8 8 8 C.2 For Dream We follow the original evaluation pipeline for Dream4 and we also adopt two datasets, MMLU and GPQA, to generate the answer instead of comparing the probabilities. We follow all the hyperparameters set in the evaluation script, including the temperature, the remasking strategy, top_p and the number of few-shot in in-context learning."
        },
        {
            "title": "D Impact of batch size on speed",
            "content": "Figure 7: Impact of batch size on decoding speed. Evaluated on LLaDA with single NVIDIA H20; prefill length fixed at 100 tokens. Our inference pipeline relies heavily on indexing operations, gathers and scatters, that generate stream of small, non-contiguous memory accesses. At batch size of one, these uncoalesced reads make the inference memory-bound. As result, the GPUs compute units sit idle waiting for data. In some cases with the batch size equal to 1, inference with caching can actually underperform the unaccelerated baseline. By contrast, scaling up the batch size can solve this problem and deliver far greater, more stable speed-up ratios over the baseline."
        },
        {
            "title": "E Case Study",
            "content": "We provide the case study of our method in Table 6 and Table 7 and Table 8. 4https://github.com/HKUNLP/Dream/blob/main/eval/eval_dream_gen.sh 15 Table 6: Case study on LLaDA-8B-Instruct. Question: Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours?"
        },
        {
            "title": "Model",
            "content": "Generate Text (L=128, T=128, B=64) LLaDA-8BInstruct confidence) (low dKV-CacheDecode, Refresh = 4 dKV-CacheDecode, Refresh=16 1. Calculate the distance Lily runs in the first 4 hours: - Speed: 12 kilometers per hour - Time: 4 hours - Distance: 12 km/h * 4 = 48 km 2. Calculate the distance Lily runs in the next 4 hours: - Speed: 6 kilometers per hour - Time: 4 hours - Distance: 6 km/h * 4 = 24 km 3. Add the distances together: - Total distance: boxed{72} km 1. Calculate the distance Lily runs in the first 4 hours: - Distance = Speed Time = 12 km/h 4 = 48 km 2. Calculate the distance Lily runs in the next 4 hours: - Distance = Speed Time = 6 km/h 4 = 24 km 3. Add the distances from both periods to find the total distance Lily runs in 8 hours: - Total Distance = 48 km + 24 km = 72 km boxed{72 km} 1. Calculate the distance Lily runs in the first 4 hours: 12 km/hour * 4 hours = 48 km 2. Calculate the distance Lily runs in the next 4 hours: 6 km/hour * 4 hours = 24 km 3. Add the distances from the first 4 hours and the next 4 hours to find the total distance she runs: 48 km + 24 km = 72 km Therefore, Lily can run boxed{72} kilometers in 8 hours. dKV-CacheGreedy, Refresh = 2 To determine how many kilometers Lily can run in 8 hours, we need to break down her running schedule and sum the distances. First, calculate the distance she runs at 12 kilometers per hour for 4 hours: 12 4 = 48 After 4 hours, she runs 6 kilometers per hour for the remaining 4 hours: Now, add the distances: boxed{72} 6 4 = 24 48 + 24 = 72 dKV-CacheGreedy, Refresh = 4 (random) For the first 4 hours Lily runs at 12 kilometers per hour, so she runs 12 * 4 = 48 kilometers. 8 hours - 4 hours are left after the first 4 (8 - 4 = 4 hours). For the next 4 hours, she runs at 6 kilometers per hour, so she runs 6 * 4 = 24 kilometers. Adding the two distances together, Lily can run 48 + 24 = 72 kilometers in 8 hours. Answer: boxed{72} 16 Table 7: Case study on Dream-Instruct-7B. Request: Write story that ends with \"Finally, Joey and Rachel get married.\""
        },
        {
            "title": "Model",
            "content": "Generate Text (L=256, T=256) DreamInstruct-7B (Entropy) dKV-CacheDecode, Refresh = 16 dKV-CachePrefill Once upon time, there was young man named Joey who lived in small town in the middle of nowhere. He was kind and gentle soul who loved to sing and play the guitar. One day, he met beautiful girl named Rachel who was also talented singer and musician. They fell in love and decided to get married. However, there was problem. Joeys parents were against the marriage because they didnt know Rachel well enough. They thought that she was not good match for their son. Rachel, on the other hand, was not worried about Joeys parents opinion. She knew that she loved Joey and that he was the one for her. She decided to try and win over Joeys parents by showing them how much she cared about him. She started by cooking for them and inviting them to her house for dinner. She also took them on trip to the mountains where they could enjoy the scenery and the fresh air. After while, Joeys parents started to see Rachel in different light. They realized that she was good person and that she loved Joey very much. They decided to give their blessing to the marriage. Finally, Joey and Rachel got married and lived happily ever after. Once upon time, in small town called New York, there lived young man named Joey and young woman named Rachel. They were both very happy with their lives, but they were also very different from each other. Joey was very outgoing person who loved to be around people, while Rachel was very introverted person who loved to be alone. One day, Joey and Rachel met at coffee shop and fell in love. They spent hours talking and laughing together, and they realized that they were perfect for each other. They decided to be together and build their lives together. As the years went by, Joey and Rachel grew closer and closer. They shared their dreams and their fears, and they supported each other through thick and thin. They were perfect match, and they knew that they were meant to be together. Finally, Joey and Rachel got married. They had beautiful wedding with all their friends and family in attendance. They were so happy to be together, and they knew that they would always be together. Once upon time, in small town called New York, there lived young man named Joey. Joey was kind and caring person who always tried to help others. He had beautiful girlfriend named Rachel, who was also kind and caring. They were in love with each other and wanted to spend their lives together. One day, Joey and Rachel decided to get married. They planned their wedding for long time, and everyone in the town was excited to attend. On the day of the wedding, Joey and Rachel exchanged vows and promised to love each other forever. After the wedding, Joey and Rachel moved into beautiful house in the countryside. They were happy and content with their lives, and they had two children, boy and girl. The children grew up to be kind and caring people, just like their parents. As the years went by, Joey and Rachel continued to love each other deeply. They had many ups and downs, but they always found way to overcome their challenges and stay together. Finally, after many years of marriage, Joey and Rachel decided to retire and live in small cottage in the countryside. They were happy and content with their lives, and they knew that they had found the love of their lives in each other. 17 Table 8: Case study on Dream-Instruct-7B. Question: Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours? Model DreamInstruct-7B (Entropy) dKV-CacheDecode, Refresh = 4 dKV-CacheDecode, Refresh = dKV-CachePrefill, No refresh Generate Text (L=128, T=128) In the first 4 hours, Lily runs 12 kilometers per hour, so she runs total of 12 * 4 = 48 kilometers. In the next 4 hours, Lily runs 6 kilometers per hour, so she runs total of 6 * 4 = 24 kilometers. Therefore, in 8 hours, Lily can run total of 48 + 24 = 72 kilometers. #### 72 In the first 4 hours, Lily can run 12 kilometers per hour, so she can run total of 12 * 4 = 48 kilometers. In the next 4 hours, Lily can run 6 kilometers per hour, so she can run total of 6 * 4 = 24 kilometers. Therefore, in 8 hours, Lily can run total of 48 + 24 = 72 kilometers. #### 72 In the first 4 hours, Lily runs 12 kilometers per hour, so she runs total of 12 * 4 = 48 kilometers. In the next 4 hours, Lily runs 6 kilometers per hour, so she runs total of 6 * 4 = 24 kilometers. Therefore, in 8 hours, Lily can run total of 48 + 24 = 72 kilometers. The answer is: 72 In the first 4 hours, Lily runs 12 kilometers per hour, so she runs total of 12 4 = 48 kilometers. In the next 4 hours, she runs 6 kilometers per hour, so she runs total of 6 4 = 24 kilometers. Therefore, in 8 hours, Lily can run total of 48 + 24 = boxed{72} kilometers.The answer is:"
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}