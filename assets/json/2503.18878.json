{
    "paper_title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders",
    "authors": [
        "Andrey Galichin",
        "Alexey Dontsov",
        "Polina Druzhinina",
        "Anton Razzhigaev",
        "Oleg Y. Rogov",
        "Elena Tutubalina",
        "Ivan Oseledets"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), a method to learn a sparse decomposition of latent representations of a neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate ''reasoning features'' from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the model's reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs. Code available at https://github.com/AIRI-Institute/SAE-Reasoning"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 7 8 8 1 . 3 0 5 2 : r Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders Andrey Galichin1,3, Alexey Dontsov1,5, Polina Druzhinina1,3, Anton Razzhigaev1,3, Oleg Y. Rogov1,2,3, Elena Tutubalina1,4, Ivan Oseledets1,3 1AIRI 2MTUCI 3Skoltech 4Sber 5HSE Correspondence: galichin@airi.net; rogov@airi.net"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), method to learn sparse decomposition of latent representations of neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate reasoning features from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the models reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved remarkable success in natural language processing, evolving beyond simple token prediction tasks towards explicit reasoning behaviors, such as stepby-step problem-solving and self-reflection. This evolution was significantly accelerated by Chain-ofThought (CoT) prompting (Wei et al., 2022), which encouraged models to produce intermediate reasoning steps, substantially improving performance in arithmetic, logic, and common sense tasks. Selfconsistency decoding (Wang et al., 2022) further refined this approach by aggregating multiple reasoning paths. More recently, specialized models 1Code available at https://github.com/ AIRI-Institute/SAE-Reasoning Problem: Convert the point (0, 3) in rectangular coordinates to polar coordinates. Enter your answer in the form (r, θ), where > 0 and 0 θ < 2π. Generation (no steering): Okay, so have this problem where need to convert the rectangular coordinates (0, 3) to polar coordinates. {... 1500 tokens ...} Reasoning trace So, putting it all together, (0, 3) in rectangular coordinates is (3, π/2) in polar coordinates. Generation (steering): Okay, so have this problem here where need to convert the point (0, 3) from rectangular (which is the same as Cartesian) coordinates to polar coordinates. {... 2000 tokens ...} Increased reasoning trace think Ive covered all the bases here. Calculated r, determined θ, checked using different methods, and even considered the quadrant placement. dont see any issues with the reasoning. So, feel confident that the polar coordinates for the point (0, 3) are (3, π/2) . Figure 1: Illustration of steering (amplifying) reasoningspecific features during LLM generation. Default generation (blue) shows standard model reasoning, whereas steering (green) induces increased reasoning, self-correction, and graceful transition to the final answerevidence that the identified features are responsible for the reasoning concept. which we denote as reasoning models, such as OpenAIs o1 (OpenAI, 2024b) and DeepSeek-R1 (Guo et al., 2025) have pushed boundaries further, incorporating reasoning and reflective problem-solving through advanced fine-tuning and reinforcement learning. These advances raise new research question: How are such reasoning capabilities internally encoded within LLMs? growing body of work suggests that LLMs represent meaningful concepts as linear directions in their activation spaces (Mikolov et al., 2013; Elhage et al., 2022; Park et al., 2023; Nanda et al., 2023; Jiang et al., 2024). However, identifying these directions remains challenging. Sparse Autoencoders (SAEs) offer principled approach to disentangle activations into sparse, interpretable features (Cunningham et al., 2023; Gao et al., 2024b; Templeton, 2024; Marks et al., 2024). Given trained SAE, the interpretation of its features could be performed by activation analysis (Bricken et al., 2023), targeted interventions (Templeton, 2024), or automated methods (Paulo et al., 2024; Kuznetsov et al., 2025). Although SAEs have proven effective in discovering features for various concepts, their ability to isolate reasoningspecific features remains understudied. In this work, we employ the Sparse Autoencoder to identify reasoning-specific features within the activation space of the DeepSeek-R1 series of models. That is, we hypothesize that such features correspond to linguistic patterns associated with human-like cognitive processes: uncertainty, reflection, and exploration (Chinn and Anderson, 1998; Boyd and and, 2017; Gerns and Mortimore, 2025). Based on this, we propose ReasonScore, an automatic evaluation metric for the detection of reasoning features. Using interpretability techniques and causal interventions (or steering), we demonstrate the significance of these features in modeling the models reasoning process. Experimental results show that amplifying feature activations systematically increases structured reasoning in model outputs. The contribution of this paper is the following: We propose methodology for identifying reasoning-specific features in LLMs using Sparse Autoencoder. We introduce ReasonScore to identify the SAE features responsible for reasoning and confirm its effectiveness using interpretability techniques. We provide causal evidence from steering experiments,  (Fig. 1)  demonstrating that amplifying identified features induces reasoning behavior."
        },
        {
            "title": "Interpretability",
            "content": "Sparse Autoencoders (SAEs) aim to learn sparse decomposition of model activations to identify disentangled features that correspond to meaningful concepts (Bricken et al., 2023). Here, feature refers to an individual component of the learned representation that captures specific, interpretable characteristics of the input data. The core idea behind SAEs is to reconstruct model activations while enforcing sparsity in the learned features. Formally, we extract LLM activations from some intermediate state in the model and train two-layer autoencoder: ˆx = Wdecf + bdec, = σ(Wencx + benc), (1) where σ() is some nonlinearity (e.g. ReLU), is the input activations vector of size dmodel, and is the vector of feature activations of size dsparse. The autoencoder is parametrized by the weights and biases Wenc, benc for the encoder and Wdec, bdec for the decoder, with the columns of Wdec representing features. The hidden size is determined by the expansion factor γ > 1: dsparse = γdmodel. The training objective used to train Sparse Autoencoders minimizes reconstruction loss Lrecon and an additional sparsity-promoting loss Lsparsity. We use mean-squared-error loss and an L1 sparsity loss: = Lrecon + β Lsparsity, Lrecon(x, ˆx) = ˆx, Lsparsity = h1. (2) These constraints force the model to learn small set of interpretable features that capture the distinct properties of the activations."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present our rationale for selecting features in SAE that underpin the thought process in model. First, we examine the language space of reasoning words generated by LLM (Sec. 3). Secondly, we introduce ReasonScore to automatically find these features (Sec. 3.2). 3.1 Designing Reasoning Space Reasoning words are linguistic features associated with exploratory talk as humans talk-to-learn, explore ideas, and probe each others thinking (Boyd and and, 2017). In the original DeepSeek-R1 paper (Guo et al., 2025), the authors demonstrated that the model spontaneously exhibits sophisticated human-like behaviors, such as reflection, where it revisits and reevaluates its previous steps, and exploration of and filter out the words with high presence in Google Books Ngram Corpus (Michel et al., 2011). Fig. 2 shows the distribution of the top 30 words. We then select 10 words indicating reasoning as models reasoning vocabulary and denote it by R. 3.2 Quantifying Reasoning Relevance If an SAEs feature captures reasoning-related behavior, it should be activated primarily on words from R. Given an SAE trained to reconstruct the activations of the reasoning model, we define ReasonScore that measures the contribution of each i-th feature to reasoning. To quantify this, we use dataset of models activations (see details in Sec. 4.1), = DR DR, where (DR) contains activations for tokens in and (DR) contains activations for other tokens. For each feature, we compute its average activations: µ(i, DR) = 1 DR (cid:88) xDR fi(x), µ(i, DR) = 1 DR (cid:88) fi(x). xDR (3) We then normalize these averages by the total activation across all features to obtain the relative contribution. Finally, subtracting the normalized activation in DR from that in DR yields score: si = µ(i, DR) µ(j, DR) (cid:80) (cid:80) µ(i, DR) µ(j, DR) . (4) This score ensures that features with high scores are those that contribute greater proportion of the overall activation to reasoning, while features that are active in other contexts do not dominate. To penalize features that activate only on small fraction of R, we introduce an entropy penalty. For i-th feature, we first calculate its mean activation µ(i, Drj ) for each token rj R. We normalize these values into probability distribution over R: pi(rj) = µ(i, Drj ) (cid:80) µ(i, Drk ) . (5) The entropy of this distribution quantifies how uniformly the feature activates on tokens from R: Hi = 1 log R (cid:88) pi(rk) log pi(rj). (6) Figure 2: The distribution of top 30 words with the greatest change in frequency between reasoning traces of DeepSeek-R1 and ground-truth solutions of math problems. Orange dots show the normalized absolute frequency taken from Google Books Ngram Corpus. We remove the words with absolute frequency above the pre-defined threshold (orange line), and keep those with the high relative frequency indicating reasoning. alternative problem-solving approaches. In particular, the model explicitly employs words that mirror the introspective language humans use when thinking. We hypothesize that these moments correspond directly to its internal reasoning process, which is consistent with studies on human thinking (Chinn and Anderson, 1998; Boyd and and, 2017). Although the exact formulations of such process might differ subtly from human thinking, we can identify them using the approach similar to (Rayson and Garside, 2000). We select two subsets from the OpenThoughts-114k (OpenThoughts, 2025) dataset that differ only in the inclusion of the thinking\" process: the raw solutions of mathematical tasks, and solutions obtained from DeepSeek-R1 of the same tasks. We then calculate the frequencies of the words in both subsets, sort by the frequency, (a) Top 10 negative and positive output logits of the feature (b) Top 20 max activating examples Figure 3: Empirical analysis results for the feature 17456 in our SAE: (a) Distribution of the bottom and top logits influenced by one of the reasoning features. (b) Contexts where the reasoning feature activates the most. Increased color intensity represent higher activation values, with token highlighted in bold having the highest activation. Here, log normalizes the entropy to [0, 1], activations into interpretable features. with Hi = 1 indicating perfect uniformity. We penalize low-entropy features by adjusting the si score, which gives us the final ReasonScore: ReasonScorei = µ(i, DR) µ(j, DR) (cid:80) α µ(i, DR) µ(j, DR) (cid:80) . (7) where α controls the trade-off between specificity (α 0) and generalization (α > 1). In our experiments, we set α = 0.7."
        },
        {
            "title": "4 Evaluation",
            "content": "In this section, we empirically validate the effectiveness of the identified reasoning features in modeling reflection, uncertainty, and exploration inside the reasoning model. We examine the activation pattern of these features (Sec. 4.2), apply the auto-interpretability pipeline proposed in Kuznetsov et al. (2025) to generate explanations (Sec. 4.3), and perform causal interventions (steering) using these features on various benchmarks (Sec. 4.4). 4.1 Experimental Setup Here, we describe how we train the Sparse Autoencoder to obtain sparse decomposition of the LLM the train Model. We the DeepSeek-R1-Llama-8B model. This model was selected for its reasoning capabilities and open-source availability. SAE on Data. We train SAE on the output activations of the 19-th layer generated using 400 million tokens each from the LMSys-Chat-1M (Zheng et al., 2023) and OpenThoughts-114k (OpenThoughts, 2025) datasets. The first provides broad and diverse spectrum of real-world conversational data, while the latter provides high-quality reasoning traces generated by DeepSeek-R1 for math, science, code, and puzzle samples. The chosen datasets were selected to capture both general and reasoningspecific features. Training. We use the same settings as in the Anthropic April update (Olah et al., 2024). Mainly, we use ReLU as an activation function in Eq. (1), set the expansion factor γ = 16, and set β = 5 in Eq. (2). We use the mean L0-norm of latent activations, Exh(x)0, as measure of sparsity. At the end of training, our SAE achieves l0 = 78 on the LMSys-Chat-1M dataset and l0 = 94 on the OpenThoughts-114k dataset, allowing the sparse decomposition of complex neural activations into interpretable features. 4.2 Empirical Analysis Given trained SAE, for each i-th feature, we calculate its ReasonScore as in Eq. (7) on the subset of OpenThoughts-114k. We then choose 100 highest scoring features and manually evaluate each candidate using the interface proposed in (Bricken et al., 2023). This includes examples of when each feature activates and in which context, what effects it has on the logits. We determine whether feature qualifies as good reasoning candidate if (1) it is consistently activated in contexts that require explicit reasoning, such as logical deductions, handling uncertainty, or causal inference, (2) its activation impacts interpretable logits that characterize the reasoning process, and (3) the feature triggers in various examples of reasoning tasks. Following this analysis, we identify the set of 30 features we believe to be responsible for the reasoning-related properties of the model. In Fig. 3a, we provide the distribution of the bottom and top logits influenced by one of the features from this set (i = 17456 in our SAE). Fig. 3b shows the contexts in which the feature is activated. The results demonstrate that this feature boosts logits for reflection-oriented tokens (e.g. research, consult), and consistently activates in contexts requiring logical or evidential reasoning. These findings provide strong evidence that this feature plays an important role in the models reasoning process. For additional interpretability details for other features, please refer to Appx. A. Takeaway 1: Empirical analysis provides strong evidence that the features identified by the ReasonScore play an important role in the models reasoning process. 4.3 Automatic Interpretability via Feature Steering To further explore the role of identified reasoning features, we leverage feature steering, technique that modulates the activation of learned representations to analyze their functional influence. By systematically altering feature activations, we assess their impact on text coherence, logical consistency, and argument structure. For given feature indexed by i, we determine its peak activation Amax using random subsets from OpenThoughts-114k. During text generation, Feature index AIME 2024 MATH-500 GPQA Diamond pass@ # tokens pass@1 # tokens pass@1 # tokens No steering 15136 17456 46379 62777 46.6 60.0 50.0 53.3 56.6 12.0k 13.8k 14.2k 15.5k 12.9k 91. 89.6 93.0 92.4 93.0 3.7k 4.3k 4.2k 4.4k 4.1k 54.0 50.5 53.0 55.5 48.5 7.9k 8.9k 8.7k 9.0k 8.3k Table 1: Performance and average number of output tokens for different steering experiments on reasoningrelated benchmarks. model activations are modified as follows: = + λAmaxdi, (8) where represents the original activation, di corresponds to the respective column (feature) in Wdec , and λ is tunable parameter regulating the strength of feature influence. Unlike prior works that focus on style or fluency, our experiments emphasize reasoning-related attributes. For example, enhancing features should strengthen causal linking between statements, improve deductive inference, or introduce structured argumentation. Conversely, suppressing them should lead to fragmented reasoning or logical gaps. To quantify these effects, we systematically vary λ within predefined range and analyze the outputs using manual inspection and automated evaluation (Kuznetsov et al., 2025) via GPT-4o. Table 2 reveals that the features identified in Sec. 4.2 highlight distinct reasoning-related patterns, including depth of analysis, numerical precision, logical organization, and methodological exploration. Strengthening certain features enhances step-by-step reasoning, self-correction, and structured argumentation while weakening them results in fragmented logic, oversimplified conclusions, or reduced analytical depth. Tab. 3 provides an in-depth analysis of feature steering for 4 selected candidate features, demonstrating its ability to selectively modulate different \"cognitive\" processes and enhance logical coherence in AI-generated text. Takeaway 2: Automatic interpretability demonstrate that systematic modulation of feature activations directly impacts model reasoning capabilities, confirming that the identified features are causally linked to the models reasoning behavior. Group Name Features Possible Function Effect Type Observed Behavior Reasoning Depth Thoroughness and Numerical Accuracy and Validation Genvs. Code eration Explanatory Focus Exploration of Multiple Methods 15317, 41141, 31505, 24117, 63010, 6608, 17456, 15136, 44396, 29818 4990, 46379, 34813 3466, 26222, 49186 24117, 56973, 44396, 40464 Structural and Logical Organization 63010, 20877, 34700, 4858, Symbolic Numerical Reasoning vs. 24593, 47523, 34813 Self-Correction and Backtracking 62777, 46379 6608, Controls multi-step analysis, iteration, and self-correction in problem-solving. Governs precision in calculations, unit conversions, and error-checking. Balances executable code generation vs. verbal explanations. Encourages evaluating alternative approaches before finalizing solutions. Ensures clarity, stepby-step breakdown, and logical flow. Balances theoretical/ symbolic reasoning with direct numerical computation. Controls iterative refinement and errorchecking. & Stylistic Structural, Semantic & Logical Consistency Semantic & Logical Consistency Stylistic Structural & & Semantic Logical Exploration Structural, Semantic & Instruction Clarity Semantic & Logical Consistency Semantic & Logical Consistency iterative tracking, Strengthening: Extensive stepby-step reasoning, multiple iterations, self-corrections. Weakening: Direct answers with minimal steps. Strengthening: Meticulous unit reevaluation. Weakening: Direct results with potential errors. Strengthening: Full code implementations. Weakening: Conceptual explanations without code. Strengthening: Compares multiple strategies (e.g., DP vs. greedy). Weakening: Commits to the first viable method. Strengthening: Wellstructured explanations. Weakening: Disorganized or fragmented reasoning. Strengthening: Algebraic/ theoretical frameworks. Weakening: Immediate numerical substitution. Strengthening: Multiple self-correction. rounds Weakening: Commits to initial answers without revision. of Table 2: Reasoning clusters obtained using GPT-4o. Each cluster is defined by particular type of reasoning (depth of analysis, numerical checking, code generation), the specific features involved, their hypothesized function in shaping the models output style and logic, and the observed behaviors that emerge when using the feature steering. 4.4 Steering on Reasoning-Related Benchmarks Here, our objective is to verify if steering of reasoning features consistently improves the LLMs reasoning ability. Following the setup in DeepSeek-R1, we perform steering on reasoningsuch as AIME 2024 intensive benchmarks, (of America, 2024), MATH-500 (Hendrycks et al., 2021), and GPQA Diamond (Rein et al., 2023). For each identified reasoning feature, we modify the activations during the generation of the answers according to Eq. (8). We set the steering strength λ = 2 in all of our experiments. We report accuracy (or pass@1 (Chen et al., 2021)) and the average number of tokens used by the model before proceeding to the final answer. As shown in Tab. 1, steering yields consistent increase in the average number of output tokens before the final answer. In particular, the feature under the index = 46379 increases the average completion length in 29% for AIME 2024, 19% for MATH-500, and 14% for GPQA Diamond. Steering this feature results in consistent improvement in accuracy across all benchmarks. Examples of generated outputs without and with activation steering are provided in Appx. B. Takeaway 3: Our findings suggest that amplifying reasoning features prolongs the internal thought process and correlates with increased performance on reasoning-related tasks."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Mechanistic Interpretability Various methods exist to shed light on the inner workings of LLMs, including attention analysis, which examines the models focus on input tokens (Vaswani et al., 2017), and gradient-based methods that identify influential input features (Simonyan et al., 2014). Probing techniques offer insights into the information captured within different layers of Feature Number Observed Behavior 15136 17456 62777 Affects: Reasoning Depth and Thoroughness, Self-Correction Weakening (-4.0): \"To solve the problem, apply dynamic programming using DP table where dp[i][j] stores the maximum coins collected up to (i,j). Compute dp[N][N] and return it if >= C, else return -1.\" (Short, direct response with no reasoning) Strengthening (4.0): \"Okay, let me carefully analyze the problem. The participant starts at (1,1) and must reach (N,N). Each move is either right or down. To ensure an optimal path, we must track accumulated coins at each step. The best approach appears to be dynamic programming. Lets define dp[i][j] as the maximum coins collected at (i,j). But waitwhat if we consider another approach? Perhaps graphbased shortest path algorithm? No, because the constraint is maximizing coins, so DP makes sense. Now, lets check boundary conditions...\" (Extensive, iterative reasoning with multiple reconsiderations before formulating the solution) Affects: Reasoning Depth and Thoroughness, Self-Correction Weakening (-4.0): \"Use DP to compute the max coins. Return dp[N][N].\" (Direct, without intermediate considerations) Strengthening (4.0): \"Lets carefully check all possible paths. We start by defining dp[i][j] to store max coins up to (i,j). However, we should verify if greedy approach works. Lets compare outputs from both strategies before concluding...\" (Persistent, multi-step reasoning) Affects: Numerical Accuracy and Validation Weakening (-4.0): \"Apply standard DP with dp[i][j] transition. Compute final result.\" (Straightforward, minimal reasoning) Strengthening (4.0): \"Before solving, lets analyze constraints. The participant moves through grid and must maximize coins. Should we use DP approach or consider graph-based traversal? Lets derive the recurrence relation first...\" (Detailed breakdown of reasoning) Affects: Self-Correction and Backtracking Weakening (-4.0 to -2.5): \"We assume the input is valid and proceed with computation.\" Strengthening (+2.5 to +4.0): \"We must ensure that when n=2, the grid still allows valid path.\" Table 3: Observed effects of specific feature steering on the model reasoning behavior. Each feature is associated with changes in reasoning depth, thoroughness, self-correction, and numerical accuracy, as demonstrated by contrasting responses with negative and positive steering strengths. an LLM (Alain and Bengio, 2016). Mechanistic interpretability aims to reverse-engineer the computations of LLMs, employing techniques like activation patching (Meng et al., 2022) and feature steering (Cao et al., 2024; Soo et al., 2025) to understand and control model behavior. The logit lens provides way to observe the models token predictions at different processing stages (Nostalgebraist, 2020). 5.2 Sparse Autoencoders Sparse Autoencoders (SAEs) have emerged as key tool for understanding the internal representations of LLMs, particularly in the context of interpretability research (Gao et al., 2024a; Huben et al., 2024). By learning sparse decomposition of model activations, SAEs can identify disentangled features that correspond to meaningful concepts (Marks et al., 2025). Previous work has shown that SAE features are significantly more monosemantic than individual neurons, making them an effective tool for mechanistic interpretability (Leask et al., 2025). key challenge in using SAEs for interpretability is ensuring that the extracted features are monosemantic and robust. Yan et al. (Yan et al., 2024) propose using feature decorrelation losses to enforce better separation between learned latents, preventing redundancy in feature representations. Furthermore, recent advances in cross-layer SAEs (Shi et al., 2025) have allowed more abstract, high-level reasoning patterns by analyzing activations across multiple transformer layers. 5.3 Reasoning LLMs Recent innovations in LLMs have shifted the focus toward models with explicit reasoning abilities. Notable examples include OpenAIs o1 (OpenAI, 2024b), DeepSeek-R1 (Guo et al., 2025) and QwQ-32B-preview (Team, 2024). These methods train LLMs by rule-based reinforcement learning that employs the correctness score (whether the final answer is correct or not) and format score (to ensure outputs follow predetermined structure) which leads to the emergence of advanced reasoning behaviors like self-correction and reflection, denoted as an aha moment in the DeepSeek-AI (Guo et al., 2025) report. Despite the success of rule-based reinforcement learning in enabling reasoning capabilities, how these models encode their internal reasoning remains an open question. Our study explores this problem by using Sparse Autoencoder to find interpretable features responsible for the underlying reasoning mechanisms inside these LLMs, which, to the best of our knowledge, have not been done yet."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present novel approach for uncovering the internal mechanisms of reasoning in Large Language Models (LLMs) through the lens of Sparse Autoencoders (SAEs). We introduce ReasonScore, metric that identifies reasoningrelevant features based on their activation patterns across curated set of introspective vocabulary. Through empirical analysis, automatic interpretability, and controlled feature steering, we provide strong evidence that the features selected by ReasonScore directly correspond to reasoning behaviors such as reflection, uncertainty handling, and structured problem-solving. Our findings demonstrate that amplifying these features consistently enhances the reasoning quality of model outputs and improves performance across multiple reasoning benchmarks. Our work provides the first mechanistic evidence that specific, interpretable components of LLM representations are causally linked to complex cognitive behaviors."
        },
        {
            "title": "References",
            "content": "Guillaume Alain and Yoshua Bengio. 2016. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644. Maureen Boyd and Yiren Kong and. 2017. Reasoning words as linguistic features of exploratory talk: Classroom use and what it can tell us. Discourse Processes, 54(1):6281. Towards monoseAskell, and 1 others. 2023. manticity: Decomposing language models with dictionary learning, 2023. URL https://transformercircuits.pub/2023/monosemantic-features/index. html, page 9. Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, and Jinghui Chen. 2024. Personalized steering of large language models: Versatile steering vectors through bi-directional preference optimization. arXiv preprint arXiv:2406.00045. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Clark A. Chinn and Richard C. Anderson. 1998. The structure of discussions that promote reasoning. Teachers College Record, 100(2):315368. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. 2023. Sparse autoencoders find highly interpretable features in language models. Preprint, arXiv:2309.08600. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, and 1 others. 2022. Toy models of superposition. arXiv preprint arXiv:2209.10652. Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. 2024a. Scaling and arXiv preprint evaluating sparse autoencoders. arXiv:2406.04093. Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. 2024b. Scaling and evaluating sparse autoencoders. Preprint, arXiv:2406.04093. Pilar Gerns and Louisa Mortimore. 2025. Towards exploratory talk in secondary-school clil: An empirical study of the cognitive discourse function explore. Language Teaching Research. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Preprint, arXiv:2103.03874. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Robert Huben, Hoagy Cunningham, Logan R. Smith, Aidan Ewart, and Lee Sharkey. 2024. Sparse autoencoders find highly interpretable features in language models. In Proceedings of the International Conference on Learning Representations (ICLR). Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch. 2024. On the origins of linear representations in large language models. arXiv preprint arXiv:2403.03867. Kristian Kuznetsov, Laida Kushnareva, Polina Druzhinina, Anton Razzhigaev, Anastasia Voznyuk, Irina Piontkovskaya, Evgeny Burnaev, and Serguei Barannikov. 2025. Feature-level insights into artificial text detection with sparse autoencoders. arXiv preprint arXiv:2503.03601. Patrick Leask, Bart Bussmann, Michael T. Pearce, Joseph I. Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, and Neel Nanda. 2025. Sparse autoencoders do not find canonical units of analysis. In Proceedings of the International Conference on Learning Representations (ICLR). Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. 2024. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647. Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. 2025. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. In Proceedings of the International Conference on Learning Representations (ICLR). Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. In Advances in Neural Information Processing Systems. Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew Gray, Google Books Team, Joseph Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, and 1 others. 2011. Quantitative analysis of culture using millions of digitized books. science, 331(6014):176182. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word arXiv preprint representations in vector space. arXiv:1301.3781. Neel Nanda, Andrew Lee, and Martin Wattenberg. 2023. Emergent linear representations in world models of In Proceedings self-supervised sequence models. of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 1630. Nostalgebraist. 2020. Interpreting gpt: the logit lens. Mathematical Association of America. 2024. Aime. Chris Olah, Shan Carter, Adam Jermyn, Josh Batson, Tom Henighan, Jack Lindsey, Tom Conerly, Adly Templeton, Jonathan Marcus, Trenton Bricken, and 1 others. 2024. Circuits updates-april 2024. Transformer Circuits Thread. OpenAI. 2024b. llms. learning-to-reason-with-llms/. Learning to reason with https://openai.com/index/ OpenThoughts. 2025. Open Thoughts. https://openthoughts.ai. Kiho Park, Yo Joong Choe, and Victor Veitch. 2023. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658. Gonçalo Paulo, Alex Mallen, Caden Juang, and Nora Belrose. 2024. Automatically interpreting millions of features in large language models. arXiv preprint arXiv:2410.13928. Paul Rayson and Roger Garside. 2000. Comparing corpora using frequency profiling. In Proceedings of the Workshop on Comparing Corpora - Volume 9, WCC 00, pages 16, USA. Association for Computational Linguistics. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. Preprint, arXiv:2311.12022. Wei Shi, Sihang Li, Tao Liang, Mingyang Wan, Guojun Ma, Xiang Wang, and Xiangnan He. 2025. Route sparse autoencoder to interpret large language models. arXiv preprint arXiv:2503.08200. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR (Workshop Poster). Samuel Soo, Wesley Teng, and Chandrasekaran Balaganesh. 2025. Steering large language models with feature guided activation additions. arXiv preprint arXiv:2501.09929. Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face. Adly Templeton. 2024. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Anthropic. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 59986008. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Hanqi Yan, Yanzheng Xiang, Guangyi Chen, Yifei Wang, Lin Gui, and Yulan He. 2024. Encourage or inhibit monosemanticity? revisiting monosemanticity from feature decorrelation perspective. arXiv:2406.17969. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. 2023. Lmsys-chat1m: large-scale real-world llm conversation dataset. Preprint, arXiv:2309.11998."
        },
        {
            "title": "A Examples of Feature Interfaces",
            "content": "(a) Top 10 negative and positive output logits of the feature (b) Top 20 max activating examples Figure 4: Empirical analysis results for the feature 15136 in our SAE: (a) Distribution of the bottom and top logits influenced by one of the reasoning features. (b) Contexts where the reasoning feature activates the most. Increased color intensity represent higher activation values, with token highlighted in bold having the highest activation. (a) Top 10 negative and positive output logits of the feature (b) Top 20 max activating examples Figure 5: Empirical analysis results for the feature 17456 in our SAE: (a) Distribution of the bottom and top logits influenced by one of the reasoning features. (b) Contexts where the reasoning feature activates the most. Increased color intensity represent higher activation values, with token highlighted in bold having the highest activation. (a) Top 10 negative and positive output logits of the feature (b) Top 20 max activating examples Figure 6: Empirical analysis results for the feature 46379 in our SAE: (a) Distribution of the bottom and top logits influenced by one of the reasoning features. (b) Contexts where the reasoning feature activates the most. Increased color intensity represent higher activation values, with token highlighted in bold having the highest activation. (a) Top 10 negative and positive output logits of the feature (b) Top 20 max activating examples Figure 7: Empirical analysis results for the feature 62777 in our SAE: (a) Distribution of the bottom and top logits influenced by one of the reasoning features. (b) Contexts where the reasoning feature activates the most. Increased color intensity represent higher activation values, with token highlighted in bold having the highest activation."
        },
        {
            "title": "B Examples of Feature Steering",
            "content": "Problem: How many rs are in the word strawberry? Generation (no steering): need to determine how many times the letter \"r\" appears in the word \"strawberry.\" First, Ill write down the word and break it into individual letters: S, T, R, A, W, B, E, R, R, Y. Next, Ill identify which of these letters are \"r\": the third, eighth, and ninth letters. Finally, Ill count these \"r\"s, which gives me total of three . Generation (steering): Okay, so need to figure out how many rs are in the word \"strawberry.\" Let me think about this step by step. First, should probably write down the word and break it down into its individual letters. The word is spelled S-T-R-A-W-B-E-R-R-Y. Wait, let me make sure have that right. Sometimes get confused with the spelling. think the correct spelling is \"strawberry.\" So, let me write it out again: S, T, R, A, W, B, E, R, R, Y. Hmm, now that write it, see there are two Rs after the E. Let me count the letters again to make sure dont miss any. (1), (2), (3), (4), (5), (6), (7), (8), (9), (10). So the word has 10 letters in total. Now, focusing on the Rs. The third letter is R, thats one. Then after E, which is the seventh letter, there are two more Rs: the eighth and ninth letters. So thats two more. Let me double-check the spelling again to make sure Im not miscounting. Sometimes might confuse the letters. think its S-T-R-A-W-B-E-R-R-Y, so yes, the two Rs are towards the end. Wait, is there another in the word? Let me think. The word is \"strawberry.\" think its spelled with an after the E, and another after that. So thats two Rs. dont recall any other Rs in the word. Let me go through each letter again: S, T, R, A, W, B, E, R, R, Y. So, yes, the Rs are the third, eighth, and ninth letters. Wait, thats three Rs. Did miscount earlier? Wait, no, think made mistake. Let me recount the letters with their positions. Lets write it out: 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. So, the Rs are at positions 3, 8, and 9. That makes three Rs in total. think initially thought it was two, but upon closer examination, its actually three. Let me check another source or think about how the word is pronounced. The word \"strawberry\" is pronounced with double sound, so that would correspond to two Rs, but when break it down, see that the letters are R, R. Wait, no, the letters are E, R, R. So, thats two Rs after the E. So, the third letter is R, and then the eighth and ninth letters are Rs as well. So thats three Rs. Wait, Im getting confused. Let me visualize the word again: S-T-R-A-W-B-E-R-R-Y. So, the letters are S, T, R, A, W, B, E, R, R, Y. So, the appears at the third position, and then again at the eighth and ninth positions. That makes three Rs. So, the correct count is three Rs. think initially miscounted because focused on the double at the end, but theres another earlier in the word. So, to clarify, the word \"strawberry\" has three Rs: one in the third position, and two more at the end. So, the correct answer is three Rs. Table 4: An example of How many rs are in the word strawberry? problem and corresponding full outputs from DeepSeek-R1-Llama-8B and its steered version. not defined? is undefined at . So, the denominator is x2 9, which factors to (x 3)(x + 3). So, if were to Problem: For how many values of is the expression x5 x29 Generation (no steering): Okay, so have this problem here: \"For how many values of is the expression x5 not defined?\" Let me try to figure this out step by step. First, remember that fraction is x29 undefined when its denominator is zero because you cant divide by zero. So, to find where this expression is not defined, need to figure out when the denominator equals zero. The denominator here is x2 9. So, need to solve the equation x2 9 = 0. Hmm, how do solve that? Well, think its difference of squares because x2 is like (x)2 and 9 is (3)2. So, the equation factors into (x 3)(x + 3) = 0. That makes sense because when you multiply those two binomials, you get x2 3x + 3x 9 = x2 9, which is correct. So, setting each factor equal to zero gives the solutions: 1. 3 = 0 leads to = 3 2. + 3 = 0 leads to = 3 Therefore, the denominator is zero when = 3 or = 3. That means the expression x5 x29 these two points. Wait, let me double-check. If plug = 3 into the denominator: 32 9 = 9 9 = 0. Yep, thats zero. Same with = 3: (3)2 9 = 9 9 = 0. So, both of these make the denominator zero. Is there chance missed any other values? Hmm, let me think. The denominator is quadratic equation, which can have at most two real roots. Since we found both roots, 3 and 3, there are only two values where the denominator is zero. What about the numerator? The numerator is 5. Thats linear term, so it only affects the value of the expression, not its definition. So, the numerator doesnt cause any undefined behavior on its own. Its only when the denominator is zero that the whole expression becomes undefined. Therefore, the only values of where the expression is not defined are = 3 and = 3. That means there are two values of for which the expression is undefined. Just to make sure, let me consider the original expression again: x5 x29 graph this function, it would have vertical asymptotes at = 3 and = 3, which are the points where the function is undefined. Theres no division by zero anywhere else, so these are the only two points where the function doesnt exist. dont think made any mistakes here. The problem is pretty straightforward once you recognize that the denominator needs to be factored and solved. So, yeah, feel confident that the answer is two values: = 3 and = 3. **Final Answer** The expression is not defined for 2 values of x. Generation (steering): Okay, so have this problem: need to find how many values of make the expression (x - 5)/(x² - 9) undefined. Hmm, let me think. remember that fraction is undefined when its denominator is zero because you cant divide by zero. So, should figure out when x² - 9 equals zero. Alright, let me write that down. The denominator is x² - 9. So, need to solve x² - 9 = 0. Hmm, thats quadratic equation. think can factor this. Let me recall, the difference of squares formula is a² - b² = (a - b)(a + b). So, applying that here, x² - 9 is the same as x² - 3². Therefore, it factors into (x - 3)(x + 3). So, now have (x - 3)(x + 3) = 0. To find the values of that make this equation true, can set each factor equal to zero. That gives me: - 3 = 0 or + 3 = 0 Solving these, get = 3 and = -3. So, these are the values that make the denominator zero, which would make the entire expression undefined. Therefore, need to check if these are the only values or if there are more. Wait, let me make sure. The original expression is rational expression, so the only way its undefined is if the denominator is zero. There are no other denominators or functions here, so think thats all. Therefore, cannot be 3 or -3. Let me just double-check my factoring. x² - 9: if expand (x - 3)(x + 3), get x*x + x*3 - 3*x - 3*3, which is x² + 3x - 3x - 9, which simplifies to x² - 9. Yep, thats correct. So, are there any other possibilities where the denominator could be zero? Well, if consider the quadratic equation x² - 9 = 0, it only has two roots: 3 and -3. Theres no other x-value that can satisfy this equation because quadratic equation can have at most two real roots. So, thats it. There are only two values of that make the denominator zero, hence the expression is undefined for those x. Wait, let me think again. Maybe should plug these x-values into the original expression to confirm. Lets try = 3 first. Plugging in, the denominator becomes 3² - 9 = 9 - 9 = 0. So, the expression becomes (3 - 5)/0, which is (-2)/0. Thats undefined, so thats correct. Now, = -3. Plugging in, the denominator is (-3)² - 9 = 9 - 9 = 0 again. So, the expression becomes (-3 - 5)/0, which is (-8)/0. Thats also undefined. So, both values definitely make the expression undefined. Is there any other way this could be undefined? For example, sometimes, if the numerator is also zero, might have an indeterminate form, but in this case, lets see. When = 3, the numerator is 3 - 5 = -2, which is not zero. When = -3, the numerator is -3 - 5 = -8, which is also not zero. So, both times, the numerator is non-zero number, and the denominator is zero. Therefore, its just undefined, not something like 0/0, which is actually undefined as well, but in that case, both numerator and denominator are zero. But in this case, its just clear division by zero. Therefore, can confidently say that the expression is undefined for = 3 and = -3. So, how many values is that? Let me count. 3 and -3. Thats two different values. So, the number of values where the expression is undefined is 2. Wait, let me just visualize the graph of the function. If were to graph = (x - 5)/(x² - 9), know that the function will have vertical asymptotes where the denominator is zero, provided that the numerator isnt zero there. So, as found earlier, at = 3 and = -3, there are vertical asymptotes because the denominator is zero, and the numerator isnt zero. So, those points are not in the domain of the function. So, thats another way to confirm. Is there possibility of any other points where the function is undefined? Let me think. Since the denominator is quadratic, it can have at most two real roots, and weve already found both. So, nope, no other points where the denominator is zero. So, that should be it. So, summarizing, the expression is undefined when is 3 or -3. Therefore, there are two values of where the expression is undefined. So, the answer should be 2. Wait, just to make sure didnt make any mistake in my calculations. Let me go through the steps again. 1. Start with the expression (x - 5)/(x² - 9). 2. To find when its undefined, set denominator equal to zero: x² - 9 = 0. 3. Factor the quadratic: (x - 3)(x + 3) = 0. 4. Solve for x: = 3 and = -3. 5. Check if these x-values make the numerator zero as well. For = 3: numerator is 3 - 5 = -2 = 0. For = -3: numerator is -3 - 5 = -8 = 0. So, both are valid points where the expression is undefined. 6. Confirm that these are the only points by noting that quadratic equation can have at most two real roots, and we found both. 7. Visualize the graph to check for vertical asymptotes at these points, which confirms the function is undefined there. Therefore, Im pretty confident that the number of values is 2. **Final Answer** The expression is undefined for 2 values of x. Table 5: problem from MATH-500, and corresponding outputs from DeepSeek-R1-Llama-8B and its steered version. Problem: regular octagon has the same perimeter as the regular hexagon shown here with side length 16 cm. How long is each side of the octagon? [asy]size(80); pair = dir(120); pair B=dir(60); pair M=(A+B)/2; draw(dir(360)BAdir(180)dir(240)dir(300)cycle); label(\"16 cm\", M, N);[/asy] Generation (no steering): Okay, so have this problem where regular octagon has the same perimeter as regular hexagon. The hexagon has side length of 16 cm, and need to find the side length of the octagon. Hmm, lets break this down step by step. First, remember that the perimeter of regular polygon is just the number of sides multiplied by the length of each side. So, for the hexagon, which is six-sided polygon, can calculate its perimeter by multiplying 6 by 16 cm. Let me write that out: Perimeter of hexagon = 6 * 16 cm = 96 cm. Got that. So the perimeter of the octagon must also be 96 cm since its equal to the hexagons perimeter. Now, regular octagon has 8 sides, right? So, if let be the length of each side of the octagon, then the perimeter of the octagon would be 8 * s. Since we know the perimeter is 96 cm, can set up the equation: 8 * = 96 cm. To find s, just need to divide both sides by 8: = 96 cm / 8 = 12 cm. Wait, is that it? It seems straightforward, but let me double-check to make sure didnt miss anything. So, the hexagon has 6 sides, each 16 cm, so 6*16=96 cm. The octagon has 8 sides, so each side is 96/8=12 cm. Yep, that seems correct. Hmm, just to make sure, maybe should visualize the polygons. regular hexagon has six equal sides and six equal angles, and regular octagon has eight equal sides and eight equal angles. Since both are regular, all sides and angles are equal, so the calculation should hold. Another way to think about it: the perimeter is just the sum of all sides. So, for the hexagon, adding up six sides each of 16 cm gives 96 cm. For the octagon, we need to add up eight sides, each of which is the same length, so 8 times that length equals 96 cm. Solving for that length gives 12 cm. dont see any mistakes in the logic here. Its direct calculation based on the definition of perimeter for regular polygons. So, think the side length of the octagon is 12 cm. **Final Answer** The length of each side of the octagon is 12 centimeters. Generation (steering): Okay, so have this problem here where need to figure out the length of each side of regular octagon. The octagon has the same perimeter as regular hexagon, and the hexagon is shown with side length 16 cm. Hmm, okay, let me think step by step about how to approach this. First, remember that the perimeter of shape is the total length around it. For regular polygons, which both octagons and hexagons are, the perimeter is just the number of sides multiplied by the length of each side. So if can figure out how many sides each shape has and then use the given information, can find the opposite. Wait, the problem says its regular hexagon. Right, hexagon has six sides. So, since the side length is 16 cm, the perimeter of the hexagon would be 6 times 16 cm. Let me write that down. Perimeter of hexagon = 6 * 16 cm = 96 cm. So, the octagon has the same perimeter, which is 96 cm. Now, an octagon has eight sides. So, if can find the length of each side of the octagon, that would be the perimeter divided by 8. Let me write that formula: Perimeter of octagon = 8 * (length of each side) But wait, we know the perimeter is 96 cm, so: 96 cm = 8 * (length of each side) To find the length of each side, need to divide both sides by 8. So, length of each side = 96 cm / 8 = 12 cm. Hmm, that seems straightforward. But let me just double-check to make sure didnt make mistake. Wait, is it correct that the octagons perimeter is 96 cm? Because the hexagons side is 16 cm, so 6 times 16 is 96. So, yes, the octagon has to have the same perimeter. Since octagons have more sides, each side should be shorter. So, 96 divided by 8 is 12, which is shorter than 16, so that makes sense. Wait, another way to think about it: if you have polygon with more sides, each side is actually shorter for the same perimeter. So, since an octagon has more sides than hexagon, each side should be shorter, which aligns with 12 cm being shorter than 16 cm. So that seems right. Is there another way to approach this problem? Maybe using the formula for perimeter in terms of side length? Yes, for regular polygons, the perimeter is just the number of sides multiplied by the side length. So, if denote the side length of the octagon as s, then: Perimeter of octagon = 8 * But since its equal to the perimeter of the hexagon, which is 6 * 16 = 96, then: 8s = 96 Solving for gives = 96 / 8 = 12 cm. Yep, same result. think got it. So, the length of each side of the octagon is 12 cm. Wait, let me think again. Sometimes, when Im dealing with perimeters, might confuse side length with something else, but in this case, since both are regular polygons, its straightforward. So, for regular polygons, the perimeter is directly additive. So, each side contributes equally. Therefore, inverting the number of sides inversely affects the side length. So, in the hexagon, 6 sides, each 16 cm, so 16 times 6 is 96. The octagon, which has more sides, 8, so each side is 96 divided by 8, which is 12. So, that makes sense. Wait, just to make sure, if the octagon had 8 sides, each 12 cm, the perimeter would be 8*12=96 cm, which is the same as the hexagon. So, thats correct. think thats solid. So, the length of each side of the octagon is 12 cm. **Final Answer** The length of each side of the octagon is 12 cm. Table 6: problem from MATH-500, and corresponding outputs from DeepSeek-R1-Llama-8B and its steered version. Problem: Thirty-five students attend math club meeting. The number of girls at the meeting is multiple of 13, and there are more girls than boys attending the meeting. How many boys are at the meeting? Generation (no steering): Alright, so have this problem here: There are 35 students at math club meeting. The number of girls is multiple of 13, and there are more girls than boys. need to find out how many boys are at the meeting. Hmm, okay. Lets break this down. First, total students are 35. Girls are multiple of 13, so lets denote the number of girls as G. Then, the number of boys would be B. So, + = 35. Also, its given that there are more girls than boys, so > B. And since is multiple of 13, lets think about what multiples of 13 are possible here. 13 times 1 is 13, 13 times 2 is 26, 13 times 3 is 39. Wait, but 39 is more than 35, so that cant be the number of girls because we only have 35 students in total. So, possible number of girls are 13 or 26. So, could be 13 or 26. Lets check both possibilities. Case 1: = 13. Then, = 35 - 13 = 22. So, number of boys would be 22. But wait, the problem says there are more girls than boys. So, 13 girls and 22 boys. That would mean more boys than girls, right? Because 22 is more than 13. That contradicts the given condition. So, this case is not possible. Case 2: = 26. Then, = 35 - 26 = 9. So, number of boys would be 9. Now, 26 girls and 9 boys. That means more girls than boys, which fits the condition. So, this is the valid case. Wait, let me double-check. If = 26, which is 2 times 13, then is 9. 26 + 9 = 35, correct. And 26 > 9, so the condition holds. So, that seems to be the answer. Is there any other multiple of 13 that need to consider? Lets see. The next multiple is 39, but as thought earlier, thats more than 35, so thats not possible. So, the only possible number of girls is 26, leading to 9 boys. Alternatively, lets approach this algebraically. Let me write the equations: + = 35 is multiple of 13, so = 13k, where is an integer. Also, > B, so 13k > B. But since + = 35, then = 35 - = 35 - 13k. So, substituting back into > B: 13k > 35 - 13k So, lets solve this inequality: 13k > 35 - 13k Adding 13k to both sides: 26k > 35 Divide both sides by 26: > 35/26 35 divided by 26 is approximately 1.346. Since must be an integer, the smallest possible is 2. So, = 2, which gives = 26, then = 35 - 26 = 9. So, thats the same result as before. Alternatively, if was 1, which gives =13, then B=22, but that doesnt satisfy > B, so must be 2. Hence, the number of boys is 9. think thats thorough. considered both cases, checked the algebra, and made sure that the conditions are satisfied. So, the answer should be 9 boys. **Final Answer** The number of boys at the meeting is 9 . Generation (steering): Okay, so have this problem here: there are 35 students at math club meeting. The number of girls is multiple of 13, and there are more girls than boys. need to find out how many boys are at the meeting. Hmm, let me try to break this down. First, let me note down the given information: - Total number of students = 35. - Number of girls is multiple of 13. - Number of girls is more than the number of boys. need to find the number of boys. Let me denote the number of girls as and the number of boys as B. So, + = 35. Since is multiple of 13, that means = 13 * k, where is positive integer. Because there are more girls than boys, > B. So, since + = 35, if is more than B, then must be greater than 35/2, which is 17.5. So, must be at least 18. But since is multiple of 13, lets see which multiples of 13 are greater than 17.5. Multiples of 13: 13, 26, 39, etc. But since the total number of students is 35, cant be 39 because that would mean more students than the total. So, the possible multiples of 13 for are 13 and 26. But wait, earlier thought must be more than 17.5, so 13 is less than that, which would mean is 13. But 13 is less than 17.5, so that would mean is 22, which is more than G. But the problem says more girls than boys, so must be greater than B. So, cant be 13 because that would make = 35 - 13 = 22, which is more. So, actually, must be 26. Wait, let me think again. If is 26, then would be 35 - 26 = 9. So, = 26, = 9. But is 26 multiple of 13? Yes, 26 is 13 * 2. So that seems to fit. Also, 26 is more than 9, which satisfies the condition that there are more girls than boys. But hold on, let me double-check. If is 26, then is 9, so 26 + 9 = 35. Yes, that adds up. So, is 26 the only possible multiple of 13? Let me check the next multiple. 13 * 3 is 39, which is more than 35, so that cant be. So, the only possible multiple of 13 is 26 for G. Therefore, the number of boys is 35 - 26 = 9. Wait, but hold on, is 9 the number of boys? Let me confirm. If is 26, which is multiple of 13, and 26 > 9, so yes, that makes sense. So, the number of boys is 9. Wait, is there another way to think about this? Let me approach it algebraically. Let me write = 13k, where is positive integer. So, since + = 35, then = 35 - = 35 - 13k. We know that > B, so 13k > 35 - 13k. Lets solve for k: 13k > 35 - 13k Adding 13k to both sides: 26k > 35 Divide both sides by 26: > 35 / 26 Calculating 35 / 26, which is approximately 1.346. Since must be an integer, the smallest possible value of is 2. So, = 2: = 13 * 2 = 26 Then, = 35 - 26 = 9. If were 3, would be 39, which is more than the total number of students, which is 35. So, thats not possible. Therefore, the only possible value is k=2, leading to G=26 and B=9. Wait, so is 9 the number of boys? That seems consistent. Let me just double-check my reasoning. If there are 26 girls, thats multiple of 13, more than the number of boys, which is 9. 26 + 9 is 35, so that adds up. There are no other multiples of 13 in the opposite direction because the next multiple would be 39, which is too high. So, think Ive got it. The number of boys is 9. But wait, just to make sure, let me think about it again. If is 13, then is 22. But since has to be more than B, thats not possible because 13 < 22. So, cannot be 13 because that would make more boys than girls, which contradicts the given condition. So, must be 26, which makes 9. Therefore, the number of boys is 9. **Final Answer** The number of boys at the meeting is 9 . Table 7: problem from MATH-500, and corresponding outputs from DeepSeek-R1-Llama-8B and its steered version. Problem: curve is parameterized by (x, y) = (t3 + 7, 3t2 6t 5). Find the point the curve passes through at = 2. Generation (no steering): Okay, so have this problem here where curve is parameterized by the equations = t3 + 7 and = 3t2 6t 5. need to find the point the curve passes through when = 2. Hmm, let me think about how to approach this. First, remember that when you have parameterized curve, each value of corresponds to specific point (x, y) on the curve. So, to find the point when = 2, just need to substitute = 2 into both the equations for and and compute the values. That makes sense because each parameter defines unique point on the curve. Alright, lets start with the equation for x. The equation is = t3 + 7. So, substituting = 2 into this should give me the x-coordinate of the point. Let me compute that step by step. Calculating t3 when = 2: 23 = 8. Then, adding 7 to that: 8 + 7 = 15. So, the x-coordinate is 15. That seems straightforward. Now, moving on to the y-coordinate. The equation is = 3t2 6t 5. Again, substituting = 2 into this equation. Let me break it down. First, compute t2 when = 2: 22 = 4. Then, multiply that by -3: 3 4 = 12. Next, compute 6t when = 2: 6 2 = 12. Lastly, we have the constant term, which is -5. So, putting it all together: = (12) + (12) + (5). Wait, hold on. Let me double-check that. The equation is = 3t2 6t 5. So, substituting = 2, its 3 (2)2 6 (2) 5. That is 3 4 12 5. So, -12 -12 -5. Adding those together: -12 -12 is -24, and -24 -5 is -29. So, the y-coordinate is -29. Let me make sure did that correctly. So, 3 (2)2 is 3 4 = 12. Then, -6*2 is -12, and then subtracting 5. So, adding all those up: -12 -12 is -24, minus another 5 is -29. Yeah, that seems right. So, combining both coordinates, when = 2, the point on the curve is (15, -29). Hmm, let me just verify once more in case made any calculation errors. Starting with x: t3 + 7. = 2, so 23 is 8, plus 7 is 15. Thats correct. For y: 3t2 6t 5. = 2: 3 (4) = 12, 6 2 = 12, and the constant is -5. So, adding them: -12 -12 is -24, minus 5 is -29. Thats correct. Okay, so think thats the point. Just to visualize, when = 2, the curve is at (15, -29). dont see any mistakes in my calculations, so feel confident about this answer. **Final Answer** The point the curve passes through at = 2 is (15, 29) . Generation (steering): Alright, so have this problem here where curve is parameterized by the equations = t3 + 7 and = 3t2 6t 5. need to find the point the curve passes through when = 2. Hmm, okay, let me think about how to approach this. First, remember that parameterized equations give both the and coordinates in terms of third variable, which in this case is t. So, when they ask for the point when = 2, need to substitute = 2 into both equations for and and then find the corresponding (x, y) pair. Let me write that down step by step. So, for x, the equation is = t3 + 7. If plug in = 2, that should give me the x-coordinate. Similarly, for y, the equation is = 3t2 6t 5, so substituting = 2 there should give me the y-coordinate. Once have both values, can combine them into point. Okay, let me start with calculating x. Plugging = 2 into = t3 + 7: = (2)3 + 7. Calculating 23, thats 8, right? So, 8 + 7 is 15. So, the x-coordinate when = 2 is 15. That seems straightforward. Now, moving on to the y-coordinate. The equation is = 3t2 6t 5. Plugging in = 2: = 3(2)2 6(2) 5. Let me compute each term step by step. First, 22 is 4, so multiplying that by 3 gives 12. Next, 6 2 is 12, so with the negative sign, that term becomes 12. Then, the last term is just 5. So, putting it all together: 12 12 5. Lets add those numbers. 12 12 is 24, and then subtracting another 5 gives 29. So, the y-coordinate is 29. Wait, let me double-check that calculation because sometimes when have multiple negative signs, might mix them up. So, let me recast it: = 3(4) 6(2) 5. Thats 12 12 5. Yes, thats correct. 12 12 is indeed 24, and 24 5 is 29. So, that seems right. So, to recap, when = 2, the x-coordinate is 15 and the y-coordinate is 29. Therefore, the point the curve passes through is (15, 29). Wait, just to make sure didnt make mistake in the substitution, let me verify each step again. For x, substituting = 2 into t3 + 7: 23 = 8, so 8 + 7 = 15. Thats correct. For y, substituting = 2 into 3t2 6t 5: First term: 3 (2)2 = 3 4 = 12. Second term: 6 2 = 12. Third term: 5. Adding them up: 12 12 5 = (12 12) 5 = 24 5 = 29. Yep, thats correct. Hmm, think thats solid. Maybe should just think about whether the parameterization makes sense or if theres another way to represent this curve. Wait, actually, might recall that sometimes parameterizations can be tricky, but in this case, its pretty straightforward. Its giving and in terms of t, so plugging in t=2 should give the point. Alternatively, can also compute the values for t=2 in both equations and plot them mentally or on paper. Let me sketch rough graph in my mind. So, when t=2, is 15, which is further out on the x-axis, and is -29, which is quite low on the y-axis. That seems plausible for cubic and quadratic parameterization. Wait, another thought: sometimes, when parameterize, especially with higher powers, might have to check for consistency, but in this case, since both and are given, and the question is just about plugging in specific t, think its safe. So, to sum up, substituting t=2 into both equations gives x=15 and y=-29, so the point is (15, -29). think thats the correct answer. **Final Answer** The point the curve passes through at = 2 is (15, 29) . Table 8: problem from MATH-500, and corresponding outputs from DeepSeek-R1-Llama-8B and its steered version."
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE",
        "MTUCI",
        "Sber",
        "Skoltech"
    ]
}