{
    "paper_title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "authors": [
        "Jiaming Tang",
        "Yufei Sun",
        "Yilong Zhao",
        "Shang Yang",
        "Yujun Lin",
        "Zhuoyang Zhang",
        "James Hou",
        "Yao Lu",
        "Zhijian Liu",
        "Song Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash"
        },
        {
            "title": "Start",
            "content": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference Jiaming Tang1, Yufei Sun1,3, Yilong Zhao4 Zhuoyang Zhang1 James Hou1,6 Yao Lu2 1MIT 2NVIDIA 3Tsinghua University 4UC Berkeley Zhijian Liu2,5 Shang Yang1 Yujun Lin2 Song Han1,2 5UCSD 6Caltech 5 2 0 2 0 3 ] . [ 1 1 3 0 1 0 . 2 1 5 2 : r https://github.com/mit-han-lab/vlash Figure 1. VLASH enables VLA to play ping-pong rallies with humans. Snapshots showing 洧랢0.5 [16] with VLASH successfully tracking and striking fast-moving ping-pong ball during rally. The robot initiates its reaction by the third frame, demonstrating low-latency perception-to-action response. The task requires both fast reaction and smooth continuous motion, which are enabled by our asynchronous inference with future-state-awareness. Under synchronous inference, the robot fails to achieve this dynamic interaction."
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10 to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03 speedup and reduces reaction latency by up to 17.4 compared to synindicates equal contributions. chronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. 1. Introduction advances Recent in Vision-Language-Action models (VLAs) such as 洧랢0.5 [16], Gemini [1, 34] and Gr00t [26] have demonstrated remarkable capabilities in solving complex robotic tasks. In real-world deployment, these models are typically executed under synchronous inference paradigm: the robot first performs model inference to generate an action chunk [41], then sequentially executes the actions before initiating the next inference cycle. This sequential pipeline introduces action stalls and delayed reactions to environmental changes, since the model remains idle during action execution and cannot update its perception in real time [4]. As result, many VLA demonstration videos are sped up by several times to mask the discontinuous and slow motion. To prevent this stop-and-go behavior, researchers have proposed asynchronous inference [4, 24, 29, 31]. In nutshell, asynchronous inference allows the robot to execute the current action chunk while simultaneously performing 1 tween prediction and execution. VLASH integrates seamlessly into existing fine-tuning pipelines and introduces no additional cost or latency. With clean and lightweight implementation, VLASH provides full-stack asynchronous inference framework from fine-tuning to inference at deployment, making asynchronous control practical and easy to adopt for real-time VLA systems. We build and evaluate VLASH across various VLA models, including 洧랢0.5 [16] and SmolVLA [31]. On simulation benchmarks [25], VLASH achieves up to 30.5% accuracy improvement compared to naive asynchronous inference and consistently outperforms all baselines. On real-world benchmarks [31], VLASH achieves up to 2.03 speedup and reduces reaction latency by up to 17.4 compared to synchronous inference while fully preserving the original accuracy. Beyond quantitative gains, VLASH demonstrates that large VLA models can handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, which were previously infeasible under synchronous inference. We hope these results will inspire future research toward extending VLAs to more dynamic and physically interactive robotics. 2. Related Work Vision-Language-Action Models (VLAs). Recent advances in Vision-Language-Action models have demonstrated remarkable capabilities in robotic manipulation by leveraging large-scale pretraining on diverse and internetscale vision-language data. Models such as 洧랢0.5 [16], RT2 [43], and Gr00t [26], etc. [3, 19] combine visual encoders with large language models to enable generalist robotic policies that can follow natural language instructions and generalize across tasks and embodiments. These models are typically deployed under synchronous inference, where the robot waits for model inference to complete before executing actions, resulting in action stall and slow reaction to environmental changes [4, 29]. Our work addresses this limitation by enabling efficient asynchronous inference for VLAs. Asynchronous VLA Inference. Asynchronous inference offers promising way to eliminate action stalls and improve reaction speed of VLAs, but existing approaches still face significant barriers to adoption in VLA community. SmolVLA [31] implements naive asynchronous inference by directly switching to new action chunks, but this causes severe prediction-execution misalignment and unstable control. Real-time Chunking (RTC) [4] mitigates this by freezing actions guaranteed to execute and inpainting the remaining actions, but this introduces additional runtime overhead for the inpainting process and complicates deployment. concurrent work A2C2 [29] adds an additonal correction heads to the model to mitigate the prediction-execution misalignment, but this also introduces runtime overhead and Figure 2. Prediction-execution misalignment in asynchronous inference. Due to inference delay 풊, the model predicts actions for the prediction interval [洧노, 洧노 + 洧) but they execute during the execution interval [洧노 + 풊, 洧노 + 풊 + 洧). inference for the next one. Because the execution duration of an action chunk is typically longer than the model inference time, the robot can immediately switch to the next chunk once the inference completes, avoiding idle period between chunks [4, 24, 29, 31]. This design eliminates action stalls and allows the robot to perform smooth, continuous motion. Moreover, since inference is performed continuously, the robot can maintain real-time perception and thus react to environmental changes more promptly and accurately [4, 24]. In summary, asynchronous inference provides promising way to achieve smooth, accurate, and fast reaction control for VLAs. However, asynchronous inference faces fundamental challenge that makes it unstable and inaccurate in practice. Since both the robot and the environment continue to evolve during inference, temporal misalignment arises between the prediction interval starting when inference begins and the execution interval starting when inference finishes [4, 29]. As result, the newly generated action misaligns with the robots execution-time state and environment, leading to severe instability and degraded control accuracy. For example, naive asynchronous inference reduces reaction latency but exhibits unstable and laggy control performance [4]. RTC [4] mitigates this by freezing the actions guaranteed to execute and inpainting the rest, but it introduces additional runtime overhead and complicates the deployment. In addition, current implementations [24, 29, 31] often require multi-threaded redesign of the inference framework to support asynchronous inference efficiently. Together, these create significant barrier for the adoption of asynchronous inference for VLAs. To address these challenges, we propose VLASH, general asynchronous inference framework for VLAs that achieves smooth, accurate, and fast reaction control without additional overhead or architectural changes. In nutshell, VLASH makes the model future-state-aware by accurately estimating the execution-time robot state using the previously issued action chunk, effectively bridging the gap beFigure 3. Comparison between VLASH and existing methods. (a) Synchronous inference: the robot stalls during inference, introducing slow reactions. (b) Naive async: the model predicts based on stale state 洧1 while execution begins at future state 洧3, causing misalignment and discontinuity. (c) VLASH rolls forward the robot state (洧3 = 洧1 + 洧녩1 + 洧녩2) and condition on the execution-time state, achieving fast reaction and smooth actions. requires architecture changes to the model. In contrast, our method achieves asynchronous inference through futurestate-awareness without additional overhead. 3. Background Action chunking policy. We consider an action chunking policy 洧랢 洧랚 ( 洧냢洧노 洧녶洧노 , 洧멇롐 ) [16, 31, 42], where 洧녶洧노 is the environment observation (e.g., image, multi-view visual input), 洧멇롐 is the robot state (e.g., joint positions, gripper state), and 洧노 is the controller timestep. At each timestep 洧노, the policy generates chunk of future actions 洧냢洧노 = [洧녩洧노 , 洧녩洧노+1, . . . , 洧녩洧노+洧냩 1], Asynchronous inference and interval misalignment. With asynchronous inference, the robot continues executing the previous action chunk while 洧랢 洧랚 computes 洧냢洧노 in the background. As illustrated in Fig. 2, when 풊 > 0, the action chunk 洧냢洧노 is planned for the prediction interval 洧냪 pred = [洧노, 洧노 + 洧) but actually executed over the shifted ex洧노 ecution interval 洧냪exec = [洧노 + 풊, 洧노 + 풊 + 洧). Intuitively, the actions in 洧냢洧노 are not wrong for the original prediction interval [洧노, 洧노 + 洧). However, under asynchronous inference, by the time they are executed, the environment and robot state have changed, so the same action sequence is applied to different state and scene, leading to unstable and discontinuous behavior [4, 29]. 洧노 where 洧냩 is the number of actions in the chunk. We refer to 洧냩 as the prediction horizon. 4. VLASH 4.1. Future State Awareness Prediction and execution intervals. In practice, only the first 洧 洧냩 actions from each chunk are executed before the next inference to ensure control accuracy. We denote 洧 as the execution horizon. For chunk 洧냢洧노 predicted at timestep 洧노, we define the prediction interval 洧냪pred 洧노 = [洧노, 洧노 + 洧) as the time interval where the first 洧 actions from the action chunk 洧냢洧노 are planned to be executed. During actual execution, however, the 洧 actions from 洧냢洧노 will start being applied later due to inference latency [4, 31]. Let 풊 > 0 be the inference latency measured in control steps. Then the 洧 actions from 洧냢洧노 are actually executed on the robot over the execution interval 洧냪 exec 洧노 = [洧노 + 풊, 洧노 + 풊 + 洧). In asynchronous inference, the robot keeps moving while the VLA performs forward pass, so the state at inference start generally differs from the state at which the new actions actually begin execution. Our key idea is to make the policy future-state-aware: instead of conditioning on the current robot state 洧멇롐 , we condition on the robot state at the beginning of the next execution interval 洧멇롐+풊. Although the future environment observation is unknown, the robot state at the beginning of the execution interval 洧멇롐+풊 is determined by the current robot state 洧멇롐 and the actions executed during the inference delay 洧녩洧노:洧노+풊1. As shown in Fig. 3(c), when inference for the new chunk starts at state 洧1, the robot will still execute the remaining actions 洧녩1, 洧녩2 from the previous chunk before the new chunk is ready to take over. Since the actions 洧녩1, 洧녩2 are already known, we can roll the state forward under them to obtain the execution-time state. In the Fig. 3(c), this corresponds to computing 洧3 = 洧1 + 洧녩1 + 洧녩2, which gives the robot state 3 Figure 4. Attention pattern for efficient fine-tuning with shared observation. We pack one shared observation 洧녶洧노 and multiple offset branches (洧멇롐+ 洧, 洧냢洧노+ 洧) into single sequence. Blue and yellow cells indicate allowed attention, while gray cells indicate masked attention. Positional encodings of each offset branch are reassigned to start at the same index, equal to the length of observation tokens. at the start of the execution interval. During the forward pass, VLASH feeds both the current environment observation 洧녶1 and this rolled-forward future state 洧3 into the VLA. In this way, the model generates actions for the state at the execution-time rather than for the stale state at inference start, bridging the gap between prediction and execution in terms of robot state. While the future environment is still unknown, this mechanism mirrors how humans act under reaction delays: we react to the world with slightly outdated visual input, but use our internal body state to anticipate what we will do when the action actually takes effect. Thus, humans inherently have the ability to compensate for such reaction delay, and we expect VLAs to possess the same capability. 4.2. Fine-tuning with Offsets to States and Actions The future-state-awareness assumes that the VLA is able to leverage the rolled-forward robot state. However, we find that existing VLAs often fail to exploit this future state properly. Even more, current VLAs appear to largely rely on visual input and under-utilize the robot state. In our experiments with 洧랢0.5  (Table 1)  , fine-tuning without state input (visual only) consistently outperforms fine-tuning with state input on LIBERO [23]. Therefore, simply feeding future robot state at test time is insufficient to achieve accurate and stable asynchronous control. Since large VLAs are almost always fine-tuned on downstream data before deployment, we design training augmentation that can be seamlessly integrated into the standard fine-tuning stage with no additional overhead. We keep the architecture and fine-tuning pipeline unchanged, and only Figure 5. Action quantization for efficient execution. We group consecutive fine-grained micro-actions into coarser macro-actions to accelerate robot motion. The original trajectory with finegrained actions 洧녩0, 洧녩1, 洧녩2, . . . (gray) is quantized into shorter trajectory with macro-actions 틙洧녩0, 틙洧녩1, 틙洧녩2, 틙洧녩3 (black), where each macro-action summarizes 洧 consecutive fine-grained actions (e.g., 틙洧녩0 = 洧녩0 + 洧녩1 + 洧녩2 for quantization factor 洧 = 3). modify how training samples are constructed. Concretely, given trajectory {(洧녶洧노 , 洧멇롐 , 洧녩洧노 )}, standard fine-tuning trains the model to predict the action chunk 洧녩洧노:洧노+洧냩 1 from (洧녶洧노 , 洧멇롐 ). We instead apply simple temporaloffset augmentation with two key steps: (i) Offset state and action together. We sample random offset 洧 from predefined range (e.g., 洧 {0, . . . , 풊max}) and construct training targets from the future state 洧멇롐+ 洧 and future action chunk 洧녩 (洧노+ 洧 ):(洧노+ 洧+洧냩 1) on the same trajectory. (ii) Fix the environment observation. For each timestep 洧노, we always use the same visual input 洧녶洧노 when varying 洧. Therefore, the model is trained to predict 洧녩 (洧노+ 洧 ):(洧노+ 洧+洧냩 1) from the pair (洧녶洧노 , 洧멇롐+ 洧). Under this scheme, the same image 洧녶洧노 can correspond to different ground-truth actions depending on the offset robot state 洧멇롐+ 洧. To fit the data, the VLA is forced to attend to the state input rather than overfitting purely to visual features. In particular, it learns to interpret 洧멇롐+ 洧 as meaningful future state for action selection. We randomly sample 洧 during training because, in practice, the same VLA may be deployed on hardware with different compute budgets, leading to different inference delays 풊, and sometimes even in synchronous settings where there is no gap between prediction and execution. By training over range of offsets, our augmentation makes the model compatible with different inference delays while preserving performance in the synchronous case. At deployment with asynchronous inference, we can then feed the rolled-forward execution-time state together with the current observation, and the fine-tuned VLA naturally leverages this future state to produce actions that are aligned and stable over the execution interval. 4.3. Efficient Fine-tuning with Shared Observation The temporal-offset augmentation creates multiple stateaction pairs for the same observation 洧녶洧노 . naive implementation would treat each offset 洧 as separate training example, i.e., run the VLA independently on (洧녶洧노 , 洧멇롐+ 洧, 洧냢洧노+ 洧) for each sampled 洧. This implementation is completely plug-and-play and can be seamlessly integrated into existing VLA fine-tuning pipeline. However, it repeatedly encodes the same observation 洧녶洧노 for every offset, leaving substantial room for further efficiency gains. Instead, we exploit the fact that all offsets share the same observation 洧녶洧노 and design an efficient attention pattern that reuses the observation tokens across offsets in single pass  (Fig. 4)  . Concretely, we pack one observation and multiple offset branches into single sequence: [洧녶洧노 , (洧멇롐 , 洧냢洧노 ), (洧멇롐+1, 洧냢洧노+1), . . . , (洧멇롐+풊max , 洧냢洧노+풊max )], where each (洧멇롐+ 洧, 洧냢洧노+ 洧) corresponds to one temporal offset. We then apply block-sparse self-attention mask with the following structure: All observation tokens (e.g., image tokens from two views and language prompt, about 700 tokens for 洧랢0.5) can attend to each other, as in standard VLA fine-tuning. state-action tokens (洧멇롐+ 洧, 洧냢洧노+ 洧) can attend to all observation tokens and to tokens within the same offset, but cannot attend to tokens from other offsets. each offset branch, For the This attention map, illustrated in Fig. 4, makes different offsets condition on shared observation while remaining independent of each other. For each offset branch, the positional encodings of (洧멇롐+ 洧, 洧냢洧노+ 洧) are assigned to start at the same index, equal to the length of observation tokens. From the models perspective, this is equivalent to training on multiple (洧녶洧노 , 洧멇롐+ 洧, 洧냢洧노+ 洧) examples that share the same 洧녶洧노 , but we only encode 洧녶洧노 once. For 洧랢0.5, an observation with two images and language prompt corresponds to 700 tokens, while one state and action chunk are about 50 tokens [16]. Therefore, packing 洧녜 洧 = 5 offsets into single sequence therefore increases the token length by only 20%, while the number of effective training trajectories becomes 5 larger. In practice, under the same effective batch size as standard fine-tuning, this method can significantly improve training efficiency by reusing each observation across multiple offset targets in single pass. 4.4. Action Quantization With asynchronous inference and future-state-awareness, the model inference time is effectively hidden behind execution. Once this inference latency is removed, the overall speed of the system is primarily limited by how fast the robot can physically execute the action sequence. To push the execution speed further, we need to accelerate the motion itself. Our approach is to quantize actions, in analogy to weight quantization for LLMs [11, 22, 37]. State-of-the-art VLAs are typically trained on fine-grained teleoperation data (e.g., 5 Figure 6. Performance on Kinetix benchmark. We evaluate the success rate under different execution horizons 洧 and inference delays 풊. Left: Fixed inference delay 풊 = 1 with varying execution horizon 洧. Right: Execution horizon adapts to inference delay, i.e., 洧 = max(풊, 1), with varying 풊. For the Sync baseline, inference delay is always 풊 = 0, but the execution horizon 洧 follows the same settings as other baselines for fair comparison. 50 Hz control with small deltas at each step) [3, 16], which leads to action sequences with high granularity. However, many short micro-movements are more precise than what is actually required to solve the tasks. In LLMs, 16-bit weights provide high numerical precision, but quantizing them to 8bit or 4-bit can substantially accelerate inference with only mild drop in accuracy [11, 22, 37]. We apply the same philosophy to robot control. Given fine-grained action sequence {洧녩0, 洧녩1, . . . , 洧녩洧녢 }, we group consecutive actions into coarser macro-actions. For chosen quantization factor 洧, we construct new sequence { 틙洧녩0, 틙洧녩1, . . . } where each macro-action summarizes block of 洧 fine-grained actions. For delta actions, this can be implemented as 틙洧녩洧녰 = 洧녩洧녰洧 + 洧녩洧녰洧+1 + + 洧녩 (洧녰+1)洧 so that 틙洧녩洧녰 takes the robot approximately from the start state of 洧녩洧녰洧 to the end state of 洧녩 (洧녰+1)洧1 in single, longer step. Fig. 5 illustrates this process: the original fine-grained trajectory (gray) is replaced by shorter, quantized trajectory (black) with macro-actions 틙洧녩0, 틙洧녩1, 틙洧녩2, 틙洧녩3, where 틙洧녩0 = 洧녩0 + 洧녩1 + 洧녩2. Executing macro-actions instead of all micro-actions increases the distance moved per control step, effectively speeding up the robots motion. The temporal granularity of control becomes coarser, but in many tasks the robot does not need to visit every intermediate waypoint explicitly; moving directly between sparser waypoints is sufficient to achieve the goal. As result, action quantization offers tunable speed-accuracy trade-off: small quantization factors behave like the original fine-grained policy, while larger factors yield progressively faster but less fine-grained motion. In practice, we select task-dependent quantization factors that maintain success rates close to the unquantized policy while substantially reducing the number of executed steps. Table 1. Performance on LIBERO benchmarks with different inference delays. We evaluate 洧랢0.5 [16] across four LIBERO subbenchmarks (Spatial, Object, Goal, LIBERO-10) under various inference delays (0 to 4 steps). SR: average success rate; Steps: average execution steps to task completion; Time: completion time on laptop RTX 4090 GPU (inference latency: 103ms for 2 images). Sync (w/o state): fine-tuned and evaluated with synchronous inference without robot state input. Method Delay Success Rate (%) Average Improvement Spatial Object Goal LIBERO-10 SR Steps Time (s) 풊 SR Speedup Sync Sync (w/o state) VLASH (Async) 0 1 2 3 4 97.3 98.5 98.8 97.5 94.4 92. 99.6 99.6 99.2 99.2 98.8 96.9 96.7 97.3 96.7 97.3 93.3 93.3 93.5 95.4 94.4 94.6 91.9 89. 96.8 97.7 97.2 97.1 94.6 93.1 156.0 157.2 153.9 157.6 167.3 176.7 8.4 8.4 7.2 6.4 5.7 5. - +0.9 +0.4 +0.3 -2.2 -3.7 - - 1.17 1.31 1.47 1.45 5. Experiments We design experiments to investigate the following questions: 1. Performance. How does our method compare to synchronous control, naive asynchronous and baselines in terms of accuracy and latency? (Sec. 5.1.1, Sec. 5.2) 2. Generalization. How well does our method generalize across different inference delays? Does it hurt the original model performance? How well does our method generalize across different VLAs? (Sec. 5.1.2) 3. Speed-accuracy trade-off. What is the speed-accuracy trade-off of action quantization at deployment? (Sec. 5.2) 4. Fine-tuning efficiency. How does our method compare to the standard fine-tuning in terms of training cost and data efficiency? How much the shared observation finetuning can reduce the training cost? (Sec. 5.3) 5.1. Simulated Evaluation We evaluate VLASH on simulated robotic manipulation benchmarks including Kinetix [25] and LIBERO [23]. 5.1.1. Kinetix Experimental Setup. Kinetix [25] is highly dynamic simulated robotic manipulation benchmark that demands asynchronous execution to handle rapidly changing environments. The tasks are designed to test dynamic reaction capabilities, including throwing, catching, and balancing. Following the setup in RTC [4], we train action chunking flow policies with prediction horizon of 洧냩 = 8 and 4-layer MLP-Mixer [35] architecture for 32 epochs. We report average success rates across 12 tasks, each evaluated with 1,024 rollouts per data point, under simulated delays ranging from 0 to 4 steps. We compare against the following baselines: Sync. This baseline serves as an optimal baseline for all tasks. The inference delay is explicitly set to 0 at all times. Naive async. This baseline is the naive asynchronous inference baseline, which simply switches chunks as soon as the new one is ready [31]. RTC. This baseline is the Real-time Chunking [4], which freezes the actions guaranteed to execute and inpaints the rest. This introduces additional overhead at runtime. Results. As shown in Fig. 6, VLASH tracks the synchronous upper bound closely across execution horizons, while other baselines drop more noticeably as the execution horizon increases. When the inference delay increases, VLASH remains robust and consistently achieves high success rates, while RTC degrades rapidly and the Naive Async baseline collapses under larger delays. Notably, at inference delay of 4 steps, VLASH achieves 81.7% success rate compared to only 51.2% for Naive Async, which is substantial 30.5% accuracy improvement. Overall, VLASH effectively mitigates prediction-execution misalignment, delivering high success rates under asynchronous operation. 5.1.2. LIBERO Experimental Setup. We evalute on LIBERO benchmark [23], one of the popular benchmarks for evaluating VLA, which includes 4 different sub-benchmarks (Spatial, Object, Goal, and LIBERO-10) that contain 10 tasks each. We evaluate on 2 state-of-the-art VLAs: 洧랢0.5 [16] and SmolVLA [31]. We report the performance by fine-tuning all models on the training dataset for 30K iterations with batch size of 32. Following the setup in 洧랢0.5 [16], we set the execution horizon to 洧 = 5 [10]. Since LIBERO tasks involve slowly changing environments with mild state transitions, different asynchronous methods behave similarly. Therefore, we focus our comparisons on synchronous inference to evaluate the effectiveness of VLASH under various inference delays. For time measurement, we use laptop RTX 4090 GPU where the inference latency with 2 input images is 103ms. For synchronous inference, the time per action chunk is the sum of execution duration (166ms for 6 Figure 7. Real-world evaluation results on manipulation tasks. We evaluate 洧랢0.5 [16] on three tasks with different inference methods. Left: Score percentages (based on 2-point scoring: 1 for success of picking up the object, 1 for task completion) of VLASH and baselines across three tasks. Right: Task completion times with green arrows indicating speedup of VLASH (洧=2) relative to synchronous baseline. VLASH (洧) applies action quantization with quantization ratio 洧. 洧 = 5 steps at 30Hz) and inference time. For asynchronous inference, larger delays are needed to overlap with the inference latency, so the time per action chunk is: execution duration + max(0, inference time execution duration delay). 洧 Results. As shown in Table 1, VLASH demonstrates strong performance across all LIBERO benchmarks under various inference delays. With small inference delays, VLASH maintains comparable accuracy to synchronous inference while achieving speedups of 1.17 and 1.31, respectively. As the inference delay increases, the time advantages become more pronounced, achieving up to 1.47 speedup at delay 3. Although accuracy decreases slightly at higher delays, VLASH still achieves strong performance across all tasks, demonstrating an effective accuracy-latency trade-off. We also evaluate on SmolVLA [31], with detailed results provided in supplementary materials. 5.2. Real-World Evaluation To evaluate VLASH in real-world settings, we deploy 洧랢0.5 [16] on two robotic platforms: the Galaxea R1 Lite [13] and the LeRobot SO-101 [15]. The R1 Lite is dual-arm robot equipped with two 7-DOF arms from Galaxea [12]. The SO-101 is 6-DOF collaborative robotic arm from LeRobot [5]. For 洧랢0.5, we apply projection layer to map the robot state into an embedding, bypassing the tokenizer instead of incorporating it into the language prompt in the original implementation. We design our real-world experiments to evaluate three key aspects: (1) Accuracy: the success rate of completing manipulation tasks; (2) Efficiency: the task completion time and motion smoothness; and (3) Reaction speed: the latency to react to dynamic changes in the environment. 5.2.1. Accuracy and Efficiency the setup Experimental Setup. Following in SmolVLA [31], we evaluate 洧랢0.5 (洧냩 = 50) on three manipulation tasks that test different aspects of robotic control. We set the execution horizon to 洧 = 24 steps at 30Hz. All experiments are conducted on laptop with NVIDIA RTX 4090 GPU, with an inference delay of 4 steps. On our robotic platforms, we evaluate three tasks: Pick and Place: pick up cube from varying starting positions and place it into fixed box; Stacking: pick up blue cube and stack it on top of an orange cube, where both cubes initial positions vary across episodes; Sorting: sort cubes by color, placing the orange cube in the left box and the blue cube in the right box, with cube positions varying across episodes. For each task, we conduct 16 rollouts per method and report both the score percentage and the task completion time. The score percentage is calculated based on 2-point scoring system per rollout: 1 point for successfully picking up the object, and 1 point for completing the task. We compare synchronous inference, naive asynchronous inference, and VLASH across these tasks. Results. As shown in Fig. 7, VLASH delivers better or comparable score percentage to synchronous inference while significantly reducing task completion time across all tasks. Specifically, VLASH maintains an 94% average score percentage, outperforming synchronous baseline (83%) and naive asynchronous inference (89.7%), while completing tasks in 18.8 seconds on average compared to 21 seconds for synchronous inference, which is 1.12 speedup. Furthermore, by applying action quantization, we can Table 2. Reaction speed comparison across devices. Latency of 洧랢0.5 [16] with 1 image input, 洧 = 25 at 50Hz. Execution duration is 500ms. Max reaction latency = execution duration + inference latency for Sync, inference latency only for Async. Inference Delay RTX 5090 RTX 4090 RTX 5070 (in ms) (in action steps) Reaction (ms) Sync Async Speedup 30.4 1.52 530.4 30.4 17.4 36.1 1.81 536.1 36.1 14.9 64.1 3.21 564.1 64.1 8.8 achieve greater speedups with minimal accuracy loss. VLASH with 洧=2 achieves up to 2.03 speedup, while maintaining the original accuracy. With more aggressive quantization ratio of 洧=3, VLASH achieves the faster execution at up to 2.67 speedup, with only modest 4.7% drop in average score percentage, which demonstrates favorable speed-accuracy trade-off. 5.2.2. Reaction Speed Experimental Setup. To evaluate the reaction speed improvement of asynchronous inference, we compare the maximum reaction latency between synchronous and asynchronous inference across different hardware configurations. Following the setup in 洧랢0.5 [16], we set the execution horizon to 洧 = 25 for synchronous inference and control frequency of 50Hz [4, 16], resulting in an execution duration of approximately 0.5 seconds per action chunk. We measure the model inference latency of 洧랢0.5 on three different GPUs: RTX 5090, RTX 4090, and RTX 5070, using torch.compile to enable CUDAGraph optimization and kernel fusion for minimal latency [2]. Results. As shown in Table 2, asynchronous inference significantly reduces the maximum reaction latency compared to synchronous inference, achieving up to 17.4 speedup. To showcase the fast reaction and smooth control capabilities of VLASH, we train 洧랢0.5 to perform highly dynamic interactive tasks: playing ping-pong with human and playing whack-a-mole. These tasks demand both rapid reaction to dynamic changes and smooth continuous motion to maintain control accuracy. To the best of our knowledge, we are the first to demonstrate VLA successfully playing pingpong rallies with human. Under synchronous inference, the robots reaction is too slow to track the fast-moving ball, while VLASH enables real-time response and stable rallies. We encourage readers to view the demo videos in the supplementary materials to see the dynamic performance of VLASH in action. Table 3. Fine-tuning efficiency. Original (without offset augmentation) vs VLASH (with offset augmentation and shared observation) on LIBERO with 洧랢0.5 [16]. Training on 4H100 GPUs using DDP, with effective batch size 16 per GPU (total 64). We report average LIBERO scores at different training steps. Both evaluated under synchronous inference. Method Time/Step (ms) Original VLASH Speedup 420.99 129.29 3.26 Fine-tuning Steps 10K 20K 30K 94.1 87.1 97.1 94. 96.8 96.6 - - - 5.3. Fine-tuning Efficiency Experimental Setup. We evaluate the training efficiency gains from our efficient fine-tuning with shared observation approach. key consideration is that training with multiple temporal offsets using shared observation effectively increases the effective batch size by factor equal to the number of offsets. Therefore, we compare our method against standard fine-tuning under the same effective batch size to ensure fair comparison. Specifically, we conduct experiments on the LIBERO benchmark using 洧랢0.5 [16] trained on 4H100 GPUs with DDP [21]. For our method, we use 풊max = 3 with physical batch size of 4 per GPU, resulting in an effective batch size of 16 per GPU and 64 in global. The standard baseline uses physical batch size of 16 per GPU to match this effective batch size. Both methods are trained for 10K, 20K, and 30K iterations, and we report the average success rate across all LIBERO tasks. We also measure the training time per forward-backward pass to quantify the speedup. Results. As shown in Table 3, VLASH converges more slowly in the early stages but ultimately achieves comparable accuracy to standard fine-tuning. Although more training steps are needed for convergence, each step is significantly faster, achieving 3.26 speedup per step. This efficiency gain comes from encoding the shared observation only once and reusing it across all temporal offsets. Furthermore, since both methods are evaluated under synchronous inference, these results also demonstrate that VLASH does not hurt the original synchronous performance of the model. 6. Conclusion We present VLASH, general and efficient framework for enabling asynchronous inference in Vision-LanguageAction models. By making the policy future-state-aware through simple state rollforward, VLASH effectively bridges the prediction-execution gap that has hindered asynchronous control. Experiments on both simulated and real-world 8 benchmarks demonstrate that VLASH achieves smooth, accurate, and fast-reaction control, consistently matching or surpassing the accuracy of synchronous inference while providing substantial speedups. Moreover, we demonstrate that VLAs can perform highly dynamic tasks such as playing ping-pong rallies with humans. We hope these results will inspire future research toward extending VLAs to more dynamic and physically interactive domains."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank MIT-IBM Watson AI Lab, Amazon and National Science Foundation for research. We thank NVIDIA for donating the DGX server. supporting this"
        },
        {
            "title": "References",
            "content": "[1] Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer. arXiv preprint arXiv:2510.03342, 2025. 1 [2] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, C. K. Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Shunting Zhang, Michael Suo, Phil Tillet, Xu Zhao, Eikan Wang, Keren Zhou, Richard Zou, Xiaodong Wang, Ajit Mathews, William Wen, Gregory Chanan, Peng Wu, and Soumith Chintala. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, page 929947, New York, NY, USA, 2024. Association for Computing Machinery. 8 [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. 洧랢0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 2, 5 [4] Kevin Black, Manuel Galliker, and Sergey Levine. RealarXiv time execution of action chunking flow policies. preprint arXiv:2506.07339, 2025. 1, 2, 3, 6, 8 [5] Remi Cadene, Simon Alibert, Alexander Soare, Quentin Gallouedec, Adil Zouitine, Steven Palma, Pepijn Kooijmans, Michel Aractingi, Mustafa Shukor, Dana Aubakirova, Martino Russi, Francesco Capuano, Caroline Pascal, Jade Choghari, Jess Moss, and Thomas Wolf. Lerobot: State-ofthe-art machine learning for real-world robotics in pytorch. https : / / github . com / huggingface / lerobot, 2024. 7 [6] Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Joao Silverio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Muhammad Zubair Irshad, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart캼n-Mart캼n, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Vitor Guizilini, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models, 2025. [7] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. [8] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. [9] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels, 2024. [10] Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, and Sergey Levine. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better, 2025. 6 [11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. 5 [12] Ltd. Galaxea AI Technology Co. Galaxea ai. https:// galaxea-ai.com/cn, 2025. [13] Ltd. Galaxea AI Technology Co. R1 lite. https:// galaxeaai.com/cn/products/R1Lite, 2025. 7 [14] Weifan Guan, Qinghao Hu, Aosheng Li, and Jian Cheng. Efficient vision-language-action models for embodied manipulation: systematic survey. arXiv preprint arXiv:2510.17111, 2025. [15] Inc. Hugging Face. So-101. https://huggingface. co/docs/lerobot/en/so101, 2025. 7 [16] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. 洧랢0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. 1, 2, 3, 5, 6, 7, 8 [17] Titong Jiang, Xuefeng Jiang, Yuan Ma, Xin Wen, Bailin Li, Kun Zhan, Peng Jia, Yahui Liu, Sheng Sun, and Xianpeng Lang. The better you learn, the smarter you prune: Towards efficient vision-language-action models via differentiable token pruning, 2025. [18] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, Vitor Guizilini, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Muhammad Zubair Irshad, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Mart캼n-Mart캼n, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset, 2025. [19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2 [20] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success, 2025. [21] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704, 2020. 8 [22] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. In MLSys, 2024. 5 [23] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. 4, 6, 10 Peng, Feifei Feng, and Jian Tang. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation, 2025. [37] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models, 2024. 5 [38] Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, and Heng Tao Shen. survey on efficient vision-language-action models, 2025. [39] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, and Xin Jin. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge, 2025. [40] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, and Tsung-Yi Lin. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models, 2025. [41] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with lowcost hardware. arXiv preprint arXiv:2304.13705, 2023. 1 [42] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with lowcost hardware, 2023. 3 [43] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. 2 [24] Yunchao Ma, Yizhuang Zhou, Yunhuan Yang, Tiancai Wang, and Haoqiang Fan. Running vlas at real-time speed. arXiv preprint arXiv:2510.26742, 2025. 1, 2 [25] Michael Matthews, Michael Beukman, Chris Lu, and Jakob Foerster. Kinetix: Investigating the training of general agents through open-ended physics-based control tasks. 2025. 2, 6 [26] NVIDIA, Nikita Cherniadev Johan Bjorck andFernando Castaneda, Xingye Da, Runyu Ding, Linxi Jim Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: An open foundation model for generalist humanoid robots. In ArXiv Preprint, 2025. 1, 2 [27] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for visionlanguage-action models, 2025. [28] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. Spatialvla: Exploring spatial representations for visual-language-action model, 2025. [29] Kohei Sendai, Maxime Alvarez, Tatsuya Matsushima, Yutaka Matsuo, and Yusuke Iwasawa. Leave no observation behind: Real-time correction for vla action chunks. arXiv preprint arXiv:2509.23224, 2025. 1, 2, 3 [30] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. [31] Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. 1, 2, 3, 6, 7 [32] Zhi Su, Bike Zhang, Nima Rahmanian, Yuman Gao, Qiayuan Liao, Caitlin Regan, Koushil Sreenath, and Shankar Sastry. Hitter: humanoid table tennis robot via hierarchical planning and learning. arXiv preprint arXiv:2508.21043, 2025. [33] Galaxea Team. Galaxea g0: Open-world dataset and dualsystem vla model. arXiv preprint arXiv:2509.00576v1, 2025. [34] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. 1 [35] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlpmixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:2426124272, 2021. [36] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin 11 VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Appendix 7.1. SmolVLA Results on LIBERO Benchmarks To further evaluate the generalization of VLASH across different VLAs, we conduct additional experiments on SmolVLA-450M [31], compact yet efficient visionlanguage-action model. Following the same experimental setup as described in Sec. 5.1.2, we fine-tune SmolVLA on the LIBERO benchmark [23] for 30K iterations with batch size of 32. We evaluate the model across four LIBERO subbenchmarks (Spatial, Object, Goal, and LIBERO-10) under various inference delays ranging from 0 to 4 steps, with an execution horizon of 洧 = 5. As shown in Table 4, VLASH achieves consistent speedups across all inference delays when applied to SmolVLA. At delay 2 and 3, VLASH achieves up to 1.35 speedup compared to synchronous inference. While the success rate shows minor variations across different delays, VLASH at delay 3 achieves 79.06% success rate, which is comparable to the synchronous baseline (78.96%), demonstrating that VLASH can maintain performance while providing significant latency improvements. These results further validate that VLASH generalizes effectively across different VLA architectures. 7.2. Experimental Details We present the detailed training hyperparameters used for fine-tuning VLAs in our experiments in Table 5. For all experiments on LIBERO benchmarks and real-world tasks, we use same hyperparameters to ensure fair comparison across different methods and models. These hyperparameters are carefully tuned to balance training stability and convergence speed while preventing overfitting on the downstream tasks. Table 5. Training hyperparameters for fine-tuning VLAs. We use these hyperparameters for fine-tuning 洧랢0.5 and SmolVLA on LIBERO and real-world tasks. Hyperparameter Value Training Configuration Batch Size Training Steps 32 30, Optimizer (AdamW) Learning Rate Betas Weight Decay Learning Rate Scheduler 5e-5 [0.9, 0.95] 1e-10 Type Warmup Steps Peak Learning Rate Decay Learning Rate Decay Steps Cosine Decay with Warmup 1,000 5e-5 2.5e-6 30,000 7.3. Supplementary Demo Video We provide comprehensive video demonstrations comparing our method against synchronous and naive asynchronous baselines across various real-world manipulation tasks. All demonstrations are conducted using 洧랢0.5 [16] deployed on laptop with NVIDIA RTX 5090 GPU, achieving an inference frequency of 15Hz."
        },
        {
            "title": "We showcase the following tasks in the supplementary",
            "content": "materials: Ping-pong: Interactive rallies with human player, demonstrating rapid reaction capabilities. Whack-a-mole: Fast-response game requiring quick detection and precise striking motions. Pick and place: Standard manipulation task showing smooth motion control. Table 4. Performance on LIBERO benchmarks with SmolVLA-450M and different inference delays. We evaluate SmolVLA across four LIBERO sub-benchmarks (Spatial, Object, Goal, LIBERO-10) under various inference delays (1 to 4 steps). SR: average success rate; Steps: average execution steps to task completion; Latency: inference latency in seconds."
        },
        {
            "title": "Delay",
            "content": "Success Rate (%)"
        },
        {
            "title": "Goal",
            "content": "LIBERO-10 SR"
        },
        {
            "title": "Steps",
            "content": "Latency (s) 풊 SR"
        },
        {
            "title": "Sync",
            "content": "VLASH (Async) 0 1 2 3 4 81.25 80.00 78.54 79.79 73.13 92. 85.83 92.91 92.08 94.17 93.54 82.29 86.88 87.50 84.38 55.83 53.13 55.00 54.79 53.33 78.96 198.70 77.08 78.12 79.06 76.09 199.08 197.83 197.68 203.64 8.82 7.53 6.53 6.52 6. - -1.88 -0.84 +0.10 -2.87 - 1.17 1.35 1.35 1.31 ditional parameters introduced by this state projection layer are negligible: it consists only of linear mapping from the state dimension to the hidden dimension. Moreover, because it is zero-initialized, it completely preserves the pretrained models performance during the initial stages of fine-tuning. Folding clothes: Complex manipulation requiring coordinated movements. We compare three inference modes: synchronous inference, naive asynchronous inference, and VLASH. Additionally, we demonstrate the effects of action quantization, showing how our method can achieve further speedups while maintaining task performance. The video demonstrations clearly show that VLASH produces noticeably smoother motions and faster task completion compared to both synchronous and naive asynchronous baselines. The synchronous baseline often exhibits stuttering behavior due to action stalls, while naive asynchronous inference suffers from prediction-execution misalignment that leads to erratic movements. In contrast, VLASH maintains fluid motion throughout task execution while achieving significant speedup. We encourage readers to view the video to appreciate the dynamic performance improvements of our approach. 7.4. Architectural Modifications key advantage of VLASH is that it requires no architectural modifications to achieve effective performance across diverse VLA models. Since all current VLA models accept robot state inputs, VLASH can be applied directly by simply offsetting the state information during fine-tuning to account for inference delay. This straightforward approach enables the model to learn the temporal alignment between delayed observations and corresponding actions without any changes to the model architecture. like 洧랢"
        },
        {
            "title": "For",
            "content": "standard VLA architectures [3] and SmolVLA [31], which incorporate state projection layer to embed proprioceptive state vectors into continuous representations before feeding them into the transformer backbone, VLASH integrates seamlessly and achieves excellent results out of the box. We further note that VLASH also works directly with 洧랢0.5 [16] without modifications, as demonstrated in our experiments in Table 1. However, 洧랢0.5 employs unique design that converts numerical state values into text tokens and appends them to the language prompt. This text-based encoding forces numerical state values through tokenization and one-hot encoding, disrupting their inherent numerical structure and making it more challenging for the model to learn from state information. For such architectures, we find that adding lightweight state projection like the design of 洧랢0 and injecting the resulting embeddings back into their original positions can further enhance smoothness and stability. simpler alternative is to incorporate the projected state embeddings into the AdaRMSNorm layers as conditioning signals alongside timestep embeddings. While entirely optional (and VLASH already performs well without it), this small architectural enhancement consistently improves control smoothness for 洧랢0.5. Importantly, the ad-"
        }
    ],
    "affiliations": [
        "Caltech",
        "MIT",
        "NVIDIA",
        "Tsinghua University",
        "UC Berkeley",
        "UCSD"
    ]
}