{
    "paper_title": "AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting",
    "authors": [
        "Mohammadreza Ghaffarzadeh-Esfahani",
        "Yousof Gheisari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, a generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates a protein language model (PLM) with supervised fine-tuning (SFT) and a reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by a composite reward signal derived from three ESM-2-based regression predictors, each trained to predict a key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces a diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization. Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes a foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 2 ] . - [ 1 5 1 9 8 1 . 2 0 6 2 : r AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting Mohammadreza Ghaffarzadeh-Esfahani1, Yousof Gheisari1,2, 1Regenerative Medicine Research Center, Isfahan University of Medical Sciences, Isfahan, Iran 2Department of Genetics and Molecular Biology, Isfahan University of Medical Sciences, Isfahan, Iran Correspondence: ygheisari@med.mui.ac.ir, Tel/Fax: +98-3136687087 Abstract Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates protein language model (PLM) with supervised fine-tuning (SFT) and reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by composite reward signal derived from three ESM-2-based regression predictors, each trained to predict key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization. Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics. Keywords: Artificial intelligence, Generative AI, Reinforcement learning, Protein Language Model (PLM), Capsid Engineering, Adeno-associated virus (AAV), Kidney 1. Introduction Gene therapy has emerged as revolutionary approach for treating inherited and acquired diseases, with adeno-associated viruses (AAVs) standing as one of the most widely adopted vectors in clinical applications[1]. These AAVs are highly valued as delivery vectors due to their non-pathogenic nature, long-term gene expression, and ability to infect different cell types[2]. However, wild-type (WT) AAV serotypes have limitations such as restricted tissue tropism, immune recognition, and variable transduction efficiency[3]. Engineered AAV capsids can evade pre-existing neutralizing antibodies[4], penetrate physiological barriers such as the blood brain 1 barrier[5], and achieve targeted delivery of therapeutic genes to desired tissues, including the liver[6], muscle[7], and the central nervous system[8]. Notably, the kidney represents challenging target for AAV-mediated gene therapy, given its critical role in metabolic homeostasis and genetic disorders[9]. The unique anatomical features of the kidney, including the glomerular filtration barrier and heterogeneous cellular populations, pose distinct challenges for efficient viral transduction[10]. Existing AAV serotypes demonstrate limited kidney tropism and variable transduction efficiency across renal cell types, underscoring the need for precision-engineered vectors with enhanced renal selectivity[11]. Several techniques have emerged to engineer AAV capsids, each leveraging distinct principles to overcome the constraints of native serotypes. One foundational approach involves harnessing natural variants[12], where sequences from diverse AAV isolates such as AAV2, AAV8, or AAV9 are shuffled or combined to incorporate advantageous traits, as exemplified by the creation of Anc80L65 for improved central nervous system transduction[13]. Subsequently, rational design relies on modification of the capsid using high-resolution structural insights to introduce targeted mutations, such as altering surface loops to modify receptor binding or inserting peptides for tissue-specific targeting, thereby enabling precise control over vector properties without exhaustive screening[14]. Alternatively, directed evolution is used to generate large mutant libraries through error-prone PCR or DNA shuffling, followed by iterative selection in cell cultures or animal models for desired phenotypes[15]. More recently, artificial intelligence-based approaches have accelerated capsid design by training predictive models on sequence-function datasets to predict the viability of generated capsids[16]. While these methods have yielded clinically promising capsids such as those advancing in clinical trials for neuromuscular disorders[17], they face important challenges like simultaneously optimizing multiple traits, such as production fitness, tissue tropism, and thermostability. In this study, we have developed AAVGen, generative protein language model (PLM) that integrates supervised fine-tuning (SFT)[18] with reinforcement learning via Group Sequence Policy Optimization (GSPO)[19] to design functionally optimized AAV capsids for kidney tropism. By coupling large-scale PLMs with data-driven reward functions derived from experimentally validated assays, AAVGen learns the complex sequencefunction relationships underlying AAV capsid performance. AAVGen is trained using three regression models derived from ESM-2 that estimate production fitness, renal tropism, and thermostability, and these predictions are integrated into composite multi-objective reward function to steer the reinforcement learning process. This framework enables the de novo generation of diverse, biologically plausible AAV capsid variants with enhanced functional properties, while preserving the structural integrity of the WT scaffold. Together, these innovations establish AAVGen as powerful platform for datadriven viral capsid engineering, offering scalable and generalizable paradigm for multi-property optimization in synthetic capsid design. 2. Results To develop AAVGen, generative PLM for designing AAV capsids, we used SFT along with reinforcement learning via GSPO. The process began by training three ESM-2-based regression models[20] to serve as reward functions, predicting production fitness, kidney tropism, and thermostability from sequence. Production fitness reflects the capsids packaging efficiency during expression, kidney tropism quantifies the vectors ability to transduce the kidney, and thermostability measures resistance to thermal degradation, critical for storage and in vivo delivery. The ProtGPT2 model[21] was first fine-tuned on AAV2 and AAV9 VP1 datasets to learn foundational residueresidue relationships. GSPO was then applied to refine this model, using composite reward from the regression predictors to promote the generation of capsids with improved multi-property profiles. Finally, we assessed the generated sequences based on their uniqueness, sequence length distribution, and alignment. We also used regression scoring and 2 AlphaFold3 structural modeling to compare their predicted function and structure to WT AAV2. The schematic design of the study is illustrated in Figure 1. Figure 1: Development and assessment workflow of AAVGen. The left upper panel illustrates the dataset curation process. The right upper panel details the model training phase, which includes supervised fine-tuning (SFT), custom reward modeling, and group sequence policy optimization (GSPO). The lower panel outlines the assessment procedure, comprising generation analysis, evaluation of production fitness, kidney tropism, thermostability, and structural analysis. 2.1. Regression models effectively predict production fitness, kidney tropism, and thermostability of AAV variants To develop AAVGen, we trained three regression models on the AAV2 dataset to predict production fitness, kidney tropism, and thermostability of generated capsid variants. Regression models were employed to capture the correlation between sequences and experimentally measured values. The ESM-2 model was first fine-tuned on the VP1 sequencefitness dataset until the loss plateaued at approximately 14,100 training steps. Subsequently, the fitness model was further fine-tuned on the kidney tropism dataset to transfer knowledge from the fitness dataset and trained for 26,600 steps. Similarly, the fitness model was used to fine-tune thermostability model on its respective dataset until convergence at 37,500 steps (Figure 2A, Supplementary files 1, 2, and 3). Furthermore, the predictive performance of models was evaluated by measuring the correlation between predicted and actual values. The model trained for production fitness achieved Spearman correlation coefficient of 0.91 with p-value of < 10n, demonstrating strong predictive performance. The kidney tropism model showed moderate correlation 3 (Spearman ρ = 0.35, p-value < 10n), while the thermostability model exhibited weaker but statistically significant correlation (Spearman ρ = 0.26, p-value < 10n) (Figure 2B). Figure 2: Evaluation of regression models. (A) Training loss progression for models predicting production fitness, kidney tropism, and thermostability. (B) Correlation between experimentally determined (true) and model-predicted scores for production fitness, kidney tropism, and thermostability. 2.2. Policy optimization with customized rewards produced enhanced AAV variants To guide AAVGen training with the GSPO framework, we used three regression models trained before as reward functions to fine-tune ProtGPT2-based model that had been previously finetuned on AAV2 and AAV9 datasets. Training proceeded until the total composite reward function plateaued, indicating convergence (Figure 3A, Supplementary file 4). This composite reward consisted of three main components: production fitness, kidney tropism, and thermostability rewards (Figure 3B) plus two auxiliary rewards that stabilized training, including length control reward which prevents the model from collapsing into generating sequences identical or highly similar to the WT length and uniqueness reward which encourages the model to generate unique sequences in each training batch (Supplementary file 5). To obtain set of high-quality capsid variants with improved properties relative to the WT and to mitigate potential prediction errors from the regression models, we designed reward logic mapper that translates the predicted scores from each of these three models into corresponding reward signals by categorizing scores exceeding the WT sequence (Figure 3C). Through this combination of primary and auxiliary rewards, along with the reward logic mapper, the model achieved stable optimization state, as evidenced by each reward component reaching plateau during training. 2.3. AAVGen produces diverse and biologically plausible capsid library To evaluate the generative capability of AAVGen, we analyzed set of 500,000 generated protein sequences across three key metrics. First, we measured the uniqueness of generation to understand how many repetitive sequences our model generates and to quantify the models capacity to explore diverse regions of sequence. Second, we evaluated length distribution to confirm biologically realistic protein dimensions, as deviations from natural VP1 length could compromise 4 Figure 3: Reward progression in training with group sequence policy optimization (GSPO). (A) Total reward over the course of training. (B) Reward progression during training of AAVGen for reward functions that produce production fitness, kidney tropism, and thermostability. (C) Reward assignment logic for each objective, defined by the mean absolute error of model predictions on the validation set relative to the wild-type (WT) reference for production fitness, kidney tropism, and thermostability. capsid assembly and stability (Figure 4A). Finally, we assessed the fidelity to the WT AAV2 to measure the difference between the generated sequences and the WT (Figure 4B). Our analysis of sequence uniqueness revealed that approximately 4% of the generated set was repetitive. Furthermore, after removing duplicates, 1,787 sequences were exact matches to those in the training set. When compared to WT sequences, 230 generated sequences were identical to AAV2, while none matched the AAV9 WT, indicating low rate of exact template replication 5 Figure 4: Sequence diversity and alignment metrics of the AAVGen-generated library. (A) Performance benchmarks of the generative model regarding sequence novelty. Left: Cumulative repetitiveness of generated sequences across increasing subset sizes (N = 1,000) for library of 500,000 variants. Right: Distribution of sequence lengths for the training set (blue) and generated sequences (orange) relative to the wild-type (WT) AAV2 VP1 sequence (dashed line). (B) Alignment-based divergence of generated variants from the AAV2 WT reference. Left: Frequency distribution of edit distances from the WT sequence. The majority of synthetic variants contain between 10 and 15 mutations compared to the WT template. Right: Correlation between sequence similarity and identity percentages. Data points are colored by alignment score (ranging from 1425 to > 1460), illustrating that the generated sequences maintain high conservation (> 98.5% identity) while exploring diverse landscape of substitution patterns. and successful bias toward generating novel variants. The length distribution of the generated sequences (median: 741; IQR: 740743) closely matched that of the training set (median: 741; IQR: 737743). The generated sequences showed high sequence similaritythe percentage of aligned positions with either identical or biochemically similar amino acids(median: 99.32%, IQR: 99.0599.46%) and identitythe percentage of aligned positions with identical amino acids(median: 99.18%, IQR: 98.9199.32%) to AAV2, finding further supported by high alignment scorea numerical value reflecting the overall quality of sequence alignment rewarding matches and penalizing mismatches and gaps (median: 1443.5; IQR: 1438.51448.5). We also used edit distance[22]the minimum number of single-amino-acid substitutions, insertions, or deletions required to convert one sequence into anotherto quantify sequence divergence, finding median difference of 13% (IQR: 1015%) from the AAV2 WT. Collectively, these results confirm that AAVGen successfully generates diverse and expansive library of novel capsid variants that retain the core structural and functional hallmarks of WT AAVs. 2.4. AAVGen generates high-quality sequences in terms of production fitness, kidney tropism, and thermostability To evaluate the functional quality of the generated capsids, we assessed the quality of 500,000 generated sequences. After filtering out repetitive, WT, and training-set duplicates, 436,765 sequences remained. Using regression models, sequences were categorized based on their predicted scores relative to WT and the models mean absolute error (MAE) on the validation set margin: Best (exceeding WT + 4 MAE), Good (between WT + 1 and +4 MAE), Uncertain (between WT and WT + 1 MAE), and Bad for sequences that have lower than WT predicted score. The analysis demonstrates that AAVGen consistently produces sequences with high predicted performance across all three key properties (Figure 5A). Regarding production fitness, the model exhibited exceptional performance in generating high-fitness sequences. The vast majority of records, 435,448 sequences (99.7%), were classified as Best, while 669 (0.15%) were Good, 128 (0.03%) and 559 (0.4%) sequences were Uncertain and Bad respectively. Additionally, for kidney tropism, the output was predominantly composed of Good sequences (491,439; 98.27%). smaller proportion were Uncertain (5,416; 1.24%), 2,155 (0.4%) were Bad, and one sequence achieved the Best classification. For thermostability, no sequences met the criteria for the Best category. Nonetheless, the model produced large number of variants with improved predicted thermostability, with 386,844 sequences (88.57%) classified as Good, alongside smaller fractions categorized as Uncertain (43,626; 9.99%) or Bad (6,295; 1.44%). Having confirmed that the majority of generated sequences were classified as Good or Best with raw predictions exceeding WT levels, we next investigated whether these desirable properties tend to co-occur within individual sequences. To address this, we computed Spearman correlations between the predicted scores for each pair of properties. We observed strong positive correlations across all optimized properties, indicating that improvements in one property are consistently associated with improvements in the others (Figure 5C). This finding demonstrates that AAVGen successfully generates sequences that jointly optimize for all three design objectives rather than trading one property for another (Figure 5D). 2.5. AAVGen diversifies sequences while preserving the structural scaffold To further assess the structural robustness and diversity of the generated sequences, we randomly selected 500 proteins from the Good and Best classes among the 500,000 generated sequences. For each protein, five independent structures were predicted using AlphaFold3[23]. To establish baseline, we randomly generated 250 VP1 sequences by identifying the variable regions of these 500 sequences relative to the WT through sequence alignment and inserting random amino acids matched to the length distribution of the 500 generated sequences. Each baseline sequence was folded using the same strategy. Structural similarity to the WT protein was quantified by calculating the root-mean-square deviation (RMSD). For each sequence, the median RMSD across the five predicted structures was considered as the representative value. As shown in Figure 6A, the RMSD distribution of AAVGen-generated sequences exhibited two distinct modes: one centered around 0.42 Å and another around 0.47 Å, compared with median RMSD of 0.48 Å for the randomly generated baseline sequences. While RMSD generally increases with sequence length across all groups, this relationship was substantially weaker for both AAVGen low and high RMSD subgroups (Spearman ρ = 0.17 and 0.20, respectively) compared to the random baseline (Spearman ρ = 0.61), suggesting that AAVGens sequence diversity is not simply byproduct of length variation. Subsequently, we examined the relationship between RMSD and predicted production fitness, kidney tropism, and thermostability within each group (Figure 6B). In the lower-RMSD sub-group of AAVGen (RMSD < 0.45 Å), we observed statistically significant negative Spearman correlations between RMSD and all three metrics: production fitness (ρ = 0.22), kidney 7 Figure 5: Functional property analysis of generated AAVGen sequences. (A) Qualitative classification of generated sequences into Best, Good, Uncertain, and Bad categories based on predicted production fitness, kidney tropism, and thermostability scores. (B) Pairwise correlation analyses between predicted production fitness, kidney tropism, and thermostability scores. (C) Joint three-dimensional distribution of predicted production fitness, kidney tropism, and thermostability scores, with each point colored according to the average of these three scores. tropism (ρ = 0.23), and thermostability (ρ = 0.25), indicating that sequences more strucIn contrast, no turally similar to the WT tend to score higher across functional predictions. significant correlations were detected in the higher-RMSD group (RMSD 0.45 Å), suggesting that beyond structural deviation threshold, predicted functional properties decouple from structural similarity to the WT. Notably, both AAVGen subgroups vastly outperformed the random baseline across all functional metrics. In the lower-RMSD group, median predicted production fitness, kidney tropism, and thermostability were 0.81, 1.21, and 1.10, respectively, while the higher-RMSD group scored even more favorably at 0.92, 1.34, and 1.22. Both stand in sharp 8 Figure 6: Structural analysis of the generated sequences. (A) Left: Distribution of Root Mean Square Deviation (RMSD) for = 500 sampled protein sequences from AAVGen and = 250 randomly generated relative to the wild-type (WT) reference. For each sequence, the median RMSD was calculated across five independent AlphaFold3 structure predictions. Right: Relationship between sequence length and median RMSD, indicating modest increase in RMSD with longer sequences, while AAVGen sequences consistently maintain lower RMSD compared with random sequences. (B) Correlation analysis between median RMSD and predicted functional attributes. (C) Representative structural alignments of the four generated sequences exhibiting the lowest median RMSD. Structures are superimposed on the WT model. Green regions denote high spatial conservation, where Cα atoms lie within 0.5 Å of the WT reference. Structural divergence is highlighted by red (non-conserved residues in the mutant) and blue (corresponding residues in the WT). contrast to the random baseline, which yielded median values of 4.65, 4.19, and 4.37 for the same metrics. This consistent and substantial gap across all three dimensions confirms that AAVGen generates functionally plausible variants with meaningful biological properties, rather than sequences that happen to resemble the WT structure by chance. Finally, the four variants 9 with the lowest RMSD values are visualized in Figure 6C. 3. Discussion In this study, we introduced AAVGen, generative PLM for designing AAV capsids with enhanced functional properties. By integrating supervised fine-tuning of the ProtGPT2 model on diverse corpus of AAV sequences with GSPO, we developed model capable of generating novel AAV2 VP1 protein variants. Our results demonstrate that AAVGen successfully produces vast and diverse library of capsid sequences that are highly novel yet retain strong structural similarity to WT AAV2. Importantly, the generated variants are predicted by specialized regression models to exhibit enhanced production fitness, kidney tropism, and thermostability, showcasing the models ability to perform multi-trait optimization and generate high-quality, biologically plausible AAV candidates for further development. Building on these results, we developed generalizable framework for training AAVGen for high-quality, versatile AAV sequence generation, enabling effective generation of protein sequences through comprehensive understanding of the VP1 capsid protein. To achieve this, we constructed regression models capable of processing and scoring VP1 sequences of variable lengths, unlike prior studies such as Bryant et al.[24], who employed different neural networks, and Eid et al.[25], who used LSTM-based regression limited to fixed-length inputs. Our approach thus allows for the generation of proteins with variable lengths while maintaining sequence quality. This was accomplished by fine-tuning the ESM-2 model to interpret the entire sequence context and perform scoring directly. In contrast to prior methods such as CAP-PLM[26], which relied on extracted embeddings of ESM-2 and one-hot encodings for regression training, our method fine-tunes the full ESM-2 model jointly with the embedding component. This approach outperformed the earlier technique, achieving superior predictive performance. Nevertheless, while regression and classification models applied to fitness datasets have shown promising results, the development of multifaceted and multi-objective AAV design framework remains major challenge. Zheng et al.[27] attempted to address this by introducing the ALICE system, which leverages contrastive and reinforcement learning to guide AAV capsid generation. However, ALICEs reinforcement learning phase focuses solely on CNS tropism and fails to account for other attributes such as thermostability. Additionally, the reliance of their method on MSA considerably slows training, particularly when scaling to larger datasets. An additional novel aspect of our study is the use of multiple AAV serotypes (AAV2 and AAV9) to fine-tune our base model. Liu et al.[28] have shown that introducing the same mutations from AAV2 into the AAV9 sequence can enhance its fitness, suggesting that different serotypes share meaningful functional and structural relationships. By leveraging sequences from multiple serotypes during model training, we were able to capture these shared features, improving the generalizability and effectiveness of AAVGen. This approach, alongside multipleobjective training, enabled more capable model that addresses previous challenges while enabling comprehensive and scalable AAV design. Based on these methodological foundations, AAVGen effectively learns and exploits the complex sequencefunction relationships underlying AAV capsid performance to design variants with superior multi-trait characteristics. The regression models trained on experimental datasets captured distinct yet interrelated aspects of AAV biology, as reflected by their strong predictive correlations, particularly for production fitness. Integration of these models as reward functions in the GSPO framework enabled AAVGen to perform multi-objective optimization, simultaneously improving fitness, kidney tropism, and thermostability. The generated library of 500,000 sequences exhibited high novelty and diversity while maintaining structural fidelity to WT AAV2. Notably, the vast majority of generated variants displayed predicted fitness surpassing that of the WT, and substantial subsets showed concurrent enhancement in kidney tropism and thermostability. The strong positive correlations among the optimized properties indicate that AAVGen 10 captures shared evolutionary and biophysical constraints governing AAV capsid stability and tropism, suggesting that reinforcement learning guided by multi-property rewards can steer generative models toward biologically coherent and functionally balanced solutions. Collectively, these findings validate the capacity of AAVGen to generate high-quality, biologically plausible AAV capsid sequences with improved multi-trait performance. Complementing these multi-trait functional improvements, structural analysis of the AAVGens generated sequences uncovered an intriguing aspect of the optimization landscape. Specifically, the RMSD values of the generated variants relative to the WT exhibited bimodal distribution. This pattern likely arises from the design of the reward functions and the nature of multi-objective optimization performed by the model. Rather than converging on single structural optimum, AAVGen appears to have learned two primary modes of high-performing solutions, one closely resembling the WT conformation and another representing distinct, yet still viable, structural variant. Both modes yield sequences with substantially improved predicted functional properties compared to random baseline sequences, underscoring the models ability to explore biologically meaningful regions of sequence space while maintaining overall capsid integrity. Although our study yielded promising results, it has several limitations. Foremost, the generated sequence was not experimentally validated in vitro, which precludes definitive conclusions regarding its functional performance. Additionally, training regression models for kidney tropism and thermostability was hindered by the scarcity of high-quality data, resulting in limited predictive power and higher MAE on the validation set. Future studies could benefit from incorporating wet-lab experiments and employing strategies such as those described by Jiang et al.[29], which involve modifying non-surface amino acids to generate viable capsids. Such approaches could expand the dataset size and improve model performance. In summary, this study demonstrates that AAVGen represents significant advancement in AAV capsid design, combining generative modeling with multi-objective optimization to produce highly diverse, novel, and biologically plausible variants. By leveraging sequencefunction relationships through fine-tuned regression models and GSPO-guided generation, AAVGen effectively enhances multiple functional traits while maintaining structural fidelity. Although experimental validation and expanded datasets remain necessary to fully confirm its predictions, our findings establish scalable and versatile framework for AAV engineering, paving the way for the development of next-generation gene therapy vectors with tailored properties. 4. Methods 4.1. Data collection To develop AAVGen, we integrated AAV capsid fitness data from three independent studies. comprehensive fitness landscape for AAV2 was established by Ogden et al.[30] through deep mutational scanning, which included libraries of 31,579 VP1 sequences assessed for production fitness, 24,984 sequences assessed for kidney tropism, and 30,889 sequences evaluated for thermostability. To expand the mutational scope, we incorporated data from Bryant et al.s study[24], which focused on specific region (residues 561588) of the AAV2 VP1 capsid. This study provided large-scale multiple-mutation dataset comprising 296,896 sequences with associated fitness scores. Finally, to include data on different serotype and additional functional attributes, we utilized the AAV9 dataset from Eid et al.[25]. This dataset comprises 100,000 sequences characterized for production fitness, along with key functional outcomes, including binding/transduction in HEPG2 and THLE-2 cell lines and liver biodistribution in mouse model. The combination of these datasets provided robust and multifaceted foundation for training AAVGen. 11 4.2. Data pre-processing To construct unified training dataset from the three independent studies, we implemented comprehensive processing pipeline. The primary challenge was harmonizing diverse data formats, mutation types (insertions, substitutions, and deletions), and experimental assays into consistent structure suitable for model input. Our strategy involved reconstructing the fulllength VP1 amino acid sequence for each variant, normalizing fitness scores to common scale, and removing low-quality or ambiguous data points. The following sections describe the specific processing steps. 4.2.1. Data preparation for regression models We fine-tuned AAVGen to generate high-quality AAV2 capsids by developing three distinct regression models to predict production fitness, kidney tropism, and thermostability of the capsid. Two studies that provided the AAV2 datasets were used to fine-tune regression models. These models were constructed by fine-tuning the ESM-2 PLM to enable estimation of functional properties from amino acid sequences. The resulting regression models served as reward functions within the GSPO framework, guiding the generation process toward variants with superior functional properties. Production fitness. The production fitness dataset was compiled by processing data from two studies: Bryant et al. and Ogden et al. The dataset from Bryant et al.s study provided amino acids and corresponding fitness scores for VP1 positions 561588, which were normalized to the WT score and log2-transformed. From the Ogden et al. study, production fitness was derived from packaging dataset. We obtained deep mutational scanning read counts for AAV2 variants from this source and excluded technical replicates with low coverage. Selection values were then calculated using variant frequencies from plasmid and viral pools, normalized to the WT score, and log2-transformed. The insertion, substitution, and deletion libraries were processed separately. For each variant, the full-length AAV2 sequence was reconstructed by applying the respective mutation. Finally, the datasets from both studies were merged into single variantlevel table containing the reconstructed sequence and its normalized production fitness score. Kidney tropism. To process the kidney tropism dataset, the packing dataset was loaded from the Ogden et al. study, and technical replicates with poor coverage were excluded. Mouse variant counts were further filtered to remove low-abundance variants (< 10 reads) and extremely highabundance variants (> 31,000 reads) to reduce noise from sequencing artifacts. Selection values were computed as the ratio of variant frequencies in mouse tissue samples to their corresponding frequencies in packaged viral pools, normalized to WT variants. Furthermore, replicate measurements were aggregated, and median selection coefficients were calculated. Selection scores were log2-transformed, and non-finite values were removed. For reconstruction of VP1, insertion, substitution, and deletion libraries were processed separately. Full-length AAV2 capsid sequences were regenerated for each variant by applying the corresponding mutation to the reference sequence. Insertion, substitution, and deletion datasets were merged, yielding final dataset that included the reconstructed sequence and the normalized kidney tropism score. Thermostability. The thermostability dataset was constructed using raw AAV2 variant counts obtained from the packaging datasets reported by the Ogden et al. study. This assay measured capsid thermostability by incubating the library at different temperatures and then digesting any exposed genomes. For data processing, technical replicates with low coverage were first excluded. We then calculated selection values by normalizing variant frequencies in the thermostabilityselected pools against their corresponding packaged viral frequencies. The resulting thermostability scores were averaged across replicates and log2-transformed. The insertion, substitution, 12 and deletion libraries were processed separately. Finally, the datasets were merged into unified table containing the reconstructed sequence and normalized thermostability score, from which any variants with missing or non-finite scores were removed. Each dataset was subsequently divided into training and validation subsets using stratified sampling method. Targets were first grouped into 10 quantiles to maintain their distribution across both subsets. This process ensured that the datasets were properly prepared for training the regression models. 4.2.2. Data preparation for SFT To develop generalizable generative model capable of understanding the residue-residue relationships within AAV capsids, we fine-tuned ProtGPT2 on diverse collection of VP1 sequences. This included integrating variants from multiple serotypes, specifically AAV2 and AAV9, to expose the model to broader spectrum of viable amino acid combinations. This foundational step allowed the model to learn the underlying grammar of functional AAV capsids. For supervised fine-tuning of the model, we integrated comprehensive collection of VP1 sequences from the AAV2 (Bryant et al., Ogden et al.) and AAV9 (Eid et al.) studies to teach the model the residue-residue relationships across serotypes. The AAV9 dataset, which characterizes variants based on production fitness, binding/transduction in HEPG2 and THLE-2 cell lines, and liver biodistribution in mice, required specific processing. We generated full-length AAV9 VP1 sequences by using the WT serotype of AAV9 capsid as reference and inserting each variant peptide at the specified site between residues 588 and 589. To select high-performing variants, we included all sequences with fitness scores exceeding that of the WT. These selected sequences were then deduplicated to create the final dataset. The final consolidated corpus contained 192,199 non-redundant VP1 sequences, each annotated with its serotype (AAV2 or AAV9). To ensure compatibility with the ProtGPT2 model architecture, sequences were formatted into FASTA-like structure with amino acids grouped into blocks of 60 characters. An end-of-sequence (EOS) token was appended to the start and end of each formatted sequence. This comprehensive dataset was then partitioned into training (80%) and validation (20%) subsets using fixed random seed, with stratification by serotype to maintain balanced representation of both AAV types. 4.3. AAVGen development 4.3.1. Regression models as reward functions To enable precise prediction of AAV capsid functional traits, we developed three regression models for each of production fitness, kidney tropism, and thermostability properties. These models were created by fine-tuning an ESM-2 PLM with 8 million parameters. This model architecture was selected for its efficiency in capturing sequence-level representations while maintaining computational feasibility for iterative fine-tuning. We employed sequential transfer learning strategy to leverage shared functional features across properties: the production fitness model served as the foundational checkpoint, from which the kidney tropism and thermostability models were further adapted. All models were implemented using the Hugging Face Transformers library (version 4.56.1)[31], with EsmForSequenceClassification configured for regression (num_labels=1). Training involved tokenizing amino acid sequences to fixed maximum length of 755 residues with padding and truncating as needed. Hyperparameters were optimized for convergence and generalization, incorporating mixed-precision training (FP16), gradient accumulation, and early stopping based on validation loss. Training optimization employed the mean squared error (MSE) as the primary objective function across all three regression tasks with AdamW as optimizer, ensuring consistent gradient behavior and stable convergence. This approach ensured robust, property-specific predictors that could be deployed as reward functions within the GSPO framework. 13 Production Fitness. The regression model for production fitness prediction was trained on the merged AAV2 dataset from Bryant et al. and Ogden et al. studies, comprising deduplicated VP1 sequences with log2-transformed, WT-normalized fitness scores. After stratified trainvalidation split, sequences were tokenized using the ESM-2 tokenizer. Training commenced from the ESM-2, utilizing the AdamW optimizer with linear learning rate scheduler (initial rate: 1 104, weight decay: 0.01, warmup steps: 10). We configured 10 epochs, per-device batch size of 16 (effective batch size 128 via 8 gradient accumulation steps), evaluation every 100 steps, and checkpointing every 50 steps (retaining up to 10 checkpoints). Kidney Tropism. Building upon the production fitness checkpoint (selected at step 14,100), kidney tropism prediction model was fine-tuned using the processed AAV2 dataset from the Ogden et al. study. This dataset comprised log2-transformed selection coefficients derived from mouse kidney enrichment ratios, following the exclusion of lowand high-abundance outliers and the aggregation of biological replicates. Tokenized representations of the sequences by the ESM-2 tokenizer were generated before training. Transfer learning was performed using cosine learning rate scheduler with an initial learning rate of 2 106, weight decay of 0.01, and 20 warm-up steps, over total of 10 epochs. The batch sizes were maintained at 16 for training with the gradient accumulation of 32 (effective batch size of 128) and 32 for evaluation. Model evaluation and checkpointing were conducted every 100 training and evaluation steps, respectively. Early stopping with patience of three epochs was implemented to mitigate overfitting. Thermostability. The thermostability prediction model was also adapted from the production fitness checkpoint (selected at step 14,100) using the AAV2 dataset from the Ogden et al. study, which included log2-transformed selection values from temperature-selection assays. Data handling followed the established protocol of aggregation, cleaning, and stratified splitting. Tokenization and dataset formatting were consistent. Due to the subtler signal in thermostability data, we extended training to 500 epochs with highly conservative cosine scheduler (initial rate: 5 107, weight decay: 0.01, warmup steps: 50), preserving batch and evaluation settings (100-step intervals, 20-checkpoint limit). Early stopping patience of 20 epochs was used to allow gradual convergence. These regression models contributed to multi-objective reward system, quantifying tradeoffs in production fitness, kidney tropism, and thermostability to steer generative sampling toward viable AAV variants. 4.3.2. Supervised fine-tuning of the ProtGPT To enable AAVGen to capture the underlying residue-residue relationships across AAV serotypes, we fine-tuned the pretrained ProtGPT2 PLM using curated corpus of high-fitness AAV capsid sequences. ProtGPT2 is decoder-only autoregressive transformer comprising 36 layers with model dimensionality of 1,280 and approximately 738 million parameters pretrained in selfsupervised manner on the UniRef50 (version 2021_04)[32] protein sequence database with It was loaded from the Hugging Face training set of approximately 44.9 million sequences. model hub and fine-tuned using the transformer reinforcement learning (TRL) (version 0.23.1)[33] librarys SFTTrainer implementation. Training was performed on GPU hardware under mixedprecision (FP16) settings to balance computational speed and numerical stability. The model was trained with per-device batch size of 4 and gradient accumulation of 4 steps (effective batch size of 16). Optimization employed the AdamW algorithm (β1 = 0.9, β2 = 0.999, ε = 1 108) with learning rate of 1 104, following linear decay schedule incorporating 1% warm-up ratio. Weight decay was set to 0.01 to mitigate overfitting, and training was carried out for three epochs with maximum sequence length of 300 tokens. Model checkpoints were saved every 100 training steps. 14 4.3.3. Reinforcement learning with GSPO To further optimize the generative models performance and specifically train it to produce AAV capsid sequences with enhanced thermostability, kidney tropism, and production fitness, we employed reinforcement learning technique termed GSPO. This approach builds on the principles of policy gradient methods but emphasizes sequence-level rewards, diverging from token-level optimization strategies like Group Relative Policy Optimization (GRPO). GSPO treats the complete generated sequence as the atomic unit for reward computation and policy updates, ensuring that the optimization signal directly reinforces holistic sequence viability. During training, for each sequence from the dataset D, the current policy πθold (the model at the start of the training step) generates group of candidate sequences: yi πθold( x), = 1, . . . , (1) These sequences are evaluated using suite of reward functions, and the policy parameters θ are updated to favor high-reward generations while penalizing deviations from the reference policy to maintain stability. Given completion to sequence x, its likelihood under the policy πθ is denoted as: πθ(y x) = (cid:89) t= πθ(yt x, y<t) (2) with denoting the number of tokens in y. Each sequence-completion pair (x, y) is scored by composite reward function r(x, y), which aggregates multiple property-specific signals. The advantage ˆAi for each sequence yi is estimated via group-wise normalization to reduce variance: ˆAi = r(x, yi) σr where = 1 (cid:80)G j=1 r(x, yj) and σr = (cid:113) 1 (cid:80)G j=1 (r(x, yj) r(x))2. To quantify policy improvement, GSPO computes sequence-level importance ratio: si(θ) = (cid:18) πθ(yi x) πθold(yi x) (cid:19)1/yi (3) (4) which measures the geometric mean deviation of the new policy πθ from the reference policy πθold across the tokens in sequence yi, normalized by sequence length to account for variable generation lengths. The GSPO loss objective is then formulated as an expectation over the dataset and generations, incorporating clipped surrogate to prevent destructive large updates: JGSPO(θ) = xD, i=1πθold {yi}G (x) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:16) min si(θ) ˆAi, clip(si(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:35) (cid:17) (5) where ϵ is clipping hyperparameter (set to 0.2) that bounds the importance ratio within [1 ϵ, 1 + ϵ]. This objective encourages the policy to increase the probability of high-advantage sequences, while clipping mitigates instability from policy shifts. The importance sampling level was configured at the sequence granularity to align with the reward philosophy. 4.3.4. Reward functions To guide the model toward generating AAV capsid sequences with desired properties, we defined set of sequence-level reward functions. These functions quantify key objectivesproduction fitness, kidney tropism, and thermostabilityalongside auxiliary metrics for sequence length deviation and intra-batch uniqueness. The composite reward r(x, y) was defined as linear combination of five specialized functions that contributed equally to the multi-objective optimization. Each reward was computed post-generation by first converting from FASTA to normal sequences. Together, they provide balanced multi-objective signal that promotes both functional optimization and generative diversity. Production fitness reward. Sequences were passed through the production fitness prediction regression model to obtain predicted scores pi. Rewards were mapped via linear function to emphasize gains over predicted WT production fitness (wfitness 2.5) while accounting for model MAE on the validation set (efitness 0.40): rfitness(yi) = 3a 4 2 2 pi if pi > wfitness + 4efitness, if wfitness + 4efitness pi > wfitness + efitness, if wfitness + efitness pi > wfitness, 2 if wfitness pi , otherwise, (6) where = minj(pj) is the negative minimum of the validation set predictions (pj). The reward mapping and decision thresholds are shown in Figure 3C. Kidney tropism reward. Analogous to fitness, using the kidney tropism prediction model (wkidney 0.16, ekidney 0.83) with the same mapping logic. Thermostability reward. 0.43, ethermostability 1.29). Identical structure via the thermostability prediction model (wthermostability Sequence length controller reward. To ensure that the policy does not converge exclusively toward producing the WT VP1 sequence (with length lwt = 735), we introduced sequence length controller reward function with tolerance parameter σ = 3. This function encourages exploration of sequence lengths that deviate from the WT while maintaining compact and potentially functional designs. The reward is defined as: rlength(yi) = 1 exp (cid:32) (l(yi) lwt)2 2σ2 (cid:33) , (7) where l(yi) represents the length of the generated sequence. This formulation penalizes sequences that closely match the WT length (yielding values near zero) and rewards greater deviations, thus promoting diversity in the generated outputs. Intra-batch uniqueness reward. To discourage mode collapse, sequences were checked for duplicates within the group: runique(yi) = (cid:40) 0 if {j : yj = yi} > 1, 1 otherwise. (8) 4.3.5. Implementation and training procedure Implementation was carried out using custom GRPOTrainer extension from the TRL library, adapted for sequence-level surrogates. The supervised fine-tuned ProtGPT2 model served as the initial policy, with sequences constructed as the ProtGPT2 tokenizers end-of-sequence (EOS) 16 token followed by and the initiating amino acid (i.e., = EOS + nM), mimicking the start of VP1 sequence in FASTA-like format. For each training step, = 32 sequences were autoregressively sampled per sequence using temperature = 1.0, top-p = 1.0, and no top-k filtering, with repetition penalty of 1.0 to encourage diversity. Generations were truncated at maximum completion length of 754 tokens. Training spanned 4 epochs, with per-device batch size 4 and gradient accumulation of 8 (effective batch size 32). Optimization used AdamW (β1 = 0.9, β2 = 0.999, ε = 108) at learning rate 2 106 with cosine decay, weight decay 0.01, max gradient norm 1.0, and 50 warmup steps. FP16 mixed precision, gradient checkpointing, and periodic cache emptying (every 5 steps) ensured memory efficiency. Logging occurred every step, with checkpoints saved per epoch. The initial dataset comprised 500 samples, enabling rapid iteration on the fitness landscape. This GSPO regime refined the policy to produce AAV2-like capsids with superior multi-property profiles, balancing exploration and exploitation through the sequence-centric objective. 4.4. AAVGen Evaluation The performance of AAVGen was evaluated through large-scale inference using custom generation pipeline. total of 500,000 protein sequences were generated, each initiated with the fixed token M, to assess the models ability to produce coherent outputs. Inference was executed using batch size of 64, leveraging sampling-based decoding strategy (temperature = 1.0, top-p = 1.0, top-k = None) to enable diverse sequence generation, with each sequence limited to maximum length of 500 tokens. Furthermore, the quality of the generated sequences was evaluated using different modalities, including basic and alignment analysis, prediction of functional properties, and structural modeling. 4.4.1. Basic and sequence alignment analysis To ensure the novelty and integrity of the generated sequences, we applied multi-step evaluation protocol. To assess the diversity of AAVGens outputs, we quantified the extent of sequence repetition among the generated proteins. Specifically, we cumulatively sampled non-overlapping subsets of 1,000 sequences from the total pool of 500,000 generated sequences, progressively increasing the sample size until the full set was evaluated. This analysis enabled us to estimate the frequency of repetitive sequences as function of the number of generated samples. All duplicate sequences were subsequently removed. We then compared the length distribution of the remaining unique generated sequences with that of the training set, as sequences that are excessively long or short may disrupt protein structure and function. To further quantify the similarity between the generated protein sequences and the WT sequence, we computed suite of sequence-based metrics. Each generated sequence was globally aligned to the WT sequence using the PairwiseAligner implemented in Biopython (version 1.85)[34], with standard scoring parameters (match score = 2, mismatch score = 1, gap opening penalty = 2, gap extension penalty = 0.5). From the optimal alignment, we calculated the percentage identity, defined as the proportion of aligned positions with exactly matching amino acids relative to the total alignment length. We also calculated the percentage similarity, defined as the proportion of aligned positions with either identical amino acids or conservative substitutions, as determined by the alignment scoring scheme. In addition to these alignmentbased measures, we assessed sequence divergence using the edit distance, defined as the minimum number of single-residue insertions, deletions, or substitutions required to transform the generated sequence into the WT sequence. The distributions of these alignmentand distance-based metrics collectively characterize the relationship between the generated sequences and the WT reference in terms of exact matches, biochemical similarity, and overall divergence. 4.4.2. Functional property analysis To evaluate the generative capacity of AAVGen in predicting production fitness, kidney tropism, and thermostability, we assessed the generated sequences using trained regression models for each functional metric. Predicted scores were obtained for all generated variants and compared to the corresponding AAV2 WT values. Relationships among the predicted functional properties were quantified using Spearman correlation to assess potential trade-offs or co-optimization across metrics. Based on the reward model design, generated sequences were further stratified according to their predicted performance relative to WT and the uncertainty of the regression models, as estimated by the MAE on the validation set. Variants were classified as Best if their predicted score exceeded WT by more than four times the MAE, Good if their score fell between one and four MAE above WT, Uncertain if their predicted score fell between the WT score and one MAE above it, and Bad if their predicted score was lower than that of WT. This functional stratification enabled the selection of representative variants across range of predicted score levels for downstream structural analysis. By integrating functional predictions with structural modeling, we aimed to determine whether improvements or degradations in predicted properties were associated with measurable changes in capsid structure. 4.4.3. Structural alignment and analysis Protein structures were generated using AlphaFold3, which is diffusion-based structure prediction deep learning model. To assess the structural similarity of generated sequences with respect to the WT protein, we used the PDB structure of AAV2 obtained from the Protein Data Bank (PDB)[35], which represents only the VP3 subunit responsible for forming the AAV capsid surface. From 500,000 generated sequences, after the preprocessing phase and removal of all the sequences labeled as Bad and Uncertain, we randomly sampled 500 sequences for structural prediction. To establish baseline for structural comparison, we first aligned the 500 sampled sequences with the WT sequence and identified region of variability where specific residues were mutated in at least 20 sequences. Within this region, we randomly inserted amino acids, taking into account the length distribution observed in the 500 generated sequences. This process resulted in dataset of 250 randomly generated sequences. For each sequence, we sampled 5 structures to ensure robustness. Structural alignment between the predicted mutant and WT structures was performed in PyMOL (version 3.1.1)[36]. The RMSD was then calculated as the square root of the average squared distance between equivalent Cα atoms after optimal superposition, reflecting the mean positional deviation between the two structures. Additionally, Cα atoms with positional differences within 0.5 Å were selected, expanded to their corresponding residues, and residues with larger deviations were identified as differing regions. Finally, we analyzed the distribution of RMSD and evaluated the correlation between predicted scores and RMSD. 4.5. Hardware and training time The computational resources for model training consisted of dedicated server featuring an NVIDIA V100 GPU with 32 GB of VRAM and an AMD Epyc 7502 CPU with 32 GB of RAM. Under this configuration, the training durations for the models were: 11 hours and 25 minutes for the fitness regression model; 3 hours and 24 minutes for the kidney regression model; 3 hours and 29 minutes for the thermostability regression model; 9 hours and 5 minutes for the SFT phase of training model; and 9 hours and 38 minutes for the GSPO phase."
        },
        {
            "title": "Data availability",
            "content": "The data used in this study are publicly available on Hugging Face: the input dataset can be accessed at Moreza009/AAV_datasets and the generated output dataset produced in this study is available at Moreza009/AAVGen-dataset-out"
        },
        {
            "title": "Code availability",
            "content": "All code for this study is publicly available on GitHub (mohammad-gh009/AAVGen). The AAVGen model and associated regression models for production fitness, kidney tropism, and thermostability are available on Hugging Face at: Moreza009/AAVGen, Moreza009/AAV-Fitness, Moreza009/AAV-Kidney-Tropism, and Moreza009/AAV-Thermostability, respectively. An inference notebook for AAVGen is additionally available on Kaggle (notebook)."
        },
        {
            "title": "Acknowledgment",
            "content": "Not applicable."
        },
        {
            "title": "Funding",
            "content": "No funding was received for this study or its publication."
        },
        {
            "title": "Competing interest",
            "content": "The authors declare no competing interests."
        },
        {
            "title": "Authors contribution",
            "content": "Conceptualization: M.G, Y.G. Dataset preparation: M.G. Model development: M.G. Model assessment: M.G, Y.G. Data interpretation: M.G, Y.G. Drafting original manuscript: M.G, Y.G. Revising the manuscript: M.G, Y.G. All the authors have read and approved the final version for publication and agreed to be responsible for the integrity of the study."
        },
        {
            "title": "References",
            "content": "[1] Yin L, He H, Zhang H, Shang Y, Fu C, Wu S, et al. Revolution of AAV in Drug Discovery: From Delivery System to Clinical Application. Med Virol. 2025 Jun;97(6):e70447. doi:10.1002/jmv.70447 [2] Suarez-Amaran L, Song L, Tretiakova AP, Mikhail SA, Samulski RJ. AAV vecfuture. Mol Ther. 2025 May 7;33(5):19031936. tor development, back to the doi:10.1016/j.ymthe.2025.03.064 [3] Pupo A, Fernández A, Low SH, François A, Suárez-Amarán L, Samulski RJ. AAV vectors: The Rubiks cube of human gene therapy. Mol Ther. 2022 Dec 7;30(12):35153541. doi:10.1016/j.ymthe.2022.09.015 [4] Barnes C, Scheideler O, Schaffer D. Engineering the AAV Capsid to Evade Immune Responses. Curr Opin Biotechnol. 2019 Dec;60:99103. doi:10.1016/j.copbio.2019.01.002 [5] Huang Q, Chen AT, Chan KY, Sorensen H, Barry AJ, Azari B, et al. Targeting AAV vectors to the central nervous system by engineering capsidreceptor interactions that enable crossing of the bloodbrain barrier. PLOS Biol. 2023 Jul 19;21(7):e3002112. doi:10.1371/journal.pbio. [6] Rodríguez-Márquez E, Meumann N, Büning H. Adeno-associated virus (AAV) capsid engineering in liver-directed gene therapy. Expert Opin Biol Ther. 2021 Jun 3;21(6):749766. doi:10.1080/14712598.2021.1865303 [7] McGowan TJ, Lewerenz N, Maino E, Thürkauf M, Jörin L, Rüegg MA. AAV capsids target muscle-resident cells with different efficienciesA comparative study between AAV8, AAVMYO, and AAVMYO2. Mol Ther Methods Clin Dev. 2025 Jun 12;33(2):101451. doi:10.1016/j.omtm.2025.101451 [8] Huang Q, Chen AT, Chan KY, Sorensen H, Barry AJ, Azari B, et al. Targeting AAV vectors to the central nervous system by engineering capsidreceptor interactions that enable crossing of the bloodbrain barrier. doi:10.1371/journal.pbio.3002112 [9] Issa SS, Shaimardanova AA, Solovyeva VV, Rizvanov AA. Various AAV Serotypes and Their Applications in Gene Therapy: An Overview. Cells. 2023 Mar 1;12(5):785. doi:10.3390/cells12050785 [10] Finch NC, Neal CR, Welsh GI, Foster RR, Satchell SC. The unique structural and functional characteristics of glomerular endothelial cell fenestrations and their potential as therapeutic target in kidney disease. Am Physiol-Ren Physiol. 2023 Oct;325(4):F465 F478. doi:10.1152/ajprenal.00036.2023 [11] Wu G, Liu S, Hagenstein J, Alawi M, Hengel FE, Schaper M, et al. Adeno-associated virusbased gene therapy treats inflammatory kidney disease in mice. 2024 Sep 3. doi:10.1172/JCI [12] Lopez-Gordo E, Chamberlain K, Riyad JM, Kohlbrenner E, Weber T. Natural AdenoAssociated Virus Serotypes and Engineered Adeno-Associated Virus Capsid Variants: Tropism Differences and Mechanistic Insights. Viruses. 2024 Mar 12;16(3):442. doi:10.3390/v16030442 [13] Hudry E, Andres-Mateos E, Lerner EP, Volak A, Cohen O, Hyman BT, et al. Efficient Gene Transfer to the Central Nervous System by Single-Stranded Anc80L65. Mol Ther Methods Clin Dev. 2018 Jul 23;10:197209. doi:10.1016/j.omtm.2018.07.006 20 [14] Lee EJ, Guenther CM, Suh J. Adeno-associated virus (AAV) vectors: Rational design strategies for capsid engineering. Curr Opin Biomed Eng. 2018 Sep 1;7:5863. doi:10.1016/j.cobme.2018.09.004 [15] Tabebordbar M, Lagerborg KA, Stanton A, King EM, Ye S, Tellez L, et al. Directed evolution of family of AAV capsid variants enabling potent muscle-directed gene delivery across species. Cell. 2021 Sep 16;184(19):49194938. doi:10.1016/j.cell.2021.08.028 [16] Tan F, Dong Y, Qi J, Yu W, Chai R. Artificial Intelligence-Based Approaches for AAV Vector Engineering. Adv Sci. 2025;12(9):2411062. doi:10.1002/advs.202411062 [17] Spencer M. Improving AAV Vectors for Neuromuscular Disease Indications. Neuromuscul Disord. 2025 Sep 1;53:105827. doi:10.1016/j.nmd.2025.105827 [18] Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, et al. Train2022. to follow instructions with human feedback. arXiv; ing language models doi:10.48550/arXiv.2203. [19] Zheng C, Liu S, Li M, Chen XH, Yu B, Gao C, et al. Group Sequence Policy Optimization. arXiv; 2025. doi:10.48550/arXiv.2507.18071 [20] Lin Z, Akin H, Rao R, Hie B, Zhu Z, Lu W, et al. Evolutionary-scale prediction of atomiclevel protein structure with language model. Science. 2023 Mar 17;379(6637):11231130. doi:10.1126/science.ade2574 [21] Ferruz N, Schmidt S, Höcker B. ProtGPT2 is deep unsupervised language model for protein design. Nat Commun. 2022 Jul 27;13(1):4348. doi:10.1038/s41467-022-32007-7 [22] Berger B, Waterman MS, Yu YW. Levenshtein Distance, Sequence Comparison and Biological Database Search. IEEE Trans Inf Theory. 2021 Jun;67(6):32873294. doi:10.1109/tit.2020. [23] Abramson J, Adler J, Dunger J, Evans R, Green T, Pritzel A, et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3. Nature. 2024 Jun;630(8016):493 500. doi:10.1038/s41586-024-07487-w [24] Bryant DH, Bashir A, Sinai S, Jain NK, Ogden PJ, Riley PF, et al. Deep diversification of an AAV capsid protein by machine learning. Nat Biotechnol. 2021 Jun;39(6). doi:10.1038/s41587-020-00793-4 [25] Eid FE, Chen AT, Chan KY, Huang Q, Zheng Q, Tobey IG, et al. Systematic multi-trait AAV capsid engineering for efficient gene delivery. Nat Commun. 2024 Aug 4;15(1):6602. doi:10.1038/s41467-024-50555-y [26] Wu J, Qiu Y, Lyashenko E, Torregrosa T, Pfister EL, Ryan MJ, et al. Prediction of AdenoAssociated Virus Fitness with Protein Language-Based Machine Learning Model. Hum Gene Ther. 2025 May;36(910):823829. doi:10.1089/hum.2024.227 [27] Zheng H, Guo B, Mo A, Wei H, Wu Y, Lin X, et al. Mapping AAV capsid sequences to functions through function-guided in silico evolution. bioRxiv; 2024. doi:10.1101/2024.10.11.617764 [28] Liu L, Yang J, Song J, Yang X, Niu L, Cai Z, et al. AAVDiff: Experimental Validation of Enhanced Viability and Diversity in Recombinant Adeno-Associated Virus (AAV) Capsids through Diffusion Generation. arXiv; 2024. doi:10.48550/arXiv.2404. 21 [29] Jiang Z, Laosinwattana S, Dalby PA. Fully functional AAV viral vectors with highly altered structural cores and subunit interfaces using ProteinMPNN. 2025 Jul 26. doi:10.1101/2025.07.24.666527 [30] Ogden PJ, Kelsic ED, Sinai S, Church GM. Comprehensive AAV capsid fitness landscape reveals viral gene and enables machine-guided design. Science. 2019 Nov 29;366(6469):1139 1143. doi:10.1126/science.aaw2900 [31] Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. Transformers: Stateof-the-Art Natural Language Processing. Association for Computational Linguistics; 2020. p. 3845. [32] UniProt. 2021. https://www.uniprot.org/release-notes/2021-11-17-release [33] von Werra L, Belkada Y, Tunstall L, Beeching E, Thrush T, Lambert N, et al. TRL: Transformers Reinforcement Learning. 2020. https://github.com/huggingface/trl [34] Cock PJ, Antao T, Chang JT, Chapman BA, Cox CJ, Dalke A, et al. Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics. 2009;25(11):14221423. [35] wwPDB consortium. Protein Data Bank: the single global archive for 3D macromolecular structure data. Nucleic Acids Res. 2019 Jan 8;47(D1):D520D528. doi:10.1093/nar/gky949 [36] Schrödinger, LLC. The PyMOL Molecular Graphics System, Version 1.8. 2015."
        }
    ],
    "affiliations": [
        "Department of Genetics and Molecular Biology, Isfahan University of Medical Sciences, Isfahan, Iran",
        "Regenerative Medicine Research Center, Isfahan University of Medical Sciences, Isfahan, Iran"
    ]
}