{
    "paper_title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation",
    "authors": [
        "Minho Park",
        "Taewoong Kang",
        "Jooyeol Yun",
        "Sungwon Hwang",
        "Jaegul Choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing demand for AR/VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existing approaches either fine-tune pretrained diffusion models on limited ERP datasets or attempt tuning-free methods that still rely on ERP latent representations, leading to discontinuities near the poles. In this paper, we introduce SphereDiff, a novel approach for seamless 360-degree panoramic image and video generation using state-of-the-art diffusion models without additional tuning. We define a spherical latent representation that ensures uniform distribution across all perspectives, mitigating the distortions inherent in ERP. We extend MultiDiffusion to spherical latent space and propose a spherical latent sampling method to enable direct use of pretrained diffusion models. Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality in the projection process. Our method outperforms existing approaches in generating 360-degree panoramic content while maintaining high fidelity, making it a robust solution for immersive AR/VR applications. The code is available here. https://github.com/pmh9960/SphereDiff"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 6 9 3 4 1 . 4 0 5 2 : r SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation Minho Park* Taewoong Kang*"
        },
        {
            "title": "Jooyeol Yun Sungwon Hwang Jaegul Choo",
            "content": "Korea Advanced Institute of Science and Technology (KAIST) {m.park, keh0t0, jchoo}@kaist.ac.kr Figure 1. 360-degree panoramic video generated by SphereDiff. Click to play the animation clips. Best viewed with Acrobat Reader."
        },
        {
            "title": "Abstract",
            "content": "The increasing demand for AR/VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existing approaches either fine-tune pretrained diffusion models on limited ERP datasets or attempt tuning-free methods that still rely on ERP latent representations, leading to discontinuities near the poles. In this paper, we introduce SphereDiff, novel approach for seamless 360-degree panoramic image and video generation using state-of-the-art diffusion models without additional tuning. We define spherical latent representation that ensures uniform distribution across all perspectives, mitigating the distortions inherent in ERP. We extend MultiDiffusion to spherical latent space and propose spherical latent sampling method to enable direct use of pretrained diffusion models. Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality in the projection process. Our method outperforms existing approaches in generating 360-degree panoramic content while maintaining high fidelity, making it robust solution for immersive AR/VR applications. The code is available here. 1. Introduction indicates equal contributions. The growing demand for AR/VR applications has significantly increased the need for high-quality immersive content. AR/VR technologies offer highly engaging environments, providing sense of presence that traditional displays (e.g., phones and laptops) cannot. key element in delivering such experiences is the 360 180 panoramic scene, or 360-degree panorama, which provides an omnidirectional view of the virtual world. This allows users to explore their surroundings from any perspective, setting it apart from standard visual content. However, as 360-degree panoramas require specialized cameras, their availability is limited, and VR users have yet to experience broad range of realistic content beyond simulations. Recently, diffusion models have demonstrated remarkable performance in generating standard images and videos [9, 13, 21, 26]. Given their success, there is growing interest in synthesizing 360-degree panoramic images or videos by leveraging the recent state-of-the-art diffusion models. 360-degree panorama is typically represented using an equirectangular projection (ERP), which maps spherical imagery onto 2D rectangular plane, e.g., the projection of globe onto world map. Due to the limitations of the 2D rectangular representation, the ERP introduces significant distortion, known as ERP distortion, in which high-latitude regions appear disproportionately large due to projection effects. For example, as shown in Fig. 1, the fireworks near the pole appear significantly larger than the others because our generated panoramic video is visualized in ERP. This ERP distortion causes significant distribution shift in 360degree panorama compared to standard perspective images or videos, making it challenging for standard diffusion models to generate omnidirectional panoramic content. To mitigate the distribution shift, several previous studies have fine-tuned pretrained diffusion models using ERP datasets [5, 16, 24, 30]. However, due to the limited availability of text-ERP pairs, they often failed to generate seamless 360-degree panoramas, particularly near the poles, as shown in Fig. 2. On the other hand, some studies [2, 18] have proposed methods for generating arbitrarily sized panoramas without tuning pretrained models based on the MultiDiffusion framework [2]. Nevertheless, they are also limited to the ERP latent representation, which causes discontinuities near the poles. In this paper, we present novel framework, SphereDiff, which effectively generates 360-degree panoramic images and videos by leveraging recent state-of-the-art diffusion models without additional tuning. First, we define spherical latent representation that pairs each latent with its corresponding position on the spherical surface. Then, we extend MultiDiffusion [2] to the spherical latent space. Since the spherical representation is equally distributed across all perspectives, we can handle every perspective uniformly, allowing for seamless image generation even near the poles. While the number of spherical latents is nearly equal across all perspectives, the projected spherical latents Figure 2. Motivation. Previous finetuning approaches [15, 30] often fail to generate continuous scenes near the pole due to the limited ERP dataset. The tuning-free approach [18] also fails to generate seamless frame due to the ERP latent representation. are distributed in continuous positions. Thus, we propose novel spherical latent sampling method to discretize the spherical latents onto 2D grid, enabling the use of readily available state-of-the-art diffusion models [9, 13, 14, 26]. Lastly, we introduce distortion-aware weighted averaging to further improve the minor distortions caused by sphericalto-perspective projection. Our contributions can be summarized as follows: We propose SphereDiff, tuning-free pipeline for highquality and seamlessly continuous 360-degree panoramic image and video generation with minimal distortion. We introduce dynamic latent sampling that discretizes spherical latents onto 2D grid, which enables the utilization of state-of-the-art diffusion models. We propose distortion-aware weighted averaging technique that guarantees seamless content generation with superior visual quality. Extensive experiments demonstrate that SphereDiff highly outperforms existing methods in terms of visual quality and robustness to distortion. 2. Related Work Latent Diffusion Models. Recent advancements in diffusion models have enabled the generation of high-quality images [4, 14, 21, 26] and videos [9, 13, 17, 23, 28, 31], achieving impressive visual results across various video generation tasks within the standard perspective of visual content. However, generating content beyond the standard perspective, such as regular or 360-degree panoramas, remains relatively underexplored. In this paper, we aim to generate 360-degree panoramas, which differ significantly from standard perspective scenes, by solely leveraging pre2 Figure 3. Overall Pipeline. We initialize uniform spherical latents and extract perspective latents for multiple views at each denoising step using dynamic latent sampling. These latents are then denoised and fused using the MultiDiffusion [2] with distortion-aware weighted averaging. This process enables seamless and distortion-free 360-degree panoramic image and video generation in tuning-free manner. trained diffusion models designed for standard perspectives. Furthermore, as our method is tuning-free, it is highly adaptable and can be seamlessly integrated with state-ofthe-art diffusion-based image and video generation models. Panoramic Scene Generation. Most 360-degree panoramic generation methods rely on equirectangular projection (ERP), which maps spherical panorama coordinates onto 2D rectangular plane, where latitude and longitude correspond to vertical and horizontal coordinates, respectively. However, ERP inherently introduces severe nonlinear distortions, particularly near the poles, degrading both visual quality and the processing of equirectangular panoramas. Although previous studies [5, 7, 16, 24, 25, 30, 32] attempt to address this issue by fine-tuning on panoramic ERP datasets, they often fail to generate seamless panoramas, especially near the poles, or struggle with text controllability due to the domain-specific nature of indoor environments). CubeDiff [12] introduces an alternative approach using cube map representations for panoramic image generation. While this method effectively reduces distortions near the poles, it still struggles with discontinuities at cube-face boundaries. Another line of research [6, 16, 29, 32] focuses on scene-level generation, primarily producing static 3D scenes using Gaussian-based representations. While they these methods achieve high-quality static outputs, require optimization procedures that reduce efficiency. In contrast, we replace the ERP latent representation with spherical latent representation, providing natural solution to eliminate distortion across all perspectives. these datasets (e.g., Tuning-free Panorama Generation. For 360-degree panoramic video generation, far fewer datasets exist compared to ERP images. As result, recent research trends favor utilizing perspective models without additional training. DynamicScaler [18] attempts to mitigate ERPs inher3 ent distortions by employing panoramic-projected denoising, leveraging the MultiDiffusion framework [2] with adjusted windows. On the other hand, 4K4DGen [16] seeks to avoid distortion by utilizing ERP images with an imageto-video model, limiting its applicability to more complex dynamic content generation. However, both methods struggle to generate seamless scenes near the poles due to severe interpolation or sampling artifacts. In contrast, our method overcomes these limitations by directly utilizing uniformly distributed spherical latents, ensuring both efficiency and reduced distortion without requiring additional training. 3. Method In this section, we introduce SphereDiff, novel tuningfree framework for generating 360-degree panoramic images and videos. First, we present the spherical latent representation and spherical-to-perspective projection in Section 3.1. Next, we extend the MultiDiffusion framework [2] to the spherical latent space in Section 3.2. We then introduce spherical latent sampling, which discretizes the continuous coordinates of the perspective-projected spherical latent onto 2D grid in Section 3.3. Finally, we propose distortion-aware weighted averaging method to mitigate minor distortions from the spherical-to-perspective projection in Section 3.4. The overall pipeline is illustrated in Figure 3. 3.1. Spherical Latent Representation Definition. We introduce spherical representation of latent features for generating 360-degree panoramas. We define latent feature RC paired with the corresponding spherical coordinate on spherical surface. The set of spherical coordinates can be represented as follows: S2 = {d = (x, y, z) x, y, R, = 1}. (1) Then, we pair each latent feature with its associated position, i.e., = (d, ), referred to as spherical latent. For The goal of this framework is to construct the Spherical MultiDiffuser Ψ : S, which takes noisy spherical latent St and set of text conditions as inputs and produces the denoised spherical latent St1, as illustrated in Figure 3. Based on the MultiDiffuser, clean spherical latent S0 can be obtained from pure noise ST through an iterative denoising process using diffusion models as: ST , ST 1, ..., s.t. St1 = Ψ(Stz). (4) To construct MultiDiffuser Ψ, we first leverage pretrained diffusion model trained on standard perspective latents Φ : I, which takes noisy latent It and text condition as inputs and produces the denoised latent It1. The pretrained diffusion model gradually denoises the pure Gaussian noise IT into clean image I0. IT , IT 1, ..., I0 s.t. It1 = Φ(Ity) (5) Next, we define mapping function between the spherical and perspective latent spaces, Fi : I, along with corresponding condition mapping λi : Y, where {1, ..., n}. The mapping function Fi can be formulated in various ways, which will be discussed in Section 3.3. = Fi(St), yi = λi(z) (6) Finally, The denoising step in of MultiDiffuser can be formulated by closed-form [2]. Ψ(Stz) = (cid:88) i=1 1 (Φ(I yi)). (7) In the following sections, we define Fi (Section 3.3) and the per-pixel weight (Section 3.4) to ensure seamless and consistent 360-degree panorama. 3.3. Sampling Spherical Latent Based on the MultiDiffusion framework [2], we define the mapping function that transforms spherical latent representation into perspective latent space I. To define mapping function , we first apply the transformation TSI based on the view direction S2 and focal length , which projects the coordinates of the spherical latents onto the perspective plane P. Formally, the spherical-toperspective latent transformation can be written as TSP (Sv, ) = = {pi = (ui, fi)ui [1, 1]2}, (8) where ui = TS2P2 (div, ). Note that, does not contain elements since the perspective are cropped by [1, 1]. Since visual diffusion models based on DiT [19] support continuous 1D representations, we explore denoising process based on continuous coordinates with RoPE [9, 22]. However, this approach introduces unstructured outputs due 4 Figure 4. Comparison of ERP and Spherical Latent Representations. When changing perspective, the ERP latent representation shows significant density variations in latent density depending on position, especially near the poles, while our spherical representation maintains nearly uniform density across all perspectives. spherical latents, we define the spherical latents as: = {si = (di, fi) di S2, fi RC , for [1, ]}. (2) which is now composed of multiple latents similar to standard 2D or 3D latent features. We refer to the domain of spherical latents as S, i.e., S. Equirectangular Projection (ERP) latents also can be written in our spherical latent representation. However, as shown in Fig. 4 (a), due to the 2D grid constraint of ERP latent, its spherical coordinates are not uniformly distributed on the spheres surface. In contrast, we define the spherical latents using the Fibonacci Lattice [10], which offers the number of spherical latents is nearly equal across all perspectives, as shown in Fig. 4 (b). Perspective Latent Representation. Since standard diffusion models operate in perspective space, we utilize spherical-to-perspective projection which transforms the spherical coordinate to the perspective coordinate. To achieve this, we first define the domain of perspective coordinates as discretized 2D plane as P2 = (cid:26) = (cid:19) (cid:18) 2j , 2k (cid:20) (cid:21) , 2 (cid:20) , (cid:21)(cid:27) , 2 , 2 (3) where H, indicates the height and width of the bounded 2D perspective plane, respectively. We use view direction S2 and predefined focal length to define the spherical-to-perspective projection function = TS2P2(dv, ). The detailed formula of the projection function is available in Section B.1. 3.2. MultiDiffusion for Spherical Latent. The MultiDiffusion [2] framework is often utilized for generating arbitrary-shaped images by leveraging pretrained diffusion models [21] trained on standard perspective images. In this section, we introduce an extension of the MultiDiffusion framework to the spherical latent representation. : Projected Latent = TSI(Sv, ) Algorithm 1 Dynamic Latent Sampling (in case of H, are even and equal) Input Output: Arranged Perspective Latent Sort the latents of by ui. H, HW for [1, H/2] do Get the number of latents Get smaller box Initialize , (2i)2 (2i 2)2 first latents from the sorted Set i-th border of to Pop first latents from count of i-th border fill the center first end return Ignore elements of achieve this, we propose novel dynamic latent sampling algorithm that first selects the center-positioned points and then ignores the remaining points at the outermost region of the current window. The dynamic latent sampling comprises three major components: 1) queue, 2) FoV adjustment, and 3) center-first selection. First, we prevent selecting the same spherical latent twice by leveraging queue. Once latent is selected, it is removed from the queue. Second, we dynamically adjust the FoV, ensuring that and are not fixed within our framework. Lastly, we prioritize selecting the center-positioned spherical latent first. Since our framework already supports dynamic FoV, reducing the FoV is not an issue, as it can be compensated by increasing the density of view directions. However, if the selected latents within the FoV are unevenly distributed, even closely spaced view directions may fail to exchange information effectively, leading to problems. The entire algorithm is demonstrated in Algorithm 1. 3.4. Distortion-Aware Weighted Averaging Although dynamic latent sampling improves seamless continuity, minor distortion remains in the spherical-toperspective projection. While the spherical-to-perspective distortion is relatively smaller than the ERP distortion, it can still cause latent position misalignment for other viewpoints. To address this, we proposed distortionaware weighted averaging within the MultiDiffusion framework [2]. Specifically, we adjust the per-pixel weight to account for the spherical-to-perspective distortion. Since distortion increases with distance from the origin of the perspective image, we introduce simple yet effective exponential weighting function in the image space I. jk]j[1,H],k[1,W ] RHW , i = [W jk = exp ( ujk /τ ) , (9) (10) where ujk is the distance from the origin in the perspec5 Figure 5. Comparison of Nearest and Dynamic Sampling. Nearest sampling often resamples the selected latents or omits central ones, while dynamic sampling selects latents from the center outward, discarding only the outermost ones. to distribution shift in positional embeddings, as detailed in Section B.2. To address this, we apply an additional discretization transformation that maps the continuous latent coordinates to P2, i.e., = D(TSI(Sv, )). In the following sections, we introduce two simple yet effective methods for discretizing the perspective coordinates. Nearest Point Sampling. straightforward approach to discretizing continuous coordinates is nearest-neighbor sampling, where the nearest projected spherical latent is selected for each pixel position. Specifically, the latent closest to the center of grid is retrieved and used as input for denoising, as illustrated in Fig. 5 (a). Despite its simplicity, this method introduces two critical issues. First, the same latent may be selected multiple times, altering the latent distribution, which often degrades generation performance [3]. Second, some spherical latents may not be chosen even if they fall within the field of view of the current camera view direction. This phenomenon, referred to as the undersampling problem, has particularly detrimental consequences for generating seamless panorama. Undersampling Problem. The undersampling of spherical latents disrupts information flow across neighboring windows. As illustrated in Fig. 5, the green points lack information from the current field of view (FoV) since they are not denoised in this step. If the next windows FoV captures green point, not the blue point, it receives no information from the current window, causing discontinuities even when there is large overlap. To address this, we propose dynamic latent sampling algorithm that effectively collects projected spherical latents. Dynamic Latent Sampling. As mentioned earlier, we aim to ensure that all points within the field of view (FoV) are selected, enabling seamless panorama generation. To Figure 6. Qualitative results of our method. Visualization results for the entire scene using the ERP representation and 3 perspectives views across various elevation multiple perspective images or frames. Additional results are available in the supplementary materials. tive image, and τ is scaling factor controlling how quickly the weight decays toward the edges. Then, the weighting function can be represented as I). This perpixel weighted average enables seamless panorama generation by assigning higher weights to the center of the FoV, where distortion is minimal. = 1(W 4. Experiments Our method is applicable to both image and video diffusion models, and we conduct experiments on both modalities. We utilize SANA [26], and LTX Video [9] for image and video generation, respectively. In all MulitDiffusion setups [2], the base height and width are fixed at 512 pixels, and the temporal frame length is set to 121 frames for video experiments. Additionally, we use 2,600 points on the sphere and 89 view directions for denoising, where each perspective overlaps by 60%. To visualize the results, we decode the denoised latent representation S0 for each view direction using VAE decoder and stitch them together to construct an ERP image. During this decoding process, we apply distortion-aware weighted averaging techniques to ensure seamless integration. Additional implementation details are provided in Section E. All source code and configurations will be made publicly available. 4.1. Experimental Setup For evaluation, we use 20 fixed text prompts designed for generating outdoor scenes and assess all methods based on perceptual image quality. Metrics are measured using images corresponding to 14 view directions evenly distributed across the sphere. Perspective transformations are performed with fixed 90-degree FoV and resolution of 512512 pixels. We evaluate our method across four domains: panorama, image, text adherence, and video, using two metrics per category. For panorama, we assess distortion level and end continuity, while for image quality, we measure visual fidelity and aesthetic appeal. These aspects are quantitatively evaluated using LLM-based visual scores from the GPT-4o model [1], with detailed instructions in Section C.1. For text adherence and video quality, we adopt evaluation criteria from VBench [11] and CLIP-Score [20]. Additionally, we conduct user study to assess panorama, image, and video quality. Further details on the evaluation protocol and user study are provided in Section C. Baselines. For image panorama baselines, we include 360LoRA [15], LoRA finetuned model from CivitAI, Text2Light [5], and Panfusion [30]. While DynamicScaler [18] targets panoramic video generation, it can be used to generate panoramic images. So we extend it to image generation and use it as baseline for comparison. For video generation, we use 360DVD [24] and DynamicScaler as primary baselines. Furthermore, we apply 360LoRA for inference using AnimateDiff [8] and include it as an additional baseline. While DynamicScaler was originally implemented on DynamicCraft [27], we reimplemented it on the 6 Figure 7. Qualitative comparison of all image and video baselines. Each sample presents perspective images from the top view to the bottom view, highlighting end-to-end continuity and distortion. Other methods exhibit noticeable artifacts, such as split seams, severe distortions near the poles, blurriness, or spots due to inadequate handling of these issues. In contrast, our method generates seamless, high-quality panoramic content without such artifacts. Panorama Creteria Image Creteria Text Adherence Video Creteria Method Distortion End Continuity Image Quality Aesthetic Appearance Scene CLIP-Score Motion Smoothness Temporal Flickering 360 LoRA Text2Light [5] PanFusion [30] DynamicScaler [18] SphereDiff (Ours) 360 LoRA (+ AnimateDiff [8]) 360DVD [24] DynamicScaler [18] SphereDiff (Ours) 2.027 2.381 1.965 2.854 3.238 1.939 2.086 1.971 2. 3.423 3.454 3.696 3.985 4.892 3.482 3.246 2.971 4.496 2.965 2.415 2.819 4.496 4.496 3.179 2.929 2.711 3.050 3.492 2.777 3.450 4.577 4.685 3.571 3.396 3.236 3. 0.2875 0.0000 0.2125 0.2750 0.5875 0.2914 0.0719 0.4836 0.5703 26.40 21.03 25.70 26.63 28.65 26.34 23.13 26.89 27.52 - - - - - 0.9908 0.9843 0.9943 0. - - - - - 0.9847 0.9777 0.9918 0.9941 Table 1. Quantitative comparison with the best results in bold and the second best underlined. SphereDiff outperforms existing methods across all metrics except image quality for video generation, where it ranks second. same models, SANA and LTX Video, for fair comparison. For our method and DynamicScaler, we use identical text prompts, while other models accept only single prompt, so we provide the main reference prompt for consistency. 4.2. Results Qualitative Results. As shown in Tab. 3, all baseline methods operate in the equirectangular latent space. The detailed comparison is available in Section D. These methods do not fully address the inherent distortions of equirectangular projection. This limitation is evident in Fig. 7, where noticeable artifacts appear near the poles, which become even more pronounced when viewed in perspective image. In contrast, our method effectively mitigates these distortions, producing significantly improved results. Another major limitation of ERP-based methods is the requirement for end continuity, where the left and right edges of the image must seamlessly connect. This issue is not fully resolved in baselines, particularly when observed from specific view directions. In contrast, our method performs denoising on uniform spherical representation, ensuring consistency across all viewing angles and eliminating such discontinuities as shown in Fig. 6. Quantitative Results As presented in Tab. 1, our method outperforms all baselines across most of metrics for both image and video generation. Notably, our approach achieves significantly better scores in distortion and end continuity. This demonstrates its effectiveness in produc7 Method Distortion End Continuity Image Quality Text Alignment Motion Smoothness Temporal Flickering Panorama Creteria Image Creteria Video Creteria 360 LoRA Text2Light [5] PanFusion [30] DynamicScaler [18] SphereDiff (Ours) 360 LoRA (+ AnimateDiff [8]) 360DVD [24] DynamicScaler [18] SphereDiff (Ours) 21.43 5.95 14.29 20.24 38.10 25.00 11.90 30.95 32.14 23.81 4.76 10.71 25.00 35.71 27.38 13.10 26.19 33.33 21.43 8.33 10.71 25.00 34.52 27.38 16.67 28.57 27. 20.24 5.95 16.67 27.38 29.76 27.38 20.24 22.62 29.76 - - - - - 27.38 8.33 28.57 35.71 - - - - - 23.81 14.29 29.76 32. Table 2. User study results. All images and videos are presented as perspective images, and the evaluation is conducted using multiplealternative forced-choice survey. Our method demonstrates the highest preference in most performance aspects, with particularly in distortion and end continuity. text alignment, distortion, end continuity, motion smoothness, and temporal flickering. The study involved 21 participants, each selecting their preferred option for each metric. As shown in Tab. 2, our method outperformed all baselines across almost all metrics in both image and video evaluations. While DynamicScaler achieve higher image quality rating, it does so by sacrificing the 360-degree panoramic constraint. Specifically, it employs early stopping to prioritize image quality at the cost of panoramic continuity. In contrast, our approach preserves 360-degree consistency while achieving comparable image quality, balancing panoramic integrity and visual fidelity. 4.4. Ablation Study We conducted ablation studies to evaluate the impact of each component in SphereDiff, spherical latent sampling methods, as well as the effect of distortion-aware weighted averaging. For clarity, we performed denoising on only two views and visualized the results in ERP representation. As shown in Fig. 8, nearest sampling fails to facilitate information exchange between different views, leading to unnatural transitions in overlapping regions. Our proposed dynamic latent sampling method improves information exchange between views, generating more seamless images. Furthermore, by incorporating our distortion-aware weighted averaging technique, we achieve significantly cleaner and more coherent outputs for both sampling methods. These results demonstrate that each component in our framework plays critical role in seamlessly integrating multiple perspectives into single spherical representation, ensuring high-quality panoramic generation. 5. Conclusion We introduced SphereDiff, tuning-free framework for omnidirectional panoramic generation that leverages spherical latent representations to minimize distortions and ensure Figure 8. Ablation on latent sampling and weighted averaging. Nearest sampling lacks information exchange between views, leading to inconsistencies and visible overlap artifacts caused by undersampling problem. In contrast, dynamic sampling facilitates information sharing, resulting in more integrated outputs. With weighted averaging, both sampling methods improve seamlessness. However, nearest sampling still fails to maintain connectivity between adjacent regions, leading to discontinuities. ing high-quality panoramic content. However, while the image quality of our video generation is lower than that of AnimateDiff, it remains comparable. The overall video generation score is lower than the image generation score, which may be attributed to the denoising models quality. Since our method is tuning-free, it can achieve higher performance by leveraging more advanced denoising model, given sufficient memory availability. 4.3. User Study To further evaluate the effectiveness of our panorama generation method, we conducted user study using multiplealternative forced-choice survey. Participants were asked to compare 20 pairs of images and videos based on quality, 8 seamless continuity. Our method integrates dynamic latent sampling and distortion-aware weighted averaging, significantly improving panoramic content quality. Despite these advancements, limitations remain. Each viewpoint is currently processed independently, leading to lack of global context. Future work will focus on addressing this challenge by incorporating global context-aware refinement methods. Nevertheless, we believe our framework is solid starting point for 360-degree panorama generation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6 [2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: fusing diffusion paths for controlled image generation. In Proceedings of the 40th International Conference on Machine Learning, pages 17371752, 2023. 2, 3, 4, 5, 6 [3] Pascal Chang, Jingwei Tang, Markus Gross, and Vinicius Azevedo. How warped your noise: temporally-correlated noise prior for diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 5 [4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Pixart-α: Fast training of diffusion Huchuan Lu, et al. transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [5] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light: Zero-shot text-driven hdr panorama generation. ACM Transactions on Graphics (TOG), 41(6):116, 2022. 2, 3, 6, 7, 8, 12 [6] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. [7] Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Jonathan Eisenmann, Siavash Khodadadeh, and JeanGuided co-modulated gan for 360 Francois Lalonde. arXiv preprint {deg} field of view extrapolation. arXiv:2204.07286, 2022. 3 [8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 6, 7, 8 [9] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime arXiv preprint arXiv:2501.00103, video latent diffusion. 2024. 2, 4, 6, 13 [11] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 6 [12] Nikolai Kalischek, Michael Oechsle, Fabian Manhardt, Philipp Henzler, Konrad Schindler, and Federico Tombari. Cubediff: Repurposing diffusion-based image models for panorama generation. In The Thirteenth International Conference on Learning Representations, 2025. 3, 12 [13] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [14] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2 [15] LatentLabs360. Latentlabs360. https://civitai. com/models/10753/latentlabs360, 2023. 2, 6 [16] Renjie Li, Panwang Pan, Bangbang Yang, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, Zhengzhong Tu, et al. 4k4dgen: Panoramic 4d generation at 4k resolution. arXiv preprint arXiv:2406.13527, 2024. 2, 3, 12 [17] Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, et al. Lumina-video: Efficient and flexible video generation with multi-scale next-dit. arXiv preprint arXiv:2502.06782, 2025. 2 [18] Jinxiu Liu, Shaoheng Lin, Yinxiao Li, and Ming-Hsuan Yang. Dynamicscaler: Seamless and scalable video generation for panoramic scenes. arXiv preprint arXiv:2412.11100, 2024. 2, 3, 6, 7, 8, 12 [19] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 4, [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [21] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 4 [22] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [23] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2 [10] Doug Hardin, TJ Michaels, and Edward Saff. comparison of popular point configurations on S2. arXiv preprint arXiv:1607.04590, 2016. [24] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation In Proceedings of with 360-degree video diffusion model. 9 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69136923, 2024. 2, 3, 6, 7, 8, 12 [25] Tianhao Wu, Chuanxia Zheng, and Tat-Jen Cham. Panodiffusion: 360-degree panorama outpainting via diffusion. arXiv preprint arXiv:2307.03177, 2023. 3 [26] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synarXiv preprint thesis with linear diffusion transformers. arXiv:2410.10629, 2024. 2, 6, 13 [27] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating openIn European domain images with video diffusion priors. Conference on Computer Vision, pages 399417. Springer, 2024. 6 [28] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [29] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d arXiv preprint scene generation from single image. arXiv:2406.09394, 2024. 3 [30] Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, and Jianfei Cai. Taming stable diffusion for text to 360 panorama image In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 6347 6357, 2024. 2, 3, 6, 7, 8, 12 [31] Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, and Ping Luo. Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. 2 [32] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. In European Conference on Computer Vision, pages 324342. Springer, 2024. 3 10 SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Appendix A. Appendix B. Spherical Latent Sampling B.1. Perspective Latent Representation. . B.2. Directly Use Continuous Position . C. Experimental Details C.1. LLM-based visual score . . C.2. User Study Details . . . . . . . . . . . . . 11 11 11 11 11 12 12 . . . . . . . . . . . . . . . . . . . . D. Additonal Qualitative/Quantitative Comparison 12 D.1. Detailed Comparison with Spherical and ERP 12 E. Implementation Details E.1. Text Prompt Examples . . . . . . . . . . . . 13 13 B. Spherical Latent Sampling B.1. Perspective Latent Representation. For perspective latent representation, we define virtual camera centered at the origin. The points in world coordinates, denoted as = (x, y, z), are projected onto image space using the projection matrix = K[Rt] Here, is the intrinsic camera matrix derived from predefined focal length , represents the viewing direction, and is set to zero. The spherical-to-perspective projection function TS2P2 can be formulated as: = K[Rt] d, (11) where = (x, y, z, 1) is the homogeneous coordinate representation of the 3D point d, and = (u, v, w) represents the projected homogeneous coordinates in image space. The final 2D perspective coordinates = (u, v) are obtained via perspective division: = (cid:18) w , w (cid:19) . (12) To ensure proper visibility, points located behind the view direction are masked out using their inner product values, retaining only the points in the frontal view. B.2. Directly Use Continuous Position Recent visual generation models, including those based on DiT [19], support continuous 1D representations through corresponding positional embeddings. naive approach Figure 9. Similarities between positional embeddings calculated with discrete and continuous positions. The small squares represent the similarity distribution when the central pixel is used as query. As shown in the figure, when discretization is applied, within the same row or column, resulting in high similarity. However, when positions vary slightly (as in the continuous case), the similarity drops significantly. would be to leverage this property and treat the latent representations as continuous without discretization. However, this approach leads to unstructured outputs due to distribution shift in positional embeddings. Specifically, in the original positional embedding space, latent similarities are high within the same row or column, ensuring spatial consistency. In contrast, when using continuous positional embeddings, the similarity between two adjacent points is not necessarily high, even if their spatial coordinates are close, as shown in Fig. 9. This discrepancy causes the model to fail in generating structured content. Although DiT can process continuous inputs, discretization remains essential for tuning-free panoramic visual generation to maintain structured and consistent latent relationships. C. Experimental Details All metrics were measured using perspective images. The selected view directions include four azimuth angles (0, 90, 180, 270) at elevation angles of 0, 45, and -45, as well as one view each at elevations of 90 and -90, resulting in total of 14 view directions. 11 Method Latent Space Tuning-Free Open-Sourced Panoramic image generation Text2Light [5] PanFusion [30] Cubediff [12] ERP ERP Cube Map Panoramic video generation 360DVD [24] 4K4DGen [16] ERP ERP Panoramic image and video generation DynamicScaler [18] SphereDiff (Ours) ERP Spherical Table 3. Comparison of 360-degree panorama generation approach. Most existing panoramic generation models perform denoising in the equirectangular latent space, except for CubeDiff, which utilizes cube map representation. In contrast, our method leverages spherical latents. Among the compared methods, only DynamicScaler and ours support tuning-free generation. C.1. LLM-based visual score GPT-4o evaluation prompt used for assessing image quality, aesthetic appeal, distortion level, and connectivity. Tab. 4 shows the prompt that we used for evaluate. C.2. User Study Details We divided the 20 pairs of images and videos into five sets, each containing four pairs, allowing users to select one set for evaluation based on their convenience. To accurately assess distortion and end-continuity, images and videos were presented from viewpoint with an azimuth angle (θ = 0) while varying the elevation angle (ϕ) across five positions: 90, 45, 0, 45, 90. The presentation order was randomized to minimize bias. For image evaluation, participants selected the most suitable model from five available options, while for video evaluation, they chose from four models, selecting the one they found most appropriate for each criterion. The evaluation criteria were instructed to be used similarly to those described in Tab. 4, which were designed for evaluation using GPT-4o. D. Additonal Qualitative/Quantitative Comparison D.1. Detailed Comparison with Spherical and ERP ERP projects latitude and longitude coordinates on the sphere onto vertical and horizontal coordinates on rectangular grid, forming an representation, where and denote the resolution, and represents the number of channels. While this format maintains uniform distribution in the 2D rectangular grid, it results in nonuniform distribution on the sphere. Specifically, points become densely concentrated near the poles, as shown in Figure 10. Comparison with spherical and ERP. The red box highlights blurry artifacts that appear near the polar regions. As the number of view directions decreases, more latents remain unsampled and unprocessed during denoising, making the issue more severe. Fig. 4. If this characteristic is ignored and the 2D grid is directly used for tuning, it leads to severe artifacts when viewed in perspective. As illustrated in Fig. 2, significant number of points are concentrated in certain areas, creating an artifact where points appear to be pulled toward the center, disrupting spatial consistency. To address these issues, recent works [16, 18] attempt to leverage the spherical properties of ERP for generating 360degree panoramic videos. However, these methods also encounter challenges during the projection between perspective and ERP spaces. Interpolation distorts the latent distribution, while sampling-based methods result in missing 12 Figure 11. Additional Qualitative Results (Images) points, which disrupt multi-view stitching and prevent effective information exchange between different views, leading to independent generation of each segment. In particular, applying the Offset Shifting Denoiser (OSD) from DynamicScaler can cause certain points to remain unprocessed during denoising steps, leading to blurry artifacts, as shown in Fig. 10. In contrast, our spherical representation ensures uniform latent distribution across the sphere, effectively eliminating these issues. As result, our method enables seamless tuning-free image and video generation. E. Implementation Details We used 89 view directions with an 80 FoV and 2,600 points for all experiments. Inference time per sample is approximately 30 seconds for image generation and 20 minutes for video generation. All experiments were conducted on an A100-40GB. Since our method does not require additional memory beyond what is needed by the models used in our experiments, SANA and LTX Video, we verified that it can also run on an RTX 3090 with 24GB VRAM. For text prompts, we use three descriptions corresponding to the top, middle, and bottom regions of the scene. E.1. Text Prompt Examples We conducted evaluations across 20 different scene concepts, ranging from urban to natural landscapes. Since the prompt generation rules differ between SANA [26] and LTX Video [9], slight variations in results may occur. Specifically, LTX Video struggles with very short prompts, so minor modifications were made to improve generation quality. Nevertheless, all prompts will be publicly released. Here are some example text prompts. 13 Figure 12. Additional Qualitative Results (Videos). The animated results are available on our project page. Prompts for generating panoramic images in the main manuscript. Underwater An upward view from underwater, looking towards the surface where sunlight beams penetrate the clear ocean. Gentle ripples create shimmering effect, and the water transitions from deep blue to lighter, almost turquoise hue near the surface. The light refracts beautifully, creating dreamlike underwater glow. stunning underwater scene filled with vibrant tropical fish swimming gracefully through the crystal-clear water. Various species, from small neon-colored fish to larger, elegant ones, move in harmony. The environment is serene, with the water gently flowing and reflecting the sunlight from above. The clarity of the water highlights the intricate details of the marine life. breathtaking top-down view of colorful tropical coral reef. The ocean floor is covered with vibrant corals, ranging from bright orange and pink to deep purple and blue. Small fish dart between the coral formations, while gentle waves create mesmerizing play of light and shadow on the seabed. The details of the marine ecosystem are incredibly vivid, showing the beauty of underwater biodiversity. Snow Village An upward view of the night sky. Soft moonlight filters through wispy clouds, casting serene glow over the winter landscape. From the snowy fields, view toward peaceful village nestled among snow-covered hills. Warm lights glow from the windows of small wooden cabins, contrasting with the crisp, cold air under the moonlit sky. high-angle view of snow-covered paths winding through the landscape. The fresh snow glistens under the moonlight, while the warm glow of lanterns and fireplaces reflects off the frosty roads, creating cozy contrast against the cold night. Green grass Dark clouds churned in slow, twisting spirals overhead, their shifting forms casting fleeting shadows below. The clouds thickened, their edges curling like ink dissolving in water, deepening into shades of charcoal and silver. Occasionally, bright patches pierced through the dense formations, creating stark contrasts of light and darkness in the sky. Dark clouds churned in slow, twisting spirals above the rolling expanse of vibrant green grass, their shifting forms casting fleeting shadows across the land. soft breeze rustled the blades, carrying the crisp scent of damp earth. The clouds thickened, their edges curling like ink dissolving in water, deepening in shades of charcoal and silver. Patches of sunlight briefly pierced through, creating shifting patterns of light and shadow that danced across the swaying field. rolling expanse of vibrant green grass stretched endlessly, each blade rustling softly in the gentle breeze. 14 The crisp scent of damp earth lingered in the air as the grass swayed rhythmically, creating subtle waves across the field. The surface shimmered with varying shades of green, highlighting the texture and movement of the landscape. tensity. White foam swirls atop the restless sea, contrasting against the dark depths. Gusts of wind carve patterns into the waves, while distant flashes of lightning momentarily reveal the chaotic movement below. Firefly An upward view of the vast night sky above an open field, with scattered fireflies drifting gently, their faint glows blending with distant stars. serene nighttime meadow, where countless fireflies flicker softly, casting warm, golden light that dances over the swaying grass and wildflowers. dramatic top-down view of vast open field grass, where waves of grass ripple in the night breeze, dotted with countless fireflies drifting just above, their soft glow flickering across the landscape. Prompts for generating panoramic videos in the main manuscript. Fireworks Vibrant fireworks burst across the night sky, painting the heavens with shimmering trails of vivid colors. Some fireworks transform into heart shapes before fading, adding touch of elegance to the display. The camera focuses on explosive arcs and sparkling embers, capturing every brilliant flash against an infinite, celestial canvas. The citys skyline stretches below, clearly visible as vibrant fireworks light up the night sky. The fireworks burst in various colors, scattering across the air, while their reflections shimmer on the glass windows of skyscrapers. The camera smoothly pans across the city, capturing the river and bridges, with distant car lights creating flowing effect. breathtaking city skyline stretches below, illuminated by countless lights reflecting off towering skyscrapers. The camera smoothly pans across the landscape, revealing river winding through the metropolis and bridges glowing under streetlights. Distant car headlights flow like streams of light, adding dynamic rhythm to the urban nightscape. Storm Dark storm clouds swirl overhead as multiple lightning bolts strike at different moments, briefly illuminating the chaotic sky. The jagged bolts cut through the darkness, revealing shifting cloud formations in flashes of electric blue and white. The perspective is an upward view, emphasizing the storms immense scale. The scene is dynamic, with each lightning strike casting sharp contrasts of light and shadow across the turbulent sky. mid-angle view reveals the vast ocean meeting the storm-filled sky, where towering waves rise and fall beneath the relentless tempest. Lightning bolts crack through the heavy clouds, their electric glow reflecting off the turbulent water. The horizon is barely visible, obscured by mist and rain as gusting winds whip across the seas surface. Each flash of light briefly exposes the chaos, illuminating the swirling storm above and the restless waves below. high-angle view captures the vast, turbulent deep blue ocean as powerful waves crash and churn beneath the storms force. The waters surface ripples with energy, each undulating motion reflecting the storms inTable 4. Evaluation prompt used for assessing 360-degree panoramic generation quality based on four criteria. Evaluation Prompt You are an evaluator assessing an image generation model based on single image at time. Your evaluation is based on the following four criteria: 1. Image Quality: Assess the overall quality of the image. 2. Aesthetic Appeal: Evaluate how visually pleasing the image is. 3. Distortion Level: Determine whether the image appears distorted. If it does not resemble photo taken with normal camera, it will receive lower score. 4. Connectivity: Check if the middle of the image appears disconnected. If there is noticeable break, the score will be lower. Each criterion is rated on five-point scale: Excellent (5), Good (4), Fair (3), Poor (2), and Awful (1). You will receive one image at time. For each criterion, provide concise reason for the score before listing the rating. Format your response as follows: - Image Quality: (Brief reason) Score - Aesthetic Appeal: (Brief reason) Score - Distortion Level: (Brief reason) Score - Connectivity: (Brief reason) Score {image} Please evaluate the image with the given criteria."
        }
    ],
    "affiliations": [
        "Korea Advanced Institute of Science and Technology (KAIST)"
    ]
}