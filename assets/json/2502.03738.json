{
    "paper_title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
    "authors": [
        "Feng Wang",
        "Yaodong Yu",
        "Guoyizhe Wei",
        "Wei Shao",
        "Yuyin Zhou",
        "Alan Yuille",
        "Cihang Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling."
        },
        {
            "title": "Start",
            "content": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More Feng Wang 1 Yaodong Yu 2 Guoyizhe Wei 1 Wei Shao 3 Yuyin Zhou 4 Alan Yuille 1 Cihang Xie 4 1 Johns Hopkins University 2 UC Berkeley 3 University of Florida 4 UC Santa Cruz 5 2 0 2 ] . [ 1 8 3 7 3 0 . 2 0 5 2 : r Abstract Since the introduction of Vision Transformer (ViT), patchification has long been regarded as de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 11, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving competitive test accuracy of 84.6% with base-sized model on the ImageNet1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/ wangf3014/Patch_Scaling. 1. Introduction In the past few years, we have witnessed the great success of Vision Transformers (ViTs) in representation learning, with series of visual foundation models learned with this plain architecture achieving highly competitive performance and establishing effective connections to other modalities such as natural language (Dosovitskiy et al., 2021; Caron et al., 2021; Radford et al., 2021; Yu et al., 2022a; Rombach et al., 2022; Kirillov et al., 2023; Liu et al., 2023). key insight 1 behind the ViT-like architectures lies in compressive encoding paradigm: instead of directly processing raw pixels that introduces significant complexity, these architectures leverage patchification layer to compress images into spatially smaller feature maps, making the representation space of an image roughly equivalent to that of medium-length text consisting of few hundred tokens. However, we argue that this operation often incurs irreversible information loss to visual inputs. For example, intuitively, we believe the information contained in 224224 resolution image is generally much richer than that in text consisting of 196 words; however they have nearly the same size of representation space under ViT encoder with patch size 1616 (we suppose the vision and language encoders share the same embedding dimension). The difference in information content between visual and textual data can also be directly reflected in their storage requirements: storing an uncompressed 24-bit, 224224 resolution image requires approximately 147KB, whereas storing 196-word text only needs about 1.15KB. Empirically, if we manually reduce the compression rate, for example, by changing the patch size of DeiT-Base from 1616 to 88, we can observe significant accuracy improvement from 81.8% to 83.5% on the ImageNet-1k classification benchmark (Deng et al., 2009). Nonetheless, since the computation of self-attention scales quadratically with sequence length, ViT architectures are sensitive to the patch size. At the time when ViT was first introduced in late 2020, it needed to ensure that its computational cost was comparable to that of the CNN counterparts; and given the computational capacity at that time, the models had to be computationally manageable in terms of memory consumption and training time, especially when trained with the medium-resolution, medium-scale ImageNet (Deng et al., 2009) and beyond (Sun et al., 2017). As result, the architectural design of ViT had to compromise with compressive encoding paradigm achieved through patchification. The success of this design, patchification with typical 1616-pixel kernel, has led to its widespread adoption as default component in various subsequent architectures, even including those non-attention models such as ConvNeXt (Liu et al., 2022) and Vision Mamba (Zhu et al., 2024), while the impact of information loss posed by this compressive encoding paradigm has not been well studied. Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More (a) DeiT-B, 6464 Input, CLS (b) Adventurer-B, 128128 Input, CLS (c) Adventurer-B, 224224 Input, CLS (d) ADE20k Semantic Segmentation (e) COCO Object Detection (f) COCO Instance Segmentation (g) DeiT-B, 128128 Input, CLS (h) Adventurer-L, 128128, CLS (i) Adventurer-T, 224224, CLS Figure 1. Patchification Scaling Laws. We observe smooth and consistent decrease in test loss across different vision tasks, input resolutions, and model architectures when reducing the patch size. The performance gains remain considerably significant even when scaling down the patch size to 11. In all sub-figures, both and axes are in log scale. CLS denotes ImageNet-1k classification. In this work, we aim to thoroughly examine how compressive encoding affects visual representations and whether patch size can be new scaling dimension for modern visual architectures. While the concept of Scaling Laws (Kaplan et al., 2020) has been broadly testified in natural language processing, leading to great prosperity of Large Language Models over the past few years (Touvron et al., 2023; Team et al., 2023; Achiam et al., 2023), the scaling-up of vision models faces practical issues in the dimensions of both parameter size and input size (detailed in Section 4.3). Here, we aim to revisit the scaling potential of vision models from new perspective of spatial compression, attempting to unlock the compressed information by reducing the patch size. We highlight that through patchification, there is significant room for scaling up the models computation, and new scaling law may emerge during this process. Thanks to the rapid advancements in hardware, efficient attention mechanisms (Dao et al., 2022; Kwon et al., 2023), as well as linear-complexity structures (Katharopoulos et al., 2020; Peng et al., 2023; Gu & Dao, 2023), we can now extensively validate the impact of patchification at standard input size (e.g., 224224 for ImageNet) with manageable computing resources. We conduct series of straightforward scaling experiments on patchification, gradually reducing the models patch size from the typical 1616 down to 11 to lower the compression rate and observe how performance changes. We employ both ViT and Adventurer (Wang et al., 2024b), Mamba-based (Dao & Gu, 2024) linear-complexity architecture, to make our conclusions generalizable and experiments affordable in computation. To our surprise, this simple scaling study delivers three intriguing discoveries: First, as shown in Figure 1, we excitedly observe new scaling law for patchification in vision models. Similar to the scaling laws discovered in early studies (Kaplan et al., 2020) on languagewhere continuously increasing model parame2 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More ters reliably leads to consistent performance gainswe have identified new scaling dimension for vision models, which is reflected in the observation that as the compression rate (i.e., patch size) decreases, the models test loss smoothly declines, reaching its limit at single-pixel patch sizes that essentially form non-compressive encoding paradigm. This conclusion broadly holds true for various vision tasks, diverse input scales, and different visual architectures. Second, we confirm that visual encoding can be performed in very long token sequence, while patchification is not requisite for building effective vision models, but rather compromise to memory and computation overhead when resource is limited. The information lost in the compression of the patchification layer is actually crucial for the models prediction: on the standard 224224-resolution ImageNet1k classification benchmark, we remove the patchification operation and form super-long visual sequence consisting of 50,176 tokens, by which we boost the models test accuracy from 82.6% to remarkable result of 84.6%. Finally, we observe compelling phenomenon in semantic segmentation: as we transit from patch-based tokenization to pixel-level modeling, the traditional necessity for decoder headlong considered default component since the inception of deep network architecturescan be eliminated without compromising performance. This architectural simplification is potentially profound, suggesting the possibility of developing decoder-free dense prediction models and illuminating the path toward universal, encoder-only visual architecture capable of learning from every pixel. 2. Related Work Generic visual backbones. The development of visual backbones has fundamentally shaped the field of computer vision. Initially dominated by Convolutional Neural Networks (CNNs), these architectures have evolved to gain increasing capabilities for visual representation learning. Pioneering works such as LeNet (LeCun et al., 1998) and AlexNet (Krizhevsky et al., 2012) have proven the significant effectiveness of convolutional architectures in largescale image classification tasks. Following these foundational models, the architecture has been refined with the innovations in model depth (Simonyan & Zisserman, 2015), residual connection (He et al., 2016; Huang et al., 2017), and efficient neural architecture search (Tan & Le, 2019). The landscape of visual backbones underwent another round of significant transformation with the introduction of ViTs (Dosovitskiy et al., 2021) in late 2020, where novel plain architecture was proposed that treats images akin to language sequences. This model utilizes simple patchification layer to convert images into sequences of tokens, which are then processed using mechanisms adapted from language models. This approach opened new avenues in handling visual data without the inductive biases inherent in CNNs, demonstrating competitive performance on several benchmarks. The success of ViTs have spurred rapid development and innovations in data-efficient training strategies (Touvron et al., 2021a; 2022a;b), self-supervised learning techniques (Caron et al., 2021; Chen et al., 2021b; Bao et al., 2022; He et al., 2022), vision-language understanding (Radford et al., 2021; Jia et al., 2021; Liu et al., 2023; Alayrac et al., 2022; Yu et al., 2022a), and hierarchical architecture designs (Liu et al., 2021; Chen et al., 2021a; Yuan et al., 2021; Yu et al., 2022b). Inspired by the patchification design of transformers, there have been many CNN-based (Liu et al., 2022) and State Space Model (Kalman, 1960; Gu et al., 2022; 2021) based architectures (Zhu et al., 2024; Wang et al., 2024a;b) following the same paradigm. Notably, the Mamba (Gu & Dao, 2023; Dao & Gu, 2024) token mixer, due to its advantage of linear complexity, has recently been widely used to explore vision tasks and has achieved competitive results (Zhu et al., 2024; Wang et al., 2024a;b; Ren et al., 2024; Liu et al., 2024; Yang et al., 2024; Hatamizadeh & Kautz, 2024; Huang et al., 2024; Li et al., 2024; Lieber et al., 2024). Among them, the Adventurer (Wang et al., 2024b) architecture, which significantly simplifies the overall model, has demonstrated superior speed compared to the Transformer. In this paper, we employ it as one of the primary experimental models. Visual architecture scaling. Scaling laws was initially studied in natural language processing (Kaplan et al., 2020). In vision, similar concept has guided the community to scale up foundational models in both parameter size and data volume. For example, in the age of CNNs, EfficientNets (Tan & Le, 2019; 2021) have proposed to scale-up the models in depth, width, and resolution. These advancements were then integrated into ResNets, leading to nearly Billion-level parameter CNNs (Xie et al., 2020; Kolesnikov et al., 2020; Huang et al., 2019; Bello et al., 2021; Wightman et al., 2021). Scaling the parameter count of Vision Transformers has also shown great success in modern visual understanding benchmarks and has exhibited state-of-theart results (Touvron et al., 2021b; Zhou et al., 2021; Zhai et al., 2022; Dehghani et al., 2023). 3. Method 3.1. Problem Formulation This work aims to investigate the impact of spatial compression on the representation capability of modern visual architectures by scaling the downsampling rate of the patchification operation. The primary experiments are conducted on ViT-like plain architectures, with their definition as follows: The image encoder : R3wh RLD con3 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More sists of patchification layer at the beginning, positional embeddings, and number of cascade token mixers and channel mixers. The patchification layer divides the input image R3wh into non-overlapping patches of size p, flattening and projects them into 1D token sequence RLD. The following mixer layers extract deep visual features while keeping the sequence length and the feature dimensionality unchangedwhich means the patchification layer makes the only spatial compression throughout the whole visual encoder. To eliminate the influence of different mixer types on the results of patchification scaling, we conduct the main experiments using two visual encoders: the standard ViT (Dosovitskiy et al., 2021) and Adventurer (Wang et al., 2024b). Due to the significant memory and computation challenges posed by the quadratic complexity of self-attention, ViT is only used for context lengths within 4,096 in this work. For longer sequence tasks, we employ Adventurer, recent Mamba-based (Gu & Dao, 2023; Dao & Gu, 2024) efficient architecture that excels in modeling long range dependencies with linear complexity. Adventurer shares the same plain framework as ViT, with spatial compression only presents in the initial patchification layer, while the key difference is that Adventurer leverages the recent Mamba (Dao & Gu, 2024) module as its token mixer, which has linear complexity relative to sequence length and allows us to perform pixel tokenization for even the standard 224224 resolution inputs within reasonable computational resources (e.g., 256 A100 GPUs). Remarkably, in our experiments, we form super-long visual sequence of 50,176 tokens for ImageNet inputs by scaling down the patch size to 11. 3.2. Technical Details We conduct patchification scaling experiments on image classification, semantic segmentation, object detection and instance segmentation tasks. Following the standard design of ViTs (Dosovitskiy et al., 2021) and Adventurer, we extract holistic visual features by learnable [CLS] token for classification. For object detection and instance segmentation, we load backbones pretrained with classification and employ Cascade Mask R-CNN (Cai & Vasconcelos, 2019) as decoder head. Note that we use the same patch size for classification pretraining and downstream finetuning to ensure consistency in the scaling property. For semantic segmentation, in addition to evaluating the standard encoder-decoder structure, we also explore decoder-free approach to observe the emerging properties of patchification scaling. Specifically, instead of using deep UperNet (Xiao et al., 2018) as the default segmentation head, we employ simple linear layer to project the dense features extracted by the backbone into the category dimension for training the semantic segmentation task. This modification is based on the following prior assumption: in dense prediction tasks like semantic segmentation, the decoder head serves two main functions. The first is addressing the issue where the backbones high downsampling rate results in feature granularity that is insufficient for pixel-level predictionstypically mitigated by designs such as atrous convolution and multi-scale feature fusion (Chen et al., 2017; 2018). The second function is enhancing the models learning capacity by introducing additional trainable parameters. Under this assumption, we believe that if the backbones compression rate is already very low, the decoders benefits would be limited to the second aspect. Therefore, task-specific decoder head designs become less critical, and training general high-fidelity backbone alone would be sufficient to handle various vision tasks. 4. Experiments 4.1. Experimental Setup The experiments are conducted on the standard ImageNet1k (Deng et al., 2009) classification, ADE20k (Zhou et al., 2019) semantic segmentation, and COCO (Caesar et al., 2018) object detection and instance segmentation benchmarks. For ViTs, we follow the data-efficient strategy of DeiT (Touvron et al., 2021a) to train the model for 300 epochs by an AdamW (Loshchilov & Hutter, 2019) optimizer with 1024 batch size, 0.001 learning rate and 0.05 weight decay. For Adventurer, we basically refer to their optimized multi-stage training recipe to improve efficiency and obtain competitive results. The details of the training strategy can be found in Appendix. In semantic segmentation, we follow the prior practice of DeiT and Adventurer to finetune the classification models with an AdamW optimizer, 5e-5 learning rate, 0.01 weight decay, total batch size of 16 for 160k iterations. We train object detection and instance segmentation with AdamW optimizer, 1e-4 learning rate and 0.05 weight decay for 12 epochs. 4.2. Main Results As shown in Figure 1, we first evaluate the models patchification scaling performance using test loss as unified metric across different input sizes, tasks, and parameter scales. We observe an interesting phenomenon that the models predictive performance consistently improves as the patch size decreases. This observation effectively highlights the negative impact of the existing compressive encoding approach in visual models and supports our initial hypothesis: patchification is not necessary component for visual encoders; its primary role is to improve computational efficiency at the cost of partial information loss. Although this efficiency gain is significant for Transformer models with quadratic complexity, our findings suggest that when the computing resource allowsand indeed, computational power has 4 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More Model DeiT-Base (Touvron et al., 2021a) DeiT-Base (Touvron et al., 2021a) Adventurer-Base (Wang et al., 2024b) Adventurer-Base (Wang et al., 2024b) Adventurer-Base (Wang et al., 2024b) Input size 6464 128128 6464 128128 224224 patch tokenization p=16 68.2 78.1 69.2 79.0 82. p=8 76.9 81.0 77.2 81.5 83.9 p=4 80.1 82.3 80.0 81.8 84.3 p=2 80.8 82.9 80.5 82.2 84.5 pixel tokenization p=1 seq. length 81.3 4,096 - - 80.9 4,096 82.4 16,384 50,176 84.6 Table 1. Detailed ImageNet classification results. As patch size (denoted as p) decreases, the test accuracy (%) on ImageNet-1k (Deng et al., 2009) consistently improves and reaches the best performance with pixel tokenization. We highlight that we successfully scale up the visual token sequence to an unprecedented length of 50,176, with competitive 84.6 test accuracy obtained by base-sized model. evolved rapidly over yearswe should reconsider the traditional compressive encoding approach and begin embracing the notion of pixel is worth token that stands for non-compressive representation learning paradigm. We also observe that reducing the patch size not only improves performance in dense prediction tasks like semantic segmentation and instance segmentationwhich naturally favor fine feature granularities and for which smaller patch size is direct solutionbut also benefits holistic tasks like image classification, which inherently do not require finegrained representations. This result indicates that the primary benefit of reducing the patch size comes from unlocking the visual information that is previously compressed by patchification. This information, often considered insignificant low-level features in the past, is actually considerably critical for visual understanding. ImageNet classification results are elaborated in Table 1. As shown, in terms of test accuracy, the models also experience smooth and consistent performance improvement with patch size decreasing. Notably, with the help of Adventurers linear time complexity and efficient memory consumption, we successfully scale up the visual token sequence to length of 50,176 in the ImageNet classification task. To our knowledge, this is the first time that modern visual architectures have extended the input sequence to such length and processed it directly without partitioning. It not only achieves highly competitive 84.6% test accuracy with base-sized model (100M parameters), but more importantly, it demonstrates that visual understanding can be effectively performed from very long contexts. ADE20k semantic segmentation results are summarized in Table 2. As shown, we observe the same scaling behavior in this dense prediction task, with its test loss smoothly decreasing (see Figure 1) and mIoU score consistently improving as patch size shrinks. It is worth noting that even though we eliminate the task-specific decoder head in this experiment, the encoder-only modelswhether the 13Mparameter tiny-sized model or the 100M-parameter basesized modelcan still produce competitive results when the encoding compression rate becomes sufficiently low. Figure 2 presents direct comparison on the impact of deModel Adventurer-T Adventurer-B Decoder UperNet None None None None UperNet None None None None Params Patch size mIoU 41.3 1616 1616 40.0 88 41.6 44 42.1 22 42.5 45.7 1616 1616 44.0 88 45.5 44 46.3 22 46.8 17M 12M 12M 13M 13M 112M 99M 99M 100M 100M Table 2. ADE20k semantic segmentation. We focus on decoderfree structures and observe the mIoU score improves smoothly when patch size shrinks. We highlight the results that reach the limits of hardware capabilities in blue and best results bolded. Figure 2. Decoders impact on semantic segmentation. We train semantic segmentation model with the same backbone but different decoder heads: an UperNet with 13M parameters and simple linear layer with 0.2M parameters. We observe that as patch size decreases, the impact of the decoder head diminishes. coders in semantic segmentation, where we load the same pretrained backbone (Adventurer-Base) and finetune it separately with UperNet (Xiao et al., 2018) and simple linear projection layer. As shown, with high spatial compression rate such as 16, the model can easily benefit from decoder head; however, as the patch size decreases and the encoder itself can produce sufficiently fine-grained features, the functionality of decoders starts to be marginalized. 5 Model - u v Patch 3232 1616 88 44 22 6464 3232 1616 88 44 - u v Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More APb APb 63.3 44.7 65.2 46.5 66.7 48.0 67.1 48.5 67.3 48.7 62.8 44.1 65.0 46.4 67.2 48.4 67.9 49.5 68.5 50.3 50 APb 48.6 50.4 51.8 52.3 52.4 48.0 50.3 52.4 53.3 54.0 75 APm APm 60.4 38.4 62.2 40.3 63.6 41.7 64.1 42.2 64.3 42.4 60.1 38.3 62.5 40.6 64.8 42.0 65.5 42.9 66.0 43.4 50 APm 75 41.4 43.5 45.0 45.4 45.7 41.8 43.1 45.0 46.1 46.6 Table 3. COCO object detection and instance segmentation. Similar to classification and semantic segmentation results, these two tasks exhibit consistently enhanced performance as patch size decreases. We highlight the results that reach the limits of hardware capabilities in blue and best results bolded. Interestingly, this experiment validates our hypothesis presented in Section 3.2, demonstrating that the core component of developing dense prediction models lies in reducing the spatial compression rate, while the help that decoder heads can provide is very limited. This insight further suggests that with non-compressive encoders, it becomes feasible to build visual foundation model that could provide pixel-level representations and effectively supports various downstream tasks without requiring significant efforts to adapt to their specific objectives. In this work, we keep focusing on the exploration of patchification scaling and leave the development of pixel foundation models for future research. We hope that our findings here can provide solid theoretical foundation for such endeavors. COCO object detection and instance segmentation tasks also showcase similar effect of patchification scaling. As summarized in Table 3, both tasks achieve their best performance when the patch size reaches the hardwares computational limits (22). Compared to the high compression baselines, both Adventurer-Tiny and Base models demonstrate significant precision improvements, such as 48.7% vs. 44.7% for Adventurer-Tiny and 50.3% vs. 44.1% for Base. Notably, we have conducted patch size scaling experiments across four tasks: object classification, semantic segmentation, object detection, and instance segmentation. These experiments span variety of input resolutions (from 6464 to short side of 800), different training objectives, and different token mixer types (self-attention and Mamba (Gu & Dao, 2023)). Despite these variations, consistent and generalizable conclusion emerges: Reducing patch size reliably guarantees performance gains. 4.3. Ablation Studies Patchification scaling vs. parameter scaling. In Figure 3a, we compare the impact of scaling down the patch size versus scaling up the parameter count on the performance of Adventurer. As shown, for fixed patch size and input (a) Scaling from Adventurer-Base/16, 224224 input. (b) Scaling form ViT-Base/16, 128128 input. Figure 3. Patch size scaling vs. parameter scaling. Given an Adventurer-Base with 224224-resolution inputs, we scale up the model along two dimensions respectively. The model struggles to achieve further accuracy improvements beyond 760M parameters, whereas scaling down the patch size continues to show consistent upward trend in performance. size, increasing the parameter count within certain range (e.g., up to 760M parameters) yields significant performance gains. However, further scaling beyond this point does not necessarily lead to additional benefits. In fact, overcoming the parameter scaling bottleneck in vision models is both technically challenging and costly. It often requires investing in higher-quality training data (Zhai et al., 2022; Radford et al., 2021), incorporating self-supervised learning approaches (Caron et al., 2021; He et al., 2022), and making extensive hyperparameter tuning efforts (Touvron et al., 2022b). In contrast, patch size scaling not only exhibits better computation-accuracy tradeoff and achieves higher performance limits than parameter scaling, but it also offers simpler and more straightforward learning process: when training with different patch sizes, there is no need to modify training strategies or datasets, and all experiments can be 6 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More Model size 1616 Patch size 44 88 22 11 with 128128 resolution inputs: Tiny Small Base Large 72.6 77.6 79.0 79.8 78.5 80.5 81.5 82.2 with 224224 resolution input: Tiny Small Base 78.2 81.8 82. 80.9 83.0 83.9 80.4 80.9 81.8 82.6 81.3 83.5 84.3 80.6 81.2 82.2 82.9 81.7 83.7 84.5 80.7 81.4 82.4 83. 81.9 83.8 84.6 Table 4. Scaling both patch and model sizes. The gains from patch size scaling and model size scaling are not conflicting; combining both can lead to further performance improvements. The numbers denote ImageNet accuracy (%) with Adventurer models. We associate the results with different shades for clear observation. despite having similar parameter counts and FLOPs. However, beyond this input size, further increasing the input dimensions provides diminishing returns in performance gains. If we fix the sequence lengthscaling up the input size while proportionally increasing the patch sizethe model undergoes rapid growth in parameter count in the patchification layer, which may easily result in reduced model efficiency and stability during scaling. We showcase this issue in Figure 4, where it is observed that resizing inputs beyond their original resolutions does not provide additional information gains. Instead, the over-parameterization of the patchification layer leads to considerable performance degradation. As comparison, the direct scaling of patch size can effectively avoid the over-parameterization issue, making it flexible and practical scaling dimension for modern visual architectures. Scaling in both dimensions. In Table 4, we provide more ImageNet classification results with Adventurer models, where we scale them in both model size (parameter count) and patch size. As shown, the two scaling dimensions work synergistically when offering performance gains, with the highest accuracies consistently achieved by either the largest models or the smallest patch sizes. As analyzed earlier, the function of patch size scaling lies in reducing the spatial compression rate and thereby enabling the extraction of richer information from the data itself. Intuitively, this effect does not conflict with scaling up the model size, where performance gains mainly stem from enhanced fitting capabilities provided by increased parameters. The results in this experiment suggest that, given sufficient computing resources, we can easily transfer past advancements in parameter scaling to patchification scaling. In other words, patchification scaling can serve as complement to model size scalingwith the current data volume, we have already observed the limitations of parameter scaling in vision models (Zhai et al., 2022; Dehghani et al., 2023), while Figure 4. Input size scaling with fixed sequence length. We fix the ratio of image size/patch size and scale up the input size for ImageNet classification. As shown, when the input size is scaled beyond its original resolutions (e.g., typically 460 for ImageNet), further interpolating the input images does not yield additional accuracy gains. Instead, it leads to rapid increase in patchification parameters, resulting in training instability that ultimately harms performance. done in single run using the same set of hyperparameters. The potential of patchification scaling is even more evident in ViT. With the same input scale, reducing the patch size yields greater performance improvements for ViT compared to the linear-complexity Adventurer. Additionally, in terms of FLOPs, ViT has more room for scaling, as its computation grows quadratically with sequence length. As shown in Figure 3b, due to this quadratic complexity, ViT experiences larger increase in FLOPs than Adventurer when scaling down the patch size, leading to similar accuracy growth over FLOPs as that of parameter scaling (e.g., ViTBase/8 vs. ViT-Large/16). However, when investing higher FLOPs, parameter scaling falls significant short and may easily collapse with higher parameter counts. Limitations of input size scaling. Compared to scaling down the patch size at fixed input size, another methoddirectly scaling up the input sizecan achieve similar effect of reducing the compression rate and extending the token sequence. However, we contend that changing the input size is not flexible and applicable approach for effective scaling, as its upper bound is easily constrained by the original resolution of the image. For example, in the standard ImageNet classification benchmark, images are resized to 224224 for both training and evaluation stages (Touvron et al., 2021a; Liu et al., 2022; Wang et al., 2024b). This input size has actually compressed the visual information, as the average ImageNet image size is approximately 490430 pixels. Within this range, scaling the input size is generally more effective than scaling the patch size. For example, Adventurer-B/16 with 448448 input outperforms Adventurer-B/8 with 224224 input by 0.4%, 7 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More Model DeiT-Base, 128128 input Adventurer-Base, 224224 input Length By extending 78.1 78.2 (+0.1) 78.2 (+0.1) 82.6 82.7 (+0.1) 82.8 (+0.2) 82.8 (+0.2) 64 256 1,024 196 784 3,136 12,544 By scaling 78.1 81.0 (+2.9) 82.3 (+4.2) 82.6 83.9 (+1.3) 84.3 (+1.7) 84.5 (+1.9) Table 5. Ablation of sequence length. Extending the sequence length alone does not yield significant improvements (column by extending), whereas reducing patch size and lowering information compression rate is the primary source of performance gains (column by scaling). Performance is measured by ImageNet-1k accuracy (%), with longest sequences highlighted in blue. it is promising to see more future breakthroughs in visual encoding with the help of this new scaling dimension. Impact of sequence length. Intuitively, scaling down the patch size has two direct effects. First, smaller patch sizes allow the model to receive richer, more fine-grained input information, which can greatly benefit its inference abilities. Second, reducing the patch size directly extends the token sequence, and for token mixers like self-attention or Mamba, longer sequences inherently expand the models representational space, enhancing its capabilities in feature processing. Both factors can potentially have significant impact. We seek to demonstrate that the performance improvement from reducing the patch size primarily arises from the information gain due to lower compression rate, rather than from the enhanced representational capacity associated with an extended sequence length. We conduct direct ablation study: in contrast to our patch size scaling approach, we set up an additional experiment that extends the input sequence interpolating on existing tokens. Specifically, in this comparison, we retain the original large patch size (1616) but perform spatial interpolation on the tokens produced by the patch embedding, by which we extend the input sequence without introducing any new information. As shown in Table 5, this approach does not bring substantial improvements to the models performance (see column By extending). In contrast to the significant gains achieved through patchification scaling (e.g., 4.2% accuracy on ImageNet), this ablation study effectively demonstrates that the benefits of our approach primarily stem from unlocking the visual information compressed by large patch sizes, enabling the model to focus on more detailed visual features, while simply scaling the sequence length itself has only minimal impact on performance. 4.4. Discussions We summarize the computational requirements involved in the patchification scaling experiments in Table 6. As shown, the super-long visual token sequences associated with small Patch Length 16 8 4 2 1 196 784 3,136 12,544 50,176 Memory GPU hours (per image) DeiT-Base Adv-Base 0.45 1.76 6.86 27.45 115. 62MB 252MB 1,024MB 4,057MB 16,118MB 0.36 1.86 9.79 80.06 967.99 Table 6. Computational overhead for training DeiT-Base and Adventurer-Base at 224224 resolution inputs and different patch sizes. Memory usage is calculated based on the per-image consumption in ViT. GPU hours (for each ImageNet epoch) are estimated on single A100 GPU. The models are trained at Float16 precision with FlashAttention (Dao et al., 2022) applied in ViT. The detailed evaluation protocol can be found in Appendix. patch sizes impose significant hardware overhead on ViT architectures. This overhead was indeed major challenge around five years ago, when V100 GPUs with 16/32GB memory remained the mainstream hardware for AI training. However, with rapid advancements in hardware development, efficient parallel computing mechanisms, as well as low-complexity visual architectures, the idea of learning from pixels has become increasingly feasible. In the experiments, we have demonstrated many key benefits of patchification scaling, such as direct performance improvements, reduced dependence on decoders, and the ability to overcome many limitations of parameter scaling and input size scaling. These emerging properties suggest that, when computational resources allow, we should gradually reduce or even abandon the spatial compression mechanisms in vision encoders, fully exploiting all the information inherently provided by the data. We hope this paper can provide insights and inspiration for transition from the current patch-based compressive encoding paradigm to pixel-based non-compressive visual foundation models. 5. Conclusion In this work, we conduct extensive studies in reducing the spatial compression rate in patchification layers and discover new scaling dimension for visual encoding, which we term Patchification Scaling Laws. The new scaling laws suggest that, with more computational resources invested, leveraging smaller patch sizes consistently leads to improved predictive performance. This conclusion is broadly applicable across various vision tasks, different input resolutions, and diverse model architectures. As by-product, we also identify an interesting emerging property of patchification scaling: when the encoder patch size becomes sufficiently small, the benefits provided by task-specific decoder heads diminish significantly. We hope the discoveries in this paper can provide solid theoretical foundation for the future pixel learning paradigm and the development of non-compressive visual foundation models. 8 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More"
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank TPU Research Cloud (TRC) program and Google Cloud Research Credits program for partially supporting our computing needs."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. Bao, H., Dong, L., and Wei, F. BEiT: BERT pre-training of image transformers. In ICLR, 2022. Bello, I., Fedus, W., Du, X., Cubuk, E. D., Srinivas, A., Lin, T.-Y., Shlens, J., and Zoph, B. Revisiting resnets: Improved training and scaling strategies. NeurIPS, 2021. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers to 22 billion parameters. In ICML, 2023. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: large-scale hierarchical image database. In CVPR, 2009. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and Re, C. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In NeurIPS, 2021. Caesar, H., Uijlings, J., and Ferrari, V. Coco-stuff: Thing and stuff classes in context. In CVPR, 2018. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In ICLR, 2022. Cai, Z. and Vasconcelos, N. Cascade r-cnn: High quality object detection and instance segmentation. IEEE transactions on pattern analysis and machine intelligence, 2019. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In ICCV, 2021. Chen, C.-F. R., Fan, Q., and Panda, R. Crossvit: Crossattention multi-scale vision transformer for image classification. In ICCV, 2021a. Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE TPAMI, 2017. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. Chen, X., Xie, S., and He, K. An empirical study of training self-supervised vision transformers. In ICCV, 2021b. Dao, T. and Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In ICML, 2024. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. NeurIPS, 2022. Hatamizadeh, A. and Kautz, J. Mambavision: hybrid mamba-transformer vision backbone. arXiv preprint arXiv:2407.08083, 2024. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, 2016. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In CVPR, 2022. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In CVPR, 2017. Huang, T., Pei, X., You, S., Wang, F., Qian, C., and Xu, C. Localmamba: Visual state space model with windowed selective scan. arXiv preprint arXiv:2403.09338, 2024. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. NeurIPS, 2019. Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. Kalman, R. E. new approach to linear filtering and prediction problems. 1960. 9 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Nguyen, D.-K., Assran, M., Jain, U., Oswald, M. R., Snoek, C. G., and Chen, X. An image is worth more than 16x16 patches: Exploring transformers on individual pixels. arXiv preprint arXiv:2406.09415, 2024. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In ECCV, 2020. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, 2023. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 1998. Li, K., Li, X., Wang, Y., He, Y., Wang, Y., Wang, L., and Qiao, Y. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024. Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., ShalevShwartz, S., et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., and Liu, Y. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. convnet for the 2020s. In CVPR, 2022. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Ren, S., Li, X., Tu, H., Wang, F., Shu, F., Zhang, L., Mei, J., Yang, L., Wang, P., Wang, H., et al. Autoregressive pretraining with mamba in vision. arXiv preprint arXiv:2406.07537, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Simonyan, K. and Zisserman, A. Very deep convolutional In ICLR, networks for large-scale image recognition. 2015. Sun, C., Shrivastava, A., Singh, S., and Gupta, A. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017. Tan, M. and Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. Tan, M. and Le, Q. Efficientnetv2: Smaller models and faster training. In ICML, 2021. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transformers & distillation through attention. In ICML, 2021a. Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and Jegou, H. Going deeper with image transformers. In ICCV, 2021b. Touvron, H., Cord, M., El-Nouby, A., Verbeek, J., and Jegou, H. Three things everyone should know about vision transformers. ECCV, 2022a. Loshchilov, I. and Hutter, F. Decoupled weight decay reguTouvron, H., Cord, M., and Jegou, H. Deit iii: Revenge of larization. In ICLR, 2019. the vit. In ECCV, 2022b. 10 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Wang, F., Wang, J., Ren, S., Wei, G., Mei, J., Shao, W., Zhou, Y., Yuille, A., and Xie, C. Mamba-r: Vision mamba also needs registers. arXiv preprint arXiv:2405.14858, 2024a. Wang, F., Yang, T., Yu, Y., Ren, S., Wei, G., Wang, A., Shao, W., Zhou, Y., Yuille, A., and Xie, C. Causal image modeling for efficient visual understanding. arXiv preprint arXiv:2410.07599, 2024b. Wightman, R., Touvron, H., and Jegou, H. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified perceptual parsing for scene understanding. In ECCV, 2018. Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training with noisy student improves imagenet classification. In CVPR, 2020. Yang, C., Chen, Z., Espinosa, M., Ericsson, L., Wang, Z., Liu, J., and Crowley, E. J. Plainmamba: Improving nonhierarchical mamba in visual recognition. arXiv preprint arXiv:2403.17695, 2024. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are imagetext foundation models. arXiv preprint arXiv:2205.01917, 2022a. Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. Metaformer is actually what you need for vision. In CVPR, 2022b. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.-H., Tay, F. E., Feng, J., and Yan, S. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In ICCV, 2021. Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling vision transformers. In CVPR, 2022. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., and Torralba, A. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Jiang, Z., Hou, Q., and Feng, J. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., and Wang, X. Vision mamba: Efficient visual representation learning with bidirectional state space model. In ICML, 2024. 11 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More"
        },
        {
            "title": "Appendix",
            "content": "A. More Technical Details The detailed configuration of the models used in this paper are elaborated in Table 7. For ViTs, We basically follow the configurations in the DeiT series models (Touvron et al., 2021a; 2022b), but change the default patch size of DeiT-Huge to 1616, the same as the other DeiT models. For Adventurer, we scale up the model to huge size following the same rule of DeiT; we set its embedding dimension to 1,280, keeping its original MLP ratio and employ 32 blocks in total. Model DeiT-Tiny (Touvron et al., 2021a) DeiT-Small (Touvron et al., 2021a) DeiT-Base (Touvron et al., 2021a) DeiT-Large (Touvron et al., 2022b) DeiT-Huge (Touvron et al., 2022b) Adventurer-Tiny (Wang et al., 2024b) Adventurer-Small (Wang et al., 2024b) Adventurer-Base (Wang et al., 2024b) Adventurer-Large (Wang et al., 2024b) Adventurer-Huge (Wang et al., 2024b) Embedding dimension MLP dimension Blocks Parameters 768 1,536 3,072 4,096 5,120 640 1,280 1,920 2,560 3,200 5M 22M 86M 304M 631M 12M 44M 99M 346M 759M 192 384 768 1,024 1,280 256 512 768 1,024 1, 12 12 12 24 32 12 12 12 24 32 Table 7. Model configurations. All models have 1616 patch size by default. Protocols of estimating memory and GPU hours. In Table 6, we present an estimation of the GPU memory and training hours required for DeiT and Adventurer. Here we give more details of how they are evaluated. We calculate the memory consumption by each image. That means, the reported numbers have excluded the memory used for storing the model, optimizer, and other hyper-parameters. The actual memory demand increases linearly with batch size. To evaluate the GPU hours required for training, we set total batch size of 1,024 and use the minimum number of nodes necessary for training (depends on the total memory demand). Each node is equipped with 8 A100/80GB GPUs. The estimated training hours are then multiplied by the total number of GPUs used to ensure that the reported numbers are normalized. Config optimizer base learning rate weight decay epochs optimizer betas batch size warmup epochs stochastic depth (drop path) layer-wise lr decay label smoothing random erasing Rand Augmentation repeated augmentation ThreeAugmentation Tiny/Small/Base Large/Huge"
        },
        {
            "title": "AdamW",
            "content": "5e-4 0.05 300 0.9, 0.999 1024 5 0.1 2e-4 0.3 200 0.9, 0.95 4096 20 0.2 Table 8. Recipe of the pretraining stage, for 6464 or 128128 pixel inputs. Training recipes. In this work, we train DeiT-Tiny, Small, and Base with the official repository (Touvron et al., 2021a) and recipe. For DeiT-Large and Huge, there is not training configuration in the original DeiT paper so we follow the supervised training pipeline reported in (He et al., 2022). Note that the Pixel Transformer (Nguyen et al., 2024) which conduct pixel tokenization experiments with low resolution images (2828) employs the same training recipe. For Adventurer, we mostly follow its original multi-stage strategy (Wang et al., 2024b) to train our models. Specifically, for 6464 resolution inputs, we simply perform the pretraining stage (shown in Table 8) for 300 epochs for all model sizes. For 128128 resolution inputs, we additionally perform finetuning stage (shown in Table 9) for enhanced results. For the standard 224224 resolution inputs, we follow the practice of Mamba-Reg (Wang et al., 2024a) and Adventurer (Wang 12 Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More et al., 2024b) to load the pretrained model at 128128, performing an intermediate training stage (shown in Table 10) for 100 epochs and then finetuning stage for 20 epochs. We highlight that this multi-stage training strategy is highly efficient for our experiments as we can fully exploit the models pretrained at lower resolutions. For example, we only need to train the 224224-input models for 120 epochs since we can load the weights pretrained at 128128 resolution inputs. Notably, for both DeiT and Adventurer, there is no need to adjust training recipes for different patch sizes, which we consider to be one of the flexible and practical advantages of patchification scaling. Config optimizer base learning rate weight decay epochs optimizer betas batch size warmup epochs stochastic depth (drop path) layer-wise lr decay label smoothing random erasing Rand Augmentation repeated augmentation ThreeAugmentation Small/Base Large AdamW 1e-5 0.1 20 0.9, 0.999 512 5 0.4 (S), 0.6 (B) 2e-5 0.1 50 0.9, 0.95 512 5 0.6 0.95 0.1 rand-m9-mstd0.5-inc1 Table 9. Recipe of the finetuning stage, for 128128 or 224224 pixel inputs. Config optimizer base learning rate weight decay epochs optimizer betas batch size warmup epochs stochastic depth (drop path) layer-wise lr decay label smoothing random erasing Rand Augmentation repeated augmentation ThreeAugmentation Small/Base"
        },
        {
            "title": "AdamW",
            "content": "5e-4 0.05 100 0.9, 0.999 1024 5 0.2 (S), 0.4 (B) 8e-4 0.3 50 0.9, 0.95 4096 20 0.4 0.9 Table 10. Recipe of the intermediate training stage, for 224224 pixel inputs."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "UC Berkeley",
        "UC Santa Cruz",
        "University of Florida"
    ]
}