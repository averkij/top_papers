{
    "paper_title": "COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation",
    "authors": [
        "Xueqing Deng",
        "Qihang Yu",
        "Ali Athar",
        "Chenglin Yang",
        "Linjie Yang",
        "Xiaojie Jin",
        "Xiaohui Shen",
        "Liang-Chieh Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to large-scale datasets. This dataset sets a new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 9 8 5 2 0 . 2 0 5 2 : r COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation"
        },
        {
            "title": "Xueqing Deng Qihang Yu Ali Athar Chenglin Yang Linjie Yang",
            "content": "Xiaojie Jin Xiaohui Shen Liang-Chieh Chen"
        },
        {
            "title": "Project Page",
            "content": "xueqingdeng@bytedance.com Figure 1. COCONut-PanCap Dataset. Top: The proposed COCONut-PanCap dataset features detailed captions grounded with dense panoptic segmentation masks. Bottom: COCONut-PanCap supports various fine-grained understanding and generation tasks, including detailed captioning, panoptic segmentation grounded caption, and text-to-image generation. The dataset also facilitates several downstream tasks, such as visual question-answering (VQA) and referring segmentation."
        },
        {
            "title": "Abstract",
            "content": "This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to largescale datasets. This dataset sets new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning. 1. Introduction Recent advancements in multi-modal foundation models have been largely driven by the availability of large-scale paired text-image datasets. These datasets, often collected via web crawling with basic filtering techniques [14, 52, 53], contain low-quality, web-sourced captions that lack depth and accuracy. In contrast, human-annotated caption datasets, such as COCO-caption [6], offer higher-quality descriptions but are limited in scale and tend to be concise, with an average caption length of 10 words. To overcome the limitations of short captions, the research community has leveraged vision-language models (VLMs) [5, 31, 32, 38, 60] to generate detailed synthetic captions. While these machine-generated captions improve visual understanding [5, 32] and generation tasks [31], they remain inferior to high-quality, human-verified annotations [44]. Addressing this challenge requires balancing scalability and annotation quality, as generating detailed and accurate image descriptions at scale remains labor-intensive [15, 44]. In this paper, we introduce an efficient annotation approach that combines dense mask annotations with commercial VLMs [5] to produce high-quality image captions. Our goal is to minimize human effort while generating rich, structured descriptions. To achieve this, we base our work on the COCO-caption dataset [6] due to its widespread use and diverse image content. We revisit the COCO-caption dataset to provide more detailed and comprehensive caption annotations. Our approach involves creating holistic captions synthesized from region-based dense captions that describe distinct areas within each image. Specifically, we build on recent COCONut panoptic segmentation annotations [9] to generate new set of detailed captions by: (a) annotating each segmentation region with VLM-generated draft, carefully refined through human corrections, and (b) summarizing these region captions into comprehensive image caption while preserving the grounding correspondence between image masks and object references. This enables novel task that integrates panoptic segmentation with grounded captioning. Our structured annotation process ensures that the captions are both complete, covering the majority of objects in each image, and grounded, with precise segmentation masks. The final dataset, named COCONut-PanCap, is designed for wide range of vision-language applications, combining Panoptic segmentation and grounded Captioning. It comprises 118K image-text pairs for training, with an average caption length of 203 words, as well as an additional 25K image-text pairs, with an average caption length of 233 words for validation. We demonstrate that COCONut-PanCap significantly boosts the performance of both VLM and text-to-image generation models at the instruction tuning and fine-tuning stages, outperforming recent detailed caption datasets [44]. This highlights the potential of our grounding-based captions for both visionlanguage understanding and image generation tasks. Our contributions are summarized as follows: We propose caption annotation pipeline leveraging panoptic segmentation to create high-quality, detailed caption dataset comprising 143K annotated images. The resulting annotations are comprehensive, accurate, and include grounding masks, making this dataset substantially larger than recent detailed caption datasets. Our COCONut-PanCap dataset facilitates new challenging task combining Panoptic segmentation and Grounded Captioning (PGC). We establish evaluation metrics and settings for this PGC task and benchmark several recent methods to assess performance on this novel challenge. We validate the utility of our proposed dataset across various fine-grained Image-to-Text (I2T) and Text-to-Image (T2I) tasks, including detailed caption generation, PGC, visual question answering (VQA), referring segmentation, and text-conditioned image generation. Experimental results show that our dataset significantly enhances model performance across all these tasks. 2. Related Work Detailed Captions from VLMs. Researchers are increasingly interested in creating large-scale datasets with detailed captions generated from advanced vision-language models. DenseFusion1M [32] utilizes pretrained perceptual model to prompt VLMs, facilitating more detailed image descriptions. Recap-DataComp1B [31] first fine-tunes the Llama-38B powered LLaVA-1.5 model [36], then applies it to recaption approximately 1.3 billion images from the DataComp1B dataset [14], generating rich repository of detailed image descriptions. On similar front, the PixelProse dataset [59] offers general-purpose image captions designed to serve various applications, from visual question answering (VQA) to pre-training tasks. Unlike datasets targeting single applications, PixelProse captions are dense, versatile image descriptions that can be adapted to other formats, such as VQA and instructional data, with the help of large language models (LLMs). Although these detailed caption datasets are large-scale, they are directly generated by VLMs without human verification, falling behind humanannotated captions on quality. Our proposed COCONutPanCap dataset leverages extensive human effort to ensure high-quality annotations. Human-annotated Detailed Captions. Several efforts have been made toward this goal, utilizing fully humanAnnotated by Avg. Words Masks Dataset Name BLIP-LCS DenseFusion1M [32] LLaVA-Recap118K [38] LLaVA-Details-23K [37] ShareGPT4V [5] ShareGPT4V-PT [5] PixelLM-MUSE [51] Osprey [69] GLaMM-GCG [50] COCO-caption [6] DCI [61] DOCCI [44] IIW [15] COCONut-PanCap (ours) Image Source Sample LAION [53], CC [4], SBU [45] 558K BLIP [30] LAION [53] 1,059K Vision Specialist Models LLaVA-NEXT [38] COCO [35] 118K GPT4 23K COCO [35] LAION [53], CC [4], SBU [45], COCO [35] etc. 100K GPT4-Vision LAION [53], CC [4], SBU [45], COCO [35] etc. 1,246K Share-Captioner [5] GPT4-Vision LVIS [17] 246K GPT4-Vision COCO [35] 724K RefCOCOg [40],PSG [65],Flick30K [47] 214K Vision Specialist Models Human COCO [35] 118K Human 8K SA-1B [24] Human 9.6K DOCCI [44] Human WebLI [15] 8.5K Human COCO [35] 118K 54 191 186 105 162 144 - - 128 11 144 136 217 203 3.7 - 3.6 13.2 Table 1. Dataset (training set) Comparison. Our proposed COCONut-PanCap dataset stands out for its detailed (2nd highest in Average Words), high-quality (human interactive annotated) captions and high-density segmentation masks (1st in Average Masks). denotes the mask number for referring segmentation which only counts the targets in QA format. Note that Samples means the number of collected annotations, where there may exist one image with multiple different annotation, i.e., in region-level datasets like Osprey. Dataset Name COCO-30K [6] DOCCI-test [44] IIW-test [15] GenEval [16] T2I-CompBench val [20] GLaMM-GCG val-test [50] COCONut-PanCap val (ours) Samples Avg. Words Caption T2I Grd. Seg. 30,000 5,000 445 553 2400 2,000 25,000 11 136 217 8 9 128 233 Table 2. Dataset (evaluation set) Comparison. Our COCONutPanCap validation set provides detailed captions and supports multiple multi-modal tasks, including image captioning, text-toimage generation (T2I), and grounded segmentation (Grd. Seg.). annotated data or human-in-the-loop approaches. One example is DOCCI [44] which is small, high-detailed image caption dataset that is entirely human-annotated, containing only 15K samples but providing diverse details, such as key objects, their attributes, spatial relationships, and text rendering. Two small-scale detailed caption datasets, ImageInWords [15] and DCI [61], use combination of automatic annotation models with human involvement, both with fewer than 10K samples. Pixmo-Cap [8] introduces large-scale dataset of detailed image captions from speechbased descriptions, offering richer visual annotations than text-based methods. Our proposed COCONut-PanCap dataset yields smaller scale compare to Pixmo-Cap but we have different focuses where Pixmo-Cap focuses on pretraining the VLMs while we focus on the instruction tuning and finetuning stages of VLMs and image generation models. Our work also shares similar annotation pipeline with recent video captioning dataset Shot2Story [18] where both VLM draft and human corrections are used to create complete and accurate annotations. Grounded Captions with Segmentation Masks. Existing work have made significant strides in creating datasets with region-level captions linked to entity segmentation masks [69] or bounding boxes [70]. few datasets associate grounded segmentation directly with captions. GLaMM [50] proposes Grounding-anything Dataset (GranD) using an automated annotation pipeline that encompasses 7.5M unique concepts grounded in total of 810M regions available with segmentation masks. However, Later, MGLMM [72] further explore the multigranularity GLaMM model to generate multi-granularity dataset. Our proposed COCONut-PanCap dataset follows similar approach of grounding captions to dense masks but offers significantly denser masks per caption, as shown in Tab. 1, with an average of 13.2 masks per image compared to 3.6 in GLaMM. Note that we focus on grounded segmentation for detailed captions, rather than descriptions of all levels of segmentation masks (objects or parts) as provided in the GranD dataset [50], which is outside the scope of our study. 3. COCONut-PanCap Dataset We construct novel dataset based on COCO images to provide detailed captions at both image and mask levels, using COCONut panoptic masks as foundation for comprehensive region descriptions. Specifically, we leverage panoptic masks from COCONut-S [9] to annotate detailed region captions, incorporating both thing and stuff masks to cover wide range of semantic regions. 3.1. Dataset Description Comprehensively understanding diverse visual elements in complex scenes can benefit multiple tasks including perception, understanding, and generation. In this section, we describe the annotation pipeline for our dataset leveraging the human annotated panoptic masks. We first show the staFigure 2. Annotation Pipeline. Given an input image, human-annotated panoptic segmentation masks are overlaid using set-of-marks [66] visualization techniques to prompt the vision-language model (VLM). After generating an initial draft, human effort is investigated for editing and verification. Finally, the annotated metadata will be formatted to construct the datasets for various tasks at instruction tuning or finetuning stage. Figure 3. Designed Prompt Template. By giving the concatenated set-of-marks images, the right side (round-1) shows the initial response and the corresponding human edits. Once finalized by humans, these edits will be merged into single detailed caption grounded with panoptic segmentation masks, as shown in the left side (round-2). tistical analysis of our final dataset in Tab. 1. On average, our captions contain 203 words spanning 11 sentences. We follow the same split setting in COCO2017 [35] dataset, which includes 118K training images. To provide comprehensive evaluation set, we adopt the same 25K images from COCONut-val split (which contains COCO2017-val and another 20K Objects365 [55] validation images). 3.2. Dataset Construction We argue that high-quality descriptions should provide sufficient details of key objects and their attributes, as well as information about secondary objects and background elements. To achieve this, as shown in Fig. 2, we use humanannotated panoptic segmentation masks to decide the set of objects to reference in the caption. These masks include both thing and stuff classes, representing single objects and semantic regions, respectively. We adopt the panoptic segmentation masks from the COCONut-S [9] dataset. The masks are overlaid on the images, labeled with class names c1, c2, . . . , cn C, where is the set of COCOs 133 panoptic classes. We then construct prompt with both the edited image and the original image and textual question for GPT-4V, as illustrated in Fig. 3. The resulting region captions from GPT-4V are reviewed and corrected by human raters for accuracy and consistency. Figure 4. Frequency of Extracted Nouns from the COCONutPanCap Dataset. The top 10 most frequent nouns are: people, table, room, street, dining, man, person, cars, chairs, and field. 3.3. Dataset Analysis Concepts Beyond COCOs 133 Classes. To clarify the goal of our annotation task, we focus on key visual features such as objects, attributes, spatial relationships, and counting. As shown in Fig. 4, we utilize the panoptic segmentation mask from COCONut-S, which includes 133 classes in the word vocabulary. Our proposed dataset, however, incorporates additional concepts beyond these 133 classes, such as vegetable and parking. This demonstrates that our human annotators delivers accurate and diverse descriptions when using the provided label names as reference. User Study for Caption Quality. We randomly sample 1,000 images from our COCONut-PanCap training set and asked human evaluator to perform single-choice selection task. The question is: Please select the best description for the image, considering the correctness of object names, attributes, counting, spatial relationships, and action. The compared captions are generated using GPT4V [1], Qwen2-VL [64], and InternVL-2 [7], resulting in single-choice four-option question. Fig. 5 illustrates the results, showing that our GPT-assisted human-annotated captions receives the highest ratings. More details can be found in the supplementary. 4. PGC Baseline: PanCaper In this section, we introduce our baseline method for joint panoptic segmentation and grounded captioning (PGC), namely PanCaper. We start with an overview of the pixel grounding task and then present our proposed approach, which incorporates panoptic segmentation module specifically designed for grounding objects in captions. Revisiting the Pixel Grounding Task. Our baseline model builds upon LISA [28], model that combines the language generation capabilities of VLMs with the ability to produce segmentation mask. LISA consists of three main components: VLM, vision backbone , and mask decoder D. With given text prompt, the VLM (typically LLaVA [36, 37]) generates an output containing SEG token. For instance, with the input prompt, Could you segment the food with high Vitamin C? LISA generates the response It is SEG. This process extracts the last-layer Figure 5. Caption Quality via User Study. The study involved human evaluators assessing random sample of 1,000 captions, with strong preference shown for captions from our dataset. embedding of the LLM from LLaVA. Then language-toprompt (L-P) projection layer (g) transforms the last-layer embeddings corresponding to SEG tokens (lseg) into the decoders feature space. Meanwhile, the vision backbone extracts dense visual features from the input image. Finally, both the dense features and the CLIP image embedding from LLaVA are fed into the mask decoder to produce the final segmentation mask. Prompt Instruction for Grounded Captioning. We propose baseline method for the PGC task by modifying LISA to enable grounded captioning with segmentation masks. Since LISA was originally designed for generating segmentation with single output mask, two main adjustments are necessary: (1) the use of multiple SEG tokens, and (2) extracting noun phrases from the caption for grounding. To facilitate grounded segmentation, we modify the prompt to the VLM as Please provide detailed description of the image and segment each part. This prompt triggers the model to generate caption responses with corresponding SEGi tokens, where [1, ] and is the total number of predicted segmentations. Given predicted caption for the image, aligning each SEGi token requires pairing it with noun phrase, pphrasei/p, where phrasei is the relevant part in the caption to be grounded. With these prompt tokens defined, the model uses the vision backbone and mask decoder to facilitate fine-grained, pixel-level grounding, with producing segmentation masks . Enable Panoptic Grounding. To achieve panoptic segmentation from captions, we first classify SEG tokens into two types: SEGt for thing classes and SEGs for stuff classes. These tokens are then processed by our segmentation modules to produce panoptic segmentation masks. We initialize the vision backbone with pretrained kMaX-DeepLab encoder [67] and fine-tune the decoder using our COCONut-PanCap dataset. Since kMaX-DeepLab operates as closed-set segmenter, we align text embeddings of the associated noun phrases with COCOs 133 panoptic classes. To accomplish this alignment, we use BERT [26] to generate the text embeddings and to calculate cosine similarity, selecting the bestmatching category. Panoptic grounding provides mapping between detailed captions and image regions, which improves interpretability of VLM predictions. Training Objectives. Our training objective aims to minimize the following losses: = 位textLtext + 位maskLmask, (1) where Ltext is the auto-regressive cross-entropy loss for text generation, and Lmask is the mask loss [63], encouraging the model to produce high-quality segmentation results. 位text and 位mask are the respective loss weights. We use the same loss weights as LISA [28]. Evaluation Metrics for Caption Quality. We conduct the analysis with multiple metrics to evaluate the quality and completeness of the generated captions. We introduce benchmarking suite for the PGC task, with validation set of 25K images. For the caption quality, we report the caption metrics including CIDEr [62], METEOR [2], ROUGEL [34], BLEU@4 [46] and CAPTURE [10]. For grounded panoptic segmentation, we report PQ scores [23]. 5. Experimental Results We assess the effectiveness of human-annotated caption data by performing three primary tasks utilizing our dataset in the fine-tuning/instruction tuning stage: detailed captioning, panoptic grounded captioning (PGC), and text-toimage generation. Additionally, we demonstrate the transferability of the knowledge learned from our dataset through two downstream tasks: VQA and referring segmentation. Detailed Captioning. We conduct instruction tuning with LLaVA-NeXT framework [38] for this task. We replace the caption data (23k) from the original LLaVA instructiontuning set with detailed captions from our dataset, keeping the same amount of instruction data size. We follow the same training setup used for LLaVA-NeXT with Llama3-8B [11]. Treating it as QA task, we use the prompt, Could you please describe the image in detail? and collect the corresponding response as the caption for the image. We evaluate caption quality using CIDEr [62], METEOR [2], BLEU@4 [46], ROUGE-L [34] and CAPTURE [10] metrics. We also extend the model by adding the mask-pooled features from the panoptic segmentation masks as additional signals to the LLaVA model and name it LLaVA-NeXT-pool. During training, we use the ground truth mask to extract the features while during inference we use the mask proposals from the pretrained kMaXDeepLab [67]. Besides, we also experiment with synthetic captions directly generated using InternVL-2 [7], Qwen2VL [64] and GPT-4V [1]. We follow the same data preparation settings as our dataset to build these instruction datasets for these 23K images with different sources of synthetic detailed captions, namely LLaVA 665K-InternVL2-Cap , LLaVA 665K-Qwen2VL-Cap, and LLaVA 665K-GPT4VCap. These datasets are used to produce models LLaVANeXT-I, LLaVA-NeXT-Q, and LLaVA-NeXT-G respectively. More details can be found in the supplementary. The results are presented in Tab. 3. LLaVA-NeXT models show improved performance when fine-tuned on the custom instruction-tuning dataset. Among these, LLaVA-NeXTpool achieves the highest scores in all metrics, with CAPTURE of 61.4, CIDEr of 13.1, BLEU@4 of 5.3, and METEOR of 17.1, significantly higher than the original model variant LLaVA-NeXT, indicating the benefit of added region features for additional visual cues. Models trained on synthetic captions (LLaVA-NeXT-I, LLaVA-NeXT-Q, and LLaVA-NeXT-G) generally show lower scores, showing advantage of our human-annotated caption. PGC: Stronger Detail Reasoning Performance. We implement our proposed PanCaper based on LISA which uses pre-trained LLaVA-NeXT with LLM of Llama3-8B, with LoRA [19] adopted. The vision encoder uses fixed CLIPViT-L/14-336 model, modified with linearly interpolated position embeddings to process 448 resolution images. The trainable components of our model include the mask decoder of kMaX-DeepLab, and the tunable parts in LLaVA same as in LISA. To enhance model performance in visual understanding, we initialize our PanCaper using pretrained LLaVA-NeXT models from the detailed captioning task. We also experiment with model variant that uses mask pooled features similar to LLaVA-NeXT-pool, and name it PanCaper-Pro. For comparison, we select 3 related methods LISA, PixelLM [51] and GLaMM [50] for evaluation. It is noteworthy that LISA is not able to perform multi-mask prediction. We therefore adapt LISA [28] for the multi-mask generation with grounded segmentation, namely LISA+. The implementation details can be found in the supplementary. Tab. 4 shows the quantitative results. Our proposed PanCaperPro achieves the highest scores across all captioning metrics (CIDEr: 12.5, CAPTURE: 64.3, BLEU@4: 6.4, METEOR: 17.9), outperforming all other models. Both PanCaper models show significant improvements over other models in all captioning metrics, highlighting the effectiveness of the COCONut-PanCap dataset for detailed caption generation. On grounding segmentation, PanCaper-Pro again leads, with PQ score of 0.61, PQthing of 0.58, and PQstuff of 0.68, reflecting its robustness on both thing and stuff classes. Notably, enabling mask pooling in our proposed PanCaper-Pro further enhances segmentation metrics. The baseline models (LISA+ and GLaMM with GranD) achieve much lower PQ scores, due to incomplete segmentation anTraining recipe Method finetune Pretrain Dataset Instruction-tuning dataset LAION-CC-SBU LLaVA 665K LLaVA-NeXT* LLaVA-NeXT LAION-CC-SBU LLaVA 665K-COCONut-PanCap LLaVA-NeXT-pool LAION-CC-SBU LLaVA 665K-COCONut-PanCap LLaVA-NeXT-I LAION-CC-SBU LLaVA 665K-InternVL2-Cap LLaVA-NeXT-Q LAION-CC-SBU LLaVA 665K-Qwen2VL-Cap LLaVA-NeXT-G LAION-CC-SBU LLaVA 665K-GPT4V-Cap Mask pooled CAPTURE CIDEr BLEU@4 METEOR ROUGE-L 55.4 58.7 61.4 53.9 55.4 56.2 10.8 11.2 13.1 9.4 8.9 9.6 4.2 4.8 5.3 4.4 4.6 4.7 13.2 16.2 17.1 11.5 12.9 13. 23.1 24.6 26.8 21.4 22.5 22.8 Table 3. Caption Benchmark Results Evaluated on Our COCONut-PanCap Val Set. Note that the amount of data in the instruction dataset remains the same; only the sources of the detailed captions vary, with total of 23K images that have detailed captions. Caption Grounding segmentation Method LISA+ * LISA+ GLaMM GCG * GLaMM GCG PanCaper (ours) PanCaper-Pro (ours) Pretrain dataset LAION-CC-SBU LAION-CC-SBU LAION-CC-SBU+GranD LAION-CC-SBU+GranD COCONut-PanCap (ours) COCONut-PanCap (ours) COCONut-PanCap (ours) Instruction dataset Mask pooled CAPTURE CIDEr BLEU@4 METEOR PQ PQthing 0.43 0.41 0.50 0.49 0.27 0.35 0.55 0.54 0.56 0.55 0.61 0.58 GranDf COCONut-PanCap (ours) GranDf LAION-CC-SBU LAION-CC-SBU 6.6 8.1 6.5 7.8 12.0 12.5 46.2 57.9 43.2 56.8 62.6 64.3 9.8 13.8 10.6 14.3 15.4 17. 3.8 4.9 3.6 5.2 5.8 6.4 PQstuff 0.45 0.44 0.21 0.46 0.66 0.68 Table 4. Joint Panoptic Segmentation and Grounded Captioning (PGC) on COCONut-PanCap Val Set. * denotes reproduced results. Training dataset SD3 PT dataset [12] COCO-caption [6] DOCCI [44] COCONut-PanCap (ours) SD3 PT dataset [12] COCO-caption [6] DOCCI [44] COCONut-PanCap (ours) Evaluation dataset FID FDdinov2 CLIPScore DOCCI test set [44] COCONut-PanCap val set (ours) 30.2 27.6 22.1 21.4 31.8 28.0 24.3 23.1 345 321 300 290 300 294 267 260 74.9 76.8 77.8 77.9 73.8 74.0 75.1 77.3 Table 5. Benchmark Results on Text Conditioned Image Generation. Stable-Diffusion-3 (SD3) medium is finetuned with COCO-Caption (short), DOCCI and our COCONut-Panoptic and evaluated on DOCCI test set [44] and our COCONut-PanCap val set. SD3 PT dataset denotes the pretraining dataset of SD3, and thus the rows correspond to zero-shot evaluation of SD3. w/o FT COCO-caption [6] DOCCI [44] COCONut-PanCap color attribution 0.37 0.73 colors 0.33 position 0.65 counting 0.96 single object 0.80 two objects 0.64 overall score 0.34 0.70 0.30 0.64 0.94 0.78 0.62 0.38 0.74 0.36 0.65 0.95 0.81 0.65 0.40 0.75 0.36 0.70 0.96 0.89 0.68 Table 6. Effects of Fine-tuning the SD3-medium (T2I model) with Different Datasets on GenEval [16]. w/o FT denotes the model is not finetuned with any datasets (i.e., zero-shot testing). notations in the GranD dataset. Text-to-Image Generation. We adopt the Stable Diffusion 3 (SD3) medium model1 for text to image generation with LoRA finetuning. We adopt the default training settings but only with different text-image datasets for training. We evaluate with two types of training images from COCO [35] and DOCCI [44] datasets. In details, for the COCO images, 1https://huggingface.co/docs/diffusers/stable diffusion/stable diffusion 3 we explore the short COCO-caption and detailed captions from our dataset. For DOCCI images, we directly use the captions from their dataset. Tab. 5 shows the quantitative results. Traning on COCONut-PanCap achieves the best performance across all metrics when evaluated on DOCCItest, with the lowest FID (21.4), lowest FDdinov2 (290), and the highest CLIPScore (77.9), indicating superior generation quality and high image-text relevance. When evaluated on COCONut-PanCap-val set, training on COCONutPanCap again shows the best results with the lowest FID (23.1), FDdinov2 (267), and high CLIPScore of 77.3. Tab. 6 shows the results on GenEval benchmark [16]. Finetuning SD3-medium with COCONut-PanCap consistently scores the highest in most categories, particularly those requiring image details like color attribution, object positioning, and handling multiple objects. Our proposed dataset enables more accurate image generation that requires understanding of relationships, multiple objects and counting, tasks that other datasets struggle with. VQA. To evaluate the effectiveness of the proposed COCONut-PanCap dataset, we utilize these captions during the instruction-tuning stage and follow the setup of LLaVANeXT [38] across various visual question answering (VQA) and multi-modality understanding benchmarks. We evaluate on MM-Vet [68], SEED-IMG [29], MMBench-en [39], MME [13], POPE [33], and TextVQA [58], covering broad range of evaluation dimensions. We experiment with different amount of our COCONut-PanCap caption data injected into the instruction tuning stage by replacing the original COCO captioning data with our dataset. As shown in Tab. 7, the baseline model LLaVA-NeXT (using its original recaptioned COCO) achieves relatively lower performance across all metrics, with scores such as 43.5 on MMVet, 70.1 on Seed-IMG, and 68.9 on TextVQA. Building LLM Instruction-tuning Dataset Llama3-8B orginal LLaVA 665K [38] Method 43.5 LLaVA-NeXT * 44.1 LLaVA-NeXT-20K Llama3-8B LLaVA 665K-COCONut-PanCap-20K LLaVA-NeXT-50K Llama3-8B LLaVA 665K-COCONut-PanCap-50K 44.6 LLaVA-NeXT-Full Llama3-8B LLaVA 665K-COCONut-PanCap-118K 45.5 37.8 LLaVA-1.5 38.5 LLaVA-1.5 Vicuna-7B LLaVA 665K-ShareGPT4V-100K Vicuna-7B LLaVA 665K-COCONut-PanCap-20K MM-Vet Seed-IMG MMBench-en TextVQA POPE MME 1523 1552 1600 1612 1519 1521 71.4 73.6 74.2 75.1 70.5 70.9 68.9 69.8 70.0 70.7 64.6 64.5 70.1 72.5 73.1 74.3 67.4 67. 85.4 86.1 87.1 87.9 84.7 84.9 Table 7. Benchmark Results and Ablation Study on VQA. By adding extra detailed caption data for instruction tuning, the models show increased improvement. * denotes reproduced results. Using only 20K human labeled data can still achieve comparable performance to 100K synthetic data. Method GLaMM* [50] PixelLM [51] LISA-7B [28] PanCaper+ PanCaper+ + COCONut-PanCap refCOCO refCOCO+ val 77.5 73.0 74.1 74.5 76.2 testA testB 74.9 79.2 68.2 76.5 71.1 76.5 69.9 76.7 72.3 77.1 val 71.3 66.3 62.4 69.9 70.5 testA testB 61.5 74.7 58.3 71.7 56.5 67.4 59.5 73.4 60.1 73.9 refCOCOg test val 71.9 71.3 70.5 69.3 68.5 66.4 70.6 69.8 71.6 72.1 Table 8. Benchmark Results on Referring Segmentation. * denotes reproduced results. It is noted that GLaMM uses extra data from the GranD dataset for pretraining. + denotes our PanCaper model is adapted for referring segmentation task. on LLaVA-NeXT baseline, we progressively incorporated varying amounts of COCONut-PanCap data (20K, 50K, and 118K (full), as indicated by postfixes in the baseline names) during instruction-tuning. Consistent improvements are observed across all evaluated benchmarks as more of our data is integrated. Referring Segmentation. In this task, the model processes an image and textual referring expression to output segmentation mask corresponding to the expression. The prompt used is, Please segment the referring text in the image. The target model response is Sure, it is SEG., where the SEG token is decoded to obtain the mask. We follow the setup in LISA [28], using multiple segmentation datasets to jointly train the models. Tab. 8 shows the quantitative results. Our model achieves superior performance, particularly when additionally trained with the COCONutPanCap dataset (last row), outperforming all models except GLaMM [50]. This improvement underscores our models efficacy in handling complex referring expressions, likely due to the additional data that enhances model generalization and accuracy. It is worth noting that GLaMM performs competitively with our method, though the comparison is uneven given their additional use of the SA-1B dataset [25]. Synthetic vs. Human Annotated Data. Generating synthetic data for captioning has been popular for recent tasks in either training vision encoders [48] or text-to-image generation [31]. We investigate the effect of varying the mix ratio of synthetic captions generated by GPT-4V and our human-annotated data for fine-tuning (where 0 indicates fully synthetic data), using the COCONut-PanCap dataset for training and the COCONut-PanCap-val set for evaluation. We adopt LLaVA-NeXT for the captioning task and SD3-medium for the image generation task. As shown in Fig. 6, adding 25% human-annotated data yields significant performance improvements in both captioning and generation, with reduced FID of 26 from 31 (lower is better) and an increased CAPTURE score of 53.6 from 47.5 (higher is better). Consistent improvements are observed as more human-annotated data is incorporated. Figure 6. Varying Synthetic and Human-Annotation Ratios. CAPTURE is used to evaluate the performance of LLaVA-NeXT on detailed captioning, while FID assesses the performance of SD3-medium on text-conditioned image generation. 6. Conclusion and Discussion In this work, we proposed novel dataset designed to support detailed captioning and grounded segmentation tasks built on COCO images. We demonstrated that our dataset can enhance model performance during instruction tuning and fine-tuning stages across various multi-modal understanding and generation tasks, such as captioning, VQA, grounded segmentation, and text-to-image generation. We hope that COCONut-PanCap, with its detailed captions grounded with dense panoptic masks, will foster future advancements in multi-modal learning research. Limitations. High-quality human-labeled data offers significant benefits for instruction tuning in multi-modal tasks, but scaling such datasets is challenging. To address this, we introduce COCONut-PanCap as starting point for large-scale human-annotated data exploration. Recognizing the relatively smaller dataset size compared to other large dataset, future work may involve using this dataset to train seed models to generate more high-quality synthetic data."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5, 6 [2] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL Workshop, 2005. 6 [3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. CocoIn CVPR, 2018. stuff: Thing and stuff classes in context. 13 [4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text preIn CVPR, training to recognize long-tail visual concepts. 2021. 3 [5] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 2, [6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 2, 3, 7 [7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 5, 6 [8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 3 [9] Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, and Liang-Chieh Chen. Coconut: Modernizing coco segmentation. In CVPR, 2024. 2, 3, 4 [10] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption. arXiv preprint arXiv:2405.19092, 2024. 6 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 7 [13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 7 [14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. NeurIPS, 2024. 2 [15] Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bunner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. Imageinwords: Unlocking hyper-detailed image descriptions. arXiv preprint arXiv:2405.02793, 2024. 2, 3 [16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. In NeurIPS, 2023. 3, 7 [17] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In CVPR, 2019. [18] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehenarXiv preprint sive understanding of multi-shot videos. arXiv:2311.17043, 2023. 3 [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6, 12 [20] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv: 2307.06350, 2023. 3 [21] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 13 [22] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 13 [23] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollar. Panoptic segmentation. In CVPR, 2019. [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 3 [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 8 [26] Mikhail Koroteev. Bert: review of applications in natural language processing and understanding. arXiv preprint arXiv:2103.11943, 2021. 6 [27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 13 [28] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. 5, 6, 8, 12, 13 [29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. 3 [31] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. 2, 8 [32] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024. 2, 3 [33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 7 [34] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In ACL Workshop, 2004. [35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 3, 4, 7 [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Improved baselines with visual instruction tuning. Lee. arXiv:2310.03744, 2023. 2, 5, 13 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 3, 5, 12, 13 [38] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 3, 6, 7, 8, 13 [39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023. 7 [40] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. 3, [41] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. 13 [42] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In CVPR, 2019. 13 [43] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 13 [44] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. DOCCI: Descriptions of Connected and Contrasting Images. In ECCV, 2024. 2, 3, 7 [45] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. In NeurIPS, 2011. 3 [46] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. [47] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV, 2015. 3 [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 8 [49] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In CVPR, 2023. 13 [50] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. 3, 6, 8, 13 [51] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In CVPR, 2024. 3, 6, 8 [52] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 2 [53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. 2, 3 [54] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In ECCV, 2022. 13 understanding with visual instruction tuning. In CVPR, 2024. 3 [70] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. 3 [71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ade20k dataset. In CVPR, 2017. 13 [72] Li Zhou, Xu Yuan, Zenghui Sun, Zikun Zhou, and Jingsong Lan. Instruction-guided multi-granularity segmentation and captioning with large multimodal model. arXiv preprint arXiv:2409.13407, 2024. [55] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In ICCV, 2019. 4 [56] ShareGPT. ShareGPT. https://sharegpt.com/. 13 [57] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In ECCV, 2020. 13 [58] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 7 [59] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions. arXiv preprint arXiv:2406.10328, 2024. 2 [60] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2 [61] Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano. picture is worth more than 77 text tokens: Evaluating clipstyle models on dense captions. In CVPR, 2024. [62] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 6 [63] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In CVPR, 2021. 6 [64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5, 6 [65] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In ECCV, 2022. 3 [66] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 4 [67] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022. 5, 6, 12, 13 [68] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In ICML, 2024. [69] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel The appendix is organized as follows. In Sec. A, we show implementation details for Detailed Captioning (Sec. A.1), Panoptic segmentation and Grounded (Sec. A.2), and VQA (Sec. A.3). In Sec. B, we show more visualization examples of our proposed COCONut-PanCap dataset (Sec. B.1), and analysis of the tier cases in our dataset annotation user study (Sec. B.2). A. Experimental Details (a) LLaVA-NeXt-AnyRes In this section, we provide more experimental details for detailed captioning (Sec. A.1), PGC (Sec. A.2), and VQA (Sec. A.3). A.1. Detailed Captioning Detailed Captioning Instruction Dataset Construction. The key step in conducting the experiment is constructing the dataset. The original LLaVA-665K dataset consists of LLaVA-158K combined with other VQA datasets. Within LLaVA-158K, subset of detailed captions corresponds to 23K COCO images. To create our-LLaVA-665K (referred to as LLaVA 665K-COCONut-PanCap in the table), we replace the detailed caption annotations for these 23K COCO images with our annotations. Importantly, the total amount of training data remains unchanged (only the captions for these 23K images are updated), ensuring fair comparison of the impact of data quality on model performance. Synthetic Annotation for Detailed Caption. To build the synthetic dataset with state-of-the-art VLM, we use three models, including open-sourced InterVL-2, Qwen2-VL and close-sourced GPT-4V to generate the detailed captions for COCO 118K train set images. We use the same text prompts that is used in LLaVA [37] for prompting the model to create the detailed captions. LLaVA-NeXT-pool implementation details. Fig. 7 shows the comparison of the original LLaVA-NeXT and our proposed LLaVA-NeXT-pool. As shown in Fig. 7a, in order to preserve the details for the high-resolution images and representations, the original design employs grid configuration which can also balance the performance efficiency with operational costs. Then both the patch-level and imagelevel features are later concatenated and sent to the LLM. Directly splitting the image into patches could cause prolems, for example, in the figure, the upper part of the dogs head is partitioned into different patches which may result in incomplete feature extraction for single object. To overcome this drawback, we propose LLaVA-NeXT-pool to extract the dense feature and preserve the object details by utilizing the panoptic segmentation masks in our COCONutPanCap dataset. Fig. 7b shows the details. Compared to the original design, LLaVA-NeXT-pool could effectively extract the features for the dog in our example. Our design (b) our LLaVA-NeXt-pool Figure 7. Comparison of LLaVA-NeXt and our proposed LLaVA-NeXt-pool. enables more complete region-level feature extraction and is potential in understanding the details better. A.2. PGC We provide more implementation details for the proposed task: Panoptic segmentation and Grounded Captioning (PGC). PanCaper Implementation Details. We introduce the PanCaper architecture details in this section. Following the architecture in LISA [28], there are three components including the vision backbone, mask decoder and multimodal LLM. Fig. 8 shows the architecture details for PanCaper. We made modification on the vision backbone, and mask decoder part in terms of model architecture. To preserve the learned knowledge of the pre-trained multimodal LLM (i.e., LLaVA-NeXT in our experiments), we leverage LoRA [19] to perform efficient fine-tuning, and completely freeze the vision backbone. The mask decoder is fully finetuned. Additionally, the LLM token embeddings (embed tokens), the LLM head (lm head), and the projection layer are also trainable. The weights of the text generation loss 位text and the mask loss 位mask are set to 1.0 and 1.0, respectively. For the PQ-style mask loss, we follow the same settings in kMaX-DeepLab [67], where it consists of mask-level cross entropy loss, dice loss and pixel loss. Adapting Baseline Methods for PGC Task. We adopt the same text prompt template to enable the model to perform PGC tasks. For LISA+, we follow the same design Figure 8. Architecture of PanCaper. We utilize pretrained vision encoder from kMaX-DeepLab [67] as our vision backbone, which effectively extracts dense features essential for panoptic segmentation. in GLaMM [50] to design the multi entity mask output by utilizing the the GranDf dataset. As the intruction dataset of GranDf is constructed similarly grounding the phrase in the image-level caption, it will output multiple SEG tokens. The reasoning results of the number of SEG tokens decide the number of output entity mask which are often binary masks. As result, the model can generate detailed caption along with interleaved segmentation masks, employing the format pA man/pSEG ... next to pa tree/pSEG. And thus the format of instruction dataset is significat in task design. Therefore, we formulate our dataset as pA man/pSEGt ... next to pa tree/pSEGs, where SEGt represents the seg token for instance masks of thing and SEGs represents for semantic masks of stuff respectively in panoptic setting. Similarly, utilizing the PanCap dataset and special token design, GLaMM [50] is able to generate the entity masks with the tag of thing and stuff. Training Data Formulation. We adopt the same training data from LISA [28] which comprises mainly three parts, all of which are derived from widely-used public datasets. These include 1) Semantic Segmentation datasets including ADE20K [71], COCO-Stuff [3], and LVIS-PACO [49] part datasets with the generated QA data, 2) Vanilla Referring Segmentation Datasets: refCOCO, refCOCO+, refCLEF [22] and refCOCOg [40] datasets, 3) ReasonSeg dataset [28], and 4) Visual Question Answering Dataset: LLaVA-v1.5-mix665k [36]. To enable the multi-mask generation for grounded caption, there are two options for instruction datasets, GranDf and our COCONut-PanCap where GranDf consists of entity masks while COCONutPanCap consists of panoptic masks. A.3. VQA We provide more implementation details for the VQA experiments. We follow the same setting in LLaVANeXT to create the experimental results for VQA tasks. We focus on the instruction tuning stage by adopting the pretrained weights from the stage-1 across the trainings for all the model variants mentioned in Tab. 7 in the paper. The dataset we used is exactly the same as in LLaVA 665K [36] which includes the earlier version of instruction data proposed in LLaVA 158K [37], ShareGPT [56], VQAv2 [41], GQA [21], openknowledge VQA (OKVQA [42], A-OKVQA [54]), OCR (OCRVQA [43], TextCaps [57]), region-level VQA datasets (Visual Genome [27], RefCOCO [22]). Among these data, LLaVA 158K comprises 77K complex reasoning, 58K conversation and 23K detailed captions. To build the dataset variants shown in Tab. 7, we simply remove the subset of detailed caption 23k, and subsequently add 20K, 50K and 118K COCONut-PanCap dataset to build LLaVA 665K-COCONut-PanCap-20K, LLaVA 665K-COCONutPanCap-50K and LLaVA 665K-COCONut-PanCap-118K. By these steps, we add more detailed caption data to construct the instruction tuning dataset. This results in the total amount of training data of 662K for LLaVA 665K-COCONut-PanCap-20K, 692K for LLaVA 665KCOCONut-PanCap-50K and 760K for LLaVA 665KCOCONut-PanCap-118K. And thus the size of LLaVA 665K-COCONut-PanCap-20K is slightly smaller than the original LLaVA 665K dataset, but the model trained on it yields better performance. For the evaluation settings, we follow the exact settings in LLaVA-NeXT [38] using lmms eval2. B. More Qualitative Results In this section, we present additional qualitative results of COCONut-PanCap annotations (Sec. B.1) and detailed analysis of tier cases from the user study (Sec. B.2). 2https://github.com/EvolvingLMMs-Lab/lmms-eval B.1. Data Examples We show more visualization of our proposed COCONutPanCap dataset in Fig. 9 and Fig. 10. B.2. PanCaper and GPT-4V Tier Showcases In the user study involving 1,000 samples, captions generated by GPT-4V were preferred in 87 cases. Among these, actually, 46 were tier cases where human raters considered both GPT-4V and COCONut-PanCap captions equally good. Fig. 11, Fig. 12 and Fig. 13 illustrate qualitative examples, highlighting the reasons for the tier classification and instances where GPT-4V was chosen. Figure 9. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks. Figure 10. Visualization of the Panoptic Grounded Caption. Our annotated captions ground the panoptic segmentation masks. Figure 11. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases. Figure 12. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases. Figure 13. Tier Examples for the User Study. Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases."
        }
    ],
    "affiliations": [
        "bytedance.com"
    ]
}