{
    "paper_title": "3D Arena: An Open Platform for Generative 3D Evaluation",
    "authors": [
        "Dylan Ebert"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons. Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource. Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving a 16.6 ELO advantage over meshes and textured models receiving a 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platform's community engagement establishes 3D Arena as a benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 7 8 7 8 1 . 6 0 5 2 : r 3D Arena: An Open Platform for Generative 3D Evaluation Dylan Ebert Hugging Face dylan@huggingface.co"
        },
        {
            "title": "Abstract",
            "content": "Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons. Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource. Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving 16.6 ELO advantage over meshes and textured models receiving 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platforms community engagement establishes 3D Arena as benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D."
        },
        {
            "title": "Introduction",
            "content": "The field of Generative 3D has experienced rapid growth in recent years, with state-of-the-art models emerging every few months [22]. These systems generate 3D assetsdigital representations including meshes, point clouds, and volumetric datathat serve critical roles in downstream applications ranging from video game development and film production to architectural visualization and virtual reality experiences [2]. While technical capabilities have advanced rapidly, standardized evaluation methods have failed to keep pace [36]. Current benchmarks rely predominantly on automated metrics that poorly capture real-world human preferences [12, 30], creating an evaluation gap that hinders progress in the field. To address this challenge, we introduce 3D Arena 1, an open platform for evaluating image-to3D generation models through human preferences. Inspired by the success of Chatbot Arena [8], our platform employs pairwise comparisons via an intuitive web interface where users compare anonymized 3D outputs side-by-side, as shown in Figure 1. This approach enables direct assessment of subjective quality factors, including visual appeal, structural coherence, and perceived utility, which automated metrics fail to capture. 1https://huggingface.co/spaces/dylanebert/3d-arena Preprint. Under review. Figure 1: The 3D Arena voting interface, showing side-by-side comparison of anonymized 3D outputs. Users can rotate, zoom, and examine 3D models before submitting their preference. Since launching in June 2024, 3D Arena has established itself as the largest human preference benchmark for Generative 3D. The platforms impact extends beyond evaluation metrics, with research groups acknowledging the influence of 3D Arena rankings on their development priorities and research directions, demonstrating its practical value for guiding progress in the field. This paper presents an analysis of 3D Arenas methods, data, and the insights we have derived from this human preference data. Our key contributions include: 1. Platform validation: Documentation of the methods used to create, operate, and maintain data integrity for crowdsourced 3D evaluation. 2. Evaluation insights: Analysis of human preference data revealing patterns in how humans perceive and evaluate 3D content quality. 3. Methods framework: Evidence-based recommendations for improving evaluation methods in Generative 3D, informed by observed preference patterns and cognitive mechanisms. Through analysis of this established benchmark, we provide insights for the Generative 3D community while advancing understanding of human-centered evaluation in this evolving domain. To ensure reproducibility and enable further research, we provide open access to both the platform at https://huggingface.co/spaces/dylanebert/3d-arena and the iso3d evaluation dataset at https://huggingface.co/datasets/dylanebert/iso3d."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 The Generative 3D Evaluation Challenge The field of Generative 3D has advanced rapidly in recent years, with state-of-the-art models emerging at an accelerating rate [36]. Recent advances range from Shap-E [19] in 2023, to major leaps in quality like InstantMesh [33] in early 2024, to systems like TRELLIS [32], Hunyuan3D-2 [37], and Hi3DGen [34] released in late 2024 and early 2025. Concurrently, proprietary systems from organizations such as CSM [9], Rodin [16], Tripo [29], and Meshy [24] have demonstrated highquality Generative 3D results. However, evaluation methods have failed to keep pace with this rapid advancement, creating significant assessment bottleneck. Current approaches predominantly rely on traditional image-based metrics such as PSNR [12] and SSIM [30], which evaluate only the final rendered output while ignoring the underlying 3D structure. While T3Bench [13] represents an important advancement by incorporating multi-view image consistency evaluation, these image-centric approaches fail to capture the underlying 3D structure. Attempts to address 3D structure have employed geometric metrics such as Chamfer Distance [1], which measure shape accuracy through point-to-point distances. However, these geometric measures fail to capture crucial aspects of 3D quality including perceptual appeal, aesthetic coherence, and mesh topology, which are factors that determine real-world utility and professional applicability. The importance of mesh topology, in particular, has been largely overlooked by the Generative 3D research community despite its critical role in professional workflows [2]. Industry practitioners consistently emphasize that clean topology with proper edge flow is essential for animation, deformation, and production pipelines [26]. Recent work such as MeshAnything [6] addresses this gap by prioritizing mesh topology over visual fidelity. Their evaluation approach involves user studies where participants vote on topology quality. However, these user studies are limited by scale and cost, requiring significant effort for small number of models and participants. 2.2 Arena-Style Evaluation: Proven Success in Other Domains Arena-style evaluation has emerged as powerful paradigm for large-scale model assessment, addressing the limitations of traditional benchmarking approaches through crowdsourced pairwise comparisons. Chatbot Arena [8] and GenAI Arena [17] have demonstrated the effectiveness of this method, providing complementary insights to expert-designed benchmarks by capturing subjective preference aspects that automated metrics cannot assess. Similar arena-style approaches have been applied across domains including image generation [17], video generation, and text-to-image evaluation, establishing the broader applicability of community-driven preference data. These platforms leverage well-established ranking algorithms including ELO rating systems [10] and the Bradley-Terry model [5], which infer relative model performance from sparse pairwise comparisons. The core method presents users with side-by-side anonymous outputs, enabling black-box preference judgments that aggregate into statistical rankings. The arena approach offers advantages for 3D evaluation: it captures subjective quality aspects that automated metrics cannot measure, scales to large participant populations for statistical reliability, and provides continuous assessment as new models emerge. These characteristics make arena-style evaluation well-suited for Generative 3D, where quality encompasses multiple dimensions including visual appeal, structural coherence, and downstream utility. Recent concurrent work has applied arena-style evaluation to Generative 3D through 3DGenBench [35]. While 3DGen-Bench claims to introduce \"the first 3D Arena,\" our 3D Arena platform launched earlier in June 2024 and has since accumulated significantly more preference data with 123,000 votes compared to 3DGen-Benchs 13,800 votes. The platforms serve complementary roles: 3D Arena functions as an open, continuously-updated leaderboard with public voting focused specifically on image-to-3D generation, while 3DGen-Bench encompasses both text-to-3D and image-to-3D evaluation through closed expert annotation with an emphasis on an automated scoring model."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Platform Design Principles 3D Arenas effectiveness stems from four design principles that address limitations in existing 3D evaluation approaches: Anonymous Pairwise Comparison: The platform presents users with side-by-side anonymous 3D outputs, eliminating model identity bias while enabling direct quality assessment. Users can rotate, zoom, and examine 3D assets before submitting preferences, ensuring informed decisionmaking. Model identities remain hidden until after vote submission, preventing reputation effects from influencing evaluation. Multi-Format Support: The interface supports both traditional mesh formats (.obj, .glb) and emerging representations like Gaussian splats (.ply, .splat), enabling fair comparison across different Generative 3D approaches. This design acknowledges the diversity of current Generative 3D techniques while maintaining evaluation consistency. Natural Preference Capture: Rather than directing users toward specific metrics, the platform intentionally minimizes evaluation guidance, allowing natural preference patterns to emerge. This approach reveals how users evaluate 3D content when given freedom to determine their own criteria, providing insights into real-world quality perception. Accessible Participation: The platform balances scientific rigor with broad accessibility, using Hugging Face OAuth for authentication while maintaining low barriers to participation. This design enables large-scale data collection while preserving data quality through statistical quality control. 3.2 Evaluation Infrastructure 3.2.1 The iso3d Dataset To ensure fair model comparison, we present iso3d, curated dataset of 100 isolated object images designed for standardized 3D evaluation. The dataset was constructed from the Karlo-v1 prompt dataset [21], which contains 1,630 text prompts originally used for text-to-image model evaluation. The generation process extended each Karlo-v1 prompt with standardizing suffixes (isolated object render, white background) to ensure clean, isolated objects. Images were generated using DreamShaper-XL2 with White Background LoRA3, followed by automated background removal. From 1,630 candidates, 100 images were selected through manual review based on visual clarity and object isolation. Physical plausibility was deliberately not controlled, including challenging inputs that test how image-to-3D models handle imperfect or physically ambiguous cases. This design choice represents real-world usage scenarios where users input diverse images of varying quality, often with AIgenerated images as intermediate steps in text-to-3D pipelines. 3.2.2 User Interface The platform provides side-by-side 3D viewers as shown in Figure 1. The standard rendered view enables evaluation of overall visual appeal and surface quality, while the wireframe view (Figure 2) reveals underlying structural characteristics. Users can toggle between these two viewing modes to assess different aspects of 3D asset quality. Additionally, polygon count is displayed for each asset. The standard view uses default lighting and camera settings from the Gradio Model3D4 component: Babylon.js5 for meshes (.obj, .glb) and gsplat.js6 for Gaussian splats (.ply, .splat). 2https://huggingface.co/Lykon/dreamshaper-xl-v2-turbo 3https://civitai.com/models/119388/white-background 4https://www.gradio.app/docs/gradio/model3d 5https://babylonjs.com/ 6https://github.com/huggingface/gsplat.js 4 Figure 2: Wireframe view of two models side-by-side showing the underlying topological differences between mesh representation (left) and Gaussian splat (right). This view reveals structural elements typically hidden in the standard view. 3.3 Quality Assurance and Statistical Methods 3D Arena employs quality control mechanisms to ensure data integrity while preserving platform accessibility. User Authentication: Users authenticate via Hugging Face OAuth to vote, associating each vote with verified user ID. This approach mitigates automated voting and large-scale manipulation. Fraud Detection: Our statistical fraud detection system employs binomial tests comparing individual voting patterns against community consensus, flagging users whose behavior deviates significantly from expected patterns (p < 0.00001). This approach maintains 99.75% user authenticity, with 31 accounts flagged among 8,096 total users. ELO Rating System: Model rankings utilize standard ELO rating systems, providing robust statistical estimates of relative model performance. While platforms like Chatbot Arena employ Bradley-Terry models for enhanced statistical validity, our analysis demonstrates that ELO ratings maintain statistical robustness at our data scale, with consistent ranking patterns across evaluation periods. All models begin with identical initial ratings (1200 points), with updates following established algorithms adapted for pairwise 3D model comparison. 5 3.4 Submission and Integration Process 3D Arena operates as an open platform designed to serve the broader research community while maintaining evaluation standards. Model developers submit systems for evaluation through Hugging Face dataset pull requests7, creating transparent, community-reviewed submission process. Submission Standards: Current submissions require working model implementations with consistent output format specifications. While anonymous submissions were initially permitted and remain included in internal rankings 1, new submissions must provide model URLs and documentation to ensure reproducibility. Anonymous models are excluded from the public leaderboard to maintain transparency. Community-Driven Evolution: The platform evolves based on community needs and research requirements. Submission requirements, evaluation protocols, and interface features reflect ongoing dialogue with model developers and researchers, ensuring the platform serves as practical research tool rather than static benchmark. Research Workflow Integration: The open submission process enables rapid assessment of emerging techniques, allowing researchers to evaluate new approaches against established baselines upon development. This integration has made 3D Arena standard evaluation step for many research groups, establishing its role as key infrastructure for the field."
        },
        {
            "title": "4 Results and Findings",
            "content": "4.1 Platform Adoption and Dataset Scale Since launching in June 2024, 3D Arena has demonstrated widespread community adoption with 123,243 total votes from 8,096 users across 19 state-of-the-art models, representing the largest scale human preference evaluation for Generative 3D. The platform exhibits healthy participation patterns with median of 8 votes per user and mean of 15.2 votes per user. Vote distribution shows 61.6% of users contributing 1-10 votes, 34.7% contributing 11-50 votes, and 3.7% contributing more than 50 votes. This distribution indicates broad community engagement rather than concentration among few \"super voters,\" supporting the statistical validity of preference patterns. The analyses presented in this paper are based on snapshot of voting data from May 30, 2025, while the arena itself continues to function as living, continuously updating benchmark with new votes and model submissions. 4.2 Model Rankings Table 1 presents the leaderboard results. The leaderboard reveals several interesting patterns in how users evaluate 3D content. 4.2.1 Mesh vs. Splat Preferences 3D Arena data establishes preference for Gaussian splat outputs over mesh outputs. Splats achieve 16.6 ELO advantage (1215.1 vs. 1198.5), with splat formats achieving 51.9% win rate compared to 49.7% for mesh formats when weighted by vote volume. This preference pattern maintains statistical significance (p < 3.5 1030) throughout the evaluation period. The preference occurs despite significant technical trade-offs between formats. Gaussian splats utilize unlit rendering that appears bright and vibrant, while meshes rely on dynamic lighting models. Splats also require substantially higher computational resources and have limited compatibility with downstream applications including animation, editing, and integration with existing 3D workflows. controlled comparison using TRELLIS demonstrates this format effect directly: the splat version achieves 78 ELO advantage over its mesh counterpart despite sharing the same underlying model. 7https://huggingface.co/datasets/dylanebert/3d-arena 6 Table 1: 3D Arena Leaderboard (All Models)"
        },
        {
            "title": "1 CSM/Cube [9]\n2 TRELLIS-3DGS [32]\nStrawberrry1\n3\nStrawb3rry1\n4\n5 TRELLIS [32]\n6 Zaohaowu3D\n7 Hunyuan3D-2 [37]\n8\nInstantMesh [33]\n9 Meshy [24]\n10 Unique3D [31]\n11 Hi3DGen [34]\n12 MeshFormer [23]\n13\nSF3D [4]\n14 Real3D [18]\n15\n16 LGM [27]\n17 TripoSR [28]\n18\n19",
            "content": "IM-MA [33, 6]2 3DTopia-XL [7] SPAR3D [15]"
        },
        {
            "title": "ELO",
            "content": "1405 1384 1382 1370 1306 1302 1298 1278 1243 1230 1207 1192 1190 1158 1144 1100 1089"
        },
        {
            "title": "Format",
            "content": "3027 3648 4892 5121 4877 582 4195 10575 7023 8959 1565 5394 6267 8541 3783 10344 10296 7519 5425 83.3% 80.1% 80.9% 79.4% 67.0% 64.8% 65.5% 63.8% 58.1% 55.1% 47.7% 48.6% 48.4% 42.5% 38.6% 32.7% 31.4% 19.2% 15.6%"
        },
        {
            "title": "Splat\nSplat\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nMesh\nSplat\nMesh\nMesh\nMesh",
            "content": "4.2.2 Textured vs. Untextured Models Textured models achieve 144.1 ELO advantage over untextured geometry (1241.1 vs. 1097.1), representing 24.5 percentage point win rate increase (56.9% vs. 32.4%, < 1.4 10104). This aggregate pattern exhibits notable heterogeneity, with several untextured models outperforming textured counterparts. Hi3DGen exemplifies this complexity, achieving higher ELO ratings than multiple textured models despite producing untextured meshes. Performance variance within categories is significant (textured: 89.3 ELO standard deviation; untextured: 67.8 ELO standard deviation), indicating that texture presence alone does not determine preference outcomes. With 74.8% of mesh outputs incorporating textures, the data reveals that users evaluate both immediate visual appeal and structural characteristics simultaneously. While texture provides advantages, geometric and topological factors maintain measurable influence on preference formation. Additional controlled studies would be required to disentangle the relative contributions of these factors. 4.2.3 Geometric Complexity Effects Analysis of 1,606 mesh files reveals significant variation in geometric complexity (mean: 172,571 polygons; median: 63,708). Polygon count patterns are confounded by fundamental method differences between models. The lowest polygon count category (<1K polygons, 1016 average ELO, 19.1% win rate) is dominated by IM-MA, hybrid system combining InstantMesh [33] generation with MeshAnything [6] retopology. IM-MA represents topology-aware approach prioritizing mesh structure over polygon density, contrasting with topology-unaware methods across other leaderboard models. While low polygon count is typically desirable for rendering performance and computational efficiency, IMMAs performance reflects different optimization criteria focused on mesh topology rather than visual fidelity. Among topology-unaware models, polygon count shows moderate positive correlation with preference (r = 0.147), with higher-density meshes (5K-20K polygons) achieving 58.8-60.9% win rates. The relationship exhibits diminishing returns beyond moderate complexity, indicating limited preference benefits from geometric detail within conventional generation approaches. 7 4.3 Stated vs. Revealed Preferences Our analysis reveals disconnect between stated preferences and observed voting behavior. Professional 3D workflows prioritize clean topology for animation compatibility, mesh formats for standard pipelines, and technical usability for downstream applications [2]. Anecdotal evidence from community discourse consistently indicates similar preferences among users, with informal feedback frequently emphasizing the importance of clean mesh topology and technical usability. However, voting patterns systematically favor visual impact through vibrant rendering and aesthetic appeal over downstream utility. This disconnect manifests clearly in the systematic advantages of splats over meshes (16.6 ELO points) and textured over untextured models (144.1 ELO points), despite widespread industry recognition that clean mesh topology is essential for professional workflows. This preference gap reflects established cognitive mechanisms [11]: the human visual system processes surface features (color, brightness) within 150-200 milliseconds [3], while geometric details require additional processing time [14]. Features requiring deliberate evaluation are systematically underweighted compared to immediately accessible visual characteristics [25]. This aligns with dual-process theory from cognitive psychology [11], where rapid pairwise comparisons default to intuitive (System 1) evaluation, prioritizing immediate visual impressions over technical considerations requiring analytical (System 2) thinking. The preference patterns provide evidence for the aesthetic-usability effect [20], where aesthetically pleasing designs are perceived as more usable regardless of actual functionality. The TRELLIS vs. TRELLIS-3DGS comparison exemplifies these mechanisms: identical underlying models achieve 78 ELO point advantage solely through rendering differences that enhance immediate visual appeal. Notable exceptions exist, however, with models like Hi3DGen achieving higher ratings than multiple textured alternatives despite producing untextured meshes."
        },
        {
            "title": "5 Discussion",
            "content": "Our preference analysis reveals gap between how 3D generation quality is perceived and what professional workflows require. These preference patterns show that immediate visual appeal drives user preferences despite the importance of technical characteristics like clean topology for downstream applications. This disconnect creates an optimization challenge for the field. Model developers must balance competing objectives: creating models that satisfy user preferences while maintaining technical quality for professional utility. The cognitive mechanisms underlying preference formationprioritizing immediate visual impact over structural qualityfavor surface features over technical characteristics that require deliberate evaluation. 3D Arena and similar platforms can address this gap in future work by implementing separate evaluation modes that isolate different quality aspects. For topology assessment, users could be presented with only the wireframe view 2 and polygon count information, rather than rendered surfaces. These results could be used to calculate separate topology ELO score. This would disentangle the two quality dimensions and provide more accurate assessment of model capabilities. To ensure reproducibility and enable further research, we provide: Platform Access: 3D Arena remains publicly accessible at https://huggingface.co/spaces/ dylanebert/3d-arena, enabling continued community participation and real-time leaderboard updates. Dataset: The iso3d evaluation dataset of 100 curated prompts is available through Hugging Face datasets at https://huggingface.co/datasets/dylanebert/iso3d, providing standardized evaluation protocols for future Generative 3D research."
        },
        {
            "title": "6 Conclusion",
            "content": "We present 3D Arena, an open platform for evaluating Generative 3D models through large-scale human preference collection using pairwise comparisons. Since launching in June 2024, the platform 8 has achieved significant scale and impact, establishing the largest human preference evaluation for Generative 3D. Through robust quality assurance and statistical fraud detection, we demonstrate that community-driven evaluation can achieve scientific reliability while preserving accessibility. The platform has become established infrastructure for the field, with research groups integrating 3D Arena evaluation into their development workflows and acknowledging its influence on research priorities. This practical adoption validates the platforms role as valuable benchmark that bridges the gap between automated metrics and real-world model assessment. Through analysis of this large-scale preference data, we contribute insights into human evaluation patterns for 3D content. Our findings reveal systematic preferences for visual presentation features over technical characteristics, reflecting cognitive mechanisms where immediate visual features are weighted more heavily than technical aspects requiring deliberate analysis. Importantly, these preferences capture real value for presentation-focused applications while highlighting the need for complementary evaluation approaches for technical workflows. We provide recommendations for improving evaluation methods in Generative 3D. Multi-criteria assessment can disentangle aesthetic appeal from technical utility, context-aware evaluation can target specific use cases, and expert vs. general user analysis can reveal how domain knowledge affects preference formation. These approaches can provide more nuanced model assessment while preserving the valuable insights from broad community preferences. By revealing patterns in human preference formation, our work advances understanding of quality perception in complex visual domains while providing practical guidance for model developers seeking to balance competing quality objectives. The platforms sustained community engagement and our findings create foundation for continued evolution of human-centered evaluation approaches in Generative 3D."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We thank the Generative 3D community for their sustained participation in 3D Arena. Special acknowledgment goes to all model developers who have submitted their systems for evaluation and the thousands of users whose votes make this research possible."
        },
        {
            "title": "References",
            "content": "[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In International conference on machine learning, pages 4049. PMLR, 2018. [2] Tomas Akenine-Moller, Eric Haines, and Naty Hoffman. Real-time rendering. AK Peters/crc Press, 2019. [3] Kaoru Amano, Naokazu Goda, Shinya Nishida, Yoshimichi Ejima, Tsunehiro Takeda, and Yoshio Ohtani. Estimation of the timing of human visual perception from magnetoencephalography. Journal of Neuroscience, 26(15):39813991, 2006. [4] Mark Boss, Zixuan Huang, Aaryaman Vasishta, and Varun Jampani. Sf3d: Stable fast 3d mesh reconstruction with uv-unwrapping and illumination disentanglement. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1624016250, 2025. [5] RALPH ALLAN BRADLEY and MILTON E. TERRY. Rank analysis of incomplete block designs: The method of paired comparisons. Biometrika, 39(3-4):324345, 12 1952. [6] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. [7] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling high-quality 3d 9 asset generation via primitive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2657626586, 2025. [8] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [9] Common Sense Machines. Common sense machines: 3d ai for creators and developers. https://www.csm.ai/, 2024. Accessed: 2024. [10] Arpad E. Elo. The Rating of Chess Players, Past and Present. Ishi Press International, Bronx, N.Y., 1978. [11] Jonathan St BT Evans. Dual-process theories of reasoning: Contemporary issues and developmental applications. Developmental review, 31(2-3):86102, 2011. [12] Fernando A. Fardo, Victor H. Conforto, Francisco C. de Oliveira, and Paulo S. Rodrigues. formal evaluation of psnr as quality measurement parameter for image segmentation algorithms, 2016. [13] Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, and Yong-Jin Liu. T3bench: Benchmarking current progress in text-to-3d generation, 2024. [14] Liqiang Huang. Visual features for perception, attention, and working memory: Toward three-factor framework. Cognition, 145:4352, 2015. [15] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James Rehg, and Varun Jampani. Spar3d: Stable point-aware reconstruction of 3d objects from single images. arXiv preprint arXiv:2501.04689, 2025. [16] Hyper3D. Hyper3d: Ai-powered 3d generation platform. https://hyper3d.ai/, 2024. Accessed: 2024. [17] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. Advances in Neural Information Processing Systems, 37:7988979908, 2024. [18] Hanwen Jiang, Qixing Huang, and Georgios Pavlakos. Real3d: Scaling up large reconstruction models with real-world images. arXiv preprint arXiv:2406.08479, 2024. [19] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. [20] Masaaki Kurosu and Kaori Kashimura. Apparent usability vs. inherent usability: experimental analysis on the determinants of the apparent usability. In Conference companion on Human factors in computing systems, pages 292293, 1995. [21] Donghoon Lee, Jiseob Kim, Jisu Choi, Jongmin Kim, Minwoo Byeon, Woonhyuk Baek, and Saehoon Kim. Karlo-v1.0.alpha on coyo-100m and cc15m. https://github.com/ kakaobrain/karlo, 2022. [22] Jian Liu, Xiaoshui Huang, Tianyu Huang, Lu Chen, Yuenan Hou, Shixiang Tang, Ziwei Liu, Wanli Ouyang, Wangmeng Zuo, Junjun Jiang, and Xianming Liu. comprehensive survey on 3d content generation, 2024. [23] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, et al. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. arXiv preprint arXiv:2408.10198, 2024. [24] Meshy LLC. Meshy: The ultimate ai 3d model generator for creators. https://www.meshy. ai/, 2024. Accessed: 2024. [25] Jacob Lund Orquin and Simone Mueller Loose. The role of visual attention in decision making. In 24th Subjective Probability, Utility and Decision-making Conference, 2013. 10 [26] Rick Parent. Computer animation: algorithms and techniques. Newnes, 2012. [27] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. [28] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. [29] Tripo AI. Tripo3d: Generate 3d model powered by ai. https://www.tripo3d.ai/, 2024. Accessed: 2024. [30] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. [31] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [32] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation, 2025. [33] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [34] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging, 2025. [35] Yuhan Zhang, Mengchen Zhang, Tong Wu, Tengfei Wang, Gordon Wetzstein, Dahua Lin, and Ziwei Liu. 3dgen-bench: Comprehensive benchmark suite for 3d generative models. arXiv preprint arXiv:2503.21745, 2025. [36] Ke Zhao and Andreas Larsen. Challenges and opportunities in 3d content generation, 2024. [37] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Shaoxiong Yang, Song Zhang, Yang Liu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Jingwei Huang, and Chunchao Guo. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025."
        }
    ],
    "affiliations": [
        "Hugging Face"
    ]
}