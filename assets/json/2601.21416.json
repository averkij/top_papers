{
    "paper_title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
    "authors": [
        "Alexandre Chapin",
        "Bruno Machado",
        "Emmanuel Dellandréa",
        "Liming Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 6 1 4 1 2 . 1 0 6 2 : r Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation Alexandre Chapin1, Bruno Machado1, Emmanuel Dellandrea1, Liming Chen1 Abstract The ability of visuomotor policies to generalize across tasks and environments critically depends on the structure of the underlying visual representations. While most state-of-the-art robot policies rely on either global or dense these approaches features from pre-trained vision models, often entangle task-relevant and irrelevant signals, limiting their robustness to visual distribution shifts. In this paper, we investigate Slot-based Object-Centric Representations (SOCRs) as structured alternative that decomposes scenes into finite set of object-like entities. We present the first large-scale, systematic comparison of global, dense, and object-centric representations across diverse simulated (METAWORLD, LIBERO) and realworld robotic manipulation tasks. Our results show that SOCRs enable superior policy performance and significantly improve generalization under shifts in lighting, texture, and clutter even without task-specific tuning. We further demonstrate that pretraining SOCRs on robotic video datasets amplifies these benefits. Our findings highlight the importance of structured visual abstraction in robotic perception and suggest that SOCRs offer promising pathway toward more robust and generalizable manipulation policies. I. INTRODUCTION visual-motor policy (or visuomotor policy) tells robot what to do based on what it sees given goal, mapping raw visual inputstypically camera imagesto motor actions in robotic system. State-of-the-art approaches for visuomotor policy learning are data-driven based on deep learning, e.g. imitation learning (IL), in leveraging few expert demonstrations without the need for explicit programming. As such, the visual representation of the robot visual input is key to the robots generalization capability when the learned visuomotor policy must adapt to objects, environments, and tasks that differ from those encountered during training. Recent work has intensified efforts to improve visual representation learning and their pre-training methodology as the foundation for robust robot policy learning. Advances include large-scale pre-training on egocentric video with time-contrastive and language-aligned objectives [1], [2], [3], distilling vision foundation models into compact encoders [4], and adopting self-supervised schemes such as masked autoencoding [5]. However, despite their diversity, these methods mostly produce one of two features types: Global features: single vector summarizing the entire image, typically obtained by pooling operations (e.g., max or average pooling in CNNs) or by extracting special CLS token in ViTs. 1Ecole Centrale de Lyon, CNRS, Universite Claude Bernard Lyon 1, INSA Lyon, Universite Lumi`ere Lyon 2, LIRIS, UMR5205, 69130 Ecully, France Correspondance to : alexandre.chapin@ec-lyon.fr Dense features: patch-wise embedding from one of the encoders layers (usually the last). These features preserve spatial detail but do not explicitly represent individual objects. But these visual representations are far removed from human intelligence. By perceiving our surroundings as distinct entities and decomposing them into elementary components, we can quickly spotlight on the relevant ones for manipulation and adapt our behavior to achieve novel tasks [6], [7]. In contrast, as illustrated in Figure 1, both approaches tend to entangle task-relevant and irrelevant cues, and often focus too narrowly on specific image regions, making them brittle to distribution shifts such as lighting changes, novel textures, and clutter [8], [9], [10]. Motivated by these limitations, we posit that visuomotor policy learning should rely on object-centric visual representations for better learning efficiency and generalization capability, and we propose focusing on Slot-based ObjectCentric Representations (SOCRs) [11], [12]. SOCRs introduce an intermediate layer, Slot Attention [11], which restructures dense features into small set of object-like vectors, called slots. This slot-based approach preserves spatial organization and yields more compositional, entitylevel representation, enabling the policy to focus on taskrelevant aspects of the scene while ignoring irrelevant variations. This structured representation aligns more closely with how robots need to parse and interact with the world, enabling models to track, manipulate, and abstract objects effectively. Most prior work on SOCRs has emphasized image decomposition and controllable image generation, with comparatively very little attention to their role in control and decisionmaking. Initial studies exploring their use in reinforcement learning or visuomotor control have largely been confined to highly simplified, toy environments [13], [14], [15], [16], [17] which limits the conclusions that can be drawn about their effectiveness in realistic robotic manipulation scenarios. This gap leaves several critical questions unresolved: 1) Can SOCRs improve robotic manipulation policy learning compared to conventional visual representations ? 2) Can SOCRs enable stronger policy generalization under realistic distribution shifts, including distractors, novel textures, and lighting variations ? To address these questions, we make the following contributions: We develop and open-source unified evaluation frameFig. 1. Overview of visual representations. (Left) We use set of pre-trained visual models with different latent-space structures: global, dense, and object-centric. Dense representations are extracted from one of the encoders layers (CNN or ViT) before linear projection, while global representations are obtained after pooling operations. Slot-based object-centric representations emerge from an additional Slot-Attention layer that binds every dense feature to finite set of slots. (Right) We visualize how each representation attends to different parts of the image. Most methods focus narrowly and may be distracted by irrelevant regions. In contrast, object-centric representations (DINOSAUR*) attend to multiple parts, naturally separating task-relevant from irrelevant information. broader analysis appears in Section VI. work for systematically comparing diverse visual representations for multi-task robotic manipulation in simulation and the real world. We benchmark seven distinct visual representation methods, spanning global, dense, and object-centric paradigms, across two simulated environments and multiple real-world manipulation tasks. We demonstrate that slot-based object-centric representations not only facilitate more effective policy learning but also substantially improve generalization under realistic distractors and domain shifts, without requiring task-specific tuning. We show that, contrary to prior assumptions [18], object-centric methods can benefit from large-scale pretraining to further enhance their downstream performances. By presenting the first comprehensive evaluation of Slotbased object-centric representations (SOCRs) across both simulation and real-world robotic tasks, and showing that SOCRs offer superior robustness and generalization over current dominant approaches, this work fills critical gap in visuomotor policy learning and lays the groundwork for future structured and symbol-aware perception in robot learning. II. RELATED WORKS a) Pretrained vision based models for robot learning: The computer vision community has developed largescale pretraining strategies for visual representation learning, yielding models such as MoCo [19], DINO [20], DINOv2 [21], and CLIP [22]. Though originally trained for tasks like classification and image-text alignment, these models have proven surprisingly effective for visuomotor policy learning in robotics [9], even without access to robotspecific data. To better align visual representations to manipulation tasks, subsequent work introduced domain-aligned pretraining. For example, R3M [1] leverages egocentric video from Ego4D [23] with time-contrastive and video-language alignement objectives, enabling efficient imitation learning. VC1 [2] combines masked autoencoding with mixed-domain training (ImageNet [24] + robotic manipulation videos), while Theia [4] distills multiple vision foundation models (e.g., CLIP, SAM, DINOv2) into compact backbone optimized for robotics. More recently, MCR [8] proposed contrastive objective tailored to robotic video data, achieving strong performance on manipulation tasks. Interestingly, recent studies have challenged the assumption that robot, or manipulation-specific datasets, are always superior. Dasari et al. [25] showed that curated, generalpurpose datasets often outperform specialized collections like Ego4D, highlighting that dataset quality and diversity matter more than strict domain alignment. Similarly, Burns et al. [10] found that vision transformers with strong emergent object-segmentation properties generalize better under distribution shifts such as lighting or texture changes. These insights motivate shift away from flat features toward structured, object-centric visual representations, which naturally capture the compositional and task-relevant structure of scenes. Our work builds on this trajectory by explicitly evaluating object-centric pretraining for visuomotor policy learning. representations: b) Slot-based object-centric Slotbased object-centric representations aims to decompose visual scenes in an unsupervised fashion into structured latent entities, often called slots, each corresponding to an object. SOCR methods have gained traction across domains such as autonomous driving [26], robotics [14], [27], and explainability [28]. Early approaches relied on generative modeling [29], followed by encoder-decoder architectures for disentangled latent spaces [30], [16]. Slot Attention [11] emerged as landmark method due to its simplicity and efficiency, inspiring extensions with diffusion decoders [31], [32], transformer-based decoders [33], and pretrained backbones [12] for real-world data. More recent work integrates recurrence and transformers to handle video sequences [17], [34], [35], [36]. Despite their natural ability to decompose input images into distinct components, potentially aiding generalization in imitation learning [8], [10], SOCR methods have mostly been evaluated on controllable image generation and segmentation tasks rather than control. Only few exceptions [14], [15], [13] investigate their potential in control or imitation learning settings but are limited to very simple, synthetic environments. Our work extends this line of inquiry by systematically evaluating SOCRs in realistic robotic manipulation tasks and under distributional shifts, comparing them against strong state-of-the-art global and dense visual representations. object decomposition c) Segmentation-driven for robotics: An alternative line of work leverages pretrained supervised foundation models such as the Segment Anything Model (SAM) [37] to generate object masks, which are then paired with frozen vision backbones (e.g., DINOv2). including POCR [38], GROOT [39], and Approaches HODOR [40] fall into this category. While effective, these pipelines face several key limitations. First, they rely on external supervision during pretraining: SAM, for example, requires large-scale annotated datasets, which constrains scalability. Second, inference is memory and compute intensive, as mask generation remains computationally expensive even when aided by mask trackers [39], limiting robots. real-time Finally, to achieve accurate masking, these methods often require raw masks or bounding boxes to identify objects of interest, which may not always be available in practice. resource-constrained applications on In contrast, SOCRs learn object decomposition in selfsupervised, end-to-end manner, making them more flexible across domains. SOCRs are also computationally efficient at inference, making them better suited for real-time robotics. III. METHOD The overview of the policy framework we used to compare the different visual features types is shown in Figure 2. It consists of three components: the frozen visual model, which extracts either global, dense, or object-centric features from raw images, transformer-based observation trunk that integrates different input modalities (e.g., text for instructions and proprioception), and an action head that predicts the final action given an additional action token. We first introduce the process of extracting object-centric representations from dense features, and subsequently detail their integration into policy training to ensure fairness in evaluation with other visual features. a) Object-centric representation: Object-centric methods have emerged as powerful approach in computer vision due to their ability to generalize across scenes by modeling object-level structure and interactions [18]. Fig. 2. Overview of the robotic manipulation policy architecture. We use pre-trained visual model to extract visual features from raw images. These features are then combined with other modalities (e.g., language instructions, proprioception) in an observation trunk with an additional action token. Finally, policy head predicts the next action given the additional token. We experiment with different types of visual models that produce global, dense or object-centric representations. Given an input image O, the goal is to produce set of object representations, or slots, = s1, . . . , sK. To achieve this, the image is first encoded by visual backbone into dense feature tokens = f1, . . . , fN , where K. Slot Attention [11] then extracts compact set of object representations by iteratively attending to these features. Slot Attention is differentiable module that performs iterative cross-attention with competition, encouraging different slots to specialize in distinct parts of the input image. Formally, attention weights are computed as: = softmax (cid:18) QKT (cid:19) , S(i+1) = AV, (1) where the queries are projections of the current slot representations S(i), and the keys and values are projections of the feature tokens . This iterative refinement process yields the final object-centric slots S. Following DINOSAUR [12], we use strong pretrained visual backbone (e.g. DINO [20]) and use decoder to reconstruct the backbone features to avoid the pitfalls of pixel-level reconstruction. This permits training on complex, real-world images while leveraging the rich features of modern vision models. We updated the original DINO [20] backbone with the more recent DINOv2 [21]. Inspired by [36], we extend Slot-Attention to video by incorporating transformer layer between timesteps to recursively transmit informations. At each timestep, the slots first go through the transformer layer with slot-wise self-attention to then being used as initial slots for Slot-Attention. We pre-train this model from scratch on the COCO dataset [41] following [12]. In this specific case, as the dataset is composed of independent images, the transformer layer is only used to initialize the first slots during training but used to transmit slots between timesteps during inference. We call our adapted model (DINOSAUR + DINOv2 + temporal) DINOSAUR*. b) Robotic pre-training: Although object-centric models show promise in structured scene decomposition, they are typically trained on internet-scale in-the-wild datasets, which depict data diversity but are often misaligned with the distribution of robotic manipulation environments. To bridge this gap and further increase training data diversity in line with the findings in [25], we introduce dedicated pretraining stage on robotic video datasets. We then train two version of the object-centric model: DINOSAUR* which is trained on COCO dataset, for fair comparison against baselines that have been trained on no-robotic data, and DINOSAUR-Rob* which is pre-trained on robotic data. We compile and preprocess collection of real-world robotic datasets spanning wide variety of manipulation skills, environments, and embodiments. In total, our training set includes over 188,000 trajectories, drawn from three major sources: BridgeData V2 [42], diverse set of demonstrations across household tasks using the WidowX-250 arm; Fractal [43], large-scale dataset collected with fleet of Everyday Robots across hundreds of kitchen manipulation tasks; and DROID [44], which contains unconstrained robot interactions across multiple laboratories and setups. The combined dataset offers rich visual diversity, including varied viewpoints, object types, and lighting conditions. This model also allows us to fully leverage the temporal layer of our adapted Slot Attention module during training and inference. c) Policy training: Given dataset of expert demonstrations = τ1, . . . , τn, where each trajectory τi = [(o0, a0), . . . , (oT , aT )] pairs observations with actions, we aim to learn policy π that learns to map the current observation ot (e.g. visual inputs) to the next action at. Importantly, each policy is trained in multi-task setting, enabling learning from demonstrations across multiple tasks rather than single one. To ensure fair comparison across different feature types, we design policy architecture that can flexibly accommodate global features, dense features, or slot-based object-centric features within unified framework. As in prior work [1], [10], [8], all pretrained vision encoders are kept frozen during policy training to isolate the effect of representation choice. We experiment with policy model based on the BAKU architecture [45], which consists of an encoder, an observation trunk, and policy head, as shown in Figure 2. The original observation trunk is transformer encoder that encodes sequence of past observations, each containing visual features, proprioceptive states, and language embeddings, interleaved with learnable action tokens. The policy head, an MLP, generates the next action from the final action token. The architecture can with very few modifications handle object-centric, global, or dense features, as it treats visual inputs as sets of tokens. This property ensures that the transformer can attend over both structured (slot-based) and unstructured (global or dense) features, thereby preserving fairness across representation types while explicitly exploiting the compositional structure of slots. For real-world experiments, the architecture is slightly modified to use transformer encoder-decoder structure as observation trunk, with an adapted version of the Action Chunking Transformers (ACT) [46]. We adapt our implementation from the LeRobot library1, and extended it with task-specific language embeddings (derived from ModernBERT [47]) input. IV. BENCHMARKS AND EXPERIMENTAL SETUP a) Environments and tasks: To comprehensively evaluate visual representations for robot manipulation, we selected three environments, two in simulation and one in the real world, chosen for their diversity in task complexity, embodiment, and visual structure. In simulation, we use MetaWorld [48], widely adopted benchmark consisting of tabletop manipulation tasks with Sawyer robotic arm. MetaWorld provides controlled, standardized setting, making it ideal for evaluating representations in simple, single-object scenarios and testing basic generalization (distractors, texture, and lighting). To assess scalability and multi-object reasoning, we also include LIBERO-90 [49], recent benchmark featuring complex scenes across kitchens, offices, and living rooms. LIBERO tasks involve multiple objects with diverse appearances and affordances, emphasizing combinatorial generalization and reasoning about object interactions, conditions under which object-centric representations are expected to provide strong advantage over traditional dense or global features. However, LIBEROs benchmark do not introduce any distributional shifts for evaluation. So we only report results on the training distribution. For real-world evaluation, we deploy Franka robotic arm on set of four tabletop manipulation tasks, Figure 3 provides an overview of the setup used. This setup allows us to test the practical effectiveness of object-centric representations under real-world embodiment constraints and visual noise. b) Pre-trained visual models: Based on recent stateof-the-art evaluations [2], [10], we selected 7 pre-trained in Table I. These models visual models that we depict span range of backbone architectures, including ResNet variants [50] and Vision Transformer-based models [51], as well as diverse training objectives such as supervised learning, self-supervised learning, contrastive learning and distillation. All selected models have demonstrated stateof-the-art performance within their respective categories in benchmark evaluations. We also aimed to capture all feature representation types to support comprehensive comparison. Additionally, our comparison includes naıve segmentation-driven object-centric baseline. The model, inspired by POCR [38], integrates the Segment Anything Model (SAM) [37] and DINOv2 [21]. The GroundingDINO [52] model is used to extract bounding boxes given text prompts. These bounding boxes are then converted to object masks using the SAM model. Finally, mask pooling operation is applied to the DINOv2 dense features to obtain object-centric features. Masks are then tracked over time to ensure temporal consistency using the DEVA pipeline [53]. For fair comparison with DINOSAUR*, we use DINOv2 1https://github.com/huggingface/lerobot Fig. 3. Overview of real-world setup. We evaluate the different visual models on Franka robotic arm on four tabletop manipulation tasks (From left to right): Stacking bowls into pan, Opening drawer placing screwdriver inside and closing the drawer, Putting cans into bin and Placing plates into dish rack. as the backbone. This choice ensures that differences in performance can be attributed to the representation structure rather than the underlying feature extractor. V. RESULTS We evaluate the role of different visual features types in learning and generalizing robotic manipulation policies. Our results aim to answer the following questions introduced in Section 1: Q1: Can SOCRs improve robot policy learning efficiency over other visual representations? Q2: Can SOCRs enhance policy generalization under visual distribution shifts ? We assess models in both simulation and real-world settings. In simulation, we report mean success rates over three random seeds with 50 rollouts per task; in the real world, we conduct 12 rollouts per task and per generalization level (1000 rollouts in total by combining all experiments). All policies are trained on multiple-task using frozen visual encoders. Figure 4 summarizes the average success rate across MetaWorld, LIBERO, and our real-world benchmark. The green dots represent in-domain performance, while the orange dots indicate average success rates across various generalization scenarios (distractors, novel textures, lighting changes). The red numbers denote the relative drop in performance from in-domain to generalization settings. A. Q1: Do SOCRs improve manipulation policy learning ? Policy models based on object-centric features, especially DINOSAUR-Rob*, consistently achieve the highest overall performance, in par with or surpassing the best dense and global baselines across all environments. This confirms that SOCRs not only permits to achieve effective policy learning but also scale well to complex, multi-object scenarios. We can observe that the naıve segmentation-driven representation (SAM+DINOv2) does not permit to learn effective policies in any setting. Indeed, contrary to prior work [38], [39], we use naıve version only encoding the what (appearance) of objects, without any spatial information (bounding boxes or masks coordinates). This choice was made to ensure fair comparison with other models, as they do not have access to such spatial cues. However, this design decision likely hampers the models ability to capture object relationships and spatial arrangements, which are crucial for effective manipulation. This also highlights the limitations of segmentation-driven approaches compared to end-to-end learned object-centric representations that can capture both appearance and spatial structure jointly during training. In MetaWorld, all models except the VC-1-based perform above 60%. The low VC-1 performance may result from its MAE-based pretraining, which is sensitive to domain mismatch when not fine-tuned. Object-centric-based models perform comparably to top baselines here, despite the simplicity of the environment. In LIBERO, which features complex scenes with multiple objects, SOCRs-based policies perform on par with or better than dense-based (Theia, DINOv2). The global-based models (R3M, ResNet, VC-1) lag behind, likely due to their inability to capture fine-grained object interactions. In the real-world, the ResNet-based policy is surprisingly strong, likely due to the diversity of ImageNet pretraining, consistent with [25]. The DINOSAUR-Rob* policy performs in par with ResNets and outperforms all other baselines by significant margin, achieving 56% success rate. Notably, even without robotic pretraining, the DINOSAUR* policy remains highly competitive, achieving 48% success rate on real-world tasks and improve 20% over the DINOv2 policy. This suggests that the intermediate object-centric structuration itself confers robustness and effectiveness. Overall, our results for Q1 confirm that object-centric models are not only effective but scalable across domains, offering performance benefits in both structured simulation tasks and noisy real-world environments. B. Q2: Do SOCRs enhance generalization under visual distribution shifts? We now evaluate generalization to out-of-distribution conditions, including novel distractors, unseen textures, and lighting changes. The Figure 4 summarizes the average success rate across all generalization scenarios. Table II, and III provide detailed results for each environment and shift type. As SAM+DINOv2 fails to learn effective policies, we exclude it from the generalization analysis. Overall, SOCRs-based policies exhibit substantially better robustness to distribution shifts compared to dense and global representations. Notably, DINOSAUR-Rob* policy consistently achieves the highest success rates overall across environments, with the smallest relative performance drop from in-domain to generalization settings. Even without robotic pretraining, DINOSAUR* remains highly competitive in simulation and clearly outperforms all baselines in the real world. Notably, we can have direct comparison TABLE MODELS OVERVIEW. COMPARISON OF THE DATA AND SIZES OF THE DIFFERENT PRE-TRAINED VISUAL MODELS . VIT: VISION TRANSFORMER, SOCR: SLOT-BASED OBJECT-CENTRIC LAYERS, G: GLOBAL, D: DENSE, SM: SEGMENTATION. Model ResNet-50 [50] R3M [1] DINOv2 [21] VC-1 [2] Theia [4] SAM [37] + DINOv2 DINOSAUR* DINOSAUR-Rob* Backbone ResNet-50 ResNet-50 ViT ViT ViT ViT + SM ViT + SOCR ViT + SOCR Pre-training Dataset ImageNet [24] Ego4D [23] LVD-142M [21] Ego4D [23] and ImageNet [24] ImageNet [24] LVD-142M [21] + SA-V [37] COCO [41] Robot Mixt. # of params. 25.6M 25.6M 86M 86M 140M 400M 88M 88M Features G SM Slot Slot Fig. 4. Overall success rate on in-domain and generalization scenarios. Mean success rate over all tasks for each visual model on MetaWorld (left), LIBERO (middle) and Real robot using Franka (right). Green dot: in-domain performance, Orange dot: average performance over all generalization scenarios (distractors, novel textures, lighting changes). Red number: relative drop in performance from in-domain to generalization settings. between DINOSAUR* and DINOv2 as they share the same backbone with only the additional adapted Slot-Attention layer on top of the frozen model. This comparison highlights the benefits of object-centric intermediate representations for generalization, as DINOSAUR* significantly outperforms DINOv2 across all scenarios. TABLE II GENERALIZATION METAWORLD. COMPARISON OF DIFFERENT LEVELS OF GENERALIZATION IN METAWORLD. Models ResNet-50 R3M DINOv2 VC1 Theia DINOSAUR* DINOSAUR-Rob* Distractors 0.04 0.02 0.0 0.11 0.02 0.06 0.02 0.65 0.10 0.21 0.04 0.46 0.14 Textures 0.0 0.0 0.0 0.03 0.01 0.0 0.0 0.28 0.06 0.48 0.06 0.36 0.05 Lighting 0.22 0.02 0.0 0.39 0.04 0.23 0.03 0.48 0.10 0.71 0.06 0.65 0.14 Over. 0.10 0.0 0.18 0.10 0.47 0.46 0.49 In MetaWorld (Table II), DINOv2 and Theia-based policies outperform ResNet-based policies on average, confirming previous findings (e.g.,[10]). Notably, Theia performs best in the distractor scenario, likely due to CLIP-based text-image alignment helping the policy to ignore irrelevant patches. Remarkably, the SOCRs-based models excel under texture and lighting shifts, where they outperform all baselines by large margin. This suggests that object-centric representations effectively capture invariant object properties, enhancing robustness to appearance changes. Indeed, Figure 1 shows that SOCRs can distinguish objects from background clutter, likely enabling policies to focus on taskrelevant elements. TABLE III GENERALIZATION REAL-WORLD. COMPARISON OF DIFFERENT LEVELS OF GENERALIZATION IN THE REAL ROBOT. Models ResNet-50 R3M DINOv2 VC1 Theia DINOSAUR* DINOSAUR-Rob* Distractors 0.15 0.0 0.06 0.03 0.06 0.27 0.37 Textures 0.10 0.0 0.08 0.02 0.08 0.29 0.44 Overall 0.12 0.0 0.07 0.03 0.07 0.28 0.41 In real-world evaluations (Table III), both SOCRsbased models excels in the two generalization levels and overperform by large margin every other features types. The ResNet-50-based policy, which performed competitively in-domain, struggles significantly under distribution shifts, likely due to its limited capacity to disentangle objects from background noise. Dense-based models also see substantial performance drops, underscoring their sensitivity to visual perturbations. In contrast, SOCRs-based models maintain robust performance, with DINOSAUR-Rob* achieving 41% success rate on average across shifts, highlighting the practical benefits of object-centric representations for realworld robotic manipulation. In summary, results for Q2 demonstrate that objectcentric representations generalize better across diverse distribution shifts, particularly those that perturb low-level appearance. This robustness likely stems from SOCRs ability to filter task-irrelevant background and focus on object-level structure. VI. FOCUS ANALYSIS OF VISUAL REPRESENTATIONS In order to better understand how different visual features aggregate information in complex scenes, we conduct model focus analysis. The goal is to assess whether models can effectively concentrate on task-relevant objects while ignoring irrelevant background elements. As we are working with different types of architectures (CNN, ViT, SlotAttention), we adapt the visualization method to each model type. For ResNet-based models, we use Grad-CAM [54] to generate class-discriminative localization maps. For ViTbased models, we compute attention rollout [55] to visualize how attention is distributed across image patches. For slot-based object-centric models like DINOSAUR* and our robotic-adapted DINOSAUR-Rob*, we visualize the attention weights from the Slot Attention module to see which image regions each slot focuses on. The Figure 1 shows thant different representations exhibit distinct focus patterns. ResNet-based models tend to have single, broad focus area, which can lead to distractions from irrelevant parts of the image. ViT-based models show more distributed attention but still often concentrate on few key regions, which may not always align with task-relevant objects. In contrast, the intermediate slot-based object-centric permits to the dense features to be decomposed into multiple slots that each attend to different objects or parts of the scene. Each slot appears to specialize in different parts of the scene, effectively separating task-relevant information from background noise. VII. LIMITATIONS While we tried to conduct extensive and thorough experiments, several limitations remains and should be addressed in future works. First, the object-centric methods employed in this study lack semantic grounding. Some slots are allocated to background regions without capturing meaningful semantic content, and in some failure cases slots also capture distractors inside existing slots. This limitation suggests that incorporating semantic information, could enhance the interpretability and the practical utility of the object-centric representations. Furthermore, it may provide pathway to improve the generalization capabilities of SOCRs in the presence of distractors. Second, our work does not account for the alignment with robot dynamics or with the scene dynamics. We posit that integrating this aspect could significantly improve the practical applicability of our methods [36], [56]. Future research should explore this avenue to better align objectcentric models with robotic systems. VIII. CONCLUSION In this work, we investigate the potential of an intermediate slot-based object-centric representations to enhance the generalization and robustness of robotic manipulation policies. Unlike traditional visual features, SOCR-based models introduce inductive biases that better reflect the structured nature of physical interaction, yielding consistently superior performance across diverse set of simulated and real-world manipulation tasks under distributional shifts. Our results highlight the importance of rethinking visual representations structure to advance both the efficiency and generalization of robotic agents: moving away from pixel-level features toward more structured, object-based encodings. By leveraging object-centric biases, we can bridge the gap between low-level visual input and high-level symbolic reasoning, enabling robots to better understand and interact with their surroundings. Future research should explore how SOCRs can be further integrated with diverse architectures, multimodal inputs, and self-supervised learning frameworks to maximize their scalability and downstream utility. In particular, we are currently developing semantic-aware version of SOCR that explicitly incorporates semantic grounding to overcome the limitations discussed in Section VII, which we expect to yield even greater interpretability, robustness, and generalization in realworld robotic systems."
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, R3m: universal visual representation for robot manipulation, 2022. [Online]. Available: https://arxiv.org/abs/2203.12601 [2] A. Majumdar, K. Yadav, S. Arnaud, et al., Where are we in the search for an artificial visual cortex for embodied intelligence? 2024. [Online]. Available: https://arxiv.org/abs/2303.18240 [3] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, reward and [Online]. and A. Zhang, representation via value-implicit pre-training, 2023. Available: https://arxiv.org/abs/2210.00030 Vip: Towards universal visual [4] J. Shang, K. Schmeckpeper, B. B. May, M. V. Minniti, T. Kelestemur, D. Watkins, and L. Herlant, Theia: Distilling diverse vision foundation models for robot learning, 2024. [Online]. Available: https://arxiv.org/abs/2407. [5] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell, Real-world robot learning with masked visual pre-training, 2022. [Online]. Available: https://arxiv.org/abs/2210.03109 [6] E. S. Spelke, Principles of object perception, Cognitive science, vol. 14, no. 1, pp. 2956, 1990. [7] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, Building machines that learn and think like people, 2016. [Online]. Available: https://arxiv.org/abs/1604.00289 [8] G. Jiang, Y. Sun, T. Huang, H. Li, Y. Liang, and H. Xu, Robots pretrain robots: Manipulation-centric robotic representation from largescale robot dataset, arXiv preprint arXiv:2410.22325, 2024. [9] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta, The unsurprising effectiveness of pre-trained vision models for control, 2022. [Online]. Available: https://arxiv.org/abs/2203. [10] K. Burns, Z. Witzel, J. I. Hamid, T. Yu, C. Finn, K. Hausman, for successful https://arxiv.org/abs/2312.12444 What makes pre-trained visual robust manipulation? 2023. and representations [Online]. Available: [11] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf, Objectcentric learning with slot attention, 2020. [Online]. Available: https://arxiv.org/abs/2006.15055 [12] M. Seitzer, M. Horn, A. Zadaianchuk, et al., Bridging the gap [Online]. Available: to real-world object-centric learning, 2023. https://arxiv.org/abs/2209.14860 [13] J. Yoon, Y.-F. Wu, H. Bae, and S. Ahn, An investigation into pre-training object-centric representations for reinforcement learning, 2023. [Online]. Available: https://arxiv.org/abs/2302.04419 [14] N. Heravi, A. Wahid, C. Lynch, P. Florence, T. Armstrong, J. Tompson, P. Sermanet, J. Bohg, and D. Dwibedi, Visuomotor control in multi-object scenes using object-aware representations, 2023. [Online]. Available: https://arxiv.org/abs/2205.06333 [15] D. Haramati, T. Daniel, and A. Tamar, Entity-centric reinforcement [Online]. learning for object manipulation from pixels, 2024. Available: https://arxiv.org/abs/2404.01220 [16] N. Watters, L. Matthey, M. Bosnjak, C. P. Burgess, and A. Lerchner, Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration, 2019. [Online]. Available: https://arxiv.org/abs/1905.09275 [17] T. Kipf, G. F. Elsayed, A. Mahendran, A. Stone, S. Sabour, G. Heigold, R. Jonschkowski, A. Dosovitskiy, and K. Greff, Conditional object-centric learning from video, 2022. [Online]. Available: https://arxiv.org/abs/2111.12594 [18] A. Didolkar, A. Zadaianchuk, A. Goyal, M. Mozer, Y. Bengio, G. Martius, and M. Seitzer, Zero-shot object-centric representation learning, 2024. [Online]. Available: https://arxiv.org/abs/2408.09162 [19] X. Chen, H. Fan, R. Girshick, and K. He, Improved baselines with momentum contrastive learning, 2020. [Online]. Available: https://arxiv.org/abs/2003.04297 [20] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. in self-supervised vision transformers, 2021. [Online]. Available: https://arxiv.org/abs/2104. 14294 Joulin, Emerging properties [21] M. Oquab, T. Darcet, T. Moutakanni, et al., Dinov2: Learning robust visual features without supervision, 2024. [Online]. Available: https://arxiv.org/abs/2304.07193 [22] A. Radford, J. W. Kim, C. Hallacy, et al., Learning transferable visual models from natural language supervision, 2021. [Online]. Available: https://arxiv.org/abs/2103.00020 [23] K. Grauman, A. Westbury, E. Byrne, et al., Ego4d: Around the world in 3,000 hours of egocentric video, 2022. [Online]. Available: https://arxiv.org/abs/2110.07058 [24] O. Russakovsky, scale visual https://arxiv.org/abs/1409.0575 J. Deng, H. Su, recognition challenge, 2015. et al., Imagenet large [Online]. Available: [25] S. Dasari, M. K. Srirama, U. Jain, and A. Gupta, An unbiased look at datasets for visuo-motor pre-training, 2023. [Online]. Available: https://arxiv.org/abs/2310.09289 [26] S. Hamdan and F. Guney, Carformer: Self-driving with learned [Online]. Available: https: object-centric representations, 2024. //arxiv.org/abs/2407.15843 [27] M. Mosbach, J. N. Ewertz, A. Villar-Corrales, and S. Behnke, relational [Online]. Available: Sold: Slot object-centric latent dynamics models for manipulation learning from pixels, 2025. https://arxiv.org/abs/2410.08822 [28] B. Wang, L. Li, J. Zhang, Y. Nakashima, and H. Nagahara, Explainable image recognition via enhanced slot-attention based classifier, 2024. [Online]. Available: https://arxiv.org/abs/2407.05616 [29] R. Kabra, D. Zoran, G. Erdogan, L. Matthey, A. Creswell, Simone: representations via [Online]. Available: M. Botvinick, A. Lerchner, View-invariant, temporally-abstracted object unsupervised video decomposition, 2021. https://arxiv.org/abs/2106.03849 and C. P. Burgess, [30] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner, Monet: Unsupervised scene decomposition and representation, 2019. [Online]. Available: https: //arxiv.org/abs/1901. [31] J. Jiang, F. Deng, G. Singh, and S. Ahn, Object-centric slot diffusion, 2023. [Online]. Available: https://arxiv.org/abs/2303.10834 [32] Z. Wu, J. Hu, W. Lu, I. Gilitschenski, and A. Garg, Slotdiffusion: Object-centric generative modeling with diffusion models, 2023. [Online]. Available: https://arxiv.org/abs/2305.11281 [33] G. Singh, F. Deng, and S. Ahn, Illiterate dall-e learns to compose, 2022. [Online]. Available: https://arxiv.org/abs/2110.11405 [34] G. F. Elsayed, A. Mahendran, S. van Steenkiste, K. Greff, M. C. Mozer, and T. Kipf, Savi++: Towards end-to-end objectcentric learning from real-world videos, 2022. [Online]. Available: https://arxiv.org/abs/2206.07764 [35] G. Singh, Y.-F. Wu, and S. Ahn, Simple unsupervised object-centric [Online]. learning for complex and naturalistic videos, 2022. Available: https://arxiv.org/abs/2205.14065 [36] A. Zadaianchuk, M. Seitzer, and G. Martius, Object-centric learning for real-world videos by predicting temporal feature similarities, 2023. [Online]. Available: https://arxiv.org/abs/2306.04829 [37] A. Kirillov, E. Mintun, N. Ravi, et al., Segment anything, 2023. [Online]. Available: https://arxiv.org/abs/2304.02643 [38] J. Shi, J. Qian, Y. J. Ma, and D. Jayaraman, Composing pre-trained object-centric representations for robotics from what and where foundation models, 2024. [Online]. Available: https: //arxiv.org/abs/2404.13474 [39] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu, Learning generalizable manipulation policies with object-centric 3d representations, 2023. [Online]. Available: https://arxiv.org/abs/2310.14386 [40] J. Qian, Y. Li, B. Bucher, and D. Jayaraman, Task-oriented hierarchical object decomposition for visuomotor control, 2024. [Online]. Available: https://arxiv.org/abs/2411.01284 [41] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollar, Microsoft coco: Common objects in context, 2015. [Online]. Available: https://arxiv.org/abs/1405.0312 [42] H. Walke, K. Black, A. Lee, et al., Bridgedata v2: dataset [Online]. Available: https: scale, 2024. learning at robot for //arxiv.org/abs/2308.12952 [43] A. Brohan, N. Brown, J. Carbajal, transformer at Available: https://arxiv.org/abs/2212.06817 real-world control for et al., scale, 2023. Rt-1: Robotics [Online]. [44] A. Khazatsky, K. Pertsch, S. Nair, et al., Droid: large-scale in-the-wild robot manipulation dataset, 2024. [Online]. Available: https://arxiv.org/abs/2403.12945 [45] S. Haldar, Z. Peng, and L. Pinto, Baku: An efficient transformer for multi-task policy learning, 2024. [Online]. Available: https: //arxiv.org/abs/2406.07539 [46] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning fine-grained [Online]. bimanual manipulation with low-cost hardware, 2023. Available: https://arxiv.org/abs/2304. [47] B. Warner, A. Chaffin, B. Clavie, et al., Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference, 2024. [Online]. Available: https://arxiv.org/abs/2412.13663 [48] T. Yu, D. Quillen, Z. He, R. Julian, A. Narayan, H. Shively, A. Bellathur, K. Hausman, C. Finn, and S. Levine, Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning, 2021. [Online]. Available: https://arxiv.org/abs/1910.10897 [49] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, Libero: Benchmarking knowledge transfer for lifelong robot learning, arXiv preprint arXiv:2306.03310, 2023. [50] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, 2015. [Online]. Available: https://arxiv.org/abs/ 1512.03385 [51] A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al., An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [Online]. Available: https://arxiv.org/abs/2010.11929 [52] S. Liu, Z. Zeng, T. Ren, et al., Grounding dino: Marrying dino with grounded pre-training for open-set object detection, arXiv preprint arXiv:2303.05499, 2023. [53] H. K. Cheng, S. W. Oh, B. Price, A. Schwing, and J.-Y. Lee, Tracking anything with decoupled video segmentation, in ICCV, 2023. [54] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, Grad-cam: Visual explanations from deep networks via gradient-based localization, International Journal of Computer Vision, vol. 128, no. 2, p. 336359, Oct. 2019. [Online]. Available: http://dx.doi.org/10.1007/s11263-019-01228-7 and W. Zuidema, Quantifying attention flow in transformers, 2020. [Online]. Available: https://arxiv.org/abs/2005. 00928 [55] S. Abnar [56] A. Manasyan, M. Seitzer, F. Radovic, G. Martius, and A. Zadaianchuk, Temporally consistent object-centric learning by contrasting slots, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025."
        }
    ],
    "affiliations": [
        "École Centrale de Lyon"
    ]
}