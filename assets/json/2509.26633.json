{
    "paper_title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction",
    "authors": [
        "Lujie Yang",
        "Xiaoyu Huang",
        "Zhen Wu",
        "Angjoo Kanazawa",
        "Pieter Abbeel",
        "Carmelo Sferrazza",
        "C. Karen Liu",
        "Rocky Duan",
        "Guanya Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum."
        },
        {
            "title": "Start",
            "content": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction Lujie Yang*1,2, Xiaoyu Huang*1,3, Zhen Wu*1, Angjoo Kanazawa1,3, Pieter Abbeel1,3, Carmelo Sferrazza1, C. Karen Liu1,4, Rocky Duan1, Guanya Shi1,5 1Amazon FAR (Frontier AI & Robotics), 2MIT, 3UC Berkeley, 4Stanford University, 5CMU 5 2 0 2 8 ] . [ 2 3 3 6 6 2 . 9 0 5 2 : r Fig. 1: OMNIRETARGET enables reinforcement learning policies to learn complex, long-horizon loco-manipulation skills in challenging environments that transfer zero-shot from simulation to Unitree G1 humanoid. Thanks to the high-quality interaction-preserving motion retargeting, these policies are trained and deployed in minimal and unified way: it involves only 5 rewards, 4 robot domain randomization terms, and purely proprioceptive observation space, shared by all tasks. Demonstrated behaviors include (a) 30-second parkour course involving chair moving, stepping & vault, and jump & roll, (b) object transportation, (c) crawling on slope, and (d) fast platform climbing and sitting. Abstract dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OMNIRETARGET, an interactionpreserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OMNIRETARGET generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OMNIRETARGET by retargeting motions from OMOMO [1], LAFAN1 [2], and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute longhorizon (up to 30 seconds) parkour and loco-manipulation skills on Unitree G1 humanoid, trained with only 5 reward terms * Equal contribution, work done while interning at Amazon FAR. Amazon FAR team co-lead. and simple domain randomization shared by all tasks, without any learning curriculum. All code, retargeted datasets, and trained policies will be publicly released. Result videos can be found at https://omniretarget.github.io I. INTRODUCTION The quest to enable humanoid robots to perform complex whole-body sceneand object-interaction tasks has long been constrained by fundamental data bottleneck. While deep reinforcement learning (RL) has shown remarkable success in robot control, efficient exploration is highly sensitive to reward engineering [3]. This challenge is further amplified on humanoids, whose high-dimensional action spaces and complex dynamics make learning natural, expressive behaviors from scratch both difficult and inefficient. To address these challenges, imitating human motions offers powerful alternative for learning whole-body control, especially for complex scene interactions. Human demonstrations capture dynamic coordination, such as lifting objects while walking on uneven terrain, and have been used effectively in animation [4], [5], [6]. critical challenge arises in robotics: unlike virtual characters, physical humanoids only approximate human morphology, with significant differences in shape, proportion and degrees of freedom. This embodiment gap means that simply adapting human motions is insufficient; it is essential to also adapt their scene interactions to the robots specific form to generate usable references. To this end, researchers have pursued two main strategies. The first one is teleoperation [7], [8], [9], where only human operators motions are retargeted to control the robot online. This approach leverages the human operator for realtime adaptation, which sidesteps the need for automatic interaction retargeting. However, despite the advantage of online feedback, the method remains labor-intensive and does not scale well for large-scale data generation. The second and more scalable strategy is offline interaction retargeting, which holistically adapts both the humans motion and their scene interactions to the robots specific embodiment. However, most existing retargeting methods [10], [9], [11] fall short in this regard. They predominantly rely on unconstrained or softly-penalized optimization, resulting in implausible motions with artifacts such as foot skating and penetration. More importantly, they do not explicitly consider interaction preservationi.e., maintaining spatial and contact relationshipin the retargeting formulation, relying instead on simple keypoint matching. Consequently, the resulting references are of lower quality, which in turn complicates the downstream RL policy training [12], [8], [13]. In this work, we introduce OMNIRETARGET, an opensource data generation engine that transforms human demonstrations into diverse, high-quality kinematic references for humanoid whole-body control. By modeling spatial and contact relationships between robots, objects, and terrains via an interaction mesh [14], OMNIRETARGET preserves essential interactions and generates kinematically feasible variations. While existing methods require separate demonstrations for each variationmaking data collection costly and limiting coverageOMNIRETARGET addresses this bottleneck directly. Inspired by data augmentation frameworks for contact-rich manipulation [15], our framework automatically augments single demonstration into large number of training examples across object configurations, shapes, robot embodiments, and environments. Our pipeline employs constrained optimization to enforce physical feasibility, including collision avoidance, joint limits, and foot contact stability, while minimizing interaction mesh deformation. The resulting motions are interactionpreserving and exhibit only minimal kinematic artifacts, providing dense learning signals that accelerate RL with minimal reward engineering. On diverse suite of wholebody interaction tasks such as box lifting, platform climbing, and slope crawling, policies trained on OMNIRETARGET datasets outperform those from prior retargeting methods in both motion quality and robustness, with successful zero-shot sim-to-real transfer onto physical humanoid robot. Our contributions are fourfold: 1) The first interaction-preserving humanoid retargeting framework that handles rich robot-object-terrain interactions while enforcing hard physical constraints. 2) systematic data augmentation pipeline that transforms single human demonstration into diverse, large-scale set of high-quality kinematic trajectories on various robot embodiments. 3) large-scale, open-source dataset of retargeted, kinematically-feasible loco-manipulation trajectories. 4) Successful zero-shot sim-to-real transfer of proprioceptive RL policies on physical humanoid, demonstrating diverse set of scene-interaction tasks, including long, agile sequence of object carrying, platform climbing, jumping, rolling and wall-flipping. II. RELATED WORKS A. Motion Retargeting In computer graphics, transferring motions across characters has been extensively explored. Researchers have employed optimization-based methods to retarget human motions to avatars by preserving distances and orientations between keypoints [16], minimizing deformation energy [14], [17], or scaling the motions to satisfy hard constraints [18]. Others leverage data-driven methods that map diverse skeletons to canonical representation [19], solve inverse kinematics with neural networks [20], or use reinforcement learning to preserve an interaction graph [21]. Retargeting motions to humanoid robots introduces challenges beyond character animation, particularly the need to enforce physical constraints. For example, PHC [10], graphics method adopted in robotics [13], [8], uses keypoint matching with unconstrained optimization, often leading to penetration, foot skating, and lack of object or scene awareness. Similarly, GMR [9] extends keypoint matching to orientations but suffer the same issues. VideoMimic [11] improves realism with soft contact and collision penalties but offers no guarantees and requires careful tuning. The closest method to ours is Interaction Mesh based Motion Adaptation (IMMA) [22], which also leverages an interaction mesh [14] to preserve the spatial relationship between body parts. However, it is not open-sourced and ignores kinematic limits and interactions with the environment or manipulated objects. In contrast, OMNIRETARGET including foot sticking, nonunifies all hard constraints, penetration, and joint and velocity limits, while explicitly preserving environment and object interactions. B. Learning-Based Humanoid Whole-Body Control"
        },
        {
            "title": "Recent",
            "content": "learning-based whole-body control has enabled humanoids to traverse dynamic scenes and manipulate objects [23], [24], [25], [26], [27], [28], [29], [30], [31]. These methods typically train with hand-crafted rewards or task interfaces (e.g., velocity tracking, contact schedules, endeffector targets) but depend on extensive reward engineering and mostly fail to yield natural, human-level motions. Motion imitation offers promising alternative. In graphics, DeepMimic [4] shows that using human references yields natural, human-like behaviors with agile, dynamic motions. However, applying this approach to humanoid robots remains difficult due to the lack of reliable open-source kinematic retargeting pipelines. With suboptimal reference motions, practitioners are forced to either manually clean the data [12] or re-introduce extensive reward engineering, such as ad-hoc regularizers for contact, slipping, and air time, to compensate for artifacts [9], [13], [32]. In contrast, trackers with minimal Fig. 2: OMNIRETARGET overview. Human demonstrations are retargeted to the robot via interaction-meshbased constrained optimization. Each spatial and shape augmentation is solved as new optimization, producing diverse trajectories that serve as references for RL training with minimal reward design and domain randomization, enabling zero-shot transfer to real-world humanoids. reward formulation like BeyondMimic [33] achieve state-ofthe-art results on hardware with high-fidelity references [34], but those are scarce and robot-only, without interactions. Beyond single-character motion, humanscene interaction data has proven effective for terrain traversal and locomanipulation in character animation [5], [6], but translating this to robotics remains challenging. VideoMimic [11] applies this idea to humanterrain traversal by reconstructing motions and terrains from video, but suffers from artifacts and is limited to staticscene interactions. To bridge this gap, OMNIRETARGET enables natural, agile robot-objectscene interactions with high-quality reference from retargeting without manual post-processing or reward engineering. C. Data Generation for Humanoid Loco-Manipulation The demand for whole-body interaction data has motivated many prior works on data generation. One approach is direct human teleoperation [35], [7], [8], [9], [36]. While it provides online feedback, teleoperation is difficult to scale: its labor-intensive, prone to operator fatigue, and limited by the embodiment gap between human and robot kinematics. The lack of rich haptic feedback and difficulty stabilizing extreme motions (e.g., deep squats) further constrain its applicability. To address these scaling challenges, automated data augmentation has been explored, particularly for robotic manipulation. Many works leverage state-of-the-art generative models for visual [37], [38], [39] and semantic [40], [41], [42] augmentations, while others rely on simple openloop kinematic replay of base trajectories [43], [44], [45] or trajectory optimization [15] in simulation. Despite the advances in manipulation, data augmentation for whole-body loco-manipulation remains largely unexplored. The closest prior work [46] interpolates keypoints to augment objects of different shape, but it cannot deal with varied object poses either. OMNIRETARGET directly addresses this gap. III. INTERACTION-PRESERVING MOTION RETARGETING A. Interaction Mesh with Hard Constraints We leverage the interaction mesh [14] to preserve spatial relationships between body parts, objects, and the environment. The interaction mesh is defined as volumetric structure whose vertices consist of key robot or human joints together with points sampled from objects and the environment. By shrinking or stretching this mesh, we can warp human motion onto the robot while preserving relative spatial configurations and contact relationships. Interaction Mesh Construction. We construct the interaction mesh by applying Delaunay tetrahedralization [48] to user-defined key joint positions and randomly sampled object and environment points. To more accurately maintain contact relationships, we sample the object and environment surfaces more densely than the body joints. Optimization Objectives and Constraints. To preserve the spatial relationships between the body parts, objects and terrains, our primary objective is to minimize the Laplacian deformation energy of the interaction meshes [49], [50] constructed from two corresponding sets of keypoints. The source set at frame t, source , is composed of userdefined anatomical points on the human, and points randomly sampled on the manipulated object and the environment. The target set for the retargeted motion, target , consists of corresponding anatomical points on the robot, the same manipulated object and environment points. Our method is relatively robust to the precise placement of these keypoints, requiring only semantically consistent correspondence between the human and robot (e.g., hand to hand). The Laplacian coordinate of the i-th keypoint pt,i Pt is defined as the difference between the point and the weighted average of its neighbors (i): L(pt,i) = pt,i jN (i) wi pt, j, (1) Methods IMMA [22] PHC [10] GMR [9] VideoMimic [11] OMNIRETARGET (Ours) Hard Kinematic Constraints Interaction w/ Object Interaction w/ Terrain Data Augmentation Soft Penalty Optimization Method QP Gradient Descent Mink [47] JAX L-M Sequential SOCP TABLE I: Comparison of prior retargeting methods across different aspects. where wi is the normalized weight and we omit Ls dependence on {pt, j} j=i in the function definition for conciseness. For all our experiments, we use uniform weights, setting wi = 1/N (i). The deformation energy measures the change in these Laplacian coordinates between the source and the retargeted mesh target demonstration mesh source : EL = psource t,i ,ptarget Psource t,i Ptarget L(psource t,i ) L(ptarget t,i )2. (2) We seek the robot configuration qt , consisting of the floating base pose (quaternion and translation) and all joint angles, that minimizes this deformation energy while satisfying set of hard kinematic constraints. The robots keypoints are determined by its configuration qt via forward kinematics fi as probot . At each time step, we solve the following constrained, nonconvex program: (qt ) = fi(qt ) target t,i q = arg min qt L(psource t,i ) L(ptarget t,i (qt ))2 + qt qt12 s.t. φ j(qt ) 0, qmin qt qmax vmin dt qt qt1 vmax dt = pF pF t1, stance foot, (3a) (3b) (3c) (3d) (3e) where is cost matrix that encourages temporal smoothness, φ denotes the signed distance function for the j-th collision pair, qmin/qmax and vmin/vmax are the configuration and velocity bounds, pF denotes the foot position. foot is considered to be in the stance phase if its horizontal velocity in the source motion (in the xy-plane) falls below threshold of 1 cm/s. This optimization program solves for temporally consistent robot trajectory that minimizes interaction mesh deformation, subject to hard constraints for collision avoidance (3b), joint and velocity limits (3c)(3d), and preventing foot skating (3e). We solve (3) sequentially for each timestep using customized Sequential Quadratic Programming (SQP)-style solver. Within each iteration, the objective (3a) is quadratically approximated and the hard constraints (3b)(3e) are linearized around the solution from the previous iteration. To ensure temporal consistency and accelerate convergence, the optimization at frame is warm-started with the optimal solution from the previous frame t1. key challenge in this formulation is computing derivatives involving the quaternion-based floating base orientation; our implementation leverages the automatic differentiation framework in Fig. 3: Cross-embodiment robot-object-terrain interaction. Drake [51], which correctly handles the differential geometry of rotations on the S3 manifold [52]. Our interaction-mesh-based kinematic pipeline is highly general. It adapts to different robot embodiments, including the Unitree G1, H1, and Booster T1  (Fig. 3)  , by modifying only the keypoint correspondences in the interaction mesh and the robots collision model. It also supports diverse interaction types: robot-object interactions from the OMOMO [1], robot-terrain interactions from in-house MoCap data, and robot-only motions on flat terrain from LAFAN1 [2]. B. Terrain, Object Shape and Spatial Augmentation key advantage of our framework is its capability for systematic data augmentation, which eliminates the need for collecting numerous, repetitive demonstrations with minor spatial variations. Our method can transform single human demonstration into rich and diverse dataset by parametrically altering object configurations, shapes, or terrain features. For each new scenario, we re-solve the optimization and augmented Pt : minimizing problem with fixed source the interaction mesh deformation finds new, kinematically valid robot motion {qt } that preserves the essential spatial and contact relationships of the original interaction. Robot-Object. We generate diverse interactions by augmenting both the objects spatial configuration and its shape. We apply translations and rotations to modify the objects initial pose (Fig. 4b) and blend the new initial pose with the original object motion via interpolation with an exponential schedule detailed in (14). In addition, we scale the three dimensions of the object (Fig. 4c). critical component of this process is constructing the interaction mesh in the objects local frame, which ensures that the robots interacting body Fig. 4: OMNIRETARGET generates systematic variations of (a) terrain height, (b) object initial pose, and (c) object shape from single human demonstration, with optimized motions in simulation (top) transferring consistently to hardware (bottom). parts naturally follow the objects transformation (Sec. VIC.2). However, this alone can lead to trivial augmentations where the entire robot undergoes rigid transformation along with the object. To generate more meaningful data diversity, we introduce cost terms and constraints that anchor parts of the robots body to the nominal trajectory { }. For instance, in pick-up task, we encourage the robot to discover new upper-body coordination by penalizing lowerbody deviations from the original motion: Observations. To show that high-quality reference motions provide sufficient prior for complex tasks, we design minimal proprioceptive observation space, as listed below, where the agent is blind to explicit scene and object information and must follow the reference trajectory precisely. Reference Motion: Reference Joint Position/Velocity, Reference Pelvis Position/Orientation Error; Proprioception: Pelvis Linear/Angular Velocity, Joint Position/Velocity; Previous Action: Policy action from last timestep. qt W , (4) For agile motions where state estimation is unreliable, we where heavily penalizes the lower-body entries, constraining the initial foot poses to match the nominal trajectory 0 = pF pF for left and right feet. (5) These added objectives prevent the optimization from collapsing to simple rigid transform of the nominal trajectory and instead produce genuinely new and diverse interactions. Robot-Terrain. We generate diverse terrain scenarios by scaling environmental features, such as varying the platform height and depth (Fig. 4a), and introducing additional constraints. For instance, to encourage stable ground contact when the terrain is elevated, we uniformly sample grid of points on the ground surface into the interaction mesh. IV. RL TRAINING WITH MINIMAL FORMULATION Having established our method for generating high-quality kinematic references, we use RL to bridge the gap to dynamics by training low-level policy that converts these trajectories into physically realizable actions, enabling zeroshot transfer from simulation to hardware. Reward engineering is often the main difficulty in humanoid RL: prior works [9], [13], [32] rely on many ad-hoc regularizers (e.g., foot flight and contact time) to compensate for artifacts in noisy references, but tuning these terms is tedious and fragile. In contrast, BeyondMimic [33] shows that when references are clean [34], minimal reward is already sufficient for high-quality tracking. Since OmniRetarget produces such artifact-free, interaction-preserving references, we can follow this minimal formulation directly, achieving faithful tracking of dynamic interactions and zeroshot sim-to-real transfer without any hyperparameter tuning. mask out the pelvis linear position error and velocity. Rewards. To show the benefits of high-quality reference and avoid reward tuning, we use only five reward terms: Body Tracking: DeepMimic-style tracking term for body position, orientation, linear and angular velocity; Object Tracking (where applicable): DeepMimic-style tracking term for object position and orientation; Action Rate: Penalize rapid changes in action; Soft Joint Limit: Penalize robot joint limit violation; Self-Collision: Binary penalty on each body if its selfcollision force exceeds 1 N. We use the same weights and hyperparameters from [33] out of the box without tuning. For object tracking, we use the same hyperparameters as body tracking terms. Termination. We terminate training episodes with large body tracking deviations [33]. For object loco-manipulation, episodes terminate when the object deviates more than 1.0m and 45 from the reference trajectory. We only apply this criterion after the policy achieves reasonable body tracking. Domain Randomization. To improve generalization across object properties for single reference motion, we randomize the objects physical parameters: mass (0.12 kg), center of mass (0.08 m), inertia (50150%), and shape (10%). For the robot specifically, in contrast to the many terms in prior works (e.g., random force injection (RFI), motor PD, action delay), we only apply four terms: Torso COM Position: 0.025 in x, 0.05 in y, 0.075 in z; Joint default position: 0.01 rad; Random push: 0.3 m/s, 0.78 rad/s for (13) s; Fig. 5: Additional hardware results showing diverse, agile and human-like behaviors. Observation noise: 0.05 for orientation in Rot6D, 0.5 m/s and 0.2 rad/s for linear and angular velocity, 0.01 rad and 0.5 rad/s for joint position and velocity. Policy Training. We group similar motions for faster training. All box-moving motions share single multi-task policy, while platform climbing uses one policy per reference. V. EXPERIMENTAL RESULTS In this section, we present comprehensive experimental validation of OMNIRETARGET. We first demonstrate the breadth of complex behaviors enabled by our approach, including natural object manipulation and terrain interaction. We then provide quantitative benchmark against state-ofthe-art baselines, evaluating performance across kinematic quality metrics and downstream policy performance. A. Whole-Body Scene-Object-Interaction a) Agile Loco-Manipulation: OMNIRETARGET enables RL policies to learn agile, whole-body motions for complex scene interactions and loco-manipulation in simulation, culminating in successful zero-shot sim-to-real transfer to hardware. Shown in Fig. 5, policies trained on our data reproduce diverse range of expressive behaviors on Unitree G1 humanoid, including natural box-carrying motions retargeted from the OMOMO dataset, dynamically climbing 0.9mhigh platform (70% of the robots height), and crawling over slopes, showing clean and accurate contact sequences. To showcase the full capabilities of our framework, we present long-horizon, dynamic sequence inspired by the Boston Dynamics Atlas tool-use demo [53]. Visualized in Fig. 1, our retargeted data enables the robot to carry 4.6 kg chair to platform, use it as stepstone to climb up, and then leap off, performing parkour-style roll to absorb the landing impact. This 30-second, complex, multi-stage task highlights OMNIRETARGETs ability to produce precise and versatile reference motions, pushing the boundaries of what is possible for humanoids learning agile, human-like behaviors. We additionally showcase high-dynamic wall-flip motion1 in Fig. 6. The robot completes the full flip in approximately 0.5 second, reaching peak angular velocity of 15 rad/s. Unlike the human foot, which can flex at the arch to maintain contact and generate friction, the robot foot is rigid. As result, it must align more closely to the wall to achieve sufficient contact area and friction. To account for this physical difference and give RL more freedom to learn this skill, we relaxed the termination condition during RL training by increasing the end-effector position error threshold to 0.5 meter (compared to 0.25 meter used in other motions) and removed the foot joint orientation tracking term from the reward function. All other components of the tracking objective remain consistent with other motions. The trained policy is robust and achieves 5/5 success rate in our real-world experiments. b) Sim-to-real with Augmented Data: We show that the augmented motions from our pipeline can be used for training and deployment effectively. As shown in Fig. 4, the interaction mesh formulation allows OMNIRETARGET to generalize single nominal motion into box-picking across shapes and positions, as well as platform climbing at different heights. Notably, these augmented motions transfer to hardware without reward tuning, effectively expanding the repertoire of scenes and behaviors we can achieve in real. In comparison, relying solely on domain randomization which perturbs object shapes and poses only during training performs poorly under our RL formulation, as the policies struggle to explore far beyond the nominal reference. Policies 1The motion is acquired from https://actorcore.reallusion. com/3d-motion?asset=parkour-tic-tac-backflip . An IMU capable of measuring angular rates above 15 rad/s is required for this motion. Fig. 6: Hardware results showing high-dynamic wall-flip motion. The robot reaches maximum linear velocity of 3.5 m/s and peak angular velocity of 15 rad/s. 2) Foot Skating: Quantified by the time duration (normalized by the total desired foot sticking length) and maximum skating velocity of stance foot. 3) Contact Preservation: Quantified by the time duration (normalized by the desired contact length). For robotobject tasks, we measure hand-object contact. For robot-terrain tasks, we measure contact between the robots hands, toes, and heels with the terrain surface. As illustrated in Tab. II, OMNIRETARGET significantly outperforms all baselines across most kinematic metrics. While OMNIRETARGET occasionally incurs minor penetration due to the linearization of constraints (3b) in the sequential SOCP solver, the violations are minimal and can be efficiently fixed by RL. GMR achieves the highest contact preservation score for robot-object this outcome largely reflects its keypoint-matching objective. In practice, scaling human hand keypoints to the robots size often drives the robots hands inside the object, leading to substantial penetration errors (Fig. 7b). Overall, all baselines exhibit significant penetration and foot skating  (Fig. 7)  , degrading the downstream RL performance, as discussed next. interaction tasks; however, For direct comparison on pure locomotion, we retarget motions from the LAFAN1 MoCap dataset [2] and benchmark them against the publicly available Unitree LAFAN1 retargeted dataset [34]. This serves as strong baseline, as it is widely considered high-quality data source for RL-based locomotion training [33]. Table II shows that OMNIRETARGETs motions exhibit fewer physical artifacts, achieving better satisfaction of hard constraints. b) Downstream RL Performance: central observation from prior works [33], [12] is that the quality of retargeted motions strongly influences the performance of downstream RL. To verify this, we select 39 challenging motions for OMNIRETARGET and baselines, and train RL policies using identical hyperparameters from [33] without manual tuning. We evaluate the policies in simulation, and success is measured by training termination criteria. Shown in Tab. II, retargeting quality directly impacts RL success rates. OMNIRETARGET consistently achieves the highest performance across tasks, exceeding baselines by over 10% with lower variance, which indicates more stable learning across different motions. PHC performs better than Fig. 7: Artifacts resulting from the retargeting baselines. trained on our augmented data instead yield reliable success (see video for comparison). Admittedly, additional reward engineering could help, but it contradicts our minimal design goal. Quantitatively, training and evaluating on the full augmented dataset achieves 79.1% success rate, comparable to 82.2% when evaluating on nominal motions only, showing that kinematics augmentation substantially enlarges coverage without significant performance degradation. B. Benchmark Against Prior Retargeting Pipelines We compare OMNIRETARGET against several widely-used open-source retargeting baselines2: PHC [10], GMR [9] and VideoMimic [11]. The generated dataset including 2.78 hours of box carrying in OMOMO, 1 hour of in-house MoCap and 4.6 hours of LAFAN1 will be open-sourced. a) Kinematics Quality: We evaluate the kinematic quality of retargeted motions on Unitree G1 with three criteria: 1) Penetration: Measured by the time duration (normalized by the trajectory length) and maximum depth of intersections between the robot, objects, and terrain. 2Baseline performance may depend on their hyperparameters. We initialized from the default settings in their public codes, and further improved to ensure consistent performance across different tasks. Method Duration Max Depth (cm) Duration Max Vel. (cm/s) Duration Success Rate Penetration Foot Skating Contact Preservation Downstream RL Policy Robot-Object Interaction (Retargeting from the OMOMO Dataset) PHC [10] GMR [9] VideoMimic [11] OMNIRETARGET 0.68 0.21 0.83 0.14 0.60 0.27 0.00 0.01 5.11 3.09 8.50 3.94 7.48 4.95 1.34 0.34 0.05 0.05 0.02 0.01 0.12 0.07 0 1.40 0.80 1.46 0.45 1.50 0.70 Robot-Terrain Interaction (Retargeting from the In-House MoCap Dataset) PHC GMR VideoMimic OMNIRETARGET 0.66 0.36 0.91 0.16 0.83 0.11 0.01 0.02 7.74 4.53 5.72 3.84 5.97 3.58 1.37 0.18 0.15 0.04 0.04 0.05 0.14 0.05 0 2.03 1.83 1.75 3.01 1.85 1.38 Robot-Only (Retargeting from the LAFAN1 Dataset) 0.96 0.09 0.99 0.04 0.77 0.25 0.96 0.09 0.45 0.28 0.67 0.26 0.47 0.25 0.72 0.19 71.28% 22.55% 50.83 % 23.89% 3.85% 8.41% 82.20% 9.74% 52.63% 49.93% 78.94% 40.77% 51.75% 49.23% 94.73% 22.33% Unitree [34] OMNIRETARGET 0.09 0.13 0.00 0.00 3.22 2.64 1.07 0.00 0.06 0.03 0 1.46 0.01 0 N/A N/A 100% 100% TABLE II: Quantitative comparison of kinematic retargeting quality and downstream RL performances. GMR in object manipulation, likely due to lower penetration with sufficient contact preservation, but worse in terrain interaction, where its contact preservation drops by nearly 50%. Specifically for terrain interaction, we see that contact preservation is directly proportional to the success rate. These results suggest that both contact preservation and penetration reduction are critical for generalizing RL policies across diverse tasks, and OMNIRETARGET shows strength in both. VideoMimic shows the weakest interaction preservation among all baselines (Fig. 7c), likely due to its collision avoidance soft cost conflicting with the keypoint matching cost. This is compounded by its coarse collision model originally designed for heightmaps, which is ill-suited for precise loco-manipulation. Consequently, while its terraininteraction results are comparable to PHC, its performance on object manipulation is poor. Although this could be partially attributed to the tuning of its soft penalties, OMNIRETARGET demonstrates that hard-constraint formulation avoids such sensitivities altogether. VI. CONCLUSION In this work, we tackled key data bottleneck caused by lack of high-quality, interaction-aware retargeting pipeline in humanoid whole-body loco-manipulation. We introduced OMNIRETARGET, unified, interaction-preserving data generation engine that leverages an interaction mesh within single constrained optimization. Our experiments showed that OMNIRETARGET significantly outperforms prior methods in kinematic quality, producing diverse set of artifactfree trajectories from single demonstrations. This highquality data enabled proprioceptive RL policy, trained with minimal formulation, to achieve long-horizon dynamic skills on physical humanoid via zero-shot sim-to-real transfer. Ultimately, OMNIRETARGET demonstrates paradigm shift from patching lower-quality reference motions with complex reward engineering to solving the problem at its source with principled data generation. While our current frame-by-frame optimization is mostly effective, future work could explore jointly optimizing the entire trajectory to enhance to noisier motion sources, such as video data, or learning frameworks robustness the autonomous visuomotor policies. By open-sourcing our complete framework and the large-scale dataset of retargeted trajectories, we hope to accelerate progress towards more agile, capable, and versatile humanoid robots."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Li, J. Wu, and C. K. Liu, Object motion guided human motion synthesis, ACM Transactions on Graphics (TOG), 2023. [2] F. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. Pal, Robust motion in-betweening, vol. 39, no. 4, 2020. [3] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, Learning quadrupedal locomotion over challenging terrain, Science robotics, 2020. [4] X. B. Peng, P. Abbeel, S. Levine, and M. Van de Panne, Deepmimic: Example-guided deep reinforcement learning of physics-based character skills, ACM Transactions On Graphics (TOG), 2018. [5] M. Xu, Y. Shi, K. Yin, and X. B. Peng, Parc: Physics-based augmentation with reinforcement learning for character controllers, in Proceedings of the SIGGRAPH Conference Papers, 2025. [6] Z. Wu, J. Li, P. Xu, and C. K. Liu, Human-object interaction from human-level instructions, arXiv preprint arXiv:2406.17840, 2024. [7] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus: Humanoid shadowing and imitation from humans, CoRL, 2024. [8] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. M. Kitani, C. Liu, and G. Shi, Omnih2o: Universal and dexterous human-tohumanoid whole-body teleoperation and learning, in CoRL, 2025. [9] Y. Ze, Z. Chen, J. P. Ara Aˇsjo, Z.-a. Cao, X. B. Peng, J. Wu, and C. K. Liu, Twist: Teleoperated whole-body imitation system, CoRL, 2025. [10] Z. Luo, J. Cao, A. W. Winkler, K. Kitani, and W. Xu, Perpetual humanoid control for real-time simulated avatars, in ICCV, 2023. [11] A. Allshire, H. Choi, J. Zhang, D. McAllister, A. Zhang, C. M. Kim, T. Darrell, P. Abbeel, J. Malik, and A. Kanazawa, Visual imitation enables contextual humanoid control, CoRL, 2025. [12] T. Zhang, B. Zheng, R. Nai, Y. Hu, Y.-J. Wang, G. Chen, F. Lin, J. Li, C. Hong, K. Sreenath et al., Hub: Learning extreme humanoid balance, CoRL, 2025. [13] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He, N. Sobanbabu, C. Pan, Z. Yi, G. Qu, K. Kitani, J. Hodgins, L. J. Fan, Y. Zhu, C. Liu, and G. Shi, Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills, arXiv, 2025. [14] E. S. L. Ho, T. Komura, and C.-L. Tai, Spatial relationship preserving character motion adaptation, ACM Transactions on Graphics, 2010. [15] L. Yang, H. Suh, T. Zhao, B. P. Graesdal, T. Kelestemur, J. Wang, T. Pang, and R. Tedrake, Physics-driven data generation for contactrich manipulation via trajectory optimization, RSS, 2025. [16] T. Cheynel, T. Rossi, B. Bellot-Gurlet, D. Rohmer, and M.-P. Cani, Sparse motion semantics for contact-aware retargeting, in ACM SIGGRAPH Conference on Motion, Interaction and Games, 2023. [17] Y. Kim, H. Park, S. Bang, and S.-H. Lee, Retargeting human-object interaction to virtual avatars, IEEE transactions on visualization and computer graphics, vol. 22, no. 11, pp. 24052412, 2016. [18] M. Gleicher, Retargetting motion to new characters, in Proceedings of the 25th annual conference on Computer graphics and interactive techniques, 1998, pp. 3342. [19] K. Aberman, P. Li, D. Lischinski, O. Sorkine-Hornung, D. Cohen-Or, and B. Chen, Skeleton-aware networks for deep motion retargeting, ACM Transactions on Graphics (TOG), vol. 39, no. 4, pp. 621, 2020. [20] R. Villegas, J. Yang, D. Ceylan, and H. Lee, Neural kinematic networks for unsupervised motion retargetting, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018. [21] Y. Zhang, D. Gopinath, Y. Ye, J. Hodgins, G. Turk, and J. Won, Simulation and retargeting of complex multi-character interactions, in ACM SIGGRAPH 2023 Conference Proceedings, 2023. [22] S. Nakaoka and T. Komura, Interaction mesh based motion adaptation for biped humanoid robots, in Humanoids, 2012. [23] J. Dao, H. Duan, and A. Fern, Sim-to-real learning for humanoid box loco-manipulation, in ICRA. IEEE, 2024. [24] J. Long, J. Ren, M. Shi, Z. Wang, T. Huang, P. Luo, and J. Pang, Learning humanoid locomotion with perceptive internal model, arXiv, 2024. [25] J. He, C. Zhang, F. Jenelten, R. Grandia, M. Bacher, and M. Hutter, Attention-based map encoding for learning generalized legged locomotion, Science Robotics, vol. 10, no. 105, p. eadv3604, 2025. [26] X. He, R. Dong, Z. Chen, and S. Gupta, Learning getting-up policies for real-world humanoid robots, RSS, 2025. [27] Y. Kuang, H. Geng, A. Elhafsi, T.-D. Do, P. Abbeel, J. Malik, M. Pavone, and Y. Wang, Skillblender: Towards versatile humanoid whole-body loco-manipulation via skill blending, arXiv, 2025. [28] Z. Zhang, C. Chen, H. Xue, J. Wang, S. Liang, Y. Liu, Z. Zhang, H. Wang, and L. Yi, Unleashing humanoid reaching potential via real-world-ready skill space, arXiv preprint arXiv:2505.10918, 2025. [29] Y. Xue, W. Dong, M. Liu, W. Zhang, and J. Pang, unified and general humanoid whole-body controller for versatile locomotion, RSS, 2025. [30] C. Zhang, W. Xiao, T. He, and G. Shi, Wococo: Learning whole-body humanoid control with sequential contacts, 2024, arXiv, 2024. [31] Y. Zhang, Y. Yuan, P. Gurunath, T. He, S. Omidshafiei, A.-a. Aghamohammadi, M. Vazquez-Chanlatte, L. Pedersen, and G. Shi, Falcon: Learning force-adaptive humanoid loco-manipulation, arXiv, 2025. [32] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath, Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control, IJRR, 2025. [33] Q. Liao, T. E. Truong, X. Huang, G. Tevet, K. Sreenath, and C. K. Liu, Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion, arXiv e-prints, pp. arXiv2508, 2025. [34] U. Robotics and Contributors, Unitree lafan1 retargeting dataset, https://huggingface.co/datasets/lvhaidong/LAFAN1 Retargeting Dataset, 2025. [35] M. Seo, S. Han, K. Sim, S. H. Bang, C. Gonzalez, L. Sentis, and Y. Zhu, Deep imitation learning for humanoid loco-manipulation through human teleoperation, in Humanoids, 2023. [36] Q. Ben, F. Jia, J. Zeng, J. Dong, D. Lin, and J. Pang, Homie: Humanoid loco-manipulation with isomorphic exoskeleton cockpit, RSS, 2025. [37] X. Zhang, M. Chang, P. Kumar, and S. Gupta, Diffusion meets dagger: Supercharging eye-in-hand imitation learning, in RSS, 2024. [38] S. Tian, B. Wulfe, K. Sargent, K. Liu, S. Zakharov, V. C. Guizilini, and J. Wu, View-invariant policy learning via zero-shot novel view synthesis, in CoRL, 2025. [39] L. Y. Chen, C. Xu, K. Dharmarajan, Z. Irshad, R. Cheng, K. Keutzer, M. Tomizuka, Q. Vuong, and K. Goldberg, Rovi-aug: Robot and viewpoint augmentation for cross-embodiment robot learning, in Conference on Robot Learning (CoRL), 2024. [40] Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Rajeswaran, and V. Kumar, Cacti: framework for scalable multi-task multi-scene visual imitation learning, arXiv preprint arXiv:2212.05711, 2022. [41] Z. Chen, S. Kiami, A. Gupta, and V. Kumar, Genaug: Retargeting behaviors to unseen situations via generative augmentation, RSS, 2023. [42] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, learning with C. Tan, J. Peralta, B. Ichter et al., Scaling robot semantically imagined experience, RSS, 2023. [43] A. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y. Zhu, and D. Fox, Mimicgen: data generation system for scalable robot learning using human demonstrations, in CoRL, 2023. [44] Z. Jiang, Y. Xie, K. Lin, Z. Xu, W. Wan, A. Mandlekar, L. Fan, and Y. Zhu, Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning, ICRA, 2025. [45] C. Garrett, A. Mandlekar, B. Wen, and D. Fox, Skillmimicgen: Automated demonstration generation for efficient skill learning and deployment, in Conference on Robot Learning (CoRL), 2024. [46] S. Starke, H. Zhang, T. Komura, and J. Saito, Neural state machine for character-scene interactions, ACM Transactions on Graphics, vol. 38, no. 6, p. 178, 2019. [47] K. Zakka, Mink: Python inverse kinematics based on MuJoCo, May 2025. [Online]. Available: https://github.com/kevinzakka/mink [48] H. Si and K. Gartner, Meshing piecewise linear complexes by constrained delaunay tetrahedralizations, in Proceedings of the 14th international meshing roundtable. Springer, 2005, pp. 147163. [49] M. Alexa, Differential coordinates for local mesh morphing and deformation, The Visual Computer, vol. 19, no. 2, pp. 105114, 2003. [50] K. Zhou, J. Huang, J. Snyder, X. Liu, H. Bao, B. Guo, and H.-Y. Shum, Large mesh deformation using the volumetric graph laplacian, in ACM SIGGRAPH 2005 Papers. ACM, 2005, pp. 496503. [51] R. Tedrake and the Drake Development Team, Drake: Model-based design and verification for robotics, 2019. [52] B. E. Jackson, K. Tracy, and Z. Manchester, Planning with attitude, IEEE Robotics and Automation Letters, 2021. [53] Boston Dynamics, Atlas Gets Grip, YouTube, available: https:// www.youtube.com/watch?v=-e1 QhJ1EhQ. [54] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black, SMPL: skinned multi-person linear model, ACM Trans. Graphics (Proc. SIGGRAPH Asia), vol. 34, no. 6, pp. 248:1248:16, Oct. 2015."
        },
        {
            "title": "APPENDIX",
            "content": "A. Different Sources of Human Motion Data Human motion datasets contain rich pose and shape information, but they differ both in data format and in the physical attributes (e.g., height, body proportions) of the demonstrators. To make them compatible across different sources and suitable for retargeting, we need to convert these inputs into consistent representation, typically time series of global 3D keypoint positions {psource 0:T,i }. This process must account for differences between human demonstrators and the target robot."
        },
        {
            "title": "The datasets used in this work represent two common",
            "content": "formats: Parametric Human Models: The OMOMO dataset uses the SMPL format [54], parametric model representing human body shape and pose-dependent variations using shape (β ) and pose (q) parameters. Skeleton Hierarchy: Both our in-house MoCap data and the LAFAN1 dataset utilize the skeleton hierarchy defined in the BVH format. Different retargeting pipelines use different strategies to handle these formats. We detail these preprocessing steps below, denoting the human demonstrators pose as qdemo , the SMPL forward model for the i-th keypoint as Mi, the original demonstrator shape as β demo, and the demonstrators i-th keypoint position as pdemo . 1) SMPL Data: To handle data from parametric models like SMPL, methods typically follow one of two strategies: fitting the model to the robots morphology or directly scaling the humans keypoints. t,i a) Model Fitting (PHC, VideoMimic): This strategy fits scaled SMPL model to the robots morphology. PHC first optimizes for an overall scaling factor α, and set of SMPL shape parameters β that best match the robots link length in canonical T-pose, as detailed in Alg. 1. The final source keypoint positions are then generated from this fitted model: t,i = α Mi(qdemo psource ; β ). (6) Algorithm 1 Fit SMPL Shape (PHC) Require: SMPL model M, robot urdf with forward kinematics , qsmpl = 0ns , qrobot = 0nx Ensure: scaling factors α, β 1: α, β 1, 010 2: for iter = 1, . . . , max iter do 3: 4: 5: 6: end for L(α, β ) = α α ηα α β β ηβ β (cid:13)( fi( qrobot) α Mi( qsmpl; β )(cid:13) (cid:13) 2 (cid:13) VideoMimic adopts similar philosophy but integrates the scaling directly into its main retargeting optimization, solving for per-link scale factors jointly with the robots motion. b) Direct Scaling (GMR & OMNIRETARGET): In contrast, GMR and OMNIRETARGET use more direct approach. They generate keypoints from the humans original SMPL parameters β demo and then scale them to the robots proportions. Both methods support detailed morphological adaptation via per-bone scaling factors based on corresponding human-robot link lengths. For simplicity in this work, however, we adopt single global scaling factor α, set to the robot-to-human height ratio: t,i = α Mi(qdemo psource ; β demo), α = hrobot hdemo . (7) 2) Skeleton Hierarchy Data: For formats like BVH, keypoint positions are derived from the skeletons forward kinematics skeleton. This data is then typically scaled to the robots size using the height ratio: psource t,i = hrobot hdemo skeleton (qdemo ). (8) key distinction among methods is their data compatibility. While GMR and OMNIRETARGET are designed to process both parametric model data and raw skeleton hierarchies, frameworks like PHC and VideoMimic are primarily designed for SMPL data. Fitting other data formats to the SMPL format is yet another tedious process. B. Different Kinematic Retargeting Formulations Once human motion is preprocessed into series of source keypoint positions {psource 0:T,i }, different methods formulate the retargeting problem in distinct ways. As summarized in Tab. III, these approaches vary in their optimization strategy and objectives. The following sections detail the mathematical formulation of each baseline method and our proposed approach, OMNIRETARGET. 1) PHC: PHC formulates retargeting as large-scale trajectory-wise optimization problem. It applies gradient descent to minimize the error between the source keypoint positions and the robots keypoint positions over the entire trajectory, as shown in Alg. 2. 2) GMR: GMR performs retargeting by solving an inverse kinematics (IK) problem at each frame (3). At each timestep, GMR finds the robot configuration qt that matches the Algorithm 2 Retarget Robot Motion (PHC) Require: Robot urdf, source keypoint positions {psource 0:T,i } Ensure: q0:T 1: q0:T [0nx ]T 2: for iter = 1, . . . , max iter do 3: (q0:T ) = 4: 5: end for (cid:13) 2 (cid:13) (cid:13) (q0:T ), qmin, qmax) (cid:13) (cid:13) (cid:13) fi(qt ) psource q0:T clamp(q0:T q0:T t=0 t,i source keypoint positions and orientations via the following optimization program: (cid:13) (cid:13) (qt ) psource t,i (cid:13) 2 + (cid:13) (cid:13) (cid:13) θ (cid:13) (qt ) θ source t,i (cid:13) 2 (cid:13) (cid:13) = arg min qt s.t. qmin qt qmax, and θ (9) where are the robot forward kinematics for the i-th keypoints position and orientation, respectively. Leveraging the mink [47] library, GMR solves this program in Sequential Quadratic Programming fashion. Algorithm 3 Retarget Robot Motion (GMR) Require: Robot urdf, source keypoint positions {psource 0:T,i } and orientations {θ source 0:T,i } Ensure: q0:T 1: for = 0, . . . , do 2: 3: end for qt Solve IK (9) 3) VideoMimic: VideoMimic jointly optimizes for the robot motion q0:T and SMPL per-link scaling factor β over the entire trajectory. The primary objective is to preserve the scaled pairwise distance and orientation between each keypoint pair (i, j): Lpairwise = )( fi(qt ) j(qt ))2 2, t,i pdemo βi (pdemo t, t,iN ( j) (10) with soft penalties on foot contact matching Lcontact, foot skating Lskating, collision Lcollision, joint limits Ljoint and temporal smoothness Lsmooth: 0:T , β = arg min q0:T ,β λcl Lcollision + λ Ljoint + λsm Lsmooth + . . . Lpairwise + λc Lcontact + λs Lskate+ (11) Algorithm 4 Retarget Robot Motion (VideoMimic) Require: Robot urdf, demonstrators original keypoint positions {pdemo 0:T,i } Ensure: q0:T , β Solve (11) for the entire trajectory 4) IMMA Multi-stage Optimization: IMMA relies on complex, multi-stage pipeline: first, it optimizes the intermediate robot keypoint positions to warp the interaction mesh Method PHC GMR VideoMimic IMMA Optimization Type Primary Objective Preprocessing Data Formats Trajectory-wise Optimization Per-Frame Optimization Trajectory-wise Optimization Multi-Stage Trajectory-wise Optimization Model Fitting Keypoint Position Matching Direct Scaling Keypoint Position & Orientation Matching Pairwise Distance & Orientation Preservation Model Fitting Interaction Mesh Deformation + IK Unknown SMPL SMPL, BVH SMPL Unknown OMNIRETARGET Per-Frame Optimization Interaction Mesh Deformation Direct Scaling SMPL, BVH TABLE III: Comparison of different retargeting methodologies from the human to the robot with minimal deformation by solving the following program t,i = arg min pti L(psource t,i ) L(pt,i)2 s.t. φ j(qt ) 0, (12) pt,i pt, j2 = li j, bone = pF pF t1, stance foot, where li is the bone length between the i-th and j-th joints. Then, it solves separate IK problem to recover joint angles that best match the intermediate keypoints: fi(qt ) t,i2 2. = arg min (13) qt In later stages, additional hard constraints on the feet and waist are imposed to prevent foot slipping and ensure dynamic balancing. This sequential and fragmented approach produces dynamically consistent motions but fails to consider crucial kinematic constraints like joint and velocity limits. Algorithm 5 Retarget Robot Motion (OMNIRETARGET) Require: Robot urdf, source keypoint positions {psource 0:T,i } Ensure: q0:T 1: for = 0, . . . , do 2: 3: end for qt Solve interaction mesh optimization (3) 5) OMNIRETARGET: OMNIRETARGET, as outlined in Alg. 5, operates frame-by-frame by minimizing the Laplacian deformation of the interaction meshes. The core objective (3a) is flexible and can be augmented with task-specific costs, such as the orientation matching term from GMR, providing unified and extensible framework for motion retargeting. C. Data Augmentation Details 1) Augmented Object Trajectory: To generate perturbed object trajectory, we introduce transient offset that decays exponentially over time. Let the original trajectory be denoted by (pob j(t), θob j(t)). We define an initial positional offset pob and rotational offset θob that are applied at the onset of object motion, tm. The augmented trajectory, ( pob j(t), θob j(t)), is then formulated as: pob j(t) = (cid:40) pob + pob j(0) pob e(ttm)/τp + pob j(t) θob j(t) = (cid:40) θob θob j(0) θob e(ttm)/τθ θob j(t) if < tm if tm if < tm if tm (14a) (14b) Fig. 8: The Laplacian coorinate should stay the same when the object rotates 180. Fig. 9: The actual target (left) and source (right) interaction meshes used for optimization. where τp and τθ are time constants governing the rate of decay for the translational and rotational perturbations, respectively. The operator denotes composition for orientations (e.g., quaternion multiplication). 2) Interaction Mesh Construction in Object Frame: For robot-object interactions, it is crucial to construct the interaction mesh in the objects local coordinate frame. This ensures that the Laplacian coordinates, which encode relative spatial relationships, are invariant to the objects global rotation and translation. As illustrated in Fig. 8, when the object rotates by 180 (indicated by the black arrow), the Laplacian coordinate of the object in the world frame LW changes from (0, 1) to (0, 1), while the Laplacian coordinate calculated in the object frame LO remains constant. Using object-frame coordinates is therefore essential for preserving the intended interaction geometry during object spatial transformation. D. Sequential SOCP Details At each time step t, we iteratively solve Second-Order Cone Program (SOCP) for the optimal change in the robots configuration dq util convergence for up to 10 iterations. For conciseness, we omit the time index for variables within the Sequential SOCP loop. (15f) (15g) Fig. 10: Histograms from the downstream RL evaluation showing the failure patterns for the baselines in different tasks. somewhat better references but still fails on four motions, while OmniRetarget fails on only one. These results show that OmniRetarget not only provides superior robustness under variation but also higher reference accuracy. For the one remaining failure, we believe that it is limited by the simple RL formulation we use. For future work, an interesting direction could be to extend the current RL formulation with curriculum learning to support these extremely difficult motions."
        },
        {
            "title": "The optimization finds the increment dqn at",
            "content": "the n-th iteration. The configuration is updated using this increment, starting from the previous time steps solution ( q0 = t1): qn+1 = qn + dq n. The optimal increment dq SOCP, which is linearized around the current iterate qn: is the solution to the following dq = arg min dqn Lsource (Jn dqn + Ltarget )2 (15a) s.t. + qn + dqn qt12 Jn dqn + φ j( qn) 0, qmin qn + dqn qmax vmin dt qn + dqn qt1 vmax dt (15b) (15c) (15d) (15e) ( qn) + Jn pF dqn = pF t1, stance foot dqn2 ε, where (q))}) t,i Lsource = vec({L(psource )}) Ltarget(q) = vec({L(ptarget t,i = vec({L(ptarget Ltarget ( qn))}) t,i Jn = Ltarget/ qq= qn Jn = φ j/ qq= qn = pF Jn / qq= qn. The second-order cone constraint (15g) is trust region constraint with radius ε (we use ε = 0.2) that keeps the step size small, ensuring the linear approximations remain valid. E. Downstream RL Evaluation Breakdown Shown in Fig. 10, we present histograms from the downstream RL evaluation (Sec. V-B.0.b) to illustrate failure patterns and variance across OmniRetarget and baselines. These histograms break down failure rates by each motion interaction and robotterrain for two tasks: robotobject interaction, highlighting not only overall averages but also how failures distribute across different motions. We do not include augmented motions as baselines do not support augmentation. In robotobject interaction, the motions are modest while object properties are heavily randomized. Since the motions are not aggressive, most policies can adapt even to lowquality references and achieve at least one success, except VideoMimic, which fails systematically due to poor interaction preservation. This task therefore measures robustness rather than accuracy. We see that GMR shows broader failure spread with lower success rates, likely due to penetration issues that reduce robustness under placement changes. PHC shows improved robustness, while OmniRetarget achieves the most robust performance, with results concentrated in the high-success region. In contrast, climbing terrains requires much more agile and challenging motions and thus, demands precise reference motions: if the quality is low, the agent fails outright with no successes. Here, PHC and VideoMimic perform the worst, with nearly half the motions failing entirely. GMR delivers"
        }
    ],
    "affiliations": [
        "Amazon FAR (Frontier AI & Robotics)",
        "CMU",
        "MIT",
        "Stanford University",
        "UC Berkeley"
    ]
}