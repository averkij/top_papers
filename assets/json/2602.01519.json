{
    "paper_title": "You Need an Encoder for Native Position-Independent Caching",
    "authors": [
        "Shiju Zhao",
        "Junhao Hu",
        "Jiaqi Zheng",
        "Guihai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb."
        },
        {
            "title": "Start",
            "content": "You Need an Encoder for Native Position-Independent Caching Shiju Zhao, Junhao Hu, Jiaqi Zheng, Guihai Chen State Key Laboratory for Novel Software Technology, Nanjing University, China School of Computer Science, Peking University, China 6 2 0 2 2 ] . [ 1 9 1 5 1 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. PositionIndependent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-FirstToken (TTFT) by 51-94% and increases throughput by 3 with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) exhibit strong capabilities across wide range of complex tasks, such as automatic software engineering [9, 12, 30], document-question answering [2], and reasoning-intensive problems [36]. LLMs also offer user-friendly interface through natural-language-based promptssequences of tokens. As LLMs capabilities and ease of use expand, their usage patterns have shifted from simple conversational tasks to more complex scenarios, such as multi-turn planning, reasoning, tool use, and few-shot learning. This shift has resulted in long prompts with repeated tokens across requests, such as system messages, few-shot learning examples, and documents, whose content is less changing (i.e., static) than user-specific instructions (being dynamic). Prefix-Based Context Caching (Prefix caching) is an efficient approach that reuses Key-Value (KV) vectors, the intermediate representations of repeated tokens in previous requests, to reduce computation. Prefix caching matches the Figure 1: Position-Independent Caching (PIC) workflow. KVi, KVj, KVk indicate KV vectors in arbitrary order/position. current request against previous ones to reuse the KV vectors of the longest common prefix. Prefix caching remains the dominant approach in existing systems [3,6,11,44], but it requires exact prefix matches across requests, limiting reuse cases in settings such as few-shot learning and Retrieval-Augmented Generation (RAG), where static chunks (e.g., documents) remain unchanged across requests but are preceded by varying prefixes. To address the limitations of prefix caching, PositionIndependent Caching (PIC) [7, 35] enables modular reuse of the KV vectors of static tokens, regardless of their prefixes. Specifically, the PIC technique is two-stage framework analogous to compilation and linking (Figure 1). First, the compile stage involves submitting individual static chunks to the LLM to generate and store their respective KV vectors from position zero. Second, the link stage loads and concatenates needed KV vectors (in any order/position), with optional computations to recover accuracy. PIC significantly increases reuse opportunities, but it deviates from standard attention mechanisms, resulting in potential accuracy degradation; thus, ensuring accurate recovery becomes its main challenge. Subsequent studies largely conform to the PIC framework and can be categorized into two paradigms: post-training 1 PIC and training-aware PIC. Post-training PIC, typically recompute subset of tokens in the link stage to recover accuracy [1, 7, 35, 42, 43]. These approaches are non-intrusive (they do not train the model or modify model architectures) and can be deployed as plug-ins, but they generally suffer from reduced accuracy. Training-aware PIC explicitly trains models to be aware of the PIC usage during the compile stage, thereby achieving near-zero overhead in the link stage [16, 17, 34]. These approaches achieve high accuracy, but they require fine-tuning and thus permanently alter model behavior, which may lead to catastrophic forgetting and performance degradation on other tasks (Figure 6). To address these limitations and combine the advantages of both paradigms, we propose COMB, which reintroduces an encoder plug-in into mainstream decoder-only LLMs and trains only the encoder to be PIC-aware. This design offers three key benefits. First, compared to post-training approaches, COMB achieves the highest accuracy by integrating PIC-specific componentthe encoderand training it explicitly for PIC generation, thereby making PIC native capability of the model, analogous to native sparse attention in DeepSeek [39]. Second, compared to training-aware approaches, COMB provides maximal flexibility: the encoder functions as plug-in whose cross-attention can be entirely removed without affecting the standard decoding workflow. Third, compared to all approaches, despite introducing additional parameters (from the encoder), COMB achieves the lowest Time-To-First-Token (TTFT) due to the efficiency of COMBs encoder-decoder architecture (Section 4). We implement COMB with two components: model component and system component. For the model component, we reintroduce an encoder into standard decoder-only architectures, freeze the decoder parameters, and explicitly train the encoder to accommodate PIC. Input chunks are independently processed by the encoder to generate their KV vectors, and queries over these chunks are supervised using maximum-likelihood objective against ground-truth outputs. Architecturally, the encoder layers are interleaved with the decoder in comb-like manner and perform cross-attention only, motivating the name of COMB. For the system component, we implement PIC management system on top of the existing inference framework, including HuggingFace transformers and vLLM [11], comprising approximately 5K lines of Python code. The code and models that we trained are publicly available online and have been anonymized. Experimental results on LongBench [2] show that COMB achieves the highest accuracy and the lowest TTFT among all evaluated approaches. When cache hits occur, COMB reduces TTFT by up to 94% and improves throughput by up to 3, while matching or exceeding the accuracy of prefixbased attention. Importantly, these gains are achieved without permanently modifying the underlying decoder-only model: COMB remains plug-in mechanism that can be enabled or disabled on demand, reverting to standard prefix caching when PIC is not used. Furthermore, COMB saves memory usage for KV vectors by 75%."
        },
        {
            "title": "2 Background",
            "content": "This section provides primer on transformers, context caching, and its variant, Position-Independent Caching (PIC), along with review of existing PIC approaches."
        },
        {
            "title": "2.1 Autoregressive Generation and KV Cache",
            "content": "The generation process of Large Language Models (LLMs) consists of two distinct stages: the prefill stage and the decode stage. In the prefill stage, the model processes sequence of prompt tokens all at once. It computes the KV vectors for all prompt tokens, stores these vectors in the KV cache, and generates the first output token to initiate the decode stage. The time required to generate the first token is referred to as the Time-To-First-Token (TTFT). In the decode stage, the model iteratively processes each newly generated token. It computes the KV vectors for the new token, appends these vectors to the KV cache, and generates the next token. This process repeats until specified stopping criterion is met."
        },
        {
            "title": "2.2 Position-Independent Caching (PIC)",
            "content": "Context caching techniques can be broadly categorized into prefix-based caching and PIC. In this work, we focus on PIC, which enables the reuse of cached KVs of static prompt segments independent of their positions or surrounding context. Existing PIC approaches generally follow two-stage framework analogous to compilation and linking (Figure 1), as introduced in Section 1. Prior PIC approaches can be further classified into posttraining PIC and training-aware PIC. Post-training PIC approaches, including EPIC [7] and CacheBlend [35], treat the underlying decoder-only model as fixed and recover accuracy through selective recomputation during the link stage. For example, EPIC recomputes initial tokens of each chunk [7], while CacheBlend recomputes subset of tokens with the largest discrepancies [35]. Training-aware PIC approaches explicitly incorporate PIC into model training [16, 17, 34], enabling near-zero-overhead reuse while maintaining high accuracy. For example, BlockAttention [17] enforces PIC by training the model to restrict attention within document blocks, preventing cross-document interactions. KVLink [34] introduces and trains special linking tokens that reestablish effective self-attention across independently cached documents."
        },
        {
            "title": "3 System Overview",
            "content": "We start by elucidating the workflow of COMB serving system as shown in Figure 2, and explain how we render the PIC 2 Figure 2: COMB Overview. intrinsic to the model architecture in the next section. The input of COMB consists of question and possibly none or multiple contexts, which is expected to be reused in an arbitrary position. We let users distinguish between context and query because users know their own needs best. 1 For multiple contexts, COMB looks up whether their PositionIndependent Caches (PICaches1) exist through hash table. 2 For those contexts without caches, chunk processor will generate their KV caches, 3 store the KV caches, and 4 update the hash table. Then the PICaches of these contexts are fetched 5 and transferred to an LLM inference engine (e.g., HuggingFace transformers, vLLM, or SGLang) 6. 7 The question is passed directly to the LLM inference engine, and is responded to with the PICaches 8. The blue and orange rectangles in the lower right corner of the figure represent the self-attention layers and cross-attention layers of LLM, which will be detailed in the next section. Note that COMB can be seamlessly integrated into the existing disaggregated prefill-decode serving paradigm. The chunk processor can be considered the prefill node, while the LLM inference engine can be considered the decode node."
        },
        {
            "title": "4 Model Design",
            "content": "We now describe the architecture of COMB and analyze its computational and memory complexity. As illustrated in Figure 3, COMB resembles classical encoderdecoder model but differs in key aspects. The decoders goal is text generation. During decoding, newly generated tokens are appended to the query sequence and treated identically to the initial query tokens. The encoder is trained to support PIC. Since all decoder parameters are frozen during training, disabling the encoder and the cross-attention layers in Figure 3 recovers the original decoder-only LLM. Throughout, we denote the hidden dimension by d, the document length by ldoc, and the query length by lquery. We 1In this paper, we use the term PIC to refer to the position-independent caching technology, and the term PICache to refer to the actual positionindependent KV cache. 3 Figure 3: Model architecture. present all attention computations in the single-head case; the extension to multi-head attention is standard and does not affect asymptotic complexity."
        },
        {
            "title": "4.1 Self-Attention Layer",
            "content": "The decoder (blue self-attention layers in Figure 3) is taken directly from an existing decoder-only LLM that handles only queries and generated tokens. The encoder (orange selfattention layers in Figure 3) shares the same per-layer structure as the decoder (self-attention plus feed-forward), but has fewer layers than the decoder (e.g., 8 encoder layers interleaved with 32 decoder layers in Figure 3). Its embedding layer inherits from the decoder and remains frozen. The encoder processes document tokens and produces PICaches. Both the encoder and decoder comprise multiple selfattention layers. Let the input of self-attention layer be Rlqueryd. In each layer, we first compute = XWQ, = XWK,V = XWV , (1) where WQ,WK,WV Rdd are the query, key, and value projection matrices. Masked self-attention is given by (cid:18) QK (cid:19) + , = softmax = AV, Rlquerylquery, Rlqueryd, (2) (3) where Rlquerylquery is the causal mask and is the obtained hidden states to be processed by the next module. The remaining feed-forward, residual, and normalization components also follow the standard decoder-only architecture."
        },
        {
            "title": "4.2 Cross-Attention Layer",
            "content": "Encoderdecoder interactions occur in the orange crossattention layers in Figure 3. In these layers, decoder tokens (query tokens or newly generated tokens) attend to the encoder-produced document KVs. Let the output of an encoders self-attention layer be HE Rldocd, and the output of the previous decoders self-attention layer be Rlqueryd. The query, key, and value are given by Qcross = DW cross , Kcross = HEW cross cross = HEW cross , , Qcross Rlqueryd, Kcross Rldocd, cross Rldocd, (4) (5) (6) ,W cross Rdd are the query, key, value where cross projection of the orange cross-attention in Figure 3. Crossattention then computes ,W cross Across = softmax (cid:18) Qcross(Kcross) (cid:19) , Across Rlqueryldoc , (7) Hcross = AcrossV cross, Hcross Rlqueryd. (8) The cross-attention output Hcross is passed through the standard decoder pathway via residual connections inside the decoder blocks. There are two options to implement PIC: (1) store HE , or (2) store Kcross and cross. The first option saves storage space, but introduces computational overhead after the PICache is loaded. As result, we store (Kcross,V cross) for efficiency in the current design. The KV vectors form the document-side representations which are stored and reused across requests by COMB. The PICaches of multiple documents can be generated independently from position zero during the compile stage and later concatenated along the ldoc during inference (link stage). Decoder self-attention operates only on query tokens, with per-layer cost O(l2 query) and total TD,self = O(cid:0)LD l2 (cid:1). (11) query Each cross-attention layer (orange blocks in Figure 3) mixes lquery decoder queries with ldoc document KVs, for per-layer cost O(lquery ldoc) and total Tcross = O(cid:0)Lcross lquery ldoc where Lcross LD is the number of cross-attention layers (e.g., Lcross = 8 interleaved layers in Figure 3). The overall attention cost of COMB is (12) (cid:1), Tcomb = O(cid:0)LE doc + LD l2 query + Lcross lquery ldoc (cid:1). (13) Since LE and Lcross are less than LD, Tcomb < Tdec-only. Two things are worth noting. First, in practice, each document is independently prefilled by the encoder. When prefilling documents with lengths ldoc,1, . . . , ldoc,m such that ldoc = i=1 ldoc,i, the encoder prefill cost scales as doc,i rather than l2 doc. Second, modern inference engines already employ chunked prefilling for long prompts by processing them in smaller segments iteratively. As result, the initial PIC compilation (cold start) of new document typically aligns with this existing procedure and, in cache-miss scenarios, does not introduce noticeable overhead beyond that of the baseline system. doc, and we have doc,i l2 i=1 l2 i=1 l2 Memory Complexity. We now compare the memory required for KV vectors. In the standard decoder-only architecture, each of the LD layers stores KVs for all tokens, for total KV memory Mdec-only = O(cid:0)LDNd(cid:1) = O(cid:0)LD(lquery + ldoc)d(cid:1). (14) In COMB, encoder KV vectors are stored for document"
        },
        {
            "title": "4.3 Complexity Analysis",
            "content": "tokens across LE layers, with memory ME = O(cid:0)LE ldocd(cid:1). We now compare the computational and memory complexity of COMB with standard decoder-only architecture. Let = ldoc + lquery denote the total sequence length. Decoder KV vectors are stored only for query tokens, MD = O(cid:0)LD lqueryd(cid:1). (15) (16) (17) Thus, the total KV memory of COMB is Mcomb = O(cid:0)LE ldocd + LD lqueryd(cid:1). Since LE < LD, Mcomb < Mdec-only. Finally, LE can be chosen in the range [1, LD]. Larger LE increases expressivity and potentially accuracy, while even LE = LD does not exceed the time or memory complexity of decoder-only model. In our experiments, we set LE = 8 to balance accuracy and computational resources (Section 6). Moreover, under our limited GPU resources, this configuration avoids out-of-memory errors and accelerates training by enabling larger batch sizes due to reduced parameter counts, optimizer states, and activation memory. Time Complexity. standard decoder-only architecture with LD layers (e.g., LD = 32) applies self-attention to all tokens in each layer, yielding per-layer complexity O(N2) and total Tdec-only = O(cid:0)LDN2(cid:1) = O(cid:0)LD(lquery + ldoc)2(cid:1). (9) In COMB, we use LD decoder layers and LE encoder layers (e.g., LD = 32, LE = 8 in Figure 3). The encoder attends only over document tokens, with per-layer cost O(l2 doc) and total TE = O(cid:0)LE l2 doc (cid:1). (10) 4 Table 1: Model size and training overhead for COMB. MODEL NAME # OF FROZEN PARAMETERS # OF LEARNABLE PARAMETERS GPU HOURS (A100 * HOUR) COMBLLAMA COMBDEEPSEEK 8.0 3.5 2966 15.7 3."
        },
        {
            "title": "5 Implementation",
            "content": "This section describes how we instantiate, train, and deploy COMB. We first detail the model information, then summarize the training data and recipe, and finally discuss how COMB is integrated into production serving system."
        },
        {
            "title": "5.1 Models",
            "content": "We implement COMB by augmenting frozen decoder-only LLM with trainable encoder described in Section 4. First, the decoder is directly adopted from existing open-source models. Specifically, we use two open-source LLMs: Llama-3.1-8BInstruct [18] and DeepSeek-V2-Lite-Chat2 as the decoder backbones. Llama represents the most widely used architecture in the current LLM ecosystem, whereas DeepSeek departs from the standard KV-cache design by employing Multi-head Latent Attention (MLA) for improved memory efficiency. Together, these models cover diverse architectural choices and training paradigms. Second, we integrate an encoder into the decoder via cross-attention layers, where only the encoder parameters and the query-side projections in the cross-attention layers are trainable. The resulting models are referred to as COMBLLAMA and COMBDEEPSEEK, respectively. Table 1 summarizes the parameter counts and training overhead. We train all models using the AdamW optimizer with linear warmup followed by cosine decay. Unless otherwise stated, gradient clipping and weight decay follow standard practices for large-scale language model fine-tuning. Additional implementation details are provided in the released code."
        },
        {
            "title": "5.2 Training Data and Recipe",
            "content": "COMB is trained to emulate the behavior of the underlying decoder-only model under Position-Independent Caching. We select SQuAD [27], Natural-Instructions [21], XSum [22], and Super-Natural-Instructions [32] as training datasets. Each training example is constructed as tuple (D, Q,Y ), where is (possibly multi-document) static context, is user query or instruction, and is the ground-truth response. During training, each document in is first independently en2https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat coded by the encoder to obtain document-side KVs (KE ,V ), and the frozen decoder then generates outputs conditioned on (KE ,V ) and query tokens. We optimize token-level cross-entropy loss between the decoder outputs and the target sequence , using teacher forcing. The ground-truth sequence is generated by Llama-3.18B-Instruct given (D, Q) as input, resulting in on-policy training for the Llama-based model. For DeepSeek, whose native generations are of lower quality, we also adopt Llamagenerated outputs as supervision, which substantially improves its accuracy (Figure 7b). All models are trained on four NVIDIA A100 80GB GPUs with tensor parallelism set to 4."
        },
        {
            "title": "5.3 System Details",
            "content": "We build COMB system on top of existing LLM inference frameworks (e.g., huggingface transformers and vLLM [11]), with 5K lines of code in Python. COMB consists of four key components: PIC manager, PIC allocator, Chunk processor, and Inference engine. When the system starts, the PIC manager reserves portion (specified by the operator) of GPU memory, initializes the PIC allocator, and launches the Chunk processor and the Inference engine. After initialization, the PIC manager will serve users requests, which consist of query and several documents (possibly none or multiple). First, the documents will be looked up to check if their PICaches exist. If not, the PIC allocator will allocate free memory for their PICaches, and the Chunk processor computes the encoder part for them. Then the PICaches are stored asynchronously. After collecting the PICaches, the PIC manager will send their handle to the Inference engine through the CUDA IPC (Inter-Process Communication) API. This enables tensors to be passed in place between processes. Finally, the Inference engine generates response using the query and the related PICache."
        },
        {
            "title": "6 Evaluation",
            "content": "We begin by describing the experimental setup, including datasets, models, evaluation metrics, and software/hardware environment. We then present our key evaluation results."
        },
        {
            "title": "6.1 Experimental Setup",
            "content": "Dataset. Following CacheBlend [35] and EPIC [7], we evaluate on the following LongBench datasets [2]: 2WikiMQA & HotpotQA (multi-document question answering), MuSiQue (long-document question answering), SAMSum (few-shot instruction following), and MultiNews (multi-document summarization). All datasets contain 200 test cases, with the distribution of prompt (prefill) lengths and answer (decode) lengths shown in Figure 4. Static tokens constitute approximately 5 (a) Prompt length distribution (b) Answer length distribution Figure 4: Prefill and decode length distribution. 95%-99% of the prompt, while dynamic tokens are fewer than 50. Metrics. We use the following three metrics to evaluate performance and model accuracy. First, Time-To-First-Token (TTFT) [11] (lower is better) is used to evaluate all datasets. This metric measures the prefill-stage time: the time from when users send request to when users receive the first token; this time could be reduced by using context caching. Second, F1 score [2] (higher is better) is used to evaluate 2WikiMQA, MuSiQue, and HotpotQA. This metric measures the similarity between LLMs output and the ground-truth answer based on their common words. Third, Rough-L score [13] (higher is better) is used to evaluate SAMSum and MultiNews. This metric measures the similarity between LLMs output and the ground-truth answer by calculating the length of their longest common subsequence. Baselines. We compare COMB against four representative approaches: Prefix caching, CacheBlend [35], EPIC [7], and BlockAttention [17]. Prefix caching follows the standard attention mechanism; EPIC and CacheBlend are post-training PIC approaches, while BlockAttention is training-aware PIC approach. We set the recomputation ratio of CacheBlend to 20%, and set the number of recomputed tokens per document to 64 for EPIC. Environment. We run experiments on single NVIDIA A100 server with four A100-80GB GPU available. The server has 128-core Intel(R) Xeon(R) Platinum 8358P CPU@2.60GHz with 2 hyperthreading and 1TB DRAM. We use Ubuntu 20.04 with Linux kernel 5.16.7 and CUDA 12.6. The Inference engine that we use for evaluations is vLLM 0.12.0."
        },
        {
            "title": "6.2 End-to-End Accuracy under PIC",
            "content": "We first examine whether COMB preserves the generation quality of the underlying decoder-only models when PIC is enabled. Figures 5b and 5c report F1 and Rouge-L scores for Llama-3.1-8B-Instruct, and Figures 7b and 7c report the same (a) TTFT upon cache hit (b) F1 score (c) RougeL score Figure 5: Comparison of TTFT ( Better) and accuracy ( Better) using Llama-3.1-8B-Instruct. Note that the TTFT of BlockAttention is not shown because it is not integrated with vLLM currently. metrics for DeepSeek-V2-Lite-Chat. Across all datasets and both model families, COMB attains high accuracy among the evaluated PIC-based approaches. We do not treat absolute accuracy as primary contribution, as the comparison is not fully controlled. In particular, COMB is explicitly trained to be PIC-aware, whereas EPIC is training-free, and other baselines fine-tune fewer parameters for fewer steps. Rather, these results substantiate our central claim: in PIC usage scenarios, incorporating PICaware component into the model architecture and training it accordingly enables the model to recoverand in some cases exceedthe accuracy of standard prefix-based attention. The strong and consistent performance of COMB across datasets and across two architecturally distinct model families (Llama and DeepSeek with MLA) thus provides empirical evidence for the effectiveness of PIC-aware training. Two additional observations are worth noting. First, despite being training-free, EPIC achieves unexpectedly strong performance on the SAMSum dataset, in some cases surpassing the prefix-caching baseline. This behavior has been analyzed in detail in the EPIC study [7], and our results are consistent with their findings. Second, COMBDEEPSEEK attains substantially higher accuracy than its underlying prefix-caching baseline because the base DeepSeek model produces lowerquality outputs; using higher-quality responses generated by Llama as supervision during training (as discussed in Section 5) leads to significant accuracy improvement. 6 (a) F1 score (b) RougeL score (a) TTFT upon cache hit Figure 6: Comparison of F1 score and RougeL score between using separated context as input to BlockAttention and not using it."
        },
        {
            "title": "6.3 Non-Intrusiveness of COMB",
            "content": "We emphasize that COMB is non-intrusive. In practical or industrial settings, the encoder can be enabled or disabled on demand. When the encoder is disabled, COMB reduces exactly to the underlying decoder-only model and recovers its original prefix-caching behavior and accuracy, without any degradation (just as the prefix caching shown in Figure 6). In contrast, approaches such as BlockAttention are inherently intrusive. When PIC is not desired, and multiple documents are directly fed into such models, accuracy can collapse. The reason is that BlockAttention is explicitly trained to restrict attention within predefined document blocks. If users do not carefully chunk inputs and apply block attention in its intended manner, the models assumptions are violated, leading to severe performance degradation (Figure 6a and Figure 6b, blue bars vs. orange bars)."
        },
        {
            "title": "6.4 TTFT under Cache Hits and Misses",
            "content": "We next analyze TTFT with and without PICaches. For each sample in the test set, we first feed it into the inference system and record the TTFT for cache misses as shown in Figure 8. Then, we modify the samples instruction to simulate another request with the same context but different prompt, thereby recording the TTFT for cache hits as shown in Figures 5a and 7a. As expected, all PIC approaches substantially reduce TTFT under cache hits, since most document-side computation is amortized across requests. Among existing PIC approaches, EPIC attains particularly low TTFT because it recomputes only several tokens at each chunk boundary during the link stage, incurring minimal marginal overhead. Training-aware approaches such as BlockAttention offer algorithmic advantages, but in practice have not been integrated into production-grade inference engines (e.g., vLLM), and thus are typically evaluated only with vanilla HuggingFace runtime. Therefore, the TTFT of BlockAttention is significantly higher than other approaches, and we do not show it in the figures. (b) F1 score (c) RougeL score Figure 7: Comparison of TTFT ( Better) and accuracy ( Better) using DeepSeek-V2-Lite-Chat. Note that BlockAttention is not shown because it was not trained on DeepSeek-V2-LiteChat. COMB achieves the lowest TTFT due to the lightweight encoder architecture and the time complexity analyzed in Section 4: encoder self-attention is applied only to document tokens, decoder self-attention only to query tokens, and crossattention couples them in O(lquery ldoc) time. PICaches for each document are compiled once and reused across subsequent requests, so when caches hit, the decoder only needs to mix relatively short query sequences with precomputed document PICaches via cross-attention. When caches miss, COMB performs cold-start compilation to generate PICaches for new documents. Even in this regime, TTFT remains competitive with or better than the decoder-only baseline for two reasons. First, the encoder is substantially shallower than the decoder and is applied only to static document tokens, so the additional prefill cost is modest. Second, production-level inference engines such as vLLM already employ chunked prefill: instead of processing extremely long prompts in single pass, they split inputs into manageable segments and prefill them incrementally to avoid large intermediate activations and out-of-memory errors. PIC compilation in COMB naturally aligns with this chunked prefill behavior, so much of the cold-start work is overlapped with computation that the baseline system would perform regardless. As result, COMB provides low TTFT in both cache-hit and cache-miss scenarios while enabling accurate, model-aware PIC. 7 Figure 8: TTFT without KV cache using Llama-3.1-8B. Figure 9: The KV Cache Size of 1,024 tokens."
        },
        {
            "title": "6.5 Online Latency and Throughput under In-",
            "content": "creasing Load Finally, we evaluate COMB under increasing request rates in an online serving setting. Figures 10a and 10b report TTFT and throughput, respectively, as the number of concurrent users and, correspondingly, the amount of GPU memory used by position-independent caches increase. Across the evaluated load range, COMB maintains the lowest TTFT while simultaneously achieving the highest sustained throughput among all approaches. These gains are closely tied to the memory complexity in Section 4. In standard decoder-only architecture, KV memory scales with the entire prompt length (documents plus query), quickly exhausting GPU HBM and limiting concurrency. By contrast, COMB stores encoder KVs only for static document tokens across small number of encoder layers, and decoder KVs only for the much shorter query sequence. This lightweight encoder design reduces per-request KV memory, freeing HBM capacity that can instead be used to batch more requests or admit more users. As shown in Figure 9, COMB saves KV memory by 75% for Llama-3.1-8B-Instruct, and by 78% for DeepSeek-V2-Lite. Taken together with the accuracy results above, these findings show that native, PIC-aware training delivers favorable accuracylatencythroughput trade-off in realistic serving scenarios."
        },
        {
            "title": "7.1 Why does COMB Work?",
            "content": "Our journey began with optimizing PIC accuracy, driven by the goal of integrating PIC as native architectural feature rather than an afterthought. This led us to an encoderdecoder design leveraging cross-attentiona conviction we held firmly even before committing substantial resources to large-scale training. (a) TTFT (b) Throughput Figure 10: The performance of COMB as the request rate increases. This approach rests on two foundational observations. First, the Transformer architecture was originally conceived as an encoder-decoder framework [29]. The encoders inherent strength in comprehension tasks makes it ideally suited to process arbitrarily ordered contexts, perfectly complementing the decoders generation capabilities. Second, this paradigm has already been validated in multimodal LLMs such as Whisper [26] and Mllama [19]. Multimodal tasks are inherently more challenging than pure language tasksrequiring the interpretation of text potentially embedded within images and complex visual reasoning. If this architecture proves effective for such demanding multimodal scenarios, its applicability to pure language modeling follows naturally."
        },
        {
            "title": "7.2 The Potential of PIC",
            "content": "We have now entered the era of AI Agents. What is an Agent, and how does it differ from an LLM? widely accepted view holds that Agents can formulate short-term actions based on long-term goals and are capable of using tools. Consequently, retrieval emerges as crucial capability for Agents. Beyond consulting references to gather information, Agents must also retrieve which tools are best suited for specific tasks. Whenever retrieval is involved, PIC becomes essential, because retrieved items can arrive in arbitrary order under various permutations and combinations. Using prefix caching in this scenario proves highly inefficient: essentially only the first item can reuse the KV cache, while all subsequent items fail to do so. We can address this by equipping the Agent with an Encoder: all retrieved contentarriving in arbitrary orderis fed into the Encoder, while the question and the models reasoning remain in the Decoder. When the model needs to retrieve new information, previously retrieved content is discarded and replaced with new items in the Encoder. Imagine how powerful such an Agent would be if the Decoders 128K context window contained only the question and 8 the models own chain of thought, free from noisy reference materials."
        },
        {
            "title": "8 Related Work",
            "content": "Our work is unique in proposing native position-independent caching. In this section, we briefly navigate the whole design space. LLM Serving Optimizations. Several serving systems have emerged in recent years. vLLM [11] is pioneering work in this space featuring PagedAttention for higher throughput. SGLang [44] is another serving system featuring novel frontend language and backend runtime. Aside from full systems, there are also many scheduling optimizations, such as disaggregated prefill and decode [23, 24, 45], continuous batching [37], speculative decoding [20], etc. Context Caching (CC). CC has two main categories. The first is position-dependent caching, which can be further divided into prefix-based caching [15, 38, 44] and relative position-dependent caching, such as PromptCache [5]. By mid-2024, vendors such as Gemini [6] and DeepSeek [3] began incorporating explicit prefix-based CC features into their systems. The second category is Position-Independent Caching (PIC) [7, 16, 17, 35]. Although EPIC [7] and CacheBlend [35] address initial aspects of PIC, COMB advances the paradigm by rendering PIC intrinsic to the model architecture. Post-Training vs. Training-Aware Approaches. recurring trajectory in LLM research involves initially validating hypotheses via post-training adjustments before integrating them into the model training process. For instance, in the realm of attention sparsity, researchers first explored inference-time heuristics such as H2O [41], Quest [28], and RaaS [8]. Following the validation of these sparsity patterns [10], DeepSeek introduced Native Sparse Attention (NSA) [39] to incorporate sparse structures directly into the model. This progression from post-training experimentation to training-aware design is also evident in quantization [14, 33], pruning [31, 40], speculative decoding [4], and attention sink [25]. The underlying principle is that once an inductive biaswhether sparsity or position-independent cachingproves beneficial post-training, it merits integration into the models native architecture. While EPIC [7] and CacheBlend [35] represent post-training approaches, BlockAttention [17], KVLink [34], TurboRAG [16], and COMB fall under the training-aware category. Notably, COMB combines the best of both worlds, achieving the accuracy benefits of training-aware approaches while retaining the non-intrusive, plug-in flexibility of post-training approaches. 9 In this paper, we have proposed native PIC by reintroducing an encoder into decoder-only LLMs and explicitly training it for PIC-compatible generation. We have also presented COMB, PIC-aware caching system integrated with existing inference frameworks. Extensive experiments demonstrate that COMB achieves the highest accuracy under PIC while substantially improving inference efficiency, reducing TTFT by up to 94% and increasing throughput by up to 3. In addition, COMB remains plug-in mechanism that can be enabled or disabled on demand, reverting to standard prefix caching when PIC is not used."
        },
        {
            "title": "References",
            "content": "[1] Shubham Agarwal, Sai Sundaresan, Subrata Mitra, Debabrata Mahapatra, Archit Gupta, Rounak Sharma, Nirmal Joshua Kapu, Tong Yu, and Shiv Saini. Cachecraft: Managing chunk-caches for efficient retrievalaugmented generation. Proc. ACM Manag. Data, 3(3), June 2025. [2] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long In Proceedings of the Sixtycontext understanding. Second Annual Meeting of the Association for Computational Linguistics, pages 31193137, 2024. [3] Deepseek. Deepseek api introduces context caching on disk, cutting prices by an order of magnitude. https: //api-docs.deepseek.com/news/news0802, 2024. [4] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. [5] In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong. Prompt cache: Modular attention reuse for low-latency inference. Proceedings of Machine Learning and Systems, 6:325338, 2024. [6] Google. Gemini context caching. https://ai.google. dev/gemini-api/docs/caching, 2026. [7] Junhao Hu, Wenrui Huang, Haoyi Wang, Weidong Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, and Tao Xie. EPIC: efficient position-independent caching for serving large language In Proceedings of the 42nd International models. Conference on Machine Learning, pages 2439124402, 2025. [8] Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, and Yizhou Shan. RaaS: Reasoning-aware attention sparsity for efficient llm reasoning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, pages 25772590, 2024. [9] Junhao Hu, Chaozheng Wang, Hailiang Huang, Huang Luo, Yu Jin, Yuetang Deng, and Tao Xie. Predicting compilation resources for adaptive build in an industrial setting. In Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering, pages 18081813, 2023. [10] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context arXiv preprint llms via dynamic sparse attention. arXiv:2407.02490, 2024. [11] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [12] Qingyuan Liang, Zeyu Sun, Qihao Zhu, Junhao Hu, Yifan Zhao, Yizhou Chen, Mingxuan Zhu, Guoqing Wang, and Lu Zhang. Directional diffusion-style code editing pre-training. IEEE Trans. Software Eng., 51(9):2583 2600, 2025. [13] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [14] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, WeiMing Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: activationaware weight quantization for on-device LLM compression and acceleration. In Proceedings of the 7th Annual Conference on Machine Learning and Systems, 2024. [15] Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, et al. Cachegen: Fast context loading for language model applications. arXiv preprint arXiv:2310.07240, 2023. [16] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang. TurboRAG: Accelerating retrievalaugmented generation with precomputed KV caches for chunked text. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 65886601, Suzhou, China, November 2025. Association for Computational Linguistics. [17] Dongyang Ma, Yan Wang, and Tian Lan. Blockattention for efficient prefilling. In The Thirteenth International Conference on Learning Representations, 2025. 10 [18] Meta. Introducing Llama 3.1: Our most capahttps://ai.meta.com/blog/ ble models to date. meta-llama-3-1/. [19] Meta AI. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models, sep 2024. [20] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating large language model serving with tree-based speculative inference and In Proceedings of the 29th ACM Interverification. national Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS 24, page 932949, New York, NY, USA, 2024. Association for Computing Machinery. [21] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In ACL, 2022. [22] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topicaware convolutional neural networks for extreme summarization. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 17971807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. [23] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 118132, 2024. [24] Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Feng Ren, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: Trading more storage for less computation - KVCache-centric architecture for serving LLM chatbot. In Proceedings of the 23rd USENIX Conference on File and Storage Technologies, pages 155170, 2025. [25] Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 11 [26] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. [27] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for SQuAD. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia, July 2018. Association for Computational Linguistics. [28] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. QUEST: query-aware sparsity for efficient long-context LLM inference. In Proceedings of the 41st International Conference on Machine Learning, pages 4790147911, 2024. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, In and Illia Polosukhin. Attention is all you need. Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 60006010, Red Hook, NY, USA, 2017. Curran Associates Inc. [30] Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng. How practitioners expect code completion? In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 12941306, 2023. [31] Yite Wang, Dawei Li, and Ruoyu Sun. NTK-SAP: improving neural network pruning by aligning training In Proceedings of the 11th International dynamics. Conference on Learning Representations, 2023. [32] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP, 2022. [33] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 3808738099, 2023. [34] Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, and Shiyu Chang. KVLink: Accelerating large language models via efficient KV cache reuse. In The Thirty-ninth Annual analysis of parallel context encoding with full-attentionbased pre-trained language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98409855, Vienna, Austria, July 2025. Association for Computational Linguistics. [43] Shiju Zhao, Junhao Hu, Rongxiao Huang, Jiaqi Zheng, and Guihai Chen. Mpic: Position-independent multimodal context caching system for efficient mllm serving, 2025. [44] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. SGLang: Efficient execution of In The Thirtystructured language model programs. eighth Annual Conference on Neural Information Processing Systems, 2024. [45] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. DistServe: Disaggregating prefill and decoding for goodputIn 18th optimized large language model serving. USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193210, Santa Clara, CA, July 2024. USENIX Association. Conference on Neural Information Processing Systems, 2025. [35] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, and Junchen Jiang. Cacheblend: Fast large language model serving for rag with cached knowledge fusion. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 94109, New York, NY, USA, 2025. Association for Computing Machinery. [36] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In The 11th International Conference on Learning Representations, 2023. [37] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for Transformer-Based generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521538, Carlsbad, CA, July 2022. USENIX Association. [38] Lingfan Yu and Jinyang Li. Stateful large language model serving with pensieve. arXiv preprint arXiv:2312.05516, 2023. [39] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, pages 2307823097, 2025. [40] Zeru Zhang, Jiayin Jin, Zijie Zhang, Yang Zhou, Xin Zhao, Jiaxiang Ren, Ji Liu, Lingfei Wu, Ruoming Jin, and Dejing Dou. Validating the lottery ticket hypothesis with inertial manifold theory. In Proceedings of the 35th Annual Conference on Neural Information Processing Systems, pages 3019630210, 2021. [41] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Proceedings of the 37th Annual Conference on Neural Information Processing Systems, pages 3466134710, 2023. [42] Zhisong Zhang, Yan Wang, Xinting Huang, Tianqing Fang, Hongming Zhang, Chenlong Deng, Shuaiyi Li, and Dong Yu. Attention entropy is key factor: An"
        }
    ],
    "affiliations": [
        "School of Computer Science, Peking University, China",
        "State Key Laboratory for Novel Software Technology, Nanjing University, China"
    ]
}