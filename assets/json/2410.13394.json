{
    "paper_title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
    "authors": [
        "Sumanth Doddapaneni",
        "Mohammed Safi Ur Rahman Khan",
        "Dilip Venkatesh",
        "Raj Dabre",
        "Anoop Kunchukuttan",
        "Mitesh M. Khapra"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area."
        },
        {
            "title": "Start",
            "content": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs Sumanth Doddapaneni*1,2 Mohammed Safi Ur Rahman Khan*1,2 Dilip Venkatesh1 Raj Dabre1,2,4 Anoop Kunchukuttan1,3 Mitesh M. Khapra1,2 1Nilekani Centre at AI4Bharat 2IIT Madras 3Microsoft 4National Institute of Information and Communications Technology, Kyoto, Japan Correspondence: {sumanthd, miteshk}@cse.iitm.ac.in, safikhan@ai4bharat.org huggingface.co/CIA-Suite github.com/CIA 4 2 0 2 7 1 ] . [ 1 4 9 3 3 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Evaluating machine-generated text remains significant challenge in NLP, especially for nonEnglish languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (HERCULE) and novel test set (RECON) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, HERCULE, is cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that HERCULE aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of crosslingual evaluation using LLMs, presenting scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area."
        },
        {
            "title": "Introduction",
            "content": "Evaluating machine-generated text has long been central challenge in Natural Language Processing (NLP). Substantial progress has been made in English-language evaluations, using (i) automated metrics (Papineni et al., 2002; Lin, 2004; *Equal Contribution. 1 Figure 1: We present cross-lingual Evaluator LLM, HERCULE, where the Instruction & Response provided to the model are in the target language, while all other fields are in English. The model generates feedback & score in English for given evaluation example. Rei et al., 2020; Sellam et al., 2020), (ii) human evaluations (Watts et al., 2024; Chiang et al., 2024), and (iii) more recently, automated evaluations using Large Language Models (LLMs) (Zheng et al., 2023a; Kim et al., 2023b; Dubois et al., 2023; Li et al., 2023). However, comprehensive framework for multilingual evaluation which goes beyond English remains missing. This gap is largely due to the absence of robust multilingual benchmark covering complex open-ended tasks and reliable evaluation metric. Time and again, benchmarks have proven essential in driving progress in NLP (Rajpurkar et al., 2016; Hu et al., 2020a; Wang et al., 2018; Conneau et al., 2018). This leads us to our first observation: {A} there is an urgent need to develop robust multilingual benchmark and an easy-to-use evaluation framework to further advance the field. Even if one develops comprehensive benchmark that encompasses wide variety of generation tasks and instruction-following capabilities, the challenge of achieving quick and automated evaluation persists. Traditionally, human evaluations have been the most reliable method for assessing models. However, as these models become more sophisticated and creative, it has become difficult for non-experts to evaluate model outputs accurately. Many non-experts often depend on superficial indicators of correctness, resulting in human evaluations that have devolved into mere vibe checks1 (Chiang et al., 2024), wherein, evaluators rely on personal biases rather than objective criteria to determine winner. This limitation has led to the growing adoption of LLMs as evaluators, given that LLMs often possess more extensive knowledge bases than human evaluators. While LLM-based evaluation comes with its own challenges (Doddapaneni et al., 2024; Zeng et al., 2023; Zheng et al., 2023a), LLMs offer faster, cheaper, and, in certain contexts, more reliable assessments compared to human evaluations. Grounded in this modern reality, we focus on framework for LLM based cross-lingual evaluation. To begin with, we make few observations. First, prior works (Kim et al., 2023b, 2024b; Wang et al., 2023b) have shown that trained evaluators significantly outperform untrained ones, matching the performance of proprietary LLMs. {B} Hence, it is prudent to build trained crosslingual LLM evaluator. Second, within the space of trained evaluators, some studies (Doddapaneni et al., 2024; Kim et al., 2023a) have shown that referencebased approaches (Kim et al., 2023a) where the evaluator model is provided with reference answer are more accurate and reliable than referencefree approaches (Hada et al., 2023; Zheng et al., 2023b). The latter solely rely on their parameterized knowledge and hence do not produce very reliable evaluations. {C} We, thus focus on building reference-based evaluator LLM. However, such references are scarcely available for non-English languages but available in abundance for English. {D} We thus make case for cross-lingual evaluation, wherein responses generated in non-English language are assessed using reference answer available in English. Based on A, B, and D, we introduce the Cross LIngual Auto Evaluation (CIA) Suite, an extensible framework of evaluator LLMs and datasets designed for multilingual evaluation. In this setup, 1hf.co/blog/clefourrier/llm-evaluation both the questions and responses are provided in the target language, while the reference answers, evaluation instructions, and rubrics remain in English, facilitating cross-lingual evaluation (ref. Fig. 1). We also introduce the RECON test set, human-annotated, multi-purpose benchmark spanning six languages: Bengali, German, French, Hindi, Telugu, & Urdu. This test set aims to benchmark general-purpose multilingual LLMs across various tasks and to meta-evaluate the Evaluator LLMs. We construct the INTEL training set by automatically translating the Feedback-Collection dataset (Kim et al., 2023b). Finally, we release HERCULE,A series of cross-lingual, referencebased Evaluator LLMs fine-tuned on INTEL using the Llama-3.1-8B model series. We present the results of our evaluation on the RECON test set, highlighting the improved performance of fine-tuned models over their zero-shot counterparts (Sec. 5.1). Our findings demonstrate that models trained on INTEL not only outperform large, proprietary LLMs but also exhibit greater alignment with human judgments, particularly in low-resource languages (5.2). The ablation studies show that models trained on one language can effectively perform zero-shot evaluations on others (6.1), demonstrating the potential of cross-lingual transfer. We also assess the impact of reference answers (6.2). We show that training from an instruction-tuned model accelerates convergence. We also highlight the benefits of LoRA training (6.3).Finally, we use weight merging techniques to create unified Evaluator LLM for all target languages (6.4). All artifacts will be made publicly available."
        },
        {
            "title": "2 Related Work",
            "content": "LLMs as Evaluators. With the growing openended generation capabilities of LLMs (Meta, 2024; Team et al., 2024a; Zhao et al., 2024) and the challenges of costly, inconsistent human evaluations (Chiang and Lee, 2023; Chen et al., 2023), many studies now propose using LLMs to score model outputs (Zheng et al., 2023a; Dubois et al., 2023; Kim et al., 2023b). This approach is gaining traction due to the absence of reliable taskspecific metrics and strong correlations between LLM and human scores (Dubois et al., 2023; Zheng et al., 2023a). Evaluation strategies with LLMs fall into two main types: absolute grading (Liu et al., 2023; Hada et al., 2023) and pairwise com2 parison (Wang et al., 2023a; Zheng et al., 2023a), with methods relying on either prompt-based evaluations (Zheng et al., 2023a) or fine-tuning models for evaluation (Kim et al., 2023b; Wang et al., 2023b). These methods operate in both referencedriven (Fu et al., 2023; Kim et al., 2023b) and reference-free scenarios (Liu et al., 2023). Studies show that reference-based approaches are generally more reliable (Doddapaneni et al., 2024; Kim et al., 2023b), and trained evaluators demonstrate better task adaptability and correlation with human judgments compared to those relying on parametric knowledge (Kim et al., 2024b). Advanced techniques also explore multi-agent interactions (Chan et al., 2023) or external evaluators (Min et al., 2023). While LLM-based evaluation has its limitations (Doddapaneni et al., 2024; Zeng et al., 2023), it remains the preferred approach due to the lack of scalable, cost-effective alternatives. Multilingual Evaluators. The lack of comprehensive benchmarks and reliable evaluation methods hinders progress in multilingual model development (Hu et al., 2020b; Doddapaneni et al., 2023). Existing multilingual benchmarks are limited in scale, domain, and rely heavily on costly human evaluations, focusing mainly on classification and sentence generation tasks (Singh et al., 2024; Watts et al., 2024; Doddapaneni et al., 2023; Ahuja et al., 2024). Though human evaluations and Elo ratings help create leaderboards, reliable metric for iterative model development remain missing. Additionally, prior work shows that GPT-4, as multilingual evaluator, delivers inconsistent results, highlighting the need for robust multilingual benchmark and evaluation framework (Hada et al., 2023). Weight Merging. Weight merging has proven effective in creating unified models and improving performance across tasks such as language modeling (Matena and Raffel, 2022; Li et al., 2022; Ilharco et al., 2023), instruction following (Jang et al., 2023b; Yu et al., 2024), preference learning (Jang et al., 2023a; Ramé et al., 2023), and multilingual applications (Chronopoulou et al., 2023). Techniques like linear merging (Wortsman et al., 2022) average model weights, while task vector arithmetic (Ilharco et al., 2023) uses element-wise subtraction to represent fine-tuned models and TIES (Yadav et al., 2023) resolves interference. These methods are increasingly popular for building unified multitask models. In this work, we explore merging techniques to develop unified model capable of evaluating multiple languages."
        },
        {
            "title": "3 CIA: Cross Lingual Auto Evaluation",
            "content": "We introduce the CIA suite, comprehensive framework for cross-lingual evaluation. In this setup, the questions and responses are provided in the target language, while the reference answers, evaluation instructions, and scoring rubrics remain in English, enabling effective cross-lingual evaluation (ref. Fig. 1). This section presents the human-annotated RECON test set (3.1), details the training data used in our experiments (3.2), and describes the HERCULE evaluator models trained on this data (3.3). Figure 2: Distribution of task capabilities in RECON."
        },
        {
            "title": "3.1 RECON: Test Data",
            "content": "We introduce RECON, human-annotated, generalpurpose multilingual evaluation benchmark. The input prompts are fully human-generated with multiple levels of supervision. This benchmark serves two key purposes: (i) to assess the multilingual capabilities of LLMs and (ii) to meta-evaluate the performance of Evaluator LLMs. Each instance in RECON consists of tuple (P , CEn, RX eval, REn ref , s), with superscripts indicating the language. Here, represents the input prompt, REn ref denotes the reference response, CEn defines the evaluation criteria and rubrics. Further, for meta-evaluation of Evaluator LLMs, RX eval is provided as the response with an associated expected score (s). To ensure the integrity of the benchmark, all components undergo thorough manual review and validation. The input prompts (P ) are sourced from various benchmarks, ensuring all are human-written and reflective of real-world scenarios (see Fig. 2). The benchmark consists of 500 prompts, of which 250 are from BigGenBench (Kim et al., 2024a), which includes per-example rubrics and tasks like planning, instruction following, and reasoning. The 3 remaining 250 are curated from test sets such as UltraEval (Ding et al., 2023), WizardLM (Xu et al., 2023), LIMA (Zhou et al., 2023), MT Bench (Zheng et al., 2023a), and FBI (Doddapaneni et al., 2024), covering tasks like long-form writing, creativity, and factual questions. Following an approach similar to BigGenBench (Kim et al., 2024a) and Prometheus (Kim et al., 2023b), we generate scoring criteria for each question. We prompt GPT-4o to first generate question-specific criteria that could be used to evaluate response, followed by detailed rubrics for assigning scores from 1 to 5. To guide the process, we provide three manually written criteria and rubric examples as in-context demonstrations. All generated rubrics are manually reviewed and verified by the authors. To generate reference answers Rref , we prompt GPT-4o with the question and corresponding rubric, instructing it to produce an answer that scores 5, formally expressed as (P, C, = 5) Rref . manual review of sampled references confirmed their high accuracy. Similarly, we generate evaluation responses Reval by prompting GPT-4o to produce answers scoring based on the rubrics, ensuring uniform score distribution across the benchmark. This process is represented as (P, C, Rref , = i) Reval, where 1 5. All responses are manually verified by the authors to ensure they align with the intended score. Detailed prompts are provided in Appendix C. Finally, all the prompts (P ) in the test set are manually translated into six target languages: Bengali (bn), German (de), French (fr), Hindi (hi), Telugu (te), and Urdu (ur), with one dedicated inhouse translator assigned to each language. Additionally, the responses to be evaluated (Reval) are translated using GPT-4o, followed by thorough human verification and correction. Annotators were specifically instructed to ensure that errors in lowscored answers were accurately translated without any unintended corrections. 3.2 INTEL: Training Data swers (Rref ) remain in English. Using GPT-4o,2 we translate the data into six target languages, relying on automated translation due to the impracticality of manual translation. To assess translation quality, we sampled 100 examples per language and had human experts evaluate them on binary scale. Translations were marked valid if they conveyed the intended meaning without major errors. We found that fewer than 5% samples were deemed invalid (See Appendix D). Based on this feedback, we decided to rely on GPT-4o for our use case. Notably, this auto-translated data is used only for training and is not part of the manually verified RECON benchmark. The final dataset after autotranslation includes 100k training samples and 1k validation samples per language, formatted as {P , CEn, REn eval, En, s}, where (F, s) repref , RX resents the feedback and score."
        },
        {
            "title": "3.3 HERCULE: Fine-tuned Evaluator LLM",
            "content": "Using INTEL (3.2), we fine-tune LLAMA-3.1-8BI (Meta, 2024) to equip the model with evaluation capabilities, resulting in the creation of the HERCULE series for all six target languages. We train HERCULE on the absolute grading objective, where, given prompt (P ), an LLM response (RX eval) in the target language, an evaluation criteria (CEn), and reference answer in English (REn ref ), the evaluator LLM is tasked with providing feedback (F En) and assigning score (s) on Likert scale from 1 to 5. Formally, we denote this obeval) (F En, s). jective as (P , CEn, REn Building on prior work (Wei et al., 2022; Kim et al., 2023a), we train the model to first generate an explanation for the evaluation, followed by score. ref , RX"
        },
        {
            "title": "4 Experimental Setup",
            "content": "Our goal is to use our RECON test set to assess the utility of HERCULE and other LLM-based evaluators for crosslingual evaluation. In this section, we outline our experimental setup for doing so. We begin by outlining the training details (4.1), metrics (4.2), followed by description of the various models considered in our experiments (4.3). We use the Feedback-Collection (Kim et al., 2023b) dataset for training and the Feedback-Bench (Kim et al., 2023b) dataset for validation. During training and validation, prompts (P ) and answers (Reval) are translated into the target languages, while evaluation instructions, rubrics (C), and reference an-"
        },
        {
            "title": "4.1 Training Details",
            "content": "The HERCULE model is trained on INTEL with sequence length of 4096, using FlashAttention 2 (Dao, 2024) and optimized with AdamW at 2https://openai.com/index/hello-gpt-4o/ 4 1e-5 learning rate for 3 epochs. All experiments were run on 8 Nvidia H100 GPUs. Evaluations are performed on RECON (3.1), using human-written prompts reflecting real-world scenarios (see Fig. 2). 4.2 Metrics As mentioned earlier, every response in the RECON test set has ground truth score associated with it. To assess the agreement between the ground truth scores and the scores assigned by the evaluator LLM, we use linear weighted Cohens Kappa (κ) (Sakai, 2021). An agreement score approaching 1 indicates strong correlation between the evaluator and the ground truth, while scores approaching 0 indicate weak alignment. 4.3 Models Considered We consider the following models for both ZeroShot and Fine-Tuning experiments. For fine-tuning, we train GEMMA, SARVAM, and AYA23 using the same setup as HERCULE. In the zero-shot setting, we evaluate LLAMA-3.1-405B-I, GPT-4O, and GEMINI-1.5-PRO, as their large size and proprietary nature limit fine-tuning. GEMMA: An open-source LLM trained on 6T tokens with better tokenizer fertility compared to LLAMA-3.1 (Team et al., 2024b). SARVAM3: An open-source LLM trained for Indian languages with 2T synthetic tokens. inAYA23: Aya23 is an 8B open-weight struction fine-tuned model, showcasing highly advanced multilingual capabilities in 23 languages (Aryabumi et al., 2024). LLAMA-3.1-405B-I: One of the largest opensource models currently available, evaluated in zero-shot setting on RECON (3.1) benchmark (Dubey et al., 2024). GEMINI-1.5-PRO: powerful proprietary LLM, known for its advanced multilingual capabilities compared to other closed-source models (Team et al., 2024a). GPT-4O: powerful proprietary LLM, often regarded as the leading choice for evaluation tasks."
        },
        {
            "title": "5 Results",
            "content": "In this section, we analyze the results of the CIA framework on the RECON test set (5.1). We then compare its evaluation capabilities against human assessments (5.2) and conclude with qualitative evaluation of the LLM evaluations (5.3). 3huggingface.co/models/sarvamai/sarvam-2b-v0.5 5.1 Does Cross Lingual Evaluation Work? We evaluate all models and baselines on the RECON test set, reporting the Cohens Kappa (κ) score (Sakai, 2021) between the Evaluator LLM and the ground truth scores in Table 1. Our results demonstrate that models fine-tuned with INTEL consistently outperform their zero-shot counterparts. Notably, models trained on INTEL significantly surpass even large, proprietary, black-box LLMs in performance (e.g., our HERCULE model outperforms the GPT-4O model). It is important to note that, despite some models having high fertility for the languages being trained and evaluated (ref. Fig. 4 in Appendix B), they still significantly outperform zero-shot evaluations conducted with large LLMs. This emphasizes that, even when the base model does not have fair representation for the languages of interest, fine-tuned models demonstrate superior alignment and performance compared to generic large-scale models that havent been specifically trained on the evaluation task."
        },
        {
            "title": "5.2 Evaluation in the Wild",
            "content": "So far, we evaluated the models using responses from RECON (Sec. 3.1), constructed in controlled setting. To further assess the Evaluator LLMs in real-world scenarios, we conducted human evaluations. We sampled 100 prompts per language from RECON and generated responses using LLAMA-3.1-8B-I, GEMMA-2-2B, and GPT4O-MINI. Native speakers with formal training in their respective languages scored each response, with the final human score averaged across three annotators. We then evaluate these responses using GPT-4O, GEMINI-1.5-PRO, LLAMA-3.1-405BI, and HERCULE and compared their correlations with human judgments. On average, HERCULE demonstrated stronger alignment with human evaluations than both GPT-4O and GEMINI-1.5-PRO. For high-resource languages like hi, which are officially supported by these models (OpenAI, 2024; Meta, 2024; Aryabumi et al., 2024), zero-shot GPT4O achieved the highest performance. However, for other languages, our cross-lingual fine-tuning provided clear advantages. We also found reasonable inter-annotator agreement (IAA in Table 2). Note that, even in this in-the-wild setup, the Evaluator LLMs maintain the same relative rankings as in RECON, validating RECON as reliable benchmark for evaluating Evaluator LLMs. We report other metrics, including Kendalls Tau (τ ) and Spearmans 5 Model Type bn de fr hi te ur Zero-Shot GPT-4O GEMINI-1.5-PRO Zero-Shot LLAMA-3.1-405B-I Zero-Shot LLAMA-3.2 3B GEMMA 7B AYA23 8B HERCULE 8B HERCULE 8B FFT FFT FFT FFT LoRA 0.64 0.54 0.60 0.68 0.47 0.70 0.74 0.72 0.66 0.58 0.66 0.72 0.39 0.72 0.75 0.74 0.65 0.59 0. 0.71 0.36 0.73 0.75 0.72 0.64 0.57 0.62 0.71 0.43 0.72 0.74 0.72 0.61 0.53 0.51 0.70 0.33 0.65 0.69 0.70 0.64 0.57 0. 0.72 0.38 0.71 0.74 0.70 avg. 0.64 0.56 0.62 0.71 0.39 0.70 0.73 0.72 Table 1: Evaluation results of all models on the RECON test set. We report the Linear Weighted Cohens Kappa (κ) score between the ground truth scores and the model predictions. Higher the value, better is the correlation. The upper half of the table presents zero-shot evaluations, while the lower half shows the results of fine-tuned models. Refer to Sec. 5.1 for detailed results. Model bn hi te ur 0.37 0.61 0.62 0.67 GPT-4O 0.31 0.51 0.61 0.62 GEMINI-PRO LLAMA 405B-I 0.38 0.59 0.67 0.72 0.42 0.53 0.74 0.78 HERCULE 8B"
        },
        {
            "title": "IAA",
            "content": "0.38 0.38 0.44 0.46 Table 2: Pearson correlation (ρ) between human annotator scores and Evaluator LLM scores on sample of 100 prompt-response pairs. higher value indicates stronger alignment with human judgments. See Sec. 5.2 for detailed results. Figure 3: Comparison of LLM score vs True score when the difference between the predictions is =1 and 2. We see that LLM Evaluator is more generous and awards higher scores. Refer Sec. 5.3 for detailed results. (ρs) correlations, in Appendix E."
        },
        {
            "title": "5.3 Qualitative Results",
            "content": "We now analyze the predictions made by the models, focusing on instances where the difference between the LLM score and the true score is equal to 1, as well as those where the difference is greater than or equal to 2. These results are illustrated in Fig. 3. Our observations indicate that, on average, the LLM evaluator tends to be generous, awarding higher scores to the responses in both cases. We manually examined the examples where the difference between the true score and the LLM score is greater than or equal to 2. In these cases, we observed that for complex reasoning questions, the model often relies on its parametric knowledge to evaluate the output, sometimes overlooking the reference answer. In the case of logical or mathematical reasoning questions, the model applies its knowledge to solve the problem, again neglecting the reference answer. These solutions tend to be accurate in high-resource languages like German (de) and French (fr), but are frequently incorrect in lowresource languages. We believe that incorporating more training examples from these languages could help address these issues, and we plan to explore this in future work. We provide these examples in Appendix F. We further observed that due to the high fertility of the Llama tokenizer (see Fig. 4 in Appendix B), some examples in certain languages exceed the maximum sequence length of 4096 tokens. Specifically, we found that approximately 5% of Bengali examples and 20% of Telugu examples fall into this category. We believe that base models with improved tokenizer fertility could help mitigate this issue, and we urge the community to consider this aspect when developing tokenizers."
        },
        {
            "title": "6 Ablations",
            "content": "In this section, we present the ablation studies conducted in our research. We begin with an analysis of zero-shot evaluation on unseen languages ( 6.1), followed by an assessment of the impact of reference answers ( 6.2). Next, we explore 6 bn de fr hi te ur Avg. Model bn de fr hi te ur 0.64 0.66 0.65 0.64 0.61 0.64 0.64 0.61 0.69 0.71 0.08 0.50 0.39 0.50 bn 0.74 0.76 0.74 0.74 0.57 0.72 0.71 de 0.64 0.75 0.72 0.70 0.62 0.69 0.69 fr 0.62 0.75 0.75 0.69 0.60 0.68 0.68 hi 0.62 0.76 0.77 0.74 0.56 0.69 0.69 te 0.65 0.71 0.72 0.72 0.69 0.72 0.70 ur 0.64 0.76 0.77 0.73 0.59 0.74 0.70 -8B w/o Ref w/ Ref 0.74 0.75 0.75 0.74 0.69 0.74 0.66 0.68 0.67 0.66 0.63 0.65 - - 0.73 - - - Table 4: Performance comparison of Evaluator LLMs with and without reference answers, including those using reference answers in the target language (w/ Ref). Refer to Sec. 6.2 for more details. 0.74 0.75 0.75 0.74 0.69 0.74 0.73 Model bn hi te Table 3: We present the zero-shot evaluation scores, where the rows indicate the language the model was trained on and the columns show the language it was represents the scores for in-language evaluated on. training. refers to LLAMA-3.1-8B model trained on English Feedback-Collection (Kim et al., 2023b) and zero-shot evaluated on target languages. Refer to Sec. 6.1 for detailed results. various modeling choices ( 6.3), and finally, we investigate weight merging techniques to develop unified Evaluator LLM for all languages ( 6.4)."
        },
        {
            "title": "6.1 Evaluation on Unseen Languages",
            "content": "Our primary evaluation focuses on in-language training and testing, where the Evaluator LLM is trained and tested on target language X. In this ablation, we explore cross-lingual evaluation by training the Evaluator LLM on language and testing it on other languages. The second row of Table 3 presents results for LLAMA-3.1-8B-I, trained on the English Feedback-Collection (Kim et al., 2023b) and evaluated on the RECON test set. Subsequent rows show models trained on their respective languages, with evaluation results displayed across different languages. The findings indicate that zero-shot transfer from English is less effective. While zero-shot performance from other languages does not match the best results from HERCULE, it significantly outperforms zero-shot evaluations using large proprietary LLMs and English training data. These results suggest that models trained on language can effectively generalize to related languages, enhancing the utility of Evaluator LLMs for unseen languages. 6."
        },
        {
            "title": "Importance of Reference Answer",
            "content": "Our primary hypothesis is that Evaluator LLMs benefit from having reference answer, particularly when it is provided in English. To test this, 7 0.64 GEMMA-2B 0.58 SARVAM-2B GEMMA-2B-IT 0.64 LLAMA 3.2 3B 0.68 0.62 0.56 0.67 0.71 0.60 0.58 0.61 0.70 avg. 0.62 0.57 0.64 0.70 Table 5: Evaluation scores of comparable 2B parameter sized models on RECON test set. Refer to Sec. 6.3 for detailed results. we trained an Evaluator LLM without reference answer and evaluated it on the RECON test set. As shown in Table 4, the results indicate that the Evaluator LLM without reference performs worse than one with reference, consistent with findings from Kim et al. (2023b) and Doddapaneni et al. (2024). We also examined the impact of using reference answers in the target language by translating them for training4. The results in Table 4 show that while the Evaluator with target-language references performs slightly worse than the one with English references, the difference is not significant. However, as modern LLMs are heavily optimized for English, their tokenization performance on non-Latin scripts is notably weaker (see Fig. 4), which increases the input sequence length. Therefore, we recommend using English reference answers given their easy accessibility and computational efficiency."
        },
        {
            "title": "6.3 Modeling Choice",
            "content": "Base vs IFT. Current LLMs are released both as pretrained base models as well as instruction finetuned models. While pretraining is done on large multilingual corpus, instruction fine-tuning typically focuses on higher-resource languages. This raises the question of which model to select for fine-tuning Evaluator LLMs. Comparing rows 1 and 3 of Table 5, we observe that despite being 4Due to the high cost of translation and the limitation of sequence lengths exceeding 4096 tokens, we could not conduct this experiment across all languages. Model bn de fr hi te ur avg. 6.4 Single / Joint training / Weight Merging Single 0.74 0.75 0.75 0.74 0.69 0.74 0.73 Joint 0.70 0.70 0.70 0.69 0.68 0.67 0.69 Linear 0.71 0.75 0.77 0.73 0.64 0.73 0.72 TIES 0.68 0.74 0.77 0.76 0.64 0.72 0.72 Table 6: Evaluation scores of merging methods on RECON test set. Refer to Sec. 6.4 for detailed results. predominantly fine-tuned on English, Gemma IFT gives improved performance on Hindi, while performance on other languages remains largely consistent. This suggests that even limited instruction fine-tuning in higher-resource languages can benefit other lower-resource languages. LoRA vs. FFT. critical design choice is whether to use LoRA adapters or full fine-tuning (FFT). LoRA updates only small subset of the models weights (around 5%), making it particularly memory-efficient for large LLMs (>8B), whereas FFT updates all parameters and can be impractical at similar scales. Comparing the last two rows of Table 1, we find that the model trained with LoRA achieves performance comparable to that of the FFT model, indicating that LoRA is viable option in resource-constrained scenarios. Language-Specific LLMs. While many English LLMs are released with limited multilingual training data, there are also language-specific models that are trained exclusively on data in their target languages. For example, Llama-3.1 has been trained on 15.6 trillion tokens, with only 1 trillion being multilingual, whereas the Sarvam-2B model focuses only on Indian languages and is trained on 2 trillion tokens. Upon fine-tuning both Sarvam2B and the comparably sized Gemma model, we found that Gemma consistently outperformed Sarvam across all languages (see Table 5), which is counter-intuitive. We hypothesize that this performance gap arises from the larger and more diverse dataset used for Gemmas training, while Sarvams dataset is more limited in scope. Previous studies (Madaan et al., 2022; MA et al., 2024) have shown that increasing data diversity in pretraining, including math and code, enhances reasoning capabilities. These findings are preliminary for 2Bsized models, and we plan to conduct more detailed experiments in future work. In our initial experiments, we trained separate LLAMA-3.1-8B-I model for each language, resulting in six distinct models. Recent research has investigated model merging, where models trained on different tasks or languages are combined to form unified model. We apply linear (Wortsman et al., 2022) and TIES (Yadav et al., 2023) merging techniques to create single Evaluator LLM for all six languages (Goddard et al., 2024), comparing these methods to joint fine-tuning, which combines data from all languages for training. Notably, all methods utilize the same total GPU hours across languages. The results in Table 6 show that model merging generally outperforms joint training and achieves performance comparable to individually trained models, particularly for high-resource languages like German and French. However, individually trained models still excel in low-resource languages. Overall, model merging proves to be promising approach for developing unified multilingual evaluator LLMs, especially when balancing performance across high-resource languages. We also examined the rationales generated by the merged model and found them to be coherent, effectively justifying the assigned scores. Examples are provided in Appendix G."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced the Cross Lingual Auto Evaluation (CIA) Suite, comprehensive framework for multilingual evaluation using LLMs. Our analysis demonstrated that fine-tuning LLMs on INTEL significantly improves evaluation accuracy, particularly in low-resource languages. Results from the RECON test set indicate that our fine-tuned models outperform even large proprietary models. Additionally, our evaluation against human assessments revealed strong alignment between our models and human judgments, highlighting the effectiveness of cross-lingual fine-tuning in enhancing evaluation metrics across languages. Through extensive ablation studies, we explored zero-shot evaluation with our HERCULE model, established the importance of reference answers, examined various modeling choices, and assessed the effectiveness of weight merging techniques. By making our code, datasets, and models publicly available, we aim to encourage further research in developing and evaluating robust multilingual models."
        },
        {
            "title": "Limitations",
            "content": "This work has few limitations. First, due to the costs associated with translation, we were unable to perform experiments on broader range of languages, which may limit the generalizability of our findings. Second, the availability of multilingual models for testing our framework is limited, which restricts our ability to evaluate the performance of various models within the proposed CIA Suite comprehensively. Additionally, we did not explore different configurations of the weight merging techniques, such as balancing the contributions from various languages to achieve optimal performance."
        },
        {
            "title": "Ethics",
            "content": "Annotators who participated in the annotation and/or verification task are paid competitive monthly salary to help with the tasks. The salaries were determined based on the qualification and the prior experience working on similar tasks and adhering to the norms of the government of our country. The annotators were made aware that the datasets will be publicly released. The annotated datasets have no personally identifying information. The models developed in this work are intended solely for evaluation purposes. However, they may inadvertently exhibit biases stemming from the training data used. The code, datasets and model created in this work will be made available under permissible licenses. We only used ChatGPT5 for assistance purely with the language of the paper, e.g., paraphrasing, spell-checking, or polishing the authors original content, without suggesting new content. The released code and models will have an MIT License6. The dataset will be released under CC-0 License7."
        },
        {
            "title": "References",
            "content": "Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2024. MEGAVERSE: benchmarking large language models across languages, modalities, models and tasks. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, 5https://chat.openai.com/ 6https://opensource.org/licenses/MIT 7https://creativecommons.org/share-your-work/ public-domain/cc0/ June 16-21, 2024, pages 25982637. Association for Computational Linguistics. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv: 2405.15032. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv: 2308.07201. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023. Exploring the use of large language models for reference-free text quality evaluation: preliminary empirical study. CoRR, abs/2304.00723. David Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human In Proceedings of the 61st Annual evaluations? Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1560715631. Association for Computational Linguistics. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Alexandra Chronopoulou, Jonas Pfeiffer, Joshua Maynez, Xinyi Wang, Sebastian Ruder, and Priyanka Agrawal. 2023. Language and task arithmetic with parameter-efficient layers for zero-shot summarization. CoRR, abs/2311.09344. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 24752485. Association for Computational Linguistics. Tri Dao. 2024. Flashattention-2: Faster attention with In The better parallelism and work partitioning. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models 9 by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 3029 3051. Association for Computational Linguistics. Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, and Pratyush Kumar. 2023. Towards leaving no Indic language behind: Building monolingual corpora, benchmark and models for Indic languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1240212426, Toronto, Canada. Association for Computational Linguistics. Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, and Mitesh M. Khapra. 2024. Finding blind spots in evaluator llms with interpretable checklists. CoRR, abs/2406.13439. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan 10 Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. arXiv preprint arXiv: 2407.21783. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacafarm: simulation framework for methods that 11 learn from human feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv: 2302.04166. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. 2024. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257. Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, M. Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? FINDINGS. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020a. Xtreme: massively multilingual multi-task benchmark for evaluating cross-lingual generalization. ArXiv, abs/2003.11080. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020b. XTREME: massively multilingual multitask benchmark for evaluating cross-lingual generalization. CoRR, abs/2003.11080. Gabriel Ilharco, Marco Túlio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. 2023a. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. CoRR, abs/2310.11564. Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2023b. Exploring the benefits of training expert language models over instruction tuning. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1470214729. PMLR. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, S. Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2023a. Prometheus: Inducing fine-grained evaluation capability in language models. International Conference on Learning Representations. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2023b. Prometheus: Inducing finegrained evaluation capability in language models. CoRR, abs/2310.08491. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024a. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv: 2405.01535. Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer. 2022. Branch-train-merge: Embarrassingly parallel training of expert language models. CoRR, abs/2208.03306. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. Conference on Empirical Methods in Natural Language Processing. YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At which training stage does code data help LLMs reasoning? In The Twelfth International Conference on Learning Representations. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. arXiv preprint arXiv: 2210.07128. Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta. com/blog/meta-llama-3/. Accessed: 2024-06-14. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. OpenAI. 2024. Hello gpt-4 turbo. https://openai. com/index/hello-gpt-4o/. Accessed: 2024-1014. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 23832392. The Association for Computational Linguistics. Alexandre Ramé, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. 2023. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 26852702. Association for Computational Linguistics. Tetsuya Sakai. 2021. Evaluating evaluation measures for ordinal classification and ordinal quantification. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 27592769, Online. Association for Computational Linguistics. Michael Matena and Colin Raffel. 2022. Merging models with fisher-weighted averaging. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78817892, Online. Association for Computational Linguistics. 12 Harman Singh, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, and Partha Talukdar. 2024. Indicgenbench: multilingual benchmark to evaluate generation capabilities of llms on indic languages. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1104711073. Association for Computational Linguistics. Gemini Team, Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry, Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, Luke Vilnis, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman, Nithya Attaluri, Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma, Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala, Martin Chadwick, Juliette Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai, Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Yingjie Miao, Lukas Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago Ontanon, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta, Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau, Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May, Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, Anja Hauth, Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Ramasesh, Anton Briukhov, Da13 Woon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra, Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara Sainath, Shane Gu, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan, Mark Omernick, Colin Gaffney, Gabriela Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin Chiu, Rory Blevins, Zach Gleicher, Adrià Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek, Sébastien M. R. Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek, Kashyap Krishnakumar, Shuo yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir, Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Sébastien Cevey, Jonas Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Kiran Vodrahalli, Samer Hassan, Diana Mincu, Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravi Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee, Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma, Mario Luˇcic, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjösund, Elena Buchatskaya, Elspeth White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Zeyncep Cankara, Jane Labanowski, Nicola De Cao, David Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Lora Aroyo, Zhufeng Pan, Zachary Nado, Jakub Sygnowski, Stephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore, Yamini Bansal, Xavier Garcia, Mehran Kazemi, Piyush Patil, Ishita Dasgupta, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Qingze Wang, ChungCheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Raphaël Lopez Kaufman, Fred Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen, Ruibo Liu, Chris Welty, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika Rogozinska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang, Dave Lacey, Anastasija Ilic, Yao Zhao, Adam Iwanicki, Alejandro Lince, Alexander Chen, Christina Lyu, Carl Lebsack, Jordan Griffith, Meenu Gaba, Paramjit Sandhu, Phil Chen, Anna Koop, Ravi Rajwar, Soheil Hassas Yeganeh, Solomon Chang, Rui Zhu, Soroush Radpour, Elnaz Davoodi, Ving Ian Lei, Yang Xu, Daniel Toyama, Constant Segal, Martin Wicke, Hanzhao Lin, Anna Bulanova, Adrià Puigdomènech Badia, Nemanja Rakicevic, Pablo Sprechmann, Angelos Filos, Shaobo Hou, Víctor Campos, Nora Kassner, Devendra Sachan, Meire Fortunato, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp, Nejc Trdin, Anitha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel, Somer Greene, Anaïs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana Kulizhskaya, Sonam Goenka, Brennan Saeta, Ying Xu, Christian Frank, Dario de Cesare, Brona Robenek, Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy, Yelin Kim, Dinghua Li, Bill Rosgen, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan, Hongkun Yu, Çaglar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier Dousse, Willi Gierke, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu, Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic, Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi Lam, Josef Broder, Dan HoltmannRice, Nina Martin, Bramandia Ramadhana, Mrinal Shukla, Sujoy Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi, Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, Félix de Chaumont Quitry, Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia, Clement Farabet, Pedro Valenzuela, Quan Yuan, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse, Nandita Dukkipati, Adam Paszke, Andrew Bolt, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha Vashisht, Rebeca SantamariaFernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, Andrey Khorlin, Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Norman Casagrande, Jay Hoover, Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa, Praveen Kumar, Thibault Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun, Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar, Sanjay Ghemawat, Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A. Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel Kaed, Jing Li, Shreyas Rammohan Belle, Zhe Chen, Jaclyn Konzelmann, Siim Põder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne Liu, Azade Nova, Jun Xu, Alanna Walton, Alicia Parrish, Mark Epstein, Sara McCarthy, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2024a. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv: 2403.05530. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige BaiAnnual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. CoRR, abs/2310.07641. Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky, Piero Molino, Travis Addair, and Devvret Rishi. 2024. Lora land: 310 fine-tuned llms that rival gpt-4, technical report. CoRR, abs/2405.00732. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. CoRR, abs/2305.11206. ley, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024b. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv: 2403.08295. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. Glue: multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023a. Large language models are not fair evaluators. arXiv preprint arXiv: 2305.17926. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2023b. Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. CoRR, abs/2306.05087. Ishaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Swami Manohar, and Sunayana Sitaram. 2024. Pariksha: scalable, democratic, transparent evaluation platform for assessing indic large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903. Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. arXiv preprint arXiv: 2203.05482. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. CoRR, abs/2304.12244. Prateek Yadav, Derek Tam, Leshem Choshen, Colin A. Raffel, and Mohit Bansal. 2023. Ties-merging: Resolving interference when merging models. In Advances in Neural Information Processing Systems 36:"
        },
        {
            "title": "A Model Name",
            "content": "We named our models HERCULE to reflect both literary and mythological influences. The name honors Hercule Poirot, the renowned Belgian detective created by Agatha Christie, celebrated for his sharp intellect and meticulous approachqualities we aspire to emulate in our evaluation framework. Additionally, our work is inspired by Prometheus (Kim et al., 2023b), reinforcing the Greek connection (Hercules) to the name used in their paper."
        },
        {
            "title": "B Fertility of Tokenizers",
            "content": "The fertility scores of various tokenizers used in our experiments are presented in Fig. 4. τ ρs τ ρs τ ρs τ ρs bn 0.28 0.35 0.22 0.28 0.33 0.40 0.35 0.43 hi 0.43 0.52 0.38 0.47 0.40 0.48 0.36 0.43 te 0.50 0.62 0.51 0.63 0.57 0.67 0.61 0.75 ur 0.54 0.66 0.53 0.64 0.57 0.70 0.65 0.77 Table 7: Kendal Tau (τ ) and Spearman Correlation (ρs) between human annotator scores and Evaluator LLM scores on sample of 100 prompt-response pairs."
        },
        {
            "title": "G Weight Merging Examples",
            "content": "Examples with weight merging experiments are provided in Figs. 19 and 20. TIES merging continues its generation after EOS, but the feedback remains coherent. We use the (F, s) from the first generation. Figure 4: Fertility scores of tokenizers for all baseline models."
        },
        {
            "title": "C RECON Test Set Creation Prompts",
            "content": "The prompts used for creating the scoring rubrics, along with the scored responses and reference answers, are illustrated in the Figures 5, 6, 7."
        },
        {
            "title": "D Instructions for Human Evaluation",
            "content": "Prompts used for Human Evaluation are presented in Figure 8."
        },
        {
            "title": "E Human Evaluation Extended Results",
            "content": "The Kendalls Tau (τ ) and Spearman correlation (ρs) scores are presented in Table 7."
        },
        {
            "title": "F Qualitative Examples",
            "content": "Examples of HERCULE evaluations on the RECON test set are shown in Figures 9, 11, 13, 15, 17 and their corresponding translations presented in Figures 10, 12, 14, 16, 18. 16 Figure 5: Prompt used for generating the scoring rubrics to create RECON test set. 17 Figure 6: Prompt used for generating score specific answer in the RECON test set. Figure 7: Prompt used for generating the reference answer in RECON test set. 19 Figure 8: Instructions to annotators for generating the human scores on RECON subset. Refer to Sec. 5.2 for detailed results. 20 Figure 9: German example from RECON test set, where the Evaluator LLM used its own reasoning to evaluate the response, ignoring the reference answer. Translations are available in Figure 10. See Sec. 5.3 for detailed results. Figure 10: German-to-English translation for the example in Fig. 9, provided for reference. 22 Figure 11: Telugu example from the RECON test set, where the Evaluator LLM relies on its own reasoning to evaluate the response but generates incorrect reasoning, disregarding the reference answer. Translations are available in Figure 12. See Sec. 5.3 for detailed results. 23 Figure 12: Telugu-to-English translation for the example in Fig. 11, provided for reference. Figure 13: French example from the RECON test set, where the Evaluator LLM relies on its own reasoning to evaluate the response and generates correct reasoning (In contrast to Example in Fig. 11). Translations are available in Figure 14. See Sec. 5.3 for detailed results. 25 Figure 14: French-to-English translation for the example in Fig. 13, provided for reference. 26 Figure 15: Hindi example from the RECON test set, where the Evaluator LLM follows the rubrics correctly. Translations are available in Figure 16. See Sec. 5.3 for detailed results. Figure 16: Hindi-to-English translation for the example in Fig. 15, provided for reference. 28 Figure 17: Bengali example from the RECON test set, where the Evaluator LLM overestimates the score (should be 4). Translations are available in Figure 18. See Sec. 5.3 for detailed results. 29 Figure 18: Bengali-to-English translation for the example in Fig. 17, provided for reference. Figure 19: German example from the RECON test set, evaluated using Linear Weight Merging. See Sec. 6.4 for detailed results. 31 Figure 20: German example from the RECON test set (same as Fig. 19), evaluated using TIES Merging. See Sec. 6.4 for detailed results."
        }
    ],
    "affiliations": [
        "IIT Madras",
        "Microsoft",
        "National Institute of Information and Communications Technology, Kyoto, Japan",
        "Nilekani Centre at AI4Bharat"
    ]
}