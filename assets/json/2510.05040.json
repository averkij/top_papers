{
    "paper_title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
    "authors": [
        "Jihoon Lee",
        "Hoyeon Moon",
        "Kevin Zhai",
        "Arun Kumar Chithanar",
        "Anit Kumar Sahu",
        "Soummya Kar",
        "Chul Lee",
        "Souradip Chakraborty",
        "Amrit Singh Bedi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 4 0 5 0 . 0 1 5 2 : r Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts Jihoon Lee1, Hoyeon Moon1, Kevin Zhai5, Arun Kumar Chithanar, Anit Kumar Sahu2, Soummya Kar3, Chul Lee, Souradip Chakraborty,4, Amrit Singh Bedi,5 1Yonsei University, 2Oracle, 3CMU, 4UMD, 5UCF Project Page"
        },
        {
            "title": "Abstract",
            "content": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), training-free inference method that ensembles across heterogeneous block schedules. By doing majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56 (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays critical role in determining performance during inference. Correspondence: Amrit Singh Bedi at amritbedi@ucf.edu"
        },
        {
            "title": "Introduction",
            "content": "Diffusion-based large language models (dLLMs) are rapidly emerging as promising alternative to traditional autoregressive LLMs generalizing beyond the next token prediction [2]. Unlike autoregressive models, dLLMs generate text via an iterative mask-and-unmask process, allowing them to decode tokens in essentially arbitrary order [1]. This fundamental change in the generation mechanism during training grants dLLMs remarkable flexibility at inference time. In fact, recent dLLMs have already demonstrated competitive (and sometimes superior) performance compared to their autoregressive counterparts on similar scale [3]. These early successes indicate that the masking strategy during inference plays crucial role. Gaps in our understanding about dLLMs. The freedom to choose the generation order, the masking strategy, is the central advantage of dLLMs. Recent works [1, 2] have tried to harness this flexibility by relying on prediction confidence, progressively unmasking high-confidence tokens (top-K margin in Figure 1). However, such an approach often leads to inherently biased solutions as they overlook the crucial sequential Equal contribution. 1 Figure 1 Overview of our proposed HEX framework. Left: HEX leverages multiple semi-autoregressive hidden experts, guided by different masking schedules, to produce concatenated outputs and final answer. Right: HEX outperforms Top-K, Top-K margin [1] and Random expert selection strategies [2] on reasoning tasks (GSM8K, MATH, ARC-C), surpassing the training-based GRPO baseline (d1) [3]. structure present in the language training data, which induces implicit biases in the learned masking strategies. As result, these methods might perform worse than random unmasking, as shown in Figure 1. Why this happens and what we can do to avoid such failure remains an open question in the literature. In this work, we take first step towards resolving this. Our key finding: hidden semi-autoregressive experts. We uncover new dimension of test-time scaling for diffusion LLMs, centered on the masking strategy. We find that the effective design of masking strategy is influenced by the structural biases encoded in the training data. The sequential nature of language data, for instance, causes dLLMs to implicitly learn mixture of semi-autoregressive experts during training. Each of these hidden experts is biased toward distinct masking distributions, often favoring those that reflect left-to-right or autoregressive-like generation orders. We, for the first time, demonstrate that this latent mixture can be deliberately accessed during inference. By varying the block size used in semi-autoregressive decoding, we can activate different experts, mirroring the conditions the model saw during training. This insight unlocks novel method for test-time scaling in dLLMs. By marginalizing across these block schedules, we can exploit the latent ensemble of experts, resulting in significantly more robust and optimal inference as shown in Figure 5. Hence, we propose HEX (Hidden semi-autoregressive EXperts), training-free inference method that uncovers new dimension of test-time scaling for dLLMs. HEX marginalizes across block schedules, treating block size and order as latent variables that define an additional scaling dimension, and aggregates predictions via majority voting. In doing so, it robustly avoids the pitfalls of committing to any single decoding path, turning dLLMs hidden flexibility into principled mechanism for test-time scaling. We summarize our contributions as follows. (i) Highlighting Key Limitation in dLLM Inference. We identify and analyze key limitation of existing inference strategies for dLLMs. We show that current methods can lead to suboptimal performance and generation instability because they overlook implicit structural biases that the model learns during training. This pinpoints crucial, unaddressed aspect of dLLM generation (Section 4). (ii) HEX: New dimension of test-time scaling in dLLMs. Our key insight is that dLLMs implicitly learn mixture of semi-autoregressive experts, and block scheduling helps to uncover this latent structure. Based on this finding, we introduce HEX, novel, training-free inference algorithm specifically designed to leverage this latent structure by ensembling diverse semi-autoregressive schedules with majority vote aggregation (Algorithm 2), turning ordering into new test-time scaling dimension for dLLMs. (Section 5). (iii) Comprehensive experimental analysis: matching GRPO-level performance. HEX achieves GRPO-level results on GSM8K, MATH, ARC-C, and TruthfulQA, without retraining, establishing test-time scaling as powerful new paradigm for diffusion LLMs. HEX outperforms existing state-of-the-art inference methods [1] on reasoning tasks, boosting accuracy by up to 3.56 (from 24.72% to 88.10%.) HEX even produces massive gains on more challenging tasks, including MATH [4] (from 16.40% to 40.00%), ARC-C [5] (from 54.18% to 87.80%), and TruthfulQA [6] (from 28.36% to 57.46%). (Section 6)."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion Large Language Models. Diffusion models have achieved state-of-the-art performance in image generation [7, 8], and recent advances extend them to the discrete domain of language. The early approaches applied continuous diffusion to latent text representations [911], but faced challenges with scalability and discretization. masked diffusion paradigm soon emerged as more tractable discrete alternative [12, 13], with large-scale implementations such as DiffuLLaMA [14], Dream [15] and LLaDA [2] demonstrating that diffusion LLMs (dLLMs) can rival similarly sized autoregressive models, even on complex reasoning [3, 16]. This potential extends even to multimodal understanding [1721]. Inference-Time Methods for dLLMs. In autoregressive models, inference-time scaling has been extensively studied, ranging from chain-of-thought prompting [22] and self-consistency [23] to scaling the allocation of test-time compute [24]. In contrast, inference-time methods for dLLMs remain sparse [25, 26]. Most gains in dLLM performance so far have come from training-time improvements, such as applying GRPO [3, 16] or post-training method such as temporal consistency reinforcement [26]. Additional discussion is provided in Appendix B.1."
        },
        {
            "title": "3 Problem Formulation",
            "content": "Masked Diffusion Language Models (MDM). Let = (x1, . . . , xn) be length-n token sequence over vocabulary V. masked diffusion large language model (dLLM) specifies conditional denoiser pθ(x[M ] x[M c]) for any mask [n], where = [n] . The notation x[M ] is defined as the subsequence of tokens from on the indices of , i.e. x[M ] = (xi)iM , where x[M ] denotes the masked tokens in x. In the forward (corruption) process, random subset of tokens [n] is masked (replaced by special symbol [MASK], and model pθ is tasked with recovering the original tokens in given the unmasked tokens in the complement := [n] . Formally, for random mask pattern , the model produces conditional distribution pθ(x[M ] x[M c]) on the masked tokens. The training objective is to maximize the likelihood of ground-truth tokens in these masked positions. Hence, the training problem can be written as θ arg min θ Lmask(θ) := ExDEℓUnif([n]) EM [n],M =ℓ (cid:104) (cid:88) (cid:105) log pθ(xi x[M c]) , (1) iM where is the data distribution, ℓ Unif([n]) is uniformly sampled number of masked tokens ℓ {1, . . . , n} and [n] is the randomly selected subset of length = ℓ. The summation in (1) runs over all masked token positions , and the loss on each such position is the negative log-likelihood of the true token xi given the remaining context (unmasked) x[M c]. The objective in (1) trains the model to predict randomly masked-out tokens, and can be viewed as averaging next-token losses over all token permutations, i.e., an any-order objective [2]. Algorithm 1: Vanilla MDM Inference Input: prompt xprompt, output length L, steps ; mask schedule {Mt}T t=1 Initialize: x(0) [MASK]L; for = 1, 2, . . . , do , model pθ(). Predict all masked tokens simultaneously via (cid:0) [xprompt, x(t1)](cid:1) ; pθ x(t) Fill with predicted tokens Fix tokens at location Mask tokens at location Mt (cid:0)t1 k=1M (cid:1) Inference as the Core Challenge. After learning θ from (1), generation happens step by step. For instance, for given prompt xprompt, it starts by selecting the number of tokens to be generated (say L), and then requires choosing how to reveal tokens. Let decoding trajectory be sequence of masks Output: xT 3 τ = (M1, . . . , MT ) that partition [n] (such that (cid:70)T model predicts all masked tokens conditioned on the currently revealed context x(cid:2) t1 trajectory τ and prompt xprompt, functional of natural log-likelihood is = [n]), with per-step sizes ℓt = Mt. At step t, the (cid:3). For fixed s=1 s t=1 (τ ; θ xprompt) = (cid:88) (cid:88) t=1 iMt log pθ (cid:32) xi xprompt, (cid:104) t1 (cid:91) s=1 (cid:33) (cid:105) ."
        },
        {
            "title": "M c\ns",
            "content": "(2) We summarize the vanilla inference procedure in Algorithm 1. The ideal (but empirically intractable0) goal is to choose τ such that we obtain sample which maximizes (τ ; θ xprompt). Because (1) trains on all mask patterns, many training subproblems are intrinsically ill-posed (e.g., extremely large masks with scant context) due to the implicit sequential bias in the language training distribution. For example, some conditionals are rarely observed or provide little meaningful context, making them effectively unsolvable. As consequence, the model ends up learning only subset of subproblems or conditionals, while others remain poorly learned or ignored. uniform masking objective forces the model to put equal weight on every subproblem, including those that confound the masking sequence, resulting in suboptimal masking strategy overall. This mismatch creates gap at inference time: the models behavior becomes highly sensitive to the masking schedule, and strong performance depends on selecting strategies that align with the sequential biases implicitly learned during training. Key Open Question. Thus, the central question becomes: how can we design an inference strategy that faithfully reflects what the model has learned during training, given that dLLMs leave the process fundamentally under-specified and require us to decide the optimal masking trajectory."
        },
        {
            "title": "4 Limitations of SoTA and Our Key Insight",
            "content": "Failure of existing inference methods for reasoning tasks. Existing dLLMs rely on heuristic inferencetime strategies to choose which tokens to unmask at each denoising step. Common approaches include random sampling (randomly picking masked positions to predict) and confidence-based selection (choosing [1] showed that the token positions with high model confidence or probability). For example, Kim et al. for Sudoku puzzles, simple confidence-based top-K margin method can boost accuracy from 7% to nearly 90%. Intuitively, one might expect similar benefit for reasoning tasks. Surprisingly, we find the opposite in reasoning benchmarks. In our experiments (see Figure 2) on GSM8K math problems, random unmasking far outperforms top-K margin (high-confidence) decoding. Instead of guiding generation, high confidence derails the unmasking trajectory into producing degenerate outputs. In Figure 2, the top-K margin strategy consistently unmasked the [AfterEoT] (endoftext) token at all positions, proceeding backward from the end to the front. This leads to degenerate outputs (shown in red text) in Figure 2. This surprising result challenges our intuition built through studying prior work. Unexpected reversal of intuition. Our findings highlight key limitation of relying on common heuristics from autoregressive models and prior work [1]. While one would expect that follow the models own highestprobability tokens is reliable strategy, our results show that methods relying solely on token confidence are not sufficient for strong performance in complex reasoning tasks. Our results thus raise basic questions: Why does random sampling outperform top-K margin? Why does the model overconfidently pick the [AfterEoT] so early? To answer these questions, we propose new perspective on the dLLMs internal structure. Our key insight is that the dLLM can be viewed as an implicit mixture of experts, which allows us to mitigate the risk of overconfidently predicting [AfterEoT] tokens too early. By aggregating predictions from experts conditioned on different subsets of tokens, effectively marginalizing over contexts, our approach avoids prematurely committing to high-confidence tokens like the [AfterEoT] token. From failure to mechanism. The surprising failure of confidence-based schedules suggests that local token probabilities are unreliable under many masked contexts induced at inference. Because the dLLM objective (in (1)) averages over wide variety of maskings, including ill-posed ones, some conditionals are poorly 0Because simply making consecutive greedy choices does not guarantee that the overall is maximized, and performing global search would require evaluating all possible trajectories, which is practically infeasible. Figure 2 Random vs. Top-K margin inference on GSM8K. Left: Random decoding achieves 50.87% accuracy, while Right: Top-K margin only 24.72%. For each method, the text box shows the result at the last unmasking step. Top-K margin generates output tokens in reverse, from the end toward the beginning, and exhibits catastrophic collapse in which all tokens are [AfterEoT] (shown in red). Over 55.5% of top-K margin runs suffered this collapse, yielding very low accuracy. These failures cast doubt on methods that rely solely on token confidence. learned and disproportionately favor special tokens such as [AfterEoT]. Our view is to treat each semi-AR block schedule as querying different conditional expert, then marginalize across experts to recover robust predictions. This replaces brittle confidence-following with consensus-seeking and is the core rationale behind HEX. Our Key Insight: dLLM is an implicit mixture of experts. From (2), it is evident that the diffusion LLM training leads to model with family of maskedtoken conditionals (cid:8) pθ(xi [xprompt, x[U ]]) : [n], [n] {i} (cid:9), which we can view as implicit experts indexed by the visible/unmasked set of tokens . For fixed target index at test-time, the goal is to aggregate the relevant experts {pθ(xi [xprompt, x[U ]])}U [n]{i} into single prediction rule. principled aggregation strategy is the mixture-of-experts predictor given by pmix(xi = xprompt) = (cid:88) π(U xprompt) pθ(xi = [xprompt, x[U ]]), (3) where π(U xprompt) denotes the weight or trust placed on expert for prompt xprompt. Toy Example. In our toy example (Figure 3), we examine the models answer to the question Who invented telephone? (ground truth: The inventor was Bell.). We treat each latent conditional context (or expert) as separate setting that produces distribution to predict masked tokens. Figure 3 plots these distributions for particular position, = 4 (the token Bell). As the plot makes clear, most experts concentrate probability on Bell (the correct token), but few contexts produce flat or incorrect distributions that never predict Bell. Correct-Answer Contexts (Experts): The majority of conditionals yield distribution that peaked at the correct answer token Bell. These contexts effectively act as experts on this question. Non-Expert Contexts: Some conditionals do not produce clear peak in Bell. These contexts fail to predict the correct answer. We note that even though the model as whole can answer correctly, not all conditionals are experts. The figure shows that only subset of experts know the answer, while others do not. Hence, our toy example shows that it is difficult to identify which context is the right expert. We do not know priori which conditional context (or expert) will yield the correct token. The hidden gating distribution π(U ) that governs how likely each context knows the answer is unknown. The dLLMs does not learn the underlying gating distribution. 5 Figure 3 The distribution of the 4th token Bell in the output sequence changes significantly depending on the 23 masking conditions applied to the previous three tokens: The, inventor, was. The star mark indicates the highest confidence for each distribution generated by . Some masking conditions (violet and green) produce collapsed distribution: \"Bell Bell was invented.\" (ungrammatical sentence), \"The telephone was invented.\" (missing target information), respectively."
        },
        {
            "title": "5 Hidden Semi-autoregressive Experts for Test-time Scaling",
            "content": "However, in order to estimate pmix, we would need to estimate the likelihood for dLLM, which is extremely challenging, as also highlighted in [2, 3, 16]. Ideally, we would compute the Bayes-optimal conditional probability pmix(xi = xprompt) by fully marginalizing over all possible sequences of tokens in . In practice, this is infeasible for diffusion LLMs, since their likelihood is intractable and must be approximated. To sidestep this, we approximate the ideal mixture by ensemble-averaging over small set of semi-autoregressive, each defined by particular block-schedule B. Concretely, we sample several semi-AR decoding schedules b, each of which queries exactly one expert Ub, and then averaging: pmix(xi = xprompt) EbB (cid:2)pθ(xi = [xprompt, x[Ub]])(cid:3). (4) The final prediction can be made using the Bayes rule ˆa = arg maxa pmix(xi = xprompt). simple Monte Carlo approximation of this decision rule is majority vote: draw one sample ˆab from each queried expert Ub, and return the mode of the sampled values. Thus, either by mixture averaging or majority vote, test-time aggregation amplifies the correct prediction by combining the specific conditionals the model actually learned. Based on this insight, we summarize our proposed approach in Algorithm 2. Algorithm 2: Hidden semi-autoregressive EXperts for test-time scaling Input: prompt xprompt, output length L, output parser , steps , experts B; semi-AR mask schedule t=1}B {{Ut,b}T Initialize outputs [ ]; for = 1, 2, . . . , do b=1 , model pθ(). Initialize: x(0) [MASK]L; for = 1, 2, . . . , do Predict all masked tokens simultaneously via pθ x(t) Fill with predicted tokens; Fix tokens at location t,b Mask again tokens at location Ut,b Append parsed output (x(T )) to outputs; t1 k=1U k,b (cid:16) ; (cid:17) (cid:0) [xprompt, x(t1)](cid:1) ; Output: majority of outputs, earliest if tie Why semi-autoregressive? Diffusion LLMs allow all trajectories to reveal masked tokens, but uniformly 6 random orders are suboptimal for language: they create unnatural partial contexts that the model was never intended to generate at test time. practical Table 1 Semi-AR based decoding eliminates [Afrestriction is semi-autoregressive left-to-right decoding terEoT] collapse and improves accuracy. (semi-AR): fix block size {1, . . . , Bmax} (where Bmax = n) and partition [n] into consecutive blocks Collapsed (, %) Accuracy (, %) Dataset Mt = {(t 1)b + 1, . . . , min(tb, n)}, Baseline (nonsemi-AR) GSM8K 55.80 MATH 29.80 22.52 16.60 for = 1, . . . , (b) = n/b, revealing blocks left-to-right while denoising within each block via diffusion. This preserves strong prefix structure (natural for language), yet allows parallel denoising inside block. Empirically, we find (see Table 1) that semi-AR decoding avoids pathologies seen in fully parallel decoding. In particular, when using single large block (i.e., non-semi-AR parallel decode), we often observe an [AfterEoT] collapse: the model erroneously floods the tail with [AfterEoT] tokens or repeats (Figure 8). By contrast, constraining to moderate block sizes (decoding left-to-right) eliminates this collapse and dramatically improves accuracy. (See Table 1: semi-AR has 0% collapse and much higher accuracy than non-AR decoding.) Intuitively, focusing first on the left part of the output prevents the model from prematurely committing to length or drifting with high-confidence tail tokens. In addition, the semi-AR setting (Figure 4) follows the pattern observed in the toy example (Figure 3) we presented above. Semi-AR GSM8K MATH 76.27 32.80 0.00 0. Figure 4 When asked about the 2024 Turing Award winners, names other than the actual recipients (such as Michael or David) might be generated due to different block sizes, which in turn risks producing incorrect information in the subsequent token sequence. However, if we generate outputs with various block sizes and then select the most frequently produced answer, that answer is more likely to be correct, since it was probably derived through valid reasoning (Andrew) during the inference process. 7 Figure 5 HEX improves reasoning accuracy. On LLaDA-8B-Instruct, HEX outperforms training-free baselines (Random, Top-k, Top-k-margin) on GSM8K, MATH, ARC-C, and TruthfulQA. In GSM8K, MATH, ARC-C, it even outperforms the model trained with GRPO without any training."
        },
        {
            "title": "6 Experiments",
            "content": "(i) Effectiveness: We first demonstrate that HEX In this section, we empirically validate our claims. significantly outperforms existing training-free and fine-tuned methods on suite of reasoning benchmarks. (ii) Scaling behavior: We then analyze the performance-computation trade-off, showing how accuracy scales with more diverse generation paths. (iii) Working mechanism: Finally, through series of ablations and qualitative examples, we explore the mechanisms behind HEXs success, confirming that its gain comes from ensembling latent mixture of semi-AR experts rather than relying on heuristics like model confidence."
        },
        {
            "title": "6.1 Setup\nDatasets and Metrics. We follow standard reasoning benchmarks: GSM8K [27] consisting of high-quality\nproblems with diverse linguistic expressions, MATH [4] is a more challenging math benchmark that includes\ncompetition-level math problems, ARC-C [5] is the Challenge Set from AI2’s ARC dataset, consisting of\nscience knowledge-based questions that are difficult to solve with simple keyword matching or retrieval, and\nTruthfulQA [6] which evaluates the tendency of language models to generate false information by following\nhuman misconceptions or false beliefs.1 Primary metric is task accuracy.",
            "content": "Models and Baselines. All experiments with inference methods were performed using the LLaDA-8B-Instruct model [2], and the application of d1 (GPRO) [3] is subsequently based on this model. For all methods, when the output length is 256 tokens, the number of unmasking steps is 128. At each step, two masked tokens are unmasked, and this process is repeated until all tokens are revealed. Random unmasks two randomly chosen masked tokens per step. Top-k margin unmasks, at each step, the two masked tokens with the highest margin defined as (top-1 confidence top-2 confidence) at their positions. d1 (GRPO) row uses the reported best value [3] for GSM8K and MATH, and for ARC-C we report value reproduced after 1 epoch of training. TruthfulQA trained on d1 (GRPO) is excluded because there is no training data available, and neither were checkpoints released. HEX draws five samples at temperature = 0.9 for each of the block sizes [8, 16, 32, 64, 128], yielding 25 samples in total. If tie occurs for the most frequent value, the value generated with the smallest block size is selected (Algorithm 2)."
        },
        {
            "title": "6.2 Main Results: HEX Establishes a New State-of-the-Art\nOverall performance. Figure 5 shows that HEX achieves the strongest results across all four reasoning\nbenchmarks, outperforming both training-free and fine-tuned baselines. Compared to existing decoding",
            "content": "1We use official evaluation scripts; numeric parsing strips LaTeX wrappers/whitespace/commas. 8 strategies [1, 2], HEX delivers large and consistent gains. In GSM8K, for example, HEX reaches 88.10% accuracy, far higher than Random decoding (50.87%) and Top-k margin (24.72%). These results show that confidence-based heuristics are unreliable in diffusion LLMs, whereas consensus-based voting in HEX is robust (Figure 7). Comparison with GRPO fine-tuned models. Perhaps most strikingly, HEX also surpasses d1 (GRPO), which requires costly reinforcement learning fine-tuning. On GSM8K (88.10% vs. 79.80%), MATH (40.00% vs. 37.20%), and ARC-C (87.80% vs. 82.68%), HEX sets new state of the art without updating model parameters. Intuitively, fixed inference scheduled in existing techniques sometimes asks the model to guess hard tokens too early, which leads to mistakes. In contrast, HEX tries several semi-autoregressive schedules and then picks the answer that many schedules agree on. In practice, answers that show up across schedules are more reliable than answers from any single schedule. Takeaway. These results suggest that the reasoning ability of diffusion LLM remains latent and can be unlocked at inference time through block-marginalized ensembling, without any fine-tuning."
        },
        {
            "title": "6.3 Analysis of Scaling and Compute Trade-off",
            "content": "Figure 6 shows that HEXs accuracy improves monotonically as the number of voting samples increases, while the tie rate, an indicator of ambiguity, steadily declines. Intuitively, different semi-AR schedules make different mistakes but tend to agree on the correct answer; adding schedules cancels schedule-specific errors and strengthens consensus, so ties resolve and accuracy improves. This trend holds consistently across all four benchmarks. Because sampling more trajectories linearly increases compute cost, HEX effectively exposes tunable accuracy, compute knob: practitioners can trade inference cost for accuracy in predictable way, without retraining. Figure 6 As the number of majority voting samples in HEX increases, accuracy improves and the tie rate decreases. The block sizes used are [8, 16, 32, 64, 128], and sampling was performed while increasing the number of seeds (1-6). Takeaway. HEX not only establishes state-of-the-art performance but also provides principled mechanism for test-time scaling, ensuring accuracy improves with more inference budget."
        },
        {
            "title": "6.4 Ablation Studies",
            "content": "Next, we analyze the mechanisms behind the HEX improvements, focusing on two key factors: the role of block diversity and the role of likelihood versus frequency in candidate selection. Effect of block diversity. Beyond using fixed set of block sizes, we test whether ensembling over more varied (and even randomly generated) block schedules further boosts performance. As shown in Table 2, increasing the number of dynamic trajectories from 5 to 30 on GSM8K improves accuracy from 81.96% to 84.15% while reducing the tie rate to less than half. This reinforces our hypothesis that performance gains come from aggregating diverse semi-AR experts. We note that diversity matters, but structured diversity (fixed block set with multiple seeds) is even stronger (as in Table 3), yielding the highest overall gains. Frequency vs. likelihood. We then examine whether HEXs gains could simply come from likelihood-based re-ranking. Table 3 shows that the selection of the lowest negative log-likelihood candidate (NLL) performs 9 Table 2 HEX dynamic block size results. Accuracy and tie rate (%) on GSM8K across dynamic block size. See Figure 10 for details. Size Accuracy ( %) Tie ( %) 5 10 15 20 81.96 82.34 82.49 82.79 83.47 3.87 3.18 1.59 1.59 1.52 30 84.15 1.06 poorly, in some cases worse than Random decoding (e.g., ARC-C: 70.05% vs. 60.84%). In contrast, HEXs frequency-based majority vote achieves much higher accuracy (74.57%), confirming that consensus among diverse trajectories is more reliable than model confidence scores. This shows that the key driver of HEXs success is ensemble agreement. Tie break and Latency. HEX defaults to the smallest block size in tie situations, as Table 5 indicates that jointly considering frequency and log-likelihood does not bring clear advantage. In addition, we present the wall-time latency of HEX and the baseline inference methods in Table 6. Table 3 Ablations across datasets. NLL selects the candidate with the lowest NLL. HEXs tie issue diminishes as the number of samples increases. Block sizes: [8, 16, 32, 64, 128]. GSM8K MATH ARC-C TruthfulQA Acc (%) Tie (%) Acc (%) Tie (%) Acc (%) Tie (%) Acc (%) Tie (%) 50.87 22.52 24.72 79.80 16.80 16.60 16.40 37.20 70.05 47.87 54.18 82.68 42.40 29.82 28.36 Method Baselines Random top-k top-k margin d1 (GRPO) Likelihood-based NLL HEX HEX HEX 5 seeds 76.72 4.09 34.40 16. 60.84 2.99 28.07 4.24 82.18 88. 4.09 1.36 38.40 40.00 16.00 10. 74.57 87.80 2.99 1.11 45.91 57. 4.24 2."
        },
        {
            "title": "7 Conclusion and Limitation",
            "content": "In this work, we study how diffusion-based language models (dLLMs) generate text. We found that their performance is fundamentally tied to the decoding schedule, the order in which tokens are generated. This is because dLLMs implicitly learn \"set\" of semi-autoregressive experts during training. Different schedules activate different experts, and choosing the right one is crucial for getting high-quality answer. This single insight helps explain common dLLM issues, such as why they sometimes stop generating text too early or fail even when they seem confident. Based on this insight, we introduced HEX (Hidden semi-autoregressive EXperts), powerful inference method that requires no extra training. Instead of relying on single schedule, HEX tries many different schedules at once and lets the experts \"vote\" on the best final answer. By combining the strengths of the entire hidden team, HEX turns the models flexibility into reliable tool for boosting performance. On challenging reasoning benchmarks, HEX doesnt just beat standard methods; it even surpasses models fine-tuned with costly techniques like reinforcement learning (GRPO). 10 HEX has some limitations. It requires more computation at test time, and we have mainly evaluated it on reasoning tasks. Applying this method to more creative areas like open-ended stories, image generation, or long conversations remains promising area for future work. Further, we have not established any theoretical understanding of HEX, which is valid scope of future work."
        },
        {
            "title": "References",
            "content": "[1] Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. [2] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [3] Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. [4] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [6] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. [7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [8] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [9] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. [10] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. [11] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, and Jonas Adler. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022. [12] Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. [13] Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [14] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. [15] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. [16] Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. Weighted policy optimization for reasoning in diffusion language models. arXiv preprint arXiv:2507.08838, 2025. [17] Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. [18] Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, and Xiaoyan Sun. Llada-vla: Vision language diffusion action models. arXiv preprint arXiv:2509.06932, 2025. 11 [19] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. [20] Jiaming Zhou, Hongjie Chen, Shiwan Zhao, Jian Kang, Jie Li, Enzhi Wang, Yujie Guo, Haoqin Sun, Hui Wang, Aobo Kong, Yong Qin, and Xuelong Li. Diffa: Large language diffusion models can listen and understand. arXiv preprint arXiv:2507.18452, 2025. [21] Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, and Yalin Wang. Llada-medv: Exploring large language diffusion models for biomedical image understanding. arXiv preprint arXiv:2508.01617, 2025. [22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [23] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [24] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [25] Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, and Lingpeng Kong. Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models. arXiv preprint arXiv:2402.07754, 2024. [26] Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, and Chunhua Shen. Time is feature: Exploiting temporal dynamics in diffusion language models. arXiv preprint arXiv:2508.09138, 2025. [27] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [28] GSAI-ML. LLaDA-8B-Instruct. https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct. MIT License, accessed 2025-09-18. [29] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, s1: Simple test-time scaling. arXiv preprint Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. arXiv:2501.19393, 2025. [30] Learning to reason with llms. 2024. [31] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [32] Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning, 2025. [33] Daman Arora and Andrea Zanette. Training language models to reason efficiently. 2025. [34] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: Llm learns when to think. arXiv preprint arXiv:2505.13379, 2025. [35] Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Haodong Zhao, Hao Li, Jiansong Chen, Ke Zeng, and Xunliang Cai. When to continue thinking: Adaptive thinking mode switching for efficient reasoning. arXiv preprint arXiv:2505.15400, 2025. [36] Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, and Furu Wei. Think only when you need with large hybrid-reasoning models. arXiv preprint arXiv:2505.14631, 2025. [37] Guosheng Liang, Longguang Zhong, Ziyi Yang, and Xiaojun Quan. Thinkswitcher: When to think hard, when to think fast. arXiv preprint arXiv:2505.14183, 2025. [38] Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417, 2025. 12 [39] Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, and Yi Fung. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting. arXiv preprint arXiv:2505.18822, 2025. [40] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. [41] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [43] Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Args: Alignment as reward-guided search. arXiv preprint arXiv:2402.01694, 2024. [44] James Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchhoff, and Dan Roth. Deal: Decoding-time alignment for large language models. arXiv preprint arXiv:2402.06147, 2024. [45] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023. [46] Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, and Furong Huang. Transfer star: Principled decoding for llm alignment. arXiv preprint arXiv:2405.20495, 2024. [47] Ruizhe Shi, Yifang Chen, Yushi Hu, Alisa Liu, Hannaneh Hajishirzi, Noah A. Smith, and Simon S. Du. Decodingtime language model alignment with multiple objectives. The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024."
        },
        {
            "title": "A Qualitative results",
            "content": "A.1 Qualitative analysis of Baselines vs. HEX Figure 7 An instance of generated text responses of different decoding strategies. A.2 Qualitative analysis of semi-AR vs. non-semi-AR As shown on the right side of Figure 8 in confidence-based non-semi-AR decoding, the phenomenon where [AfterEoT] tokens accumulate from the end of the output towards the front indicates that the model is assigning high confidence to [AfterEoT] tokens throughout the unmasking steps. The input to the dLLM consists of the number of tokens that make up the prompt and the number of tokens in the desired output sequence, and during training it is subject to limits on the input sequence length for parallel computation. For LLaDA-8B-Instruct [28], this limit is 4,096 tokens. However, in the training of reasoning tasks, most of the output finishes within 256 tokens. In other words, the majority of ground truth tokens in the output sequence (more than 93.75%) are [AfterEoT]: Given that the training objective is to maximize the average likelihood, we can infer that the dLLM is most strongly taught to generate the [AfterEoT] token. This suggests that confidence-only decoding is fundamentally limited in its ability to prevent such phenomena during inference, and highlights why the positions of tokens to be unmasked should not be selected based solely on confidence. Figure 8 Blue denotes mask tokens, red denotes [AfterEoT] tokens, white denotes text tokens, and green denotes [EoT] tokens (note that in the LLaDA-8-Instruct model, [EoT] and [AfterEoT] are represented as < eot_id > and < endof text >, respectively [28]). As unmasking proceeds, two mask tokens are unmasked at each step (output length = 256, unmasking steps = 128). Under semi-AR regime with block size = 32, positional constraints force reasoning to progress left-to-right while still allowing diffusion-style generation within each block. By contrast, when the positional constraint is removed with block size = 256 (non-semi-AR), the model starts from the last token with the highest confidence[AfterEoT]and, due to the inertia of repeatedly generating the same token backward, ultimately collapses into catastrophic output in which all tokens become [AfterEoT]. Figure 9 Few cases (2.96%) where the block size of output length hits and semi-AR fails. However, in those cases, the unmasking order converges from both ends toward the center. It means the model commits to an answer before engaging in full reasoning processi.e., it states the answer first and only then provides derivation. This is not only unintuitive; [26] shows that the answer the dLLM settles on can flip repeatedly during the reasoning process. Consequently, when the model locks in an answer first, it is hard to claim that it truly knows the answer with certainty. 15 Furthermore, confidence-only decoding can lead to issues such as those shown in Figure 9, even when the answer is correct. This arises in reasoning tasks where the correct answer should be derived through step-by-step justification. Instead, the model often generates the answer first without providing supporting reasoning (with the answer tokens generated on the right first), and only afterward begins to generate the reasoning (by unmasking tokens in the middle). Fundamentally, unless the already-generated answer tokens are revised, this generation pattern means that the reasoning cannot actually influence the answer. Besides, as observed in [26], the probability distribution corresponding to the answer tokens, whether they are correct or incorrect, continues to undergo substantial changes during the reasoning process. This indicates that the models confidence in the produced early answer cannot be considered reliable. Additional Details, Examples, and Results Which Can be Useful B.1 Additional context of related work Test-Time Scaling in Autoregressive LLMs. growing body of work in autoregressive LLMs has begun to formalize the phenomenon of test-time scaling, the empirical observation that allocating more inferencetime compute (e.g., longer reasoning traces or more sampled candidates) can significantly improve model performance without retraining. [29] introduced budget forcing to emulate the compute-dependent reasoning behaviors observed in o1-style models [30], while [31] proposed length-controlled policy optimization to explicitly regulate the reasoning horizon. There are also complementary efforts, such as thinking-optimal scaling [32] and adaptive reasoning policies [3339], that highlight the goal of dynamically adjusting cognitive effort based on task complexity and available compute budgets. In parallel, researchers have explored how controlled sampling and decoding strategies can steer model behavior toward desired objectives without fine-tuning. Best-of-N selection [4042] provides simple form of test-time search (which requires reward signal access) over diverse outputs. We also include an ablation where we compare an instance of Best-of-N with majority vote as the reward signal in Table 4. We note that this is lower than random sampling (cf. Figure 5(a)). On the other hand, more structured methods recast decoding as reward-guided search [4346]. Recent multi-objective decoding frameworks [47] further generalize this idea, balancing several alignment criteria simultaneously. Table 4 Comparison of majority voting between another nonsemi-AR (BoN) and semi-AR experts on GSM8K. The number preceding [ ] represents the output length, and the numbers inside [ ] correspond to the block sizes used. Method 256 [256, 256, 256, 256, 256], BoN with majority voting 256 [8, 16, 32, 64, 128], HEX Accuracy ( %) Tie ( %) 44. 82.18 44.58 4.09 16 B.2 Details of dynamic HEX block settings Following the semi-AR left-right generation scheme while dynamically adjusting block size induces more diverse trajectories, thereby increasing candidate diversity in majority voting. However, as shown in Table 2 and Table 3, it does not empirically outperform structured diversity (a fixed block set with multiple seeds). Figure 10 Examples of the block sizes and counts used in the dynamic HEX block settings. Block sizes and counts were randomly chosen and adjusted to match the total output length. The output length is 256 and the number of unmasking steps is 128, meaning that each step unmasks 2 tokens. Accordingly, all block sizes are multiples of 2, and decoding was performed in semi-autoregressive manner. B.3 Experimental results of HEXs tie-breaking methods Frequency-based majority voting has the characteristic that ties can occur within finite set of candidates. To determine single final output, we experimented with various tie-breaking rules. We found that likelihoodbased selection did not provide any significant benefit. As shown in Figure 11, since we observed that performance tends to degrade as the block size approaches the output length, we adopt selecting the output with the smallest block size in tie situations as our default setting. Table 5 Evaluation on tie breaking methods. If the most frequent output is in tie situation, TIED: NLL selects the result with the lowest negative log-likelihood in tie situations, TIED: first selects the result generated from the smallest block size when tied, and TIED: any treats the case as correct if correct option exists among the tied candidates. The results of TIED: any clearly highlight that majority voting of HEX works well across datasets. GSM8K MATH ARC-C TruthfulQA Method Acc (%) Tie (%) Acc (%) Tie (%) Acc (%) Tie (%) Acc (%) Tie (%) HEX (tie-breaking rules) HEX, TIED: NLL HEX, TIED: first HEX, TIED: any 82.18 82.18 83. 4.09 4.09 4.09 38.00 38.40 41.00 16.00 16.00 16.00 74.49 74.57 76.11 2.99 2.99 2.99 46.20 45.91 47. 4.24 4.24 4.24 17 Figure 11 Accuracy across tasks when the block size (B) is increased in fixed increments of 16. Here, block count = (256 // B) when 256 % is 0 otherwise (256 // B) + 1. Despite the uniform increase in block size, the performance drops drastically in the 128256 range. B.4 Analysis of decoding latency across inference methods We measured the latency of each inference method through wall-time evaluation. As shown in Figure 6, increasing the number of candidate samples used in HEXs majority voting leads to improved performance and lower tie rate. This indicates that the increase in inference cost for HEX is accompanied by clear and consistent improvement in performance. Table 6 Decoding latency across different inference methods. We report the generation time (in seconds) for GSM8K, along with the corresponding scaling factor. Method random top-k top-k margin HEX GSM8K test dataset (1319) per batch (8) per datapoint (1) ratio 2775.73 2921.70 3187.56 14613.72 16.76 17.64 19.25 88.23 2.09 2.20 2.41 11. 1.00 1.05 1.15 5.26 18 B.5 Further semi-AR analysis Figure 12 shows that it is rare for outputs generated with different block sizes under the same prompt to all fail simultaneously. It also reveals that no particular block size consistently dominates in performance. Through qualitative analysis, we identified the existence and patterns of block sizes that successfully lead to correct answers for specific prompts, namely, their frequency of agreement, as illustrated in Figure 13. Figure 12 Visualization of the correct (blue) and incorrect (white) answers for each data point in GSM8K when reasoning with various block sizes. Apart from the case where the block size is equal to the output size, i.e., nonsemiAR generation (256), there is no consistent pattern across block sizes in this figure that would allow us to conclude which block size is most suitable for semi-AR reasoning for given tasks. However, the observation that it is rare for all semi-AR outputs to fail on given data point suggests that the model has the potential to arrive at the correct answer even without additional training, simply by selecting the appropriate decoding policy for the question. Figure 13 For the same input prompt (blue), the output (orange) varies depending on the block size, because the accumulated context changes at each unmasking step of the reasoning process. Refer to Figure 4. 19 B.6 Ablation study of HEX across different output lengths Across various output lengths, HEX consistently proves to be an effective method. As shown in Table 7, for output lengths of 128, 256, and 512, HEX consistently surpasses both other inference methods and GRPO trained models. Table 7 Ablation study of HEX across output lengths of 128, 256. The results demonstrate that HEX consistently exhibits robust performance irrespective of output length. Block sizes used are the same as Table 8. Refer to Table 5 for detailed explanation about (b), (c), (d). Method Random top-k top-k margin d1 (GRPO) NLL (a) HEX, TIED: NLL (b) HEX, TIED: first (c) HEX, TIED: any (d) HEX 5 seeds, TIED: first (e) GSM8K (acc, tied) MATH (acc, tied) ARC-C (acc, tied) TruthfulQA (acc, tied) Output Length 128 48.82%, 53.30%, 55.57%, 72.60%, 70.81%, 7.28% 73.69%, 7.28% 74.22%, 7.28% 75.82%, 7.28% 20.60%, 20.60%, 22.60%, 33.20%, 29.80%, 20.20% 32.00%, 20.20% 30.60%, 20.20% 35.60%, 20.20% 66.21%, 56.74%, 61.35%, 82.68%, 78.41%, 1.54% 85.24%, 1.54% 85.41%, 1.54% 85.92%, 1.54% 41.96%, 36.40%, 38.30%, , 47.95%, 4.24% 52.63%, 4.24% 53.36%, 4.24% 54.39%, 4.24% 81.05%, 3.64% 33.40%, 15.60% 88.40%, 0.60% 53.80%, 2.78% Method Random top-k top-k margin d1 (GRPO) NLL (a) HEX, TIED: NLL (b) HEX, TIED: first (c) HEX, TIED: any (d) HEX 5 seeds, TIED: first (e) GSM8K (acc, tied) MATH (acc, tied) ARC-C (acc, tied) TruthfulQA (acc, tied) Output Length 256 50.87%, 22.52%, 24.72%, 79.80%, 76.72%, 4.09% 82.18%, 4.09% 82.18%, 4.09% 83.09%, 4.09% 16.80%, 16.60%, 16.40%, 37.20%, 34.40%, 16.00% 38.00%, 16.00% 38.40%, 16.00% 41.00%, 16.00% 70.05%, 47.87%, 54.18%, 82.68%, 60.84%, 2.99% 74.49%, 2.99% 74.57%, 2.99% 76.11%, 2.99% 42.40%, 29.82%, 28.36%, , 28.07%, 4.24% 46.20%, 4.24% 45.91%, 4.24% 47.66%, 4.24% 88.10%, 1.36% 40.00%, 10.20% 87.80%, 1.11% 57.46%, 2.78% Method Random top-k top-k margin d1 (GRPO) NLL (a) HEX, TIED: NLL (b) HEX, TIED: first (c) HEX, TIED: any (d) HEX 5 seeds, TIED: first (e) GSM8K (acc, tied) MATH (acc, tied) ARC-C (acc, tied) TruthfulQA (acc, tied) Output Length 512 51.71%, 9.10%, 10.08%, 81.90%, 78.85%, 4.25% 82.11%, 4.25% 82.03%, 4.25% 83.47%, 4.25% 16.60%, 10.00%, 9.40%, 39.20%, 25.00%, 20.80% 40.20%, 20.80% 40.20%, 20.80% 44.80%, 20.80% 73.29%, 50.17%, 54.35%, 83.28%, 63.82%, 5.03% 83.11%, 5.03% 84.13%, 5.03% 84.81%, 5.03% 43.86%, 29.68%, 31.14%, , 34.50%, 7.75% 54.97%, 7.75% 55.85%, 7.75% 57.75%, 7.75% 88.40%, 1.14% 47.60%, 10.40% 87.12%, 0.85% 59.80%, 2.63% 20 Table 8 illustrates that the average performance of the samples used in HEXs majority votingthat is, the individual semi-AR expertsconsistently falls short of HEX across all output lengths (128, 256, and 512). This indicates that dLLMs have implicitly learned diverse set of inference behaviors from multiple semi-AR experts. Therefore, uncovering and leveraging the correct judgment among these hidden experts is essentialan ability that HEX demonstrates with remarkable effectiveness. Method 128 [4, 8, 16, 32, 64] 1 seed 256 [8, 16, 32, 64, 128] 1 seed 512 [16, 32, 64, 128, 256] 1 seed 128 [4, 8, 16, 32, 64], HEX 1 seed 256 [8, 16, 32, 64, 128], HEX 1 seed 512 [16, 32, 64, 128, 256], HEX 1 seed GSM8K MATH ARC-C TruthfulQA 68.57 75.39 66. 74.22 82.18 83.24 28.00 33.16 32.88 30.60 38. 39.60 79.44 67.27 66.35 85.41 74.57 84.81 48.74 39.59 44. 53.36 45.91 57.02 Table 8 HEX (the bottom three rows) consistently surpasses the mean accuracy (the top three rows) of samples used in majority voting across various output length settings. The number preceding [ ] represents the output length, and the numbers inside [ ] correspond to the block sizes used."
        }
    ],
    "affiliations": [
        "CMU",
        "Oracle",
        "UCF",
        "UMD",
        "Yonsei University"
    ]
}