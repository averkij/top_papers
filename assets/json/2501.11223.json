{
    "paper_title": "Reasoning Language Models: A Blueprint",
    "authors": [
        "Maciej Besta",
        "Julia Barth",
        "Eric Schreiber",
        "Ales Kubicek",
        "Afonso Catarino",
        "Robert Gerstenberger",
        "Piotr Nyczyk",
        "Patrick Iff",
        "Yueling Li",
        "Sam Houliston",
        "Tomasz Sternal",
        "Marcin Copik",
        "Grzegorz Kwaśniewski",
        "Jürgen Müller",
        "Łukasz Flis",
        "Hannes Eberhard",
        "Hubert Niewiadomski",
        "Torsten Hoefler"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with a broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and experimentation."
        },
        {
            "title": "Start",
            "content": "Reasoning Language Models: Blueprint Maciej Besta1, Julia Barth1, Eric Schreiber1, Ales Kubicek1, Afonso Catarino1, Robert Gerstenberger1, Piotr Nyczyk2, Patrick Iff1, Yueling Li3, Sam Houliston1, Tomasz Sternal1, Marcin Copik1, Grzegorz Kwasniewski1, urgen uller3, Łukasz Flis4, Hannes Eberhard1, Hubert Niewiadomski2, Torsten Hoefler1 Corresponding author 1ETH Zurich 2Cledar 3BASF SE 4Cyfronet AGH 1 AbstractReasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAIs o1 and o3, DeepSeek-V3, and Alibabas QwQ, have redefined AIs problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architecturesuniquely combining Reinforcement Learning (RL), search heuristics, and LLMspresent accessibility and scalability challenges. To address these, we propose comprehensive blueprint that organizes RLM components into modular framework, based on survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprints versatility and unifying potential. To illustrate its utility, we introduce x1, modular implementation for rapid RLM prototyping and experimentation. Using x1 and literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between rich AI and poor AI by lowering barriers to RLM development and experimentation. Index TermsSurvey of Reasoning Language Models, Survey of RLMs, Reasoning Language Model, Large Reasoning Model, RLM, LRM, Reasoning LLMs, Reinforcement Learning for LLMs, MCTS for LLMs, Large Language Model, LLM, Generative AI."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Reasoning Language Models (RLMs), such as OpenAIs o1 [66], o3 [48], and Alibabas QwQ [88], also referred to as Large Reasoning Models (LRMs)1, represent transformative breakthrough in AI, on par with the advent of ChatGPT [64]. These advanced systems have fundamentally redefined AIs problem-solving capabilities, enabling nuanced reasoning, improved contextual understanding, and robust decision-making across wide array of domains. By extending the capabilities of standard large language models (LLMs) with sophisticated reasoning mechanisms, Corresponding author 1We use the term Reasoning Language Model instead of Large Reasoning Model because the latter implies that such models are always large. This does not necessarily have to be the case as matter of fact, smaller RLM can outperform larger LLMs [36]. RLMs have emerged as the new cornerstone of cutting-edge AI, bringing us closer to AGI. However, the high cost and proprietary nature of stateof-the-art RLMs, such as those developed by OpenAI, risk exacerbating the divide between rich AI and poor AI, raising significant concerns about accessibility and equity. Even the publicly available QwQ only comes with its model weights, and Alibaba does not disclose details about their training or data generation methodologies. Businesses and individuals unable to afford these advanced systems face growing disadvantage, threatening to stifle innovation and reinforce systemic inequities. As RLMs become integral to critical applications, from healthcare to science, management, and beyond, it is imperative to address these disparities and ensure that the benefits of advanced reasoning capabilities are broadly accessible. 5 2 0 2 J 0 2 ] . [ 1 3 2 2 1 1 . 1 0 5 2 : r Fig. 1: Summary of the contributions made by this paper. Fig. 2: The history of RLMs. This class of models has been the result of the development of three lines of works: (1) Reinforcement Learning based models such as AlphaZero [79], (2) LLM and Transformer based models such as GPT-4o [65], and (3) the continuous growth of compute power and data processing capabilities of supercomputers and high performance systems. The technical foundations of RLMs remain opaque and complex, compounding the accessibility challenge. Emerging analyses suggest that their design likely integrates elements such as Monte Carlo Tree Search (MCTS), reinforcement learning (RL), process-based supervision (PBS) [55], [55], [91], [91], and advanced in-context learning (ICL) techniques like Chain-of-Thought (CoT) [98] or Tree of Thoughts (ToT) [101], and possibly even retrieval-augmented generation (RAG) [10], [37], [51], [52]. Additionally, these architectures employ multiple specialized subcomponentssuch as synthetic data generation engines and policy, value, and reward modelstrained through some form of novel loss functions and possibly several fine-tuning schemes. However, the intricate interplay of these components and their integration into cohesive and effective architecture remains poorly understood. Here, the holy-grail question is: what is the detailed design of an RLM and how to make it simultaneously achieve effectiveness (i.e., high accuracy in delivered answers), low cost, and scalability? To help answer this question and to address the above challenges, we propose comprehensive blueprint for constructing, analyzing, and experimenting with RLMs (contribution #1; roadmap of all the contributions and the paper is in Figure 1). Our approach identifies and crystallizes the fundamental building blocks of RLMs, organizing them into cohesive framework. This blueprint is presented with increasing levels of granularity, starting from highlevel overview, finishing at low-level details that can be directly harnessed when implementing. Further, to maximize the clarity and comprehensiveness, we present the blueprint using three perspectives: (1) architecture diagrams and descriptions, (2) detailed mathematical formulations, and (3) in-depth algorithmic specifications. By employing these complementary perspectives, we aim to provide clear and actionable guide for developing RLMs tailored to specific applications, settings, and constraints. Our blueprint comprehensively encompasses the potential building blocks of RLMs, offering flexible and modular framework. It incorporates variety of reasoning structures, such as chains, trees, graphs, and even higherorder structures such as hierarchical (or nested) trees, along with numerous operations that transform and advance the reasoning process. The blueprint supports different granularities of reasoning steps, ranging from individual tokens to full sentences or structured segments. Additionally, it enables diverse training schemes, including Output-Based Supervision (OBS) and PBS. Furthermore, in order to illustrate the capability of the blueprint to accommodate novel design ideas, we describe several novel schemes and how they fit within the blueprint. One such example is TraceBased Supervision (TBS), which extends PBS by incorporating labeled traces of traversal paths through entire reasoning structures, rather than just linear chains of reasoning steps. By unifying all these components, our blueprint serves as versatile toolbox for constructing RLMsranging from simple models to sophisticated designstailored to specific reasoning tasks and performance objectives. We conduct broad analysis of existing reasoning 3 Fig. 3: Hierarchy of language models (right) and the three pillars of RLMs (left). schemes (contribution #2), demonstrating how they fit into our blueprint as special cases. This analysis encompasses not only standard MCTS and reinforcement learningbased designs, such as LLaMA-Berry [105], but also models like QwQ [88]. Additionally, we include paradigms diverging from standard MCTS, such as Journey Learning [69], which redefines reasoning through implicit longchain structures, and advanced structured prompting techniques like CoT [98], ToT [101], and Graph of Thoughts [6]. By mapping these diverse approaches to one blueprint, we showcase its versatility and expressive power, highlighting its ability to unify wide range of reasoning methodologies within coherent framework. To demonstrate the utility of our framework, we introduce x1, modular and user-friendly implementation2 designed to simplify the process of developing and experimenting with new RLM architectures, covering not only training and inference, but also synthetic data generation (contribution #3). We design x1 to facilitate supporting various optimizations, design decisions, and overall scalability, such as batch processing, making it well-suited foundation of experimentation infrastructure. By providing both theoretical insights and practical tools, this work aims to democratize access to advanced RLMs, enabling researchers and practitioners to design, train, and deploy sophisticated reasoning models with reduced complexity and cost. Our blueprint offers clear and adaptable framework that lowers the barriers to entry, fostering broader experimentation and innovation. Additionally, the modular implementation of x1 serves as foundation for rapid prototyping and large-scale experimentation, empowering users to explore new reasoning paradigms and optimize performance across diverse applications. By bridging the gap between conceptual advancements and practical implementations, this work seeks to accelerate progress in the field, unlock new possibilities for intelligent systems across research, industry, and education, and to mitigate the risk of the growing gap between rich AI and poor AI. 2https://github.com/spcl/x"
        },
        {
            "title": "2 EVOLUTION & FOUNDATIONS OF RLMS\nWe first summarize the evolution and foundations of rea-\nsoning language models. Figure 2 shows an overview of the\nhistory of the development of these models.",
            "content": "2.1 Basic Pillars of Reasoning LMs The development of reasoning-capable LLMs represents convergence of three critical threads: (1) advances in LLMs such as GPT-4, (2) RL designs such as AlphaZero, and (3) High-Performance Computing (HPC) resources. Together, these threads have shaped models capable of efficient System 2 Thinking level of reasoning that combines explicit deliberation with novel problem-solving abilities, distinct from the intuitive, fast, and automatic heuristics of System 1 Thinking. Figure 2 compares example designs in these pillars while Figure 3 (left side) further discusses the details of these pillars. 2.1.1 Large Language Models: Reservoir of Knowledge LLMs such as GPT-4o [65] or Llama [34] represent an extraordinary leap in the field of AI, constituting vast repository of world knowledge encoded directly in their weights. Trained on huge corpora of text from diverse sources, LLMs are capable of understanding and generating human language with remarkable fluency. However, their reasoning abilities largely align with the fast, automatic, and intuitive System 1 Thinking. While they can generate coherent responses and even perform simple reasoning tasks, LLMs have limitations. The reasoning they exhibit is often shallow, rooted in the simple mechanism of predicting the next most probable token in sequence rather than engaging in explicit problem-solving or structured analysis. While LLMs may generate plausible-sounding solutions to problem, these outputs are the result of statistical language modeling rather than deliberate, iterative reasoning process. This distinction highlights the need for integrating more advanced mechanisms capable of explicit reasoning into AI systemspaving the way for hybrid designs that combine the knowledge-rich foundation of LLMs with structured reasoning methodologies. 2.1.2 Reinforcement Learning: Exploring and Innovating RL has historically provided framework for decisionmaking and exploration in environments where an agent must learn optimal strategies through trial and error. Landmark systems like AlphaZero [79] and long line of others such as AlphaGo [78] or MuZero [76] demonstrated the profound potential of RL by achieving superhuman performance in games such as chess, shogi, and Go. Unlike traditional AI systems, AlphaZero began with no embedded domain knowledge. Instead, it mastered these games purely through self-learning, discovering novel strategies that even human experts had not considered. One of the most striking examples of RLs innovative capacity came during an AlphaZero match, where the system made move initially deemed mistake by human observers. This move [61] later proved to be both surprising and strategically brilliant, illustrating the capacity of RL agents to explore unconventional solutions that lie outside the bounds of human intuition. Such capabilities are fundamentally rooted in RLs ability to navigate vast search spaces effectively. However, traditional RL systems lacked the ability to encode real-world knowledge or handle complex, multifaceted reasoning tasks. This limitation spurred the integration of RL principles with LLMs, combining the structured exploration and optimization capabilities of RL with the knowledge-rich reasoning foundation of language models. 2.1.3 HPC: Scalability & Efficiency The growth of LLM and RL systems has been propelled by advancements in High-Performance Computing (HPC). Initially driven by Moores Law, which enabled doubling of transistor density approximately every two years, HPC benefited from both technological advancements and the economic feasibility of manufacturing smaller transistors. However, as the costs of further miniaturization have risen sharply, Moores Law has reached practical limits, necessitating alternative strategies like parallelism and heterogeneous computing. Modern HPC systems rely heavily on GPUs, TPUs, and AI accelerators for their parallel processing capabilities, alongside CPUs for sequential and general-purpose tasks. Heterogeneous computing leverages these components to optimize task-specific performance. Distributed frameworks, employing techniques such as data, model, and pipeline parallelism [5], [9], [13], further enable the training of enormous models across thousands of compute nodes. Energy efficiency innovations, including sparsity, quantization, and pruning, mitigate the growing energy demands of scaling AI systems. These advancements ensure that HPC remains cornerstone for developing and deploying AI models, supporting the combination of vast knowledge, reasoning capabilities, and computational scalability allowing AI evolution to continue beyond the limits of traditional Moores Law scaling. 2.2 The Convergence: System 2 Thinking in AI The intersection of these three threads LLMs, RL, and HPC has culminated in the emergence of models capable of 4 System 2 Thinking. These advanced systems combine the knowledge-rich foundation of LLMs with the exploratory and optimization capabilities of RL, all supported by the scalability and performance of modern HPC. The result is new class of AI models that can engage in explicit, deliberate reasoning processes. These models possess world model encoded in the weights of their LLM components, allowing them to reason about complex scenarios and contexts. Their RL capabilities combined with the HPC capabilities enable them to navigate truly immense decision spaces, evaluate multiple strategies, and iteratively refine solutions. 2.3 Interpolation (LLMs) vs. Extrapolation (RLMs) Standard LLMs, driven by their autoregressive token prediction mechanism, primarily perform interpolation within the vast search space of solutions. They excel at generating responses that align with patterns seen in their training data, effectively synthesizing knowledge from known contexts. However, this process limits them to producing outputs that remain within the boundaries of their training distribution. In contrast, reasoning LMs enable extrapolation beyond these boundaries. By combining structured exploration, reasoning LMs navigate uncharted areas of the solution space, generating novel insights and solutions that extend past the limits of their training data. This enables shift from basic pattern completion to active problem-solving. 2.4 Hierarchy of Reasoning-Related Models The evolution of RLMs can be understood as hierarchical progression, with earlier models such as GPT-4o being less capable in terms of reasoning, and the o1-like architectures demonstrating increasing sophistication and explicit reasoning abilities. This hierarchy reflects the integration of System 1 (LLMs) and System 2 (RLMs) Thinking. RLMs can be further divided based on how reasoning is implemented into Implicit RLMs and Explicit RLMs; the details of this categorization can be found in Figure 3 (the right side). 2.4.1 Implicit Reasoning Models In this subclass, the reasoning structure is embedded entirely within the models weights. Models such as QwQ [88] operate as black boxes, where reasoning is implicit and cannot be explicitly disentangled or manipulated. While these models exhibit improved reasoning capabilities compared to standard LLMs, their reasoning processes are opaque and rely on the internalized patterns learned during training. 2.4.2 Explicit Reasoning Models These models introduce explicit reasoning mechanisms external to the models core weights. Examples include designs such as LLaMA-Berry [105], Marco-o1 [108], and potentially OpenAIs o3, which incorporate mechanisms like explicit MCTS combined with RL for decision-making. This explicit structure enables the model to simulate, evaluate, and refine solutions iteratively, facilitating novel problemsolving and extrapolation. By separating reasoning from the static knowledge encoded in the weights, these models achieve greater flexibility and interpretability in their reasoning processes. Note that the explicit reasoning can be internalized via training making it implicit we discuss it later in the blueprint."
        },
        {
            "title": "3 ESSENCE OF REASONING LMS",
            "content": "We now describe the general architecture of RLMs, which we summarize in Figure 4. In the following sections, we generalize this description to the full RLM blueprint. 3.1 Basic Architecture, Pipelines, & Concepts We now outline the foundational architecture, operational pipelines, and core concepts. Figure 4 offers three levels of detail. In general (the top-left part), the whole RLM architecture consists of three main pipelines: inference, training, and data generation. The inference serves user requests, using models (e.g., the value or policy model) provided by the training pipeline. Data generation mirrors the inference pipeline in its internal design; the main difference is that it runs independently of the user requests, generating data that is then used to re-train the models. As such, training combined with data generation from various domains [74], [104] offers self-learning capabilities and is analogous to the self-play setting of AlphaZero [79]. 3.1.1 Inference The inference process begins when the user provides an input prompt , which typically describes the problem or question to be addressed by the RLM. This input serves as the root of the reasoning process and initiates the construction of reasoning structure that organizes RLMs progress. The structure is usually represented as tree. The root of this tree corresponds to the users input, and subsequent nodes are generated to explore the search space the domain of possible reasoning paths or solutions. The purpose of this reasoning structure is to systematically investigate potential solutions, progressively refining and extending reasoning paths to converge on an optimal or satisfactory answer. An individual point in the search space, represented as node in the reasoning structure, corresponds to reasoning step . reasoning step is defined as coherent and self-contained unit of thought sequence of tokens that advances the solution by either exploring new branch of the problem or building upon existing progress. These steps form the building blocks of the reasoning process. The details of how the structure evolves are usually governed by the MCTS scheme, enhanced with policy and value models (we also distinguish other reasoning strategies, described below). This approach, inspired by methods used in AlphaZero, ensures that the search process is both efficient and directed toward promising solutions. The policy model is responsible for generating new reasoning steps at each node, predicting the next most likely and logical steps to expand the reasoning process. Meanwhile, the value model evaluates the quality of reasoning path starting at given node, helping the system prioritize the most promising steps to follow. Sometimes, the 5 reward model3 could also be used, to assess the quality of an individual specific node and its corresponding reasoning step . In our blueprint, as detailed in the next section, we abstract the models into more general notion of operators to enable more flexibility in how they are implemented. The search and reasoning processes continue iteratively until terminal step is reached . This terminal step represents completion of the reasoning chain that forms the final answer to the posed problem. It serves as the leaf node in the tree, concluding that particular reasoning path. This architecture provides unified framework that accommodates wide range of reasoning tasks. Whether reasoning steps are fine-grained (e.g., individual token sequences) or coarse-grained (e.g., entire reasoning chains treated as single nodes), the architecture adapts seamlessly. By structuring the search space explicitly and guiding exploration with policy and value models, the RLM achieves level of reasoning capability bridging intuitive pattern recognition and deliberate problem-solving. detailed specification of the inference pipeline can be found in Appendix C.1 and in Algorithm 1. 3.1.2 Training Training details depend on what model is trained (value, policy, reward, ...). In general, we assume fine-tuning model such as Llama. Here, we follow an approach where one first harnesses supervised data, usually coming from existing datasets such as PRM800K [55] , which becomes part of the framework supervised training data used in the supervised training pipeline to train some, or all, of the models considered in the blueprint . The second part of the overall training framework in RLMs is the unsupervised (self-learning) training pipeline, in which data is being continually generated and used to improve , the models. The data can be obtained from inference assuming quality control [36], but also from dedicated synthetic data generation pipeline that mirrors that of the . To collect the data, one executes the respecinference tive RLM pipeline for given input task and gathers the results ; depending on how detailed is the gathering process, the data collected can contain only output-based labels , or some other variant such as trace-based labels suggested in our blueprint, that generalize process-based samples to samples that contain also information about operators applied during the task solution process . All this data becomes part of the and is used in the unsupervised training replay buffer scheme or it can also be used to train model that would become an Implicit RLM . , process-based labels detailed specification of the pipelines for different training phases and paradigms can be found in Appendix C.2, Appendix C.3, Appendix C.4, and in Algorithms 27. The data generation pipeline is detailed in Appendix D. 3We use naming scheme in which model used to estimate the quality of whole reasoning path starting at given node, is called the value model, while model used to estimate the quality of given reasoning step, is called the reward model. Fig. 4: Overview of general RLM design and core concepts. We provide high-level overview (the top-left part), more detailed medium-level overview (the top-right part), and very detailed diagram showing the inference and training pipelines (the bottom part). detailed specification of the inference pipeline can be found in Appendix C.1 and in Algorithm 1. Pipelines for different training phases and paradigms can be found in Appendix C.2Appendix C.4, and in Algorithms 27. The data generation pipeline is detailed in Appendix D. 7 3.2 Encompassing Diverse RLM Architectures The above-described design is applicable to many RLM designs. However, there are numerous other variants of architectures, some of which do not fully conform to this framework. In this section, we discuss these variants, highlighting how our blueprint accommodates such variations. In some RLM designs [105], single node in the MCTS tree could represent an entire reasoning structure, such as complete chain of reasoning steps. In this case, the action space involves transitioning between different reasoning chains rather than individual steps. This approach changes the nature of the search, as the focus shifts from iteratively constructing single reasoning path to evaluating and refining entire structures within the search space. Our blueprint accommodates this with the concept of nesting, where node in the reasoning structure can contain another reasoning structure. this structure evolves in order to solve given input task. Second, there is set of operators (e.g., Refine) that can be applied to the reasoning structure (as specified by the reasoning strategy) in order to evolve it and make progress towards solving the input task. Operators are specified based on what they do (i.e., what effect they have on the reasoning structure). How this effect is achieved, depends on how given operator is implemented. Here, many operators rely on neural models (e.g., Policy Model), which together with their training paradigms form the third class of the blueprint components. Finally, we also distinguish set of pipelines, i.e., detailed specifications of operations that orchestrate the interaction between the reasoning scheme and the operators in order to achieve specific objective, such as training, inference, or data generation. Hence, an RLM can be defined as composition of reasoning scheme, set of operators and associated models, and set of pipelines. Other instance, introduce architectures even more novel Journey Learning [69] adds paradigms. For an additional layer of complexity by incorporating transformation step that rewires the search or reasoning structure. This transformation consolidates multiple paths in the tree, synthesizing them into new form that is used as input for subsequent reasoning iterations. Despite these variations, our blueprint is sufficiently general to encompass all these cases and beyond, as we illustrate more formally in the following. This generality ensures that the blueprint is not only applicable to existing designs but also provides foundation for future innovations in RLM development. 3.3 Integration with Broader LLM Agent Ecosystems The integration of RLMs into broader LLM agent ecosystems would enable these models to interact dynamically with external tools, databases, and resources during execution. This interaction can occur within the inference or data generation pipeline, leveraging value or policy models to extend the reasoning process through access to retrievalaugmented generation (RAG), web queries, and specialized tools. For example, during reasoning task, the value or the reward model could query database to verify intermediate steps, ensuring factual correctness or retrieving additional context to refine its reasoning. Similarly, these models could utilize computational tools for mathematical or symbolic computations, thereby expanding the scope and accuracy of their reasoning."
        },
        {
            "title": "4 BLUEPRINT FOR REASONING LMS\nWe now introduce our RLM blueprint that can be used to\ndevelop novel reasoning models and to provide ground for\nanalysis, evaluation, and comparison of such designs. We\noverview the blueprint in Figure 5.",
            "content": "4.2 Reasoning Scheme reasoning scheme is the part of the blueprint that specifies the details of the reasoning steps progressing toward the solution, how they are interconnected to form coherent chains, trees, or more complex reasoning structures, and how these structures evolve in the course of solving the input task. 4.2.1 Reasoning Step reasoning step is fundamental unit of the reasoning structure sequence of tokens that advances the RLM towards the solution. Reasoning steps can vary in length, ranging from single token to entire segments of text. The variability in their granularity depends on the user design choice. In existing schemes, reasoning step is typically conceptualized as coherent and self-contained unit of thought. For instance, in mathematical proofs, this may correspond to an individual logical argument or deduction. The flexibility in defining reasoning steps allows models to adapt to different problem domains, balancing finegrained and coarse-grained reasoning. Coarse steps, such as logical arguments (or even complete reasoning pathways [105]), simplify preparation and adoption of training data, enhance interpretability, and as we discuss in Section 8 reduce computational overhead. On the other hand, single-token steps enable the utilization of concepts like token entropy [60] to incorporate the models uncertainty, as well as the integration of advanced decoding schemes (e.g., speculative decoding [50] or contrastive decoding [53]) explicitly into the RLM design. Yet, while making the reasoning steps more fine-grained allows for more detailed exploration of solution paths, this increased flexibility results in greater computational demands, particularly when combined with search algorithms such as MCTS. 4.2.2 Reasoning Structure 4.1 Overview & Main Components The blueprint specifies toolbox of components that can be used to build an arbitrary RLM. We identify several classes of such components. First, an RLM includes reasoning scheme, which specifies reasoning structure (e.g., tree) together with reasoning strategy (e.g., MCTS) of how The reasoning structure specifies how individual reasoning steps are connected and organized. Common structures include chains (linear sequences), trees (hierarchical branching), and graphs (arbitrary connections). Chains are sequential reasoning flows, where each step builds directly on the preceding one. Chain structures are 8 Fig. 5: blueprint for reasoning LMs. It consists of four main toolboxes: the reasoning scheme (the top part), operators (the bottom-left part), and models (the bottom-right part); pipelines are mentioned in the center and detailed in Appendix C.1 and in Algorithm 1 (the inference pipeline), Appendix C.2Appendix C.4, and in Algorithms 27 (the training pipelines), and in Appendix (the data generation pipeline). prevalent in CoT-based models, where each reasoning step follows logically from the previous step in linear progression. In tree structures, each reasoning step can branch into multiple continuations, forming decision tree. This structure is commonly used in MCTS-based frameworks, where multiple potential paths are explored before selecting branch that will be further investigated. It enables more effective exploration of the space of reasoning steps, but simultaneously makes the RLM design more complex and costly. Finally, graph structures allow for arbitrary dependencies between reasoning steps, enabling graph-based reasoning, such as that found in the Graph of Thoughts (GoT) framework [6]. Further generalization involves nested structures, where reasoning nodes themselves may contain substructures. For example, node in tree structure may represent CoT chain, as proposed in LlaMa-Berry [105]. This hierarchical organization could be particularly useful for multi-step tasks where high-level decisions guide low-level computations, such as meta-reasoning frameworks [105]. One could harness any other higher-order structures, such as hypergraphs, motifs, and others [7], [8], [11], [14]. 4.2.3 Reasoning Strategy The reasoning strategy governs how the reasoning structure evolves, specifying the process by which new reasoning steps are added and integrated. Example strategies include: MCTS [49] popular approach that balances exploration and exploitation by simulating multiple reasoning paths and selecting the most promising one based on scoring function. Beam Search [81] breadth-limited search that keeps fixed number of top-ranked continuations at each step. While commonly used for decoding token sequences, beam search can also apply to reasoning steps. Ensemble Methods These methods involve aggregating multiple independent reasoning strategies, such as combining chains and trees to enhance robustness and accuracy. One example is Best-of-N [30], [97] strategy where multiple independent reasoning paths are generated, and the most effective solution is selected based on predefined criteria, e.g., accuracy or completeness. Another example is tree ensemble (Forest) [15] where, instead of single reasoning tree, reasoning forest consists of multiple disconnected trees, which may eventually converge at shared solution node. This approach supports diverse reasoning pathways that parallelize exploration. Reasoning Strategy vs. Decoding Strategy It is crucial to distinguish reasoning strategies from token-level decoding strategies. While decoding strategies, such as greedy search and nucleus sampling [42], generate the internal token sequences within reasoning step, reasoning strategies focus on the higher-level process of integrating and expanding reasoning steps within the reasoning structure. 4.3 Operators Operators specify operations that can be applied to various parts of the reasoning structure to progress the reasoning process. We now provide an extensive toolbox of operators. 9 Many of them have been widely used in RLM-related designs, but some to our best knowledge are still unexplored, we include them to foster innovation and propel the design of more effective and more efficient RLMs. 4.3.1 Structure Operators Structure operators transform the reasoning structure by taking it as input and producing modified version, typically through addition or refinement of reasoning steps. For instance, they may add new children to specific node, facilitating the exploration of alternative reasoning paths. Generate The Generate operator adds one or more new reasoning steps to reasoning structure. Within the MCTS reasoning strategy, this operator is typically implemented as policy model to generate new steps. In other strategies, the generation operator may involve sequentially appending steps (CoT) or exploring multiple candidate steps in parallel (Beam Search). Refine The Refine operator enhances given individual reasoning step. For example, it could address ambiguities, correct errors, and optimize inefficiencies, resulting in more robust version of the step [59]. It could also integrate suggestions from self-critique [75] (evaluates steps to identify weaknesses and suggest targeted improvements), summarization [110] (condenses key elements into concise representations to streamline the reasoning structure), or rephrasing [28] (reformulates steps to improve clarity and coherence while preserving their logical integrity). Aggregate This operator combines multiple reasoning steps, paths, or structures into the next individual step. This enables consolidating information or improving coherence. It is used in Ensemble Methods [15] or in Graph of Thoughts [6]. Prune This operator removes nodes or reasoning steps from the structure that are deemed suboptimal or irrelevant based on evaluation metrics. It enables optimizing the reasoning structure in order to, e.g., reduce token costs. Restructure The Restructure operator applies arbitrary transformations to the reasoning structure, enabling flexible reorganization of its components. notable example is the conversion of reasoning tree into linear chain by rearranging its branches into sequential series of steps, as done in Journey Learning [69]. This restructuring facilitates the integration of insights from diverse branches into cohesive flow, flattening it and making it easier for the model to process and utilize information within single, unified context. Discussion on Diversity In structure operators, there is notion of how diverse the outcomes of the operator are. For example, when generating new reasoning steps, one may want to make the contents of these steps as different to one another as possible. While different mechanisms to steer diversity exist, typical approach is the use of the policy model temperature. We additionally propose to consider the Diverse Beam Search [92] which promotes diversity by maintaining multiple diverse candidate sequences during decoding. In MCTS, there is also distinction between exploitation (expanding the structure by applying generation operators within an already established tree branch) and exploration (generating new branches). Here, one impacts diversity by manipulating the exploitation-exploration tradeoff, as determined by the Upper Confidence Bound for Trees (UCT) formula [49] or its variants. 4.3.2 Traversal Operators Traversal operators define how the reasoning process navigates through the existing reasoning structure. These operators play crucial role in shaping the flow of reasoning by determining which paths to pursue. Select The Select operator determines which reasoning steps to pick for further exploration, evaluation, or refinement within the reasoning process. It evaluates existing elements based on predefined criteria, such as heuristic scores, likelihood estimates, performance metrics or search strategies like PUCT [73] or UCT [49], selecting the most promising candidates to guide the next stages of reasoning. By balancing exploration (considering diverse alternatives) and exploitation (focusing on high-potential paths), the selection operator optimizes resource allocation and ensures efficient reasoning progression. Backtrack The Backtrack operator enables the model to explicitly return to previous reasoning step and continue along different reasoning path. This operator supports error correction, divergence handling, and hypothesis revision by abandoning unproductive directions in favor of alternative trajectories. The QwQ model output indicates that the reasoning structures used as training data in this model harnessed Backtrack. 4.3.3 Update Operators The Update operator enhances specific parts of the reasoning structure without altering the structure itself. common example is the backpropagation phase in MCTS, where evaluation scores are propagated and updated along existing reasoning steps to inform future decisions. Another form of update involves refining the content of individual nodes or subsets of nodes, replacing their original versions with improved iterations, such as the enhance thought transformation in Graph of Thoughts [6]. 4.3.4 Evaluate Operators Evaluate operators take as input segment of the reasoning structure and output value without any modifications to the structure. They are widely used with reasoning strategies, such as MCTS. One important type of evaluation occurs when the reasoning structure reaches terminal state, allowing the full reasoning sequence to be assessed against known solutionapplicable to tasks with definitive answers, such as mathematical problems. This terminality evaluation verifies whether the final step provides correct and complete solution. One can also evaluate intermediate steps (i.e., nonterminal ones). This can involve estimating the reward associated with specific reasoning steps, using heuristics, aggregated simulation outcomes, or trained reward model for more efficient assessments. Other methods such as embedding-based verification could also potentially be harnessed [12]. contribution to correct final outcome. This method evaluates both the correctness of the step and its alignment with the overall solution goal. Such evaluations can be performed through simulations, as in the original MCTS algorithm, or more efficiently using learned value model [80]. critical aspect of evaluation is the selection of appropriate metrics. For instance, in value estimation, an ideal metric considers both the correctness of reasoning step and the extent of progress it represents toward the final solution, ensuring balanced assessment of its contribution. 4.4 Models Models are used to implement various types of operators. Most common are the value model (implementing the value evaluation operator) and the policy model (implementing the generate operator). 4.4.1 Training Paradigm Each model must be trained according to specified paradigm, which outlines the methodology for optimizing its performance. This paradigm defines key training components such as the loss function, data generation and labeling procedures, and other critical training details. wide range of training schemes has been developed for models used in RLMs, with early foundational work stemming from advancements related to AlphaZero. These schemes have since evolved to support the complex requirements of reasoning tasks within LLMs. Common training paradigms include supervised fine-tuning (SFT), where models are trained on reasoning sequences labeled with Q-values; rejection sampling [17], [84], which involves filtering generated outputs based on quality criteria; and RL-based methods such as Proximal Policy Optimization (PPO) [77], Direct Preference Optimization (DPO) [71], and reasoning-specific variants like Reasoning Policy Optimization (RPO) [67]. Several training paradigms also incorporate self-learning, where the model iteratively improves by generating and evaluating its own reasoning sequences, thereby simulating competitive or cooperative reasoning scenarios. 4.4.2 Training Data Scope The training data for RLMs can vary significantly in terms of how much of the reasoning structure it captures. We now outline two established approaches, output-based supervision (OBS) and process-based supervision (PBS). More details regarding both OBS and PBS can be found in Appendix B.1. In output-based supervision (also known as sparse training signal) [25], [91] each training sample consists solely of the input and the corresponding output. For example, in mathematical problem-solving, sample may include the task statement and the final solution, labeled as correct or incorrect. This approach is straightforward to implement, and the required data is relatively easy to collect. However, it can limit the models reasoning accuracy, as it provides minimal insight into the intermediate steps that led to the solution [55]. Another form of evaluation employs value estimator, which judges given reasoning step based on its expected An alternative approach is process-based supervision (also known as dense training signal) [55], [95], where training sample reflects the entire reasoning structure. In this case, the sample contains not only the input and final output but also all intermediate reasoning steps, annotated with labels indicating the quality of each step. This richer training data allows the model to learn more granular reasoning patterns, improving its ability to generate accurate and interpretable solutions by understanding the reasoning process in detail. However, such data is much more challenging to generate or gather [55]. OBS vs. PBS By varying the training data scope, developers can strike balance between ease of data collection and the depth of reasoning insights provided to the model, with dense supervision generally offering improved performance at the cost of increased data complexity. Trace-based supervision (TBS) is potential way to extend PBS by incorporating detailed information about the sequence of applied operators, including traversal operators, within the reasoning structure. By capturing the full trace of how reasoning steps are generated, refined, or revisited, TBS would provide richer supervision that teaches the model to internalize not just the reasoning steps but also the process of navigating and manipulating the reasoning structure itself. This approach could enable the training of more powerful Implicit RLMs by guiding them to replicate the reasoning dynamics of explicit structures, improving their ability to reason flexibly and efficiently. 4.5 Pipelines pipeline is detailed specification of operations that orchestrates the details of the interaction between the reasoning scheme and the operators and models to achieve specific objective. Typically, an RLM would incorporate single pipeline for inference and separate pipeline for training each model used in an RLM. Moreover, there could also be pipelines for synthetic data generation used for training models. One can also distinguish pipeline that trains an Implicit RLM using the provided reasoning traces from the Explicit RLM. The details of pipelines depend on arbitrary design choices. In Section 3, we provided general description of how these pipelines work. In Appendix C, we present detailed algorithmic specifications of our pipelines, along with insights into the reasoning behind these design choices. Specifically, the inference pipeline can be found in Appendix C.1 and in Algorithm 1. Pipelines for different training phases and paradigms can be found in Appendix C.3, Appendix C.4, and in Algorithms 27. The data generation pipeline is detailed in Appendix D."
        },
        {
            "title": "5 EXPRESSING EXISTING SCHEMES",
            "content": "We now showcase the expressivity of our blueprint, by illustrating how it can be used to model broad scope of existing RLMs and other related works. We summarize the outcomes of the analysis in Table 1. We start with typical and most prevalent Explicit RLM architectures based on MCTS and policy and/or value models, where single reasoning step is an individual logical argument (Section 5.1). We also discuss there schemes that generalize this typical design, by harnessing nesting or Linearization Structure operators. Finally, we study Implicit RLMs (Section 5.2) and various structured prompting schemes such as Cot or ToT (Section 5.3), showing that they also fit our blueprint. 11 5.1 Explicit RLMs We start with the most widespread variant of RLMs that follows the architecture outlined in Section 3.1. These reasoning models such as TS-LLM [30], AlphaLLM [89], MCTSDPO [99], and others [18], [36], [93], [105], [106], [108] generally employ an explicit tree structure in which node represents distinct reasoning step. The reasoning strategy is based on the MCTS and focuses on iterative exploration, expansion and evaluation of nodes within the tree. By incorporating value mechanismssuch as prompt-based evaluation or dedicated value models, the system identifies and prioritizes promising branches, facilitating more informed decision-making and refinement of the reasoning process. All MCTS based reasoning models implement at least next-step generation operator, an evaluation operator, and the update operator for back-propagating the values. In addition, ReST-MCTS*, LLaMA-Berry, and Marco-o1 support refinement operator to further improve produced reasoning steps. Journey Learning [69] exhibits two main differences to typical MCTS-based RLMs. First, it harnesses the Linearization Structure operator, in which the tree reasoning structure is transformed into chain, by extracting several selected reasoning chains from it and combining them together into an individual long chain. This way, the scheme attempts to harness insights from different tree branches. By maintaining chain-based structure, Journey Learning preserves the simplicity of linear reasoning while embedding the capacity for self-correction and exploration of multiple hypotheses. Additionally, Journey Learning introduces pipeline for the internalization of such long reasoning chains into its weights. This enables the final model to generate such long reasoning chains, possibly containing different reasoning branches, directly from its weights, making it an implicit RLM. 5.2 Implicit RLMs Qwenss QwQ [88] embodies fully implicit reasoning model, characterized by an implicit reasoning structure that is generated autoregressively directly by the model weights. The reasoning strategy in QwQ as indicated by the model output harnesses next-step generation, backtracking, summarization, and critique generation to derive the final solution. At each step, the model implicitly generates new node within the chain by employing one of these four implicit generate operators, presumably implemented using special tokens. 5.3 Structured Prompting Schemes Finally, we also illustrate that advanced structured prompting schemes, such as CoT, ToT, and GoT, constitute fully explicit RLM structure without any implicit reasoning than what is originally presented in the used LLM, i.e., no models nor training or data generation pipelines. Reasoning Reasoning Operator Models Pipeline Structure Traversal Update Evaluation Remarks Structure Step Strategy Gen. Ref. Agg. Pr. Res. Sel. BT Bp. Inter. Final. PM VM Inf. Tr. DG Tree Multiple Chains Token Thought + Code Block MCTS Best-of-N MCTS Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Tree Search Ø Ø Ø Ø Ø Best-of-N Beam Search MCTS MCTS MCTS MCTS MCTS MCTS MCTS Tree Search Ø Ø Ø Ø Ø Ø Ø * Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø * *Separate Entry Ø *Advice by critic * * *Single model * * *Single model Scheme Explicit RLMs (Section 5.1) rStar-Math [36] PRIME [26], [102] Marco-o1 [108] Tree Journey Learning (Tr.) [69] OpenR [93] Tree Tree Thought Token Sequence Thought Thought Thought Tree of Chains Solution LLaMA-Berry [105] Thought ReST-MCTS* [106] Tree Thought AlphaMath Almost Zero [18] Tree Token Sequence Tree MCTS-DPO [99] Option Tree AlphaLLM [89] Token Tree TS-LLM [30] Sentence Implicit RLMs (Section 5.2) QwQ [88] Journey Learning (Inf.) [69] Chain* Chain* Token Thought Ø DFS Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø *Linearized Tree Ø Ø Ø Ø *Linearized Tree Structured Prompting Schemes (Section 5.3) Thought Graph of Thoughts (GoT) [6] Graph* Tree of Thoughts (ToT) [101] Tree Thought Self-Consistency (SC) [97] Multiple Chains Thought Thought Chain of Thought (CoT) [98] Chain Controller Tree Search Ø Majority Voting Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø *DAG Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø TABLE 1: Comparison of RLMs with respect to the provided taxonomy (Section 4 and Figure 5). Reasoning: Details of the reasoning approach, specifically what is its Structure and its Strategy? Reasoning Operator: Does given scheme support operators on the reasoning structure? If yes, which classes (and specific functionalities) are supported Structure (Gen.: generate, Ref.: refine, Agg.: aggregate, Pr.: prune, Res.: restructure), Traversal (Sel: select, BT: backtrack), Update (Bp.: backpropagate), and Evaluation of Inter.: intermediate steps and Final.: final steps? Model: Does given scheme use models to implement its operators and if so, which ones (PM: policy model, VM: value model)? Pipeline: Which pipelines are harnessed by given scheme (Inf.: inference, Tr.: training, DG: data generation)? When describing representations, we use the following abbreviations: E: explicit, I: implicit. F: fine-grained. C: coarsegrained. : full support (i.e., YES), : partially [supported], Ø: no support (i.e., NO). CoT [98] utilizes an implicit reasoning structure consisting of chain of reasoning steps. The reasoning strategy employed in CoT is oriented towards constructing single coherent chain of reasoning, culminating in solitary solution, thus only needing the generation operator. CoT serves as the foundational framework for range of advanced reasoning strategies, including prompting methodologies such as Self-Consistency and Self-Refinement, among others. Self-Consistency (SC) [97] extends the CoT framework by introducing redundancy into the reasoning process. It generates multiple reasoning chains and employs majority-voting mechanism to determine the most consistent solution, which implements Select operator from our blueprint. ToT [101] adopts an explicit reasoning structure organized in hierarchical, tree-based format. Within this framework, each node corresponds to distinct reasoning step, and branching facilitates exploration across multiple inferential pathways (the Generate operator). Additionally, an evaluation operator, implemented via specialized prompt and the LLM itself, assesses branches of the tree. GoT [6] introduces more intricate reasoning structure by employing an explicit graph-based representation. In this framework, nodes represent individual reasoning steps, and the graph architecture supports non-linear, interdependent relationships between these steps. The reasoning strategy in GoT is orchestrated by an external controller, realized as separate LLM, which guides the exploration, refinement and aggregation of the graphs nodes."
        },
        {
            "title": "6 HOW TO USE THE BLUEPRINT\nWe now outline how to use our blueprint for the user’s\napplication; we keep this section in a tutorial style.",
            "content": "6.1 Part 1: Define the Reasoning Scheme The first step in using the blueprint is to define the reasoning scheme, which specifies the foundational structure and strategy of your RLM. Start by selecting the reasoning structure. Chains are the most affordable in terms of token costs, at least when it comes to ICL [11]. Trees, while the most expensive, offer rich branching that enhances exploratory reasoning. Graphs, though slightly cheaper than trees, introduce additional challenges in implementation but can yield significant accuracy gains due to their flexibility. Next, decide on the granularity of reasoning steps. Coarse-grained steps, such as thoughts or sentences, are widely used due to their simplicity and ease of scaling. However, token-based granularity, which operates at the level of individual tokens, offers the potential for greater precision and unexplored accuracy improvements. This approach, while promising, demands significantly more computational resources and careful design. This decision defines your action space (possible operations) and state space (configuration of the reasoning structure). Another decision is choosing reasoning strategy to govern how the reasoning structure evolves. MCTS combined with some variants of policy and value models remains the most widely adopted approach due to its balance of exploration and exploitation. However, alternative strategies that have not been deeply studied, such as ensembles of reasoning structures, may offer untapped potential. Finally, determine the specific details of your chosen strategy, including parameters like exploration coefficients, decoding strategy, scoring functions, and step evaluation methods. These choices will significantly impact the models reasoning dynamics, scalability, and overall effectiveness. Each decision at this stage lays the foundation for tailoring the RLM to your specific application requirements. reasoning paths (terminal states) or strong signals (high absolute advantages) for further training. 13 6.2 Part 2: Define the Operators The next step is to specify the set of operators that will govern the reasoning process. For an MCTS-based design, the simplest approach is to implement the core operators: Generate (often called Expand for MCTS), Select, and Backpropagate. These fundamental operations suffice for many scenarios, providing straightforward framework for reasoning. Beyond the basics, consider whether you want to incorporate less mainstream operators, such as Backtrack. By explicitly including Backtrack, you enable clearer tracking of progress within the search tree, making it potentially easier to revisit and refine earlier reasoning steps. This approach also facilitates advanced training schemes, like Trace-based Supervision, by generating richer and more structured data. Consider using this and other operators within our toolbox. You will also need to determine the implementation details for each operator. Decide which operators will be implemented as neural modelssuch as using policy model to guide selection or value model for backpropagationand which will rely on non-neural methods. This choice affects both the computational complexity and the flexibility of the system, so its important to align these decisions with your reasoning scheme and performance goals. 6.3 Part 3: Determine the Training Details In this phase, you need to outline the specifics of training for the models that will implement operators. For an MCTSbased design, consider the typical approach of using the policy model to implement Generate (Expand) and the value model for Simulate. If necessary, you might also train separate model to calculate the reward at individual nodes, enhancing the precision of the reward signals. Identify the application or training domain in order to address generalization requirements. This step ensures that your models are trained on data representative of the tasks you want them to handle. Define the models, including their architectures and the selection of suitable base models. Consider how the design of these modelssuch as transformer-based architectures or more specialized designsaligns with your reasoning structure and overall objectives. Collect training data for both the policy and value models. For the policy model, consider generating data automatically with our pipeline or using scheme such as CoT prompting, and include special end-of-step token to ensure clean segmentation. For the value model, generate data through MCTS full simulations, which provide rich, structured information about reasoning paths and outcomes. Fine-tune the models as needed. If using coarse reasoning steps, perform supervised fine-tuning (SFT) on the policy model to teach it how to reason step-by-step. Similarly, apply SFT to the value model to initialize it as reliable evaluator. Run MCTS with initialized models to collect additional data. You might filter this data to keep only high-quality Finally, train both models either by additional SFT rounds or with reinforcement learning methods such as Proximal Policy Optimization (PPO). This ensures that the models are optimized not only for accuracy but also for the efficiency and robustness needed in complex reasoning tasks."
        },
        {
            "title": "7 FRAMEWORK X1: DESIGN & IMPLEMENTATION\nWe now introduce x14, an extensible and minimalist frame-\nwork that can serve as ground to design and experiment\nwith RLMs, and currently provides one example of the\nblueprint.5 An overview of the framework is in Figure 6.",
            "content": "7.1 Reasoning Scheme The x1 framework employs tree reasoning structure in conjunction with MCTS as the reasoning strategy. This combination allows for systematic exploration of reasoning paths while balancing exploration of new possibilities and exploitation of promising solutions judged by value model. The framework achieves this alignment through the implementation of series of operators that guide the construction, traversal, evaluation, and updating of the reasoning tree. 7.2 Operators The Generate operator plays crucial role in expanding the tree by adding new children to selected node. To improve the diversity of these newly generated nodes, we employ diverse beam search [92], which ensures variability among the children. Alternatively, high-temperature sampling can be used to introduce stochasticity into the generation process, fostering the exploration of different reasoning paths. Traversal of the reasoning tree is managed by the Select operator, which uses the PUCT function to identify the next node to expand. This operator balances trade-off between exploration, favoring less-visited nodes, and exploitation, reinforcing nodes with higher potential based on previous evaluations. Always starting from the root node, the traversal mechanism ensures that the system can dynamically explore alternative paths and recover from suboptimal decisions by backtracking and selecting new branches. The Backpropagation Update operator refines the Qvalues which can be used as guidance for the select operator along the path from an expanded node back to the root. This process incorporates new information from downstream nodes, leading to progressively more accurate Q-values for the intermediate nodes. These refined Q-values subsequently inform future decisions, making the reasoning process increasingly robust over time. The framework implements two different Evaluate Operators. First, the Reasoning Path Evaluation operator predicts the discounted expected future reward for chain extending from the root to specific node. This prediction 4https://github.com/spcl/x1 5We are working continuously on expanding the framework as well as adding more RLMs. is derived from the Q-value model, offering quantitative measure of the paths quality. Second, when the ground truth is available, the Ground Truth-Based Reward operator directly evaluates leaf nodes for correctness, assigning fixed rewards to verified solutions. These rewards are incorporated into the Q-values of upstream nodes, ensuring that the reasoning process is informed by both model predictions and objective validation. 7.3 Models & Training Paradigms Both the value and the policy model in x1 are fine-tuned versions of an LLM6, without reliance on prompting, which is used in several other RLM architectures [36], [106]. This design decision aims to maximize the quality of results. We now outline briefly selected key aspects of how we train these models, full details can be found in Appendix and C.3. 7.3.1 Training the Policy Model The policy model also leverages an LLM to generate new nodes during the MCTS. It is fine-tuned to output an individual next reasoning step instead of whole chain of thoughts towards completion (which LLMs commonly do). We achieve this by introducing novel token, the end of intermediate step (eois) token, which denotes the completion of each reasoning step. The eois token complements the standard end of sequence (eos) token, which indicates the conclusion of an entire reasoning chain. By incorporating the eois token, the framework enables the explicit identification of intermediate reasoning steps, allowing for greater interpretability and precise determination of whether the reasoning process is complete or ongoing. This dual-token strategy enhances the LLMs capability to decompose complex problems into manageable substeps while ensuring the model recognizes when solution has been reached. 7.3.2 Training the Value Model The value model is designed to estimate the sum of the expected discounted future rewards for sequence of reasoning steps and newly proposed reasoning step, quantifying the value of the node modeling this step. For given node in the MCTS tree, its value (referred to in the MCTS literature as state action value or Q-value) is defined as the expected cumulative reward discounted by the number of steps required to achieve it. Formally, the Q-value Qπ(st, at) for traversing the edge to node st+1 when taking action at from st at depth in the MCTS tree is expressed as Qπ(st, at) = (cid:104)"
        },
        {
            "title": "1\nN",
            "content": "γT tr(sT , aT ) st, at (cid:88) γT tr(s(i) , a(i) ) (cid:105) (1) (2) i=1 where γ is the discount factor, marks the last reasoning step aT that is added resulting the terminal state sT +1 containing the complete reasoning structure and rewards are modeled sparse. The terminal state sT +1 is defined as the state in which no additional reasoning steps can be added. It 6We currently use Llama-3.1-8B-Instruct as base model. 14 typically represents the state containing the final solution to the problem at hand. Accordingly, r(sT , aT ) is the terminal reward. We chose to model rewards sparse, where only the final reasoning step receives non-zero reward, since for most reasoning tasks, only the final answer can be evaluated against the true solution. As result, one can only obtain reward signal when the last step is reached. We can approximate the q-value by sampling reasoning chains until the terminal state, as in 2, and averaging the terminal rewards discounted by the depth required. The Q-value model is trained using data from completed MCTS searches. Initially, when the Q-value model is unavailable, simulations (complete rollouts) are performed, and the average discounted reward is used to initialize the Q-values for each node. More information can be found in the Appendix D.2. 7.4 Enabling Scalability and Efficiency The current implementation is built to scale to multiple GPUs on multiple nodes. To further enhance the scalability and computational efficiency, several architectural and operational improvements have been implemented. One design decision involves the decoupling of the value and policy models. The deployment of dedicated Value and Policy servers confers several advantages: Scalability The decoupling of Value and Policy servers from the MCTS instance facilitates scalability and the execution of multiple parallel MCTS instances. Batch Processing The policy server incorporates batching capabilities, allowing the concurrent processing of multiple queries, thereby enhancing throughput. Resource Optimization The independent allocation of computational resources to the value and policy models is inherently supported by the frameworks architecture, enhancing efficient resource utilization. Replication and Distribution The separation of value and policy models facilitates the application of distinct replication and distribution strategies. Figure 6 illustrates the implementation of the framework as server architecture, demonstrating how these structural enhancements contribute to improved scalability and efficiency. Building on these architectural enhancements, we employ the following strategies to further optimize the frameworks efficiency and scalability, focusing on inference and parallelization. In the framework, we incorporate the standard optimizations of batching, quantization, and KV caching. Inference calls are batched in the policy model, enabling simultaneous processing of multiple queries. To expedite the reasoning process, the framework creates multiple child nodes in parallel during the node expansion phase. Specifically, new nodes are generated concurrently in each expansion step, reducing computational overhead and enhancing overall system performance. Further optimization of inference speed is achieved through KV caching and quantization. KV caching mechanisms mitigate redundant computations, while quantization techniques reduce the memory consumption of both policy and value models. 15 Fig. 6: An overview of the x1 framework is presented, highlighting its two-phase training process. In phase 1, the models are initialized, while in phase 2, the models are iteratively refined by alternating between constructing sufficient number of MCTS trees and training the models on data derived from these trees. 7.5 Token Analysis 7.5.1 Example Results Our framework enables detailed analyses of strategies for generating and selecting fine-grained token-based reasoning steps. We consider variance, entropy, VarEntropy and the Gini coefficient as metrics for the analysis of the token probability distribution. The variance in the probability distribution over the vocabulary could be measure of uncertainty. low variance would mean high amount of similar values, indicating flat distribution. However the variance does not capture well the form of the distribution. Traditionally, entropy measures the uncertainty in probability distribution. The information theoretic definition also describes the information content of given systems state. high entropy means lot of information is necessary to quantify system. This is the case if all positions have roughly the same probability, indicating flat distribution. distribution with high peaks would be characterized by low entropy. The entropy is defined as = (cid:88) pi log2(pi) where pi is the probability of the i-th token. VarEntropy is usually only applied in information theory and is defined as the variance of entropy, measuring its change. high VarEntropy and lower Entropy mean that system has few very probable outcomes, while low VarEntropy and high Entropy indicate rather flat distribution. The VarEntropy is defined as pi( log(pi) H)2 (cid:88) The Gini coefficient is traditional measure to gauge the inequality in system, which plots the area covered by the sorted percentages of group. perfectly equal distribution, i.e. flat distribution, would have coefficient of 0. High inequality is characterized in turn by value close to 1. We show visualization of the token probabilities in Figure 8 for the following model output shown in Figure 7. Fig. 7: Example model output with highlighted tokens. The output has been colored in the following way: Purple if the highest probability is below 0.8 (not so certain but no contention), blue if the second highest probability is above 0.1 (very certain, but maybe another one) and red if both are true (uncertain). By examining the highest probabilities, the secondhighest probabilities, and the sum of the remaining ones, one can get first impression of the underlying distribution, that can later be quantified through metrics. There are regions, where the top two probabilities are very close and all other values are significantly smaller. This scenario likely represents situation where forking in the sampling process could disproportionately benefit future outcomes. Other potential key regions include instances where the model is highly confident (with high first probability, low second, and minimal remaining probabilities) and cases where the remaining probabilities are nearly as high as the first and second, indicating flatter distribution and high uncertainty. In such cases, rather than forking, it may be more beneficial to correct potential errors in previous generations. We now further investigate these results in Figure 9 with the help of the uncertainity metrics introduced earlier. It is evident that the zero-shot prompt seems less uncertain and therefore probably provides better output. However the use of the value 472 as token could mean that the question has been incorporated at least into the tokenizer and may also have been leaked into the training in some form. Another immediate standout is the high uncertainty associated with all scores around the <thought> tokens, which seems 16 Fig. 8: Token probabilities of example model outputs We show the two highest probabilities as well as the sum of the other probabilities. Fig. 9: Uncertainty metrics (variance, entropy, VarEntropy, and the Gini coefficient) plotted against the output token sequence. challenging to predict. Additionally, the token granularity results in most words being represented by just single token, which implies that, statistically, the next token (given random one) is likely to be roughly evenly distributed across the English language. low uncertainty indicates the model has high confidence in predicting the next token based on the given context. Consequently, these uncertainty values reflect the models learned statistical patterns rather than the actual statistical structure of the language used. Finally we illustrate ways to harness these metrics, specifically combination of entropy and VarEntropy, in the design of reasoning strategy. Cases of high VarEntropy and lower Entropy could benefit from tree search, since some of the outcomes have equally highly probability, whereas the other outcomes have low probability. In contrast, low VarEntropy and high Entropy indicates situations that could benefit from clarification as all outcomes are equally improbable. We observe that these metrics highlight tokens lot of times which are not essential for the overall task but for design and formatting decisions. The Gini coefficient shows more promising patterns in vital areas."
        },
        {
            "title": "8 EXAMPLE INSIGHTS FOR EFFECTIVE RLMS\nWe provide example insights gathered from the literature\nand from our analyses of design decisions using x1.",
            "content": "Use Process-Based Evaluation Process-based evaluation, in which the reasoning structure as whole is assessed, has been shown to be more reliable than alternative methods such as Outcome-Based Reward Models (ORMs). By examining the reasoning steps and their relationships within the structure, process-based evaluation provides richer signal that helps models refine their reasoning paths and improve overall accuracy. This approach ensures that each intermediate step contributes positively to the final outcome, resulting in more robust reasoning and better generalization across tasks. Use Two Phases for Training Adopting two-phase training strategysplitting SFT and RLhas proven effective in several contexts. This phased approach allows the model to first learn solid foundation of reasoning patterns in phase one, followed by fine-tuning under more complex, adaptive conditions in phase two. For instance, research on Process Reinforcement through Implicit Rewards demonstrates that models trained with dedicated SFT phase can maintain performance on standard benchmarks while achieving improved reasoning capabilities during RL. This separation also helps mitigate instability and ensures that each phase targets specific learning objectives, ultimately leading to more robust RLMs. Train on Familiar Distributions Training on familiar data distributions can significantly influence models initial performance and subsequent improvements. For example, PRIME [26], [102] shows that training on carefully curated token sequence (such as the eois token approach) avoids performance degradation. Similarly, in tasks like rStar-Math [36], models trained on well-defined, familiar distributions tend to stabilize more quickly and produce higher-quality reasoning outputs. By focusing on familiar distributions, researchers can ensure that the models effectively internalize the fundamental reasoning patterns before moving on to more diverse or challenging tasks. Be Careful with Prompting LLMs to Critique and Evaluate Relying on prompting alone to encourage large language models to critique and evaluate their own outputs often leads to instability. Research indicates that models struggle to self-correct reliably when prompted to refine their reasoning without external guidance. For example, recent study [44] illustrates that such prompting typically fails to produce consistently improved results. Another work [70] demonstrates that explicitly training the model to output better responses through iterative refinement outperforms simple prompting. These findings highlight the importance of structured training approaches and careful operator design when aiming for self-improvement capabilities in RLMs."
        },
        {
            "title": "9 BENCHMARKING",
            "content": "Sun et al. [85] provide clear distinction between various types of reasoning including mathematical, logical, casual, and commonsense. Below, we highlight selection of benchmarks for each category. 9.1 Mathematical Reasoning Mathematical reasoning benchmarks involve arithmetic, geometry, and other mathematical tasks that use logical constructs and symbolic computation. They can be further categorized into benchmarks with fixed datasets and templatebased benchmarks [62], [83]. GSM8K [25] consists of train set (7,473 samples) and test set (1,319 samples) of high-quality grade schoollevel mathematical word problems. Early breakthroughs in mathematical problem-solving by language models were achieved by training on the training subset of this benchmark. GSM Symbolic [62] introduces generator that can use 100 templated questions, which are derived from the questions of the GSM8K dataset. This approach emphasizes the limited generalization capabilities of current RLMs and highlights the importance of templated benchmarks in evaluating LLMs performance in mathematical reasoning. MATH [41] benchmark contains questions ranging in difficulty from high school to competition-level mathematics, containing 12,500 problems, split into 7,500 for training and 5,000 for testing. These problems are sourced from various mathematics competitions such as the AMC 10, AMC 12, and AIME (Level 5). Functional MATH [83] builds upon the MATH dataset by introducing templated problem formats designed to assess the functional understanding of mathematical concepts by LLMs. However, the code and templates remain inaccessible to the public, limiting its broader adoption. AIME [4], AMC [3], and GaoKao [54] feature mathematical tasks ranging from Olympiad level to college entrance level difficulty. The AMC is generally easier, the GaoKao offers broader range of difficulty levels, while the AIME is likely the most challenging. AIME consists of 30 problems, the AMC includes 40 problems and the GaoKao contains around 300 questions. OlympiadBench [39] is more advanced benchmark that spans Olympiad-level mathematics and physics problems, comprising 8,476 problems sourced from international and Chinese Olympiad competitions, as well as the Chinese College Entrance Exam (GaoKao). CollegeMATH [87] is designed for evaluating collegelevel mathematics, with dataset that contains 1,281 training problems and 2,818 test problems. These problems are sourced from textbooks, extracted with the help of LLMs. U-MATH [22] benchmark features 880 university-level test problems without images sourced from ongoing courses across various institutions, currently available through the Gradarius platform. This benchmark presents unpublished, open-ended problems balanced across six core subjects. FrontierMath [33] is an expert-level benchmark containing exceptionally challenging mathematics problems covering wide array of modern mathematical domains. The dataset size remains undisclosed, but the problems have 17 been carefully crafted and tested by expert mathematicians. Notably, current state-of-the-art models can solve less then 2% of the problems, revealing still significant gap between AI capabilities and human expertise in the field of mathematics. In general, it is recommended to utilize templated versions of these benchmarks where available, rather than relying solely on question-answer (QA) pairs. Templated benchmarks minimize the likelihood of contamination from prior exposure during model training, thus providing more accurate measure of performance [62], [83]. 9.2 Logical Reasoning reasoning emphasizes Logical from propositional and predicate logic to automated theorem proving. formal processes, PrOntoQA [74] generates ontology graphs, similar to causality graphs, which do not necessarily reflect natural patterns. From these graphs, it constructs statements and poses questions that necessitate logical reasoning for resolution. Due to the abstract and artificial nature of some ontology graphs, models must focus more on step-by-step logical reasoning rather than relying on commonsense inference to derive correct conclusions. BIG-Bench [82] is one of the most extensive benchmarks for reasoning tasks encompassing over 200 tasks, each potentially comprising numerous questions. It encompasses broad range of domains and employs templated question formats, enabling systematic evaluation of reasoning capabilities across diverse contexts. ARC Challenge [23] assesses the ability to understand formal patterns, rules, and transformations within structured, grid-based environments. Tasks focus on identifying logical structures such as conditional relationships and sequences. For instance, deducing transformations between grids based on abstract rules exemplifies the application of formal logical reasoning paradigms. 9.3 Causal Reasoning Causal reasoning involves understanding and analyzing cause-effect relationships, including counterfactual reasoning and causal inference. This domain challenges models to predict or reason about events based on causal dynamics. ubingen Cause-Effect Pairs Dataset [63] comprises 108 cause-effect pairs drawn from diverse domains such as meteorology, biology, medicine, engineering, and economics. It serves as comprehensive benchmark for assessing causal reasoning across various contexts. Neuropathic Pain Dataset [90] captures complex relationships between nerve function and symptoms in patients. It requires domain-specific knowledge and causal inference to accurately interpret the data. Arctic Sea Ice Dataset [46] consists of 12-variable graph that models the dynamics of Arctic sea ice based on satellite data generated since 1979. It provides structured environment to explore causal relationships within climatological systems. CRASS Benchmark [31] focuses on counterfactual reasoning tasks using 274 sample multiple choice questions. It evaluates models abilities to answer counterfactual questions, using top-k accuracy as the primary performance metric. benchmarks contain at least 200 samples each and that minimum of 500 samples be evaluated per category to ensure robust performance comparisons. 18 Many of these benchmarks have either been largely solved by current state-of-the-art models, or their applicability in real-world language model tasks remains limited, rendering them unsuitable for benchmarking current RLMs. 9.4 Commonsense Reasoning Commonsense reasoning encompasses tasks that require the application of everyday knowledge, including questions that rely on implicit cultural, social, or contextual understanding. This category also extends to specialized domain knowledge tasks. GPQA (Diamond) [72] is multiple-choice benchmark spanning disciplines such as chemistry, genetics, biology, and physics. The questions are designed to be solvable by experts (PhDs) within their respective fields but remain challenging for experts from unrelated domains. The diamond subset contains 198 samples. MMLU (STEM) [40] incorporates questions across spectrum of difficulty, ranging from general commonsense reasoning to highly specialized domain knowledge. 9.5 Benchmarking RLMs Including multiple models in reasoning scheme tends to increase its output variance. Therefore, we emphasize the importance of benchmarking RLMs on sufficiently large sample sizes to ensure fair comparisons across different approaches. Benchmarks such as AIME or AMC, which only provide limited number of samples in the two-digit range, pose risk of selective reporting, where researchers might highlight subsets of results where their models performed well. Experimental results (see Figure 10) testing the variability of LLM generation indicate that achieving low error variability in the single-digit percentage range requires evaluating on over 500 samples. As RLMs can have much larger variability due to the increased complexity and based on these findings, we recommend that individual Fig. 10: Estimated 95%-confidence interval length for different question set sizes using sampled generated answers from subset of 1000 questions with eight generated answers per question at temperature 1. The confidence interval is calculated over the eight different pass@1 subsets of each question with 32 sets randomly sampled with replacement for each set size."
        },
        {
            "title": "10 RELATED ANALYSES\nRLMs have been explored from several angles in prior\nworks, yet significant gaps remain in providing a systematic\nblueprint and open-sourced framework for their construc-\ntion. Below, we categorize prior efforts and describe how\nour work advances the field.",
            "content": "10.1 Reasoning with Standard LLMs Several works explore techniques for enhancing the reasoning capabilities of standard LLMs. These approaches use straightforward mechanisms applied during pre-training, fine-tuning or inference. Enhancing Reasoning with Training Huang and Chang [43] outline pre-training and fine-tuning on reasoning datasets, and advanced prompting strategies. Sun et al. [85] contribute additional insights, including techniques such as alignment training and the integration of Mixture of Experts architectures. Furthermore, Huang et al. [45] demonstrate the possibility of self-improvement on reasoning tasks with additional training on self-generated labels. Reasoning with Prompting & In-Context Learning Qiao et al. [68] provide an overview of prompting-only techniques, classifying prompting methods into two main categories: strategy-enhanced reasoning and knowledgeenhanced reasoning. Besta et al. [11] provide taxonomy of different advanced in-context reasoning topologies. These include the Chain-of-Thought (CoT) [98], Tree of Thoughts (ToT) [101], and Graph of Thoughts (GoT) [6]. Some of these works further provide overviews of different reasoning tasks, reasoning datasets, and reasoning benchmarks [43], [68], [85]. Others focus on enhancing domain-specific reasoning, such as mathematical [2], [56], [100] or logical reasoning [58]. These studies remain largely limited to reviewing existing literature. Therefore, they lack code implementation and rarely employ formal language. Most importantly, they rarely cover explicit reasoning models. Our blueprint integrates most of these techniques within broader, modular structure. 10.2 Explicit Reasoning Models The following works explore techniques that extend beyond basic mechanisms applied during pre-training or inference. These methods involve additional computation to iteratively refine reasoning paths, often increasing computational demands during training and/or inference. Dong et al. [29] provide taxonomy and survey of inference-time self-improvement methods, including independent, context-aware, and model-aided approaches. Guan et al. [35] propose verifier engineering, post-training paradigm for foundation models involving three stages: Search, Verify, and Feedback, to enhance model outputs with scalable supervision signals. Zeng et al. [103] provide comprehensive roadmap for reproducing OpenAIs o1 reasoning model from reinforcement learning perspective. 19 Although the work thoroughly examines all core components: policy initialization, reward design, search, and learning, no implementation is provided. Various specific implementations of RLMs exist, we provide summary in Table 1. There are also other works related to Explicit RLMs, considering both coarse reasoning steps [96], [99] and fine reasoning steps [27], [96], [99]. Our blueprint provides more foundational and universally applicable framework for RLMs. We further supplement the theoretical and algorithmic overview with modular and scalable implementation to enable practical development and experimentation."
        },
        {
            "title": "11 CONCLUSION\nThis work introduces a comprehensive blueprint for rea-\nsoning language models (RLMs), providing a flexible and\nmodular toolbox that clarifies the intricate design and oper-\nation of these advanced systems. By encompassing diverse\nreasoning structures, operations, and training schemes, the\nblueprint serves as a robust foundation for constructing,\nanalyzing, and extending RLMs tailored to various appli-\ncations. The x1 implementation further enhances this con-\ntribution, offering a modular, minimalist, and user-friendly\nplatform for experimentation and the rapid prototyping of\nnovel RLM architectures.",
            "content": "Our blueprint and x1 pave the way for future advancements in reasoning AI. The blueprint can inspire the development of new RLM architectures, such as nested designs where reasoning structures like trees and graphs are embedded hierarchically for solving multi-layered tasks. x1 serves as playground for exploring these novel models, enabling researchers to experiment with enhancements such as refined training paradigms, innovative reward structures, and adaptive search strategies. ACKNOWLEDGEMENTS We thank Hussein Harake, Colin McMurtrie, Mark Klein, Angelo Mangili, and the whole CSCS team granting access to the Ault, Piz Daint and Alps machines, and for their excellent technical support. We thank Timo Schneider for help with infrastructure at SPCL. This project received funding from the European Research Council (Project PSAP, No. 101002047), and the European HighPerformance Computing Joint Undertaking (JU) under grant agreement No. 955513 (MAELSTROM). This project received funding from the European Unions HE research and innovation programme under the grant agreement No. 101070141 (Project GLACIATION). We gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2024/017103. APPENDIX MATHEMATICAL FOUNDATION OF MARKOV DECISION PROCESSES FOR REASONING TASKS In this section, we provide rigorous mathematical framework for RLMs by integrating Markov Decision Processes (MDPs) with Monte Carlo Tree Search (MCTS) algorithm. The MDP serves as foundational formulation for modeling reasoning chains, which constitute the reasoning structure of the RLMs. Simultaneously, MCTS serves as an efficient search algorithm for exploring and navigating the expansive space of possible reasoning chains. The resulting state space is then used as basis for modeling the RLM. An overview of the notation used in this section is provided in Table 2. A.1 Markov Decision Process Markov Decision Process (MDP) is defined as 5-tuple = (S, A, p, r, γ), where is the state space, is the action space with As denoting the set of actions which can be taken in the state s, represents the dynamics of transitions between states, i.e., : [0, 1] where p(s, a, s) is the probability of transitioning to the state when action was selected in the state s, : is the reward function, i.e., r(s, a, s) represents the reward for arriving in the state after selecting the action in the state s, and γ [0, 1] is discount factor. Solving an MDP trajectory τπ = (s0, a0, . . . , sT , aT , sT +1) is sequence of interleaved states and actions, selected according to the policy π. Each trajectory starts at an initial state s0 and ends with sT +1 which represents the terminal state where no further actions can be taken. policy π(a s) is function assigning probability distribution over the action space to given state s; π : (A). The expression π(a s) denotes the probability of selecting the action in the state according to the policy π. State value function (st) represents the expected cumulative future reward for given state st under policy π: Vπ(st) = (cid:34) (cid:88) k=t γktr(sk, ak, sk+1) st (3) (cid:35) where is predefined time-horizon. The goal of solving an MDP is to find policy which maximizes the value function as defined above for all states S, π = arg max Vπ (s) π The State-Action value function Q(st, at) The stateaction value function Q(st, at) extends the state value function to evaluate specific action at: Qπ(st, at) = Eπ (cid:34) (cid:88) k=t γktr(sk, ak, sk+1) st, at = r(st, at) + γEst+1 [Vπ(st+1) st, at] , where Bellmans equation is used in the second equality. 20 , . . . , tMi A.1.1 MDPs in the RLM Setting In the context of RLMs state is typically defined as sequence of reasoning steps = (z0 . . . zn), where each reasoning step zi is sequence of Mi tokens zi = (t0 ). Each tj is token from the RLMs vocabulary, and the total number of tokens per reasoning step Mi can vary. One can use special token tMi = tend to indicate the end of the reasoning step. Typically, the initial query is used as the first reasoning step z0 = q. In the study of RLMs, an action As usually represents appending new reasoning step z(a) to the current state = (z0, ..., zn) resulting in new state = (z0, ..., zn, z(a)). Since every action is uniquely associated with exactly one reasoning step, the transitions between states are deterministic i.e for = (z0, ..., zn) and = (z0, ..., zn, zn+1). p(s, a, s) = (cid:40)1 0 (cid:41) if zn+1 = z(a) if zn+1 = z(a) The definition of the reward function depends on the specific task. reward commonly seen in reasoning tasks assigns non-zero reward only in the terminal states and hence only at the final reasoning step. This approach reflects the fact that for most tasks, the only final answer can be evaluated against the ground-truth solution to the original query. We call such reward functions sparse to cleary distinguish it from other setting in which an intermediate rewards can be observed by the algorithm in the nonterminal states. The discount factor γ determines how future rewards influence the current decision-making process. higher discount factor (γ 1) places greater emphasis on long-term reasoning success, allowing the model to generate long reasoning sequences, while lower discount factor prioritizes immediate rewards, incentivizing faster progress and shorter reasoning sequences. In the RLM setting, = trajectory (s0, a0, . . . , sT , aT , sT +1) the progression of states st and actions at ending with terminal state sT +1 in which no further reasoning steps can be added. The final reasoning step contains the RLMs answer to the original query. represents τπ The policy π(a s) in the context of RLMs defines the probability of selecting an action that corresponds to appending reasoning step z(a) to the current reasoning sequence represented by the state s. Since there exists bijective mapping : between the action space and the reasoning step space Z, the probability distributions can be equated using the change of variables. Formally: π(a s) = π(z s), where = (a). Based on the definition of the reasoning step and applying the chain rule we can then rewrite the policy as: (cid:35) π(zt+1 st) = Mt+1 (cid:89) j=0 π(tj t+1 st, z0 t+1, . . . , zj1 t+1 ), In the RLM setting, the state value function (st) assesses the expected cumulative reward of partial reasoning sequence st, estimating its overall potential to lead to successful solution. The state-action value function Q(st, at) extends this by quantifying the expected cumulative reward TABLE 2: Notation Overview 21 Symbol Description = (S, A, p, r, γ) Markov Decision Process (MDP) definition. As p(s s, a) r(s, a, s) γ [0, 1] πθ(a s) Vπ(s) Qπ(s, a) τπ state in the state space, representing sequence of reasoning steps. An action in the action space, corresponding to selecting the next reasoning step. set of actions available in state s. The probability of transition to state from state taking action in state s. Reward received after taking action in state resulting in state s. Discount factor, determining the present value of future rewards. Policy parameterized by θ, representing the probability of taking action in state s. Value function under policy π, representing the expected return starting from state s. State-action value function under policy πθ, representing the expected return of taking action in state s. trajectory consisting of states and actions, (s0, a0, s1, . . . , sT +1) following policy π. for taking specific action at (e.g., appending reasoning step zt+1) to the current state st and then following the policy π. It incorporates both the immediate reward for appending the reasoning step and the anticipated future rewards from completing the reasoning sequence. Together, these functions inform and guide the policy π to prioritize actions that maximize the expected cumulative reward. By leveraging (st) or Q(st, at), the policy can be trained to select reasoning steps that progress toward correct and complete solutions, transforming an LLM into RLM. A.2 Monte Carlo Tree Search (MCTS) Monte Carlo Tree Search (MCTS) is heuristic search algorithm used for solving MDP problems. MCTS iteratively builds search tree, representing the underlying MDP stateaction space, by aggregating the information obtained from executed MDP trajectories. Let = (N, E) denote the MCTS search tree where is the set of nodes and is the set of directed edges between the nodes. Every node in the MCTS search tree corresponds to single state in the MDP and every edge corresponds to single action. Every path from the root to the leaf of the search tree corresponds to single trajectory in the underlying MDP. Edge statistics The MCTS algorithms stores the following three values for every edge s, in the search tree: (s, a) - the visit count of the edge (s, a) by the algorithm, q(s, a) - the estimated state action value of (s, a), r(s, a) = r(s, a, s) - the reward received after taking the action in the state leading to state s. The Algorithm At the high level, the MCTS begins by initializing the tree with single starting state s0 as root node and performing the following three phases in loop: 1) Selection - leaf-node in the current tree is selected for expanding its child (children). 2) Expansion - if the selected node does not correspond to terminal state, it is expanded by taking an action (or multiple actions) in the underlying MDP and by adding the resulting state (states) to the tree as children of the current node. trajectory unroll is performed for every added node to obtain reward. Unroll refers to simulating sequence of steps from newly added node in the tree down to terminal state. This simulated trajectory represents hypothetical path the system might take if it continued from the current node. Once the simulation reaches terminal state, reward value is calculated based on the outcome of that path. 3) Backpropagation - update the value estimates and the visit counts for the selected node and all its ancestors based on the obtained reward. The MCTS algorithm finishes when the stop criterion such as the the number of iterations, the predefined computational budget, or the convergence criterion is met. APPENDIX VALUE AND REWARD MODELS We now proceed to discuss details of value and reward models. B.1 Outcome-Based Reward Models (ORM) vs. Process-Based Reward Models (PRM) In reinforcement learning environments, reward models estimate the reward for taking an action in state which leads to state s. For reasoning tasks and algorithms like MCTS, which rely on evaluating intermediate steps, it is essential to have models capable of estimating the quality of each step. Two primary families of reward models for such process-based tasks are Outcome-Based Reward Models (ORMs) and Process-Based Reward Models (PRMs). Figure 11 compares both classes of models. Outcome-Based Reward Models (ORMs), first introduced by Uesato et al. [91], evaluate the reasoning process solely based on the final outcome. These models estimate the reward of the final step in the chain, often modeled in the literature as the likelihood of correct final answer given the entire reasoning chain (correct(zT +1) z0, ..., zT +1) [55], [91] where sT +1 := z0, ..., zT +1 is the complete reasoning chain consisting of reasoning steps zi and + 1 marks the last reasoning step. ORMs are particularly ill-suited for evaluating intermediate steps for several reasons. First, the training data and objective are inherently misaligned with step-wise evaluation, as they focus exclusively on final outcomes. Second, ORM evaluations tend to be overly pessimistic for intermediate steps since subsequent erroneous step can obscure the correctness of earlier steps. This observation aligns with Havrilla et al. [38], who noted that ORMs often underestimate the solvability of problem from an intermediate state and are prone to high false-negative rate. Furthermore, ORMs lack robustness against false positives, potentially favoring erroneous reasoning steps and misleading the evaluation process. := z0, ..., zt z0, ..., zt) where st Process-Based Reward Models (PRMs), introduced by Lightman et al. [55] and Uesato et al. [91], evaluate reasoning on step-by-step basis. These models estimate the reward of step, which can be seen as the likelihood of correctness for the t-th step given its preceding context (correct(zt) is potentially incomplete reasoning chain and zi are reasoning steps and z0 is the query. PRMs provide more fine-grained feedback and can pinpoint errors in the chain. This stepwise evaluation provides dense rewards given partial responses and helps identify where reasoning deviates from correctness, offering improved interpretability and enabling more targeted improvements in reasoning processes. However, PRMs are computationally expensive to train and require extensive annotations of reasoning steps. These annotations, whether provided by humans or other LLMs, often suffer from limitations: human annotations are scarce, costly, and prone to bias, while prompted LLM-generated annotations [94] are typically of lower quality due to their limited self-evaluation capabilities [59]. Automated methods using for example MCTS such as [57], [95] introduce large computational costs and are prone to false negatives. B.2 Outcome-Driven Process-Based Reward Models Motivated by the need for process-based reward models but constrained by the lack of annotated step-wise labels, certain models that we will refer to as Outcome-Driven Process-Based Reward Models (O-PRMs) have been proposed; they combine outcome-based signals with process-based objectives. We show these models in Figure 11. These models rely on processbased data, often automatically generated using MCTS algorithms, where simulations starting from given step st are performed. The final correctness of these simulated paths is aggregated to create step-wise labels [57], [95] (for other, non-MCTS approaches see [38]). This automation enables scalable data generation for O-PRMs, eliminating the need for extensive human annotation. Although O-PRMs can be categorized as process-based models due to their approximation of step-wise rewards, they remain inherently tied to outcome signals. Some authors [91] suggest that, under certain conditions, outcome signals in mathematical domains can approximate intermediate labels. However, O-PRMs inherit many limitations of ORMs, including susceptibility to false negatives, false positives, and an over-reliance on terminal outcomes. While the aggregation of multiple simulations helps reduce variance, the backtracking process may still oversimplify complex dependencies within reasoning chains. B.3 Reward Models vs. Value Models While the distinction between reward models and value models is often blurred in the literatureand their terminology is sometimes used interchangeablywe types for explicitly differentiate between these model evaluating reasoning steps. Additionally, we distinguish two variants of value modes: V-Value and Q-Value Models. This differentiation arises from the distinct roles these models play in reinforcement learning environments. 22 Fig. 11: Comparison of Outcome vs. Process-Based label generation, and the introduction of Outcome-Driven Process Based Reward Models (O-PRMs). Gray nodes mark terminal nodes. B.3.1 Reward Model (RM) reward model predicts immediate rewards. In RL, this corresponds to the reward obtained for transition (s, a, s) from state when taking action which results in step s. For reasoning, this corresponds to adding new reasoning step to the structure. The new structure is then represented by s. Specifically, PRMs which are preferred over ORMs for MCTS due to the need for action-based evaluation learn these rewards and can be used to evaluate states (or the transition into state). This formulation provides localized, step-level evaluation independent of the overall outcome of the reasoning chain. The reward model is typically trained using labeled data where individual reasoning steps are associated with reward values. While this localized view is advantageous for step-by-step evaluation, it lacks the ability to consider how the current step contributes to the long-term success of the reasoning process. This limitation motivates the introduction of value models. B.3.2 Value Model (VM) Value models provide more abstract, global evaluation of states and actions by estimating their contribution to future rewards. Unlike reward models, which focus on immediate outcomes, value models consider both current and future rewards, enabling broader perspective on reasoning quality. For example in reinforcement learning and MCTS, value models play critical role in guiding the search process. By providing estimates of state or state-action values, they enable more informed decisions about which nodes to expand and explore. We now discuss variants of Value Models. V-Value Model (V-VM). One variant of value model is the V-Value Models which predicts the expected cumulative future reward of state, denoted as (s). This is equivalent to the state value function in reinforcement learning, which evaluates the long-term potential of the current state s. key advantage of V-VMs is their global perspective, as they aggregate future rewards across all possible trajectories originating from the current state. However, V-VMs do not explicitly evaluate individual actions, which may limit their utility in step-level decision-making. Additionally, V-values are often ill-defined at terminal states, where rewards may substitute for state values during training. Q-Value Model (Q-VM). Another variant of value model is the Q-Value Model. Q-VMs predicts the expected cumulative future reward of taking specific action in given state s, denoted as Q(s, a). Unlike V-VMs, Q-VMs explicitly associate values with state-action pairs, offering more granular evaluation. This granularity makes Q-VMs particularly useful for MCTS, where decisions about which edge (action) to expand at given node (state) are critical. By directly evaluating actions, Q-VMs align naturally with the selection mechanisms in MCTS, guiding the search toward promising paths. Similar to V-VMs, Q-VMs can also be categorized as PQVMs (Process-based Q-Value Models), OQVMs (Outcome-based Q-Value Models), and O-PQVMs (Outcome-driven Process-based Q-Value Models). The choice between V-VMs and Q-VMs depends on the reasoning task and the specific requirements of the evaluation framework. While V-VMs provide broader, state-centric evaluation, Q-VMs enable more precise, actionspecific guidance. In practice, MCTS often benefits from the use of Q-VMs due to their compatibility with edge-based selection. Fig. 12: Comparison of reward, v-value and q-value models in sparse reward setting (only terminal states receive non-zero rewards). Gray nodes mark terminal nodes. The reward model should predict the rewards for transitioning from one state to another which is 0 for non-terminal states and not providing information. V-VMs and Q-VMs however, predict global value and are therefore informative for non-terminal states. B.3.3 Example: Solving Mathematical Equation To illustrate the differences between reward models, value models, and Q-value models, consider the task of solving x2 + y2 = 1 step-by-step. Reward Model (RM): process-based reward model (PRM) might assign reward r(st, at, st+1) for the 1 x2. This reasoning step at = Substitute = reward quantifies the quality of the resulting state st+1, independent of whether it leads to correct solution. However, in sparse reward settings (only final steps receive reward), this reward would be 0. V-Value Model (V-VM): V-VM estimates (st), representing the expected cumulative reward for the entire expected solution process starting from st. For instance, if st = (Start with x2 + y2 = 1), (st) considers the long-term potential of all reasoning paths originating from this state. Q-Value Model (Q-VM): Q-VM evaluates Q(st, at), predicting the cumulative reward of taking specific 1 x2) in state st. action at (e.g., substituting = This value directly informs whether the action at is likely to lead to high-quality solution, providing more granular evaluation compared to the V-VM. 23 B.3.4 Summary By differentiating reward models and value models, and further categorizing value models into V-VMs and Q-VMs, we provide nuanced framework for evaluating reasoning steps. Reward models offer localized evaluations, while value models incorporate global, long-term perspectives. This global evaluation enables the model to better prioritize reasoning paths that are likely to lead to correct solutions while mitigating the challenges posed by sparse or delayed rewards. Therefore, we advocate for the use of processbased value model due to the sparsity of reward signals for reasoning tasks. Among value models, Q-VMs are particularly well-suited for MCTS due to their action-specific granularity, which aligns naturally with the trees edge-based exploration mechanism. We will demonstrate the practical implications of these distinctions in Appendix D.3. B.4 Evaluation Schemes We also provide additional categorizations and details regarding overall evaluation. B.4.1 Evaluation Types Evaluating reasoning steps in RLMs involves assessing their quality and contribution toward solving task. Numerical evaluations can be categorized as relative or absolute. Relative evaluations compare multiple steps, often using ranking mechanisms and can be created with, for example, the Bradley-Terry model [16], which is optimized based on pairwise preferences by maximizing the reward gap between chosen and rejected steps. Absolute evaluations assign scalar values to each step, ssessing aspects such as coherence, correctness, or helpfulness, using regression-based models. Moreover, evaluation dimensions can also be modeled as binary with classification models. While regression models provide more information, classification models capture correctness more naturally since statement is usually correct or incorrect. On the other hand, the former ones are more suitable for measuring quality, such as the degree of coherence. Depending on the specific quality being evaluated, the choice between regression and classification models should align with the evaluations goals. Additionally, absolute scores can be transformed into rankings if needed, providing flexibility across various applications. In addition to numerical evaluations, there are textbased evaluations, which are commonly used to provide detailed feedback and guidance for refining reasoning steps. Examples include LLM-as-a-Judge [109] (which uses larger LLM to provide pairwise comparison or single graded answer with an explanation) and self-critique approaches [75] that allow models to reflect on and evaluate their own reasoning. These textual evaluations, often including rationales, are particularly useful for structural transformations rather than numerical guidance, enhancing interpretability by offering context and detail. 24 B.4.2 Evaluation of Reasoning Steps"
        },
        {
            "title": "APPENDIX C\nALGORITHMIC DESCRIPTIONS",
            "content": "for integrating reasonStep-wise evaluations are vital ing into MCTS. Numerical evaluations-whether relative or absolute-provide straightforward metrics to compare nodes and steer exploitation and exploration. Text-based evaluations, in contrast, are better suited for guiding structural refinements rather than directly influencing search paths. Given that reasoning steps are typically textual sequences, language models are natural fit for such evaluation tasks. LLM-based approaches can involve external model approaches, where dedicated value model is trained to predict scores, or internal model approaches, which leverage existing policy models. External model approaches include value models that predict scalar reward signals (Reward models) [24], [55], [91], reinforcement learning values like state-values (Vvalue models) [79], state-action values (Q-value models), or pairwise models like the Bradley-Terry and PairRM frameworks. more detailed comparison of reward models, vvalue, and q-value models can be found in Appendix B.3.2. There exist large range of internal model approaches as substitutes for value models. They typically rely on methods like prompting the policy to output scores. Examples include MCT Self-Refine (MCTSr) [104], querying for binary feedback (e.g., Is the answer correct? answeryes or no) [107] and evaluating the probability of the output, leveraging uncertainty metrics such as token entropy or aggregated probabilities [108], and others [106]. C.1 Reasoning with Monte Carlo Tree Search C.1.1 Setup and Notation We now introduce the necessary notation to understand MCTS and the training pipeline algorithmically. Moreover, we will propose our design choices for x1. MDP Design x1 defines MDP = (S, A, p, r, γ) for RLMs as defined in Appendix A.1. We chose to adopt sparse reward structure, where only transitions into terminal states yield non-zero rewards. In these terminal states, the final reasoning step provides an answer that can be extracted and, during training, compared to given solution (commonly referred to as the golden answer). Therefore, the reward objective is tied to the correctness of the final answer. In many reasoning tasks like math, access is typically limited to the final answer, with no ground truth available for intermediate reasoning steps (which are often also non-unique). As result, the reward structure is inherently sparse. We assign reward of r(sT , aT , sT +1) = 1 for correct answers and r(sT , aT , sT +1) = 1 for incorrect ones where + 1 denotes the terminal state. All other MDP components are chosen as in Appendix A.1. γ can be chosen freely but we propose to select values between [0.95, 1] to not overpenalize long reasoning sequences. Since rewards are sparse and take values between -1 and 1, the state value function simplifies to Vπθ (st) = Eπ (cid:104) γT tr(sT , aT ) st (cid:105) [1, 1] (4) Heuristics may also serve as substitutes for evaluations and the state action function breaks down to: in resource-constrained scenarios. Simulating reasoning steps to terminal states for evaluation against golden answers is another option as done for example in MCTS, though often computationally prohibitive. External tools provide an alternative path for evaluation, especially in domain-specific tasks. For programming, compilers can supervise tasks, as seen in Codex [19], self-debugging [21], and similar methods. Program-ofThought [20] and Program-aided-Language (PAL) [32] use formal language and Python interpreters to evaluate solutions. In mathematical tasks, ensemble approaches like MathPrompter [47] generate multiple algebraic expressions or Python functions to validate steps. These tool-based approaches excel at detecting errors due to their reliance on precise domain-specific rules, such as compilers for programming or interpreters for mathematics. While their applicability is limited to well-defined domains, they provide objective and verifiable feedback that complements language models. By injecting precise knowledge into the evaluation process, external tools mitigate model-specific limitations like hallucinations and offer actionable feedback for iterative refinement. This hybrid approach enhances reliability and ensures that the evaluation benefits from both the flexibility of language models and the precision of formal systems. Qπ(s, a) = (cid:40) r(sT , aT ), γVπ(st+1), otherwise if = [1, 1] (5) Due to our definition of rewards, we obtain bounded state value and state action functions great property that we will use in the MCTS design and the design of the value model architecture. MCTS Design We define the MCTS tree as in Appendix A.2 as = (N, E), where is set of nodes, and is the set of edges. Furthermore, each state si defines reasoning sequence: si = (z0, . . . , zi) which starts from an initial question z0 followed by reasoning steps z1, z2, ..., zi. The MCTS search process constructs the tree with the aim of achieving terminal node sT +1 = (z0, ..., zT +1) that contains sound sequence of reasoning steps to final answer zT +1. Individual reasoning steps zj are obtained by querying the policy LM with the context sj1. Thus MCTS enables an efficient search over the space of reasoning sequences, towards valid final answer. To simplify, we use the notation of node-edge-node relationship denoted by (s, a, s), where represents the origin node, describes the action corresponding to an edge, and denotes the target node. This notation symbolically ties the action and the target state together, as the action uniquely identifies the target state and is therefore indicative of it. Notation. We chose to store all values in nodes instead of edges, which defines the following set of statistics saved for each node s: (s) - the visit count of node by the algorithm, q(s) - the running estimate of the q-value for the state s, T(s) - the binary value indicating if the node is Fig. 13: Example MCTS generated tree of reasoning sequences. The policy model Given question z0, the algorithm uses parametric policy model πθ (a pretrained LM) to generate candidate reasoning steps. In our implementation, special tokens are used to delimit individual reasoning steps. To help the policy model distinguish between individual reasoning steps, special token called the End of Intermediate Step (eois) is defined to mark the end of reasoning step. In addition, we need method to detect if state is terminal for the MCTS algorithm. By introducing separate token for the end of intermediate steps, we can leverage the end-of-sequence (eos) token from the RLM itself. The eos token naturally signals the conclusion of models reasoning process, indicating that the generated response is complete. The presence of an eos token can then be used to identify terminal states. The value model parametric value model is used to evaluate the quality of states. While MCTS traditionally approximates these values through extensive simulations, such an approach is computationally expensive and impractical in the RLM context. Inspired by AlphaZero [79], which replaces simulations with parameterized value model, we estimate state-action values (short q-value) for reasoning sequences using value modeleffectively employing process-based q-value model Qφ (see Appendix B.3). The value model is instantiated as pretrained transformer-based LM, modified by adding three linear layers and shifted, rescaled sigmoid activation to align the output domain to the state action function domain [1, 1] (see Eq. 5). This setup proved more stable than alternatives, such as tanh activation or cropped linear layer. We will show in the following how such model can be trained and provide description for the data generation process in Appendix D. During training, we assume access to final answer verifier, which evaluates the correctness of the models final answer and provides the true reward. C.1.2 MCTS Algorithm We now present the algorithmic steps of Monte Carlo Tree Search variant similar to AlphaZero as implemented in the x1 reasoning framework. The MCTS search operates in two distinct modes. The core difference is that, during training, final answer verifier evaluates and scores terminal nodes, providing true reward signal that is backpropagated through the tree. This reward serves as reliable learning signal for the value model Qφ. During inference, however, the verifier is unavailable, and decisions rely solely on the value model. terminal (0: not terminal, 1: terminal). Note that the q-value mathematically correspond to the value Q(s, a) where is the parent of and is the result of taking action from state s. This corresponds to traversing the directed edge (s, a, s), as the q-value describes the quality of taking action in state to reach s. Since only can uniquely characterize this transition, we chose it as the location to store the value. Additionally, we denote the set of children of state as C(s) and we denote the children as sc. The transition from to its child sc is then tied to the action ac. Inversely, we denote the parent of node as sp and the corresponding action that connects and sp as ap. Selection. The selection phase iteratively identifies the most promising child node with selection policy. We use the following selection policy which is the node-based variant of the PUCT algorithm in AlphaZero [80] (which is defined on edge-based values) without prior for finding selecting child of s: arg max scC(s) q(sc) + (cid:112)N (s) 1 1 + (sc) (cid:18) c1 + log (cid:19) (s) + c2 c2 where c1 and c2 are hyperparameters controlling the exploration bias, and the other values can be taken from the node statistics. Expansion. We opted for an explorative expansion phase, where nodes are appended to the selected leaf. In more greedy frameworks, only the highest-ranked candidate is appended to the MCTS. However, one of the major challenges when searching with RLMs is the lack of diversity and the extensive exploration required to cover the search space effectively. We found that an explorative expansion, where suboptimal nodes are also appended, provides additional diversity, as such nodes can sometimes lead to successful directions needed to solve complex tasks. To achieve this, each selected node is prompted times to the policy model to generate potential reasoning steps. Additionally, we employ diverse beam search to further enhance exploration across nodes, leveraging diversity in the decoding strategy. Once generated, all nodes are scored using the Q-value model q(sc) = Qφ(s, ac), sc C(s) and corresponding actions ac. Finally, terminality is assessed by checking whether the newly generated children include the (eos) token, which signals final answer. Backpropagation. The backpropagation step serves to propagate information from the terminal nodes back to their ancestors. In our implementation, the running estimates of the Q-values for the nodes along the selection path are updated using convex combination of their previous values and the discounted child values, weighted by an empirical distribution derived from the visit counts. For node s, this update rule is defined as: qnew(s) =(1 α)qold(s) + α πMCTS(ac s)γq(sc) , (cid:88) scC(s) where we look at the node-edge-node tuples (s, ac, sc) and sc C(s). The MCTS distribution πMCTS is defined over the visit scores of nodes as follows: πMCTS(ac s) = (sc) scC(s) (sc) (cid:80) . To motivate this formula, note that the visit scores of sc are directly tied to the action ac, due to the deterministic transitions in the MDP. Consequently, the probability of visiting the edge corresponding to action ac in state is determined by the visit scores of the resulting state sc, compared to the visit scores of all other child states sc accessible from s. This update rule effectively corresponds to SARSA update for sparse rewards, utilizing values weighted by the visit scores. Additionally, the visit counts are incremented for each node along the path during this process. True Reward Propagation. We propose to improve the quality of the q-values by propagating the real final rewards back through the tree when terminal state is reached. During training, terminal nodes can be evaluated against reference golden answer using an external verifier. reasoning sequence leading to valid final answer can then be evaluated with better evaluator than the q-value model. We suggest to replace the initialization of the running estimate of terminal state with the true reward based on the evaluation of the external verifier which in terminal states is equal to the q-value see Eq. 5. The reward is then backpropagated via the q-values through the tree with our backpropagation operator. This adjustment anchors the q-value model predictions with real reward signals and prevents the q-value model predictions to diverge. Due to the need for access to true answer, this correction can not be applied at inference. Therefore, we do not use this adjustment step in the expansion phase of the inference mode. Best Path Selection. After iterations, MCTS will have formed tree of explored reasoning trajectories. The path with the highest terminal value estimate is ultimately selected and its final reasoning step is returned as the final solution. 26 Algorithm 1 MCTS for Reasoning (Training mode in blue) Input: Policy model πθ, value model Qφ, question z0, golden answer g, binary correctness verifier Γ, number of MCTS iterations , number of children expanded in every selection phase , exploration constants c1, c2, MCTS reward-value convexity constant α. Output: Search tree = (N , E) with the best path τ . 1: s0 (z0) 2: (s0) = 0 3: {s0} 4: 5: 1 6: while or T(s) = 1 do {Start from root node} 7: 8: Selection 9: while is not leaf node do 10: {Initialize node set} {Initialize edge set} sc arg maxscC(s) q(sc) + {Initialize root node} s0 (s)1 1+N (sc) (cid:16) c1 + log (s)+c2 {Move to the selected child} (cid:17) s sc end while for = 1 to do 12: 13: 14: Expansion 15: 16: 17: 18: 19: 20: 21: zc (t1, . . . tzc) πθ{Sample new reasoning step} {Append zc to the current state s} sc zc {Predict with the Q-VM} q(sc) Qφ(s, ac) (sc) 1 {Initialize visit count} if sc terminal then {Mark as terminal} T(sc) (cid:40) r(s, ac) 1, 1, rectness with golden answer} q(sc) r(s, ac) if Γ(sc, g) = 1, if Γ(sc, g) = 0. {Check for cor- {Overwrite by true reward} end if {sc} {(s, sc)} 23: 24: 25: 26: 27: 28: Backpropagation 29: while = s0 do 30: 31: (s) (s) + 1 q(s) (1α)q(s)+αγ (cid:80) {Add the node to the tree} {Add the edge to the tree} {Update the visit count} scC(s) πM CT S(acs)q(sc) end for 11: 22: {Update the value} {Move to the parent} sp end while + 1 32: 33: 34: 35: 36: end while 37: Best Path Selection: 38: Select the best reasoning sequence . 39: 40: return , all reasoning sequences {s(i) }j C.2 Training Pipeline To adequately employ the MCTS-based reasoning scheme introduced in the Appendix C.1, the policy model must be fine-tuned to generate responses in the format of semantically-relevant reasoning steps. The value model 27 Fig. 14: The two phases of the training pipeline. q-value model in our case must be trained to accurately estimate the values of the sequences of reasoning steps. We propose two-phase training approach designed to let the policy effectively leverage the structured exploration and iterative refinement capabilities of the search process to generate optimal sequences of reasoning steps. detailed algorithmic description of the pipeline is in Figure 14. C.3 Phase 1: Supervised Fine-Tuning The first phase focuses on preparing the policy and value models to generate and evaluate reasoning trajectories effectively. This is achieved by supervised fine-tuning (SFT) training on dataset of example sequences of reasoning steps (where intermediate reasoning steps are terminated by an End of Intermediate Step eois token). The objective is twofold: (1) to fine-tune the policy model πθ to produce semantically coherent reasoning steps, and (2) to train the q-value model Qφ to accurately assign scalar scores to reasoning trajectories, distinguishing between high-quality and suboptimal reasoning paths. This supervised fine-tuning phase ensures that the policy can generate reasoning steps consistent with the structured format required for downstream MCTS-based exploration, while the q-value model provides reliable evaluations of intermediate and terminal states. Together, these components form the foundation for the subsequent online reinforcement learning in Phase 2, where the policy and qvalue models are further refined through interaction with the reasoning framework. (cid:17) SFT C.3.1 Datasets Generation and Preparation Dataset for SFT of the Policy. Performing SFT of the policy requires dataset of high-quality reasoning sequences (cid:16) SFT, y(i) x(i) denoted as DSFT = { }. Each pair in the dataset consists of prompt x(i) SFT composed of sequence of reasoning steps (for example x(i) SFT = (z0, ..., zj)), and target completion y(i) SFT = zj+1 which is the subsequent reasoning step or final answer. Appendix contains detailed account of the dataset creation and processing. It covers how the special eois token is appended to reasoning steps mark the end of step during inference. Dataset for Q-Value Model Training. Similarly to SFT, training the q-value model requires supervised dataset of reasoning sequences and corresponding scores. We denote this dataset DQVM-train = {(x(i) QVM-train)}, with reasoning sequences x(i) ) and target q-value y(i) QVM-train. Appendix explains how this dataset can be generated using an initial list of questions, base LLM for querying, and verifier program to label reasoning sequences as conducive to correct final answer or not. QVM-train, y(i) 0 , ..., z(i) QVM-train = (z(i) Policy Model Algorithm 2 SFT of Policy Model πθ (completion-only) Input: dataset DSFT = {(x(i), y(i))}, training hyperparameters (optimizer, learning rate η, batch size B, and maximum number of epochs E, ...). Output: Fine-tuned policy model πθ. tokenized πθ, 1: for epoch = 1 to do Shuffle dataset DSFT. 2: 3: Divide DSFT into batches {Bk} of size B. 4: 5: 6: 7: Initialize batch loss: Lbatch = 0. for each sample (x(i), y(i)) Bk do Iteratively predict completion tokens: for each batch Bk do (ˆy(i) x(i), πθ) = πθ(x(i) 1:t1), 1:t1 represents the context (prompt + where x(i) previously predicted tokens). Compute CE loss for each completion token: L(i) = (cid:80)y(i) t=1 log (ˆy(i) Accumulate the loss: Lbatch += L(i). = y(i) x(i), πθ). end for Normalize batch loss: Lbatch = Lbatch/Bk. Backpropagate gradients, update θ via optimizer. 8: 9: 10: 11: 12: 13: 14: end for end for C.3.2 SFT of the Policy Supervised fine-tuning (SFT) of the policy is performed on the dataset DSFT of prompts and target completions of the next reasoning step. The policy πθ is instantiated as general pretrained LLM. Specifically, we perform completiononly SFT such that for every (prompt, target completion) is trained to minimize the crosspair, the base model entropy loss between its predicted token probabilities and the ground-truth target completion. finetuned policy and q-value model. C.3.3 Q-Value Model Training The q-value model Qφ is trained on DQVM-train to assign appropriate scalar scores to the candidate reasoning trajectories. It is instantiated as pre-trained LLM with additional linear layer and to which shifted and rescaled classification head is added; we denote all of its trainable weights as φ. Depending on the reward design, the q-value model can be trained via scalar (least squares) regression if continuous rewards are chosen, or with classification objective such as the Binary Cross-Entropy (BCE) loss, if trajectories are labelled with binary rewards or as chosen-rejected preference pairs. By the end of training, Qφ should output accurate q-value scores, which will later guide policy refinement in Phase II and will improve the search accuracy when used in MCTS. Algorithm 3 Fine-Tuning the Q-Value Model Qφ Input: Q-Value Model Qφ (QVM), dataset DQVM-train = {(x(i), y(i))}, training hyperparameters (optimizer, learning rate η, batch size B, and maximum epochs E, ...). Output: Fine-tuned q-value model Qφ. Shuffle the dataset DQVM-train. 1: for epoch = 1 to do 2: 3: Divide DQVM-train into batches {Bk} of size B. 4: 5: 6: for each sample (x(i), y(i)) Bk do for each batch Bk do the q-value usind the QVM ˆy(i) = Predict Qφ(x(i)). Compute the loss: if Regression Loss then (cid:80) = 1 end if if Classification Loss then (x(i),y(i))(ˆy(i) y(i))2. = 1 (cid:80) (x(i),y(i)) BCE(ˆy(i), y(i)). end if Backpropagate gradients, update φ via optimizer. 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for end for end for C.4 Phase 2: RL Tuning of Policy with MCTS Phase 2 involves generating reasoning reasoning sequences from the policy with MCTS and the q-value model, and fine-tuning the policy with an RL-based alignment algorithm to generate better completions. The q-value model must also be continually updated in this training loop to keep in-distribution with the policys outputs. Sufficient Phase 1 pre-training of the policy and q-value model is crucial to ensure stable training of the models in Phase 2. The MCTS structure which provides balanced exploration-exploitation search combined with repeated sampling of the policy ensures sufficient exploration during this online-RL phase. This final training phase returns the C.4.1 Phase 2 Algorithm Notation. We present our notation with RL terminology which will be used in phase 2 training. For reminder, the MCTS builds tree of nodes that each define reasoning sequence: for example si = (z0, . . . , zi). These nodes represent state. An action ai corresponds to the next reasoning step zi+1 obtained when an LLM is queried with the state si as context. Finally, q-value estimate q(s) is associated to each node s. Thus, each reasoning sequence in the tree can be decomposed into set of state, action and q-value (s, a, q) triplets: s(i) , z(i) )}jt. These triplets can be used to perform reinforcement learning. = {s(i) j+1, q(s(i) Phase 2 assumes an initial dataset Dq = {z(i) 0 } of prompt questions - these questions may be isolated from the phase 1 dataset DSFT. The training process (Algorithm 4) involves repetition of rollout phase followed by training (reinforcement) phase. In the rollout phase, an MCTS tree is constructed (Algorithm 1 in training mode) which generates sequences of reasoning steps. The individual states of the tree are attributed value (see 1). These sequences are then stored in replay buffer R. The reinforcement phase samples batch of reasoning sequences from the replay buffer. From each trajectory, constituent states, actions and value estimatesand uses the corresponding values attributed during MCTS to perform RL training (for example with PPO or Reinforce). Alternative schemes may involve selecting preference pairs among trajectories and then aligning the policy using DPO, or simply selecting the most desirable trajectory per question and performing further SFT training. During this reinforcement phase, the value model is also updated to mimic the (backpropagated) values from the MCTS process (Algorithm 7). Algorithm 4 Phase 2: RL of the Policy and Q-Value Model Input: Policy πθ, q-value model Qφ, dataset Dq = {z(i) MCTS and RL hyperparameters. Output: Trained πθ and updated φ. 0 }, for each question z(i) 1: for each training iteration do 2: Rollout - 3: 4: 0 Dq do Generate MCTS tree with πθ guided by Qφ (Algorithm 1) Extract individual trajectories and store them in replay buffer = {τ (j)} 5: end for for each epoch do 6: 7: Training 8: 9: 10: 11: 12: 13: end for Sample batch from replay buffer R. Update policy πθ (Algorithm 5). Update q-value model Qφ (Algorithm 7). end for C.4.2 Policy Update The policy update is performed on batch of reasoning sequences. As mentioned above, the reasoning sequences can be decomposed into state-action-value triplets to then perform RL training. We distinguish between three reinforcement methods: standard RL, preference-based RL, or SFT training. Standard Policy Gradient RL Methods. Standard policy gradient methods such as Proximal Policy Optimization (PPO) [77] or REINFORCE [1], [86] are particularly suited for tasks where trajectories are collected (online) and reliably evaluated by the q-value model Qφ. PPO relies on the computation of trajectory (reasoning sequence) advantages ˆA(st), which quantify how much better or worse an action taken in given state is compared to the expected baseline value of that state. The advantage function is estimated by: ˆA(st) = Rt + γV (st+1) (st), where Rt is the immediate environment reward at step t, (st) is the state value of of state st, and γ is the discount factor. We can derive the state value easily from the q-values obtained via the q-value model or the running estimates in the MCTS as follows: (st+1) = 1 γ Qφ(st, at) since rewards are sparse. The standard PPO approach trains the critic model from scratch on bootstrapped rewards for this purpose. We introduce an alternative advantage computation scheme that leverages the backpropagated values from Monte Carlo Tree Search (MCTS) in conjunction with Qψ, as detailed in Algorithm 6. This integration combines MCTSs exploration and evaluation capabilities with the RL update, enhancing robustness and efficiency in reasoning tasks. Further regularization can be imposed on the PPO training procedure. To align the policy πθ with reference policy πref (usually instantiated as πθ before phase 2) during training, the KL divergence KL(πθπref), between the two distributions can be added to the training loss. Additionally, to maintain the diversity of policy generations (and exploration during training), the entropy of the policy distribution can be enhanced by substracting it from the loss. We denote the additional entropy loss term over batch of state-action pairs (s, a) as: LH = 1 (cid:88) (cid:88) (s,a)D πθ(as) log πθ(as). Direct Preference Optimization (DPO). DPO aligns the policy to user preferences expressed as pairwise comparisons between reasoning sequences. Given pairs (s+, s), where s+ is preferred over s. This method doesnt require reward/value model. Supervised Fine-Tuning (SFT). As straightforward alternative to RL, high-value reasoning sequences can be selected to perform SFT, i.e. train the policy to maximize the likelihood of these reasoning steps. This approach is inspired by AlphaZero-like frameworks, focusing on iteratively refining the policy to generate high-quality reasoning trajectories without requiring explicit rewards. C.4.3 Advantage Calculation (for PPO Policy Updates) 29 While standard advantage computation in PPO (e.g., via Generalized Advantage Estimation (GAE) [77]) is widely applicable, we propose an alternative approach tailored to our reasoning framework in Algorithm 6. Specifically, for each state/node s, we leverage the q-value estimates q(s) obtained during the MCTS process. They were updated in the backpropagation phase to provide more informed estimate of the q-values incorporating the estimates of the children and potentially true reward signals from terminal paths in the tree. We expect these MCTS-derived values to be more reliable as they incorporate the ground-truth terminal reward, propagated back through the tree, ensuring that nodes value reflects both its immediate reward and the aggregated values of subsequent child states. Algorithm 5 Policy Update (PPO, DPO, or SFT) Input: Batch D, policy πθ, reference policy πref, learning rate η, clipping parameter ε, preference data Dpref for DPO. Output: Updated policy πθ. 1: Train via PPO 2: Select state-action-value triplets from sequences in 3: for each (st, at, qt) do 4: Compute the policy ratio: rθ = πθ(atst) (atst) . Compute the advantages ˆA(st) (Algorithm 6). Compute the PPO loss: LPPO = min(rθ ˆA(st), clip(rθ, 1 ε, 1 + ε) ˆA(st)). πθref 5: 6: 7: end for 8: Optional: add KL divergence or entropy regularization. LPPO LPPO + λKLKL(πθπref) + λH LH . 9: Perform gradient update to refine πθ. 10: 11: Train via DPO (pairwise preferences) - 12: Select preference pairs of reasoning sequences in 13: for each pair (s+, s) Dpref do Compute DPO objective: 14: LDPO = 1 Dpref (cid:88) (cid:18) (cid:18) β log log σ (s+,s) πθ(s+) πθ(s) (cid:19)(cid:19) . 15: end for 16: Perform gradient update to refine πθ. 17: 18: Train via SFT (single target sequence) - 19: Select high-value reasoning sequences s+ from 20: for each reasoning sequence s+ do 21: 22: end for Perform SFT on s+ Algorithm 6 Advantage Calculation in MCTS Framework Input: MCTS Tree = (N, E), node statistics: rewards and q-values, Q-Value model Qφ, discount factor γ, and λ. Output: Advantages { ˆA(st)}. 1: for each node si do 2: 3: 4: Compute state values: vMCTS si+1 Compute state values: vMCTS Compute the advantage on the TD error: ˆA(si) = r(si, ai) + γvMCTS . si+1 γ qMCTS(si) γ qMCTS(si1) vMCTS si = 1 = 1 si 5: end for C.4.4 Q-Value Model Update During phase 2, the q-value model Qφ is also updated to track MCTS-backtracked value estimates qMCTS(st) which should be of higher quality (thanks to the final answer verifier and score aggregation from child nodes). For each subsequence st = (z0, z1, . . . , zt), we train the q-value model Qφ via squared error minimization, to match its values Qφ(st) as closely as possible to the MCTS-values QMCTS(st). This has the benefit of both improving the accuracy of the value model, and keeping it in-distribution with the new policy outputs during this online-RL training. Algorithm 7 Q-Value Model Update Input: Batch D, q-value model Qφ, learning rate η. Output: Updated Qφ. 1: for each (st, rst) do 2: Compute loss: Lq = 1 (cid:80) 3: end for 4: Perform gradient update on Lq. (st,rst )(Qφ(st) qMCTS(st))2."
        },
        {
            "title": "APPENDIX D\nDATA GENERATION",
            "content": "D.1 Generating Data for Phase 1 Policy Model Training The objective of this training process is to introduce new End of Intermediate Step (EOIS) token that serves to delimit individual reasoning steps while preserving the original distribution of the model as much as possible. To achieve this, the model is trained on data generated by itself using greedy decoding. The training data are derived from eight chain-ofthought (CoT) completions generated for 1,000 questions sampled from the training split of the MATH dataset [41]. These completions are produced using the same model intended for subsequent training with greedy decoding. During this generation process, the reasoning steps in the data are observed to be separated by two consecutive nn. This observation informs the method of delimitation used to construct pairs of questions and their corresponding sequences of reasoning steps. For each data point, consisting of question prompt and its associated target response comprising multiple reasoning steps (q(i), [s(i) ]), additional tokens are introduced 1 , . . . , s(i) to explicitly mark the boundaries of the reasoning steps. Specifically, the End of Intermediate Step (EOIS) token is defined and inserted after each reasoning step s(i) , resulting in modified step s(i) . Additionally, the End of Sequence (EOS) token is appended to the final reasoning step s(i) , yielding s(i) ; eos]. This augmentation ensures that the model can consistently identify when final solution has been reached during inference. = [s(i) For Llama models, it has been empirically observed that introducing an assistant token after each reasoning step enhances the models effective utilization of the EOIS token. However, this behavior may not generalize to other base models, necessitating careful consideration when applying this approach. Accordingly, the target sequence for supervised finetuning (SFT) is constructed as: y(i) SFT = [s(i) 1 , eois, assistant, s(i) 2 , . . . , s(i) , eos]. This approach yields training dataset comprising pairs of prompts and their corresponding target completions, formally represented as: DSFT = {(q(i), y(i) SFT)}. D.2 Generating Data for Phase 1 Value Model Training The original MCTS framework relies on simulations to evaluate state. Given the state, rollouts are performed till terminal state is reached. The terminal states usually can be evaluated (e.g., in math by comparing it with the golden answer). This enables the distribution of terminal rewards based on their success which are then aggregated to provide value estimate of the state. These Monte Carlo simulations serve as an estimate of states ability to lead to correct answer. The value estimated in this manner corresponds to the expected cumulative future reward for given state: Vπθ (s) = Eτ πθ (cid:34) (cid:88) (cid:35) γtir(st, at) si = t=i where is the terminal step of the (sub-) reasoning chain τ = (si, ai, ri, si+1, . . . , sT , aT , rT , sT +1). Since rewards are sparse (i.e., r(st, at) = 0 for all < ), the value function simplifies to: Vπθ (st) = Eπθ (cid:104) γT tr(sT , aT ) st (cid:105) This represents the expected terminal reward, which can be empirically estimated using Monte Carlo (MC) estimates: Vπθ (st)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 γT tr(s(i) , a(i) ) := ˆV (st) , s(i) where is the number of sampled reasoning chains, and , a(i) s(i) +1 denote the last transition of the simulation trajectory τ (i) = (st, a(i) t+1, . . . , s(i) +1) for {1, . . . , }. , a(i) , s(i) , s(i) To avoid sample inefficiencies and high computational burdens, AlphaGo Zero [80] and AlphaZero [79] introduce value model to replace simulations by using its predictions for state. We follow this approach by defining processbased value model Vφ. Notably, we train this model with simulation data (instead of true value functions), thereby building model that predicts state value function estimates ˆV . We denote this model as ˆVφ, parameterized by φ. Given that the input of value model is sequence of reasoning steps - therefore sequence of tokens, the natural value model architecture is to use an LLM on which one adds linear layer(s) and suitable ouput activation function. Typically, it is designed to output scalar value ˆVφ(st) R. The core distinction between different modeling approaches to state value functions lies in how rewards are modeled. Depending on whether binary reward setting or continuous (bounded) one is used, the aggregation mechanism, model architecture, training loss, and interpretation of the predictions vary. We provide an overview of both scenarios and, although often omitted for simplicity, we consider both γ = 1 and γ (0, 1] for continuous rewards in our analysis. D.2.1 Binary Rewards: Modeling the Likelihood of Correct Terminal State For this approach the rewards are modeled binary, therefore r(sT , aT ) = +1 for correct solutions and r(sT , aT ) = 0 for incorrect solutions. We will adopt discount factor of γ = 1 which we will see aligns more with the interpretation this reward model provides and is widely adopted in literature. This approach corresponds to the value model proposed in AlphaGo Zero [80]. D.2.1.1 State Value Estimation: The value function then further simplifies to: Vπθ (st) = Eπθ [r(sT , aT ) st] = Pπθ (r(sT , aT ) = 1 st) This formulation represents the probability of reaching correct terminal state from given state st. Empirically, this probability is estimated using simulations as follows: Vπθ (st) #correct simulations #simulations := ˆV (st). D.2.1.2 Data Generation: To generate labels for estimating the state-value function during the training of value model, we use MCTS with simulations till terminal node is reached and calculate the ratio between the number of correct simulations to the number of simulations. There is one very important detail, for trajectory τ = (si, ai, ri, si+1, . . . , sT +1) where sT +1 is terminal state. By definition, the true state value function at sT +1 is zero. However, in training the value model, we avoid instructing it to output zero for terminal states. Instead, in supervised learning setting, we can identify terminal states and directly compare the models predictions against the known correct outcomes (referred to here as golden answers). This comparison negates the need to rely solely on the value model to estimate the value of terminal states or to determine the reward associated with transitioning into these states. During inference, while we can still recognize terminal states, we cannot evaluate them by comparing the models output to golden answer. Therefore, an alternative metric is necessary. We train the value model to predict whether transitioning to sT +1 leads to correct terminal outcome. By learning the relationship between nodes content and the correctness 31 of the resulting terminal state, the model can estimate the likelihood that terminal state leads to correct answer. To approximate the terminal reward during inference, we define:r(sT , aT , sT +1) 1[0.5,1]( ˆVφ(sT +1)) Here ˆVφ(sT +1) represents the value predicted by the value model for the terminal state sT +1. If this predicted likelihood exceeds threshold (e.g., 0.5), we assign terminal reward of 1; otherwise, we assign reward of 0. This approach allows the value model to indirectly influence the terminal reward by predicting the likelihood of correct outcome. Consequently, during training, terminal rewards serve as labels for terminal states in the value model. It is important to note that ˆVφ(sT +1) is not used in any other context but solely to estimate the terminal reward. ˆVφ(sT +1) = ˆV (sT +1) This distinction clarifies that the predicted value for the terminal state ˆVφ(sT +1) differs from the standard value functions definition ˆV (sT +1) = 0. D.2.1.3 Model Training ˆVφ : [0, 1]: When trained with these labels we obtain value model ˆVφ, parameterized by φ, that represents the likelihood of correct terminal state emanating from state st. Therefore, the model will output values between 0 and 1. To accommodate the binary classification nature of this task, the model should employ sigmoid activation function in the output layer. The training objective is then to minimize the binary crossentropy (CE) loss between the predicted probabilities and the empirical estimates derived from the simulations: L(φ) = (cid:88)"
        },
        {
            "title": "1\nN",
            "content": "i=1 (cid:104) yi log (cid:16) ˆVφ(s(i) ) (cid:17) + (1 yi) log (cid:16) 1 ˆVφ(s(i) ) (cid:17)(cid:105) where yi {0, 1} denotes the binary label indicating whether the i-th simulation resulted in correct terminal state. Employing binary reward structure offers several benefits. First of all, simplicity since binary rewards simplify the learning process, reducing the complexity associated with continuous reward signals. Moreover, the clear distinction between correct and incorrect states facilitates faster convergence during training making this approach effective. In addition, binary classification is less susceptible to noise in reward signals, ensuring more stable value estimates. Furthermore, this approach aligns with the objectives of reinforcement learning in achieving clear and unambiguous rewards, thereby streamlining the optimization of the policy πθ. D.2.2 Continuous and Bounded Rewards: Modeling the Expected Future Reward We model the rewards to be continuous and bounded by allowing values in [a, b]: Vπθ (st) [a, b] common design, is to set the borders to 1 and 1 such that terminal reward is r(sT , aT ) = +1 for correct terminal states and r(sT , aT ) = 1 for incorrect states. This approach models the expected future reward as continuous and bounded value, capturing the degree of correctness or quality of the terminal state. In contrast to the binary reward structure, continuous and bounded rewards provide more nuanced representation of the outcomes in reasoning tasks. Note, that without discounting this approach resembles the proposed value model of AlphaZero [79]. D.2.2.1 Bounded rewards: By constraining rewards within predefined interval [a, b], we effectively create correctness scale where the extremities represent the definitive outcomes of the reasoning process. Specifically, the lower bound corresponds to reaching an incorrect terminal state, while the upper bound signifies correct terminal state. This bounded framework mirrors the spectrum of possible correctness, allowing the model to capture varying degrees of solution quality between these extremes. Such scale facilitates more nuanced evaluation of intermediate states, reflecting partial correctness or varying levels of reasoning quality. Moreover, this approach ensures that the reward signals remain interpretable and consistent, fostering clear distinction between successful and unsuccessful outcomes. D.2.2.2 State Value Estimation: With discount factor γ (0, 1], the value function is defined as: (cid:105) (cid:104) Vπθ (st) = γT tr(sT , aT ) st where r(sT , aT ) = for correct terminal states and r(sT , aT ) = for incorrect ones. Empirically, this expectation is approximated by averaging the rewards of the simulations: Vπθ (st)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 γT tr(s(i) , a(i) ) := ˆV (st) , s(i) +1) represent , a(i) t+1, . . . , s(i) the number of , s(i) , a(i) where denotes sampled reasoning chains, and (s(i) the final the i-th simulation trajectory τ (i) = transition of (st, a(i) +1) for {1, . . . , }. If discount factor is applied γ (0, 1) then each terminal reward is discounted proportional to the number of steps needed to reach the terminal state. This corresponds to the soft estimation proposed by Wang et al. [95]. We want to note that this estimator typically underestimates due to its proneness to false negatives [38], [102]. , s(i) D.2.2.3 Data Generation: Therefore, to generate labels for state-value function estimate pairs to train value model, we use MCTS with simulations and average the outcomes of the simulations. Therefore, at each newly generated node we simulate till terminal node is reached and we record the depth - the number of steps needed starting from (since is not identical per trajectory). We then record the the terminal reward which in our case is r(sT , aT ) = 1 for correct and r(sT , aT ) = 1 for incorrect answers. Discounted by the depth we can average these rewards nd obtain an estimation of the node value which serves as label for the initial value model training. D.2.2.4 Model Training ˆVφ : [a, b]: The value model ˆVφ, parameterized by φ, is designed to predict the expected terminal reward from any given state st. To accommodate the continuous and bounded nature of this task, 32 the model employs scaled and shifted sigmoid activation function in the output layer, ensuring that the predictions remain within the range [a, b]. The training objective is to minimize the mean squared error (MSE) loss between the predicted values and the empirical estimates derived from the simulations: L(φ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:16) ˆVφ(s(i) ) γT tr(s(i) , a(i) ) (cid:17) We also experimented with tanh activation output and linear layer with clipping of the values. However, both methods proved to be unstable in training in contrast to the scaled and shifted sigmoid layer. tanh and sigmoid layer naturally bound the output but also push values towards the extremes, enhancing the separation between high and low value estimates. This characteristic can improve the models ability to distinguish between highly correct and highly incorrect states which is why we are particularly interested in these activation functions. D.2.2.5 Discounting: Introducing discount factor γ aligns the value function with the incremental nature of reasoning tasks. Unlike traditional games, where all moves contribute indirectly and trajectories are not penalized for length, reasoning benefits from discouraging the unnecessary or redundant steps. The inclusion of discount factor γ ensures that rewards achieved sooner have greater impact on the value function, the model incentivizes reaching correct solutions with fewer steps which ultimately enhances efficiency and suppresses redundancies. Moreover, this models the uncertainty decay in the trajectories; the further into the future reward lies, the more uncertain its prediction becomes. Discounting naturally reduces the reliance on these uncertain longterm rewards, thereby stabilizing the learning process by focusing on more predictable and immediate outcomes. However, the models performance becomes sensitive to the choice of γ, requiring careful tuning to balance immediate versus long-term rewards. the influence of Balancing the discount factor is essential to ensure that the model effectively captures the importance of both progress and the final correctness of the reasoning chain. Employing continuous and bounded reward structure offers several benefits. Unlike binary rewards, continuous rewards provide finer distinction between varying degrees of correctness, allowing the model to capture subtle differences in terminal states. Continuous rewards can encode more information about the quality of solutions, facilitating more informed decision-making during the search process. Bounded rewards prevent extreme values, promoting numerical stability and consistent training dynamics. However, this also shows that the choice of reward values and their scaling can significantly impact the learning process, necessitating careful calibration to ensure effective training. D.3 State Action Value Function Modeling The state-action value function, commonly denoted as Qπθ (st, at), represents the expected cumulative reward of taking action at in state st under policy πθ. Formally, it is defined in our framework as: Qπθ (st, at) = Eτ πθ (cid:34) (cid:88) i=t γitr(si, ai) st, at (cid:35) = r(st, at) + γEτ πθ γi(t+1)r(si, ai) st, at (cid:35) (cid:34) (cid:88) i=t+ = r(st, at) + γEst+1 [Vπθ (st+1) st, at] det. = r(st, at) + γVπθ (st+1), where denotes the terminal step of the (sub-) reasoning chain τ = (st, at, rt, st+1, . . . , sT , aT , rT , sT +1). In environments characterized by sparse rewards, where r(st, at) = 0 for all < , the Q-value simplifies to: Qπθ (st, at) = γVπθ (st+1). At terminal states, where the state value Vπθ (sT +1) = 0, the Q-value further reduces to: Qπθ (sT , aT ) = r(sT , aT ). D.3.1 Process-Based Q-Value Modeling process-based Q-value model utilizes the same architecture as process-based Value Model, typically leveraging LLM enhanced with additional linear layers and an appropriate output activation function. The output is scalar value ˆQφ(st, at) R. Specifically, the Q-value model takes state-action paircomprising sequence of past steps and the current actionand predicts the corresponding Q-value based on the aforementioned formulation. D.3.1.1 Training Data Generation: To train the Qvalue model, it is essential to compute the Q-values for various state-action pairs. For < , Q-values can be estimated using Monte Carlo simulations as follows: Qπθ (st, at) = r(st, at) + γVπθ (st+1) = γVπθ (st+1) 1 γT (t+1)r(s(i) γ (cid:88) i=1 , a(i) ) (since r(st, at) = 0) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 γT tr(s(i) , a(i) ) := ˆQ(st, at), where is the number of sampled reasoning chains, and τ (i) = (st, a(i) +1) represents the i-th simulation trajectory for {1, . . . , }. This estimation aligns with the state value estimation under the sparse reward formulation: t+1, . . . , s(i) , a(i) , s(i) , s(i) ˆQ(st, at) = ˆV (st). For = , the Q-value is directly given by the immediate reward: Qπθ (sT , aT ) = r(sT , aT ) = Vπθ (sT +1) = ˆV (sT ) = 0. D.3.1.2 Reward Modeling: For Q-value models the same discussions about reward modeling apply here since the models are trained very similar. This is why omit it here. 33 D.3.2 The Difference between Value and Q-Value Models The difference of VMs and QVMs can be easily shown in how they are used in the evaluation processes of an MCTS algorithm. QVMs predict ˆQφ(st, at), which evaluates the action at taken in state st that deterministically transitions to st+1. Thus, the value ˆQ(st, at) is used to evaluate adding the node st+1 to the tree. On the other hand, for VMs, adding node st+1 to the tree is determined by ˆV (st+1) = 1 γ ˆQφ(st, at), where γ is the discount factor. This distinction is making the training processes different. Note that st at = st+1. For QVMs, the training tuples are ((st, at), ˆQ(st, at)) = (st+1, ˆQ(st, at)) due to the deterministic transition. For VMs, the corresponding training tuples are (st+1, ˆV (st+1)). Since we propose training VMs on terminal rewards for terminal states instead of assigning label of 0, VMs and QVMs become equivalent under the following transformation for any {0, . . . , } for evaluating adding node st+1: ˆV (st+1) = 1 γ ˆQφ(st, at) We introduced Q-value models since they address critical inconsistency of value models in terminal states. Specifically, while value models assign flat value of zero to terminal states, Q-value models provide meaningful evaluation of the final actions correctness through Qπθ (sT , aT ) = r(sT , aT ). This distinction is essential for accurately assessing whether terminal step leads to correct or incorrect response during inference."
        },
        {
            "title": "REFERENCES",
            "content": "[1] [2] [3] [4] [5] A. Ahmadian, C. Cremer, M. Galle, M. Fadaee, J. Kreutzer, O. Pietquin, A. Ust un, and S. Hooker. Back to Basics: Revisiting REINFORCE-Style Optimization for Learning from Human In L.-W. Ku, A. Martins, and V. Srikumar, Feedback in LLMs. editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 24, pages 1224812267, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. J. Ahn, R. Verma, R. Lou, D. Liu, R. Zhang, and W. Yin. Large Language Models for Mathematical Reasoning: Progresses and Challenges. In N. Falk, S. Papi, and M. Zhang, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, EACL 24, pages 225237, St. Julians, Malta, Mar. 2024. Association for Computational Linguistics. AI-MO. Aime 2024. https://huggingface.co/datasets/AI-MO/ aimo-validation-aime, July 2024. accessed 2025-01-19. AI-MO. Amc 2024. https://huggingface.co/datasets/AI-MO/ai mo-validation-amc, July 2024. accessed 2025-01-19. T. Ben-Nun and T. Hoefler. Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency Analysis. ACM Comput. Surv., 52(4):65:165:43, Aug. 2019. [6] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H. Niewiadomski, P. Nyczyk, and T. Hoefler. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1768217690, Mar. 2024. [7] M. Besta, A. C. Catarino, L. Gianinazzi, N. Blach, P. Nyczyk, H. Niewiadomski, and T. Hoefler. HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers. In S. Villar and B. Chamberlain, editors, Proceedings of the Second Learning on Graphs Conference (LOG 23), volume 231 of Proceedings of Machine Learning Research, pages 15:115:20, Virtual Event, Nov. 2023. PMLR. [8] M. Besta, R. Grob, C. Miglioli, N. Bernold, G. Kwasniewski, G. Gjini, R. Kanakagiri, S. Ashkboos, L. Gianinazzi, N. Dryden, and T. Hoefler. Motif Prediction with Graph Neural Networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 22, pages 3545, Washington DC, USA, Aug. 2022. Association for Computing Machinery. [9] M. Besta and T. Hoefler. Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(5):25842606, May 2024. [10] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzendorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. uller, et al. Multi-Head RAG: Solving Multi-Aspect Problems with LLMs, Nov. 2024. arXiv:2406.05085. [11] M. Besta, F. Memedi, Z. Zhang, R. Gerstenberger, N. Blach, P. Nyczyk, M. Copik, G. Kwasniewski, J. uller, L. Gianinazzi, et al. Demystifying Chains, Trees, and Graphs of Thoughts, Apr. 2024. arXiv:2401.14295. [12] M. Besta, L. Paleari, A. Kubicek, P. Nyczyk, R. Gerstenberger, P. Iff, T. Lehmann, H. Niewiadomski, and T. Hoefler. CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks, June 2024. arXiv:2406.02524. [13] M. Besta, P. Renc, R. Gerstenberger, P. Sylos Labini, A. Ziogas, T. Chen, L. Gianinazzi, F. Scheidl, K. Szenes, A. Carigiet, P. Iff, G. Kwasniewski, R. Kanakagiri, C. Ge, S. Jaeger, J. Was, F. Vella, and T. Hoefler. High-Performance and Programmable Attentional Graph Neural Networks with Global Tensor Formulations. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 23, Denver, CO, USA, Nov. 2023. Association for Computing Machinery. [14] M. Besta, Z. Vonarburg-Shmaria, Y. Schaffner, L. Schwarz, G. Kwasniewski, L. Gianinazzi, J. Beranek, K. Janda, T. Holenstein, S. Leisinger, P. Tatkowski, E. Ozdemir, A. Balla, M. Copik, P. Lindenberger, M. Konieczny, O. Mutlu, and T. Hoefler. GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra. Proc. VLDB Endow., 14(11):19221935, July 2021. [15] Z. Bi, K. Han, C. Liu, Y. Tang, and Y. Wang. Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning, Dec. 2024. arXiv:2412.09078. [16] R. A. Bradley and M. E. Terry. Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons. Biometrika, 39(3/4):324345, Dec. 1952. [17] E. Charniak and M. Johnson. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In K. Knight, H. T. Ng, and K. Oflazer, editors, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL 05, pages 173180, Ann Arbor, MI, USA, June 2005. Association for Computational Linguistics. [18] G. Chen, M. Liao, C. Li, and K. Fan. AlphaMath Almost Zero: Process Supervision without Process. In Proceedings of the Thirtyeighth Annual Conference on Neural Information Processing Systems (NeurIPS 24), volume 37 of Advances in Neural Information Processing Systems, Vancouver, Canada, Dec. 2024. Curran Associates. [19] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating Large Language Models Trained on Code, July 2021. arXiv:2107.03374. [20] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. Transactions on Machine Learning Research, Nov. 2023. [21] X. Chen, M. Lin, N. Scharli, and D. Zhou. Teaching Large Language Models to Self-Debug, Oct. 2023. arXiv:2304.05128. [22] K. Chernyshev, V. Polshkov, E. Artemova, A. Myasnikov, V. Stepanov, A. Miasnikov, and S. Tilga. U-MATH: UniversityLevel Benchmark for Evaluating Mathematical Skills in LLMs, Jan. 2025. arXiv:2412.03205. [23] F. Chollet. On the Measure of Intelligence, Nov. 2019. arXiv:1911.01547. 34 [24] P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei. Deep Reinforcement Learning from Human Preferences, Feb. 2023. arXiv:1706.03741. [25] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training Verifiers to Solve Math Word Problems, Nov. 2021. arXiv:2110.14168. [26] G. Cui, L. Yuan, Z. Wang, H. Wang, W. Li, B. He, Y. Fan, T. Yu, Q. Xu, W. Chen, et al. Process Reinforcement through Implicit Rewards. https://curvy-check-498.notion.site/Process-Reinfor cement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2ea f896f, Jan. 2025. [27] M. DeLorenzo, A. B. Chowdhury, V. Gohil, S. Thakur, R. Karri, S. Garg, and J. Rajendran. Make Every Move Count: LLMbased High-Quality RTL Code Generation Using MCTS, Feb. 2024. arXiv:2402.03289. [28] Y. Deng, W. Zhang, Z. Chen, and Q. Gu. Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves, Apr. 2024. arXiv:2311.04205. [29] X. Dong, M. Teleki, and J. Caverlee. Survey on LLM InferenceTime Self-Improvement, Dec. 2024. arXiv:2412.14352. [31] [30] X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. AlphaZero-Like Tree-Search Can Guide Large Language Model Decoding and Training, Feb. 2024. arXiv:2309.17179. J. Frohberg and F. Binder. CRASS: Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models. In N. Calzolari, F. Bechet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, J. Odijk, and S. Piperidis, editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 22, pages 21262140, Marseille, France, June 2022. European Language Resources Association. [32] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. PAL: Program-Aided Language Models, Jan. 2023. arXiv:2211.10435. [33] E. Glazer, E. Erdil, T. Besiroglu, D. Chicharro, E. Chen, A. Gunning, C. F. Olsson, J.-S. Denain, A. Ho, E. de Oliveira Santos, O. Jarviniemi, M. Barnett, R. Sandler, M. Vrzala, J. Sevilla, Q. Ren, E. Pratt, L. Levine, G. Barkley, N. Stewart, B. Grechuk, T. Grechuk, S. V. Enugandla, and M. Wildon. FrontierMath: Benchmark for Evaluating Advanced Mathematical Reasoning in AI, Dec. 2024. arXiv:2411.04872. [34] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. AlDahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The Llama 3 Herd of Models, Nov. 2024. arXiv:2407.21783. [35] X. Guan, Y. Liu, X. Lu, B. Cao, B. He, X. Han, L. Sun, J. Lou, B. Yu, Y. Lu, and H. Lin. Search, Verify and Feedback: Towards Next Generation Post-Training Paradigm of Foundation Models via Verifier Engineering, Nov. 2024. arXiv:2411.11504. [36] X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang, and M. Yang. rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking, Jan. 2025. arXiv:2501.04519. [37] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-Augmented Language Model Pre-Training, Feb. 2020. arXiv:2002.08909. [38] A. Havrilla, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, and R. Raileanu. GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning (ICML 24), volume 235 of Proceedings of Machine Learning Research, pages 1771917733, Vienna, Austria, July 2024. PMLR. [39] C. He, R. Luo, Y. Bai, S. Hu, Z. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun. OlympiadBench: Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 24, pages 38283850, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. [40] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring Massive Multitask Language Understanding. In Proceedings of the Ninth International Conference on Learning Representations, ICLR 21, Virtual Event, May 2021. [41] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring Mathematical Problem Solving with the MATH Dataset. In Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems: Datasets and Benchmarks Track, NeurIPS 21, Virtual Event, Dec. 2021. [43] [42] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The Curious Case of Neural Text Degeneration. In Proceedings of the Eighth International Conference on Learning Representations, ICLR 20, Virtual Event, Apr. 2020. J. Huang and K. C.-C. Chang. Towards Reasoning in Large Language Models: Survey. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 10491065, Toronto, Canada, July 2023. Association for Computational Linguistics. J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large Language Models Cannot Self-Correct Reasoning Yet. In Proceedings of the Twelfth International Conference on Learning Representations, ICLR 24, Vienna, Austria, May 2024. J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large Language Models Can Self-Improve, Oct. 2022. arXiv:2210.11610. [45] [44] [46] Y. Huang, M. Kleindessner, A. Munishkin, D. Varshney, P. Guo, and J. Wang. Benchmarking of Data-Driven Causality Discovery Approaches in the Interactions of Arctic Sea Ice and Atmosphere. Frontiers in Big Data, 4(32):642182:1642182:19, Aug. 2021. S. Imani, L. Du, and H. Shrivastava. MathPrompter: Mathematical Reasoning using Large Language Models, Mar. 2023. arXiv:2303.05398. [47] [48] W. Knight. OpenAI Unveils New A.I. That Can Reason Through Math and Science Problems. https://www.nytimes.com/2024/1 2/20/technology/openai-new-ai-math-science.html, Dec. 2024. accessed 2024-12-27. [49] L. Kocsis and C. Szepesvari. Bandit Based Monte-Carlo Planning. In J. urnkranz, T. Scheffer, and M. Spiliopoulou, editors, Proceedings of the European Conference on Machine Learning ECML 06, volume 4212 of Lecture Notes in Computer Science (LNAI), pages 282293, Berlin, Germany, Sept. 2006. Springer. [50] Y. Leviathan, M. Kalman, and Y. Matias. Fast Inference from Transformers via Speculative Decoding. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning (ICML 23), volume 202 of Proceedings of Machine Learning Research, pages 1927419286, Honolulu, HI, USA, July 2023. PMLR. [51] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. uttler, M. Lewis, W.-t. Yih, T. Rocktaschel, S. Riedel, and Retrieval-Augmented Generation for KnowledgeD. Kiela. Intensive NLP Tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Proceedings of the Thirtyfourth Annual Conference on Neural Information Processing Systems (NeurIPS 20), volume 33 of Advances in Neural Information Processing Systems, pages 94599474, Virtual Event, Dec. 2020. Curran Associates. [52] X. Li, G. Dong, J. Jin, Y. Zhang, Y. Zhou, Y. Zhu, P. Zhang, and Z. Dou. Search-o1: Agentic Search-Enhanced Large Reasoning Models, Jan. 2025. arXiv:2501.05366. [53] X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis. Contrastive Decoding: Openended Text Generation as Optimization. In A. Rogers, J. BoydGraber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 23, pages 1228612312, Toronto, Canada, July 2023. Association for Computational Linguistics. [54] M. Liao, W. Luo, C. Li, J. Wu, and K. Fan. MARIO: MAth Reasoning with code Interpreter Output Reproducible Pipeline, Feb. 2024. arXiv:2401.08190. [55] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets Verify Step by Step. In Proceedings of the Twelfth International Conference on Learning Representations, ICLR 24, Vienna, Austria, May 2024. [56] P. Lu, L. Qiu, W. Yu, S. Welleck, and K.-W. Chang. Survey In A. Rogers, of Deep Learning for Mathematical Reasoning. J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 23, pages 1460514631, Toronto, Canada, July 2023. Association for Computational Linguistics. [57] L. Luo, Y. Liu, R. Liu, S. Phatale, M. Guo, H. Lara, Y. Li, L. Shu, Y. Zhu, L. Meng, J. Sun, and A. Rastogi. Improve Mathematical 35 Reasoning in Language Models by Automated Process Supervision, Dec. 2024. arXiv:2406.06592. [58] M. Luo, S. Kumbhar, M. shen, M. Parmar, N. Varshney, P. Banerjee, S. Aditya, and C. Baral. Towards LogiGLUE: Brief Survey and Benchmark for Analyzing Logical Reasoning Capabilities of Language Models, Mar. 2024. arXiv:2310.00836. [59] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-Refine: Iterative Refinement with Self-Feedback, May 2023. arXiv:2303.17651. [60] A. Malinin and M. Gales. Uncertainty Estimation in Autoregressive Structured Prediction. In Proceedings of the Ninth International Conference on Learning Representations, ICLR 21, Virtual Event, May 2021. [62] [61] C. Metz. In Two Moves, AlphaGo and Lee Sedol Redefined the Future. https://www.wired.com/2016/03/two-moves-alphago -lee-sedol-redefined-future/, Mar. 2016. Wired. I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, and M. Farajtabar. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models, Oct. 2024. arXiv:2410.05229. J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Sch olkopf. Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks. Journal of Machine Learning Research, 17(32):1102, 2016. [63] [64] OpenAI. Introducing ChatGPT. https://openai.com/index/cha tgpt/, Nov. 2022. accessed 2024-12-27. [65] OpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4 o/, May 2024. accessed 2025-01-01. [66] OpenAI. Introducing OpenAI o1. https://openai.com/o1/, 2024. accessed 2024-12-27. [67] R. Y. Pang, W. Yuan, H. He, K. Cho, S. Sukhbaatar, and J. E. Weston. Iterative Reasoning Preference Optimization. In Proceedings of the Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 24), volume 37 of Advances in Neural Information Processing Systems, Vancouver, Canada, Dec. 2024. Curran Associates. S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang, and H. Chen. Reasoning with Language Model Prompting: Survey. J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 23, pages 53685393, Toronto, Canada, July 2023. Association for Computational Linguistics. In A. Rogers, [68] [69] Y. Qin, X. Li, H. Zou, Y. Liu, S. Xia, Z. Huang, Y. Ye, W. Yuan, H. Liu, Y. Li, and P. Liu. O1 Replication Journey: Strategic Progress Report Part 1, Oct. 2024. arXiv:2410.18982. [70] Y. Qu, T. Zhang, N. Garg, and A. Kumar. Recursive Introspection: Teaching Language Model Agents How to Self-Improve, July 2024. arXiv:2407.18219. [71] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS 23), volume 36 of Advances in Neural Information Processing Systems, pages 5372853741, New Orleans, LA, USA, Dec. 2023. Curran Associates. [72] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: Graduate-Level GoogleProof Q&A Benchmark, Nov. 2023. arXiv:2311.12022. [73] C. D. Rosin. Multi-Armed Bandits with Episode Context. Annals of Mathematics and Artificial Intelligence, 61(3):203230, Mar. 2011. [74] A. Saparov and H. He. Language Models Are Greedy Reasoners: Systematic Formal Analysis of Chain-of-Thought. In Proceedings of the Eleventh International Conference on Learning Representations, ICLR 23, Kigali, Rwanda, May 2023. [76] [75] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-Critiquing Models for Assisting Human Evaluators, June 2022. arXiv:2206.05802. J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, T. Lillicrap, and D. Silver. Mastering Atari, Go, Chess and Shogi by Planning With Learned Model. Nature, 588:604609, Dec. 2020. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and [77] 36 O. Klimov. Proximal Policy Optimization Algorithms, Aug. 2017. arXiv:1707.06347. Integration in LLMs for Enhanced Mathematical Reasoning, Oct. 2023. arXiv:2310.03731. [78] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the Game of Go With Deep Neural Networks and Tree Search. Nature, 529:484489, Jan. 2016. [79] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan, , and D. Hassabis. General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go Through Self-Play. Science, 362(6419):11401144, Dec. 2018. [80] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the Game of Go without Human Knowledge. Nature, 550:354359, Oct. 2017. [81] C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Model Parameters, Aug. 2024. arXiv:2408.03314. [82] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. GarrigaAlonso, et al. Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, June 2023. arXiv:2206.04615. S. Srivastava, A. M. B, A. P. V, S. Menon, A. Sukumar, A. S. T, A. Philipose, S. Prince, and S. Thomas. Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap, Feb. 2024. arXiv:2402.19450. [83] [84] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, Learning to A. Radford, D. Amodei, and P. F. Christiano. Summarize with Human Feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Proceedings of the Thirty-fourth Annual Conference on Neural Information Processing Systems (NeurIPS 20), volume 33 of Advances in Neural Information Processing Systems, pages 30083021, Virtual Event, Dec. 2020. Curran Associates. J. Sun, C. Zheng, E. Xie, Z. Liu, R. Chu, J. Qiu, J. Xu, M. Ding, H. Li, M. Geng, et al. Survey of Reasoning with Foundation Models, Jan. 2024. arXiv:2312.11562. [85] [86] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2015. [87] Z. Tang, X. Zhang, B. Wang, and F. Wei. MathScale: Scaling Instruction Tuning for Mathematical Reasoning, Mar. 2024. arXiv:2403.02884. [88] Q. Team. QwQ: Reflect Deeply on the Boundaries of the Unknown. https://qwenlm.github.io/blog/qwq-32b-preview/, Nov. 2024. accessed 2025-01-01. [89] Y. Tian, B. Peng, L. Song, L. Jin, D. Yu, L. Han, H. Mi, and D. Yu. Toward Self-Improvement of LLMs via Imagination, Searching, In Proceedings of the Thirty-eighth Annual Conand Criticizing. ference on Neural Information Processing Systems (NeurIPS 24), volume 37 of Advances in Neural Information Processing Systems, Vancouver, Canada, Dec. 2024. Curran Associates. [90] R. Tu, K. Zhang, B. Bertilson, H. Kjellstrom, and C. Zhang. Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm Evaluation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Proceedings of the Thirty-third Annual Conference on Neural Information Processing Systems (NeurIPS 19), volume 32 of Advances in Neural Information Processing Systems, pages 1279312804, Vancouver, Canada, Dec. 2019. Curran Associates. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, Solving Math Word A. Creswell, G. Irving, and I. Higgins. Problems with Process-and Outcome-Based Feedback, Nov. 2022. arXiv:2211.14275. [91] [93] [92] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Crandall, and D. Batra. Diverse Beam Search for Improved Description of Complex Scenes. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1):73717379, Apr. 2018. J. Wang, M. Fang, Z. Wan, M. Wen, J. Zhu, A. Liu, Z. Gong, Y. Song, L. Chen, L. M. Ni, L. Yang, Y. Wen, and W. Zhang. OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models, Oct. 2024. arXiv:2410.09671. [94] K. Wang, H. Ren, A. Zhou, Z. Lu, S. Luo, W. Shi, R. Zhang, L. Song, M. Zhan, and H. Li. MathCoder: Seamless Code [95] P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-Shepherd: Verify and Reinforce LLMs Stepby-Step without Human Annotations. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 24, pages 94269439, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. [96] X. Wang, L. Song, Y. Tian, D. Yu, B. Peng, H. Mi, F. Huang, and D. Yu. Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning, Oct. 2024. arXiv:2410.06508. [98] [97] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In Proceedings of the Eleventh International Conference on Learning Representations, ICLR 23, Kigali, Rwanda, May 2023. J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou. Chain-of-Thought Prompting In S. Koyejo, Elicits Reasoning in Large Language Models. S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Proceedings of the Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS 22), volume 35 of Advances in Neural Information Processing Systems, pages 24824 24837, New Orleans, LA, USA, Dec. 2022. Curran Associates. [99] Y. Xie, A. Goyal, W. Zheng, M.-Y. Kan, T. P. Lillicrap, K. Kawaguchi, and M. Shieh. Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning, June 2024. arXiv:2405.00451. [100] Y. Yan, J. Su, J. He, F. Fu, X. Zheng, Y. Lyu, K. Wang, S. Wang, Q. Wen, and X. Hu. Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges, Dec. 2024. arXiv:2412.11936. [101] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS 23), volume 36 of Advances in Neural Information Processing Systems, pages 1180911822, New Orleans, LA, USA, Dec. 2023. Curran Associates. [102] L. Yuan, W. Li, H. Chen, G. Cui, N. Ding, K. Zhang, B. Zhou, Z. Liu, and H. Peng. Free Process Rewards without Process Labels, Dec. 2024. arXiv:2412.01981. [103] Z. Zeng, Q. Cheng, Z. Yin, B. Wang, S. Li, Y. Zhou, Q. Guo, X. Huang, and X. Qiu. Scaling of Search and Learning: Roadmap to Reproduce o1 from Reinforcement Learning Perspective, Dec. 2024. arXiv:2412.14135. [104] D. Zhang, X. Huang, D. Zhou, Y. Li, and W. Ouyang. Accessing GPT-4 Level Mathematical Olympiad Solutions via Monte Carlo Tree Self-Refine with LLaMa-3 8B, June 2024. arXiv:2406.07394. [105] D. Zhang, J. Wu, J. Lei, T. Che, J. Li, T. Xie, X. Huang, S. Zhang, M. Pavone, Y. Li, W. Ouyang, and D. Zhou. LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning, Nov. 2024. arXiv:2410.02884. [106] D. Zhang, S. Zhoubian, Z. Hu, Y. Yue, Y. Dong, and J. Tang. ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree In Proceedings of the Thirty-eighth Annual Conference on Search. Neural Information Processing Systems (NeurIPS 24), volume 37 of Advances in Neural Information Processing Systems, Vancouver, Canada, Dec. 2024. Curran Associates. [107] L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal. Generative Verifiers: Reward Modeling as NextToken Prediction, Oct. 2024. arXiv:2408.15240. [108] Y. Zhao, H. Yin, B. Zeng, H. Wang, T. Shi, C. Lyu, L. Wang, W. Luo, and K. Zhang. Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions, Nov. 2024. arXiv:2411.14405. [109] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS 23), volume 36 of Advances in Neural Information Processing Systems, pages 4659546623, New Orleans, LA, USA, Dec. 2023. Curran Associates. [110] D.-H. Zhu, Y.-J. Xiong, J.-C. Zhang, X.-J. Xie, and C.-M. Xia. Understanding Before Reasoning: Enhancing Chain-ofThought with Iterative Summarization Pre-Prompting, Jan. 2025. arXiv:2501.04341."
        }
    ],
    "affiliations": [
        "BASF SE",
        "Cledar",
        "Cyfronet AGH",
        "ETH Zurich"
    ]
}