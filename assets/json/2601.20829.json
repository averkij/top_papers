{
    "paper_title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning",
    "authors": [
        "Minwu Kim",
        "Safal Shrestha",
        "Keith Ross"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems."
        },
        {
            "title": "Start",
            "content": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning Minwu Kim 1 Safal Shrestha 1 Keith Ross"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failureprone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the models robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.1. 6 2 0 2 8 2 ] . [ 1 9 2 8 0 2 . 1 0 6 2 : r 1. Introduction Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning capabilities of large language models (LLMs) (Lambert et al., 2025; Guo 1New York University Abu Dhabi. Correspondence to: Minwu Kim <mwk300@nyu.edu>. Preprint. January 29, 2026. 1Code available at https://github.com/minwukim/ training-on-saturated-problems 1 Figure 1. Illustration of standard GRPO training and failure-prefix conditioning on saturated problems. While standard GRPO predominantly generates correct rollouts, failure-prefix conditioning exposes the model to failure-prone reasoning states, making informative failures more accessible. et al., 2025; OpenAI, 2024). However, as models improve, an increasing number of training problems become saturated, meaning the model solves them correctly in nearly all rollouts. On these problems, training often stalls because rewards become nearly deterministic, leaving little learning signals. Importantly, saturation does not imply that the learning signal is exhausted. Even on problems with near-perfect rollout accuracy, incorrect reasoning trajectories still exist, albeit extremely rarely under standard sampling. As established in prior work, such informative failures play crucial role in training (Setlur et al., 2025; Zhu et al., 2025b). However, most of the sampling budget is spent generating redundant correct solutions, with failures becoming exceedingly sparse (Wang et al., 2025). This suggests that the core challenge in learning from satTraining Reasoning Models on Saturated Problems via Failure-Prefix Conditioning urated problems is not the absence of informative failures, but their poor accessibility. If failures could be encountered more frequently, RLVR could continue to make progress even on saturated problems. To this end, we propose failure-prefix conditioning, simple and effective method for learning from saturated problems in RLVR. As shown in Figure 1, instead of sampling from the original question, this method explicitly targets failureprone reasoning states. We identify rare incorrect rollouts from saturated questions, slice them into prefixes, and select the specific prefix lengths that maximize the learning signal (i.e., target accuracy τ = 0.5) (Li et al., 2025). Training on the resulting failure-prefix-conditioned dataset reallocates exploration toward failure-prone regions of the response space, enabling effective learning from saturated problems. We validate the effectiveness of this approach through an in-depth experimental study. Using DeepSeek-R1-DistillQwen-1.5B (Guo et al., 2025) as our base model, we identify 1,000 saturated math questions where the model achieves rollout accuracy 31/32 ( 97%). We apply failure-prefix conditioning to these questions and train our failure-prefix model using RLVR on the resulting prefix-conditioned dataset. We compare our approach against two baselines: (i) the saturate model, trained via standard RLVR on the same 1,000 saturated questions without prefix conditioning, and (ii) the medium model, trained with standard RLVR on 1,000 medium-difficulty questions with rollout accuracy close to 50%, the regime commonly regarded as maximizes the learning signal (Zhan et al., 2025; Li et al., 2025). We evaluate these models on various math reasoning benchmarks spanning wide range of difficulty. The saturate model shows negligible improvement relative to the base model, confirming that standard RLVR training stalls on saturated questions. In contrast, training on the failure-prefixconditioned dataset yields consistent and substantial gains over the base model across all benchmarks. This performance is on par with that of the medium model, demonstrating that failure-prefix conditioning can effectively recover learning signal from saturated problems. Notably, these improvements are achieved without inflating response length, indicating that our methodology preserves token efficiency. We also demonstrate that our approach is robust to the target accuracy hyperparameter τ ; while our default τ = 0.5 yields the best performance, ablation studies confirm that the method achieves comparable peak performance across range of values. Next, we analyze why failure-prefix conditioning is effective. While standard RLVR primarily incentivizes better decision-making from the initial state, failure-prefix conditioning explicitly trains the model to recover from incorrect intermediate reasoning states. Consequently, the method promotes robustness to misleading partial trajectories early in the reasoning process. Empirically, we find that the failure-prefix model exhibits greater robustness to failure prefixes, showing smaller drop in rollout accuracy compared to baseline methods. Additionally, we also observe mild trade-off, where improved robustness to failure prefixes can occasionally reduce adherence to correct intermediate reasoning. Finally, we explore whether learning from saturated problems can be further extended by iteratively refreshing failure prefixes as the model improves. As training progresses, performance under fixed failure-prefix-conditioned dataset eventually plateaus, suggesting that previously informative prefixes may become less effective as the policy updates. Motivated by this observation, we perform second iteration of failure-prefix conditioning, resampling new failures from the updated model and reconstructing the prefix-conditioned dataset. Empirically, this iterative procedure yields additional gains beyond the initial plateau, suggesting that periodically updating failure prefixes may further recover additional learning signal and enable continued improvement. We summarize our contributions as follows: We propose failure-prefix conditioning, method that reallocates exploration toward failure-prone reasoning states to enable effective RLVR training on saturated problems. We show that failure-prefix conditioning enhances robustness to incorrect intermediate reasoning trajectories, enabling better recovery from misleading partial solutions compared to standard RLVR baselines. We demonstrate that iteratively refreshing failure prefixes can yield additional gains after performance plateaus, offering potential pathway for further extending learning on saturated problems. Overall, our results indicate that failure-prefix conditioning provides an effective and efficient strategy to unlock the value of saturated training data. 2. Related Work Task Difficulty for RL training of LLMs. Prior work has theoretically shown that policy updates in RL training of LLMs are biased by task difficulty, with the learning signal maximized when task success rates lie at an intermediate level (Razin et al., 2024; Li et al., 2025). Zhan et al. (2025) further empirically validates this observation by stratifying training tasks by difficulty and showing that performance improvements peak on moderate-difficulty tasks. Building on this insight, several approaches regulate task difficulty during RL training. DAPO dynamically filters prompts with extremely low or high rollout accuracy that contribute little 2 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning to parameter updates (Yu et al., 2025). Some approaches arrange static training tasks in increasing order of difficulty to improve training efficiency (Du et al., 2025b; Shi et al., 2025; Chen et al., 2025). Instead of reordering fixed task set, RLVE introduces adaptive training environment that directly controls task difficulty relative to the model (Zeng et al., 2025). Self-play approaches have also been explored, where LLMs autonomously propose tasks at an appropriate difficulty level (Zhao et al., 2025; Liu et al., 2025). Our work shares the goal of maintaining effective task difficulty; however, rather than filtering or reordering tasks, we focus on saturated problems and recover their utility focusing on failure-prone states by conditioning on partial incorrect trajectories. Curriculum learning and context-enhanced learning for RL training of LLMs. Curriculum learning has been widely studied as way to regulate task difficulty and improve training efficiency (Baker et al., 2020; Portelas et al., 2020; Jiang et al., 2020), and has recently been extended to RL training for large language models (Gao et al., 2025; Chen et al., 2025). In the context of LLMs, many such approaches operate by augmenting the input context with additional information that facilitates reasoning during training (Xi et al., 2024). BREAD and POPE provide partial ground-truth hints in the context for difficult problems where the base model fails to produce successful rollouts, enabling the model to generate correct trajectories during training and yielding improvements that transfer effectively to settings without such hints (Zhang et al., 2025; Qu et al., 2025). Similarly, Zhu et al. (2025a) inject additional information into the contextexcluded from autoregressive gradient computationto achieve exponentially improved sample efficiency on multi-step reasoning tasks. Our approach also modifies the input context during RL training, but essentially inverts this paradigm: whereas prior methods inject correct information to assist with difficult problems, we condition on incorrect trajectories to increase the difficulty of easy (saturated) problems. This exposes failure-prone states and allows the recovery of learning signals that are otherwise poorly accessible. Scaling RL training for performance boosts. Scaling RL training has repeatedly been shown to improve the reasoning performance of LLMs (Ettinger et al., 2025; Guo et al., 2025). ProRL demonstrates that substantially increasing the number of gradient updates can improve both pass@1 and pass@k performance (Du et al., 2025a). BroRL further extends this line of work by dramatically increasing per-problem exploration, sampling hundreds of rollouts per prompt to surpass the saturation point observed in ProRL and achieve additional gains (Hu et al., 2025). Relatedly, Wang et al. (2025) show that intensive training on single problem can extract sufficient learning signal to match the performance gains obtained from training on datasets with over thousand examples. Taken together, these results indicate that scaling RL training through increased gradient steps or exploration can drive further performance gains. However, it incurs prohibitive computational costs and memory overhead, with the vast majority of the sampling budget is consumed by generating redundant correct solutions during training. In contrast, our approach extracts learning signals from saturated problems efficiently, reallocating exploration to failure-prone states rather than simply increasing the sampling budget. 3. RLVR and Saturated Problems In this section, we explain why training with GRPO (Shao et al., 2024), standard RLVR algorithm, stalls on saturated problems. Section 3.1 reviews the training procedure and notation, and Section 3.2 analyzes why saturation leads to stalled learning. 3.1. GRPO Training Let = {(q, a)} denote dataset of questions with verifiable ground-truth answers a. Given policy πθ, we generate independent rollouts yi πθ( q), = 1, . . . , N, and assign each rollout binary reward ri = r(yi; q) {0, 1}, where ri = 1 if yi is correct solution and ri = 0 otherwise. For each rollout in {yi}N i=1, the advantage is given by Ai = ri µr σr + ϵ , where µr = 1 i=1 ri, σr = std({ri}N i=1), and ϵ > 0 is small constant that prevents division by zero when σr = 0. (cid:80)N The policy πθ is then updated by minimizing the following loss, where we omit the clipping and KL-regularization terms for clarity: LGRPO(θ) = E(q,a)D (cid:34) 1 (cid:88) i=1 (cid:35) Ai log πθ(yi q) . 3.2. Training with Saturated Problems We say that problem is saturated under policy πθ if the rollout accuracy below is close to 1. pθ(q) = Pr [r(y; q) = 1] yπθ(q) For such problem, with high probability all sampled rollouts satisfy ri = 1, yielding µr = 1, σr = 0, 3 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning and consequently Ai = 0 for all i. As result, the policy gradient vanishes and training stalls. Even when rare incorrect rollout occurs, reward variance remains small. For binary rewards, std[r] = (cid:112)pθ(q)(1 pθ(q)), which approaches zero as pθ(q) 1. Since the policy gradient magnitude scales with this standard deviation value (Li et al., 2025) (see Appendix A.5 for full derivation), learning signals on saturated problems are weak. This analysis is also empirically supported by the findings of Zhan et al. (2025), who observe substantially smaller gains from training on easy questions (pθ(q) [0.75, 1.00)) compared to moderately difficult ones (pθ(q) (0.25, 0.75]). 4. Methodology We propose failure-prefix conditioning, simple and effective method for learning from saturated problems in RLVR. The core idea is to reshape exploration by conditioning training on failure-prone reasoning states, instead of initiating exploration from the original question. Section 4.1 provides the motivation for our approach, and Section 4.2 presents the proposed methodology. 4.1. Motivation As analyzed in 3.2, learning stalls on saturated problems not because the model is perfect, but because its failures are rare and difficult to access under standard RLVR. Even when rollout accuracy is close to 1, incorrect responses still exist in the models response space. However, during training, these failure-prone states are encountered too rarely to provide meaningful learning signal. In other words, the fundamental limitation is not the absence of informative failures, but their poor accessibility. To address this, rather than repeatedly sampling from the question as in the standard setting, the solution we suggest is to target exploration toward regions of the response space where uncertainty is higher and failures occur more frequently. Concretely, we propose failure-prefix conditioning: condition training on prefixes obtained from rare incorrect responses, enabling RLVR to begin exploration directly from failure-prone states. This simple change dramatically improves accessibility. Consider saturated problem with rollout accuracy 0.97. Under standard sampling, failures occur with probability 1 0.97 = 0.03, meaning only small number of incorrect responses are expected, and most rollouts are redundant successes that contribute little learning signal. If, instead, training begins from failure-prone state where, say, 0.50, the same sampling budget yields substantially more incorrect responses (1 0.50 = 0.50). As Algorithm 1 Failure-Prefix Conditioning Require: Saturated dataset Dsaturated = {(q, a)}, policy πθ, target accuracy τ = 0.5, number of rollouts Ensure: Prefix-conditioned dataset 1: Initialize 2: for each (q, a) Dsaturated do 3: 4: 5: Obtain an incorrect rollout πθ( q) Let Construct prefix set S(q) = {y1:αk }K k=1, 6: 7: 8: for each prefix S(q) do Sample rollouts {yi}N Estimate rollout accuracy i=1 πθ( s) pθ(q, s)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 I[r(yi; q) = 1] 9: 10: end for Select prefix s(q) arg min sS(q) pθ(q, s) τ Add (q s(q), a) to 11: 12: end for result, informative failures become far more accessible. 4.2. Methodology: Failure-Prefix Conditioning Let πθ be fixed policy and Dsaturated be set of saturated questions, defined as questions whose rollout accuracy pθ(q) is close to 1 under standard sampling, as in Section 3.2. For each such question q, we obtain at least one incorrect rollout produced by πθ. From each incorrect rollout y, we construct set of prefixes S(q) = {y1:αk }K k=1, where y1:αk denotes the first αk tokens of y. In our experiments in Section 5, we sweep prefix lengths corresponding to 10%, 20%, . . . , 90% of the total trajectory length, yielding = 9 candidate prefixes per question. Each prefix S(q) defines prefix-conditioned prompt s, from which we estimate the prefix-conditioned rollout accuracy pθ(q, s) = Pr yπθ(qs) [r(y; q) = 1] using fixed number of rollouts. Intuitively, when generation starts from the original question q, the rollout accuracy is near one for saturated problems. As progressively longer 4 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning Model MATH500 AMC12 AIME24 AIME25 HMMT25 Average Base Saturate Medium Failure-Prefix (Ours) 83.5 84.2 (+0.7) 85.0 (+1.5) 85.7 (+2.2) 53.1 51.7 (-1.4) 57.8 (+4.7) 56.3 (+3.2) 28.4 30.0 (+1.6) 32.0 (+3.6) 33.0 (+4.6) 24.0 24.0 (+0.0) 26.2 (+2.2) 25.9 (+1.9) 14.2 13.5 (-0.6) 15.0 (+0.8) 16.2 (+2.0) 40.6 40.7 (+0.1) 43.2 (+2.6) 43.4 (+2.8) Table 1. Performance comparison on math reasoning benchmarks. Numbers in parentheses indicate absolute improvements over the base model. Best results are shown in bold, and second-best results are underlined. Figure 2. Performance comparison across models. Left: Mean Pass@k values. Middle: Average token count of responses across benchmarks. Right: Mean accuracy under different inference token limits. Failure-prefix conditioning consistently improves performance while maintaining token efficiency. prefixes of an incorrect trajectory are appended, the conditioning context increasingly constrains the model toward failure states, and the rollout accuracy tends to decay towards 0 (Wen et al., 2025). While this decay is not necessarily monotonic, sweeping over prefix lengths reliably yields intermediate prefixes whose rollout accuracy lies between these extremes. We select single prefix s(q) whose rollout accuracy is closest to target value τ (0, 1): s(q) = arg min sS(q) pθ(q, s) τ . In our main experiments, we set τ = 0.5, where binary rewards exhibit the highest variance and GRPO provides the strongest learning signal (Li et al., 2025). Using the selected prefixes, we construct new training set = {(q s(q), a) Dsaturated}. GRPO training is then performed on following the standard procedure described in Section 3.1. 5. Experiment 5.1. Experiment Settings Model and Dataset We assume setting where all the available training questions are saturated for the model. We use the DeepSeek-R1-Distill-Qwen-1.5B (Guo et al., 2025) for the reasoning model. We identify saturated question sets from the MATH training set (7.5k questions) (Hendrycks et al., 2021) and the DeepScaleR dataset (40.3k questions) (Agentica-org, 2025). For each question, we generate 32 rollouts and randomly select 1,000 questions with rollout accuracy 31/32 ( 0.97), which we treat as saturated. For each selected question, we retain the single incorrect response and apply failure-prefix conditioning following Algorithm 1 with target accuracy τ = 0.5, where reward variance is maximized. We also check τ = 0.25 and τ = 0.75 for ablation studies. Additional details for the inference are provided in Appendix A.3. Baseline We train our model using GRPO on the failureprefix-conditioned dataset. We compare against three baselines: (1) the base model without further training; (2) model trained with standard GRPO on the same saturated questions, without prefix conditioning; and (3) model trained with standard GRPO on 1,000 randomly selected questions with rollout accuracy 16/32 (0.5), where learning signal is expected to be maximized. For convenience, we refer to these four models as base, saturate, medium, and failure-prefix models, respectively. All models are trained using the same GRPO configuration, with 16 rollouts per question, representing standard and non-aggressive exploration setting. Implementation details for training are provided in Appendix A.1. 5 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning Evaluation We evaluate all models on five math reasoning benchmarks spanning wide range of difficulties (from easiest to hardest): MATH500 (Hendrycks et al., 2021), AMC12 (AI-MO, 2025), AIME24 (math-ai, 2025a), AIME25 (math-ai, 2025b), and HMMT25 (MathArena, 2025). For each question, we generate 32 samples and report the mean accuracy (pass@1) to ensure statistical robustness. Additionally, to determine if the method is merely sharpening the distribution (Yue et al., 2025; Kim et al., 2025) or genuinely improving capability (Du et al., 2025a; Setlur et al., 2025), we report pass@k metrics up to pass@32 using the 32 responses. All evaluations are performed with maximum inference token limit of 32000. Full details of the evaluation setup are provided in Appendix A.2. 5.2. Main Results Failure-prefix conditioning improves model performance. We observe that training on the failure-prefixconditioned dataset consistently improves performance across all math benchmarks. As shown in Table 1, compared to the base model, the failure-prefix model achieves accuracy gains ranging from the easiest benchmark (MATH500) to the most challenging one (HMMT25). Averaged across the five benchmarks, it reaches an accuracy of 43.4%, representing +2.8 absolute point improvement over the base model (40.6%). This performance is on par with that of the model trained on medium-difficulty questions (43.2%). In contrast, training with standard GRPO on saturated questions without prefix conditioning stalls with no meaningful improvement, achieving an average accuracy of 40.7%, essentially identical to the base model. Beyond pass@1 accuracy, similar trends hold for pass@k. As shown in Figure 2 (left), for all values of to 32, the failure-prefix model, with the medium model, consistently outperforms the base model. This indicates that the gains are not merely due to sharpening, but reflect broader improvements in solution quality and diversity. In contrast, the saturate model shows no noticeable gains across the entire range of k. Together, these results indicate that failure-prefix conditioning enables effective learning progress on saturated problems that standard GRPO training fails to achieve. Failure-prefix conditioning maintains token efficiency. For each model, we compute the mean number of generated tokens per response across all benchmarks. As shown in Figure 2 (middle), the failure-prefix model produces responses of comparable length to the base model across benchmarks. While the saturated-trained and medium-trained models tend to generate slightly shorter responses, the differences are modest. Importantly, as shown in Figure 2 (right), the failure-prefix model, with the medium model. consistently outperforms the base and saturate models across wide range of inference token limits, from 8,000 to 32,000 tokens. These results indicate that failure-prefix conditioning Figure 3. Ablation study on target accuracy τ . We plot the mean accuracy across benchmarks for τ {0.25, 0.50, 0.75} over 800 gradient steps. Peak performance points are highlighted for each setting. improves performance without increasing inference-time token usage. 5.3. Ablation Study: Sensitivity to Target Accuracy τ As described in Section 5.1, we set the target accuracy at τ = 0.5 for failure-prefix conditioning. This is because, as detailed in Section 4.2, this value theoretically maximizes reward variance, thereby amplifying the gradient signal. To investigate the sensitivity of our method to this hyperparameter, we repeat the training process using τ = 0.25 and τ = 0.75, keeping all other settings identical. We observe that while τ = 0.5 yields the best results, the method remains effective across different target values. As shown in Figure 3, the model trained with τ = 0.75 plateaus at an average accuracy of 43.1, slightly below the 43.4 achieved by τ = 0.5. The model trained with τ = 0.25 achieves comparable accuracy of 43.3; however, it exhibits slightly slower improvement, requiring 500 gradient steps to reach this peak compared to 400 steps for the τ = 0.5 model. These empirical findings align with our theoretical analysis that τ = 0.5 is optimal, while simultaneously demonstrating that failure-prefix conditioning is robust to variations in the target accuracy parameter. 6. Why Failure-Prefix Conditioning Works Since failure-prefix conditioning does not train directly on the original questions, it may appear to introduce traintest mismatch. As result, the source of the performance improvements observed in Section 5.2 is not immediately obvious. In this section, we provide an interpretation that explains why our method is nevertheless effective. 6 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning 6.1. MDP Model We model autoregressive generation as Markov Decision Process (MDP), where each state corresponds to partial reasoning trajectory followed by question. Specifically, let state st represent the concatenation of the question and the first generated tokens, and let actions correspond to generating the next token. complete response corresponds to trajectory (s0, a0, s1, . . . , sT ), where s0 = is the initial state. In standard RLVR training, rollouts are always initialized from s0. Failure-prefix conditioning alters this training distribution by explicitly initializing rollouts from intermediate states along incorrect trajectories. Concretely, instead of always starting from s0, training rollouts are initialized from states of the form st = (q, y1:t), where y1:t is prefix of previously observed rare incorrect response. These states correspond to regions of the state space where the policy exhibits higher uncertainty and where recovery from misleading reasoning is required to obtain reward. From an MDP perspective, failure-prefix conditioning does not introduce traintest mismatch. Assuming the prefix is not excessively off-policy relative to the models action distribution (Agarwal et al., 2025), inference from the initial state s0 naturally traverses intermediate states that may be misleading, and that are similar to those encountered during training under failure-prefix conditioning. Under this interpretation, failure-prefix conditioning improves model performance through mechanism distinct from standard RLVR training. While standard RLVR primarily incentivizes the model to improve decision-making from the initial state s0, failure-prefix conditioning explicitly trains the model to recover from early incorrect reasoning trajectories. As result, the method targets robustness to misleading partial reasoning rather than solely improving performance from the beginning of the trajectory. In the following subsection, we empirically evaluate this interpretation by examining whether failure-prefix conditioning improves robustness to misleading early reasoning. 6.2. Recovery from Failure Prefixes Recall that, in Section 5.2, for each MATH500 question evaluated we generate 32 responses from each of the four models. We select the subset of questions for which all four models produce at least one correct and one incorrect response, resulting in 176 questions. This ensures that all the models have the capability to recover from misleading reasoning for all the questions. Then, for each model and each selected question, we randomly sample one incorrect response and construct prefixes corresponding to 10%, 20%, . . . , 90% of the response length. Conditioning on each prefix, we generate 32 continued rollouts and compute the reFigure 4. Rollout accuracy versus prefix length (% of trajectory) when conditioning on correct and incorrect prefixes. sulting rollout accuracy (inference details in Appendix A.4). We group and average rollout accuracies by the percentage values and analyze how accuracy degrades as the prefix extends further into an incorrect reasoning trajectory. This measures the models ability to recover from increasingly misleading early reasoning. As shown in Figure 4, all four models consistently exhibit declining rollout accuracy as the failure prefix extends. This is consistent with prior observation that LLMs suffer from tunnel vision, where misleading early reasoning steps increasingly constrain subsequent generation and lock the model into suboptimal reasoning paths (Wen et al., 2025). However, the failure-prefix model exhibits substantially smaller drop in accuracy compared to the other three models. For example, at 30% failure prefix, the failure-prefix model shows an 11.5-point drop in rollout accuracy (65.2 to 53.7), whereas the base model drops by 23.8 points (59.1 to 35.5), the saturated-trained model by 22.2 points (60.9 to 38.7), and the medium-trained model by 22.5 points (63.3 to 40.8). This gap in degradation persists consistently across the entire range of prefix lengths. These results suggest its stronger robustness to misleading prefixes. Notably, this pattern is not observed not only for the base model and the saturate model, which show little overall performance improvement, but also for the medium model, whose aggregate performance is on par with that of the failure-prefix model. This implies that improved robustness to failure prefixes is specific to failure-prefix conditioning, rather than byproduct of overall performance gains alone. 7 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning saturated data, we conduct second iteration of failureprefix conditioning. Starting from the checkpoint at step 400, which achieves the best performance, we resample responses for each of the 1,000 saturated questions identified in Section 5.1 until an incorrect response is observed, with maximum of 128 attempts per question. We exclude 440 questions for which the model attains perfect rollout accuracy (128/128). For the remaining questions, we reapply Algorithm 1 to construct an updated prefix-conditioned dataset using the newly observed failures. During prefix selection, we extend the sweep to include the 0% prefix (i.e., the original question), as small subset of questions exhibits considerable drop in rollout accuracy. The detailed implementation for the new dataset is provided in Appendix A.3. We then continue GRPO training for an additional 400 gradient steps on this refreshed dataset. We find that this second iteration yields additional performance gains. As shown in Figure 5 (right), prolonged training in iteration 1 fails to surpass the peak accuracy of 43.4 achieved at step 400, whereas iteration 2 improves performance to maximum of 44.0, corresponding to gain of 0.6 points over the original checkpoint. These results point to the possibility that iteratively refreshing failure prefixes may help recover additional learning signal and enable continued improvement. 8. Conclusion In this paper, we introduce failure-prefix conditioning, simple and effective method that extends RLVR training on saturated problems where standard methods stall. By explicitly initializing training from rare incorrect partial trajectories, our approach recovers critical learning signals, yielding performance gains comparable to training on mediumdifficulty tasks while enhancing robustness to misleading reasoning. Moreover, we demonstrate that iteratively refreshing these prefixes enables sustained improvement beyond initial plateaus. Ultimately, our work establishes that saturated problems remain valuable resource for training reasoning models, provided exploration is directed toward their sparse but informative failure modes."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Anubhav Shrestha and Aadim Nepal for their valuable feedback. This research is supported by the Center for AI and Robotics (CAIR) at New York University Abu Dhabi. Figure 5. Effect of iterative failure-prefix conditioning on training dynamics. Left: Training reward curves for prolonged iteration 1 (steps 0800) and iteration 2 that forks at step 400 and proceeds with updated failure prefixes through step 800. Right: Mean accuracy across benchmarks for both iterations measured from steps 400800, with peak performance point for each model highlighted. Additionally, we examine whether failure-prefix conditioning induces excessive backtracking that causes the model to deviate from correct reasoning when given an initially correct partial solution, which is an undesired behavior. To assess this, we also perform an identical experiment using prefixes sampled from correct response. Figure 4 shows that the failure-prefix model exhibits slightly weaker improvement when conditioned on success prefixes compared to the other models, indicating modest tendency to deviate from correct reasoning paths. For example, at 30% prefix length, the failure-prefix model improves by 5.0 points (from 65.2 to 70.2), whereas the base model improves by 8.4 points (59.1 to 67.5), the saturate model by 9.3 points (60.9 to 70.2), and the medium model by 10.0 points (63.3 to 73.4). This pattern persists across the entire range of prefix lengths. Overall, this experiment shows mild trade-off: while failure-prefix conditioning substantially improves robustness to misleading early reasoning, it slightly reduces the benefit obtained from correct prefixes. Addressing this behavior may require further investigation in future work. Importantly, this effect is limited in magnitude and does not outweigh the robustness gains achieved under failure-prefix conditioning, as reflected in the overall performance improvements reported in Section 5.2. 7. Iterative Failure-Prefix Conditioning We observe that as training with failure-prefix conditioning progresses, performance eventually plateaus, with the mean reward flattening (Figure 5, left). This behavior may arise from diminishing learning signal in the conditioned dataset, or from fixed failure prefixes becoming increasingly offpolicy as the model improves. To assess whether additional gains can be obtained from 8 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning"
        },
        {
            "title": "References",
            "content": "Agarwal, S., Zhang, Z., Yuan, L., Han, J., and Peng, H. The unreasonable effectiveness of entropy minimization in LLM reasoning. In Proceedings of the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025), 2025. URL https://openreview .net/forum?id=UfFTBEsLgI. Agentica-org. Deepscaler-preview-dataset. https://hu ggingface.co/datasets/agentica-org/D eepScaleR-Preview-Dataset, 2025. Hugging Face dataset. AI-MO. Amc12 2022, 2023 dataset. https://huggin gface.co/datasets/AI-MO/aimo-validat ion-amc, 2025. Hugging Face dataset. Baker, B., Kanitscheider, I., Markov, T. M., Wu, Y., Powell, G., McGrew, B., and Mordatch, I. Emergent tool use from multi-agent autocurricula. In Proceedings of the International Conference on Learning Representations (ICLR 2020), 2020. Chen, X., Lu, J., Kim, M., Zhang, D., Tang, J., Piche, A., Gontier, N., Bengio, Y., and Kamalloo, E. Self-evolving curriculum for llm reasoning. arXiv:2505.14970, 2025. Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., and ... Prorl: Prolonged reinforcement learning expands reasoning strategies. In Proceedings of the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025), 2025a. arXiv:2505.24864. Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G., and ... Kimi k1.5: Scaling reinforcement learning with llms. arXiv:2501.12599, 2025b. Ettinger, A., Bertsch, A., Kuehl, B., Graham, D., Heineman, D., Groeneveld, D., Brahman, F., Timbers, F., Ivison, H., Morrison, J., Poznanski, J., Lo, K., Soldaini, L., Jordan, M., Chen, M., Noukhovitch, M., Lambert, N., Walsh, P., Dasigi, P., Berry, R., Malik, S., Shah, S., Geng, S., Arora, S., Gupta, S., Anderson, T., Xiao, T., Murray, T., Romero, T., Graf, V., Asai, A., Bhagia, A., Wettig, A., Liu, A., Rangapur, A., Anastasiades, C., Huang, C., Schwenk, D., Trivedi, H., Magnusson, I., Lochner, J., Liu, J., Miranda, L. J. V., Sap, M., Morgan, M., Schmitz, M., Guerquin, M., Wilson, M., Huff, R., Le Bras, R., Xin, R., Shao, R., Skjonsberg, S., Shen, S. Z., Li, S. S., Wilde, T., Pyatkin, V., Merrill, W., Chang, Y., Gu, Y., Zeng, Z., Sabharwal, A., Zettlemoyer, L., Koh, P. W., Farhadi, A., Smith, N. A., and Hajishirzi, H. Olmo 3. arXiv:2512.13961, 2025. Gao, Z., Kim, J., Sun, W., Joachims, T., Wang, S., Pang, R. Y., and Tan, L. Prompt curriculum learning for efficient llm post-training. arXiv:2510.01135, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., and ... Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025. doi: 10.1038/s41586-025-09422-z. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Proceedings of the Thirty-Fifth Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks, 2021. Hu, J., Liu, M., Lu, X., Wu, F., Harchaoui, Z., Diao, S., Choi, Y., Molchanov, P., Yang, J., Kautz, J., and Dong, Y. Brorl: Scaling reinforcement learning via broadened exploration. arXiv:2510.01180, 2025. Jiang, M., Grefenstette, E., and Rocktaschel, T. Prioritized level replay. In Proceedings of the 37th International Conference on Machine Learning (ICML 2020), 2020. Kim, M., Shrestha, A., Shrestha, S., Nepal, A., and Ross, K. Reinforcement learning vs. distillation: Understanding accuracy and capability in llm reasoning. arXiv:2505.14216, 2025. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 23), pp. 611626, Koblenz, Germany, 2023. Association for Computing Machinery. doi: 10.1145/3600006.3613165. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Le Bras, R., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training. In Proceedings of the Conference on Language Models (COLM 2025), 2025. Li, G., Lin, M. C., Galanti, T., Tu, Z., and Yang, T. Disco: Reinforcing large reasoning models with discriminative constrained optimization. In Proceedings of the ThirtyNinth Conference on Neural Information Processing Systems, 2025. arXiv:2505.12366. Liu, B., Guertler, L., Yu, S., Liu, Z., Qi, P., Balcells, D., Liu, M., Tan, C., Shi, W., Lin, M., Lee, W. S., and Jaques, 9 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning N. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learnIn Proceedings of the International Conference ing. on Learning Representations 2026 (ICLR 2026), 2025. arXiv:2506.24119. math-ai. Aime24 dataset. https://huggingface.co /datasets/math-ai/aime24, 2025a. Hugging Face dataset. math-ai. Aime25 dataset. https://huggingface.co /datasets/math-ai/aime25, 2025b. Hugging Face dataset. MathArena. Hmmt25 dataset (february 2025). https: //huggingface.co/datasets/MathArena/ hmmt_feb_2025, 2025. Hugging Face dataset. OpenAI. Openai o1 system card, 2024. arXiv:2412.16720. Portelas, R., Colas, C., Weng, L., Hofmann, K., and Oudeyer, P.-Y. Automatic curriculum learning for deep reinforcement learning: short survey. In Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI 2020), 2020. Qu, Y., Setlur, A., Smith, V., Salakhutdinov, R., and Kumar, A. How to explore to scale rl training of llms on hard problems? https://blog.ml.cmu.edu/2025/1 1/26/how-to-explore-to-scale-rl-train ing-of-llms-on-hard-problems, 2025. CMU MLD Blog. Razin, N., Zhou, H., Saremi, O., Thilak, V., Bradley, A., Nakkiran, P., Susskind, J. M., and Littwin, E. Vanishing gradients in reinforcement finetuning of language models. In International Conference on Learning Representations (ICLR 2024), 2024. Setlur, A., Yang, M. Y. R., Snell, C. V., Greer, J., Wu, I., Smith, V., Simchowitz, M., and Kumar, A. e3: Learning to explore enables extrapolation of test-time compute for llms. arXiv:2506.09026, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. arXiv:2402.03300. Shi, T., Wu, Y., Song, L., Zhou, T., and Zhao, J. Efficient reinforcement finetuning via adaptive curriculum learning. arXiv:2504.05520, 2025. Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du, S. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training In Proceedings of the Thirty-Ninth Conferexample. ence on Neural Information Processing Systems, 2025. arXiv:2504.20571. Wen, H., Su, Y., Zhang, F., Liu, Y., Liu, Y., Zhang, Y.- Q., and Li, Y. Parathinker: Native parallel thinking as new paradigm to scale llm test-time compute. 2025. arXiv:2510.02245. Xi, Z., Chen, W., Hong, B., Jin, S., Zheng, R., He, W., Ding, Y., Liu, S., Guo, X., Wang, J., Guo, H., Shen, W., Fan, X., Zhou, Y., Dou, S., Wang, X., Zhang, X., Sun, P., Gui, T., Zhang, Q., and Huang, X. Training large language models for reasoning through reverse curriculum reinforcement learning. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), pp. 5403054048, 2024. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.-Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. Dapo: An open-source llm reinforcement learning system at scale. In Proceedings of the Thirty-Ninth Conference on Neural Information Processing Systems, 2025. arXiv:2503.14476. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? 2025. arXiv:2504.13837. Zeng, Z., Ivison, H., Wang, Y., Yuan, L., Li, S. S., Ye, Z., Li, S., He, J., Zhou, R., Chen, T., Zhao, C., Tsvetkov, Y., Du, S. S., Jaques, N., Peng, H., Koh, P. W., and Hajishirzi, H. Rlve: Scaling up reinforcement learning for language models with adaptive verifiable environments. arXiv:2511.07317, 2025. Zhan, R., Li, Y., Wang, Z., Qu, X., Liu, D., Shao, J., Wong, D. F., and Cheng, Y. Exgrpo: Learning to reason from experience. 2025. arXiv:2510.02245. Zhang, X., Huang, Z., Li, Y., Ni, C., Chen, J., and Oymak, S. Bread: Branched rollouts from expert anchors bridge sft & rl for reasoning. In Proceedings of the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025), 2025. arXiv:2506.17211. Zhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Lin, M., Wang, S., Wu, Q., Zheng, Z., and Huang, G. Absolute zero: Reinforced self-play reasoning with zero data. In Proceedings of the Thirty-Ninth Conference on Neural Information Processing Systems, 2025. arXiv:2505.03335. Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning Zhu, X., Panigrahi, A., and Arora, S. On the power of In Proceedings of context-enhanced learning in llms. the 42nd International Conference on Machine Learning (ICML 2025), 2025a. arXiv:2503.01821. Zhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The surprising effectiveness of negative reinforcement In Proceedings of the Thirty-Ninth in llm reasoning. Conference on Neural Information Processing Systems, 2025b. arXiv:2506.01347. 11 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning A. Appendix A.1. GRPO Training Details Library For GRPO training in this work, we use the GRPOTrainer from the TRL2 library Prompt Setting We use the Qwen-Math chat template for all GRPO training and evaluation. When applying failure-prefix conditioning, the prefix is inserted immediately after the <think> token, as shown below. Qwen-Math template <im_start>system Please reason step by step, and put your final answer within boxed{}. <im_end> <im_start>user {question} <im_end> <im_start>assistant <think>{prefix} Reward Function We adopt minimalistic reward setting. response received reward of 1 if it contained the correct final answer, and 0 otherwise. Answer verification is performed using the math verify3 package. (cid:40) R(q, a, y) = if the response to question matches the ground truth answer 1 0 otherwise Clip-Higher Following DAPO (Yu et al., 2025), we adopt the clip-higher strategy to mitigate entropy collapse during training. Specifically, we set ϵhigh = 0.4 and ϵlow = 0.2, with number of iterations per batch as 2. Training Hyperparameters Table 2 summarizes the key hyperparameters used in RLVR training. Hyperparameter Value Optimizer Learning rate scheduler Maximum response length Temperature Top-p Number of rollouts per question Number of iterations per batch ϵhigh ϵhlow Global batch size Learning rate Tensor parallel size KL divergence weight AdamW Constant 6000 1.0 1.0 16 2 0.4 0.2 4 (per device) 4 (GPUs) 10 (accumulation) = 160 1 106 2 0 Table 2. Key hyperparameters used for RLVR training. 2https://github.com/huggingface/trl 3https://github.com/huggingface/Math-Verify Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning A.2. Evaluation Details Libraries For inference, we use vLLM (Kwon et al., 2023) for response generation. For response grading, we use the math verify package, consistent with the one used for rewarding during RLVR training. Inference Hyperparameters Table 3 summarizes the key hyperparameters used in inference for evaluation in Section 5.2 and Section 7. Hyperparameter Maximum token length Temperature Top-p Number of rollouts per question Value 32000 0.6 1.0 32 Table 3. Key hyperparameters used for inference during evaluation. Full Evaluation Results Table 4 reports full evaluation results for the main experiments in Section 5.2 and the iterative failure-prefix conditioning experiments in Section 7. Model Main Experiments MATH500 AMC12 AIME24 AIME25 HMMT25 Average Base Saturate Medium Failure-Prefix (Checkpoint 400) 83.5 84.2 (+0.7) 85.0 (+1.5) 85.7 (+2.2) 53.1 51.7 (-1.4) 57.8 (+4.7) 56.3 (+3.2) 28.4 30.0 (+1.6) 32.0 (+3.6) 33.0 (+4.6) 24.0 24.0 (+0.0) 26.2 (+2.2) 25.9 (+1.9) 14.2 13.5 (-0.6) 15.0 (+0.8) 16.2 (+2.0) 40.6 40.7 (+0.1) 43.2 (+2.6) 43.4 (+2.8) Iterative Failure-Prefix Conditioning Failure-Prefix (Extended Iteration 1) Failure-Prefix (Iteration 2) 56.3 (+0.0) 58.3 (+2.0) Table 4. Full evaluation results on math reasoning benchmarks. For the main experiments, numbers in parentheses indicate absolute improvements over the base model. For the iterative failure-prefix conditioning results, numbers in parentheses are computed relative to the failure-prefix model at checkpoint 400. 33.4 (+0.4) 34.7 (+1.7) 16.5 (+0.3) 15.6 (-0.6) 85.1 (-0.6) 86.0 (+0.3) 25.5 (-0.4) 25.7 (-0.2) 43.4 (+0.0) 44.0 (+0.6) 13 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning A.3. Failure-Prefix-Conditioned Dataset Details Dataset Details We construct failure-prefix-conditioned datasets for both the main experiment (Section 5) and the iterative prefix-refresh experiment (Section 7). Figure 6 shows the distributions of selected prefix lengths (as percentage of the original failure trajectory) and the corresponding rollout accuracies after conditioning. Figure 6. Distributions of prefix percentages and rollout accuracies for Iteration 1 and Iteration 2. Dashed lines indicate mean values. Inference Settings for Measuring Rollout Accuracy To measure rollout accuracy for identifying saturated questions and determining the best prefix length, we run inference from each prefix-conditioned question using the same setting as in generating rollouts during GRPO training (Appendix A.1). Specifically, we use the configuration shown in Table 5. Hyperparameter Maximum token length Temperature Top-p Number of rollouts per question Value 6000 1.0 1.0 32 Table 5. Key hyperparameters used for inference for identifying saturated questions and determining best prefix length. Identifying saturated questions As described in Section 5, we identify saturated questions as those with rollout accuracy 31/32. If the single incorrect rollout exceeds the maximum generation length of 6,000 tokens, we exclude the corresponding question from the dataset, since responses truncated at the token limit cannot be reliably classified as correct or incorrect. In practice, this exclusion affects very few questions: problems with rollout accuracy 31/32 are generally not difficult, and almost all generated responses terminate well within the 6,000-token limit. A.4. Recovery from Failure Prefixes Inference Settings. As described in Section 4, we evaluate how well different models recover from failure prefixes and maintain correct reasoning trajectories by measuring rollout accuracy from prefix-conditioned prompts. Inference is performed using the same configuration as in the final evaluation  (Table 4)  . 14 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning A.5. Derivation of Question-Level Weights and Their Relation to Reward Variance We provide detailed derivation showing how question-level weights arise in GRPO-style objectives and why they are directly tied to the standard deviation of the reward distribution, following the analysis of Li et al. (2025). Setup We consider binary reward setting where, for given question q, each generated output receives reward"
        },
        {
            "title": "Let",
            "content": "r(o q) {0, 1}. p(q) := Eoπold(q)[r(o q)] denote the probability that the current policy πold produces correct answer for question q. Under this setting, the reward variance and standard deviation satisfy Var[r(o q)] = p(q)(cid:0)1 p(q)(cid:1), std[r(o q)] = (cid:113) p(q)(cid:0)1 p(q)(cid:1). Normalized advantage GRPO and its variants employ normalized advantage function of the form Since the reward is binary, this admits the explicit piecewise representation A(o q) = r(o q) p(q) (cid:112)p(q)(1 p(q)) . A(o q) = (cid:115) 1 p(q) p(q) , if r(o q) = 1, (cid:115) p(q) 1 p(q) , if r(o q) = 0. GRPO-style objective We consider the expectation form of the GRPO-family objective (omitting KL regularization for clarity): J0(θ) = EqEoπold(q) 1 o (cid:88) t=1 (cid:18) πθ(ot q, o<t) πold(ot q, o<t) (cid:19) , A(o q) . Here, (x, y) is surrogate objective function inherited from PPO-style policy optimization. For GRPO specifically, (x, y) = min(cid:0)xy, clip(x, 1 ϵ, 1 + ϵ) y(cid:1), where is likelihood ratio and is the advantage. Structural assumption on Following Li et al. (2025), we assume that (x, y) is non-decreasing in and admits the decomposition (x, y) = 1{y > 0} +(x, 1) 1{y 0} (x, 1), for some non-decreasing functions + and . This assumption holds for GRPO and its common variants, and separates the effect of the sign and magnitude of the advantage. Decomposition by reward outcomes. Let π+ old( q) denote the conditional distributions of outputs given r(o q) = 1 and r(o q) = 0, respectively. Applying the law of total expectation and substituting the piecewise form of A(o q) yields old( q) and π (cid:34) (cid:115) J0(θ) = Eq p(q) 1 p(q) p(q) (cid:115) (1 p(q)) oπ+ old(q) 1 o (cid:88) t=1 + (cid:18) πθ πold (cid:19) , 1 oπ old(q) 1 o (cid:88) t=1 (cid:18) πθ πold (cid:19)(cid:35) . , p(q) 1 p(q) 15 Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning Emergence of the variance-based weight Factoring out common terms, the objective simplifies to J0(θ) = Eq (cid:113) p(q)(cid:0)1 p(q)(cid:1) (cid:104) oπ+ old(q)sθ(o, q) (cid:105) old(q)sθ(o, q) , oπ where sθ(o, q) denotes the corresponding token-averaged scoring function. Therefore, each question contributes to the objective with multiplicative weight (cid:113) ω(q) = p(q)(cid:0)1 p(q)(cid:1) = std[r(o q)]. Implication This derivation shows that GRPO-style objectives implicitly scale each question by the standard deviation of its reward distribution. As result, questions that are nearly always correct (p(q) 1) or nearly always incorrect (p(q) 0) receive vanishing weight, while questions of intermediate difficulty are emphasized."
        }
    ],
    "affiliations": [
        "New York University Abu Dhabi"
    ]
}