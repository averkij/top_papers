{
    "paper_title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
    "authors": [
        "Han Li",
        "Yuling Shi",
        "Shaoxin Lin",
        "Xiaodong Gu",
        "Heng Lian",
        "Xin Wang",
        "Yantao Jia",
        "Tao Huang",
        "Qianxiang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin."
        },
        {
            "title": "Start",
            "content": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Yuling Shi Shanghai Jiao Tong University China yuling.shi@sjtu.edu.cn Shaoxin Lin Huawei China 2120200411@mail.nankai.edu.cn Han Li Shanghai Jiao Tong University China lihan0421@sjtu.edu.cn 5 2 0 2 1 ] . [ 1 8 4 3 3 2 . 7 0 5 2 : r Xiaodong Gu Shanghai Jiao Tong University China xiaodong.gu@sjtu.edu.cn Yantao Jia Huawei China jamaths.h@163.com Heng Lian Xidian University China lianheng23@163.com Tao Huang Huawei China 276255565@qq.com Xin Wang Huawei China betterwangx@foxmail.com Qianxiang Wang Huawei China wangqianxiang@huawei.com ABSTRACT Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agentbased issue resolution approaches are primarily based on agents independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing code dependency graph. Then, it organizes three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by large margin1."
        },
        {
            "title": "1 INTRODUCTION\nAutomated repository-level issue resolution has emerged as a criti-\ncal challenge in software engineering. The task aims to automat-\nically localize and fix the defective code snippets, based on re-\nported issues. In software development, developers spend a major-\nity of their debugging efforts in understanding code and making\nchanges [15, 52]. Meanwhile, automated tools often struggle with\nthe same challenge [4, 33, 39]. Inadequate code understanding leads",
            "content": "Equal contribution. Xiaodong Gu is the corresponding author. 1Our code and data are available at https://github.com/YerbaPage/SWE-Debate to incomplete fixes, introduces new bugs, and significantly extends development cycles [5, 7]. The key challenge in effective issue resolution is fault localization, namely, identifying the code snippets triggering the specific issue [43]. Unlike conventional code retrieval, fault localization requires deeper connection between natural language issue descriptions and programming language structures. This process requires reasoning over the structural and semantic properties of code, often across complex dependency graphs [6, 14, 18], and entails comprehensive understanding of software architecture as well as strategic decision-making. The emergence of LLMs has significantly advanced this area by leveraging code understanding and reasoning capabilities [35, 42, 44]. More recently, agent-based methods [1, 3, 38, 46, 52] have emerged, simulating autonomous agents capable of tool use and high-level decision-making. These approaches use iterative exploration and planning to enable systematic codebase traversal, representing shift toward structured and interactive issue resolution processes [22, 43]. While agent-based approaches have shown notable progress on standard benchmarks such as SWE-bench [15], they mainly rely on agents independent exploration, that is, the agents individually understand code repository and propose their modification plans. As result, they often get stuck in local solutions and fail to identify issue patterns that span across large, complex codebases [5, 7]. This fundamental limitation stems from core challenge we term limited observation scope [32, 48]: when multiple code locations appear relevant to the issue description, correct resolution often depends on deep understanding of code structure and component relationships. However, independent exploring agents lack the diverse analytical perspectives needed to systematically compare and rank these competing alternatives. This limitation becomes more pronounced when agents are faced with multiple plausible fix strategies or modification points, each with different implications for maintainability, compatibility, and architectural soundness [6, 14, 18, 52]. Without sufficient reasoning capacity to evaluate these trade-offs holistically, independent exploring agents often fail to directly identify Conference17, July 2017, Washington, DC, USA Li et al. the correct fix location and strategy, relying instead on repeated trial-and-error that is both inefficient and error-prone [1, 10]. To address these challenges, we propose SWE-Debate, competitive multi-agent debate framework that promotes diverse reasoning paths and achieves more consolidated fault localization. SWE-Debate reframes issue resolution through graph-guided localization and structured debate mechanisms. The framework operates through three-stage pipeline. First, it creates multiple fault propagation traces as localization proposals by dependency analysis across the codebase. Specifically, static dependency graph is built to represent relationships among code entitiessuch as function calls, class inheritance, module imports, and variable references. Using language model-based semantic matching, SWE-Debate identifies entities that are most relevant to the issue description, which serve as high-confidence entry points for chain construction. SWEDebate traverses the graph from each entry point, yielding set of candidate localization chains. Each chain captures potential fault propagation path, reflecting different structural viewpoints, i.e., alternative code organization contexts in which the issue may appear, such as along call hierarchy, inheritance structure, or shared data flow. Next, the algorithm creates consensus fix plan through structured three-round debate process. In the first round, multiple agents engage in competitive ranking to select the most promising fault propagation trace. Based on the selected trace, agents independently propose candidate modification plans based on different reasoning perspectives, then engage in competitive refinement to defend their proposals while critiquing alternatives. discriminator selects the most promising plan, synthesizing insights from the debate to produce coherent and actionable modification plan. In the final stage, the modification plan is used to initialize Monte Carlo Tree Search (MCTS) framework [1] for patch generation. Our experimental evaluation on the SWE-Bench-Verified dataset systematically compares SWE-Debate against state-of-the-art baselines. SWE-Debate achieves new state-of-the-art results under opensource agent frameworks and outperforms baseline methods by large margin. Ablation studies show that the multiple chain generation mechanism provides the largest contribution to overall performance, validating our hypothesis that the fault propagation traces proposal enables more accurate fault localization and issue resolution. Our main contributions include: novel method to generate multiple candidate fault propagation traces. The method captures diverse potential fault propagation paths through code dependencies and structural relationships. competitive multi-agent debate framework for precise fault localization through diverse reasoning perspectives and structured argumentation. Extensive experiments show that our competitive debate paradigms achieve 6.7% improvement in issue resolution rate and 5.1% improvement in fault localization accuracy."
        },
        {
            "title": "2 MOTIVATION\nRepository-level issue resolution reveals the fundamental limited\nobservation scope that agentic approaches face in complex software",
            "content": "engineering scenarios [5, 7]. While individual agents can successfully handle straightforward localization tasks, they systematically fail when multiple code locations appear relevant to issue descriptions, requiring comprehensive architectural understanding and careful evaluation of competing modification plans [32, 48]. The core challenge lies in the inherent perspective limitations that prevent single agents from accurately assessing the trade-offs between multiple viable solutions [2, 9]. The Individual Exploration Problem As illustrated in Figure 1, single-agent individual exploitation approaches rely on individual exploration where agents independently understand code repositories and propose modification plans without systematic evaluation of alternative approaches. Consider the Django-11999 issue where users cannot override get_FOO_display() methods in Django 2.2+. The isolated agent performs semantic search with query \"get_FOO_display impl\" and immediately focuses on django/ db/models/base.py, specifically the _get_FIELD_display method. This individual exploration path appears reasonable from single perspective but represents fundamental misunderstanding of the issues root cause. The individual exploration approach exemplifies the core limited observation scope problem [6, 14]: the agent cannot systematically evaluate whether the base method implementation or the field registration mechanism contains the actual fault source [32], lacks the diverse reasoning perspectives necessary to compare runtime workarounds against structural solutions [18, 52], and cannot effectively analyze the architectural trade-offs between different modification plans [1, 10]. This individual exploration limitation prevents the agent from reconsidering its fundamental localization strategy when initial approaches fail, leading to inefficient trial-anderror cycles that characterize single-agent individual exploration methods. Multi-Agent Debate Resolution The correct resolution requires recognizing that method override failures stem from the field registration process in Field.contribute_to_class, where Django unconditionally overwrites user-defined methods during class construction. This structural insight emerges through multiagent debate where different agents examine alternative code regions and systematically defend their localization strategies against competing interpretations, moving beyond individual exploration to structured competitive analysis [2, 9]. In the Django-11999 case, multi-agent debate directly addresses the limited observation scope: Agent advocates for modifying the base _get_FIELD_display method while Agent argues for intervention in Field.contribute_to_class, forcing systematic comparison of these structurally different approaches. Through structured debate, different agents defend competing modification philosophiesruntime flexibility versus source-level preventionrevealing architectural trade-offs and maintainability implications that are invisible to individual exploration. When initial approaches fail, competitive pressure prevents agents from abandoning promising directions and instead drives systematic analysis of why specific strategies succeed or fail, transforming individual exploration into collaborative reasoning that enables comprehensive evaluation of the architectural soundness of different solutions. Through this debate process, agents discover that the contribute_ to_class approach provides superior design properties: it prevents SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA Figure 1: Motivating example of multi-agent debate. method overwriting at the source rather than attempting runtime workarounds, requires minimal code changes at lines 765-767 in django/db/models/fields/__init__.py with simple existence check using if not hasattr(cls,method_name), and maintains backward compatibility while enabling user method preservation. This debate dynamic creates productive analytical tension where agents must defend their approaches against alternatives [19, 50], transforming issue resolution from individual exploration to structured multi-perspective reasoning that enables precise architectural understanding and modification plans that individual agents cannot achieve [31, 53]."
        },
        {
            "title": "3 METHODOLOGY\n3.1 Problem Formulation\nGiven an issue description 𝑝 and a codebase state represented as\na set of code entities 𝑉 = {𝑣1, . . . , 𝑣𝑚 }, an agent must identify\nthe subset of entities to be modified 𝑉𝑚𝑜𝑑 ⊂ 𝑉 through an explo-\nration chain {(𝑎1, 𝑜1), . . . , (𝑎𝑛, 𝑜𝑛)}, where 𝑎𝑡 and 𝑜𝑡 denote the\naction and observation at time step 𝑡. Current approaches, however,\nface two fundamental limitations. First, their exploration is ineffi-\ncient as it overlooks structural relationships between code entities\n(e.g., classes, methods). Second, they struggle with modification\ndisambiguation when multiple locations seem relevant but require\ndifferent reasoning perspectives for a correct evaluation.",
            "content": "Our approach addresses both limitations through competitive reasoning on code dependency graph 𝐺 = (𝑉 , 𝐸), where 𝑉 is the set of code entities and 𝐸 represents their structural relationships. We construct localization chains 𝐶 = (𝑣1, . . . , 𝑣𝑘 ) composed of code entities 𝑣𝑖 𝑉 that trace fault propagation paths. Subsequently, we employ multi-agent debate to resolve modification disambiguation and determine the most effective fix plan. Our framework operates on the principle that accurate fault localization requires diverse structural viewpoints combined with rigorous evaluation mechanisms, as illustrated in Figure 2. To achieve this, we employ dual-stage competitive debate architecture. The first stage (Section 3.2) tackles the structural exploration problem by efficiently identifying potential fault propagation paths through graph traversal. The second stage (Section 3.3) performs competitive debate on chain selection and modification disambiguation among multiple agents. Finally, the selected modification plan is integrated into an MCTS-based agentic framework for patch generation (Section 3.4)."
        },
        {
            "title": "3.2 Fault Propagation Traces Proposal\nSince issue descriptions rarely pinpoint exact modification loca-\ntions, agents need to identify the fault propagation trace for issue\nresolution. To enable competitive fault localization, our algorithm\ngenerates multiple candidate fault propagation traces as localiza-\ntion proposals, followed by a competitive multi-agent debate to\nselect the most promising trace. A fault propagation trace refers\nto a structured chain of code entities (e.g., classes, methods, func-\ntions, variables) that reflect how defects may propagate through\nthe codebase.\nDependency Graph Construction. The algorithm begins by\nbuilding a static dependency graph 𝐺 = (𝑉 , 𝐸), where the node\nset 𝑉 represents code entities and the edge set 𝐸 captures their de-\npendency relationships including function calls, class inheritance,\nmodule imports, and variable references. This graph serves as the\nstructural backbone for tracing potential fault propagation traces,\nallowing agents to explore the codebase in a systematic way.",
            "content": "Conference17, July 2017, Washington, DC, USA Li et al. Figure 2: Overview of SWE-Debate framework. Identifying Entry Nodes via Semantic Matching. Next, the algorithm identifies the top-𝐾 entities 𝐸𝑝 = 𝑒1, . . . , 𝑒𝐾 that are most relevant to the issue description through semantic matching. We employ language model-based approach to extract structural identifiers that are explicitly referenced in the issue text from the dependency graph represented as an adjacency matrix and entity metadata. These entities encompass specific code-level structural elements such as class names, function names, parameter names, or error names (e.g., \"UserSession\", \"Redis\", \"wholesale\") that appear directly in the issue description. The extraction process prioritizes such structural identifiers while maintaining diversity through deduplication mechanisms. To ensure precision, the selection is constrained to only include entities with direct textual correspondence in the issue description, preventing the introduction of spurious entry points that could mislead subsequent chain construction. These high-confidence entities act as entry points for subsequent chain construction. Chain Construction via Graph Traversal. To capture diverse propagation patterns, we systematically traverse the dependency graph using two-phase strategy for each seed entity 𝑒𝑖 𝐸𝑝 : (1) Breadth-First Expansion: We identify top-𝑊 most issue-relevant neighboring nodes based on semantic and structural relevance. We first extract the top-K entities from the issue text, which are strongly associated with the described problem. However, in many cases, the root cause of an issue is not explicitly linked to specific functions or components within the issue description. To address this, we expand the context by retrieving code snippets related to each of the top-K entities using the dependency graph. These code snippets, along with the original issue, are then fed into the LLM individually. This enriched context allows the model to identify more diverse and informative entities as potential starting points for the localization chain, effectively expanding the search space and improving localization accuracy. (2) Depth-First Search: From each selected neighbor, we perform depth-limited traversal (maximum depth of 𝐿), selecting the most promising next entity at each step. The selection is guided by composite scoring function that considers both semantic similarity to the issue and structural importance in the dependency graph. This process results in total of top-𝐾 top-𝑊 localization chains, each representing plausible fault propagation path. These localization chains capture fault propagation patterns through dependency relationshipsdefects in one component affecting dependent components via call chains, inheritance hierarchies, or data flow. This structured approach efficiently identifies propagation paths that would require extensive exploration to discover through search-based methods, enabling the competitive debate process described next."
        },
        {
            "title": "3.3 Multi-Agent Debate\nWith the localization chains identified through graph-guided anal-\nysis, we synthesize a consolidated fix plan through a competitive\nmulti-agent debate. This requires evaluating competing architec-\ntural approaches—some chains may target core system components\nwhile others suggest more localized fixes, each with different impli-\ncations for maintainability and system robustness. Our competitive\ndebate forces agents to propose, defend, and refine their modifi-\ncation plans, ultimately converging on the most consolidated fix\nplan.\nLocalization Chain Selection. From the 𝐾 × 𝑊 candidate chains,\nwe form a diverse set by selecting the longest chain plus the (𝑚 −\n1) most distinct chains computed based on semantic embeddings.\nMultiple specialized agents then engage in competitive ranking,\nwhere each agent independently evaluates and ranks these 𝑚 chains\nbased on their analytical perspective. Through this localization-\nlevel debate, agents must defend their chain preferences against\nalternatives, revealing structural insights that single-agent selection",
            "content": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA would miss. We select the chain with the highest aggregate vote as the optimal localization path, which serves as the foundation for the subsequent modification plan debate. Modification Plan Proposal. Based on the selected localization chain from the competitive debate, we generate comprehensive modification plan, which systematically specifies the exact code locations requiring changes, the types of modifications needed, and their implementation priorities. The transformation from localization chains to modification proposals is implemented through specialized prompt-driven analysis framework. Given localization chain 𝐶 = {𝑒1, 𝑒2, ..., 𝑒𝑘 } and issue description 𝐼 , each agent applies structured analysis prompt that guides the examination of each entity 𝑒𝑖 in the chain. The prompt instructs agents to: (1) analyze code structure and functionality, (2) identify specific modification targets, (3) determine modification types (fix_bug, add_feature, refactor, optimize), (4) assess priority levels, and (5) provide implementation reasoning. The output is structured JSON specification that maps each chain entity to concrete modification targets with precise location descriptions, priority rankings, and implementation strategies. We use 𝑁 such agents to form pool of 𝑁 diverse modification proposals. This multi-agent approach ensures comprehensive coverage of potential modification strategies as each agent contributes its unique analytical viewpoint to form diverse pool of modification proposals. Competitive Strategy Refinement. Each agent reviews all proposals from the independent analysis phase and engages in structured argumentation to defend their approach while critically evaluating alternatives. This competitive refinement phase addresses the limitation of independent analysis by forcing agents to explicitly justify their reasoning against competing perspectives, revealing hidden assumptions and identifying potential weaknesses in initial proposals. Agents generate refined modification plans that incorporate insights from cross-agent critique while maintaining their specialized analytical focus, driving deeper understanding of the fault localization. Synthesize modification plan. Based on the refined proposals from the competitive refinement phase, discriminator agent synthesizes insights from all refined proposals to produce coherent, actionable modification plan with prioritized steps and rationale. This final selection phase is essential because competitive refinement may produce multiple valid but incompatible strategies that require unified resolution for practical implementation. The discriminator evaluates trade-offs between competing architectural approaches, considers implementation complexity and risk factors, and generates structured plan that guides downstream repair processes with both strategic direction and tactical specificity. This competitive process addresses the limitations of agents limited individual observation and exploration scope by leveraging diverse specialized perspectives and finding architecturally sound solutions through argumentative rigor. The debate produces structured modification plan with strategic insights, and this high-level plan can then be leveraged to guide the subsequent repair process."
        },
        {
            "title": "3.4 Patch Generation\nBased on the modification plans, the final stage generates patches\nand fixes the issue by employing a Monte Carlo Tree Search (MCTS)\nframework [1]. The MCTS process allows the agent to systemati-\ncally explore the codebase, refine its modification plans, and itera-\ntively evaluate the impact of each action on the codebase [21]. Un-\nlike previous approaches that begin with arbitrary exploration [1],\nour MCTS process starts from the structured modification plan\ngenerated by the competitive debate framework, enabling focused\nexploration with pre-specified target locations and strategic rea-\nsoning.",
            "content": "The MCTS process unfolds as search through tree where nodes represent states of the codebase and edges represent actions (Search for code exploration, Plan for strategic reasoning, and Edit for code modification). The initial branches of this search tree are constructed from the steps outlined in our modification plan, ensuring the exploration is grounded in the debates strategic insights. The agent then iteratively navigates and expands this tree. At each step, it selects an action based on modified Upper Confidence Bound for Trees (UCT) [1] criterion that balances exploiting known high-reward paths with exploring less-visited states. After an action is executed, its outcome is assessed by value function. This function, initially informed by the rationale from our modification plan, provides not only numerical score but also written explanation of the decisions quality. After each edit action, the agent can also execute existing tests and create new ones to better evaluate the current state. This qualitative feedback is then propagated back up the search tree, refining the agents value estimates and guiding future decisions toward successful resolution. This process continues until the agent reaches satisfactory resolution of the issue or reaches predefined exploration depth 𝐷𝑚𝑎𝑥 ."
        },
        {
            "title": "4.3 Baselines\nWe compare SWE-Debate with the following baselines on issue\nresolution:",
            "content": "Agentless [43]: non-agentic pipeline that breaks down the repair process into different phases of localization, repair, and patch validation. Conference17, July 2017, Washington, DC, USA Li et al. AutoCodeRover [52]: software engineering-oriented approach that combines LLMs with sophisticated code search capabilities. SWE-Agent [46]: custom agent-computer interface enabling LM agents to interact with repository environments through defined actions. SWE-Search [1]: repository issue resolution agent that uses Monte Carlo Tree Search (MCTS) to explore the solution space. SWESynInfer [21]: An open-source LLM series trained with development-process-centric data, simulating repository analysis, fault localization, and patch generation via three-stage Chain-of-Thought workflow. OpenHands [38]: An open-source platform for building generalpurpose AI agents that solve software and web tasks through code, terminal, and browser interaction. Additionally, we select the following baselines to compare the performance on fault localization: CodeActAgent [38]: An agent that interact with environments through executing file system search commands to locate faults. LocAgent [6]: graph-guided LLM-agent framework designed to enhance code localization through powerful multi-hop reasoning. KGComposs [45]: framwework ridges semantic gaps in repository-level repair by constructing repository-aware knowledge graph and leveraging path-guided reasoning to enhance LLM-based patch generation."
        },
        {
            "title": "4.4 Metrics\nWe employ the following metrics to evaluate the performance of\nSWE-Debate:",
            "content": "Pass@1: The percentage of issues that are resolved successfully within the first attempt, following the evaluation protocol established by [1, 46]. This metric directly measures the frameworks ability to generate correct patches without requiring multiple iterations, representing the most practical scenario for real-world deployment. Acc@1 (File): The localization accuracy at top-1 predictions at file level, where localization is considered successful only when all required modification points are included within the top-1 predicted locations [6, 43]. This metric evaluates the models capacity to precisely and comprehensively identify all code regions that require modification, providing finegrained assessment of fault localization performance prior to the patch generation stage."
        },
        {
            "title": "4.5 Implementation Details\nWe implement SWE-Debate by extending the SWE-Search [1] frame-\nwork with our graph-based localization and multi-agent debate\ncomponents. Due to unsuccessful testbed setup, we did not utilize it\nin our experiments. The code dependency graph is constructed us-\ning static analysis tools2, and the multi-agent debate employs offical\nDeepSeek-V3-0324 [8] with different system prompts to simulate\ndiverse reasoning perspectives. For the graph traversal parameters,",
            "content": "2https://github.com/python/cpython/blob/3.13/Lib/ast.py we set 𝐾 = 5 for the number of entry points, 𝑊 = 4 for breadthfirst expansion width, and 𝐿 = 5 for maximum chain length. The multi-agent debate involves 𝑚 = 6 chains for competitive ranking and 𝑁 = 5 specialized agents in the competitive refinement process. These parameters are set based on the held out set in the full SWE-Bench dataset [15]. For baseline methods, we directly use the reported results either from the official leaderboard [15] or from the official paper or repository. For experiments on DeepSeek-V3-0324, we reproduce the results on representative baselines with their official repositories."
        },
        {
            "title": "5.1 RQ1: Effectiveness on Issue Resolution\nTable 1 presents the main experimental results comparing SWE-\nDebate with state-of-the-art baselines on the SWE-Bench-Verified\ndataset. We observe that SWE-Debate is able to solve 207 out of 500\nproblems, achieving 41.4% success rate. While individual baseline\napproaches show varying performance across different language\nmodels, SWE-Debate demonstrates consistent superiority over ex-\nisting methods. Specifically, when comparing with methods using\nthe same DeepSeek-V3-0324 model, SWE-Debate achieves signifi-\ncant improvements: 6.0% over SWE-Search, improving from 35.4%\nto 41.4%, and 2.6% over SWE-Agent, improving from 38.8% to 41.4%.",
            "content": "Table 1: Main effectiveness results on SWE-Bench-Verified."
        },
        {
            "title": "Method",
            "content": "SWE-Agent SWE-Search Moatless Tools Agentless"
        },
        {
            "title": "Model",
            "content": "Pass@1 GPT-4o (2024-05-13) Claude-3.5 Sonnet DeepSeek-V3-0324 DeepSeek-V3-0324 DeepSeek-V3-0324 GPT-4o (2024-05-13) DeepSeek-V3-0324 GPT-4o (2024-05-13) GPT-4o (2024-05-13) Claude-3.5 Sonnet GPT-4o (2024-05-13) Lingma SWE-GPT 72B DeepSeek-V3-0324 23.0% 33.6% 38.8% 35.4% 34.6% 36.2% 36.6% 38.4% 30.0% 35.4% 31.8% 30.2% 38.8% SWE-Debate DeepSeek-V3-0324 41.4% It is important to note that SWE-Debate outperforms even the strongest baseline configurations, including OpenHands with DeepSeekV3-0324 and SWE-Agent with DeepSeek-V3-0324, both achieving 38.8%. This demonstrates that our competitive multi-agent debate framework provides substantial benefits beyond what can be achieved through model selection alone. SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA Finding 1: SWE-Debate achieves 41.4% Pass@1 on issue resolution, representing 2.6 percentage point improvement over the strongest baseline using the same model, demonstrating the effectiveness of competitive multi-agent debate for repositorylevel issue resolution."
        },
        {
            "title": "5.2 RQ2: Ablation Study\nTo understand the contribution of each component in SWE-Debate,\nwe conduct comprehensive ablation studies by systematically re-\nmoving key components and measuring performance degradation\non the SWE-Bench-Verified dataset. Table 2 shows the results of\nthis analysis, revealing distinct contributions from different archi-\ntectural elements.",
            "content": "Table 2: Ablation study results showing the contribution of different components."
        },
        {
            "title": "Method",
            "content": "SWE-Debate w/o Multiple Chain Generation w/o Multi-Agent Debate w/o Edit plan Pass@1 Δ 41.4% 31.4% 37.2% 35.4% - -10.0% -4.2% -6.0% We observe that removing the multiple chain generation component causes the most significant performance drop, with the method achieving only 31.4% Pass@1, representing 10.0 percentage point degradation. This suggests that exploring diverse fault propagation paths through graph traversal significantly improves localization accuracy. When this component is removed, the method must rely on single-path exploration, which frequently misses critical dependency relationships that span multiple files or modules. Similarly, removing the edit plan component results in 6.0 percentage point performance drop, declining from 41.4% to 35.4%. This demonstrates that the structured modification plans generated through competitive debate are essential for guiding the downstream patch generation process. Without these plans, the MCTSbased editing agent lacks strategic direction, leading to suboptimal exploration patterns and reduced fix accuracy. The multi-agent debate component also plays critical role. Its removal leads to 4.2 percentage point drop, reducing performance to 37.2%. This highlights the importance of competitive reasoning in resolving modification disambiguation. Without structured debate, the system relies on individual agent exploration, which often gets stuck in local solutions when multiple plausible fix locations exist. Our experiments reveal that when the localization chain contains numerous candidate files, the editing process becomes inefficient, with agents spending excessive exploration time without converging on optimal solutions. Finding 2: Multiple chain generation provides the largest contribution to performance (+10.0%), followed by edit plan generation (+6.0%) and Multi-agent debate (+4.2%), demonstrating that each component addresses distinct limitations in repository-level issue resolution."
        },
        {
            "title": "5.3 RQ3: Localization Performance Comparison",
            "content": "Table 3: Localization Performance on SWE-Bench-lite."
        },
        {
            "title": "Model",
            "content": "Acc@1 (File)"
        },
        {
            "title": "Agentless",
            "content": "SWE-Agent SWE-Search"
        },
        {
            "title": "KGCompass",
            "content": "GPT-4o (2024-05-13) Claude-3.5 Sonnet GPT-4o (2024-05-13) Claude-3.5 Sonnet DeepSeek-V3-0324 GPT-4o (2024-05-13) Claude-3.5 Sonnet GPT-4o (2024-05-13) Claude-3.5 Sonnet Qwen2.5-7B (FT) Qwen2.5-32B (FT) Claude-3.5 Sonnet Claude-3.5 Sonnet 67.15 72.63 57.30 77.37 67.00 73.36 72.63 60.95 76.28 70.80 75.91 77.74 76.67 SWE-Debate DeepSeek-V3-0324 81.67 (+3.93) Table 3 shows the localization performance comparison across different methods on the SWE-Bench-Lite dataset, which we adopt to facilitate direct comparison with existing approaches [1, 6, 20, 43, 46], referecing the results from LocAgent [6]. SWE-Debate achieves 81.67% file-level localization accuracy, significantly outperforming all baseline methods. When comparing with methods using the same DeepSeek-V3-0324 model, SWE-Debate demonstrates substantial improvements: 14.67% over SWE-Agent, improving from 67.00% to 81.67%. This represents an 3.93 percentage point improvement over the strongest baseline across all model configurations, surpassing LocAgent with Claude-3.5 Sonnet which achieves 77.74%. The improvement in localization accuracy is largely attributed to our graph-guided approach for constructing multiple fault propagation traces. By systematically exploring code dependency relationships and generating diverse candidate chains, SWE-Debate captures structural patterns that single-pass exploration methods frequently miss. The dramatic improvement over SWE-Agent using the same modelfrom 67.00% to 81.67%particularly highlights the effectiveness of our structured reasoning approach compared to traditional search-based methods. Compared to baseline approaches that perform localization once, our method aggregates multiple potential paths, significantly increasing the likelihood that at least one trace contains the correct fix location. The results demonstrate that our architectural innovations provide benefits that transcend model capabilities. While methods using stronger language models like Claude-3.5 Sonnet generally achieve better localization performance than those using GPT4o, SWE-Debate with DeepSeek-V3-0324 surpasses even the best Claude-3.5 Sonnet results. More importantly, the consistent superiority over other methods using the identical DeepSeek-V3-0324 model confirms that performance gains stem from enhanced reasoning frameworks rather than model sophistication alone. Furthermore, the gap between our localization performance and that of specialized localization methods like LocAgent demonstrates Conference17, July 2017, Washington, DC, USA Li et al. substantial improvement, with SWE-Debate achieving 81.67% compared to LocAgents 77.74%. This shows that competitive multiagent debate can enhance even domain-specific approaches. The reason is that our debate process forces agents to systematically evaluate competing localization hypotheses, preventing premature convergence on suboptimal solutions. Finding 4: chain depth of 5 achieves the best trade-off between reasoning depth and relevance, yielding the highest localization accuracy. Further increases introduce distracting information that hinders effective decision-making during the debate. Finding 3: SWE-Debate achieves 81.67% file-level localization accuracy, surpassing the strongest baseline by 3.93%, demonstrating that graph-guided fault propagation traces combined with competitive debate enable more accurate fault localization than individual exploration approaches."
        },
        {
            "title": "5.4 RQ4: Impact of the Chain Depth.\nTo investigate the optimal configuration for multi-agent reason-\ning in software fault localization, we study the impact of chain\ndepth on localization performance. To balance computational effi-\nciency with representative evaluation, we constructed a dataset of\n75 instances, termed SWE-Bench-Verified-S, consisting of 50 sam-\nples from SWE-Bench-verified-mini 3 and 25 additional instances\nrandomly selected in SWE-Bench-Verified.",
            "content": "Figure 3 shows the impact of varying chain depth in the competitive debate process on this dataset. We observe that increasing the chain depth from 1 to 5 consistently boosts file-level localization accuracy, reaching peak Acc@1(File) of 86.7%. This suggests that deeper reasoning chains enable more effective exploration of the code graph, capturing complex dependencies and contextual signals that shallow chains often miss. These results highlight the benefits of multi-step reasoning in guiding the model toward more informed and accurate localization decisions. Figure 3: Impact of the Chain Depth. However, we also find that increasing the chain depth beyond 5 leads to diminishing returns and even slight performance degradation. This suggests trade-off between reasoning depth and relevance. Longer chains are more likely to include information unrelated to resolving the issue, which can distract the model and reduce its ability to judge which chain is most likely to lead to correct fix. As result, the debate process becomes less focused, making it harder to converge on accurate localization decisions. 3https://huggingface.co/datasets/MariusHobbhahn/swe-bench-verified-mini"
        },
        {
            "title": "5.5 Case Study\nTo further verify the effectiveness of SWE-debate in actual use,\nwe analyzed a case in SWE-bench. In this case, we investigate an\nissue in SymPy related to the incorrect evaluation of powers ap-\nplied to TensorProduct expressions. Specifically, expressions like\nTensorProduct(1,Pauli(3))*TensorProduct(1,Pauli(3)) fail\nto simplify into TensorProduct(1,1) or 1, even though Pauli(3)**2\n=1. This failure arises because neither the expand(tensorproduct\n=True) method nor the tensor_product_simp function are equipped\nto handle exponentiation of TensorProduct objects. The issue\nis non-trivial as it requires coordinated reasoning over symbolic\npower expressions and tensor algebra simplification.",
            "content": "Our method first generates localization chains based on graphbased reasoning over symbolic dependencies, and then applies Multi-Agent Debate process to guide the repair. In the first stage, the agent constructs multiple candidate localization chains. Among the generated candidates, Chain 2 demonstrates high accuracy by precisely capturing all necessary modules involved in the failure, such as tensor_product_simp_Mul, tensor_product_simp, and TensorProduct.eval_expand_tensorproduct. This chain provides comprehensive view of the symbolic rewriting pipeline for tensor expressions, effectively guiding the system to the correct set of files and even narrowing down the specific functions requiring modification. Based on this chain, our debate process formulates multistage plan. It proposes four-step approach: first, modifying the simplification logic in tensor_product_simp; second, updating tensor_product_simp_Mul to support powers; third, extending eval_expand_tensorproduct for power handling; and fourth, adding an _eval_power method in the TensorProduct class. The edit derived from this plan introduces recursive rule that distributes the exponent over the arguments of TensorProduct, enabling correct evaluation of symbolic powers. Notably, all modified functions were covered by the selected localization chain, demonstrating the accuracy and completeness of our code navigation strategy. This case highlights the strength of our method in both locating and resolving issues. The localization chain effectively surfaces the relevant symbolic manipulation points, while the structured planning and debate framework facilitates coordinated edits across multiple modules. Together, they contribute to generating valid and verifiable patch that resolves the issue as expected."
        },
        {
            "title": "6 DISCUSSION\n6.1 Strengths\nSWE-Debate demonstrates three key advantages over existing ap-\nproaches. First, the graph-based localization significantly improves\nfault localization accuracy. By building dependency graphs from\ncode structure and generating multiple fault propagation traces, our\nmethod achieves 81.67% file-level accuracy. When comparing with",
            "content": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA Figure 4: Case study of SWE-Debate with instance sympy-13974. methods using identical models, SWE-Debate demonstrates substantial improvements: 14.67% over SWE-Agent with DeepSeek-V3-0324 and 8.31% over SWE-Search with GPT-4o , representing an 3.93 percentage point improvement over the strongest baseline across all configurations. The graph traversal strategy effectively captures structural relationships that single-agent individual exploration often misses, particularly for multi-file issues where traditional methods frequently fail due to insufficient architectural understanding. This superior localization performance directly translates to better issue resolution, with SWE-Debate achieving 41.4% Pass@1 compared to baseline methods ranging from 23.0% to 38.8%. Second, the competitive multi-agent debate resolves modification ambiguity more effectively than individual agent reasoning. Our ablation study shows that removing the debate component causes 4.2 percentage point drop in resolution rate. The three-round debate processindependent analysis, competitive refinement, and final selectionenables agents with different specializations to systematically evaluate competing fix strategies. This structured competitive approach achieves 2.6 percentage point improvement over the strongest baseline using the same model, demonstrating that architectural innovations can transcend model capabilities when multiple code locations appear relevant but require different architectural considerations for correct resolution. Third, the framework integrates seamlessly with existing issue resolution systems without requiring major modifications. Our approach can improve the localization modules in frameworks like SWE-Search and Agentless, providing better starting points for downstream repair while maintaining compatibility with their existing architectures. This plug-and-play design enables practical adoption in real software engineering workflows."
        },
        {
            "title": "6.2 Limitations and Future Work\nDespite the promising results, SWE-Debate has several limitations\nthat suggest directions for future work. The graph construction\nprocess can be computationally expensive for large codebases, lim-\niting scalability. Future work chould explore more efficient graph\nconstruction algorithms and incremental analysis techniques to\nhandle enterprise-scale repositories. Additionally, the current static\nanalysis approach may miss dynamic relationships and runtime\nbehaviors that could improve localization accuracy for certain types\nof issues. Our multi-agent debate currently relies on a single model\nwith different prompts to simulate diverse reasoning perspectives,\nwhich may not fully capture the breadth of real-world developer\nreasoning styles. While our specialized prompts enforce distinct\nanalytical viewpoints and our ablation study confirms significant\nperformance gains from the debate mechanism, integrating multi-\nple heterogeneous models or incorporating domain-specific knowl-\nedge bases could further enhance the diversity and quality of the\ndebate process. Future work could explore how different founda-\ntion models with varying reasoning capabilities can be orchestrated\nwithin the competitive debate framework to achieve even greater\nanalytical diversity. The current batch processing approach limits\nintegration with real-time development workflows. Future work\ncould investigate lightweight continuous analysis modes and tighter",
            "content": "Conference17, July 2017, Washington, DC, USA Li et al. integration with development environments to provide immediate issue resolution assistance during coding."
        },
        {
            "title": "7 THREATS TO VALIDITY\nInternal. The primary internal threat stems from potential data\ncontamination, as the pre-training corpus of DeepSeek-V3-0324\nmay contain repositories from SWE-Bench. To mitigate this con-\ncern, we emphasize that our evaluation focuses on reasoning pro-\ncesses rather than memorized solutions. Our method generates fault\npropagation traces through systematic graph traversal and struc-\ntured multi-agent debate, relying on analytical reasoning rather\nthan direct code recall. The substantial improvements over base-\nline methods using identical models provide evidence that perfor-\nmance gains derive from enhanced reasoning capabilities rather\nthan memorization effects. Future work will include evaluation on\ncontamination-free datasets to further validate these findings.",
            "content": "A second internal threat arises from experimental scope limitations imposed by time and budget constraints. Our evaluation is restricted to the open-source DeepSeek-V3-0324 model and subset of the SWE-Bench-Verified dataset. While our results demonstrate that SWE-Debate outperforms numerous methods across both identical and different foundation models, this constraint limits our ability to comprehensively validate the generalizability of our competitive debate framework across diverse language model architectures. Future work will expand evaluation to encompass broader range of foundation models and larger datasets to establish more comprehensive performance benchmarks. External. The main external threat comes from evaluation on single dataset SWE-Bench-Verified limited to Python repositories, which may not generalize to other programming languages or software domains. To address this, our key componentsdependency graph construction, semantic matching, and debate frameworksare designed to be language-agnostic, focusing on structural reasoning rather than language-specific patterns. And we will evaluate our method on more diverse datasets like Multi-SWE-Bench [49] in the future."
        },
        {
            "title": "8 RELATED WORK\n8.1 Fault Localization\nTraditional fault localization techniques, including spectrum-based\nfault localization (SBFL) [16], mutation-based fault localization\n(MBFL) [28], and learning-based methods [17, 24, 36], mainly relied\non test execution data and program analysis to identify buggy code\nregions. However, these techniques face fundamental limitations\nwhen applied to repository-level issue resolution: they require com-\nprehensive test suites to trigger fault patterns [25, 37], struggle with\ncomplex dependency relationships across multiple files, and cannot\neffectively bridge the semantic gap between natural language issue\ndescriptions and code structures.",
            "content": "Recent LLM-based approaches have advanced fault localization through sophisticated code understanding and repository navigation capabilities. Methods like RCAgent [40] integrate multiple analysis tools for decision support, and AgentFL [32] scales fault localization through multi-agent collaboration with static analysis tools [32, 40, 42, 44]. LocAgent [6] leverages graph-based representations to enable multi-hop reasoning across code dependencies, while OrcaLoca [48] improves localization accuracy through priority-based scheduling and distance-aware context pruning. And CoSIL [14] reduces search space using module call graphs with iterative context-aware exploration. However, these methods remain fundamentally limited by single-agent reasoning perspectives and struggle with modification disambiguation scenarios where multiple locations match issue descriptions but require different architectural viewpoints for correct evaluation. Our work addresses this limitation by introducing competitive multi-agent debate that systematically evaluates competing localization hypotheses."
        },
        {
            "title": "8.2 Repository-Level Issue Resolution\nAutomated repository issue resolution has evolved through two\nmain paradigms-agent-based and pipeline-based. Agent-based sys-\ntems model software engineering as sequential decision-making,\nwhere language models interact with code environments through\nstructured action spaces. SWE-Agent [46] established foundational\nprinciples for agent-environment interaction, AutoCodeRover [52]\nfocused on search-based localization, SWE-Search [1] introduced\nMonte Carlo Tree Search for systematic exploration, and CodeR [3]\nexplored collaborative multi-agent architectures with pre-defined\ntask graphs. Pipeline-based approaches break down issue reso-\nlution into specialized computational workflows. Agentless [43]\npioneered this paradigm by separating localization, repair, and vali-\ndation into targeted stages, while CodeMonkeys [10] investigated\niterative refinement through test-time computation scaling. Recent\nadvances include long-context models with appropriate prompt-\ning [13], training-based approaches for specialized model fine-\ntuning [27, 29, 47], and RepoUnderstander [23] which constructs\nrepository knowledge graphs for enhanced whole-repository un-\nderstanding.",
            "content": "However, existing methodologies face fundamental limitation stemming from limited observation scope. They often get stuck in local solutions and fail to resolve ambiguities when multiple code locations appear plausible, as they lack the diverse analytical perspectives needed to systematically evaluate competing modification plans [1, 7, 51]. Our approach targets this localization bottleneck by providing more accurate fault localization through competitive multi-agent analysis, enabling seamless integration with current issue resolution systems while improving their overall issue resolution rates."
        },
        {
            "title": "8.3 LLM Multi-Agent Systems\nMulti-agent systems have emerged as a promising approach for\ncomplex problem-solving by leveraging diverse specialized per-\nspectives and collaborative reasoning. In software engineering\ncontexts, these systems have shown success across various tasks\nincluding code generation [12, 41], automated testing and debug-\nging [11, 34]. Current multi-agent architectures mainly use col-\nlaborative paradigms that emphasize consensus-building and in-\nformation sharing, with scaling achieved through either cognitive\nenhancement of individual agents or population scaling through\nlarge agent collectives [30, 53].",
            "content": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA Multi-agent debate systems represent particularly relevant approach for decision-making scenarios requiring systematic evaluation of competing alternatives. Existing debate frameworks typically follow collaborative models where agents engage in structured argumentation to reach consensus through iterative refinement [2, 9]. However, these collaborative approaches face critical limitations in technical domains: agents often suffer from thought degeneration and resist modification despite potentially incorrect stances. Recent work has attempted to address these issues through role assignment strategies and agreement modulation techniques [19, 50], but these approaches still maintain collaborative consensus-seeking paradigms that may not generate sufficient analytical pressure for complex architectural decision-making. In contrast, our work introduces competitive debate framework for software fault localization that creates analytical tension by forcing agents to rigorously defend their localization hypotheses against competing proposals. Our structured, multi-round debate, combined with graph-based dependency analysis, is designed to excel at tasks requiring precise disambiguation and strategic architectural reasoning, overcoming the limitations of purely collaborative systems."
        },
        {
            "title": "9 CONCLUSION\nIn this paper, we presented SWE-Debate, a competitive multi-agent\ndebate framework that addresses the agents’ limited observation\nscope problem in repository-level issue resolution. Our approach\ncombines graph-based fault propagation trace generation with\nstructured multi-agent debates to systematically evaluate compet-\ning localization hypotheses, overcoming the perspective limitations\nof single-agent methods. Experimental evaluation on SWE-Bench-\nVerified shows that SWE-Debate achieves 6.7% improvement in\nissue resolution rate over state-of-the-art baselines. The framework\nalso demonstrates 5.1% improvement in fault localization accuracy,\nwith potential to enhance the fault localization phases of other\nautomated issue resolution methods.",
            "content": "ACKNOWLEDGMENT This research is funded by the National Key Research and Development Program of China (Grant No. 2023YFB4503802) and the Natural Science Foundation of Shanghai (Grant No. 25ZR1401175). REFERENCES [1] Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, and William Wang. 2024. SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement. arXiv:2410.20285 [2] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. In The Twelfth International Conference on Learning Representations. [3] Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, Jie Wang, Xiao Cheng, Guangtai Liang, Yuchi Ma, Pan Bian, Tao Xie, and Qianxiang Wang. 2024. CodeR: Issue Resolving with Multi-Agent and Task Graphs. arXiv:2406.01304 [cs] [4] Yuxiao Chen, Jingzheng Wu, Xiang Ling, Changjiang Li, Zhiqing Rui, Tianyue Luo, and Yanjun Wu. 2024. When Large Language Models Confront RepositoryLevel Automatic Program Repair: How Well They Done? [5] Zhi Chen, Wei Ma, and Lingxiao Jiang. 2025. Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution. arXiv:2503.12374 [cs] [6] Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, and Xingyao Wang. 2025. LocAgent: Graph-Guided LLM Agents for Code Localization. arXiv:2503.09089 [cs] [7] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. 2025. The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks. [8] DeepSeek-AI. 2025. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs] [9] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024. Improving Factuality and Reasoning in Language Models through Multiagent Debate. In Proceedings of the 41st International Conference on Machine Learning (ICML24, Vol. 235). JMLR.org, Vienna, Austria, 1173311763. [10] Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher Ré, and Azalia Mirhoseini. 2025. CodeMonkeys: Scaling Test-Time Compute for Software Engineering. [11] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. arXiv:2308. [12] Dong Huang, Qingwen Bu, Jie M. Zhang, Michael Luck, and Heming Cui. 2023. AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. arXiv:2312.13010 [13] Mingjian Jiang, Yangjun Ruan, Luis Lastras, Pavan Kapanipathi, and Tatsunori Hashimoto. 2025. Putting It All into Context: Simplifying Agents with LCLMs. arXiv:2505.08120 [cs] [14] Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, and Zhongxin Liu. 2025. CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching. arXiv:2503.22424 [cs] [15] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-world Github Issues?. In ICLR. [16] James A. Jones and Mary Jean Harrold. 2005. Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique. In Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering. ACM, Long Beach CA USA, 273282. [17] Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. DeepFL: Integrating Multiple Fault Diagnosis Dimensions for Deep Fault Localization. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, Beijing China, 169180. [18] Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, and Chao Peng. 2024. MarsCode Agent: AI-native Automated Bug Fixing. arXiv:2409.00899 [cs] [19] Yuhan Liu, Yuxuan Liu, Xiaoqing Zhang, Xiuying Chen, and Rui Yan. 2025. The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News. arXiv:2505.08532 [cs] [20] Weijie Lv, Xuan Xia, and Sheng-Jun Huang. 2024. CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs. arXiv:2408.02193 [cs] [21] Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. 2024. Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement. arXiv:2411.00622 [cs] [22] Yingwei Ma and Yue Liu. 2025. Improving Automated Issue Resolution via Comprehensive Repository Exploration. In ICLR 2025 Third Workshop on Deep Learning for Code. [23] Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. 2024. How to Understand Whole Software Repository? arXiv:2406.01422 [cs] [24] Xiangxin Meng, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2022. Improving Fault Localization and Program Repair with Deep Semantic Features and Transferred Knowledge. In Proceedings of the 44th International Conference on Software Engineering. ACM, Pittsburgh Pennsylvania, 11691180. [25] Niels Mündler, Mark Niklas Mueller, Jingxuan He, and Martin Vechev. 2024. SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Verified. [26] OpenAI."
        },
        {
            "title": "Introducing",
            "content": "SWE-bench 2024. https://openai.com/index/introducing-swe-bench-verified/. [27] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024. Training Software Engineering Agents and Verifiers with SWE-Gym. [28] Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: Mutation-based Fault Localization. Software Testing, Verification and Reliability 25, 5-7 (Aug. 2015), 605628. [29] Minh V. T. Pham, Huy N. Phan, Hoang N. Phan, Cuong Le Chi, Tien N. Nguyen, and Nghi D. Q. Bui. 2025. SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs. arXiv:2504.14757 [cs] [30] Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2024. Scaling Large Language Model-based Multi-Agent Collaboration. In The Thirteenth International Conference on Learning Representations. Conference17, July 2017, Washington, DC, USA Li et al. [31] Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2025. Scaling Large Language Model-based Multi-Agent Collaboration. arXiv:2406.07155 [cs] [32] Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, and Xiaoguang Mao. 2025. AgentFL: Scaling LLM-based Fault Localization to Project-Level Context. arXiv:2403.16362 [cs] [33] Yuchen Shao, Yuheng Huang, Jiawei Shen, Lei Ma, Ting Su, and Chengcheng Wan. 2025. Are LLMs Correctly Integrated into Software Systems?. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE, 11781190. [34] Yuling Shi, Songsong Wang, Chengcheng Wan, and Xiaodong Gu. 2024. From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging. arXiv:2410.01215 [cs] [35] Yuling Shi, Hongyu Zhang, Chengcheng Wan, and Xiaodong Gu. 2024. Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE Computer Society, 5162. [36] Jeongju Sohn and Shin Yoo. 2017. FLUCCS: Using Code and Change Metrics to Improve Fault Localization. In Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, Santa Barbara CA USA, 273283. [37] Xinchen Wang, Pengfei Gao, Xiangxin Meng, Chao Peng, Ruida Hu, Yun Lin, and Cuiyun Gao. 2024. AEGIS: An Agent-based Framework for General Bug Reproduction from Issue Descriptions. arXiv:2411.18015 [cs] [38] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2024. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. arXiv:2407.16741 [39] You Wang, Michael Pradel, and Zhongxin Liu. 2025. Are \"Solved Issues\" in SWE-bench Really Solved Correctly? An Empirical Study. [40] Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, and Qingsong Wen. 2024. RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM 24). Association for Computing Machinery, New York, NY, USA, 49664974. [41] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W. White, Doug Burger, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. arXiv:2308.08155 [cs] [42] Yonghao Wu, Zheng Li, Jie M. Zhang, Mike Papadakis, Mark Harman, and Yong Liu. 2023. Large Language Models in Fault Localisation. arXiv:2308.15276 [cs] [43] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying LLM-based Software Engineering Agents. arXiv:2407.01489 [44] Aidan Z. H. Yang, Claire Le Goues, Ruben Martins, and Vincent Hellendoorn. 2024. Large Language Models for Test-Free Fault Localization. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE 24). Association for Computing Machinery, New York, NY, USA, 112. [45] Boyang Yang, Haoye Tian, Jiadong Ren, Shunfu Jin, Yang Liu, Feng Liu, and Bach Le. 2025. Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs. arXiv preprint arXiv:2503.21710 (2025). [46] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik R. Narasimhan, and Ofir Press. 2024. SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [47] John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. 2025. SWE-smith: Scaling Data for Software Engineering Agents. [48] Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, and Jishen Zhao. 2025. OrcaLoca: An LLM Agent Framework for Software Issue Localization. arXiv:2502.00350 [cs] [49] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. 2025. Multi-SWE-bench: Multilingual Benchmark for Issue Resolving. arXiv:2504.02605 [cs] [50] Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, and Dawei Cheng. 2025. G-Designer: Architecting Multi-agent Communication Topologies via Graph Neural Networks. In ICLR 2025 Workshop on Foundation Models in the Wild. [51] Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, and Dongmei Zhang. 2025. SWE-bench Goes Live! arXiv:2505.23419 [cs] [52] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. AutoCodeRover: Autonomous Program Improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2024). Association for Computing Machinery, New York, NY, USA, 15921604. [53] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. 2024. Gptswarm: Language Agents as Optimizable Graphs. In Forty-First International Conference on Machine Learning. SWE-BENCH-VERIFIED-S SWE-Bench-verified-mini4 is subset of SWE-Bench-Verified, containing 50 instead of 500 datapoints, requiring 5GB instead of 130GB of storage, while maintaining similar distribution of performance, test pass rates, and task difficulty as the original dataset. Building on SWE-Bench-verified-mini, we augment it with 25 additional instances to better approximate the distribution and performance characteristics of the full dataset, resulting in our constructed benchmark, SWE-Bench-Verified-S. Table 4: Instance Id in SWE-Bench-Verified-S django__django-11790 django__django-11848 django__django-11885 django__django-11964 django__django-12039 django__django-12143 django__django-12193 django__django-12262 django__django-12276 django__django-12308 django__django-12406 django__django-12713 django__django-9296 sympy__sympy-12481 sympy__sympy-16766 sympy__sympy-13974 sympy__sympy-13647 sympy__sympy-12489 sympy__sympy-23824 sympy__sympy-24661 sympy__sympy-18189 sympy__sympy-24539 sympy__sympy-19495 sympy__sympy-19346 sympy__sympy-15809 sphinx-doc__sphinx-10323 sphinx-doc__sphinx-10466 sphinx-doc__sphinx-11510 sphinx-doc__sphinx-7748 sphinx-doc__sphinx-7985 sphinx-doc__sphinx-8056 sphinx-doc__sphinx-8269 sphinx-doc__sphinx-8548 sphinx-doc__sphinx-8638 sphinx-doc__sphinx-9229 sphinx-doc__sphinx-9281 sphinx-doc__sphinx-9367 sphinx-doc__sphinx-9698 django__django-11815 django__django-11880 django__django-11951 django__django-11999 django__django-12050 django__django-12155 django__django-12209 django__django-12273 django__django-12304 django__django-12325 django__django-12708 django__django-12774 sympy__sympy-13852 sympy__sympy-17318 sympy__sympy-15976 sympy__sympy-13798 sympy__sympy-20916 sympy__sympy-24562 sympy__sympy-23950 sympy__sympy-16792 sympy__sympy-12096 sympy__sympy-13757 sympy__sympy-18698 sympy__sympy-17139 sympy__sympy-22456 sphinx-doc__sphinx-10435 sphinx-doc__sphinx-10673 sphinx-doc__sphinx-7590 sphinx-doc__sphinx-7757 sphinx-doc__sphinx-8035 sphinx-doc__sphinx-8265 sphinx-doc__sphinx-8475 sphinx-doc__sphinx-8551 sphinx-doc__sphinx-8721 sphinx-doc__sphinx-9230 sphinx-doc__sphinx-9320 sphinx-doc__sphinx-9461 4https://huggingface.co/datasets/MariusHobbhahn/swe-bench-verified-mini SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA Table 5: MCTS Hyperparameters Hyperparameter Main Search Parameters c_param max_expansions max_iterations provide_feedback best_first value_function_temperature max_depth UCT Score Calculation Parameters Description Default UCT exploration parameter Max children per node Max MCTS iterations Enable feedback Use best-first strategy Value function temperature Max tree depth exploration_weight depth_weight depth_bonus_factor high_value_threshold low_value_threshold very_high_value_threshold high_value_leaf_bonus_constant high_value_bad_children_bonus_constant High-value bad children bonus high_value_child_penalty_constant UCT exploration weight Depth penalty weight Depth bonus factor High-value node threshold Low-value node threshold Very high-value threshold High-value leaf bonus High-value child penalty Action Model Parameters action_model_temperature Discriminator Parameters number_of_agents number_of_round discriminator_temperature Action model temperature Number of Discriminator Agents Number of debate rounds Discriminator temperature 1.41 2 20 True True 0.2 1.0 0.8 200.0 55.0 50.0 75.0 20.0 20.0 5.0 0.7 5 3 1 HYPERPARAMETERS OF MCTS The Monte Carlo Tree Search (MCTS) algorithm used in this study employs several hyperparameters as following [15]: PROMPT TEMPLATES In the following section, we enumerate all the prompts used throughout our entire workflow, from the initial entity extraction to the final plan generation. Prompt 1: INITIAL ENTITY EXTRACTION PROMPT You are code analysis expert . Given an issue description , your task is to identify the most relevant code entities ( classes , methods , functions , variables ) that are likely involved in the issue . Important : Only extract entities that are explicitly mentioned or strongly implied by the issue description . Do not invent names that are not referenced in the text . ** Issue Description :** { issue_description } ** Instructions :** 1. Analyze the issue description to identify : - ** Classes **: e.g., ` UserAuthenticator `, ` PaymentProcessor ` - ** Methods / Functions **: e.g., ` validate_credentials () `, ` process_payment () ` - ** Variables / Parameters **: e.g., ` user_id `, ` transaction_amount ` - ** Error Types / Exceptions **: .g., ` RateLimitExceededError `, ` DatabaseConnectionError ` 2. ** Focus on direct mentions **: Only include entities that are clearly referenced in the issue . 3. ** Avoid redundancy **: If multiple terms refer to the same entity ( e. g. , \" the payment handler \" and ` PaymentProcessor `) , pick the most precise name . 4. ** Prioritize key components **: Rank entities by how central they are to the issue . 5. ** Return only names **: Do not include paths , modules , or extra descriptions . 6. ** Limit to { max_entities } entities **: Select only the { max_entities } most relevant and important entities for this issue . ** Output Format :** Return JSON list of exactly { max_entities } entity names in order of relevance ( most relevant first ): [\" entity_name1 \", \" entity_name2 \" , \" entity_name3 \" , ...] ** Examples :** 1. ** Issue Description :** Query syntax error with condition and distinct combination Description : Count annotation containing both Case condition and distinct = True param produces query error on Django 2.2 ( whatever the db backend ). space is missing at least (... COUNT ( DISTINCTCASE WHEN ...) . ** Output ( if max_entities =3) :** [\" Count \", \" DISTINCTCASE \", \" distinct \"] 2. ** Issue Description :** \" After upgrading to v2 .0 , the ` UserSession ` class sometimes fails to store session data in Redis , causing login loops .\" ** Output ( if max_entities =2) :** [\" UserSession \", \" Redis \"] 3. ** Issue Description :** \" The ` calculate_discount () ` function applies incorrect discounts for bulk orders when ` customer_type = ' wholesale ' `.\" ** Output ( if max_entities =3) :** [\" calculate_discount \", \" customer_type \", \" wholesale \"] Note : Return only the simple names like \" __iter__ \", \" page_range \", \" MyClass \", \" my_function \" , etc . Do not include file paths or full qualified names . Return exactly { max_entities } entities , prioritizing the most important ones if there are more candidates . Conference17, July 2017, Washington, DC, USA Li et al. Prompt 2: CODE SNIPPET ENTITY EXTRACTION PROMPT Based on the following code snippets and problem statement , identify the 4 most relevant entities ( files , classes , or functions ) that are likely involved in solving this issue . ** Problem Statement :** { problem_statement } ** Code Snippets :** { code_snippets } ** Instructions :** 1. Analyze the problem statement to understand what needs to be fixed / implemented 2. Review the code snippets to identify relevant entities 3. ** PRIORITIZE DIVERSITY **: Select entities from different files whenever possible to ensure comprehensive coverage 4. ** BALANCE RELEVANCE AND DIVERSITY **: Choose entities that are both highly relevant to the issue AND come from different modules / files 5. Avoid selecting multiple entities from the same file unless absolutely necessary 6. Select exactly 4 entities that collectively provide the best coverage for solving the issue \" relevance_reason \": \" Contains user - related functionality mentioned in the issue \", \" diversity_value \": \" Model layer from different file \" \" entity_id \": \" src / views . py : UserView \" , \" entity_type \": \" class \", \" relevance_reason \": \" Handles user interface logic that may need modification \", \" diversity_value \": \" View layer from different file \" \" entity_id \": \" src / utils / validators . py : validate_user_input \", \" entity_type \": \" function \", \" relevance_reason \": \" Input validation logic relevant to the user issue \", \" diversity_value \": \" Utility function from different module \" }} , {{ }} , {{ }} , {{ \" entity_id \": \" src / config . py \", \" entity_type \": \" file \", \" relevance_reason \": \" Configuration settings that may affect user behavior \" , 7. For each entity , provide the exact entity ID in \" diversity_value \": \" Configuration file the format expected by the codebase from different location \" ** Selection Strategy :** - First priority : High relevance to the problem + Different file locations - Second priority : High relevance to the problem ( even if some files overlap ) - Ensure the selected entities represent different }} ] ``` ** Remember **: Maximize both relevance to the issue AND diversity across different files / modules to ensure comprehensive localization chain aspects or layers of the solution generation . ** Output Format :** Return JSON list containing exactly 4 entities , each with the following format : ``` json [ {{ \" entity_id \": \" file_path : QualifiedName or just file_path \", \" entity_type \": \" file class function \", \" relevance_reason \": \" Brief explanation of why this entity is relevant to the issue \", \" diversity_value \": \" How this entity adds diversity (e.g., ' different file ', ' different layer ', ' different functionality ') \" }} ] ``` ** Example :** ``` json [ {{ \" entity_id \": \" src / models . py : UserModel \", \" entity_type \": \" class \", Prompt 3: NEIGHBOR PREFILTERING PROMPT You are code analysis expert helping to select the most relevant and diverse neighbors for exploring dependency graph to solve specific issue . ** Issue Description :** { issue_description } ** Current Entity :** { current_entity } ** Current Entity Type :** { current_entity_type } ** Traversal Depth :** { depth } ** Available Neighbor Entities ({ total_count } total ) :** { neighbor_list } ** Your Task :** From the { total_count } available neighbors , select up to { max_selection } most relevant and diverse entities that would be most promising to explore next . SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA ** Selection Criteria :** 1. ** Relevance to Issue **: How likely is this neighbor to contain code related to solving the issue ? - We are performing graph traversal to find code locations relevant to solving this issue - Each neighbor represents related code entity ( file , class , or function ) 2. ** Diversity **: Avoid selecting too many - We need to select the most promising node to entities from the same file or with similar names 3. ** Strategic Value **: Prioritize entities that could lead to discovering the root cause or solution continue exploration ** Instructions :** 1. Analyze how each neighbor might relate to solving the issue 4. ** Entity Type Variety **: Balance between files , 2. Consider the traversal depth and whether we classes , and functions when possible should continue or stop ** Instructions :** 1. Analyze each neighbor entity ID to understand what it likely represents 2. Consider file paths , entity names , and types to assess relevance 3. Ensure diversity by avoiding redundant selections from the same file / module 4. Select entities that complement each other in exploring different aspects of the issue 5. Return exactly the entity IDs that should be explored further ( up to { max_selection }) ** Output Format :** Return JSON object with your selection : ``` json {{ \" selected_neighbors \": [ \" neighbor_entity_id_1 \", \" neighbor_entity_id_2 \", ... ], \" selection_reasoning \": \" Brief explanation of your selection strategy and why these neighbors were chosen \", \" diversity_considerations \": \" How you ensured diversity in your selection \" }} ``` Focus on strategic exploration that maximizes the chance of finding issue - relevant code while maintaining diversity . Prompt 4: NODE SELECTION PROMPT"
        },
        {
            "title": "You are a code analysis expert helping to navigate\na dependency graph to solve a specific issue",
            "content": ". Given the current context and available neighboring nodes , determine which node would be most promising to explore next . ** Issue Description :** { issue_description } ** Current Entity :** { current_entity } ** Current Entity Type :** { current_entity_type } ** Traversal Depth :** { depth } ** Available Neighbor Nodes :** { neighbor_info } ** Context :** 3. Evaluate which neighbor is most likely to contain relevant code for the solution 4. Return your decision on whether to continue exploration and which neighbor to select ** Output Format :** Return JSON object with your decision : ``` json {{ \" should_continue \": true / false , \" selected_neighbor \": \" neighbor_entity_id or null \", \" reasoning \": \" Explanation of your decision \", \" confidence \": 0 -100 }} ``` If should_continue is false , set selected_neighbor to null . If should_continue is true , select the most promising neighbor_entity_id . Prompt 5: CHAIN VOTING PROMPT You are an expert software engineer tasked with identifying the optimal modification location for solving specific software issue . ** Issue Description :** { issue_description } ** Available Localization Chains :** { chains_info } ** Your Task :** Analyze each localization chain as potential modification target and vote for the ONE chain where making changes would most likely resolve the issue described above . ** Evaluation Criteria :** 1. ** Problem Location Accuracy **: Does this chain contain the actual location where the bug / issue manifests ? 2. ** Modification Impact **: How directly would changes to this code path affect the described problem ? 3. ** Code Modifiability **: Is the code in this chain well - structured and safe to modify ? 4. ** Solution Completeness **: Would fixing this chain likely resolve the entire issue , not just symptoms ? Conference17, July 2017, Washington, DC, USA Li et al. 5. ** Risk Assessment **: What are the risks of modifying this particular code path ? ** Key Questions to Consider :** - Which chain contains the root cause rather than just related functionality ? - Where would developer most likely need to make changes to fix this specific issue ? - Which code path , when modified , would have the most direct impact on resolving the problem ? - Which chain provides the clearest entry point for implementing fix ? ** Instructions :** 1. For each chain , analyze whether modifying its code would directly address the issue 2. Consider the logical flow : which chain is most likely to contain the problematic code ? 3. Evaluate implementation feasibility : which chain would be safest and most effective to modify ? 4. Vote for exactly ONE chain that represents the best modification target 5. Focus on where to make changes , not just what ' related to the issue ** Output Format :** Return JSON object with your vote : ``` json {{ \" voted_chain_id \": \" chain_X \", \" confidence \": 85 , \" reasoning \": \" Detailed explanation of why this chain is the best modification target for solving the issue \", \" modification_strategy \": \" Brief description of what type of changes would be needed in this chain \", \" chain_analysis \": {{ \" chain_1 \": \" Assessment of this chain as modification target \", \" chain_2 \": \" Assessment of this chain as modification target \", ... }} }} ``` ** Example :** ``` json {{ \" voted_chain_id \": \" chain_2 \", \" confidence \": 88 , \" reasoning \": \" Chain 2 contains the pagination iterator __iter__ method which is where the infinite loop issue described in the problem statement actually occurs . Modifying the logic in this method to properly handle the iteration termination would directly solve the reported bug .\" , \" modification_strategy \": \" Add proper boundary checking and iteration termination logic in the __iter__ method \", \" chain_analysis \": {{ \" chain_1 \": \" Contains utility functions but modifications here would not address the core iteration logic issue \", \" chain_2 \": \" Contains the actual iterator implementation where the bug manifests - ideal modification target \", \" chain_3 \": \" Related display logic but changes here would not fix the underlying iteration problem \" }} }} Prompt 6: ROUND 1 MODIFICATION LOCATION PROMPT You are an expert software engineer tasked with identifying specific code locations that need to be modified to solve given issue . ** Issue Description :** { issue_description } ** Selected Localization Chain :** { chain_info } ** Your Task :** Analyze the localization chain and identify the specific locations within this chain that need to be modified to solve the issue . Focus on pinpointing the exact functions , methods , or code blocks that require changes . ** CRITICAL REQUIREMENT FOR INSTRUCTIONS :** - Each suggested_approach must be DETAILED , STEP -BY - STEP instruction - Include specific code examples , parameter names , and implementation details - Specify exact lines to modify , functions to add , and variables to change - Provide concrete implementation guidance that developer can directly follow - Include error handling , edge cases , and validation requirements - Mention specific imports , dependencies , or setup needed ** Instructions :** 1. Examine each entity in the localization chain and its code 2. Identify which specific parts of the code are causing the issue or need enhancement 3. Determine the precise locations where modifications should be made 4. Explain why each location needs modification and what type of change is required 5. Prioritize the modifications by importance ( most critical first ) 6. For each modification , provide DETAILED implementation instructions with specific code examples ** Output Format :** Return JSON object with your analysis : ``` json {{ \" modification_locations \": [ {{ SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA \" entity_id \": \" specific_entity_id \", \" location_description \": \" Specific function / method / lines that need modification \", \" modification_type \": \" fix_bug add_feature refactor optimize \", \" priority \": \" high medium low \", \" reasoning \": \" Detailed explanation of why this location needs modification \", \" suggested_approach \": \" DETAILED step - by - step implementation instructions with specific code examples , parameter names , exact function signatures , error handling , and complete implementation guidance that can be directly executed by developer \" }} ], \" overall_strategy \": \" Overall approach to solving the issue using these modifications \", \" confidence \": }} ``` ** Example of DETAILED suggested_approach :** Instead of : \" Add proper termination condition \" Provide : \" Modify the __iter__ method in the Paginator class by adding counter variable ' current_page = 1' at the beginning . Then add while loop condition ' while current_page <= self . num_pages : ' to replace the infinite loop . Inside the loop , yield ' self . page ( current_page ) ' and increment ' current_page += 1 '. Add try - catch block to handle PageNotAnInteger and EmptyPage exceptions by catching them and breaking the loop . Import the exceptions ' from django . core . paginator import PageNotAnInteger , EmptyPage ' at the top of the file .\" Prompt 7: ROUND 2 COMPREHENSIVE MODIFICATION PROMPT \"\"\" You are an expert software engineer participating in collaborative code review process to determine the best approach for solving software issue . ** Issue Description :** { issue_description } ** Selected Localization Chain :** { chain_info } ** Your Initial Analysis :** { your_initial_analysis } ** Other Agents ' Analyses :** { other_agents_analyses } ** Your Task :** Based on the issue , the localization chain , your initial analysis , and insights from other agents , provide refined and comprehensive analysis of where and how the code should be modified . ** CRITICAL REQUIREMENT FOR REFINED INSTRUCTIONS :** - Each suggested_approach must be EXTREMELY DETAILED with complete implementation guidance - Include specific code snippets , exact function signatures , and parameter details - Provide line -by - line modification instructions where applicable - Specify all necessary imports , dependencies , and setup requirements - Include comprehensive error handling and edge case considerations - Mention testing requirements and validation steps - Provide specific examples of input / output or before / after code states ** Instructions :** 1. Review your initial analysis and the analyses from other agents 2. Identify common patterns and disagreements in the proposed modifications 3. Synthesize the best insights from all analyses 4. Refine your modification recommendations based on collective wisdom 5. Provide more comprehensive and well - reasoned final recommendation 6. Ensure each suggested_approach contains exhaustive implementation details ** Output Format :** Return JSON object with your refined analysis : ``` json {{ \" refined_modification_locations \": [ {{ }} \" entity_id \": \" specific_entity_id \", \" location_description \": \" Specific function / method / lines that need modification \", \" modification_type \": \" fix_bug add_feature refactor optimize \" , \" priority \": \" high medium low \", \" reasoning \": \" Enhanced reasoning incorporating insights from other agents \", \" suggested_approach \": \" EXHAUSTIVE step -by - step implementation guide including : exact code snippets to add / modify / remove , complete function signatures , all required imports , parameter validation , error handling , edge cases , testing considerations , and specific examples of before / after states \", \" supporting_evidence \": \" References to other agents ' insights that support this decision \" Conference17, July 2017, Washington, DC, USA Li et al. ], \" overall_strategy \": \" Comprehensive strategy refined through collaborative analysis \", \" confidence \": 90 , \" key_insights_learned \": \" What you learned from other agents ' analyses \", \" potential_risks \": \" Potential risks or challenges identified through collaborative review \" }} ``` Remember : Each suggested_approach should be so detailed that developer can implement it without additional research or clarification . Prompt 8: FINAL DISCRIMINATOR PROMPT You are the lead software architect making the final decision on code modification plan . Multiple expert engineers have provided their analyses for solving software issue . ** Issue Description :** { issue_description } ** Selected Localization Chain :** { chain_info } ** All Agents ' Final Analyses :** { all_agents_analyses } ** Your Task :** Synthesize all the expert analyses and create definitive , actionable modification plan that will solve the issue effectively and safely . ** CRITICAL REQUIREMENTS FOR INSTRUCTIONS :** - Every instruction MUST be concrete modification action ( Add , Remove , Modify , Replace , Insert , etc .) - NO verification , checking , or validation instructions ( avoid \" Verify \", \" Ensure \", \" Check \", \" Maintain \", etc .) - Each instruction should specify exactly WHAT to change and HOW to change it - Focus on direct code modifications that implement the solution ** Instructions :** 1. Analyze all the expert recommendations and identify the most reliable and consistent suggestions 2. Resolve any conflicts between different expert opinions using technical merit 3. Create prioritized , step -by - step modification plan with ONLY concrete modification actions 4. Ensure the plan is practical , safe , and addresses the root cause of the issue 5. Include specific instructions for each modification 6. The output context should be as detailed as possible 7. Use action verbs like : \" Add \", \" Modify \", \" Replace \" , \" Insert \", \" Update \", \" Change \", \" Remove \", \" Implement \" ** Output Format :** Return comprehensive modification plan : ``` json {{ \" final_plan \": {{ \" summary \": \" High - level summary of the modification approach \", \" modifications \": [ {{ \" step \": 1, \" instruction \": \" Concrete modification instruction using action verbs ( Add / Modify / Replace / etc .) \", \" context \": \" File path and specific location (e .g ., function , method , line range )\", \" type \": \" fix_bug add_feature refactor optimize \", \" priority \": \" critical high medium low \", \" rationale \": \" Why this modification is necessary and how it contributes to solving the issue \", \" implementation_notes \": \" Specific technical details for implementation \" }} ], \" execution_order \": \" The recommended order for implementing these modifications \", \" testing_recommendations \": \" Suggested testing approach for validating the modifications \", \" risk_assessment \": \" Potential risks and mitigation strategies \" }} , \" confidence \": 95 , \" expert_consensus \": \" Summary of areas where experts agreed \", \" resolved_conflicts \": \" How conflicting expert opinions were resolved \" }} ``` ** Examples of GOOD instructions :** - \" Add maxlength attribute to the widget configuration \" - \" Modify the widget_attrs method to include max_length parameter \" - \" Replace the current field initialization with max_length support \" - \" Insert validation logic for maximum length \" ** Examples of BAD instructions ( DO NOT USE ) :** - \" Verify the max_length setting \" - \" Ensure proper validation \" - \" Check if the field is configured correctly \" - \" Maintain the existing functionality \" SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution Conference17, July 2017, Washington, DC, USA Focus on creating plan that can be directly executed by modification agent with clear , actionable steps ."
        }
    ],
    "affiliations": [
        "Huawei China",
        "Shanghai Jiao Tong University China",
        "Xidian University China"
    ]
}