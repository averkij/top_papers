{
    "paper_title": "Mixture of Contexts for Long Video Generation",
    "authors": [
        "Shengqu Cai",
        "Ceyuan Yang",
        "Lvmin Zhang",
        "Yuwei Guo",
        "Junfei Xiao",
        "Ziyan Yang",
        "Yinghao Xu",
        "Zhenheng Yang",
        "Alan Yuille",
        "Leonidas Guibas",
        "Maneesh Agrawala",
        "Lu Jiang",
        "Gordon Wetzstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 5 0 1 2 . 8 0 5 2 : r a"
        },
        {
            "title": "Mixture of Contexts for Long Video Generation",
            "content": "Shengqu Cai1,, Ceyuan Yang2,, Lvmin Zhang1, Yuwei Guo4, Junfei Xiao3, Ziyan Yang2, Yinghao Xu1, Zhenheng Yang5, Alan Yuille3, Leonidas Guibas1, Maneesh Agrawala1, Lu Jiang2, Gordon Wetzstein1 1Stanford University, 2ByteDance Seed, 3Johns Hopkins University, 4CUHK, 5ByteDance Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Long video generation is fundamentally long-context memory problem: models must retain and retrieve salient events across long range without collapsing or drifting. However, scaling diffusion transformers (DiTs) to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes. Date: August 29, 2025 Project Page: https://primecai.github.io/moc/"
        },
        {
            "title": "Introduction",
            "content": "Video generation has emerged as central problem in generative modeling, powering content creation, simulation for autonomous systems, and interactive storytelling. Recent Transformer-based diffusion models can synthesize increasingly realistic clips by modeling complex spacetime dependencies; yet, pushing them to minuteor hour-long horizons exposes deeper challenge: long-term memory. Models must retain and retrieve salient events across extended timelines without drift, collapse, or loss of identity. Dense self-attention becomes computationally prohibitive as sequences grow, and moreover, the core difficulty is not merely computational, but learning to selectively recall the right context at the right time. salient characteristic of video data is its high degree of temporal redundancy: consecutive frames frequently exhibit much pixel similarity or only minor motion, resulting in substantial repetition of information across the sequence. Therefore, prior efforts reduce cost either by compressing history into compact representations (e.g., keyframes [16, 43], frame packs [55], and latent states [7, 29]), or by imposing fixed sparse or selective patterns that thin interactions across the sequence [25, 41, 44, 49, 54]. These strategies lengthen the feasible horizon but hard-code compromise between efficiency and fidelity: compressed summaries lose detail, and static sparsity or selection cannot adapt to which past events matter at each step, thereby limiting the preservation 1 of long-range dependencies and narrative coherence. In this work, we reformulate long-context video generation as an internal information retrieval process, where each token dynamically accesses only the most relevant context through learnable sparse attention routing. To realize this, we propose an adaptive Mixture of Contexts (MoC) framework that learns to route each query to the most relevant segments of the video sequence, instead of relying on uniform or static sparse attention or fixed selection strategy. Specifically, MoC partitions the multi-modal token stream into content-aligned chunks along frames, shots, and captions, then lets each query select only few relevant chunks via parameter-free yet trainable top-k router. Two mandatory anchors: cross-modal links to all text tokens and intra-shot local window links are activated to stabilize local fidelity while reserving routing capacity for genuinely long-range recall. causal routing mask is additionally applied to prevent pathological loop closures by enforcing directed acyclic interaction graph, improving roll-out robustness over minute-scale sequences. For efficient implementation, the selected key tokens are directly processed by the flash-attention kernel, which supports variable sequence lengths and high throughput. During training, we progressively adjust the granularity of chunks and the selectivity of the routing mechanism, resulting in gradual sparsification that encourages the model to focus on the most informative context as training progresses. We show that replacing dense self-attention with our Adaptive Mixture of Contexts (MoC) reframes long-video generation as internal in-context retrieval. learned sparse context routing policy allocates compute to salient history and sustains cross-shot identities, actions, and layouts over minutes-long sequences, without modifying the diffusion backbone or its training recipe. Efficiency follows as an enabler, as MoC prunes over 85% of token pairs and reduces the attention FLOPs budget by up to 7, yielding measured 2.2 end-to-end generation speedup on minute-scale scenes (180k tokens). In short, our MoC is the first work that demonstrates learned sparse context routing could overcome the practical barriers of quadratic attention, and effectively deliver minutes-level long-context video memory at near short-video cost, while maintaining and often surpassing the fidelity and consistency of dense baselines."
        },
        {
            "title": "2 Related Work",
            "content": "The prohibitive O(L2) computational cost of standard self-attention mechanisms in Transformer architectures [28, 37] becomes the primary obstacle when applied to the vast sequence lengths involved, and the difficulty of maintaining coherence and preventing visual degradation over long time horizons. Our work builds upon prior efforts in efficient sequence modeling and long-video generation frameworks. Long Video Generation. Existing video generation models [1, 5, 6, 12, 13, 15, 17, 23, 36, 47, 51] are mostly limited to few seconds. Several recent frameworks specifically target longer video generation using autoregressive models that operate on frames, chunks, or segments, such as CausVid [48]. While these frameworks extend generation capabilities, they often grapple with error accumulation [38] inherent in sequential prediction or face uncertain computational scaling to longer durations. To mitigate these issues, RollingDiffusion [31] and Diffusion Forcing [3] inject controlled noise into the historical context and train the model to denoise it, increasing robustness to compounding errors. MAGI-1 [32] and SkyReels-V2 [4] scale up these ideas by employing autoregressive denoising, aiming for potentially longer durations. An orthogonal strategy is to distill the entire past into constant-size latent. TTTVideo [7] uses learnable MLP to encode the context during inference, while FramePack [55] encodes arbitrarily many frames into fixed vector for next-frame prediction. FramePack [55] also proposes early planning of future frames to mitigate the error accumulation issue. This is similar to using keyframes or anchor frames [16, 19, 26, 39, 43, 45, 46, 59], where certain frames are predefined and the video generation model only does an interpolation sampling job. These methods extend video generation to the one-minute range but still face hard ceiling on maintaining long-context coherence going forward, as they rely on lossy compression of the contexts. The work most closely related to ours is Long-Context Tuning [14] (LCT), which starts from single-shot DiT and expands its context window to scene comprising up to eight shots (8s, 2.3104 tokens each). LCT [14] keeps the attention mechanism dense: all text and video tokens inside the enlarged window attend to one another after being positioned with an interleaved 3D RoPE. While this design elegantly re-uses the pretrained weights and yields impressive multi-shot coherence, it inherits the quadratic cost of full self-attention FLOPs and memory scale with (8Lshot)2. 2 Sparse Attention for Video Generation. Sparse attention leverages the observation that attention matrices are often sparse (many scores are near zero) and computes attention only for subset of important token pairs, natural fit for video generation given spatiotemporal redundancy. Training-free pruners include SparseVideoGen [41], which profiles heads that dynamically specialize into spatial vs. temporal and selects per-head pattern, and STA [57], which exploits localized 3D windows by operating tile-by-tile over FlashAttention-friendly blocks [8, 9]. Universal filters such as SpargeAttn/SageAttention [5254] combine selective token compression with softmax-aware pass to skip parts of QK /P , and AdaSpa [42] proposes blockified dynamic pattern with Fused LSE-Cached Search that reuses sparse indices across denoising steps. Jenga [58] uses training-free block-wise attention carving plus progressive resolution. Beyond these post-hoc pruners, recent trainable or structured designs include VMoBA [40], which learns mixture-of-block scheme with layer-wise partitions and global/thresholded block selection for VDMs. VSA [56] proposes hardware-efficient coarse-to-fine sparse kernel that replaces full attention at both training and inference. Radial Attention [25] instead, uses static O(n log n) mask derived from spatiotemporal energy-decay that enables longer generations with near-dense quality. While these advances substantially reduce costs and accelerate video generation, most methods either prune emergent dense maps or impose fixed sparsity priors, focusing on accelerating the generation of short videos. By contrast, our Mixture of Contexts learns deliberate, end-to-end routing of context sources and focuses on long context memory/consistency. Context Learning in Visual Generation. complementary line of work treats contextpast frames, states, or reference imagesas first-class signal for learning and control. For video world models, where action and camera position signals are available, WORLDMEM [44] augments simulators with an external memory bank of frames and states and retrieves relevant entries via Field-of-View (FoV) overlapping to preserve long-term scene consistency. similar work, Context-as-Memory [49], targets interactive long videos, explicitly retrieving small set of historical frames as conditions for each step to sustain scene consistency, also via FoV overlapping to select the relevant frames. Back to the image space, IC-LoRA [20] demonstrates that DiTs already exhibit incontext abilities and proposes concatenating reference images with lightweight task-specific LoRA [18] to adapt across tasks with few samples. DSD [2] turns in-context generation into paired supervision via self-distillation: curate image grids with VLM, then fine-tune text+image-to-image model. OminiControl [35] offers parameter-efficient, unified framework for image-conditioned control in DiTs, enabling broad conditioned tasks without auxiliary modules. Recent open-sourced models, such as FLUX-Context [24], concatenate text and images to unify in-context image generation and editing, with improved consistency. These works demonstrate that, given sufficiently large training scale, routing and in-context learning are very powerful in extracting useful information from contexts. Our Mixture of Contexts follows this routine, and proposes to learn to route among multiple context sources end-to-end, enabling deliberate selection and composition of contextual signals rather than relying solely on fixed retrieval or single conditioning pathway."
        },
        {
            "title": "3 Method",
            "content": "To generate long videos without incurring the quadratic cost of standard self-attention, our method replaces the DiT [28] backbones dense attention with an adaptive, content-aligned Mixture of Contexts (MoC) layer. At high level, MoC (i) routes each query only to the most relevant chunks of context, (ii) aligns those chunks with natural video boundaries such as frames, shots, and caption tokens, and (iii) enforces causality so information flows strictly forward in time. The following subsections detail the routing formulation (Sec. 3.1), chunking and selection strategy for the interleave text-to-video generation (Sec. 3.2), computation efficiency (Sec. 3.3). The overall pipeline of our method is shown in Fig. 1."
        },
        {
            "title": "3.1 Mixture of Contexts",
            "content": "Vanilla Attention in Diffusion Transformers. We first revisit the attention module commonly used in Diffusion Transformers (DiT) [28, 37], the backbone of state-of-the-art video generation models. An attention module is defined as: , (1) (cid:19) (cid:18) QK Attn(Q, K, ) = Softmax 3 Figure 1 Overview of our Adaptive Mixture of Contexts. Given long multi-modal token stream, we first tag natural boundaries (frames, shots, text segments) and slice the sequence into content-aligned chunks (blue and pink blocks for texts and videos, respectively). Each chunks keys are then mean-pooled to obtain single representative vector. For every query token (green), we compute the dot-product between and every pooled key, apply top-k operation, and add mandatory links (global caption and intra-shot edges). The result fetches only selected subset of chunks, which are forwarded to Flash-Attention while all other tokens are skipped, yielding near-linear compute and memory in the number of retrieved chunks rather than quadratic in total sequence length. where Q, K, and denote the query, key, and value features, respectively, while stands for the feature dimension. Note that when we consider = {qi} as set of independent vectors, Eq. 1 can be written as Attn(qi, K, ) = Softmax(qiK/ d) that performs in query-wise. Dynamic Routing via Top-k Selection. In video DiT [28], the sequence length easily scales up to nearly 200k for 480p, 1-minute-long video. This makes the O(L2) computation of self-attention extremely expensive. Due to feature redundancy, common practice is to divide the video sequence into several chunks, allowing query token to interact with only subset of these chunks. Autoregressive video generation works [3, 4, 48] often split context by frames as chunks, where the query qi attends only to the closest few chunks, losing context beyond limited distance. Instead, we adopt learned routing strategy, where each qi is routed to the most relevant chunks with Attn(qi, K, ) = Softmax (cid:33) (cid:32) qiK Ω(qi) VΩ(qi), (2) where Ω() yields set of routed indices, and Ω(qi) is the indices of all interested context positions for the query qi. Given the list of all chunks Φ, for every qi, only few chunks are considered for attention computation with top-k operation (cid:34) Ω(qi) = arg max Ω (cid:35) (cid:0)q ϕ(Kω)(cid:1) (cid:88) ωΩ where Ω Φ and Ω = k, (3) where [] concatenate and join all indices of the top-k chunks. The relevance between the qi and the chunk sequence Kω is determined by the inner product of qi and the descriptor for Kω denoted as ϕ(Kω). For this work, we use the simple, efficient, yet effective mean pooling operation as the descriptor transformation ϕ. We argue that such mean pooling operation is highly sufficient and expressive for video generation tasks. First, mean pooling is well-motivated by established practices in multimodal matching, such as in CLIP [30], where global image representations are derived by averaging token embeddings to compute similarity with text embeddings. This approach effectively captures dominant semantic features while being robust to local variations, property that translates naturally to video chunks where spatially and temporally adjacent 4 Figure 2 Illustration of loop closures without causality. Left: successive frames from an ablation model without causal masking. After café scene (top row), the story is meant to cut to riverbank shot of the same woman looking at her phone (bottom row). However, because shot 9 strongly routes to shot 11 while shot 11 simultaneously routes back to shot 9, the model becomes trapped in two-node feedback loop, so that shot 9 and 11 have limited communication with earlier shots, as shown in the routing counts (right). tokens often represent redundant or correlated visual elements (e.g., static backgrounds or gradual motions). In our trainable framework, mean pooling is not static heuristic but an adaptive mechanism: while top-k itself is non-differentiable, the model learns indirectly through the attention mechanism on selected chunks. Specifically, if selected chunk proves irrelevant during attention computation, gradients from the loss will flow back through its keys/values, which is the source of the mean-pooled descriptor. This process attenuates unhelpful representations and encourages the query/key projections to produce more discriminative similarities over training iterations. This self-correcting process aligns with indirect adaptation seen in hard-routing MoE systems and sparse attention frameworks (e.g., where downstream modules provide the learning signal despite discrete and non-differentiable selections). This end-to-end differentiability and parameter-less router ensures that the seemingly simple dot-product routing becomes highly expressive, as the network shapes embeddings to emphasize discriminative features for sparse attention, without introducing additional parameters or computational overhead. Empirical zero-shot application to pretrained models further validates its efficacy, as will be detailed in our supplementary material. Context Drop-off and Drop-in. To enhance the robustness of our Mixture of Contexts (MoC) and mitigate issues akin to the dead expert problem in Mixture-of-Experts (MoE) systems, where certain experts or routes may become underutilized or ignored during training, we introduce two complementary regularization techniques: context drop off and context drop in. These perturbations are applied stochastically during training to simulate noisy or imperfect routing decisions, encouraging the model to learn more resilient representations that do not over-rely on specific chunks while ensuring balanced utilization across the context space. Motivated by the observation that routing may suffer from inaccuracies due to noise in embeddings or evolving data distributions, context drop off randomly removes subset of the top-k selected chunks for each query token. Specifically, for given query qi, after computing the routed indices Ω(qi) in Eq. 3, we sample drop probability pdrop Uniform(0, pmax) and mask out pdrop randomly chosen chunks from Ω(qi). This forces the model to generate coherent outputs even when certain chosen context is sporadically unavailable, promoting redundancy in the learned dependencies and preventing catastrophic failure from routing errors. Conversely, context drop in injects extraneous chunks into the selected set to simulate over-inclusive routing. For each query, we randomly sample Poisson(λ) chunks to be included in the selected pool Ω(qi). This technique combats the dead route problem by artificially activating underutilized chunks, ensuring gradients flow through broader range of context segments and balancing the routing distribution over time. Since our router is parameter-less and relies solely on mean-pooled feature similarity, these regularization techniques do not interfere with the learning of the routing mechanism itself. If chunk is truly important, its relevance will 5 be naturally enhanced through backpropagation in the attention modules, as the model adjusts the query and key projections to amplify meaningful similarities and de-emphasize irrelevant ones. In essence, the end-to-end differentiability of the system means that the attention process implicitly serves as the routers learning signal, making the framework self-correcting and adaptive without dedicated routing parameters."
        },
        {
            "title": "3.2 Attention Chunking and Routing",
            "content": "Content-aligned Chunking. critical and often overlooked design axis in Mixture of Contexts is how we carve the gigantic token stream into candidate chunks. In long-context LLMs this decision is trivial: the input is homogeneous 1D sequence of sub-word tokens endowed with single RoPE [34], so slicing it into fixed-length windows, such as in MoBA [27], both preserves local semantic coherence and matches the monotone positional metric. Video generation DiTs [28], by contrast, are often multi-modal, and operate on heterogeneous 3D+modality lattice: flattened order that interleaves spatial patches, temporal frames, text tokens, which have separate 3D RoPE [34] factors. Two neighboring indices may therefore lie far apart in space-time or span an abrupt shot cut, while static background patch can repeat for hundreds of frames next to single highly entropic motion token. Uniform windows blur these disparate signals, polluting the mean-pooled key used in Eq. 3 and forcing the top-k selector to waste slots on keys that are internally inconsistent. We instead partition the sequence along content-aware boundariesframes, shots, and modality stripes so that each chunk is semantically homogeneous and geometrically local in the 3D positional manifold. This alignment preserves the discriminative power of Eq. 3s mean-pooled keys, yields more informative top-k retrieval, and slashes quadratic overhead without sacrificing long-range coherence. Such chunking strategy can not only deal with existing single-shot text-to-video generators, but also is compatible with the existing long-video generation approach [14], which directly computes attention on an extremely long sequence with interleaved text-video pairs. Fixed Cross-Modal Selection. In addition to dynamically routed visual chunks, we explicitly require every visual query token to attend to all text tokens in the sequence since text tokens, while typically constituting less than 1% of all tokens, encode the most semantically informative signalsspecifying global style, character identities, and key actions. The computational overhead is negligible, yet the benefits are substantial: anchoring generation to the prompt significantly reduces prompt-drift errors and prevents the fading of rare attribute words during long video roll-outs. Furthermore, this hard cross-modal link facilitates joint gradient propagation into both text and visual embeddings, tightening their shared latent space and markedly improving editability in downstream tasks such as text-guided video editing. Fixed Intra-Shot Selection. Long videos naturally exhibit strict hierarchical structure, with frames nested within shots and shots within scenes. To leverage this, we explicitly enforce the intra-shot connections in the attention mechanism, ensuring that each token always attends to its belonging shotscapturing object trajectories, lighting continuity, and other predictive cues. This design allows the Mixture of Contexts (MoC) framework to allocate its sparse attention budget to genuinely long-range dependencies, rather than redundantly it guards against semantic modeling local context. Enforcing such connections offers several benefits: discontinuities at scene cuts where adjacent tokens may become unrelated; it guarantees that every attention matrix contains at least one well-conditioned block; and it provides contiguous, memory-efficient fallback path even under aggressive adaptive pruning. This strategy is particularly effective when fine-tuning pretrained video generation models, as it preserves the fidelity of each shot from the outset and enables the model to gradually learn to align broader contextual information during training. Causality in sparse MoC. Sparse routing inherently introduces directionality into the token interaction graph, as each chunk selects limited set of other chunks for attention. However, in the absence of explicit ordering constraints, this process can degenerate into pathologically closed loops. For example, in ablation studies where each chunk was permitted to select only single peer, we frequently observed cases where chunk 5 routed to chunk 6 while chunk 6 simultaneously routed back to chunk 5, forming an isolated two-node cycle (see Fig. 2). Such self-loops localize information, obstruct gradient propagation, and manifest as stalled motion or repeated frames during bidirectional generation. To address this, we impose causal mask at the routing stage, restricting each chunk to attend only to keys from earlier positions in the sequence; specifically, any edge (i j) with is masked out prior to top-k selection. This constraint transforms the routing graph 6 Method Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Base Model Ours 0.9380 0. 0.9623 0.9670 0.9816 0.9851 0.6875 0.7500 0.5200 0.5547 Image Quality 0.6345 0. Sparsity FLOPs 0% 83% 1.9 1010 4.1 109 Table 1 Single-shot video generation quantitative comparison. We report VBench [21] metrics and computation efficiency metrics. Our method is on par with or better than the base model for all VBench [21] metrics despite aggressive sparsification (83%). into directed acyclic graph (DAG), ensuring that information flows strictly forward in time and structurally precluding closed cycles. Empirically, causal routing not only eliminates isolated feedback pairs but also promotes richer long-range dependencies, resulting in smoother temporal dynamics and more stable training."
        },
        {
            "title": "3.3 Computation Efficiency",
            "content": "Combination with Flash-Attention Kernels. Dealing with content-aligned and highly unequal chunk sizes is substantially more complex than the evenly split setting, such as in MoBA [27] and NSA [50]. To accommodate frame, shot, and modality structure while preserving efficiency, we implement an adaptive attention mechanism that operates entirely on GPU, while explicitly exploiting the structural cues in video DiTs [28]. We first tag the flattened token stream with frame, shot, and caption boundaries and use torch.bucketize and prefix-sum tables (cu_seqlen, cu_shot, etc.) to derive content-aligned, variable-length chunks whose start and end indices coincide with those boundaries, ensuring that each chunk is semantically homogeneous. Boundary information is also used to build pre-routing mask: forced links (e.g., captionvisual, intra-shot self edges) are inserted before the top-k sparsification step, guaranteeing that the router never spends budget on chunk that is already mandatory. For each surviving chunk, we obtain single representative key by on-the-fly segment_reduce mean pooling, thus avoiding materializing whole chunks and keeping memory flat even when chunk sizes differ by orders of magnitude. Tokens are gathered in head-major order (via rearrange(..., h d) so that the ensuing gathers are coalesced, and the heterogeneous (query, key) pairs are packed into single Flash-Attention [8, 9] var-len call. This design yields an attention kernel that respects video-specific constraints while remaining memoryand compute-efficient across millions of tokens. Since all operations involved are head-independent, we can fully utilize tensor parallelization and sharding computations across devices. Saved FLOPs. For each attention head, let be the sequence length or number of query tokens, be the number of content-aligned chunks, be the top-k chunks query token keeps, be the average length of those selected chunks, and be the head dimension. Mean-pooling keys inside each chunk costs only Ld adds and is negligible. Routing then evaluates one inner product per querychunk pair, costing 2LCd FLOPs (2 since an inner product is one multiplication + one addition per dimension). Finally, fine-grain attention on the pruned set performs QK and products over at most keys per query token, for roughly 4Lk md FLOPs. Summing the three terms yields: For the same and d, vanilla full attention head costs FLOPsMoC Ld + 2LCd + 4Lk md. Their ratio then simplifies to: FLOPsdense = 4L2d."
        },
        {
            "title": "FLOPsdense\nFLOPsMoC",
            "content": "2L Cd + 2k , (4) (5) (6) which grows linearly with sequence length. For example, given popular compression ratio of VAE (16 spatial and 4 temporal downsampling rate), video with resolution of 480P, 12fps, and 1-minute duration becomes sequence with around 180k tokens. Supposing we use 1024, = 5, = 36, = 128, we can calculate that FLOPsMoC 2.32 1012, while in comparison, dense self-attention on the same sequence costs FLOPsdense 1.66 1013, hence the adaptive Mixture of Contexts layer reduces multiplyadds by factor of > 7. 7 Method LCT [14] Ours Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality 0.9378 0.9421 0.9526 0.9535 0.9859 0.9920 0.4583 0. 0.5436 0.5454 Image Quality 0.5140 0.5003 Sparsity FLOPs 0% 85% 1.7 1013 2.3 1012 Table 2 Multi-shot video generation quantitative comparison. Under an 85% sparsity, our method reduced FLOPs by >7, while the overall performances often improved."
        },
        {
            "title": "4 Experiments",
            "content": "We mainly conduct our experiment on two tasks: shot-level text-to-video generation and scene-level text-tovideo generation with multiple shot cuts. Base model. We build our model on long-context video generator, LCT [14], which supports both single-shot and multi-shot video generation. LCT adapts 3B-parameter MMDiT [10] architecture that was trained on mixture of image, single-shot, and multi-shot videos at their native resolutions and durations. The models full self-attention is expanded from per-shot scope to scene-level context window of up to eight shots (roughly 8 seconds, 22k tokens each), using an interleaved 3D RoPE [34] to give every shot distinct absolute coordinates while preserving the relative layout of text and video tokens. We initialize our model weights from pretrained LCT [14] and replace its attention module with our MoC, then fine-tune using the identical training scheme as LCT [14]. Baselines. For single-shot video generation, we compare against the native 3B MMDiT [10] video generation model that is used as the very foundation of LCT [14] and our work. We test on 8-second videos with resolution 320192, 12FPS, yielding roughly 6300 tokens per video. For multi-shot video generation, we compare with our base model LCT [14]. For these experiments, we test on 8-shot sequences, where each shot is an 8-second 480p video with 12 FPS. This yields roughly 180k tokens per 64-second scene. Evaluation Metrics. For both short single-shot video generation and long multi-shot video generation, we follow prior work [48, 55] and evaluate on the popular VBench [21, 22] benchmark. Specifically, Subject Consistency and Background Consistency indicate how faithfully the primary subject and background from the input image are preserved throughout the video, Motion Smoothness evaluates the fluidity of movement (lack of jitter or abrupt transitions), and Dynamic Degree measures the extent of motion in the video (encouraging the generation of dynamic content rather than static scenes). We also report Aesthetic Quality and Image Quality for single-shot videos, to quantify each frames visual appeal and technical quality. In addition, we report computational metrics such as sparsity, FLOPs, and inference speedup compared with Flash Attention [8, 9]. Quantitative Results. Tab. 1 and Tab. 2 present quantitative comparison between our content-aligned Mixture of Contexts (MoC) model and dense attention baselines on both short single-shot clips and long multi-shot scenes. For short single-shot videos (6k tokens), despite the aggressive sparsification, our method matches or surpasses the dense baseline across all VBench metrics. This demonstrates that directing computational resources toward the most relevant chunks not only reduces FLOPs but also enables the model to maintain character fidelity and scene coherence better. However, for such short sequences, the additional overhead from index gathering and pooling outweighs the computational savings, resulting in slower end-to-end pipeline. In contrast, for long multi-shot videos (180k tokens), MoC exhibits clear computational advantages. By discarding 85% of the context, our approach achieves 2.2 speedup. Furthermore, it substantially enhances the performance of our model, particularly in terms of motion diversity, as evidenced by an increase in Dynamic-Degree from 0.46 to 0.56, while maintaining Motion-Smoothness. Although this increased motion budget leads to slight reduction in appearance fidelity, all quality metrics remain high. Collectively, these results validate the core premise of our approach: learned, structure-aware sparsity reallocates computation from redundant frames to salient visual events, delivering significant efficiency gains without compromising (and in some cases improving) perceptual quality. Qualitative Results. We present qualitative comparisons in Figs. 3 and 4 for single-shot and multi-shot video generation tasks, respectively. We argue that such mean pooling operation is highly suitable for videos since pixels that lie close in space and neighboring frames tend to depict the same object or 8 Figure 3 Single-shot video generation qualitative comparison. Our results are on par, if not better than, our base model despite aggressive sparsification. background region. After the DiT [28]s patch embedding, these tokens occupy very narrow subspace: their first principal component often explains >90% of the local variance in practice. The arithmetic mean is exactly that first-component estimator for centered data, so simple average already captures the dominant semantics of the whole chunk while discarding high-frequency noise. Zero-shot experiments support this claim applying such routing strategy directly to pretrained video generation model, as will be shown in our supplementary material. Although the routing score in Eq. 3 is literally just dot-product between the query and mean-pooled key, it is not fixed heuristic: the key vectors being averaged and the query vector doing the scoring are both produced by weights that are updated during training. Gradients flow through the mean-pool operation and the subsequent top-k mask back to the projection matrices, allowing the model to learn how to shape each chunks pooled key and each query in way that best separates useful from irrelevant context. In practice, this makes the ostensibly simple mean + top-k rule highly expressive without introducing extra routing parameters or computation, as the network continuously adapts its internal representations to exploit it. MoC Implementation Benchmark. We benchmark our adaptive MoCs performance with full attention (implemented with Flash Attention 2 [8, 9]) in Fig. 5, where our method stays near-linear in terms of FLOPs and latency with respect to the number of shots, or in other words, the sequence length L. On top of sparsity, the key to this efficiency lies in three design decisions: (1) the use of on-the-fly segment_reduce pooling avoids materializing variable-length chunks in memory; (2) tokens are organized in head-major order to ensure coalesced memory access during gather operations; and (3) the entire routing + attention computation is wrapped in single Flash Attention [8, 9] var-len call, preserving kernel fusion and minimizing overhead. 9 Figure 4 Multi-shot video generation qualitative comparison. Our results are visually indistinguishable from LCT [14], despite having pruned more than three-quarters of the attention calculation. Figure 5 Performance benchmark of our content-aligned Mixture of Contexts implementation with full attention (implemented with Flash Attention 2 [8, 9]). Our method stays near linear with respect to the shot number (xaxis, assuming 8 seconds, 12 FPS, roughly 23k tokens), or in other words, the sequence length L."
        },
        {
            "title": "5 Conclusion",
            "content": "Adaptive Mixture of Contexts (MoC) demonstrates that learnable sparse attention routing can function as powerful, data-driven memory retrieval engine. Our work is arguably the first to show that by scaling up training data with an efficient and learnable sparse routing mechanism, model can develop sophisticated method for long-term recall. This approach achieves minute-scale memory at cost comparable to short-video generation. Critically, this capability emerges without explicit heuristics like 3D priors or fixed-rule selection; the model learns entirely from data which historical context is salient. Because the routing is learned and the implementation is fast during inference, MoC provides blueprint for the next generation of scalable, controllable, and responsible long-video generative models. It proves that removing the quadratic attention bottleneck is not just an efficiency gain but direct path to unlocking emergent, long-term memory in video generation. Limitation and Future Work. So far, we have trained and tested on the identical setups as LCT [14]. However, the ability of MoC to save computation on even longer sequences is yet to be explored. While our method already enables minute-scale context at near short-video cost, the current runtime relies on general-purpose variable-length attention and framework-level gathers. Given our FLOPs saving of 7, substantial headroom for further speedups remains, which could be achieved with hardwaresoftware co-design, e.g., block-sparse, chunkaware var-len attention and more efficient customized CUDA/Triton kernels, fused routing+attention operators, persistent execution, and improved K/V layouts or quantization. Furthermore, it is worth exploring MoC on broader range of applications, such as video world model datasets. We leave these extensions to future research."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia, 2024. [2] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon. Wetzstein. Diffusion self-distillation for zero-shot customized image generation. In CVPR, 2025. [3] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In NeurIPS, 2025. [4] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-v2: Infinite-length film generative model. In arXiv, 2025. [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. In arXiv, 2023. [6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. [7] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, Tatsunori Hashimoto, Sanmi Koyejo, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In arXiv, 2025. [8] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. [9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. [10] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In arXiv, 2024. [11] Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. In arXiv, 2025. [12] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In CVPR, 2023. [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. [14] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. In arXiv, 2025. [15] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. In arXiv, 2024. [16] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In CVPR, 2025. [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2023. [18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [19] Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, and Xiaodan Liang. Storyagent: Customized storytelling video generation via multi-agent collaboration. In arXiv, 2025. 11 [20] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. In arXiv, 2024. [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [22] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench++: Comprehensive and versatile benchmark suite for video generative models. In arXiv, 2024. [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. In arXiv, 2024. [24] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. In arXiv, 2025. [25] Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, and Song Han. Radial attention: O(n log n) sparse attention with energy decay for long video generation. In arXiv, 2025. [26] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos. In ECCV, 2024. [27] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Yutao Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu. Moba: Mixture of block attention for long-context llms. In arXiv, 2025. [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [29] Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. In ICCV, 2025. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In arXiv, 2021. [31] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models. In arXiv, 2024. [32] Sand-AI. Magi-1: Autoregressive video generation at scale. In arXiv, 2025. [33] Chenyang Si, Weichen Fan, Zhengyao Lv, Ziqi Huang, Yu Qiao, and Ziwei Liu. Repvideo: Rethinking cross-layer representation for video generation. In arXiv, 2025. [34] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. In arXiv, 2021. [35] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In arXiv, 2025. [36] Genmo Team. Mochi 1. In GitHub repository, 2024. [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [38] Jing Wang, Fengzhuo Zhang, Xiaoli Li, Vincent Y. F. Tan, Tianyu Pang, Chao Du, Aixin Sun, and Zhuoran Yang. Error analyses of auto-regressive video diffusion models: unified framework. In arXiv, 2025. [39] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, Chong Luo, Yueyi Zhang, and Zhiwei Xiong. Artv: Auto-regressive text-to-video generation with diffusion models. In arXiv, 2023. 12 [40] Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, and Yunhai Tong. Vmoba: Mixture-of-block attention for video diffusion models. In arXiv, 2025. [41] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. In arXiv, 2025. [42] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. In arXiv, 2025. [43] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. In arXiv, 2025. [44] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. In arXiv, 2025. [45] Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, and Saad Ezzini. Dreamfactory: Pioneering multi-scene long video generation with multi-agent framework. In arXiv, 2024. [46] Dingyi Yang, Chunru Zhan, Ziheng Wang, Biao Wang, Tiezheng Ge, Bo Zheng, and Qin Jin. Synchronized video storytelling: Generating video narrations with structured storyline. In arXiv, 2024. [47] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In arXiv, 2024. [48] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. [49] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. In arXiv, 2025. [50] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In arXiv, 2025. [51] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. In IJCV, 2023. [52] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In ICML, 2025. [53] Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR), 2025. [54] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In ICML, 2025. [55] Lvmin Zhang and Maneesh Agrawala. Packing input frame contexts in next-frame prediction models for video generation. In arXiv, 2025. [56] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. In arXiv, 2025. [57] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. In arXiv, 2025. [58] Yuechen Zhang, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengfei Wan, Eric Lo, and Jiaya Jia. Training-free efficient video generation via dynamic token carving. In arXiv, 2025. [59] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequences. In ICLR, 2025."
        },
        {
            "title": "A Training Details",
            "content": "For our single-shot video generation model, we train jointly on images and videos. We use chunk size of 256 and top-k=3, while enabling intra-chunk link and forced cross-modal link, where all chunks are forced to attend to themselves and the prompt tokens. We do not activate causality since we do not observe the pathologically closed-loop effect. For our multi-shot generation model, we train our model jointly on images, single-shot videos, and multi-shot videos using chunk size gradually decreasing from 10240, 5120, 2560 to 1280, and top-k=5, while enabling intra-shot link and forced cross-modal link, where each shot always performs self-attention, and each chunk attends to both local and global prompts. Both models are trained using learning rate of 9e 5, where the single-shot model is trained for 10k iterations and the multi-shot model is trained for 20k iterations."
        },
        {
            "title": "B Ablation Study",
            "content": "We systematically disentangle two design axes of our Mixture of Contexts: (1) effects of different chunk sizes and in our top-k routing, and (2) the benefit of our forced links (cross-modal and intra-shot edges). For the former, we evaluate on single-shot video generation; for the latter, we focus on multi-shot video generation, where the forced links are more important. For the ablation study, we uniformly use 16 H100s and train 30k iterations for the single-shot experiments and 10k iterations for the multi-shot experiments, using learning rate of 2e 5. Chunk Size 64 128 256 512 1024 256 256 256 256 256 256 3 3 3 3 1 2 3 4 5 6 Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Image Quality Sparsity FLOPs 0.9868 0.9909 0.9916 0.9649 0.9614 0.9994 0.9968 0.9916 0.9827 0.9793 0.9722 0.9884 0.9934 0.9933 0.9780 0.9736 0.9995 0.9966 0.9933 0.9863 0.9848 0.9805 0.9928 0.9937 0.9938 0.9873 0. 0.9956 0.9949 0.9938 0.9898 0.9886 0.9903 0.3413 0.2875 0.4612 0.5156 0.5938 0.1313 0.2781 0.4612 0.4531 0.3594 0.4219 0.4964 0.4634 0.5283 0.5275 0.5518 0.3485 0.4325 0.5283 0.5127 0.5158 0.5380 0.6374 0.6673 0.6813 0.6546 0. 0.7421 0.6940 0.6813 0.6276 0.6456 0.6629 96% 92% 83% 68% 35% 92% 88% 83% 80% 76% 72% 1.2109 1.7109 4.1109 6.6109 1.31010 2.1109 3.1109 4.1109 5.2109 6.2109 7.2109 Table 3 Ablation study on different chunk sizes and routing top-k. Chunk size and k. Ablation results on different chunk sizes and are presented in Tab. 3. When we fix the number of retrieved chunks at k=3 and sweep the chunk length from 64 to 1024 tokens, we notice that tiny chunks (64,128) prune aggressively but harm motion, potentially because queries often lose access to far-context frames and are stuck with local optimums. We see similar trends with fixing the chunk size at 256 and varying (each query also keeps its own chunk, so the effective fan-out is (k + 1) This is strong indication that progressive approach that starts from larger chunks and larger k, then gradually switches to smaller chunks and smaller might be desired in order to achieve very aggressive sparsification."
        },
        {
            "title": "Force",
            "content": "Intra-shot Cross-modal"
        },
        {
            "title": "Subject\nConsistency",
            "content": ""
        },
        {
            "title": "Background\nConsistency",
            "content": ""
        },
        {
            "title": "Motion\nSmoothness",
            "content": "Dynamic Degree"
        },
        {
            "title": "Aesthetic\nQuality",
            "content": ""
        },
        {
            "title": "Image\nQuality",
            "content": "0.8532 0.8305 0.9238 0.9318 0.9391 0.9358 0.9446 0.9592 0.9949 0.9952 0.9910 0. 0.0000 0.0208 0.5729 0.5654 0.2957 0.2934 0.5406 0.5446 0.1552 0.1572 0.4472 0.5147 Table 4 Ablation study on the effect of forced links."
        },
        {
            "title": "Subject\nConsistency",
            "content": ""
        },
        {
            "title": "Background\nConsistency",
            "content": ""
        },
        {
            "title": "Motion\nSmoothness",
            "content": ""
        },
        {
            "title": "Dynamic\nDegree",
            "content": ""
        },
        {
            "title": "Aesthetic\nQuality",
            "content": "Dense Attention MoC (ours) 0.9512 0.9549 0.9339 0.9537 0.9869 0.9833 0.4219 0. 0.5154 0."
        },
        {
            "title": "Image\nQuality",
            "content": "0.5831 0.6016 Sparsity 0% 81% Table 5 Single-shot video generation quantitative comparison on Wan-2.1-1.3B. Figure 6 Zero-shot sparsification. We replace every dense attention block in pretrained DiT with our Mixture of Contexts (>75% sparsity) without any fine-tuning. The model still preserves certain amount of subject identity, background layout, and coarse motion, confirming that simple mean-pooled chunk key already provides usable retrieval signal even when the weights have never been exposed to sparse masks. Force links. Ablation on the effects of forced routing links is presented in Tab. 4. Experiments are conducted with chunk size of 5120 and k=5. When the intro-shot link is not forced to be selected, we compensate the model to be able to select four additional chunks, which is roughly the number of tokens per shot. We notice that the training becomes extremely unstable when there are no forced intra-shot links to provide sufficiently reasonable lower bound. Empirically, we find this to be highly relevant to the learning rate and batch size, while adding the intra-shot links makes the training much more stable. We also find that adding cross-modal links improves the overall performance of the model. Wan-2.1-1.3B Experiment To demonstrate the generalization ability of MoA on general open-sourced backbones, we implemented and tested MoC on the Wan-2.1-1.3B model. We compare two settings: fine-tune the pretrained model using dense attention and our proposed Mixture-of-Attention. Since Wan-2.1-1.3B is not an MMDiT model but regular DiT model, we apply MoC only on its self-attention modules using the same hyperparameters as our single-shot experiment. We train these two settings, each on 32 GPUs for 1 day (2000 iterations), using the Vchitect [11, 33] dataset at resolution of 480p, with chunk size set at 1560 number of tokens for frame in Wan-2.1-1.3B. Results are presented in Tab. 5. We observe similar trend to the single-shot experiment presented in our paper, where sparsity is at least on par and often better than dense attention. This is solid proof of the generalization ability of MoC to other backbones, even without any model-wise adaptation of the MoC algorithm. We also find that our MoC performs reasonably well without many visible artifacts on Wan-2.1-1.3B, even without fine-tuning, as long as the sparsity does not become too low. Zero-shot Experiment To isolate the benefit of the mean-pooled descriptor, independent of fine-tuning, we plug our MoC kernel directly into the pretrained dense model while freezing all weights. As shown in Fig. 6, despite never seeing sparse attention during training and high sparsity (>75%), the model maintains consistency reasonably. Because the descriptor is simply the arithmetic mean, it approximates the first principal component of each chunk, which is already well-aligned with dominant foreground/background patterns. This experiment highlights that the routing rule itself is data-adaptive, even without weight updates, while learning can refine the query/key projections to make better use of it and increase its accuracy. These results validate our design choice: the parameter-free, mean-pooled descriptor is strong, low-overhead signal that converts dense attention into retrieval step, even in zero-shot settings. We note concurrent work such as VSA [56] has similar observations."
        },
        {
            "title": "E Outer Loop Context Routing",
            "content": "To further scale our approach to extremely long video sequences, we introduce an outer loop context routing mechanism in practice, which operates independently of the inner attention computation. Unlike the querywise routing in Mixture of Contexts, which refines attention within selected chunks, the outer loop performs preliminary selection of large-scale context chunks such as entire shot segments before any attention is computed. This pre-selection acts as coarse filter, dynamically curating subset of the global context to be fed into the subsequent Mixture of Contexts layers, thereby reducing the overall token pool and enabling linear scaling for sequences exceeding millions of tokens. Formally, given flattened token stream partitioned into high-level chunks Ψ = {Ψ1, Ψ2, . . . , ΨP } where each Ψj encompasses multiple lower-level chunks, the outer router computes global relevance score for each Ψj relative to the current generation context. We employ the simple yet effective scorer again: mean-pooled descriptor ϕ(Ψj) = mean_pool(X[Ψj]), where X[Ψj] denotes the token features from all tokens in Ψj. For the current query block (e.g., the tokens of the shot being generated), we aggregate its token features into single representative vector xg = mean_pool(Xg) and compute the similarity score as xg, ϕ(Ψj), where the top-M large chunks are then selected Ωg = arg maxΩΨ,Ω=M jΩ sj. The selected high-level chunks Ωg are concatenated with mandatory elements (e.g., the global caption) to form reduced context stream, which is then passed to the inner Mixture of Contexts for more fine-grained routing and sparser attention. This outer-inner hierarchy decouples coarse global retrieval from local refinement: the outer loop prunes redundant historical segments, while the inner loop focuses on precise token-level interactions within the curated subset. This is particularly helpful when dealing with extremely long contexts that scale beyond our training maximum length, as the outer loop compresses the effective context size to within the models trained capacity, rendering the approach invariant to length extrapolation issues. Unlike dense attention mechanisms that suffer from positional embedding degradation (e.g., RoPE [34] extrapolation problems leading to instability or performance drops beyond trained lengths), our hierarchical routing maintains stable positional encodings by operating on curated, shorter subsequence, ensuring consistent performance even for arbitrarily long inputs without requiring specialized extrapolation techniques or retraining. The outer loop routing can effectively increase the number of shots we could generate by 2-3 times, under an autoregressive sampling strategy. (cid:80)"
        },
        {
            "title": "F Social Impact",
            "content": "Long-form video generators can democratize animation and documentary production, educational content, and simulation. Still, like all powerful generative models, they also lower the barrier for misinformation and non-consensual media synthesis. We advocate for gated release, watermarking, and prompt filtering similar to current large-image and language models."
        }
    ],
    "affiliations": [
        "ByteDance",
        "ByteDance Seed",
        "CUHK",
        "Johns Hopkins University",
        "Stanford University"
    ]
}