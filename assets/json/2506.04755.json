{
    "paper_title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning",
    "authors": [
        "Shenshen Li",
        "Kaiyuan Deng",
        "Lei Wang",
        "Hao Yang",
        "Chong Peng",
        "Peng Yan",
        "Fumin Shen",
        "Heng Tao Shen",
        "Xing Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 5 5 7 4 0 . 6 0 5 2 : r Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning Shenshen Li1, Kaiyuan Deng1, Lei Wang3, Hao Yang4, Chong Peng4, Peng Yan4, Fumin Shen1, Heng Tao Shen2, Xing Xu1,2 1University of Electronic Science and Technology of China 2School of Computer Science and Technology, Tongji University 3Salesforce AI Research, 4Meituan"
        },
        {
            "title": "Abstract",
            "content": "While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through key observation: meaningful multi-modal reasoning is triggered by only sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each samples potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https: //github.com/Leo-ssl/RAP."
        },
        {
            "title": "Introduction",
            "content": "Improving complex reasoning in multi-modal large language models (MLLMs) [1, 2] remains fundamental challenge. While large-scale reinforcement learning (RL) [3, 4, 5] has shown promise in enhancing reasoning capability, the prevailing assumption [6, 7] suggests that scaling training data is necessary condition for developing advanced reasoning ability, thus leading to data redundancy and substantial training costs. Recent studies [8, 9] indicate that LLMs trained on high-quality curated datasets can outperform those trained on full corpora. However, it remains unclear whether this principle generalizes Figure 1: Comparison of (a) accuracy under varying training dataset sizes and (b) performanceefficiency trade-offs on various methods. Preprint. Under review. Figure 2: Illustrative examples for two ineffective training sample types: (a) language-prior biased samples and (b) attention-biased samples. to multi-modal contexts, where effective cross-modal integration is important. This raises critical question: can smaller high-value data achieve competitive or superior multi-modal reasoning compared to training on full corpora in MLLMs ? To investigate this, as shown in Figure 1(a), we empirically analyze the effect of data scale on multi-modal reasoning performance. Notably, training with only 20% of the data leads to merely 0.8% performance degradation compared to the full dataset, suggesting that indiscriminate data scaling may have minimal or even negative effects. We hypothesize that such data augmentation diminishes the influence of high-value samples, termed cognitive samples, which are essential for guiding effective cross-modal integration during reasoning. To validate this assumption, we analyze the characteristics of training samples and find that most fail to encourage joint attention to both modalities during reasoning. Specifically, we identify two main types of ineffective samples: 1) Language-prior biased samples (Figure 2(a)), where the model produces nearly indistinguishable outputs given text-only and multi-modal inputs due to over-reliance on language priors [10, 11]. Such samples enable models to solve tasks with minimal utilization of visual semantics, thus impairing their ability to cross-modal integration. (2) Attention-biased samples (Figure 2(b)), where the model over-attends to semantically irrelevant tokens (e.g., the punctuation .), thereby obstructing the exploration of crucial cross-modal relationships. The above findings highlight the need to prioritize cross-modal interactions in data selection for multi-modal reasoning. However, existing methods rely on unimodal textual quality, such as human-annotated difficulty estimation [8, 9] or reward-based sampling [12]. These approaches not only incur substantial manual annotation costs [8, 12] and considerable filtering time [9], but also fail to estimate whether samples effectively facilitate cross-modal integration. Motivated by the above observations, we propose novel data selection paradigm termed Reasoning Activation Potential (RAP) for enhancing multi-modal reasoning while reducing training costs. RAP aims to identify cognitive samples that effectively trigger multi-modal reasoning during RL training. Specifically, RAP estimates the reasoning potential of each sample through two complementary perspectives: output-level reasoning discrepancy and process-level reasoning confidence. For the former, we are inspired by the intuition that if model predictions remain invariant regardless of visual input presence, the model may merely exploit linguistic biases rather than engaging in genuine multi-modal reasoning. We formalize this notion through Causal Discrepancy Estimator (CDE), which employs the Potential Outcome Model (POM) to estimate the causal effect of input modality on model predictions by simulating counterfactual outcomes, i.e., what the model would output if one modality were removed. Consequently, CDE effectively eliminates language-prior biased samples by measuring discrepancies between multi-modal and text-only predictions. However, relying solely on output-level measures neglects the reliability of internal reasoning dynamics. Therefore, we propose the Attention Confidence Estimator (ACE) to model the quality of internal reasoning behavior based on token-level attention distributions, thus removing attentionbiased samples characterized by high attention to irrelevant tokens. Despite their efficacy, combining these two estimators alone might retain overly simplistic samples while discarding challenging yet valuable instances, thereby constraining the models reasoning upper bound. To address this limitation, we propose Difficulty-aware Replacement Module (DRM) to replace trivial samples with suitable challenging alternatives, which ensure sufficient data complexity for robust multi-modal reasoning. Finally, the results in Figure 1(b) demonstrate that RAP achieves state-of-the-art performance with only 5,159 samples, compared to the full dataset of 54,931 samples, while reducing training costs by over 43%. These findings validate our insight that data quality is more important than blind data scaling for multi-modal reasoning in RL, revealing the truth in the few phenomenon. Our main contribution: 1) We reveal truth in the few phenomenon that smaller high-quality datasets can outperform full corpora for multi-modal reasoning in MLLMs. 2) We propose two novel estimators: Causal Discrepancy Estimator (CDE) to eliminate samples that overly rely on 2 Figure 3: The overall pipeline of our RAP method. First, the Causal Discrepancy Estimator (CDE) filters out samples that overly rely on language priors via output-level discrepancy. Then, the Attention Confidence Estimator (ACE) excludes attention-biased samples by token-level attention distributions. Finally, the Difficulty-aware Replacement Module (DRM) selectively replaces trivial instances with cognitively challenging ones, yielding refined subset of cognitive samples. language priors, and an Attention Confidence Estimator (ACE) to filter out attention-biased samples with irrelevant semantic focus. 3) We introduce Difficulty-aware Replacement Module (DRM) to preserve sufficient data complexity, effectively improving the models reasoning performance ceiling."
        },
        {
            "title": "2 Related Work",
            "content": "Reinforcement Learning for (M)LLM Reasoning. Reinforcement learning (RL), initially popularized by RL from Human Feedback (RLHF) [4, 13], has become key paradigm for improving the reasoning ability of both LLMs and MLLMs. Recent methods [14, 15, 16] extend RL beyond human preference alignment, explicitly achieving reasoning improvement via policy-gradient algorithms such as Proximal Policy Optimization (PPO) [17] and reward-centric optimizations including Reinforce Leave-One-Out (RLOO) [18] and Group Relative Policy Optimization (GRPO) [3]. Moreover, several methods [19, 5, 20, 21] explore the use of RL to enhance the visual reasoning in MLLMs. However, these methods typically rely on large-scale data [22, 23], which fails to adequately consider the quality of training samples. To this end, we introduce novel RAP method to select valuable samples, ensuring that the training process stimulates effective multi-modal reasoning. Data Selection for Reasoning. Traditional approaches [3, 7, 22] generally highlight the importance of data scaling, suggesting that larger data volumes can lead to better model performance. Contrary to this, recent methods [12, 9, 8] demonstrate that curated datasets can outperform those trained on full corpora in complex textual reasoning tasks. For example, LIMO [12] and s1 [8] demonstrate that models trained with just 817 or 1000 curated samples achieve better performance compared to models trained on larger datasets. Inspired by these, we focus on whether and how minimal but valuable dataset can similarly enhance multi-modal reasoning within multi-modal contexts. Causal Mechanism. Recent advances have demonstrated the significance of causal mechanisms in complex reasoning tasks [24, 25]. To better capture causal relationships in the reasoning processes of MLLMs, prior studies have used existing causal inference frameworks, such as Structural Causal Models (SCM) [26] and Potential Outcome Models (POM) [27]. For example, the POM offers rigorous analytical structure to examine causal dependencies [24, 28], facilitating the isolation of causal factors directly influencing model reasoning. Therefore, we adopt the POM framework to systematically quantify differences in MLLM outputs given multi-modal and text-only inputs."
        },
        {
            "title": "3 Method",
            "content": "Overview of RAP Method. As shown in Figure 3, our RAP method aims to identify high-value training samples xcd, termed cognitive samples, that effectively stimulate multi-modal reasoning in MLLMs during RL training. Given training instance = (xt, xv), we estimate its reasoning activation potential from two perspectives. First, we adopt the potential outcome model to quantify the output-level discrepancy between the model predictions under multi-modal inputs, Y1(x), and text-only inputs, Y0(xt). Samples with low discrepancy values be considered as language-prior biased and are discarded accordingly. Second, we compute confidence score ψ(A) from the self-attention matrix Rdd of the models final layer to assess the models focus on semantically meaningful 3 tokens. Samples with attention focused on irrelevant tokens, below threshold λa, are excluded as attention-biased. Moreover, we replace overly easy samples with an equal number of hard examples potentially missed by the initial model due to limited reasoning capacity, thereby enhancing the models reasoning upper bound. The resulting cognitive samples form refined training dataset that supports more efficient and robust multi-modal reasoning in MLLMs. Finally, we utilize these cognitive samples xcd to optimize the model by maximizing the objective of GRPO [3, 29]. 3.1 Causal Discrepancy Estimator To identify samples where the model genuinely engages in multi-modal reasoning, rather than overly relying on language priors, we interpret modality influence in reasoning as output discrepancy between multi-modal and text-only inputs, formulated under the Potential Outcome Model (POM). Background of Potential Outcome Model. The foundation of causal inference is rooted in the Neyman-Rubin POM [30, 27], which aims to estimate the effect of treatment on an outcome for individuals described by covariates X. In this work, we consider the treatment variable to be binary, i.e., {0, 1}. Under this framework, each unit is associated with two potential outcomes: Y1(u), the outcome if the unit receives the treatment (T = 1), and Y0(u), the outcome under control (T = 0). The Individual Treatment Effect (ITE) [31, 32] is defined as the difference Y1(u) Y0(u). However, due to the fundamental problem of causal inference [33, 34], only one of these outcomes can ever be observed for given individual, rendering the ITE fundamentally unidentifiable. To address this, the prior work [35] proposes the Conditional Average Treatment Effect (CATE), which represents the expected treatment effect conditioned on covariates: E[Y1 Y0 = x] = E[Y = 1, = x] E[Y = 0, = x], (1) where denotes the observed covariates of the unit. Output-level Discrepancy Calculation. Inspired by the intuition that model generating nearly identical outputs in the presence or absence of visual input may fail to use multi-modal information for reasoning, we employ the POM to formalize the influence of the visual modality on model predictions, by defining outcomes under distinct treatment conditions. Specifically, we treat the presence of the image as binary treatment variable {0, 1}, where = 1 indicates the inclusion of the image and = 0 denotes its absence. Given an input = (xt, xv), we define two potential outcomes: Y1(x), the models output given both text and image, and Y0(xt), the counterfactual output when only the text is provided. The text-only output Y0(xt) can be calculated as follows: yi Y0(xt) softmax [logθ (yi xt)] , The multi-modal output Y1(x) can be calculated as follows: yi Y1(x) softmax [logθ (yi xv, xt)] , (2) (3) To quantify the discrepancy between model outputs Y0(xt) and Y1(x) under multi-modal and textonly inputs, we compute the consistency of these model outputs with the ground truth Yg. If the models output matches the ground truth in given condition, we assign value of 1; otherwise, value of 0. The discrepancy D(x) for each sample is then quantified as the normalized difference in the number of correct predictions between these conditions, which can be formulated as: D(x) = [I(Y1 = Yg) I(Y0 = Yg) x] = 1 i=1 (cid:88) (cid:104) I(Y1(x(i)) = (i) ) I(Y0(x(i) ) = (i) (cid:105) ) , (4) where is the number of rollout outputs generated for each sample set in GRPO, and I() is the indicator function that equals 1 if the condition is true, and 0 otherwise. Based on the mean and standard deviation of discrepancies across all samples, we set threshold for the discrepancy score. Specifically, samples with discrepancy less than µc + λc σc, where µc is the mean discrepancy, σc is the standard deviation, and λc is tunable hyperparameter, are excluded from the training set. 3.2 Attention Confidence Estimator While the CDE effectively identifies samples requiring multi-modal reasoning from an output-level perspective, it does not assess the quality of internal reasoning processes. Recent studies [36, 37] 4 reveal an insightful phenomenon: tokens that receive excessive attention weights can dominate the prediction without using meaningful semantic context. Motivated by this, we explicitly quantify the internal reasoning quality via self-attention distributions, thus filtering out attention-biased samples. Attention Confidence Formulation. Given an input = (xt, xv) to transformer-based MLLM, we denote the self-attention matrix Rdd from its final transformer layer as: Ai,j = softmax (cid:32) (cid:33) , QiK (5) where Qi, Kj Rd represent the query and key vectors for token positions and j. To systematically characterize attention-bias patterns, ACE analyzes the entire self-attention matrix A. An attention-biased pattern at token position is identified if the corresponding attention column exhibits pronounced concentration of attention weights, identifying excessive reliance on single token. Formally, the degree of attention bias at position j, ψj(A) is quantified by computing multiplicative attention score across subsequent token interactions: ψj(A) = (cid:89) (σ Ai,j), (6) i=j where σ is scaling factor ensuring numerical stability and emphasizing prominent attention patterns. denotes the total length of the input sequence. The ψj(A) metric effectively quantifies attention confidence, with elevated values indicating unreliable multi-modal reasoning. We define position as attention-biased if ψj(A) exceeds threshold λa. Instances containing more than one attentionbiased position are filtered out as attention-biased samples. 3.3 Difficulty-aware Replacement Module Although CDE and ACE select valuable samples, they inevitably limit the reasoning upper bound due to the exclusion of challenging yet informative samples. For example, in the CDE selection process, if the output discrepancy threshold is set above 0.2, scenario may arise where text-only model consistently produces incorrect outputs across all five trials, whereas multi-modal outputs succeed once in five trials. Such challenging yet valuable samples, which could significantly contribute to improving reasoning, are thus discarded. This exclusionary process reduces the complexity of the training data, thereby constraining the model to achieve more complex reasoning ability. To address this, we introduce Difficulty-aware Replacement Module (DRM) to refine the selected sample. First, we define the difficulty score Di if to quantify the challenge of correctly answering sample, which can be defined as: Di if = 1 (cid:80)M j=1 ci,j , (7) where ci,j denotes the correctness of the j-th rollout generation for the i-th sample, and is the total number of outputs in the group. higher Di if indicates greater difficulty. In particular, the DRM involves two steps: First, we exclude easy samples, denoted as Di if = 0, which are characterized by consistent correct answers across all trials. Second, we reintroduce hard samples that have been previously discarded due to difficulty but are still valuable for training. Specifically, based on the difficulty score Di if , the set of reintroduced samples Shard is given by: Shard = argmaxk (cid:18) {xi Di if [ 1 , 1), I(ψi(A) > λa) = 0} (cid:19) , (8) where is the number of easy samples. This formulation identifies the top-k most challenging samples according to the difficulty metric Di if , while excluding those that do not meet the specified criteria. Note that the hardest samples, i.e., Di = 1, would be neglected, as they are demonstrated to be meaningless for training [29]. This DRM can enhance the upper bound of the models ability to handle complex tasks, without introducing data redundancy and training costs. Finally, by filtering through CDE and ACE and refining with DRM, we ensure that the model is trained with cognitive samples xcd, thereby enhancing the multi-modal reasoning ability, while simultaneously reducing training costs and data redundancy. 5 Table 1: Comparison with state-of-the-art methods. Experiments are conducted using the Qwen2.5VL-3b [1] and Qwen2.5-VL-7b [1], employing GRPO as the RL method. Time denotes the total computation cost, including data selection and training. Bold font denotes the best result. Method - Qwen2.5-VL-7b 54,931 Qwen2.5-VL-7b-Full 1,000 Qwen2.5-VL-7b-s1 [8] (2025) Qwen2.5-VL-7b-LIMO [12] (2025) 4,093 8,136 Qwen2.5-VL-7b-LIMR [9] (2025) Qwen2.5-VL-7b-RAP (Ours) 5,159 - Qwen2.5-VL-3b Qwen2.5-VL-3b-Full 54,931 1,000 Qwen2.5-VL-3b-s1 [8] (2025) Qwen2.5-VL-3b-LIMO [12] (2025) 2,679 21,303 Qwen2.5-VL-3b-LIMR [9] (2025) Qwen2.5-VL-3b-RAP (Ours) 4,374 Sample Time (h) MathVista MMStar MathVerse WeMath MMVet LogicVista Avg. 50.61 39.31 54.32 48.43 53.01 45.79 52.58 45.74 55.19 48.02 56.04 48.65 39.63 9.01 46.64 38.35 45.78 37.41 44.77 35.33 45.69 35.43 47.58 39. 59.13 60.51 61.05 59.08 62.86 63.31 51.81 53.41 53.02 53.12 53.25 54.63 44.52 46.09 45.86 44.74 45.81 46.53 39.59 40.03 39.59 39.14 40.95 41.61 68.70 70.70 68.50 69.90 71.10 73.20 61.30 64.50 62.60 61.80 63.10 64.90 56.07 61.53 61.80 61.33 62.12 62.53 54.46 55.25 54.53 54.73 54.66 55.67 35.90 38.67 35.05 34.67 41.21 42.00 21.62 28.29 27.52 24.48 26.76 29.33 - 93.2 55.9 111.9 122.0 52.8 - 46.5 38.4 120.5 60.9 32."
        },
        {
            "title": "4 Experiments",
            "content": "Training dataset. Main results in the Table 1 are based on models trained with the MM-Eureka dataset [23], high-quality multi-modal dataset for mathematical reasoning. To further validate the generalization of our RAP method, we evaluate models trained on the subset of Mulberry-260k dataset [22], multi-modal learning-to-reason-and-reflect dataset. Evaluation. Similar to [22, 23], we evaluate models on both mathematical and general multi-modal reasoning tasks using the pass@1 metric, where pass@1 measures the percentage of problems correctly solved on the first attempt, under zero-shot setting. For mathematical reasoning, we assess the models ability on four benchmarks: MathVista [38], MMStar [39], MathVerse [40], and WeMath [41]. For universal multi-modal reasoning, we evaluate on MMVet [42] and LogicVista [43]. Implementation details. Following prior methods [23, 22], we conduct our primary experiments on mathematical reasoning tasks, employing Qwen2.5-VL-3B and Qwen2.5-VL-7B [1] as baseline models. First, we apply RAP to select cognitive samples using the initial model without any training. These samples are then used to train models within the EasyR1 [44], employing the AdamW optimizer with learning rate of 1e-6. Full-data training requires 1 epoch (107 steps), all others use 40 training steps with batch size of 512 across 8 GPUs. The system prompt is available at our supplementary material. For accelerated generation in GRPO [3], we utilize the vLLM package [45]. Finally, we set the hyperparameters σ to 2.0, λc and λa to 0.5 and 0.1 for the CDE and ACE, respectively. 4.1 Overall Comparison Results Comparing methods. We compare our approach with existing data selection methods, including: 1) s1 [8], which utilizes the large-scale MLLMs to identify high-quality data; 2) LIMO [12] that designs difficulty-aware selection method to identify crucial samples; and 3) LIMR [9], which employs learning impact measurement to select subset of training samples. Moreover, we also evaluate models trained on the full dataset (Full) as the baseline. Comparisons with state-of-the-art methods. We compare our RAP model with the latest dataefficient methods on six diverse datasets, including LIMO [12], s1 [8] and LIMR [9]. The results shown in Table 1 reveal several key findings: 1) RAP consistently outperforms models trained with full corpora on all datasets. Remarkably, these improvements are achieved using only 9.5% or 7.9% of training data while reducing training time by 43% or 31%, supporting our hypothesis truth in the few that selected cognitive samples can achieve more effective multi-modal reasoning. 2) Moreover, the RAP method demonstrates significant improvement of 7.33% and 6.95% over the LIMO [12] and s1 [8] methods on WeMath [41], respectively, which rely on data quality and manual selection. Such results indicate that focusing on the potential of each sample to improve multi-modal reasoning. Effectiveness of RAP on different base models. As shown in Table 2, our method consistently surpasses other recent data selection methods when applied to the base model InternVL3-2b [46]. This outcome highlights the broad applicability and generalizability of the RAP framework, as the introduced CDE and ACE components effectively select training samples that activate the models multi-modal reasoning capability. Crucially, these components do not rely on exploiting modelspecific inductive biases, thereby ensuring RAPs adaptability on wide range of model architectures. More results are available in our supplementary material. 6 Table 2: Comparison with state-of-the-art methods using Qwen2.5-VL-7b [1] with the RL method RLOO [18], evaluating our RAP method on different training datasets and RL algorithms. MathVista MMVet We-Math Avg. Method 54.58 Qwen2.5-VL-7b 55.46 Qwen2.5-VL-7b-Full 54.51 Qwen2.5-VL-7b-s1 [8] 54.71 Qwen2.5-VL-7b-LIMO [12] 55.45 Qwen2.5-VL-7b-LIMR [9] 55.86 Qwen2.5-VL-7b-RAP (Ours) 68.70 69.10 68.50 68.80 68.90 69.20 35.90 36.95 35.06 35.23 36.74 37.05 59.13 60.32 59.96 60.11 60.71 61. Table 3: Comparison with state-of-the-art methods using InternVL3-2b [46] with the RL method GRPO [3], evaluating our RAP method across different model architectures. Method InternVL3-2b InternVL3-2b-Full InternVL3-2b-s1 [8] InternVL3-2b-LIMO [12] InternVL3-2b-LIMR [9] InternVL3-2b-RAP (Ours) MathVista MMVet We-Math Avg. 42.13 43.30 42.97 42.81 43.62 44.16 56.10 57.20 56.80 56.70 57.10 57.40 12.06 12.84 11.63 11.92 12.44 13.05 58.22 59.86 60.47 59.82 61.33 62.02 Effectiveness of RAP for various RL methods and training datasets. To further validate the generalizability of RAP, Table 3 presents results using the Qwen2.5-VL-7B model, trained under two different configurations: 1) the RLOO RL paradigm [18], and 2) the reduced Mulberry-10K dataset, subset of Mulberry-260K [22]. Despite these variations, RAP maintains consistent superiority compared to other methods, suggesting its generalization to different RL strategies and training datasets. We attribute this robustness to cognitive samples that facilitate genuine multi-modal reasoning rather than simply fitting data distributions."
        },
        {
            "title": "4.2 Further Analysis",
            "content": "Ablation study. As presented in Table 4, we list the following conclusions: 1) The comparison between No.0 and No.3 indicates that integrating CDE and ACE can improve multi-modal reasoning in MLLMs. These results underscore the efficacy of RAP in eliminating language-prior biased and attention-biased samples, thereby enabling models to focus on essential cognitive samples and improving reasoning performance. 2) Comparing No.1 with No.3 shows that only using the CDE can improve performance, but worse than the full RAP. We hypothesize the reason is that while CDE identifies critical samples by detecting output discrepancies, it overlooks the intermediate reasoning process. 3) Moreover, the comparison between No.2 and No.3 demonstrates that the RCC can further refine the reasoning performance by replacing easy samples with more appropriate hard samples. Such improvements are due to that the DRM addresses limitation of the CDE and ACE approaches, i.e., the tendency to retain simpler instances while neglecting challenging yet informative samples. Table 4: Ablation study of RAP using Qwen2.5-VL-7B, trained using GRPO method for 40 steps. No. CDE ACE DRM MathVista MMStar MathVerse MMVet 58.91 - 0 1 60.92 60.68 2 3 61.74 61.33 4 5 62.78 6 63.31 Comparison on cross-modal reasoning utilization. We evaluate the effectiveness of cognitive samples in enhancing cross-modal reasoning utilization on multi-modal reasoning tasks. We define cross-modal reasoning utilization as the proportion of instances in the MMStar dataset where the model correctly uses multi-modal inputs to answer questions, but fails when relying solely on textual inputs. As shown in Figure 5(c), models trained with cognitive samples show superior integration of cross-modal information, compared to baseline and the latest method LIMR [9]. Such results highlight that the proposed CDE can effectively reduce models reliance on superficial linguistic prior by filtering out training samples exhibiting excessive language bias. Hence, models are encouraged to discover the relationship between image and text, thus improving the cross-modal reasoning utilization. 46.07 47.13 46.86 47.82 47.90 48.74 48.65 59.32 60.64 60.21 61.28 60.93 61.76 62. 69.10 70.80 70.20 72.00 71.50 72.60 73.20 - - Comparison on cross-model generalization. To examine the cross-model generalizability of cognitive samples identified by RAP, we evaluate whether cognitive samples obtained using the Qwen2.5-VL3b are useful for improving the reasoning of distinctly structured model, InternVL3-2b, and vice versa. Comparative results presented in Figure 4 demonstrate that cognitive samples selected by our RAP method outperform the latest method LIMR [9], confirming the generalization of RAP in enhancing multi-modal reasoning for varying model frameworks. Note that we exclude comparisons with LIMO [12] due to its similarity with s1 [8]. Figure 4: Cross-model generalization of cognitive samples selected by RAP. Performance with InternVL3-2B trained on samples from Qwen2.5-VL-3B (left), and vice versa (right). 7 Figure 5: (a) Visualization of output discrepancies between multi-modal and text-only inputs on the full MM-Eureka training dataset. (b) Performance variation with respect to the hyperparameters λa and λc on MMstar. (c) Comparative analysis of multi-modal reasoning utilization on four datasets. Figure 6: Illustration of (a) characteristics of cognitive samples selected by our RAP method and (b) Comparison of the reasoning processes between our RAP method and the state-of-the-art LIMR [9]. Analysis on hyperparameter sensitivity. We further investigate the sensitivity of the hyperparameters λc and λa employed within the CDE and ACE components, respectively. As shown in Figure 5(a), we visualize the distribution of output discrepancies between multi-modal and text-only inputs on the full MM-Eureka dataset, revealing significant presence of samples where multi-modal reasoning is not necessary to solve the task. Experimental results depicted in Figure 5(b) suggest that optimal performance is achieved with λc = 0.1 and λa = 0.5. Performance degradation is observed when both parameters fall below these optimal thresholds, with values lower than 0.1 for λc and 0.5 for λa leading to significant deterioration. This decline is attributed to the inherently uneven distribution of the output-level discrepancy and attention confidence, which are concentrated at the extremes. Qualitative analysis. We provide qualitative analysis by visualizing cognitive samples and comparing the reasoning processes of our RAP with the latest LIMR method as follows: 1) Visualization of cognitive samples. As shown in Figure 6(a), we present typical example of the cognitive samples selected by our RAP method, where the model effectively integrates both geometric information and logical reasoning to infer the area. This case suggests that the cognitive samples selected by our RAP method exhibit two important characteristics: (a) the necessary of multi-modal information, as evidenced by the significant discrepancy between reasoning outcomes using multi-modal and text-only inputs; and (b) the avoidance of overemphasis on irrelevant or meaningless tokens, ensuring that the model focuses on the most informative features for accurate reasoning. 2) Comparison case analysis. As shown in Figure 6(b), we compare the reasoning process of our RAP method with that of the state-of-the-art LIMR approach [9]. For example, LIMR fails to integrate cross-modal information, leading to incorrect computations of both the central angle and the inscribed angle. In contrast, the model trained using samples selected by RAP correctly applies geometric principles and multi-modal integration to arrive at the correct solution. These comparisons underscore the advantage of training with cognitive samples derived through our RAP method, which enables the model to effectively leverage multi-modal information, resulting in more reliable reasoning outcomes. 4.3 Key Insights and Discussion Effect of RAP on reasoning upper bound. As illustrated in Figure 7(a), our RAP method converges faster than the baseline, achieving optimal performance within 40 training steps compared to 100 for the baseline. These results demonstrate RAPs efficiency in enhancing multi-modal reasoning while reducing training overhead and data redundancy. However, further analysis reveals differences between the full RAP model and its variant using only ACE and CDE, particularly during later training stages. This occurs because ACE and CDE inevitably retain easy examples, while discarding 8 Figure 7: (a) Trade-off analysis between efficiency and performance and effect of data selection on upper bound. (b-c) Impact of varying the proportion of samples with different difficulty levels on reasoning. (c) Comparison with our RAP method and RAP augmented with dynamic selection. challenging yet informative samples. For example, multi-modal examples predicted correctly in only one out of five attempts but consistently mispredicted under text-only conditions may be mistakenly eliminated when the discrepancy threshold λc exceeds 0.2. Such filtering reduces dataset complexity, limiting the models overall reasoning upper bound. To address this, we propose the Difficulty-aware Replacement Module (DRM) to explicitly replace easy samples with these informative yet challenging examples, thus elevating reasoning performance in later training stages. Why does less data outperform the full dataset? The above empirical analysis shows that models trained on carefully curated data can notably outperform those trained on the entire dataset. To further elucidate the underlying mechanisms driving this truth in the few phenomenon, we examine sample difficulty distributions quantified by group-wise response accuracy in the GRPO generation paradigm in Figure 7(b). The results reveal that cognitive samples contain significantly fewer easy samples compared to the full dataset. Therefore, under conventional large-scale training, which usually restricts training to 1 or 2 epochs, the models lack sufficient repeated exposure to challenging samples, thus limiting the improvements brought from these valuable instances. Moreover, to further validate our hypothesis, we conduct comparison in Figure 7(c), where models are trained on equivalent numbers of easy, hard, and cognitive samples. The results reveal that the abundance of easy examples contributes little to advancing the models reasoning capability and predominantly introduces redundant information. While challenging samples yield better performance than easy samples, they still fall short compared to the cognitive samples selected through our RAP method. We argue that the superiority of cognitive samples arises not merely from sample difficulty but from their ability to activate the models multi-modal reasoning capacity. In multi-modal contexts, the training samples must facilitate the activation of the models multi-modal reasoning mechanisms. Discussion on potential optimization during RL training. The static nature of our current RAP method motivates us to explore potential optimizations through the dynamic adjustment of the training dataset during RL training. Specifically, we examine the evolving distribution of cognitive samples identified by RAP criteria across successive epochs. Initially, as depicted in Figure 7(d), approximately 5,000 samples meet the cognitive sample criteria. However, the number of samples that continues to satisfy this criterion progressively declines as training advances. This observed trend suggests the promise of adaptive data sampling, wherein cognitive samples are re-sampled dynamically from the entire dataset after the first and third training epochs. Preliminary experiments indicate that such adaptive strategies yield tangible improvements in model performance. However, despite these advantages, the re-screening of the entire dataset after each epoch incurs significant computational overhead. As result, our final approach strategically refrains from adopting this adaptive data selection in order to maintain computational efficiency. This highlights the inherent trade-off between performance enhancements and computational cost when incorporating dynamic data selection strategies into the RL training process."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce Reasoning Activation Potential (RAP) data selection paradigm, which reduces training costs and improves multi-modal reasoning in MLLMs. RAP utilizes the Causal Discrepancy Estimator (CDE) and Attention Confidence Estimator (ACE) to effectively eliminate attention-biased samples and language-prior biased samples, leading to more accurate and efficient reasoning. For future work, we plan to investigate the efficacy of RAP in SFT training and introduce dynamic mechanisms to further improve the efficiency of multi-modal reasoning."
        },
        {
            "title": "References",
            "content": "[1] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [2] OpenAI. Gpt-4 technical report, 2023. [3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [4] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [5] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. CoRR, abs/2503.10615, 2025. [6] OpenAI. Learning to reason with llms, 2024. [7] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [8] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. CoRR, abs/2501.19393, 2025. [9] Xuefeng Li, Haoyang Zou, and Pengfei Liu. LIMR: less is more for RL scaling. CoRR, abs/2502.11886, 2025. [10] Yudong Han, Liqiang Nie, Jianhua Yin, Jianlong Wu, and Yan Yan. Visual perturbation-aware collaborative learning for overcoming the language prior problem. CoRR, abs/2207.11850, 2022. [11] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In CVPR, pages 1387213882, 2024. [12] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. LIMO: less is more for reasoning. CoRR, abs/2502.03387, 2025. 10 [13] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Rémi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, volume 238, pages 44474455. PMLR, 2024. [14] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. CoRR, abs/2406.18629, 2024. [15] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust DPO: aligning language models with noisy feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. [16] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. [18] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In ACL, pages 1224812267, 2024. [19] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. LMM-R1: empowering 3b lmms with strong reasoning abilities through two-stage rule-based RL. CoRR, abs/2503.07536, 2025. [20] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [21] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [22] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and Dacheng Tao. Mulberry: Empowering MLLM with o1-like reasoning and reflection via collective monte carlo tree search. CoRR, abs/2412.18319, 2024. [23] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. CoRR, abs/2503.07365, 2025. [24] Raanan Y. Rohekar, Yaniv Gurwicz, and Shami Nisimov. Causal interpretation of self-attention in pre-trained transformers. In NeurIPS, 2023. [25] Congzhi Zhang, Linhai Zhang, and Deyu Zhou. Causal walk: Debiasing multi-hop fact verification with front-door adjustment. In AAAI, pages 1953319541, 2024. [26] Bernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012. [27] Donald Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American statistical Association, 100(469):322331, 2005. [28] Tianyu Du, Ayush Kanodia, Herman Brunborg, Keyon Vafa, and Susan Athey. LABORLLM: language-based occupational representations with large language models. CoRR, abs/2406.17972, 2024. [29] Qihan Huang, Long Chan, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, Jingyuan Chen, Chang Yao, and Jie Song. Boosting MLLM reasoning with text-debiased hint-grpo. CoRR, abs/2503.23905, 2025. 11 [30] Donald Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational Psychology, 66(5):688, 1974. [31] Fredrik D. Johansson, Uri Shalit, and David A. Sontag. Learning representations for counterfactual inference. In ICML, volume 48, pages 30203029, 2016. [32] Uri Shalit, Fredrik D. Johansson, and David A. Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In ICML, volume 70, pages 30763085, 2017. [33] Judea Pearl. Causal inference in statistics: An overview. 2009. [34] Gemma Hammerton and Marcus Munafò. Causal inference with observational data: the need for triangulation of evidence. Psychological medicine, 51(4):563578, 2021. [35] Jason Abrevaya, Yu-Chin Hsu, and Robert Lieli. Estimating conditional average treatment effects. Journal of Business & Economic Statistics, 33(4):485505, 2015. [36] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. OPERA: alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In CVPR, pages 13418 13427, 2024. [37] Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In EMNLP, pages 98409855, 2023. [38] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. [39] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models? In NeurIPS, 2024. [40] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, Peng Gao, and Hongsheng Li. MATHVERSE: does your multi-modal LLM truly see the diagrams in visual math problems? In ECCV, pages 169186, 2024. [41] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? CoRR, abs/2407.01284, 2024. [42] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In ICML, 2024. [43] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal LLM logical reasoning benchmark in visual contexts. CoRR, abs/2407.04973, 2024. [44] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [45] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [46] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CVPR, 2024."
        }
    ],
    "affiliations": [
        "Meituan",
        "Salesforce AI Research",
        "School of Computer Science and Technology, Tongji University",
        "University of Electronic Science and Technology of China"
    ]
}