{
    "paper_title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding",
    "authors": [
        "JingLi Lin",
        "Chenming Zhu",
        "Runsen Xu",
        "Xiaohan Mao",
        "Xihui Liu",
        "Tai Wang",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: https://rbler1234.github.io/OSTBench.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 8 9 7 0 . 7 0 5 2 : r OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding JingLi Lin1,2, Chenming Zhu1,3, Runsen Xu1,4, Xiaohan Mao1,2, Xihui Liu3 Tai Wang1, Jiangmiao Pang1 1Shanghai AI Laboratory, 2Shanghai Jiao Tong University, 3The University of Hong Kong, 4The Chinese University of Hong Kong Equal contribution Co-corresponding https://rbler1234.github.io/OSTBench.github.io/ Figure 1: OST-Bench is designed from the perspective of an embodied agent dynamically exploring static indoor environments, with focus on online and spatio-temporal understanding. Compared to the conventional offline setting (top right), which answers questions based on fixed-length video of the scene, the bottom section illustrates our online setting: for the same question, the agents answers evolve as it explores the scene, changing from blue (t1) to red (t2) to green (t3), reflecting its continuously updated understanding."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with fixed set of pre-recorded inputs, we introduce OST-Bench, benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OSTBench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex cluebased spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the Preprint. Under review. core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available at https://github.com/rbler1234/OST-Bench."
        },
        {
            "title": "Introduction",
            "content": "In the real world, humans continuously perceive and update their understanding of the environment through sequential visual observations. At every moment, we are aware of our spatial state and how it evolves with respect to surrounding objects and scenes. We expect embodied agents to possess similar online scene understanding capabilities. For instance  (Fig. 1)  , in an embodied navigation task[6, 49, 12, 27, 41], an agent should be able to incrementally construct representation of its surroundings (\"I have seen brown pillow on the bed in the bedroom.\"), track its current status (\"I am now in the living room next to the bedroom, facing south.\"), and reason about dynamic spatial relationships (\"The brown pillow is now on my rear left.\"). Such awareness enables the agent to instantly respond to commands ( \"Go and get the brown pillow.\") and take correct actions. Recent advances in multimodal large language models (MLLMs)[9, 18, 29, 38, 28, 46, 25] have shown remarkable capabilities in integrating vision and language for complex reasoning. However, most existing benchmarks [15, 30, 31, 8, 23, 5], evaluate models under offline settings, where reasoning is performed over fixed set of pre-recorded inputs, such as reconstructed 3D scenes or images and videos, this does not capture the online nature of embodied tasks. To address this gap, we introduce OST-Bench, benchmark designed to evaluate Online SpatioTemporal understanding from the perspective of an agent actively exploring scene. The term Online emphasizes the agents need to perceive, remember, and reason over incrementally received observations, rather than complete, pre-recorded scene data. The term Spatio-Temporal highlights the need to integrate current visual observation with historical memory to support dynamic spatial reasoning. To more accurately simulates real-world embodied perception, OST-Bench defines task from these perspectives: the agent, its surrounding environment, and their relationship, contains three main categories  (Fig.2)  : (1) Agent State: the agents understanding of its own state, (2) Agent Visible Info: the agents dynamic interpretation of visible scene information, and (3)Agent-object Spatial Relationship: the agents dynamic understanding of spatial relationships with objects, all posed in an online, temporally grounded fashion. OST-Bench comprises 1.4k real-world scenes sourced from the test and validation splits of ScanNet[20], Matterport3D[14], and ARKitScenes[11], accompanied by 10k QA pairs covering diverse range of subtypes. Our benchmark provides rigorous testbed for assessing the online spatio-temporal reasoning ability of MLLMs in realistic, embodied settings. We evaluate leading MLLMs on OST-Bench and find its online spatio-temporal nature poses significant challenges for the models, even the most advanced models lag behind human performance by over 30%. Models perform poorly on tasks requiring complex spatio-temporal reasoning, with accuracy declining as exploration steps increase and memory grows under the online setting. Based on an in-depth experimental analysis, we observe phenomenon which we term Spatio-temporal Reasoning Shortcut-when reasoning over long-term memory, models tend to avoid retrieving key information, instead taking shortcuts and relying on shallow, unsupported inferences; further, we design four tasks with different levels of difficulty to better delineate the models capability limits, along both the spatial dimension (from singleto multi-step spatial reasoning) and the temporal dimension (from keyframeto sequence-baesd context), and observe clear performance drop on both two dimensions. This reveals that both complex clue-based spatial reasoning and long-term memory retrieval are two distinct weaknesses that hinder the models performance on OST-Bench, highlighting the core challenges that must be addressed to advance online embodied reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Spatial Reasoning Benchmarks. Early scene understanding benchmarks[8, 31, 23, 30, 47, 50] introduced diverse task taxonomies to comprehensively evaluate various aspects of visual scene interpretation, with spatial understanding consistently recognized as the most fundamental component. Benchmarks such as ScanQA[8], SQA3D[31], SceneVerse[23], and MMScan[30] emphasized semantic understanding and incorporated object locations and spatial relations, they largely treated spatial relationships as semantic attributes, focusing primarily on complex spatial semantics rather than 2 Dataset ScanQA [8] SQA3D [31] SceneVerse [23] MMScan [30] SpatialRGPT-Bench [19] CV-Bench [37] VSI [43] OST-Bench Input Modality Video/PC. Video/PC. Video/PC. Video/PC. Image Image Video Video Settings Spatio-Temporal Awareness Output Format Text Num. Offline Offline Offline Offline Offline Offline Offline Online Table 1: Comparison with other spatial reasoning datasets. \"PC.\" abbrev for \"Point cloud\". \"Text\" and \"Num.\" represent whether the output is string or numerical value. Compared to other benchmarks, ours is clearly distinguished by its focus on the online setting and the requirement for spatio-temporal awareness in models. explicitly targeting spatial reasoning. With the rapid advancement of Multimodal Large Language Models (MLLMs), recent benchmarks have begun to place greater emphasis on spatial reasoning evaluation, SpatialR-GPT[19] and CV-Bench[37] require models to reason about 3D information, such as depth and distance from single image, VSI[43] proposed finer-grained categorization of spatial reasoning tasks, systematically evaluating models ability to infer 3D scene layouts from 2D video inputs, covering both relative and absolute spatial relationships. Existing spatial reasoning benchmarks predominantly operate in an offline setting, focusing on static scenes and requiring models to perform reasoning over fixed set of images or videos of predefined length. In contrast, our OST-Bench adopts an online setting, emphasizing dynamic scene understanding from an agent-centric perspective, and offers an alternative perspective for evaluating spatial reasoning capabilities. It includes wider range of complex question types to assess more diverse and fine-grained spatio-temporal reasoning abilities. Video Benchmarks for Temporal Understanding. Video benchmarks for temporal understanding require models to reason over both temporal and visual dimensions. Early efforts in video temporal understanding primarily focused on semantic comprehension from third-person perspective[42, 44, 21, 40], mostly without considering 3D spatial perception. More recent benchmarks, driven by embodied task settings, have introduced characteristics such as: (1) egocentric perspective[22, 32, 13], where tasks are presented from first-person viewpoint, (2) online inference[45, 16, 26, 16], requiring online processing of continuously streaming video input, and (3) spatial understanding[32, 26, 13], which evaluates models awareness of spatial elements. However, spatial tasks in these benchmarks are often limited to 2D relationships or short-term motion cues, reflecting more of content-level understanding rather than deeper spatial reasoning, lacking complex 3D spatial reasoning that requires integrating multi-view 2D observations into coherent 3D representation. In contrast, OST-Bench is an egocentric, online temporal video benchmark that uniquely emphasizes 3D spatial reasoning, core ability for real-world embodied tasks such as navigation and exploration."
        },
        {
            "title": "3 OST-Bench\nIn this section, we present our comprehensive methodology for establishing OST-Bench, which\ncomprises three core components: task formulation, the data collection and processing pipeline, and\nbenchmark sample generation.",
            "content": "3.1 Task Formulation Before introducing OST-Bench, we clarify the assumptions underlying our formulation of scene understanding. (1) While existing real-world datasets predominantly feature static scenes, we specifically focus on static environments in our current benchmark design, meaning the positions and states of objects remain unchanged during exploration; the agent is the only dynamic element. (2) There is no defined absolute coordinate system in the scene, so all spatial references are defined relative to an anchor such as an object, viewpoint, or the agent itself. As result, the position of any object or agent cannot be defined in isolation. All spatial measurements fall into four categories: relative distance, absolute distance, relative direction, and absolute direction. 3 static scene consists of set of immobile objects, and understanding such scene involves reasoning about individual entities and their relationships[30], such as the object attribute(intrinsic properties of individual objects, including category, color, material, shape, size, and function), the attribute / spatial relationship between objects, and the spatial relationship between objects and given viewpoint (provided either textually or via virtual camera input). When dynamic agent is introduced, it introduces new relational dynamics and opens up additional avenues for investigation. These can be categorized into three main categories that form the core focus of our benchmark evaluation: (1) Agent state: The position and orientation of the agent, which continuously change as the agent explores. (2) Agent visible info: The perceptual information available from the agents point of view at given moment includes the existence of visible objects, their count, diversity, and the timing of their appearance. The information visible to the agent is continuously updated as the agent explores the scene. (3) Agent-object spatial relationship: 3D spatial relations between the agent and objects, described by relative or absolute distance/direction, constantly change as the agent explores. 3.2 Meta-dataset Collection and Processing Base Dataset Acquisition. The three real scene datasets, ScanNet[20], ARKitScenes[11] and Matterport3D[14], contain rich scene information along with RGB-D videos/images and their corresponding camera information, totaling 7.6k scenes. Building on this foundation, EmbodiedScan[39] provided large number of high-quality 9-DOF bounding box annotations for the objects in these scenes. MMScan[30] further enriched these scenes with large number of highly quality, manually annotated objectand region-level semantic annotations. We selected total of 1.4k scenes from the validation/test splits of these three datasets and constructed our dataset based on the annotations from EmbodiedScan and MMScan. Exploration Route Generation. To construct an agent-centric exploration dataset, we require firstperson videos of environments accompanied by camera parameters. While ScanNet and ARKitScenes provide such first-person videos along with camera pose data suitable for modeling agent trajectories, Matterport3D offers only multi-view images without continuous exploration paths. To address this limitation, we generate synthetic exploration trajectories within Matterport3D by constructing graph of camera viewpoints and applying the minimum-spanning tree algorithm[35]. This ensures coherent movement and obstacle-free transitions between connected nodes. To maintain observation continuity, we enforce an image-overlap threshold between adjacent viewpoints. This approach enables us to simulate first-person exploration videos for Matterport3D scenes, complete with associated camera parameters. Visible Information Processing. OST-Bench requires fine-grained visibility annotations at the frame level, which we define in two forms: attribute visibility and spatial visibility. Attribute visibility refers to the ability to determine the existence of an object based on single frame. Even if an object is partially visible in frame, as long as its visible portion is sufficiently large, you can infer attributes such as the objects type or color. Spatial visibility is used to generate questions in the OST-Bench that are related to the objects 3D spatial information. Therefore, for spatially visible objects, in addition to being attribute visible, we require that their center position, size, shape, and other spatial information can be inferred from observation. In practice, the attribute and spatial visibility of an object are determined by thresholding the projected area of its point cloud and the visibility of the vertices of its 9-DoF bounding box. Additional implementation details are provided in Appendix A.2. 3.3 Benchmark Samples Generation Rule-based Generation. OST-Bench is designed in multi-round dialogue format. In each round, the model receives sequence of newly observed, temporally ordered frames, which are appended to all previously seen frames to simulate streaming video input. At the end of the round, new question is posed based on the accumulated observations. As the dialogue progresses, the input sequence grows incrementally, requiring the model to perform reasoning over an expanding spatio-temporal context. All questions are framed from an online perspective, grounded in the agents current situation. Our questions span three major categories: Agent State, Agent Visible Info, and AgentObject Spatial Relationships. As illustrated in Fig.2, each main category contains multiple subtypes. Across all categories, the questions fall into four general formats: Judgment, Counting, Temporal Localization, and Estimation. Judgment questions evaluate the models qualitative understanding of factswhether something is true or not, or whether something has occurred; Counting questions assess the models Figure 2: OST-Bench categorizes questions into three main categories. Each main category includes several subtypes; in total, the benchmark comprises 15 fine-grained question subtypes. ability to quantitatively enumerate information; Temporal Localization questions test the models ability to locate events along the time axis. (We use the round index as discrete timestamp in OST-Bench); Estimation questions evaluate the models ability to approximate measurable quantities, such as physical distances or angular differences. Based on the processed meta-datasets, we define dedicated rule-based generation templates to construct corresponding data samples for each subtype within the three main categories. Detailed generation procedures for each fine-grained subtype, including rule definitions and templates used, are provided in Appendix A.3. Several representative samples of these subtypes are illustrated in Fig.2. Our benchmark comprises approximately 1.4k test and validation scenes selected from ScanNet, Matterport3D, and ARKitScenes. For each scene, we generate single agent exploration trajectory. Along each trajectory, multiple dialogue rounds are defined, each containing single question, resulting in total of 10k questions across the dataset. Data Quality. Ensuring high-quality benchmark data is crucial. Based on the high-quality manual annotations from Embodiedscan and MMScan, we design and iteratively refine tailored rule-based generation strategies for each subtask to ensure semantic validity, robustness, and clarity, avoiding common corner cases and ambiguities. To assess dataset quality, we employ rigorous validation protocol in which questions are randomly sampled for manual review. Samples lacking sufficient information or containing incorrect answers are marked as invalid. Human evaluation results confirm that the dataset meets our strict quality standards, with an error rate below 5%, thereby ensuring reliable and high-quality benchmark."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Benchmark Models & Evaluation Metrics We evaluate the performance of multiple multi-modal large language models (MLLMs), including both proprietary models (Claude-3.5-Sonnet[7], GPT-4o[33], GPT-4.1[34], Gemini-2.0-Flash[36], and its thinking variant) and open-source models (InternVL-2.5[17], QwenVL-2.5[10], LLaVAOnevision[24], and LLaVA-Video[48] of different scales). Each model is tested in zero-shot setting and conducts inference in multi-turn dialogue format. To establish performance boundaries, we include two baselines: human baseline and chance-level baseline. For the human baseline, we ensure that participants have no prior exposure to the test scenes. The chance-level method adopts random selection approach, randomly picks one answer from all possible choices for Judgement/Counting/Temporal-Localization questions, and for Estimation questions, it always outputs the mean value calculated from all potential numeric. As for the evaluation metrics, Judgement questions are considered correct if the model selects the same option as the ground truth. For 5 Agent State Agent Visible Info Quantity Diversity Order JUD. CNT. JUD. Agent-object Spatial Relationship Direction Distance JUD. TEMP. EST. JUD. TEMP. EST. Methods Proprietary Claude-3.5-Sonnet Gemimi-2.0-Flash Gemimi-2.0-Flash(Thinking) GPT-4o GPT-4.1 Open-source InternVL-2.5-8B InternVL-2.5-38B InternVL-2.5-78B QwenVL-2.5-7B QwenVL-2.5-32B QwenVL-2.5-72B LLaVA-Video-7B LLaVA-Video-72B LLaVA-Onevision-7B LLaVA-Onevision-72B Baseline Human-Level Chance-Level Position JUD. EST. Orientation JUD. EST. Existence JUD. TEMP. 65.1 59.7 57.4 55.6 64.2 51.7 56.7 60.8 49.8 51.0 57.0 50.4 51.0 53.8 53.8 93.2 50.0 36.2 27.3 36.3 20.5 30. 26.1 31.8 34.4 19.3 31.1 27.6 25.4 18.0 11.6 13.9 58.9 37.8 50.6 56.7 61.1 45.6 60.8 49.3 54.6 49.9 51.8 53.5 52.2 46.1 49.2 51.2 51.6 92.8 50.0 30.3 36.5 33.4 33.6 33. 40.4 38.4 40.7 40.8 39.4 37.1 12.1 41.6 7.7 36.2 54.4 39.3 85.8 89.6 88.1 90.6 90.8 86.2 91.7 90.7 78.6 85.3 86.1 90.4 88.0 90.0 89.0 95.7 50.0 67.0 70.8 74.8 75.6 78. 23.4 74.7 74.4 37.3 64.8 64.5 32.3 38.8 34.8 41.8 94.7 29.1 57.9 59.4 61.9 59.8 60.6 55.4 61.1 65.9 62.1 59.2 61.5 63.1 51.0 66.9 45.8 91.3 25.0 57.1 78.7 63.6 78.2 82. 60.5 79.8 77.9 56.3 73.4 75.7 66.5 70.9 51.1 74.8 94.4 33.0 60.0 55.6 73.1 59.6 70.8 38.4 62.1 61.2 28.5 41.8 34.5 39.3 53.7 33.4 56.6 90.9 25.0 39.2 42.0 50.9 46.1 51. 37.3 42.1 43.4 41.0 39.5 41.4 35.4 35.5 35.7 37.8 90.5 36.0 18.3 17.8 51.7 19.5 23.4 14.7 20.6 22.4 28.9 24.9 21.1 27.3 27.7 27.0 28.9 93.3 33.2 21.9 21.5 23.3 21.4 28. 22.8 27.7 17.8 12.2 25.7 8.2 16.2 30.9 38.1 27.3 54.3 47.6 43.8 45.5 51.1 43.1 44.9 43.0 42.7 46.7 44.9 43.6 44.5 41.3 43.8 43.5 48.2 93.4 36.0 54.9 48.9 56.8 50.6 53. 29.4 42.5 44.4 43.6 39.1 39.3 41.8 46.2 35.6 47.0 94.5 31.2 19.0 27.1 22.7 20.4 23.9 27.9 28.1 22.9 18.6 20.3 18.7 10.8 26.3 21.9 28.2 60.1 30.3 Table 2: Full evaluation results of OST-Bench. This table reports the performance of each model across all fine-grained question subtypes, \"JUD.\"/ \"CNT.\" / \"TEMP.\" / \"EST.\" abbrev for \"judgement\",\"counting\",\"temporal-localization\", and \"estimation\". Methods Proprietary Claude-3.5-Sonnet Gemimi-2.0-Flash Gemimi-2.0-Flash(Thinking) GPT-4o GPT-4.1 Open-source InternVL-2.5-8B InternVL-2.5-38B InternVL-2.5-78B QwenVL-2.5-7B QwenVL-2.5-32B QwenVL-2.5-72B LLaVA-Video-7B LLaVA-Video-72B LLaVA-Onevision-7B LLaVA-Onevision-72B Baseline Human Level Chance Level Avg A. State A. Info AO. JUD. TEMP. CNT. EST. 47.8 49.5 54.2 48.7 53.4 39.0 50.8 51.1 41.2 46.9 45.6 39.3 43.2 40.4 43.4 83.5 36. 45.6 45.1 47.1 38.8 47.2 41.9 45.4 46.5 40.4 43.8 43.5 33.5 40.0 31.1 38.9 74.8 44.3 65.6 70.8 72.3 72.8 76.5 52.8 73.9 74.0 52.6 64.9 64.5 58.3 60.5 55.2 61.6 93.4 32. 32.9 33.8 42.8 33.5 37.7 29.2 34.0 32.9 31.5 32.2 28.9 28.8 35.1 33.6 36.2 81.0 35.7 57.4 61.1 63.6 59.8 66.4 52.3 61.4 61.5 50.1 55.4 55.9 52.8 56.0 51.2 58.8 93.0 40. 46.7 45.8 61.1 48.6 51.7 22.5 45.9 47.1 36.6 42.9 41.6 33.8 37.6 32.5 39.2 94.2 31.2 57.9 59.4 61.9 59.8 60.6 55.4 61.1 65.9 62.1 59.2 61.5 63.1 51.0 66.9 45.8 91.3 25. 26.9 28.1 28.9 24.0 29.1 29.3 31.5 29.0 22.7 29.1 22.9 16.1 29.2 19.8 26.4 56.9 38.8 Table 3: Model performance across main categories and question formats. \"A.\" abbrev for \"Agent\" and \"AO.\" abbrev for \"Agent-Object Spatial Relationship\". The open-source and proprietary models with the highest and second-highest overall average scores are highlighted with bright green and light green marks. Counting and Temporal Localization questions, the models outputwhether number or turn indexmust exactly match the ground truth to be deemed correct. For Estimation questions, we adopt the Mean Relative Accuracy (MRA) metric from VSI[43] to score the similarity between the models floating-point output and the ground truth. 4.2 Main Results We report the performance of various models on our benchmark. Tab. 2 presents the performance of each model in all different subtype. Tab.3 summarizes the models overall performance, including their overall average scores, average scores for each of the three main categories, and performance across different question formats. Substantial Gap between MLLMs and Humans Performance. Model accuracy lags significantly behind human performance, with consistent gaps across all question types as shown in Tab.2. According to Tab.3, even the most advanced models lag behind human performance by over 30% in the overall average score. This performance gap remains substantial across three main task categories and all question formats. These findings suggest that current MLLMs fall short on OST-Bench, illustrating how our benchmark presents novel challenge, demanding stronger online spatio-temporal perception 6 Figure 3: Model performance over exploration time. The right side shows general decline in answer accuracy for all models; the left side illustrates the accuracy trends across three main categories for InternVL-2.5-38B and GPT-4.1. and reasoning capabilities. This observation motivates us to investigate further the reasons for the subpar performance of the models on this benchmark. Weak Spatio-Temporal Reasoning in MLLMs. As shown in Tab. 2 and 3, striking contrast can be observed across the three main task categories. Although most models achieve average scores close to 70% in the Agent Visible Info category, with performance in each subtype significantly above chance level, their scores in the Agent State and Agent-Object Spatial Relationship categories remain near chance level across all subtypes. This suggests that current models are capable of dynamically perceiving scene information with temporal awareness, but lack the ability to perform complex spatio-temporal reasoning. Performance Drop During Exploration. As illustrated in Fig. 3, we observe significant decline in model accuracy as the agent continues to explore with an increasing number of sequential observations in the online setting. This is expected: for each question, the agent must reason based on both the current observation and its historical memory. As the number of exploration turns grows, the amount of relevant past information the agent needs to retain also increases, naturally raising the difficulty of both perception and reasoning. We further analyze how performance evolves for two representative models, InternVL-2.5-38B and GPT-4.1, across the three main question categories. For Agent Visible Info questions, accuracy declines gradually and consistently over turns. In contrast, for Agent-Object Spatial Relationship and Agent State questions, performance drops sharply within the first few steps (typically within 2 to 4 turns) to near chance level, and remains low in subsequent turns. Comparison of Different Models. When comparing the performance of different models in Tab. 2 and 3, we find that proprietary models demonstrate significantly stronger performance compared to open-source ones. For different variants of the same open-source model, scaling from smaller configurations (7B/8B) to larger ones (>32B) consistently leads to notable performance gains, particularly on the questions under the Agent visible info category; Enabling the \"thinking\" mode in Gemini-2.0-Flash results in substantial improvements over the original version, especially on the questions with Temporal Localization format and the those under the Agent-Object Spatial Relationship category. This suggests that the thinking mode effectively enhances both spatial and temporal awareness. 4.3 Experiment Analysis 4.3.1 Insights from Model Explanations To gain deeper insight into the weaknesses of models on OST-Bench, we prompt them to output not only their final answers but also their reasons. We manually examine the model outputs and identify the sources of errors. Since the models inference process involves three key stages: understanding 7 Figure 4: Distribution of three error types across the three task categories in OST-Bench. Figure 5: An example of Spatio-temporal Reasoning Shortcut, the green text indicates correct reasoning by the model, while the red text highlights wrong reasoning. and following prompts, extracting information from observations, and performing spatio-temporal reasoning. Based on the stage at which the failure occurs, we categorize errors into three types: (1) Prompt Analysis Error, arising from the models failure to correctly interpret the task setup or follow the given instructions; (2) Perception Error, where the model fails to accurately extract information from the visual observations by overlooking or misidentifying objects; (3) Reasoning Error, caused by incorrect spatio-temporal reasoning based on the information perceived. These three error types exhibit clear progressive relationship. We select several representative open-source/proprietary models (GPT-4o, Gemini-2.0-Flash-Thinking, InternVL-2.5-78B) and examine 30 error cases for each major category per model, totaling 270 manual in-depth inspections. Error Distribution Statistics on OST-Bench. The statistical results in Fig.4 show that Prompt Analysis Errors are relatively rare across all three major task categories, indicating that models generally understand the novel tasks and instructions introduced by OST-Bench. Perception Errors are the dominant failure mode for the Agent Visible Info category. In contrast, for tasks requiring more complex spatio-temporal reasoning, such as AgentObject Spatial Relationships and Agent State, Reasoning Errors constitute substantial portion of the failures. Based on the number of errors per task category and their distribution across the three error types, we estimate that Reasoning Errors account for over 60% of all errors, making them the primary bottleneck limiting current MLLM performance on OST-Bench. Spatio-temporal Reasoning Shortcut of MLLMs. OST-Bench requires models to reason online over space and time, leveraging past observations to build spatial connections between the current state and prior states or previously seen objects. Within our in-depth error analysis, the models Reasoning Error reflects lack of this ability and reveals common phenomenon as follows: The model tends to take shortcuts in reasoning, performing shallow and unsupported inference based on minimal information, and is reluctant to retrieve and utilize key information from long-term memory that could aid in answering the question. We name this phenomenon as Spatio-temporal Reasoning Shortcut. As shown in Fig. 5 example, the model correctly identifies that television appeared in earlier frames and recognizes its own positional change over time. However, it makes an unfounded inference that the TV must now be behind it based solely on the fact that the TV is currently not visible, without using available spatial anchors such as the locations of table or chair that could help establish grounded reference frame. Additional examples of such shortcut behaviors are provided in Appendix C.2 to further illustrate their prevalence. 4.3.2 Cross-View Analysis While most models struggle with complex spatio-temporal reasoning over sequentially growing memory, we introduce targeted subset of OST-Bench to better delineate the capability boundaries of models. Questions of this subset focus on the spatial relationship between the agent and two objects that appear in different frames (e.g., \"Which object is more to your left?\"). It requires the model to construct cross-view spatial connections to answer correctly. It evaluates performance across two dimensions: (1) Singlevs. Multi-step Spatial Connection. In the single-step setting, the spatial connection between two target objects can be directly inferred by analyzing the pair of the two frames that 8 Figure 6: Singlevs. multi-step spatial connection settings. Target objects and spatial clues are highlighted. Figure 7: Model performance across four task settings: keyframevs. sequence-based context, and singlevs. multi-step spatial connection. contain them. In contrast, the multi-step setting demands higher-level reasoning capabilities, where single-step pairwise frame analysis proves insufficient. This scenario requires the model to integrate spatial cues across multiple keyframes (typically more than two), iteratively establishing pairwise relationships between frames to enable chained reasoning through intermediate steps. As illustrated in Fig. 6, in the single-step case, the spatial connection between the computer and the white bin can be directly inferred through the shared objects (chair and table) in the single pair of images. While in the multi-step case, establishing the spatial connection between the bathtub and the gray trash bin necessitates an anchor image to bridge intermediate objects (trash bin toilet potted plant bathtub), forming multi-step spatial reasoning chain. (2) Keyframevs. Sequence-based Context. In the keyframe-based setting, all keyframes that contain target objects or spatial cues sufficient to solve the problem are directly provided as input. In contrast, the sequence-based setting embeds these keyframes within longer memory sequence that includes many irrelevant frames. The model must identify and leverage the relevant ones, thereby testing its capacity for long-term memory retrieval and reasoning. This subset provides an opportunity to examine the models performance across different levels of difficulty. We construct this dataset using hybrid approach of rule-based generation and manual filtering. We curate 200 questions and evaluate three advanced MLLMs: Gemini-2.0-Flash (Thinking), GPT-4o, and Claude-3.5-Sonnet. For each question, models are required to provide both the answer and its reason. Only when both the final answer and the reason are correct is the response counted as correct. Based on our evaluation, as the results shown in Fig.7, we report the following key findings: (1) As tasks change from single to multi-step spatial connecting setting, which requires more complex reasoning, all models experience substantial drop in accuracy; (2) Long-memory challenges further degrade performance. When models are required to locate relevant frames from sequence-based input rather than keyframe provided directly, accuracy drops significantly. In the most challenging tasks, which need to establish multi-step spatial connection in sequence-based context, all models fall to around 10% accuracy. The results show that the models performance drops significantly when faced with either complex clue-based spatial reasoning requirements or long-term memory retrieval demands. OST-Bench exemplifies this dual challenge, as it requires models to retrieve information from long, temporally extended memory while simultaneously constructing spatial relationships by integrating cues from multiple images to perform multi-step reasoning. These two factors jointly contribute to the poor performance observed on OST-Bench, highlighting the need to advance both capabilities in future model development."
        },
        {
            "title": "5 Limitations and Conclusion\nIn this work, we propose OST-Bench, a novel benchmark for evaluating the online spatio-temporal\nreasoning capabilities of MLLMs. By emphasizing both online processing and spatio-temporal\nunderstanding, OST-Bench more accurately reflects the complexities of real-world perception and\nreasoning. Our extensive evaluation of leading MLLMs shows that OST-Bench poses significant chal-\nlenges for models, particularly in tasks requiring complex spatio-temporal reasoning and maintaining",
            "content": "9 answer accuracy as input accumulates over time in an online setting. We hope the public release of OST-Bench will serve as catalyst for future research in online embodied understanding. We assume that the environment remains static. However, in real-world scenarios, object states and positions often change due to interactions with humans or agents. Additionally, our benchmark focuses solely on the agents online perception and reasoning abilities, capturing only one aspect of real-world embodied tasks. Other crucial capabilities, such as interactive behaviors and active manipulation, are not considered in our current setting. These limitations highlight promising directions for future research and benchmark development."
        },
        {
            "title": "References",
            "content": "[1] Arkitscenes license. https://github.com/apple/ARKitScenes/blob/main/LICENSE. [2] Embodiedscan access. 1FAIpQLScUXEDTksGiqHZp31j7Zp7zlCNV7p_08uViwP_Nbzfn3g6hhw/viewform. mmscan https://docs.google.com/forms/d/e/ and [3] Matterport3d license. https://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf. [4] Scannet license. https://kaldir.vc.in.tum.de/scannet/ScanNet_TOS.pdf. [5] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 422440. Springer, 2020. [6] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir. On evaluation of embodied navigation agents, 2018. [7] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. [8] D. Azuma, T. Miyanishi, S. Kurita, and M. Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1912919139, 2022. [9] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu. Qwen technical report, 2023. [10] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report, 2025. [11] G. Baruch, Z. Chen, A. Dehghan, T. Dimry, Y. Feigin, P. Fu, T. Gebauer, B. Joffe, D. Kurz, A. Schwartz, and E. Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [12] R. Bigazzi, F. Landi, M. Cornia, S. Cascianelli, L. Baraldi, and R. Cucchiara. Out of the box: Embodied navigation in the real world. In N. Tsapatsoulis, A. Panayides, T. Theocharides, A. Lanitis, C. Pattichis, and M. Vento, editors, Computer Analysis of Images and Patterns, pages 4757, Cham, 2021. Springer International Publishing. [13] K. Chandrasegaran, A. Gupta, L. M. Hadzic, T. Kota, J. He, C. Eyzaguirre, Z. Durante, M. Li, J. Wu, and L. Fei-Fei. Hourvideo: 1-hour video-language understanding, 2024. [14] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV), 2017. [15] D. Z. Chen, A. X. Chang, and M. Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pages 202221. Springer, 2020. [16] J. Chen, Z. Lv, S. Wu, K. Q. Lin, C. Song, D. Gao, J.-W. Liu, Z. Gao, D. Mao, and M. Z. Shou. Videollmonline: Online video large language model for streaming video, 2024. 10 [17] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, L. Gu, X. Wang, Q. Li, Y. Ren, Z. Chen, J. Luo, J. Wang, T. Jiang, B. Wang, C. He, B. Shi, X. Zhang, H. Lv, Y. Wang, W. Shao, P. Chu, Z. Tu, T. He, Z. Wu, H. Deng, J. Ge, K. Chen, K. Zhang, L. Wang, M. Dou, L. Lu, X. Zhu, T. Lu, D. Lin, Y. Qiao, J. Dai, and W. Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [18] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P. Luo, T. Lu, Y. Qiao, and J. Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, 2024. [19] A.-C. Cheng, H. Yin, Y. Fu, Q. Guo, R. Yang, J. Kautz, X. Wang, and S. Liu. Spatialrgpt: Grounded spatial reasoning in vision language models, 2024. [20] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [21] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, P. Chen, Y. Li, S. Lin, S. Zhao, K. Li, T. Xu, X. Zheng, E. Chen, R. Ji, and X. Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024. [22] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Southerland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Z. Zhao, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022. [23] B. Jia, Y. Chen, H. Yu, Y. Wang, X. Niu, T. Liu, Q. Li, and S. Huang. Sceneverse: Scaling 3d visionlanguage learning for grounded scene understanding. arXiv preprint arXiv:2401.09340, 2024. [24] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, and C. Li. Llava-onevision: Easy visual task transfer, 2024. [25] F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. Ma, and C. Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024. [26] Y. Li, J. Niu, Z. Miao, C. Ge, Y. Zhou, Q. He, X. Dong, H. Duan, S. Ding, R. Qian, P. Zhang, Y. Zang, Y. Cao, C. He, and J. Wang. Ovo-bench: How far is your video-llms from real-world online video understanding?, 2025. [27] J. Lin, H. Gao, X. Feng, R. Xu, C. Wang, M. Zhang, L. Guo, and S. Xu. The development of llms for embodied navigation. CoRR, abs/2311.00530, 2023. [28] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S. Han. Vila: On pre-training for visual language models, 2024. [29] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning, 2024. [30] R. Lyu, T. Wang, J. Lin, S. Yang, X. Mao, Y. Chen, R. Xu, H. Huang, C. Zhu, D. Lin, and J. Pang. Mmscan: multi-modal 3d scene dataset with hierarchical grounded language annotations, 2024. [31] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. [32] A. Majumdar, A. Ajay, X. Zhang, P. Putta, S. Yenamandra, M. Henaff, S. Silwal, P. Mcvay, O. Maksymets, S. Arnaud, K. Yadav, Q. Li, B. Newman, M. Sharma, V. Berges, S. Zhang, P. Agrawal, Y. Bisk, D. Batra, M. Kalakrishnan, F. Meier, C. Paxton, A. Sax, and A. Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1648816498, 2024. 11 [33] OpenAI, :, A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, A. adry, A. Baker-Whitcomb, A. Beutel, A. Borzunov, A. Carney, A. Chow, A. Kirillov, A. Nichol, A. Paino, A. Renzin, A. T. Passos, A. Kirillov, A. Christakis, A. Conneau, A. Kamali, A. Jabri, A. Moyer, A. Tam, A. Crookes, A. Tootoochian, A. Tootoonchian, A. Kumar, A. Vallone, A. Karpathy, A. Braunstein, A. Cann, A. Codispoti, A. Galu, A. Kondrich, A. Tulloch, A. Mishchenko, A. Baek, A. Jiang, A. Pelisse, A. Woodford, A. Gosalia, A. Dhar, A. Pantuliano, A. Nayak, A. Oliver, B. Zoph, B. Ghorbani, B. Leimberger, B. Rossen, B. Sokolowsky, B. Wang, B. Zweig, B. Hoover, B. Samic, B. McGrew, B. Spero, B. Giertler, B. Cheng, B. Lightcap, B. Walkin, B. Quinn, B. Guarraci, B. Hsu, B. Kellogg, B. Eastman, C. Lugaresi, C. Wainwright, C. Bassin, C. Hudson, C. Chu, C. Nelson, C. Li, C. J. Shern, C. Conger, C. Barette, C. Voss, C. Ding, C. Lu, C. Zhang, C. Beaumont, C. Hallacy, C. Koch, C. Gibson, C. Kim, C. Choi, C. McLeavey, C. Hesse, C. Fischer, C. Winter, C. Czarnecki, C. Jarvis, C. Wei, C. Koumouzelis, D. Sherburn, D. Kappler, D. Levin, D. Levy, D. Carr, D. Farhi, D. Mely, D. Robinson, D. Sasaki, D. Jin, D. Valladares, D. Tsipras, D. Li, D. P. Nguyen, D. Findlay, E. Oiwoh, E. Wong, E. Asdar, E. Proehl, E. Yang, E. Antonow, E. Kramer, E. Peterson, E. Sigler, E. Wallace, E. Brevdo, E. Mays, F. Khorasani, F. P. Such, F. Raso, F. Zhang, F. von Lohmann, F. Sulit, G. Goh, G. Oden, G. Salmon, G. Starace, G. Brockman, H. Salman, H. Bao, H. Hu, H. Wong, H. Wang, H. Schmidt, H. Whitney, H. Jun, H. Kirchner, H. P. de Oliveira Pinto, H. Ren, H. Chang, H. W. Chung, I. Kivlichan, I. OConnell, I. OConnell, I. Osband, I. Silber, I. Sohl, I. Okuyucu, I. Lan, I. Kostrikov, I. Sutskever, I. Kanitscheider, I. Gulrajani, J. Coxon, J. Menick, J. Pachocki, J. Aung, J. Betker, J. Crooks, J. Lennon, J. Kiros, J. Leike, J. Park, J. Kwon, J. Phang, J. Teplitz, J. Wei, J. Wolfe, J. Chen, J. Harris, J. Varavva, J. G. Lee, J. Shieh, J. Lin, J. Yu, J. Weng, J. Tang, J. Yu, J. Jang, J. Q. Candela, J. Beutler, J. Landers, J. Parish, J. Heidecke, J. Schulman, J. Lachman, J. McKay, J. Uesato, J. Ward, J. W. Kim, J. Huizinga, J. Sitkin, J. Kraaijeveld, J. Gross, J. Kaplan, J. Snyder, J. Achiam, J. Jiao, J. Lee, J. Zhuang, J. Harriman, K. Fricke, K. Hayashi, K. Singhal, K. Shi, K. Karthik, K. Wood, K. Rimbach, K. Hsu, K. Nguyen, K. Gu-Lemberg, K. Button, K. Liu, K. Howe, K. Muthukumar, K. Luther, L. Ahmad, L. Kai, L. Itow, L. Workman, L. Pathak, L. Chen, L. Jing, L. Guy, L. Fedus, L. Zhou, L. Mamitsuka, L. Weng, L. McCallum, L. Held, L. Ouyang, L. Feuvrier, L. Zhang, L. Kondraciuk, L. Kaiser, L. Hewitt, L. Metz, L. Doshi, M. Aflak, M. Simens, M. Boyd, M. Thompson, M. Dukhan, M. Chen, M. Gray, M. Hudnall, M. Zhang, M. Aljubeh, M. Litwin, M. Zeng, M. Johnson, M. Shetty, M. Gupta, M. Shah, M. Yatbaz, M. J. Yang, M. Zhong, M. Glaese, M. Chen, M. Janner, M. Lampe, M. Petrov, M. Wu, M. Wang, M. Fradin, M. Pokrass, M. Castro, M. O. T. de Castro, M. Pavlov, M. Brundage, M. Wang, M. Khan, M. Murati, M. Bavarian, M. Lin, M. Yesildal, N. Soto, N. Gimelshein, N. Cone, N. Staudacher, N. Summers, N. LaFontaine, N. Chowdhury, N. Ryder, N. Stathas, N. Turley, N. Tezak, N. Felix, N. Kudige, N. Keskar, N. Deutsch, N. Bundick, N. Puckett, O. Nachum, O. Okelola, O. Boiko, O. Murk, O. Jaffe, O. Watkins, O. Godement, O. Campbell-Moore, P. Chao, P. McMillan, P. Belov, P. Su, P. Bak, P. Bakkum, P. Deng, P. Dolan, P. Hoeschele, P. Welinder, P. Tillet, P. Pronin, P. Tillet, P. Dhariwal, Q. Yuan, R. Dias, R. Lim, R. Arora, R. Troll, R. Lin, R. G. Lopes, R. Puri, R. Miyara, R. Leike, R. Gaubert, R. Zamani, R. Wang, R. Donnelly, R. Honsby, R. Smith, R. Sahai, R. Ramchandani, R. Huet, R. Carmichael, R. Zellers, R. Chen, R. Chen, R. Nigmatullin, R. Cheu, S. Jain, S. Altman, S. Schoenholz, S. Toizer, S. Miserendino, S. Agarwal, S. Culver, S. Ethersmith, S. Gray, S. Grove, S. Metzger, S. Hermani, S. Jain, S. Zhao, S. Wu, S. Jomoto, S. Wu, Shuaiqi, Xia, S. Phene, S. Papay, S. Narayanan, S. Coffey, S. Lee, S. Hall, S. Balaji, T. Broda, T. Stramer, T. Xu, T. Gogineni, T. Christianson, T. Sanders, T. Patwardhan, T. Cunninghman, T. Degry, T. Dimson, T. Raoux, T. Shadwell, T. Zheng, T. Underwood, T. Markov, T. Sherbakov, T. Rubin, T. Stasi, T. Kaftan, T. Heywood, T. Peterson, T. Walters, T. Eloundou, V. Qi, V. Moeller, V. Monaco, V. Kuo, V. Fomenko, W. Chang, W. Zheng, W. Zhou, W. Manassra, W. Sheu, W. Zaremba, Y. Patil, Y. Qian, Y. Kim, Y. Cheng, Y. Zhang, Y. He, Y. Zhang, Y. Jin, Y. Dai, and Y. Malkov. Gpt-4o system card, 2024. [34] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kaftan, Łukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Łukasz Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, 12 D. Mély, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. OKeefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph. Gpt-4 technical report, 2024. [35] R. C. Prim. Shortest connection networks and some generalizations. Bell System Technical Journal, 36(6):13891401, 1957. [36] G. R. Team, S. Abeyruwan, J. Ainslie, J.-B. Alayrac, M. G. Arenas, T. Armstrong, A. Balakrishna, R. Baruch, M. Bauza, M. Blokzijl, S. Bohez, K. Bousmalis, A. Brohan, T. Buschmann, A. Byravan, S. Cabi, K. Caluwaerts, F. Casarini, O. Chang, J. E. Chen, X. Chen, H.-T. L. Chiang, K. Choromanski, D. DAmbrosio, S. Dasari, T. Davchev, C. Devin, N. D. Palo, T. Ding, A. Dostmohamed, D. Driess, Y. Du, D. Dwibedi, M. Elabd, C. Fantacci, C. Fong, E. Frey, C. Fu, M. Giustina, K. Gopalakrishnan, L. Graesser, L. Hasenclever, N. Heess, B. Hernaez, A. Herzog, R. A. Hofer, J. Humplik, A. Iscen, M. G. Jacob, D. Jain, R. Julian, D. Kalashnikov, M. E. Karagozler, S. Karp, C. Kew, J. Kirkland, S. Kirmani, Y. Kuang, T. Lampe, A. Laurens, I. Leal, A. X. Lee, T.-W. E. Lee, J. Liang, Y. Lin, S. Maddineni, A. Majumdar, A. H. Michaely, R. Moreno, M. Neunert, F. Nori, C. Parada, E. Parisotto, P. Pastor, A. Pooley, K. Rao, K. Reymann, D. Sadigh, S. Saliceti, P. Sanketi, P. Sermanet, D. Shah, M. Sharma, K. Shea, C. Shu, V. Sindhwani, S. Singh, R. Soricut, J. T. Springenberg, R. Sterneck, R. Surdulescu, J. Tan, J. Tompson, V. Vanhoucke, J. Varley, G. Vesom, G. Vezzani, O. Vinyals, A. Wahid, S. Welker, P. Wohlhart, F. Xia, T. Xiao, A. Xie, J. Xie, P. Xu, S. Xu, Y. Xu, Z. Xu, Y. Yang, R. Yao, S. Yaroshenko, W. Yu, W. Yuan, J. Zhang, T. Zhang, A. Zhou, and Y. Zhou. Gemini robotics: Bringing ai into the physical world, 2025. [37] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, Z. Wang, R. Fergus, Y. LeCun, and S. Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. [38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023. [39] T. Wang, X. Mao, C. Zhu, R. Xu, R. Lyu, P. Li, X. Chen, W. Zhang, K. Chen, T. Xue, X. Liu, C. Lu, D. Lin, and J. Pang. Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [40] H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: benchmark for long-context interleaved videolanguage understanding, 2024. [41] Y. Wu, P. Zhang, M. Gu, J. Zheng, and X. Bai. Embodied navigation with multi-modal information: survey from tasks to methodology. Information Fusion, 112:102532, 2024. [42] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. [43] J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, and S. Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces, 2024. [44] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao. Activitynet-qa: dataset for understanding In Proceedings of the AAAI Conference on Artificial complex web videos via question answering. Intelligence, volume 33, pages 91279134, 2019. [45] H. Zhang, Y. Wang, Y. Tang, Y. Liu, J. Feng, J. Dai, and X. Jin. Flash-vstream: Memory-based real-time understanding for long video streams, 2024. [46] P. Zhang, K. Zhang, B. Li, G. Zeng, J. Yang, Y. Zhang, Z. Wang, H. Tan, C. Li, and Z. Liu. Long context transfer from language to vision, 2024. 13 [47] Y. Zhang, Z. Gong, and A. X. Chang. Multi3drefer: Grounding text description to multiple 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1522515236, 2023. [48] Y. Zhang, J. Wu, W. Li, B. Li, Z. Ma, Z. Liu, and C. Li. Video instruction tuning with synthetic data, 2024. [49] D. Zheng, S. Huang, L. Zhao, Y. Zhong, and L. Wang. Towards learning generalist model for embodied navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1362413634, June 2024. [50] Z. Zhu, X. Ma, Y. Chen, Z. Deng, S. Huang, and Q. Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29112921, 2023."
        },
        {
            "title": "Appendix",
            "content": "A Benchmark Details A.1 Exploration Route Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Visible Information Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Rule-based Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Statistics . . . . . . . . A.5 Benchmark Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details Experiment Analysis Details C.1 Cases of Three Error Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Cases of Spatio-temporal Reasoning Shortcut . . . . . . . . . . . . . . . . . . . . C.3 Subset Construction Process for Cross-View Analysis . . . . . . . . . . . . . . . . C.4 More Findings in Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inference Time of the Models Social Impact License and Acess F.1 License and Acess for Existing Assets . . . . . . . . . . . . . . . . . . . . . . . . F.2 License and Acess for OST-Bench . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 16 17 20 20 21 21 21 23 25 25 26"
        },
        {
            "title": "A Benchmark Details",
            "content": "This section provides additional details on the construction of our benchmark, including the algorithm used for route generation, the method for determining object visibility, the rules for benchmark sample generation, and summary statistics of the generated data. A.1 Exploration Route Generation While ScanNet and ARKitScenes offer egocentric video sequences with associated per-frame camera parameters, Matterport3D provides, for each scene, camera positions distributed throughout the environment. From each position, images are captured at different viewing angles, as illustrated in Fig. 8. We aim to leverage this information to construct simulated trajectory of an agent exploring the scene from first-person perspective. As mentioned in the main paper, the trajectory must satisfy two key requirements: (a) Path continuity, the movement between adjacent frames should be smooth, avoiding abrupt spatial jumps over short time intervals. (b) Observation continuity, adjacent frames in the video must have certain degree of visual overlap, which is crucial for providing the cross-frame visual continuity necessary for constructing coherent 3D understanding of the scene. The videos provided by ScanNet and ARKitScenes naturally satisfy both of these requirements. The video we aim to generate is sequence of tuples {(ni, ki, ci)}, where ni denotes the camera position index among the predefined locations, ki indicates the viewing angle index among the available viewing angles at that position, and ci is the corresponding captured image. Based on the two aforementioned requirements  (Fig.8)  , (a) We first construct minimum spanning tree(MST) (N, E) over all camera positions using Prims algorithm, where edge weights are defined by the Euclidean distances between positions. We constrain the agents movement to either transitions 15 Figure 8: Illustration of the route generation process. The radial arrows represent multiple different viewing angles at position, and the red edges denote connections generated by the MST algorithm. The right part shows the captured images for each viewing angle of two adjacent nodes. The agent can only move along the edges of the tree, and adjacent frames are required to have certain amount of overlap. between neighboring positions connected by edges in the MST (Eni,nj E), or changes in viewing angles at the same position. This design ensures path continuity throughout the simulated trajectory. (b) We enforce that adjacent images in the sequence must have sufficient visual overlap. That is, for any 0, the overlap between images ci, ci+1 must satisfy Overlap(ci, ci+1) > threshold. This constraint preserves observation continuity across frames. Based on these two rules, we perform random walk over the nodes to generate the sequence. Starting from randomly selected initial state with random tuple (n0, k0, c0), at each step, we randomly select valid and previously unseen tuple representing the next state and append it to the sequence. This process continues until no valid tuples remain or the sequence reaches predefined length. It is important to note that the generated videos ensure continuity in terms of paths and observations, but do not guarantee temporal continuity (i.e., they only provide discrete frame ordering without information on the time intervals between frames). However, since our benchmark setting uses rounds as discrete timestamps, such temporal information is not required, and the provided data is sufficient for our purposes. A.2 Visible Information Processing Attribute Visibility. For the attribute visibility of objects, to reduce computational complexity, we first apply necessary condition: if an object is visible, then at least one of its 3D points must be projectable onto the 2D image plane within the image boundaries and without occlusion. This condition allows for the rapid elimination of most invisible objects in each image. For objects satisfying this condition, we project the surface points of their bounding boxes onto the images 2D plane. We first compute the projected area A2 without considering occlusion or image boundaries. Then, we calculate the visible area A1 by accounting for occlusions and restricting projections to within the image bounds. An object is deemed visible if either (1) the ratio of visible to total projected area, A1/A2, exceeds predefined threshold, or (2) the absolute visible area A1 is sufficiently large. Spatial Visibility. For the spatial visibility of objects, building on attribute visibility, we further check whether at least five vertices of the objects 9-DoF bounding box are visible in the frames observed so far. If this condition is met, we assume the objects center position, size (length, width, height), and related spatial information are all available, thus satisfying the criteria for spatial visibility. A.3 Rule-based Generation Our OST-Bench comprises three major categories: Agent State, Agent Visible Info, and Agent-object Spatial Relationship. Within these categories, we define total of 15 question subtypes. Data samples are generated through rule-based approach, guided by set of principles outlined below. (1) Multi-round Dialogue Format. OST-Bench adopts multi-round dialogue setup. In each round, 45 new frames from the video are selected sequentially in chronological order as new observations and appended to the historical observation sequence. Each question is asked at the timestamp of the last frame in the current round. All information after this timestamp is considered unavailable, and we ensure that the question is answerable based solely on the observations up to that timestamp. (2) Sample Pool Construction and Selection. For each question subtype, we exhaustively generate all possible data samples to form candidate pool. We ensure that no identical question-answer pair appears across different dialogue rounds (although the same question might occur, the answers must differ). In each round, we first randomly select question subtype and then randomly select data sample from its corresponding candidate pool as the question for that round. (3) Object Reference. Object references in questions are divided into two types. The first is categorylevel reference, where category word is used to refer to all instances of that category (e.g., How many books are there in the room?). The second is instance-level reference, where specific grounded description is used to uniquely identify single object.(e.g., Where is the yellow-covered book labeled with the word atomic?). These descriptions are sourced from MMScans object-level annotations. To eliminate ambiguity, we ensure that this referred object is the only instance of its category within historical observations. (4) Memory-based Reasoning Requirement. To rigorously test models ability to reason over long-term memory and avoid overly simple questions, we ensure that no question can be answered using only the newly added observations in the current round. Each question requires integrating information from both the current and previous dialogue rounds. For example, we ensure that at least one relevant object is absent from the observations in the current round, thereby requiring the model to recall it from prior rounds. (5) Ensuring Clarity and Avoiding Ambiguity. To ensure the validity and clarity of the questions and to avoid controversial or ambiguous cases, we impose specific thresholds during sample generation so that the answers are unambiguous and clearly inferable. For example, when question involves comparing two distances, we require the difference between the distances to exceed predefined threshold to ensure significant contrast. Similarly, for questions such as determining whether an object is on the left or right, we require the object to be clearly positioned on one side. Objects located near the decision boundary (e.g., close to the center) are excluded to prevent ambiguity in interpretation. Fig.9 presents the predefined templates used for generating questions across different subtypes. The specific generation strategies for each subtype are detailed below: Agent State. This category encompasses tasks that require the agent to judge or estimate its own spatial state, including its position and orientation. Since there is no globally defined coordinate system in OST-Bench, all measurements are made relative to specific historical time point. Position (Judgement): In this type, the task is to determine whether the agent has moved to the left or right (forward or backward), relative to its position and orientation at the end of previous round T1. The question is formulated as binary choice, with the correct answer being either left or right(forward or backward). Let P1 and O1 denote the position and orientation at the end of round T1, and P2 denote the current position. We compute the parallel and perpendicular components of the vector P2 P1 with respect to O1. question is generated only if the absolute value of either component exceeds predefined threshold (1 meter). The correct answer is determined by the sign of the respective component: positive value indicates forward or right, while negative value indicates backward or left. Position (Estimation): In this subtype, the task is to estimate how far the agent has moved from its position at the end of previous round T1. The ground-truth answer is defined as the Euclidean distance between the agents current position and its position at T1. Figure 9: Rule-based generation templates for all subtypes in OST-Bench. Placeholders to be filled with specific content are marked in red, and question focal points are highlighted in blue. \"JUD.\"/ \"CNT.\" / \"TEMP.\" / \"EST.\" are abbreviations for \"judgement\",\"counting\",\"temporal-localization\", and \"estimation\"; \"Q\" and \"O\" denote \"Question\" and \"Options\" Orientation (Judgement): This binary-choice question asks whether the agent has rotated clockwise or counterclockwise by an angle(less than 180 degrees) relative to its orientation at the end of round T1. We compute the angle between the current orientation vector and the one at the end of T1. To exclude ambiguous borderline cases, questions are generated only if the angle lies within the intervals [θ, 180 θ] or [180 + θ, 360 θ], where θ is threshold used to exclude borderline cases. Angles within the first interval indicate clockwise rotation, while those within the second indicate counterclockwise rotation. Orientation (Estimation): In this question type, the task is to estimate how many degrees the agent has rotated, clockwise or counterclockwise, relative to its orientation at the end of previous round T1. The answer is given as the angle between the current orientation and the orientation at the end of round T1. Agent Visible Info. All objects involved in this category of questions must satisfy the attribute visibility constraint, meaning that their existence must be identifiable from past observations. This category evaluates the models understanding of agent visible information, including subtasks such as object existence, quantity, diversity, and the order of appearances. Existence (Judgement): This type asks whether certain category was visible in any of the previous observations. The answer is binary: yes or no. To balance positive and negative samples, we generate questions for object categories that do not appear in prior observations with 50% probability. Existence (Temporal Localization): This type includes two forms of queries: (1) Identifying the earliest/latest round in which specific object was visible; (2) Identifying the round in which two specific objects were simultaneously visible. For both forms of queries, we ensure the answer is uniquei.e., there is exactly one round that satisfies the condition. Quantity (Counting): This task requires counting how many objects of specified category were visible in past observations. To avoid trivial cases, we exclude questions where the correct answer is one. Additionally, to balance the distribution, negative sampleswhere the target category does not appear at allare introduced to constitute 25% of the total samples. Diversity (Judgement): This question type asks which object is newly observed in the current round. The agent must choose one object from three candidates, all of which are visible in the current observation. Among them, only one has not appeared in any previous round, while the other two have been seen before. Order (Judgement): This question type involves determining the appearance order of three different object categories. The agent must select the correct sequence from four given permutations. We ensure that the first appearance round of each object category is distinct to avoid ambiguity in ordering. Agent-Object Spatial Relationship. This category focuses on constructing spatial metric relationships between the agent and specific object at specific time . The distance between the agent and object at time is defined as the shortest distance from the camera coordinate to any point in the objects point cloud. The angle of object relative to the agent at time is computed as the angle between the cameras horizontal orientation vector and the vector pointing from the camera to the center of object O. All objects involved in this category must satisfy the spatial visibility constraint, which means that their center coordinates, dimensions (length, width, height), and other spatial properties must be reliably obtainable from previous observations. Distance (Judgement): This question type includes three forms of queries: (1) determining which of the three objects is currently farthest from or closest to the agent; (2) judging whether the current distance between the agent and specific object is greater or smaller than the distance at the end of previous round; (3) judging whether the current distances between the agent and two specific objects are greater or smaller than those at the end of previous round, with four possible answer choices. For the first form, at least one object must be invisible in the current round, and the distance to the correct answer object must differ significantly (i.e., by more than predefined threshold) from the distances to the other two objects. For the second and third forms, the change in distance between the two time points must also exceed the threshold to ensure meaningful distinction. Distance (Temporal Localization): This task asks the agent to identify the round in which it was closest to or farthest from specific object. The distance in the correct round must be significantly smaller (for closest) or larger (for farthest) than in all other rounds. Distance (Estimation): This query requires estimating the current distance between the agent and specific object, which is invisible in the current round and thus requires recalling information from previous rounds. Direction (Judgement): This question type includes three forms of queries: (1) judging whether specific object is currently on the agents left or right side; (2) judging whether specific object currently lies in the left-front, left-back, right-front, or right-back quadrant relative to the agent; (3) identifying which two out of three objects are currently on the same side of the agent. For the first two forms, we enforce angular thresholds by excluding objects whose relative angles fall within 10 degrees of the decision boundaries between sides or quadrants, thereby avoiding ambiguity. For the third form, at least two of the three objects are invisible in the current round, forcing the model to rely on memory. Direction (Temporal Localization): This query asks the agent to identify the round in which both objects and were located on the same side (left or right) relative to the agent. We ensure that in each round, both objects are clearly on either the left or right side (at least 10 degrees away from the decision boundary), and that there is exactly one round satisfying this condition. Direction (Estimation): This query requires estimating the angle, clockwise or counterclockwise, of specific object relative to the agents current orientation. The object is not visible in the current round, requiring retrieval from prior observations. Figure 10: Distribution of sample counts across different subtypes in OST-Bench. Figure 11: Word cloud (top) and dialogue length distribution (bottom) of OST-Bench. A.4 Statistics Based on the generation methods described above, OST-Bench totally consists of 1.4k trajectories(a trajectory per scene) and 10k data samples. The distribution of sample counts across different subtypes is shown in Fig. 10. We also present in Fig. 11 the word frequency distribution in OSTBench (visualized as word cloud), as well as the distribution of dialogue lengths. A.5 Benchmark Examples In Fig. 16 and 17 we provide more examples from our benchmark, including total of 12 data samples from two scenes (exploration trajectories)."
        },
        {
            "title": "B Implementation Details",
            "content": "For the multi-round dialogue, we first provide system prompt to inform the models of the task setup. In each round, we sequentially input set of images representing new video frames, along with prompt containing question, as illustrated in Fig.12. For judgment questions, we include the options in the prompt. For the other three qusetion formats (estimation, counting, and temporal-localization), we prompt the model to output specific numerical value and explicitly instruct it to answer the question. This instruction is necessary, as we observed during experiments that models may otherwise refuse to respond, claiming insufficient information. For proprietary models, we interact with the OpenAI and Anthropic APIs, both of which support multiround dialogue with image inputs. In these APIs, models are invoked by explicitly specifying their model names. For the OpenAI API, we use gpt-4o for GPT-4o, gpt-4.1 for GPT-4.1, gemini-2.0-flash for Gemini-2.0-Flash, and gemini-2.0-flash-thinking-exp for its thinking variant. For the Anthropic API, we use claude-3-5-sonnet-latest to access Claude-3.5-Sonnet. The system prompt is set to the task description, and each rounds input includes newly added images and questions. For open-source models (InternVL, QwenVL, LLaVA-Onevision, and LLaVA-Video), we manually construct the multi-round context by concatenating the dialogue history, new images, and the current prompt as the input at each round. To avoid out-of-memory errors, input images are resized accordingly. For 20 Figure 12: Model input content, including the system prompt and inputs for each round. Text placeholders to be filled are highlighted in red, while the green <image> token represent image placeholders to be filled. models with up to 8 billion parameters, inference is run on single NVIDIA A100 GPU. For models with 32 billion parameters or more, we perform multi-GPU inference using 8 NVIDIA A100 GPUs via model and data parallelism. Additionally, we implement multithreaded processing to accelerate the inference of open-source models."
        },
        {
            "title": "C Experiment Analysis Details",
            "content": "C.1 Cases of Three Error Types In Fig.13, we present examples of the three types of errors: Prompt Analysis Error, Perception Error, and Reasoning Error. In the first example of Prompt Analysis Error, the prompt explicitly requires the model to output specific quantity. However, the model fails to interpret this requirement correctly and responds with \"no\" instead of providing numerical answer such as \"0\". In the second example, the model misunderstands the meaning of the word \"discover\" in the prompt. It assumes that partially seeing the keyboard in Round 1 does not count as discovery and that only fully observing it in Round 2 qualifies as such. This misinterpretation leads to an incorrect answer. In the two Perception Error examples, the model fails to correctly identify washbin located in the corner of the room and only detects one of the two lamps in the bedroom, missing the other. In the Reasoning Error examples, although the model correctly understands the prompt and accurately perceives the location of the target objects, it makes an error in reasoning about their spatial relation with the agent, leading to incorrect conclusions. C.2 Cases of Spatio-temporal Reasoning Shortcut In the main paper, we have discussed the Spatio-temporal Reasoning Shortcut phenomenon exhibited by the models. In Fig.14, we provide additional examples to further demonstrate the prevalence of this behavior. For clarity, we display only the key video frames relevant to each question. Temporal expressions in the questions and model responses are replaced with t1, t2, and t3, and marked above the corresponding frames. All of these examples demonstrate the models tendency to rely on shortcuts in spatio-temporal reasoning. In the first example, GPT-4o incorrectly infers that the blackboard has moved closer simply based on its transition from being invisible to visible, ignoring spatial cues such as the chairs and the decorations on the wall. In the second example, Gemini-2.0-Flash performs seemingly correct inference using only two frames (the current and target frames), concluding that the wall currently in front of the agent is adjacent and perpendicular to the wall in t1, while disregarding intermediate frames that Figure 13: Illustrative Examples of the Three Error Types: Prompt Analysis Error, Perception Error, and Reasoning Error. 22 contain crucial contradictory evidence. In the third example, InternVL-2.5-78B observes that the TV was on the right side of the room in earlier frames and then directly assumes it remains there when it becomes invisible. In the fourth and fifth examples, the models make incorrect judgments due to the target object being invisible in the specific frames. In the sixth example, the model only focuses on the frames where the stand appears and the current frame, while skipping over intermediate frames that indicate the agent turned around, wrongly assuming that the current orientation is aligned with the previous one. C.3 Subset Construction Process for Cross-View Analysis As mentioned in the main paper, when constructing the dataset for the Cross-View subset, we first generate an initial batch of data using rule-based method and then manually filter the data to obtain the final set of 200 samples. Our rule-based construction method for generating the Cross-view subset with different levels of difficulty is described as follows: (a) Single-Step Spatial Connection. We first iterate over all possible object pairs (O1, O2) in the scene. For each object pair, we traverse all possible frame pairs (F1, F2) within the video sequence. frame pair is selected if it satisfies the following conditions: (1) O1 is visible in F1 but not in F2; (2) O2 is visible in F2 but not in F1; (3) F1 and F2 share at least one overlapping object. This setup ensures that the spatial relationship between O1 and O2 can be inferred via single-step reasoning. All tuples (O1, O2, F1, F2) satisfying these constraints are collected as initial data for the keyframebased context. To construct the sequence-based context, we embed F1 and F2 into video sequence that includes frames not containing O1 or O2, resulting in tuples of the form (O1, O2, ). (b) Multi-Step Spatial Connection. Similarly, we iterate over all object pairs (O1, O2) and traverse all frame triplets (F1, F2, F3) from the video sequence. triplet is selected if it meets the following conditions: (1) O1 is visible in F1 but not in F2 or F3; (2)O2 is visible in F2 but not in F1 or F3;(3) F1 or F3 share at least one overlapping object;(4) F2 or F3 share at least one overlapping object;(5) F1 and F2 have no overlapping objects. This configuration ensures that solving the problem requires multi-step reasoning. All valid tuples (O1, O2, F1, F2, F3) satisfying these constraints are collected as initial data for the keyframe-based context. Similarly, to construct the sequence-based context, we embed F1, F2 and F3 into video sequence that includes frames not containing O1 or O2, resulting in tuples of the form (O1, O2, ). C.4 More Findings in Tables Difficulty of Estimation Tasks. As shown in Table 2 in the main paper, models perform particularly poorly on estimation tasks, achieving scores well below the chance-level baseline. Humans also struggle with these questions, obtaining significantly lower scores compared to other task categories. This is because estimation questions go beyond innate human perceptual abilities. Humans are better at perceiving spatial relationships approximately than estimating spatial measurements precisely, requiring not only spatial reasoning but also extensive empirical knowledge accumulated from experience. Detection Success vs. Counting Failure. As shown in Table 2 in the main paper, models achieve notably high scores on object-existence questions, demonstrating strong ability to identify whether and when objects appear. However, their performance drops significantly for object-quantity tasks, which require counting. Upon examining specific cases, we found that models frequently confuse whether objects across frames are the same or distinct, mistaking two different objects as identical or failing to track the same object across frames. This suggests that the task demands not just detection capabilities but also cross-frame reasoning. The Illusion of Better Distance Understanding. As shown in Table 2 in the main paper, models appear to perform slightly better on Agent-object distance questions compared to Agent-object direction, but this advantage is superficial. This is primarily due to the Spatio-temporal Reasoning Shortcut phenomenon: models tend to assume that objects currently visible are closer, while those out of view are farther away, without engaging in genuine spatial reasoning. Although this heuristic can occasionally lead to correct answers, since such patterns do occur in small portion of our benchmark, it fails to generalize. As result, models still perform poorly on Agent-object distance questions overall. Figure 14: More examples of Spatio-temporal Reasoning Shortcuts. Green text marks correct reasoning; red indicates errors. For clarity, only key video frames relevant to each question are shown, with temporal references replaced by t1, t2, and t3. 24 Figure 15: The trend of the models inference time per question as the duration of exploration increases. Inference Time of the Models Although OST-Bench does not impose real-time constraints, we conducted supplementary study on models inference time, indirectly reflecting the delay in decision-making exhibited by the models in real-world embodied tasks. Since the inference time of proprietary models is also affected by network latency, we restrict our analysis to open-source models and report their inference time per question. The Fig.15 illustrates how the models inference time per question changes as the duration of exploration increases. The results reveal clear trend: as exploration time increases and more historical context accumulates, inference latency grows rapidly. When the number of dialogue rounds becomes large (e.g., beyond 10), the inference time becomes prohibitively high, especially for large-scale models, making real-time interaction impractical. This latency surge stems from the fact that any frame in history may contain critical information, forcing the model to attend to growing number of frames at every step. Thus, inference time scales approximately linearly with history length. To provide context, we also measured human inference time. While average latency isnt directly comparable due to individual variation, we find that for all human testers, response time remained stable regardless of how long the exploration had lasted. This starkly contrasts with model behavior. The underlying reason is that humans can actively abstract and compress information throughout the exploration process, forming an internal knowledge base. Rather than treating each question as fresh input, humans recall previously formed abstractions, enabling efficient reasoning without reprocessing all historical data. This comparison highlights critical need: for models to perform well in real-world embodied tasks, they must learn to dynamically distill and retain knowledge during exploration. Instead of passively accumulating history or answering questions in isolation, models should develop mechanisms to summarize and store essential information in an efficient, retrievable form, paving the way for scalable and real-time embodied reasoning. Social Impact OST-Bench aims to advance the development of multimodal large language models (MLLMs) with stronger online spatio-temporal reasoning capabilities, which are critical for real-world embodied tasks such as assistive robotics, autonomous navigation, and human-robot interaction. By introducing more realistic and challenging benchmark, we hope to drive progress toward more reliable and 25 generalizable agents capable of perceiving and reasoning in real-world environments under online settings. However, as the benchmark assumes static environment and focuses only on perception and reasoning, there is risk of overestimating model readiness for real deployment. Caution is needed to avoid misuse or overreliance on models without broader capabilities like interaction or manipulation, which are essential for safe and responsible AI integration in the real world."
        },
        {
            "title": "F License and Acess",
            "content": "F.1 License and Acess for Existing Assets As mentioned in the main paper, our real-world scene data is sourced from ScanNet, Matterport3D, and ARKitScenes. To access and use these three datasets, users should follow their original licenses [4, 3, 1], and ask their official hosts for authorization. Additionally, our annotated data come from EmbodiedScan and MMScan, access to these datasets requires submitting request via Google Form [2] and following the license attached to the form. We use ScanNet, Matterport3D, and ARKitScenes as the scene data and leverage the video information provided in them. We adopt the bounding box annotations and textual annotations from EmbodiedScan and MMScan as the base datasets for our benchmark. Throughout the usage of these datasets, their licenses and terms of use are properly respected. F.2 License and Acess for OST-Bench The OST-Bench dataset is distributed under the Creative Commons Attribution 4.0 International License (CC BY 4.0) and available for direct download at https://github.com/rbler1234/ OST-Bench or https://www.kaggle.com/datasets/jinglilin/ost-bench/data. We release our benchmark under the CC-BY license and Terms of Use, and require that any use of the dataset for model evaluation be properly disclosed. This license supplements but does not override the original licenses of source materials; users must also comply with all relevant legal requirements concerning data subjects. This statement clarifies the obligations and liabilities associated with using this benchmark. While we strive to ensure the accuracy and legality of all samples, we do not guarantee their absolute completeness or correctness. We assume no responsibility for any legal or other issues that may arise from the use of OST-Bench, including but not limited to copyright infringement, privacy violations, or the misuse of sensitive information. By accessing, downloading, or using OST-Bench, you acknowledge that you accept this statement and agree to comply with the full terms of the CC-BY license. If you do not agree with these terms or the CC-BY license, you are not permitted to use this benchmark. OST-Bench will be hosted and maintained on GitHub and the Kaggle platforms. 26 Figure 16: Example 1 of OST-Bench data samples. Each row represents the newly added observations in each round, with images input from left to right within each round. The example shows the question-answer pairs from the first six rounds. 27 Figure 17: Example 2 of OST-Bench data samples. Each row represents the newly added observations in each round, with images input from left to right within each round. The example shows the question-answer pairs from the first six rounds."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong"
    ]
}