{
    "paper_title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation",
    "authors": [
        "Iman Barati",
        "Mostafa Amiri",
        "Heshaam Faili"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised Fine-Tuning (SFT) is essential for training large language models (LLMs), significantly enhancing critical capabilities such as instruction following and in-context learning. Nevertheless, creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity. In this paper, we propose SearchInstruct, an innovative method explicitly designed to construct high quality instruction datasets for SFT. Our approach begins with a limited set of domain specific, human generated questions, which are systematically expanded using a large language model. Subsequently, domain relevant resources are dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question. Experimental evaluation demonstrates that SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, we show that beyond dataset generation, the proposed method can also effectively facilitate tasks such as model editing, enabling efficient updates to existing models. To facilitate reproducibility and community adoption, we provide full implementation details, the complete set of generated instruction response pairs, and the source code in a publicly accessible Git repository: [https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)"
        },
        {
            "title": "Start",
            "content": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation Iman Barati1 Mostafa Amiri2 Heshaam Faili2 1 Iran University of Science and Technology 2 University of Tehran iman_barati@comp.iust.ac.ir {mostafa.amiri,hfaili}@ut.ac.ir 5 2 0 2 2 1 ] . [ 1 8 0 7 0 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Supervised Fine-Tuning (SFT) is essential for training large language models (LLMs), significantly enhancing critical capabilities such as instruction-following and in-context learning. Nevertheless, creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity. In this paper, we propose SearchInstruct, an innovative method explicitly designed to construct high-quality instruction datasets for SFT. Our approach begins with limited set of domain-specific, human-generated questions, which are systematically expanded using large language model. Subsequently, domainrelevant resources are dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question. Experimental evaluation demonstrates that SearchInstruct notably enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, we demonstrate that beyond dataset generation, the proposed method can also effectively facilitate tasks such as model editing, enabling efficient updates to existing models. To facilitate reproducibility and community adoption, we provide full implementation details, the complete set of generated instructionresponse pairs, and the source code in publicly accessible Git repository."
        },
        {
            "title": "Introduction",
            "content": "The rapid progress of LLMs and their success in solving general-purpose natural language processing (NLP) tasks (Wei et al., 2021; Sanh et al., 2022) have led to growing focus on adapting these models to specialized domains. Achieving this goal 1https://github.com/mostafaamiri/ SearchInstruct requires not only access to cleaned raw corpora, but also the construction of structured datasets suitable for the SFT stage (Ouyang et al., 2022). Generating high-quality SFT dataespecially for expert domainspresents several major challenges. The data must be both diverse and capable of reflecting the inherent complexity of realworld user queries. Existing approaches either rely on extensive manual annotation, as in InstructGPT (Ouyang et al., 2022), suffer from limited coverage and task diversity (Ziegler et al., 2024), or fail to simulate real user needs accurately. Furthermore, many methods depend solely on the internal knowledge of LLMs, which may be outdated or insufficient for domain-specific adaptation (Lewis et al., 2020; Lazaridou et al., 2022). At the same time, LLMs have shown thatwhen provided with appropriate contextthey can generate accurate and informative responses to even complex domain-specific questions (Izacard et al., 2022). This observation suggests that, by systematically generating diverse and realistic queries and retrieving relevant, up-to-date contextual information, it is possible to construct powerful training datasets without relying on human annotation. Such an approach enables the integration of external domain-specific knowledge into the training process, offering path to updating and extending model capabilities (Guu et al., 2020; Wang et al., 2024). In this paper, we introduce SearchInstruct, novel framework for automatically generating supervised training data. The method operates by expanding small set of seed questions using an LLM, then dynamically retrieving relevant documents to construct accurate, context-grounded answers. Our empirical results demonstrate that this approach not only increases the quality and diversity of SFT data, but also effectively enhances and updates LLM behavior in specialized domains by leveraging up-to-date external knowledge. Our contributions are as follows: 1. We introduce SearchInstruct, an automated framework for constructing SFT datasets by combining targeted document retrieval with grounded answer generation; 2. We design and implement four-stage pipeline for query expansion, diversification, document retrieval, and answer synthesis; 3. We empirically show that, in certain specialized domains, this pipeline leads to improvements in model performance and instructionfollowing accuracy; 4. We demonstrate the effectiveness of SearchInstruct in model editing scenarios, enabling efficient updates to model outputs using newly acquired knowledge."
        },
        {
            "title": "2 Related Work",
            "content": "Instruction tuning has emerged as key strategy for adapting LLMs to follow user instructions. The FLAN approach by Wei et al. (Wei et al., 2021) demonstrated that fine-tuning on broad range of instruction-formatted tasks leads to strong zero-shot generalization. Similarly, T0 by Sanh et al. (Sanh et al., 2022) employed multitask prompted training to enable cross-task generalization. Wang et al. (Wang et al., 2022b) extended this paradigm with the Super-NaturalInstructions dataset, collecting over 1,600 crowdsourced tasks paired with declarative instructions to promote task diversity. InstructGPT (Ouyang et al., 2022) introduced method for aligning LLMs with human preferences using instructionresponse pairs ranked via human feedback. Despite their utility, these datasets are static, expensive to construct, and often insufficient for domain-specific applications or ongoing model updates. To address limitations in scalability and recency, Self-Instruct (Wang et al., 2022a) bootstraps base LLM to generate new instructions and responses from small seed set, filtering outputs using heuristics. While it enhances general instructionfollowing capabilities, it still relies entirely on the models internal knowledge, which may be outdated or incomplete for specialized domains. Recent work has focused on automating instruction dataset creation to reduce dependence on manual annotation. The Alpaca project (Taori et al., 2023) used GPT-3.5 to generate 52,000 instructionresponse pairs, enabling effective fine-tuning of LLaMA model. Evol-Instruct by Xu et al. (Xu et al., 2023) introduced an evolutionary strategy to rewrite and complicate instructions, leading to the creation of WizardLM. InstructZero (Chen et al., 2023) proposed label-free tuning method using reinforcement learning with black-box models. While these approaches improve scale and diversity, they generally lack grounding in real-world evidence and do not facilitate domain-specific adaptation. Retrieval-augmented methods aim to improve factuality and relevance by incorporating external documents. RAG (Lewis et al., 2020) combines dense retrieval with generation to condition responses on retrieved passages. REALM (Guu et al., 2020) pre-trains language models with latent document retrieval to support open-domain QA. More recently, InstructRetro (Wang et al., 2024) integrates retrieval with instruction tuning to enhance grounding, and RA-DIT (Lin et al., 2024) jointly trains retriever and generator using dual instruction signals. However, these methods often require costly retriever training and may not be ideal for rapid, low-resource domain updates. SearchInstruct addresses the limitations of static and hallucinated instruction datasets by incorporating real-time document retrieval during data generation. Unlike Self-Instruct, which depends solely on internal knowledge, our method grounds both the instruction and the response in up-to-date, domain-specific documents. Unlike methods such as RAG and REALM, which apply retrieval at inference time, SearchInstruct applies it during dataset constructionallowing the fine-tuned model to internalize retrieved knowledge. This enables scalable, lightweight domain adaptation and continual knowledge refresh, bridging the gap between static SFT and real-world evolving needs."
        },
        {
            "title": "3 Methodology",
            "content": "The core idea behind our method is that an LLM, when provided with relevant contextual informationeven for domain-specific queriescan generate accurate and informative answers. These instructionresponse pairs can then be used as highquality data for SFT, improving the models ability to follow instructions and generalize in specialized domains. While prior work such as Self-Instruct (Wang et al., 2022a) demonstrated the value of gen2 Figure 1: The four-stage SearchInstruct pipeline: seed generation, query expansion, document retrieval, and response construction. in the current erating large quantities of instructionresponse examples, we argue that landscapewhere LLMs often outperform crowdannotated answersthe primary focus should shift from generating more answers to diversifying the questions themselves. broader and more realistic query distribution enables better coverage of domain-specific tasks, especially when answers are grounded in retrieved, up-to-date evidence. To operationalize this idea, we propose fourstage pipeline, illustrated in Figure 1, consisting of the following steps: 1. Seed Generation: Start with small set of domain-relevant, human-written or curated instruction prompts; 2. Query Expansion: Use an LLM to generate diverse variations and paraphrases of the seed prompts; 3. Document Retrieval: Retrieve domainspecific and up-to-date documents relevant to each expanded query (using web search, RAG, or external APIs); 4. Response Construction: Generate accurate, context-aware answers grounded in the retrieved evidence. In the following sections, we describe each stage of the pipeline in detail. 3.1 SearchInstruct Dataset Formulation We formally define our instruction dataset as set of instructionresponse pairs constructed through four-stage pipeline. Let = {q1, q2, . . . , qQ} denote the initial set of domain-specific seed queries. From Q, we generate set of additional instructions via LLM-based expansion. Let denote the set of newly generated instructions, and define the final instruction pool as: total = For each instruction total, we retrieve set of relevant textual contexts (e.g., documents, passages), denoted Ci = {ci,1, . . . , ci,k}. We then use an LLM to generate an answer Ai grounded in the retrieved context: Ai = LLManswer(i, Cfiltered ) The final instruction dataset is constructed as: = (cid:8)(i, Ai) (cid:12) (cid:12) total(cid:9) During training, the model learns the conditional instruction-following mapping: (i) Ai Note that while Ci is essential for answer generation during dataset construction, it is not required at inference or training time. This enables to internalize external knowledge through SFT and improve response quality in dynamic or specialized domains. 3.2 Stage 1: Seed Generation The SearchInstruct pipeline begins with the creation of diverse set of seed queries = 3 {q1, q2, . . . , qQ} that serve as the foundation for subsequent query expansion. Because all downstream instructions are generated based on these seeds, ensuring high quality and topical diversity in is critical. We consider two approaches for seed construction: Self-Instruct framework (Wang et al., 2022a), but adapted to focus exclusively on instruction generation rather than full instructionresponse pairs. At each expansion step, subset of seed queries = {q1, q2, . . . , qn} is selected, and the LLM is prompted to synthesize new instruction-style queries: Human-Crafted Seeds. In the fully manual setup, domain experts are given detailed instruction-writing guideline that includes: Requirements for seed diversity and realism, Types of questions that are difficult to synthesize from retrieved documents (e.g., abstract reasoning, subjective judgment), Subcategories within the target domain. Each annotator is asked to write several domainspecific instructions based on this guideline. This ensures that contains high-coverage, realistic questions that may not emerge from automatic generation methods. HumanLLM Collaborative Seeds. In the hybrid setup, annotators are guided to use powerful LLMs (e.g., GPT-4o, Claude 3.5 Sonnet, Gemini 2 Pro) to generate seeds. separate guideline is provided, which instructs annotators to: 1. Ask the LLM to suggest comprehensive list of subtopics within the target domain; 2. For each subtopic, prompt the LLM to generate multiple instruction-style queries based on predefined types; 3. Select, edit, or rewrite these outputs into final seeds. This method enables rapid scaling while retaining human supervision and correction. In practice, we collected approximately twice as many seeds per annotator compared to the fully manual setting. After initial training, we identify weaknesses in specific subdomains or instruction types. Additional seeds are then generatedusing either method aboveto target these weaknesses and enrich low-performing areas. Appendix provides detailed example of how seed queries were constructed in the domain of Iranian culinary and tourism. 3.3 Stage 2: Query Expansion To increase the diversity and coverage of the instruction dataset, we expand the initial seed set using LLM. This process is inspired by the IS = {i1, i2, . . . , ik} Each instruction ij IS is either paraphrase of single qi or novel combination of multiple seed queries in S. This process is repeated over iterations to generate sufficiently diverse instruction pool. We denote the union of all generated instructions across iterations as: = (cid:91) t=1 ISt The final instruction set used for downstream processing is defined as: total = Appendix A.1 shows the prompt template used to diversify queries during expansion. 3.4 Stage 3: Document Retrieval In this stage, we retrieve domain-relevant content to ground each instruction total in external evidence. The goal is to identify high-quality sources that can support accurate and informative answer generation in the next stage. Depending on the application setting and available resources, we consider multiple retrieval strategies: RAG-style retrieval: If access to structured document collection is available, we apply dense or hybrid retrieval to identify the topk text chunks Ci = {ci,1, . . . , ci,k} that are semantically similar to the instruction i. This is done using retriever models such as DPR or OpenAI embeddings. Web search: For open-domain instructions or in the absence of curated corpora, we use web search engines to collect set of relevant URLs and associated snippets as external context. 4 External APIs: In some cases, domainspecific APIs (e.g., tourism databases, legal knowledge graphs) are queried to extract structured or semi-structured evidence."
        },
        {
            "title": "4 Applications of SearchInstruct",
            "content": "The advantages of our proposed approach can be summarized in three key aspects: To improve the effectiveness of retrieval, we do not use the instruction directly. Instead, we construct search-oriented query qsearch from using an LLM-based rewriting module: qsearch = LLMrewriter(i) This transformation helps reduce ambiguity and match the query to the structure and vocabulary of the underlying retrieval source. Prompt template used for query rewriting is provided in Appendix A.2. 3.5 Stage 4: Response Construction In the final stage of the SearchInstruct pipeline, we synthesize high-quality answers for each instruction total using retrieved evidence. Each instruction is paired with its corresponding set of retrieved contexts Ci = {ci,1, . . . , ci,k} obtained from Stage 3. To ensure efficient and accurate generation, we pre-process the retrieved documents to filter out irrelevant or noisy content. This step is especially important when dealing with long documents, as excessive input length can degrade LLM performance and increase computational cost. We apply one of the following filtering strategies: LLM-based chunk filtering: lightweight language model is used to rank or extract segments of each document that are most relevant to the instruction; Rule-based filtering: Heuristics are applied to remove common noise sources such as HTML tags, ads, metadata, or user comments. After filtering, we concatenate the instruction with the cleaned context Cfiltered and prompt powerful LLM to generate contextually grounded answer: Ai = LLManswer(i, Cfiltered ) This produces the final instructionresponse pair (i, Ai) to be included in the training dataset D. Prompt templates for this stage are provided in Appendix A.3. 1. It enables the generation of accurate and contextually grounded responses for questions that require domain-specific knowledgeparticularly in cases where no existing LLM is capable of providing correct answer. 2. The instructionresponse pairs generated by this method provide close approximation of real end-user queries. 3. The method remains effective even when using smaller or open-source LLMs. As long as relevant documents are retrieved and appropriately incorporated, it is possible to construct high-quality responses to complex queries without relying on large proprietary models. In this work, we apply SearchInstruct in two primary settings: Constructing SFT datasets in the domain of Iranian culture, with focus on two subdomains: traditional cuisine and domestic tourism; Updating model knowledge with up-to-date information, to enhance its ability to respond to recent or evolving queries in specialized domains. In the following sections, we provide detailed analysis of both application scenarios. 4.1 SFT Dataset Construction for Iranian Culture Iran is vast and culturally diverse country, home to wide range of historical sites, regional customs, and intangible heritage. Many of these cultural and geographical details are either underrepresented or entirely absent in current LLMs. To support this claim, Appendix presents examples of domainspecific queries for which existing models failed to provide correct answers, while the SearchInstruct framework successfully generated accurate responses. Another challenge is that Persian speakers, particularly in informal contexts, often use colloquial and context-rich language that differs significantly from formal documents. Except for few major companies that serve Persian-speaking users, there 5 (a) Tourism domain (b) Culinary domain Figure 2: Human evaluation results comparing baseline models with those fine-tuned using the SearchInstruct framework across two domains. is limited access to large-scale, real-world Persian queries. Attempting to synthesize such instructions purely from documents using LLMs often fails to capture the diversity of real user intent and task formulations. For example, typical user might describe their own travel constraints and ask for multi-day itinerary for specific region of Irana type of instruction that is rarely (if ever) found explicitly in documents or websites. To evaluate the practical effectiveness of our method, we applied SearchInstruct in two cultural subdomains: traditional Iranian cuisine and domestic tourism. As discussed in the following sections, our results demonstrate the value of this approach in generating realistic, high-quality SFT data in resource-limited domains. Improvements in Cuisine and Tourism 4.1.1 In the MATINA project (Hosseinbeigi et al., 2025), multiple LoRA-tuned LLaMA-based models were trained across various domains, including traditional cuisine and tourism. The training data was generated using mix of document-based questionanswering and Evol-Instruct methods. However, during human evaluation, it became evident that the models failed to perform adequately in several cultural subdomains. Upon analysis, these shortcomings were largely attributed to gaps in training dataparticularly the absence of certain types of queries. These included questions such as: List multiple examples of... Imagine the following scenario... Recommend something based on personal constraints... Such instructions are rarely present in documents and are not well-covered by typical synthetic data pipelines. To address these gaps, we curated specialized seed set as described in Section 3.2 and Appendix C. The SearchInstruct pipeline was then applied to produce high-quality SFT data specifically targeting underrepresented question types and subdomains. The number of generated instances for each category using this method is reported in Table 1. Each stage in the table corresponds to one full iteration of the SearchInstruct pipelineincluding query expansion, document retrieval, and answer generationwhich was repeated in response to human feedback or model performance analysis. In each stage, the pipeline was refined to specifically address the issues and gaps identified during the previous round, leading to incremental improvements in coverage, quality, and diversity. Domain Stage 1 Stage 2 Stage 3 Overall Culinary Tourism 4273 2886 2378 1773 1250 8932 Table 1: Number of SFT samples generated in culinary and tourism domains across three iterative stages. Each stage reflects one cycle of the SearchInstruct pipeline applied to address feedback from the previous round. To evaluate the effectiveness of the proposed method, we conducted blind human evaluation. Five independent annotatorsnone of whom were involved in seed constructionwere asked to each design 20 diverse questions, resulting in 100question benchmark. This benchmark was used to evaluate two versions of the MATINA model: one trained prior to SearchInstruct augmentation, and one trained with our additional SFT data. Annotators were blind to the model identities2. As shown in Figure 2, models trained with SearchInstruct data performed significantly better on previously weak areas. These results validate the methods utility in real-world settings and highlight its ability to fill content gaps that standard data generation techniques miss. This setup also enables iterative refinement: once specific weaknesses in the models behavior 2Model identifiers were hidden from annotators during evaluation. 6 Figure 3: Iterative refinement loop enabled by the SearchInstruct framework. After initial fine-tuning, specific model weaknesses are identified through targeted evaluation. New instructionresponse data is then generated to directly address these shortcomings, creating feedback loop that leads to focused and incremental improvements in model performance. are identified through human evaluation or domainspecific testing, additional instructionresponse pairs can be generated using the SearchInstruct framework to target those areas. As shown in Figure 3, this creates continuous improvement loop in which domain-specific shortcomings are systematically addressed. The process can be repeated until performance stabilizes or marginal gains diminish, allowing for efficient and focused model enhancement over time. 4.2 Model Updating via SearchInstruct Another practical use case for the SearchInstruct framework is model updatingspecifically, targeted modification of model outputs based on recent information. The underlying intuition is that by retrieving up-to-date documents relevant to query, it is possible to revise the models response in lightweight and localized manner. To constrain the task for evaluation purposes, we focused on narrow update scope: tracking recent political changes and selected current events. Our goal was to update only specific knowledge while preserving the rest of the models internal representations, effectively performing controlled model editing. We applied this strategy to the Gemma 27B model. First, using set of manually curated seed instructions (see Appendix C.3), we generated queryresponse pairs via SearchInstruct. For each instruction, we obtained the models original output and retrieved relevant, up-to-date docu7 ments. These documents included new factssuch as political appointments, resignations, or verified current affairs. To incorporate the updated knowledge, we used secondary model such as DeepSeek to minimally revise the original answer from Gemma. The editing model was guided to only modify incorrect or outdated information based on the retrieved documents, without altering unrelated content. The prompt system used for editing is described in Appendix A.4. As result, only selected spans within the models responses were modified, ensuring that the updates were localized, accurate, and minimally invasive. schematic overview of the updatespecific data generation process is shown in Figure 4, and examples of before-and-after outputs are provided in Appendix D. 4.2.1 Evaluation of the Edited Model To evaluate the effectiveness of model updating via SearchInstruct, we constructed preference dataset for fine-tuning using the ORPO (Hong et al., 2024). For each instruction edit (instructions related to recent knowledge), we defined pair: (i, Areject , Achosen ) where Areject is the original output from the base model (e.g., Gemma), and Achosen is the edited response generated by secondary editing model (e.g., DeepSeek) based on up-to-date documents. Figure 4: Pipeline for constructing update-specific instruction data used in model editing. Starting from userprovided queries, relevant documents are retrieved and used to construct grounded instructionanswer pairs. Category STEM Social Sciences Humanities Other Average Gemma3 27B (original) Gemma3 27B (updated) 66.04 79.30 60.60 73.66 68.88 62.86 (-3.18) 77.87 (-1.43) 59.02 (-1.58) 71.65 (-2.01) 66.89 (-1.99) Table 2: Performance of Gemma3 27B before and after the update. Score decreases are highlighted in red. Using this preference data, we fine-tuned the model with ORPO to align its outputs toward the corrected information. We then evaluated the updated model in two ways: Targeted evaluation: new set of instructions related to the same domain (e.g., political changes) was submitted to both the original and edited models. Human evaluation confirmed that updated responses were correct and aligned with recent information. General evaluation: We also measured model performance on the MMLU benchmark (Hendrycks et al., 2021) to assess whether general knowledge was degraded. Results in Table 2 show no significant drop in accuracy, suggesting that the editing process preserved the broader knowledge of the model. However, further analysis revealed critical limitation. In some cases, the model was only capable of answering specific updated questions correctly (e.g., \"Who is the current president of Iran?\"), but lacked deeper knowledge about the subject. For instance, although the edited model correctly answered Masoud Pezeshkian as the current president, it failed to provide accurate information about his past roles or political backgroundfacts not present in the updated dataset and not previously encoded in the models internal knowledge. This indicates that the editing process effectively replaces surface-level facts for specific queries but does not deeply integrate new knowledge into the models reasoning capabilities. While model editing is efficient and minimally invasive, its ability to introduce deeply connected knowledge remains limited."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced SearchInstruct, novel framework for constructing high-quality instruction datasets for SFT. The method combines LLMdriven instruction generation with targeted document retrieval to address challenges related to diversity, realism, and recencyespecially in specialized domains. We demonstrated the effectiveness of this approach in two key scenarios: (1) enhancing model performance in cultural domains such as Iranian cuisine and tourism, and (2) updating factual knowledge in large language models using minimally invasive editing strategies. Experimental results showed that SearchInstruct can generate contextually accurate and domain-aligned data, leading to measurable improvements without degrading existing knowledge. Overall, SearchInstruct provides flexible and scalable solution for data construction and model editing, making it promising tool for the continued development of adaptive and domain-aware 8 language models."
        },
        {
            "title": "6 Limitations",
            "content": "Despite its effectiveness, the SearchInstruct framework has several limitations: Dependency on retrieved documents: The quality and accuracy of generated answers are directly tied to the reliability of retrieved content. In domains with sparse or noisy resources, this can limit performance. Shallow model editing: Our findings suggest that editing responses through surface-level substitution does not lead to deep conceptual integration. The model may memorize specific updates without understanding broader context. Limited scalability: Although semiautomated, applying the method across large or fast-changing domains still requires substantial resources for document retrieval, filtering, and validation. Reliance on strong LLMs: Several stages, including query expansion and response construction, depend on high-quality LLMs. In low-resource settings, performance may degrade. Risk of bias propagation: Web-based retrieval introduces potential biases from source documents, which can affect the neutrality and fairness of generated data."
        },
        {
            "title": "References",
            "content": "Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2023. Instructzero: Efficient instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning (ICML 2020), pages 39293938. PMLR. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR 2021). Also available as arXiv:2009.03300. Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1117011189, Miami, Florida, USA. Association for Computational Linguistics. Sara Bourbour Hosseinbeigi, MohammadAli SeifKashani, Javad Seraj, Fatemeh Taherinezhad, Ali Nafisi, Fatemeh Nadi, Iman Barati, Hosein Hasani, Mostafa Amiri, and Mostafa Masoudi. 2025. Matina: culturally-aligned Persian language In Findings model using multiple LoRA experts. of the Association for Computational Linguistics: ACL 2025, pages 2087420889, Vienna, Austria. Association for Computational Linguistics. Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internet-augmented language models through search. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84608478, Dublin, Ireland. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 94599474. Curran Associates, Inc. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Wen-tau Yih. 2024. Ra-dit: Retrieval-augmented dual instruction tuning. In Proceedings of the Eighth International Conference on Learning Representations (ICLR 2024). OpenReview.net / ICLR. Originally published as arXiv:2310.01352 (Oct 2, 2023). Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M. Saiful Bari, Canwen Xu, Urmish 9 Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, and 22 others. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations (ICLR), Virtual Event. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Xinyang Geng, Deepak Narayanan, Percy Liang, and Tatsunori B. Zhang. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca. Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Instructretro: Instruction tuning post retrieval-augmented pretraining. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), pages 5125551272. PMLR. Originally available as arXiv:2310.07713 (Oct 11, 2023). Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022a. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, and 19 others. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5085 5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. In Advances in Neural Information Processing Systems (NeurIPS). Canwen Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Zachary Ziegler, Ethan Perez, Tiago Pimentel, Emily Reif, Tianyi Zhang, Christopher Manning, and Colin Raffel. 2024. Craft: Conceptual retrieval-augmented fine-tuning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024). Association for Computational Linguistics."
        },
        {
            "title": "Appendix A\nSystem Prompt Designs",
            "content": "Each stage of our pipeline uses dedicated system prompt tailored to the specific objective of that component. This appendix presents the design of each prompt through structured illustrations. A.1 System Prompt for Query Expansion The system prompt used in this stage instructs the model to generate diverse and semantically rich variations of an initial instruction seed. This system prompt is illustrated in Figure 5 A.2 System Prompt for Search-Oriented Rewriting This prompt transforms general natural language instructions into search-optimized queries that enhance document retrieval performance. Figure 6: System prompt design for search-oriented query rewriting. A.3 System Prompt for Response Construction This prompt guides the model to synthesize grounded and context-aware response using retrieved documents as input. Figure 7: System prompt design for evidence-grounded response construction. A.4 System Prompt for Answer Updating The final system prompt is designed to apply local edits to existing answers when new or corrected information is introduced."
        },
        {
            "title": "Appendix B\nFailure Cases of Current LLM Models",
            "content": "answers to questions in specialized domains due to insufficient domain knowledge. However, when provided with relevant supporting documents, their performance improves significantly. This observation reflects one of the core ideas behind our proposed method, SearchInstruct. To demonstrate the effectiveness of our approach in situations where the base model fails to produce reliable response, we present set of representative examples in Table 3. For each case, we include the models original response as well as the revised answer generated using the SearchInstruct pipeline. These examples highlight how retrievalaugmented prompting can compensate for gaps in model knowledge and substantially improve response quality in specialized domains."
        },
        {
            "title": "Appendix C\nSeeds Generation",
            "content": "To generate high-quality SFT data, we begin with curated set of instruction seeds designed to ensure realism, topic diversity, and domain coverage. We employ two complementary strategies for constructing these seeds: (1) Human-Crafted Seeds. In the fully manual setup, domain experts were given detailed instruction-writing guideline that included: Requirements for seed diversity and contextual realism; Emphasis on question types that are difficult to synthesize from retrieved documents (e.g., abstract reasoning, subjective judgment); comprehensive taxonomy of subcategories within each domain. Each annotator was tasked with writing several domain-specific instructions that reflect real-world user needs and domain-specific reasoning challenges. This ensures that contains high-coverage, high-quality questions that may not emerge from automatic generation methods. (2) HumanLLM Collaborative Seeds. In the hybrid setup, annotators used powerful LLMs (e.g., GPT-4o, Claude 3.5 Sonnet, Gemini 2 Pro) to assist with ideation and rewriting. Annotators were instructed to: As discussed in the main body of the paper, current language models often fail to provide accurate 1. Ask the LLM to suggest list of subtopics within the domain; 11 Figure 5: System prompt design for query expansion. 2. For each subtopic, prompt the LLM to generate instruction-style questions based on predefined types; and reasoning-oriented prompts. Example questions include: My Fesenjan stew turned out too sour. How 3. Select, edit, or rewrite the outputs into final can fix the taste? seeds that match our quality criteria. This collaborative setup enabled faster idea generation, controlled diversity, and linguistic variation, while maintaining human oversight to ensure instructional clarity and domain relevance. In the following sections, we describe the seed construction process for each target domain in detail. C.1 Culinary Domain To construct seeds for the culinary domain, we followed dual strategy: fully human-written instructions and humanLLM collaborative generation. Our objective was to capture real-world cooking challenges that require reasoning beyond standard recipes, while ensuring topical diversity and realistic phrasing. C.1.1 Human-Crafted Seeds In the manual setup, domain experts wrote wide range of instruction-style questions grounded in common cooking situations. Rather than relying on simple recipe queries, the focus was placed on mistakes, decision-making, ingredient constraints, How can grill kebab without using charcoal grill? To encourage topic diversity, annotators were provided with guide listing more than 30 thematic categories. These included examples such as food preservation, regional dishes, dietary limitations, scientific explanations of cooking phenomena, ingredient reuse, children-oriented meals, and presentation aesthetics. These categories served purely as inspiration, not constraints, and annotators were encouraged to propose novel or cross-cutting question types that go beyond the listed themes. C.1.2 HumanLLM Collaborative Seeds In the collaborative setup, annotators were encouraged to interact with large language models (e.g., GPT-4o, Claude 3.5) to co-develop seeds. This included using the model to generate candidate questions, rewrite drafts in different tones, or suggest prompts within topic area. Common interaction patterns included: Suggest question about common mistakes in cooking rice. 12 Figure 8: System prompt design for answer refinement and updates. Rewrite this to be more natural: Why is the center of my cake undercooked? Human reviewers selected and refined the outputs to ensure clarity, relevance, and natural phrasing. This collaborative process enhanced topical coverage and linguistic diversity, while accelerating seed creation. C.2 Tourism Domain To construct seeds in the tourism domain, we aimed to capture real-world questions that reflect the complex needs, constraints, and reasoning challenges of travelers within Iran. The objective was to go beyond basic location lookups or generic suggestions and instead focus on experiential, cultural, seasonal, and logistical aspects of domestic tourism. We adopted dual strategy: (1) fully humanauthored seeds and (2) humanLLM collaborative construction. C.2.1 Human-Crafted Seeds In the manual setup, expert annotators were instructed to write instruction-style questions that simulate authentic user concerns in diverse travel scenarios. These included comparisons between destinations, cultural experiences, regional infrastructure challenges, weather-related planning, and cost-aware decisions. To ensure coverage, annotators were given guide listing more than 30 example categories. These included: historical landmarks, local cuisines, nature tourism, rural and nomadic travel, eco-tourism, handicrafts and souvenirs, accommodation types, seasonal destinations, transportation, adventure activities, underexplored sites, wellness tourism, and even environmental concerns such as climate impact on desert and coastal areas. These categories were intended for inspiration only, not as limitations. Annotators were encouraged to mix themes (e.g., cultural + seasonal + budget-focused) and explore niche or unconventional travel scenarios. Representative examples of well-formed questions include: If want to travel in winter to warm region with eco-lodges and low tourist density, where should go? What makes staying in traditional Yazdi house different from modern hotel, and which one would be better for three-day cultural trip? During my visit to Kandovan, the cave-like architecture stood out. How does it compare to stepped villages like Masuleh in terms of tourism experience? Why is Hormuz Island, although less famous than Qeshm or Kish, often described as more unique and immersive? 13 Table 3: Examples of partial outputs from GPT-4o and SearchInstruct (GPT4o mini):This demonstrates that in certain instructions, the SearchInstruct method is capable of producing better responses than larger models, as it benefits from access to highly relevant contextual information, even when using smaller model These questions require the model to reason across multiple dimensions, including cultural insight, regional comparison, travel logistics, and user preference. C.2.2 HumanLLM Collaborative Seeds In the collaborative setup, annotators interacted with large language models (e.g., GPT-4o, Claude 3.5) to generate, refine, or extend questions. They could prompt the model with general topic or theme and request multiple formulations, rewrites, or angle shifts. Typical prompts used in the process included: Generate questions about seasonal travel in northern Iran. Give me few ideas on how to ask about nomadic tourism experiences. Annotators reviewed and adjusted the models output for realism, clarity, and tone. This approach supported exploration of lesser-covered subdomains, stylistic variation, and faster seed creation, especially in cases where human inspiration plateaued. Together, these two methods enabled the creation of rich, diverse, and high-quality set of instruction seeds for the tourism domain, covering both typical and edge-case user needs. 14 C.3 Model Updating Domain Each entry consists of time-sensitive question Language models are often unable to respond accurately to questions involving real-world facts that have changed after the models training cut-off. These include updates to public figures, international events, institutional changes, or recent sports outcomes. In this section, we focus on designing instruction seeds that explicitly aim to identify and correct outdated model knowledge in such contexts. The goal was to create seed questions that prompt the model to revise its internal responses, particularly in scenarios where factual updates are essential. These seeds support fine-tuning or model editing workflows and serve as targeted evaluation tool for temporal generalization. Seeds were manually constructed by identifying areas where the model is likely to return outdated or incorrect answers. Each question was designed to be direct, fact-seeking, and anchored in wellknown public events. Special care was taken to select topics that are newsworthy and important yet unlikely to be politically sensitive in the context of scientific publication. Representative examples include: Who won the 2024 U.S. presidential election? Who is currently the president of Iran in 1404 (2025)? Has the war between Russia and Ukraine ended or is it still ongoing? What was Irans medal ranking in the 2024 Summer Olympics? Such questions are simple in structure but powerful in purpose: they help surface blind spots in the models temporal awareness and provide clear entry points for knowledge injection. The resulting dataset serves both as an editing benchmark and as fine-tuning input for time-sensitive model correction. Appendix Examples from the Updated Dataset This section presents examples from the final dataset constructed for model updating. These instances are designed to help the model revise outdated or incorrect knowledge in localized and controlled manner, without disrupting its broader behavior. accompanied by two responses: Rejected: An outdated, incorrect, or uncertain responsetypically resembling what an unedited, pre-trained model might produce. Chosen: minimally edited and factually updated version of the rejected answer. key principle in constructing the chosen responses is that the edits are minimal and highly targeted. Instead of rewriting the entire answer, only the incorrect portion is replaced with updated information, while preserving the original structure, phrasing, and tone as much as possible. This design allows the model to learn localized corrections through fine-tuning or model editing, rather than general retraining. Table 4 provides several representative samples. Each row shows real-world question, the outdated (rejected) model response, and the minimally revised, accurate version (chosen). These examples span topics such as elections, leadership updates, international conflicts, and sports results. Appendix Training Hyperparameters This appendix provides consolidated overview of the most relevant training hyperparameters used across different stages of our experiments. We categorize the configurations into three groups: SFT (Supervised Fine-Tuning) on domainspecific tasks using the Matina-8B and Matina-70B models, applied separately to culinary and tourism instruction datasets. ORPO (Optimal Response Preference Optimization) training on the Gemma-3-27B-IT model using LoRA adapters and custom ORPO objective. Table 5 presents unified summary of these hyperparameters, including key factors such as LoRA configuration, learning rate, dropout rate, batch size, training duration, and preference optimization parameters (where applicable) Appendix F: Future Work While SearchInstruct demonstrates promising results, several directions remain open for future exploration: 15 Table 4: Representative examples of minimally revised responses with factual updates. Each row includes user instruction, rejected response containing outdated or incorrect information, and chosen response with concise, targeted edits. The examples cover domains such as politics, international affairs, and sports, demonstrating factual correction with minimal changes to phrasing and tone. Deeper exploration of RAG methods and external APIs: Our current pipeline only briefly references the use of retrievalaugmented generation (RAG) techniques and external APIs, without thorough analysis of their practical impact. Future work could investigate how different RAG configurations, API sources, and retrieval strategies affect data diversity and factual accuracy across domains. agents are increasingly used to perform multistep tasks such as guided retrieval, source evaluation, answer synthesis, and refinement. Capturing these rich, interactive sequences offers an opportunity to create environments or trajectories for reinforcement learning (RL). This could facilitate training of more capable agents in real-world, information-seeking scenarios. Leveraging agent-based multi-step interactions for RL training: Large language model edge sources: resourcessuch 16 Integration with structured knowlIncorporating structured graphs, as knowledge Model / Task LoRA Rank LoRA Alpha DeepSpeed Stage Learning Rate Dropout Batch Size Epochs Pref. β Matina-8B (SFT) Culinary Tourism Matina-70B (SFT) Culinary Tourism 128 256 128 256 Gemma-3-27B-IT (ORPO) Update 16 256 512 256 512 32 Z0 Z3 Z3 Z3 1 104 1 104 1 104 1 104 0.05 0.05 0.05 0. 2 105 0.005 64 64 8 8 4 3 2 3 6 0.05 Table 5: Unified view of key hyperparameters for all training stages. Rows are grouped into three sections: SFT with Matina-8B, SFT with Matina-70B, and ORPO on Gemma-3-27B-IT. ontologies, or semantic lexiconscould improve answer precision and coherence, especially in knowledge-intensive domains like law, medicine, or education. Future pipelines may combine retrieved text with structured representations to enhance factual grounding. Automated optimization of the feedback loop: Our current framework includes an iterative improvement loop driven by human feedback and manual adjustments. promising future direction is to formalize this loop using reinforcement learning or other optimization techniques, allowing the system to continuously refine its query generation, retrieval, and synthesis strategies based on performance signals. Automated quality assessment tools: Evaluating the quality of generated training data remains bottleneck. Future work could explore automatic metrics or learned models that assess dimensions such as evidential coverage, factual accuracy, and stylistic consistency, reducing reliance on manual review."
        }
    ],
    "affiliations": [
        "Iran University of Science and Technology",
        "University of Tehran"
    ]
}