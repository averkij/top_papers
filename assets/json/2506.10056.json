{
    "paper_title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput",
    "authors": [
        "Gabriel Orlanski",
        "Nicholas Roberts",
        "Aws Albarghouthi",
        "Frederic Sala"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The standard paradigm for solving coding tasks via large language models (LLMs) is to generate-then-rank programs, where the latter step uses a verifier in the ranking process. The growing consensus is that a comprehensive verifier (e.g., a full test suite) should be prioritized over an outcome reward model (ORM) whenever possible, with little consideration given to the trade-offs involved. We aim to challenge this assumption by systematically exploring the tradeoff between speed and accuracy. We find that ORMs play a crucial role in scaling verification through trading accuracy for speed, even when a comprehensive verifier is available. Their value becomes especially apparent when used in a generate-prune-then-rank approach, where a faster but less accurate verifier removes incorrect solutions prior to ranking -- leading to a system that is 11.65x faster while only being 8.33% less accurate than the full test suite. We analyze the generate-prune-then-rank approach and show that it works by filtering out incorrect but highly ranked solutions. These findings enable the design of scalable and accurate program ranking systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 5 0 0 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Aws Albarghouthi",
            "content": "Frederic Sala University of Wisconsin-Madison {gorlanski,nick11roberts}@cs.wisc.edu"
        },
        {
            "title": "Abstract",
            "content": "The standard paradigm for solving coding tasks via large language models (LLMs) is to generate-then-rank programs, where the latter step uses verifier in the ranking process. The growing consensus is that comprehensive verifier (e.g., full test suite) should be prioritized over an outcome reward model (ORM) whenever possible, with little consideration given to the trade-offs involved. We aim to challenge this assumption by systematically exploring the tradeoff between speed and accuracy. We find that ORMs play crucial role in scaling verification through trading accuracy for speed, even when comprehensive verifier is available. Their value becomes especially apparent when used in generate-prune-then-rank approach, where faster but less accurate verifier removes incorrect solutions prior to rankingleading to system that is 11.65x faster while only being 8.33% less accurate than the full test suite. We analyze the generate-prune-then-rank approach and show that it works by filtering out incorrect but highly ranked solutions. These findings enable the design of scalable and accurate program ranking systems."
        },
        {
            "title": "Introduction",
            "content": "A popular approach to solving programming problems with large language model (LLM) is to generate-then-ranksample large number of candidates, rank them according to their correctness,1 and use the highest ranked candidate as the final answer. While practical and straightforward, this paradigm is bottlenecked by the verification process, as checking if program is correct can be prohibitively slow (e.g., running CI/CD for an entire enterprise codebase). The structure of programs creates trade-off between verification accuracy and speed: the fastest, yet least accurate, verifier is simple compilation check, while on the other end of the spectrum is complete test suite. However, neural verifierssuch as outcome reward models (ORMs)may not be subject to the same constraints. This raises an important question: Can ORMs significantly improve verification speed without sacrificing massive amount of accuracy? An emerging line of thought, derived from the application of reinforcement learning techniques for LLM training [14, 41, 31], suggests that when reliable and comprehensive verifier is available, reward models should be discarded due to their relative inaccuracy. Indeed, repeated sampling with such verifier has achieved strong performance at inference time in both software engineering [15] and general reasoning tasks [34, 7]. However, exclusively relying on verifiers in training and inference ignores the fact that the cost of verification scales with the complexity of the problem. This effect has previously been masked: as models struggled to solve any but the simplest of problems, verification 1That is, the program is correct according to the task definition. This is different from the traditional notion of verification in the programming language community. Preprint. Under review. Figure 1: Overview of our core contributions. We identify the trade-off between accuracy and throughput for systems that require verifying large number of programs. Through generate-prunethen-rank strategy, we show that the trade-off can be improved. By filtering candidates with weak verifier, the ORM can then rank more accurately than the ORM alone and efficiently compared to the strongest verifier. appeared nearly cost-free. However, as models have become capable of increasingly challenging software tasks, verification costs have grownrunning the comprehensive test suites SWE-Bench [26] is significantly more expensive than running the Python test cases for HumanEval [10]. These costs will drown out any performance gains from sampling more candidates. This notion suggests that ORMs are of practical value. The time to verify program with an ORM is determined by sequence length and number of parameters. It further does not require any of the infrastructure needed to execute program. Indeed, general reward models have shown promise in ranking solutions to math problems [11, 62, 35, 63]. However, the verification process for this problem setting is trivially expensive, as only single correct answer must be checked with primitive equivalence. Some works have explored using models to rank programs [24, 72, 42, 60], with some additionally examining how to incorporate filtering [56], but none have examined their role when considering the trade-off between accuracy and throughput. We address this gap by studying the role ORMs play in creating scalable code verification systems through an empirical analysis on the trade-off between accuracy and throughput. We examine their performance across four different programming tasks with increasingly larger verification costs. We create suite of non-model weak verifiers (i.e., syntax checking, linting, and running subset of test cases) that allow us to mitigate accuracy degradations by removing clearly incorrect candidates. Next, we train different-sized ORMs on the outcome of program verification and evaluate their ability to rank candidates from much larger LLM. We examine these in both the traditional generate-then-rank paradigm and in generate-prune-then-rank. Finally, we examine why pruning with weak verifiers can explore other areas of the trade-off curve. Our contributions are:2 ORMs have clear purpose for scalable program verification: We show that ORMs can be used to trade accuracy for throughput by achieving average speedups of 9.55 compared to the strongest verifier while still being 33.55% better than filtering with linter and taking the most common response across all of our tasks. Pruning with weak verifier prior to ranking improves accuracy and throughput: The generate-prune-then-rank strategy can reduce accuracy loss and further improve throughput when using an ORM. simple one test filter improves accuracy by 2.85% with an additional 2We release our code here https://github.com/SprocketLab/orm-code-verifier. 2 16.93% speedup over the ORM only strategy. Increasing the number of tests to 10 improves accuracy by 10.38% while only losing 16.69% throughput, yet this is still 29.71% faster than the full test suite. Weak verifiers mitigate ORM mistakes and reduce variance: We empirically show that the performance gains from pruning are due to the removal of both high and low ranked candidatesemphasizing the importance of weak verifiers to mitigate ORM inaccuracies."
        },
        {
            "title": "2 Accuracy-Speed Trade-Off For Verification",
            "content": "Our focus is on studying the role of ORMs in generate-prune-then-rank approach to coding tasks with LLMs. We first start with the formulation of the ranking problem. Given natural language programming problem x, set of candidate programs = {c1, . . . , cn} are generated from generator model, πG. verifier provides score to each ci C. These scores yield ranking : {1, . . . , n} where lower ranks are better: < (r(i)) > (r(i)). (1) The goal is to have the top ranked candidate r(1) be correct solution. We use Best-of-k [22] to evaluate the quality of the ranking: Best-of-k := 1 (cid:1) (cid:0)n nk (cid:88) i=0 (cid:19) (cid:18)ns 1 1 αr(i). (2) Here, αr(i) = 1 if r(i) is correct and 0 otherwise. Best-of-k measures the probability that in subset of solutions, the top ranked solution is correct. While other metrics for ranking exist (e.g. nDCG [25]), Best-of-k is more closely aligned with the process of generating large number of candidate solutions to be selected from. To measure the speed of verifier, we use the average programs processed per second (PPS). We refer to the cost of running verifier by pps(V ). This measurement incorporates parallelization and thus is more comprehensive measurement of ranking systems speed. The alternative would be to measure the average time taken to process single program, but this only measures the time spent on the processor and does not include any unavoidable OS-specific overhead that comes with parallelization (e.g. context switching)."
        },
        {
            "title": "2.1 Trade off between accuracy and speed",
            "content": "We assume that every programming problem has set of test cases {t1, . . . , tnt} that are used to determine the correctness of candidate ci through execution. In tasks such as competitive programming [34, 25] and large-scale software development [26], this is common and reasonable assumption. More broadly, even in the absence of predetermined set of test cases, there are many realistic and practical methods to obtain them. For example, one can use fuzzer or an LLM to generate test cases, including for large scale repositories [15]. Under the assumption that the test cases are sufficient to determine the correctness of program, the strongest verifier, Vall, runs all nt test cases. On the other end of the spectrum, the weakest verifier Vci is check that ci will compile. Between these two extremes, there exists set of weak verifiers, VSj , that run subset of the test cases Sj {t1, . . . , tnt}. This forms partial ordering on the set of verifiers in the verifier space V: Vall VSj Vci. Crucially, the result of Vall implies the result of all weaker verifiers. For example, candidate that passes Vall must, by definition, pass all of the test cases in {t1, . . . , tnt}. Thus ci must also pass VSj for all Sj {t1, . . . , tnt}. Further, for ci to pass any test cases, it must compile and pass the Vci . Intuitively, compilation check will be the fastest verifier to run while the Vall will be the slowest as it must run all nt test cases. This therefore creates trade-off between accuracy and speed. Let pps(V ) be the throughput of the verifier . We then have the reverse partial ordering of the verifier space: (3) pps(Vall) pps(VSj ) pps(Vci). (4)"
        },
        {
            "title": "2.2 Using an Outcome Reward Model as a Verifier",
            "content": "An ORM is value model: these evaluate how desirable generation is [61]. ORMs are trained to predict the overall outcome of process given potential solution to problem. In our setting, this means that the ORM is estimating the probability that given candidate, ci, is correct given the problem and the associated Vall. We further consider only binary outcomes of correctness where 1 means ci passed all tests, otherwise 0. We do so because establishing levels of correctness (e.g. 50% of tests passed means partially correct) requires setting an arbitrary threshold. We set the ORMs parameters to be θ. It estimates the probability of ci passing all of the test cases. That is, Rθ(cix) = Pθ(ci is correctx, ci) = Pθ(Vall(ci) = ntx, ci). (5) We use Bradley-Terry [5] preference objective to train the ORM following prior works on reward modelings [59, 47, 22]. We detail the objective in Appendix B. As Rθ produces score for each candidate, we can use it as verifier by setting Vθ(ci) := Rθ(cix). As mentioned in Sec. 2.1, the result of Vall can imply the result of weaker verifiers. Conversely, negative result of weaker verifier implies that ci is incorrect. For example, program that fails the first test case will never pass Vall. Running them through Vθ is thus unnecessary. Based on this idea, we propose using verifier VW to prune incorrect candidates prior to ranking with Vθ. Let VW+θ be the verifier that runs Vθ on the candidates that passed the VW . In the case of Vall, no candidates are ranked with Rθ as this is the strongest verifier available and thus we cannot achieve any better ranking without creating new Vall+1. In this setting, Vall now only tests subset of the test cases and becomes weak verifier."
        },
        {
            "title": "3 Experiment Design",
            "content": "In this section we first list then motivate the four research questions we aim to answer. We next describe the data generation, training, and evaluation details that are constant across our experiments. RQ1: Do ORMs provide scalable alternative to strong verifiers? The number of times one needs to run Vall linearly increases with the number of candidates. If Vall is fast, then this will not be an impediment to scaling up generate-then-rank pipeline. In the more realistic scenario, the verification time is not negligible (i.e., numerous test cases, significant setup time) but rather bottleneck. Thus using Vθ should be able to provide faster but less accurate ranking. Yet the performance should be better than naive pruning with weak verifier using majority vote via duplicates to break ties. RQ2: Can ORMs be combined with weak verifiers to further improve the tradeoff? Regardless of whether one uses Vθ or Vall, there will be compute wasted on candidates that are obviously incorrect. If candidate ci has incorrect syntax, then it can never be correct and thus trying to verify it is waste of compute. Therefore, pruning with some weak verifier should be able to reduce the amount of time spent verifying candidates and thus improve throughput. RQ3: Why does pruning with weak verifier improve ORM performance? As we observe that pruning with weak verifier improves the Best-of-k score, we next examine why this is the case. The immediate assumption is that this occurs because Vθ ranks incorrect candidates higher than they should be due to number of contributing factors. We additionally look at the reasons why this occurs through qualitative analysis of the candidates removed."
        },
        {
            "title": "3.1 Data Generation",
            "content": "To generate our training data for our ORMs, we use the training splits of CodeContests-Python [34] and GSM8K [11] in Program-aided Language Models (PAL) format [18]. We choose these datasets due their high quality and large training split size. All data is generated with Qwen 2.5 Coder Instruct 7B [23] with Int8 GPTQ quantization [16] to generate our training data. We sample maximum of 1024 tokens 128 times per problem with temperature of = 1.0 and nucleus sampling with topp = 0.95 [21]. We use the VLLM [29] library to generate our data. Besides these datasets, we use HumanEval [10] and Mostly Basic Programming Problems (MBPP [3]) to evaluate our models on shifted distribution of programs. We use the EvalPlus variant of these two datasets [36, 37]. We 4 format all of our data using Black3 to mitigate the whitespace perturbations required for the EvalPlus framework. Full details of our training data are found in Appendix B. For evaluation, we additionally deduplicate the candidate pool prior to running any ranking method. As this is not verifier and Rθ will return the same score for all programs that are identical, this is orthogonal to our focus. However, this does create the issue where the real size of the candidate pool is, , may not be the same for all problems in dataset. Thus to get fair measure of Best-of-k we construct α by repeating each outcome based on the number of duplicates of each program. Formally, ] where nDup(i) is the number of duplicates of ci , . . . , αC, . . . , αC , α2, . . . , α2 α = [α1, . . . , α1 (cid:125) (cid:124) (cid:123)(cid:122) (cid:124) (cid:125) (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:124) nDup(2) times nDup(1) times nDup(C) times and αi is the outcome of ci."
        },
        {
            "title": "3.2 Training Details",
            "content": "We use The Transformers Library [64] with Hugging Face Datasets [33] for training our models. We use FlashAttention 2.0 [13] and BFloat16 precision. We use the Adam optimizer [28] with cosine learning rate schedule that peaks at 5e 6 and 10% warmup steps. We use batch size of 64 with maximum sequence length of 2048. We train the ORM for 2 epochs on our dataset and use the last checkpoint for evaluation. Each setup is trained three times across three random seeds and we report the mean results. We discuss the full details of our training setup in the Appendix."
        },
        {
            "title": "3.3 Evaluation Details",
            "content": "For CodeContests and GSM8K we follow prior work and use the subprocess4 library to execute the programs written to disk [8, 44, 25]. For both CodeContests and GSM8K we set 30s timeout for each command in accordance with prior work [34]. HumanEval and MBPP are executed using the EvalPlus framework [36, 37]. All verifiers are run 5 times, and we use the average time taken to mitigate any variance in the runtime caused by outside factors. We only consider any preprocessing, execution, and postprocessing time of the programs while excluding the time it takes to write or read programs from the disk, as this is highly variable and beyond the scope of this work. For Vθ setups, we run all experiments on machine with single 48GB A6000 GPU, 16 cores, and 64GB of memory. For all non-Vθ setups, we run all verifiers with 32 cores and 128GB of memory.5 In the generate-then-rank (Vall) setup we ensure all tests run using the default execution settings for each dataset. At the time of writing, this setup is roughly equivalent in cost to the cost of VM with the same specifications as the machine we use to run the ORM ranking. To create α, we order the programs by the number of test cases returned by (in the case of Syntax, this is 1, and Lint is the negative number of errors). The Best-of-k score for the verifier-only setup we use is similar to the Majority Voting with Tests baseline from Ehrlich et al. [15] and Li et al. [34]. The number of times the exact program appeared in the full candidate pool is used as tiebreaker. For processing sequences with the ORM, we use dynamic batch size akin to packing [51] to have fair comparison to program execution. An operating system does not wait until all processes have finished before starting the next batch; it continuously starts new processes from the queue. Thus, it would be an unfair comparison to use fixed batch size that is upper bounded by the unrepresentative longest programs. This allows us to fully utilize the GPU in both memory and FLOPs similar to multiprocessing workloadsthus giving fair comparison to program execution."
        },
        {
            "title": "4 Trading Accuracy for Speed",
            "content": "We begin with summary of our results and then describe the results for each research question in detail. Our key results address the research questions described in the previous section: RQ1: We find that Vθ, on average, trades 30.64% Best-of-64 performance for an increase of 9.55 PPS over Vall. 3https://github.com/psf/black 4https://docs.python.org/3/library/subprocess.html 564 Cores and 256GB of memory are used to run the strongest verifier for the CodeContests. 5 Table 1: Overall results for the different pruning with weak verifier then ranking methods. If is that means no pruning is done. Green backgrounds is higher performance while Red backgrounds is lower performance with respect to the entire column. Vall is the case where all test cases are run. Syntax and Lint remove any programs with the respective errors. Test prunes out any programs that do not pass the first test cases. The evaluation dataset is generated with Qwen 2.5 Coder 7B Instruct using = 1.0, = 128, topp = 0.95, and 1024 tokens. The results are averaged over 3 runs. We report the standard deviations in Table 4. CodeContests PPS HumanEval PPS GSM8K MBPP Bof64 Bof64 Bof Bof64 Filter PPS PPS Vθ Syntax Lint 1 Test 3 Tests 10 Tests 500M 1.5B N/A 500M 1.5B N/A 500M 1.5B 5.21 6.36 2.55 5.65 6.89 2.67 5.09 6.73 15.72 N/A 500M 15.73 16.93 1.5B N/A 16.60 500M 16.93 17.74 1.5B N/A 17.12 500M 17.43 18.13 1.5B 52.45 22.78 6122.37 59.74 24.40 302.09 44.06 21.59 131.73 69.53 58. 126.10 69.19 61.93 120.93 73.23 65.82 83.95 86.76 83.32 84.57 87.98 83.39 84.03 86.56 262.40 114.25 6256.05 304.15 124.28 488.17 150.36 87.56 79.33 80.81 84.22 80.05 81.07 84.23 79.30 81.61 87.62 84.21 85. 91.11 87.76 87.20 92.46 90.28 89.63 143.52 64.31 6774.73 143.61 65.15 633.15 96.72 53.18 1020.54 117.10 60. 1239.61 158.92 76.95 650.77 134.92 72.85 66.09 69.29 66.63 65.21 70.45 67.44 66.55 69.86 76.07 75.25 76. 79.19 76.61 78.58 86.06 85.16 86.58 299.45 133.33 5679.06 347.07 145.19 613.41 162.56 97.69 930.69 214.06 126. 1112.31 274.44 157.05 345.21 169.31 121.32 Vall 20.69 2.95 97. 88.21 95.97 22.19 90.04 15.01 RQ2: We find that VS1+θ, is able to improve Best-of-64 performance by 2.85% and 16.93% PPS over Vθ. Increasing the number of tests to 10 decreases PPS to 16.69% while still being 11.65 faster than Vall. RQ3: We find that the top 5 highest ranking solutions removed by VS1 had an average rank of 54.73, while with VS10, the average rank of the top five candidates removed is 42.62. These results indicate that pruning with weak verifier works because it removes highly ranked but incorrect candidates."
        },
        {
            "title": "4.1 Do ORMs provide a scalable alternative to strong verifiers?",
            "content": "Table 1 details the overall results for evaluating different ranking methods on the same distribution as our training data. The candidates are generated by Qwen 2.5 Coder 7B Instruct model with temperature of 1.0, topp = 0.95, and maximum of 1024 tokens. We first detail the setup we use to compare Vθ and then discuss the results. Setup: To investigate our primary research question on whether Vθ can provide scalable alternative to Vall, we first detail the baselines we use to compare Vθ to. For Vall, we execute all of the problems and solutions with their datasets default execution settings and framework. We then create the ranking with the number of test cases passed, with the number of exact duplicates as the tiebreaker. This, essentially, forms the highest Best-of-64 score that can be achieved. On the other end of the spectrum, we use simple majority voting with linter, where solutions are ranked by the inverse number of linting errors then tiebroken by the number of exact duplicates. Results: As expected, Vθ is strong compromise between Majority Voting W/ Linter and Vall in terms of accuracy and throughput. Compared to Vall, Vθ is, on average, 9.55 faster while being 30.64% less accurate. Yet this is still 33.55% better than the simple Majority Voting W/ Linter. 6 Figure 2: Trade-off curves for the 14B generator. The colors represent different ranking strategies, while the markers represent different pruning methods. Majority Voting is the verifier-only setup where we use majority voting to select the best candidate after pruning with the weak verifier. We report the full results in Appendix and other model curves in Appendix H. Further, the models size allows for more precise selection of ones location in this trade-off. The 500M Vθ is 1.46 faster than the 1.5B Vθ while being 4.48% less accurate. To look at how this scales in terms of the size of πG, we examine the results for Vθ when using 3B and 14B πG. We find that the 500M Vθ is 12.34 faster while the 1.5B Vθ is 4.35 faster than Vall. These speedups cost Best-of-64 dropoff of 32.14% and 26.82%, respectively. We next ablate on the temperature used in generation, finding that the dropoff in accuracy Vθ is 28.01% and 24.03%, for the 500M and 1.5B Vθ, respectively. When lowering the generation temperature, the number of unique solutions generated goes down as there are more duplicates. Therefore, this is perfect method to also examine how Vθ can scale with larger number of samples. Across our five different temperatures, we find the average speedup compared to Vall is 217.51(113.61) and 89.31(46.56), for the 500M Vθ and 1.5B Vθ, respectively. This demonstrates their scalability with respect to the number of samples compared to Vall."
        },
        {
            "title": "4.2 Can ORMs be combined with weak verifiers to further improve the tradeoff?",
            "content": "Setup: We first begin by describing the verifiers we use in our experiments, then describe the method for measuring throughput, and, finally, describe how we produce Best-of-64 score for the verifier-only setup. The verifiers we use, in increasing order of strength, are: Vc : This is the weakest verifier that only checks if the solution is syntactically correct using Pythons AST parser.6 For the majority voting setup, we use the binary outcome as our initial sorting value. VLint: This verifier checks if the solution has any linting errors using PyLint.7 In the Majority Voting setup, we use the inverse number of lint errors as the score for the verifier. VSN : Run the first test cases of the problem with low timeout. Because we use low timeout, we keep all programs that pass at least one test case or timeout. For the Majority Voting setup, we use the number of test cases passed. Note that these are not running for GSM8K as Vall consists of single test case. Therefore, running this is equivalent to Vall. For all pruning methods, we run them 5 times using the same 16-core and 64GB setup, using the average time taken to mitigate any variance in the runtime caused by outside factors. For the majority voting setup, we use the number of test cases passed for the execution verifiers. Again, we use 32 cores and 128GB of memory to set up the majority voting setup. For the cases of the lower timeout execution verifiers, we keep all programs that pass the tests or timeout, as there can be correct programs that may take longer to run. In these cases, they would have been marked correct by Vall and thus we keep them. We include information about the tokens pruned in Table 5. Results: We show the different trade-offs for the 14B model in Figure 2. Looking at the CodeContests graph, we can clearly see that the VS1+θ and VS10+θ have much higher throughput than the Vθ. This is because the VS1+θ and VS10+θ can remove more tokens than it costs to run and the size of the Rθ. For HumanEval and MBPP, this is not the case because they are not removing as many solutions, and the average tokens per solution is lower, thus the cost to verify does not get outpaced by the 6https://docs.python.org/3/library/ast.html 7https://pylint.org/ 7 inference cost. In the case of the 500M Rθ, VSyntax+θ and VLint+θ only prune out an average of 1.30% and 2.62% candidates respectively (we report the complete token information in Table 5). Hence, we observe that these pruning methods lead to an overall 10.25 and 4.11 decrease in throughput for the 500M Rθ, respectively. In contrast, we observe general speed-ups for the 1.5B Rθ across all but the VLint+θ. Even though the candidates filtered across the runs are the same, the difference in throughput now saves FLOPs, as the cost of verification is now lower than the inference cost. Pruning with VSN +θ leads across the board to 12.45 increase in throughput compared to Vall. This is because the significantly higher number of candidates removed is beneficial regardless of size. Intriguingly, across all but VLint+θ, there is corresponding increase in Best-of-64 score. For the 500M Rθ, we see an increase of 45.74% in Best-of-64 score while for the 1.5B Rθ, we see an average rise of 35.88%. The best weak filter combination, VS10+θ with the 1.5B Rθ, is only, on average, 7.62% worse than Vall while still being 10.23 faster. We observe similar results when using 14B πG with speed up of 8.62 and only 7.68% drop in Best-of-64 score compared to Vall. Again in the temperature ablations, we observe that the PPS of the pruning with tests strategy has an average PPS of 146.16 (68.86) and 89.26 (33.68) for the 500M and 1.5B Vθ, respectively. This is, on average, 13.44 faster than Vall while only being 8.88 worse in Best-of-64 score. These results imply that pruning not only improves throughput but also improves the performance of the verifier. Thus, they can be seen as way to move on the trade-off curve for the verifier and get near Vall performance at significant speedup. It is easy to see that this approach has massive potential for both inference and training applications. For the latter specifically, this generate-prune-then-rank approach can likely significantly speed up each step by reducing the number of candidates that need to be ranked or checked with the full verifier."
        },
        {
            "title": "4.3 Why does pruning with a weak verifier improve ORM performance?",
            "content": "Figure 3: Distribution of failed candidates based on Vθ rank. rank of 1 means the candidate was the top-ranked, while 128 means it was the lowest-ranked. The rows are the individual verifiers while the columns are the datasets. This is for the 1.5B Vθ. The graph for the 500M Vθ is in Figure 6. Setup: By construction in Equation 2, Best-of-64 can be increased by improving the ranking model and thus moving correct solutions higher in the ranking. We do not further train Rθ after the initial training in our generate-prune-then-rank setup. Therefore, the only other way to improve the rank of correct solutions is by removing incorrect candidates that were ranked higher. To see if this is the reason, we examine the rank distribution of candidates removed by the verifier. Results: We plot the results in Figure 3. Specifically, consider the two left-most bins for the MBPP column. As we traverse down the column, and thus increase the strength of the verifier, we see that those two bins grow in size. This indicates that the pruning removes candidates that Rθ had ranked highly during the Vθ setting. Removing these candidates should improve Best-of-64. Quantitatively, we can look at the original ranks of the five top-ranked candidates removed. If the average rank of these solutions is high, then that means that the Vθ ranked them towards the end of the candidate pool, 8 thus correctly. If it is low, then that means the Vθ incorrectly ranked them as correct solutions, and thus we should see Best-of-64 improvement. For the Syntax pruner, the average rank of the top five candidates removed is 108.33. Increasing the strength of the verifier to VS1 further lowers this to 54.73. Finally, when using VS10 , the average rank of the top five candidates removed is 42.62. We provide the full results in Table 6. These results also indicate that the pruning denoises the results from ranking with Rθ. When using VS10+θ, the average σ of Best-of-64 goes down by 17.53% and 45.72% for the 500M and 1.5B Rθ, respectively. As we hypothesized, the verifier is removing high-ranked but incorrect candidates, thus indicative of the underlying weaknesses of the Rθ. Specifically, these findings confirm the consensus that ORMs are noisy and unreliable. But, pruning with weak verifier can overcome this issue, as we have shown. Further, when looking at the results for the 1.5B Rθ, we observe that the Vθ can outperform the majority voting baseline for CodeContestsindicating that the Rθ does surprisingly have the ability to distinguish between nearly correct and entirely correct candidates."
        },
        {
            "title": "5 Related Work",
            "content": "Complexity of Verification in Programming Tasks: Until recently, the majority of evaluation works in the programming domain relied on surface level evaluation to check if it was correct [4, 66, 68, 65, 38]. The introduction of HumanEval [10], MBPP [3], and APPS [20] spurred on new wave of works that included verification through execution for general programming tasks [30, 8, 43, 2, 44, 52, 25, 34]. Recently, the focused has shifted from self-contained programming problems to general software engineering tasks with the introduction of SWE-Bench [26]. In this, verification involves launching Docker container, applying the model generated edit, then running the entire test suite of repository. Ranking Solutions with LLMs: Reward models gained popularity with the introduction of Reinforcement Learning From Human Feedback [59, 47]. In this setting, reward model is trained on human preference annotations and then used during training to optimize policy. However, they can also be used to rank solutions at test-time. Most prominently, outcome reward models have been used to rank solutions to math problems [11, 55, 70, 67, 27]. Recently there has been more interest in using process reward models that determine if individual steps are correct rather than the final outcome of solution [62, 35, 63, 39, 6, 19, 17, 53]. Beyond that there has been interest recently attempting to reason through solution to verify if it is correct through generation with LLM [73, 71]. The closest work to ours is Singhi et al. [58] who look into when it is best to use an LLM as verifier with chain-of-thought or sample more. Ranking Programs: There has been growing body of work in this direction, specifically using techniques that work for math problems [69, 22]. Ni et al. [42] includes execution results on few public tests in their training and test-time prompts. Another intuitive and proven approach is to have an LLM trace through program with given input and determine if the correct output will be produced [24, 60]. Some works have proposed using secondary LLM as reviewer to judge the correctness of solution [72, 49, 32, 74] but this additional overhead removes any efficiency gains. Shi et al. [56] do filter programs prior to ranking with broad executability\" filter, but do not examine the interplay between the accuracy of ranking model and the cost of verification. Recently, Ehrlich et al. [15] looks into selection methods when scaling the number of sampled trajectories when under constrained monetary budget."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work we show that ORMs play crucial in creating high throughput program verification systems through trading accuracy for speed. We then demonstrate that through generate-prunethen-rank approach, we can create system that is more accurate than using an ORM alone while still being faster than full test suite. Finally we demonstrate that this approach works by filtering out incorrect solutions that are highly ranked and saving tokens needed to be ranked. Our results setup future work on creating execution free verifiers based on the invariant properties of both the task and the specific problem further improving the trade-off between speed and accuracy. In the training application, future work can now focus on applying the generate-prune-then-rank approach to reinforce learning from verifiable rewards to create faster training systems."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We would like to thank Ryan Carelli, Sungjun Cho, and Xavier Garcia for their feedback on the manuscript. We would also like to thank Abtin Molavi and Amanda Xu for their helpful comments on the visualizations and the paper. We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision) and the Wisconsin Alumni Research Foundation (WARF). This research was done using services provided by the OSG Consortium [48, 54, 45, 46], which is supported by the National Science Foundation awards #2030508 and #2323298."
        },
        {
            "title": "References",
            "content": "[1] L. B. Allal, N. Muennighoff, L. K. Kumar Umapathi, B. Lipkin, and L. von Werra. URL https:// github.com/bigcode-project/bigcode-evaluation-harness. original-date: 2022-0809T12:58:56Z. bigcode-project/bigcode-evaluation-harness, Apr. 2025. [2] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, S. K. Gonugondla, H. Ding, V. Kumar, N. Fulton, A. Farahani, S. Jain, R. Giaquinto, H. Qian, M. K. Ramanathan, R. Nallapati, B. Ray, P. Bhatia, S. Sengupta, D. Roth, and B. Xiang. Multi-lingual Evaluation of Code Generation Models. ArXiv, abs/2210.14868, 2022. URL https://api.semanticscholar.org/CorpusID:253116642. [3] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton. Program Synthesis with Large Language Models, Aug. 2021. URL http://arxiv.org/abs/2108.07732. arXiv:2108.07732 [cs]. [4] A. V. M. Barone and R. Sennrich. Parallel Corpus of Python Functions and Documentation In International Joint Strings for Automated Code Documentation and Code Generation. Conference on Natural Language Processing, 2017. URL https://api.semanticscholar. org/CorpusID:21616326. [5] R. A. Bradley and M. E. Terry. Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons. Biometrika, 39(3/4):324345, 1952. ISSN 0006-3444. doi: 10.2307/ 2334029. URL https://www.jstor.org/stable/2334029. Publisher: [Oxford University Press, Biometrika Trust]. [6] D. Brandfonbrener, S. Henniger, S. Raja, T. Prasad, C. Loughridge, F. Cassano, S. R. Hu, J. Yang, W. E. Byrd, R. Zinkov, and N. Amin. VerMCTS: Synthesizing Multi-Step Programs using Verifier, Large Language Model, and Tree Search, May 2024. URL http://arxiv. org/abs/2402.08147. arXiv:2402.08147 [cs]. [7] B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. Ré, and A. Mirhoseini. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling, Dec. 2024. URL http://arxiv.org/abs/2407.21787. arXiv:2407.21787 [cs]. [8] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee, Y. Zi, C. J. Anderson, M. Q. Feldman, A. Guha, M. Greenberg, and A. Jangda. MultiPL-E: Scalable and Extensible Approach to Benchmarking Neural Code Generation, Dec. 2022. URL http://arxiv.org/abs/2208.08227. arXiv:2208.08227 [cs]. [9] Center for High Throughput Computing. Center for high throughput computing, 2006. URL https://chtc.cs.wisc.edu/. [10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating Large Language Models Trained on Code, July 2021. URL http://arxiv.org/abs/2107.03374. arXiv:2107.03374 [cs]. 10 [11] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training Verifiers to Solve Math Word Problems, Nov. 2021. URL http://arxiv.org/abs/2110.14168. arXiv:2110.14168 [cs]. [12] N. Dai, Z. Wu, R. Zheng, Z. Wei, W. Shi, X. Jin, G. Liu, C. Dun, L. Huang, and L. Yan. Process supervision-guided policy optimization for code generation, 2025. URL https: //arxiv.org/abs/2410.17621. [13] T. Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, July 2023. URL http://arxiv.org/abs/2307.08691. arXiv:2307.08691 [cs]. [14] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, Jan. 2025. URL http://arxiv.org/abs/2501.12948. arXiv:2501.12948 [cs]. [15] R. Ehrlich, B. Brown, J. Juravsky, R. Clark, C. Ré, and A. Mirhoseini. CodeMonkeys: Scaling Test-Time Compute for Software Engineering, Feb. 2025. URL http://arxiv.org/abs/ 2501.14723. arXiv:2501.14723 [cs]. [16] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, Mar. 2023. URL http://arxiv.org/abs/ 2210.17323. arXiv:2210.17323 [cs]. [17] B. Gao, Z. Cai, R. Xu, P. Wang, C. Zheng, R. Lin, K. Lu, D. Liu, C. Zhou, W. Xiao, J. Hu, T. Liu, and B. Chang. LLM Critics Help Catch Bugs in Mathematics: Towards Better Mathematical Verifier with Natural Language Feedback, July 2024. URL http://arxiv.org/abs/2406. 14024. arXiv:2406.14024 [cs]. [18] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. PAL: Program-aided Language Models. ArXiv, abs/2211.10435, 2022. URL https://api. semanticscholar.org/CorpusID:253708270. [19] M. He, Y. Shen, W. Zhang, Z. Tan, and W. Lu. Advancing Process Verification for Large Language Models via Tree-Based Preference Learning, June 2024. URL http://arxiv.org/ abs/2407.00390. arXiv:2407.00390 [cs]. [20] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. X. Song, and J. Steinhardt. Measuring Coding Challenge Competence With APPS. ArXiv, abs/2105.09938, 2021. URL https://api.semanticscholar.org/CorpusID: 234790100. [21] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The Curious Case of Neural Text Degeneration, Feb. 2020. URL http://arxiv.org/abs/1904.09751. arXiv:1904.09751 [cs]. 11 [22] A. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni, and R. Agarwal. V-STaR: Training Verifiers for Self-Taught Reasoners, Aug. 2024. URL http://arxiv.org/abs/2402.06457. arXiv:2402.06457 [cs]. [23] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, K. Dang, Y. Fan, Y. Zhang, A. Yang, R. Men, F. Huang, B. Zheng, Y. Miao, S. Quan, Y. Feng, X. Ren, X. Ren, J. Zhou, and J. Lin. Qwen2.5-Coder Technical Report, Nov. 2024. URL http: //arxiv.org/abs/2409.12186. arXiv:2409.12186 [cs]. [24] J. P. Inala, C. Wang, M. Yang, A. Codas, M. Encarnación, S. K. Lahiri, M. Musuvathi, and J. Gao. Fault-Aware Neural Code Rankers. Oct. 2022. URL https://openreview.net/ forum?id=LtJMqnbslJe. [25] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code, June 2024. URL http://arxiv.org/abs/2403.07974. arXiv:2403.07974 [cs]. [26] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. SWEbench: Can Language Models Resolve Real-World GitHub Issues?, Apr. 2024. URL http: //arxiv.org/abs/2310.06770. arXiv:2310.06770 [cs]. [27] Z. Kang, X. Zhao, and D. Song. Scalable Best-of-N Selection for Large Language Models via Self-Certainty, Feb. 2025. URL http://arxiv.org/abs/2502.18581. arXiv:2502.18581 [cs]. [28] D. P. Kingma and J. Ba. Adam: Method for Stochastic Optimization, Jan. 2017. URL http://arxiv.org/abs/1412.6980. arXiv:1412.6980 [cs]. [29] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention, Sept. 2023. URL http://arxiv.org/abs/2309.06180. arXiv:2309.06180 [cs]. [30] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. Yih, D. Fried, S.-y. Wang, and T. Yu. DS-1000: Natural and Reliable Benchmark for Data Science Code Generation. ArXiv, abs/2211.11501, 2022. URL https://api.semanticscholar.org/CorpusID: 253734939. [31] N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V. Miranda, A. Liu, N. Dziri, S. Lyu, Y. Gu, S. Malik, V. Graf, J. D. Hwang, J. Yang, R. L. Bras, O. Tafjord, C. Wilhelm, L. Soldaini, N. A. Smith, Y. Wang, P. Dasigi, and H. Hajishirzi. Tulu 3: Pushing Frontiers in Open Language Model Post-Training, Feb. 2025. URL http://arxiv.org/abs/ 2411.15124. arXiv:2411.15124 [cs]. [32] K. Levin, N. van Kempen, E. D. Berger, and S. N. Freund. ChatDBG: An AI-Powered Debugging Assistant. 2024. doi: 10.48550/ARXIV.2403.16354. URL https://arxiv.org/ abs/2403.16354. Publisher: arXiv Version Number: 1. [33] Q. Lhoest, A. V. d. Moral, Y. Jernite, A. Thakur, P. v. Platen, S. Patil, J. Chaumond, M. Drame, J. Plu, L. Tunstall, J. Davison, M. Šaško, G. Chhablani, B. Malik, S. Brandeis, T. L. Scao, V. Sanh, C. Xu, N. Patry, A. McMillan-Major, P. Schmid, S. Gugger, C. Delangue, T. Matussière, L. Debut, S. Bekman, P. Cistac, T. Goehringer, V. Mustar, F. Lagunas, A. M. Rush, and T. Wolf. Datasets: Community Library for Natural Language Processing, Sept. 2021. URL http://arxiv.org/abs/2109.02846. arXiv:2109.02846 [cs]. [34] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. d. M. dAutume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. d. Freitas, K. Kavukcuoglu, and O. Vinyals. Competition-Level Code Generation with AlphaCode. Science, 378(6624):10921097, Dec. 2022. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.abq1158. URL http://arxiv.org/abs/2203.07814. arXiv:2203.07814 [cs]. 12 [35] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets Verify Step by Step, May 2023. URL http://arxiv.org/ abs/2305.20050. arXiv:2305.20050 [cs]. [36] C. Liu, S. Lu, W. Chen, D. Jiang, A. Svyatkovskiy, S. Fu, N. Sundaresan, and N. Duan. Code Execution with Pre-trained Language Models, May 2023. URL http://arxiv.org/abs/ 2305.05383. arXiv:2305.05383. [37] J. Liu, S. Xie, J. Wang, Y. Wei, Y. Ding, and L. Zhang. Evaluating Language Models for Efficient Code Generation, Aug. 2024. URL http://arxiv.org/abs/2408.06450. arXiv:2408.06450 [cs]. [38] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. Liu. CodeXGLUE: Machine Learning Benchmark Dataset for Code Understanding and Generation. ArXiv, abs/2102.04664, 2021. URL https: //api.semanticscholar.org/CorpusID:231855531. [39] L. Luo, Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu, Y. Zhu, L. Meng, J. Sun, and A. Rastogi. Improve Mathematical Reasoning in Language Models by Automated Process Supervision, June 2024. URL http://arxiv.org/abs/2406.06592. arXiv:2406.06592 [cs]. [40] Q. Ma, H. Zhou, T. Liu, J. Yuan, P. Liu, Y. You, and H. Yang. Lets reward step by step: Step-Level reward model as the Navigators for Reasoning, Oct. 2023. URL http://arxiv. org/abs/2310.10080. arXiv:2310.10080 [cs]. [41] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-time scaling, Feb. 2025. URL http://arxiv. org/abs/2501.19393. arXiv:2501.19393 [cs]. [42] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. I. Wang, and X. V. Lin. LEVER: Learning to Verify Language-to-Code Generation with Execution, Sept. 2023. URL http://arxiv.org/ abs/2302.08468. arXiv:2302.08468 [cs]. [43] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Conversational Paradigm for Program Synthesis. ArXiv, abs/2203.13474, 2022. URL https://api.semanticscholar.org/CorpusID:247749083. [44] G. Orlanski, K. Xiao, X. Garcia, J. Hui, J. Howland, J. Malmaud, J. Austin, R. Singh, and M. Catasta. Measuring The Impact Of Programming Language Distribution, May 2023. URL http://arxiv.org/abs/2302.01973. arXiv:2302.01973 [cs]. [45] OSG. Ospool, 2006. URL https://osg-htc.org/services/open_science_pool.html. [46] OSG. Open science data federation, 2015. URL https://osdf.osg-htc.org/. [47] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. E. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. URL https: //api.semanticscholar.org/CorpusID:246426909. [48] R. Pordes, D. Petravick, B. Kramer, D. Olson, M. Livny, A. Roy, P. Avery, K. Blackburn, T. Wenaus, F. Würthwein, I. Foster, R. Gardner, M. Wilde, A. Blatecky, J. McGee, and R. Quick. The open science grid. In J. Phys. Conf. Ser., volume 78 of 78, page 012057, 2007. doi: 10.1088/1742-6596/78/1/012057. [49] C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y. Su, Y. Dang, J. Li, J. Xu, D. Li, Z. Liu, and M. Sun. Communicative Agents for Software Development, Dec. 2023. URL http: //arxiv.org/abs/2307.07924. arXiv:2307.07924 [cs]. [50] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model, Dec. 2023. URL http: //arxiv.org/abs/2305.18290. arXiv:2305.18290 [cs]. 13 [51] A. Roberts, C. Raffel, and N. Shazeer. How Much Knowledge Can You Pack Into the Parameters of Language Model?, Oct. 2020. URL http://arxiv.org/abs/2002.08910. arXiv:2002.08910 [cs]. [52] T. Schuster, A. Kalyan, O. Polozov, and A. T. Kalai. Programming Puzzles. ArXiv, abs/2106.05784, 2021. URL https://api.semanticscholar.org/CorpusID: 235390706. [53] A. Setlur, S. Garg, X. Geng, N. Garg, V. Smith, and A. Kumar. RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold, June 2024. URL http: //arxiv.org/abs/2406.14532. arXiv:2406.14532 [cs]. [54] I. Sfiligoi, D. C. Bradley, B. Holzman, P. Mhashilkar, S. Padhi, and F. Wurthwein. The pilot way to grid resources using glideinwms. In 2009 WRI World Congress on Computer Science and Information Engineering, volume 2 of 2, pages 428432, 2009. doi: 10.1109/CSIE.2009.950. [55] J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and Q. Liu. Generate & Rank: Multi-task Framework for Math Word Problems, Sept. 2021. URL http://arxiv.org/abs/ 2109.03034. arXiv:2109.03034 [cs]. [56] F. Shi, D. Fried, M. Ghazvininejad, L. Zettlemoyer, and S. I. Wang. Natural Language to Code Translation with Execution. ArXiv, abs/2204.11454, 2022. URL https://api. semanticscholar.org/CorpusID:248377325. [57] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, Oct. 2023. URL http://arxiv.org/abs/2303. 11366. arXiv:2303.11366 [cs]. [58] N. Singhi, H. Bansal, A. Hosseini, A. Grover, K.-W. Chang, M. Rohrbach, and A. Rohrbach. When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning. Apr. 2025. URL https://www.semanticscholar.org/paper/ When-To-Solve%2C-When-To-Verify%3A-Compute-Optimal-and-Singhi-Bansal/ d170e69de7519e926bee6af648c9ffd625689600. [59] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, In Advances and P. F. Christiano. in Neural Information Processing Systems, volume 33, pages 30083021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1f89885d556929e98d3ef9b86448f951-Abstract.html. Learning to summarize with human feedback. [60] Z. Sun, Y. Wan, J. Li, H. Zhang, Z. Jin, G. Li, and C. Lyu. Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates. 2024. doi: 10.48550/ARXIV.2408.13976. URL https://arxiv.org/abs/2408.13976. Publisher: arXiv Version Number: 3. [61] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Bradford Book, Cambridge, MA, USA, Oct. 2018. ISBN 978-0-262-03924-6. [62] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with processand outcome-based feedback, 2022. URL https://arxiv.org/abs/2211.14275. Publisher: arXiv Version Number: 1. [63] P. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations, Feb. 2024. URL http://arxiv.org/abs/2312.08935. arXiv:2312.08935 [cs]. [64] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. v. Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. HuggingFaces Transformers: State-of-theart Natural Language Processing, July 2020. URL http://arxiv.org/abs/1910.03771. arXiv:1910.03771 [cs]. 14 [65] Z. Yao, D. S. Weld, W.-P. Chen, and H. Sun. StaQC: Systematically Mined Question-Code Dataset from Stack Overflow. Proceedings of the 2018 World Wide Web Conference, 2018. URL https://api.semanticscholar.org/CorpusID:3396350. [66] P. Yin, B. Deng, E. Chen, B. Vasilescu, and G. Neubig. Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow, May 2018. URL http://arxiv.org/abs/ 1805.08949. arXiv:1805.08949 [cs]. [67] F. Yu, A. Gao, and B. Wang. OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning. In K. Duh, H. Gomez, and S. Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 858875, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.55. URL https://aclanthology.org/2024.findings-naacl.55. [68] T. Yu, R. Zhang, K.-C. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Z. Li, Q. Yao, S. Roman, Z. Zhang, and D. R. Radev. Spider: Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. ArXiv, abs/1809.08887, 2018. URL https://api.semanticscholar.org/CorpusID:52815560. [69] Z. Yu, W. Gu, Y. Wang, Z. Zeng, J. Wang, W. Ye, and S. Zhang. Outcome-Refining Process Supervision for Code Generation, Dec. 2024. URL http://arxiv.org/abs/2412.15118. arXiv:2412.15118 [cs]. [70] H. Zhang, P. Wang, S. Diao, Y. Lin, R. Pan, H. Dong, D. Zhang, P. Molchanov, and T. Zhang. Entropy-Regularized Process Reward Model, Dec. 2024. URL http://arxiv.org/abs/ 2412.11006. arXiv:2412.11006 [cs]. [71] L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal. Generative Verifiers: Reward Modeling as Next-Token Prediction, Oct. 2024. URL http://arxiv.org/abs/2408. 15240. arXiv:2408.15240. [72] T. Zhang, T. Yu, T. B. Hashimoto, M. Lewis, W.-t. Yih, D. Fried, and S. I. Wang. Coder Reviewer Reranking for Code Generation, Nov. 2022. URL http://arxiv.org/abs/2211.16490. arXiv:2211.16490. [73] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena, Dec. 2023. URL http://arxiv.org/abs/2306.05685. arXiv:2306.05685 [cs]. [74] L. Zhong, Z. Wang, and J. Shang. LDB: Large Language Model Debugger via Verifying Runtime Execution Step-by-step, Feb. 2024. URL http://arxiv.org/abs/2402.16906. arXiv:2402.16906 [cs] version: 1."
        },
        {
            "title": "A Broader Impact and Limitations",
            "content": "Broader Impact While making code generation models more efficient has the potential to increase productivity, automation also has the potential to impact jobs that are centered on basic software engineering tasks. The net results and full downstream impact of improvements made to code generation algorithms is perhaps up for debate, but this should be noted by any work in this area. Limitations Our work has two primary limitations. First, we cannot directly evaluate on popular benchmarks like SWE-bench [26] because the training data is not publicly available and it is too expensive to generate enough trajectories for our experiments. Second, we do not analyze the performance of chain of thought (CoT) verifiers [40], process reward models [12], or other iterative refinement methods [57] on our method, as multiple forward passes linearly decreases throughput."
        },
        {
            "title": "B Training Details Continued",
            "content": "Table 2: Details of our training datasets. # Probs is the number of problems in the dataset. Prob. Tok is the average number of tokens in the problem description. Tokens is the average number of tokens in the problem and solutions when formatted with the prompt. Sols. is the average number of unique non-syntax error solutions per problem. # Passed is the mean number of passing solutions per problem. % Passed is the percentage of problems that had at least one passing solution. # Pairs is the average number of possible pairs per problem, where are pair is 1 correct and 1 incorrect solution. Dataset # Passed % Passed Prob. Tok Tokens # Probs # Pairs Sols."
        },
        {
            "title": "CodeContests",
            "content": "GSM8K"
        },
        {
            "title": "Train\nVal\nTrain\nVal",
            "content": "13,213 117 7,373 100 776.76 921.66 160.16 177.42 508.08 625.66 59.55 66.63 124.78 126.31 83.50 105.93 14.00 4.36 67.03 78.66 35.92 23.08 98.87 100. 6.58 3.61 9.26 15.16 B.1 Labels To determine correctness, we follow existing works [8, 44, 25] in using the subprocess8 library to execute the programs with the test cases. For GSM8K, we check that the outputted scalar is correct using an assertion. In the case of CodeContests, we check that the program prints the correct output to STDOUT and stop execution on the first error. We detail our training data in Table 2. We filter out any programs with syntax errors as these could be caused by the model hitting the token limit rather than poor generation. B.2 Hyperparameters We use The Transformers Library [64] with Hugging Face Datasets [33] for training our models. We use FlashAttention 2.0 [13] and BFloat16 precision. We use the Adam optimizer [28] with β1 = 0.9, β2 = 0.999, ϵ = 1e 8. We use cosine learning rate schedule that peaks at 5e 6 and 10% warmup steps. We use batch size of 64 with maximum sequence length of 2048. We employ gradient checkpointing and accumulation to be able to train our models on single A6000 GPU. We train the ORM for 2 epochs on our dataset and use the last checkpoint for evaluation. We train each setup three times across three random seeds (1,1999,2024) and report the mean results. To create our training data we first apply Black formatting to the generated code as mentioned in subsection 3.1. We then filter out any any programs with syntax errors and problems which do not have at least 1 correct and 1 incorrect solution. Then we randomly sample at most six examples per problem. For our pairwise objectives this means we sample total of 12 programs per problem. We use the CodeContests validation set along with 100 randomly sampled problems from the GSM8K validation set to create our validation data. 8https://docs.python.org/3/library/subprocess.html 16 B.3 Objective An ORM does not necessarily need to be language model, but given the strong overall results of prior work in using language model as an ORM [11], we choose to use these. To train an ORM we perform empirical risk minimization with the following objective [59]: LRM = E(x,cw,cℓ)D[log σ(Rθ(cwx) Rθ(cℓx))] = E(x,cw,cℓ)D[log σ(Pθ(cw is correctx, cw) Pθ(cℓ is correctx, cℓ))]. (6) Here, is the training dataset, cw is correct candidate, and cℓ is an incorrect candidate. This objective is based on the Bradley-Terry [5] preference model that models (cw cℓ). Intuitively this objective pushes Rθ(cwx) to be higher than Rθ(cℓx) by maximizing the margin between correct and incorrect candidates. Thus for any new candidate, ci, the ORM will produce score based on the likelihood it is preferred to the incorrect candidates."
        },
        {
            "title": "C Example Problem",
            "content": "1 r i Listing 1: Example Problem Description From CodeContests l q e h some i r e i n \" \" , i \" a \" , \" \" , \" b \" and \" \" a \" and \" t g e row . example , b \" s r . But d \" n q e . 2 3 a e r s e n 4 5 u 6 7 The p a i f s number e a . i s r . t s an e t ( 1 <= <= 1 0 0 ) 8 9 s e e r l e by s . The e r s e , h t i d r i f one s o o w a a n and e g b e 1 and 100 l v . 10 11 p 12 13 e t 14 e , p on p t n : * YES * NO e s . e r i e r o n s s s r , 16 17 18 19 20 You o u YES and NO any e ( example , yes , Yes and YES l be o z s s v r s yEs , p e ) . 21 22 Example 23 24 u 25 26 27 10 28 29 30 31 a 32 b 33 a 17 34 c 35 xxyy 36 xyyx 37 xyxy 38 39 40 p 41 42 43 NO 44 YES 45 NO 46 YES 47 YES 48 YES 49 NO 50 NO 51 NO 52 YES 1 o y Listing 2: Example Correct Solution from i 2 3 # Read u p = . i . d 4 5 a = u ( ) . i ( ) 6 7 # The r = ( a [ 0 ] ) m i e number e a s 9 10 # r o e t i n ( 1 , 11 12 + 1 ) : = a [ ] g = ( ) # Check f l t % 2 == 0 : e n o e i s n 13 14 16 17 18 19 20 22 23 1 2 f = g / / 2 # Check f e [ : f ] == [ f : ] : s l e l h c h f i ( \"YES\" , end = \" \" ) e : n ( \"NO\" , end = \" \" ) e : n ( \"NO\" , end = \" \" ) o s Listing 3: Example Incorrect Solution 3 4 5 6 7 9 10 11 12 _ a ( ) : = ( ) r % == 0 : a ( 1 , / / 2 + 1 ) : e = [ : ] r x * ( / / u \"YES\" ) == : u \"NO\" 18 13 14 main ( ) : 15 u = . i . d a = u ( ) . i ( ) = ( a [ 0 ] ) u = [ ] r r e ( 1 , + 1 ) : u . e ( _ a ( a [ ] ) ) u r ( u ) e t : i __name__ == \" __main__ \" : main ( )"
        },
        {
            "title": "D Prompts Used",
            "content": "Listing 4: Prompt used for training and inference i d t h . r ? 1 # s n 2 Tom and Tim h u 4 , 3 How many a 4 5 # p d u n 6 7 8 h i s e u n ( ) : e _ _ s = 4 e _ _ = 6 a _ e = e _ _ s * 2 a _ e = a _ e * e _ _ r l = a _ e t r l 16 17 18 20 21 22 23 24 26 9 10 11 12 14 15 16 < _ _ t >"
        },
        {
            "title": "E Execution Details Continued",
            "content": "HumanEval tests function completion where the model must write the correct function given docstring and signature. MBPP, on the other hand, tasks ths model with synthesizing function given single sentence specification and single test case. Then we use the execution framework provided by EvalPlus that executes the program from inside of Python process. In both of these cases, the evaluation relies on assertions that call specific entrypoint function. Thus, we use EvalPlus sanitize method to extract all relevant code and comments using treesitter9 to parse the AST. This does come with the caveat that it heavily perturbs the whitespace of the original code in unnatural ways. Thus we use the Black formatter10 to format both our evaluation and training data. We run all of our trials on cluster designed for high throughput computing [9]."
        },
        {
            "title": "F Data Generation Details",
            "content": "To generate our training data for our ORMs, we use the training splits of CodeContests-Python [34] and GSM8K [11] in Program-aided Language Models (PAL) format [18]. The CodeContests dataset is collection of competitive programming problems from platforms such as CodeForces.11 Each 9https://github.com/tree-sitter/tree-sitter 10https://github.com/psf/black 11https://codeforces.com/ 19 Table 3: Breakdown of evaluation data, where πG is the generator size, is the temperature, DS is the dataset, Prob Pass is the probability of problem having at least one passing solution, % Pass is the percentage of solutions that pass, Net Sols is the total number of solutions, Prob. Tok is the average number of tokens in the problem statement, Tok. is the average number of tokens in the solutions, # Pass is the average number of passing solutions per problem, and Avg. Sols is the average number of solutions per problem. Prob Pass % Pass Net Sols # Pass Avg. Sols Prob. Tok Tok. DS πG 500M 1.0 1.5B 1.0 3B 1.0 0.2 0.4 7B 0.6 0.8 1.0 14B 1.0 CC GSM8K HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP 68.48 80.36 100.00 95.77 78.18 95.00 100.00 98. 88.48 97.80 100.00 99.47 84.24 91.66 99.39 96.83 88.48 95.68 99.39 97.88 90.30 96.89 99.39 98.94 93.33 97.88 99.39 99.21 92.12 98.56 99.39 99.74 92.73 98.94 100.00 98.94 2.75 18.75 41.99 40.42 6.03 50.33 57.90 54.72 9.83 66.16 75.27 58. 14.54 82.20 86.09 72.24 14.92 82.11 83.88 71.08 14.80 81.23 83.23 69.65 14.47 80.62 81.76 68.69 14.11 79.44 80.98 66.64 18.76 87.69 83.97 70.23 21,014 151,608 16,444 40,187 20,932 135,144 16,643 41,018 21,108 129,375 14,935 44,896 11,905 14,135 2,643 10,281 18,331 40,932 7,071 24,167 20,300 69,711 11,127 34,328 20,834 94,225 14,241 40,961 21,022 114,882 16,625 44, 19,852 129,548 15,410 24,411 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60.51 620.74 71.31 144.22 60. 878.31 155.68 359.51 156.05 868.14 160.51 320.82 132.90 921.48 163.22 336.42 138.26 902.13 162.98 323.89 144.11 895.37 163.93 324.28 144.51 892.90 164.16 319.59 144.02 893.17 164.30 315.55 143.52 897.16 165.06 310.11 144.73 988.10 170.84 314.70 142.26 0.04 18.31 33.98 39. 0.13 45.92 51.06 53.15 0.47 59.09 63.43 65.64 0.21 7.10 12.14 14.60 0.32 21.26 33.40 37.24 0.42 37.52 52.44 55.72 0.38 52.23 67.17 68.29 0.32 64.48 78.46 74.42 0.10 83.86 75.10 38.29 127.36 114.94 100.27 106.31 126.86 102.46 101.48 108. 127.93 98.09 91.07 118.77 72.15 10.72 16.12 27.20 111.10 31.03 43.12 63.93 123.03 52.85 67.85 90.81 126.27 71.44 86.84 108.36 127.41 87.10 101.37 118.82 120.32 98.22 93.96 64.58 program requires taking in STDIN inputs and printing the results to STDOUT. Most, but not all, problems have few private test cases. We follow the LiveCodeBench [25] prompts and formatting for generation. For GSM8K, we opt for the PAL format as it allows us to have more programming training data from different domain. We additionally use the BigCodeEval [1] in context examples for GSM8K. We use Qwen 2.5 Coder Instruct 7B [23] with Int8 GPTQ quantization [16] to generate our training data. We sample maximum of 1024 tokens 128 times per problem with temperature of = 1.0 and top-p of 0.95. We use the VLLM [29] library to generate our data. Full details of our training data are found in Appendix B."
        },
        {
            "title": "G Is using a causal language model head with pairwise losses the best",
            "content": "approach? Intuitively, causal language modeling head should be able to provide more accurate ranking than purely regression head for two reasons: the open source weights are already pre-trained on causal language modeling objective and model that is good at generating correct programs should assign higher probability to correct programs. Further, pairwise loss should be able to provide more accurate ranking than pointwise loss as it is able to capture the relative ordering of the candidates. Figure 4: Results from training 1.5B ORM with different objectives. Setup: We train three other ORM setups: RM: Point-wise regression to test whether pair-wise training is better for training ORMs for Code. CLM: Causal Language Modeling to test whether language modeling objective is better for training ORMs for Code. To make use of the negative examples, like the other three settings, we add an additional sequence of Is the solution correct? then append either [Yes] or [No] to the end of the program, depending on whether it is correct or incorrect. DPO: Direct Preference Optimization [50] with causal language modeling head to test whether preference objective is better for training ORMs for Code. We use β = 0.1 for all experiments. To ensure fair comparison, we keep the same training data, hyperparameters, and training procedure as subsection 3.2. The only difference between our settings is that we keep the evaluation batch size the same as our previous experiments for the regression head that outputs single scalar. However, for the language modeling head (LM) settings, we had to use smaller batch size of 24K and 16K tokens for the 500M and 1.5B models, respectively, due to higher memory overhead. Causal language modeling head is implemented as linear layer that takes in the last hidden layer and outputs vocab matrix where is the batch size, is the sequence length and vocab is the vocabulary size. The output is the probability of each token in the vocabulary given the previous tokens in the sequence. This layer is not initialized from scratch but rather from the trained weight after post-training. While regression head is implemented as linear layer that takes in the last hidden layer and outputs tensor of scalars representing the logit of the correct class. As this layer is heavily task specific, it needs to be initialized from scratch. It is therefore expected then that the causal language modeling head will yield better results than the regression head. The difference in throughput should accordingly be marginal indicating that the causal language modeling head is the optimal choice for training Rθ. Results: We find that there is minimal Best-of-64 difference between the causal language modeling head and the regression head. The same is true for the pair-wise vs point-wise comparison. However, Figure 5: Trade-off curves for CodeContests when using the 3B generator. The colors represent different ranking strategies, while the markers represent different pruning methods. Majority Voting is the verifier-only setup where we use majority voting to select the best candidate after pruning with the weak verifier. We report the full results in Appendix I. the throughput heavily depends on the architecture and, therefore, the objective. For the 500M Vθ, the causal language modeling head is slower than the regression head. While not as striking in the case of the 1.5B Vθ, it is still slower. This is entirely due to the additional sequence length and vocabulary size needed for the causal language modeling head. This leads to significantly higher memory usage and thus lower batch size. We needed to set maximum tokens per batch to 24K and 16K for the 500M and 1.5B models, respectively. This is PH and PH less, respectively, compared to what was possible for the regression head. Further, we observe further degradation in throughput for the point-wise CLM objective. This is likely due to the additional tokens we added to utilize the negative examples."
        },
        {
            "title": "H Scale Graphs",
            "content": "Figure 6: Distribution of the failed candidates based on where Vθ had ranked them. rank of 1 means the candidate was the top-ranked, while 128 means it was the lowest-ranked. The rows are the individual verifiers while the columns are the datasets. This is for the 500M Vθ."
        },
        {
            "title": "I All result tables",
            "content": "22 Table 4: Standard deviation of the results for the different pruning with weak verifier then ranking methods. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run. Syntax and Lint remove any programs with the respective errors. Test prunes out any programs that do not pass the first test cases. The evaluation dataset is generated with Qwen 2.5 Coder 7B Instruct using = 1.0, = 128, topp = 0.95, and 1024 tokens. For the Non-ORM methods, there is no standard deviation for best-of-64 as the ordering is the same. CodeContests PPS Bof"
        },
        {
            "title": "HumanEval\nPPS",
            "content": "GSM8K"
        },
        {
            "title": "MBPP",
            "content": "Bof64 Bof64 Bof"
        },
        {
            "title": "PPS",
            "content": "Vθ 500M 0.71 0.71 1.5B 15.40 4.05 1.04 1.28 77.45 21.19 1.13 0. 40.77 11.52 1.16 0.89 82.75 24.55 N/A 500M 0.79 1.5B 0.77 399.40 445.70 517.91 742.06 21.38 13.76 18.80 11. 0.29 0.64 0.94 0.68 3.85 2.50 7.68 5.91 0.70 0."
        },
        {
            "title": "10 Tests",
            "content": "N/A 500M 0.85 0.87 1.5B N/A 500M 0.33 0.40 1.5B N/A 500M 0.33 1.5B 0.11 N/A 500M 0.09 0.13 1.5B"
        },
        {
            "title": "Vall",
            "content": "92.07 9.52 2.64 1.44 2.22 9.88 18.88 7.50 0.89 0.81 4.69 12.10 4. 2.72 1.84 9.62 11.83 6.91 101.45 150.46 47.39 0.82 20.29 1.75 26.96 9.25 1.65 1.08 132.56 169.01 0.79 0. 0.29 0.07 0.46 0.28 0.09 0.04 0.70 0.38 54.43 0.04 0. 0.92 0.54 0.10 44.48 0.03 0.06 0.43 1.48 0.31 0. 2.04 0.00 0.01 2.27 0.01 0.01 0.09 37.90 Table 5: Information about the tokens filtered from the datasets by the 5 verifiers we look at on the 3B, 7B, and 14B models. The percentage filtered is the percentage of tokens filtered from the dataset, the average sequence length is the average sequence length of the tokens that were filtered, and the total tokens filtered is the total number of tokens filtered from the dataset. πG DS % Filtered Avg. Seq Length Total Tokens Filtered 3B 7B 14B"
        },
        {
            "title": "10 Tests",
            "content": "CC GSM8K HE MBPP CC GSM8K HE MBPP CC HE MBPP CC HE MBPP CC HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC HE MBPP CC HE MBPP CC HE MBPP CC GSM8K HE MBPP CC GSM8K HE MBPP CC HE MBPP CC HE MBPP CC HE MBPP 1,377.77 193.23 134.00 57.12 1,138.75 203.19 425.30 133.69 939.92 423.95 158.75 930.33 396.60 156.56 929.85 391.45 153.40 1,222.81 201.33 135.00 65.00 1,131.51 203.86 408.92 159.68 919.54 378.60 173.66 908.04 364.37 170.55 906.89 364.83 165.21 1,610.94 213.89 130.29 61.49 1,411.36 216.55 315.97 135.12 1,019.14 375.57 182.11 1,005.72 369.63 178.78 1,003.10 373.36 173.15 1.90 0.88 0.00 0.13 12.57 2.83 1.15 1.73 91.00 13.02 32.74 94.01 20.79 37.36 94.30 27.10 45.89 0.76 0.45 0.00 0.00 5.36 1.33 1.00 1.05 88.90 8.44 22.88 92.85 13.97 27.58 93.39 18.84 37.92 4.87 0.16 0.04 0.13 7.76 0.81 0.95 0.94 85.08 5.99 21.93 89.01 11.18 26.60 89.95 15.53 38. 24 1.95e+07 2.23e+07 5.49e+06 6.29e+06 1.95e+07 2.23e+07 5.49e+06 6.29e+06 1.95e+07 5.49e+06 6.29e+06 1.95e+07 5.49e+06 6.29e+06 1.95e+07 5.49e+06 6.29e+06 1.89e+07 2.04e+07 5.47e+06 6.63e+06 1.89e+07 2.04e+07 5.47e+06 6.63e+06 1.89e+07 5.47e+06 6.63e+06 1.89e+07 5.47e+06 6.63e+06 1.89e+07 5.47e+06 6.63e+06 1.96e+07 2.35e+07 5.22e+06 3.93e+06 1.96e+07 2.35e+07 5.22e+06 3.93e+06 1.96e+07 5.22e+06 3.93e+06 1.96e+07 5.22e+06 3.93e+06 1.96e+07 5.22e+06 3.93e+06 Table 6: Information on filtered solutions for the 500M and 1.5B Vθ across the datasets when using the 7B Qwen 2.5 Coder Instruct πG and temperature of 1.0. # Rem is the average number of solutions removed per question. Avg. Rank is the average rank of the solutions that were removed. M1 Rank is the average highest rank of the solutions that were removed. M5 Rank is the average rank of the 5 top ranks of the solutions that were removed. % Removed is the total percentage of solutions removed across all questions. % Prob is the percentage of problems where all solutions were removed. Spread is the average spread of the solutions that were removed across the three seeds. Vθ # Rem Avg. Rank M1 Rank M5 Rank % Removed % Prob"
        },
        {
            "title": "Spread",
            "content": "V 500M 1.5B"
        },
        {
            "title": "1 Test\n10 Tests\nLint\nSyntax",
            "content": "51.38 63.53 4.76 5.13 51.38 63.53 4.76 5.13 82.90 77.81 102.31 106.50 89.27 83.47 108.70 112.99 38.87 27.63 86.49 98.53 47.87 35.41 96.02 107. 51.04 39.36 97.84 104.93 58.42 45.89 104.91 111.73 40.14 49.63 3.72 4.01 40.14 49.63 3.72 4.01 6.86 12.57 0.00 0.00 6.86 12.57 0.00 0. 20.00 17.90 31.91 32.08 19.14 17.14 30.16 30.28 Table 7: Results for the generation with Qwen 2.5 Coder 500M instruct with = 1.0, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run."
        },
        {
            "title": "Filter",
            "content": "Vθ"
        },
        {
            "title": "CodeContests\nPPS",
            "content": "Bof"
        },
        {
            "title": "10 Tests",
            "content": "500M 0.58 0.61 1.5B"
        },
        {
            "title": "1.39\nN/A\n500M 1.74\n2.02\n1.5B",
            "content": "N/A 1.65 500M 1.76 1.5B 2.16 68.47 25.69 126.19 79.55 74.00 125.72 72.75 68.89 125.65 87.38 82.09 GSM8K Bof64 37.47 53."
        },
        {
            "title": "PPS",
            "content": "375.11 146."
        },
        {
            "title": "HumanEval\nPPS",
            "content": "Bof"
        },
        {
            "title": "MBPP",
            "content": "Bof"
        },
        {
            "title": "PPS",
            "content": "52.71 66.18 64.05 65.67 73.08 72.13 74.13 77.78 82.88 83.16 83.10 157.59 64.13 851.41 178.46 89. 814.36 213.13 113.68 620.28 181.40 112.90 47.97 58.03 61.41 62.47 66.81 64.44 65.43 68.12 70.64 71.54 72. 353.50 145.88 870.83 322.60 196.68 483.11 333.03 209.62 378.63 198.64 152."
        },
        {
            "title": "Vall",
            "content": "2.16 3.32 73.63 391.53 86.79 19. 73.73 13.99 25 Table 8: Results for the generation with Qwen 2.5 Coder 1.5B instruct with = 1.0, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run."
        },
        {
            "title": "Filter",
            "content": "Vθ"
        },
        {
            "title": "CodeContests\nPPS",
            "content": "Bof"
        },
        {
            "title": "10 Tests",
            "content": "500M 1.54 2.45 1.5B"
        },
        {
            "title": "4.92\nN/A\n500M 5.60\n5.64\n1.5B",
            "content": "N/A 5.39 500M 5.78 5.66 1.5B N/A 5.73 500M 5.96 1.5B 5.86 66.44 25.90 124.94 73.96 67.05 122.97 79.61 73.56 122.34 79.08 73. GSM8K Bof64 62.91 74."
        },
        {
            "title": "PPS",
            "content": "342.03 134."
        },
        {
            "title": "HumanEval\nPPS",
            "content": "Bof"
        },
        {
            "title": "MBPP",
            "content": "Bof"
        },
        {
            "title": "PPS",
            "content": "65.41 71.11 72.17 73.28 77.16 80.10 80.06 82.26 88.60 86.36 87.45 166.61 67.78 905.65 160.14 79. 1026.14 178.66 91.38 571.02 155.87 90.02 58.28 66.08 72.81 72.28 74.60 76.22 75.14 76.33 81.66 82.32 82. 387.46 158.49 1195.51 310.38 183.33 1070.13 314.29 190.02 511.41 188.92 140."
        },
        {
            "title": "Vall",
            "content": "7.83 2.23 92.72 173.71 94.06 20. 85.64 20.97 Table 9: Results for the generation with Qwen 2.5 Coder 3B instruct with = 1.0, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run. Filter Vθ CodeContests PPS Bof64 1 Test 3 Tests 10 Tests 500M 1.5B 4.53 6.02 N/A 9.65 500M 10.81 11.47 1.5B N/A 11.34 500M 12.39 12.86 1.5B 12.16 N/A 500M 12.64 13.03 1.5B 62.81 24.39 122.58 66.60 57. 118.91 63.95 58.16 116.15 70.68 63.91 GSM8K Bof64 75.07 81.90 PPS 333.65 130.86 HumanEval PPS Bof64 MBPP Bof64 PPS 74.84 78.81 82.39 78.79 83.22 85.78 84.79 87.80 89.54 89.02 91.04 156.81 63.44 1057.72 140.64 65. 1208.37 151.93 71.34 684.03 135.89 70.24 60.55 67.19 74.99 72.15 74.65 77.72 74.84 77.18 84.10 83.12 84. 380.45 155.19 1217.04 292.12 166.56 1074.66 299.16 174.10 509.97 177.44 129.60 Vall 14. 2.54 96.63 647.01 95.41 22.13 88. 12.48 26 Table 10: Results for the generation with Qwen 2.5 Coder 14B instruct with = 1.0, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run. Filter Vθ CodeContests PPS Bof64 1 Test 3 Tests 10 Tests 500M 10.71 12.84 1.5B 22.76 N/A 500M 23.90 25.87 1.5B N/A 24.32 500M 25.51 26.77 1.5B N/A 25.83 500M 27.69 28.33 1.5B 58.68 22.86 132.92 65.93 51.95 127.07 58.95 50. 120.53 67.00 57.53 GSM8K Bof64 90.37 91.36 PPS 318.11 124. HumanEval PPS Bof64 MBPP Bof64 PPS 83.44 84. 88.67 85.23 86.37 90.36 88.31 88.46 91.62 90.69 90.77 169.91 69.10 1197.05 141.41 65.92 1130.49 146.71 68. 662.39 126.80 66.06 68.82 73.19 79.82 75.00 77.17 81.65 76.84 78.48 87.72 85.74 86.57 322.25 133. 1360.04 261.45 137.04 1127.35 264.55 142.21 349.60 156.67 113.06 Vall 31.98 3. 98.61 153.10 96.57 24.43 90.29 17. Table 11: Results for the generation with Qwen 2.5 Coder 7B instruct with = 0.2, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run. Filter Vθ CodeContests PPS Bof64 1 Test 3 Tests 10 Tests 500M 1.5B 5.32 5.25 N/A 10.82 500M 10.81 1.5B 11. N/A 11.69 500M 12.12 12.40 1.5B 12.09 N/A 500M 12.35 12.40 1.5B 61.82 24.63 173.82 81.41 66.73 163.05 90.60 80.77 157.14 88.56 80. GSM8K Bof64 84.98 86.24 PPS 258.83 111.03 HumanEval PPS Bof64 MBPP Bof64 PPS 84.68 86.02 88.41 85.68 87. 89.63 87.79 89.24 90.85 89.15 90.07 131.61 57.38 961.83 116.63 56.17 1196.20 120.81 59.47 611.21 105.09 56. 70.98 73.84 78.24 76.96 77.32 79.63 77.51 78.25 81.60 80.86 81.33 257.44 115.71 316.36 235.16 128. 1010.78 240.98 135.96 373.46 143.97 104.80 Vall 12.40 3.09 91. 504.11 91.46 11.63 82.40 6.97 Table 12: Results for the generation with Qwen 2.5 Coder 7B instruct with = 0.4, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run."
        },
        {
            "title": "Filter",
            "content": "Vθ"
        },
        {
            "title": "CodeContests\nPPS",
            "content": "Bof64 GSM8K Bof"
        },
        {
            "title": "HumanEval\nPPS",
            "content": "Bof"
        },
        {
            "title": "MBPP",
            "content": "Bof"
        },
        {
            "title": "PPS",
            "content": ""
        },
        {
            "title": "10 Tests",
            "content": "500M 1.5B 5.54 7."
        },
        {
            "title": "13.11\nN/A\n500M 14.25\n15.07\n1.5B",
            "content": "N/A 15.50 500M 15.60 16.19 1.5B N/A 15.90 500M 15.93 1.5B 16.26 65.70 26.11 163.12 77.28 64.06 153.76 78.19 69.94 147.99 82.17 73. 86.04 88.28 299.64 121.60 82.24 85.16 87.80 83.54 87.07 89.56 86.86 88.86 90.78 88.91 89. 151.66 63.69 1146.83 133.91 62.75 1143.86 137.78 65.49 639.57 116.62 61.65 68.70 72.45 79.15 75.32 76. 80.52 76.91 78.29 83.29 82.31 83.05 317.45 135.34 1257.67 254.89 138.53 1207.37 261.77 145.06 358.78 157.57 112."
        },
        {
            "title": "Vall",
            "content": "16.74 3.83 95.21 60.02 92.57 15. 84.55 14.80 Table 13: Results for the generation with Qwen 2.5 Coder 7B instruct with = 0.6, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run. Filter Vθ CodeContests PPS Bof64 1 Test 3 Tests 10 Tests 500M 1.5B 5.71 6.93 N/A 12.99 500M 13.42 15.15 1.5B N/A 14.76 500M 14.82 15.86 1.5B 15.63 N/A 500M 15.22 16.14 1.5B 66.90 26.56 145.28 74.06 62. 142.47 78.81 70.11 136.34 80.82 72.55 GSM8K Bof64 85.45 88.14 PPS 317.44 127.33 HumanEval PPS Bof64 MBPP Bof64 PPS 83.01 84.10 86.77 84.75 86.31 89.71 87.61 88.49 90.93 89.45 89.52 163.34 67.84 1280.37 140.68 66. 1232.14 150.14 70.75 504.65 125.36 66.56 66.68 71.68 77.83 74.52 77.09 80.06 76.57 78.89 85.36 84.00 85. 352.70 147.09 1183.06 264.62 144.54 1011.88 268.14 150.11 362.63 159.61 114.26 Vall 17. 2.13 96.40 590.56 94.05 18.06 86. 26.87 28 Table 14: Results for the generation with Qwen 2.5 Coder 7B instruct with = 0.8, = 128, topp = 0.95, and 1024 tokens. Green backgrounds is higher performance while Red backgrounds is lower performance. If is that means no pruning is done. Vall is the case where all test cases are run. Filter Vθ CodeContests PPS Bof64 1 Test 3 Tests 10 Tests 500M 1.5B 5.42 7.18 N/A 13.54 500M 14.01 1.5B 15.02 14.46 N/A 500M 15.39 15.69 1.5B N/A 14.56 500M 15.59 15.79 1.5B 67.07 25.15 137.85 70.38 58. 131.96 64.26 57.73 126.77 75.09 66.93 GSM8K Bof64 84.98 87.80 PPS 328.18 124.41 HumanEval PPS Bof64 MBPP Bof64 PPS 80.70 81.36 87.43 83.25 83.56 90.72 86.43 86.48 92.10 89.79 88.95 172.10 71.53 1327.06 141.55 66. 1178.13 149.74 70.12 680.40 128.98 66.81 65.64 70.94 76.03 74.63 77.08 79.32 76.74 78.95 85.77 84.97 86. 374.34 148.51 1097.09 265.54 143.06 1063.62 268.08 147.80 459.95 168.96 117.27 Vall 17. 3.35 97.30 582.08 95.65 19.81 89. 19."
        }
    ],
    "affiliations": [
        "University of Wisconsin-Madison"
    ]
}