{
    "paper_title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
    "authors": [
        "Yilun Zhao",
        "Lujing Xie",
        "Haowei Zhang",
        "Guo Gan",
        "Yitao Long",
        "Zhiyuan Hu",
        "Tongyan Hu",
        "Weiyuan Chen",
        "Chuhan Li",
        "Junyang Song",
        "Zhijian Xu",
        "Chengye Wang",
        "Weifeng Pan",
        "Ziyao Shangguan",
        "Xiangru Tang",
        "Zhenwen Liang",
        "Yixin Liu",
        "Chen Zhao",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark for evaluating foundation models in video understanding. MMVU includes 3,000 expert-annotated questions spanning 27 subjects across four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to prior benchmarks, MMVU features three key advancements. First, it challenges models to apply domain-specific knowledge and perform expert-level reasoning to analyze specialized-domain videos, moving beyond the basic visual perception typically assessed in current video benchmarks. Second, each example is annotated by human experts from scratch. We implement strict data quality controls to ensure the high quality of the dataset. Finally, each example is enriched with expert-annotated reasoning rationals and relevant domain knowledge, facilitating in-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal foundation models on MMVU. The latest System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest performance among the tested models. However, they still fall short of matching human expertise. Through in-depth error analyses and case studies, we offer actionable insights for future advancements in expert-level, knowledge-intensive video understanding for specialized domains."
        },
        {
            "title": "Start",
            "content": "Zhao et al. (2025) MMVU: MEASURING EXPERT-LEVEL MULTI-"
        },
        {
            "title": "DISCIPLINE VIDEO UNDERSTANDING",
            "content": "Yilun Zhao Lujing Xie Haowei Zhang Guo Gan Yitao Long Zhiyuan Hu Tongyan Hu Weiyuan Chen Chuhan Li Junyang Song Zhijian Xu Chengye Wang Weifeng Pan Ziyao Shangguan Xiangru Tang Zhenwen Liang Yixin Liu Chen Zhao Arman Cohan Yale NLP MMVU Team"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce MMVU, comprehensive expert-level, multi-discipline benchmark for evaluating foundation models in video understanding. MMVU includes 3,000 expert-annotated questions spanning 27 subjects across four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to prior benchmarks, MMVU features three key advancements. First, it challenges models to apply domain-specific knowledge and perform expert-level reasoning to analyze specialized-domain videos, moving beyond the basic visual perception typically assessed in current video benchmarks. Second, each example is annotated by human experts from scratch. We implement strict data quality controls to ensure the high quality of the dataset. Finally, each example is enriched with expert-annotated reasoning rationals and relevant domain knowledge, facilitating in-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal foundation models on MMVU. The latest System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest performance among the tested models. However, they still fall short of matching human expertise. Through in-depth error analyses and case studies, we offer actionable insights for future advancements in expert-level, knowledge-intensive video understanding for specialized domains. Project Page: MMVU Data: MMVU Code: mmvu-benchmark.github.io huggingface.co/datasets/yale-nlp/MMVU github.com/yale-nlp/MMVU 5 2 0 2 1 ] . [ 1 0 8 3 2 1 . 1 0 5 2 : r Figure 1: Overview of the MMVU benchmark. MMVU includes 3,000 expert-annotated examples, covering 27 subjects across four core disciplines. It is specifically designed to assess multimodal foundation models in expert-level, knowledge-intensive video understanding and reasoning tasks. Core Contributors. All authors contributions are detailed in the Contribution section. Zhao et al. (2025)"
        },
        {
            "title": "INTRODUCTION",
            "content": "Foundation models have demonstrated remarkable capabilities in reasoning across various domains, yet their ability to handle expert-level knowledge remains critical area of evaluation (Hendrycks et al., 2021; Yue et al., 2024a). In recent years, researchers have developed numerous benchmarks to assess these models proficiency in specialized domains, primarily focusing on text-based reasoning (Hendrycks et al., 2021; Wang et al., 2024d; Feng et al., 2024; Sun et al., 2024) and image-based contexts (Lu et al., 2024; Yue et al., 2024a;b; Zhang et al., 2024a; Li et al., 2024g). However, as capabilities of foundation models expand across multiple modalities, there is significant gap in evaluating expert-level reasoning over specialized-domain videos. This gap is particularly concerning as video is one of the most information-rich and naturalistic modalities, and is widely used to convey complex, dynamic information in specialized fields like healthcare, engineering, and scientific research (He et al., 2024). Unlike static text or images, expert-level videos often capture temporal dynamics, procedural knowledge, and complex interactions that are essential in many specialized domains. For example, in science, expert-level and knowledge-intensive reasoning might involve analyzing chemical reaction video (Figure 1). model must identify key reaction stages based on subtle visual cues like color changes or the formation of precipitates, which requires integrating chemical knowledge in addition to recognizing visual patterns. To bridge this gap, we introduce MMVU, comprehensive benchmark measuring Multimodal foundation models in expert-level, Multi-discipline Video Understanding and reasoning. MMVU consists of 3,000 expert-annotated QA examples over 1,529 specialized-domain videos, spanning 27 subjects across four key disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. To ensure both the breadth of domain knowledge and the depth of reasoning required for MMVU, we implement textbook-guided data annotation process. Expert annotators first locate key concepts from textbooks in their fields, then source relevant videos and create corresponding questions that require domain knowledge and expert-level reasoning to comprehend the videos. Each example also includes expert-annotated reasoning rationale and relevant domain knowledge, facilitating fine-grained evaluation of model performance. Thorough data quality controls are implemented to ensure high quality of MMVU. We conduct an extensive evaluation on MMVU, covering 32 frontier multimodal foundation models from 17 organizations. Notably, the latest o1 model demonstrates the highest performance among all tested models, approaching the expertise of human experts. Despite this progress, other models still fall noticeably short of human-level capabilities. For instance, GPT-4o achieves score of 66.7%, which is substantially lower than the benchmark set by human experts (i.e.,, 86.8%) in the openbook setting. Our analysis highlights the effectiveness of CoT reasoning, which generally enhances model performance compared to directly generating final answers without intermediate reasoning steps. To deepen understanding of the current models limitations, we perform an in-depth error analysis of frontier models, including numerous case studies reviewed by human experts. These insights provide valuable guidance for future advancements in the field."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Video Understanding Benchmark. Existing video understanding benchmarks primarily focus on general-purpose video comprehension tasks, such as action recognition (Heilbron et al., 2015; Sigurdsson et al., 2016; Liu et al., 2020; Deng et al., 2023), captioning and description (Xu et al., 2016; Krishna et al., 2017; Li et al., 2024c; Takahashi et al., 2024; Wu et al., 2021), grounding (Lei et al., 2018; Wang et al., 2022; Chen et al., 2023c; Kesen et al., 2023), temporal reasoning (Jang et al., 2017; Liu et al., 2024b; Shangguan et al., 2024; Cores et al., 2024; Cai et al., 2024; Kesen et al., 2024; Li et al., 2024e), and long video understanding (Zhang et al., 2023b; Wang et al., 2024b; Nagrani et al., 2024; Ataallah et al., 2024; Fang et al., 2024). The rise of video-based foundation models (Tang et al., 2023; Zhang et al., 2023a; Fei et al., 2024; Huang et al., 2024b) has driven the development of new benchmarks that include diverse video comprehension tasks for more comprehensive evaluation (Xiao et al., 2021; Ning et al., 2023; Li et al., 2024d; Fu et al., 2024; Li et al., 2024f; Khattak et al., 2024; Yang et al., 2024b). However, these benchmarks remain predominantly focused on natural scenes and general-purpose tasks. significant gap persists in benchmarks targeting expert-level and knowledge-intensive reasoning over specialized-domain videos, where both 2 Zhao et al. (2025) Table 1: Comparison between MMVU and existing multi-discipline benchmarks for evaluating foundation models. In the QA Type column, MC denotes Multiple-Choice questions, Open denotes Open-ended questions, and T/F denotes True-False questions. Dataset QA Type Data Source College Level? Detailed Solution Rational? Knowledge? Text Exam, course, textbook MMLU (Hendrycks et al., 2021) MC Datasets Human & LLM augment MMLU-Pro (Wang et al., 2024d) MC Exam MC C-Eval (Huang et al., 2023) Internet, datasets LLM rewrite SciEval (Sun et al., 2024) MC, Open TheoremQA (Chen et al., 2023a) MC, T/F, Open Internet, exam Human rewrite SciKnowEval (Feng et al., 2024) MC, T/F, Open Textbooks, database, other datasets LLM rewrite Text + Image MC, Open VisScience (Jiang et al., 2024) MC EXAMS-V (Das et al., 2024) MC ScienceQA (Lu et al., 2022) MC, Open SceMQA (Liang et al., 2024) Open CharXiv (Wang et al., 2024e) MC MMSci (Li et al., 2024g) OlympicArena (Huang et al., 2024a) MC, T/F, Open Olympic competitions MMMU (Yue et al., 2024a) CMMMU (Zhang et al., 2024a) MMMU-Pro (Yue et al., 2024b) MC, Open Internet, exam, textbook MC, T/F, Open Internet, exam, textbook MC Internet, exam, textbook Exam Internet, course Internet, exam arXiv paper Human annotate Scientific paper LLM generate MMMU Human & LLM augment MMWorld (He et al., 2024) MC Human experts (24%) / LLM-gen (76%) 39.5% MMVU (ours) MC, Open Human experts annotate from scratch Text + Video 17.6% 2.1% 15.4% visual perception and domain-specific expertise are requiredespecially in critical fields like healthcare, engineering, and science (He et al., 2024). Multi-discipline Evaluation Benchmark. The rapid development of foundation models has significantly enhanced expert-level reasoning across various disciplines (Touvron et al., 2023; Jiang et al., 2023; Yang et al., 2024a; Google, 2024; OpenAI, 2024b). Early benchmarks focused on domain-specific tasks for textual domains, establishing foundation for assessing the models strengths and limitations in expert reasoning (Welbl et al., 2017; Clark et al., 2018b; Hendrycks et al., 2021; Suzgun et al., 2023; Zhong et al., 2024; Chen et al., 2023a; Wang et al., 2024d; Zhao et al., 2024). More recently, benchmarks have evolved to include multimodal tasks (Yue et al., 2024a; Lu et al., 2024; Zhang et al., 2024a; Yue et al., 2024b; Li et al., 2024g; Wang et al., 2024e), emphasizing visual perception and advanced reasoning with domain knowledge. However, these efforts remain largely limited to static images. Developing high-quality, multidisciplinary video benchmark presents greater challenges than those for text or image-based tasks due to the scarcity of suitable resources (e.g.,, textbooks or exam questions). This leaves the critical modality of videos and video-based expert-level reasoning significantly underexplored. Recent work, MMWorld (He et al., 2024), has made pioneering strides by incorporating videos across multiple disciplines. However, only limited portion of its dataset (39.5%) requires domain-specific expertise1, and 76.4% of the examples are generated by the GPT-4V model. Moreover, most existing benchmarks provide only the ground-truth answer, restricting researchers ability to conduct fine-grained evaluation. To address this limitation, MMVU includes expert-annotated reasoning rationales and relevant domain knowledge for each example, enabling more nuanced assessment of expert-level reasoning. Table 1 further distinguishes the difference between MMVU and existing multi-discipline benchmarks."
        },
        {
            "title": "3 MMVU BENCHMARK",
            "content": "We present MMVU, comprehensive evaluation benchmark that focuses on measuring progress on knowledge-intensive, expert-level reasoning in the video modality. MMVU has the following 1To estimate the proportion of MMWorld examples requiring domain expertise, we randomly sampled 200 instances from the human-annotated subset and engaged three annotators for evaluation. An example was classified as requiring domain expertise if at least one annotator marked it as such. 3 Zhao et al. (2025) Figure 2: An overview of the MMVU benchmark construction pipeline. key features: (1) Breadth of Domain Knowledge: We employ textbook-guided QA annotation pipeline to ensure the wide coverage of domain knowledge within each subject (3.2). (2) Depth of Expert-level Reasoning: Each example in MMVU requires models to comprehend specializeddomain video context, applying expert knowledge and reasoning (3.2). (3) True Visual Understanding: Recent studies (Yue et al., 2024b; Chen et al., 2024a; Zhang et al., 2024b) have shown that visual content is unnecessary for many examples in current multimodal benchmarks. To alleviate this issue, each example in MMVU is carefully validated by human experts to confirm that video comprehension is required for accurate answering (3.3). (4) Support of Fine-grained Evaluation: We provide expert-annotated solutions and the requisite knowledge for each example (3.2), enabling more comprehensive analysis for future research (4.3). Figure 2 provides an overview of the three stages involved in constructing MMVU, which is detailed in the following subsections. 3.1 PRELIMINARY SETUP We first discuss the preliminary setup for data construction. Subject Selection. To ensure broad and accurate representation of expert-level video understanding across diverse disciplines, we conduct user study involving 133 college and graduate students for subject selection. We ask them to curate two QA examples requiring expert-level video understanding in subjects relevant to their field of study, and provide feedback on their experiences during the curation process. Such user study-guided approach helps us identify subjects within each discipline that may not be obvious from top-down selection process. It also offers insights into the challenges of designing expert-level video examples, helping us design and refine the textbook-guided QA annotation process (detailed in 3.2). The authors manually analyze the collected examples and select 27 subjects (as listed in Figure 1) across four disciplines that align best with our benchmarks construction desiderata discussed earlier. Expert Annotator Recruitment and Training. For each subject, we assign at least two annotators with relevant expertise. We include total of 67 expert annotators (detailed biographies are presented in Appendix A.1), comprising 22 thirdor fourth-year undergraduate students, 36 graduate students, and nine of the authors. All the annotators also participated in our initial user study. Each annotator is required to finish training session to learn the annotation protocol (detailed in Appendix A.3) before official annotation. 3.2 TEXTBOOK-GUIDED QA EXAMPLE ANNOTATION Constructing high-quality, expert-level, multi-disciplinary benchmark for video-based tasks is more challenging than the ones for textor image-based, as there is no existing resources (e.g.,, textbooks or exam questions) that can adapted from and each example has to be curated from scratch. Therefore, it is crucial to establish structured approach that ensures the quality and comprehensiveness of the benchmark. We employ textbook-guided example annotation pipeline designed to capture both the breadth of knowledge and depth of reasoning. In brief, annotators first identify key concepts from the textbook and locate relevant videos that align with these concepts. The textbooks for each subject (listed in Appendix A.2) are selected by expert annotators and are recognized as authoritative references in their respective fields. Annotators then curate QA examples and detailed solution rationales. We detail the annotation procedure as follows: Concept-Driven CC-Licensed Video Collection. Annotators are instructed to first review each chapter of the textbook to identify key concepts that inherently require dynamic visual representa4 Zhao et al. (2025) Figure 3: dataset example from MMVU with the discipline of chemistry. Each example in MMVU includes expert annotation of relevant domain knowledge and step-by-step reasoning rational. tion, such as experimental procedures in science or mechanical operations in engineering. They then search for related videos on YouTube having Creative Commons license2 that effectively illustrate the selected concept. To ensure the collected videos effectively challenge the models visual reasoning capabilities, the video should be vision-intensive, requiring models to focus solely on visual information for comprehension. To this end, we ensure that audio tracks are excluded to eliminate potential shortcuts models might exploit through auditory cues; and the video should contain minimal on-screen text, as an overabundance of text may detract from the core visual understanding task. Consequently, videos such as lecture recordings, which typically include slides or text-based explanations that simplify the task of answering associated questions, are excluded. QA Annotation. After identifying suitable videos, annotators are required to create two or three questions, either multiple-choice or open-ended. Each question is designed to test the models expert-level reasoning by applying domain-specific knowledge to interpret the video content and derive solution. Annotators are also required to specify the start and end timestamps of the video clip relevant to answering each question. For annotating multi-choice question, the annotators are required to carefully craft the four distractor options to reflect common misconceptions or plausible alternatives, ensuring that models cannot easily eliminate incorrect options without reasoning over video content. Once the five options are finalized, the annotation interface randomly shuffles them. Solution Rationale Annotation. For each annotated question, annotators must also provide detailed solution for the correct answers. As shown in Figure 3, the solution comprises two key components: (1) relevant domain knowledge, which includes list of domain-specific concepts or keywords necessary for answering the question, with each concept linked to its corresponding Wikipedia page. (2) reasoning rationale, which details the step-by-step reasoning process to reach the correct answer. These solution annotations are critical for enhancing transparency in the evaluation process and facilitating future research focused on understanding model failure modes. 3.3 DATA QUALITY CONTROL We next discuss our methods to ensure high data quality. Time-Based Annotation Compensation. As discussed earlier, annotating examples for MMVU can be particularly time-intensive, especially when there is limited availability of videos with Creative Commons licenses in the required subjects. To accommodate this and ensure high-quality 2The Creative Commons license enables reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. We use YouTube Data API v3 (https://developers.google.com/youtube/v3) to verify the license type. Existing video benchmarks typically utilize YouTube videos, yet do not confine their selections to content with CC licenses, introducing potential copyright concerns. We recognize that by restricting our selection to CC-licensed content, we are compelled to forgo coverage of certain subjects (e.g., sports), where CC-licensed videos is scarce. Zhao et al. (2025) Table 2: Key statistics of the MMVU benchmark. Statistics Total Questions Validation Set Test Set Unique Videos Video Length (Seconds, avg/max) Number of Disciplines Number of Subjects Multiple Choice Questions Question Length (avg/max) Single Choice Length (avg/max) Number of Choices per Question Open-ended Questions Question Length (avg/max) Ground-truth Answer Length (avg/max) Value 3,000 1,000 2, 1,529 51.4 / 228 4 27 1,858 16.8 / 70 7.6 / 42 5 1,142 16.4 / 39 1.5 / 7 Number of Required Knowledge per Question (avg/max) Solution Rationale Length (avg/max) 4.3 / 7 56.6 / Total Number of Unique Knowledge (i.e.,, Wikipedia pages) 4,770 benchmark, we compensate annotators based on the time they spend rather than the number of examples completed, preventing them from rushing through tasks (See Appendix A.5 for annotation compensation details). On average, annotating one example takes 20 minutes and 17 seconds, while validation requires 4 minutes and 12 seconds. Human Expert Validation. To ensure that the final dataset remains high-quality and meets expertlevel standards without introducing unnecessary biases, each example in MMVU undergoes expert review by one of the authors or top-performing annotators to verify the accuracy of its annotations. Recent studies (Yue et al., 2024b; Chen et al., 2024a; Zhang et al., 2024b; Shangguan et al., 2024) have shown that visual content is unnecessary for many examples in current multimodal benchmarks. To address this concern, each example in MMVU is carefully validated by human experts to ensure that video comprehension is required for accurate answering. If an example is determined to be answerable solely through the textual components of the question, single video frame, or if it contains annotation errors, evaluators first attempt to revise the example. If revision is not feasible, detailed feedback is provided to the original annotator, who then revises and submits it for second iteration. total of 523 examples were revised during the data validation process. Among them, 72 examples were still found to be misaligned with our design criteria and were excluded from the final benchmark. Overall, 1 523 3,000+72 = 83.0% of the initial examples met our design criteria without requiring revisions, indicating the high quality of initial annotation. 3.4 MMVU BENCHMARK ANALYSIS Data Statistics. Table 2 presents the key statistics of MMVU. It consists of 3,000 examples, which are randomly divided into two subsets: validation and test. The validation set contains 1,000 examples, and is intended for model development and validation. The test set, comprising the remaining 2,000 examples, is strictly reserved for standard evaluation to prevent data contamination (Jacovi et al., 2023; Deng et al., 2024; Glazer et al., 2024). To further promote fair benchmarking, the test set remains hidden. We are developing an online evaluation pipeline on public platform, enabling researchers to benchmark their models and participate in public leaderboard. Human Performance. To provide rough but informative estimate of human-level performance on MMVU, we randomly sampled 30 questions per discipline from the test set, resulting in total of 120 questions for evaluation. Five participantsthree graduate students specializing in biology, anesthesiology, and East-Asian literature, along with two of the authorsindividually answered these questions. The evaluation proceeded in three phases: (1) Closed-book Setting: In the first 6 Zhao et al. (2025) phase, participants had 3.5 hours to answer questions without access to external resources. The average accuracy across the four participants was 49.7%. (2) Open-book Setting: In the second phase, participants were permitted to use external resources (e.g.,, internet and textbooks) to review answers they felt uncertain about. They were not informed of the correctness of their initial responses, and 4-hour time limit was set. This open-book approach led to an increase in average accuracy to 86.8%. (3) Oracle Setting: Finally, participants were required to revise each incorrect answer based on ground-truth domain knowledge and self-sourced online resources. The average accuracy after this final revision was 95.3%."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "This section discusses the experiment setup and our key findings. 4.1 EXPERIMENT SETUP Evaluated Multimodal Foundation Models. To establish comprehensive understanding of the challenges posed by MMVU and provide reference points for future research, we evaluate broad range of frontier multimodal foundation models that support video or multiple images as input. Specifically, we evaluate 16 series of open-source models, including InternVL-2 & 2.5 (Chen et al., 2023b; 2024b), Qwen2-VL (Wang et al., 2024a; Yang et al., 2024a), LLaVA-NeXT (Liu et al., 2024a), Pixtral (MistralAI, 2024), DeepSeek-VL2 (Wu et al., 2024), H2OVL Mississippi (Galib et al., 2024), Idefics2 (Laurencon et al., 2024), Aria (Li et al., 2025), LLaVA-NeXT-Video (Li et al., 2024b), LLaVA-OneVision (Li et al., 2024a), Llama-3.2-Vision (Dubey et al., 2024), Phi-3.5Vision (Abdin et al., 2024), InternVideo2 (Wang et al., 2024c), and VideoLLaMA2 & 2.1 (Cheng et al., 2024). We also evaluate eight series of proprietary models, including OpenAI o1 (OpenAI, 2024a) and GPT-4o (OpenAI, 2024b), Gemini-1.5 & 2 and Gemini-Thinking (Google, 2024), GLM-4V-Plus (GLM et al., 2024), Grok-2-Vision (xAI, 2024), and Claude-3.5 (Anthropic, 2024). For open-source models, we prioritize the vLLM pipeline (Kwon et al., 2023) for model inference; otherwise, we use the Transformers pipeline (Wolf et al., 2020). We use the official API service for proprietary models. For models without native video support, following VideoMME (Fu et al., 2024), we provide visual input using the maximum number of images that fits within the models context window. B.1 details the parameter settings and model configurations. We evaluate the models with both Direct Answer and Chain-of-Thought prompts (presented in appendix B.2), which is adapted from the versions used in MMMU-Pro (Yue et al., 2024b). Accuracy Evaluation. We use accuracy as the primary metric to evaluate model performance on MMVU. Following recent benchmarks for foundation model evaluation (Wang et al., 2024e; Lu et al., 2024; He et al., 2024), we employ GPT-4o to assess accuracy. Specifically, given question, its ground truth answer, and the models response, GPT-4o is instructed to extract the final answer from the model response and determine its correctness. The evaluation prompts for both multiplechoice and open-ended questions are presented in Appendix B.3. 4.2 MAIN FINDINGS Table 3 presents the evaluated models CoT performance on the MMVU benchmark, while Figure 4 illustrates comparison between the model performance in CoT reasoning and direct answering. Our key findings are as follows: MMVU presents substantial challenges for current multimodal foundation models. Even the top-performing model falls well short of human expert performance. For instance, GPT-4o achieves 66.7% accuracy with CoT prompting, significantly lower than the 86.8% accuracy achieved by human experts in an open-book setting. Notably, while GPT-4o has narrowed the performance gap with human experts in text-based expert-level reasoning on MMLU (88.7% vs 89.8% (Hendrycks et al., 2021)) and image-based expert-level reasoning on MMMU (69.1% vs 82.6% (Yue et al., 2024a)), the gap remains large on MMVU. This disparity underscores MMVUs critical role in advancing and evaluating multimodal foundation models capabilities in video-based expert reasoning across specialized domains. 7 Zhao et al. (2025) Table 3: Accuracy of evaluated foundation models on the MMVU validation and test sets using CoT prompts. Model performance is ranked based on overall results on the test set. : For o1, as the API access for its multimodal version has not been granted, we randomly sampled 100 examples from the validation set and 200 examples (50 for each core discipline) from the test set. The models performance was manually evaluated on Jan 10, 2025, using CoT prompts on ChatGPT platform. Release Science Healthcare Human. & Social Sci. Engineering Test Set Avg. Validation Avg. Test Human Performance Human Oracle Human Open-book Human Closed-book o1 Gemini 2.0 Flash Thinking GPT-4o Gemini 2.0 Flash Gemini 1.5 Pro Claude 3.5 Sonnet Grok-2-Vision GPT-4o-mini Gemini 1.5 Flash GLM-4V-Plus Qwen2-VL-72B DeepSeek-VL2 InternVL2.5-38B Aria Llama-3.2-90B-Vision DeepSeek-VL2-Small Qwen2-VL-7B-Instruct InternVL2.5-8B VideoLLaMA2.1-7B Llama-3.2-11B-Vision Phi-3.5-Vision LLaVA-OneVision-7B Qwen2-VL-2B InternVL2-8B Idefics3-8B VideoLLaMA2-7B DeepSeek-VL2-Tiny Pixtral-12B LLaVA-NeXT-Video-34B InternVideo2-8B H2OVL Mississippi-2B LLaVA-NeXT-Video-7B 2024-12 2024-12 2024-08 2024-12 2024-09 2024-10 2024-12 2024-07 2024-09 2025-01 2024-09 2024-12 2024-11 2024-11 2024-09 2024-12 2024-08 2024-11 2024-10 2024-09 2024-08 2024-09 2024-08 2024-06 2024-08 2024-06 2024-12 2024-09 2024-06 2024-08 2024-10 202495.3 86.7 54.7 80.0 69.3 67.2 70.8 67.2 60.5 60.6 60.3 56.8 52.2 93.3 84.7 42.7 Proprietary Models 78.0 71.2 71.8 62.7 68.1 64.0 72.5 60.9 57.3 57.3 Open-sourced Models 48.0 50.3 50.3 46.8 46.5 47.5 43.6 39.2 35.3 40.5 38.3 34.3 32.6 36.7 37.0 32.3 34.3 36.1 31.8 29.6 29.1 27.0 53.6 53.4 45.6 43.3 43.5 48.7 42.5 36.8 38.9 39.4 29.5 38.6 40.9 32.9 35.5 27.7 33.4 24.6 24.6 31.1 29.5 31.1 96.0 92.7 44.7 76.0 73.4 72.0 71.6 67.0 70.9 72.0 70.6 66.3 64.9 61.7 58.9 52.8 61.0 53.9 47.5 43.6 47.2 45.4 44.0 45.4 40.8 40.4 36.9 44.0 44.3 35.8 37.9 35.8 37.2 29.4 27.3 96.7 83.3 56. 74.0 67.3 61.6 63.0 62.8 64.5 57.4 59.3 58.2 55.4 53.9 48.6 52.8 49.9 48.1 45.1 41.2 42.3 41.6 35.7 41.1 38.8 35.7 37.2 31.2 35.7 30.1 30.8 30.3 26.5 28.0 29.5 95.3 86.8 49.7 77.0 69.5 66.7 66.5 65.8 64.1 63.4 61.5 58.8 56.2 53.2 51.5 50.7 49.3 47.6 46.9 42.5 41.0 39.8 39.0 38.7 37.7 36.5 36.2 35.6 34.4 32.8 32.2 30.4 29.9 28.8 28.7 79.0 69.1 67.4 65.9 65.4 65.2 62.7 61.6 58.8 56. 53.0 52.1 50.5 49.3 47.1 46.9 42.1 41.1 39.5 38.9 38.1 37.9 36.5 36.3 35.3 34.4 33.0 32.3 30.5 29.9 29.1 28.6 Performance of open-sourced models. As for open-source multimodal foundation models, they still lag behind the proprietary models. However, the Qwen2-VL-72B and DeepSeek-VL2 models have achieved performance levels that exceed human benchmarks in closed-book settings and are approaching the performance of leading proprietary models. These advancements highlight the significant progress being made in the development of open-source models. CoT reasoning generally improves model performance compared to directly outputting the answer. However, the degree of improvement varies across different foundation models. For instance, Claude 3.5 Sonnet demonstrated remarkable enhancement, achieving notable performance gain of 11.0%, as corroborated by the findings in MMMU-Pro (Yue et al., 2024b). 8 Zhao et al. (2025) Conversely, models like GPT-4o exhibited only marginal improvements. These results indicate that the impact of CoT reasoning is not uniformly beneficial across all models on MMVU. System-2 thinking demonstrates effectiveness. Models capable of System-2 thinking and employing long CoT demonstrate significant performance advantages. Notably, the o1 and Gemini 2.0 Flash Thinking models achieved the top two results on MMVU, illustrating that increasing test-time compute and applying long CoT can significantly enhance model performance in expert-level video reasoning tasks. These results highlight the potential of developing opensource models designed to facilitate and advance System-2 thinking capabilities. 4.3 QUALITATIVE ANALYSIS Figure 4: Comparison of model performance between CoT and direct answering on the validation set. The full results are provided in C.1. To gain deeper understanding of the capabilities and limitations of frontier models on MMVU, we perform comprehensive case studies and error analysis by humans. The inclusion of expertannotated reasoning rationales and domain knowledge for each example in MMVU facilitate more effective analysis compared to datasets that provide only answers. We focus on four top-performing models, GPT-4o, Qwen2-VL-72B, Llama-3.2-90B-Vision, and DeepSeek-VL2, for human evaluation. From the MMVU validation set, we randomly sample 50 error cases for each model. These cases are analyzed by the authors using ground-truth features (i.e., expert-annotated reasoning rationales and required domain knowledge) as references. We identify following six primary errors: Visual Perception Error (18%): The model fails to accurately interpret spatial, temporal, or semantic aspects of visual information within video. Additionally, it might hallucinate, detecting objects or events that are not actually present in the video. Figure 5 (left) is typical related instance where the model fails to correctly perceive the traversal order of the binary tree. Similarly, Figure 18 shows that the model mistakenly identifies the device shell in the video as water, leading to completely wrong reasoning about the devices function. Misuse or Lack Domain Knowledge in Visual Perception (20%): The model fails to apply the domain-specific expertise required to accurately interpret specialized concepts or elements within the video. For example, in medical video, it may identify objects but fail to recognize their technical terms or misunderstand their importance within the procedure being demonstrated. Moreover, as shown in Figure 20, the model correctly perceives the ascending numbers (array indices), but misuses its pretrained knowledge and misidentifies them as the numbers to be sorted. It leads to the wrong conclusion that the video demonstrates sorting algorithm. This limitation underscores gap in the models ability to integrate domain knowledge with visual perception effectively. Misuse or Lack Domain Knowledge in Reasoning (27%): The model fails to effectively recall and apply domain knowledge during its reasoning processes. For instance, when addressing questions over chemistry videos, it may fail to correctly apply relevant chemical equations, leading to errors in computing the reaction mass. notable example is Figure 5 (right), where the model misuses the domain knowledge that bats often live in unsanitary environments and makes the wrong inference that poor hygiene conditions are the cause of virus outbreaks. Besides, in Figure 25, the model lacks the domain knowledge about relevant chemical equations, so that it cannot correctly answer the question. This limitation underscores the models inability to integrate domain knowledge into its reasoning processes effectively. Heavy Reliance on Textual Information (20%): The model predominantly depends on textual information for problem-solving, especially when addressing multiple-choice questions, as it evaluates each option individually without leveraging the actual video content. For instance, Figure 26 shows the model ignores the video information about the reason of the disease and overly focuses on the textual question. Similar limitations have been observed in other multimodal benchmarks (Fu 9 Zhao et al. (2025) Figure 5: Illustrations of visual perception error and misuse or lack domain knowledge in reasoning. et al., 2024; Yue et al., 2024a). This gap suggests future work in enhancing multimodal reasoning by more effectively incorporating non-textual content into the reasoning process. Logical Reasoning Error (6%): The model exhibits inconsistencies between its reasoning process and final answer, leading to self-contradiction. As depicted in Figure 28, the analysis of one specific option contradicts with the other reasoning steps, which is typical self-contradiction logical error. Other Error (9%): This includes other errors, such as refusing to answer question due to insufficient context or safety concerns, generating response that exceeds the output limit, generating repetitive information, or making incorrect math computation."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce MMVU, high-quality, multi-disciplinary benchmark designed to assess the expertlevel, knowledge-intensive reasoning capabilities of multimodal foundation models on specializeddomain videos. Each example in MMVU is annotated by human experts from scratch. We employ textbook-guided example annotation pipeline designed to capture both the breadth of knowledge and depth of reasoning. In our evaluation of 32 frontier multimodal foundation models, we find that while the latest o1 model achieves the highest performance among all tested modelsapproaching human expert-level proficiencya notable performance gap remains between other models and human experts. Additionally, models employing CoT reasoning consistently outperform those that generate final answers directly. Through comprehensive error analysis and case studies, we identify persistent challenges of MMVU, offering valuable insights for advancing foundation models capabilities to achieve expert-level video understanding in specialized domains. 10 Zhao et al. (2025)"
        },
        {
            "title": "AUTHOR CONTRIBUTION",
            "content": "The author contributions are summarized below: Project Lead: Yilun Zhao Project Conception: Yilun Zhao, Lujing Xie, Yitao Long, Zhiyuan Hu, Zhenwen Liang, Xiangru Tang, Yixin Liu, Chen Zhao, Arman Cohan User Study: Every author Data Annotation Protocol Development: Yilun Zhao, Lujing Xie, Chengye Wang Data Annotation Task Management: Lujing Xie, Haowei Zhang Data Annotation: Lujing Xie, Haowei Zhang, Tongyan Hu, Weiyuan Chen, Junyang Song, Zhijian Xu, Weifeng Pan, Guo Gan, Yitao Long Data Validation: Lujing Xie, Haowei Zhang, Tongyan Hu, Weiyuan Chen, Yilun Zhao, Junyang Song Data Annotation Expense: Yilun Zhao Codebases and Results: Yilun Zhao, Guo Gan Error Analysis and Case Study: Haowei Zhang, Lujing Xie, Yilun Zhao, Weiyuan Chen Manuscript Writing: Yilun Zhao, Haowei Zhang, Arman Cohan Manuscript Editing: Every author"
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel PerezBecker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Bruce Alberts, Alexander Johnson, Julian Lewis, Martin Raff, Keith Roberts, and Peter Walter. Molecular Biology of the Cell. Garland Science, 6th edition, 2014. Phillip Allen and Douglas Holberg. CMOS analog circuit design. Elsevier, 2011. Anthropic. Introducing the next generation of claude, 2024. URL https://www.anthropic. com/news/claude-3-family. Mumtaz Anwar, Riyaz Ahmad Rather, and Zeenat Farooq. Fundamentals and advances in medical biotechnology. Springer, 2022. 11 Zhao et al. (2025) Steven Ascher and Edward Pincus. The Filmmakers Handbook: Comprehensive Guide for the Digital Age. Plume, Penguin Random House, 5th edition, 2012. Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, and Mohamed Elhoseiny. Infinibench: comprehensive benchmark for large multimodal models in very long video understanding, 2024. URL https://arxiv.org/abs/2406.19875. Peter William Atkins, Julio De Paula, and James Keeler. Atkins physical chemistry. Oxford university press, 2023. Eugene A. Avallone, Theodore Baumeister, and Ali M. Sadegh. Marks Standard Handbook for Mechanical Engineers. McGraw-Hill Education, 12th edition, 2018. Ashwani Bedi and Ramsey Dabby. Structure for Architects: Case Study in Steel, Wood, and Reinforced Concrete Design. Routledge, 1st edition, 2019. Fred Bell. Engineering geology and construction. CRC Press, 2004. Olivier Blanchard. Macroeconomics. Pearson, 9th edition, 2024. David S. Bright, Anastasia H. Cortes, et al. Principles of Management. OpenStax, Rice University, 2019. Available at https://openstax.org/details/books/principles-management. Theodore L. Brown, H. Eugene LeMay, Bruce E. Bursten, Catherine J. Murphy, Patrick M. Woodward, and Matthew E. Stoltzfus. Chemistry: The Central Science. Pearson, 15th edition, 2023. Laurence L. Brunton, Randa Hilal-Dandan, and Bjorn Knollman. Goodman & Gilmans: The Pharmacological Basis of Therapeutics. McGraw-Hill Education, 13th edition, 2017. Randal Bryant and David Richard OHallaron. Computer systems: programmers perspective. Prentice Hall, 2011. Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, and Jianwei Yang. Temporalbench: Benchmarking fine-grained temporal understanding for multimodal video models, 2024. URL https://arxiv.org/abs/2410.10818. William Callister Jr and David Rethwisch. Materials science and engineering: an introduction. John wiley & sons, 2020. Krishan K. Chawla. Composite Materials: Science and Engineering. Springer, 3rd edition, 2012. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large visionlanguage models?, 2024a. URL https://arxiv.org/abs/2403.20330. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 78897901, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.489. URL https: //aclanthology.org/2023.emnlp-main.489. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023b. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt4v? closing the gap to commercial multimodal models with open-source suites, 2024b. URL https://arxiv.org/abs/2404.16821. 12 Zhao et al. (2025) Zhihong Chen, Ruifei Zhang, Yibing Song, Xiang Wan, and Guanbin Li. Advancing visual grounding with scene knowledge: Benchmark and method. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1503915049, June 2023c. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. URL https://arxiv.org/abs/2406.07476. Mary Ann Clark, Jung Choi, and Matthew Douglas. Biology. OpenStax, Rice University, 2nd edition, 2018a. Available at https://openstax.org/details/books/biology-2e. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018b. Jonathan Clayden, Nick Greeves, and Stuart Warren. Organic chemistry. Oxford University Press, USA, 2012. Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Cees G. M. Snoek, and Yuki M. Asano. Tvbench: Redesigning video-language evaluation, 2024. URL https://arxiv.org/abs/ 2410.07752. Thomas Cormen, Charles Leiserson, Ronald Rivest, and Clifford Stein. Introduction to algorithms. MIT press, 2022. Braja M. Das. Principles of Geotechnical Engineering. Cengage Learning, 9th edition, 2017. Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, and Preslav Nakov. Exams-v: multi-discipline multilingual multimodal exam benchmark for evaluating vision language models, 2024. URL https://arxiv.org/abs/2403.10378. Mackenzie L. Davis and David A. Cornwell. Introduction to Environmental Engineering. McGrawHill Education, 5th edition, 2012. Andong Deng, Taojiannan Yang, and Chen Chen. large-scale study of spatiotemporal representation learning with new benchmark on action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2051920531, October 2023. Chunyuan Deng, Yilun Zhao, Yuzhao Heng, Yitong Li, Jiannan Cao, Xiangru Tang, and Arman Cohan. Unveiling the spectrum of data contamination in language model: survey from detection In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the to remediation. Association for Computational Linguistics: ACL 2024, pp. 1607816092, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 951. URL https://aclanthology.org/2024.findings-acl.951/. Avi Domb, Boaz Mizrahi, and Shady Farah. Biomaterials and Biopolymers. Springer, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, More, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. John D. Enderle and Joseph D. Bronzino. Introduction to Biomedical Engineering. Academic Press, 4th edition, 2017. Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding, 2024. URL https://arxiv.org/abs/2406.14515. Adam Feather, David Randall, and Mona Waterhouse. Kumar and Clarks Clinical Medicine EBook: Kumar and Clarks Clinical Medicine E-Book. Elsevier Health Sciences, 2020. 13 Zhao et al. (2025) Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos, 2024. URL https://arxiv.org/abs/2408.14023. Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. Sciknoweval: Evaluating multi-level scientific knowledge of large language models, 2024. URL https://arxiv.org/abs/2406.09098. Harry Field and John Long. Introduction to agricultural engineering technology: problem solving approach. Springer, 2018. Paul Flowers, Klaus Theopold, Richard Langley, and William R. Robinson. Chemistry. OpenStax, Rice University, 2nd edition, 2019. Available at https://openstax.org/details/books/chemistry-2e. Erin Fouberg and Alexander Murphy. Human Geography: People, Place, and Culture. John Wiley & Sons, 2020. Fabrizio Frigeni. Industrial Robotics Control: Mathematical Models, Software Architecture, and Electronics Design. Springer, 2022. Victoria Fromkin, Robert Rodman, and Nina Hyams. An Introduction to Language. Cengage Learning, 11th edition, 2017. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The firstever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024. URL https://arxiv.org/abs/2405.21075. Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, and Sri Satish Ambati. H2ovl-mississippi vision language models technical report, 2024. URL https://arxiv.org/abs/2410.13611. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Jarviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai, 2024. URL https://arxiv.org/abs/2411.04872. Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. URL https://arxiv.org/ abs/2406.12793. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. Nikolai V. Gorbunov. Tissue Barriers in Disease, Injury and Regeneration. Elsevier, 1st edition, 2022. Steven A. Greenlaw, David Shapiro, and Daniel MacDonald. Principles of Economics. OpenStax, Rice University, 3rd edition, 2023. Available at https://openstax.org/details/books/principleseconomics-3e. 14 Zhao et al. (2025) David Griffiths. Introduction to electrodynamics. Cambridge University Press, 2023. Allan Hambley. Electrical Engineering: Principles and Applications. Pearson London, UK, 2018. Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, and Xin Eric Wang. Mmworld: Towards multi-discipline multi-faceted world model evaluation in videos, 2024. URL https://arxiv.org/abs/2406.08407. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 961970, 2015. doi: 10.1109/CVPR.2015. 7298698. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Darrel Hess and Tom L. McKnight. McKnights Physical Geography: Landscape Appreciation. Pearson, 13th edition, 2021. HLTCOE@JHU. Turkle: web-based tool for managing annotation tasks. https://github. com/hltcoe/turkle, 2024. Accessed: 2024-11-01. Paul Horowitz and Winfield Hill. The art of electronics, 2015. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models, 2023. URL https: //arxiv.org/abs/2305.08322. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei Liu. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai, 2024a. URL https://arxiv.org/abs/2406.12753. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2180721818, June 2024b. Peter Huber and Alastair Mullis. The CISG: new textbook for students and practitioners. Sellier de Gruyter, 2009. Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 50755084, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.308. URL https://aclanthology.org/2023.emnlp-main.308/. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatioIn Proceedings of the IEEE conference on temporal reasoning in visual question answering. computer vision and pattern recognition, pp. 27582766, 2017. 15 Zhao et al. (2025) Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. ArXiv, URL https://api.semanticscholar.org/CorpusID: abs/2310.06825, 2023. 263830494. Zhihuan Jiang, Zhen Yang, Jinhao Chen, Zhengxiao Du, Weihan Wang, Bin Xu, Yuxiao Dong, and Jie Tang. Visscience: An extensive benchmark for evaluating k12 educational multi-modal scientific reasoning, 2024. URL https://arxiv.org/abs/2409.13730. Eric R. Kandel, James H. Schwartz, Thomas M. Jessell, Steven A. Siegelbaum, and A.J. Hudspeth. Principles of Neural Science. McGraw-Hill Education, 6th edition, 2021. Ilker Kesen, Andrea Pedrotti, Mustafa Dogan, Michele Cafagna, Emre Can Acikgoz, Letitia Parcalabescu, Iacer Calixto, Anette Frank, Albert Gatt, Aykut Erdem, and Erkut Erdem. Vilma: zero-shot benchmark for linguistic and temporal grounding in video-language models, 2023. URL https://arxiv.org/abs/2311.07022. Ilker Kesen, Andrea Pedrotti, Mustafa Dogan, Michele Cafagna, Emre Can Acikgoz, Letitia Parcalabescu, Iacer Calixto, Anette Frank, Albert Gatt, Aykut Erdem, and Erkut Erdem. ViLMA: zero-shot benchmark for linguistic and temporal grounding in video-language models. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=liuqDwmbQJ. Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Jameel Hassan, Muzammal Naseer, Federico Tombari, Fahad Shahbaz Khan, and Salman Khan. How good is my video lmm? complex video reasoning and robustness evaluation suite for video-lmms, 2024. URL https: //arxiv.org/abs/2405.03690. Richard R. Kibbe, Roland O. Meyer, John E. Neely, and Warran T. White. Machine Tool Practices. Pearson, 11th edition, 2019. David R. Klein. Organic Chemistry as Second Language: First Semester Topics. John Wiley & Sons, 2024. Fred S. Kleiner. Art Through the Ages: Global History, Volume I. Cengage Learning, 16th edition, 2020. Ann Kordas, Ryan J. Lynch, et al. World History Volume 1. OpenStax, Rice University, 2022. Available at https://openstax.org/details/books/world-history-volume-1. Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pp. 706715, 2017. Vinay Kumar, Abul K. Abbas, and Jon C. Aster. Robbins and Cotran Pathologic Basis of Disease. Elsevier, 10th edition, 2020. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. URL https://arxiv.org/abs/2405.02246. Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. TVQA: Localized, compositional video question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 13691379, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1167. URL https://aclanthology.org/D18-1167/. 16 Zhao et al. (2025) Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024a. URL https://arxiv.org/abs/2408.03326. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Peng Liu, Lihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model, 2025. URL https://arxiv.org/abs/2410.05993. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024b. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark, 2024c. URL https://arxiv.org/abs/2311.17005. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024d. Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of video-language models, 2024e. URL https://arxiv.org/abs/2311.17404. Xinhao Li, Zhenpeng Huang, Jing Wang, Kunchang Li, and Limin Wang. Videoeval: Comprehensive benchmark suite for low-cost evaluation of video foundation model, 2024f. URL https://arxiv.org/abs/2407.06491. Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, Linda Ruth Petzold, Stephen D. Wilson, Woosang Lim, and William Yang Wang. Mmsci: dataset for graduate-level multi-discipline multimodal scientific understanding, 2024g. URL https://arxiv.org/abs/2407.04903. Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, and Xiangliang Zhang. Scemqa: scientific college entrance level multimodal question answering benchmark, 2024. URL https://arxiv.org/abs/2402.05138. Samuel J. Ling, Jeff Sanny, and William Moebs. University Physics Volume 1. OpenStax, Rice University, 2016a. Available at https://openstax.org/details/books/university-physics-volume-1. Samuel J. Ling, Jeff Sanny, and William Moebs. University Physics Volume 2. OpenStax, Rice University, 2016b. Available at https://openstax.org/details/books/university-physics-volume-2. Samuel J. Ling, Jeff Sanny, and William Moebs. University Physics Volume 3. OpenStax, Rice University, 2016c. Available at https://openstax.org/details/books/university-physics-volume-3. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Jiaying Liu, Sijie Song, Chunhui Liu, Yanghao Li, and Yueyu Hu. benchmark dataset and comparison study for multi-modal human action analytics. ACM Trans. Multimedia Comput. Commun. Appl., 16(2), May 2020. doi: 10.1145/3365212. URL https://doi.org/10.1145/3365212. ISSN 1551-6857. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and In Lun-Wei Ku, Andre Lu Hou. TempCompass: Do video LLMs really understand videos? Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 87318772, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.517. URL https://aclanthology.org/ 2024.findings-acl.517/. 17 Zhao et al. (2025) William Lowrie and Andreas Fichtner. Fundamentals of geophysics. Cambridge university press, 2020. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Learn to explain: Multimodal reaOyvind Tafjord, Peter Clark, and Ashwin Kalyan. soning via thought chains for science question answering. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 25072521. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KUNzEQMWU7. Liqun Luo. Principles of neurobiology. Garland Science, 2020. Marina MacKay. The Cambridge introduction to the novel. Cambridge University Press, 2010. Upamanyu Madhow. Introduction to communication systems. Cambridge University Press, 2014. PK Mallick. Fiber-reinforced composites: Materials, manufacturing, and design, 2007. Gregory N. Mankiw. Principles of Microeconomics. Cengage Learning, 9th edition, 2020. Kenneth Fuller Maxcy, Milton Joseph Rosenau, John Last, Robert Wallace, Neal Kohatsu, and Ross Brownson. Maxcy-Rosenau-Last public health & preventive medicine. McGraw-Hill, 2008. MistralAI. Announcing pixtral 12b, 2024. URL https://mistral.ai/news/ pixtral-12b/. Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, and Tobias Weyand. Neptune: The long orbit to benchmarking long video understanding. 2024. Jill Nelmes (ed.). Introduction to Film Studies. Routledge, 5th edition, 2012. Donald Nield and Adrian Bejan. Convection in Porous Media. Springer, 2017. Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models, 2023. URL https://arxiv.org/abs/2311.16103. Katsuhiko Ogata. Modern Control Engineering. Prentice Hall, 5th edition, 2010. OpenAI. Openai o1 system card. 2024a. URL https://api.semanticscholar.org/ CorpusID:274611667. OpenAI. Hello gpt-4o, 2024b. URL https://openai.com/index/hello-gpt-4o/. Judith A. Owen, Jenni Punt, and Sharon A. Stranford. Kuby Immunology. W.H. Freeman, 8th edition, 2018. David A. Patterson and John L. Hennessy. Computer organization and design: The hardware/software interface. Elsevier, 6th edition, 2022. Onno Rudolf Pols. Stellar structure and evolution. Astronomical Institute Utrecht NY, 2011. Dale Purves, GJ Augustine, David Fitzpatrick, WC Hall, AS LaMantia, RD Mooney, ML Platt, and LE White. Neuroscience (sixth edit), 2018. Gonzalez Rafael and Woods Richard. Digital Image Processing. Pearson Education, 2018. 18 Zhao et al. (2025) Colin Renfrew and Paul Bahn. Archaeology: Theories, Methods, and Practice. Thames & Hudson, 7th edition, 2016. Robert E. Ricklefs. The Economy of Nature. W.H. Freeman, 7th edition, 2013. Barbara Ryden and Bradley Peterson. Foundations of astrophysics. Cambridge University Press, 2020. Daniel V. Schroeder. An introduction to thermal physics. Oxford University Press, 2020. Robert Sedgewick and Kevin Wayne. Algorithms (4th edn). Google Scholar Google Scholar Digital Library Digital Library, 2011. Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models, 2024. URL https://arxiv.org/abs/2410.23266. Gunnar Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. In Computer Hollywood in homes: Crowdsourcing data collection for activity understanding. VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pp. 510526. Springer, 2016. Abraham Silberschatz, Peter B. Galvin, and Greg Gagne. Operating System Concepts. John Wiley & Sons, 10th edition, 2018. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. Scieval: multi-level large language model evaluation benchmark for scientific research. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):1905319061, Mar. 2024. doi: 10.1609/aaai.v38i17.29872. URL https://ojs.aaai.org/index.php/AAAI/ article/view/29872. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench In Anna Rogers, Jordan Boyd-Graber, tasks and whether chain-of-thought can solve them. and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023. findings-acl.824/. Rikito Takahashi, Hirokazu Kiyomaru, Chenhui Chu, and Sadao Kurohashi. Abstractive multivideo captioning: Benchmark dataset construction and extensive evaluation. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 5769, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.5. Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, and Chenliang Xu. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, 19 Zhao et al. (2025) Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. URL https://api.semanticscholar.org/ CorpusID:259950998. Chris Turner. Contract law. Routledge, 2013. Ray Turner. Arbitration awards: practical approach. John Wiley & Sons, 2008. Cornelis Van Kooten. Land resource economics and sustainable development: economic policies and the common good. UBC Press, 2011. Hal R. Varian. Intermediate Microeconomics: Modern Approach. W.W. Norton & Company, 8th edition, 2010. William Wagner, Shelly Sakiyama-Elbert, Guigen Zhang, and Michael Yaszemski. Biomaterials Science: An Introduction to Materials in Medicine. Elsevier, 2020. Jinfeng Wang. Intelligent Manufacturing System and Intelligent Workshop. Springer. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024a. URL https://arxiv.org/abs/2409. 12191. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark, 2024b. URL https://arxiv.org/abs/2406.08035. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenting Wang, Guo Chen, Baoqi Pei, Ziang Yan, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2: Scaling foundation models for multimodal video understanding, 2024c. URL https://arxiv.org/ abs/2403.15377. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024d. URL https://arxiv.org/abs/2406.01574. Yuxuan Wang, Difei Gao, Licheng Yu, Weixian Lei, Matt Feiszli, and Mike Zheng Shou. Geb+: benchmark for generic event boundary captioning, grounding and retrieval. In European Conference on Computer Vision, pp. 709725. Springer, 2022. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal llms, 2024e. URL https://arxiv.org/abs/2406.18521. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science quesIn Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin (eds.), Proceedings of tions. the 3rd Workshop on Noisy User-generated Text, pp. 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology.org/W17-4413/. Edward J. Wing and Fred J. Schiffman. Cecil Essentials of Medicine. Elsevier, 10th edition, 2021. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6. 20 Zhao et al. (2025) Bo Wu, Shoubin Yu, Zhenfang Chen, Josh Tenenbaum, and Chuang Gan. situated reasoning in real-world videos. of benchmark for and S. Yeung tems Track on Datasets and Benchmarks, datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/ 2021/file/5ef059938ba799aaa845e1c2e8a762bd-Paper-round2.pdf. Information Processing the Neural Proceedings volume 1, (eds.), 2021. Star: In J. Vanschoren SysURL https:// Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding, 2024. URL https: //arxiv.org/abs/2412.10302. xAI. Grok-2 beta release, 2024. URL https://x.ai/blog/grok-2. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 97779786, June 2021. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging In Proceedings of the IEEE conference on computer vision and pattern video and language. recognition, pp. 52885296, 2016. John Yagiela, Frank Dowd, Bart Johnson, Angelo Mariotti, and Enid Neidle. Pharmacology and Therapeutics for Dentistry-E-Book: Pharmacology and Therapeutics for Dentistry-E-Book. Elsevier Health Sciences, 2010. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671. Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces, 2024b. URL https://arxiv.org/abs/2412.14171. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding the IEEE/CVF Conference and reasoning benchmark for expert agi. on Computer Vision and Pattern Recognition (CVPR), pp. 95569567, June 2024a. URL https://openaccess.thecvf.com/content/CVPR2024/html/Yue_MMMU_A_ Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_ Benchmark_for_CVPR_2024_paper.html. In Proceedings of Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark, 2024b. URL https://arxiv. org/abs/2409.02813. Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, and Jie Fu. Cmmmu: chinese massive multi-discipline multimodal understanding benchmark, 2024a. URL https://arxiv.org/abs/2401.11944. 21 Zhao et al. (2025) Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding. In Yansong Feng and Els Lefever (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 543553, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-demo.49. URL https://aclanthology.org/2023. emnlp-demo.49/. Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao. Movqa: benchmark of versatile question-answering for long-form movie understanding, 2023b. URL https://arxiv.org/abs/2312.04817. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?, 2024b. URL https://arxiv.org/abs/ 2403.14624. Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, and Arman Cohan. Financemath: Knowledge-intensive math reasoning in finance domains. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1284112858, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.693. URL https://aclanthology.org/2024.acl-long.693/. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 22992314, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.149. URL https://aclanthology.org/2024.findings-naacl.149/."
        },
        {
            "title": "Appendix Contents",
            "content": "A MMVU Preliminary Setup A.1 Annotator Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Textbook for Each Subject A.3 Annotation Guideline and Interface . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Validation Guideline and Interface . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Data Annotation and Validation Payment . . . . . . . . . . . . . . . . . . . . . . . Experiment Setup B.1 Configuration of Evaluated Models . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Chain-of-Thought and Direct Answer Prompts . . . . . . . . . . . . . . . . . . . . B.3 Prompts for Accuracy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment C.1 Comparison Between CoT Reasoning and Direct Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Error Case Analysis: Visual Perception Error C.3 Error Case Analysis: Misuse or Lack Domain Knowledge in Visual Perception . . C.4 Error Case Analysis: Misuse or Lack Domain Knowledge in Reasoning . . . . . . C.5 Error Case Analysis: Heavy Reliance on Textual Information . . . . . . . . . . . . C.6 Error Case Analysis: Logical Reasoning Error . . . . . . . . . . . . . . . . . . . . 24 24 26 29 31 32 32 33 35 36 36 37 40 43 46"
        },
        {
            "title": "A MMVU PRELIMINARY SETUP",
            "content": "A.1 ANNOTATOR BIOGRAPHY The detailed biographies of the annotators involved in MMVU construction are presented in Table 4. All annotators are from universities ranked in the Top 500 of the 2024 QS Global Rankings3 and are fluent in English. ID Year Major Assigned Subject(s) Author? Validator? 1st year Master Biomedical Engineering Agricultural and Biosystems Engineering Biomedical Engineering Architecture Civil Engineering 1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 25 26 27 28 30 31 32 33 Bioinformatics Biological Engineering Biomedical Engineering 1st year Master 1st year Master 2nd year Master 5th year PhD 2nd year Master 3rd year PhD 3rd year Undergraduate Electrical Engineering 2nd year Master Electrical Engineering 2nd year Master Electrical Engineering 3rd year Undergraduate 2nd year Master Software Engineering Computer Science 1st year PhD Electrical Engineering 1st year PhD 1st year Master Electrical Engineering Electrical Engineering 1st year PhD 3rd year PhD 4th year PhD 4th year Undergraduate Aerospace Engineering Electrical Engineering Food Science Materials Science 4th year Undergraduate Mechanical Engineering 2nd year PhD 1st year PhD 1st year Master Mechanical Engineering Mechanical Engineering Medicine 1st year Master Radiology 1st year Master Dentistry 1st year PhD Nursing 3rd year Undergraduate Epidemiology 3rd year Undergraduate Medicine 2nd year PhD Medicine Biomedical Engineering Computer Science Electrical Engineering Biomedical Engineering Biomedical Engineering Biomedical Engineering Electronics and Communication Civil Engineering Civil Engineering Mechanical Engineering Computer Science Electrical Engineering Computer Science Electronics and Communication Computer Science Mechanical Engineering Computer Science Computer Science Electrical Engineering Computer Science Electronics and Communication Electrical Engineering Electrical Engineering Mechanical Engineering Electronics and Communication Mechanics Materials Science Materials Science Mechanical Engineering Materials Science Mechanical Engineering Mechanical Engineering Mechanical Engineering Basic Medicine Clinical Medicine Basic Medicine Clinical Medicine Basic Medicine Dentistry Basic Medicine Pharmacy Basic Medicine Preventive Medicine Clinical Medicine Clinical Medicine Pharmacy Table 4: Biographies of 73 annotators involved in MMVU construction (Author biographies are hidden to protect identity confidentiality). 3https://www.topuniversities.com/world-university-rankings 24 Back to Appendix Table of Contents ID Year Major Assigned Subject(s) Author? Validator? 34 35 36 37 38 39 40 41 42 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 63 64 65 66 67 68 69 70 71 72 73 4th year PhD Dentistry 3rd year Undergraduate Dentistry Dentistry 4th year PhD Public Health 1st year PhD 4th year Undergraduate 3rd year PhD 4th year PhD Pharmacy East Asian Studies Literature 1st year PhD Economics 4th year Undergraduate Accounting 4th year PhD 3rd year PhD Finance Public Administration 1st year Master 5th year PhD 3rd year Undergraduate 5th year PhD 2nd year Master Literature Linguistics Public Administration Astronomy Astronomy 3rd year PhD 1st year PhD Biology Biology 3rd year PhD Marine Biology 1st year PhD Chemistry 3rd year Undergraduate Chemistry 1st year PhD 4th year Undergraduate Physics Physics 4th year PhD 1st year PhD Physics Physics 1st year Master Physics Dentistry Dentistry Dentistry Pharmacy Preventive Medicine Pharmacy Art Art History Literature History Economics Economics Law Economics Law Management Literature Literature Management Astronomy Astronomy Geography Biology Biology Neurobiology Biology Chemistry Chemistry Chemistry Electromagnetism Electromagnetism Thermodynamics Electromagnetism Electromagnetism Mechanics Thermodynamics Thermodynamics Electromagnetism 3rd year Undergraduate Agricultural and Environmental Sciences Geography 4th year PhD Physics Physics Physics Physics 1st year PhD 3rd year PhD 4th year PhD 3rd year Undergraduate Neurobiology Neurobiology 1st year PhD 3rd year Undergraduate Biology Biology 1st year Master Thermodynamics Mechanics Modern Physics Mechanics Mechanics Modern Physics Neurobiology Neurobiology Neurobiology Neurobiology Table 5: Biographies of 73 annotators involved in MMVU construction (Author biographies are hidden to protect identity confidentiality). 25 Back to Appendix Table of Contents A.2 TEXTBOOK FOR EACH SUBJECT As discussed in Section 3.2, we design textbook-guided example annotation pipeline to encompass both the breadth of knowledge and the depth of reasoning. The textbooks used for each subject are detailed in the following tables. They are selected by expert annotators and are recognized as authoritative references in their respective fields. Subject Astronomy Biology Chemistry 1. Foundations of Astrophysics (Ryden & Peterson, 2020) 2. Stellar Structure And Evolution (Pols, 2011) Textbook Introduction to Agricultural Engineering Technology: Problem Solving 1. Biology, 2nd Edition (Clark et al., 2018a) 2. Approach, 4th Edition (Field & Long, 2018) 3. well, 2012) 4. The Economy of Nature, 7th Edition (Ricklefs, 2013) 5. The Molecular Biology of the Cell, 6th Edition (Alberts et al., 2014) Introduction to Environmental Engineering, 5th Edition (Davis & Corn1. Atkins Physical Chemistry, 12th Edition (Atkins et al., 2023) 2. Chemistry, 2nd Edition (Flowers et al., 2019) 3. Chemistry: The Central Science, 15th Edition (Brown et al., 2023) 4. Organic Chemistry As Second Language (Klein, 2024) 5. Organic Chemistry, 2nd Edition (Clayden et al., 2012) Electromagnetism Introduction to Electrodynamics, 4th Edition (Griffiths, 2023) 1. 2. University Physics Volume 2 (Electromagnetism) (Ling et al., 2016b) Geography 1. Fundamentals of Geophysics, 2nd Edition (Lowrie & Fichtner, 2020) 2. Human Geography, 12th Edition (Fouberg & Murphy, 2020) 3. Physical Geography: Landscape Appreciation, 10th Edition (Hess & McKnight, 2021) Mechanics 1. University Physics Volume 1 (Ling et al., 2016a) Modern Physics 1. University Physics Volume 3 (Ling et al., 2016c) Neurobiology 1. Neuroscience, 6th Edition (Purves et al., 2018) 2. Principles of Neural Science, 6th Edition (Kandel et al., 2021) 3. Principles of Neurobiology (Luo, 2020) Thermodynamics 1. An Introduction to Thermal Physics (Schroeder, 2020) 2. University Physics Volume 2 (Thermodynamics) (Ling et al., 2016b) Table 6: List of textbooks and corresponding example numbers for the Science discipline. 26 Back to Appendix Table of Contents Subject Biomedical Engineering Civil Engineering Computer Science Textbook 1. Biomaterials Science: An Introduction to Materials in Medicine, 4th Edition (Wagner et al., 2020) 2. Biomaterials and Biopolymers (Domb et al., 2023) 3. 2022) 4. 2017) Introduction to Biomedical Engineering, 4th Edition (Enderle & Bronzino, Fundamentals and Advances in Medical Biotechnology (Anwar et al., 1. Engineering Geology and Construction (Bell, 2004) 2. Principles of Geotechnical Engineering, 9th Edition (Das, 2017) 3. Concrete Design (Bedi & Dabby, 2019) Structure for Architects: Case Study in Steel, Wood, and Reinforced 1. Algorithms, 4th Edition (Sedgewick & Wayne, 2011) 2. Computer Organization and Design: The Hardware/Software Interface, 6th Edition (Patterson & Hennessy, 2022) 3. Computer Systems: Programmers Perspective, 3rd Edition (Bryant & OHallaron, 2011) 4. Deep Learning (Goodfellow et al., 2016) 5. Digital Image Processing, 4th Edition (Rafael & Richard, 2018) Introduction to Algorithms, 4th Edition (Cormen et al., 2022) 6. 7. Operating System Concepts, 10th Edition (Silberschatz et al., 2018) Electrical Engineering 1. Electrical Engineering: Principles and Applications, 7th Edition (Hambley, 2018) Electronics and Communication 1. CMOS Analog Circuit Design, 3rd Edition (Allen & Holberg, 2011) Introduction to Communication Systems (Madhow, 2014) 2. 3. The Art of Electronics, 3rd Edition (Horowitz & Hill, 2015) Materials Science Mechanical Engineering 1. Composite Materials: Science and Engineering, 3rd Edition (Chawla, 2012) 2. Convection in Porous Media, 5th Edition (Nield & Bejan, 2017) 3. Fiber-Reinforced Composites Materials, Manufacturing, and Design, 3rd Edition (Mallick, 2007) 4. Materials Science and Engineering: An Introduction, 10th Edition (Callister Jr & Rethwisch, 2020) Intelligent Manufacturing System and Intelligent Workshop (Wang) Industrial Automation: An Engineering Approach Industrial Robotics Control: Mathematical Models, Software Architecture, 1. 2. and Electronics Design (Frigeni, 2022) 3. 4. Machine Tool Practices, 11th Edition (Kibbe et al., 2019) 5. Marks Standard Handbook for Mechanical Engineers, 12th Edition (Avallone et al., 2018) 6. Modern Control Engineering, 5th Edition (Ogata, 2010) Table 7: List of textbooks and corresponding example numbers for the Engineering discipline. 27 Back to Appendix Table of Contents Subject Basic Medicine Textbook 1. Kuby Immunology, 8th Edition (Owen et al., 2018) 2. Robbins and Cotran Pathologic Basis of Disease, 10th Edition (Kumar et al., 2020) 3. Tissue Barriers in Disease, Injury and Regeneration (Gorbunov, 2022) Clinical Medicine 1. Cecil Essentials of Medicine, 10th Edition (Wing & Schiffman, 2021) 2. Kumar and Clarks Clinical Medicine, 10th Edition (Feather et al., 2020) Dentistry Pharmacy Preventive Medicine 1. Pharmacology and Therapeutics for Dentistry, 7th Edition (Yagiela et al., 2010) 1. The Pharmacological Basis of Therapeutics, 13th Edition (Brunton et al., 2017) 1. Public Health and Preventive Medicine, 15th Edition (Maxcy et al., 2008) Table 8: List of textbooks and corresponding example numbers for the Healthcare discipline. Subject Art Economics History Law Literature Management Textbook 1. Art Through the Ages: Global History Volume I, 16th Edition (Kleiner, 2020) 2. 3. The Filmmakers Handbook: Comprehensive Guide for the Digital Age, 5th Edition (Ascher & Pincus, 2012) Introduction to Film Studies, 5th Edition (Nelmes, 2012) Intermediate Microeconomics: Modern Approach, 8th Edition (Varian, 1. 2010) 2. Land Resource Economics and Sustainable Development: Economic Policies and the Common Good (Van Kooten, 2011) 3. Macroeconomics, 9th Edition (Blanchard, 2024) 4. Principles of Economics, 3rd Edition (Greenlaw et al., 2023) 5. Principles of Microeconomics, 9th Edition (Mankiw, 2020) Archaeology: Theories Methods and Practice, 7th Edition (Renfrew & 1. Bahn, 2016) 2. World History Volume 1: to 1500 (Kordas et al., 2022) 1. Arbitration Awards: Practical Approach (Turner, 2008) 2. Contract Law (Turner, 2013) 3. The CISG: new textbook for students and practitioners (Huber & Mullis, 2009) 1. An Introduction to Language, 11th Edition (Fromkin et al., 2017) 2. The Cambridge Introduction to the Novel (MacKay, 2010) 1. Principles of Management (Bright et al., 2019) Table 9: List of textbooks and corresponding example numbers for the Humanities and Social Science discipline. 28 Back to Appendix Table of Contents A.3 ANNOTATION GUIDELINE AND INTERFACE With the goal of ensure the high quality of data, MMVU adheres to the following four benchmark construction desiderata, we develop the following annotation interface based on Turkle (HLTCOE@JHU, 2024), an open-source clone of Amazons Mechanical Turk: Figure 6: Annotation Interface - Step 1: Video Collection. In this step, annotators are required to input the YouTube video URL and select the desired question type. The backend system of the interface will automatically verify whether the provided YouTube video is under Creative Commons license using the YouTube Data API v3. If the video does not meet this requirement, as shown in the figure, warning message will be displayed, and the submission will be blocked. Once valid example is submitted, the annotation interface will proceed to Step 2, which is illustrated in the following two figures. Figure 7: Annotation Interface - Step 2: Multiple-choice Question Annotation. 29 Back to Appendix Table of Contents Figure 8: Annotation Interface - Step 2: Open-ended Question Annotation. 30 Back to Appendix Table of Contents A.4 VALIDATION GUIDELINE AND INTERFACE To ensure that the final dataset remains high-quality and meets expert-level standards without introducing unnecessary bias, each example in MMVU undergoes expert review by one of the authors or top-performing annotators to verify the accuracy of its annotations, following the annotation guideline detailed in Appendix A.3. The examples of validation interface are presented as follows: Figure 9: Validation Interface. Human validators are required to thoroughly review each annotation feature to ensure alignment with benchmark construction criteria and annotation guidelines. If revisions are not feasible, detailed feedback must be provided to the original annotator, who will then revise and resubmit the annotation for second review. Additionally, validators may discard examples deemed to be of low quality and unlikely to meet the desired criteria through revision. A.5 DATA ANNOTATION AND VALIDATION PAYMENT The annotation and validation process for MMVU spans three months. As outlined in Section 3.2, annotating examples for MMVU can be particularly time-intensive, especially when there is limited availability of videos with Creative Commons licenses in the required subjects. To accommodate this and ensure high-quality dataset, we compensate annotators based on the time they spend rather than the number of examples completed, preventing them from rushing through tasks. Annotators are required to record their screens throughout the annotation process, which enables us to verify time reporting accuracy and maintain productivity standards. This also helps us identify any distractions and precisely track the total time spent on each task. We offer base rate of 6 USD per hour for both annotation and validation work, with an additional 2 USD per completed annotation and 0.40 USD per validated example. On average, annotating single question for MMVU takes 20 minutes and 17 seconds, while validation requires 4 minutes and 12 seconds. This compensation structure ensures that annotators earn wages that are competitive with the average payment for teaching assistants at their respective universities. To reduce pressure and maintain comfortable pace, we recommended that annotators limit their work to maximum of 10 QA example annotations or 50 QA example validations per day. 31 Back to Appendix Table of Contents"
        },
        {
            "title": "B EXPERIMENT SETUP",
            "content": "B.1 CONFIGURATION OF EVALUATED MODELS Table 10 detail the configuration of each evaluated models. We use the default settings from the official implementation of each model to process vision input. Across all experiments, the temperature is set to 1.0, with maximum output length of 1024 tokens. However, for Gemini-2-Flash-Thinking, the maximum output length is set as 8192 tokens to accommodate its long CoT reasoning mechanism. All inferences are reproducible on workstation equipped with two NVIDIA A100-80G GPUs. Organization Model Release Version Support Video? Input Frames # Inference Pipeline OpenAI Google Anthropic xAI Zhipu AI Mistral AI Microsoft Shanghai AI Lab Alibaba Meta DAMO DeepSeek o1 GPT-4o GPT-4o-mini Gemini 2.0 Flash Thinking Gemini 2.0 Flash Gemini 1.5 Pro Gemini 1.5 Flash Claude-3.5-Sonnet Grok-2-Vision GLM-4V-Plus 2024-12 2024-8 2024-7 2024-12 2024-12 2024-9 2024-9 2024-10 20242025-1 Proprietary Models o1-2024-12-17 gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 gemini-2.0-flash-thinking-exp-1219 gemini-2.0-flash-exp gemini-1.5-pro gemini-1.5-flash claude-3-5-sonnet-20241022 grok-2-visionglm-4v-plus-0111 Open-source Multimodal Foundation Models Pixtral-12B Phi-3.5-Vision InternVL2.5-38B InternVL2.5-8B InternVL2-8B Qwen2-VL-2B Qwen2-VL-7B Qwen2-VL-72B Llama-3.2-11B-Vision Llama-3.2-90B-Vision VideoLLaMA2-7B VideoLLaMA2.1-7B DeepSeek-VL2 DeepSeek-VL2-Small DeepSeek-VL2-Tiny 2024-9 2024-7 2024-11 2024-11 20242024-8 2024-8 2024-9 2024-9 2024-9 2024-6 2024-10 2024-12 2024-12 2024-12 2024-11 2024-9 2024-6 20242024-8 2024-8 Pixtral-12B-2409 Phi-3.5-vision-instruct InternVL2.5-38B InternVL2.5-8B InternVL2-8B Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Llama-3.2-11B-Vision-Instruct Llama-3.2-90B-Vision-Instruct VideoLLaMA2-7B VideoLLaMA2.1-7B-16F deepseek-vl2 deepseek-vl2-small deepseek-vl2-tiny Aria-Chat llava-onevision-qwen2-7b-ov-chat-hf LLaVA-NeXT-Video-34B-hf LLaVA-NeXT-Video-7B-hf Idefics3-8B-Llama3 InternVideo2-Chat-8B Rhymes Aria Llava Hugging Face LLaVA-OneVision-7B LLaVA-NeXT-Video-34B LLaVA-NeXT-Video-7B HuggingFaceM4 Idefics3-8B OpenGVLab InternVideo2-8B 32 32 32 32 32 32 32 32 4 8 16 4 4 4 1fps 1fps 1fps 8 8 1fps 1fps 2 2 2 8 1fps 8 16 1fps 4 API API API API API vLLM vLLM vLLM vLLM vLLM HF HF vLLM vLLM vLLM vLLM vLLM vLLM vLLM vLLM HF vLLM H2O H2OVL Mississippi-2B 2024-10 h2ovl-mississippi-2b Table 10: Details of the multimodal foundation models evaluated in MMVU. The Source column includes URLs for proprietary models and Hugging Face model names for open-source models. The # Input Frames column, for those models only support multi-image input, represents the default number of input frames, chosen from 2, 4, 8, 16, 32, based on the maximum value that does not exceed the models context window. HF means Hugging Face. 32 Back to Appendix Table of Contents B.2 CHAIN-OF-THOUGHT AND DIRECT ANSWER PROMPTS The following figures illustrates the CoT reasoning and Direct Answer prompts applied in this study for answering multiple-choice and open-ended questions, respectively. Question:{question} A: {option a} B: {option b} C: {option c} D: {option d} E: {option e} Visual Information: {processed video} Answer the given multiple-choice question step by step. Begin by explaining your reasoning process clearly. Conclude by stating the final answer using the following format: Therefore, the final answer is: $LETTER (without quotes), where $LETTER is one of the options. Think step by step before answering. Figure 10: CoT reasoning prompt, adopted from MMMU-Pro (Yue et al., 2024b), for answering multiple-choice question. Question:{question} Visual Information: {processed video} Answer the given question step by step. Begin by explaining your reasoning process clearly. Conclude by stating the final answer using the following format: Therefore, the final answer is: Answer: $ANSWER (without quotes), where $ANSWER is the final answer of the question. Think step by step before answering. Figure 11: CoT reasoning prompt for answering open-ended question. Question:{question} A: {option a} B: {option b} C: {option c} D: {option d} E: {option e} Visual Information: {processed video} Do not generate any intermediate reasoning process. Answer directly with the option letter from the given choices. Figure 12: Direct Answer prompt, adopted from MMMU-Pro (Yue et al., 2024b), for answering multiple-choice question. 33 Back to Appendix Table of Contents Question:{question} Visual Information: {processed video} Do not generate any intermediate reasoning process. Directly output the final answer. Figure 13: Direct Answer prompt for answering open-ended question. 34 Back to Appendix Table of Contents B.3 PROMPTS FOR ACCURACY EVALUATION [Instruction] Evaluate whether the models final answer is correct by comparing it to the ground-truth answer provided for the given question. You should first extract the final answer from the models response, and then compare the extracted answer with the ground-truth answer to determine its accuracy. Output your response in the following structured format: { extracted answer: // str value B D E, should be single character correct: // boolean value, True if the answer is correct, False otherwise } [User] Question:{question} A: {option a} B: {option b} C: {option c} D: {option d} E: {option e} Ground Truth Answer: {ground truth} Model Response to the Question: {model response} Figure 14: Evaluation prompt used for assessing the accuracy of multi-choice QA. [Instruction] Evaluate whether the models final answer is correct by comparing it to the ground-truth answer provided for the given question. You should first extract the final answer from the models response, and then compare the extracted answer with the ground-truth answer to determine its accuracy. The final answer generated by the model does not need to match the ground-truth answer word-for-word. However, it should only be considered correct if it demonstrates the exact same technique or concept explicitly and unambiguously equivalent to the ground-truth answer. Output your response in the following structured format: { extracted answer: // str value, the short final answer extracted from the models response, do not hallucinate one that is not present in the response correct: // boolean value, True if the answer is correct, False otherwise } [User] Question:{question} Ground Truth Answer: {ground truth} Model Response to the Question: {model response} Figure 15: Evaluation prompt used for assessing the accuracy of open-ended QA. Back to Appendix Table of Contents"
        },
        {
            "title": "C EXPERIMENT",
            "content": "C.1 COMPARISON BETWEEN COT REASONING AND DIRECT ANSWERING Figure 16: Comparison of model performance between CoT reasoning and direct answering on the validation set. 36 Back to Appendix Table of Contents C.2 ERROR CASE ANALYSIS: VISUAL PERCEPTION ERROR Figure 17: An error case of Thermodynamics. 37 Back to Appendix Table of Contents Figure 18: An error case of Electromagnetism. 38 Back to Appendix Table of Contents Figure 19: An error case of Art. 39 Back to Appendix Table of Contents C.3 ERROR CASE ANALYSIS: MISUSE OR LACK DOMAIN KNOWLEDGE IN VISUAL PERCEPTION Figure 20: An error case of Computer Science. Back to Appendix Table of Contents Figure 21: An error case of Electrical Engineering. 41 Back to Appendix Table of Contents Figure 22: An error case of Pharmacy. Back to Appendix Table of Contents C.4 ERROR CASE ANALYSIS: MISUSE OR LACK DOMAIN KNOWLEDGE IN REASONING Figure 23: An error case of Computer Science. 43 Back to Appendix Table of Contents Figure 24: An error case of Biology. 44 Back to Appendix Table of Contents Figure 25: An error case of Chemistry. 45 Back to Appendix Table of Contents C.5 ERROR CASE ANALYSIS: HEAVY RELIANCE ON TEXTUAL INFORMATION Figure 26: An error case of Clinical Medicine. 46 Back to Appendix Table of Contents Figure 27: An error case of Management. 47 Back to Appendix Table of Contents C.6 ERROR CASE ANALYSIS: LOGICAL REASONING ERROR Figure 28: An error case of Mechanical Engineering. 48 Back to Appendix Table of Contents Figure 29: An error case of Clinical Medicine. Back to Appendix Table of Contents"
        }
    ],
    "affiliations": [
        "Yale NLP"
    ]
}