{
    "paper_title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
    "authors": [
        "Ming Hu",
        "Chenglong Ma",
        "Wei Li",
        "Wanghan Xu",
        "Jiamin Wu",
        "Jucheng Hu",
        "Tianbin Li",
        "Guohang Zhuang",
        "Jiaqi Liu",
        "Yingzhou Lu",
        "Ying Chen",
        "Chaoyang Zhang",
        "Cheng Tan",
        "Jie Ying",
        "Guocheng Wu",
        "Shujian Gao",
        "Pengcheng Chen",
        "Jiashi Lin",
        "Haitao Wu",
        "Lulu Chen",
        "Fengxiang Wang",
        "Yuanyuan Zhang",
        "Xiangyu Zhao",
        "Feilong Tang",
        "Encheng Su",
        "Junzhi Ning",
        "Xinyao Liu",
        "Ye Du",
        "Changkai Ji",
        "Cheng Tang",
        "Huihui Xu",
        "Ziyang Chen",
        "Ziyan Huang",
        "Jiyao Liu",
        "Pengfei Jiang",
        "Yizhou Wang",
        "Chen Tang",
        "Jianyu Wu",
        "Yuchen Ren",
        "Siyuan Yan",
        "Zhonghua Wang",
        "Zhongxing Xu",
        "Shiyan Su",
        "Shangquan Sun",
        "Runkai Zhao",
        "Zhisheng Zhang",
        "Yu Liu",
        "Fudi Wang",
        "Yuanfeng Ji",
        "Yanzhou Su",
        "Hongming Shan",
        "Chunmei Feng",
        "Jiahao Xu",
        "Jiangtao Yan",
        "Wenhao Tang",
        "Diping Song",
        "Lihao Liu",
        "Yanyan Huang",
        "Lequan Yu",
        "Bin Fu",
        "Shujun Wang",
        "Xiaomeng Li",
        "Xiaowei Hu",
        "Yun Gu",
        "Ben Fei",
        "Zhongying Deng",
        "Benyou Wang",
        "Yuewen Cao",
        "Minjie Shen",
        "Haodong Duan",
        "Jie Xu",
        "Yirong Chen",
        "Fang Yan",
        "Hongxia Hao",
        "Jielan Li",
        "Jiajun Du",
        "Yanbo Wang",
        "Imran Razzak",
        "Chi Zhang",
        "Lijun Wu",
        "Conghui He",
        "Zhaohui Lu",
        "Jinhai Huang",
        "Yihao Liu",
        "Fenghua Ling",
        "Yuqiang Li",
        "Aoran Wang",
        "Qihao Zheng",
        "Nanqing Dong",
        "Tianfan Fu",
        "Dongzhan Zhou",
        "Yan Lu",
        "Wenlong Zhang",
        "Jin Ye",
        "Jianfei Cai",
        "Wanli Ouyang",
        "Yu Qiao",
        "Zongyuan Ge",
        "Shixiang Tang",
        "Junjun He",
        "Chunfeng Song",
        "Lei Bai",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 4 1 1 2 . 8 0 5 2 : r Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers Ming Hu1,2 Chenglong Ma1,3 Wei Li1,4 Wanghan Xu1,4 Guohang Zhuang Jiaqi Liu1,7 Yingzhou Lu8 Ying Chen1 Chaoyang Zhang1 Cheng Tan1 Jucheng Hu1,6 Tianbin Li1 Jie Ying1 Jiamin Wu1,5 Guocheng Wu1 Shujian Gao1 Pengcheng Chen1 Jiashi Lin1 Haitao Wu1 Lulu Chen9 Fengxiang Wang1 Yuanyuan Zhang10 Xiangyu Zhao1 Feilong Tang1,2 Encheng Su Junzhi Ning1 Xinyao Liu1 Ye Du1 Changkai Ji1 Cheng Tang1 Huihui Xu1 Ziyang Chen1 Ziyan Huang1 Yizhou Wang1 Chen Tang1 Shiyan Su2 Shangquan Sun1 Runkai Zhao1 Zhisheng Zhang12 Yu Liu13 Fudi Wang14 Yuanfeng Ji8 Jianyu Wu1 Yuchen Ren1 Siyuan Yan2 Zhonghua Wang2 Zhongxing Xu2 Jiyao Liu1,3 Pengfei Jiang1 Yanzhou Su15 Hongming Shan3 Chunmei Feng16 Jiahao Xu Jiangtao Yan1 Wenhao Tang1 Diping Song1 Lihao Liu1 Yanyan Huang11 Lequan Yu11 Bin Fu1 Shujun Wang17 Xiaomeng Li18 Xiaowei Hu19 Yun Gu4 Ben Fei5 Zhongying Deng20 Benyou Wang21 Yuewen Cao1 Minjie Shen9 Haodong Duan1 Jie Xu1 Yirong Chen1 Fang Yan1 Hongxia Hao1 Jielan Li1 Jiajun Du22 Yanbo Wang Imran Razzak24 Chi Zhang1 Lijun Wu1 Conghui He1 Zhaohui Lu4 Jinhai Huang3 Yihao Liu1 Fenghua Ling1 Yuqiang Li1 Aoran Wang1 Qihao Zheng1 Nanqing Dong1 Tianfan Fu25,1 Dongzhan Zhou1 Yan Lu1 Wenlong Zhang1 Jin Ye1,2 Zongyuan Ge2 Shixiang Tang1,5 Junjun He1 Jianfei Cai2 Wanli Ouyang1,5 Yu Qiao1 Lei Bai1 Bowen Zhou1 Chunfeng Song1 1Shanghai Artificial Intelligence Laboratory 2Monash University 3Fudan University 4Shanghai Jiao Tong University 5The Chinese University of Hong Kong 6University College London 7UNC-Chapel Hill 8Stanford University 9Virginia Tech 10Purdue University 11The University of Hong Kong 12China Pharmaceutical University 13Beijing Institute of Heart, Lung and Blood Vessel Diseases 14Chinese Academy of Sciences 15Fuzhou University 16University College Dublin 17The Hong Kong Polytechnic University 18The Hong Kong University of Science and Technology 19South China University of Technology 20University of Cambridge 21The Chinese University of Hong Kong, Shenzhen 22Caltech 23North University of China 24MBZUAI 25Nanjing University https://github.com/open-sciencelab/Awesome-Scientific-Datasets-and-LLMs AbstractScientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents comprehensive, datacentric synthesis that reframes the development of Sci-LLMs as co-evolution between models and their underlying data substrate. We formulate unified taxonomy of scientific data and hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demandsheterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace shift from static exams toward processand discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to living, evolving knowledge base. Collectively, this work provides roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as true partner in accelerating scientific discovery. Index TermsLarge Language Model; AI for Science; Scientific Data; Data4LLM Corresponding Author, Project Leader, Scientific Director"
        },
        {
            "title": "CONTENTS",
            "content": "Background II-A II"
        },
        {
            "title": "III",
            "content": "IV II-B II-C II-D II-E III-D IV-B IV-C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Taxonomy of Scientific Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Textual Formats . . II-A1 Visual Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-A2 . Symbolic Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-A3 Structured Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-A4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Time-Series Data II-A5 II-A6 Multi-omics Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Hierarchical Structure of Scientific Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-B1 Factual Level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-B2 Theoretical Level Methodological and Technological Level . . . . . . . . . . . . . . . . . . . . . . . . . . II-B3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Modeling and Simulation Level II-B4 Insight Level . II-B5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dynamic Interactions and Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-B6 Implications for Sci-LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-B7 Key Challenges in Scientific AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Interpretability in Scientific AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-C1 Cross-scale and Multimodal Integration . . . . . . . . . . . . . . . . . . . . . . . . . . II-C2 II-C3 Dynamic Knowledge Evolvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Quality Standards for Scientific Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-D1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-D2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-D3 II-D4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dimensions for Evaluating Scientific AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Expert-Level Scientific Knowledge Comprehension and Retrieval II-E1 . . . . . . . . . . . . Scientific Reasoning and Problem Solving . . . . . . . . . . . . . . . . . . . . . . . . . II-E2 Multimodal Scientific Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . II-E3 Accuracy . . Completeness . Timeliness . Traceability . . . . . . . . . . . . . . . . . . . . . Scientific Large Language Models III-A III-B III-C . Introduction of Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . General-purpose Sci-LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Domain-specific Sci-LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Physics . III-C1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Chemistry . III-C2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Materials Science . III-C3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Life Sciences III-C4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Astronomy . III-C5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Earth Science . III-C6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sci-LLMs Analysis . . . . . . . . . . . . . . . . . . . . . Scientific Data for Pre-training IV-A . . . . . . . . . . . . Physics . Chemistry . . Materials Science . Physics, Chemistry and Material Sciences: the Foundation for Understanding the Material World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . IV-A1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . IV-A2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . IV-A3 Life Sciences: Complexity from Molecules to Systems . . . . . . . . . . . . . . . . . . . . . . . Molecular and Cell Biology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . IV-B1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Multi-Omics . IV-B2 Neuroscience . IV-B3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Healthcare and Medical Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . IV-B4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Agriculture . IV-B5 Astronomy and Earth Science: Understanding Our Planet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . IV-C1 Astronomy . . . . . . . . . . . . . . . . . 5 8 8 8 9 11 12 13 13 15 15 16 16 17 17 17 18 18 18 18 19 19 19 19 19 20 20 20 20 20 21 21 23 23 24 24 25 28 28 29 30 30 31 31 32 32 32 33 33 33 34 34 34 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI"
        },
        {
            "title": "VII",
            "content": "VI-B VI-C VI-D VII-B VII-C IV-D IV-C2 Earth Science . Pre-training Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scientific Data for Post-training V-A . . . . . . . Current Landscape Across Scientific Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . V-A1 V-A2 V-A3 . V-A4 . V-A5 V-A6 . Post-training Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Physics . Chemistry . . Materials Science . . . Life Sciences . . Astronomy . . Earth Science . . . . . . . . . . V-B Evaluation of Sci-LLMs VI-A . . . . . . . . . . . . . . . . . Physics . . Chemistry . . Materials Science . . . . Life Sciences . . Astronomy . . . Earth Science . . . General Science . . . . Current Landscape Across Scientific Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-A1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-A2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-A3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-A4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-A5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-A6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-A7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation Data Analysis . Tiered Regime in Data Generation and Annotation . . . . . . . . . . . . . . . . . . . . VI-B1 Skewed Knowledge Level with Increasing Difficulty . . . . . . . . . . . . . . . . . . . VI-B2 Shift towards Domain-Specific Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . VI-B3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LLM / Agent as Judge . Inspiration from Test-Time Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scientific Data Development VII-A Data Collection and Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Source Heterogeneity and Acquisition Strategies VII-A1 Annotation Methodologies and Quality Control . . . . . . . . . . . . . . . . . . . . . . VII-A2 VII-A3 Cross-Domain Patterns and Domain-Specific Considerations . . . . . . . . . . . . . . . Limitations of Current Scientific Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scarcity of Experimental Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VII-B1 Over-reliance on Text Modality Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . VII-B2 VII-B3 Representation Gap between Static Knowledge and Dynamic Processes . . . . . . . . . VII-B4 Multi-level Biases in Scientific Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . Systematic Issues in Data Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Traceability Crisis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VII-C1 Scientific Data Latency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VII-C2 The Lack of AI-readiness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VII-C3 VIII New Paradigms for Data-Driven Sci-LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VIII-A Scientific Agent VIII-A1 LLMs as Scientific Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VIII-A2 Multi-Agent Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VIII-A3 Tool Use . . Self-evolving Agents VIII-A4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VIII-A5 Evaluation Frameworks and Benchmarking . . . . . . . . . . . . . . . . . . . . . . . . VIII-A6 Autonomous Scientific Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Data Bottleneck Behind the Rise of Scientific Agents . . . . . . . . . . . . . . . . VIII-B1 VIII-B2 Building an Operating System-level Interaction Protocol . . . . . . . . . . . . . . . . . VIII-B3 Design Principles for Next-Generation Scientific Data Architecture . . . . . . . . . . . VIII-B4 Sustainable Data Sharing Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . VIII-B5 Data Safety and Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VIII-B Data Ecosystems for Sci-LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 35 35 36 36 36 36 36 37 37 38 39 39 39 39 39 40 40 40 41 41 42 42 43 43 44 44 45 45 46 46 46 47 47 47 47 48 48 48 48 48 48 49 49 49 50 50 50 51 51 52 52 52 3 IX Challenges and Outlook . IX-A . IX-B . . . . . . . . . . Challenges . IX-A1 IX-A2 IX-A3 IX-A4 Future Work . IX-B1 IX-B2 IX-B3 IX-B4 IX-B5 IX-B6 IX-B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scientific Data Selection for Efficient Pretraining . . . . . . . . . . . . . . . . . . . . . Optimizing Data Processing Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . Representing Non-Sequential and Non-Textual Data . . . . . . . . . . . . . . . . . . . LLM Knowledge Update and Version Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Integrated Scientific Data Ecosystems . . . . . . . . . . . . . . . . . . . . . . . . . . . Automated Scientific Data Standardization Pipeline . . . . . . . . . . . . . . . . . . . . Comprehensive Evaluation System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Advanced Scientific Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Autonomous Scientific Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . From Sci-LLMs to Scientific Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . Ethical Governance for Responsible Scientific AI Innovation . . . . . . . . . . . . . . . . . . . . . . . . X"
        },
        {
            "title": "References",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 53 53 53 53 53 53 53 54 54 54 54 54 54 54 4 I. INTRODUCTION Science is built up with facts, as house is with stones. But collection of facts is no more science than heap of stones is house. Henri Poincare The rapid advancement of large language models (LLMs) has sparked paradigm shift across numerous domains, demonstrating unprecedented transformative potential through task automation, productivity enhancement, and breakthrough innovations [1][5]  (Fig. 1)  . These models have fundamentally transformed scientific research by introducing unified approach that replaces traditional task-specific methods, extending beyond natural language processing to encompass diverse scientific data types, including molecules [6], proteins [7], tables [8], and complex metadata. LLMs have already revolutionized fields such as software engineering [2], [9], [10], law [11], [12], materials science [13], [14], healthcare [15] [17], and biomedical research [18], and have been applied across disciplines from mathematics [19] and physics to chemistry [20], biology [21], and geoscience [22]. The evolution of scientific LLMs (Sci-LLMs) has undergone through four distinct data-driven phases paradigm shift transfer learning from 2018 to 2025  (Fig. 2)  . The initial phase (20182020) witnessed domain-specific adaptations of BERT [23] architecture, with models like SciBERT [24], BioBERT [25], and PubMedBERT [26] trained on largescale scientific corpora, showing that continued pre-training on domain literature yields sizable gains in downstream tasks that require scientific text understanding. These models provided reliable, static concept representations for specific downstream uses, but struggled to synthesize or generate novel scientific content at scale. The subsequent scaling phase (20202022) embraced parameter and token-count expansion, marking critical transition. Models like GPT-3 [27] with 175 billion parameters, along with later data/compute-optimal training rules [28], [29] demonstrated that massive parameter scaling with diverse training data could achieve emergent knowledge integration capabilities, fundamentally altering the landscape of scientific AI. Galactica [30] extended this lesson to science, with 120 billion parameters trained on more than 48 million scientific papers, textbooks, and encyclopedias, designing specialized tokenization schemes for mathematical formulas, chemical structures, and citations. MedPaLM-2 [31], further instruction-tuned on multiple medical-domain datasets and achieved over 85% accuracy on USMLE-style questions, becoming the first AI system to exhibit expert-level medical reasoning capabilities comparable to those of licensed physicians. However, scaling ran into data wall for Sci-LLMs: unlike general-domain crawls with hundreds of billions to trillions of tokens, high-quality scientific text corpora were orders of magnitude smaller, with abundant scientific raw data underutilized in early large-scale attempts. The instruction-following phase (20222024) shifted focus from capacity to alignment, introducing task adaptation via reinforcement learning from human feedback (RLHF). Examples include InstructGPT [32] and ChatGPT [33], enabling more precise scientific task execution. Subsequently, foundational architectures represented by open-source LLMs (e.g., LLaMA [34], Qwen [35], ChatGLM [36], and Mistral [37]) have enabled unprecedented diversity in scientific applications. Concurrently, the unprecedented expansion of instruction datasets has given rise to series of milestone Sci-LLMs. Specifically, in the biomedical field, Meditron [38], pre-trained on 48.1 billion tokens from medical literature, demonstrates the potential of open-source models in professional medical reasoning. ProteinChat [39], trained on 1.5 million proteinprompt-answer triplets, facilitates protein research; LLaMAGene [40] integrates gigabytes of DNA, protein, and text data and 500 millions of instruction examples in DNA/protein tasks for training, achieving cross-modal biological sequence understanding. The multidisciplinary model SciGLM [41] leverages the efficient architecture of ChatGLM, fine-tuned on 254,000 carefully constructed instruction examples, achieving cross-disciplinary knowledge integration capabilities. Notably, several works demonstrate strong correlation between data scale and model performance: HuatuoGPT-II [42] utilizes an 11 TB medical corpus with million-scale documents for pretraining, while NatureLM [43] is pre-trained on 143 billion tokens and fine-tuned using 45.1 million instruction-response pairs. This dual-drive paradigm of architectural diversity + data scaling has become the core framework for current scientific large language model development. Beyond excelling at analyzing existing scientific data, these models demonstrate remarkable potential in accelerating scientific discovery via hypothesis generation, theorem proving, experiment design, drug discovery, and weather forecasting, fundamentally reshaping how complex challenges are approached and solved in the era of AI-driven research [44] [46]. As prominent example of this trend, Intern-S1 [47] is scientific multimodal Mixture-of-Experts (MoE) [48] foundation model with general understanding and reasoning capabilities alongside specialized expertise in scientific data analysis. Continually pre-trained on massive scientific data with 2.5 trillion tokens and enhanced with Mixture-ofRewards reinforcement learning, it surpasses existing closedsource state-of-the-art models in professional tasks such as molecular synthesis, reaction condition prediction, and crystalline thermodynamic stability prediction, while maintaining leading performance on general reasoning tasks. The latest paradigm of agentic science (2023now) is enabling AI systems with scientific agency, able to plan, act, and iterate across stages of discovery. Many works demonstrate end-to-end scientific workflows [44], [49], with increasing focus on multi-agent [50], [51] and tool ecosystems [18], [52]. Multi-agent designs emulate laboratory hierarchies from principal investigators to domain specialists, coordinating through formalized meeting protocols and critiqueiteration loops [53], [54]. Such systems generate scientific ideas with improved novelty and feasibility by explicitly modeling research teamwork [55] and scientific law constraints [56]. At scale, cooperative frameworks manage entire research lifecycles (problem scoping, manuscript drafting, etc.), preserving persistent artifacts and audit trails [57], while embodied variants integrate robotic execution with adaptive planning [58]. Parallel 5 Fig. 1: Cumulative trend of publications on major preprint platforms whose titles or abstracts mention the keyword language model or the combination language model + scientific domain (e.g., chemistry, physics, multi-omics, medicine, etc.). Left: Results from January 2018 to August 2025, from arXiv and PubMed. For arXiv, the matching includes language model in combination with additional science-related keywords; PubMed results are limited to occurrences in titles and abstracts. Both platforms show rapid growth. Right: Results from 2020 to August 2025, from bioRxiv, medRxiv, and ChemRxiv, all based on direct matches of language model in titles and abstracts. While the overall volumes are smaller than arXiv and PubMed, all three platforms, especially bioRxiv, show rapid acceleration, reflecting growing interdisciplinary interest in large language models across biomedical, chemical, and computational sciences. Fig. 2: Evolution of Sci-LLMs reveals four paradigm shifts from 2018 to 2025, including (1) the progression from transfer learning approaches, (2) through the scaling era marked by knowledge integration in larger models, (3) instruction-following capabilities enabling flexible task adaptation, to (4) the latest paradigm introduces scientific agentsAI systems capable of autonomously conducting scientific research, from hypothesis generation and experimental design to data analysis and discovery. Note: Model positions reflect their release dates (x-axis) rather than strict paradigm classification. The four paradigms represent evolving trends in Sci-LLM development with overlaps and continuities, not mutually exclusive categories. advances in tool integration center on knowledge-graphdriven orchestration [59] and domain-scale agents interfacing with hundreds of software tools, databases, and instruments with provenance tracking [18]. Despite these promising results, Sci-LLMs encounter fundamental challenges stemming from the unique characteristics of scientific data and knowledge representation. Unlike the relatively homogeneous text corpora for general-purpose LLM development, scientific datasets exhibit extreme heterogeneity across modalities and formats. For instance, in 6 chemistry alone, models must reconcile molecular strings, 3D molecular coordinates, spectroscopic data, and reaction mechanisms, each requiring distinct processing strategies [60]. This heterogeneity extends beyond chemistry to encompass the full spectrum of scientific disciplines. In life sciences, models must simultaneously process genomic sequences, protein structures, multi-omics data, and clinical imaging [61] [63], while astronomical applications demand integration of time-series photometry, spectroscopic observations, and multiwavelength imaging across vastly different spatial and temporal scales [64], [65]. The challenge is further compounded by the hierarchical nature of scientific knowledge itself, which spans from raw observational data to abstract theoretical frameworks, each with its own representational requirements [66], [67]. Moreover, scientific data often embodies domain-specific semantics that resist straightforward tokenization or embedding. Mathematical equations carry precise symbolic relationships that must be preserved during processing [68], [69], while crystallographic information files encode 3D structural constraints essential for materials science applications [70], [71]. Time-series data from instruments like Laser Interferometer Gravitational-Wave Observatory (LIGO) contain subtle signals buried in noise, requiring specialized preprocessing for physical interpretability [65], [72]. These diverse data types cannot be adequately represented through conventional text-based approaches, necessitating novel architectures that preserve domain-specific invariance while enabling cross-modal reasoning [73][75]. The integration of such heterogeneous data sources poses additional computational and methodological challenges. Crossscale modeling, from quantum mechanical calculations to macroscopic phenomena, demands architectures capable of capturing multi-resolution dependencies [76]. Furthermore, the uncertainty in experimental measurements require models to propagate error bounds and maintain scientific rigor throughout the reasoning process [77][79]. These constraints fundamentally distinguish scientific AI from general-purpose language modeling, requiring specialized solutions that respect the unique epistemological foundations of scientific inquiry. The inherent complexity of scientific data and reasoning naturally extends to the evaluation of Sci-LLMs, where conventional natural language processing benchmarks prove insufficient for capturing domain-specific competencies. Recent efforts have produced comprehensive evaluation suites such as ScienceQA [80], which tests multimodal scientific understanding across elementary to graduate levels, and MMLU-Pro [81], which includes rigorous assessments in specialized fields like quantum physics and molecular biology. However, these benchmarks often fail to capture the nuanced requirements of scientific discovery, e.g., the ability to generate novel hypotheses, identify non-obvious connections between disparate findings, or design experiments that test theoretical predictions. To address this gap, Liu et al. propose ResearchBench [82], large-scale scientific discovery benchmark spanning 12 disciplines to systemically evaluate the hypothesis generation capabilities of LLMs. Furthermore, researchers have also begun developing process-oriented evaluations that assess intermediate reasoning steps rather than just final answers, exemplified by Fig. 3: Six main scientific domains covered in this survey. The figure illustrates the primary disciplines investigated in our study on science-oriented large language models, encompassing Chemistry, Materials Science, Physics, Life Sciences, Astronomy, and Earth Science, along with representative subfields within each domain. frameworks like ScienceAgentBench [83] that evaluate models on complex scientific workflows, including literature review, experimental design, and result interpretation. Benchmarks such as MultiAgentBench [84] and WorkflowBench [85] now quantify collaboration, coordination, and workflow synthesis skills, marking shift toward measurable, safety-aware, and reproducible science automation. The community has also recognized that scientific validity requires more than linguistic fluency; models must respect fundamental constraints such as physical laws, chemical valence rules, and biological feasibility [21], [86], [87]. This has led to the integration of symbolic reasoning modules and constraint satisfaction systems that act as guardrails during generation, ensuring that model outputs remain within scientifically plausible bounds while still allowing for creative exploration at the frontiers of knowledge. To address these gaps, several survey papers look into adjacent facets of the problem. few works [88], [89] focused on models and tasks for biomedical data; Zhang et al. [21] examined Sci-LLMs under broader perspective that involves both biological and chemical domains. Other works [60] explored the application of Sci-LLMs in scientific discovery. Wei et al. [90] and Wang et al. [91] reviewed scientific agent paradigms and system designs for autonomous research and scientific discovery. Ni et al. [92] conducted survey on existing benchmarks for LLMs involving several science fields. However, these reviews are theme-specific and limited to models with only cursory touch on the underlying substratescientific datasets, throughout pre-training, posttraining and evaluation. Complementing these perspectives, our survey contributes unified, cross-disciplinary synthesis that explicitly links data foundations to agent frontiers. We summarize the contributions as follows: By introducing unified taxonomy of scientific data and hierarchical model of scientific knowledge, we provide novel epistemological framework for analyzing the challenges in representing scientific information, from raw observational data and symbolic notations to abstract theoretical insights. We deliver comprehensive and structured account of the rapidly evolving landscape of scientific large language models across six main scientific domains (i.e., physics, chemistry, life sciences, Earth Science, astronomy, and materials science; as in Fig. 3). By systematically analyzing over 270 preand posttraining datasets, we provide comprehensive panorama of current scientific datasets for Sci-LLM development, distilling the multimodal, cross-scale, and domainspecific challenges that distinguish Sci-LLMs from their general-purpose counterpart. We conduct comprehensive review of over 190 evaluation datasets for Sci-LLMs, discussing the shift of evaluation from static exams to research-level scientific discovery, the increasing employment and combination of domain-specific metrics, and the emergence of advanced evaluation methodologies. We identify structural failures in scientific data curation and translate them into forward-looking data development agenda that supports advanced scientific intelligence, advocating for closed-loop feedback between autonomous scientific discovery and scientific data infrastructure. Collectively, these contributions establish consolidated reference and clear roadmap for building trustworthy, continually evolving Sci-LLMs capable of accelerating data-driven scientific discovery. The paper is organized as follows: Sec. II formulates unified taxonomy of scientific data grounded in hierarchical model of scientific knowledge. Sec. III shows the landscape of Sci-LLMs across six main scientific domains. Secs IV, V, and VI provide an extensive catalog and analysis of existing pre-training, post-training, and evaluation datasets for SciLLMs. Sec. VII analyzes how scientific data shapes LLM development and identify systemic issues that impede AIreadable corpora. Sec. VIII outlines forward directions for scientific discovery empowered by advanced scientific agents and data ecosystems. Secs. IX and summarize challenges, outlook, and conclusion distilled from the paper. II. BACKGROUND This section provides the foundations for understanding scientific AI systems. We first examine the diverse taxonomy of scientific data across disciplines (Sec. II-A), followed by an analysis of the hierarchical structure of scientific knowledge (Sec. II-B), which reveals that scientific understanding forms sophisticated multilevel system rather than simple information repository. Then, we identify critical challenges unique to scientific AI (Sec. II-C), including knowledge consistency, interpretability, and the integration of cross-scale multimodal data. We conclude by establishing frameworks for evaluating both data quality standards (Sec. II-D) and AI system capabilities specific to scientific domains (Sec. II-E). These elements collectively define the requirements for AI systems designed to support rigorous scientific discovery and reasoning. A. Taxonomy of Scientific Data Scientific data manifests in striking diversity across disciplines, shaped by the fundamental questions and methodological paradigms unique to each field. In this subsection, we review and summarize the primary data types and modalities across scientific domains, examining how they appear and function within different scientific contexts, including: textual formats (papers, experimental reports) in Sec. II-A1, visual data (medical scans, astronomical observations) in Sec. II-A2, symbolic representations (formulas, chemical structures) in Sec. II-A3, structured data (databases, knowledge graphs) in Sec. II-A4, and time-series data (neurophysiological recordings, astronomical light curves) in Sec. II-A5. In addition to these general types, we also discuss multi-omics integration in Sec. II-A6 as special case, as it represents an emerging paradigm that requires combining heterogeneous data across multiple biological transcriptomics, proteomics). This taxonomy sets the stage for understanding how scientific data collectively support AI-driven scientific discovery across domains, and also establishes the foundation for developing multimodal large language models (MLLMs) which aim to process and integrate heterogeneous scientific data within unified framework. layers (e.g., genomics, 1) Textual Formats: Scientific textual data forms the foundational substrate for knowledge representation across disciplines, encompassing rich hierarchy from primary experimental documentation to synthesized knowledge repositories. At the most granular level, laboratory notebooks, experimental protocols, and field observations capture the raw process of scientific discovery, documenting not only successful experiments but also failed attempts and methodological refinements that prove invaluable for reproducibility and knowledge transfer [93]. This primary documentation feeds into specialized databases and repositories that have become central to modern scientific practice: genomic sequences in GenBank [94], protein structures in RCSB [95], chemical compounds in PubChem [96], [97], and astronomical observations in NASAs Astrophysics Data System (ADS) [98], collectively housing petabytes of structured information linked to their textual descriptions and metadata. The scholarly communication layer builds upon this foundation through peer-reviewed journals, comprehensive textbooks, and increasingly, preprint repositories that accelerate knowledge dissemination. Traditional venues like Physical Review Letters, The Astrophysical Journal, and Monthly Notices of the Royal Astronomical Society maintain rigorous standards while platforms such as arXiv [99] and ChemRxiv [100] enable rapid sharing of emerging findings across physics, astronomy, chemistry, and interdisciplinary domains. This academic corpus is complemented by educational resources ranging from 8 Fig. 4: Examples of visual data across typical medical imaging modalities, involving radiology (PET, CT, mammography, X-ray, MRI, and ultrasound), dermatology, ophthalmology (CFP, FFA, UWF-SLO, and OCT), endoscopy, histopathology, and cellular microscopy. The figure is sourced from open-source medical datasets. open-access textbooks like OpenStax series [101], [102] and The Feynman Lectures [103] to specialized training materials including agricultural extension question-answering (QA) records [104], examination questions, and curated datasets for AI model evaluation such as ScholarChemQA [105], ScienceQA [106], and materials science benchmarks [107] [109]. Beyond traditional academic outputs, scientific textual data increasingly encompasses regulatory documentation, real-time observational streams, and computational artifacts that reflect the evolving nature of modern research. Clinical trial registries [110], institutional review protocols [111], and biosafety guidelines [112] ensure responsible research conduct, while electronic health records [113], [114], citizen science annotations from projects like Galaxy Zoo [115], and realtime environmental monitoring data [116] bridge laboratory findings with societal applications. The integration of computational approaches has spawned new textual categories, including bioinformatics pipelines [117], systems biology models [118], synthesis planning frameworks [119], and code generation benchmarks [120], [121], all requiring extensive documentation for reproducibility. This diverse textual ecosystem not only archives scientific progress but enables metaanalyses [122], knowledge synthesis efforts, and increasingly sophisticated AI-driven discovery across the full spectrum of scientific inquiry. 2) Visual Data: Visual data in scientific domains broadly fall into two categories: instrumental imaging that directly captures physical subjects through various sensing technologies, and diagrammatic representations that abstract and visualize concepts, relationships, and analytical results. These visual data span an extraordinary range of scales and modalities, from sub-atomic particle interactions to cosmic structures, providing essential foundations for multimodal AI systems to understand scientific phenomena. At the smallest scales, as shown in Fig. 5, advanced microscopy techniques, including scanning and transmission electron microscopy (SEM/TEM) [131], [132], atomic force microscopy (AFM) [133], and scanning tunneling microscopy (STM) [134], reveal atomic structures and molecular arrangements critical for physics, materials science and chemistry. [125]; STM of Si Fig. 5: Examples of visual data in physics. SEM of epoxy with/without AlN [123]; TEM of W-doped CuPt nanoalloys [124]; AFM topography of hyper-stoichiometric (111)-(77) at multiple scan UO2 sizes [126]; UV/Vis contour map (500680 nm) [127]; Infrared thermographs of directional emitter [128]; Raman helicity-resolved maps of 1T-TaS2 [129]; NMR of yttrium hydrides [130]. All panels are reused or adapted under the stated licenses (CC-BY-4.0 or CC-BY), with minor cropping only. Visual spectrum data, including ultraviolet-visible spectrophotometry (UV/Vis) [135], infrared [136], Raman [137], and nuclear magnetic resonance (NMR) [138] spectroscopy, serve as molecular fingerprints across chemistry, materials science, and physics, with visual representations proven effective for spectrum learning [139], [140]. In life sciences, light microscopy (brightfield, confocal) and fluorescence microscopy capture cellular structures and protein localizations, with datasets like the Human Protein Atlas [141] and Broad Bioimage Benchmark Collection [142] supporting cell segmentation and phenotype classification tasks. These microscopy images, typically stored in formats like TIFF [143] or ND2 [144], have been increasingly leveraged for training visual-language models [145], [146]. Moving up in scale, whole-slide digital pathology produces gigapixel images stored in SVS format, essential for cancer diagnosis, with large cohorts like TCGA [147] and CPTAC [148] providing thousands of images paired with diagnostic reports [149][151]. At tissue and organ scales, radiological imaging en9 Fig. 6: Data from Earth sciences six major domains, including the lithosphere, anthroposphere, biosphere, cryosphere, hydrosphere, and atmosphere. Each panel consists of geospatial data, maps, satellite imagery, charts, etc. These data sources are highly diverse, encompassing wide range of spatial and temporal resolutions, as detailed in Sec. II-B1. The figure is sourced from MSEarth [152], and authorization for its use has been obtained from the original author. mology [176] and optical coherence tomography (OCT) [177], [178], dermatology for skin lesion analysis [179], [180] ophthalmic surgical microscopy for high-resolution intraoperative visualization in ophthalmic procedures [181][184], and endoscopy for surgical guidance [185][187]. These visual data, once paired with their descriptions and reports, hold great potential in developing healthcare MLLMs; visualization examples are shown in Fig. 4. At macroscopic scales, natural photographs capture biodiversity through datasets like iNaturalist [188], while agricultural visual data span from micro-level plant imaging to macrolevel UAV and satellite imagery for crop monitoring [189] [191]. Earth science leverages satellite remote sensing [192], [193] and atmospheric datasets [194], [195] for climate modeling and environmental monitoring. As shown in Fig. 6, due to the diversity of their collection sources, earth observation data exhibit significant variability. For instance, some data are obtained from ground-based observation stations, offering long-term and continuous records at specific locations. Other datasets are derived from multispectral remote sensing technologies, which provide comprehensive information on surface and atmospheric characteristics across larger spatial scales. Additionally, reanalysis data [194] integrate observational records with numerical models, resulting in meteorological and environmental parameters with enhanced temporal and spatial consistency. These various types of data each possess unique features in terms of spatial coverage, temporal resolution, and observational content, offering multi-dimensional information foundation for research in earth system science. Beyond Earth, astronomical observations across the radio Fig. 7: Examples of astronomical data, demonstrating the application of radio signals, optical signals, and infrared signals in imaging different astronomical objects. The image is sourced from NASA. imaging (MRI) including X-rays compasses multiple modalities [153], [154], computed tomography (CT) [155][157], histopathol- [159], resonance ogy [158], magnetic [160], ultrasound [161], [162], positron emission tomography (PET) [163], [164], and mammography [165], each revealing different aspects of internal anatomy and function. These images, commonly stored in DICOM [166] or NIfTI [167] formats with rich metadata, can be processed using specialized viewers like RadiAnt [168] and MRIcroGL [169] or programmatic libraries such as pydicom [170] and SimpleITK [171]. Clinical imaging extends to specialized domains like ophthalmology with color fundus photography (CFP) [172] [174], fundus fluorescein angiography (FFA) [175], ophthalinterferometry [196] to optical [64], [197] and infrared [198], capture celestial phenomena, complemented by spectroscopic data from instruments like Large sky Area Multi-Object fiber Spectroscopic Telescope (LAMOST) [199] that reveal chemical compositions and stellar dynamics, as illustrated in Fig. 7. Complementing direct imaging, diagrammatic figures and spectroscopic visualizations provide crucial abstractions of scientific knowledge that cannot be captured through photography alone. Molecular structure diagrams, increasingly recognized as natural interfaces for chemical AI systems [200], have been curated into large-scale datasets for tasks ranging from image captioning to property prediction [96], [201], [202]. Schematic diagrams and conceptual illustrations from scientific literature [203][206] distill complex processes and experimental setups into accessible forms, essential for both human understanding and AI interpretation. These diverse visual modalities from atomic-resolution microscopy to cosmic surveys, and from molecular diagrams to climate visualizations, collectively form rich multimodal foundation for scientific AI systems. The integration of these varied visual elements into comprehensive datasets like MaCBench [207] and MMSci [75] enables models to synthesize knowledge across disciplines, though challenges remain in aligning dense visual information with semantic textual descriptions, particularly for complex phenomena in molecular biology, materials science, and mathematical physics that require advanced multimodal learning techniques. 3) Symbolic Representations: Symbolic representations constitute fundamental data modality in scientific computing, providing abstract, non-numeric encodings of scientific entities, relationships, and laws that are both humaninterpretable and machine-processable. These representations include molecular structures encoded as string notations, such as Simplified Molecular-Input Line-Entry System (SMILES) strings [208], International Chemical Identifier (InChI) codes [209], Self-Referencing Embedded Strings (SELFIES) [210]), Crystallographic Information Files (CIF) for material structures, and parameterized equations for physics and Earth system modeling. The significance of symbolic data lies in its ability to encode complex scientific knowledge in compact, manipulable forms that preserve semantic meaning while enabling automated reasoning, transformation, and discovery operations critical for modern scientific computing. The most prevalent symbolic representations in chemistry and materials science are string-based molecular encodings, with SMILES [208] being the de facto standard since the 1980s. SMILES is specification in the form of line notation for describing the structure of chemical species using short ASCII strings, encoding molecular structures using ASCII strings with specific rules: atoms are represented by their chemical element symbols (often with brackets omitted), bonds by symbols including - (single), = (double), # (triple), : (aromatic), rings by breaking cycles and adding matching numbers (e.g., O1CCOCC1 for 1,4-Dioxane), aromatic rings using lowercase letters or alternating bonds (e.g., c1ccccc1 for benzene), and branches using parentheses (e.g., CCC(=O)O for propionic acid). An extension of SMILES for polymers is BigSMILES [211], which represents polymers as stochastic objects with monomers enclosed in curly brackets, as illustrated in Fig. 8. However, SMILES suffers from syntactic fragilitysmall perturbations can render strings invalid. To address this, SELFIES (SELF-referencing Embedded Strings) [212] was introduced in 2020, guaranteeing 100% validity through formal grammar rules. SELFIES uses vocabulary of tokens like [C], [=O], [Branch], [Ring] with localized markers for branches and rings, enabling robust left-to-right parsing that gracefully handles errors. Fig. 9 shows examples of Formaldehyde and Phenols molecular graphs and corresponding SMILES and SELFIES strings. The difference between SMILES, BigSMILES, and SELFIES is demonstrated in Table I. Beyond strings, molecular graphs provide more intuitive representations where nodes correspond to atoms and edges to bonds, with adjacency matrices encoding connectivity and bond types [213]. Recent benchmark [214] reveals that SMILES remains most expressive for molecular optimization tasks, while SELFIES often underperforms due to redundancy. For crystalline materials, the CIF format serves as the standard, encoding unit cell parameters (lattice constants a, b, c, angles α, β, γ), atomic positions in fractional coordinates, space group symmetries, and experimental metadata in structured key-value format readable by tools like pymatgen and VESTA. These representations underpin major databases including ZINC [215], ChEMBL [216], USPTO [217], ICSD, and the Materials Project [70], as well as benchmarks like MoleculeNet [218] and MatBench [71]. In physics and astronomy, symbolic representations extend beyond structural encodings to encompass mathematical expressions, differential equations, and theoretical frameworks that enable automated scientific discovery. At the core are algebraic equations, differential/integral forms, and probability distributions, with recent work demonstrating that LLMs performing symbolic derivation, i.e., keeping variables symbolic before late-stage numerical substitution, tend to achieve higher accuracy on physics problem solving compared with numericfirst approaches [68]. Equation graphs represent variables and operators as nodes, enabling graph-based symbolic regression; for instance, graph networks trained on force-law data successfully recover Newtons law through messagepassing outputs [219]. Building on this foundation, LLMpowered methods like Dual Reasoning Symbolic Regression integrate language model reasoning with reflective optimization for equation extraction [69]. In astronomy, systems like PhyE2E [220] demonstrate end-to-end neural symbolic regression, generating dimensionally consistent formulas from diverse sources including NASAs THEMIS mission data [221], AI Feynman datasets [222], [223], and solar observation data (SILSO) [224]. Similarly, Earth science employ symbolic representations through mathematical formula fitting and regression for modeling complex phenomena governed by partially understood physics, such as the Navier-Stokes equations [225] in atmospheric motion, wave equations in seismology [226], and shallow-water equations in oceanography [227]. These models utilize parameterization schemes and regression analysis (least squares, Bayesian inference) to align theoretical predictions with observational data, demonstrating how sym11 TABLE I: Comparison of SMILES, BigSMILES, and SELFIES representations."
        },
        {
            "title": "Feature",
            "content": "SMILES [208] BigSMILES [211] SELFIES [212]"
        },
        {
            "title": "Primary domain\nSyntax basis\nConnectivity encoding\nStochastic representation\nPolymer architecture\nError tolerance\nTypical example\nAdvantages\nLimitations",
            "content": "Small molecules ASCII strings with chemical rules Explicit bonds, rings, branches Not supported Not supported Fragilesmall changes can break validity CCO (ethanol) Compact, widely supported Syntactic fragility Polymers and macromolecules SMILES syntax + curly bracket extensions"
        },
        {
            "title": "Small molecules\nTokenized grammar rules",
            "content": "Bonds, rings, branches + bonding descriptors ([*]) Encoded via grammar tokens Supported via curly brackets Supports block, random, graft, branched Same as SMILES for monomers {[*]CC[*]} (polyethylene) Encodes polymer connectivity Still fragile at monomer level Not supported Not supported Guaranteed 100% valid [C][C][O] (ethanol) Robust to syntax errors Redundancy, longer strings Fig. 8: Schematic of BigSMILES representations from Lin et al. [211]. Polymers are represented as monomers (repeating units) enclosed within curly brackets; the curly brackets indicate that the molecule is stochastic object. The monomers are represented as SMILES strings, with additional information expressing the connectivity between monomeric units. bolic representations serve as bridge between empirical observations and theoretical understanding across scientific disciplines. 4) Structured Data: Structured data in scientific domains refers to information systematically organized through explicit, formal models that enable efficient querying, storage, and computational reasoning. Across disciplines, structured data follows progression from simple tabular formats to complex knowledge representations. At the foundational level, data tables consisting of columns {ci}C i=1 and rows {lj}R j=1 serve as the basic organizational unit, with each cell vij representing measurements or annotations. These tables, prevalent in resources like GEO [228], dbSNP [229], and weather station datasets such as WEATHER-5K [230], provide straightforward data organization but lack explicit semantics or inter-attribute relationships. Building upon this foundation, relational databases = {T1, T2, . . . , TN } extend tables with schema-level constraints and referential integrity, where foreign key pairs (c(k) ) connect columns across tables, enabling complex queries over diverse entities as seen in Ensembl [231] and UniProtKB [232]. , c(h) The evolution toward more expressive representations includes ontologies and knowledge graphs that capture domainspecific semantics and relationships. Ontologies formally represent concepts and their relationships using languages like Web Ontology Language [233] or Open Biological and Biomedical Ontologies [234], defining classes, properties, and hierarchies for semantic interoperability and logical inference, exemplified by the Gene Ontology [235] and Human PhenoFig. 9: Exemplified symbolic representations (cheminformatics) of formaldehyde and phenol: molecular graph, SMILES and SELFIES string, node identity, and adjacency matrix. Hydrogens are typically omitted in SMILES and SELFIES strings. In the adjacency matrix, edge weights reflect bond types: 1 for single bonds, 2 for double bonds, and 3 for bonds in the aromatic ring. type Ontology [236]. knowledge graph is collection of relational facts ERE, where denotes the set of entities and the set of semantic relations. By integrating heterogeneous data into unified semantic representation, knowledge graphs facilitate knowledge reasoning and discovery [237], [238], as exemplified by UMLS [239] and PrimeKG [240]; similarly, CLLMate [241] aligns meteorological records with climate events. Taken together, these developments form structured data ecosystem supported by standardized exchange formatsincluding CSV, XML, JSON, YAML, HDF5, ROOT, FITS, and NetCDFthat ensure traceability and interoperability across disciplines. Large-scale repositories have emerged as critical infrastructure, from molecular libraries like ZINC [215] and ChEMBL [216] storing compounds in SMILES format [242], [243], to physics archives like CODATA [244] and particle physics databases [245], astronomical catalogs including SIMBAD [246] and VizieR [247], materials databases such as the Materials Project [70] and MatBench [71]. The sophistication of structured data extends to specialized property datasets that enable targeted scientific investigations. In chemistry, ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) databases [243], [248] provide com12 Fig. 10: Five-channel EEG recording setup and corresponding time (T); Vertical axis: time series data. Horizontal axis: individual EEG channels showing brain electrical activity patterns recorded from scalp electrodes. Figure is adapted from CSBrain [271]. prehensive pharmacokinetic properties including absorption (Bioavailability [249], HIA [250]), distribution (BBB [251], FreeSolv [252]), metabolism (Clearance-AstraZeneca [253]), excretion (VDss [62], [254]), and toxicity (ClinTox [218], ToxCast [255], Tox21 [256]) measurements crucial for drug discovery. Similarly, gravitational-wave catalogs like GWTC [65] document events with detailed source parameters in machinereadable formats, while materials databases provide multiproperty coverage including electronic, thermodynamic, and mechanical behaviors computed under standardized protocols. These structured resources leverage persistent identifiers and metadata standards, facilitating rich scholarly analyses through bibliographic knowledge graphs like INSPIRE-HEP [257] and NASA ADS [98], ultimately enabling robust predictive modeling and efficient exploration of vast scientific spaces across all disciplines. t=0, and forces {F(t) RN 3}T 5) Time-Series Data: Time series data, characterized by sequences of temporal data points collected at certain intervals [258][260], constitutes fundamental data modality across scientific disciplines, capturing dynamic phenomena from nanoseconds to decades. These data enable the analysis of temporal patterns, periodicity, and system evolution across vastly different scalesfrom molecular dynamics tracking atomic positions {X(t) RN 3}T t=0, velocities {V(t) RN 3}T t=0 in datasets like MD17 [261] and ISO17 [262], [263], to astronomical observations monitoring stellar brightness variations for exoplanet detection in missions like Kepler [264] and Fivehundred-meter Aperture Spherical Telescope (FAST) [265]. The temporal resolution spans milliseconds in neurophysiological recordings such as electroencephalogram (EEG) [266] capturing brain oscillations [267] and event-related potentials [268]  (Fig. 10)  , to hourly meteorological variables in the ERA5 dataset [194] with 0.25-degree spatial resolution, and continuous seismic waveforms from Incorporate Research Institutions for Seismology [269] and United States Geological Survey networks [270] for earthquake monitoring. The diversity of time-series modalities reflects the multiscale nature of scientific phenomena. In biological systems, time-series data capture dynamics from molecular-level gene expression patterns revealing temporal responses [272] [274] to clinical monitoring through electrocardiogram (ECG) [275] for cardiac rhythm analysis [276], electromyoFig. 11: Multi-omics data landscape. gram (EMG) [277] for muscle activity [278], and continuous glucose monitoring [279], [280]. Neuroimaging modalities provide complementary temporal and spatial resolutions: functional magnetic resonance imaging (fMRI) detects bloodoxygen-level-dependent (BOLD) signals [281] for mapping brain networks [282], while magnetoencephalography (MEG) measures magnetic fields from neuronal activity [283], [284]. In chemistry, molecular spectrum data mainly include Rainfrared (IR), ultraviolet (UV), 1H nuclear magnetic man, resonance (NMR), and 13C NMR spectroscopy [285], revealing structural and compositional information enabling AIdriven representation learning [139]. Physics leverages highfrequency strain data from LIGO/Virgo at 16,384 Hz for gravitational wave detection [65], while SDO [286] provides Atmospheric Imaging Assembly Extreme Ultraviolet images every 12 seconds and Helioseismic and Magnetic Imager (HMI) vector-magnetogram-derived Space-weather HMI Active Region Patches features at 12-minute cadence to forecast space weather [287]. These temporal datasets serve critical roles in understanding system dynamics, enabling predictive modeling, and monitoring critical events. Longitudinal clinical studies utilize serial MRI, CT, and clinical report data [288] to model disease trajectories [289], [290], while synoptic astronomical surveys like The Zwicky Transient Facility [291] and Legacy Survey of Space and Time [292] generate calibrated image sequences for transient detection. Earth science integrates atmospheric data from WeatherBench [195] and WEATHER-5K [230], oceanic measurements from the Hybrid Coordinate Ocean Model (HYCOM) [293] and NOAA Tides [294], and geophysical recordings for comprehensive Earth system monitoring. The standardization of these diverse time-series formats facilitates cross-disciplinary AI applications [295][297], establishing time-series analysis as cornerstone methodology for extracting insights from dynamic scientific phenomena across all scales. 6) Multi-omics Integration: Driven by rapid advances in high-throughput technologies, multi-omics has emerged as powerful approach for capturing the complexity of living systems through the integrated analysis of multiple layers of biological data [61]. As illustrated in Fig. 11, the multiomics landscape encompasses seven major data modalities: genomics (capturing genetic sequences and variations), epigenomics (mapping regulatory modifications), transcriptomics 13 those from ENCODE and Roadmap Epigenomics [314], layer chromatin accessibility, histone marks, DNA methylation, and transcription factor binding profiles onto the genome to reveal regulatory landscapes. (iv) Spatial genome resources [315], [316], including Hi-C datasets and 3D genome browsers, reconstruct chromatin topology to explore long-range regulatory interactions. Genomic data are inherently symbolic and sequential, with rich metadata and controlled vocabularies [236]features that make them well-suited for conversion into prompt-based representations for language models [317], [318]. Emerging methods already leverage large-scale variant catalogs [312] and knowledge graphs [240] to train foundation models for genotype-phenotype reasoning, while multiresolution integration with imaging or epigenetics supports causal inference at cellular and organismal scales. Transcriptomics captures the dynamic and context-specific landscape of gene expression, linking genome to phenotype in time and space. Its data ecosystem spans multiple layers that together provide comprehensive view of transcriptional activity. (i) Transcript annotations from sources like GENCODE [319] and RefSeq [320] define exonintron structures, splice variants, and isoform-level expression. (ii) At the foundational level, bulk RNA-seq and single-cell RNA-seq repositories such as GEO [228], and ArrayExpress [321] house millions of transcriptomic profiles across tissues, conditions, and perturbations. (iii) Expression atlases, such as the Human Cell Atlas or GTEx [322], enable comparative and tissuespecific analyses of transcriptional activity. (iv) Spatial transcriptomics platforms, including 10x Genomics Visium [323], Slide-seq [324], and Stereo-seq [325], link gene expression profiles to precise tissue coordinates, enabling spatially resolved analyses of cell-cell interactions, microenvironmental heterogeneity, and histopathological context. Public repositories like SpatialDB [326] aggregate thousands of such datasets across diverse species and conditions, facilitating cross-study comparisons and integration with histology images. (v) Gene co-expression networks, such as STRING [327] co-expression edges, provide functional grouping of genes based on correlated activity. These transcriptomic resources form rich, structured, and temporally resolved representation of cellular states, readily convertible into graph-, token-, or prompt-based formats for integration with other omics layers in large-scale modeling. Proteomics is often described as multimodal, but, strictly speaking, the field rarely couples images with free text in the way vision-language benchmarks do. Instead, it juggles molecular representations drawn from distinct information channels: (i) structured knowledge bases such as UniProtKB deliver expertly curated sequences, domains and post-translational modifications for more than 250 million proteins [232], among them, the reviewed subset UniProtKB/Swiss-Prot (0.57 million entries, as of August 2025) is the most widely used; while the Protein Data Bank (PDB) stores atomic coordinates for experimentally determined folds [328]; (ii) interaction networks fuse biochemical and genetic evidenceSTRING merges literature, co-expression and synteny to build genomewide association graphs [329], whereas BioGRID [330] and IntAct [331] record bench-validated contacts; (iii) symbolic 14 Fig. 12: Symbolic representations and 3D structure visualizations across different scientific domains: DNA, RNA and Protein. The DNA structure is split into chain and chain from PDB 1KX5 [298] and visualized by UCSF Chimera [299]. The RNA structure is from the RNAsolo with ID 7ELQ [300], [301]. The protein snapshot is from the PDB bank with ID 7CAM [302]. The DNA and protein are adapted from NatureLM [43]. (profiling gene expression), proteomics (analyzing protein abundance and function), metabolomics (measuring small molecule metabolites), microbiome (characterizing microbial communities and their functions/interactions), and exposome (tracking environmental effects). These omics layers are interconnected through biological processes, from transcription and translation at the molecular level to environmental interactions at the systems level, offering complementary insights that together enable more comprehensive understanding of biological processes than any single layer alone [303], [304]. At the molecular core of this framework, biological information flows from DNA to RNA to proteins, with each biomolecule existing in both symbolic sequence representations and threedimensional structural forms  (Fig. 12)  . Multi-omics technologies have continued to advance, offering improved resolution, accuracy, and scalability, along with enhanced methods for integrating data across different biological domains [62], [305][307]. As result, multi-omics has emerged as cornerstone of modern scientific research, providing deeper insights into the molecular mechanisms underlying health and disease, unraveling complex regulatory networks, and driving data-informed discoveries across diverse biological domains [308]. Genomics encompasses vast and evolving ecosystem of structured, symbolic and sequence-based representations. (i) Reference genomes, such as those hosted by Ensembl [309] and UCSC Genome Browser [310], provide curated nucleotide sequences and annotated genomic elements across thousands of species. (ii) Genetic variation, arising from differences is in DNA sequences across individuals or populations, central focus of genomics. Population-scale resources such as GWAS Catalog [311], dbSNP [229] and gnomAD [312] catalog common and rare variants, providing estimates of allele frequencies across diverse cohorts, while ClinVar connects specific variants to clinical phenotypes and pathogenicity interpretations [313]. (iii) Functional genomics maps, such as ontologies provide shared semantic layer, with the Gene Ontology defining controlled terms for function, process and localization [332]; (iv) image resources such as the Human Protein Atlas place thousands of proteins into tissue and cellular context by immunohistochemistry and fluorescence microscopy [333]; (v) computational structure repositories, notably the AlphaFold Protein Structure Database, extend empirical coverage with high-confidence models for millions of previously unsolved proteins [334]; and (vi) time-resolved quantitative datasets from mass-spectrometry pipelines are shared through the ProteomeXchange consortium [335], with PRIDE as its flagship archive [336]. Seamlessly combining these heterogeneous modalities yields synergistic insight, e.g., PDB experimental structures and AlphaFold DB predicted models (surfaced via PDBe-KB) jointly constrain interaction graphs from STRING, BioGRID, and IntAct; ontology-aware statistics translate large-scale microscopy screens into testable biological hypotheses; and longitudinal mass spectrometry experiments connect dynamic post-translational regulation to spatial relocalization inferred from imaging. Although corpora already formatted as dialogue for LLM training remain scarce, the underlying repositories constitute machine-readable graphs, tables and sequences that can be converted into textual prompts or retrieval-augmented contexts with minimal templating. Emerging pipelines therefore marry graph databases with transformer representation learning, reconcile identifiers across formats, and propagate uncertainty, all under FAIR standards [337] (Findable, Accessible, Interoperable, Reusable) such as MIAPE [338] and ProteomeXchange-XML [335]. As these resources expand and model architectures mature, genuinely integrative, causally grounded digital proteome becomes feasible, where each protein is simultaneously encoded as sequence, structure, dynamic profile, network node and spatial image, ready for LLM-driven reasoning across the molecular landscape. Beyond the molecular central dogma, additional omics layers provide complementary biochemical and environmental perspectives. Metabolomics profiles small-molecule metabolites to capture biochemical activity and phenotypic state, with repositories such as the Human Metabolome Database [339] and MetaboLights [340] supporting pathway-level integration with other omics. Microbiome studies characterize the composition and functional potential of microbial communities through metagenomic and metatranscriptomic sequencing, with resources like the Human Microbiome Project [341] and MGnify [342] enabling hostmicrobe interaction analyses. Exposome research examines the totality of environmental exposures, including diet, pollutants, and lifestyle factors, using chemical assays, wearable sensors, and curated biomarker databases such as Exposome-Explorer [343]. These layers extend multi-omics frameworks by linking molecular phenotypes to ecological and environmental contexts. From precision medicine and cancer research to environmental science and agriculture, multi-omics data now empower researchers to tackle complex, interdisciplinary problems and generate holistic models of biological and ecological systems [344][346]. B. Hierarchical Structure of Scientific Knowledge Scientific knowledge fundamentally differs from flat collection of information. Instead, it manifests as sophisticated hierarchical system that mirrors the progressive nature of human cognition and the evolutionary path of scientific discovery from phenomena to essence, from the concrete to the abstract. This inherent stratification resonates with established knowledge hierarchy models, most notably the DIKW (DataInformation-Knowledge-Wisdom) pyramid articulated by Ackoff [347] and systematically analyzed by Rowley [348], which posits that knowledge emerges through qualitative transformations rather than mere accumulation. However, as Zeleny [349] observed in mapping knowledge forms from knownothing through know-what and know-how to knowwhy, scientific inquiry demands more nuanced taxonomy that captures both procedural and explanatory dimensions. Building upon these theoretical foundations while addressing the unique epistemological requirements of scientific practice, we propose five-tiered framework encompassing factual, theoretical, methodological-technological, modeling-simulation, and insight levels. This stratification reflects what Baskarada and Koronios [350] characterize as the need to contextualize knowledge hierarchies within specific domains, incorporating the computational and instrumental dimensions essential to contemporary science. Each level represents not merely repository of information but distinct mode of understanding, exhibiting emergent properties that reflect the transformative nature of scientific knowledge construction. The following sections will systematically examine each stratum, revealing how this hierarchical architecture facilitates both the organization of existing knowledge and the generation of novel scientific insights. To this end, we organize this subsection into five interconnected components, each representing distinct level of scientific knowledge, as shown in Fig. 13. These levels include: the Factual Level (Sec. II-B1), the Theoretical Level (Sec. II-B2), the Methodological and Technological Level (Sec. II-B3), the Modeling and Simulation Level (Sec. II-B4), and the Insight Level (Sec. II-B5). In addition, we discuss Dynamic Interactions and Evolution (Sec. II-B6) which highlights the iterative feedback loops across levels that collectively drive scientific progress. Finally, we conclude this subsection with the implication of such hierarchy (Sec. II-B7), which not only underscores the progressive deepening from data to discovery but also provides structured foundation for developing SciLLMs that can effectively capture and utilize the multifaceted nature of scientific data. 1) Factual Level: At the foundation of scientific knowledge lies the factual leveldirect observational data, experimental measurements, and empirical evidence that constitute our primary interface with the physical world. This raw, unprocessed information serves as the bedrock for all subsequent scientific understanding. Factual data is characterized by its objectivity and minimal human intervention. When astronomers collect astronomical imaging data, such as multi-band images [351], and additional light curves and spectra from distant galaxies, parti15 in [363], global meteorological variables are represented using 128 256 tensor, providing spatial discretization suitable for modeling over the entire globe. Regarding temporal resolution, different tasks require data at distinct time intervals. For some short-term nowcasting tasks [364], [365], data are typically recorded at 10-minute intervals, enabling the capture of rapidly evolving atmospheric phenomena. In contrast, for medium-range forecasting tasks [366], [367], data are usually sampled every 6 hours to balance data volume with the relevant timescales for prediction."
        },
        {
            "title": "Inherent uncertainties and noise are integral",
            "content": "to factual data. Quantum experiments face fundamental measurement limits [77], biological studies contend with individual variation and technical noise [78], astronomical observations are severely degraded by atmospheric turbulence [368], and clinical trials must account for patient heterogeneity [369]. These uncertainties inform confidence bounds and guide robust analytical methods across all scientific disciplines. 2) Theoretical Level: The theoretical level transcends empirical observations through diverse forms of abstraction and formalization. Beyond mathematical equations such as Newtons mechanics [370], Maxwells electromagnetism [371], Schrodingers quantum mechanics [372], and Hodgkin-Huxley neural dynamics [373], scientific theories employ multiple representational frameworks. Conceptual models capture fundamental principles: the central dogma in molecular biology [374], plate tectonics in geoscience [375], and the Standard Model in particle physics [376]. Classification systems organize knowledge hierarchically: Linnaean taxonomy [377], the periodic table [378], Gene Ontology [235], and astronomical object catalogs [247]. Network representations reveal systemic relationships: protein interaction networks [379], metabolic pathways [380], ecological food webs [381], and brain connectomes [382]. Computational models bridge theory and prediction: climate circulation models [383], molecular dynamics simulations [384], population genetics algorithms [385], and pharmacokinetic compartmental models [386]. Statistical frameworks quantify uncertainty: Bayesian inference in phylogenetics [387], machine learning in multi-omics integration [61], and cosmological parameter estimation [388]. These diverse theoretical representations exhibit hierarchical organization and domain-specific validity. Mathematical formalisms enable precise predictions; conceptual models provide intuitive understanding; classification systems facilitate knowledge organization; network models reveal emergent properties; computational approaches handle complexity. Together, they transform raw data into actionable scientific knowledge, creating multi-layered theoretical infrastructure that supports discovery, prediction, and technological innovation across disciplines [67]. 3) Methodological and Technological Level: Between raw facts and abstract theories lies crucial intermediate layer of methods and tools that transform theoretical predictions into testable hypotheses and raw data into theoretical insights. Scientific methodology has evolved from simple comparative studies to sophisticated experimental designs across disciplines. Revolutionary techniques open new frontiers: CRISPR16 Fig. 13: Hierarchical structure of scientific knowledge. The framework comprises five levels: factual (raw data), theoretical (laws and principles), methodological/technological (methods and tools), modeling/simulation (computational models), and insight (discoveries). The bottom panel illustrates the iterative cycle linking these levels through data collection, pattern recognition, hypothesis testing, and theory development. cle physicists capture collision events at the Large Hadron Collider [72], gravitational-wave detectors measure strain signals [352], or biologists sequence genetic material [353], they obtain direct representations of natures state. Despite instrumental limitations, these data fundamentally reflect objective reality independent of theoretical frameworks. Modern experiments generate data of unprecedented dimensionality and structural complexity. High-energy physics experiments like Toroidal LHC Apparatus (ATLAS) and Compact Muon Solenoid (CMS) produce order-of-tens of terabytes of collision data per second [72], while LIGO and Virgo release strain data sampled at 16,384 Hz [65]. This heterogeneity spans all domains: multi-channel neural recordings capture brain dynamics at millisecond resolution [354], single-cell RNA sequencing reveals cellular heterogeneity with millions of transcripts [355], multi-omics platforms integrate genomic, proteomic, and metabolomic data [61], agricultural sensors monitor crop phenotypes across spatial and temporal scales [356], and Earth observation satellites generate multispectral imagery for climate monitoring [357]."
        },
        {
            "title": "Critical",
            "content": "to scientific data is its spatiotemporal context. Astronomical observations acquire meaning only when anchored by precise coordinates and timestamps, enabling crossinstrument calibration and transient detection. Self-supervised models that jointly encode images, spectra, and light curves demonstrate that meaningful representations emerge through multimodal fusion [358]. Similarly, seismic wave arrivals at distributed stations enable earthquake triangulation and Earth structure probing [359], [360], while drug discovery relies on temporal pharmacokinetic profiles [361] and agricultural yield predictions depend on phenological timing [362]. In the field of Earth science, the spatiotemporal characteristics of data are particularly prominent. This is primarily reflected in the fact that spatial scales of Earth science data often need to be mapped to specific geographic resolutions. For example, Cas9 enables precise genomic editing [389], ultracold atom Bose-Einstein condensation paved the way for quantum simulation [390], and high-throughput sequencing enables multiomics profiling [61]. Computational methods bridge theory and experiment. Monte Carlo algorithms [391] underpin simulations from protein folding to climate modeling. Machine learning extracts patterns from massive datasets, e.g., AlphaFold [392] predicts protein structures, while algorithms identify astronomical objects and reconstruct neural circuits [393]. Statistical frameworks ensure rigorous inference: particle physics commonly adopts five-sigma threshold for discovery [394], while Bayesian approaches provide principled uncertainty quantification across fields [79]."
        },
        {
            "title": "Instrumental",
            "content": "technologies extend observation into new realms. From Ruskas electron microscope [395] to modern cryo-electron microscopes (cryo-EM), from LIGOs detection of 1021-level spacetime strains [65] to single-cell sequencing [396], these tools fundamentally alter what questions we can ask. This creates feedback loops where better instruments enable deeper theories, which guide development of more sophisticated technologies. 4) Modeling and Simulation Level: This level involves utilizing numerical simulations to replicate complex systems. Virtual experiments enable researchers to test hypotheses and predict phenomena otherwise difficult or costly to study. Contemporary modeling emphasizes multi-scale integration. Materials science connects quantum calculations at atomic scales to macro-level material behaviors [76]. Climate modeling integrates short-term atmospheric processes with longterm ocean dynamics, bridging local weather and global climate change [397]. Astronomy links transient events like supernovae to long-term galaxy evolution spanning billions of years [398]. Physics-informed neural networks merge physical laws and data-driven approaches, enabling effective data-physics fusion for fluid dynamics simulations with notable demonstrations from aerospace to biomedical applications [399], [400]. Life sciences employ multi-scale models to explore molecular interactions and biological systems [401]. Computational simulations accelerate drug discovery by predicting molecular interactions [402]. Multi-omics approaches integrate genomic, proteomic, and metabolomic data to decipher disease mechanisms and guide personalized treatment [61]. Neuroscience simulations range from synaptic processes to brain-wide activity [403], while agronomic models forecast crop performance under varying environmental conditions [404]. Rigorous verification and validation processes ensure model reliability, confirming computational accuracy and predictive validity against experimental data, which is critical in nuclear engineering, aerospace, and medical certifications [405]. Thus, the modeling and simulation level serves as foundational tool, supporting modern scientific exploration and informed decision-making. 5) Insight Level: At the apex of the scientific hierarchy, the insight level represents transformative moments when disparate knowledge coalesces into revolutionary understanding. Cross-disciplinary fusion has repeatedly catalyzed such breakthroughs: Shannons information theory meeting molecular biology birthed bioinformatics, revealing life as an information processing system [406], [407]; neuroscience converging with physics produced brain imaging technologies that decode neural activity patterns [408]; astronomical spectroscopy combined with quantum mechanics unveiled stellar nucleosynthesis, explaining element formation across the cosmos [409]. These interdisciplinary insights demand intellectual flexibility to recognize patterns across traditional boundaries, from protein folding dynamics mirroring energy landscape theory in physics [410], to agricultural genomics borrowing population genetics models to enhance crop resilience [411]. Scientific revolutions often emerge from careful attention to anomalies that challenge existing frameworks. Classical physics predicted unbounded ultraviolet radiance at short wavelengths under the Rayleigh-Jeans law; Plancks quantization of energy in 1900 resolved this ultraviolet catastrophe and birthed quantum theory [412]. Similarly, the discovery of reverse transcriptase shattered the central dogma of molecular biology [413], while anomalous galactic rotation curves revealed dark matters existence [414]. In pharmacology, unexpected drug side effects have led to therapeutic breakthroughs: sildenafils transition from angina treatment to erectile dysfunction exemplifies serendipitous discovery through anomaly recognition [415]. True conceptual innovation transcends problem-solving to introduce novel frameworks: Darwins natural selection fundamentally altered our view of lifes relationship to time [416]; plate tectonics unified previously disparate geological phenomena [417]; systems biologys emergence revealed that biological function arises from network interactions rather than isolated components [401]. In the era of multi-omics and big data, extracting genuine insight requires navigating information overload through humanAI collaboration. Machine learning excels at pattern recognition across genomic, proteomic, and metabolomic datasets, uncovering disease signatures invisible to traditional analysis [61]. Yet human judgment remains essential for distinguishing correlation from causation, contextualizing discoveries within theoretical frameworks, and recognizing which patterns reflect fundamental principles. The future of scientific insight lies in this synergy, where computational power amplifies human creativity to reveal natures hidden connections across scales from quantum to cosmic, from molecular to ecological. Scientific progress emerges from dynamic interactions between hierarchical levels of knowledge, creating intricate feedback loops that drive discovery forward. This process manifests through three primary mechanisms: bottom-up induction, top-down deduction, and horizontal method transfer. and Evolution: 6) Dynamic"
        },
        {
            "title": "Interactions",
            "content": "Inductive processes transform observations into theoretical understanding across disciplines. In astronomy, Keplers analysis of Brahes observations yielded planetary motion laws, later unified by Newtons gravitational theory. Modern life sciences follow similar trajectories: genomic sequencing reveals patterns explained through molecular and evolutionary models; neuroimaging data drives theories of brain function; agricultural field trials inform crop optimization strategies; 17 and multi-omics integration uncovers systems-level biological principles. In physics, deduction channels theoretical insights into experimental design. Einsteins 1916 prediction of gravitational waves guided decades of detector development, culminating in LIGO and Virgos detection of spacetime strains ( 1021 m) from binary black-hole mergers in 2015, confirming century-old predictions and inaugurating gravitationalwave astronomy [418]. Horizontal method transfer catalyzes unexpected advances. X-ray crystallography transitioned from mineralogy to revealing biomolecular structures; machine learning algorithms developed for image recognition now predict protein folding and drug-target interactions; network analysis from sociology illuminates ecological interactions and neural connectivity; spectroscopic techniques from physics enable remote sensing in Earth science and metabolomics profiling. This evolution follows spiral pattern where theories transcend and include predecessors, i.e., classical mechanics subsumed within relativity and quantum mechanics, Mendelian genetics integrated with molecular biology, revealing why earlier frameworks succeeded within their domains while pointing toward more comprehensive understanding. Such dynamic interactions are essential for developing AI systems that capture sciences creative essence beyond pattern matching. 7) Implications for Sci-LLMs: This hierarchical framework carries profound implications for the development and deployment of Sci-LLMs. Each level offers distinct computational challenges and opportunities for language model integration. At the factual level, LLMs must learn to parse heterogeneous data formats, extract patterns from high-dimensional observations, and maintain spatiotemporal context, which is essential for tasks like automated literature mining and experimental data interpretation. The theoretical level demands that models internalize mathematical formalisms, causal relationships, and domain-specific ontologies, enabling them to reason about scientific laws and generate testable hypotheses. The methodological level requires LLMs to understand experimental protocols, computational workflows, and instrumental constraints, facilitating automated experiment design and method recommendation. At the modeling and simulation level, language models can serve as interfaces between natural language queries and complex computational engines, translating scientific questions into simulation parameters and interpreting results. Finally, the insight level challenges LLMs to perform cross-domain synthesis and creative hypothesis generation, capabilities that emerge from training on the full spectrum of scientific knowledge rather than isolated datasets. By incorporating data from all five levels, Sci-LLMs can transcend simple information retrieval to become active participants in the scientific discovery process, bridging human intuition with computational power. C. Key Challenges in Scientific AI In the field of scientific AI, especially within LLMs and MLLMs, several key challenges must be addressed to enable meaningful scientific understanding and reasoning. These challenges include interpretability (Sec. II-C1), cross-scale and multimodal integration (Sec. II-C2), as well as dynamic knowledge evolvement (Sec. II-C3), all of which are essential for enhancing the effectiveness of these models in scientific applications. 1) Interpretability in Scientific AI: Interpretability remains major bottleneck. Scientific reasoning is inherently logical, based on clear explanations and justifications. However, LLMs and MLLMs are typically perceived as black-box models, making it difficult to understand the rationale behind models reasoning or output. This challenge is particularly acute in scientific domains, where understanding the why and how behind an answer is just as important as the answer itself. Interpretability is crucial for building trust in Sci-LLMs, especially in high-stakes fields such as drug discovery and climate modeling. In LLM/MLLM area, prompting or training the model with chain-of-thought (CoT) [419], [420] emerges as an effective technique to elicit explicit, natural-language reasoning capability of LLMs. CoT enables the model to write step-by-step reasoning trace, breaking down complex tasks before giving the final answer. This makes the reasoning path more transparent and provides clearer insights into its decision-making. The recent work, BioReason [421], introduces this multi-step reasoning strategy into DNA foundation models, enabling deep, interpretable biological reasoning from complex genomic data. By integrating DNA foundation model with an LLM and constructing biological CoT, BioReason empowers the LLM to directly process and reason with genomic information, fostering multimodal biological understanding. Through reinforcement the model refines its multi-step reasoning capabilities, leading to biologically coherent deductions and outperforming traditional singlemodality models on biological reasoning benchmarks. Overall, conducting CoT reasoning in scientific AI models is particularly challenging due to the complexity and domain-specific nature of scientific knowledge. Unlike generalist models, scientific reasoning involves hypothesis-driven logic grounded in empirical evidence, requiring precise understanding across disciplines such as biology, chemistry, and physics. Therefore, more work is needed to develop transparent models that can offer both scientific accuracy and explainable reasoning. learning, 2) Cross-scale and Multimodal Integration: Another major hurdle in the application of LLMs and MLLMs to scientific reasoning is their ability to handle cross-scale and multimodal integration. Scientific data is often characterized by hierarchical structures that span multiple scales, from microscopic phenomena (e.g., molecular dynamics in chemistry) to macroscopic phenomena (e.g., weather patterns or ecosystem behavior). For example, in computational biology, understanding the behavior of cell involves integrating data from individual molecules to entire tissues, which can require models to simultaneously process both fine-grained details and large-scale systems. Traditional LLMs excel at processing textual data but struggle to model spatiotemporal dependencies across scales. Moreover, scientific reasoning frequently involves multimodal data, typically combining text, images, numerical data, and experimental results. This requires models to seamlessly integrate heterogeneous data sources [73], [74]. The challenge is further exacerbated when the information comes from different experimental setups or different measurement modalities, each requiring tailored processing pipelines that preserve important domain-specific features. For instance, bioinformatics deals with an extensive variety of data, including DNA, RNA, protein sequences, and drug molecules [63]. MLLMs have the potential to address this complexity by integrating text, images, audio, and other modalities. They offer promising opportunities to enhance scientific understanding by connecting disparate data points and inferring relationships across these varied modalities. Initiatives such as the National Institutes of Healths Advancing Health Research through Multimodal AI [422] exemplify this trend, aiming to develop data-driven multimodal AI approaches to model, interpret, and predict complex biological, behavioral, and health systems. However, significant challenges persist in achieving seamless multimodal integration. MLLMs frequently struggle with complex multimodal and multi-step reasoning tasks, often relying on shallow multimodal cues or defaulting to text-dominant reasoning rather than truly integrated understanding. major bottleneck in their development is the scarcity of appropriate, high-quality multimodal scientific datasets. To address these challenges, models need to move beyond isolated data streams and embrace holistic integration of cross-scale and multimodal information to create truly unified frameworks that can seamlessly integrate complex scientific data and perform rigor scientific reasoning. 3) Dynamic Knowledge Evolvement: One of the most prominent challenges in applying LLMs and MLLMs to scientific domains is ensuring knowledge update and evolvement. In scientific research, knowledge evolves dynamically, with new discoveries constantly challenging existing theories. This makes it difficult for models trained on static datasets to maintain consistency with the most current body of scientific knowledge. Models that fail to continuously update their knowledge bases risk generating outdated or conflicting information, which can undermine their utility in domains like medical research, physics, or environmental science. To fix this, we need to explore new methods like automated knowledge injection and model adaptation. These approaches would allow models to continuously integrate new research findings, ensuring they remain coherent and aligned with the rapidly changing world of scientific discovery. D. Quality Standards for Scientific Datasets Assessing the quality of scientific data is essential for developing robust scientific AI models. In this subsection, we outline four complementary dimensions that together characterize data quality in scientific contexts. First, accuracy (Sec. II-D1) assesses how faithfully data represent the underlying phenomena. Second, completeness (Sec. II-D2) concerns to which datasets capture all relevant elements the extent across content, structure, and temporal coverage. Third, timeliness (Sec. II-D3) measures the update frequency and responsiveness of datasets to real-world changes. Finally, traceability (Sec. II-D4) ensures transparency and reproducibility by documenting provenance, metadata, and version histories. Together, these aspects provide systematic framework for evaluating the reliability, usability, and long-term value of scientific datasets, standardizing data management practices and guiding optimal AI deployment. 1) Accuracy: Accuracy is one of the fundamental dimensions of scientific data quality, reflecting how closely data represent the real world in terms of spatial positioning, temporal annotation, and signal fidelity. High-accuracy data not only enhances the training efficiency and inference precision of AI models, but also directly impact the credibility of scientific in geospatial datasets, Landsat conclusions. For example, 8 satellite imagery, after ground control point correction, achieves geolocation error of 15 to 30 meters, indicating high spatial precision [423]. In contrast, location information from some social media platforms is often only annotated the city level, offering coarse granularity that hinders at fine-grained modeling [424]. In the physical sciences, the Materials Project provides data generated via first-principles calculations, controlling model errors, and ensuring reliable accuracy in band structure and lattice constants [70]. Common methods for assessing accuracy include mean squared error (MSE), root mean square error (RMSE), temporal alignment deviation, and signal-to-noise ratio (SNR), typically quantified by comparing with ground truth or high-quality benchmark datasets [425], [426]. 2) Completeness: Completeness refers to the extent to which scientific data set adequately covers content, structural fields, and temporal span, whether it contains all the data elements that should have been collected. It serves as foundation for systematic and logical data analysis. In genomics, completeness is often evaluated by sequencing depth; coverage below 10 is generally insufficient to accurately detect mutations, and modern whole genome sequencing standards typically require an average coverage of 30 or more [427], [428]. In the field of materials science, data integrity directly determines the success or failure of data-driven discovery of new materials [429]. Methods for assessing completeness include missing value statistics, field coverage analysis, breakpoint detection, and time series gap identification. For example, in Earth science, SCDNA [430] filled in missing data for precipitation, minimum temperature, and maximum temperature to ensure the data integrity across all weather stations, which improved the accuracy of spatial interpolation. Tools such as OpenRefine [431] and DataCleaner [432] can automatically detect missing entries, structural anomalies, and null fields, thus improving the overall quality of datasets. 3) Timeliness: Timeliness measures data update frequency, the latency between data collection and release, and the speed at which data respond to real-world changes. This is crucial for applications like emergency response, trend forecasting, and dynamic modeling. For instance, during the COVID-19 pandemic, the Johns Hopkins University dataset was released at daily intervals, enabling rapid epidemic modeling and policy decision-making on global scale [433]. In remote sensing, NASAs MODIS satellite products are updated daily, supporting timely environmental monitoring and disaster assessment [434]. In contrast, traditional datasets like ImageNet [435] and MNIST have not been updated for years, making them suitable for algorithm benchmarking but 19 less relevant for contemporary applications. Meanwhile, open knowledge bases like Wikidata allow real-time user editing and provide API-based updates, representing higher level of interactive timeliness [436]. Timeliness can be systematically quantified using indicators such as collection-to-release time lag, average update interval, event response delay, and timestamp consistency [425], [437]. 4) Traceability: Data traceability refers to the ability to track the complete journey of data from its origin and transformations to its final use. Traceability has increasingly become critical supplementary metric for evaluating scientific data security and trustworthiness, especially in the context of open science and data reuse. Highly traceable data should include complete metadata, change logs, version control records, and accountability information, meeting the Findability and Reusability criteria of the FAIR principles [337]. For example, each record on the OpenAIRE platform [438] includes unique DOI, data acquisition description, and license details, significantly enhancing verifiability and reuse credibility. Moderately traceable data may provide basic metadata but often lack processing chains, revision histories, or algorithmic documentation, limiting users ability to assess reliability. Low-traceability data typically lack source documentation and coherent annotation, rendering them difficult to verify. For instance, web-scraped research images or code snippets without provenance or revision records pose considerable risks in academic usage [439]. Recently, technologies such as blockchain and cryptographic hash signatures are being explored to build traceability chains and verifiable records for scientific data [440]. E. Dimensions for Evaluating Scientific AI General-purpose LLM benchmarks primarily assess core natural language processing and general reasoning abilities. Key evaluation dimensions typically include language understanding, fluency, factual knowledge recall, reasoning and problem-solving. These benchmarks are designed to evaluate broad linguistic competence and general cognitive skills across everyday or non-specialized domains. Even when covering technical subjects (e.g., STEM topics in MMLU), they often assume only basic computational skills and high schoollevel science knowledge. Evaluations of factuality and alignment are typically grounded in general content. In contrast, sciencefocused LLM benchmarks require mastery of the depth, precision, and rigor characteristic of academic research. Beyond the general dimensions listed above, scientific LLMs must be evaluated on their ability to engage with domain-specific scientific knowledge, reason with formal systems (e.g., equations, symbolic logic), retrieve and synthesize scholarly information, and support hypothesis generation or experimental design. 1) Expert-Level Scientific Knowledge Comprehension and Retrieval: Unlike general-purpose language models, scientific AI models must retrieve, comprehend, and apply cutting-edge research knowledge across diverse scientific disciplines with domain-level expertise. This knowledge extends beyond general encyclopedic facts to include domain-specific equations, physical constants, technical terminology, and theoretical coninterpret, and reason structs. models ability to access, over external academic knowledge is critical dimension of evaluation, serving as cornerstone for enabling automated scientific discovery. Key evaluation aspects include information retrieval, literature-based fact verification, and the integration of heterogeneous scientific knowledge. For example, SciBench [441] introduces benchmark tasks requiring the retrieval of mathematical equations, chemical laws, and physical theorems; SciKnowEval [442] spans domains from biology to materials science, assessing tasks such as molecule identification and reaction prediction; and SciQA [107] leverages the Open Research Knowledge Graph to support complex crossdomain scientific questions. This dimension challenges models on both the breadth and depth of scientific understanding, emphasizing accuracy, completeness, and the ability to engage with knowledge beyond surface-level facts. 2) Scientific Reasoning and Problem Solving: Scientific problems often require multi-step reasoning rooted in the principles of the scientific method. Effective models must be capable of formulating and decomposing complex problems, applying relevant scientific laws and theories, and performing precise numerical computations. SFE [443], for example, emphasizes advanced reasoning skills of Sci-LLMs, including the evaluation of scientific attribute understanding and comparative analysis. Error analyses of science-focused benchmarks reveal that key reasoning capabilities include logical decomposition, causal inference, deductive problem solving, and abstract reasoning. These tasks extend beyond the scope of general mathematical puzzles found in standard LLM benchmarks, demanding the ability to reason about experimental procedures, derive theoretical formulas, and interpret results within scientific framework. 3) Multimodal Scientific Data: Science AI models should incorporate various modalities other than language. The ability to understand data diagrams, including figures and tables, and to conduct quantitative and statistical analysis to identify scientific trends, is crucial. Furthermore, expert AI models need to comprehend specialized scientific data that requires domain-specific knowledge, such as chemical structures and laboratory images, for high-level reasoning. SciBench [441] notably includes multimodal subset with figures and graphs, highlighting that assessing the ability to interpret visual scientific information is dimension beyond typical LLMs and even MLLMs. On the other hand, it remains to be seen whether current science AI models can fully incorporate and leverage all these diverse data types effectively for truly advanced scientific discovery. III. SCIENTIFIC LARGE LANGUAGE MODELS Sci-LLMs are emerging as powerful tools for modeling, understanding, and reasoning across diverse scientific domains. This section begins with brief touch on the architecture and training of general LLMs, establishing the groundwork for their scientific extensions (Sec. III-A), followed by survey of general-purpose Sci-LLMs (Sec. III-B). We then introduce major scientific LLMs across six natural science domains (Sec. III-C), including physics (Sec. III-C1), chemistry (Sec. III-C2), materials science (Sec. III-C3), life sciences 20 Fig. 14: Research scopes of Sci-LLMs across six scientific subjects: physics, chemistry, materials science, life sciences, Earth science, and astronomy. For each subject, we present representative domain-specific Sci-LLMs and example questions that the Sci-LLMs are able to solve. (Sec. III-C4), astronomy (Sec. III-C5), and Earth science (Sec. III-C6), each with unique data modalities, modeling challenges, and scientific applications. Fig. 14 illustrates the research scope of Sci-LLMs covered in this survey. A. Introduction of Large Language Models LLMs [35], [444], [445] exhibit strong capabilities in understanding, generating, and interacting with human language. LLMs can comprehend the intricate relationships among massive amounts of text, sequential, and visual data in queries, and generate corresponding answers following user instructions. Existing LLMs are mainly based on decoder-only transformer architecture [446], which converts human natural language into sequence of textual tokens. When equipped with specific modality encoders, data from other modalities, such as images or videos, can also be converted into tokens and processed by LLMs. Then, LLMs generate or expand information when given an input or condition by extracting relationships between tokens. The generated tokens are then decoded into text or other modalities that humans can understand. To enable such capability, LLMs are usually pre-trained on vast and diverse data using the next-token prediction objective [444]. This process encodes world knowledge into the LLMs and serves as the foundation for their capabilities. The post-training process is crucial for activating and enhancing the task-specific knowledge that LLMs acquire from the largescale data, allowing them to understand user instructions and solve complex tasks in practical applications. B. General-purpose Sci-LLMs Current scientific LLMs are mainly developed from existing general-purpose LLMs through post pre-training or finetuning on data from specific scientific tasks [447]. They do not alter the model architecture of existing LLMs. Instead, domain-specific encoders are used to convert scientific data, such as medical images and protein sequences, into tokens compatible with the LLM backbones. Fig. 15 demonstrates the architecture of LLMs in the scientific domain. They can achieve significant performance improvement on certain scientific tasks, but they cannot push the capability boundary of existing LLMs due to the limited data scale and task diversity. For example, DARWIN models [448] are fine-tuned on the open-source LLaMA-7B [34] using about 60 sciencefocused instruction examples covering physics, chemistry, and materials science. These instructions are carefully collected from science exams and scholarly papers. SciGLM [41] is further fine-tuned from general-purpose LLM with the proposed SciInstruct dataset, which enhances the models ability to understand intricate scientific concepts, derive symbolic equations, and solve numerical problems. SciInstruct is built through self-reflective annotation to alleviate data scarcity in science domains such as Physics and Chemistry. However, directly fine-tuning open-source LLMs does not significantly enhance their scientific capabilities because of limited training corpus. Therefore, to improve performance on scientific tasks, model should be pre-trained on largescale scientific data, thereby strengthening its capabilities for scientific tasks. For example, Galactica [30] is 120 parameter decoder-only model trained on 106 tokens drawn from papers, reference materials, encyclopedias, and other scientific sources. Its corpus mixes text with scientific sequence representations such as protein sequences and chemical formulae, as well as LaTeX and code. At release, Galactica reported state-of-the-art results on PubMedQA [449] and MedMCQAdev [450] and strong performance on mathematical reasoning and technical knowledge probes, as well as chemistry, biology and physics capabilities SciDFM [451] adopts MoE [48] architecture with 5.6 active parameters routed across eight is pre-trained from scratch on 300 scienceexperts. It Fig. 15: Illustration of common model architectures for existing scientific large language models. (a) Left: Text-only language model architecture showing the processing pipeline where user queries are processed through text tokenizer, with scientific text inputs (including disease descriptions, DNA/RNA sequences, protein sequences, and SMILES molecular representations) as part of the query, to generate responses. (b) Right: Multimodal model architecture featuring domain-specific encoder that processes diverse scientific data types (molecular structures, DNA structures, microscopic images, etc.) alongside text inputs, enabling comprehensive scientific question-answering capabilities through the integration of textual and non-textual scientific information. domain tokens covering mathematics, chemistry, biology, geography, and general science, together with 270 generaldomain tokens. This broad training corpus strengthens the models scientific capabilities. SciDFM is then fine-tuned on customized instruction-tuning data derived from open-source datasets to improve its performance on downstream scientific benchmarks. OmniScience [452] is built on the LLaMA-3.170B model and undergoes domain-adaptive pre-training on carefully curated corpus of papers, journals, and textbooks that span general science and electrochemistry. The model is then instruction-tuned to improve its understanding of sciencespecific task prompts. Finally, OmniScience distills knowledge from the advanced reasoning model DeepSeek-R1 by finetuning on the s1K-1.1 dataset [453], thereby gaining multistep reasoning capability for complex scientific problems. Intern-S1 [47] is recently released open-source scientific multimodal foundation model developed by Shanghai AI Laboratory. It adopts MoE architecture with 241B total parameters and 28B activated per inference step. The language backbone is based on Qwen3-235B MoE and is extended with specialized encoders, including InternViT-6B for vision and time-series signal encoder, together with dynamic tokenizer designed for scientific formats such as SMILES and FASTA. Trained on over 5 trillion tokens, including 2.5T from scientific domains, Intern-S1 delivers competitive performance on general reasoning tasks while surpassing both openand closed-source systems across multiple scientific benchmarks, such as molecular synthesis planning, materials property prediction, and crystal stability. paradigm demonstrates potential in solving complex scientific tasks, which require reasoning from multiple perspectives and drawing accurate and interpretable conclusions [455]. To achieve this, recent work tries different approaches. For example, DeepSeek-R1 [456], built upon DeepSeek-V3-Base through cold-start training and reasoning-oriented Reinforcement Learning (RL) processes, achieves results comparable to OpenAI-o1 model [454] on scientific benchmarks such as MMLU-Pro [81], and GPQA-Diamond [457]. Qwen3 [458] draws inspiration from DeepSeek-R1 and is designed with MoE architecture. It integrates both thinking and nonthinking modes into unified framework, allowing the model to respond adaptively based on task difficulty to avoid unnecessary computational overhead compared to DeepSeek-R1. Kimi K2 [459] scales the MoE architecture to 1 trillion parameters with 32 billion activated parameters. K2 undergoes multistage post-training process which is powered by the proposed large-scale agentic data. Therefore, K2 can interact with real and synthetic environments using diverse tools, demonstrating its potential for addressing complex scientific tasks. Gemini 2.5 Pro [460] is reasoning model that can process multimodal and long-contextual data. It can handle text, image, video, and audio inputs with total context length of 1 million tokens. This capability makes it well-suited for complex scientific tasks that involve processing sensor or sequence data from different devices or databases. Grok 4 [461] is trained with large-scale RL at pre-training scale, achieving 50.7% accuracy on the Humanitys Last Exam (HLE) [462] and demonstrating strong scientific reasoning capabilities. Recently, beyond large-scale pre-training, existing generalpurpose LLMs [454] propose test-time scaling by introducing Chain-of-Thought reasoning process [419]. Such The test-time scaling strategy demonstrates strong potential for enhancing generalization in scientific tasks by understanding multimodal scientific data and reasoning with scientific Fig. 16: Chronological overview of notable Sci-LLMs categorized by six scientific domains, spanning from 2019 through early 2025. Due to the rapid expansion of the field, this figure presents selective overview. For detailed information, please refer to Tab. VII. tools. In the future, further scaling up the RL process could lead to frontier intelligence surpassing human capabilities, enabling novel scientific discoveries and allowing models to design and conduct experiments using real-world tools in support of the proposed hypotheses. Moreover, developing virtual laboratory environment [54] where LLMs can conduct experiments and collect experimental feedback would accelerate the training process toward more powerful general-purpose scientific intelligence. C. Domain-specific Sci-LLMs In some cases, domain-specific scientific LLMs can be more helpful for particular scientific tasks. Such models can be constructed with well-curated, domain-specific datasets and training schemes tailored to the target subject. Below, we introduce recent domain-specific scientific LLMs that cover eight subjects. Fig. 16 demonstrates the development of scientific LLMs across six subjects. 1) Physics: In the field of physics, scientific LLMs have begun to take significantly different path compared to traditional symbolic modeling and numerical simulation. By integrating LLMs with physics engines and visual modules, these models are not only capable of processing natural language descriptions of physical systems but also able to explicitly estimate physical parameters, simulate dynamic evolution, and represent physical laws symbolically. They are evolving from language understanding systems into intelligent tools that interact directly with scientific workflows. LLMPhy [463] is representative model that combines program synthesis with physics simulation feedback. Its core idea is to use an LLM to generate symbolic code that can be executed by non-differentiable physics engine, enabling iterative refinement of physical parameters like friction, stiffness, damping, and rotational inertia. In Phase 1, the system uses trajectories extracted from auxiliary videos, while in Phase 2, it processes multi-view images with vision-language model to reconstruct the scene layout. The TraySim dataset, which provides paired multi-view images and video trajectories, supports closed-loop analysis-by-synthesis framework that allows the model to align simulation results with physical reasoning. POSEIDON [464] is model designed for learning solution operators of partial differential equations (PDEs). While it does not use natural language as input, its core is scalable Operator Transformer with strong temporal modeling capabilities enabled by time-conditioned layer normalization. The architecture uses multi-scale Vision Transformer [465] to encode spatial fields, and U-Net-style module to encode input into latent space. The transformer backbone, along with time-conditioned layer normalization, enables continuous-intime evaluations, with optional autoregressive rollouts during inference. It is trained in two stages: large-scale pretraining using an all-to-all strategy on PDE trajectories (e.g., Euler, Navier-Stokes), and small-sample fine-tuning on specific PDE tasks. POSEIDON achieves high sample efficiency, requiring only 20 samples to match the performance of the widely-used FNO that needs 1024 samples. Xiwu [466] is domain-specific model for high energy physics (HEP), built on Vicuna-13Bv1.5. Its system includes data engine, LLM core, vector memory, and user-facing interfaces. It is trained in three stages: continued pretraining on 750M HEP-specific textual tokens, supervised fine-tuning on 26k human-verified QA pairs, and real-time learning via just-in-time system where expert users can inject, correct, or update knowledge in vector storage for retrieval. Xiwu outperforms Vicuna-13B in 95% of win-ordraw rate and surpasses GPT-4 in most tasks involving HEP 23 software code generation for BESIII Offline Software System(BOSS) [467], demonstrating its domain specialization. In summary, these models reflect the diverse design and training strategies adapted for physics tasks. They range from symbol-to-simulation systems and PDE operator learners, to physics QA transformers and high-energy physics retrievers. As they evolve, further integration of multimodal capabilities, improved spatiotemporal reasoning, and unified knowledge representation frameworks will be essential for expanding their scientific utility. 2) Chemistry: In this section, we review the latest advances in LLMs in the field of chemistry. We begin by examining their model architectures, training strategies, and the core data modalities employed, such as molecular structures, reaction data, spectroscopic information, and scientific literature. Next, we provide comprehensive, domain-specific overview of their applications across key areas in chemistry, including molecular design, reaction prediction, retrosynthetic analysis, catalyst discovery, quantum chemistry, and materials science. Finally, we critically discuss the major challenges and ethical considerations in the field, and offer perspective on future research directions and opportunities for AI-driven chemical innovation. ChemLLM [20] is one of the earliest LLMs that is specifically designed for chemistry. It also curates ChemData, specialized instruction-tuning dataset, and ChemBench, comprehensive benchmark covering nine core chemistry tasks. InstructMol [468] aligned molecular structure and text via using light-weighted projector, following LLaVAs alignment strategy. It leverages two-stage training scheme which starts with the multimodal alignment object followed taskspecific instruction tuning. InstructMol supports several tasks, including molecule property prediction, molecule description generation, retrosynthesis prediction, etc. ChemDFM [469] is pre-trained on 34 billion tokens from chemical literature and textbooks, and fine-tuned with 2.7 million instruction pairs. As result, ChemDFM is capable of understanding and reasoning over chemical knowledge through natural, free-form dialogue. ChemMLLM [200] is proposed to mitigate the gap in generating molecular images, establishing unified MLLM for chemical understanding and generation across text, molecular SMILES string, and molecular images. Chem3DLLM [470] addresses the inability of traditional large language models to generate accurate 3D molecular conformations due to incompatible formats, lack of multimodal alignment, and absence of chemical priors. It introduces reversible text encoding of 3D structures, enabling lossless compression token space. and integration within language-model protein-embedding projector aligns protein pocket representations, while reinforcement learning with chemical validity rewards enforces physical plausibility, yielding state-of-the-art results in structure-based drug design. 3) Materials Science: LLMs have also been widely explored in diverse tasks in materials science. Recent studies have applied transformer-based encoder to learn material representations. For example, SMILES-BERT [471] is pretrained on vast collection of SMILES corpora to learn molecular representations for property prediction. Similarly, polylearning latent compositional BERT [472] employs DeBERTa-based encoder [473] trained on about 100 million hypothetical polymer SMILES, enabling end-to-end polymer fingerprinting. MatBERT-bandgap [474] was pre-trained on about 2 million materials science abfeatures. Regression stracts, Transformer [475] adopts novel multi-task scheme, translating property regression into sequence outputs by tokenizing continuous values and alternating masked-language and regression training phases. Regression Transformer can simultaneously predict numeric properties and generate molecular strings, effectively merging regression and generation tasks. Some work applies general-purpose LLMs by utilizing Retrieval-Augmented Generation (RAG) equipped with professional databases to solve related tasks. Knowledge integration is another strength. Qwen2-KG [476] uses Qwen2-72B together with retrieved materials knowledge graph to answer questions about framework materials. By combining chain-ofthought retrieval with graph facts, it achieves about 91.7% accuracy on held-out QA benchmark, outperforming LLMonly baselines and providing cited sources. Recent work has investigated fine-tuning ChatGPT-style model (i.e., decoder-only transformer) to materials science. For example, MolXPT [477] is built on GPT-2 [478] and learns from combined PubMed abstracts and molecular SMILES data, while GPT-MolBERTa [479] is fine-tuned from BERTlike encoder on about 326K molecular descriptions synthesized by ChatGPT. In the molecular domain, MolGPT [480] uses GPT-style causal LM objective: it is pre-trained on millions of SMILES strings and then fine-tuned for conditional generation (scaffold or property guidance). XYZTransformer [481] is designed to process molecular structural data and is decoder-only transformer trained directly on the raw 3D coordinates of molecules. CrystaLLM [482] is fine-tuned from the LLaMA-2 model using text-formatted crystal structures, harnessing billions of parameters to capture atomistic symmetries. CrystaLLM can generate metastable materials with frequency of about 49% when given desired features, and significantly outperforms diffusion-based models. In synthesis planning, Okabe et al. develop three LLMs (LHS2RHS, RHS2LHS, TGT2CEQ) [483] to predict chemical equations: given reactants they predict products (LHSRHS) or vice versa, and they can generate balanced chemical equations for target compounds. Fine-tuning on text-mined inorganic syntheses raised reaction-prediction accuracy to about 90%, enabling rapid synthesis route inference. CSLLM [484] is fine-tuned from LLaMA-3-8B to predict the synthesizability and precursors of crystal structures. CSLLM reaches approximately 98.6% accuracy on the synthesizability prediction task, which is vastly higher than that of Density Functional Theory (DFT)-based filters, for identifying experimentally realized crystals. CSLLM can also predict the likely precursors and synthesis methods (solid vs solution), illustrating how LLMs can capture complex experimental domain knowledge. MechGPT [485] is tailored for material mechanics and multiscale modeling. It is built on the LLaMA2 model and fine-tuned using LoRA techniques with domain-specific question-answering (QA) data. Although its current inputs are text-based, the model is designed to eventually incorporate 24 image and structural modalities. Its demonstrated capabilities include knowledge retrieval, hypothesis generation, and the construction of interpretable ontological knowledge graphs for structural insight. While MechGPTs input is currently text-only, its architecture is designed to accommodate future multimodal extensions. 4) Life Sciences: LLMs, pre-trained on large-scale scientific data in the field of life sciences, can adapt to wide spectrum of downstream tasks, ranging from generating accurate diagnostic reports to designing previously unknown protein structures or novel drugs [21]. These tasks are closely related to human health. In this part, we review the development of LLMs in the field of life sciences, including model architectures, training schemes, and applications. Multi-Omics. DNA, RNA, and protein sequences have been seen as the language of life in computational biology in recent years [486].Recent advances in multi-omics research have developed two complementary families of domain-specific language models: (i) encoder-centric genomics/protein language models (GLMs/PLMs) that are trained from scratch on biological sequences to learn molecular representations and biological constraints like the EVO series [487], [488] and ESM series [489][491]; and (ii) LLM-augmented systems that integrate omics data into instruction-following text LLMs to generate natural-language outputs, typically leveraging models from category (i) as omics encoders. For category (i), EVO [487] represents groundbreaking advancement in genomic foundation modeling, being trained on an extensive dataset comprising over 80,000 bacterial and archaeal genomes, as well as millions of predicted phage and plasmid sequences, totaling 300 billion nucleotide tokens. This model establishes scaling laws for DNA that complement those discovered in language and vision domains. Additionally, EVO seamlessly integrates across DNA, RNA, and proteins, achieving zero-shot function prediction that rivals specialized language models. EVO2 [488] scales training to 9.3 trillion DNA bases spanning all domains of life and extends context to genome scale (up to 1M tokens), accurately predicting functional impacts of genetic variation and supporting genomelevel design. Focusing on proteomics foundation models, the methodological landscape is evolving from unconditional sequence modeling toward closed loop of controllable generationcross-modal semantic alignmentinteractive reasoning: at the outset, ESM-1b [489] demonstrates that largescale unsupervised modeling of 250M protein sequences learns representations with emergent structure/function information enabling accurate long-range contact prediction and remotehomology organization. MSA Transformer [492] applies axial attention directly to multiple-sequence alignments to capture coevolutionary dependencies, yielding unsupervised structural features and strong contact-prediction signals. ESM1v [493] introduces protein LM whose zero-shot likelihoods match state-of-the-art supervised predictors on deep mutational scanning benchmarks for functional effect prediction. ESM-IF1 [494] performs inverse folding by training on millions of predicted backbones, achieving 51% native sequence recovery (72% for buried residues) on held-out structures and generalizing to complex design settings. ESM2/ESMFold [495] enables direct single-sequence, atomic-level 3D structure prediction without MSAs, delivering strong accuracy at substantially higher speed than traditional MSAbased pipelines. ESM3 [491] unifies sequence, structure, and function in single multimodal generative model that reasons across modalities and designs novel, functional proteins far from known families. ProtGPT2 [496] learns the grammar of proteins via large-scale unsupervised training, enabling de novo generation close to natural sequence statistics; on controllability and scale, ProGen [497] injects functional and localization conditions into autoregressive modeling, and ProGen2 [498] further expands parameters and corpora to improve generalization and fitness prediction; along the path from general LLMs to domain adaptation. Ankh [499] attains strong baselines under lower compute through protein-oriented training and architectural optimizations; centering on textdriven design, As instances of (ii), LLaMA-Gene [40] adapts LLaMA2-7B via domain-adaptive continued pretraining on 39.5B-token DNA corpus from reference genomes, followed by instruction tuning with 800K synthetic multi-omics QA pairs (variant interpretation, promoter prediction, transcript identification). ProLLaMA [500] migrates general models to protein multitasking via vocabulary pruning and instruction alignment. ProteinDT [501], PAAG [502], and Pinal [503] map naturallanguage intent to controllable sequencesrespectively via text-protein alignment, annotation-sequence multi-level domain alignment, and two-stage pipelineand support sequence editing; for interactive analysis, ProteinGPT [504] and ProteinChat [39]/ProtChatGPT [505] align sequence/structure representations with LLMs to enable multi-turn QA around function and structure; in cross-modal translation, ProTranslator [506] and BioTranslator [507] achieve zero-shot transfer between text and protein/biological data (text protein/data); to enhance interpretability, Prot2Text [508] directly generates free-text functional descriptions from sequences. Evolla [509] is protein-language generative model with 80 billion parameters, designed to decode the molecular language of proteins by integrating sequence and structural information on proteins, together with user-query information. This capability is enabled by the proposed 546 million protein-centric questionanswer pairs. Taken together, this line of work progresses from learning the protein grammar to conditional controllable generation and onward to cross-modal alignment and dialogue-centric agents, converging toward design-analysisfeedback loop for proteomics foundation models. Recent work proposes to understand DNA, RNA, and protein sequences simultaneously. NatureLM [43] presents general Sci-LLM instruction-tuned across genomics, proteomics, biochemistry, and materials science. Its post-training data comprises over 1.1M instruction pairs generated from curated databases such as UniProt, Ensembl, and GenBank, spanning protein functions, gene regulatory elements, and variant effects, formatted in English QA and reasoning chains. ChatNT [510] further establishes multi-task conversational trained on curated instruction datasets across DNA, agent RNA, and protein domains. It integrates 361M English and DNA tokens from 18 task categories (e.g., methylation, splic25 ing, polyadenylation, protein melting), and uses unified textto-text objective with an English-aware Perceiver projection to align genomic sequences with natural language prompts. Collectively, these models highlight the shift toward crossomics instruction tuning that enables unified biological reasoning across diverse molecular inputs. Molecular and Cellular Biology. Some studies propose applying LLMs in the field of molecular and cellular biology. These LLMs focus on understanding the morphology and function of cells in living organisms. For example, MolecularGPT [511] is an instruction-tuned large language model (LLaMA-2-7Bbased) for molecular property prediction that operates on SMILES strings, enabling zero or few-shot inference across diverse biological molecules. It is obtained by QLoRA fine-tuning on hybrid instruction set spanning over 1,000 property tasks compiled from sources such as ChEMBL bioassays, CHEMBL physico-chemical properties, and QM9 (HOMO/LUMO), with about 3.5 GB of training tokens. scGPT [512] is transformer-based singlecell foundation model with specialized masked-attention scheme that jointly learns gene and cell embeddings to support cell-type annotation, batch correction, perturbation-response prediction, and gene network inference. It is pretrained in self-supervised manner on over 33 million normal human cells from the CELLxGENE atlas and then adapted via taskspecific fine-tuning pipelines for diverse downstream singlecell applications. LLMs have also emerged as powerful tools for de novo molecular design. By treating chemical structures as languages (e.g., using SMILES notation), models like ChemBERTa [513] and MolBERT [514] generate novel molecules with desired properties. For instance, Edwards et al. [201] fine-tuned GPT-3 on chemical datasets to produce drug-like molecules, achieving hit rates comparable to high-throughput screening. In drug discovery, LLMs accelerate lead optimization. They predict bioactivity by analyzing sequence data from proteins and ligands. notable example is the integration of LLMs with reinforcement learning in models like Chemformer, which designs molecules for specific targets, such as COVID-19 inhibitors [515]. These approaches reduce synthesis trials by 50-70%, as validated in virtual screening benchmarks. Healthcare and Medical Science. Recent LLMs in the field of healthcare and medical science are primarily adapted from existing general-purpose LLMs [516]. These models are typically further pre-trained on domain-specific corpora, such as clinical reports, medical literature, and imaging data. They are then fine-tuned with medical instruction-response pairs to serve diverse user groups, including doctors, students, and patients. Due to computational costs, recent medical LLMs only perform supervised fine-tuning (SFT) on general LLMs using medical-related instruction data. This process introduces the capability to solve medical tasks to general LLMs. For example, BioMistral [517], BioMedLM [518], ClinicalCamel [519], and MedAlpaca [520] collect medical question-answering pairs and doctor-patient dialogue data, and perform SFT on open-source LLMs such as LLaMA, achieving performance improvements on several medical benchmarks, such as MedMCQA [450], PubMedQA [449] and MedQA [521]. Med-PaLM series [31] are developed from 540B parameter LLM, PaLM, are directly instruction-tuned on PaLM, and using combination of prompt engineering technologies [522] to adpat to medical quesiton-answering tasks. Apollo [523] is lightweight multilingual medical LLM, which collects medical data covering the six most widely spoken languages. Such lightweight model can be deployed in hospitals to help protect the privacy of medical data. HuatuoGPT [524] is fine-tuned from general LLMs using medical instruction and conversation data from both real-world sources and ChatGPT, in order to introduce medical-specific skills and to distill capabilities from powerful general LLMs. Only performing SFT on existing general LLMs cannot further improve model performance in the healthcare field. Further scaling up the pre-training scale is beneficial to model performance. For example, PMC-LLaMA [525] is based on LLaMA and was pre-trained on data containing 4.8 million biomedical academic papers and 30,000 medical textbooks. HuatuoGPT-II [42] combines pre-training and instruction tuning, using over 5.2 million medical documents from encyclopedias, books, and web corpora, as well as 142,000 medical instructions. It is based on the Baichuan2-Base models. CHIMED-GPT [526] collects over 214 million multilingual tokens in Chinese and English from medical textbooks and encyclopedia data, and is pre-trained on Ziya-13-v2 [527]. This work also conducts RLHF [528] to further enhance the safety of the models responses. Zhongjing [529] also conducts complete training process including pre-training, instructiontuning and RLHF. Besides, Zhongjing supports multi-turn dialogues to meet real-world diagnosis requirements. MeLLaMA [530] is continually pre-trained on LLaMA2 with 129 billion tokens from biomedical datasets, research papers, and clinical notes, and is then fine-tuned on 214,000 instruction tuning samples from clinical domains. Baichuan-M1 [531] is trained from scratch and further scales up the pre-training process, using over 20 trillion tokens, which include both general data and medical-related data such as clinical information and patient records. Baichuan-M1 achieves significant performance across more than 17 medical-related benchmarks. Clinical practice is inherently multimodal. The diagnostic process requires physicians to synthesize information from including the patients verbal descriptions diverse sources, (text/audio), physical signs (visual), medical imaging (visual), and laboratory findings (structured data). Accordingly, MLLMs capable of processing multiple data modalities are considered critical path forward in the evolution of medical AI [532]. Recent work investigates the use of MLLMs in the medical field, mainly focusing on two primary tasks: medical reports generation [533] and medical Visual Question Answering (VQA) [534], [535]. LLaVA-Med [536], as pioneering work in this domain, successfully transferred the capabilities of general-purpose MLLM, i.e., LLaVA, to the biomedical field. It is fine-tuned on the visual instructiontuning data from PubMed papers and can understand medical images. CXR-LLaVA [537] and Radiology-LLaMA2 [538] are specifically developed for chest X-ray (CXR) imaging. They 26 utilize GPT-4 to extract impressions and findings from radiology reports in order to enhance their ability to interpret X-ray images, and they can generate reports in clinical style. Med-Flamingo [539] is continually pre-trained on paired and interleaved medical image-text data from publications and textbooks, and can solve medical VQA tasks through few-shot learning without further fine-tuning on the VQA datasets. Moreover, several works aim to extend the medical MLLMs capability to diverse medical tasks requiring more modality information and reasoning capabilities. HuatuoGPT-Vision [540] and GMAI-VL [541] collect large-scale medical multimodal data from PubMed papers and open-source medical image datasets. They are pre-trained on extensive medical imagecaption pairs and further fine-tuned on data containing diverse instructions in the medical field. Therefore, they can solve wide range of tasks from different departments. MedGemma [542] further extends the in-context length of MLLMs and can process long-context data such as medical videos or patient electronic health records. HuatuoGPTo1 [543] aims to introduce complex medical reasoning capability by fine-tuning the model on question-answer pairs with complex reasoning trajectories and conducting RL with verifier-based rewards to enhance complex reasoning. Medground-r1 [544] leverages GRPO with spatial-semantic rewards to enhance medical image grounding without CoT annotations. GMAI-VL-R1 [545] introduces multimodal medical reasoning capability by directly applying RL to verifiable multiple-choice VQA data, thereby enhancing performance on medical image diagnosis and VQA tasks without collecting complex reasoning data. Agriculture. In this section, we examine the emerging family of agricultural LLMs, covering their architectural choices, training strategies, and domain-specific capabilities. SeedLLM [546] is domain-specific large language model for seed science, built on Qwen2.5-7B. It is pre-trained on RiceCorpus (a bilingual corpus of 1.38 million agronomy papers) and GeneralCorpus [547], [548], targeting terminology and knowledge from modern breeding research. The finetuning stage uses QAs from both general and agricultural domains, synthesized using GraphGen [549], knowledgegraph-based generation framework. SeedLLM is evaluated on SeedBench, multi-task benchmark co-designed with domain experts for seed breeding applications. The model remains closed-source. PLLaMA [550] is an open-source language model tailored for plant science. It extends LLaMA-2 with 7B and 13B parameter variants, and is continuously pre-trained on 1.5 million plant-related scholarly articles curated from the S2ORC corpus. Fine-tuning employs 1,030 instruction samples adapted from LIMA. The model is evaluated on held-out plant science quiz set, showing strong comprehension of plant genetics, physiology, and breeding concepts. AgroGPT [551] is an open-source multimodal assistant for agronomic consultation, with 3B and 7B vision-language variants based on LLaVA. While no raw pretraining corpus is used, AgroGPT is fine-tuned on AgroInstructa dataset of 70k synthetic QA pairs created from agricultural images using LLM-generated captions and instructions. It is evaluated on AgroEvals, domain-specific benchmark for fine-grained crop disease and pest identification. AgroGPT demonstrates superior performance over generalist models and human baselines in image-based agronomic reasoning. Neuroscience. Recent advances in LLMs for neuroscience have integrated both neuroscience literature and neural data from multiple modalities such as EEG and fMRI to improve interpretability and performance on brain related tasks. BrainGPT [552] is domain specialized language model for neuroscience, fine tuned from Mistral 7B using low rank adaptation on 1.3 billion tokens from neuroscience literature. Evaluated on BrainBench, benchmark for neurosciencerelated question answering, BrainGPT outperformed both general models and human experts. EEG-GPT [553] is domainspecific LLM based on OpenAIs GPT-3 (da Vinci), designed for EEG classification and interpretation. It achieves strong few-shot performance using only 2% of training data and employs tree-of-thought reasoning with specialist EEG tools for interpretable, step-wise decision-making. NeuroLM [554] is multi-task foundation model that integrates EEG signals into language modeling framework. It trains vector-quantized tokenizer to convert EEG data into discrete neural tokens, and fine-tunes GPT-2 [478] language model with multichannel autoregression and instruction tuning. The model is evaluated on neural decoding tasks including sleep stage classification, epilepsy detection, motor imagery decoding, and emotion recognition, demonstrating that incorporating neural representations significantly enhances brain signal analysis. UMBRAE [555] unifies multimodal brain decoding by aligning fMRI signals with pretrained CLIP [556] visual features via universal brain encoder. Cross-subject training promotes subject-agnostic representations, which are connected through adapter modules to Vicuna-7B/13B-based multimodal language model for semantic captioning and spatial grounding. MindGPT [557] is GPT-2based model that decodes visual stimuli from non-invasive brain recordings into natural language. It integrates CLIP-guided encoder with crossattention mechanisms to align brain, visual, and linguistic representations, enabling accurate semantic interpretation of visual experiences. MindLLM [558] is subject-agnostic model for fMRI-to-text decoding that combines neuroscienceinformed encoder with Vicuna-7B. Trained via brain instruction tuning, it supports wide range of tasksincluding perception, memory retrieval, symbolic language processing, and reasoningachieving flexible and accurate semantic interpretation of brain activity. UniMind [559] is generalpurpose EEG foundation model that leverages InternLM2.5 to unify multi-task brain decoding by bridging the modality gap between neural signals and language representations. It introduces Neuro-Language Connector to distill spatiotemporal EEG patterns into LLM-interpretable embeddings and employs Task-aware Query Selection mechanism for adaptive taskspecific decoding, achieving robust performance across diverse EEG tasks without task-specific fine-tuning. Neuro-GPT [560] is built on the open-source GPT-2 model, combined with convolutional-transformer EEG encoder trained using selfsupervised learning. It reconstructs masked EEG segments from large-scale clinical data and demonstrates strong generalizability in downstream motor imagery classification tasks. 27 5) Astronomy: In this section, we review recent advances in astronomy-specific LLMs, highlighting representative models such as AstroLLaMA [561], AstroLLaVA [562], and AstroSage [563]. These models are generally built upon LLaMA2 or LLaMA-3 architectures, with LLMs focusing on text understanding and generation, and MLLMs incorporating visual encoders (e.g., CLIP ViT-L/14) and projection layers to integrate astronomical images with text. Most models follow two-stage training strategy: continual pre-training (CPT) using large-scale astronomy literature (e.g., arXiv abstracts, Wikipedia, textbooks) to enhance general domain understanding, and SFT using domain-specific tasks, such as question answering, multiple-choice reasoning, and synthetic dialogue generation. Low-Rank Adaptation (LoRA) [564] and other parameter-efficient tuning methods are commonly used for resource-effective adaptation. These developments lay the foundation for new generation of astronomy-focused models, which we detail below in terms of their architectures, training pipelines, and domain-specific capabilities. AstroLLaMA [561] is an astronomy-specific language model fine-tuned from LLaMA-2. It focuses on traditional language modeling tasks, with text as the modality. The model was finetuned using over 300,000 astronomy abstracts (approximately 95 million tokens) from the arXiv database and employs LoRA to improve resource efficiency. In text generation task, the model was tested by having it produce astronomy-related abstracts. The results showed that AstroLLaMA achieved 32.5% reduction in perplexity compared to LLaMA-2, generating text that was more specific to the astronomy field and possessed deeper insights. Furthermore, AstroLLaMAs embedding space better reflects the semantic differences within astronomical text. Despite issues such as knowledge gaps and the generation of fictitious data, AstroLLaMA outperformed general-purpose models overall. AstroLLaVa [562] is multimodal visual-language model for astronomy that combines images and text. Built on the LLaVA 1.5 architecture, its visual encoder uses the CLIP ViT-L/14 model, and its language model is based on LLaMA 7B. Fine-tuning data is sourced from publicly available images and captions from NASAs Astronomy Picture of the Day (approximately 9,962 imagetext pairs), the European Southern Observatory (approximately 14,617 image-text pairs), and the NASA/ESA Hubble Space Telescope (approximately 5,204 image-text pairs). GPT-4 is used to generate synthetic dialogue dataset from the image captions. Training utilizes two-stage fine-tuning strategy: in the first stage, only the visual-language projection layer is trained using astronomical image-text pairs, with the pretrained visual encoder and language model fixed. In the second stage, synthetic astronomy question-answer pairs are used for instruction tuning, resulting in end-to-end fine-tuning of the entire model. The evaluation used the Galaxy 10 DECaLS dataset [565]. The model was tasked with describing galaxy images from the G10 test set. The results show that AstroLLaVA performs slightly better than the LLaVA 1.5 model in the task of describing galaxy images. AstroSage-LLaMA-3.1 [566] is based on Metas LLaMA-3.1 model. Like AstroSage-LLaMA-3.1-8B, this model is trained in two main phases: CPT and SFT. It also employs model merging strategy to combine the strengths of multiple models. However, during the SFT phase, it is fine-tuned using diverse dataset, including the LLaMA-Nemotron-Post-Training Dataset, the OpenHermes 2.5 dataset, and domain-specific QA datasets. Evaluation was performed using the AstroMLab1 benchmark, which consists of 4,425 high-quality, humanverified multiple-choice questions from the Annual Review of Astronomy and Astrophysics paper, which were not included in the training set. The results show that AstroSage-LLaMA3.1 achieved an accuracy of 86.2% without enabling inference mode, surpassing all other open weights and proprietary models tested, proving that domain specialization can significantly improve the performance of the model in specific domain. These astronomy-specific models reflect the increasing maturity and specialization of LLMs and MLLMs in science. With better perplexity, semantic understanding, and strong performance on domain benchmarks, they show the value of targeted pretraining and fine-tuning. 6) Earth Science: The application of LLMs in Earth science is undergoing significant transformation, moving from general-purpose models to highly specialized, domainadapted solutions. This shift is driven by the necessity to handle unique data characteristics, such as immense volume, high granularity, and diverse modalities. The advancements discussed here are rooted in the development of sophisticated, domain-specific datasets and innovative architectural designs tailored for scientific inquiry. foundational challenge in adapting LLMs for scientific domains is the scarcity of high-quality, expert-level instruction data. To bridge this gap, several works have focused on creating specialized text-based datasets. In the field of geoscience, K2 [567] was trained on GeoSignal, the first supervised instruction dataset enabling models to understand and respond to complex queries from geoscientists. Similarly, ClimateChat [568] was built upon the ClimateChat-Corpus, large-scale, high-precision dataset constructed through semiautomated pipeline combining self-QA, web scraping, and self-instruct methods to enhance expertise in climate change topics. For ocean science, OceanGPT [569] leveraged the DoInstruct Framework, which uses multi-agent approach to automatically generate expert-level instructions, overcoming the prohibitive cost of manual annotation. the first In the multimodal domain, the unique characteristics of remote sensing (RS) imagery have necessitated the creation of equally specialized datasets. EagleVision [570] was trained on the proposed EVAttrs-95K, large-scale dataset designed for fine-grained object-level understanding, enabling comprehension and description of intricate object attributes in RS imagery beyond simple classification. EarthMarker [571] was supported by the RSVP dataset, containing approximately 3.65 million multimodal pairs of image-point-text and imageregion-text, enabling nuanced interpretations guided by visual prompts. For pixel-level grounding, GeoPixel [572] was trained on GeoPixelD, which provides over 50,000 grounded phrases and 600,000 object masks, achieving end-to-end segmentation in high-resolution images. To address ultra-highresolution imagery, GeoLLaVA-8K [573] utilized the Background Token Pruning and Anchored Token Selection meth28 (a) LLM vs MLLM ratio. (b) Base model family distribution (Top-K). (c) Parameter size distribution (Top-K). Fig. 17: Statistical overview derived from Table VII. (a) Sci-LLM vs Sci-MLLM counts. (b) Base model family distribution; only top-K are shown. (c) Parameter size distribution (all variants of multi-scale models are counted individually); only top-K are shown. ods, enabling complex dialogue and reasoning on images up to 8K resolution. The scope of these models extends beyond static image analysis to encompass dynamic and multi-source data. EarthDial [574] was trained on EarthDial-Instruct, the largest remote sensing instruction-tuning dataset, comprising over 11 million instruction pairs across modalities like RGB, Synthetic Aperture Radar, and multispectral data, enabling reasoning over diverse Earth observation data. HyperSIGMA [575] unifies HSI interpretation across tasks and scenes, scalable to over one billion parameters. SelectiveMAE [576] dynamically encodes and reconstructs semantically rich patch tokens, thereby reducing the inefficiencies of traditional MIM models caused by redundant background pixels in RS images. RoMA [577] enhances scalability for high-resolution images through tailored auto-regressive learning strategy. Furthermore, TEOChat [578] was powered by the proposed TEOChatlas, the first instruction-following dataset for temporal Earth observation data, making it the first vision-language assistant capable of engaging in dialogues about change detection and time-series analysis. These innovative models, and the specialized datasets that train them, represent significant step toward enabling more dynamic and comprehensive analysis for applications like environmental monitoring and disaster response. D. Sci-LLMs Analysis Our survey highlights key trends in the development of Sci-LLMs. Roughly three quarters of current models are textonly LLMs, while MLLMs comprise only about one quarter (Fig. 17a). This imbalance reflects the dominance of textbased scientific sources (e.g., papers, patents, manuals) and the scarcity and cost of fine-grained multimodal supervision. Where MLLMs emergesuch as in medical imaging, life sciences, or remote sensingthey typically rely on smaller but higher-quality paired datasets that enable stronger crossmodal alignment. Looking forward, as scientific discovery increasingly depends on integrating heterogeneous signals (e.g., astronomy that requires optical, radio, and X-ray observations to confirm cosmic events [579], or climate science that unites satellite images, numerical models, and field reports [580]), the demand for Sci-MLLMs capable of synthesizing diverse modalities will grow. Thus, the current text-centric dominance may gradually give way to balanced multimodal ecosystems, powered by improved dataset curation and efficient alignment techniques. [445], The base-model landscape is now characterized by the primacy of open-source, general-purpose families, with [581] constituting the largest share LLaMA [34], and Qwen [35], [458], [582] close behind, complemented by instruction-tuned derivatives (e.g., Vicuna [583]) and thinner tail of encoder-style models (e.g., BioBERT [25], ESM-2 [490]) that persist primarily in legacy or narrowdomain pipelines (Fig. 17b). Their dominance is explained by mature tooling, stable alignment recipes, scalable parameter ranges, and ultra-large pretraining corpora, which jointly enable low-cost adaptation and strong zero-/few-shot performance. In practice, open-source base models further facilitate rapid adaptation to emerging application scenarios by leveraging newly collected data via supervised fine-tuning (SFT), lightweight parameter-efficient methods, or modest instruction refinement. More broadly, progress is shaped by advances in training data curation and systems integrations, including retrieval-augmented workflows for maintaining upto-date knowledge, high-quality expert QA and protocol-style instruction sets (e.g., DoctorGLM [584], MedAlpaca [520]), targeted generation of challenging examples to improve coverage of rare cases, and the use of structured, tool-supported reasoning with simulators, analysis libraries, or code execution to support verifiable complex reasoning. Across recent public tallies and our own tabulated statistics of released scientific models, parameter sizes in practice skew strongly toward smaller scales: 7B models constitute the largest share, 13B models are also frequent, while 70B-and-above models remain comparatively rare (Fig. 17c). This distribution reflects multiple deployment constraints, including privacy and compliance requirements (e.g., HIPAA, GDPR) [585], inference latency and operational cost, the need for determinism and reproducibility, as well as on-premise, air-gapped, or data-sovereign environments. Many scientific 29 tasks, such as protein folding, gene expression modeling, and materials discovery, are knowledge-dense yet narrow in scope, where small-to-mid sized models (7B13B), when paired with retrieval augmentation or fine-tuning on scientific corpora, often achieve competitive performance relative to much larger counterparts [24]. Preferences for such models also mirror practical considerations: limited compute and memory in academic/lab settings, energy constraints, restricted access to sensitive datasets, and the complexity of deploying very large systems in regulated domains [586]. Looking ahead, as hardware efficiency (e.g., distributed training, mixed precision, memory optimization) and privacy/governance tooling advance, very large models are expected to play greater role on the centralized training side, serving as knowledge sources, data generators, and evaluators. Nevertheless, distilled or quantized 7B13B models are likely to remain the backbone for local and resource-constrained deployments, including in hospitals, laboratories, and field-deployed systems (e.g., satellites or environmental sensors) [587]. These trends and drivers may vary across disciplines and institutions, and shares should always be interpreted with respect to the specific datasets and benchmarks at hand. From datatask interface perspective, several emerging themes are shaping the design and application of Sci-LLMs. One promising direction is evidence-grounded generation with traceable provenance, which is essential for credible scientific outputs. Unlike general-purpose LLMs prone to hallucinations, Sci-LLMs are expected to produce verifiable claims with transparent source attribution, with data cards, citations, spatial or experimental coordinates, and retrieval logs serving as key scaffolds for trust and reproducibility [24], [584]. Another challenge and opportunity lies in cross-modal alignment. Highquality supervision (e.g., region-level grounding in remote sensing) consistently yields better results than weakly aligned approaches where images are abstracted into generic embeddings [573], [588]. notable trend is the move toward agentic Sci-LLMs that integrate with scientific tools and workflows. Instead of static question-answering, these models are increasingly capable of retrieving literature, querying databases, running molecular or geospatial simulations, executing code for statistical analyses, and orchestrating lab or field data pipelines. This agentic behavior enables more reproducible and actionable scientific discoveries [520], [584]. Finally, temporal awareness and continual adaptation are becoming increasingly important, since scientific knowledge evolves rapidly. Versioned corpora [589], adaptive retrieval windows, and uncertainty calibration [86] help models remain aligned with the current state of knowledge [586]. These patterns should be viewed not as fixed principles but as recurring observations that point to current bottlenecks and promising frontiers for Sci-LLM research. Overall, the current landscape of Sci-LLMs is characterized less by architectural innovation and more by strategic adaptations of general-purpose foundations to domain-specific needs. The field remains heavily influenced by open-source base models, notably the LLaMA and Qwen families, which dominate due to their scalability, robust tooling, and strong zero-shot generalization. Model size skews toward the 6B13B parameter range, reflecting pragmatic constraints around deployment costs, privacy-compliant inference, and operational efficiency in resource-limited environments such as clinics, labs, and edge devices. Performance gains are increasingly driven by sophisticated data pipelines and workflow integrations rather than pure scaling: knowledge-grounded generation provides verifiable outputs and supports hallucination mitigation [590], [591], tool-assisted reasoning enables executable simulations and code, and high-quality cross-modal alignment supports meaningfully integrated understanding of text, images, structures, and geospatial data. Looking ahead, progress will hinge on improving verifiability and temporal adaptabilityembedding provenance tracking, supporting continuous knowledge updates [590], and refining agentic capabilities for real-world scientific tasks. As multimodal and tool-using paradigms mature, Sci-LLMs are poised to evolve from passive question-answering systems into active participants in the scientific process, facilitating discovery across biomedical, chemical, material, and environmental sciences [592]. IV. SCIENTIFIC DATA FOR PRE-TRAINING Pre-training forms the foundation of LLMs and MLLMs by equipping them with broad, domain-relevant knowledge before task-specific fine-tuning. These models are typically pre-trained on massive and diverse datasets - for example, Yi [593] utilizes data from multiple sources including webpages, code, papers, and Q&A content, while LLaMA [34]s pre-training corpus spans approximately 1.4 TB across various domains such as CommonCrawl [594], GitHub, Wikipedia, and academic sources (Fig. 18a). This extensive scale and broad coverage ensure models acquire comprehensive knowledge across different domains and languages. In the scientific domain, pre-training datasets must capture both the scale and diversity of knowledge, from symbolic laws of physics to complex biological systems and planetary processes. Unlike general web corpora, scientific datasets often combine structured experimental results, simulation outputs, specialized notations, and multimodal formats. The breadth and fidelity of these datasets directly influence models ability to understand, reason, and generate within specific scientific context. Looking at the scientific pre-training landscape, Intern-S1 exemplifies this specialized approach by dedicating 2.5T tokens (45.8% of its total corpus) specifically to scientific content across six domains (Fig. 18b), providing the deep domain knowledge essential for superior performance on complex scientific tasks. In the following subsections, we move from the smallest building blocks of matter (molecules and atoms) through complex biological systems and planetary-scale phenomena, concluding with interdisciplinary datasets that bridge multiple domains. The details of the pre-training datasets are summarized in Tab. IV. A. Physics, Chemistry and Material Sciences: the Foundation for Understanding the Material World Pre-training in physics, chemistry, and materials science focuses on representing the structure, dynamics, and properties 30 (a) Pre-training dataset mixture of LLaMA [34], Yi [593] and GPT-3 [444]. (b) Distribution of continual pre-training data for Intern-S1 [47], involving 5.5T high-quality textual tokens with 2.5T scientific tokens across over six domains. Adapted from [47]. Fig. 18: Pre-training dataset distributions for different language models. (a) Dataset mixture comparison across GPT-3, LLaMA, and Yi models. (b) Detailed distribution of Intern-S1s continual pre-training data with emphasis on scientific domains. of matter. These domains benefit from combination of highfidelity simulations, experimental measurements, and textual corpora that encode formal theories and experimental procedures. The challenge is balancing the precision of synthetic data with the complexity of real-world measurements, while integrating symbolic, numerical, and visual modalities. 1) Physics: frameworks In physics, the pre-training landscape is dominated by large-scale synthetic datasets derived from such as molecular dynamics computational (LAMMPS [595]), finite-element methods, and cosmological simulations (Illustris [596], Bolshoi [597], as well as gridbased hydrodynamics with Enzo [598]). These provide highresolution spatiotemporal fields, wavefunctions, potentials, and other symbolic outputs that are invaluable for surrogate modeling and embedding physics-informed inductive biases. However, their controlled and often idealized nature makes it challenging for models to generalize to noisy or chaotic realworld conditions. Experimental and observational datasets in physics, such as those from particle physics experiments including the European Organization for Nuclear Research (CERN) [599] and the Large Hadron Collider beauty (LHCb) [600], condensed matter platforms including STM [134] and angle-resolved photoemission spectroscopy [601], or large astronomical observatories including Hubble Space Telescope (HST) [197] and Atacama Large Millimeter/submillimeter Array [602], are comparatively scarce in formats readily consumable by machine learning pipelines. The data are often fragmented across specialized repositories, use inconsistent formats, and may be restricted by access policies. Structured, standardized collections remain rare, limiting their use for large-scale pretraining. Efforts such as the Galactica simulation database [603] llustrate move toward open, FAIR-compliant dissemination of astrophysics data. By centralizing metadata and reducing datasets from diverse simulation projects, Galactica covers cosmology, galaxy formation, and the interstellar medium, and supports generation of standardized, API-accessible high-level products (e.g., 1D profiles, 2D maps, 3D cubes). Although not yet matching the sheer scale of Illustris or Bolshoi, it contributes critical data diversity for building more generalizable physical science foundation models. On the textual side, corpora such as SciBERT [24], pretrained on 1.14M full-text scientific papers (including physics literature), underscore the importance of domain-relevant language pre-training. Multimodal datasets like Multimodal ArXiv [604] which comprises 6.4M figure-caption pairs and 100K figure-based QA pairs, bridge visual and symbolic scientific reasoning. These complement simulation-heavy datasets by incorporating authentic visuals, diagrams, and plots, thus enriching models capacity for both symbolic reasoning and real-world data interpretation. 2) Chemistry: Chemistry builds directly on physical principles to describe the structures, transformations, and properties of molecules and compounds. Pre-training datasets in chemistry reflect the fields diversity of representations, including SMILES [20], [200] and SELFIES strings for linear encodings, molecular graphs [468] for connectivity patterns, and 3D coordinate formats (SDF, PDB) for spatial conformation [470]. These enable models to learn relationships between topology, stereochemistry, and molecular function. Large, curated molecular libraries form the backbone of chemical pre-training. ZINC [605] offers millions of commercially available drug-like compounds, ChEMBL [216] aggregates bioactive molecules with activity annotations, and MOSES [606] provides standardized benchmark set for generative modeling. Early pre-trained models such as SMILESBERT [471] and more recent architectures like SMILESMamba [607] demonstrate how sequence-based learning can tasks ranging from de novo molecular generasupport tion [608], [609] to property prediction [610] and structurebased drug design [611]. Chemical reaction datasets expand the scope of pre-training to transformation pathways. The USPTO dataset [612], containing million-scale reactions annotated with reactants, products, catalysts, temperatures, and other conditions, supports retrosynthesis planning, reaction outcome prediction, yield estimation, and catalyst selection [243], [613]. Together, these datasets enable LLMs/MLLMs to model both static chemical structures and dynamic processes. 3) Materials Science: Materials science extends chemistry into the design, synthesis, and characterization of substances with tailored properties. Pre-training datasets in this field span multiple modalities: crystallographic structure files, chemical notation datasets, property-specific compilations, and large textual corpora. Crystallographic datasets, encoded in CIF formats, are central for learning structural-property relationships. The Materials Project [70] offers over half million entries covering known and predicted materials, while OQMD [614] contains more than million calculated electronic property records. ICSD [615] curates inorganic crystal structures, and specialized datasets such as CoRE MOF [616], QMOF [617] and DigiMOF [618] target metal-organic frameworks. NOMAD [619] and Materials Project Trajectory [620] scale up to millions of entries, incorporating dynamic simulation data. Sequence representation datasets like USPTO [217] and JARVIS-DFT [621] provide alternative chemical encodings (InChI, IUPAC, SELFIES), while large chemical libraries such as ZINC [622] overlap with both chemistry and materials applications. Property-specific datasets [623] focus on targeted physical or mechanical attributes, enabling specialized pre-training for predictive modeling. Textual datasets like MatScholar [624], with millions of publications, complement structured data by providing unstructured knowledge on material-property relationships. Across physics, chemistry, and material sciences, pretraining datasets evolve from highly idealized simulations to richly annotated experimental corpora, and from symbolic equations to multimodal figure-caption pairs. The scale and diversity of these resources are critical: physics simulations anchor models in governing laws, chemical libraries teach molecular diversity and reactivity, and material databases bridge microscopic structures with macroscopic properties. Future progress will hinge on integrating these modalities by combining simulation outputs, experimental measurements, and literature-derived knowledge, to build foundation models capable of reasoning seamlessly from atomic-scale phenomena to engineered material systems. B. Life Sciences: Complexity from Molecules to Systems 1) Molecular and Cell Biology: At the molecular scale, the central dogma, i.e., DNA-to-RNA-to-protein, shapes the data landscape for pre-training. Sequence-based datasets dominate, with different corpora focusing on small molecules, nucleic acids, or proteins. Pre-training in the life sciences aims to equip LLMs and MLLMs with the ability to represent, reason about, and generate knowledge across the intricate hierarchy of living systems. This hierarchy begins at the smallest biological units (e.g., genes, proteins, and metabolites), progresses through cellular and tissue-scale processes, and culminates in organismal, clinical, and ecological contexts. Biological data is inherently heterogeneous: sequence strings, structural models, expression matrices, microscopy images, clinical narratives, and more. Effective pre-training datasets must therefore capture both the fine-grained molecular details and the higher-order interactions that emerge across scales, while aligning multimodal inputs into unified representations. Current biology pre-training datasets for LLMs span multiple molecular modalities, with molecular, protein, and nucleic acid sequences constituting the primary data types. For molecular representations, several notable datasets have emerged: SPICE [625], PCdes [626], PubChemSTM [627], and MoMu [628] utilize SMILES strings or molecular graphs for pre-training, while TCPA [629] focuses on protein sequences. In the protein domain, UniRef [630] databases serve as the foundational resource, with UniRef50 and UniRef90 containing approximately 49 million protein sequences after clustering at 50% and 90% sequence identity, respectively. For nucleic acid sequences, DNABERT [631] utilized the human reference genome (Hg38.p13) for pre-training, while DNABERT-2 [317] expanded to multi-species genomes from 135 species, creating dataset 12 times larger than its predecessor. RNA pre-training has leveraged RNAcentral [632] database with million-scale non-coding RNA sequences. The evolution of these datasets reflects clear trend toward multi-species, multimodal approaches and increased scale. Recent advances include sophisticated tokenization strategies, such as DNABERT-2s [317] Byte Pair Encoding (BPE) replacing traditional k-mer tokenization, and the incorporation of structural and functional annotations beyond raw sequences. Cross-modal pre-training has gained traction, with an increasing number of datasets [627], [628] bridging molecular structures with natural language descriptions, enabling more comprehensive molecular understanding. Future directions point toward larger-scale datasets that incorporate 3D structures, epigenetic modifications, and cross-species evolutionary relationships, as evidenced by emerging comprehensive bench32 marks [317] for systematic model evaluation across diverse genomic tasks. 2) Multi-Omics: Multi-omics pre-training aims to unify genomics, transcriptomics, proteomics, and beyond into integrated representations. At the genomic level, pre-training corpora often start with the complete human reference genome (GRCh37 [633]) and population-scale sequences from projects like the 1000 Genomes Project [634]. To enhance generalization and crossspecies utilization, pretraining corpora are often further expanded to encompass genomic sequences from multiple species, such as archaea, fungi, and vertebrate mammalian, collected from scientific repositories such as NCBI GenBank [94]. Omni-DNA [635] constructs 30B-token corpus by sampling and deduplicating genomic fragments from NCBIs multi-species genome database, covering bacteria, archaea, fungi, plants, and vertebrates. GeneChat [636] focuses on encoding human genomic syntax and semantics by extracting 1024 bp fragments from the GRCh38 reference genome. DNAHLM [637] adopts hybrid corpus of equal-size genomic and textual data, drawing DNA sequences from the GRCh38 human genome and papers from Wikipedia. More recently, BioReason [421] extends beyond sequence modeling by incorporating dual-channel corpus consisting of PubMed and Wikipedia texts alongside large-scale gene-gene interaction graph built from sources like STRING, Reactome, and Gene Ontology, enabling joint pretraining across natural language and biological relational structures. In transcriptomics, early large-scale pretraining efforts have focused on gene expression matrices derived from single-cell RNA sequencing (scRNA-seq) data. Foundation models [512], [638] are typically trained on datasets including HCA and Tabula Muris, where expression profiles are represented as gene tokens or gene-expression pairs. Moving beyond unimodal expression, scMMGPT [639] demonstrates large-scale dataset with natural language data, involving over six million single cells across three modalities: scRNA-seq, scATAC-seq, and RNA-protein measurements from CITE-seq. RNA-GPT [640] develops training corpus with 130,102 full-length transcripts from the GENCODE v38 reference, boosting the unification of transcript-level RNA understanding and generation with language-level reasoning. In proteomics, UniProtKB (Swiss-Prot and TrEMBL) serves as the foundational pretraining resource [641]. For example, ProteinLMDataset [642] is built by SIFTS-mediated mapping of protein data bank [328] entries to UniProt, integrating billions of tokens from PubMed abstracts, Swiss-Prot and PMC full texts; Evolla [509] extracts 14 expert-curated Information Points from Swiss-Prot and clustered TrEMBL entries, which are then transformed into high-confidence questionanswer pairs via an LLM-driven augmentation pipeline. Emerging multi-omics corpora begin to unify diverse biological modalities, integrating sequence-level data with biomedical text. NatureLM [43] assembles over 3.27 trillion tokens from 35 biomedical corpora encompassing molecular sequences, clinical narratives, literature, and imaging-derived captions. This massive collection incorporates structured omics repositories such as UniProt, GENCODE, and the Human Protein Atlas alongside unstructured text from medical corpora like PubMed, enabling alignment between textual semantics and molecular features across scales. LLaMA-Gene [40] curates multimodal biomedical instruction corpus by aligning 6.2 million natural language queries with structured molecular knowledge graphs derived from GeneCards [643], OMIM [644], and Ensembl [645]. This results in paired representations of gene-level annotations, phenotypes, diseases, and variant consequences, supporting instruction-tuned pretraining for gene-centric biomedical reasoning. ChatNT [510] constructs fully multimodal instruction dataset comprising 605 million DNA tokens and 273 million English tokens, covering 27 downstream tasks involving DNA, RNA, and protein processes. Together, these works exemplify paradigm shift toward integrative instruction datasets that fuse omics, clinical, and textual domains into unified token spaces for large-scale pretraining. 3) Neuroscience: In the field of neuroscience, pretraining primarily entails two components: extensive text corpora of neuroscience literature and modality-specific encoders pretrained on brain signals such as EEG, fMRI, and MEG. The literature corpus, exemplified by the BrainGPT [552] comprises approximately 1.3 billion words drawn from 332,807 abstracts and 123,085 full-text articles in the PubMed Central Open Access Subset, covering 100 high-impact journals (e.g., Nature, Cell, Neuron, PNAS) published between 2002 and 2022. The LaBraM [646] framework integrates over 2534.78 hours of EEG data from about 20 public and proprietary datasets, encompassing motor imagery, emotion recognition, grasp-andlift tasks, P300 spelling paradigms, epilepsy detection, and resting-state recordings, with channel counts of 19-64 and sampling rates of 160-2048 Hz. 4) Healthcare and Medical Science: Depending on the model type, pre-training strategies for medical models vary: LLMs are primarily trained on large-scale clinical and biomedical texts to acquire medical language understanding. However, when translated to MLLMs, they require another multimodal pre-training stage that aligns visual and textual modalities to develop image-grounded understanding. Accordingly, the pretraining datasets can be broadly categorized into text-only corpora for LLMs and image-text pairs for MLLMs. Medical textual data contains essential domain knowledge. The textual corpora are dominated by conversational clinical dialogues [42], [520], [647][649]. Clinical dialogues cover wide range of outpatient scenarios, but their level of expertise and reliability is difficult to guarantee due to the absence of follow-up examinations for verification. Medical textbooks and research papers [203], [520], [589], [650], [651] help address this issue, serving as critical sources of knowledge in the medical domain. Electronic Health Records (EHR) [652] [655] include basic demographic data, summaries of major diseases and health issues, and key healthcare service records, providing longitudinal health information of patients over time. However, EHR datasets suitable for reasoning over temporal patient trajectories are still scarce. Clinical reports [153], journey, ranging [656][659] document from admission and examination to diagnosis, treatment, and follow-up. However, access to such reports typically requires the entire patient 33 strict ethical review and entails potential privacy risks, which limit their overall availability and scale. For MLLMs, image-text pre-training datasets play central role. Large-scale corpora such as PMC-OA [204], ROCOv2 [660], MedICaT [661], and Open-PMC-18M contain millions of biomedical figures and their associated captions, largely sourced from academic literature. Datasets like MIMIC-CXR [153], CheXpertPlus [656], and PMCCaseReport [657], on the other hand, provide detailed diagnostic reports with finer-grained information derived from the corresponding medical images. These datasets cover wide range of modalities, including CT, MRI, X-ray, ultrasound, PET, endoscopy, and histopathology, offering diverse supervision signals for learning visual-semantic correspondence. Domainspecialized image-text corpora also exist to target specific medical subfields. For example, MM-Retinal [662] focuses on ophthalmology, while Quilt-1M [150] concentrates on histopathological imagery with expert-vetted captions. These datasets serve to refine model understanding in narrowly scoped visual domains where general medical datasets may lack coverage. Beyond static medical images, medical videos also encapsulate essential domain knowledge, including educational content for clinical training, patient simulation [663], surgical procedures [664], and other clinically relevant scenarios. Models can learn comprehensive diagnostic and therapeutic knowledge from such videos. However, there remains significant gap in scale between medical videos and medical images. Despite their scale and variety, existing datasets in the healthcare and medical sciences domain show striking modality imbalance, where medical image data occupies significant position among all datasets, with the majority centered around radiological imaging. Further, for multimodal pretraining data, the annotation quality remains variable, ranging from noisy figure caption to partially validated annotations, which can affect model reliability. 5) Agriculture: In the agricultural domain, LLMs are generally pre-trained using corpora compiled from millions of multilingual agronomy journal articles, tens of thousands of professional textbooks, and genomic sequence databases. The construction of such pre-training datasets typically involves labor-intensive pipeline including OCR processing, deduplication, and filtering of low-quality content. Although several agricultural LLMs have been introduced [546], [550], none of their domain-specific pre-training datasets have been publicly released, hindering reproducibility and further research. C. Astronomy and Earth Science: Understanding Our Planet Astronomy and Earth science datasets expand scientific LLM/MLLM pre-training into domains where spatial, temporal, and spectral diversity is immense. They provide observational records, simulation outputs, and literature that span cosmic scales and Earths interconnected physical systems. For LLMs, pre-training relies heavily on textual resources derived from research publications, mission archives, and observational metadata. For MLLMs, multimodal corpora integrate high-resolution imagery, time-series data, maps, and spectra with descriptive text, enabling models to connect visual and quantitative patterns with domain-specific narratives. 1) Astronomy: Astronomy is among the most dataintensive scientific fields, yet large-scale, open, and multimodal pre-training datasets remain rare. Existing resources are fragmented across text, image, and spectral modalities, each with distinct acquisition challenges. While simulationheavy domains like physics can generate abundant synthetic corpora, astronomical data acquisition depends on long-term sky surveys with large telescopes, such as LAMOST [199] and Gaia [665], making large-scale datasets costly and slow to compile. Moreover, observational modalities like images, spectra, and time-series differ in wavelength coverage, resolution, and signal-to-noise ratios, and are stored in heterogeneous formats with inconsistent calibration standards. Core physical parameters (e.g., stellar mass, metallicity) are often inferred indirectly via modeling rather than directly observed, limiting the availability of high-quality, labeled examples for supervised pre-training. Among existing text-based datasets, resources like NASA ADS [666] provide extensive corpora of astronomical research papers, abstracts, and technical documents. These have supported the construction of domain language models such as AstroBERT [667], trained for semantic understanding and entity recognition in astronomical contexts. SpecCLIP model [668] using LAMOST [199] and Gaia XP spectral data [665], aligns and reconstructs different spectral modalities through comparative learning. AstroPT [669], an image model built based on Dark Energy Spectroscopic Instrument Legacy Survey images, uses an autoregressive generative model to learn the potential distribution structure of galaxy images. However, such datasets typically focus on single modalities with narrow coverage, preventing the formation of generalpurpose astronomical foundation dataset. At present, text data tractable and widely used modality for remains the most pre-training in astronomy, while comprehensive multimodal datasets that integrate images, spectra, and time series are still largely absent. 2) Earth Science: Earth science remain less explored in pretraining dataset construction; most existing corpora in this field are derived from academic papers, textbooks, and similar sources. The scarcity is due in part to the dispersed and heterogeneous nature of Earth science data. Textual information is often embedded in PDFs of academic papers and textbooks, requiring complex parsing, while visual data (e.g., atmospheric variable fields, remote sensing imagery, and geological crosssections) lacks the readily captionable semantic features found in natural images, making textimage alignment particularly challenging. Despite the scarcity of public pretraining datasets, several approaches to data construction offer valuable insights. For instance, EarthSE [670] leverages approximately 100,000 Earth science-related academic papers as its corpus. By employing advanced PDF parsing tools, these papers are converted into text, followed by automated annotation and data cleaning processes to produce high-quality datasets. Similarly, studies like ClimaQA [671] extract structured corpora from Earth science 34 textbooks. K2 [567], on the other hand, gathers substantial textual data from internet sources, such as Wikipedia, relevant to Earth sciences. Although limited in scale and diversity for pretraining LLMs, these resources show that scholarly literature and curated web content remain the primary sources for Earth science textual data. Moving forward, integrating multi-source data, improving parsing techniques, and developing algorithms tailored for aligning Earth science images with text will advance pretraining dataset development in this field. D. Pre-training Data Analysis Across domains, current scientific pre-training corpora show clear strengths and equally clear gaps. Throughout the scientific landscape, the dataset ecosystem is both broad and heterogeneous, spanning text (papers, guidelines, EHR), symbolic structures (SMILES strings, CIF, gene and protein sequences), and multimodal pairings (figures, radiology, microscopy, spectra, videos). This diversity is illustrated in Fig. 19, which visualizes the relative distributions of pretraining data modalities (left) and task types (right). As shown, certain modalities such as academic papers, SMILES strings, and radiology images dominate, while others remain underrepresented; similarly, task types are heavily skewed toward raw text and classification. Such uneven coverage underscores both the breadth and imbalance of current scientific corpora, leading to several problems: First, modality imbalance persists: physics remains dominated by idealized simulations [596], [597], which transfer poorly to noisy, instrument-specific observations, underscoring the simulation-to-observation gap. Second, many MLLM datasets rely on captions or rule-based labelers, yielding weakly grounded semantics [204], [660], while even higherquality radiology resources still depend on automatic pipelines that propagate labeling bias [656]. Third, heterogeneity and poor standardization impede cross-source fusion. For example, materials repositories (Materials Project [70], NOMAD [619], OQMD [614]) expose inconsistent metadata and calculation settings, complicating integrated pre-training and evaluation. Similar issues appear in astronomy, where multi-instrument spectra [199], [665], [668] differ in bandpass, resolution, and calibration, challenging multimodal alignment. Fourth, some fields lack truly open, large-scale pre-training corpora: Earth science efforts [567], [670] remain text-centric and modest in scale, limiting broad generalization. Fifth, data governance constrains clinical/EHR corpora [672], [673], yielding smaller or temporally stale distributions relative to real-world care. Finally, scalequality trade-offs are unresolved: massive chemical/molecular pools [622], [623] offer breadth but limited property curation, whereas targeted materials sets emphasize fidelity at the expense of coverage. Such uneven landscape gives rise to fundamental tension: scientific LLMs/MLLMs require rich, multimodal pretraining to support domain-aware reasoning, but collecting such corpora is often expensive and sparse. Therefore, classical large-scale scaling for training general-domain models, which throws ever-more tokens and parameters at the problem, is much less feasible for the development of scientific models. Fig. 19: Word clouds of the pre-training dataset. The plots show the relative distributions of modalities (left) and types (right), with word size proportional to frequency. Fig. 20: Composition of the Cambrian-7M [679] instruction tuning dataset. Efficient pretraining thus emerges as critical design principle. Leveraging compute-optimal scaling laws [28], [674] (e.g., models should balance parameters and tokens for optimal compute efficiency) offers roadmap for budget-aware model design. Techniques such as carefully curated data mixtures [675], high-quality subset selection [676], and continual pretraining [677], [678] further promise to stretch domain-limited scientific resources effectively. V. SCIENTIFIC DATA FOR POST-TRAINING Post-training in scientific LLMs/MLLMs aims to align pre-trained backbone, which is already equipped with broad factual knowledge, with the specific problem-solving styles and interactive workflows of scientific practice. Unlike pretraining which focuses on coverage and scale, post-training curates domain-specific, high-quality, and task-oriented datasets that teach models to solve problems, follow instructions, and explain their reasoning in ways aligned with disciplinary norms, moving beyond simple factual memory. Across the sciences, post-training datasets have evolved from small, text-only instruction corpora toward large, multimodal, and reasoning-rich collections. However, these datasets vary greatly in sources, size, supervision type, and modality, reflecting differences in data availability, curation cost, and the maturity of AI adoption in each domain. Small proportion of scientific data in current multimodal instruction tuning is exemplified by the Cambrian-7M dataset [679]  (Fig. 20)  , where science-specific content comprises only 35 2.9% of the total training corpus, with the majority dominated by OCR (27.6%), general knowledge, and language tasks. A. Current Landscape Across Scientific Domains The details of the post-training datasets are summarized in Tab. IV. 1) Physics: Physics post-training datasets aim to move beyond fact recall toward the procedural and conceptual mastery that physicists use in practice. The scope spans multi-step derivation, formula reconstruction, unit consistency checks, experimental interpretation, and numerical estimation. These tasks demand both symbolic fluency and the ability to reason under physical constraints, which are often absent from generic LLM training corpora. Existing open resources remain dominated by text-based QA formats, often adapted from educational or competition contexts. PIQA [680] captures physical commonsense, including tool use and intuitive actions, though it stops short of formal derivations. SciBench [441] and the physics problems within the PhysicsArena [681] benchamrk introduce computational questions with numeric computation and formula application, making them suitable for fine-tuning unit handling and basic symbolic manipulation. MATH500 [682] is curated 500-problem subset of the MATH [683] benchmark spanning seven competition-style mathematics subjects; while it does not include physics category, its algebraic and symbolic problems can help evaluate skills that are often prerequisite for physics problem solving. Beyond direct extraction from exams and textbooks, synthetic or semi-synthetic resources increasingly scale coverage. Nemotron-Science [684] subset contains teacher-generated reasoning traces across scientific domains including physics; NaturalReasoning [685] contributes 2.8M challenging questions with reference answers and is widely used to distill long CoT from stronger models; and SCP-116K [686] offers 116k automatically extracted problemsolution pairs in highereducation science (including physics), providing step-wise solutions without relying on LLM-generated CoT. Overall, physics post-training datasets today provide strong base for short-form problem-solving, with growing use of synthetic CoT corpora [684], [685] to extend reasoning depth. However, most of them are text-only without the dual figures or symbolic markup, failing to represent textual-symbolic nature of physics reasoning. Further, posttraining datasets still rarely capture the multimodal richness of real-world tasks, such as interpreting force diagrams, circuit schematics, or motion graphs, despite such modalities being central to the discipline. 2) Chemistry: Chemistry post-training relies on highquality, task-specific datasets to fine-tune models for molecular property prediction, structure-based reasoning, and generative chemistry. Unlike pre-training corpora that may contain millions of weakly labeled compounds, post-training data is limited in scale due to the high cost of wet-lab experiments and structural determinations. For example, drug-discovery ADMET datasets [243] are often limited to hundreds to thousands of entries because measuring absorption, distribution, metabolism, excretion, and toxicity requires time-intensive experiments. The Cross-Docked dataset [687] contains 22.5M estimated 3D protein-ligand binding poses generated by molecular docking into multiple protein binding pockets, providing large-scale resource for training and benchmarking structure-based drug discovery models. PDBBind [688] database stands out as high-quality, manually curated resource that extracts experimentally validated protein-ligand complexes from the Protein Data Bank, each annotated with quantitatively measured binding affinity data, supporting both structural analysis and predictive modeling of binding strength. Chemistry datasets increasingly combine molecular formats (SMILES, InChI, 3D coordinates) with textual annotations [689], allowing LLMs to align symbolic chemistry representations with natural language descriptions. This multimodal pairing is key to enabling cross-format translation, e.g., predicting compounds IUPAC name from structure or vice versa. 3) Materials Science: Materials science post-training datasets are scarce and often repurposed from pretraining corpora. Molecular generation benchmarks like MOSES [690] and ChEBI-20 [691] pair SMILES with text descriptions, supporting tasks from generation to captioning. ChEBI-20MM [692] extends these with richer metadata (InChI, IUPAC, polar area), enabling cross-format translation. Apart from text and SMILE modalities, there are visual datasets from highresolution characterization resources such as the Warwick Electron Microscopy Datasets [693], containing tens of thousands of STEM/TEM images and simulated wavefunctions. These enable image captioning, defect identification, and property inference when paired with textual descriptions. However, such visual data are limited. Most datasets lack multistep reasoning traces, multimodal integration, or workflows that combine molecular design with property calculation and analysis. 4) Life Sciences: Life sciences post-training data spans diverse subdomains, each with distinct data modalities, supervision formats, and reasoning demands. include"
        },
        {
            "title": "Molecular and cell biology datasets",
            "content": "three main groups. First, sequence-to-function datasets such as PEER [694] and BEACON [695] focus on protein and RNA property prediction. Second, large-scale instruction corpora like Mol-Instructions [696], OPI [697], and PubChemSTM [627] translate biochemical facts into conversational form, covering protein, nucleic acid, and small molecule entities, moving supervised fine-tuning beyond factual recall toward interactive QA. The third stream, still emerging, involves reasoning-focused datasets that pair each biology QA with an explicit chain-of-thought, such as ProCoT [698] for pathway reasoning and ToT-Biology [699] for mechanistic explanations. For multi-omics post-training, DNA-focused datasets like Omni-DNA [635], GeneChat [636], and DNAHLM [637] frame genomics tasks (e.g., promoter detection, variant interpretation) as instruction-response pairs. RNA post-training includes single-cell and bulk expression modeling, as in scMM-GPT [639], which aligns scRNA-seq, scATAC-seq, 36 and CITE-seq modalities with prompts describing biological contexts. Proteomics leverages UniProt-derived resources such as ProteinLMDataset [642] and Evolla [509], creating hundreds of thousands to millions of protein-centric QA pairs. Multi-omics instruction sets like Biology-Instructions [700] extend post-training to integrated DNA, RNA, and proteins, typically by synthesizing instruction-response pairs from reference databases and combining them with curated variant interpretation and functional annotation tasks. In healthcare and medicine, post-training data support wide range of tasks with the most mature ecosystems: clinical dialogues (MedDialog [701], ChatDoctor [647], NoteChat [702]) for medical chatbots, medical image report generation (PMC-CaseReport [657], MIMIC-CXR [153], for structured documentation, mulCheXpertPlus [656]) timodal question-answering (EHRXQA [703], PubMedVision [540], VQA-RAD [704], GMAI-VL-5.5M [541]) for textual or visual comprehension, with chain-of-thought data (GMAI-Reasoning-10K [545]) for step-by-step diagnostic reasoning on medical images. Post-training in neuroscience refers to the alignment of measured neural signals, EEG, MEG, and fMRI, with the text embedding space of large language models to enable decoding of related semantics. The experimental tasks fall into several broad categories, including visual decoding, text decoding, sleep classification, clinical abnormality detection, motor imagery, emotion recognition, and workload assessment. In visual decoding, several rich benchmark datasets have been collected. Things-EEG1 [705] comprises EEG recordings from 50 participants responding to rapid serial visual presentation of 22,248 images covering 1,854 object concepts. Things-EEG2 [706] provides high temporal resolution EEG from 10 subjects over 82,160 image presentation trials drawn from 16,740 conditions selected from the THINGS database. The Natural Scenes Dataset (NSD) [707] contains roughly 213,000 trials from eight subjects viewing 70,566 natural images, with blood oxygen level dependent responses captured using 7 Tesla fMRI at 1.8 millimeter resolution. Things-fMRI [708] includes denoised responses from three participants to 8,740 images representing 720 objects, collected across 12 independent scanning sessions. To extend visual decoding into the realm of imagined content, NSD-Imagery [709] offers benchmark with 2,304 mental imagery trials collected from NSD, with stimuli spanning simple shapes, complex natural scenes, and conceptual words. Complementing the fMRI-based work, Things-MEG [708] records neural responses from four participants to the same 22,448 images (1,854 objects) with millisecond-level temporal precision. Neuro-3D [710] constructed the EEG-3D dataset, which contains EEG signals collected from 12 participants while they viewed 72 categories of 3D objects (both images and rotating videos). For text decoding, the ZuCo collections capture EEG and eye-tracking data during natural reading and semantic annotation. ZuCo1 [711] recorded data from 12 native English adults reading over 21,000 words in 1,107 judgment, entity sentences across tasks such as sentiment relation recognition, and extraction of targeted relations like nationality, occupation, or employer. ZuCo2 [712] refines the experimental design by gathering EEG and eye movement data from 18 participants during both free reading and annotation specific to semantic relations, using 739 English sentences to better isolate cognitive differences between conditions. Beyond decoding of visual and linguistic content, other neural domains contribute complementary signals. Sleep stage classification is supported by datasets such as HMC [713], SleepEDF [714], and SHHS [715]. Clinical abnormality detection focuses on disorders such as epilepsy, with datasets including TUEV [716], TUAB [716], and TUSL [717]. Motor imagery is studied using the SHU [718] dataset. Emotion recognition draws on SEED [719] and SEED-IV [720] to characterize affective states from neural activity. Cognitive Workload [721] has been probed by collecting EEG from 36 healthy university students engaged in continuous mental arithmetic through serial subtraction, contrasting resting state with task periods to reveal neural correlates of load. Together, these datasets form diverse and multi-task foundation for grounding brain activity in language model spaces and decoding semantics relevant to perception, cognition, clinical assessment, and internal mental states. Agriculture uses domain-specific instruction corpora (e.g., CROP [722]) and multimodal VQA datasets (e.g., AgroInstruct [551], MIRAGE [190]) to adapt LLMs/MLLMs to crop health assessment, pest identification, and farm management. Together, life sciences post-training data covers broad modality spectrum from sequences and molecular graphs to clinical images and neural recordings, requiring models to unify understanding across vastly different biological scales. 5) Astronomy: Astronomy post-training data has evolved from pure-text corpora to rich multimodal resources. Early efforts collected hundreds of thousands of arXiv astronomy paper abstracts [561], embedding field-specific terminology and style. Later expansions included texts from introductions and conclusions [723], as well as LLM-generated QA pairs from arXiv content, shifting toward interactive tasks. To support more complex joint vision-language understanding tasks, post-training data construction incorporated multimodality. For instance, AstroLLaVA [562] integrates NASAs Astronomy Picture of the Day and HST observation data [351], generating tens of thousands of imagecaption pairs. Additionally, large-scale synthetic pipelines now leverage arXiv, astronomy Wikipedia, and textbooks to produce millions of domain-specific question-answer pairs [563], [566], [724]. For fine-grained tasks, such as named entity recognition in astronomy literature, manual curation remains essential, as seen in Astro-NER [725]. These datasets collectively enable models to handle domain knowledge understanding and multimodal image-text grounding for astronomical observation. 6) Earth Science: Earth science post-training datasets now span atmospheric, oceanic, terrestrial, and ecological domains. Early examples like FloodNet [726] paired remote-sensing images with templated questions. Automated pipelines such as EarthVQA [727] and TEOChatlas [578] expanded to hundreds of thousands of GIS-derived visual QA pairs. WeatherQA [728] introduced reasoning over weather composites, and SeafloorAI [729] scaled to millions of sonar-QA pairs. Cross-sphere datasets also appeared, like GeoLLaVA8K [573], the highest-resolution vision-language datasets in remote sensing field to date, covering 22 real-world dialogue tasks. Supporting corpora like RS5M [730] and SkyScript [731] offer millions of image-caption pairs across optical, Synthetic-Aperture Radar, and Infrared (IR) modalities. With increasingly automated annotation via advanced MLLMs like GPT-4 or Gemini-Vision, Earth science posttraining data now enables not only scene captioning but also multi-step reasoning over complex Earth-system interactions. B. Post-training Data Analysis Existing post-training datasets share the following patterns and trends across domains. First, instruction-based corpora dominate, converting structured domain knowledge (e.g., databases, ontologies, benchmark tasks) into prompt-response pairs. These range from molecular biology and chemistrys SMILES-language instruction sets [689], [696] to astronomys literature-derived QA [724], and from clinical dialogue datasets [702] to Geographic Information System (GIS)-to-question pipelines [578] in Earth science. Another trend to be noted is the increasing importance of multimodal and multi-domain corpora. Domains with rich data modalities (e.g., images), such as healthcare [149], [151], [153], astronomy [562], and Earth science [573], now build VQA datasets or image-caption pairs to bridge visual and textual reasoning. Further, the multi-omics domain in life sciences typically require analysis across genomics, proteomics, and transcriptomics [40], [43]. In chemistry and materials science, SMILES strings, 3D molecular coordinates, microscopy images, and textual descriptions are increasingly co-annotated. This multimodal shift is crucial for teaching models to interpret data in heterogeneous forms and perform fluidly across related scientific subfields. As shown in Fig. 21, the source distribution of existing post-training corpora for scientific LLMs/MLLMs reveals significant domain-specific biases and cross-domain imbalances across different scientific fields. These skews highlight opportunities for future corpus building to diversify inputs, reduce training bias, and improve model generalization across disciplines. Further, across domains, there is clear trend toward explicit reasoning supervision beyond simple QA, driven by the need for models to handle complex, multi-step decision-making. However, these reasoning-oriented datasets are unevenly distributed. Biomedical sciences have begun producing chain-ofthought datasets for molecular pathways [698], [699] or multistep diagnosis [732], [733], even for multimodal tasks [545]; but large-scale, publicly available CoT corpora are relatively scarce in other domains. Finally, scalable data synthesis has emerged as practical solution to annotation bottlenecks. High-quality literature corpora, simulation outputs, and curated databases are now mined by LLMs to produce domain-specific instructionresponse pairs [151], [689], [724] and reasoning traces [543], [684] at scale, employing advanced techniques like multi-agent Fig. 21: Source distribution of existing post-training corpora for scientific LLMs/MLLMs, normalized within each domain, showing significant domain-specific biases and cross-domain imbalance. These skews highlight where future corpus building could diversify inputs to reduce training bias and improve model generalization across disciplines. Fig. 22: Word clouds of the post-training dataset. The plots show the relative distributions of modalities (left) and types (right), with word size proportional to frequency. validation [733] to maintain fidelity, enabling the production of millions of domain-relevant samples that would be infeasible to curate manually. As illustrated in the word clouds of Fig. 22, post-training datasets encompass diverse modalities (left) and types (right), ranging from scientific representations like SMILES and nucleotide sequences to text-QA, image-text, and VQA content, reflecting the fields shift toward multimodal and integrated approaches. In combination, these trends mark shift from narrow, textbound, single-domain resources toward broad, richly annotated, and operationally relevant datasets. This evolution positions post-training not merely as final polishing step, but as critical stage where scientific LLMs acquire the multimodal fluency, interdisciplinary reasoning, and tool integration skills necessary for real-world research environments. Despite these advances, significant gaps remain. Datasets 38 with multi-step reasoning traces tied to real experimental or computational workflows are still scarce in most domains. Some of existing CoT datasets [543], [684], [733] are distilled from existing reasoning models [454], [456] without extensive expert validation. Moreover, multimodal coverage is uneven: while medicine, Earth science, and astronomy have rich imagetext corpora, physics still lacks large-scale datasets that pair problems with diagrams or simulations. Licensing, privacy, and standardization also hinder dataset reuse, especially in healthcare and proprietary industrial research. Future efforts should prioritize integrated multimodal corpora; process-aware datasets with explicit reasoning traces, experiment design steps, and intermediate analyses; and toolgrounded examples showing models how to invoke simulations, parse outputs, and iterate on hypotheses. Continuous post-training pipelines will be needed to keep pace with fastevolving scientific data, blending automated ingestion with expert oversight. Synthetic data generation will remain essential, but should follow hybrid pipelines that combine automated scaling with human validation to maintain fidelity. Ultimately, the goal is to move from LLMs that recall scientific facts to models that can operate as collaborative research assistants: reasoning across disciplines, working with tools, and adapting to new knowledge in real time. VI. EVALUATION OF SCI-LLMS The evaluation of Sci-LLMs has increasingly gained attention as AI-for-Science (AI4Science) becomes integral to contemporary research. Recent developments in this area highlight the critical need for comprehensive assessment frameworks that evaluate model performance across diverse scientific disciplines, addressing multiple dimensions such as knowledge retention, understanding, reasoning, multimodality, and adherence to scientific values. Platforms such as SciHorizon [734] exemplify this trend, offering holistic benchmarking solutions that assess both AI-readiness of scientific datasets and finegrained capabilities of LLMs across domains. In the following, we will explore the evolution and current status of scientific benchmark datasets, outlining their role in driving further advancements in AI4Science evaluation methodologies. A. Current Landscape Across Scientific Domains The evaluation of scientific foundation models across diverse disciplines has led to the development of specialized benchmarks that assess both domain-specific knowledge and reasoning capabilities. These benchmarks span from fundamental physics problems to complex biological systems, each differing in sources and targeted problems, designed to capture the unique challenges within their respective fields. The details of the evaluation datasets are summarized in Tab. V. 1) Physics: In physics, evaluation benchmarks have evolved to test models across educational, competitive, and research-oriented tasks. At level, MMPhyQA [735] targets high-school physics via multimodal questions with explicit multi-image chain-of-thought prompting, while OlympiadBench [736] stresses bilingual, Olympiadgrade mathematics-and-physics problems with expert step annotations. PIQA [680] and PROST [737] earlier emphasized the foundational physical commonsense through multiple-choice plausibility tasks, establishing bridge from general commonsense QA into domain-specific physics. The progression continues through undergraduate-level challenges with PhysUniBench [738], PhysReason [739], and PhysicsArena [681], which systematically probe deeper physics reasoning through variable identification, process formulation, and solution derivation. UGPhysics [740] expands this scope by compiling bilingual undergraduate physics resources across mechanics, thermodynamics, and electromagnetism, while PhyX [741], PHYSICS [742], and SeePhys [743] integrate text with diagrams and experimental setups to test multimodal reasoning in diverse physics domains. Complementing these, TPBench [744] introduces advanced theoretical physics tasks spanning cosmology, relativity, and quantum mechanics, while PHYBench [745] targets physical perception more broadly, introducing metrics like Expression Edit Distance to distinguish genuine reasoning from shortcuts. Beyond problem-solving, physics benchmarks extend to equation discovery and symbolic regression. FSReD / AI Feynman [222] supplies physics-grounded targets for symbolic regression, while SRBench [746] establishes living benchmark suite for comparing symbolic regression methods. LLM-SRBench [747] specifically targets scientific equation discovery with large language models, carefully designing problem splits to avoid trivial memorization. Physical intuition in video is covered by IntPhys 2 [748], which presents synthetic scenarios requiring models to distinguish possible from impossible events, MVP-Bench [749] which constructs minimal video pairs to force true physical understanding, and MVBench [750] which offers broad temporal multimodal video understanding tasks. 2) Chemistry: Chemistry benchmarks have similarly evolved to encompass both knowledge assessment and practical applications. ChemBench [20] and ChemEval [751] provide comprehensive coverage of nine and 42 core chemistry tasks, respectively, while ChemMLLM [200] extends evaluation to multimodal chemistry research, including imageto-image translation for molecule optimization and text-toimage translation for molecular design. Specialized benchmarks target specific aspects: ChemSafetyBench [752] focuses on safety issues of LLM responses in chemical experiments; TrialBench [753] focuses on clinical trial problems relevant to drug development, QCBench [754] evaluates quantitative chemistry problem-solving across seven subfields from analytical to quantum chemistry, and PMO (practical molecule optimization) [214] addresses molecular optimization with 23 objectives covering diversity, synthetic accessibility, and optimization ability. The critical role of spectroscopic data is captured by SpectrumWorld [755], which introduces 14 multimodal tasks spanning over 10 major spectroscopic techniques and 1.2 million distinct chemical compounds, evaluating models on spectrum-to-structure reasoning and spectral prediction from SMILES. 3) Materials Science: The intersection of chemistry and materials leads naturally to materials science benchmarks, which have evolved from traditional machine learning evaluations to LLM-specific assessments. MoleculeNet [218] es39 tablished early standards with over 700,000 compounds for molecular property prediction, while MatBench [71] introduced specialized tasks for inorganic materials focusing on electronic structure and mechanical characteristics. Modern benchmarks like LLM4Mat-Bench [756] advance the field with 1.9 million crystal structures supporting multiple input modalities, revealing important limitations of generalpurpose LLMs in handling specialized representations like CIF files. Question answering capabilities are assessed through MaScQA [108] and MatBookQA [757], which evaluate conceptual understanding and numerical reasoning in materials science. Generative capabilities are tested by GuacaMol [758] and MOSES [690] for molecular design tasks, while multimodal understanding is challenged by MMSci [75] and MaCBench [207], mirroring real-world materials characterization workflows. 4) Life Sciences: Life sciences present particularly diverse evaluation challenges spanning molecular biology, healthcare, agriculture, and neuroscience. At the molecular level, benchmarks progress from DNA sequence understanding through DeepSEA [759], Ensembl collections [760], and GenomicsLong-Range [761] to small-molecule tasks with TOMGBench [762] and MoleculeQA [763]. Higher-level biological reasoning is assessed by LAB-Bench [764] for wet-lab competence, GeneTuring [765] for genomic knowledge retrieval, and Genome-Bench [766] for multi-step CRISPR reasoning. MicroVQA [146] bridges microscopy and molecular function through expert-verified visual question answering. In video domain, SCIVID [767] is cross-domain scientific video benchmark comprising five tasks across animal behavior, medical imaging, and weather forecasting. It includes diverse modalities (grayscale, RGB, multi-channel meteorological data), varying temporal scales, and tasks such as classification, point tracking, and spatiotemporal forecasting."
        },
        {
            "title": "Healthcare",
            "content": "evaluation emphasizes clinical knowledge through both text and visual modalities. Text-based benchmarks include BioASQ [768], PubMedQA [449], and recent comprehensive efforts like MedBench [769], MedXpertQA [770], and HealthBench [771] that approach boardexam rigor. Visual question answering progresses from focused datasets like VQA-RAD [704], PathVQA [149], and SLAKE [772] to more comprehensive multimodal assessments in AMOS-MM [155] and RP3D-DiagDS [773]. More recently, with the rapid progress in Sci-LLMs and scientific agents, some challenging benchmarks have emerged for evaluating these advanced models. RareBench [774] targets rare-disease diagnosis, compiling the largest open-source rare-patient dataset and assessing LLMs across tasks such as phenotype extraction and differential/disease screening. MedAgentBench [775] provides virtual EHR environment with 100 realistic patients and 300 clinician-authored tasks across 10 categories to benchmark medical LLM agents. AgentClinic [776] evaluates multimodal agents by simulating clinical environments that require history taking, clinical interviewing, and sequential decision making. Agents will need to use tools and actively gather useful information through doctorpatient interactions for accurate diagnosis. These suites push evaluation beyond static QA toward interactive, end-toend decision making aligned with real-world practice. Agricultural applications are evaluated through SeedBench [777] for seed breeding capabilities, AgXQA [104] for extension services, and AgEval [189] for plant stress phenotyping. Neuroscience assessment combines knowledgebased evaluation through BrainBench [552] with semantic decoding tasks spanning visual decoding [705], [706], text decoding [711], and clinical applications including sleep classification [713] and emotion recognition [719]. to multimodal assessments Multi-omics modeling has driven unified benchmark development across biological scales. RNA-specific evaluations have evolved from expression matrix tasks in scBERT [638] and scGPT [512] in scMMGPT [639] and comprehensive QA in RNA-GPT [640]s RNA-QA dataset with over 400K entries. Cross-modal integration is exemplified by LLaMA-Gene [40]s genecentric instruction-following, NatureLM [43]s 50+ biomedical dataset evaluation, and ChatNTs 18-task genomics instruction suite covering processes from RNA degradation to protein stability. 5) Astronomy: Astronomy benchmarks utilize diverse data sources, ranging from scientific literature to observational data. AstroLLaMA [561] and AstroMLab [563], [566], [778] utilize arXivs astro-ph category for training and evaluation, while specialized tasks include Astro-NER [725] for entity recognition and Astro-QA [779] for question answering. Observational data processing is addressed through Starwhisperpulsar [780] for pulsar classification, AstroPT [669] for physical simulation acceleration, and visualization tools like ASTROVISBENCH. PAPERCLIP [781] combines text and image data for literature analysis, while Pathfinder [782] provides efficient navigation of large-scale astronomical observations. 6) Earth Science: Earth science benchmarks focus on atmospheric studies and remote sensing applications. Atmospheric evaluation includes text-based ClimaQA [671] and ClimateBERT [783], alongside multimodal WeatherQA [728]. Remote sensing benchmarks such as OceanBench [569], RSIEval [784], and XLRS-Bench [785] emphasize satellite imagery interpretation through tasks including image captioning, visual question answer and visual grounding in ultrahigh-resolution RS scenarios.. Interdisciplinary efforts like OmniEarth-Bench [786], EarthSE [670], and MSEarth [152] integrate data across hydrosphere, biosphere, lithosphere, and cryosphere, challenging models with complex cross-domain reasoning. Across all these scientific domains, evaluation metrics have evolved beyond simple accuracy to include domain-specific measures: AUROC (Area Under the Receiver Operating Characteristic curve) and MCC for imbalanced biological data, exact match and MSE for symbolic regression, Expression Edit Distance for physical reasoning, validity and synthetic accessibility for molecular generation, and multimodal metrics like IoU (Intersection over Union) for visual grounding tasks. These benchmarks collectively reveal that while foundation models show promise in scientific applications, significant gaps remain in handling specialized representations, crossmodal reasoning, and the integration of domain expertise with general language understanding. 40 only models and embeds questions in images to enforce genuine visual-linguistic integration, yielding substantially lower accuracies than on the original set. Graduate-level sets like GPQA [457] and SuperGPQA [791] target expert-authored, Google-proof scientific reasoning across biology, physics, and chemistry (and hundreds of graduate disciplines in SuperGPQA), helping to expose reasoning gaps that remain hidden on easier general-purpose tests. These methodological choices clarify what is being measuredfact recall, modality integration, or multi-step reasoning; they help explain why success on broad academic exams does not automatically translate to scientific cognition under stricter evidence conditions. Notably, there is significant performance gap between general academic benchmarks and domain-specific scientific challenges. As shown in Fig. 23, while leading closed-source models achieve 80-95% accuracy on MMLU-Pro [81], their performance drops dramatically on frontier scientific stress tests like Humanitys Last Exam (HLE) [462] and Scientists First Exam (SFE) [443]. Specifically, most models score only 2-10% on HLE across various domains, with chemistry showing the best but still poor results. On SFE, despite relatively better performance in materials science, accuracy remains low at 20-40% in other scientific domains. This stark contrast reveals that current LLMs, despite excelling at general knowledge tasks, struggle significantly with tasks requiring deep scientific reasoning and domain expertise. Consequently, evaluation methodology in general science is pivoting toward designs that make reasoning requirements explicit and verifiable. Fixed-choice protocols report accuracy implicitly test calibration via distractor design, makbut ing them sensitive to ambiguity and annotation artifacts; MMLU-Pros ten-option format and curated hard negatives reduce chance performance and inflate the penalty for shallow heuristic. MMMU-Pros vision-only setting removes textual crutches, isolating visual understanding from language priors and better reflecting figure-centric scientific communication. SFE formalizes multimodal scoring with IoU, BERTScore, and LLM-as-a-Judge for structured visual tasks, while HLE introduces calibration error alongside accuracy to quantify overconfidence on hard scientific questions. Programmatic tasks like LLM-SRBench [747] enable exact-match and MSE for equations, and broad suites such as SciEval [792] and SciKnowEval [442] aggregate multiple task families with diverse metrics to reflect the varied outputs typical in science. Together, these evaluations complement broad academic tests by injecting domain-shaped modalities, harder question design, and metric pluralism, thereby offering more faithful picture of scientific reasoning than can be obtained from general benchmarks alone. B. Evaluation Data Analysis To understand the landscape of scientific evaluation benchmarks, we first examine the distribution of data sources and benchmark characteristics across domains. Fig. 24 reveals striking pattern: most scientific domains rely heavily on single dominant source type, with academic and research 41 Fig. 23: Performance of leading closed-source models drops significantly on challenging scientific benchmarks (HLE [462], SFE [443]) compared to MMLU-Pro [81] across multiple domains. Top to bottom: HLE, SFE (en), MMLU-Pro. 7) General Science: General-purpose science benchmarks have coalesced into three major strands: exam-style text QA that samples broadly across disciplines [81], [787], [788], multimodal figure/image QA that reflects the visual nature of scientific communication [80], [604], [789], [790], and specialized formats that probe symbolic or programmatic reasoning beyond free-form answers [747]. Exam-style suites such as MMLU [81], C-Eval [787], and AGIEval [788] provide wide coverage from secondary to undergraduate levels in both English and Chinese, enabling coarse-grained cross-lingual comparisons but often emphasizing short multiple-choice formats. Yet, as models saturated these leaderboards, newer variants emphasized robustness, harder distractors, and reasoning-heavy prompts (e.g., MMLU-Pro [81]). In parallel, multimodal suites such as ScienceQA [80] and MMMU [604] advanced beyond text by combining images, diagrams, tables, and interleaved text; MMMU-Pro [789] further filters out items answerable by textFig. 25: Word clouds of the scientific benchmarks. The plots show the relative distributions of modalities (left) and types (right), with word size proportional to frequency. entific domains where expert knowledge is crucial [736], [768]. For instance, MicroVQA [146] employs 12 human annotators for microscopy image question-answering, while OmniEarth-Bench [786] utilizes over 40 annotators to ensure comprehensive coverage of Earth science domains. Semi-automated pipelines balance speed and fidelity by pairing LLM/tooling with expert review: Genome-Bench drafts with GPT-4 models before human checks [766]; MM-PhyQA blends ChatGPT and scripts with over eight reviewers [793]; RP3D-DiagDS couples custom crawlers and GPT-4 with specialist adjudication [773]. However, recommended practices (e.g., annotator training, pilot studies, iterative refinement) are still too rarely documented. Fully automated pipelines achieves efficient annotation using established computational frameworks: the Genomics Long Range benchmark synthesize targets from experimental/computational protocols [759], [761]; USPTO mines 1.9M patents programmatically [612]; RSVQA-LRBEN generates million-scale remote-sensing QAs by rule-based analysis of satellite imagery [794]. This maximizes coverage and efficiency. However, benchmarks that involve LLMs as autoannotation tools raise risks of (i) circularity and contamination when the same or closely related LLMs are later evaluated on LLM-labeled data, and (ii) propagation of potential inaccuracies and biases in LLM-based annotation. Such problems are even harder to review, because LLMs can produce nuanced, plausible, yet erroneous answers at scale, which are often difficult to validate without high-level expertise. This highlights the need for careful validation even in automated pipelines [745], [795]. typical i.e., difficulty, their difficulty entirely, 2) Skewed Knowledge Level with Increasing Difficulty: The knowledge level required by the evaluation datasets, is under-specified and skewed. large fraction of recent datasets do not provide information about in integrated or web-mined corpora where provenance is diffuse (e.g., OmniMedVQA [796], VRSBench [797], SRBench [746]). Among those that do specify, there is polarization: highstakes or research-level resources tag themselves Expert (e.g., PhysicsArena [681], LLM4MatBench [756], RP3DDiagDS [773], MedXpertQA [770]) while exam/educationstyle benchmarks cluster at Undergraduate (e.g., UGPhysics [740]; PHYSICS [742], MEDIQA-AnS [798]) with very few Intermediate slices to chart capability boundaries across continuum [106]. Cross-sectioning by release date suggests the skew is increasing: 20242025 saw wave 42 Fig. 24: Source distribution of existing evaluation corpora for scientific LLMs/MLLMs, normalized within each domain. Most domains rely on single dominant source type, showing todays headline scores often reflect proficiency with one writing style or data type rather than robust, cross-domain scientific reasoning, highlighting the need for broader, more heterogeneous evaluation suites. resources dominating in Physics and Chemistry, while Life Sciences shows slightly more diversity. This homogeneity in source materials raises concerns about the robustness and generalizability of current evaluation suites, as models may overfit to specific data types rather than developing broad scientific reasoning capabilities. Fig. 25 further illustrates the composition of these benchmarks through word clouds, where the prevalence of text-based QA formats and specific modalities like VQA and Text-QA highlights the current emphasis on question-answering paradigms, while revealing gaps in coverage of other important scientific tasks such as hypothesis generation, experimental design, or cross-domain reasoning. These visualization patterns motivate deeper analysis of how scientific benchmarks are constructed and what they actually measure. Across recent benchmarks for evaluating Sci-LLMs/MLLMs, we observe several patterns. 1) Tiered Regime in Data Generation and Annotation: Annotation in scientific benchmarks shows tiered, hybrid regime: manual expert curation anchors quality in hard domains, semi-automated human-in-the-loop pipelines deliver scale with control, and fully automated systems enable extreme throughput where labels can be programmatically derived. Each of them trades off quality, scalability, and resource requirements."
        },
        {
            "title": "Manual annotation remains prevalent",
            "content": "in specialized sciof expert-labeled clinical and science sets (MedXpertQA 2025.01; PhysicsArena 2025.05) alongside new Undergraduate exam corpora (UGPhysics 2025.01; PHYSICS 2025.03). These expert-level benchmarks demand not only deep domain knowledge but also the ability to synthesize information from and reason on multimodal and cross-domain cues. Medical benchmarks particularly exemplify this with requirements of complex reasoning on rare diseases [774] and dubious cases [771]. Questions in these benchmarks are typically designed to be Google-proof [457] and entangled, requiring genuine understanding and multi-step thinking [739] rather than simple memorization, setting particularly high bar for model evaluation. The emergence of expert-level benchmarks could be attributed to the need for testing the limits of capability of frontier LLMs, and also reflects growing recognition that scientific reasoning requires not just factual knowledge but the ability to apply, analyze, and create new understanding [799]. towards Domain-Specific Metrics: In terms of evaluation methods and metrics, question-answering form is the prevailing evaluation, but the metrics are evolving from simple accuracy measurements to sophisticated multi-faceted, domain-specific assessment frameworks, reflecting the heterogeneity of scientific problems. 3) Shift Scientific benchmark datasets designed for modern Sci- (e.g., LLMs typically focus on closed-ended questions multiple-choice questions, True/False problems), where the exact answers can be easily extracted from the outputs of SciLLMs using regular expressions; the dominant evaluation metrics are simple and objective: exact match and accuracy. Such single universal score, however, provides limited insights on the capability of Sci-LLMs, and is difficult to employ in openended questions. Benchmarks that require natural language generation frequently adopt n-gram overlap (BLEU/ROUGE) to compare free-form outputs against references [149], [798]. However, these surface-form metrics do not consider semantic correctness. BERTscore [800], as employed in some benchmarks [152], [801], mitigates this problem by comparing the embedding similarity between Sci-LLMs responses and gold answers, yet the semantic similarity still does not guarantee factual correctness and underweights negation and nuanced meanings. Domain-anchored measures are strongest where the science supplies mature targets: in genomics and multi-omics, AUROC/AUPRC are standard for association and retrieval (e.g., DISEASES [802], repoDB [803]), while regression tasks [804] adopt R2, RMSE, or Pearsons correlation coefficients (PCC) to quantify effect-size prediction rather than linguistic plausibility. Chemistry emphasizes chemical validity and drug-likeness for molecular generation, rightly scoring whether molecules are synthesizable and pharmacologically plausible [215], [216]. Physics benchmarks illustrate metric specialization along two axes: exact string/structure match for symbolic regression [222], [746], which verifies whether discovered closed-form is the same function, and step-wise or explanation-sensitive grading [745] that penalizes reasoning drift even when final answers coincide. The merit of this trend is clear: metrics are increasingly aligned with the scientific target, enabling faithful model selection and revealing failure modes that generic QA accuracy would hide. But there are risks. First, narrow metrics can be gamed (e.g., maximizing BLEU without factual grounding, or optimizing AUROC under pathological class priors). Also, portfolios are inconsistent across datasets, impeding crossdomain comparison. Furthermore, many QA/VQA sets still rely on overlap-based or single-number accuracy for openended tasks, under-measuring calibration, citation faithfulness, and harm [771]. Looking forward, future scientific benchmarking should (i) pair task-native objectives with calibration and uncertainty reporting (e.g., ECE/Brier alongside AUROC for DISEASES [802]); (ii) add process-aware scoring that evaluates intermediate steps and evidence use [745]; (iii) incorporate reference-grounded factuality/citation checks for text outputs so model must justify answers beyond n-grams [149], [798]; and (iv) standardize multi-metric dashboards per domain to avoid metric gaming and improve comparability across releases [785], [797], [805]. C. LLM / Agent as Judge With the rapid advancements in LLMs and multimodal generative models, traditional evaluation methods, which often rely on single numerical score (e.g., accuracy) or require extensive manual labor, have become inadequate. To address this challenge, an emerging trend is to use agentic systems to evaluate other agents or models. This Agent-as-a-Judge paradigm is natural extension of LLM-as-a-Judge [806] and provides richer, more reliable evaluations by incorporating agentic features like dynamic planning and intermediate feedback  (Fig. 26)  . The primary advantages of the Agent-as-a-Judge framework are its flexibility, efficiency, and explainability. It typically employs multi-round, dynamically adjusting evaluation process that mimics the strategies of human experts. During this process, the agent judge can dynamically adapt its evaluation direction and test cases based on intermediate results and observed feedback. This approach moves away from reliance on fixed benchmarks and large sample sizes, significantly reducing the time and computational cost required for evaluation. For instance, in code generation [807], the agent judge can evaluate developer agents performance on multi-step tasks, not just the final outcome. In the domain of visual generation [808], an evaluation agent can conduct multi-round assessments based on an open-ended user query, ultimately providing detailed natural language analysis and summary rather than just simple numerical score. This provides deeper insights into models strengths and weaknesses. In the hypothesis generation task [809], [810], judge agent evaluates the novelty, validity, and coverage of key points in the proposed hypotheses, which is well-suited to their inherently flexible and open-ended nature. This trend has profound implications for future evaluation in the scientific domain, particularly for automated scientific discovery. Automated scientific discovery often involves complex, multi-step tasks where the outcome cannot be easily quantified with single metric. Traditional evaluation methods 43 Fig. 26: The evolution of evaluation methods for LLMs, starting from simple Right or wrong exact matches and progressing to semantic similarity comparisons for open-ended answers with metrics like BERT-Score [800]. More advanced methods include using an LLM as judge to generate reasoning reports, culminating in the use of multiple agents and tools within an experimental environment for scientific discovery to provide comprehensive model assessment. are ineffective at capturing the intermediate processes and pinpointing failures within these tasks. The Agent-as-a-Judge framework addresses this by providing rich intermediate feedback and comprehensive analysis of the entire process. D. Inspiration from Test-Time Learning Test-Time Learning (TTL) is gaining significant traction in the natural sciences due to its unique value proposition. First, scientific benchmark evaluation inherently involves working with test sets that lack ground-truth answers, which perfectly fits TTLs paradigm of adaptation at inference time without requiring labeled data [811]. On the other hand, datasets in the natural sciences exhibit strong heterogeneity and distribution shift. For example, Earth sciences encompass atmospheric, oceanic, remote sensing imagery, and textbook text, with large differences in data structure and semantics within each subdomain. Conventional, statically pretrained LLMs often underperform when confronted with data distributions markedly different from their training corpus, whereas TTL enables immediate adaptation by dynamically updating parameters or reasoning strategies using currently observed, unlabeled test samples. TTLs practical application in the natural sciences manifests in several technical pathways. MedAdapter [812] employs post-hoc adapters for TTL in biomedical applications. Across four biomedical reasoning tasks and eight datasets, the performance of white-box LLMs improved by 18.24%, while the performance of black-box LLMs improved by 10.96%. In the field of chemistry, [813] proposes scaling test-time training with reinforcement learning for chemical language models to improve chemical space exploration on their proposed benchmark, MolExp, which focuses on discovering structurally diverse molecules with similar bioactivity. Evaluation results on MolExp reveal that extending increasing the TTL will improve model performance, but the performance gains will diminish if the TTL time is too long. In theoretical physics, Gao et al. [814] proposed symbolic weak-verifier framework in TTL to enhance performance on the TPBench [744] physics dataset. VII. SCIENTIFIC DATA DEVELOPMENT This section examines how scientific data influences model development across various stages including data collection, training, and evaluation, highlighting systemic limitations and emerging opportunities. We begin by analyzing the methodologies in scientific data construction (Sec. VII-A), and then point out critical limitations of current datasets (Sec. VII-B). Finally, we identify deeper structural issues that hinder the usability of scientific data for LLM development (Sec. VII-C). A. Data Collection and Labeling The development of Sci-LLMs fundamentally depends on the quality of their training data; our analysis of existing datasets reveals complex landscape of acquisition and annotation practices that vary across domains, reflecting both the heterogeneous nature of scientific knowledge and the practical constraints of dataset construction. This subsection discusses three aspects that outline the key factors shaping how scientific datasets are constructed and curated for LLM development, including: (i) data source heterogeneity and acquisition strategies (Sec. VII-A1), which describe the diversity of infrastructures and repositories that supply scientific data; (ii) annotation methodologies and quality control (Sec. VII-A2), which address the pipelines and validation processes used to 44 Fig. 27: Scientific data construction pipeline: multi-source data acquisition, data synthesis pipelines for pre-training, posttraining and evaluation stages, and comprehensive review framework incorporating intrinsic evaluation, extrinsic validation and human-in-the-loop feedback with five quality criteria (safety, fidelity, accuracy, diversity, privacy). ensure data reliability; and (iii) cross-domain patterns and domain-specific considerations (Sec. VII-A3), which highlight recurring challenges such as bias, ethical constraints, and disciplinary practices. 1) Data Source Heterogeneity and Acquisition Strategies: The scientific data ecosystem exhibits remarkable diversity in its sources, with each domain developing distinct acquisition strategies tailored to its knowledge infrastructure. Academic and research resources constitute the primary foundation (Figs. 21 and 24), accounting for the majority of datasets across all disciplines. In life sciences, repositories like PubMed Central and specialized databases such as MIMIC-CXR [153] provide structured access to millions of medical images and clinical reports. The astronomy domain leverages arXiv extensively, with datasets like AstroLLaMA [561] utilizing over 300,000 abstracts, while materials science relies heavily on computational databases like the Materials Project and experimental repositories such as USPTO [612] patents. This reliance on established scientific infrastructure presents both advantages and limitations. While peer-reviewed sources ensure data quality and scientific validity, they introduce significant temporal delayspublications typically lag behind actual discoveries by months or years, creating what the paper identifies as data latency problem. Moreover, the dominance of English-language sources creates linguistic bias, with Chinese-language datasets primarily confined to healthcare applications like CMB-Exam [815] and agricultural resources like CROP, despite substantial scientific contributions from non-English speaking regions. Web-scraped content emerges as secondary but increasingly important source, particularly for multimodal data. Remote sensing datasets like RS5M [730] aggregate millions of satellite images from online repositories, while medical education platforms contribute to datasets like MedDialog [701]. However, the quality and reliability of web-sourced data vary considerably, necessitating sophisticated filtering mechanisms. Patent databases represent unique intersection of scientific and commercial knowledge, particularly valuable in chemistry and materials science, where USPTO provides access to nearly 2 million chemical reactions with detailed experimental procedures often absent from academic publications. 2) Annotation Methodologies and Quality Control: The scientific data synthesis employs sophisticated multi-track pipeline architecture designed to address the distinct requirements of pre-training, post-training, and evaluation phases  (Fig. 27)  . The pre-training synthesis pipeline begins with data deduplication to eliminate redundancy across heterogeneous sources, followed by quality-based filtering that removes lowvalue content. Selected data undergoes strategic mixing to ensure balanced representation across scientific domains, creating diverse foundation for initial model training. This relatively straightforward process prioritizes scale and coverage over precision, establishing broad scientific knowledge bases. the post-training synthesis pipeline implements more stringent quality controls tailored for instructionfollowing capabilities. Domain-specific filters first categorize content by scientific subdisciplines, after which quality filters apply elevated standards including factual verification and citation validation. The pipeline then enhances underrepresented domains through targeted synthesis and implements structural templates to standardize instruction-response formats. This refined approach ensures that post-training data not only maintains scientific accuracy but also follows consistent patterns that facilitate effective fine-tuning. In contrast, The evaluation data synthesis pipeline represents the most rigorous track, beginning with careful task design that spans multiple cognitive levels from basic factual recall to complex multi-step reasoning. Question creation generates diverse query types including multiple-choice questions with scientifically plausible distractors, open-ended problems requiring detailed explanations, and multi-hop challenges that test reasoning capabilities. Each answer undergoes meticulous construction with step-by-step derivations and comprehensive explanations, followed by multi-round quality assurance to 45 validate both scientific accuracy and logical coherence. These pipelines produce three distinct categories of synthesized data. Instruction-response pairs encompass sequencebased formats for procedural knowledge, symbol-based representations for mathematical and chemical notations, and code implementations for computational tasks. Knowledge and QA pairs include both alignment data for factual grounding and chain-of-thought examples that demonstrate explicit reasoning processes. Open-ended QA pairs, primarily used for evaluation, feature both multiple-choice questions and complex problems requiring detailed explanations. The synthesized evaluation data undergoes comprehensive human-in-the-loop review across six critical dimensions. Safety checks ensure no harmful scientific misinformation, while accuracy validation verifies factual correctness against authoritative sources. Diversity assessment confirms broad coverage across subdomains and question types, and fidelity review maintains consistency with established scientific principles. Privacy screening removes any personally identifiable information, and throughout this process, domain experts provide iterative feedback to refine data quality. This rigorous validation framework proves essential for evaluation datasets, as they serve as definitive benchmarks for assessing model capabilities in scientific reasoning and knowledge application. 3) Cross-Domain Patterns and Domain-Specific Considerations: Despite domain-specific variations, several patterns emerge across scientific data collection efforts. The transition from individual datasets to integrated ecosystems characterizes modern approaches, with initiatives like GMAI-VL [541] in healthcare aggregating 5.5 million multimodal examples across institutions. This consolidation addresses fragmentation but introduces new challenges in maintaining provenance and ensuring consistent quality standards across heterogeneous sources. Domain expertise requirements create natural barriers to cross-disciplinary data sharing. Medical datasets require understanding of clinical workflows and regulatory constraints, while astronomical data demands familiarity with coordinate systems and instrumental calibrations. Agriculture occupies unique position, requiring integration of biological knowledge with environmental monitoring, resulting in datasets like MIRAGE [190] that combine expert agricultural consultations with field imagery. There is concerning trend toward annotation convenience rather than scientific completeness. Datasets often reflect what is easily accessible rather than what is scientifically importantpublished positive results dominate while negative findings remain largely absent. This bias extends to experimental conditions, with datasets capturing idealized scenarios rather than the messy reality of scientific practice. Materials science datasets focus on computationally generated structures while experimental synthesis failures go unrecorded, creating an incomplete picture of the scientific process. Privacy and ethical considerations impose additional constraints, particularly in life sciences. While physics and astronomy data are generally open, medical datasets require extensive de-identification and access controls. This creates fundamental tension between data availability and patient protection, resulting in geographic and demographic biases as datasets predominantly originate from well-resourced institutions in developed countries. Agricultural datasets face similar challenges with proprietary farming data, limiting the diversity of crop varieties and growing conditions represented in publicly available resources. B. Limitations of Current Scientific Datasets Despite rapid growth in scientific corpora, current datasets exhibit significant limitations in scope, granularity, and modality coverage. This subsection characterizes fundamental challenges that constrain the training and evaluation of Sci-LLMs, including: (i) the scarcity of experimental data (Sec. VII-B1), which arises from the high cost of data acquisition and the rarity of scientific phenomena; (ii) the over-reliance on text modality data (Sec. VII-B2), which limits multimodal reasoning and reduces empirical grounding; (iii) the representation gap between static knowledge and dynamic processes (Sec. VII-B3), showing how current datasets fail to capture the evolving nature of scientific inquiry; and finally, (iv) the multilevel biases (Sec. VII-B4) that stem from publication practices, language dominance, and domain skew, all of which impact the fairness and generalizability of Sci-LLMs. 1) Scarcity of Experimental Data: The scarcity of experimental data in scientific domains stems from several inherent characteristics of scientific data. These factors collectively hinder the development of data-intensive scientific LLMs and MLLMs. The first characteristic is the high acquisition cost in experimental data generation. Scientific experimentation is often extraordinarily expensive and time-consuming. Experimental research frequently faces significant financial constraints that cause sufficient experiments to yield statistically reliable results. For instance, in drug discovery, obtaining accurate protein structures is essential for understanding molecular interactions, but it requires costly wet-lab experiments and specialized equipment like cryo-electron microscopes, X-ray crystallography. Similarly, generating high-fidelity simulation data [384], [402], [596], which can serve as proxy for experimental data in scientific machine learning, typically demands substantial computational resources and long processing time to generate datasets of adequate size. This inherent financial and temporal burden directly restricts the scale and diversity of experimental datasets. The traditional pace of scientific investigation, constrained by these resource limitations, often struggles to match the data demands of modern AI models, creating fundamental bottleneck. In healthcare, access to clinical data usually requires rigorous ethical review and carries privacy risks, constraining their widespread availability and scalability. Another inherent unique challenge causing the data scarcity is the rarity of specific scientific phenomena. Unlike other forms of scarcity that might be mitigated through increased resources or improved collection methods, this type of scarcity is intrinsic to the natural world or specific experimental conditions. For example, in healthcare, research into rare diseases is perpetually hampered by the limited availability of patient data, directly impeding the development of effective treatments and diagnostic tools. This means that AI 46 models designed for these domains must be capable of learning effectively from extremely limited examples, as the underlying phenomena themselves are inherently infrequent. The lack of AI-ready experimental data is another key challenge in building effective scientific LLM models. Experimental data in the natural sciences suffer from heterogeneity and the lack of standardization [816], as they come from diverse instruments, protocols, and domains, each with its own formats, units, and conventions. Without community-adopted standards for data schemas and metadata fields, integrating datasets across labs or domains becomes labor-intensive and error-prone task. As result, crucial contextual information (e.g., experimental conditions, calibration details) are often omitted or encoded inconsistently, forcing AI practitioners to spend disproportionate effort on data preprocessing rather than model development. 2) Over-reliance on Text Modality Data: Current scientific corpora for LLMs and MLLMs rely heavily on published articles, patents, and reviews, which are rich in descriptive content but poor in raw experimental detail [30], [41], [452], [453]. This over-reliance on the text modality introduces several issues. First, scientific datasets tend to prioritize aggregated summaries over raw measurements, leading to limited quantitative depth. Textual reports often present averaged results without revealing underlying data distributions. Consequently, models are never exposed to the full variability of experimental outcomes, limiting their capacity to reason about uncertainty or discern fine-grained trends. Second, textbased scientific literature often exhibits selection and reporting bias. Authors typically highlight statistically significant or positive findings, while omitting negative results or methodological failures. This causes skewed perception of science as linear and uniformly successful process. Beyond textual limitations, current scientific datasets suffer from scarcity of structured experimental data, as detailed in Sec. VII-B1. Machine-readable protocols, equipment settings, and raw timeseries measurements are rarely shared in standardized formats [337], [817]. Without detailed reagent tables, step-bystep procedures, or high-resolution simulation outputs, models cannot infer the precise cause-effect relationships that drive scientific discovery. Moreover, many key scientific modalities are either excluded or available only as low-resolution figures embedded in PDFs. These include spectra, microscopy images, chromatography traces, and raw sensor streams. Without high-quality multimodal signals, MLLMs lack the empirical grounding to connect textual hypotheses with experimental evidence. Overall, the imbalance between descriptive text and scientific modality data severely limits models ability to generalize from narrative summaries to the rigorous, datadriven reasoning required in cutting-edge research. Bridging this gap will require more complete, structured, and multimodal experimental datasets. 3) Representation Gap between Static Knowledge and Dynamic Processes: Scientific datasets usually provide static snapshots of knowledge at the time of collection, which fails to reflect the continuously evolving nature of scientific discovery. In contrast, scientific progress is iterative cycle of formulating hypotheses, testing them against emerging data, and refining through continuous experimentation and analysis. This mismatch between static data and the dynamic research process creates significant representation gap: models trained on these one-off datasets struggle to make reliable predictions or conduct meaningful reasoning about evolving phenomena. The gap is particularly pronounced in observational records, experimental results, and scientific QA benchmarks that often rely on predetermined questionanswer pairs from published sources. The static nature of these collections leads to knowledge expiration as new findings emerge, thereby undermining their relevance and validity. As facts change, models trained on these snapshots may yield outdated or even contradictory conclusions, which impedes their utility for real-time reasoning and hypothesis generation that requires up-to-date evidence and iterative feedback. 4) Multi-level Biases in Scientific Datasets: Scientific datasets contain systematic biases that embed skewed perspectives into the training of LLMs. These biases arise when data deviates from comprehensive scientific reality, including publication bias, domain bias, author and institutional biases. Understanding these biases is the crucial step toward building fairer and more accurate AI models. Publication bias leads to an overabundance of positive results, as studies with statistically significant findings are up to three times more likely to be published than those with null results [818]. This dismissal of negative or inconclusive data distorts the available evidence. Language bias reinforces the dominance of English, as English-language publications make up the vast majority of accessible scientific literature [819]. This causes models to misrepresent or underperform on scientific work from other languages and cultural contexts. Pervasive domain bias exists in repositories such as PubMed, which disproportionately focus on the life sciences and biomedicine, while underrepresenting disciplines like physics, chemistry, and social sciences. This impairs the ability of LLMs to generalize across scientific domains. Finally, author and institutional biases emerge when small number of prolific researchers or elite institutions contribute disproportionately. This phenomenon imprints specific writing styles and thematic focuses, causing models to mirror dominant voices rather than reflect the full diversity of scientific discourse. Addressing these systematic biases through corpus diversification, targeted augmentation of underrepresented domains, and bias-aware sampling is essential for building fairer and more reliable scientific LLMs. C. Systematic Issues in Data Quality Beyond surface-level limitations, the scientific data ecosystem suffers from systemic issues that undermine the datadriven scientific AI. This subsection highlights three critical areas that must be addressed to support robust Sci-LLM development. First, we describe the data traceability crisis (Sec. VII-C1), where missing provenance and undocumented preprocessing hinder reproducibility and trust. Next, we explore scientific data latency (Sec. VII-C2), which delays the incorporation of recent discoveries into model training and limits real-time scientific reasoning. Finally, we focus on the lack of AI-readiness (Sec. VII-C3), emphasizing how poor formatting, missing metadata, and domain-specific heterogeneity prevent many datasets from being directly used in LLM pipelines. These structural deficiencies highlight the need for end-toend redesign of scientific data practices, enabling continuous, traceable, and AI-compatible knowledge integration. 1) Data Traceability Crisis: The scientific data traceability crisis in building LLMs and MLLMs for various science domains poses significant challenge to the integrity and utility of AI-driven scientific discovery. The data traceability crisis stems from inconsistent, incomplete, and often undocumented management of the diverse scientific datasets used to train these complex models. The metadata of scientific datasets describing sample provenance, processing details and versioning information are often sparse or missing. Fundamentally, this deficiency in transparency and auditability may undermine scientific rigor and reproducibility. Subsequent researchers struggle to reconstruct how scientific data are generated and transformed. It exacerbates existing problems such as bias propagation, introduces considerable legal and ethical liabilities, and complicates the crucial process of validating AI-generated scientific hypotheses. Also, there is increasing difficulty in distinguishing synthetic from real experimental data. Recent analyses show systematic under-utilization of roughly three-quarters of online data repositories, largely due to insufficient data traceability [820]. The cumulative effect could diminish the trust in AI systems, particularly within high-stakes scientific applications ranging from novel drug discovery to precise medical diagnostics. Addressing this issue necessitates comprehensive strategy that integrates advanced technological solutions with robust data governance frameworks, clear regulatory guidelines, and sustained commitment to fostering greater transparency and accountability throughout the AI development lifecycle. 2) Scientific Data Latency: Scientific data latency refers to the delay between when new experimental results, publications, or datasets are generated and when they become available for scientific LLM to ingest. This latency issue undermines model accuracy, reliability, and relevance, particularly in fast-evolving fields such as biomedicine, climate science, and materials science, where new discoveries can quickly render older information obsolete. The data latency issue arises from several aspects. First, many scientific findings appear only after lengthy peer-review and publication processes with datasets remain inaccessible, delaying their inclusion in model training. Second, even publicly released data often lack standardized metadata or real-time update mechanisms, causing models to train on out-of-date versions of datasets. Third, highthroughput instruments and simulation platforms can produce terabytes of data daily, but bandwidth constraints, qualitycontrol pipelines, and manual curation introduce additional lags before data are transformed into machine-readable formats. As result, scientific AI models may perpetuate outdated knowledge, overlook the latest experimental protocols or discoveries, leading to increased risk of hallucination when faced with unfamiliar recent developments. Addressing data latency requires the adoption of open-access policies and development metadata standards to enable automatic updates to training corpora. 3) The Lack of AI-readiness: In the era of scientific AI, scientific data needs to be readily consumable by AI models, seamlessly integrating into their training and inference processes to support automated and scalable scientific discovery. Despite their immense potential, many scientific datasets are underutilized due to their lack of AI-readiness, posing significant challenges for scientific LLM development. This incompatibility issue stems from incomplete essential metadata, insufficient preprocessing, mismatched structures, and the inherent complexities of diverse scientific information, making direct utilization for model training difficult. Such limitations impede immediate usability, forcing researchers to invest substantial effort in data adaptation rather than accelerating LLM-driven scientific discovery. The majority of published scientific data require extensive preprocessing, curation and enrichment before they become AI-ready, significantly slowing down progress in building domain-specialized LLMs and other data-driven scientific tools. To bridge this gap, the scientific community must shift from simply making data available to ensuring it is truly actionable. VIII. NEW PARADIGMS FOR DATA-DRIVEN SCI-LLMS New paradigms are emerging that reimagines Sci-LLMs not just as passive predictors but as active, goal-directed systems, i.e., agents, capable of autonomy, interactivity, and orchestration across tools and tasks [821]. This section explores two major shifts shaping the future of Sci-LLMs. First, we examine the emergence of scientific agents (Sec. VIII-A), which transform Sci-LLMs into autonomous entities that emphasize planning, experimenting, and self-improving. Then, we analyze how data ecosystems for Sci-LLMs must be redesigned to support these agents (Sec. VIII-B). A. Scientific Agent"
        },
        {
            "title": "A key paradigm shift",
            "content": "is treating LLMs as scientific agents that can plan and execute research tasks with degree of autonomy. This subsection introduces key developments in this direction, beginning with brief introduction on the transition from Sci-LLMs to scientific agents (Sec. VIII-A1), followed by the concept of multi-agent collaboration (Sec.VIII-A2). Next, we explore the integration of external tools (Sec. VIII-A3), which enable agents to interact with databases, software, and real-world systems. We also discuss self-evolving agents (Sec. VIII-A4) that refine their skills, prompts, and tool usage through iterative feedback. Then, we highlight emerging evaluation frameworks and benchmarks (Sec. VIII-A5) that rigorously assess agents on end-to-end workflows, collaboration, and safety in scientific tasks. Finally, we introduce the application of scientific agents on autonomous scientific discovery (Sec. VIII-A5). 1) LLMs as Scientific Agents: Rather than simple question-answering, scientific LLM agent is given high-level goals (e.g., discover potential drug candidates for disease X) and autonomously decomposes the task, gathers information, performs experiments (virtually), and synthesizes results [822]. These agents maintain structured, hypothesis-driven workflows that echo the scientific method: defining hypotheses, 48 selecting experimental methods, and validating results before drawing conclusions. Crucially, they emphasize reproducibility and scientific rigor, incorporating domain-specific constraints and verification steps that generic AI assistants often lack. Studies have highlighted that accelerating discovery requires capabilities beyond generic chatbots for instance, generating novel hypotheses, designing and running experiments, and interpreting complex data in context [18], [44]. By building these capabilities, LLM-based scientific agents aim to serve as AI co-researchers that can handle tedious or complex aspects of research, allowing human scientists to focus on creativity and high-level decisions. real reflect 2) Multi-Agent Collaboration: Recent scientific agents have shifted from single monolithic planners to structured laboratory roles and social dyteams that namics [53]. The Virtual Lab [54] organizes principalinvestigator agent and specialist scientist agents into recurring research meetings, demonstrating end-to-end design of SARS-CoV-2 nanobodies and validating wet-lab outcomes; the setting formalizes division of labor, critique, and iteration, and reports meaningful human-in-the-loop oversight while preserving agent autonomy. VIRSCI [55] models team formation explicitly for idea generation, showing that diversified agent roles and controlled disagreement increase novelty without sacrificing feasibility. PiFlow [56] adds principle-aware collaboration for hypothesis refinement by constraining agent proposals with physical/biological priors to reduce aimless exploration, common failure mode in free-form multi-agent pipelines. At the system level, Agent Laboratory [57] frames an entire paper-production pipelineproblem scoping, method selection, execution, analysis, and writingvia cooperating agents with persistent artifacts and audit trails. For embodied science, ChemAgents [58] deploy hierarchical multi-agent controller onboard robotic chemist to coordinate experiment planning, execution, and self-correction across hardware and simulation. Beyond homogeneous LLM teams, hybrid collectives of agents and humans (e.g., steering committees) have become standard, with explicit critique-and-revision loops and role-switching when agents detect stale priors or tool failures [823]. Empirically, multi-agent settings yield the largest gains when: (1) roles are capability-aligned (planner, critic, executor); (2) communication channels are structured (RFA templates, meeting minutes, explicit claimevidence schemas); and (3) conflict resolution is formalized (voting, debate, or auctioning). Science-centric multi-agent benchmarks, e.g., MultiAgentBench [84] for coordination/competition and communicative multimodal tasks, now make such interaction skills measurable. 3) Tool Use: defining feature of scientific LLM agents is their heavy integration with external tools and data resources [822]. SciToolAgent [59] organizes hundreds of domain tools via knowledge-graph of capabilities, preconditions, and I/O signatures; the graph enables retrievalaugmented tool selection, multi-hop sequencing, and faultaware backoff across several domains. It reports consistent gains over vanilla tool-calling baselines on curated scientific workflows and adds policy checks for responsible use. Biomni [18] exemplifies domain-scale agent that interfaces with 150 tools, 59 databases, and 105 software packages to automate biomedical analyses end-to-end, emphasizing reproducibility and provenance. Under the hood, modern stacks increasingly adopt the Model Context Protocol (MCP) [824] to standardize tool discovery, authorization, and invocation, reducing glue code and enabling safer cross-vendor orchestration; MCP also clarifies user consent and credentials for tools that execute code or reach sensitive data. For web-facing evidence gathering, computer-using and browsercontrol agents [825] have matured from ad hoc headless scripts to trained GUI/web agents that read, click, and upload files, with reinforcement these unlock literature mining, data extraction, and online lab logistics but raise security issues (e.g., prompt-injection, DOMmismatch), motivating sandboxes and allowlists [826]. For workflow synthesis, WorkflowLLM and WorkflowBench [85] explicitly evaluate whether an agent can translate naturallanguage protocols into executable API graphs and recover from tool failures; results indicate that specialized workflowtuned models can outperform general LLMs even with incontext learning. Overall, the state of the art combines: symbolic resource models (capability graphs, ontologies), standardized tool transport, execution sandboxes (containers, rate caps), and reflective monitors that detect hallucinated tools or unsafe parameterizations before launch. learning on screen traces; 4) Self-evolving Agents: Self-evolving agents extend scientific LLM agents by adding continual adaptation loops to the standard planexperimentverify workflow, so the agent improves itself over time, not just the artifact. Intra-test-time, agents externalize feedback and update episodic memory or prompts to correct future trials, boosting sequential decision making and coding without weight updates [827], [828]. Agents also accumulate executable skills and even create tools: Voyager [829] builds growing library of programs plus an automatic curriculum that transfers to new worlds. Inter-testtime, agents update their models via self-generated supervision [830], [831]. Further, prompts and tool-use policies can be evolved automatically [832]. For example, Toolformer [833] demonstrates self-supervised acquisition of API-calling skills that persist across tasks. Together, these mechanisms instantiate agents that learn from experience, expand capabilities, and reduce brittleness over long horizons. In scientific fields, self-evolving agents hold the great potential to continually refine hypotheses, protocols, and tool-use policies from experimental and literature feedback, rather than remaining fixed. In biomedicine, STELLA [834] couples an evolving Template Library with dynamic Tool Ocean, where Tool-Creation agent autonomously discovers and integrates new bioinformatics tools; the systems accuracy on biomedical benchmarks rises as it accumulates trials, evidencing intraand inter-task self-improvement. OriGene [835] instantiates self-evolving virtual disease biologist: specialized agents refine thinking templates, tool composition, and analytic protocols using human and wet-lab feedback, and the framework generated targets (e.g., GPR160 for liver cancer) that were experimentally validated in patient-derived models. In chemistry, ChemAgent [836] maintains self-updating library that decomposes problems and reuses refined solutions, 49 yielding large gains on SciBench [441] and pointing to drugand materials-discovery use cases. However, scientific agents that reliably self-evolve across long horizons with closed-loop laboratory validation remain rare today and an important next step in AI-driven scientific discovery [46]. that supports end-to-end discovery, 5) Evaluation Frameworks and Benchmarking: Evaluation has shifted from single-turn QA to long-horizon scientific workflows with verifiable endpoints. ScienceAgentBench [83] decomposes 102 real tasks from peer-reviewed papers across four disciplines into executable subtasks with gold pipelines, expert validation, and containerized harnesses; despite multiple attempts, the best agents solved only about third of tasks, highlighting large headroom and the need for tool mastery and code debugging. CURIE [109] stresses longcontext scientific reasoning and information extraction across six domains with expert-curated problems, pushing agents to manage citations, units, experimental conditions, and crossfigure synthesis. DiscoveryWorld [837] provides simulated including environment hypothesis formation, experiment design, measurement, and model revision, while automatically scoring task completion, action relevance, and discovered knowledge to enable repeatable testing without wet-lab costs. Auto-Bench [838] targets causal discovery and hypothesis testing, rewarding agents for uncovering latent structure and justifying interventions. WorkflowBench [85] measures orchestration quality using code-level metrics (e.g., CodeBLEU, pass rates) for converting natural instructions into robust API workflows. For collaboration, MultiAgentBench [84] and communicative multimodal suites [839] quantify coordination, negotiation, and information-sharing when agents have asymmetric views. Cross-cutting surveys [840] now standardize taxonomies of what-to-evaluate (capability, reliability, safety) and how-toevaluate (interaction modes, datasets, metrics, tooling), and call for third-party harnesses, leakage controls, and safety redteaming specific to agents with execution privileges. Emerging best practices include: containerized runners; seeded randomness and pass@k for robustness; provenance logging; leakage audits for data-contaminated facts; and safety checks for tool scopes, credentials, and network access. 6) Autonomous Scientific Discovery: Autonomous scientific discovery represents transformative paradigm using LLMs [456], [841][844] and robotics to conduct scientific research independently without direct human intervention [46], [49], [845], [846]. By automating critical research tasks including data analysis, hypothesis generation, experiment design, and result interpretation, these automated systems efficiently process vast amounts of information and uncover patterns that elude human researchers [845], [847]. Chemistry has seen rapid progress with LLM-tool hybrids that couple symbolic planners with domain utilities. representative milestone is Coscientist [44], which combined GPT4 planning with code execution and instrument control in cloud laboratory to autonomously design, run, and analyze multistep chemistry experiments, including protocol synthesis, hardware documentation navigation, liquid-handling control, and data-driven optimization. ChemCrow [52] integrated GPT4 with expert-designed chemistry tools, demonstrating endto-end tasking from retrosynthesis and catalyst design to guiding discovery of new chromophores, with expert evaluation showing substantial gains over base models. In the life sciences, agentic LLMs are beginning to automate experimental design logic. CRISPR-GPT [848] illustrates how domain knowledge and tool use can turn free-form language reasoning into executable gene-editing workflows, chaining literaturegrounded analysis with constraint-aware proposal, delivery recommendations, and validation planning. Materials discovery provides complementary proving ground where scientific agents orchestrate in silico design loops and prepare hand-offs to self-driving labs. LLMatDesign [849] shows that reflective agentic loops can translate high-level targets into candidate materials, invoke calculators for property estimation, and iteratively refine compositions in low-data regimes. At the systems level, emerging frameworks [850] aim to standardize the interface between agentic planning and autonomous experimentation platforms, highlighting patterns for task specification, data management, and safety interlocks that generalize across domains. Despite these promising results, autonomous scientific discovery faces significant challenges in two aspects. (i) Generating proposals that balance scientific validity with genuine novelty requires systems to identify research gaps and formulate innovative hypotheses while maintaining scientific rigor, task complicated by AI models reliance on existing data patterns. (ii) Implementing closed-loop feedback for end-to-end experimental validation demands seamless integration across multiple domains, from robotics for experiment execution to advanced analytics for result interpretation, while adapting to real-world experimental uncertainties. Recent developments such as InternAgent [851] demonstrate progress in addressing these challenges through integrated pipelines that span from idea generation to experimental validation, achieving notable improvements in tasks like reaction yield prediction and enhancer activity prediction within significantly reduced timeframes compared to traditional human-led research. B. Data Ecosystems for Sci-LLMs While scientific agents, exemplified by systems like ChemCrow [852] and Biomni [18], independently perform complex scientific tasks, they require an equally advanced data ecosystem to truly thrive. This subsection outlines how data ecosystems must evolve to support autonomous, tool-using SciLLMs. Fig. 28 depicts this evolution: current data foundations have enabled the emergence of scientific knowledge capabilities in LLMs (Stages I-II), while the transition to agent-driven discovery (Stage III) necessitates reciprocal development of data ecosystems to establish closed-loop feedback between autonomous experimentation and data infrastructure. We first provide an analysis of the bottlenecks behind the rise of scientific agents (Sec. VIII-B1), and then introduce the concept of an operating system-level interaction protocol (Sec. VIII-B2). We propose design principles for next-generation scientific data architecture (Sec. VIII-B3), laying the foundation for closed-loop system of machine-led scientific inquiry. Finally, we discuss sustainable data sharing protocols that 50 Fig. 28: From data infrastructure to agent-assisted discovery: three-stage evolution of AI in scientific research. This figure delineates the incremental evolution of data-driven Sci-LLMs: (i) Stage establishes foundational data infrastructure with capabilities in efficiency, multimodal representation, and knowledge updating; (ii) Stage II demonstrates the emergence of scientific capabilities in LLMs driven by mature data ecosystems, enabling cross-domain generalization and scientific reasoning; (iii) Stage III envisions autonomous AI agents that assist scientific discovery while creating closed-loop feedback with data ecosystems, prospective paradigm for self-evolving discovery systems. This evolution, currently manifesting across physics, chemistry, life sciences, and other domains, illustrates both realized achievements and the expanding potential for AI-driven research as these technologies proliferate into broader scientific disciplines. may benefit the AI4Science community (Sec. VIII-B4). The ultimate vision is to develop comprehensive platforms like Intern-Discovery [853] and ScienceOne [854], which aim to support the entire research workflow through human-machine collaboration and the integration of dry computational analysis with wet lab experimentation, turning Sci-LLMs from knowledge processors to genuine reasoning engines for scientific discovery. formulas are abundant in texts, the logical derivation processes and underlying assumptions are rarely encoded, limiting an agents ability to perform rigorous, step-by-step symbolic reasoning. Most critically, the creative aspects of science, including failed experiments, serendipitous discoveries, and novel hypotheses, are almost entirely absent from training data, starving agents of the examples needed to learn genuine innovative thinking. 1) The Data Bottleneck Behind the Rise of Scientific Agents: primary bottleneck is the severe imbalance in data modalities available for training. The corpora for todays SciLLMs are overwhelmingly dominated by textual data, such as scientific papers and textbooks [24], [30]. While valuable, this creates critical gap: there is severe scarcity of highquality, AI-ready experimental and observational data. This imbalance forces models to learn description of science rather than the underlying principles from primary evidence. Consequently, their reasoning is often shallow, excelling at textual pattern matching but struggling with novel problems that require deep, causal understanding of experimental phenomena. Efforts to bridge this gap, such as Biomni [18] which integrates heterogeneous biological data from genomics to proteomics, underscore both the necessity and the immense difficulty of creating such multimodal datasets at scale. Compounding this issue is the disconnected nature of the scientific knowledge hierarchy within current datasets. Scientific knowledge is not flat collection of facts but structured hierarchy, and existing data fails to capture the rich connections between its layers (Sec. II-B). For instance, raw experimental data is often decoupled from its rich context, such as the specific instrumental settings and protocols used to generate it, making it nearly impossible for an agent to critically evaluate data quality. Furthermore, while scientific 2) Building an Operating System-level Interaction Protocol: To transcend these limitations, the solution lies not merely in better datasets but in fundamentally new architecture for how agents interact with the scientific world. This necessitates shift from monolithic, self-contained models to dynamic, agent-based systems capable of wielding external tools for experimentation, simulation, and analysis [855]. Such complex interaction demands an operating system-level interaction protocol, which would serve as the standardized interface between the agents core reasoning engine and the vast ecosystem of specialized scientific resources, including databases, computational simulators, data analysis packages, and even robotic wet-lab platforms. This operating system would empower the scientific agent to autonomously manage full research cycle. Upon receiving high-level objective, the agent would first decompose the problem into sequence of actionable steps. For each step, it would select and invoke the appropriate tool through the standardized protocolbe it querying the Materials Project database for candidate compounds, running simulation in LAMMPS [595] to test physical properties, or analyzing spectral data with dedicated library. The protocol must also enable the agent to parse the diverse outputs from these tools, including numerical results, error codes, structured data files, while integrating this new information back into its reasoning 51 context to inform its next action. By establishing this robust interaction framework, we can begin to address the core data bottlenecks directly. An agent equipped with such protocol is no longer solely dependent on static, pre-existing datasets. Instead, it can actively generate and consume AI-ready data on the fly, bridging the chasm between textual knowledge and empirical evidence. This creates closed-loop system where hypotheses are not just formulated based on past literature but are immediately tested through simulation or data retrieval, and the results iteratively refine the agents understanding. 3) Design Principles for Next-Generation Scientific Data Architecture: Realizing the vision of autonomous scientific agents necessitates fundamental rethinking of how scientific data is created, managed, and shared. Merely accumulating the next generation of scientific more data is insufficient; data infrastructure must be architected from the ground up to support agent-driven discovery. This requires paradigm shift guided by new set of design principles that prioritize the needs of intelligent systems, transforming data from passive archive into an active, operational resource. These principles aim to resolve the systemic bottlenecks of traceability, latency, and AI-readiness that currently hinder progress. The foremost principle is to ensure that all scientific data is actionable and AI-ready by design. This moves beyond the FAIR principles of Findability, Accessibility, Interoperability, and Reusability by demanding that data be immediately consumable by machine learning models with minimal preprocessing [337]. In practice, this means establishing and enforcing community-wide standards for rich, structured metadata that captures the full experimental context, from sample provenance and instrument calibration to software versions and processing parameters. Data should be published not as static, isolated files but as integrated packages that link raw outputs to their corresponding protocols and analyses, enabling an agent to understand not just what the data is, but how it was generated and why it is significant. second critical principle is the development of infrastructure for continuous integration and low-latency updates. The current lag between scientific discovery and its incorporation into training corpora renders models perpetually out-of-date, fatal flaw in fast-moving fields. Next-generation data architectures must implement automated pipelines that continuously ingest, validate, and structure new data from publications, preprints, and experimental platforms. Adopting open-access policies and version-controlled repositories with real-time API access will be crucial. This ensures that scientific agents can learn from the most current knowledge and experimental findings, reducing the risk of hallucination and enabling them to reason at the cutting edge of research. Finally, the new architecture must be built upon foundation of unambiguous traceability and comprehensive knowledge integration. To build trustworthy AI systems, every piece of data must be accompanied by an immutable record of its origin and transformation history, chain of custody that allows for complete reproducibility and auditing [440]. This requires more than just metadata; it calls for the integration of data across different modalities and levels of the scientific knowledge hierarchy. The ideal data ecosystem would seamlessly link theoretical concept in textbook to the specific formulas that formalize it, which in turn connect to the experimental datasets that validate it, and the computational code used to analyze it. By architecting this deeply interconnected web of knowledge, we provide scientific agents with the rich, multifaceted context they need to perform complex, verifiable, and truly insightful reasoning. 4) Sustainable Data Sharing Mechanism: Traditional models of data exchange, such as centralized repositories, or closed-access publications, are proving insufficient for the scale, diversity, and adaptability required to support the development of cutting-edge scientific LLMs. As LLMs increasingly depend on vast, heterogeneous, and continuously evolving datasets, data sharing is being reconceptualized as dynamic ecosystem rather than static resource. Emerging paradigms of sustainable data sharing center on principles of openness, fairness, and long-term viability. Decentralized architectures, often enabled by blockchain, create transparent systems for tracing provenance, attributing value, and rewarding contributions through automated contracts, which can foster trust and incentivize participation. Data ecosystems are shifting from static collections to automated curation pipelines that continuously integrate peer-reviewed publications, experimental outputs, and domain-specific repositories. This ensures that scientific LLMs are not only comprehensive but also current and reliable. The establishment of community-governed data commons is also important, in which stakeholders across academia, industry, and public institutions collaborate to set standards for licensing and ethical use. At the same time, recognizing data-sharing contributions within academic evaluation systems, similar to citation credit, could provide strong incentives for participation. The main challenge is to create fair and transparent rules for governance and benefit-sharing that balance the interests of institutions, companies, and individual researchers while ensuring legal, ethical, and reproducible practices. Ultimately, sustainable data sharing mechanisms represent not just technical necessity but also cultural and institutional shift, laying the foundation for scientific LLMs that can accelerate discovery while upholding the values of equity, transparency, and reproducibility. 5) Data Safety and Privacy: The transition to data-driven science is gated by critical threshold of trust: to confidently leverage high-value datasets, researchers must be assured of their safety, ethical standing, and legal compliance. Creating this trust requires comprehensive governance framework built on two core pillars: robust privacy protection and adherence to national data controls. The first pillar is rigorous privacy protection, particularly for sensitive information in fields like medicine and social sciences. Data can be stratified by risk, from low-risk aggregated statistics to high-risk genomic or personal health records [856]. primary challenge with high-risk data is preventing re-identification, where even anonymized datasets can be cross-referenced with public information to uncover individual identities [857][859]. This risk necessitates advanced de-identification techniques and strict access protocols to protect research participants."
        },
        {
            "title": "The second pillar addresses data sovereignty and national",
            "content": "52 controls. Scientific data is increasingly viewed as strategic national asset, leading nations to implement regulations that govern its cross-border flow and use. Prominent examples include the European Unions General Data Protection Regulation (GDPR) [860], which imposes strict conditions on transferring personal data of EU citizens internationally [861], and U.S. Export Administration Regulations (EAR) [862], which control the export of sensitive dual-use technologies. These legal frameworks require that international scientific collaborations could build compliance into their data management plans from the outset to avoid project-threatening legal and ethical conflicts. IX. CHALLENGES AND OUTLOOK A. Challenges 1) Scientific Data Selection for Efficient Pretraining: The sheer volume of scientific literature and data necessitates strategic approach to data selection for pretraining SciLLMs. Naively ingesting all available information is not only computationally expensive but can also be detrimental to model performance due to the varying quality of data [863]. The challenge, therefore, is to curate high-quality, diverse, and representative dataset that enables the model to learn the fundamental principles of scientific domain. significant hurdle is the inherent noise and bias present in scientific datasets. Training data can contain everything from experimental artifacts and outdated information to systemic biases present in the research literature. Filtering out such low-quality or irrelevant data is crucial for improving training efficiency and the downstream performance of the model. Furthermore, ensuring broad coverage across different subdomains, languages, and contexts is essential to prevent the model from becoming overly specialized and to foster interdisciplinary insights. Recent approaches to data selection are moving beyond simple heuristics. Model-based filtering techniques, which use trained model to identify high-quality and diverse data samples, have shown promise in improving pretraining for multilingual datasets [864]. Some methods even employ online batch selection, dynamically choosing the most informative data during the training process itself to adapt to the models evolving understanding [865], and thus create an efficient pretraining process by focusing on data that maximizes learning and generalization [866], [867]. 2) Optimizing Data Processing Pipelines: Once dataset has been selected, it must be transformed into format that large language model can understand. This involves developing robust and scalable data processing pipelines tailored to the unique characteristics of scientific information. Traditional data pipelines often struggle with the heterogeneity of scientific data, which can range from unstructured text and images to highly structured formats like tables and code. The tokenization process, which breaks down text into manageable units for the model, presents significant challenge in scientific domains. General-purpose tokenizers, such as BPE (Byte Pair Encoding), frequently fail to capture the semantic meaning of specialized scientific terms, chemical formulas, or biological sequences, leading to fragmented representations. For instance, complex molecule name might be broken into generic tokens that lose its specific chemical meaning. Consequently, specialized vocabularies and tokenization strategies are required to maintain domain fidelity. Additionally, data cleaning and normalization are crucial steps, particularly for unstructured formats like PDFs, which often contain formatting errors, figures, and tables that must be accurately extracted and converted to uniform input format for efficient processing by the model [30], [41]. 3) Representing Non-Sequential and Non-Textual Data: Large language models are fundamentally designed to process sequential data, typically text. However, significant portion of scientific knowledge is expressed in non-sequential and non-textual formats, presenting profound challenge for Sciinterpret 3D molecular LLMs. In chemistry, models must structures, which are inherently graphical and non-sequential, alongside text-based representations like SMILES strings. Similarly, in biology, protein structures, gene regulatory networks, and genomic data are challenging to represent within standard linear transformer architecture. Addressing this requires innovative approaches that bridge the gap between sequential language processing and complex data structures. This often involves multimodal or hybrid architectures. For example, some approaches utilize graph to encode structural information like molecular graphs and then project these embeddings into the Transformers input space. Other methods rely on specialized encoding schemas, such as representing complex mathematical equations or tables as structured text sequences, while still preserving their logical and spatial relationships. The challenge lies in ensuring that these representations maintain semantic fidelity and allow the model to reason across different modalities, moving beyond simple text understanding to truly grasp the complex relationships embedded in scientific data. 4) LLM Knowledge Update and Version Control: Scientific research evolves rapidly, with constant influxes of new discoveries, datasets, and revised theories across disciplines. Yet, most LLMs are trained on static snapshots of the literature, rendering them quickly outdated, especially in fastmoving domains like biomedicine, healthcare, and atmospheric science, where recent findings can directly influence critical decisions. Retrieval-augmented approaches offer partial relief by accessing external sources at inference time, but often fall short in relevance filtering, source attribution, and resolving conflicting information. To develop truly current scientific LLMs, continuous and automated updating pipelines are essential, capable of regularly ingesting peer-reviewed publications, preprints, and curated datasets with built-in version control and traceability. Although tools like ChatGPT and DeepSeek integrate web search, they lack guarantees of relevance or reliability. promising direction is to create collaborative platforms for dataset generation and distribution, leveraging adaptive strategies to ensure sustained LLM performance over time. B. Future Work 1) Integrated Scientific Data Ecosystems: The path forward requires fundamental reconceptualization of how we approach scientific AI, moving beyond incremental improvements to existing paradigms. Central to this transformation is the development of integrated scientific data ecosystems that transcend traditional repository models. These ecosystems must seamlessly connect experimental apparatus, computational simulations, theoretical frameworks, and published knowledge into living, evolving networks. Rather than static datasets, we envision active data streams where new experimental results automatically propagate through the system, updating model understanding while maintaining rigorous provenance tracking. This requires not only technical infrastructure but also new incentive structures within the scientific community that reward data curation and sharing as first-class research contributions. 2) Automated Scientific Data Standardization Pipeline: In the era of data-centric scientific AI, future work must prioritize the development of automated data standardization pipelines. These pipelines will serve as the foundational infrastructure for training robust and reproducible Sci-LLMs, with emphasis shifting from model architecture to data curation. More research work should focus on developing systems that can automatically clean, validate, and enrich raw scientific data in heterogeneous forms and modalities, ensuring highfidelity inputs for AI models. The development of robust data versioning and reproducible preparation workflows will also be essential to make Sci-LLM development not just scalable but also transparent and reproducible. The ultimate goal is to move from manual, ad hoc data curation to scalable, automated system that provides the scientific community with readily accessible, high-quality, and standardized data. 3) Comprehensive Evaluation System: Future directions for comprehensive evaluation should address challenges at both the model and data levels. From the perspective of SciLLMs, there is growing need for standardized, domainspecific benchmarks that go beyond surface-level metrics to assess reasoning depth, factual accuracy, and scientific creativity across disciplines. Evaluations should incorporate multimodal and multistep scientific tasks to better reflect real-world research scenarios. On the data side, defining and measuring dataset quality remains fundamental challenge, as current approaches often fail to capture how data supports model capabilities. Key criteria, such as AI-readiness, completeness, scientific relevance, timeliness, usability, and accessibility, must be integrated into data evaluation frameworks. key direction for future research is to develop systematic framework for data assessment, enabling more informed dataset selection and ultimately advancing model reliability and performance. Integrating these two perspectives will enable more robust, nuanced, and trustworthy evaluation frameworks that drive the development of truly capable scientific AI systems. 4) Advanced Scientific Reasoning: The evolution from current language models to genuine scientific reasoning systems demands architectural innovations that embed physical laws, causal structures, and domain-specific constraints directly into model design. Future architectures must move beyond pattern matching to incorporate symbolic reasoning capabilities, enabling manipulation of mathematical equations and chemical structures with the same fluency as natural language. These systems should exhibit compositional generalizationapplying learned principles to novel combinations never seen during trainingand maintain explicit representations of uncertainty that propagate through reasoning chains. The integration of neural and symbolic approaches, long pursued but never fully realized, becomes essential for scientific domains where interpretability and correctness are paramount. 5) Autonomous Scientific Agents: paradigm shift from passive models to active scientific agents represents perhaps the most transformative direction for future research. These agents must possess capabilities beyond current systems: proposing testable hypotheses, designing experiments to resolve uncertainties, and iterating based on empirical results. This requires developing safe interaction protocols with laboratory equipment and simulation environments, creating standardized interfaces for scientific tools and databases, and establishing frameworks for multi-agent collaboration where specialized models contribute complementary expertise. The vision extends to AI systems that not only assist human scientists but also autonomously explore hypothesis spaces too vast for human investigation. 6) From Sci-LLMs to Scientific Discovery: The ultimate objective of Sci-LLMs extends beyond the automation of routine tasks to the acceleration of pivotal scientific breakthroughs. Sci-LLMs present unique potentials to identify subtle, non-obvious correlations and patterns within vast, multimodal datasets that would be impossible for human researchers to process in short time. We are moving from phase where Sci-LLMs are primarily used for literature review and synthesis to an advanced stage where these models can serve as powerful instruments for accelerated hypothesis generation, potentially contributing to Nobel Prize-worthy discoveries. While human creativity and ethical oversight remain important, Sci-LLMs will act as collaborators to help significantly reduce the discovery cycle, allowing researchers to pursue more ambitious research. This integration has the potential to redefine the very nature of scientific method, pushing the boundaries of human knowledge in unprecedented ways. 7) Ethical Governance for Responsible Scientific AI Innovation: The responsible development of increasingly capable scientific AI systems necessitates robust ethical frameworks and governance structures. As these systems begin to influence research directions and resource allocation, ensuring equitable access becomes critical to prevent further concentration of scientific capabilities. Questions of attribution, accountability, and validation for AI-generated discoveries require careful consideration and community consensus. The environmental impact of training large-scale models shall be balanced against their potential contributions to sustainability science, demanding innovations in efficient training and model architectures. X. CONCLUSION This survey systematically reviews the emerging field of scientific large language models from the perspectives of data, model architectures, and agent-based systems. By introducing unified taxonomy of scientific data and analyzing more than 270 pre-training and post-training datasets as well as 54 over 190 evaluation datasets, we highlight the distinctive multimodal, cross-scale, and domain-specific challenges that differentiate scientific AI from general-purpose LLMs. We summarize the evolution from transfer learning and largescale foundation models to instruction-following and toolaugmented scientific agents, and examine current evaluation practices spanning static benchmarks, process-oriented assessments, and autonomous scientific discovery frameworks. We further discuss persistent issues in data quality, representation gaps, and knowledge updating, and outline future directions including operating-systemlevel data ecosystems and hybrid neuralsymbolic architectures. Together, these insights provide consolidated reference and forward-looking roadmap for building trustworthy, continually evolving Sci-LLMs capable of advancing data-driven scientific discovery. 55 TABLE II: Data source description."
        },
        {
            "title": "Books and literary works",
            "content": "High-quality web crawl datasets containing billions of pages from diverse internet sources, including news articles, blogs, and general web content. These datasets undergo extensive cleaning and deduplication processes to ensure text quality for language model training. Digitized collections of books spanning various genres, languages, and time periods. Sources include public domain texts, open-access libraries, and e-book platforms, providing rich narrative content and diverse writing styles."
        },
        {
            "title": "Encyclopedias and knowledge\nbases",
            "content": "Structured knowledge repositories like Wikipedia and other encyclopedic sources across multiple languages. These provide factual, well-organized information on diverse topics with consistent formatting and citation standards."
        },
        {
            "title": "Academic and research resources",
            "content": "Peer-reviewed papers, preprints, theses, and scholarly publications from repositories like arXiv and academic databases. These sources offer technical, specialized content with rigorous methodology and domain expertise."
        },
        {
            "title": "Social media and forums",
            "content": "User-generated content from platforms like Reddit and Stack Exchange, capturing conversational language, community discussions, and Q&A formats that reflect natural human communication patterns."
        },
        {
            "title": "Integration of existing datasets",
            "content": "Curated collections that combine and refine multiple existing open-source datasets, leveraging previous data curation efforts to create comprehensive training corpora."
        },
        {
            "title": "Patent databases",
            "content": "Specialized repositories containing structured scientific data including biomedical literature, protein sequences, chemical compounds, clinical trials, astronomical observations, and materials science data from authoritative institutions. Technical documentation from global patent offices including USPTO, EPO, and WIPO, containing detailed descriptions of innovations, technical specifications, and claims across various technological domains. Comprehensive multi-source integration Large-scale datasets that aggregate content from multiple source types (web, books, code, academic papers) to create diverse, balanced training corpora."
        },
        {
            "title": "Other sources",
            "content": "Additional specialized or proprietary content sources that dont fit into the above categories, potentially including domain-specific databases, institutional archives, or unique text collections."
        },
        {
            "title": "Text QA",
            "content": "TABLE III: Data type description."
        },
        {
            "title": "Description",
            "content": "A broad umbrella for any string-serializable content, e.g., natural language plus tables, sequences, code, logs, etc. Used for language modeling or domain pretraining without explicit prompts/answers or paired media. Text-only question-answer pairs, optionally with supporting passages, supervising reading comprehension or factual reasoning."
        },
        {
            "title": "Text QA with CoT",
            "content": "Text QA augmented with explicit multi-step explanations or derivations alongside the final answer."
        },
        {
            "title": "VQA",
            "content": "Visual question-answering pairs, where each image is with question and the corresponding answer. VQA (multi-image)"
        },
        {
            "title": "VQA with CoT",
            "content": "Image-text Video-text question grounded on two or more related images, requiring cross-image comparison, temporal alignment, or aggregation. VQA data augmented with step-by-step rationales or intermediate reasoning traces in addition to the final answer. Image-text pairs, where the text contains description (e.g., captions, reports) for alignment, captioning, retrieval, or representation learning. Video-text pairs, where the text contains description (e.g., subtitles, transcripts, narrations) for alignment, captioning, retrieval, or representation learning. Classification, regression, generation, etc. For numeric/matrix/graph records lacking natural-language pairing, annotate by supervised objective. 56 9 0 9 , 1 1 0 0 0 3 2 , 5 2 1 . , 0 0 0 0 0 0 5 , 7 7 2 , 3 1 6 5 2 9 , 4 9 1 , 1 6 7 9 2 0 1 , 3 0 8 5 5 , 0 0 2 0 9 , 4 0 0 7 1 , 2 8 6 2 3 , , 5 1 6 1 0 6 1 , 8 7 1 9 1 , 0 6 , 3 3 6 , 0 3 6 9 3 4 1 1 , , 7 4 8 8 4 5 1 , , 0 0 0 0 0 4 2 , 5 7 2 5 1 , 5 5 . 8 9 1 , 5 7 3 0 0 2 1 4 , 7 9 8 4 , , 0 7 - 1 - o u 0 7 - l - t , . 2 7 - 5 2 - Q - 1 - S D ,"
        },
        {
            "title": "T\nR\nE\nB\nl\na\nc\ni\nn\ni\nl",
            "content": "C m M , 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "O\nN\nD",
            "content": "I s , e D , 4 - o 4 - G , 0 7 - o e C i 4 - 4 - G"
        },
        {
            "title": "1\nR\n-\nk\ne\ne\nS\np\ne\ne\nD",
            "content": "o 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - - , E g l d p e l K , 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ 4 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 0 0 0 9 5 , 9 6 0 8 , 1 9 , 7 7 3 2 5 1 6 4 , 8 2 2 , 3 2 2 , 2 9 0 4 9 2 1 , r R , b C V, 4 - G"
        },
        {
            "title": "T\nR\nE\nB\ne\nc\nn\ne\nt\nn\ne\nS",
            "content": ", 4 - 4 - 4 - 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ 5 4 5 2 , 3 - L , . 5 3 - 0 0 0 4 1 1 , , 0 4 8 5 1 0 8 4 3 5 5 6 , 0 0 0 4 , 9 4 3 4 , I"
        },
        {
            "title": "C\nB\nM\nA\nL\nA",
            "content": ". 5 3 - 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 2 9 0 , 0 2 1 0 8 1 8 5 , 4 - V 4 - , 0 0 0 5 6 9 1 , 2 9 , 4 0 1 2 - , 4 - ,"
        },
        {
            "title": "T\nA\nS",
            "content": "4 - 2 0 2 1 3 1 , , 2 7 1 0 3 7 0 1 6 4 4 6 3 9 0 4 , 5 0 7 , , 0 0 0 0 0 0 1 1 , 0 8 0 0 8 , , 0 0 0 0 0 0 6 1 , , 0 0 0 0 0 1 1 , , l c"
        },
        {
            "title": "T\nA\nC\nd\ne\nM",
            "content": "o t - 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . 5 3 - m u 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ , T f"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": ") c ( 7 2 7 7 9 3 2 , 0 0 0 8 3 , 8 4 2 3 2 , 1 0 7 1 , , 5 5 4 9 4 0 , 0 0 0 , 7 0 2 6 5 5 3 , 1 1 2 . 0 0 0 8 6 , 0 0 0 , 0 7 4 6 5 1 , 2 c l r l 3 . 5 3 - 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - , . 5 3 - 0 0 , 0 5 2 6 1 2 , 1 2 4 0"
        },
        {
            "title": "A\nN",
            "content": "/ t n o C o i f D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ , 0 0 0 0 0 0 1 , 0 0 0 , 0 3 6 0 0 0 , 3 1 1 6 4 9 , 6 2 2 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ u - 5 3 - ."
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "5\nT",
            "content": ", B"
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": ". 5 3 - , 0 0 0 0 0 0 6 2 , 3 0 , 7 7 1 0 0 0 4 5 , 6 7 0 , 0 6 8 2 2 6 7 , , 2 9 5 6 4 6 1 , 0 0 0 , 5 1 1 5 9 8 , 1 1 7 2 6 , 4 6 3 0 0 5 0 0 0 , 9 4 5 0 0 0 0 2 , r n I"
        },
        {
            "title": "D\nB\nO\nR\nG",
            "content": "n a e a D"
        },
        {
            "title": "T\nR\nA\nB",
            "content": ", - O 4 - w e i d o r g D a t n t e r n a e a D"
        },
        {
            "title": "R\nT\nE\nD",
            "content": "-"
        },
        {
            "title": "C\nM\nP",
            "content": "("
        },
        {
            "title": "D\nI\ne\nD",
            "content": "- r n ,"
        },
        {
            "title": "T\nR\nE\nB\nk\nn\ni\nL\no\ni\nB",
            "content": "r T , ) g o 1 0 1 s ( , ) I 4 3 s . 5 3 - G"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ t n t e r n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a w e , v w e i a a a a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t n t e r v t t n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t n t o r g D t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ v t e r D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 6 a u - t t - t t - t t - d m a - S m e S t t - S a u t t e o l a l a u d m A a u t t e o A t t e o d m A r e r e a m c s t e a s b d fi fi fi e i i t c s t n P c s c s n m c e a e a g s g s f n a t o r n s a g s f i g I r r O c s t - i u - u i h m t t g s f i g I , r e r e a h , t t e s u r r e r e h e d n s t e a c d A m c fi fi e i i i a d e o W t - i u - u - i u - u i h m v e p n a o r g"
        },
        {
            "title": "N\nE",
            "content": ","
        },
        {
            "title": "R\nF",
            "content": ", a u - S a u - S a u d m A e n a t t n e t n W c s t i o c e s b d fi n S r I b e o s u r a r a m c t t e o d m A - i u - u e n r C s d t e o r n i g"
        },
        {
            "title": "H\nZ",
            "content": "1 1 . 3 2 0 2 1 1 . 3 2 0 2 1 1 . 3 2 0 2 1 1 . 3 2 0 2 1 1 . 3 2 0 2 1 1 . 3 2 0 0 1 . 3 2 0 2 0 1 . 3 2 0 2 0 1 . 3 2 0 2 0 1 . 3 2 0 2 8 0 . 3 2 0 2 8 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 3 2 0 2 7 0 . 3 2 0 2 7 0 . 3 2 0 2 7 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 3 2"
        },
        {
            "title": "H\nZ",
            "content": "6 0 . 3 2"
        },
        {
            "title": "H\nZ",
            "content": "5 0 . 3 2 0 2 5 0 . 3 2 0 2 5 0 . 3 2 0 2 5 0 . 3 2 0 2 5 0 . 3 2 0 2 4 0 . 3 2 0 4 0 . 3 2 0 2 4 0 . 3 2 0 2 v e r n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u u d m A a u t t - - i s d t e o r n , r e o d i h e d l o m c s - u e c r I b v e p C s t fi e n a e r D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - i u - u - i u - u e n r C t e n r C t w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S s d t e n a t e r D 3 a u - S e a i x n a t I"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - i u - u i h m o r d m A r e r e a e A v w e a t n t n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ 3 a u - S a u i r a i a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S 7 a u - S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m s b d a a fi fi e i i - i u - u i h m e o h e d c d n a o r i d o r g D v a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t e o s u r a r a e A s d t e o r n t n e t n W v t 5 a u - m - i u - u i h m o r i r a t n t e r v r"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ D D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - t t - S l a d m A a u i w e i e a w e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - t t - S w e d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S o r n a t n t n a e a i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ D v r"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 6 a u - S r e r e d i a e o d m A r e r e d i a e o h e o - t t - d m a - S m e o f n a t , a a c s e o h e h e r a c fi e c d c d A fi e s b d t t e d e a t n t n a e a i w e i a a D a D"
        },
        {
            "title": "A\nN",
            "content": "/ + 0 2 4 4 0 1 a u - d m a - t t - t t - S m m a M s d s d t e t e f i g I t e s b d fi n S a a s t fi fi e S n S a a fi e - i u - u e n r C s d t e o r n c s h e d i a , a a e a fi n S t e s d t e t t e d e s b d fi e t n e t n W a a c fi e s u e n a g"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2 0 2 5 0 . 5 2 0 2 3 . 5 2 0 2 4 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2 0 2 2 0 . 5 2 0 2 3 0 . 5 2"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "2 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2 0 2 1 1 . 4 2"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2"
        },
        {
            "title": "U\nR",
            "content": "0 1 . 4 2 0 2 8 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 4 2 0 2 7 0 . 4 2 0 2 7 0 . 4 2 0 2 7 0 . 4 2 0 2 6 0 . 4 2"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2 0 2 6 0 . 4 2 0 2 6 0 . 4 2 0 2 6 0 . 4 2 0 2 5 0 . 4 2 0 2 5 0 . 4 2 0 5 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 4 2"
        },
        {
            "title": "H\nZ",
            "content": "4 0 . 4 2 0 2 4 0 . 4 2 0 2 3 0 . 4 2 0 2 3 0 . 4 2"
        },
        {
            "title": "H\nZ",
            "content": "2 0 . 4 2 0 2 2 0 . 4 2 0 2 2 0 . 4 2 0 2 1 0 . 4 2 0 2 2 1 . 3 2"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "4 0 . 5 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x , 0 0 0 0 0 0 5 2 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - , s d t e o r n 0 0 2 1 , r - 5 3 - . v n i e a 6 a u - m g w d a p y s b"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u c s c s n m c A"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x x - m I"
        },
        {
            "title": "A\nQ\nV",
            "content": "g i - P"
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": "e l l n s e l d n a l ] l [ ] 3 3 7 [ n e R"
        },
        {
            "title": "P\nF\nC",
            "content": ","
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 8 6 8 [ 8 1 - - O - e i a e a c a ] l [ ] 9 6 8 ["
        },
        {
            "title": "A\nQ\nV\nX\ne\nR",
            "content": ") m ) m - u - u ( ("
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - P"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x T"
        },
        {
            "title": "A\nQ\nV",
            "content": "t - m I"
        },
        {
            "title": "A\nQ",
            "content": "t t - m e - i V"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - P"
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - g i - P"
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": ","
        },
        {
            "title": "P\nF\nC",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ", c c , c n g l g t e s e l d n a l ] l [ ] 2 7 8 [ 1 D o d c l c c c M e h e ] l [ ] 3 4 5 [ - n e - 1 - i m e l d e i a e a c a ] l [ ] 3 6 6 [ 5 5 - o V M"
        },
        {
            "title": "A\nQ",
            "content": "l d i r - c c c M s e l d n a l H a l ] l [ ] 0 7 8 [ 0 6 1 - i G ] l [ ] 1 7 8 [ 9 1 a A , I"
        },
        {
            "title": "R\nM",
            "content": ". , l a s , - ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ", l m ,"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 5 4 5 [ 0 1 n e - I"
        },
        {
            "title": "A\nM\nG",
            "content": "e l l n s e l d n a l y - a - - e i a e a a l s e l d n c c c M e h e a l ] l [ ] 5 9 7 ["
        },
        {
            "title": "A\nQ\nV",
            "content": "- ] l [ ] 4 7 8 [ - I"
        },
        {
            "title": "D\nC\nM\nM",
            "content": "I"
        },
        {
            "title": "R\nX\nC\nG\nC",
            "content": "- ] l [ ] 3 7 8 [ -"
        },
        {
            "title": "A\nQ\nV\nX\ne\nM\nE\nG",
            "content": "] l [ ] 2 3 7 [ a e M"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m I"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - y - ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ", c c , I"
        },
        {
            "title": "R\nM\nP\nF\nC",
            "content": ", ,"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 5 7 8 [ a - V"
        },
        {
            "title": "A\nQ",
            "content": "t ,"
        },
        {
            "title": "A\nQ\nV",
            "content": "t w R"
        },
        {
            "title": "A\nQ",
            "content": "t t - i e - i e R"
        },
        {
            "title": "A\nQ",
            "content": "t g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": ","
        },
        {
            "title": "P\nF\nC",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ", c c , c n , I"
        },
        {
            "title": "R\nM",
            "content": ". , l a s , - ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ", l m ,"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 1 4 5 [ 5 . 5 - - I"
        },
        {
            "title": "S\nU",
            "content": ", c c , , c n c c c M e h e n a m d i C n S i d r t H i l g c a p c c c M e h e ] l [ ] 4 6 6 [ p ] l [ ] 7 7 8 [ 1 - B ] l [ ] 8 7 8 [ d M"
        },
        {
            "title": "A\nQ",
            "content": "l d i c c c M e h e ] l [ ] 9 7 8 ["
        },
        {
            "title": "A\nQ\nd\ne\nM",
            "content": "l s a m c c c c M e h e g i l n s e l d n a l ] l [ ] 6 7 8 [ 2 - i ] l [ ] 3 0 2 [ I"
        },
        {
            "title": "A\nC\nD\nE\nM\nO\nB",
            "content": "I"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m i r - . , l a s , - , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", n S i d r t H ] l [ ] 0 8 8 [ 5 2 - n d t - m t - m x - m I"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - g i - , n t - , n t - g i - n a fi s t - m t - m I"
        },
        {
            "title": "A\nQ",
            "content": "t t t t T"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", a fl a - a I ,"
        },
        {
            "title": "P\nF\nC",
            "content": ", c n ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ", c c , ,"
        },
        {
            "title": "R\nM",
            "content": "y - ,"
        },
        {
            "title": "S\nU",
            "content": ", I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", o d i C o d i l r p e A"
        },
        {
            "title": "T\nC\nO",
            "content": ","
        },
        {
            "title": "A\nF\nF",
            "content": ","
        },
        {
            "title": "P\nF\nC",
            "content": "y o r a - - - X"
        },
        {
            "title": "R\nH\nE",
            "content": "s e l d n c c c M s e l d n c c c M s e l d n a l H a l e h e a l e h e ] l [ ] 3 8 8 [ -"
        },
        {
            "title": "A\nQ\nV\nR\nX\nC\nC\nM\nM",
            "content": "I - - - I ] l [ ] 1 8 8 [ n - d ] l [ ] 2 8 8 [ -"
        },
        {
            "title": "R\nV\nC\nM\nB",
            "content": "I ] l [ ] 6 5 6 [ P p C ] l [ ] 3 0 7 ["
        },
        {
            "title": "A\nQ\nX\nR\nH\nE",
            "content": "s e l d n a l ] l [ ] 0 4 5 [ s e P n S i d s e l d n c c c M s e l d n c c c M s e l d n c c c M a a l e h e a l e h e a l e h e e h e ] l [ ] 2 4 [ 0 4 1 - 4 - - 2 o u ] l [ o - i C - e y - p s ] l [ g i c M K ] l [ ] 3 5 6 ["
        },
        {
            "title": "Q\ni\nd\ne\nM",
            "content": "] l [ ] 5 8 8 [ i M - ] l [ ] 4 8 8 [ n S ] l [ u D"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t , t - m I"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - s p a i ,"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 9 5 6 [ ) t ( - a 3 M"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t , t - m e - m ,"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": "s p a i ,"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 9 5 6 [ ) t t ( - a 3 g i c l c c c M e h e ] l [ ] 7 8 8 ["
        },
        {
            "title": "H\nZ\nt\na\nh\nC\nd\ne\nM",
            "content": "y - X"
        },
        {
            "title": "T\nC",
            "content": "s e l d n c c c M e h e a l ] l [ ] 6 8 8 [ t C - n a ] l [ - n a fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t w R"
        },
        {
            "title": "A\nQ\nV",
            "content": "g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e l l n , p i a u a l n s a m c o t t H"
        },
        {
            "title": "R\nH\nE",
            "content": "s e l d n c c c M s e l d n c c c M s e l d n a l H a l e h e a l e h e ] l [ a d b c a a M ] l [ ] 9 8 8 [ r I - u ] l [ ] 8 8 8 [ a a ] l [ ] 0 9 8 [ -"
        },
        {
            "title": "A\nQ\nA\nU\nJ\nR",
            "content": "] l [ ] 3 7 6 ["
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w o t g o c fi a x - m I"
        },
        {
            "title": "A\nQ",
            "content": "t n a fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ",
            "content": "t t w x T"
        },
        {
            "title": "A\nQ",
            "content": "t , t - m I"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - g i - g i - g i - g i - t A i r - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": ","
        },
        {
            "title": "R\nM\nE",
            "content": ", o d i C - - X , ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ", , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ","
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", - ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ". , p , p , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", o D e A - n d g g i g i c l i l n C . e l s a c l c l , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", , - o s n e l d g i t a i c l i C C"
        },
        {
            "title": "T\nC",
            "content": "s e l d n c c c M s e l d n c c c M e h e a l H a l e h e ] l [ ] l [ ] 7 5 6 [ l -"
        },
        {
            "title": "C\nM\nP",
            "content": "] 7 5 6 [ p s M - ] l [ ] 0 6 6 [ 2 O ] l [ ] 7 5 6 ["
        },
        {
            "title": "D\nM\nd\ne\nM",
            "content": "s e l d n a l ] l [ ] 1 9 8 [ 1 0 0 - D - t t - a c c c M a c c c M s e l d n c c c M s e l d n c c c M s e l d n s e l d n c c c M s e l d n c c c M s e l d n a l H a l e h e a l e h e a l e h e e h e a l e h e a l e h e ] l [ ] 8 3 [ ] l [ ] 2 9 8 [ a - s l u c l ] l [ ] 2 9 8 [ L - ] l [ ] l [ ] 3 9 8 [ P ] 4 9 8 [ P A ] l [ ] 5 9 8 [ F ] l [ ] 2 0 7 [ C N ] l [ ] 6 9 8 [ - i ] l [ ] 7 9 8 ["
        },
        {
            "title": "A\nQ\nc\nn\nO",
            "content": "] l [ ] 9 9 8 ["
        },
        {
            "title": "A\nQ\nd\ne\nM\nM\nC",
            "content": "t ] l [ ] 8 9 8 [ - H ] l [ ] 0 0 9 [ - C -"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ\nV",
            "content": "t - m ,"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - ,"
        },
        {
            "title": "T\nE\nP",
            "content": ", c c , , l a s a - , ,"
        },
        {
            "title": "S\nU",
            "content": "s e l d n a l ] l [ ] 3 0 9 [ - A L e - m I"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - y o p i c c c M e h e ] l [ ] 0 5 1 [ 1 - u e e e R R R t - m i r - g i - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e e l k i o h g a e s a m c c d s e l d n c c c M s e l d n c c c M e h e e h e a l e h e ] l [ ] 2 0 9 [ r m 0 0 1 C e y ] l [ ] 1 5 6 [ t d u - P ] l [ ] 9 8 5 [ 9 1 C c M ] l [ ] 1 0 9 [ A"
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": ", c c , , l a s ,"
        },
        {
            "title": "P\nF\nC",
            "content": "e l e l l n a i C r - , ,"
        },
        {
            "title": "S\nU",
            "content": "s e l d n a l s e l d n c c c M e h e a l H ] l [ ] 6 0 9 [ a - s - t ] l [ ] 7 0 9 [ 0 2 - n Q ] l [ ] 5 0 9 ["
        },
        {
            "title": "A\nQ\nV\nC\nM\nP",
            "content": "- o d i C n S i d r t H ] l [ ] 4 0 9 [ - a M - N S"
        },
        {
            "title": "A\nQ",
            "content": "t , t R"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - s a m c , o d i C c c c M e h e H"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "T\nF\nS",
            "content": "e e l k i o e i a e a c a H"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", c c , , l a s ,"
        },
        {
            "title": "P\nF\nC",
            "content": "t e s i , o d i C - , ,"
        },
        {
            "title": "S\nU",
            "content": "s e l d n a l s e l d n a l H"
        },
        {
            "title": "A\nQ",
            "content": "l d i c c c M e h e ] l [ ] 8 0 9 [ i - 6 2 a ] l [ p - l i ] l [ ] 8 0 9 [ 6 2 - a ] l [ ] 9 0 9 [ t - ] l [ ] 0 2 5 [ a d , 3 2 8 8 6 0 2 ,"
        },
        {
            "title": "A\nN",
            "content": "/ t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , 5 1 3 4 1 1 1 ,"
        },
        {
            "title": "A\nN",
            "content": "/ v t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u e a i x n a t I"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "1 1 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - w e r"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 8 a u - S a u t t d m a - S r e c s e o s u r r e r e r e r e a a a c d c d c d c d A"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 3 2 0 2 1 1 . 3 2 0 2 1 1 . 3 2 0 2 1 1 . 3 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "t - m t - m , t - m , t - m I I"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a a fi e S"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 3 2 0 2 t fi a n a - . , c o F , a - , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", n S i d r t H ] l [ ] 3 7 7 [ a 3 - t t - S a u - S a u - t t - m e e c r I b e o h s b d a a fi fi e i i S"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 3 2 0 2 3 0 . 3 2 0 2 1 0 . 3 2 0 2 9 0 . 2 2 0 2 p n d i C t fi a x a R"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e l l n s e l d n a l s p l n s e l d n a l s a m c A"
        },
        {
            "title": "R\nH\nE",
            "content": "s e l d n c c c M e h e a l ] l [ ] 0 1 9 ["
        },
        {
            "title": "A\nQ\nd\ne\nM\ni\nk\ni\nW",
            "content": "] l [ ] 7 4 6 [ c a ] l [ ] 2 7 6 [ I - I"
        },
        {
            "title": "C\nM\nM",
            "content": "I ] l [ ] 0 5 6 ["
        },
        {
            "title": "D\nE\nR\no\ni\nB",
            "content": "d m A r e r e a e A"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 3 2 0 2 t - m i r - - t t , c n , l m ,"
        },
        {
            "title": "T\nC",
            "content": "y - ,"
        },
        {
            "title": "T\nE\nP",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ", , c c , s e l d n a l ] l [ ] 4 0 2 [ -"
        },
        {
            "title": "A\nO\nC\nM\nP",
            "content": "9 3 2 , 6"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "T\nF\nS",
            "content": "e l l n s e l d n a l ] l [ 1 - a . i w a o c d ] l [ . s"
        },
        {
            "title": "M\nL\nL\nM",
            "content": "/ s"
        },
        {
            "title": "M\nL\nL",
            "content": "c fi e r t t n a - p a i r - f a S : I"
        },
        {
            "title": "E\nL\nB\nA\nT",
            "content": "2 1 5 7 3 , . 1 4 - n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a a fi e S"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2 0 2 ) m - u ("
        },
        {
            "title": "T\nF\nS",
            "content": "s h i l g i u c A ] l [ ] 0 9 1 [ R e n a n - A o s a s a n i i n H t n c S u L e e T o P l M m u s D m fi e d o r g D i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S n M"
        },
        {
            "title": "A\nN",
            "content": "/ r e c s e o h e h e r r e a d i a i a c d A"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "9 0 . 4 2 0 2 1 0 . 5 2 0 2 7 0 . 3 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w Q"
        },
        {
            "title": "A\nQ",
            "content": "t t t T"
        },
        {
            "title": "T\nF\nS",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": "s a m c g i c l C"
        },
        {
            "title": "A\nQ",
            "content": "l d i o B l l n a e u c A ] l [ ] 8 6 7 [ t - 0 1 o ] l [ ] l [ ] 2 2 7 [ C o B - s e e 57 0 0 0 , 0 0 8 5 9 1 1 , 9 9 0 , 2 9 7 9 0 7 5 , 0 2 8 0 6 0 , 7 1 2 7 9 0 1 , 0 0 0 4 , , 0 0 0 0 0 1 1 , 6 5 1 , 8 5 0 8 6 6 4 1 , 4 5 6 9 1 , 0 0 0 4 , 1 4 4 7 4 , 2 9 2 5 1 , 0 0 3 , 2 1 2 1 , 0 0 0 0 0 0 , 5 3 8 , 7 2 2 4 8 2 3 6 , 3 9 7 1 , 0 0 0 , 8 0 1 0 0 , 5 5 4 0 0 0 1 8 , 3 1 4 6 , 2 5 4 0 3 , 5 5 9 3 , 6 5 8 1 9 4 3 6 4 3 6 3 8 8 , 0 7 8 9 9 0 0 0 , , 3 1 1 1 8 2 3 4 5 2 ,"
        },
        {
            "title": "A\nN",
            "content": "/ , 0 0 0 3 4 0 2 , 0 0 0 2 1 , 3 1 6 5 1 , ) i l ( , 0 0 0 0 4 6 1 , 0 0 8 8 , , 4 3 2 4 5 9 4 , 5 7 2 7 , 7 6 4 ) i l ( ) i l ( 0 0 5 0 2 , ) i l ( 4 7 1 2 4 2 0 0 0 , , 3 9 4 3 9 8 3 2 6 1 6 , 7 0 , 3 2 7 7 6 9 4 , 0 0 0 , 7 9 0 0 2 3 4 , 8 1 9 2 6 , 1 5 5 , 8 7 1 0 0 3 ) i l ( 9 4 2 3 . 0 7 6 1 1 , 9 9 6 0 0 6 0 1 , 3 5 7 2 . 0 5 9 0 8 , 2 4 3 6 3 4 8 , 8 4 7 9 , 3 0 2 3 1 , 4 4 8 7 , ) i l ( ) i l ( , 0 8 5 6 2 1 , 0 5 4 8 6 , 1 3 . ) i l ( ) i l ( 1 3 . 0 0 0 5 , 9 7 3 , 3 3 . 8 4 2 2 2 , 0 4 7 6 , 8 8 9 1 1 , 0 2 7 0 4 7 8 , 4 0 3 2 , 4 5 1 8 4 2 2 2 , 6 6 5 0 , p S - 1 0 1 s , g o D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ l c s r a o C l c s C"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v e r v n i e a n a e a w e a i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t n t o r g D t n t e r D i r"
        },
        {
            "title": "A\nN",
            "content": "/ D v e r t n t o r g D i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ D D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u u d m A a u t t l a t t e o d m A a u t t d m A n a M n e o - S t t e o l a u l a u M 2 a u - S n a e a t t - S t t - S r e r e a , 2 4 4 6 3 3 8 , g t"
        },
        {
            "title": "R\nE\nN",
            "content": "w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S 0 2 6 7 8 , i m u C"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S c d . 5 3 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t n t o r g D"
        },
        {
            "title": "A\nN",
            "content": "/ t n t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 2 2 9 , 9 2 3 5 1 , i o t"
        },
        {
            "title": "E\nP\nB\nt\ns\na\nf",
            "content": "s M"
        },
        {
            "title": "A\nN",
            "content": "/ p ,"
        },
        {
            "title": "T\nR\nE\nB\ni\nc\nS",
            "content": ", ,"
        },
        {
            "title": "T\nR\nE\nB\nk\nn\ni\nL\no\ni\nB",
            "content": "o 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ l c s C"
        },
        {
            "title": "B\nG\nO",
            "content": ". 5 3 - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S v t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t e o A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u a e A"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i r a t n t o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - t t - S s u r r e a a a i a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c - d k , a a fi n S - t e d e , o a e o - u i h m , t - d k B , a a - t e d e , o a e o - u i h m , t t e c fi e S i g i r e r e a e A t c n n a s b d a a fi fi e S n S r r O r e r e a s t i a fi e s u r a r a e A r r O r e r e d i a n o n n a s u r a r a e A t t e d e e c t c e c r I b e t n W r I b e o h e r a r e r e a e A t c n n a t n e t n W r e r e a m c e a i x n a t e a d s t fi fi e i i e a d s t i a fi e c d c fi n S a a s t fi fi e S n S r r O s e o i g fi n S e A t - i u - u i h m n a - i u - u i h m e o s u r r e r e a c d c d A , a a fi r e r e i g e S , r e r e a e A - , s d t e o r n s b d fi e r e c s c s h e d n s t , a a c s s b d fi a r e c fi e S m c m c fi e S"
        },
        {
            "title": "H\nZ",
            "content": ", , ,"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 0 2 0 2 9 0 . 0 2 0 2 7 0 . 0 2 0 2 5 0 . 0 2 0 2 4 0 . 0 2 0 2 3 0 . 0 2 0 2 1 . 9 1 0 2 0 1 . 9 1 0 2 9 0 . 9 1 0 2 9 0 . 9 1 0 2 9 0 . 9 1 0 2 8 0 . 9 1 0 7 0 . 9 1 0 2 3 0 . 9 1 0 2 1 1 . 8 1 0 2 1 1 . 8 1 0 2 9 0 . 8 1 0 2 9 0 . 8 1 0 6 0 . 8 1 0 2 2 0 . 8 1 0 2 8 0 . 7 1 0 2 3 0 . 6 1 0 2 8 0 . 2 1 0 2 3 0 . 4 0 0 1 1 . 8 8"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 0 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2 0 2 3 0 . 4 2 0 2 1 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 3 2 0 2 2 1 . 2 2 0 2 2 1 . 2 2 0 2 0 1 . 2 2"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 2 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 1"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2 0 2 4 0 . 5 2 0 2 3 0 . 5 2 0 2 3 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t t t t T"
        },
        {
            "title": "A\nQ\nV",
            "content": "t - m I"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ\nV",
            "content": "t - m I"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ\nV",
            "content": "t - m I"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ\nV",
            "content": "t - m t - m t - m I i fi a C"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - g i - , n t - g i - , n t - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - y - ,"
        },
        {
            "title": "S\nU",
            "content": ", g i a e"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", i l n C"
        },
        {
            "title": "A\nQ\nh\nt\nl\na\ne\nh\nr\ne\nm\nu\ns\nn\no\nC",
            "content": "e l l n y o p i r - ,"
        },
        {
            "title": "S\nU",
            "content": "s a e , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", e A e A o d i C"
        },
        {
            "title": "P\nF\nC",
            "content": "y - X"
        },
        {
            "title": "R\nH\nE",
            "content": "y - , n ,"
        },
        {
            "title": "S\nU",
            "content": ", I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ","
        },
        {
            "title": "A\nQ\nh\nt\nl\na\ne\nh\nr\ne\nm\nu\ns\nn\no\nC",
            "content": "e l l n y - r - , ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ", I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", o d i C o d i l C"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ", I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", o d i C"
        },
        {
            "title": "A\nF\nF",
            "content": "y - ,"
        },
        {
            "title": "R\nH\nE",
            "content": ") . , c n S i d e i a e a n S i d e i a e a c c c M s e l d n c c c M s e l d n c c c M s e l d n s e l d n c c c M s e l d n c c c M s e l d n c c c M a c c c M s e l d n c c c M s e l d n c c c M s e l d n s e l d n c c c M s e l d n a l e h e a l H a l e h e a l e h e a l e h e e h e a l e h e a l e h e a l H a l e h e a l e h e a l e h e e h e a l e h e a l ] l [ ] 3 2 9 [ 0 2 0 2 A - C m ] l [ ] 5 2 9 [ 9 1 0 2 A ] l [ ] 9 4 4 [ t t ] l [ ] 9 4 4 ["
        },
        {
            "title": "D\nA\nu\nQ\nd\ne\nM",
            "content": "- C m I"
        },
        {
            "title": "A\nQ\nd\ne\nM\nb\nu\nP",
            "content": "] l [ ] l [ ] 1 0 7 [ - a e ] 8 9 7 [ A M - ] l [ ] 1 2 5 ["
        },
        {
            "title": "A\nQ\nd\ne\nM",
            "content": "] l [ ] 4 2 9 [ ] l [ ] 9 4 6 [ a e ] l [ ] 9 4 1 ["
        },
        {
            "title": "A\nQ\nV\nh\nt\na\nP",
            "content": "] l [ o i ] l [ ] 0 3 9 [ 8 1 0 2 A - C m ] l [ ] 8 4 6 [ 7 1 0 2 t ] l [ ] 1 3 9 [ ] l [ B m ] l [ ] 4 5 1 ["
        },
        {
            "title": "A\nQ\ne\nv\ni\nL",
            "content": "I O t ] l [ T m o i i ] l [ ] 4 5 6 [ P ] l [ ] 3 5 1 [ -"
        },
        {
            "title": "R\nX\nC\nC\nM\nM",
            "content": "I ] l [ ] l [ ] 6 2 9 ["
        },
        {
            "title": "A\nQ\nd\ne\nM\nb\ne\nw",
            "content": "] 4 0 7 [ -"
        },
        {
            "title": "D\nA\nR\nA\nQ\nV",
            "content": "] l [ ] 7 2 9 [ 2 e ] 2 5 6 [ r - M ] l [ ] 9 2 9 [ ] l [ ] 8 2 9 ["
        },
        {
            "title": "A\nQ\nm\ne",
            "content": "r s g , t fi a x a n s e , t fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t t t w a R"
        },
        {
            "title": "A\nQ\nV",
            "content": "t w R"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x o c fi a C"
        },
        {
            "title": "G\nA\nR",
            "content": ","
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": "t - - c s"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "- v G , w p K , e s s , l r l C n e n e i l e o u s - c e P a t R"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - s o i r p m a e B i c a e - e , l r l d e i a e a , l B , n S u C i d n l l e h e l l c - u a l s o - u i - u s o - u i - u i - u l l M"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e u n o g i l e a u o M"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "h G l y o a l n l e a u o r c M ] l [ ] 0 1 5 [ ] l [ ] l [ ] 6 3 6 [ o u n h e i e ] 9 3 6 [ d M s ] l [ ] 7 9 6 [ ] l [ ] l [ ] 8 2 6 [ ] 6 2 6 [ C o ] l [ ] 4 9 6 ["
        },
        {
            "title": "R\nE\nE\nP",
            "content": "] l [ ] 4 3 9 ["
        },
        {
            "title": "T\nP\nG\no\ni\nB",
            "content": "] l [ ] 2 0 8 [ A D ] l [ ] 1 2 4 [ a i B"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e u A g i l e a u o ] l [ ] 5 9 6 ["
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "y o g i l e a u C r c M u o ] l [ ] 7 2 6 ["
        },
        {
            "title": "M\nT\nS\nm\ne\nh\nC\nb\nu\nP",
            "content": "] l [ ] 5 2 6 [ P e c d y o a l n r c M ] l [ ] 3 3 9 [ D u S"
        },
        {
            "title": "T\nF\nS",
            "content": "s t t r c m y o a l n l l ] l [ ] 6 9 6 [ i r I - 5 5 1 0 0 0 , , 3 9 1 3 9 2 5 1 0 0 1 , 0 0 0 , 7 6 1 , 3 5 7 4 9 2 1 , 6 3 2 , 6 3 1 6 1 1 4 , 0 0 5 4 , 7 6 0 , ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a n r C , a a e i r i d o r g D n a e a i a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u t t a M c s e o h e h e d n c d c d t n e t n W r r O"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 2 2 0 2 1 0 . 2 2 0 2 1 1 . 1 2 0 2 9 0 . 1 2"
        },
        {
            "title": "A\nQ",
            "content": "t t t w R"
        },
        {
            "title": "T\nF\nS",
            "content": "y - ,"
        },
        {
            "title": "S\nU",
            "content": ", I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", o d o d i l n C C"
        },
        {
            "title": "A\nQ",
            "content": "l d i c c c M s e l d n c c c M s e l d n a l H a l e h e a l ] l [ ] 7 1 9 [ 1 2 0 M ] l [ ] 6 1 9 [ -"
        },
        {
            "title": "A\nQ\nC\nE\nL\nM",
            "content": "- C m ] l [ ] l [ ] 5 1 9 [ 2 - M ] 4 1 9 ["
        },
        {
            "title": "A\nQ\nC\nM\nC",
            "content": "n a e a i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t a M r e r e d i a m f a m r r t n i s B V"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 2 2 0 2 3 0 . 2 2 0 2 2 0 . 2 2"
        },
        {
            "title": "A\nQ",
            "content": "t t t T"
        },
        {
            "title": "T\nF\nS",
            "content": "e l s e i l n C"
        },
        {
            "title": "A\nQ",
            "content": "l d i c c c M s e l d n c c c M e h e a l H a l ] l [ ] 2 1 9 [ e - e P - ] l [ ] 1 1 9 [ ] l [ ] 0 5 4 ["
        },
        {
            "title": "A\nQ\nh\nt\nl\na\ne\nH\nV",
            "content": "i"
        },
        {
            "title": "A\nQ\nC\nM\nd\ne\nM",
            "content": "w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a a fi e S"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 2 2 0 2 t g i - t e c l e i a e a c a ] l [ ] 3 1 9 [ e P - P i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S t t e d e W"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 1 2 0 2 t g i - s fl e c s e l d n a l ] l [ ] 8 1 9 [ fl o 1 4 , 9"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a 1 4 1 , 9"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a 3 3 n l a M"
        },
        {
            "title": "H\nZ",
            "content": "6 0 . 1 2"
        },
        {
            "title": "A\nQ",
            "content": "t , t fi a C"
        },
        {
            "title": "H\nZ",
            "content": "6 0 . 1 2"
        },
        {
            "title": "A\nQ",
            "content": "t , t fi a C"
        },
        {
            "title": "T\nF\nS",
            "content": "- M , o c M ,"
        },
        {
            "title": "R\nH\nE",
            "content": "- M , o c M ,"
        },
        {
            "title": "R\nH\nE",
            "content": ", i a i k t l , i l n s b t s e l d n a l H ] l [ ] 5 5 6 [ - 5 - d e i a e a c a ] l [ ] 9 1 9 [ B a t n t o r g D t n t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t a M n M"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "5 0 . 1 2 0 2 2 0 . 1 2 0 2 2 0 . 1 2 0 2 1 0 . 1 2 0 2 2 1 . 0 2 0 2 t - m e - m I"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": "n a e a 7 a u - S r e r e d i a A"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 0 2 0 2 t - m I"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - - , I"
        },
        {
            "title": "R\nM",
            "content": ", l a s , c n ,"
        },
        {
            "title": "T\nC",
            "content": "y - ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ", c c o d i C - , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", o d i C"
        },
        {
            "title": "A\nF\nF",
            "content": ","
        },
        {
            "title": "P\nF\nC",
            "content": "y - c c c M s e l d n c c c M s e l d n s e l d n a l e h e a l e h e a l H ] l [ ] 1 2 9 [ d - o d - i - n ] l [ ] 2 2 9 [ ] l [ ] 8 5 6 [ e e I"
        },
        {
            "title": "D\nV\nO\nC\nr\no\nf\nI",
            "content": "A ] l [ ] l [ ] 0 2 9 ["
        },
        {
            "title": "G\nD\nd\ne\nM",
            "content": "] 2 7 7 [ L c c c M e h e ] l [ ] 1 6 6 [ I e p o r r u n I A i i o - A o k n H t n n p a n a n e o a a a R T p y a M a t t i D fi e e c R"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - i u - u i h m o e R , t fi a C"
        },
        {
            "title": "G\nA\nR",
            "content": ", n t - - p , i v , a ( d e p b s e l d n a l ] l [ ] 2 3 9 [ - D - 9 1 - C - fi a - d m 4 - g e t"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u c s e o h e h e r a s b d c d , a a s u r a r s u r e A e A fi e c d c fi a r e S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s d t e o r n s t c fi e S"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 4 2 0 2 6 0 . 4 2 0 2 5 0 . 4 2 0 2 5 0 . 4 2 0 2 t fi a t t t"
        },
        {
            "title": "A\nQ",
            "content": "w a e i r - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e u e o u n e e P e s e u A c - u s o - u i - u i - u ] l [ ] l [ ] 2 4 6 [ ] 5 3 9 [ a M e P , ] 0 4 6 [ t A ] l [ ] 8 1 3 [ a n N ] l [ ] 0 4 6 [ -"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2 0 2 1 1 . 3 2 0 2 6 0 . 3 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x x a e R i r - g i - P"
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": "e u e o u n e e P"
        },
        {
            "title": "A\nQ",
            "content": "l d i i - u i - u i - u ] l [ ] 7 1 3 [ o s e - u ] 6 3 9 [ P - w / o n ] l [ ] 8 9 6 [ o P"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e c s c s n r e d e a s b d e A fi e S m c fi e S"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 3 2 0 2 5 0 . 3 2 0 2 1 0 . 3 2 0 2 0 1 . 2 2 0 2 r p - s p n i e - s p n n a fi a o c fi a n a - P"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - e u e o u s - c s - c s - c s o - u i - u i - u i - u ] l [ ] 2 1 5 [ i l q - c E L ] l [ ] 7 3 9 [ m e m G ] l [ ] 9 3 9 [ s D t u s ] l [ ] 8 3 9 [ r P u H"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a a fi e S"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 1 2 0 2 t g i - e u e o u s o - u ] l [ ] 1 3 6 [ n a H"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s d t e o r n I"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 5 2 0 2 t g i - e u e o u s o - u ] l [ ] 8 8 4 [ 2 n e O"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t n t o r g D t n t o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a a s t e a s b d fi fi fi fi e i i t c i i S"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 5 2 0 2 2 0 . 5 2 0 2 2 1 . 4 2 0 2 2 1 . 4 2 0 2 t n G"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e u e u e o u i l e u n o c q N s o - u i - u i - u i - u ] l [ ] 0 4 [ ) t ( ] l [ ] 0 4 [ )"
        },
        {
            "title": "A\nN\nD",
            "content": "( - G L - G L ] l [ ] 5 3 6 [ m 2 ] l [ ] 5 3 6 [ F 2 S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s d t e o r n s t c fi e s b d fi e S"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 4 2 0 2 t t w x w n a - g i - g i - e u e o u e u e o u n e i l s o - u i - u s o - u ] l [ ] 8 1 3 [ a s e t ] l [ ] 7 8 4 [ n e ] l [ ] 8 1 3 [ 0 0 0 1 T . - 5 3 - a , 4 - t u s"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e c s e o s u r a r r e a h e d e a d s t e a h e d fi e S m c m c fi e c d c fi n S e A"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 1 2 0 2 2 0 . 1 2 0 2 2 0 . 1 2 0 2 7 0 . 9 1 0 2 7 0 . 9 1 0 2 1 1 . 8 1 0 7 0 . 6 1 0 2 r p - s p n o c fi a o c fi t fi s i l e R t fi a t w n a - g i - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e u e u e o u s - c s - c i l N s - c s - c s - c i - u i - u i - u s o - u i - u i - u i - u ] l [ ] 2 4 9 [ s D t e m ] l [ ] 4 4 9 [ ] l [ ] l [ ] 3 4 9 ["
        },
        {
            "title": "D\nV\nG",
            "content": "s r S i M ] 5 4 9 ["
        },
        {
            "title": "B\nD\no\na\nl\ng\nn\na\nP",
            "content": "] l [ ] 6 4 9 [ 8 6 h ] l [ ] 1 4 9 [ l M ] l [ ] 0 4 9 ["
        },
        {
            "title": "D\nP\nG",
            "content": "n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c A"
        },
        {
            "title": "A\nN",
            "content": "/ a a fi e S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A a a c fi e s b d fi e S"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 3 1"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 4 2 0 2 9 0 . 3 1 0 2 1 1 . 2 1"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 9 0 0 2 t R"
        },
        {
            "title": "A\nQ",
            "content": "t t w x a e w n a - g i - g i - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e u n o ,"
        },
        {
            "title": "A\nN\nR",
            "content": ","
        },
        {
            "title": "A\nN\nD",
            "content": "e u e o u n e i l e u n o c q e o u i - u i - u i - u i - u i - u ] l [ ] 0 0 7 [ i r I - l ] l [ ] 3 3 6 [ 8 3 / 8 3 G ] l [ ] 4 9 [ B - I"
        },
        {
            "title": "B\nC\nN",
            "content": "] l [ ] 9 2 6 ["
        },
        {
            "title": "A\nP\nC\nT",
            "content": "] l [ ] 3 3 6 [ 9 1 / 7 3 G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - t t - t t - t t - t t - d m a - t t - t t - t t - S S S m e S S S r e c s e o s u r r e s u r r e c s e o h e h e r r e r e r e r e r e r e h e d n a d d n a c d c d A m c m c m c m c m c m c c d A"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2 0 2 4 0 . 3 2 0 2 1 1 . 2 2 0 2 8 0 . 2 2 0 2 7 0 . 2 2 0 2 7 0 . 2 2 0 3 0 . 2 2 0 2 1 0 . 2 2 0 2 9 0 . 1 2 0 2 t fi t fi t fi i fi t fi t fi t fi t fi t fi s i l s i l s i l i l s i l C"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - , n t - , n t - , n t - , n t - P , n t - , n t - , n t - P"
        },
        {
            "title": "G\nE\nE",
            "content": "I I"
        },
        {
            "title": "R\nM",
            "content": "f f"
        },
        {
            "title": "G\nE\nE",
            "content": "I"
        },
        {
            "title": "R\nM",
            "content": "f e o N e o N e o N e o N e o N n s e n s e n s e n s e ] l [ ] 6 0 7 [ ] l [ ] 8 0 7 ["
        },
        {
            "title": "G\nE\nM",
            "content": "- i 2 - i ] l [ ] 0 1 7 [ 3 - e ] l [ ] 8 1 7 ["
        },
        {
            "title": "U\nH\nS",
            "content": "] l [ ] 9 0 7 [ g -"
        },
        {
            "title": "D\nS\nN",
            "content": "] l [ ] 8 0 7 [ I"
        },
        {
            "title": "R\nM",
            "content": "f - i ] l [ ] 5 0 7 [ 1 - i ] l [ ] 3 1 7 ["
        },
        {
            "title": "C\nM\nH",
            "content": "] l [ ] 7 0 7 ["
        },
        {
            "title": "D\nS\nN",
            "content": "e t n u n 58 4 8 8 4 , 8 7 2 1 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ n a M 9 0 7 9 2 0 3 2 0 0 0 1 , 6 1 . 0 0 0 0 2 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v e r v e r v d o r g D n a e a a t n t n i e a d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ n a M n a M n M"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v e r v e r v w e a t n t n i e a d o r g D n a e a a t n t d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ n a M n a M n l a c s c s n s t e a d fi e c d c fi e s a d i x n a t - i u - u i h m s u r a r c d s a g s e n a t - i u - u i h m o r n a e o h e d i a t t s a g s g s f n a t n a t t t n i o t e s b d fi e S"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 1 2 0 2 2 1 . 0 2 0 2 1 1 . 0 2 0 2 1 1 . 0 2 0 2 5 0 . 0 2"
        },
        {
            "title": "A\nQ",
            "content": "t t t"
        },
        {
            "title": "A\nQ",
            "content": "t t w a e R 5 9 3 5 3 1 0 0 0 4 1 , 1 0 9 , 8 1 , 6 1 6 0 3 8 2 , , 1 1 8 7 1 3 1 , 3 1 8 , 7 7 5 1 1 5 8 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a w e a t n t e r a t n t e r n a e a i d o r g D v d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ n a M n a M n l a c s c s n m c e a g s f i g I a a fi e S s t fi e s b d fi e S s t n P"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 0 2 0 2 2 1 . 9 1 0 2 0 1 . 9 1 0 2 6 0 . 7 1 0 2 1 1 . 4 1 0 2 7 0 . 3 1 0 4 - w e a 4 a u - S s t fi e S"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2 0 2 t t w x w x a e R"
        },
        {
            "title": "A\nQ\nV",
            "content": "0 0 0 6 , 0 8 0 1 7 0 1 1 , 9 3 0 1 6 , 7 3 2 3 8 0 1 5 8 , , , 3 4 1 5 4 2 2 1 1 9 0 4 4 1 7 9 1 1 4 4 6 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - t t - t t - t t - t t - d m a - t t - t t - t t - t t - t t - m e S m e m e S m e o r c s e o s u r r e c s e o r c s e o s u r r e r e r e h e h e h e h e h e h e r r e r e r e a d d n a d n d n m c m c m c m c c d c d c d c d c d c d A m c A"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 9 1 0 2 1 0 . 9 1 0 2 2 1 . 8 1 0 2 1 1 . 8 1 0 2 2 0 . 8 1 0 2 1 0 . 8 1 0 2 1 . 5 1 0 2 2 1 . 5 1 0 2 5 0 . 5 1 0 2 0 1 . 3 1 0 2 1 0 . 8 9 9 1 t fi i fi a s C"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t n a fi t fi t fi t fi t fi i fi t fi a s C a s C s i l s C"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - , n t - , n t - , n t - , n t - P , n t - , n t - , n t - , n t - , n t - P"
        },
        {
            "title": "G\nE\nE",
            "content": "I"
        },
        {
            "title": "R\nM",
            "content": "f"
        },
        {
            "title": "G\nE\nE",
            "content": "8 4 6 5 1 , i s"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u s t fi e S"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 7 1"
        },
        {
            "title": "A\nQ",
            "content": "t , t fi a C"
        },
        {
            "title": "G\nA\nR",
            "content": "s r c l , h i e a d - D i d e h e n s e n s e n s e n s e n s e e i r e i r e i r e i r e i r e i r N , m P n S ] l [ ] 1 2 7 [ ] l [ ] 7 4 9 [ I"
        },
        {
            "title": "R\nD",
            "content": "d k ] l [ ] 0 2 7 [ -"
        },
        {
            "title": "D\nE\nE\nS",
            "content": "] l [ ] 1 1 7 [ 1 Z ] l [ ] 2 1 7 [ 2 Z ] l [ ] 7 1 7 [ T ] l [ ] l [ ] 6 1 7 [ ] 6 1 7 [ ] l [ ] 9 1 7 ["
        },
        {
            "title": "D\nE\nE\nS",
            "content": "] l [ ] 4 1 7 [ - l ] l [ ] 5 1 7 [ S ] l [ ] 3 0 8 ["
        },
        {
            "title": "B\nD\no\np\ne\nr",
            "content": "e s v m d i c L e S i o - A o s a s a n i i m n a n r e g e l p s u y a n o s D m fi e , 2 6 9 6 3 9 ,"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u c s c s n m c A"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 0 2 0 2 t g i - P"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "y i c ] l [ ] 6 0 6 [ O t e i ) k ( 0 0 0 i r 8 3 . , 0 5 4 3 2 ) k ( 3 3 . 6 5 3 0 1 , 4 7 6 , 6 0 1 9 5 8 1 3 , , 5 9 3 0 8 5 1 , , 3 4 4 1 4 3 4 , 0 0 , 0 6 1 1 0 5 5 1 , 6 0 7 9 2 , 5 , 3 5 2 9 3 9 1 , 4 7 0 5 , 5 9 9 4 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v e r v e r n a e a d o r g D n a e a a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t a M n e o r c s e o h e h e h e d d n m c m c m c s t t a , 2 6 4 1 6 9 1 , 2 4 3 , 0 1 1 0 7 9 0 4 3 3 . 0 5 8 7 0 7 4"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - w e i w e i r i w e i w e i d o r g D a t n t n i e a d o r g D n a e a a t n t n i e a D a t n t n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u u l a d m A a u t t e o l a c s s u r r e c s e o s u r r e s u r a r a r a r d d e a h e h e h e h e r r e a d n a m c m c m c fi e c d A m c m c m c m c A"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 2 1 0 2 5 0 . 5 2 0 2 2 0 . 5 2 0 2 8 0 . 4 2 0 2 1 0 . 3 2 0 2 5 0 . 2 2 0 0 1 . 2 1 0 2 5 0 . 5 2 0 2 9 0 . 4 2 0 2 ) k ( 4 3 ."
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ n e o h e d i a ) k ( 2 0 ."
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ n e o h e d i a K 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v e r n a e a a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t e o s u r a r r e a c d c d A"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 4 2 0 2 6 0 . 2 2 0 2 2 0 . 2 2 0 2 6 0 . 1 2 0 2 ) k ( 6 . r 6 5 3 0 1 , 0 0 0 5 g S . 5 3 - G"
        },
        {
            "title": "A\nN",
            "content": "/ M 5 9 ."
        },
        {
            "title": "A\nN",
            "content": "/ 8 3 8 9 1 , 5 2 8 3 , 3 8 7 9 , . 0 3 A"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - w e i , 7 - - S D . - 5 1 - m . - 5 1 - m 0 4 - 2 r I - 7 8 - t t t w e i d o r g D n a e a w e a t n t e r D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . - 5 1 - m e r n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ n a e a i a D"
        },
        {
            "title": "A\nN",
            "content": "/ 2 1 v t D"
        },
        {
            "title": "A\nN",
            "content": "/ n a e a i a D"
        },
        {
            "title": "A\nN",
            "content": "/ 4 a u - S n e r a i a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t a M a u t t d m A n - i u - u i h m fi e S , t t e d e , r e r e a r e s u r a r a r d e o r c s c s c s n a e A m c e c r I b e o s u r a r r e a c d c d s a d i x n a t e a c d c d A m c i g n t t n i o i g I a u c s c s n m c A"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 4 2 0 2 v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ n n o n n a w e a t n t e i"
        },
        {
            "title": "L\nF\nA",
            "content": "d m a - S r r O"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 0 2 0 2 1 0 . 8 1 0 2 1 0 . 7 1 0 2 7 0 . 5 1"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 4 2 0 2 1 0 . 0 2"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2 0 2 5 0 . 4 2 0 2 1 0 . 4 2 0 2 9 0 . 3 2 0 2 5 0 . 3 2 0 2 1 1 . 2 2 0 5 0 . 5 2 0 2 4 0 . 5 2 0 2 9 0 . 4 2 0 2 4 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2 0 2 7 0 . 3 2 0 2 5 0 . 3 2 0 2 3 0 . 3 2 0 2 2 0 . 3 2 0 2 7 0 . 2 2 0 - n , s g - n , s g , t fi a n d i - t , t fi a x a n a fi a x t"
        },
        {
            "title": "A\nQ",
            "content": "w x x x x x t t w a R R e e e i R R R t n G"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - , n t - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - g i - , n t - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - P"
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - - m G , e s t - e e , e s t e e s D"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", e s t P"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", e s t , T p r p e A m c A"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "t , ,"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "e u ,"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "e u t e r m y i C s h t e y i C e l n a e r G e G r G s h B m P m P ] l [ ] 5 0 1 [ ] l [ ] 8 4 9 [ u x h C"
        },
        {
            "title": "A\nQ\nm\ne\nh\nC\nr\na\nl\no\nh\nc\nS",
            "content": "] l [ ] 9 8 6 [ r I S ] l [ ] 9 4 9 [ m ] l [ ] 6 1 2 [ e ] l [ ] 0 5 9 [ t p ] l [ ] 3 5 7 [ n a ] l [ ] 5 1 2 [ ] l [ ] 4 1 2 ["
        },
        {
            "title": "C\nN\nI\nZ",
            "content": "y r y r y r y r ] l [ ] l [ ] 1 5 9 [ 2 ] 2 5 9 [ e B ] l [ ] 3 5 9 [ T ] l [ ] 3 4 2 ["
        },
        {
            "title": "T\nF\nS",
            "content": ", n t - , n t - , n t - , n t - P"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", e s t P"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "y i C e y a h a h a h ] l [ ] 0 1 6 [ o P D ] l [ ] 5 5 9 [ t g ] l [ ] 4 5 9 [ a r ] 2 1 6 ["
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w V"
        },
        {
            "title": "A\nQ",
            "content": "t n s e R"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t w R"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t n a fi a x - m I"
        },
        {
            "title": "T\no\nC",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": "g i - g i - P"
        },
        {
            "title": "A\nQ",
            "content": "t t t t t t T R w a R n t - P i r - g i - g i - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "s m c n s , a a e g i i o A p r p e A e A e c d s a e c d c d A e c d s a e c d c d A a s s , l t g r u , T i o A , t i r o t n e i m r c M ,"
        },
        {
            "title": "S\nE\nI\nF\nL\nE\nS",
            "content": ","
        },
        {
            "title": "C\nA\nP\nU",
            "content": "I , n e c d A"
        },
        {
            "title": "F\nI\nC",
            "content": "y o A n s o t m r y o A n s y o A n s o t m r s y r s y r A n S i a e s r e i l e c c a t n S i a e s r a M M ] l [ ] 9 1 6 [ )"
        },
        {
            "title": "D\nA\nM\nO\nN",
            "content": "( ] l [ v i a t e ] 7 5 9 [ ) h ( ] l [ ] 4 2 6 [ -"
        },
        {
            "title": "B\nD\nX\nF\nO\nM",
            "content": "r h a ] l [ ] 8 1 6 [ i ] l [ ] 0 2 6 [ o j t o l e ] l [ ] 2 9 6 [ - 0 2 - I"
        },
        {
            "title": "B\nE\nh\nC",
            "content": "e i l e ] L [ ] 6 6 5 [ ] L [ ] L [ ] 3 6 5 [ ] 4 2 7 [ 4 3 2 M s b o A M s i ] 0 8 7 [ l - s r ] L [ ] 1 8 7 [ C A ] L [ ] 7 6 6 [ B s i ] 6 5 9 [ A ] L [ ] 3 2 7 [ c - L t k ] 5 2 7 [ - s ] L [ ] 1 6 5 ["
        },
        {
            "title": "A\nM\na\nL\nL\no\nr\nt\ns\nA",
            "content": "] L [ ] 9 6 6 [ r ] L [ ] 2 6 5 ["
        },
        {
            "title": "A\nV\na\nL\nL\no\nr\nt\ns\nA",
            "content": "y o A x o - H T s l n s y a e ] l [ ] 5 3 7 [ h - ] l [ ] 0 8 6 ["
        },
        {
            "title": "A\nQ\nI\nP",
            "content": "s y P"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 2 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 2 2 0 2 t t w n a - P i r - n i m a e c c a t ] l [ ] 3 2 6 [ t p l m . e f P r r c m c c a t ] l [ ] 3 2 6 [ i p l n e . e f P n t - P i r - P"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - g i - g i - g i - g i - g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "- w e"
        },
        {
            "title": "M\nE\nT",
            "content": ", m i"
        },
        {
            "title": "M\nE\nT",
            "content": ", m i"
        },
        {
            "title": "S\nE\nI\nF\nL\nE\nS",
            "content": ","
        },
        {
            "title": "C\nA\nP\nU",
            "content": "I , n i r i fi e S"
        },
        {
            "title": "F\nI\nC",
            "content": "n c f"
        },
        {
            "title": "F\nI\nC",
            "content": "e i l e c c a t n S i a e s r e i l e M M ] l [ ] 1 2 6 [ - R ] l [ ] 1 9 6 [ 0 2 - I"
        },
        {
            "title": "B\nE\nh\nC",
            "content": "] l [ ] 2 2 6 ["
        },
        {
            "title": "C\nN\nI\nZ",
            "content": "] l [ ] 0 9 6 [ O ] l [ ] 7 1 6 [ Q e s r M ] l [ ] 3 9 6 [ s D c c i"
        },
        {
            "title": "M\nn\no\nr\nt\nc\ne\nl\nE\nk\nc\ni\nw\nr\na",
            "content": "W e s r M e s r M e s r a c c a t c c a t ] 5 1 6 [ )"
        },
        {
            "title": "D\nS\nC",
            "content": "I ( b D t t t C a n ] l [ ] 6 1 6 [ 9 1 0 2 E ] 4 1 6 [ )"
        },
        {
            "title": "D\nM\nQ\nO",
            "content": "( b D i a"
        },
        {
            "title": "M\nm\nu\nt\nn\na\nu\nQ",
            "content": "] l [ ] 7 1 2 [ )"
        },
        {
            "title": "O\nT\nP\nS\nU",
            "content": "( fi n P ] l [ O ] l [ ] l [ ] 0 7 [ j s r M"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": ", s e e e o ] l [ ] 8 2 7 ["
        },
        {
            "title": "A\nQ\nr\ne\nh\nt\na\ne",
            "content": "W e h E 4 5 5 8 0 2 6 0 3 1 7 9 4 7 9 3 , 7 6 3 1 8 , 1 5 . 4 1 . 5 6 3 . 6 . 5 2 . 8 2 . 7 . k 6 0 1 7 0 5 . 8 6 2 1 0 9 3 , 2 4 1 6 0 1 . 8 8 4 , 3 7 1 5 1 1 . - , - C - 2 D i , 1 . 3 - L i 4 1 - - C , 4 - , 4 1 / i"
        },
        {
            "title": "T\nV",
            "content": "t"
        },
        {
            "title": "X\ne\nN\nv\nn\no\nC",
            "content": "o 4 - - m V, 4 - o 4 - , 2 7 - - 2 Q 4 - , . 5 3 - B 3 1 - 5 3 1 - . . 1 - c 1 - c 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ s L o i r 4 - G"
        },
        {
            "title": "P\nI\nL\nC",
            "content": ","
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ ) k ( 3 3 . - t t , l c 0 7 - L L"
        },
        {
            "title": "1\nR\n-\nk\ne\ne\nS\np\ne\ne\nD",
            "content": "s r D , l C o i c s 5 1 4 1 , , . 5 3 - n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , s d t e s u r a r n a t i a o o G A 5 . 1 - c V"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a n a e a i e a n a e a w e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 a u - S a u t t d m A n 4 - w e a 0 t t - S e n r e , a a r - u c fi e d i a , r e , a d a p y e n r C a a s t e a d s t e a c fi fi fi fi fi e i i t c n S e S i d o r g D v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 5 3 a u - S r e r e d a"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c t t e o h e d i a d m A , s d t e e o h e r a t e c d n a t i a e r a , t n t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 5 v r"
        },
        {
            "title": "A\nN",
            "content": "/ D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u t t e r i a D"
        },
        {
            "title": "A\nN",
            "content": "/ D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u - S a u d m A v t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S n e o h e r a e A r e r e a e A e a i x n a t e a s b d fi fi e i i , s d t e s u r a r n a t i a , r e r e a a a m c fi n S s d t e o r n s t c fi e w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - u g e i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u c s c s n m c w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S v t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , r , r e a o , t t e d n m c , o a i W s w r l a o c s n m c c s c s a m c S"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2 0 2 1 0 . 5 2 0 2 2 1 . 3 2 0 2 1 1 . 3 2 0 2 5 0 . 1 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2 0 2 3 0 . 5 2 0 2 1 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m I"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m e - m I"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m I"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m I"
        },
        {
            "title": "A\nQ",
            "content": "t g i - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e t n u n C"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 4 2 0 2 1 0 . 4 2 0 2 1 0 . 4 2 0 2 2 1 . 3 2 0 2 7 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 2 2"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 9 1"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - i e - m , t - m x - m x - m I"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m I"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x T"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x x a e R i r - P"
        },
        {
            "title": "T\nF\nS",
            "content": "g i - P"
        },
        {
            "title": "F\nH\nL\nR",
            "content": ","
        },
        {
            "title": "T\nF\nS",
            "content": "g i - g i - P"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2 0 2 3 0 . 4 2 0 2 2 0 . 4 2 0 2 2 0 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m , t - m t - m t - m I i r - P"
        },
        {
            "title": "T\nF\nS",
            "content": ". y p t b , s e e i s o g n t R s e e i S m g n t R s e e i S m . , r i e , s s o g n t R , a I ,"
        },
        {
            "title": "R\nA\nS",
            "content": "d - u , i , i g n , s , s s i s m t R m t R s e e g n t R s e e i S m n e o g n g n t R m n e o g n e e i S m n e o g n t R s e e i S m R"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": "t , a a g n , s s , s , s e e o e e o R"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": ", s e e - t , h r e p d e p t e o e p t e o L L , h h e p ] l [ ] 8 7 5 [ t O ] l [ ] 9 2 7 [ I"
        },
        {
            "title": "A\nr\no\no\nfl\na\ne\nS",
            "content": "] l [ ] 7 2 7 ["
        },
        {
            "title": "A\nQ\nV\nh\nt\nr\na\nE",
            "content": "] l [ ] 6 2 7 [ d F ] l [ ] 8 8 5 [ c G ] l [ ] 3 7 5 [ 8 - L G ] l [ ] 0 7 5 [ 5 9 - t"
        },
        {
            "title": "A\nV\nE",
            "content": "] l [ ] 7 6 5 [ g e ] l [ ] 8 5 9 ["
        },
        {
            "title": "D\na\ns\nr\ne\nV",
            "content": "] l [ ] 1 7 5 [ R ] l [ ] 9 5 9 [ - ] l [ ] 1 6 9 [ N a h ] l [ ] 2 6 9 [ l - L ] l [ ] 0 6 9 [ ] l [ ] 7 9 7 [ e V 1 - M ] l [ ] 2 6 9 [ r I - L ] l [ ] 3 6 9 [ 8 6 9 - y ] l [ ] 0 3 7 [ 5 ] l [ ] 1 3 7 [ r k ] l [ ] 4 8 7 [ I d n l o i t ) s . i ( n i d u ] l [ ] 4 8 6 [ n S - t N ) c , u f . i ( T ) s h . i ( n i d u e c d ) s . i ( a p i l ] l [ ] 0 3 [ t a ] l [ ] 4 2 [ B S e ) s . i ( n i d u ] l [ ] 5 8 6 [ i a a a n S e 59 p i p f n o I A i i o - A o s a H o o e e n H t n c S u L a R T p y a n o s D a c fi e K 8 . 6 1 1 4 6 . , 0 4 8 , 1"
        },
        {
            "title": "R\nC\nO",
            "content": ", s p"
        },
        {
            "title": "F\nD\nP",
            "content": "- S D - - a - g e r - 0 7 - 3 . 3 l , r I"
        },
        {
            "title": "X\ne\nT\na\nL",
            "content": "s r D ,"
        },
        {
            "title": "3\nV",
            "content": ". 5 1 i a w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S 7 a u - S i a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , r e r e a c s c s n m c m c r r t d k s B , t t e d f i g I , o a i W a e a i x e"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2 0 2 1 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 4 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x x - m I"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x n a - P"
        },
        {
            "title": "T\nF\nS",
            "content": "e m h t . , l B , s h , s s g fi a . , l , s P o x c c - , s h , s P , c M l ] l [ ] 4 0 6 [ v r ] l [ ] 6 8 6 [ 6 1 1 - ] l [ ] 4 6 9 [ e a M 60 . i w a o c d ] l [ . s"
        },
        {
            "title": "M\nL\nL\nM",
            "content": "/ s"
        },
        {
            "title": "M\nL\nL",
            "content": "c fi e r t t o u e y m :"
        },
        {
            "title": "V\nE\nL\nB\nA\nT",
            "content": "o 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - 4 - 4 - 4 - m u 4 - ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ - r r d m u i A A ,"
        },
        {
            "title": "R\nO\nE\nT\nE\nM",
            "content": "r I , - O ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": "- O n - O , a / T ,"
        },
        {
            "title": "Q\nC\nM",
            "content": ","
        },
        {
            "title": "Q\nC\nM",
            "content": "d e r c d e - O"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", - O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e"
        },
        {
            "title": "E\nA\nM\nN",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", t fi a o e R"
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "M\nE",
            "content": "c e - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "s t o u E T S 4 6 2 , 0 0 2 1 , 6 8 1 2 , 0 2 A"
        },
        {
            "title": "Q\nC\nM",
            "content": "7 5 5 0 4 ,"
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": "c c A c c c A"
        },
        {
            "title": "Q\nC\nM",
            "content": ","
        },
        {
            "title": "Q\nC\nM",
            "content": "d e 0 7 0 0 7 , 0 6 4 4 , 0 0 6 9 9 8 8 , 0 7 2 5 2 , 1 0 9 , 0 0 3 6 2 6 1 ,"
        },
        {
            "title": "1\nF",
            "content": ", - O ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": "- ,"
        },
        {
            "title": "R\nD\nC",
            "content": ","
        },
        {
            "title": "R\nF\nI\nD",
            "content": ","
        },
        {
            "title": "R\nW\nD",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": "e S B ,"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ", , l t c"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", - O - O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e"
        },
        {
            "title": "1\nF",
            "content": "l i R , a m g a , a m g a , a m s t n a , n a s t t , A i l o e P , n a s , a m n a , s t o t n , a m n a , A n r t u C"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", c e - O n - O n - O n - O d - O - O e e , ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e , n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "e s 4 - d e - O , E - , c E - ,"
        },
        {
            "title": "1\nF",
            "content": ", c . + c A , , R d i e , - O ,"
        },
        {
            "title": "C\nT\nC",
            "content": "d e - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "0 2 1 9 4 7 5 1 2 0 0 2 0 6 2 5 3 2 , 0 0 3 2 0 7 , 5 8 1 2 , 8"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 1 2 6 v t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , r e r e a m c - i u - u i h m o r i r a i a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , r e r e a m c - i u - u i h m o r g"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a a fi e S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S s d t e n a t , , ,"
        },
        {
            "title": "O\nK",
            "content": ", ,"
        },
        {
            "title": "A\nF",
            "content": ", H"
        },
        {
            "title": "H\nZ",
            "content": "d m A n , r e r e a e A - i u - u i h m o r g"
        },
        {
            "title": "Q\nC\nM",
            "content": "5 9 9 , 7 2 1 4 - w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m n a g"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ - V 4 - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S o r n , s d t e o r n s b d fi e , a a e a fi n S t e"
        },
        {
            "title": "G\nK\ne\nM\nC",
            "content": ", . 5 3 - l - a - R"
        },
        {
            "title": "A\nN",
            "content": "/ v t e r D"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a 6 a u - S r r t n o B"
        },
        {
            "title": "H\nZ",
            "content": "3 0 . 4 2 0 2 a u u d i m i S"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t , r m , R d V , s r ,"
        },
        {
            "title": "C\nC\nP",
            "content": ","
        },
        {
            "title": "E\nS\nM",
            "content": ", s r ,"
        },
        {
            "title": "E\nS\nM\nR",
            "content": "l e A , ,"
        },
        {
            "title": "1\nF",
            "content": "2 A , , A"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", A"
        },
        {
            "title": "1\nF",
            "content": ", c ,"
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": "s c t o - O t fi a o c fi a ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e t fi a , t fi a o e R"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O , t fi a o e R n - O e e e t n u n C"
        },
        {
            "title": "E\nS\nM\nR\nN",
            "content": "d e - O"
        },
        {
            "title": "R\nR\nM",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", d e - O s ,"
        },
        {
            "title": "R\nE\nW",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", - e o C"
        },
        {
            "title": "1\nF",
            "content": ", p R"
        },
        {
            "title": "1\nF",
            "content": ", c , c , , , s r n i P , s r i t e s , i a , s b m c - , a e n fi , n l , n F ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", - O , A"
        },
        {
            "title": "1\nF",
            "content": ", r R - 1 , ,"
        },
        {
            "title": "T\nR\nU\nE\nL\nB",
            "content": "L - O 1 2 - c v ,"
        },
        {
            "title": "U\nE\nL\nB\ne\nr\nc\na\nS",
            "content": ", c E B"
        },
        {
            "title": "1\nF",
            "content": ", c , s r A"
        },
        {
            "title": "1\nF",
            "content": ", a - x ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ","
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": "c ,"
        },
        {
            "title": "R\nR\nM",
            "content": "c A"
        },
        {
            "title": "1\nF",
            "content": ", c A"
        },
        {
            "title": "R\nR\nM",
            "content": ", c c A"
        },
        {
            "title": "P\nA",
            "content": ","
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": "c c A n - O 3 7 1 3 ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "9 3 8 , 0 8 2 a / T a / T"
        },
        {
            "title": "Q\nC\nM",
            "content": "n a fi a C"
        },
        {
            "title": "R\nE\nN",
            "content": "n a fi a 0 0 0 , 0 2 6 1 2 4 . 0 6 5 1 , 8 5 8 2 0 1 5 2 , 6 3 9 0 , , l c e 0 6 - ,"
        },
        {
            "title": "T\nP\nG",
            "content": ", b C r d - O n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O"
        },
        {
            "title": "R\nE\nN",
            "content": "n a e - O - O e e , , ,"
        },
        {
            "title": "R\nE\nN",
            "content": "d e"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O"
        },
        {
            "title": "A\nQ",
            "content": "d e - O , e e R"
        },
        {
            "title": "Q\nC\nM",
            "content": ","
        },
        {
            "title": "Q\nC\nM",
            "content": "d e 0 0 5 1 1 , 3 1 2 6 6 1 0 0 3 9 8 5 , 8 0 2 1 1 2 7 4 , 0 0 1 9 2 7 6 4 , 0 7 0 2 , 4 3 0 8 2 8 0 7 1 9 a ,"
        },
        {
            "title": "A\nN",
            "content": "/ h m a e t u , a S"
        },
        {
            "title": "A\nN",
            "content": "/ 6 4 4 2 , 3 7 2 2 1 0 6 , 7 5 4 7 4 , , L o L a a M"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 0 0 0 5 4 , 0 0 0 2 6 , 3 8 2 6 , 1 5 4"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a w e a t n t o r g D i e a n a e a i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 2 n a M n a M t t a M r e r e a a a c fi e c d s u r a r a m c s t fi e s b d fi n S a a fi e S"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m n a 2 a u - S t t e d e 1 8 2 , 5 1"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a a"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 0 0 0 3 , 0 5 1 l c s C"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u u - i r - u c s e o h e h e d d i a i a v e p n a g"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 9 1 0 2 1 1 . 9 1 0 2 9 0 . 9 1 0 2 1 1 . 8 1 0 2 1 1 . 8 8 9 1 2 1 . 4 2 0 1 1 . 4 2 0 2 6 0 . 4 2 0 2 4 0 . 3 2 0 2 2 1 . 2 2 0 2 0 1 . 2 2 0 2 2 0 . 5 1 0 , t fi a o e R s g R"
        },
        {
            "title": "A\nQ",
            "content": "t t n a fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t w r E p r E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t t x E"
        },
        {
            "title": "A\nN",
            "content": "/ p a e B o d i C n S i d s e l d n a l e h e ] l [ ] 9 4 4 [ t t i"
        },
        {
            "title": "A\nQ\nd\ne\nM\nb\nu\nP",
            "content": "] l [ ] 1 8 9 [ u e P"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U i a n a e i s e l d n e h e ] l [ ] 4 2 9 ["
        },
        {
            "title": "D\nA\nu\nQ\nd\ne\nM",
            "content": "y - ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ", I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", n S i d r t H"
        },
        {
            "title": "A\nQ\nc\ni\nm\no\nn\ne\ng",
            "content": "p - u M"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "y o g i l e a u C r c M u o e u A g i l e a u o u o u o y o a l n l l g i l e a u o M"
        },
        {
            "title": "R\nH\nE",
            "content": "s e l d n a l ] l [ ] 2 6 7 [ - e O ] l [ ] 3 6 7 ["
        },
        {
            "title": "A\nQ\ne\nl\nu\nc\ne\nl\no\nM",
            "content": "] l [ ] 4 0 7 [ -"
        },
        {
            "title": "D\nA\nR\nA\nQ\nV",
            "content": "] l [ ] 4 5 6 [ P ] l [ ] 5 9 6 ["
        },
        {
            "title": "N\nO\nC\nA\nE\nB",
            "content": "] l [ ] 2 8 9 [ e ] l [ ] 6 2 6 [ C t X , g N"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t n t o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 3 a u - S n M"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A s d i x n a t n o n n a W"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 0 2 0 2 7 0 . 0 2 0 2 3 0 . 0 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nQ\nh\nt\nl\na\ne\nh",
            "content": "s a m o r - m c c c c M a c c c M s e l d n a l e h e a l d m A c s c s n m c A"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "2 0 . 1 2 0 2 n - i u - u i h m o r g"
        },
        {
            "title": "H\nZ",
            "content": "6 0 . 1 2 0 2 T , t fi a C"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ - a e ,"
        },
        {
            "title": "R\nH\nE",
            "content": ", i a i k t l d , r n S i d r t H - , I"
        },
        {
            "title": "R\nM\nT\nC",
            "content": ", n S i d r t H ] l [ ] 8 9 7 [ A M - ] l [ ] 0 8 9 [ ) ( i V ] l [ ] 9 8 5 [ 9 1 - C ] l [ ] 9 1 9 [ B ] l [ ] 2 7 7 [ L S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u c s c s n m c A"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 0 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "t x o t t s e l d n a l ] l [ ] 9 4 1 ["
        },
        {
            "title": "A\nQ\nV\nh\nt\na\nP",
            "content": "n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S t t e d e W"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 1 2 0 2 t t x e fl e c s e l d n a l ] l [ ] 8 1 9 [ fl o e r a t n t e r D v t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S s d t e n a t I"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c t t - S a a fi e S i w e a t n t o r g D v r"
        },
        {
            "title": "A\nN",
            "content": "/ D n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/ v t e r D v a D"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ v e r n a e a a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u u l a u s u r c s e o h e h e h e d d n m c m c m c e a g s f i g I"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 3 2 0 2 1 1 . 3 2 0 2 0 1 . 3 2 0 2 0 1 . 3 2 0 2 t fi a t w o c fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S t t e d e t t - m e o - S t t e d e r r t d k B"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 3 2"
        },
        {
            "title": "H\nZ",
            "content": "8 0 . 3 2 0 2 8 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/ s d t e o r n x 7 0 . 3 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t d m a - S a u u l a d i a , a a c s e o h e r r e a s u r r r O m c m c fi a r e d m a - m n o n n a d m a - S n s u r a r s b d e A fi n S"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 3 2 0 2 7 0 . 3 2 0 2 7 0 . 3 2 0 2 6 0 . 3 2 0 2 3 0 . 3 2 0 2 2 1 . 2 2 0 9 0 . 2 2"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 4 2 0 2 2 1 . 3 2 0 2 t fi a r E"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t t x e n a fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t t x E"
        },
        {
            "title": "A\nN",
            "content": "/ e o e l d i c m s e l d n s e l d n a l e h e i r r r c c c M e h e s p a i e i a e a c a ] l [ ] 0 1 9 ["
        },
        {
            "title": "A\nQ\nd\ne\nM\ni\nk\ni\nW",
            "content": "] l [ ] 9 7 9 [ N ] l [ ] l [ ] 8 6 7 [ ] 0 5 6 ["
        },
        {
            "title": "D\nE\nR\no\ni\nB",
            "content": "t x e n M , o d i C n S i d r t H ] l [ ] 8 6 7 [ t - 0 1 o r E p r E e t x e t x E"
        },
        {
            "title": "A\nN",
            "content": "/ p E"
        },
        {
            "title": "A\nN",
            "content": "/ , c c , I"
        },
        {
            "title": "R\nM",
            "content": ","
        },
        {
            "title": "P\nF\nC",
            "content": ", l m ,"
        },
        {
            "title": "P\nF\nC",
            "content": ", c n , l m ,"
        },
        {
            "title": "T\nC",
            "content": ", c c . , I"
        },
        {
            "title": "R\nM",
            "content": ", - , l a s ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ". , - ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ", l a s H"
        },
        {
            "title": "A\nQ",
            "content": "l d i , c o ,"
        },
        {
            "title": "S\nU",
            "content": ", - , I"
        },
        {
            "title": "R\nM",
            "content": ","
        },
        {
            "title": "T\nC",
            "content": ","
        },
        {
            "title": "T\nC",
            "content": ". s i s e i s a c l i r / G o c l c c c M e h e ] l [ ] 6 9 7 ["
        },
        {
            "title": "A\nQ\nV\nd\ne\nM\ni\nn\nm\nO",
            "content": "s e l d n a l ] l [ ] 4 7 9 [ E i M c c c M e h e ] l [ i u c M fl F n S i d e h e ] l [ ] 3 7 7 [ a 3 - n S i d e i a e a c c c M s e l d n a l e h e a l e h e ] l [ ] 5 7 9 [ k h B -"
        },
        {
            "title": "A\nM\nJ\nE\nN",
            "content": "] l [ ] l [ ] 3 3 9 [ ] 8 9 8 [ D u - N S ] l [ ] 5 9 8 [ F , l a s , l m ,"
        },
        {
            "title": "T\nC",
            "content": ". , - , , c c s l u c l i a i s e l d n e h e ] l [ ] 6 7 9 [ e M u c c c M a c c c M e h e a l ] l [ ] 7 7 9 [ m e i 4 - G ] l [ ] 8 7 9 ["
        },
        {
            "title": "A\nQ",
            "content": "l d t L"
        },
        {
            "title": "A\nQ\nh\nt\nl\na\ne\nh",
            "content": "r s s e l d n a l ] l [ ] 5 1 ["
        },
        {
            "title": "A\nQ\nh\nc\nr\na\ne\ns\nh\nt\nl\na\ne\nH",
            "content": "s e i s e i M c c c M e h e c c c M e h e ] l [ ] 5 1 8 [ x - ] l [ ] 5 1 8 [ -"
        },
        {
            "title": "C\nB\nM\nC",
            "content": "l a a e A r r O , a a , r e fi a r e S"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "2 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t t x s a , t l d ,"
        },
        {
            "title": "R\nH\nE",
            "content": "s e l d n a l ] l [ ] 4 7 7 [ e a R"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 4 2 0 2 ,"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t , t - m I"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "4 0 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m e t x e t x t x e E"
        },
        {
            "title": "A\nN",
            "content": "/ o l n ,"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 9 5 6 [ e 3 - a a e c c c M e h e ] l [ ] 6 7 7 [ S - i l"
        },
        {
            "title": "C\nt\nn\ne\ng\nA",
            "content": "e l l n s e l d n a l ] l [ ] 9 2 5 ["
        },
        {
            "title": "A\nQ\nd\ne\nM\nM\nC",
            "content": "t"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 5 5 1 [ - A m l d e i a e a c a ] l [ ] 3 7 9 [ e b M"
        },
        {
            "title": "R\nH\nE",
            "content": "s e l d n a l ] l [ ] 6 7 7 [ - I"
        },
        {
            "title": "C\nM\nM",
            "content": "I - i l"
        },
        {
            "title": "C\nt\nn\ne\ng\nA",
            "content": "s e i s e l d n a l ] l [ ] 6 7 7 ["
        },
        {
            "title": "A\nQ\nd\ne\nM",
            "content": "- i l"
        },
        {
            "title": "C\nt\nn\ne\ng\nA",
            "content": "5 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t t x a a e c c c M a a l ] l [ ] 6 7 7 [ L - i l"
        },
        {
            "title": "C\nt\nn\ne\ng\nA",
            "content": "d e - O 0 0 0 5 , . 1 4 - , 1 - w e a t n t 2 6 2 a u - S - i u - u i h m o r 0 6 9 3 , 4 7 3 , 0 7 - o e 4 - o T - 5 3 - . i a w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S t t e d e W"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S s d t e n a t ,"
        },
        {
            "title": "T\nR\nE\nB\nl\na\nc\ni\nn\ni\nl",
            "content": "C m M , 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v r"
        },
        {
            "title": "A\nN",
            "content": "/ D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S n e o h e r a e A s d t e o r n w e a 3 a u - S s d i x n a t I"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a a fi e 4 - w e d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c A"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2 0 2 0 1 . 4 2 0 2 7 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ\nV",
            "content": "2 3 2 3 , V, 4 - ,"
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": "o 4 - w e a t n t 4 3 a u - S 4 - w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u - i r - u i h m o r i h m o r g"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2 0 2 t - m ,"
        },
        {
            "title": "A\nQ\nV",
            "content": "e l , 4 - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c A"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t ,"
        },
        {
            "title": "A\nQ\nV",
            "content": "y s w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a 3 2 a u - m e a c fi e S"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 5 2 0 2 ) m - u ("
        },
        {
            "title": "A\nQ\nV",
            "content": "l a c s c s n m c A"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t d e - O 0 0 5 1 , 4 - w e d o r g D r r O"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2 0 2 n - O 0 0 0 , 0"
        },
        {
            "title": "A\nN",
            "content": "/ 8"
        },
        {
            "title": "A\nN",
            "content": "/ m - - , , 5 - 1 l 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m n a g"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t - , s d t e o r n c s h e d i a e a c fi e i a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s d t e o r n , ,"
        },
        {
            "title": "E\nH",
            "content": "o 4 - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a a fi e S i d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S e n a d l n s b t n t a M s d t e n a t , ,"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2 0 2 0 1 . 4 2 0 2 9 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m e E"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 4 2"
        },
        {
            "title": "H\nZ",
            "content": "6 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ p r E p E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "t x e t x e t x E"
        },
        {
            "title": "A\nN",
            "content": "/ p r E"
        },
        {
            "title": "A\nN",
            "content": "/ p r E p E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s h i o p r n e a i i r r r i t n a n r u g A"
        },
        {
            "title": "P\nF\nC",
            "content": "s e l d n u c A c a ] l [ ] 5 6 9 [ e M - n ] l [ ] 4 0 1 ["
        },
        {
            "title": "A\nQ\nX\ng\nA",
            "content": "e l r r u g ] l [ ] 7 7 7 [ e e ] l [ ] 9 8 1 [ E - s a e , o d - r u o l d a i , e . , t b e n c m , o s a , u b ,"
        },
        {
            "title": "R\nH\nE",
            "content": "s r i i , I"
        },
        {
            "title": "R\nM",
            "content": ","
        },
        {
            "title": "P\nF\nC",
            "content": ","
        },
        {
            "title": "P\nF\nC",
            "content": ", l a s . ,"
        },
        {
            "title": "G\nC\nE",
            "content": ", - , c n , l m . , - , c c , , l m , , ,"
        },
        {
            "title": "R\nM",
            "content": ","
        },
        {
            "title": "T\nC",
            "content": ", c c , I"
        },
        {
            "title": "R\nM",
            "content": ", l a s . , - ,"
        },
        {
            "title": "R\nH\nE",
            "content": ", - , c c , l m . e r - r - , l a s , - X"
        },
        {
            "title": "A\nQ",
            "content": ", I"
        },
        {
            "title": "R\nM",
            "content": "l d i ,"
        },
        {
            "title": "P\nF\nC",
            "content": ", c n , l m , c c . , I"
        },
        {
            "title": "R\nM",
            "content": ", - , l a s ,"
        },
        {
            "title": "S\nU",
            "content": ","
        },
        {
            "title": "T\nE\nP",
            "content": ","
        },
        {
            "title": "T\nC\nO",
            "content": ","
        },
        {
            "title": "T\nC",
            "content": ". ,"
        },
        {
            "title": "T\nC",
            "content": "s e l d n a l ] l [ ] 1 7 7 [ e l s e l d n e h e ] l [ ] 0 2 5 [ p e c c c M e h e ] l [ ] 3 7 8 [ -"
        },
        {
            "title": "A\nQ\nV\nX\ne\nM\nE\nG",
            "content": "s e l d n a l ] l [ ] 5 9 7 ["
        },
        {
            "title": "A\nQ\nV",
            "content": "- I -"
        },
        {
            "title": "D\nC\nM\nM",
            "content": "I n S i d r t H ] l [ ] 5 7 7 [ e e e s e l d n a l ] l [ ] 0 7 7 ["
        },
        {
            "title": "A\nQ",
            "content": "t X s e l d n a l ] l [ ] 6 6 9 [ - c M e e i a e a c a ] l [ ] 7 6 9 [ p s s e l d n a l ] l [ ] 8 6 9 [ e c l s e l d n a l ] l [ ] 9 6 9 [ -"
        },
        {
            "title": "V\nA\nQ\nd\ne\nM\nd\nl\nr\no\nW",
            "content": "s e l d n a l ] l [ ] 0 7 9 [ i - R c c c M a a l ] l [ ] 0 8 8 [ 5 2 - n d e i a e a c a ] l [ ] 1 7 9 [ e - I"
        },
        {
            "title": "A\nM\nG",
            "content": ", o d i C , p i a D"
        },
        {
            "title": "R\nH\nE",
            "content": "s e l d n a l ] l [ ] 9 6 7 [ e M l a s s e l d n a l ] l [ ] 1 5 1 [ e i t y o s e l d n a l ] l [ ] 2 7 9 [ - s n i l e i a e a c a ] l [ ] 1 0 8 [ l n C , l a s , l m , p i g , a l n ,"
        },
        {
            "title": "T\nC",
            "content": ". s e l d n a l ] l [ ] 6 7 7 [ N - i l"
        },
        {
            "title": "C\nt\nn\ne\ng\nA",
            "content": "y - c c c M e h e ] l [ ] 9 6 8 ["
        },
        {
            "title": "A\nQ\nV\nX\ne\nR",
            "content": "e i f o t a - A o s a s a n n e n H t n c S u L a R T e i o a b e a c fi e n o e t x ) d t m ( r - i l r l d a e M ] l [ ] 3 8 9 [ e - e i n e e P l B l e a u o ] l [ ] 4 9 6 ["
        },
        {
            "title": "R\nE\nE\nP",
            "content": ""
        },
        {
            "title": "U\nE\nL\nB",
            "content": ","
        },
        {
            "title": "M\nE",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", d e - O 1 7 1 , 6 5 5 A"
        },
        {
            "title": "Q\nC\nM",
            "content": "7 5 4 2 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A A"
        },
        {
            "title": "Q\nC\nM",
            "content": "2 3 3 3 , - , u i r a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A -"
        },
        {
            "title": "U\nA",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", c , s r P"
        },
        {
            "title": "C\nR\nP\nU\nA",
            "content": ","
        },
        {
            "title": "C\nO\nR",
            "content": ", n - O , a / T e e , 2 4 4 6 3 3 , g t"
        },
        {
            "title": "R\nE\nN",
            "content": "w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , r e r e a m c - , s d t e o r n s t fi e"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 1 0 2 t fi a 4 - w e a t n t 2 1 a u - S r e r e a m c A"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "s t o e - e y s i , l , n S l e c M d , l r l C a n S i d a e e h e l l c - u e h e e r r e - u , l r l d r c M e o N , m - u , l r l d a e M . , e o N , m P , m i M A t a p T"
        },
        {
            "title": "Q\nC\nM",
            "content": "e 2 4 0 1 p i p f n o L n a n - A o k n H t n n p n H t n c S u L e e T v t d i D a c fi e n o D"
        },
        {
            "title": "C\nC\nP",
            "content": ","
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": ","
        },
        {
            "title": "C\nC\nM",
            "content": ","
        },
        {
            "title": "R\nO\nE\nT\nE\nM",
            "content": ", ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": "c A"
        },
        {
            "title": "1\nF",
            "content": ", t r i n I"
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": ","
        },
        {
            "title": "C\nC\nP",
            "content": ","
        },
        {
            "title": "C\nC\nS",
            "content": "c c A"
        },
        {
            "title": "C\nC\nM",
            "content": "c A"
        },
        {
            "title": "C\nC\nM",
            "content": ", , , ,"
        },
        {
            "title": "1\nF",
            "content": ", c , s r P"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": "- N , x r o l l o , A"
        },
        {
            "title": "1\nF",
            "content": ", c , c R"
        },
        {
            "title": "1\nF",
            "content": ", c R"
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", c c A , , i e o c P"
        },
        {
            "title": "1\nF",
            "content": ", , , A"
        },
        {
            "title": "C\nC\nM",
            "content": ", s r , A"
        },
        {
            "title": "C\nC\nM",
            "content": "2 ,"
        },
        {
            "title": "C\nR\nP\nU\nA",
            "content": ","
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": ", 2 ,"
        },
        {
            "title": "C\nC\nP",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": "c A"
        },
        {
            "title": "C\nC\nS",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "C\nC\nP",
            "content": ", 2 R"
        },
        {
            "title": "1\nF",
            "content": ", A"
        },
        {
            "title": "C\nC\nM",
            "content": "d e - O n - O t fi a o e R , n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "n a fi a o e R t n e - O n - O n - O , a / T t fi a o e R n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "n a fi a o c fi a o c fi a C"
        },
        {
            "title": "Q\nC\nM",
            "content": ","
        },
        {
            "title": "Q\nC\nM",
            "content": "n a fi a C"
        },
        {
            "title": "Q\nC\nM",
            "content": "n s e R"
        },
        {
            "title": "Q\nC\nM",
            "content": "n a fi a C"
        },
        {
            "title": "Q\nC\nM",
            "content": ", t fi a o e R n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "- D , d v - D , d v c h e i c h e i A A ,"
        },
        {
            "title": "1\nF",
            "content": ", s r , c , . , c y a / f o r G t n e - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O . ,"
        },
        {
            "title": "1\nF",
            "content": ", 2 - B , d e - O ,"
        },
        {
            "title": "C\nR\nP\nU\nA",
            "content": ","
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": "l i R ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "2 ,"
        },
        {
            "title": "E\nS\nM\nR",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ - D , d v"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ", l m s e l m , e p g t"
        },
        {
            "title": "C\nR\nP\nU\nA",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "E\nA\nM",
            "content": ", s e - D , d v , , i e . , s r , 2 , c . , c . , ,"
        },
        {
            "title": "E\nS\nM",
            "content": ","
        },
        {
            "title": "E\nS\nM",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": "l m , n fi s k - D , d b i a v"
        },
        {
            "title": "D\nS\nM\nR",
            "content": ","
        },
        {
            "title": "E\nA\nM",
            "content": ", s r , c . ,"
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ","
        },
        {
            "title": "C\nC\nP",
            "content": ". ,"
        },
        {
            "title": "C\nR\nP\nU\nA",
            "content": ", c . ,"
        },
        {
            "title": "C\nR\nP\nU\nA",
            "content": ", ,"
        },
        {
            "title": "E\nS\nM",
            "content": ", ,"
        },
        {
            "title": "1\nF",
            "content": "c , . ,"
        },
        {
            "title": "E\nS\nM",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": "n a e ,"
        },
        {
            "title": "R\nE\nN",
            "content": ", t fi a , t fi a o e R n - O d - O t n , t fi a o e R n - O d - O n - O k 4 3 . n - O i r n s e d - O - n n a 7 2 1 , 9 1 6 6 4 7 3 , 5 n - O e B 2 0 . x , c m h M , c A"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ", c c A s H a - A d , d e - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "- O , a / T e e , ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e a a ,"
        },
        {
            "title": "E\nS\nM",
            "content": "n s e A"
        },
        {
            "title": "Q\nC\nM",
            "content": "9 3 8 9 1 , 2 2 8 8 3 , 1 9 9 6 , 2"
        },
        {
            "title": "A\nN",
            "content": "/ 1 2 1 0 0 2 2 2 5 2 , 4 - G r 4 - , . 5 3 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t n t o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 3 7 9 2 ,"
        },
        {
            "title": "A\nN",
            "content": "/ 4 1 8 , 3"
        },
        {
            "title": "A\nN",
            "content": "/ 6 9 2 n 5 3 . a 0 0 0 3 , 0 0 8 4 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 t n t o r g D t n t o r g D i e a 3 5 t n t e r a"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 6 1 4 , 3"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a a s t fi fi e S n S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A a u - m e o h e d i a A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e t t e o d m A a m c s t e a ; a a c s e a d s t fi fi fi e i i i i c s fi fi e i i h e d i a A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u c s c s n m c A"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A t t - S - , o a i a c s c s n o i s e s n 9 8 5 , 1 9 1 8 4 6 0 8 , 5 2 4 8 , 4 5 1 3 6 3 4 8 , 8 1 2 4 , 0 3 4 3 , 0 0 6 , 3 - ,"
        },
        {
            "title": "T\nP\nG\no\ni\nB",
            "content": ", B N"
        },
        {
            "title": "M\nL\nd\ne\nM\no\ni\nB",
            "content": ","
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": ", 2 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": "w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r r O"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u , s d t e e o r r e a t e c d A"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e c s c s c s n d i a i a o r g D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e c s e o s u r a r r e r e r e a d d i a i a i a i a 6 5 , 4 7 1 8 6 4 3 1 , 1 9 7 7 2 , 0 0 0 8 , 1 3 1 6 3 , 0 5 4 8 6 , 4 1 9 6 , 2 2 1 . 8 4 6 5 1 , 1 8 6 1 ,"
        },
        {
            "title": "A\nN",
            "content": "/ 2 4 ( n ) a 6 9 1 . 0 7 9 0 4 + 0 3 6 7 2 . 0 5 0 1 8 7 0 7 4 , 2 6 9 6 3 9 1 , , 3 5 2 9 3 9 1 , 7 4 7 1 . 4 7 0 , 0 0 0 2 , 0 2 1 6 3 7 8 1 , 2 5 2 5 7 6 0 0 0 4 , 0 4 0 1 1 , - 2 ("
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ i s"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "T\nS\nE\nR\nC",
            "content": ")"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "w e a t n t e r v n i e a d o r g D i d o r g D v n i e a w e a t n t w e a i d o r g D v n i e a D i d o r g D v n i e a w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a w e a t n t e r a t n t e r v n i e a d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/ v t D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e c s e o h e h e r r e c s c s n a a a e A e A e A e A r e h e d i a e o h e d c d s u r a r c d s u r r e a e A s d t e n a t e o d m A t t e d e s t c fi e d m A r e r e a m c u l a u s u r a r a e A r e c s c s c s d n m c m c t t e o r r e a e A n e o h e r a e A n e o h e d c d l a c s c s n m c d m A r e r e a e A a u s u r a r c d d m A r e h e d i a e o s u r a r a e A a u c s c s n c d l a c s c s n m c d m A r e r e a e A a u s u r a r c d l a c s h e d i a e o s u r a r a e A n a M r e r e d i a e a t a P"
        },
        {
            "title": "A\nN",
            "content": "/ , s d t e e o h e r a t e c d A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S v n i e a n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S 4 a u - S - i u - u i a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - i u - u i h m o r s u e e n r C r r O t g"
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": "w e a t n t + 8 a u - S"
        },
        {
            "title": "A\nN",
            "content": "/ v t 0 a u - i r - u - i u - u i h m o r i h m n a g"
        },
        {
            "title": "Q\nC\nM",
            "content": "1 6 0 , 9"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r e r e a e A"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2 0 2 6 0 . 5 2 0 2 6 0 . 5 2 0 2 4 0 . 5 2 0 2 3 0 . 5 2 0 2 2 0 . 5 2 0 2 0 . 5 2 0 2 1 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 4 2 0 2 2 1 . 4 2 0 2 0 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 3 2 0 2 6 0 . 3 2 0 2 5 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 0 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 2 1 0 2 5 0 . 5 2 0 2 2 0 . 5 2 0 2 1 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 3 2"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 2 1"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 2 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 4 2 0 2 1 1 . 2 2 0 2 6 0 . 2 2 0 2 4 0 . 2 2 0 2 2 0 . 2 2 0 2 6 0 . 1 2 0 2 1 . 0 2 0 2 1 0 . 8 1 0 2 7 0 . 5 1"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 9 1"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 0 2"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 1 2 0 2 8 0 . 1 2"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 3 2 0 2 2 0 . 1 2 0 2 2 0 . 1 2 0 2 1 0 . 1 2 0 2 t fi t fi i fi t fi a s C a s C"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 1 2 0 2 1 0 . 1 2 0 2 7 0 . 9 1 0 2 6 0 . 9 1 0 2 2 0 . 8 1 0 2 2 0 . 7 1 0 7 0 . 6 1 0 2 6 0 . 4 1"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 5 2 0 2 3 0 . 3 2 0 2 3 0 . 7 1 0 2 T t fi a n a fi t fi a s C s g o c fi s n a fi a o c fi a o c fi s , t fi a o e R t fi a , t fi a C"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/ 5 0 . 5 2 0 2 t fi a x a t w R"
        },
        {
            "title": "A\nQ",
            "content": "t t t w x a R"
        },
        {
            "title": "A\nQ",
            "content": "t t w x a e w x a e R g - t P i r n s e e R t R e R t t w R"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ p r E p r E p t x e t x E"
        },
        {
            "title": "A\nN",
            "content": "/ d e o t x e t x t x e t x E"
        },
        {
            "title": "A\nN",
            "content": "/ p r E"
        },
        {
            "title": "A\nN",
            "content": "/ p r E p r E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ m l c s"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w l c i H"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t , t fi a o e R t n o c fi s C"
        },
        {
            "title": "A\nQ",
            "content": "t t t x e E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ p x c t c r i e G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ e s e s t c d e N e s D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ e s t c c q i r n e D n e n e i l e o u n e i l N e r r e c r , t fi a n s e R"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ t fi a o c fi t fi a i l C"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U e s t c c q d e N n e n e i l e o u n e R e s R d C"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/ b e n c m n o ,"
        },
        {
            "title": "A\nB\nR",
            "content": ","
        },
        {
            "title": "A\nN\nD",
            "content": "e s / e S e s - c s - c s - c i l N n e i l e u e o u s - c n e e o u n e i l e u e o u n e e o u s - c i - u i - u i - u i - u s o - u i - u i - u i - u i - u i - u s o - u ] l [ ] 4 6 7 [ e L - ] l [ ] 4 8 9 [ e P ] l [ ] 6 6 7 [ e - n ] l [ ] 6 4 1 ["
        },
        {
            "title": "A\nQ\nV\no\nr\nc\ni",
            "content": "M ] l [ ] 2 0 8 [ A D ] l [ ] 6 3 6 [ t - C G ] l [ ] 6 3 6 [ C G ] l [ ] 0 1 5 [ i r i i e ] l [ ] 5 8 9 [ e B ] l [ ] 6 8 9 [ n n N ] l [ ] 7 3 9 [ m 2 ] l [ ] 7 3 9 [ u 2 ] l [ ] 0 4 [ ) t ( ] l [ ] 0 4 [ )"
        },
        {
            "title": "A\nN\nD",
            "content": "( - G L - G L ] l [ ] 8 1 3 [ m e s o - u i - u i - u i - u s o - u i - u i - u i - u i - u i - u s o - u i - u i - u i - u i - u i - u s o - u i - u i - u i - u i - u ] l [ ] 9 8 9 [ t fi a e p ] l [ ] 8 8 9 [ e n B ] l [ ] 0 4 6 [ -"
        },
        {
            "title": "A\nQ\nA\nN\nR",
            "content": "] l [ ] 0 2 1 [ o B ] l [ ] 1 6 7 [ m e - a - l - m g ] l [ ] 7 3 9 [ m e m G ] l [ ] 7 1 3 [ a n U ] l [ ] 5 6 7 [ r n ] l [ ] 2 4 9 [ s D t e m ] l [ ] 0 6 7 [ e s n e u - r l r u ] l [ ] 0 6 7 [ e r n H ] l [ ] 4 4 9 [ o c p u ] l [ ] 0 6 7 [ m ] l [ ] 8 3 9 [ r P u ] l [ ] 1 4 9 [ o M c n - o ] l [ ] 7 3 9 [ T - n o e h a n H ] l [ ] 4 0 8 [ R ] l [ ] 0 9 9 [ ] l [ ] 6 4 9 [ ] l [ h o k 8 6 h ] 1 9 9 [ t ] l [ ] 7 0 3 [ O c l , h i e a d - D t t - s c l t"
        },
        {
            "title": "F\nT",
            "content": ", e s , e s . , d s a e p m c m c A"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "t t t T"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", e s t e e s ,"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", e s t e u ,"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "e e a a e M"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", e s t n a f D 3 , e s t e u s ,"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "e e G"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", e s t - l d n a l y r , m P n ] l [ ] 3 9 9 ["
        },
        {
            "title": "Q\nA\nF\ns\nl\na\nc\ni\nt\nu\ne\nc\na\nm\nr\na\nh\nP\nA\nD\nF",
            "content": "] l [ ] 3 0 8 ["
        },
        {
            "title": "B\nD\no\np\ne\nr",
            "content": "y i C s h r G e y i C e G t e a e t e a e t e a e y i C e y i c y i c y r y r P a h a h a h a h a h a h y r ] l [ ] l [ ] 5 0 1 [ ] 8 4 9 [ u x h C"
        },
        {
            "title": "A\nQ\nm\ne\nh\nC\nr\na\nl\no\nh\nc\nS",
            "content": "] 2 5 7 [ e e m ] l [ ] 6 1 2 [ E ] l [ ] 6 0 6 [ O ] l [ ] 9 4 9 [ m ] 1 5 7 [ E C ] l [ ] 5 1 2 ["
        },
        {
            "title": "C\nN\nI\nZ",
            "content": "] l [ ] 4 1 2 ["
        },
        {
            "title": "O\nM\nP",
            "content": "] l [ ] 0 5 9 [ t p ] l [ ] 5 9 9 [ 2 4 P ] l [ ] 2 5 9 [ e B ] l [ ] 1 5 9 [ 2 T ] l [ ] 3 5 7 [ e i ] l [ ] 6 9 9 ["
        },
        {
            "title": "M\nO\nE\nG",
            "content": "] l [ ] 3 5 9 [ ] l [ ] 3 4 2 ["
        },
        {
            "title": "C\nD\nT",
            "content": "] l [ ] 0 1 6 [ p p s o - u , s h B ] l [ ] 4 9 9 [ n e m t e C"
        },
        {
            "title": "G\nE\nE",
            "content": "e i r ] l [ ] 2 9 9 [ e - r A"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "y i C e y a h ] l [ ] 4 5 9 [ B D ] 2 1 6 ["
        },
        {
            "title": "O\nT\nP\nS\nU",
            "content": "t t t t t o V i s c - y r . , n e , , c c m e M , t n n u a , t , s y a e i P e s y a e i P e ] l [ ] 2 2 2 ["
        },
        {
            "title": "D\ne\nR\nS\nF",
            "content": "] l [ ] 0 8 6 ["
        },
        {
            "title": "A\nQ\nI\nP",
            "content": "] l [ ] 6 4 7 [ e ] l [ ] 7 3 7 [ R ] l [ ] 5 3 7 [ h - c h i P e ] l [ ] 0 5 7 [ n M p n e t c e - O 0 0 1 5 ,"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m n a g"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t t x A n - O 7 9 2 1 ,"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a l a - i u - u e n r C t g"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t e d r U n - O"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a l a c s t O"
        },
        {
            "title": "A\nN",
            "content": "/ 2 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nN",
            "content": "/ T"
        },
        {
            "title": "Q\nC\nM",
            "content": "0 0 2 1 , 4 - w e a t n t d m a - S - i u - u i h m o r g"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t , u g n , u G p - s i s ( a i - e , e a e r ( , ) i ) i e 4 - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r e r e a m c A"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U - e , e a ) n e l p ( , i e - c , m d e c h e , , n e i g - e - l ( m a , e a , ) n e l p ( , i e - a m p , a , i t - e - l ( r D , e a , ) n e l p ( - a m p , a , i t ) t e , i ) t - T , t a t E . , m d , n e - , e g E H , l o - c , s o A , v l a - M , n e"
        },
        {
            "title": "M\nm\nu\nt\nn\na\nu\nQ",
            "content": ", t a . , n - T - T , , t a t E . , t , m d , n e i g r l . , m d , n e ] l [ ] 0 4 7 [ s G ] l [ ] 9 3 7 [ s s ] l [ ] 4 4 7 [ e ] l [ ] 2 4 7 [ S P ] l [ ] 1 8 6 [ r i P"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U b T m - u ] l [ ] 7 8 9 [ n c m n B ,"
        },
        {
            "title": "F\nr\nh\nc",
            "content": ", , c D"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": "c c A , e r g I , c n a v e c - i s s C"
        },
        {
            "title": "1\nF",
            "content": ", c , x r , s r , i e a e - , x m l o h d m , c r e v r ,"
        },
        {
            "title": "G\nC\nD\nN",
            "content": ","
        },
        {
            "title": "R\nR\nM",
            "content": ", ,"
        },
        {
            "title": "1\nF",
            "content": ", s r , c , . c c , y c ,"
        },
        {
            "title": "C\nC\nP",
            "content": "n a fi a e - O n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O n - O n - O n - O n - O n - O A - O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e , n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e - O , ,"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": "r I ,"
        },
        {
            "title": "R\nO\nE\nT\nE\nM",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": "d e - O d e - O d e - O"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ", d V C I"
        },
        {
            "title": "N\nN\nS",
            "content": ", n i ,"
        },
        {
            "title": "R\nO\nE\nT\nE\nM",
            "content": ", c , F ,"
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", c c d - O n - O n - O d - O n - O n - O , n i ,"
        },
        {
            "title": "E\nS\nM\nR",
            "content": ","
        },
        {
            "title": "C\nR\nP\nU\nA",
            "content": ", , d V e N"
        },
        {
            "title": "E\nA\nM",
            "content": "c c d - O n - O n - O n - O"
        },
        {
            "title": "C\nO\nR\nU\nA",
            "content": ","
        },
        {
            "title": "E\nA\nM",
            "content": "d e - O"
        },
        {
            "title": "1\nF",
            "content": ", c , s r , A"
        },
        {
            "title": "Q\nC\nM",
            "content": "e n e - O A"
        },
        {
            "title": "Q\nC\nM",
            "content": "c , c E ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": "- O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e d e - O . ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", . ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", - O - O , ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e . ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ","
        },
        {
            "title": "U\no\nI",
            "content": ", e S , - O - O , ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e . ,"
        },
        {
            "title": "U\nE\nL\nB",
            "content": ","
        },
        {
            "title": "U\no\nI",
            "content": "c c , d e - O n - O"
        },
        {
            "title": "Q\nC\nM",
            "content": "0 0 0 3 , 4 0 3 3 , 0 0 4 1 , 2 8 0 3 , 5 2 4 4 , 6 8 . 0 0 0 5 , 5 9 . 2 3 4 4 3 6 6 1 , 5 8 3 4 7 6 , 6 0 9 5 8 1 3 , 0 0 3 3 , 9 1 . 7 3 1 , 0 0 0 0 0 0 2 , 7 9 1 , 9 6 1 1 0 3 3 , 0 0 0 , 6 7 2 6 0 , 8 0 4 0 0 0 , 0 0 3 7 2 , 8 2 6 2 4 7 2 0 5 0 5 6 3 3 6 3 , 0 0 6 0 2 3 0 0 0 3 , 9 7 7 9 2 , 0 0 5 1 1 , 0 0 0 0 , 6 1 5 2 , 9 8 3 2 3 , 3 3 3 , 8 3 7 3 , 7 1 9 2 6 , - 3 - d m - t a , 7 - - S D 0 4 - 2 r I - 7 8 - t M u n . 5 3 - , 4 - y o o 4 - o 4 - o 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 4 - , - 2 Q 4 - G"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a w e a t n t e r a t n t e r n a e a i d o r g D v d o r g D v n i e a w e i d o r g D a t n t e r n a e a i d o r g D v d o r g D v n i e a w e a t n t e r a t n t e r v n i e a d o r g D v d o r g D v t e r D v a w e a . ,"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": "r I C D , , ,"
        },
        {
            "title": "R\nO\nE\nT\nE\nM",
            "content": ", , , . ,"
        },
        {
            "title": "R\nO\nE\nT\nE\nM",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", A"
        },
        {
            "title": "U\no\nI",
            "content": "c c A"
        },
        {
            "title": "U\nE\nL\nB",
            "content": "d e - O n - O n - O n - O n - O n - O d - O n - O 2 0 4 7 1 , 0 0 5 1 3 , 2 3 2 7 7 , 6 3 0 1 , 1 2 9 0 1 , 0 0 1 2 , 3 1 6 , 6 1 3 6 6 0 1 ,"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "R\nO\nE\nT\nE\nM",
            "content": ","
        },
        {
            "title": "U\no\nI",
            "content": ", d e - O 0 1 c V"
        },
        {
            "title": "D\nE\nE",
            "content": "d e - O 0"
        },
        {
            "title": "1\nR\n-\nk\ne\ne\nS\np\ne\ne\nD",
            "content": ", 1 i d o r g D 8 7 1 a u - m S"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U l p a R - d - O 0 0 0 5 5 , - ("
        },
        {
            "title": "P\nI\nL\nC",
            "content": "I"
        },
        {
            "title": "A\nn\ne\np\nO",
            "content": ") 4 1 / i d o r g D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S n - O 0 0 0 2 , 4 - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S e n a - i r - u i o c e e n r C t g"
        },
        {
            "title": "H\nZ\nN\nE",
            "content": ", 7 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "- , u g e d y n S , h - s i i p ( r D - e , e a e r ( , ) i ) i e - a , t a t E , n e , s o A , = t , s P c - c u u , m d e . e , o V s l n ] l [ ] 9 4 7 [ e - M"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r r O"
        },
        {
            "title": "A\nN",
            "content": "/ 6 0 . 5 2 0 2 s l n ] l [ ] 8 4 7 [ h I 4 - w e a t n t D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a D"
        },
        {
            "title": "A\nN",
            "content": "/ n - i u - u - i u - u e n r C t e n r C t - i u - u e n r C t g"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "e d r U"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t e d r U"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ - e - l ( r D , e a , ) n e l p ( , i e - a m e , a , i t - s i s ( m a - e , e a e r ( - , t r e s ( T - h , r e t , r t ) u t , ) i ) i , i ) t T j c y r , c c , n e . e , s n o , t , , t a t E , n e u u . , m d e , s c t , n e , t , t a t E , n e . , s c t ] l [ ] 5 4 7 [ e P ] l [ ] 1 4 7 ["
        },
        {
            "title": "X\ny\nh\nP",
            "content": "] l [ ] 8 3 7 [ e U P p i p f n o L T i M t a p z o t a - A o k n H t n n p n H t n c S u L e e T v t d i D a c fi e n o D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ v t e r D . 3 a , 4 - t o y u . - 5 1 - m c L D . 5 3 - G A v n i e a 4 a u - S i a w e a i a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u t t 6 a u - S t t a M n - i u - u i h m c fi e , t t e d e c s c s d i a c s n o a r I a s u r a r s u r r e a e A a a m c m c , r e i g"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2 0 2 1 1 . 4 2 0 2 5 0 . 4 2 0 2 5 0 . 4 2 0 2 9 0 . 3 2 0 2 5 0 . 3 2 0 s t fi e S"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2 0 2 t - m I"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t w R"
        },
        {
            "title": "A\nN",
            "content": "/ v n i e a + 0 3 n - i r - u i h m o r x 6 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t e d r U s g 0 0 0 8 , 3 - 2 - L p w e a t n t c h t ] l [ ] 0 2 2 [ 2 P"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 4 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x T"
        },
        {
            "title": "S\nD\nA",
            "content": ", p i a c h t ] l [ ] 2 8 7 [ a D d fi P"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ +"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - , t s u r r I r e a c d l a s a g s f i g I a u r e d i a e a c fi e , r e u M u l a u l a u l a l a u l a u l a u M u s a g s f i g I r s h e o s u r a r a r a a e A e A r e r e a m c e a i x n a t t t g s f i g I r e c s c s h e d n m c m c e a g s f i g I s d t e n a t e a c fi e s b d fi n S a a fi e S"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2 0 2 1 0 . 5 2"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 4 2"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 4 2 0 2 5 0 . 4 2 0 2 8 0 . 3 2 0 2 5 0 . 3 2 0 2 1 1 . 1 2"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 0 2"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 0 2"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 9 1"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 7 1 0 2 1 1 . 4 2 0 2 5 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t t x e t x e t x t x e t x e t x e E e t x e t x e t x t x e t x e t x a r t x e t x i l , x y o A , i e c n s i o c n O a i i o A p i a e i a s a m c e c d s a c d e m h t - m r , t e c - s l , l t g d s s m c e y a e i ,"
        },
        {
            "title": "S\nE\nI\nF\nL\nE\nS",
            "content": "e I - , t p ,"
        },
        {
            "title": "C\nA\nP\nU",
            "content": "I i C u o , n ,"
        },
        {
            "title": "A\nQ",
            "content": ","
        },
        {
            "title": "F\nI\nC",
            "content": "y p l r , t p n c s fi e l r N c m , t p c"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": "l m y p l m y p l m , e p c m ,"
        },
        {
            "title": "F\nI\nC",
            "content": "n i m o u n fi e S"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "y o A n s o t m r y o A n s y o A ] l [ ] 7 9 9 [ e v s ] l [ ] 8 7 7 [ 1 M s ] l [ ] 9 6 6 [ r ] l [ ] 1 6 5 ["
        },
        {
            "title": "A\nM\na\nL\nL\no\nr\nt\ns\nA",
            "content": "] l [ ] 5 2 7 [ - s ] l [ ] 6 5 9 [ A ] l [ ] 3 4 7 [ P ] l [ ] 9 7 7 [ - s y o A e s r M e s r e i l e M c c a t c c a t c c a t M e s r M e s r M e s r a ] l [ ] 7 5 7 ["
        },
        {
            "title": "A\nQ\nk\no\no\nB\nt\na",
            "content": "M ] l [ ] 8 0 1 ["
        },
        {
            "title": "A\nQ\nC\nS\na\nM",
            "content": "] l [ ] 9 9 9 [ - t ] l [ ] 1 9 6 [ 0 2 - I"
        },
        {
            "title": "B\nE\nh\nC",
            "content": "] l [ ] 0 9 6 [ O ] l [ ] 1 7 [ e M ] l [ ] 8 5 7 [ a G ] l [ ] 8 1 2 [ N c M ] l [ ] 7 0 2 [ e M ] l [ ] 5 7 [ S ] l [ ] 8 9 9 [ T s y r A ] l [ ] 0 8 7 [ l - s r s y r ] l [ ] 1 8 7 [ C A e i l e ] l [ ] 6 5 7 [ e M 4 L n S i a n S i a ] l [ ] 2 9 6 [ - 0 2 - I"
        },
        {
            "title": "B\nE\nh\nC",
            "content": "] l [ ] 9 1 1 [ g a"
        },
        {
            "title": "M\ne\nh\nC",
            "content": "e i l e o 4 - w e a t n t 5 t t - S r e r e a e A"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m i a w e a w e a + 0 2 a u - S r e h e d i a A"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w + 0 2 a u - S r e r e d i a A"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 5 2"
        },
        {
            "title": "T\no\nC\nh\nt\ni",
            "content": "w x + 0 1 a u - S - A , t s u r r I a r d c d W"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t d m a - S o a i a o B"
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2 0 2 + 0 1 + 0 4 + 4 a u c s h e d i a a M s d t e n a t a M t t e d e W"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 2 2 0 2 9 0 . 4 2 0 2 5 0 . 5 2"
        },
        {
            "title": "T\no\nC",
            "content": "h ) m - u ("
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ\nV",
            "content": "4 a u - S a a fi e S"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2 0 2 ) m - u ("
        },
        {
            "title": "A\nQ\nV",
            "content": "e I"
        },
        {
            "title": "M\nF\nA",
            "content": ","
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": "y i C s h , e s r , e s r M y n S h r E - t , o l n e o C o y i r p e A p t x e t x E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": ", s e e e c d s a m c g n t R"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": ", s e e o x , h h , h h L , h h , h i e o , h i e p r , h i e o e p d , h r , h m t , h r , h m , h r , h m t h m r s t h m t ] l [ ] 6 8 7 [ e - a m ] l [ ] 3 8 7 [ B m C ] l [ ] 9 6 5 [ e e ] l [ ] 8 2 7 ["
        },
        {
            "title": "A\nQ\nr\ne\nh\nt\na\ne",
            "content": "W ] l [ ] 1 7 6 ["
        },
        {
            "title": "A\nQ\na\nm",
            "content": "i ] l [ ] 2 5 1 [ a ] l [ ] 0 7 6 [ t e i t g n t R ] l [ ] 5 8 7 [ e - X e o L ] l [ ] 7 6 5 [ e G v w e a t n t e r D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S a u u M"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ a u a t n t o r g D v t n a e a i e a n a e a D"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 5 a u - S n e o A t t a M n a M n , r e h e d t t n i o r e s u r r e r e r e r e a a c d c d n a t i a c d s u r r e c s c s c s h e d n a r e c s s u r a r a r a r d d e a s b d e A e A e A fi fi e i i i a i a c d A"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "3 0 . 5 2 0 2 8 0 . 4 2 0 2 6 0 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m , t - m , t - m I"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s g n n e o e e e e i S m n e o g n t R ] l [ ] 0 0 0 1 [ - o ] l [ ] 7 9 7 [ ] l [ ] 2 5 5 [ - l - M n R V"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 3 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m I"
        },
        {
            "title": "A\nN",
            "content": "/ s e e i S m ] l [ ] 8 8 5 [ h G"
        },
        {
            "title": "N\nE",
            "content": "7 0 . 3 2 0 2 0 1 . 2 2 0 2 8 0 . 2 2 0 2 5 0 . 0 2 0 2 5 0 . 0 2 0 2 2 1 . 7 1 0 7 0 . 6 1 0 2 7 0 . 6 1"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t - m t - m t - m , t - m , t - m t - m e - m t - m I I I"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s g n n e i s s s i s s g n t R m t R o e e o e e o g n t R i S m n e o g n t R s e e i S m n e o R i S m ] l [ ] 1 0 0 1 [ ] l [ ] l [ ] 4 9 7 [ ] 4 9 7 [ i C N -"
        },
        {
            "title": "N\nE\nB\nR\nH\nA\nQ\nV\nS\nR",
            "content": "- R - S ] l [ ] 5 0 8 ["
        },
        {
            "title": "G\nV\nS\nR\nR\nO\nD",
            "content": "I - ] l [ ] 2 0 0 1 ["
        },
        {
            "title": "D\nC\nI\nS\nR",
            "content": "] l [ ] 2 0 0 1 [ i C - d ] l [ ] 2 0 0 1 [ i C U - ] l [ ] 4 8 7 [ E 63 . i w s d t i ] l [ . s"
        },
        {
            "title": "M\nL\nL\nM",
            "content": "/ s"
        },
        {
            "title": "M\nL\nL",
            "content": "c fi e r t t o u e e s r g r u : I"
        },
        {
            "title": "V\nE\nL\nB\nA\nT",
            "content": "s t o u E T e i n a n - A o s a s a n i P a n a n r e g e l e y a n o fi e t t c A"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x 0 5 5 1 1 , 4 - , 4 - G , a e r D 0 5 a u - S - i u - u i h m o r g"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 3 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "c A"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x 0 9 1 5 , 4 - , 4 - G , a e r D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m n a g"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 3 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", - ,"
        },
        {
            "title": "T\nC",
            "content": ", I"
        },
        {
            "title": "R\nM",
            "content": ","
        },
        {
            "title": "A\nQ\nV",
            "content": "c fi e , e l d i ( c M & , s h , l ( e h e , ) s , M , a e . , t g D , c M i C & T , ) a c u , m P ) . , i a ( i n E , s h , l ( e h e , ) s , M , a e ] l [ ] 9 8 7 ["
        },
        {
            "title": "U\nM\nM\nM",
            "content": ", - ,"
        },
        {
            "title": "T\nC",
            "content": ", I"
        },
        {
            "title": "R\nM",
            "content": ","
        },
        {
            "title": "A\nQ\nV",
            "content": "c fi e , e l d i ( c M & . , t g , c M i C & T , ) a i P , m P ) . e , i a ( r i ] l [ ] 0 9 7 [ P M 1 @ p n - O p c d e - O p c d - O , u g n e E A n - O u g n A"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x A"
        },
        {
            "title": "Q\nC\nM",
            "content": ", u g n e y n S , h 8 3 3 0 8 0 0 8 5 9 6 5 1 5 9 5 1 . A d - O p 5 6 5 2 , A"
        },
        {
            "title": "Q\nC\nM",
            "content": ", h r r o y n S 2 1 . 2 - , i"
        },
        {
            "title": "T\nV",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "d o r g D v n i e g a i d o r g D v d o r g D v n i e g a i + 9 n k y e d s B"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 2 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "- t n o x c c , t - t i fi e , u fi e e , s P , e h E , l . , a e , s h ] l [ ] 6 0 1 ["
        },
        {
            "title": "A\nN",
            "content": "/ n e o h e d i a A"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t y q fi e S"
        },
        {
            "title": "A\nN",
            "content": "/ n e o h e d i a A"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t n c s fi e S"
        },
        {
            "title": "A\nN",
            "content": "/ n t t n i o t e I"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": "y q fi e - e , s h , e l e M n , s h , l , e l e M t h , s , n S L , e s r M e h E , s ] l [ ] 7 0 1 ["
        },
        {
            "title": "A\nQ\ni\nc\nS",
            "content": "] l [ ] 1 2 1 [ c ] l [ ] 9 0 1 [ U C"
        },
        {
            "title": "A\nN",
            "content": "/ n - y , o a i a o s b e n a d p"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t s o s a t , s ] l [ ] 3 0 0 1 ["
        },
        {
            "title": "A\nQ\nm\ne\nr\no\ne\nh\nT",
            "content": "d o r g D v r"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r r O"
        },
        {
            "title": "N\nE",
            "content": "2 1 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t s E e y i C , s ] l [ ] 4 0 0 1 [ n E i a 7 n k y e l a o B"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": "y i C , s ] l [ ] 1 4 4 [ e S /"
        },
        {
            "title": "A\nN",
            "content": "d o r g D v 7 n s w r l s B"
        },
        {
            "title": "N\nE",
            "content": "9 0 . 0 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ\ne\nc\nn\ne\ni\nc\nS",
            "content": "l c i , s h e C ( s ] l [ ] 5 0 0 1 ["
        },
        {
            "title": "U\nL\nM\nM",
            "content": "l p o , s e l ( s - h , ) s l c i , s c A"
        },
        {
            "title": "Q\nC\nM",
            "content": ", h r r a r d r o , h k 9 3 1 ."
        },
        {
            "title": "A\nN",
            "content": "/ n a e a i 2 u s w r l s B"
        },
        {
            "title": "H\nZ",
            "content": "5 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t - d e C , i u x n s a n c A"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x 8 4 4 /"
        },
        {
            "title": "A\nN",
            "content": "d o r g D v 8 n s u e O"
        },
        {
            "title": "N\nE",
            "content": "1 1 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t s t q fi e l l - u G s , l B , s h ] l [ ] 7 5 4 ["
        },
        {
            "title": "A\nQ\nP\nG",
            "content": "c A"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x 7 8 5 , 9 4 2 4 - d n a e a i r"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S r r O"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t r n - t q g fi fi e l m ( s h , ) . e , s ] l [ ] 4 0 6 ["
        },
        {
            "title": "A\nQ\nv\ni\nX\nA",
            "content": "r - g , s r r c ( s l t h M , t - s y , l e l ( l , ) s h ) l l c i i , s h e C ( s h o e i , s h o , s e l ( s P , ) s h o e i , s l c i l c i ( l , ) s , ) l B o e i , l , c M n t ( i e , c M i C , c M a h ( e h E , ) c h o S d , a e o ) a e ] l [ ] 7 8 7 [ v - - B t t Q ( l , ) s P ) e l e ( r M , ) o a e , e p ( n A , s h ( o d ( e e i , ) c M , ) s P ) c M n ] l [ ] 6 0 0 1 [ e c A"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x 0 5 2 7 - l ,"
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": "w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m n a g"
        },
        {
            "title": "N\nE",
            "content": "5 0 . 4 2"
        },
        {
            "title": "A\nQ",
            "content": "t s e o e P"
        },
        {
            "title": "E\nS\nM",
            "content": ", a a e - O p 9 3 2 A"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x 5 6 2 . A"
        },
        {
            "title": "Q\nC\nM",
            "content": ", u g n 9 4 4 , 9 0 1 , h r i o s d e A n - O p k 5 8 . /"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "d o r g D v n i e g a i d o r g D v + 0 8 a u - S r r O"
        },
        {
            "title": "N\nE",
            "content": "2 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t T"
        },
        {
            "title": "A\nQ\ns\ne\nn\ni\nl\np\ni\nc\ns\ni\nD\ne\nt\na\nu\nd\na\nr\nG",
            "content": "e l r , e , c M ] l [ ] 1 9 7 ["
        },
        {
            "title": "A\nN",
            "content": "/ a u - S o r n , o y e d a"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "7 0 . 5 2 0 2 o , t t e d e s a g s e"
        },
        {
            "title": "A\nQ\nV",
            "content": "- q s e - l , m d b - O , r n i , ) . , m P , c M ( l , l , s ( n S u e l r , ) . ] l [ ] 7 0 0 1 ["
        },
        {
            "title": "R\nM\nM\nB",
            "content": "4 1 a u - S r r O"
        },
        {
            "title": "N\nE",
            "content": ","
        },
        {
            "title": "H\nZ",
            "content": "2 0 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "A\nQ",
            "content": "t - o i p h m f"
        },
        {
            "title": "A\nQ",
            "content": "e I , i s a t , s P ] l [ ] 6 3 7 [ e i l a t n t w e 0 a u - u - i u - u i h m n a g"
        },
        {
            "title": "N\nE",
            "content": "4 0 . 5 2 0 2 t D u r -"
        },
        {
            "title": "R\nS\nL",
            "content": ", ) e l e , s ] l [ ] 7 4 7 [ n S - , l , s h ( y - m s T r o r a , - O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e p 0 0 5 2 , 4 - d n a e a i 0 0 0 1 a a M c s c s n m c A"
        },
        {
            "title": "N\nE",
            "content": "1 0 . 5 2"
        },
        {
            "title": "A\nQ",
            "content": "t - l , y a t r a - M , i q c h , t k t , r n n s l . ,"
        },
        {
            "title": "A\nQ",
            "content": "r c M , l e a ( l B - E , l l i u C , l - a e ( s h , ) . , s y , ) . e , s h B , r i n S i a , ) s o ( ] l [ ] 2 6 4 ["
        },
        {
            "title": "E\nL\nH",
            "content": "- - -"
        },
        {
            "title": "M\nL\nL",
            "content": ", a a , c E , c e J - O"
        },
        {
            "title": "U\no\nI",
            "content": ","
        },
        {
            "title": "Q\nC\nM",
            "content": "d e p 0 6 6 1 , 4 - d n a e a i r"
        },
        {
            "title": "A\nN",
            "content": "/ n n m c , a a fi n S r e r e r"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "6 0 . 5 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ", t t s"
        },
        {
            "title": "A\nN\nR",
            "content": ", t t e P , e e , s h , n s . , t t l l c c r , e s r M ] l [ ] 3 4 4 ["
        },
        {
            "title": "E\nF\nS",
            "content": "e t n u n C"
        },
        {
            "title": "E\nS\nM",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": ", - O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": ", u g n l / T , n a r 0 0 0 8 , 4 - w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m n a g"
        },
        {
            "title": "N\nE",
            "content": "8 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t - h , u o , i q ( T ) . ,"
        },
        {
            "title": "A\nQ\nc\nfi\ni\nt\nn\ne\ni\nc\ns",
            "content": ", i e c l , s , s h ] l [ ] 2 9 7 [ E 64 p i p f n o I"
        },
        {
            "title": "V\nE\nL\nB\nA\nT",
            "content": "s t o u E T e i n a n - A o s a s a n i P a n a n r e g e l e y a n o fi e t t ,"
        },
        {
            "title": "E\nG\nU\nO\nR",
            "content": ","
        },
        {
            "title": "U\nE\nL\nB",
            "content": ","
        },
        {
            "title": "1\nF",
            "content": ", , r W - m o a , a / T n - O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "t x , u G , u g n 3 0 2 0 7 , , . 5 3 - , 4 - n ,"
        },
        {
            "title": "A\nM\na\nL\nL",
            "content": ", 3 a e r D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S - i u - u i h m - c s n m c , t - I , a a fi e , r s a g s e n a g"
        },
        {
            "title": "N\nE",
            "content": "6 0 . 4 2 0 2 - fi a ,"
        },
        {
            "title": "A\nQ",
            "content": "t ,"
        },
        {
            "title": "A\nQ",
            "content": "e a i ,"
        },
        {
            "title": "A\nQ",
            "content": "k t s r M , l , s , s h n s e , t . , i q ,"
        },
        {
            "title": "C\nA\nP\nU",
            "content": "I ,"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": "e i ] l [ ] 2 4 4 [ E K - C 4 - G ,"
        },
        {
            "title": "R\nS",
            "content": ","
        },
        {
            "title": "R\nE\nV",
            "content": ", c E d e - O p k 2 0 /"
        },
        {
            "title": "A\nN",
            "content": "d o r g D v 9 n , r e r e a e A s d t e n a t I"
        },
        {
            "title": "N\nE",
            "content": "0 1 . 4 2"
        },
        {
            "title": "A\nQ\nV",
            "content": ","
        },
        {
            "title": "G\nE\nE",
            "content": ", d t o , i s"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", a p o M . ,"
        },
        {
            "title": "U\nM",
            "content": "I ,"
        },
        {
            "title": "G\nC\nE",
            "content": "- c o t o , t o o - n a f l h g , s - e i g & l y , e e ] l [ ] 3 8 [ e e c c S"
        },
        {
            "title": "M\nE",
            "content": ", - O ,"
        },
        {
            "title": "Q\nC\nM",
            "content": "d e u g n a c , h 2 6 0 8 , 4 - ,"
        },
        {
            "title": "T\nP\nG\nt\na\nh\nC",
            "content": "w e a D"
        },
        {
            "title": "A\nN",
            "content": "/ a u - S , r e r e a m c e a i x n a t I"
        },
        {
            "title": "H\nZ",
            "content": ","
        },
        {
            "title": "N\nE",
            "content": "9 0 . 3 2"
        },
        {
            "title": "A\nQ",
            "content": "t - ,"
        },
        {
            "title": "S\nE\nL\nI\nM\nS",
            "content": ", t t , b T . , i q ,"
        },
        {
            "title": "C\nA\nP",
            "content": "s y , ) s h - ( s h , ) l - ( o B , ) s - ( ) a e - ( a e ] l [ ] 8 8 7 [ E 65 . i w o t c d ] l [ . d g n g c fi e s y m : I"
        },
        {
            "title": "V\nE\nL\nB\nA\nT",
            "content": "e o - O e r c t d e e s e a . 1 1 2 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 0 2 1 e l n i d S ] l [ ] 0 3 [ t a o p - e l M m fi e S . 8 0 3 2 0 2 . 1 1 3 2 0 2 . 1 0 4 2 0 2 . . . 9 0 4 2 0 2 3 0 5 2 0 2 8 0 5 2 0 2 0 0 3 - r I , 6 - r I"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . 0 1 3 2 0 2 . 4 0 4 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . 9 0 9 1 0 2 . . . 5 0 2 2 0 2 0 1 2 2 0 2 3 0 3 2 0 2 . 4 0 3 2 0 2 . 5 0 3 2 0 2 . 5 0 3 2 0 2 . . . . 7 0 3 2 0 2 0 1 3 2 0 2 0 1 3 2 0 2 2 0 4 2 0 2 . 6 0 4 2 0 2 . 6 0 4 2 0 2 . 1 1 4 2 0 2 . 2 1 4 2 0 2 . 7 0 5 2 0 2 . 8 0 5 2 0 2 . 7 0 5 2 0 2 . 8 0 5 2 0 2 o h g u o d E - E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . 6 0 4 2"
        },
        {
            "title": "A\nN",
            "content": "/ . . . . 0 1 4 2 0 2 0 1 4 2 0 2 0 1 4 2 0 2 2 1 4 2 0 2 . 2 1 4 2 0 2 . 2 0 5 2 0 2 . 6 0 5 2 0 2 . 7 0 5 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . 6 0 2 2 0 2 . 0 1 2 2 0 2 . 3 0 3 2 0 2 . 3 0 3 2 0 2 . 4 0 3 2 0 2 . 4 0 3 2 0 2 . 4 0 3 2 0 2 . . . . 4 0 3 2 0 2 5 0 3 2 0 2 5 0 3 2 0 2 5 0 3 2 0 2 . 5 0 3 2 0 2 . 8 0 5 2 0 2 , o n i 3 - a - s L - 5 . 2 Q N C"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . 5 0 3 2"
        },
        {
            "title": "A\nN",
            "content": "/ . 5 0 3 2 0 . 6 0 3 2 0 2 . 6 0 3 2"
        },
        {
            "title": "N\nA\nG\nQ\nV",
            "content": "-"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 8 - 3 Q , - 2 2 5 3 2 - 3 Q - G ,"
        },
        {
            "title": "M\nL\nG\nt\na\nh\nC",
            "content": ", c ,"
        },
        {
            "title": "A\nM\na\nL\nL",
            "content": "X - 3 t C"
        },
        {
            "title": "A\nN",
            "content": "/ 1 . 3 - L 2 - L L"
        },
        {
            "title": "T\nO\nc\ns",
            "content": "1 7 - c , 7 - L - d , u - 5 . 3 - l s - B"
        },
        {
            "title": "T\nR\nE\nB\nt\na",
            "content": "M E a B R"
        },
        {
            "title": "A\nN",
            "content": "/ 2 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 0 7 2 - L 6 1 - u - 5 . 3 , 4 - G"
        },
        {
            "title": "T\nR\nE\nB",
            "content": "B 8 - 3 - L 2 7 - 2 Q m - 5 r - 5 2 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 2 1 5 - 4 3 - m - m 2 r I 2 - L L"
        },
        {
            "title": "A\nM\na\nL\nL",
            "content": "4 - 2 - L 7 - 2 Q - 3 1 - L - Z , 7 - h B 6 - t C"
        },
        {
            "title": "A\nM\na\nL\nL",
            "content": "B 6 - t 2 - G"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nM\na\nL\nL",
            "content": "2 P"
        },
        {
            "title": "M\nL\na\nP",
            "content": "3 - 3 - G"
        },
        {
            "title": "A\nN",
            "content": "/ 1 - r P"
        },
        {
            "title": "A\nF\nO",
            "content": "B 7 - L B"
        },
        {
            "title": "T\nR\nE\nB",
            "content": ". . 5 0 4 2 0 2 6 0 5 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ r I - 5 . 0 - 5 . 2 Q 2 3 6 . 5 2 . 8 1 - 6 2 / 6 7 0 7 8 / 8 2 1 4 - e e i c c n S e S n S e l n a e r G e G r G ] l [ ] 2 5 4 [ e i ] l [ ] 7 4 [ 1 - t ] l [ ] l [ ] 8 4 4 [ I"
        },
        {
            "title": "N\nW\nR\nA\nD",
            "content": "] 8 0 0 1 [ O ] l [ ] 1 5 4 [ ] l [ ] 1 4 ["
        },
        {
            "title": "M\nF\nD\ni\nc\nS",
            "content": "B 6 . 0 / 2 . 0 / 2 0 . 0 5 . 0 7 / 3 1 3 1 / o u a e f a a i p s ] l [ ] 4 6 4 [ i P ] l [ ] 9 0 0 1 [ 3 i P e g s a M ] l [ ] 6 6 4 [ i ] l [ ] 5 8 4 [ h s y P"
        },
        {
            "title": "A\nN",
            "content": "/ 3 1 4 3 7 7 7"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 0"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ s h a h a h , ) s , ) s a e r c m ( ( s m c g , ) s a e ( s t , s t e e e e y r , s h , s h B e y y r l m , s , s h h ] l [ ] l [ ] 0 1 0 1 [ M - ] 8 6 4 [ ] 9 6 4 [ M r I"
        },
        {
            "title": "M\nF\nD\nm\ne\nh\nC",
            "content": "] l [ ] 0 2 ["
        },
        {
            "title": "M\nL\nL\nm\ne\nh\nC",
            "content": "] l [ ] 0 0 2 ["
        },
        {
            "title": "M\nL\nL\nM\nm\ne\nh\nC",
            "content": "] 0 7 4 [ D 3 C ] 1 1 0 1 [ e c c a t n S i a n S i a e s r e i l e c c a t n S i a e s r a n S i a e s r e i l e c c a t M M M M ] l [ ] l [ ] 5 7 4 [ ] 4 7 4 [ d - B r o a o e R ] l [ ] 1 7 4 [ B - I ] l [ ] 0 8 4 [ l ] 2 1 0 1 [ r M ] l [ ] l [ ] 9 7 4 [ ] 3 1 0 1 [ ] 2 7 4 [ B p E M - r o m C ] l [ ] 1 8 4 [ ] 7 7 4 [ l e f r x ] l [ ] 4 1 0 1 ["
        },
        {
            "title": "M\nL\nL\na\nt\ns\ny\nr\nC",
            "content": "] l [ ] 5 1 0 1 [ T M"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 0 0 2 0 7 7 2 7 7 3 n S i a e s r e i l e c c a t n S i a e s r a n S i a e s r M M M ] l [ ] 4 8 4 ["
        },
        {
            "title": "M\nL\nL\ns\ni\ns\ne\nh\nt\nn\ny\nS\nl\na\nt\ns\ny\nr\nC",
            "content": "] 8 1 0 1 [ P - ] l [ ] 7 1 0 1 [ ] l [ ] 2 8 4 [ ] l [ ] l [ ] 6 7 4 [ ] 3 8 4 [ 2 T"
        },
        {
            "title": "M\nL\nL\na\nt\ns\ny\nr\nC",
            "content": "e l - 5 m - 2 Q ] 3 8 4 [ 2 ] 3 8 4 [ 2 R"
        },
        {
            "title": "A\nN",
            "content": "/ e s r M ] l [ ] 6 1 0 1 [ t C n S i a t e C"
        },
        {
            "title": "A\nN",
            "content": "/ 8 / 5 3 / 0 5 1 / 0 5 6 / 3 / 5 1 1 5 1 / 4 6 7 / 7 . 3 3 / 3 / 0 0 3 / 9 . 8 / 9 . 3 / 5 4 3 0 / 5 3 1 / / 4 . 7 4 3 6 7 6 3 1"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ l l n , n S i d e i s e s e s e s e s e s e S c c c M l d n c M l d n i t P a l s o r a l e h e a l e h e s o - u c M l d n c M l d n a l H a l e h e a l ] l [ ] 4 8 5 [ ] l [ ] 0 2 5 ["
        },
        {
            "title": "M\nL\nG\nr\no\nt\nc\no\nD",
            "content": "a l M ] l [ ] 0 2 0 1 ["
        },
        {
            "title": "M\nL\nG\nh\np\nO",
            "content": "] l [ ] 8 9 4 [ 2 o ] l [ ] 4 3 9 [ o ] l [ ] 0 9 4 [ 2 - ] l [ ] 1 2 0 1 [ L - ] l [ ] 2 2 0 1 [ ] l [ ] 1 3 [ ] l [ ] 2 1 5 [ c a - M 2 P - ] l [ ] 3 2 0 1 [ n r G ] l [ ] 6 1 [ r t 3 / 7 n S i d r t H ] l [ ] 4 2 5 [ o u 5 . 2 Q 2 3 / 7 n S c M e h e ] l [ ] 9 1 0 1 [ n i e i f . . . . 0 1 3 2 0 2 0 1 3 2 0 2 0 1 3 2 0 2 1 1 3 2 0 2 6 1 / - I C"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ . 1 1 3 2"
        },
        {
            "title": "A\nN",
            "content": "/ . 1 1 3 2 0 2 . 1 1 3 2 0 . 1 1 3 2 0 2 . 1 1 3 2 0 2 . 1 1 3 2 0 . . 1 0 4 2 0 2 1 0 4 2 0 2 I"
        },
        {
            "title": "O\nN\nD\nD\nA\nR",
            "content": "- I"
        },
        {
            "title": "O\nN\nD\nD\nA\nR",
            "content": "- o G e n E"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ - 2 h B - , B 7 - 2 h B 7 - h B 2 - 3 1 - Z 2 - L 3 . 1 - e - 7 - Q B 3 1 - 2 - L B 7 - c 5 . 1 - 7 - c 2 - L 3 - 2 - . 6 0 3 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "T\nR\nE\nB",
            "content": "M 0 9 1 . 6 0 3 2 0 2 . 7 0 3 2 0 2 . 7 0 3 2 0 2 . 7 0 3 2 0 2 . . . 8 0 3 2 0 2 8 0 3 2 0 2 8 0 3 2 0 2 . 8 0 3 2 0 2 . 8 0 3 2 0 2 . 9 0 3 2 0 2 . . 9 0 3 2 0 9 0 3 2 0 2 i fl O"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ D 3 2 3 / - - C - 1 3 1 - L - Z 3 1 - L M 2 - L 3 1 - 2 l 7 - L 2 - G 6 - t C"
        },
        {
            "title": "T\nP\nG",
            "content": "o m fl O 3 1 - h B 3 3 - L 7 6 9 3 1 3 3 3 1 7 3 1 7 4 2 1 . 9 0 3 2"
        },
        {
            "title": "A\nN",
            "content": "/ 2 - L 3 1 2 8 1 / 3 / 3 3 6 3 3 / 0 1 1 , e o N , n S i d c - u , l l d s e s e l d n c M s o - u , l l d s e l d n a l , e o N n S i d s e s e s e s e s e s e S c c c c c M l d n c M l d n l d n c M l d n c M e h e a l H l l r t H m P c a a e e h e e h e a l e h e a l e h e a l H a l l n c e 3 . 1 3 1 7 n S n S i d a e a n S c M e h e a l s o r a l - P c c , l l d a e , l B c M e h e , e o N 0 7 / 7 7 7 3 1 4 2 1 / B"
        },
        {
            "title": "A\nN",
            "content": "/ n S i d r t H , e o N l B r G , l l d a e s e s e S c c c c c M l d n c M l d n e h e a l e h e a l . 2 0 4 2"
        },
        {
            "title": "A\nN",
            "content": "/ 1 . 0 - r I - 7 - t B 7 C r c M , n S i d r t H . 2 0 4 2 0 2 . 2 0 4 2 0 . 3 0 4 2 0 2 . 3 0 4 2 0 2 . . 3 0 4 2 0 2 4 0 4 2 0 2 . 4 0 4 2 0 2 . 4 0 4 2 0 2 . 5 0 4 2 0 2 . . 5 0 4 2 0 2 5 0 4 2 0 2 . 5 0 4 2 0 2 . 5 0 4 2 0 2 . 6 0 4 2 0 2 . 6 0 4 2 0 2 . . . 8 0 4 2 0 2 0 1 4 2 0 0 1 4 2 0 2 . 1 1 4 2 0 2 . 1 1 4 2 0 2 . 2 1 4 2 0 2 . . 1 0 5 2 0 2 1 0 5 2 0 2 ) . 2 0 5 2 0 2 . 2 0 5 2 0 2 . 5 0 5 2 0 . 6 0 5 2 0 2 . 6 0 5 2 0 2 . 7 0 5 2 0 d E , ) s ( ) o l ( d e s / 4 1 - - C d E t t I ) I"
        },
        {
            "title": "R\nM",
            "content": "f ("
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN\nR",
            "content": "( o e u s"
        },
        {
            "title": "P\nI\nL\ng\ni\nS",
            "content": ", ) s ( 4 1 / - - C ) ( - R d E ) s ( o e I Q ) s ( d E m ) t ( p ) ( d E )"
        },
        {
            "title": "A\nN\nD",
            "content": "( 2 m s T t c ) s ( o e I Q ) I"
        },
        {
            "title": "R\nM",
            "content": "f ( o ) t ( 2 - 4 1 / - i C ) t ( S"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ P"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ i 3 . 1 7 L 7 - c 2 - L a c G"
        },
        {
            "title": "A\nM\na\nL\nL",
            "content": "B 7 - 2 Q 2 - , 5 . 1 - L 7 - L 3 - L L"
        },
        {
            "title": "M\nL\nn\nr\ne\nt\nn\nI",
            "content": "5 . 2 Q , 1 . 3 - L 8 3 - L 5 . 2 r I 7 . 1 / 0 0 5 / 4 5 4 3 7 / 2 7 / 0 / 8 / 0 8 /"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ / 7 7 8 7 7 7 7 m - u , l l d s o r , l l d e i e i a e a c M s e s e l d n c M s e S c M y o l n 7 - c 5 . 2 Q"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ , m P , m o , l l B 7 - L B 7 - t 3 L 2 - L w P 5 . i B 7 / 6 / / 8 . 1 / 8 7 7 5 ."
        },
        {
            "title": "A\nN",
            "content": "/ n S i d e i e i a e a c M e h e a l e i r e h e i t P i t y o c c u n n s e o g r c M e o N c a r t H m P m o r c M c a a e y o r c M c a e h e i t e i r e i r , m P r a M"
        },
        {
            "title": "A\nM\na\nL\nL",
            "content": ", t , e 7 - c V - 2 Q 7 - c e Q"
        },
        {
            "title": "A\nN",
            "content": "/ 2 3 /"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ m - u i - u , l , m o , l s e S c M l d r t H u o e a u o c c l d n a l , e o N n S n S i d l d n a l e h e . 2 0 5 2"
        },
        {
            "title": "A\nN",
            "content": "/ 7 8 t B 7 . 6 4 , m o , l B e a u o ] l [ ] 8 3 5 [ 2 l - l a ] l [ ] 9 2 5 [ j h ] l [ ] 0 0 9 [ ] l [ ] 9 3 5 [ ] l ["
        },
        {
            "title": "M\nL\nL\nd\ne\nM\nC\nS\nI\nD",
            "content": "- ] 1 3 0 1 [ y g a - ] l [ ] 4 3 0 1 [ ] l [ ] 7 5 5 [ d A L - ] l [ ] 3 3 0 1 [ ] l [ ] 2 3 0 1 ["
        },
        {
            "title": "M\nL\nL\nP\nC",
            "content": "] l [ ] 8 2 0 1 [ c a ] l [ ] l [ ] 9 2 0 1 [ h o ] 0 3 0 1 [ A ] l [ ] 5 3 0 1 ["
        },
        {
            "title": "M\nL\nL\nd\ne\nr\ni\np\ns\nn\ni\no\ni\nB",
            "content": "] l [ ] 7 3 5 [ ] 7 3 0 1 [ t t t A L - ] l [ ] 6 2 5 [ - i ] l [ ] 6 3 0 1 [ - i ] l [ ] l [ ] l [ ] 4 2 0 1 [ d i ] 5 2 0 1 [ l n ] 6 2 0 1 [ - G ] l [ ] 7 2 0 1 [ T ] l [ ] 8 3 0 1 [ L - a ] l [ ] l [ ] l [ ] l [ ] l [ ] 0 6 5 [ - e ] 3 5 5 [ G - ] 0 5 5 [ a"
        },
        {
            "title": "M\na\nL\nL\nP",
            "content": "] l [ ] 8 3 [ ] 9 3 0 1 [ ] 0 4 0 1 [ 1 - M 2 - M r e ] l [ ] 7 1 5 [ t i"
        },
        {
            "title": "M\no\ni\nB",
            "content": "] l [ ] 1 4 0 1 [ 3 L - M ] l [ ] 2 4 0 1 ["
        },
        {
            "title": "A\nM\na\nL\nL\no\nr\nP",
            "content": "] l [ ] 4 4 0 1 [ m - ] l [ ] l [ ] 3 4 0 1 ["
        },
        {
            "title": "M\nL\nL\nt\no\nr\nP",
            "content": "] 2 5 5 [ n B ] l [ ] 3 2 5 [ a ] l [ ] 5 5 5 [ B ] l [ ] 6 4 5 ["
        },
        {
            "title": "M\nL\nL\nd\ne\ne\nS",
            "content": "] l [ ] 5 4 0 1 [ 3 f l ] l [ ] 0 4 5 [ ] l [ ] l [ ] 7 4 0 1 [ 3 r ] 1 1 5 [ r c M s - o u ] l [ ] 6 3 5 [ - A L ] l [ ] 6 4 0 1 [ e ] l [ ] 6 ["
        },
        {
            "title": "M\nL\nL\ng\nu\nr\nD",
            "content": "] l [ ] 3 4 5 [ ] l [ ] 0 4 [ ] l [ ] l [ ] l [ ] l [ ] 4 5 5 ["
        },
        {
            "title": "M\nL\no\nr\nu\ne\nN",
            "content": "] 0 4 6 [ A ] 1 5 5 [ o - G L ] 1 4 5 [ - I"
        },
        {
            "title": "A\nM\nG",
            "content": "1 - o u ] l [ ] 9 0 5 [ o ] 9 5 5 [ M U ] l [ ] 9 4 0 1 [ ] l [ ] 8 4 0 1 [ ] l [ - 1 V e e p ] 0 1 5 [ a ] l [ ] 8 5 5 ["
        },
        {
            "title": "M\nL\nL\nd\nn\ni\nM",
            "content": "] l [ ] l [ ] 0 5 0 1 [ g ] 1 5 0 1 [ d ] l [ ] 3 4 ["
        },
        {
            "title": "M\nL\ne\nr\nu\nt\na\nN",
            "content": "B 3 1 / 7 n S i d r t H ] l [ ] 2 4 [ - o u a e d i C 66 r - O e r c t d . 7 0 5 2 0 ) s ( o e I g S"
        },
        {
            "title": "M\nL\nL\ne\ns\na\nB",
            "content": "3 e . 9 0 3 2"
        },
        {
            "title": "A\nN",
            "content": "/ 2 - l a o r r u n I"
        },
        {
            "title": "V\nE\nL\nB\nA\nT",
            "content": "s e a 7 2 / 4 7 n S c M e h e ] l [ ] 2 4 5 [ e M i D d i D fi e y o A ] l [ ] 1 6 5 [ 7 - 2 - L t m r . 9 0 4 2 0 2 . 9 0 4 2 0 . 4 0 5 2 0 2 . 4 0 5 2 0 2 . 5 0 5 2 0 ) s ( / 4 1 - - C"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ B 7 - 2 - L L 7 - 2 - L L 8 - 1 . 3 - l L 5 . 1 L L 0 7 - 1 . 3 - l B 0 7 8 8 7 0 7 n s y o A n s o t m r ] l [ ] 6 6 5 [ ] l [ ] l [ ] 3 6 5 [ ] l [ ] 4 2 7 [ ] 4 2 7 [ 0 7 - 2 - L t 8 - 3 - L t ] l [ ] 2 6 5 [ 7 - L t 0 7 - 1 . 3 - L - S s B 8 - 1 . 3 - L - S s . 3 0 3 2"
        },
        {
            "title": "A\nN",
            "content": "/ Q , L 7 - S m , h h , h i , h r . 8 0 3 2 0 2 . 1 1 3 2 0 2 . 1 0 4 2 0 2 . 0 1 4 2 0 2 . 1 1 4 2 0 2 . 2 1 4 2 0 2 . 1 0 5 2 0 2 . 3 0 5 2 0 2 . 3 0 5 2 0 2 . 4 0 5 2 0 2 . 5 0 5 2"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/"
        },
        {
            "title": "A\nN",
            "content": "/ 2 7 - 5 . 2 Q , 0 7 - 1 . 3 l 4 - , 2 7 - - 2 Q 5 . 1 - c m L"
        },
        {
            "title": "A\nN",
            "content": "/ L - d 2 - L i - 3 - 5 . 2 - I h j"
        },
        {
            "title": "A\nV\ng\nn\no\nL",
            "content": "B 7 / 4 / 2 / 1 7 7 7 7 3 4 7 0 7 7 7 s e e , h h e p t r s i e p t , s e e i S m , s e e i S m g n t R , s e e i S m n e o e i C , h h , h h L h h L , s e e i ] l [ ] 3 6 9 [ e k ] l [ ] l [ ] 8 8 5 [ ] 7 6 5 [ 2 h G ] l [ ] l [ ] 8 7 5 [ C ] 1 7 5 [ r a"
        },
        {
            "title": "M\nh\nt\nr\na\nE",
            "content": "] l [ ] l [ ] 4 7 5 [ ] 2 7 5 [ D a x e ] l [ ] 3 7 5 [ ] l [ ] 2 5 0 1 [ o 8 - L G ] l [ ] l [ ] 8 6 5 [ ] 0 7 5 [ s l t e i C ] l [ ] 9 6 5 [ n O e h E"
        },
        {
            "title": "REFERENCES",
            "content": "[1] H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu, P. Chandak, S. Liu, P. Van Katwyk, A. Deac et al., Scientific discovery in the age of artificial intelligence, Nature, vol. 620, no. 7972, pp. 4760, 2023. [2] J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim, survey on large language models for code generation, arXiv preprint arXiv:2406.00515, 2024. [3] X. Zhang, L. Wang, J. Helwig, Y. Luo, C. Fu, Y. Xie, M. Liu, Y. Lin, Z. Xu, K. Yan et al., Artificial intelligence for science in quantum, atomistic, and continuum systems, Foundations and Trends in Machine Learning, vol. 18, no. 4, pp. 385912, 2025. [4] X. Zhou, J. He, W. Zhou, H. Chen, Z. Tang, H. Zhao, X. Tong, G. Li, Y. Chen, J. Zhou et al., survey of LLM data, arXiv preprint arXiv:2505.18458, 2025. [5] Y. Liu, J. Cao, C. Liu, K. Ding, and L. Jin, Datasets for large language models: comprehensive survey, arXiv preprint arXiv:2402.18041, 2024. [6] X. Liu, Y. Guo, H. Li, J. Liu, S. Huang, B. Ke, and J. Lv, Drugllm: Open large language model for few-shot molecule generation, arXiv preprint arXiv:2405.06690, 2024. [7] Y. Xiao, W. Zhao, J. Zhang, Y. Jin, H. Zhang, Z. Ren, R. Sun, H. Wang, G. Wan, P. Lu et al., Protein large language models: comprehensive survey, arXiv preprint arXiv:2502.17504, 2025. [8] X. Fang, W. Xu, F. A. Tan, J. Zhang, Z. Hu, Y. Qi, S. Nickleach, D. Socolinsky, S. Sengamedu, and C. Faloutsos, Large language models (llms) on tabular data: Prediction, generation, and understandinga survey, arXiv preprint arXiv:2402.17944, 2024. [9] T. Ridnik, D. Kredo, and I. Friedman, Code generation with alphacodium: From prompt engineering to flow engineering, arXiv preprint arXiv:2401.08500, 2024. [10] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy, and H. Wang, Large language models for software engineering: systematic literature review, ACM Transactions on Software Engineering and Methodology, vol. 33, no. 8, pp. 179, 2024. [11] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, Chatlaw: Open-source legal large language model with integrated external knowledge bases, CoRR, 2023. [12] P. Colombo, T. P. Pires, M. Boudiaf, D. Culver, R. Melo, C. Corro, A. F. Martins, F. Esposito, V. L. Raposo, S. Morgado et al., Saullm7b: pioneering large language model for law, arXiv preprint arXiv:2403.03883, 2024. [13] G. Tom, S. P. Schmid, S. G. Baird, Y. Cao, K. Darvish, H. Hao, S. Lo, S. Pablo-Garcıa, E. M. Rajaonson, M. Skreta et al., Selfdriving laboratories for chemistry and materials science, Chemical Reviews, vol. 124, no. 16, pp. 96339732, 2024. [14] Y. Zimmermann, A. Bazgir, A. Al-Feghali, M. Ansari, J. Bocarsly, L. C. Brinson, Y. Chiang, D. Circi, M.-H. Chiu, N. Daelman et al., 34 examples of llm applications in materials science and chemistry: Towards automation, assistants, agents, and accelerated scientific discovery, arXiv preprint arXiv:2505.03049, 2025. [15] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., Large language models encode clinical knowledge, Nature, vol. 620, no. 7972, pp. 172180, 2023. [16] C. Peng, X. Yang, A. Chen, K. E. Smith, N. PourNejatian, A. B. Costa, C. Martin, M. G. Flores, Y. Zhang, T. Magoc et al., study of generative large language model for medical research and healthcare, NPJ digital medicine, vol. 6, no. 1, p. 210, 2023. [17] M. Y. Lu, B. Chen, D. F. Williamson, R. J. Chen, M. Zhao, A. K. Chow, K. Ikemura, A. Kim, D. Pouli, A. Patel et al., multimodal generative ai copilot for human pathology, Nature, vol. 634, no. 8033, pp. 466473, 2024. [18] K. Huang, S. Zhang, H. Wang, Y. Qu, Y. Lu, Y. Roohani, R. Li, L. Qiu, G. Li, J. Zhang et al., Biomni: general-purpose biomedical ai agent, biorxiv, pp. 202505, 2025. [19] J. Ahn, R. Verma, R. Lou, D. Liu, R. Zhang, and W. Yin, Large language models for mathematical reasoning: Progresses and challenges, arXiv preprint arXiv:2402.00157, 2024. [20] D. Zhang, W. Liu, Q. Tan, J. Chen, X. Yue, W. Ouyang, D. Zhou, S. Zhang, M. Su, H. Zhong, and Y. Li, Chemllm: chemical large language model, arXiv preprint, 2024. [21] Q. Zhang, K. Ding, T. Lv, X. Wang, Q. Yin, Y. Zhang, J. Yu, Y. Wang, X. Li, Z. Xiang et al., Scientific large language models: survey on biological & chemical domains, ACM Computing Surveys, vol. 57, no. 6, pp. 138, 2025. [22] Z. Lin, C. Deng, L. Zhou, T. Zhang, Y. Xu, Y. Xu, Z. He, Y. Shi, B. Dai, Y. Song et al., Geogalactica: scientific large language model in geoscience, arXiv preprint arXiv:2401.00434, 2023. [23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 2019, pp. 41714186. [24] I. Beltagy, K. Lo, and A. Cohan, SciBERT: pretrained language model for scientific text, Sep. 2019. [25] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, Biobert: pre-trained biomedical language representation text mining, Bioinformatics, vol. 36, model no. 4, p. 12341240, Sep. 2019. [Online]. Available: http: //dx.doi.org/10.1093/bioinformatics/btz682 for biomedical [26] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon, Domain-specific language model pretraining for biomedical natural language processing, ACM Transactions on Computing for Healthcare (HEALTH), vol. 3, no. 1, pp. 123, 2021. [27] L. Floridi and M. Chiriatti, Gpt-3: Its nature, scope, limits, and consequences, Minds and machines, vol. 30, no. 4, pp. 681694, 2020. [28] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre, Training compute-optimal large language models, in Proceedings of the 36th International Conference on Neural Information Processing Systems, 2022. [29] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361, 2020. [30] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, Galactica: large language model for science, arXiv preprint arXiv:2211.09085, 2022. [31] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, M. Schaekermann, A. Wang, M. Amin, S. Lachgar, P. Mansfield, S. Prakash, B. Green, E. Dominowska, B. A. Arcas, N. Tomasev, Y. Liu, R. Wong, C. Semturs, S. S. Mahdavi, J. Barral, D. Webster, G. S. Corrado, Y. Matias, S. Azizi, A. Karthikesalingam, and V. Natarajan, Towards expert-level medical question answering with large language models, 2023. [Online]. Available: https://arxiv.org/abs/2305.09617 [32] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., Training language models to follow instructions with human feedback, Advances in neural information processing systems, vol. 35, pp. 27 73027 744, 2022. [33] OpenAI, Introducing chatgpt, https://openai.com/blog/chatgpt, 2022, accessed: 2025-08-11. [34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [35] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [36] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang, D. Rojas, G. Feng, H. Zhao et al., Chatglm: family of large language models from glm-130b to glm-4 all tools, arXiv preprint arXiv:2406.12793, 2024. [37] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, Mistral 7b, https://arxiv.org/abs/2310. 06825, 2023. [38] Z. Chen, A. H. Cano, A. Romanou, A. Bonnet, K. Matoba, F. Salvi, M. Pagliardini, S. Fan, A. Kopf, A. Mohtashami et al., Meditron70b: Scaling medical pretraining for large language models, arXiv preprint arXiv:2311.16079, 2023. [39] M. Huo, H. Guo, X. Cheng, D. Singh, H. Rahmani, S. Li, P. Gerlof, T. Ideker, D. A. Grotjahn, E. Villa et al., Multi-modal large language model enables protein function prediction, bioRxiv, pp. 202408, 2024. 68 [40] W. Liang, LLaMA-Gene: general-purpose gene task large language model based on instruction fine-tuning, arXiv preprint arXiv:2412.00471, 2024. [41] D. Zhang, Z. Hu, S. Zhoubian, Z. Du, K. Yang, Z. Wang, Y. Yue, Y. Dong, and J. Tang, Sciglm: Training scientific language models with self-reflective instruction annotation and tuning, arXiv preprint arXiv:2401.07950, 2024. [42] J. Chen, X. Wang, K. Ji, A. Gao, F. Jiang, S. Chen, H. Zhang, D. Song, W. Xie, C. Kong, J. Li, X. Wan, H. Li, and B. Wang, Huatuogpt-ii, one-stage training for medical adaption of llms, Proceedings of COLM (arXiv:2311.09774v2), 2024. [Online]. Available: https://arxiv.org/abs/2311.09774 [43] Y. Xia, P. Jin, S. Xie, L. He, C. Cao, R. Luo, G. Liu, Y. Wang, Z. Liu, Y.-J. Chen et al., Nature language model: Deciphering the language of nature for scientific discovery, arXiv preprint arXiv:2502.07527, 2025. [44] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes, Autonomous chemical research with large language models, Nature, vol. 624, no. 7992, pp. 570578, 2023. [45] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin et al., Metagpt: Meta programming for multi-agent collaborative framework, in The Twelfth International Conference on Learning Representations, 2023. [46] J. Gottweis, W.-H. Weng, A. Daryin, T. Tu, A. Palepu, P. Sirkovic, A. Myaskovsky, F. Weissenberger, K. Rong, R. Tanno et al., Towards an ai co-scientist, arXiv preprint arXiv:2502.18864, 2025. [47] L. Bai, Z. Cai, M. Cao, W. Cao, C. Chen, H. Chen, K. Chen, P. Chen, Y. Chen, Y. Chen, Y. Cheng, Y. Cheng, P. Chu, T. Chu, E. Cui, G. Cui, L. Cui, Z. Cui, N. Deng, N. Ding, N. Dong, P. Dong, S. Dou, S. Du, H. Duan, C. Fan, B. Gao, C. Gao, J. Gao, S. Gao, Y. Gao, Z. Gao, J. Ge, Q. Ge, L. Gu, Y. Gu, A. Guo, Q. Guo, X. Guo, C. He, J. He, Y. Hong, S. Hou, C. Hu, H. Hu, J. Hu, M. Hu, Z. Hua, H. Huang, J. Huang, X. Huang, Z. Huang, Z. Jiang, L. Kong, L. Li, P. Li, P. Li, S. Li, T. Li, W. Li, Y. Li, D. Lin, J. Lin, T. Lin, Z. Lin, H. Liu, J. Liu, J. Liu, J. Liu, K. Liu, K. Liu, K. Liu, S. Liu, S. Liu, W. Liu, X. Liu, Y. Liu, Z. Liu, Y. Lu, H. Lv, H. Lv, H. Lv, Q. Lv, Y. Lv, C. Lyu, C. Ma, J. Ma, R. Ma, R. Ma, R. Ma, X. Ma, Y. Ma, Z. Ma, S. Mi, J. Ning, W. Ning, X. Pang, J. Peng, R. Peng, Y. Qiao, J. Qiu, X. Qu, Y. Qu, Y. Ren, F. Shang, W. Shao, J. Shen, S. Shen, C. Song, D. Song, D. Song, C. Su, W. Su, W. Sun, Y. Sun, Q. Tan, C. Tang, H. Tang, K. Tang, S. Tang, J. Tong, A. Wang, B. Wang, D. Wang, L. Wang, R. Wang, W. Wang, W. Wang, Y. Wang, Z. Wang, L.-I. Wu, W. Wu, Y. Wu, Z. Wu, L. Xiao, S. Xing, C. Xu, H. Xu, J. Xu, R. Xu, W. Xu, G. Yang, Y. Yang, H. Ye, J. Ye, S. Ye, J. Yu, J. Yu, J. Yu, F. Yuan, B. Zhang, C. Zhang, C. Zhang, H. Zhang, J. Zhang, Q. Zhang, Q. Zhang, S. Zhang, T. Zhang, W. Zhang, W. Zhang, Y. Zhang, Z. Zhang, H. Zhao, Q. Zhao, X. Zhao, X. Zhao, B. Zhou, D. Zhou, P. Zhou, Y. Zhou, Y. Zhou, D. Zhu, L. Zhu, and Y. Zou, Intern-s1: scientific multimodal foundation model, arXiv preprint arXiv:2508.15763, 2025. [48] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, in International Conference on Learning Representations, 2017. [Online]. Available: https: //openreview.net/forum?id=B1ckMDqlg [49] Y. Yamada, R. T. Lange, C. Lu, S. Hu, C. Lu, J. Foerster, J. Clune, and D. Ha, The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search, arXiv preprint arXiv:2504.08066, 2025. [50] A. Ghafarollahi and M. J. Buehler, Sciagents: automating scientific discovery through bioinspired multi-agent intelligent graph reasoning, Advanced Materials, vol. 37, no. 22, p. 2413523, 2025. [51] A. E. Ghareeb, B. Chang, L. Mitchener, A. Yiu, C. J. Szostkiewicz, J. M. Laurent, M. T. Razzak, A. D. White, M. M. Hinks, and S. G. Rodriques, Robin: multi-agent system for automating scientific discovery, arXiv preprint arXiv:2505.13400, 2025. [52] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White, and P. Schwaller, Chemcrow: Augmenting large-language models with chemistry tools, arXiv preprint arXiv:2304.05376, 2023. [53] J. Luo, W. Zhang, Y. Yuan, Y. Zhao, J. Yang, Y. Gu, B. Wu, B. Chen, Z. Qiao, Q. Long et al., Large language model agent: survey on methodology, applications and challenges, arXiv preprint arXiv:2503.21460, 2025. [54] K. Swanson, W. Wu, N. L. Bulaong, J. E. Pak, and J. Zou, The virtual lab of ai agents designs new sars-cov-2 nanobodies, Nature, pp. 13, 2025. [55] H. Su, R. Chen, S. Tang, Z. Yin, X. Zheng, J. Li, B. Qi, Q. Wu, H. Li, W. Ouyang et al., Many heads are better than one: Improved scientific idea generation by llm-based multi-agent system, arXiv preprint arXiv:2410.09403, 2024. [56] Y. Pu, T. Lin, and H. Chen, Piflow: Principle-aware scientific discovery with multi-agent collaboration, arXiv preprint arXiv:2505.15047, 2025. [57] S. Schmidgall, Y. Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, M. Moor, Z. Liu, and E. Barsoum, Agent laboratory: Using llm agents as research assistants, arXiv preprint arXiv:2501.04227, 2025. [58] T. Song, M. Luo, X. Zhang, L. Chen, Y. Huang, J. Cao, Q. Zhu, D. Liu, B. Zhang, G. Zou et al., multiagent-driven robotic ai chemist enabling autonomous chemical research on demand, Journal of the American Chemical Society, vol. 147, no. 15, pp. 12 534 12 545, 2025. [59] K. Ding, J. Yu, J. Huang, Y. Yang, Q. Zhang, and H. Chen, Scitoolagent: knowledge graph-driven scientific agent for multitool [Online]. Available: https://arxiv.org/abs/ 2507.20280 integration, 2025. [60] Y. Zhang, X. Chen, B. Jin, S. Wang, S. Ji, W. Wang, and J. Han, comprehensive survey of scientific large language models and their applications in scientific discovery, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Nov. 2024, pp. 87838817. [Online]. Available: https://aclanthology.org/2024. emnlp-main.498/ [61] Y. Hasin, M. Seldin, and A. Lusis, Multi-omics approaches to disease, Genome biology, vol. 18, no. 1, p. 83, 2017. [62] L. Chen, Y. Lu, C.-T. Wu, R. Clarke, G. Yu, J. E. Van Eyk, D. M. Herrington, and Y. Wang, Data-driven detection of subtype-specific differentially expressed genes, Scientific reports, vol. 11, no. 1, p. 332, 2021. [63] W. Ruan, Y. Lyu, J. Zhang, J. Cai, P. Shu, Y. Ge, Y. Lu, S. Gao, Y. Wang, P. Wang et al., Large language models for bioinformatics, arXiv preprint arXiv:2501.06271, 2025. [64] D. G. York, J. Adelman, J. E. Anderson, S. F. Anderson, J. Annis, N. A. Bahcall, ..., and D. G. York, The sloan digital sky survey: Technical summary, The Astronomical Journal, vol. 120, no. 3, pp. 15791587, 2000. [65] R. Abbott, T. Abbott, F. Acernese, K. Ackley, C. Adams, N. Adhikari, R. Adhikari, V. Adya, C. Affeldt, D. Agarwal et al., Gwtc-3: Compact binary coalescences observed by ligo and virgo during the second part of the third observing run, Physical Review X, vol. 13, no. 4, p. 041039, 2023. [66] R. Rozzi, S. Pickett, C. Palmer, J. J. Armesto, and J. B. Callicott, Linking ecology and ethics for changing world. Springer, 2013. [67] H. A. Simon, The architecture of complexity, in The Roots of Logistics. Springer, 2012, pp. 335361. [68] N. Dan, Y. Cai, and Y. Wang, Symbolic or numerical? understanding physics problem solving in reasoning llms, arXiv preprint arXiv:2507.01334, 2025, version 2, July 3, 2025. [69] R. Wang, B. Wang, K. Li, Y. Zhang, and J. Cheng, Drsr: Llm based scientific equation discovery with dual reasoning from data and experience, arXiv preprint arXiv:2506.04282, 2025, version 1, June 4, 2025. [70] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, and K. A. Persson, Commentary: The materials project: materials innovation, APL genome Materials, vol. 1, no. 1, p. 011002, 07 2013. [Online]. Available: https://doi.org/10.1063/1.4812323 approach to accelerating materials [71] A. Dunn, Q. Wang, A. Ganose, D. Dopp, Jain, Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm, npj Computational Materials, vol. 6, no. 1, p. 138, 2020. [Online]. Available: https://doi.org/10.1038/s41524-020-00406-3 and A. [72] L. Evans and P. Bryant, LHC machine, Journal of Instrumentation, vol. 3, no. 08, p. S08001, 2008. [73] S. Steyaert, M. Pizurica, D. Nagaraj, P. Khandelwal, T. HernandezBoussard, A. J. Gentles, and O. Gevaert, Multimodal data fusion for cancer biomarker discovery with deep learning, Nature machine intelligence, vol. 5, no. 4, pp. 351362, 2023. [74] K. M. Boehm, P. Khosravi, R. Vanguri, J. Gao, and S. P. Shah, Harnessing multimodal data integration to advance precision oncology, Nature Reviews Cancer, vol. 22, no. 2, pp. 114126, 2022. [75] Z. Li, X. Yang, K. Choi, W. Zhu, R. Hsieh, H. Kim, J. H. Lim, S. Ji, B. Lee, X. Yan, L. R. Petzold, S. D. Wilson, W. Lim, and W. Y. Wang, Mmsci: dataset for graduate-level multi-discipline multimodal scientific understanding, 2025. [Online]. Available: https://arxiv.org/abs/2407.04903 [76] M. F. Horstemeyer, Multiscale modeling: review, Practical aspects of computational chemistry: methods, concepts and applications, pp. 87135, 2009. [77] W. Heisenberg, Uber den anschaulichen inhalt der quantentheoretischen kinematik und mechanik, Zeitschrift fur Physik, vol. 43, pp. 172198, 1927. [78] P. Brennecke, S. Anders, J. K. Kim, A. A. Kołodziejczyk, X. Zhang, V. Proserpio, B. Baying, V. Benes, S. A. Teichmann, J. C. Marioni et al., Accounting for technical noise in single-cell rna-seq experiments, Nature methods, vol. 10, no. 11, pp. 10931095, 2013. [79] D. Sivia and J. Skilling, Data analysis: Bayesian tutorial. OUP Oxford, 2006. [80] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan, Learn to explain: Multimodal reasoning via thought chains for science question answering, Advances in Neural Information Processing Systems, vol. 35, pp. 25072521, 2022. [81] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang et al., Mmlu-pro: more robust and challenging multi-task language understanding benchmark, Advances in Neural Information Processing Systems, vol. 37, pp. 95 26695 290, 2024. [82] Y. Liu, Z. Yang, T. Xie, J. Ni, B. Gao, Y. Li, S. Tang, W. Ouyang, E. Cambria, and D. Zhou, Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition, arXiv preprint arXiv:2503.21248, 2025. [83] Z. Chen, S. Chen, Y. Ning, Q. Zhang, B. Wang, B. Yu, Y. Li, Z. Liao, C. Wei, Z. Lu, V. Dey, M. Xue, F. N. Baker, B. Burns, D. Adu-Ampratwum, X. Huang, X. Ning, S. Gao, Y. Su, and H. Sun, ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery, in The Thirteenth International Conference on Learning Representations, 2025. [Online]. Available: https://openreview.net/forum?id=6z4YKr0GK6 [84] K. Zhu, H. Du, Z. Hong, X. Yang, S. Guo, Z. Wang, Z. Wang, C. Qian, X. Tang, H. Ji et al., Multiagentbench: Evaluating the collaboration and competition of llm agents, arXiv preprint arXiv:2503.01935, 2025. [85] S. Fan, X. Cong, Y. Fu, Z. Zhang, S. Zhang, Y. Liu, Y. Wu, Y. Lin, Z. Liu, and M. Sun, Workflowllm: Enhancing workflow orchestration capability of large language models, arXiv preprint arXiv:2411.05451, 2024. [86] X. Liu, L. Ma, Y. Li, W. Yang, Q. Zhou, J. Song, S. Li, and B. Fei, Chemau: Harness the reasoning of llms in chemical research with adaptive uncertainty estimation, arXiv preprint arXiv:2506.01116, 2025. [87] P. Kesseli, P. OHearn, and R. S. Cabral, Logic.py: Bridging the gap between llms and constraint solvers, 2025. [Online]. Available: https://arxiv.org/abs/2502. [88] K. He, R. Mao, Q. Lin, Y. Ruan, X. Lan, M. Feng, and E. Cambria, survey of large language models for healthcare: from data, technology, and applications to accountability and ethics, Information Fusion, vol. 118, p. 102963, 2025. [89] R. Mousa, A. Sarabadani, T. Taami, A. A. Bengari, O. Eslamifar, M. A. Shalmani, and E. K. Shahmarvandi, comparative survey on large language models for biological data, Preprints, 2025. [Online]. Available: https://doi.org/10.20944/preprints202504.2464.v1 [90] J. Wei, Y. Yang, X. Zhang, Y. Chen, X. Zhuang, Z. Gao, D. Zhou, G. Wang, Z. Gao, J. Cao, Z. Qiu, X. He, Q. Zhang, C. You, S. Zheng, N. Ding, W. Ouyang, N. Dong, Y. Cheng, S. Sun, L. Bai, and B. Zhou, From ai for science to agentic science: survey on autonomous scientific discovery, arXiv preprint arXiv:2508.14111, 2025. [91] X. Wang, J. Xu, A. H. Feng, Y. Chen, H. Guo, F. Zhu, Y. Shao, M. Ren, H. Yi, S. Lian, H. Yang, T. Wu, H. Hu, S. Xiang, X.-Y. Zhang, and C.-L. Liu, The hitchhikers guide to autonomous research: survey of scientific agents, TechRxiv, 2025. [Online]. Available: http://dx.doi.org/10.36227/techrxiv.175459840.02185500/v1 [92] S. Ni, G. Chen, S. Li, X. Chen, S. Li, B. Wang, Q. Wang, X. Wang, Y. Zhang, L. Fan, C. Li, R. Xu, L. Sun, and M. Yang, survey on large language model benchmarks, arXiv preprint arXiv:2508.15361, 2025. [93] S. Schnell, Ten simple rules for computational biologists laboratory notebook, p. e1004385, 2015. [94] D. A. Benson, M. Cavanaugh, K. Clark, I. Karsch-Mizrachi, D. J. Lipman, J. Ostell, and E. W. Sayers, Genbank, Nucleic acids research, vol. 41, no. D1, pp. D36D42, 2012. [95] S. K. Burley, C. Bhikadiya, C. Bi, S. Bittrich, L. Chen, G. V. Crichlow, C. H. Christie, K. Dalenberg, L. Di Costanzo, J. M. Duarte et al., Rcsb protein data bank: powerful new tools for exploring 3d structures of biological macromolecules for basic and applied research and education in fundamental biology, biomedicine, biotechnology, bioengineering and energy sciences, Nucleic acids research, vol. 49, no. D1, pp. D437D451, 2021. [96] Y. Wang, S. H. Bryant, T. Cheng, J. Wang, A. Gindulyte, B. A. Shoemaker, P. A. Thiessen, S. He, and J. Zhang, Pubchem bioassay: 2017 update, Nucleic acids research, vol. 45, no. D1, pp. D955 D963, 2017. [97] S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li, B. A. Shoemaker, P. A. Thiessen, B. Yu et al., Pubchem 2023 update, Nucleic acids research, vol. 51, no. D1, pp. D1373D1380, 2023. [98] M. J. Kurtz, G. Eichhorn, A. Accomazzi, C. S. Grant, S. S. Murray, and J. M. Watson, The nasa astrophysics data system: Overview, Astronomy and astrophysics supplement series, vol. 143, no. 1, pp. 4159, 2000. [99] Cornell University. (2025) arxiv. Accessed 7 July 2025. [Online]. Available: https://arxiv.org/ [100] L. L. Kiessling, L. E. Fernandez, A. P. Alivisatos, and P. S. Weiss, Chemrxiv: chemistry preprint server, pp. 90539054, 2016. [101] P. P. Urone and R. Hinrichs, College Physics, 2nd edition, 2nd ed. Houston, TX: OpenStax, 2022, web version last updated July 9, 2025; Licensed under Creative Commons Attribution 4.0 International (CC BY 4.0). Accessed 2025-08-26. [Online]. Available: https://openstax.org/details/books/college-physics-2e [102] OpenStax, Physics. Rice University, 2016. [Online]. Available: https://openstax.org/details/books/physics [103] R. P. Feynman, R. B. Leighton, and M. Sands, The feynman lectures on physicsonline edition, https://www.feynmanlectures. caltech.edu, 2014, authorised web release of the three-volume classic. [104] J. Kpodo, P. Kordjamshidi, and A. P. Nejadhashemi, Agxqa: benchmark for advanced agricultural extension question answering, Computers and Electronics in Agriculture, vol. 225, p. 109349, 2024. [105] X. Chen, T. Wang, T. Guo, K. Guo, J. Zhou, H. Li, Z. Song, X. Gao, and X. Zhang, Unveiling the power of language models in chemical research question answering, Communications Chemistry, vol. 8, no. 1, p. 4, 2025. [106] T. Saikh, T. Ghosal, A. Mittal, A. Ekbal, and P. Bhattacharyya, Scienceqa: novel resource for question answering on scholarly articles, International Journal on Digital Libraries, vol. 23, no. 3, pp. 289301, 2022. [107] S. Auer, D. A. C. Barone, C. Bartz, E. G. Cortes, M. Y. Jaradeh, O. Karras, M. Koubarakis, D. Mouromtsev, D. Pliukhin, D. Radyush, I. Shilin, M. Stocker, and E. Tsalapati, The sciqa scientific question answering benchmark for scholarly knowledge, Scientific Reports, vol. 13, no. 1, p. 7240, May 2023. [Online]. Available: https://doi.org/10.1038/s41598-023-33607-z [108] M. Zaki, Jayadeva, Mausam, and N. M. A. Krishnan, Mascqa: question answering dataset for investigating materials science knowledge of large language models, 2023. [Online]. Available: https://arxiv.org/abs/2308.09115 [109] H. Cui, Z. Shamsi, G. Cheon, X. Ma, S. Li, M. Tikhanovskaya, P. C. Norgaard, N. Mudur, M. B. Plomecka, P. Raccuglia et al., Curie: Evaluating llms on multitask scientific long-context understanding and reasoning, in The Thirteenth International Conference on Learning Representations, 2025. [110] D. A. Zarin, T. Tse, R. J. Williams, R. M. Califf, and N. C. Ide, The clinicaltrials. gov results databaseupdate and key issues, New England Journal of Medicine, vol. 364, no. 9, pp. 852860, 2011. [111] D. B. Resnik, The ethics of research with human subjects: Protecting people, advancing science, promoting trust. Springer, 2018, vol. 74. [112] D. Baltimore, P. Berg, M. Botchan, D. Carroll, R. A. Charo, G. Church, J. E. Corn, G. Q. Daley, J. A. Doudna, M. Fenner et al., prudent path forward for genomic engineering and germline gene modification, Science, vol. 348, no. 6230, pp. 3638, 2015. [113] P. B. Jensen, L. J. Jensen, and S. Brunak, Mining electronic health records: towards better research applications and clinical care, Nature Reviews Genetics, vol. 13, no. 6, pp. 395405, 2012. [114] B. Shickel, P. J. Tighe, A. Bihorac, and P. Rashidi, Deep ehr: survey of recent advances in deep learning techniques for electronic health record (ehr) analysis, IEEE journal of biomedical and health informatics, vol. 22, no. 5, pp. 15891604, 2017. [115] Zooniverse Team. (2007) Galaxy zoo. Citizen science project for galaxy classification, part of the Zooniverse platform. [Online]. Available: https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/ 70 [116] S. Kelling, W. M. Hochachka, D. Fink, M. Riedewald, R. Caruana, G. Ballard, and G. Hooker, Data-intensive science: new paradigm for biodiversity studies, BioScience, vol. 59, no. 7, pp. 613620, 2009. [117] I. Thiele and B. Ø. Palsson, protocol for generating high-quality genome-scale metabolic reconstruction, Nature protocols, vol. 5, no. 1, pp. 93121, 2010. [118] B. Palsson, Systems biology. Cambridge university press, 2015. [119] M. Wu, Y. Wang, Y. Ming, Y. An, Y. Wan, W. Chen, B. Lin, Y. Li, T. Xie, and D. Zhou, Chematagent: Enhancing llms for chemistry and materials science through tree-search based tool learning, 2025. [Online]. Available: https://arxiv.org/abs/2506. [120] X. Tang, B. Qian, R. Gao, J. Chen, X. Chen, and M. B. Gerstein, Biocoder: benchmark for bioinformatics code generation with large language models, Bioinformatics, vol. 40, no. Supplement 1, pp. i266i276, 2024. [121] M. Tian, L. Gao, S. D. Zhang, X. Chen, C. Fan, X. Guo, R. Haas, P. Ji, K. Krongchon, Y. Li, S. Liu, D. Luo, Y. Ma, H. Tong, K. Trinh, C. Tian, Z. Wang, B. Wu, Y. Xiong, S. Yin, M. Zhu, K. Lieret, Y. Lu, G. Liu, Y. Du, T. Tao, O. Press, J. Callan, E. Huerta, and H. Peng, Scicode: research coding benchmark curated by scientists, 2024. [122] J. Gurevitch, J. Koricheva, S. Nakagawa, and G. Stewart, Metaanalysis and the science of research synthesis, Nature, vol. 555, no. 7695, pp. 175182, 2018. [123] A. Ali, N. Zhang, and R. M. Santos, Mineral characterization using scanning electron microscopy (sem): review of the fundamentals, advancements, and research directions, Applied Sciences, vol. 13, no. 23, p. 12600, 2023. [124] D. Liu, Q. Zeng, C. Hu, D. Chen, H. Liu, Y. Han, L. Xu, Q. Zhang, and J. Yang, Light doping of tungsten into copper-platinum nanoalloys for boosting their electrocatalytic performance in methanol oxidation, Nano Res. Energy, vol. 1, no. 2, p. e9120017, 2022. [125] C. Tobi, N. Q. Khanh, Z. Homonnay, and E. Szeles, Applicability of atomic force microscopy for nuclear forensic examination, Journal of Radioanalytical and Nuclear Chemistry, vol. 334, no. 1, pp. 753761, 2025. [126] V. Mansurov, T. Malin, S. Teys, V. Atuchin, D. Milakhin, and K. Zhuravlev, Stm/sts study of the density of states and contrast behavior at the boundary between (7 7) and (8 8) structures in the sin/si (111) system, Crystals, vol. 12, no. 12, p. 1707, 2022. [127] S. Woo, H. Jung, and Y. Yoon, Real-time uv/vis spectroscopy to observe photocatalytic degradation, Catalysts, vol. 13, no. 4, p. 683, 2023. [128] Z. Fan, T. Hwang, S. Lin, Y. Chen, and Z. J. Wong, Directional thermal emission and display using pixelated non-imaging microoptics, Nature Communications, vol. 15, no. 1, p. 4544, 2024. [129] Y. Zhao, Z. Nie, H. Hong, X. Qiu, S. Han, Y. Yu, M. Liu, X. Qiu, K. Liu, S. Meng et al., Spectroscopic visualization and phase manipulation of chiral charge density waves in 1t-tas2, Nature Communications, vol. 14, no. 1, p. 2223, 2023. [130] T. Meier, A. Aslandukova, F. Trybel, D. Laniel, T. Ishii, S. Khandarkhaeva, N. Dubrovinskaia, and L. Dubrovinsky, In situ highpressure nuclear magnetic resonance crystallography in one and two dimensions, Matter and Radiation at Extremes, vol. 6, no. 6, 2021. [131] J. I. Goldstein, D. E. Newbury, D. C. Joy, C. E. Lyman, P. Echlin, E. Lifshin, L. Sawyer, and J. R. Michael, Scanning Electron Microscopy and X-ray Microanalysis, 4th ed. Springer, 2018. [132] D. B. Williams and C. B. Carter, Transmission Electron Microscopy: Textbook for Materials Science, 2nd ed. Springer, 2009. [133] G. Binnig, C. F. Quate, and C. Gerber, Atomic force microscope, Physical Review Letters, vol. 56, no. 9, pp. 930933, 1986. [134] G. Binnig and H. Rohrer, Scanning tunneling microscope, Revista de Fısica: Estudios en Fısica, vol. 0, pp. 35, 1982, nobel Prizewinning invention; original publication in 1981. [135] D. A. Skoog, F. J. Holler, and S. R. Crouch, Principles of Instrumental Analysis, 7th ed. Cengage Learning, 2017. [136] B. Stuart, Infrared Spectroscopy: Fundamentals and Applications. John Wiley & Sons, 2004. [137] J. R. Ferraro, K. Nakamoto, and C. W. Brown, Introductory Raman Spectroscopy, 2nd ed. Academic Press, 2003. [138] T. D. W. Claridge, High-Resolution NMR Techniques in Organic Chemistry, 3rd ed. Elsevier, 2016. [139] X.-Y. Lu, H.-P. Wu, H. Ma, H. Li, J. Li, Y.-T. Liu, Z.-Y. Pan, Y. Xie, L. Wang, B. Ren et al., Deep learning-assisted spectrumstructure correlation: state-of-the-art and perspectives, Analytical Chemistry, vol. 96, no. 20, pp. 79597975, 2024. [140] X. Liu, H. An, W. Cai, and X. Shao, Deep learning in spectral analysis: Modeling and imaging, TrAC Trends in Analytical Chemistry, vol. 172, p. 117612, 2024. [141] F. Ponten, K. Jirstrom, and M. Uhlen, The human protein atlasa tool for pathology, The Journal of Pathology: Journal of the Pathological Society of Great Britain and Ireland, vol. 216, no. 4, pp. 387393, 2008. [142] V. Ljosa, K. L. Sokolnicki, and A. E. Carpenter, Annotated highthroughput microscopy image sets for validation, Nature Methods, vol. 9, no. 7, p. 637, 2012. [143] C. Gohlke, cgohlke/tifffile: v2022.5.4, https://doi.org/10.5281/ zenodo.6795861, Jul. 2022, version v2022.5.4. [144] Nikon"
        },
        {
            "title": "Instruments",
            "content": "image-viewer, products/software/nis-elements/software-resources, 5.21.00. NIS-Elements Viewer: Inc., nd2 https://www.microscope.healthcare.nikon.com/ version 2020,"
        },
        {
            "title": "Free",
            "content": "[145] A. Lozano, J. Nirschl, J. Burgess, S. R. Gupte, Y. Zhang, A. Unell, and S. Yeung, Micro-bench: microscopy benchmark for visionlanguage understanding, Advances in Neural Information Processing Systems, vol. 37, pp. 30 67030 685, 2024. [146] J. Burgess, J. J. Nirschl, L. Bravo-Sanchez, A. Lozano, S. R. Gupte, J. G. Galaz-Montoya, Y. Zhang, Y. Su, D. Bhowmik, Z. Coman et al., Microvqa: multimodal reasoning benchmark for microscopy-based scientific research, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 19 55219 564. [147] C. Hutter and J. C. Zenklusen, The cancer genome atlas: creating lasting value beyond its data, Cell, vol. 173, no. 2, pp. 283285, 2018. [148] N. J. Edwards, M. Oberti, R. R. Thangudu, S. Cai, P. B. McGarvey, S. Jacob, S. Madhavan, and K. A. Ketchum, The CPTAC data portal: resource for cancer proteomics research, Journal of proteome research, vol. 14, no. 6, pp. 27072713, 2015. [149] X. He, Y. Zhang, L. Mou, E. Xing, and P. Xie, Pathvqa: 30000+ questions for medical visual question answering, arXiv preprint arXiv:2003.10286, 2020. [150] W. Ikezogwo, S. Seyfioglu, F. Ghezloo, D. Geva, F. Sheikh Mohammed, P. K. Anand, R. Krishna, and L. Shapiro, Quilt-1M: One million image-text pairs for histopathology, Advances in Neural Information Processing Systems, vol. 36, pp. 37 99538 017, 2023. [151] Y. Chen, G. Wang, Y. Ji, Y. Li, J. Ye, T. Li, M. Hu, R. Yu, Y. Qiao, and J. He, Slidechat: large vision-language assistant for wholeslide pathology image understanding, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 51345143. [152] X. Zhao, W. Xu, B. Liu, Y. Zhou, F. Ling, B. Fei, X. Yue, L. Bai, W. Zhang, and X.-M. Wu, Msearth: benchmark for multimodal scientific comprehension of earth science, arXiv preprint arXiv:2505.20740, 2025. [153] A. E. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P. Lungren, C.-y. Deng, R. G. Mark, and S. Horng, MIMIC-CXR, de-identified publicly available database of chest radiographs with free-text reports, Scientific data, vol. 6, no. 1, p. 317, 2019. [154] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan, L. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald, Preparing collection of radiology examinations for distribution and retrieval, Journal of the American Medical Informatics Association, vol. 23, no. 2, pp. 304310, 2015. [155] Y. Ji, H. Bai, C. Ge, J. Yang, Y. Zhu, R. Zhang, Z. Li, L. Zhanng, W. Ma, X. Wan et al., Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation, Advances in Neural Information Processing Systems, vol. 35, pp. 36 72236 732, 2022. [156] I. E. Hamamci, S. Er, C. Wang, F. Almas, A. G. Simsek, S. N. Esirgun, I. Doga, O. F. Durugol, W. Dai, M. Xu et al., Developing generalist foundation models from multimodal dataset for 3d computed tomography, arXiv preprint arXiv:2403.17834, 2024. [157] National Lung Screening Trial Research Team, Data from the [Online]. Available: national https://www.cancerimagingarchive.net/collection/nlst/ lung screening trial (nlst), 2013. [158] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, and A. Sethi, dataset and technique for generalized nuclear segmentation for computational pathology, IEEE transactions on medical imaging, vol. 36, no. 7, pp. 15501560, 2017. [159] T. Wald, C. Ulrich, J. Suprijadi, S. Ziegler, M. Nohel, R. Peretzke, G. Kohler, and K. H. Maier-Hein, An openmind for 3d medical vision self-supervised learning, arXiv preprint arXiv:2412.17041, 2024. [160] T. A. DAntonoli, L. K. Berger, A. K. Indrakanti, N. Vishwanathan, J. Weiß, M. Jung, Z. Berkarda, A. Rau, M. Reisert, T. Kustner 71 et al., Totalsegmentator mri: Robust sequence-independent segmentation of multiple anatomic structures in mri, arXiv preprint arXiv:2405.19492, 2024. [161] W. Al-Dhabyani, M. Gomaa, H. Khaled, and A. Fahmy, Dataset of breast ultrasound images, Data in Brief, vol. 28, p. 104863, 2020. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S2352340919312181 [162] Y. Yang, Y. Chen, X. Dong, J. Zhang, C. Long, Z. Jin, and Y. Dai, An annotated heterogeneous ultrasound database, Scientific Data, vol. 12, no. 1, p. 148, 2025. [163] S. Gatidis, T. Hepp, M. Fruh, C. La Foug`ere, K. Nikolaou, C. Pfannenberg, B. Scholkopf, T. Kustner, C. Cyran, and D. Rubin, wholebody FDG-PET/CT dataset with manually annotated tumor lesions, Scientific Data, vol. 9, no. 1, p. 601, 2022. [164] S. Gatidis, M. Fruh, M. Fabritius, S. Gu, K. Nikolaou, C. La Foug`ere, J. Ye, J. He, Y. Peng, L. Bi et al., Results from the autoPET challenge on fully automated lesion segmentation in oncologic PET/CT imaging, Nature Machine Intelligence, vol. 6, no. 11, pp. 13961405, 2024. [165] J. Suckling, The mammographic images analysis society digital mammogram database, in Exerpta Medica. International Congress Series, 1994, vol. 1069, 1994, pp. 375378. [166] N. E. M. Association et al., Digital imaging and communication in medicine (DICOM), NEMA PS 3 Supplement 23 Structured Reporting, 1997. [167] R. Cox, J. Ashburner, H. Breman, K. Fissell, C. Haselgrove, C. Holmes, J. Lancaster, D. Rex, S. Smith, J. Woodward, and S. Strother, (sort of) new image data format standard: NiFTI-1, in 10th Annual Meeting of the Organization for Human Brain Mapping, vol. 22, 01 2004. [168] Medixant, RadiAnt DICOM viewer, https://www.radiantviewer. com, version 2021.1. [169] C. Rorden, MRIcroGL: Open-source 2d/3d neuroimaging viewer, https://github.com/rordenlab/MRIcroGL, 2025, version v1.2. [170] D. Mason, scaramallion, mrbean bremen, rhaxton, J. Suever, D. P. Orfanos, Vanessasaurus, G. Lemaitre, A. Panchal, A. Rothberg, M. D. Herrmann, J. Massich, J. Kerns, K. van Golen, C. Bridge, S. Biggs, T. Robitaille, moloney, M. Shun-Shin, B. Conrad, pawelzajdel, M. Mattes, Y. Lyu, T. Cogan, Z. Baratz, F. C. Morency, Taylor, and T. Sentner, pydicom/pydicom: pydicom 3.0.1, https://doi.org/ 10.5281/zenodo.13824606, Sep. 2024, version v3.0.1. [171] B. C. Lowekamp, D. T. Chen, L. Ibanez, and D. Blezek, The design of SimpleITK, Frontiers in Neuroinformatics, vol. 7, p. 45, 2013. [172] J. Staal, M. D. Abr`amoff, M. Niemeijer, M. A. Viergever, and B. Van Ginneken, Ridge-based vessel segmentation in color images of the retina, IEEE Transactions on Medical Imaging, vol. 23, no. 4, pp. 501509, 2004. [173] A. Hoover, V. Kouznetsova, and M. Goldbaum, Locating blood vessels in retinal images by piecewise threshold probing of matched filter response, IEEE Transactions on Medical Imaging, vol. 19, no. 3, pp. 203210, 2000. [174] C. De Vente, K. A. Vermeer, N. Jaccard, H. Wang, H. Sun, F. Khader, D. Truhn, T. Aimyshev, Y. Zhanibekuly, T.-D. Le et al., AIROGS: Artificial intelligence for robust glaucoma screening challenge, IEEE Transactions on Medical Imaging, vol. 43, no. 1, pp. 542557, 2023. [175] R. Wu, C. Zhang, J. Zhang, Y. Zhou, T. Zhou, and H. Fu, Mm-retinal: Knowledge-enhanced foundational pretraining with fundus image-text expertise, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 722732. [176] L. Ding, A. E. Kuriyan, R. S. Ramchandran, C. C. Wykoff, and G. Sharma, PRIME-FP20: Ultra-widefield fundus photography vessel segmentation dataset, IEEE Dataport, 2020, [Online]. Available: http://dx.doi.org/10.21227/ctgj-1367. [177] X. Liang, M. Bian, M. Chen, L. Liu, J. He, J. Xu, and L. Li, novel ophthalmic benchmark for evaluating multimodal large language models with fundus photographs and oct images, arXiv preprint arXiv:2503.07094, 2025. [178] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L. Baxter, A. McKeown, G. Yang, X. Wu, F. Yan et al., Identifying medical diagnoses and treatable diseases by image-based deep learning, Cell, vol. 172, no. 5, pp. 11221131, 2018. [179] P. Tschandl, C. Rosendahl, and H. Kittler, The HAM10000 dataset, large collection of multi-source dermatoscopic images of common pigmented skin lesions, Scientific data, vol. 5, no. 1, pp. 19, 2018. [180] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman, B. Helba, A. Kalloo, K. Liopyris, M. Marchetti et al., Skin lesion analysis toward melanoma detection 2018: challenge hosted by the international skin imaging collaboration (isic), arXiv preprint arXiv:1902.03368, 2019. [181] M. Hu, P. Xia, L. Wang, S. Yan, F. Tang, Z. Xu, Y. Luo, K. Song, J. Leitner, X. Cheng et al., Ophnet: large-scale video benchmark for ophthalmic surgical workflow understanding, in European Conference on Computer Vision. Springer, 2024, pp. 481500. [182] M. Hu, L. Wang, S. Yan, D. Ma, Q. Ren, P. Xia, W. Feng, P. Duan, L. Ju, and Z. Ge, Nurvid: large expert-level video database for nursing procedure activity understanding, Advances in Neural Information Processing Systems, vol. 36, pp. 18 14618 164, 2023. [183] M. Hu, Z. Yu, F. Tang, K. Chen, Y. Li, I. Razzak, J. He, T. Birdal, K. Zhou, and Z. Ge, Towards dynamic 3d reconstruction of hand-instrument interaction in ophthalmic surgery, arXiv preprint arXiv:2505.17677, 2025. [184] W. Li, M. Hu, G. Wang, L. Liu, K. Zhou, J. Ning, X. Guo, Z. Ge, L. Gu, and J. He, Ophora: large-scale data-driven textguided ophthalmic surgical video generation model, arXiv preprint arXiv:2505.07449, 2025. [185] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. De Lange, D. Johansen, and H. D. Johansen, Kvasir-seg: segmented polyp dataset, in International conference on multimedia modeling. Springer, 2019, pp. 451462. [186] A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. De Mathelin, and N. Padoy, Endonet: deep architecture for recognition tasks on laparoscopic videos, IEEE Transactions on Medical Imaging, vol. 36, no. 1, pp. 8697, 2016. [187] Endoscopic Vision Challenge Organizers, Endoscopic Vision Challenge 2025, https://opencas.dkfz.de/endovis/challenges/2025/, 2025, accessed: 2025-07-12. [188] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and S. Belongie, The inaturalist species classification and detection dataset, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 87698778. [189] M. Arbab Arshad, T. Zaki Jubery, T. Roy, R. Nassiri, A. K. Singh, A. Singh, C. Hegde, B. Ganapathysubramanian, A. Balu, A. Krishnamurthy et al., Ageval: benchmark for zero-shot and fewshot plant stress phenotyping with multimodal llms, arXiv preprint arXiv:2407.19617, 2024. [190] V. Dongre, C. Gui, S. Garg, H. Nayyeri, G. Tur, D. Hakkani-Tur, and V. S. Adve, Mirage: benchmark for multimodal informationseeking and reasoning in agricultural expert-guided conversations, arXiv preprint arXiv:2506.20100, 2025. [191] M. T. Chiu, X. Xu, Y. Wei, Z. Huang, A. G. Schwing, R. Brunner, H. Khachatrian, H. Karapetyan, I. Dozier, G. Rose et al., Agriculturevision: large aerial image database for agricultural pattern analysis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 28282838. [192] C. Justice, J. Townshend, E. Vermote, E. Masuoka, R. Wolfe, N. Saleous, D. Roy, and J. Morisette, An overview of modis land data processing and product status, Remote sensing of Environment, vol. 83, no. 1-2, pp. 315, 2002. [193] A. Moreira, P. Prats-Iraola, M. Younis, G. Krieger, I. Hajnsek, and K. P. Papathanassiou, tutorial on synthetic aperture radar, IEEE Geoscience and remote sensing magazine, vol. 1, no. 1, pp. 643, 2013. [194] H. Hersbach, B. Bell, P. Berrisford, S. Hirahara, A. Horanyi, J. Munoz-Sabater, J. Nicolas, C. Peubey, R. Radu, D. Schepers et al., The era5 global reanalysis, Quarterly journal of the royal meteorological society, vol. 146, no. 730, pp. 19992049, 2020. [195] S. Rasp, P. D. Dueben, S. Scher, J. A. Weyn, S. Mouatadid, and N. Thuerey, Weatherbench: benchmark data set for data-driven weather forecasting, Journal of Advances in Modeling Earth Systems, vol. 12, no. 11, p. e2020MS002203, 2020. [196] A. R. Thompson, J. M. Moran, and G. W. Swenson, Interferometry and Synthesis in Radio Astronomy, 3rd ed. Springer, 2017. [197] Hubble space telescope, https://www.stsci.edu/hst, 2025, operated by NASA and ESA; high-resolution optical and UV imaging. [198] J. P. Gardner, J. C. Mather, M. Clampin, R. Doyon, M. A. Greenhouse, H. B. Hammel, J. B. Hutchings, P. Jakobsen, S. J. Lilly, K. S. Long et al., The james webb space telescope, Space Science Reviews, vol. 123, no. 4, pp. 485606, 2006. [199] G. Zhao, Y.-H. Zhao, Y.-Q. Chu, Y.-P. Jing, and L.-C. Deng, Lamost spectral surveyan overview, Research in Astronomy and Astrophysics, vol. 12, no. 7, p. 723, 2012. [200] Q. Tan, D. Zhou, P. Xia, W. Liu, W. Ouyang, L. Bai, Y. Li, and T. Fu, Chemmllm: Chemical multimodal large language model, arXiv preprint arXiv:2505.16326, 2025. 72 [201] C. Edwards, T. Lai, K. Ros, G. Honke, K. Cho, and H. Ji, Translation between molecules and natural language, in 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022. Association for Computational Linguistics (ACL), 2022, pp. 375413. [202] T. Fu, C. Xiao, L. M. Glass, and J. Sun, Moler: Incorporate molecule-level reward to enhance deep generative model for molecule optimization, IEEE transactions on knowledge and data engineering, vol. 34, no. 11, pp. 54595471, 2021. [203] A. Lozano, M. W. Sun, J. Burgess, L. Chen, J. J. Nirschl, J. Gu, I. Lopez, J. Aklilu, A. Rau, A. W. Katzer et al., Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 19 72419 735. [204] W. Lin, Z. Zhao, X. Zhang, C. Wu, Y. Zhang, Y. Wang, and W. Xie, PMC-CLIP: Contrastive language-image pre-training using biomedical documents, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 525536. [205] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in European Conference on Computer Vision. Springer International Publishing, 2016, pp. 235251. [206] K. V. Jobin, A. Mondal, and C. V. Jawahar, DocFigure: dataset for scientific document figure classification, in 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), vol. 1, 2019, pp. 7479. [207] N. Alampara, I. Mandal, P. Khetarpal, H. S. Grover, M. SchillingWilhelmi, N. M. A. Krishnan, and K. M. Jablonka, MaCBench: multimodal chemistry and materials science benchmark, in AI for Accelerated Materials Design - NeurIPS 2024, 2024. [Online]. Available: https://openreview.net/forum?id=Q2PNocDcp6 [208] D. Weininger, Smiles, chemical language and information system. introduction to methodology and encoding rules, Journal of 1. chemical information and computer sciences, vol. 28, no. 1, pp. 31 36, 1988. [209] S. R. Heller and A. D. McNaught, The iupac international chemical identifier (inchi), Chemistry International, vol. 31, no. 1, p. 7, 2009. [210] M. Krenn, Q. Ai, S. Barthel, N. Carson, A. Frei et al., SELFIES and the future of molecular string representations, arXiv:2204.00056, 2022. [211] T.-S. Lin, C. W. Coley, H. Mochigase, H. K. Beech, W. Wang, Z. Wang, E. Woods, S. L. Craig, J. A. Johnson, J. A. Kalow et al., BigSMILES: structurally-based line notation for describing macromolecules, ACS Central Science, vol. 5, no. 9, pp. 15231531, 2019. [212] M. Krenn, F. Hase, A. Nigam, P. Friederich, and A. AspuruGuzik, Self-referencing embedded strings (SELFIES): 100% robust molecular string representation, Machine Learning: Science and Technology, vol. 1, no. 4, p. 045024, 2020. [213] C. W. Coley, R. Barzilay, W. H. Green, T. S. Jaakkola, and K. F. Jensen, Convolutional embedding of attributed molecular graphs for physical property prediction, Journal of chemical information and modeling, vol. 57, no. 8, pp. 17571772, 2017. [214] W. Gao, T. Fu, J. Sun, and C. W. Coley, Sample efficiency matters: benchmarking molecular optimization, Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2022. [215] J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman, ZINC: free tool to discover chemistry for biology, Journal of chemical information and modeling, vol. 52, no. 7, pp. 17571768, 2012. [216] A. Gaulton, L. J. Bellis, A. P. Bento, J. Chambers, M. Davies, A. Hersey, Y. Light, S. McGlinchey, D. Michalovich, B. Al-Lazikani et al., ChEMBL: large-scale bioactivity database for drug discovery, Nucleic acids research, vol. 40, no. D1, pp. D1100D1107, 2012. [217] D. Lowe, Chemical reactions from US patents (1976-Sep2016), [Online]. Available: https://figshare.com/articles/dataset/ 6 2017. Chemical reactions from US patents 1976-Sep2016 /5104873 [218] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and V. Pande, Moleculenet: benchmark for molecular machine [Online]. Available: https://arxiv.org/abs/1703.00564 learning, 2018. [219] M. D. Cranmer, R. Xu, P. Battaglia, and S. Ho, Learning symbolic physics with graph networks, in Proc. NeurIPS Workshop on Machine Learning and the Physical Sciences, 2019, arXiv preprint arXiv:1909.05862. [220] J. Ying, H. Lin, C. Yue, Y. Chen, C. Xiao, Q. Shi, Y. Liang, S.-T. Yau, Y. Zhou, and J. Ma, neural symbolic model for space physics, arXiv preprint arXiv:2503.07994, 2025. [221] V. Angelopoulos, The themis mission, Space Science Reviews, vol. 141, no. 1, pp. 534, 2008. [222] S.-M. Udrescu and M. Tegmark, Ai feynman: physics-inspired method for symbolic regression, Science advances, vol. 6, no. 16, p. eaay2631, 2020. [223] S.-M. Udrescu, A. Tan, J. Feng, O. Neto, T. Wu, and M. Tegmark, Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity, Advances in Neural Information Processing Systems, vol. 33, pp. 48604871, 2020. [224] A. Munoz-Jaramillo and J. M. Vaquero, Visualization of the challenges and limitations of the long-term sunspot number record, Nature Astronomy, vol. 3, no. 3, pp. 205211, 2019. [225] P. Constantin and C. Foias, Navier-stokes equations. University of Chicago press, 1988. [226] P. Bormann, B. Engdahl, and R. Kind, Seismic wave propagation and earth models, in New manual of seismological observatory practice 2 (NMSOP2). Deutsches GeoForschungsZentrum GFZ, 2012, pp. 1105. [227] D. Y. Le Roux, A. Staniforth, and C. A. Lin, Finite elements for shallow-water equation ocean models, Monthly Weather Review, vol. 126, no. 7, pp. 19311951, 1998. [228] R. Edgar, M. Domrachev, and A. E. Lash, Gene expression omnibus: Ncbi gene expression and hybridization array data repository, Nucleic Acids Research, vol. 30, no. 1, pp. 207210, 2002. [229] S. T. Sherry et al., dbsnp: the ncbi database of genetic variation, Nucleic Acids Research, vol. 29, no. 1, pp. 308311, 2001. [230] T. Han, S. Guo, Z. Chen, W. Xu, and L. Bai, Weather-5k: largescale global station weather dataset towards comprehensive timeseries forecasting benchmark, arXiv e-prints, pp. arXiv2406, 2024. [231] F. Cunningham, J. E. Allen, J. Allen, J. Alvarez-Jarreta, M. R. Amode, I. M. Armean, O. Austine-Orimoloye, A. G. Azov, I. Barnes, R. Bennett et al., Ensembl 2022, Nucleic acids research, vol. 50, no. D1, pp. D988D995, 2022. [232] T. U. Consortium, Uniprot: the universal protein knowledgebase in 2023, Nucleic Acids Research, vol. 51, no. D1, pp. D523D531, 2023. [233] D. L. McGuinness, F. Van Harmelen et al., OWL web ontology language overview, W3C recommendation, vol. 10, no. 10, p. 2004, 2004. [234] B. Smith et al., The obo foundry: coordinated evolution of ontologies to support biomedical data integration, Nature Biotechnology, vol. 25, no. 11, pp. 12511255, 2007. [235] M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S. Dwight, J. T. Eppig et al., Gene ontology: tool for the unification of biology, Nature genetics, vol. 25, no. 1, pp. 2529, 2000. [236] S. Kohler et al., The human phenotype ontology in 2021, Nucleic Acids Research, vol. 49, no. D1, pp. D1207D1217, 2021. [237] J. Lin, L. Wang, X. Lu, Z. Hu, W. Zhang, and W. Lu, Improving knowledge graph completion with structure-aware supervised contrastive learning, in Proceedings of the 2024 conference on empirical methods in natural language processing, 2024, pp. 13 94813 959. [238] K. Liang, L. Meng, M. Liu, Y. Liu, W. Tu, S. Wang, S. Zhou, X. Liu, F. Sun, and K. He, survey of knowledge graph reasoning on graph types: Static, dynamic, and multi-modal, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 94569478, 2024. [239] O. Bodenreider, The unified medical language system (umls): integrating biomedical terminology, Nucleic Acids Research, vol. 32, no. suppl 1, pp. D267D270, 2004. [240] P. Chandak, K. Huang, and M. Zitnik, Building knowledge graph to enable precision medicine, Scientific Data, vol. 10, no. 1, p. 67, 2023. [241] H. Li, Z. Wang, J. Wang, A. K. H. Lau, and H. Qu, Cllmate: multimodal llm for weather and climate events forecasting, arXiv preprint arXiv:2409.19058, 2024. [242] E. Anderson, G. D. Veith, and D. Weininger, SMILES, line notation and computerized interpreter for chemical structures. US Environmental Protection Agency, Environmental Research Laboratory, 1987. [243] K. Huang, T. Fu, W. Gao, Y. Zhao, Y. Roohani, J. Leskovec, C. W. Coley, C. Xiao, J. Sun, and M. Zitnik, Therapeutics data commons: machine learning datasets and tasks for therapeutics, NeurIPS Track Datasets and Benchmarks, 2021. 73 [244] CODATA Task Group on Fundamental Constants, Codata recommended values of the fundamental physical constants: 2022, https: //physics.nist.gov/cuu/pdf/wall 2022.pdf, 2024, may 2024 update of CODATA 2022 constants. [245] Particle Data Group, Review of particle physics, Phys. Rev. [Online]. Available: D, vol. 110, no. 3, p. 030001, 2024. https://pdg.lbl.gov/2024/reviews/contents sports.html [246] M. Wenger, F. Ochsenbein, D. Egret, P. Dubois, F. Bonnarel, S. Borde, F. Genova, G. Jasniewicz, S. Laloe, S. Lesteven et al., The simbad astronomical database-the cds reference database for astronomical objects, Astronomy and Astrophysics Supplement Series, vol. 143, no. 1, pp. 922, 2000. [247] F. Ochsenbein, P. Bauer, and J. Marcout, The vizier database of astronomical catalogues, Astronomy and Astrophysics Supplement Series, vol. 143, no. 1, pp. 2332, 2000. [248] S. Wang, H. Sun, H. Liu, D. Li, Y. Li, and T. Hou, Admet evaluation in drug discovery. 16. predicting herg blockers by combining multiple pharmacophores and machine learning approaches, Molecular pharmaceutics, vol. 13, no. 8, pp. 28552866, 2016. [249] C.-Y. Ma, S.-Y. Yang, H. Zhang, M.-L. Xiang, Q. Huang, and Y.-Q. Wei, Prediction models of human plasma protein binding rate and oral bioavailability derived by using gacgsvm method, Journal of pharmaceutical and biomedical analysis, vol. 47, no. 4-5, pp. 677 682, 2008. [250] T. Hou, J. Wang, W. Zhang, and X. Xu, ADME evaluation in drug discovery. 7. prediction of oral absorption by correlation and classification, Journal of chemical information and modeling, vol. 47, no. 1, pp. 208218, 2007. [251] I. F. Martins, A. L. Teixeira, L. Pinheiro, and A. O. Falcao, bayesian approach to in silico blood-brain barrier penetration modeling, Journal of chemical information and modeling, vol. 52, no. 6, pp. 16861697, 2012. [252] D. L. Mobley and J. P. Guthrie, Freesolv: database of experimental and calculated hydration free energies, with input files, Journal of computer-aided molecular design, vol. 28, no. 7, pp. 711720, 2014. [253] A. Gaulton, A. Hersey, M. Nowotka, A. P. Bento, J. Chambers, D. Mendez, P. Mutowo, F. Atkinson, L. J. Bellis, E. Cibrian-Uhalte et al., The chembl database in 2017, Nucleic acids research, vol. 45, no. D1, pp. D945D954, 2017. [254] F. Lombardo and Y. Jing, In silico prediction of volume of distribution in humans. extensive data set and the exploration of linear and nonlinear methods coupled with molecular interaction fields descriptors, Journal of Chemical Information and Modeling, vol. 56, no. 10, pp. 20422052, 2016. [255] A. M. Richard, R. S. Judson, K. A. Houck, C. M. Grulke, P. Volarath, I. Thillainadarajah, C. Yang, J. Rathman, M. T. Martin, J. F. Wambaugh et al., Toxcast chemical landscape: paving the road to 21st century toxicology, Chemical research in toxicology, vol. 29, no. 8, pp. 12251251, 2016. [256] National Center for Advancing Translational Sciences (NCATS), NIH, Tox21 data challenge 2014: Training, testing, and final evaluation datasets, https://tripod.nih.gov/tox21/challenge/data.jsp, 2014, datasets (training, testing, and final evaluation files) hosted by NCATS/NIH; available as SMILES/SDF and assay-specific files. No DOI reported for the challenge dataset; see Tox21 public-data portal for downloads. Accessed 2025-08-27. [257] INSPIRE Collaboration, Inspire-hep rest api, https://github.com/ inspirehep/rest-api-doc, 2020, high-energy-physics bibliographic knowledge graph. [258] P. Esling and C. Agon, Time-series data mining, ACM Computing Surveys (CSUR), vol. 45, no. 1, pp. 134, 2012. series analysis: forecasting and control. [259] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time John Wiley & Sons, 2015. [260] B. Lim and S. Zohren, Time-series forecasting with deep learning: survey, Philosophical Transactions of the Royal Society A, vol. 379, no. 2194, p. 20200209, 2021. [261] S. Chmiela, A. Tkatchenko, H. E. Sauceda, I. Poltavsky, K. T. Schutt, and K.-R. Muller, Machine learning of accurate energy-conserving molecular force fields, Science Advances, vol. 3, no. 5, p. e1603015, 2017. [262] K. Schutt, P.-J. Kindermans, H. E. Sauceda Felix, S. Chmiela, A. Tkatchenko et al., Schnet: continuous-filter convolutional neural network for modeling quantum interactions, in Advances in Neural Information Processing Systems, 2017. [263] K. T. Schutt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, and K.-R. Muller, SchNeta deep learning architecture for molecules and materials, The Journal of Chemical Physics, vol. 148, no. 24, p. 241722, 2018. [264] W. J. Borucki, Kepler mission: development and overview, Reports on Progress in Physics, vol. 79, no. 3, p. 036901, 2016. [265] B. Peng, R. Nan, Y. Su, Y. Qiu, L. Zhu, and W. Zhu, Fivehundred-meter aperture spherical telescope project, in International Astronomical Union Colloquium, vol. 182. Cambridge University Press, 2001, pp. 219224. [266] C. Binnie and P. Prior, Electroencephalography. Journal of Neurology, Neurosurgery & Psychiatry, vol. 57, no. 11, pp. 13081319, 1994. [267] N. Yeung, R. Bogacz, C. B. Holroyd, and J. D. Cohen, Detection of synchronized oscillations in the electroencephalogram: an evaluation of methods, Psychophysiology, vol. 41, no. 6, pp. 822832, 2004. [268] G. A. Light, L. E. Williams, F. Minow, J. Sprock, A. Rissling, R. Sharp, N. R. Swerdlow, and D. L. Braff, Electroencephalography (eeg) and event-related potentials (erps) with human participants, Current protocols in neuroscience, vol. 52, no. 1, pp. 625, 2010. [269] T. Ahern, Iris (incorporate research institutions for seismology), Lettre dinformation Resif, pp. 67, 2015. [270] U.S. Geological Survey, Usgs earthquake catalog api, https:// earthquake.usgs.gov/fdsnws/event/1/, 2025, provides GeoJSON and QuakeML formats for seismic event time series. [271] Y. Zhou, J. Wu, Z. Ren, Z. Yao, W. Lu, K. Peng, Q. Zheng, C. Song, W. Ouyang, and C. Gou, Csbrain: cross-scale spatiotemporal brain foundation model for eeg decoding, arXiv preprint arXiv:2506.23075, 2025. [272] Z. Bar-Joseph, A. Gitter, and I. Simon, Studying and modelling dynamic biological processes using time-series gene expression data, Nature Reviews Genetics, vol. 13, no. 8, pp. 552564, 2012. [273] A. K. Shalek, R. Satija, J. Shuga, J. J. Trombetta, D. Gennert, D. Lu, P. Chen, R. S. Gertner, J. T. Gaublomme, N. Yosef et al., Singlecell rna-seq reveals dynamic paracrine control of cellular variation, Nature, vol. 510, no. 7505, pp. 363369, 2014. [274] Q. Deng, D. Ramskold, B. Reinius, and R. Sandberg, Single-cell rna-seq reveals dynamic, random monoallelic gene expression in mammalian cells, Science, vol. 343, no. 6167, pp. 193196, 2014. [275] J. L. Willems, C. Abreu-Lima, P. Arnaud, J. H. van Bemmel, C. Brohet, R. Degani, B. Denis, J. Gehring, I. Graham, G. van Herpen et al., The diagnostic performance of computer programs for the interpretation of electrocardiograms, New England Journal of Medicine, vol. 325, no. 25, pp. 17671773, 1991. [276] F. Agrafioti and D. Hatzinakos, Ecg biometric analysis in cardiac irregularity conditions, Signal, Image and Video Processing, vol. 3, no. 4, pp. 329343, 2009. [277] E. Kugelberg, Electromyograms in muscular disorders, Journal of Neurology, Neurosurgery, and Psychiatry, vol. 10, no. 3, p. 122, 1947. [278] A. Merlo, D. Farina, and R. Merletti, fast and reliable technique for muscle activity detection from surface emg signals, IEEE transactions on biomedical engineering, vol. 50, no. 3, pp. 316323, 2003. [279] D. Rodbard, Continuous glucose monitoring: review of successes, challenges, and opportunities, Diabetes technology & therapeutics, vol. 18, no. S2, pp. S23, 2016. [280] D. C. Klonoff, D. Ahn, and A. Drincic, Continuous glucose monitoring: review of the technology and clinical use, Diabetes Research and Clinical Practice, vol. 133, pp. 178192, 2017. [281] S.-G. Kim and S. Ogawa, Biophysical and physiological origins of blood oxygenation level-dependent fmri signals, Journal of Cerebral Blood Flow & Metabolism, vol. 32, no. 7, pp. 11881206, 2012. [282] M. P. Van Den Heuvel and H. E. H. Pol, Exploring the brain network: review on resting-state fmri functional connectivity, European neuropsychopharmacology, vol. 20, no. 8, pp. 519534, 2010. [283] A. L. Fred, S. N. Kumar, A. Kumar Haridhas, S. Ghosh, H. Purushothaman Bhuvana, W. K. J. Sim, V. Vimalan, F. A. S. Givo, V. Jousmaki, P. Padmanabhan et al., brief introduction to magnetoencephalography (meg) and its clinical applications, Brain sciences, vol. 12, no. 6, p. 788, 2022. spectroscopy, dynamics and applications. [284] J. Vrba and S. E. Robinson, Signal processing in magnetoencephalography, Methods, vol. 25, no. 2, pp. 249271, 2001. [285] H. H. Telle, A. G. Urena, and R. J. Donovan, Laser chemistry: John Wiley & Sons, 2007. [286] W. D. Pesnell, B. J. Thompson, and P. C. Chamberlin, The solar dynamics observatory (sdo), Solar Physics, vol. 275, no. 1-2, pp. 315, 2012. [Online]. Available: https://doi.org/10.1007/ s11207-011-9841-3 74 [287] M. G. Bobra and S. Couvidat, Solar flare prediction using sdo/hmi vector magnetic field data with machine-learning algorithm, Astrophysical Journal, vol. 798, no. 2, p. 135, 2015. [Online]. Available: https://doi.org/10.1088/0004-637X/798/2/ [288] N. Menachemi and T. H. Collum, Benefits and drawbacks of electronic health record systems, Risk management and healthcare policy, pp. 4755, 2011. [289] S. J. Vos, C. Xiong, P. J. Visser, M. S. Jasielec, J. Hassenstab, E. A. Grant, N. J. Cairns, J. C. Morris, D. M. Holtzman, and A. M. Fagan, Preclinical alzheimers disease and its outcome: longitudinal cohort study, The Lancet Neurology, vol. 12, no. 10, pp. 957965, 2013. [290] S. Niu, Q. Yin, J. Ma, Y. Song, Y. Xu, L. Bai, W. Pan, and X. Yang, Enhancing healthcare decision support through explainable ai models for risk prediction, Decision Support Systems, vol. 181, p. 114228, 2024. [291] E. C. Bellm, S. R. Kulkarni, M. J. Graham, R. Dekany, R. M. Smith, R. Riddle, F. J. Masci, G. Helou, T. A. Prince, S. M. Adams et al., The zwicky transient facility: system overview, performance, and first results, Publications of the Astronomical Society of the Pacific, vol. 131, no. 995, p. 018002, 2018. [292] D. L. Tucker, The vera c. rubin observatory legacy survey of space & time (lsst): An astronomical data set for the future, Fermi National Accelerator Laboratory (FNAL), Batavia, IL (United States), Tech. Rep., 2023. [293] E. P. Chassignet, H. E. Hurlburt, O. M. Smedstad, G. R. Halliwell, P. J. Hogan, A. J. Wallcraft, R. Baraille, and R. Bleck, The hycom (hybrid coordinate ocean model) data assimilative system, Journal of Marine Systems, vol. 65, no. 1-4, pp. 6083, 2007. [294] D. C. Guest, Solutions network formulation report. improving noaas tides and currents through enhanced data inputs from nasas ocean surface topography mission, 2006. [295] IRIS Data Management Center, Time series data from seismic stations worldwide, https://ds.iris.edu/ds/nodes/dmc/data/types/ time-series-data/, 2025, continuous waveform records in MiniSEED and related formats. [296] M. A. Morid, O. R. L. Sheng, and J. Dunbar, Time series prediction using deep learning methods in healthcare, ACM Transactions on Management Information Systems, vol. 14, no. 1, pp. 129, 2023. [297] F. Di Martino and F. Delmastro, Explainable ai for clinical and remote health applications: survey on tabular and time series data, Artificial Intelligence Review, vol. 56, no. 6, pp. 52615315, 2023. [298] C. Davey, D. Sargent, K. Luger, A. Maeder, and T. Richmond, Solvent mediated interactions in the structure of the nucleosome core particle at 1.9 resolution, J. Mol. Biol., vol. 319, no. 5, pp. 1097 1113, 2002. [299] E. Pettersen, T. Goddard, C. Huang, G. Couch, D. Greenblatt, E. Meng, and T. Ferrin, UCSF chimeraa visualization system for exploratory research and analysis, J. Comput. Chem., vol. 25, no. 13, pp. 16051612, 2004. [300] B. Adamczyk, M. Antczak, and M. Szachniuk, Rnasolo: repository of cleaned pdb-derived rna 3d structures, Bioinformatics, vol. 38, no. 14, pp. 36683670, 2022. [301] C. Bernard, G. Postic, S. Ghannay, and F. Tahi, State-of-the-rnart: benchmarking current methods for rna 3d structure prediction, NAR Genomics and Bioinformatics, vol. 6, no. 2, p. lqae048, 2024. [302] Y.-C. Wang, W.-H. Yang, C.-S. Yang, M.-H. Hou, C.-L. Tsai, Y.-Z. Chou, M.-C. Hung, and Y. Chen, Structural basis of SARS-CoV-2 main protease inhibition by broad-spectrum anti-coronaviral drug, Am. J. Cancer Res., vol. 10, no. 8, pp. 25352545, 2020. [303] I. Subramanian, S. Verma, S. Kumar, A. Jere, and K. Anamika, Multi-omics data integration, interpretation, and its application, Bioinformatics and biology insights, vol. 14, p. 1177932219899051, 2020. [304] J. Xie, Y. Chen, S. Luo, W. Yang, Y. Lin, L. Wang, X. Ding, M. Tong, and R. Yu, Tracing unknown tumor origins with biologicalpathway-based transformer model, Cell Reports Methods, vol. 4, no. 6, 2024. [305] N. Rappoport and R. Shamir, Multi-omic and multi-view clustering algorithms: review and cancer benchmark, Nucleic acids research, vol. 46, no. 20, pp. 10 54610 562, 2018. [306] Y. Lin, L. Luo, Y. Chen, X. Zhang, Z. Wang, W. Yang, M. Tong, and R. Yu, St-align: multimodal foundation model for image-gene alignment in spatial transcriptomics, arXiv preprint arXiv:2411.16793, 2024. [307] Y. Ren, W. Han, Q. Zhang, Y. Tang, W. Bai, Y. Cai, L. Qiao, H. Jiang, D. Yuan, T. Chen et al., Comet: Benchmark for comprehensive biological multi-omics evaluation tasks and language models, arXiv preprint arXiv:2412.10347, 2024. [308] K. J. Karczewski and M. P. Snyder, Integrative omics for health and disease, Nature Reviews Genetics, vol. 19, no. 5, pp. 299310, 2018. [309] T. Hubbard, D. Barker, E. Birney, G. Cameron, Y. Chen, L. Clark, T. Cox, J. Cuff, V. Curwen, T. Down et al., The ensembl genome database project, Nucleic acids research, vol. 30, no. 1, pp. 3841, 2002. [310] W. J. Kent, C. W. Sugnet, T. S. Furey, K. M. Roskin, T. H. Pringle, A. M. Zahler, and D. Haussler, The human genome browser at ucsc, Genome research, vol. 12, no. 6, pp. 9961006, 2002. [311] D. Welter, J. A. L. MacArthur, J. Morales, T. Burdett, P. Hall, Junkins, A. Klemm, P. Flicek, T. Manolio, L. Hindorff, H. and H. E. Parkinson, The NHGRI GWAS catalog, curated resource of snp-trait associations, Nucleic Acids Res., vol. 42, no. Database-Issue, pp. 10011006, 2014. [Online]. Available: https://doi.org/10.1093/nar/gkt1229 [312] K. J. Karczewski, L. C. Francioli, G. Tiao, B. B. Cummings, J. Alfoldi, Q. Wang, R. L. Collins, K. M. Laricchia, A. Ganna, D. P. Birnbaum et al., The mutational constraint spectrum quantified from variation in 141,456 humans, Nature, vol. 581, no. 7809, pp. 434 443, 2020. [313] M. J. Landrum, J. M. Lee, M. Benson, G. Brown, C. Chao, S. Chitipiralla, B. Gu, J. Hart, D. Hoffman, J. Hoover et al., Clinvar: public archive of interpretations of clinically relevant variants, Nucleic acids research, vol. 44, no. D1, pp. D862D868, 2016. [314] A. Kundaje, W. Meuleman, J. Ernst, M. Bilenky, A. Yen, P. Kheradpour, Z. Zhang, A. Heravi-Moussavi, Y. Liu, V. Amin et al., Integrative analysis of 111 reference human epigenomes, Nature, vol. 518, no. 7539, p. 317, 2015. [315] E. Lieberman-Aiden, N. L. Van Berkum, L. Williams, M. Imakaev, T. Ragoczy et al., Comprehensive mapping of long-range interactions reveals folding principles of the human genome, Science, vol. 326, no. 5950, pp. 289293, 2009. [316] S. S. Rao, M. H. Huntley, N. C. Durand, E. K. Stamenova, I. D. Bochkov et al., 3d map of the human genome at kilobase resolution reveals principles of chromatin looping, Cell, vol. 159, no. 7, pp. 16651680, 2014. [317] Z. Zhou, Y. Ji, W. Li, P. Dutta, R. Davuluri, and H. Liu, Dnabert-2: Efficient foundation model and benchmark for multi-species genome, arXiv preprint arXiv:2306.15006, 2023. [318] H. Dalla-Torre, L. Gonzalez, J. Mendoza-Revilla, N. Lopez Carranza, A. H. Grzywaczewski, F. Oteri, C. Dallago, E. Trop, B. P. de Almeida, H. Sirelkhatim et al., Nucleotide transformer: building and evaluating robust foundation models for human genomics, Nature Methods, vol. 22, no. 2, pp. 287297, 2025. [319] A. Frankish, M. Diekhans, A.-M. Ferreira, R. Johnson, I. Jungreis, J. Loveland, J. M. Mudge, C. Sisu, J. Wright, J. Armstrong et al., Gencode reference annotation for the human and mouse genomes, Nucleic acids research, vol. 47, no. D1, pp. D766D773, 2019. [320] N. A. OLeary, M. W. Wright, J. R. Brister, S. Ciufo, D. Haddad, R. McVeigh, B. Rajput, B. Robbertse, B. Smith-White, D. AkoAdjei et al., Reference sequence (refseq) database at ncbi: current status, taxonomic expansion, and functional annotation, Nucleic acids research, vol. 44, no. D1, pp. D733D745, 2016. [321] N. Kolesnikov, E. Hastings, M. Keays, O. Melnichuk, Y. A. Tang, E. Williams, M. Dylag, N. Kurbatova, M. Brandizi, T. Burdett et al., Arrayexpress updatesimplifying data submissions, Nucleic acids research, vol. 43, no. D1, pp. D1113D1116, 2015. [322] G. Consortium, The gtex consortium atlas of genetic regulatory effects across human tissues, Science, vol. 369, no. 6509, pp. 1318 1330, 2020. [323] 10x Genomics, Visium spatial platform, https://www.10xgenomics. com/platforms/visium, 2025, product page for the Visium Spatial Platform (Visium HD / Visium CytAssist, assays, and analysis tools). Accessed 2025-08-28. [324] S. G. Rodriques, R. R. Stickels, A. Goeva, C. A. Martin, E. Murray, C. R. Vanderburg, J. Welch, L. M. Chen, F. Chen, and E. Z. Macosko, Slide-seq: scalable technology for measuring genomewide expression at high spatial resolution, Science, vol. 363, no. 6434, pp. 14631467, 2019. [325] A. Chen, S. Liao, M. Cheng, K. Ma, L. Wu, Y. Lai, X. Qiu, J. Yang, J. Xu, S. Hao et al., Spatiotemporal transcriptomic atlas of mouse organogenesis using dna nanoball-patterned arrays, Cell, vol. 185, no. 10, pp. 17771792, 2022. 75 [326] Z. Fan, R. Chen, and X. Chen, Spatialdb: database for spatially resolved transcriptomes, Nucleic acids research, vol. 48, no. D1, pp. D233D237, 2020. [327] D. Szklarczyk, A. L. Gable, D. Lyon, A. Junge, S. Wyder, J. HuertaCepas, M. Simonovic, N. T. Doncheva, J. H. Morris, P. Bork et al., String v11: proteinprotein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets, Nucleic acids research, vol. 47, no. D1, pp. D607D613, 2019. [328] H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov, and P. E. Bourne, The protein data bank, Nucleic acids research, vol. 28, no. 1, pp. 235242, 2000. [329] D. Szklarczyk, R. Kirsch, M. Koutrouli, K. Nastou, F. Mehryary, R. Hachilif, A. L. Gable, T. Fang, N. T. Doncheva, S. Pyysalo, P. Bork, L. J. Jensen, and C. von Mering, The string database in 2023: proteinprotein association networks and functional enrichment analyses for any sequenced genome of interest, Nucleic Acids Research, vol. 51, no. D1, pp. D638D646, 2023. [330] R. Oughtred, C. Stark, B.-J. Breitkreutz, J. Rust, L. Boucher, C. Chang, N. Kolas, L. ODonnell, G. Leung, and et al., The BioGRID interaction database: 2019 update, Nucleic Acids Research, vol. 47, no. D1, pp. D529D541, 2019. [331] H. Hermjakob, L. Montecchi-Palazzi, C. Lewington, S. Mudali, S. Kerrien, S. Orchard, M. Vingron, B. Roechert, P. Roepstorff, A. Valencia, H. Margalit, J. Armstrong, A. Bairoch, G. Cesareni, D. Sherman, and R. Apweiler, Intact: an open source molecular interaction database, Nucleic Acids Research, vol. 32, no. Database issue, pp. D452D455, 2004. [332] The Gene Ontology Consortium, The gene ontology resource: enriching GOld mine of functional information, Nucleic Acids Research, vol. 49, no. D1, pp. D325D334, 2021. [333] M. Uhlen, L. Fagerberg, B. M. Hallstrom, C. Lindskog, P. Oksvold, A. Mardinoglu, A. Sivertsson, C. Kampf, E. Sjostedt, A. Asplund, and et al., Tissue-based map of the human proteome, Science, vol. 347, no. 6220, p. 1260419, 2015. [334] M. Varadi, D. Bertoni, P. Manana, U. Paramval, I. Pidruchna, M. Radhakrishnan, M. Tsenkov, S. Nair, M. Mirdita, M. Steinegger, D. Hassabis, S. Velankar, and et al., Alphafold protein structure database in 2024: providing structure coverage for over 214 million protein sequences, Nucleic Acids Research, vol. 52, no. D1, pp. D368D375, 2024. [335] E. W. Deutsch, N. Bandeira, V. Sharma, Y. Perez-Riverol, J. J. Carver, D. J. Kundu, and et al., The proteomexchange consortium in 2020: enabling big data approaches in proteomics, Nucleic Acids Research, vol. 48, no. D1, pp. D1145D1152, 2020. [336] Y. Perez-Riverol, J. Bai, C. Bandla, J. A. Vizcaıno, and et al., The PRIDE database resources in 2022: hub for mass spectrometry-based proteomics evidences, Nucleic Acids Research, vol. 51, no. D1, pp. D1539D1548, 2023. [337] M. D. Wilkinson, M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Axton, A. Baak, N. Blomberg, J.-W. Boiten, L. B. da Silva Santos, P. E. Bourne et al., The fair guiding principles for scientific data management and stewardship, Scientific data, vol. 3, no. 1, pp. 19, 2016. [338] C. F. Taylor, N. W. Paton, K. S. Lilley, P.-A. Binz, R. K. Julian Jr, A. R. Jones, W. Zhu, R. Apweiler, R. Aebersold, E. W. Deutsch et al., The minimum information about proteomics experiment (MIAPE), Nature biotechnology, vol. 25, no. 8, pp. 887893, 2007. [339] D. S. Wishart, Y. D. Feunang, A. Marcu, A. C. Guo, K. Liang, R. Vazquez-Fresno, T. Sajed, D. Johnson, C. Li, N. Karu et al., Hmdb 4.0: the human metabolome database for 2018, Nucleic acids research, vol. 46, no. D1, pp. D608D617, 2018. [340] K. Haug, R. M. Salek, P. Conesa, J. Hastings, P. De Matos, M. Rijnbeek, T. Mahendraker, M. Williams, S. Neumann, P. Rocca-Serra et al., Metabolightsan open-access general-purpose repository for metabolomics studies and associated meta-data, Nucleic acids research, vol. 41, no. D1, pp. D781D786, 2013. [341] The Human Microbiome Project Consortium, Structure, function and diversity of the healthy human microbiome, Nature, vol. 486, no. 7402, pp. 207214, 2012. [342] A. L. Mitchell, A. Almeida, M. Beracochea, M. Boland, J. Burgin, G. Cochrane, M. R. Crusoe, V. Kale, S. C. Potter, L. J. Richardson et al., MGnify: the microbiome analysis resource in 2020, Nucleic acids research, vol. 48, no. D1, pp. D570D578, 2020. [343] V. Neveu, A. Moussy, H. Rouaix, R. Wedekind, A. Pon, C. Knox, D. S. Wishart, and A. Scalbert, Exposome-explorer: manuallycurated database on biomarkers of exposure to dietary and environmental factors, Nucleic acids research, p. gkw980, 2016. [344] B. B. Misra, C. Langefeld, M. Olivier, and L. A. Cox, Integrated omics: tools, advances and future approaches, Journal of molecular endocrinology, vol. 62, no. 1, pp. R21R45, 2019. [345] J. Xie, Y. Song, H. Zheng, S. Luo, Y. Chen, C. Zhang, R. Yu, and M. Tong, Pathmethy: an interpretable ai framework for cancer origin tracing based on dna methylation, Briefings in Bioinformatics, vol. 25, no. 6, p. bbae497, 2024. [346] Y. He, P. Fang, Y. Shan, Y. Pan, Y. Wei, Y. Chen, Y. Chen, Y. Liu, Z. Zeng, Z. Zhou et al., Generalized biological foundation model with unified nucleic acid and protein language, Nature Machine Intelligence, pp. 112, 2025. [347] R. L. Ackoff, From data to wisdom, Journal of applied systems analysis, vol. 16, no. 1, pp. 39, 1989. [348] J. Rowley, The wisdom hierarchy: representations of the dikw hierarchy, Journal of information science, vol. 33, no. 2, pp. 163 180, 2007. [349] M. Zeleny, Management support systems: Towards integrated knowledge management, Human systems management, vol. 7, no. 1, pp. 5970, 1987. [350] S. Baskarada and A. Koronios, Data, information, knowledge, wisdom (dikw): semiotic theoretical and empirical exploration of the hierarchy and its quality dimension, Australasian Journal of Information Systems, vol. 18, no. 1, 2013. [351] B. D. Savage and K. R. Sembach, Interstellar abundances from absorption-line observations with the hubble space telescope, Annual Review of Astronomy and Astrophysics, vol. 34, no. 1, pp. 279329, 1996. [352] B. P. Abbott, R. Abbott, T. Abbott, S. Abraham, F. Acernese, K. Ackley, C. Adams, R. Adhikari, V. Adya, C. Affeldt et al., GWTC1: gravitational-wave transient catalog of compact binary mergers observed by ligo and virgo during the first and second observing runs, Physical Review X, vol. 9, no. 3, p. 031040, 2019. [353] S. Nurk, S. Koren, A. Rhie, M. Rautiainen, A. V. Bzikadze, A. Mikheenko, M. R. Vollger, N. Altemose, L. Uralsky, A. Gershman et al., The complete sequence of human genome, Science, vol. 376, no. 6588, pp. 4453, 2022. [354] G. Buzsaki and B. O. Watson, Brain rhythms and neural syntax: implications for efficient coding of cognitive content and neuropsychiatric disease. Dialogues in clinical neuroscience, vol. 14, no. 4, pp. 345367, 2012. [355] A. Regev, S. A. Teichmann, E. S. Lander, I. Amit, C. Benoist, E. Birney, B. Bodenmiller, P. Campbell, P. Carninci, M. Clatworthy et al., The human cell atlas, elife, vol. 6, p. e27041, 2017. [356] G. Yang, J. Liu, C. Zhao, Z. Li, Y. Huang, H. Yu, B. Xu, X. Yang, D. Zhu, X. Zhang et al., Unmanned aerial vehicle remote sensing for field-based crop phenotyping: current status and perspectives, Frontiers in plant science, vol. 8, p. 1111, 2017. [357] A. Savtchenko, D. Ouzounov, S. Ahmad, J. Acker, G. Leptoukh, J. Koziana, and D. Nickless, Terra and aqua modis products available from nasa ges daac, Advances in Space Research, vol. 34, no. 4, pp. 710714, 2004. [358] M. Rizhko and J. S. Bloom, Self-supervised multimodal model for astronomy, in Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges, 2024. [359] I. Fountoulakis and C. P. Evangelidis, The 20242025 seismic sequence in the santorini-amorgos region: Insights into volcano-tectonic activity through high-resolution seismic monitoring, Seismica, vol. 4, no. 1, 2025. [360] C. P. Evangelidis, N. Triantafyllis, M. Samios, K. Boukouras, K. Kontakos, O.-J. Ktenidou, I. Fountoulakis, I. Kalogeras, N. S. Melis, O. Galanis et al., Seismic waveform data from greece and cyprus: Integration, archival, and open access, Seismological Society of America, vol. 92, no. 3, pp. 16721684, 2021. [361] A. Reichel and P. Lienau, Pharmacokinetics in drug discovery: an exposure-centred approach to optimising and predicting drug efficacy and safety, New approaches to drug discovery, pp. 235260, 2015. [362] D. B. Lobell, G. Azzari, M. Burke, S. Gourlay, Z. Jin, T. Kilic, and S. Murray, Eyes in the sky, boots on the ground: Assessing satellite-and ground-based approaches to crop yield measurement and analysis, American Journal of Agricultural Economics, vol. 102, no. 1, pp. 202219, 2020. [363] W. Xu, F. Ling, T. Han, H. Chen, W. Ouyang, and L. BAI, Generalizing weather forecast to fine-grained temporal scales via physicsai hybrid modeling, Advances in Neural Information Processing Systems, vol. 37, pp. 23 32523 351, 2024. 76 [364] J. Gong, L. Bai, P. Ye, W. Xu, N. Liu, J. Dai, X. Yang, and W. Ouyang, Cascast: Skillful high-resolution precipitation nowcasting via cascaded modelling, arXiv preprint arXiv:2402.04290, 2024. [365] J. Gong, S. Tu, W. Yang, B. Fei, K. Chen, W. Zhang, X. Yang, W. Ouyang, and L. Bai, Postcast: Generalizable postprocessing for precipitation nowcasting via unsupervised blurriness modeling, arXiv preprint arXiv:2410.05805, 2024. [366] K. Chen, T. Han, J. Gong, L. Bai, F. Ling, J.-J. Luo, X. Chen, L. Ma, T. Zhang, R. Su et al., Fengwu: Pushing the skillful global medium-range weather forecast beyond 10 days lead, arXiv preprint arXiv:2304.02948, 2023. [367] W. Xu, K. Chen, T. Han, H. Chen, W. Ouyang, and L. Bai, Extremecast: Boosting extreme value prediction for global weather forecast, arXiv preprint arXiv:2402.01295, 2024. [368] J. W. Hardy, Adaptive optics for astronomical telescopes. Oxford university press, 1998, vol. 16. [369] U.S. Food and Drug Administration and JHU-CERSI, Assessing and communicating heterogeneity of treatment effects for patient subpopulations: Challenges and opportunities, Workshop summary, U.S. FDA and Johns Hopkins University CERSI, Nov. 28 2018, symposium hosted November 28, 2018; FDA emphasised the importance of accounting for patient heterogeneity in clinical trial design and communication. [370] I. Newton, Philosophiae naturalis principia mathematica. Jussu Societatis Regiae ac Typis Josephi Streater. Prostat apud plures bibliopolas, 1687. [371] J. C. Maxwell, VIII. dynamical theory of the electromagnetic field, Philos. Trans. R. Soc. Lond., vol. 155, no. 0, pp. 459512, Dec. 1865. [372] E. Schrodinger, An undulatory theory of the mechanics of atoms and molecules, Phys. Rev., vol. 28, pp. 10491070, Dec 1926. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRev.28.1049 [373] A. L. Hodgkin and A. F. Huxley, quantitative description of membrane current and its application to conduction and excitation in nerve, The Journal of physiology, vol. 117, no. 4, p. 500, 1952. [374] F. Crick, Central dogma of molecular biology, Nature, vol. 227, no. 5258, pp. 561563, 1970. [375] F. J. Vine and D. H. Matthews, Magnetic anomalies over oceanic ridges. Nature Publishing, 1963. [376] S. Weinberg, model of leptons, Physical review letters, vol. 19, no. 21, p. 1264, 1967. [377] C. Linnaeus, Systema Naturae per regna tria naturae, secundum classes, ordines, genera, species; cum characteribus, differentiis, synonymis, locis. apud JB Delamolliere, 1789, vol. 1. [378] D. Mendeleev, On the relationship of the properties of the elements to their atomic weights, Zeitschrift fur Chemie, vol. 12, pp. 405406, 1869. [379] A.-L. Barabasi and Z. N. Oltvai, Network biology: understanding the cells functional organization, Nature reviews genetics, vol. 5, no. 2, pp. 101113, 2004. [380] H. Jeong, B. Tombor, R. Albert, Z. N. Oltvai, and A.-L. Barabasi, The large-scale organization of metabolic networks, Nature, vol. 407, no. 6804, pp. 651654, 2000. [381] J. A. Dunne, R. J. Williams, and N. D. Martinez, Network structure and biodiversity loss in food webs: robustness increases with connectance, Ecology letters, vol. 5, no. 4, pp. 558567, 2002. [382] O. Sporns, G. Tononi, and R. Kotter, The human connectome: structural description of the human brain, PLoS computational biology, vol. 1, no. 4, p. e42, 2005. [383] W. M. Washington and C. Parkinson, Introduction to threedimensional climate modeling. University science books, 2005. [384] M. Karplus and J. A. McCammon, Molecular dynamics simulations of biomolecules, Nature structural biology, vol. 9, no. 9, pp. 646 652, 2002. [385] L. Excoffier, G. Laval, and S. Schneider, Arlequin (version 3.0): an integrated software package for population genetics data analysis, Evolutionary bioinformatics, vol. 1, p. 117693430500100003, 2005. [386] M. Rowland and T. N. Tozer, Clinical pharmacokinetics and pharmacodynamics: concepts and applications, (No Title), 2011. [387] J. P. Huelsenbeck, F. Ronquist, R. Nielsen, and J. P. Bollback, Bayesian inference of phylogeny and its impact on evolutionary biology, science, vol. 294, no. 5550, pp. 23102314, 2001. [388] N. Aghanim et al., Planck 2018 results. vi. cosmological parameters, Astron. Astrophys, vol. 641, p. A6, 2020. [389] J. A. Doudna and E. Charpentier, The new frontier of genome engineering with crispr-cas9, Science, vol. 346, no. 6213, p. 1258096, 2014. [390] M. H. Anderson, J. R. Ensher, M. R. Matthews, C. E. Wieman, and E. A. Cornell, Observation of bose-einstein condensation in dilute atomic vapor, Science, vol. 269, no. 5221, pp. 198201, 1995. [Online]. Available: https://www.science.org/doi/abs/10.1126/science. 269.5221.198 [391] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller, Equation of state calculations by fast computing machines, The journal of chemical physics, vol. 21, no. 6, pp. 10871092, 1953. [392] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. ˇZıdek, A. Potapenko et al., Highly accurate protein structure prediction with alphafold, nature, vol. 596, no. 7873, pp. 583589, 2021. [393] M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung, and W. Denk, Connectomic reconstruction of the inner plexiform layer in the mouse retina, Nature, vol. 500, no. 7461, pp. 168174, 2013. [394] G. Aad, T. Abajyan, B. Abbott, J. Abdallah, S. A. Khalek, A. A. Abdelalim, R. Aben, B. Abi, M. Abolins, O. AbouZeid et al., Observation of new particle in the search for the standard model higgs boson with the atlas detector at the lhc, Physics Letters B, vol. 716, no. 1, pp. 129, 2012. [395] E. Ruska, The development of the electron microscope and of electron microscopy, Reviews of modern physics, vol. 59, no. 3, p. 627, 1987. [396] T. Stuart and R. Satija, Integrative single-cell analysis, Nature reviews genetics, vol. 20, no. 5, pp. 257272, 2019. [397] K. E. Taylor, R. J. Stouffer, and G. A. Meehl, An overview of cmip5 and the experiment design, Bulletin of the American meteorological Society, vol. 93, no. 4, pp. 485498, 2012. [398] M. Vogelsberger, F. Marinacci, P. Torrey, and E. Puchwein, Cosmological simulations of galaxy formation, Nature Reviews Physics, vol. 2, no. 1, pp. 4266, 2020. [399] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational physics, vol. 378, pp. 686707, 2019. [400] S. Cai, Z. Mao, Z. Wang, M. Yin, and G. E. Karniadakis, Physicsinformed neural networks (pinns) for fluid mechanics: review, Acta Mechanica Sinica, vol. 37, no. 12, pp. 17271738, 2021. [401] H. Kitano, Systems biology: brief overview, science, vol. 295, no. 5560, pp. 16621664, 2002. [402] G. Sliwoski, S. Kothiwale, J. Meiler, and E. W. Lowe Jr, Computational methods in drug discovery, Pharmacological reviews, vol. 66, no. 1, pp. 334395, 2014. [403] H. Markram, E. Muller, S. Ramaswamy, M. W. Reimann, M. Abdellah, C. A. Sanchez, A. Ailamaki, L. Alonso-Nanclares, N. Antille, S. Arsever et al., Reconstruction and simulation of neocortical microcircuitry, Cell, vol. 163, no. 2, pp. 456492, 2015. [404] S. Asseng, F. Ewert, C. Rosenzweig, J. W. Jones, J. L. Hatfield, A. C. Ruane, K. J. Boote, P. J. Thorburn, R. P. Rotter, D. Cammarano et al., Uncertainty in simulating wheat yields under climate change, Nature climate change, vol. 3, no. 9, pp. 827832, 2013. [405] W. L. Oberkampf and C. J. Roy, Verification and validation in scientific computing. Cambridge university press, 2010. [406] C. E. Shannon, mathematical theory of communication, The Bell system technical journal, vol. 27, no. 3, pp. 379423, 1948. [407] J. D. Watson and F. H. Crick, Molecular structure of nucleic acids: structure for deoxyribose nucleic acid, Nature, vol. 171, no. 4356, pp. 737738, 1953. [408] S. Ogawa, T.-M. Lee, A. R. Kay, and D. W. Tank, Brain magnetic resonance imaging with contrast dependent on blood oxygenation. proceedings of the National Academy of Sciences, vol. 87, no. 24, pp. 98689872, 1990. [409] E. M. Burbidge, G. R. Burbidge, W. A. Fowler, and F. Hoyle, Synthesis of the elements in stars, Reviews of modern physics, vol. 29, no. 4, p. 547, 1957. [410] K. A. Dill and H. S. Chan, From levinthal to pathways to funnels, Nature structural biology, vol. 4, no. 1, pp. 1019, 1997. [411] R. K. Varshney, W. Chen, Y. Li, A. K. Bharti, R. K. Saxena, J. A. Schlueter, M. T. Donoghue, S. Azam, G. Fan, A. M. Whaley et al., Draft genome sequence of pigeonpea (cajanus cajan), an orphan legume crop of resource-poor farmers, Nature biotechnology, vol. 30, no. 1, p. 83, 2012. [412] M. Planck, Ueber das gesetz der energieverteilung im normalspectrum, Ann. Phys., vol. 309, no. 3, pp. 553563, Jan. 1901. 77 [413] D. Baltimore, Viral rna-dependent dna polymerase: Rna-dependent dna polymerase in virions of rna tumour viruses, Nature, vol. 226, no. 5252, pp. 12091211, 1970. [414] V. C. Rubin, W. K. Ford Jr, and N. Thonnard, Rotational properties of 21 sc galaxies with large range of luminosities and radii, from ngc 4605/r= 4kpc/to ugc 2885/r= 122 kpc, Astrophysical Journal, Part 1, vol. 238, June 1, 1980, p. 471-487., vol. 238, pp. 471487, 1980. [415] M. Boolell, S. Gepi-Attee, J. Gingell, and M. Allen, Sildenafil, novel effective oral therapy for male erectile dysfunction, British journal of urology, vol. 78, no. 2, pp. 257261, 1996. [416] C. Darwin, J. W. Burrow, and J. W. Burrow, The origin of species by means of natural selection: or, the preservation of favored races in the struggle for life. AL Burt New York, 2009. [417] A. Wegener and A. Vogel, Die entstehung der kontinente und ozeane. Walter de Gruyter GmbH & Co KG, 1980. [418] B. P. Abbott, R. Abbott, T. D. Abbott, M. R. Abernathy, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari et al., Observation of gravitational waves from binary black hole merger, Physical review letters, vol. 116, no. 6, p. 061102, 2016. [419] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in Neural Information Processing systems, vol. 35, pp. 24 82424 837, 2022. [420] Y. Xia, R. Wang, X. Liu, M. Li, T. Yu, X. Chen, J. McAuley, and S. Li, Beyond chain-of-thought: survey of chain-of-x paradigms for llms, arXiv preprint arXiv:2404.15676, 2024. [421] A. Fallahpour, A. Magnuson, P. Gupta, S. Ma, J. Naimer, A. Shah, H. Duan, O. Ibrahim, H. Goodarzi, C. J. Maddison et al., Bioreason: Incentivizing multimodal biological reasoning within dna-llm model, arXiv preprint arXiv:2505.23579, 2025. [422] NIH, Advancing health research through multimodal ai, https://en. wikibooks.org/wiki/LaTeX/Bibliography Management, 2025. [423] J. Storey, M. Choate, and K. Lee, Landsat 8 operational land imager on-orbit geometric calibration and performance, Remote sensing, vol. 6, no. 11, pp. 11 12711 152, 2014. [424] Z. Cheng, J. Caverlee, and K. Lee, You are where you tweet: content-based approach to geo-locating twitter users, in Proceedings of the 19th ACM international conference on Information and knowledge management, 2010, pp. 759768. [425] C. Batini and M. Scannapieca, Data quality: concepts, methodologies and techniques. Springer, 2006. [426] L. L. Pipino, Y. W. Lee, and R. Y. Wang, Data quality assessment, Communications of the ACM, vol. 45, no. 4, pp. 211218, 2002. [427] H. Li, B. Handsaker, A. Wysoker, T. Fennell, J. Ruan, N. Homer, G. Marth, G. Abecasis, R. Durbin, and . G. P. D. P. Subgroup, The sequence alignment/map format and samtools, bioinformatics, vol. 25, no. 16, pp. 20782079, 2009. [428] D. Sims, I. Sudbery, N. E. Ilott, A. Heger, and C. P. Ponting, Sequencing depth and coverage: key considerations in genomic analyses, Nature Reviews Genetics, vol. 15, no. 2, pp. 121132, 2014. [429] F. Ren, L. Ward, T. Williams, K. J. Laws, C. Wolverton, J. HattrickSimpers, and A. Mehta, Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments, Science advances, vol. 4, no. 4, p. eaaq1566, 2018. [430] G. Tang, M. P. Clark, A. J. Newman, A. W. Wood, S. M. Papalexiou, V. Vionnet, and P. H. Whitfield, SCDNA: serially complete precipitation and temperature dataset for north america from 1979 to 2018, Earth System Science Data, vol. 12, pp. 23812409, 2020. [431] A. Delpeuch, T. Morris, D. Huynh, W. (bot), S. Mazzocchi, Jacky, T. Guidry, elebitzero, O. Stephens, I. Matsunami, A. Larsson, I. Sproat, S. Santos, A. Mayer, kushthedude, L. M. [Sannita], S. Fauconnier, E. Mishra, M. Magdinier, A. Beaubien, L. Liu, F. Giroud, J. Ong, F. Tacchelli, A. Nordhøy, E. Kanye, Y. Shahrabani, and M. Saby, Openrefine/openrefine: Openrefine 3.9.3, https://doi.org/ 10.5281/zenodo.15236589, Apr. 2025, version 3.9.3. [432] DataCleaner contributors, Datacleaner: The premier open source data quality solution, https://github.com/datacleaner/DataCleaner, 2024, version 5.9.0 (released 2024-11-16); LGPL-3.0; Accessed 2025-0812. [433] E. Dong, H. Du, and L. Gardner, An interactive web-based dashboard to track covid-19 in real time, The Lancet infectious diseases, vol. 20, no. 5, pp. 533534, 2020. [434] S. Lewis, Remote sensing for natural disasters: Facts and figures, SciDev. net-Environment, 2009. [435] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition. pp. 248255. Ieee, 2009, [436] D. Vrandeˇcic and M. Krotzsch, Wikidata: free collaborative knowledgebase, Communications of the ACM, vol. 57, no. 10, pp. 7885, 2014. [437] T. C. Redman, The impact of poor data quality on the typical enterprise, Communications of the ACM, vol. 41, no. 2, pp. 7982, 1998. [438] OpenAIRE, Openaire, https://www.openaire.eu/, 2025, accessed 2025-08-12. [439] V. Krotov, L. Johnson, and L. Silva, Tutorial: Legality and ethics of web scraping, Communications of the Association for Information Systems, vol. 47, 2020. [440] X. Liang, S. Shetty, D. Tosh, C. Kamhoua, K. Kwiat, and L. Njilla, Provchain: blockchain-based data provenance architecture in cloud environment with enhanced privacy and availability, in 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID). IEEE, 2017, pp. 468477. [441] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang, Scibench: Evaluating college-level scientific problem-solving abilities of large language models, arXiv preprint arXiv:2307.10635, 2023. [442] K. Feng, K. Ding, W. Wang, X. Zhuang, Z. Wang, M. Qin, Y. Zhao, J. Yao, Q. Zhang, and H. Chen, Sciknoweval: Evaluating multilevel scientific knowledge of large language models, arXiv preprint arXiv:2406.09098, 2024. [443] Y. Zhou, Y. Wang, X. He, R. Xiao, Z. Li, Q. Feng, Z. Guo, Y. Yang, H. Wu, W. Huang et al., Scientists first exam: Probing cognitive abilities of mllm via perception, understanding, and reasoning, arXiv preprint arXiv:2506.10521, 2025. [444] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan et al., Language models are few-shot learners, in Advances in Neural Information Processing Systems, 2020. [445] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. AlDahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [446] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in neural information processing systems, 2017, pp. 5998 6008. [447] S. Eger, Y. Cao, J. DSouza, A. Geiger, C. Greisinger, S. Gross, Y. Hou, B. Krenn, A. Lauscher, Y. Li et al., Transforming science with large language models: survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation, arXiv preprint arXiv:2502.05151, 2025. [448] T. Xie, Y. Wan, W. Huang, Z. Yin, Y. Liu, S. Wang, Q. Linghu, C. Kit, C. Grazian, W. Zhang et al., Darwin series: Domain specific large language models for natural science, arXiv preprint arXiv:2308.13565, 2023. [449] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, Pubmedqa: research question answering, arXiv, [Online]. Available: https://arxiv.org/abs/ for biomedical dataset abs/1909.06146, 2019. 1909.06146 [450] A. Pal, L. K. Umapathi, and M. Sankarasubbu, Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering, arXiv, abs/2203.14371, 2022. [Online]. Available: https://arxiv.org/abs/2203.14371 [451] L. Sun, D. Luo, D. Ma, Z. Zhao, B. Chen, Z. Shen, S. Zhu, L. Chen, X. Chen, and K. Yu, Scidfm: large language model with mixtureof-experts for science, arXiv preprint arXiv:2409.18412, 2024. [452] V. Prabhakar, M. A. Islam, A. Atanas, Y.-T. Wang, J. Han, A. Jhunjhunwala, R. Apte, R. Clark, K. Xu, Z. Wang et al., Omniscience: domain-specialized llm for scientific reasoning and discovery, arXiv preprint arXiv:2503.17604, 2025. [453] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Cand`es, and T. Hashimoto, s1: Simple test-time scaling, arXiv preprint arXiv:2501.19393, 2025. [454] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney et al., Openai o1 system card, 2024. [455] J. R. Platt, Strong inference: Certain systematic methods of scientific thinking may produce much more rapid progress than others. science, vol. 146, no. 3642, pp. 347353, 1964. [456] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. 78 [457] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, Gpqa: graduate-level google-proof q&a benchmark, Nov. 2023. [458] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. [459] K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen et al., Kimi k2: Open agentic intelligence, arXiv preprint arXiv:2507.20534, 2025. [460] G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen et al., Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, arXiv preprint arXiv:2507.06261, 2025. [461] xAI, Grok 4, 2025. [Online]. Available: https://x.ai/news/grok-4 [462] L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, C. B. C. Zhang, M. Shaaban, J. Ling, S. Shi et al., Humanitys last exam, arXiv preprint arXiv:2501.14249, 2025. [463] A. Cherian, R. Corcodel, S. Jain, and D. Romeres, LLMPhy: Complex physical reasoning using large language models and world models, arXiv preprint arXiv:2411.08027, 2024. [464] M. Herde, B. Raonic, T. Rohner, R. Kappeli, R. Molinaro, E. de Bezenac, and S. Mishra, Poseidon: Efficient foundation models for PDEs, Advances in Neural Information Processing Systems, vol. 37, pp. 72 52572 624, 2024. [465] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao, Z. Zhang, L. Dong et al., Swin transformer v2: Scaling up capacity and resolution, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 12 00912 019. [467] J. Zou, W. Li, Q. Ma, Z. You, S. Sun, Z. Deng, X. [466] Z. Zhang, Y. Zhang, H. Yao, J. Luo, R. Zhao, B. Huang, J. Zhao, Y. Liao, K. Li, L. Zhao et al., Xiwu: basis flexible and learnable llm for high energy physics, arXiv preprint arXiv:2404.08001, 2024. Ji, A. Zhemchugov, W. Fang, C. Fu, K. He, X. Huang, T. Lin, C. Liu, H. Liu, Z. Mao, J. Qiu, Y. Sun, S. Wen, L. Wu, L. Wang, Y. Yuan, Y. Zhang, X. Zhang, and G. Zhao, Offline data processing system of the besiii experiment, The European Physical Journal C, vol. 84, no. 9, p. 937, 2024. [Online]. Available: https://doi.org/10.1140/epjc/s10052-024-13241-3 [468] H. Cao, Z. Liu, X. Lu, Y. Yao, and Y. Li, Instructmol: Multi-modal integration for building versatile and reliable molecular assistant in drug discovery, arXiv preprint arXiv:2311.16208, 2023. [469] Z. Zhao, D. Ma, L. Chen, L. Sun, Z. Li, Y. Xia, B. Chen, H. Xu, Z. Zhu, S. Zhu et al., Chemdfm: large language foundation model for chemistry, arXiv preprint arXiv:2401.14818, 2024. [470] L. Jiang, S. Sun, B. Qi, Y. Fu, X. Xu, Y. Li, D. Zhou, and T. Fu, Chem3dllm: 3d multimodal large language models for chemistry, arXiv preprint, 2025. [471] S. Wang, Y. Guo, Y. Wang, H. Sun, and J. Huang, Smiles-bert: scale unsupervised pre-training for molecular property Large the 10th ACM International prediction, Conference on Bioinformatics, Computational Biology and Health Informatics, ser. BCB 19. New York, NY, USA: Association for Computing Machinery, 2019, p. 429436. [Online]. Available: https://doi.org/10.1145/3307339.3342186 in Proceedings of [472] C. Kuenneth and R. Ramprasad, polybert: chemical language model to enable fully machine-driven ultrafast polymer informatics, Nature Communications, vol. 14, no. 1, [Online]. Jul. 2023. Available: http://dx.doi.org/10.1038/s41467-023-39868-6 [473] P. He, X. Liu, J. Gao, and W. Chen, Deberta: Decoding-enhanced bert with disentangled attention, arXiv preprint arXiv:2006.03654, 2020. [474] V. Korolev and P. Protsenko, Accurate, interpretable predictions of materials properties within transformer language models, Patterns, vol. 4, no. 10, p. 100803, Oct. 2023. [Online]. Available: http://dx.doi.org/10.1016/j.patter.2023.100803 Regression and M. Manica, sequence concurrent language modelling, Nature Machine no. 2023. http://dx.doi.org/10.1038/s42256-023-00639-z enables regression and generation for molecular Intelligence, 5, [Online]. Available: 432444, Apr. [475] J. Born transformer vol. 4, p. [476] X. Bai, S. He, Y. Li, Y. Xie, X. Zhang, W. Du, and J.-R. Li, Construction of knowledge graph for framework material enabled by large language models and its application, npj Computational Materials, vol. 11, no. 1, p. 51, Feb 2025. [Online]. Available: https://doi.org/10.1038/s41524-025-01540-6 [477] Z. Liu, W. Zhang, Y. Xia, L. Wu, S. Xie, T. Qin, M. Zhang, and T.-Y. Liu, Molxpt: Wrapping molecules with text for generative pretraining, 2023. [Online]. Available: https://arxiv.org/abs/2305.10688 [478] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, Language models are unsupervised multitask learners, OpenAI Blog, 2019. [479] S. Balaji, R. Magar, Y. Jadhav, and A. B. Farimani, Gpt-molberta: for molecular property features language model [Online]. Available: https://arxiv.org/abs/2310. Gpt molecular prediction, 2023. 03030 [480] V. Bagal, R. Aggarwal, P. K. Vinod, and U. D. Priyakumar, MolGPT: Molecular Generation Using Transformer-Decoder Model, Journal of Chemical Information and Modeling, vol. 62, [Online]. Available: https: no. 9, pp. 20642076, May 2022. //doi.org/10.1021/acs.jcim.1c [481] D. Flam-Shepherd and A. Aspuru-Guzik, Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files, 2023. [Online]. Available: https://arxiv.org/abs/2305.05708 [482] L. M. Antunes, K. T. Butler, and R. Grau-Crespo, Crystal structure generation with autoregressive large language modeling, Nature Communications, vol. 15, no. 1, p. 10570, Dec 2024. [Online]. Available: https://doi.org/10.1038/s41467-024-54639-7 [483] R. Okabe, Z. West, A. Chotrattanapituk, M. Cheng, D. C. Carrizales, W. Xie, R. J. Cava, and M. Li, Large language model-guided prediction toward quantum materials synthesis, 2024. [Online]. Available: https://arxiv.org/abs/2410.20976 [484] Z. Song, S. Lu, M. large language model all you need to predict the synthesizability and precursors of crystal structures? 2024. [Online]. Available: https://arxiv.org/abs/2407.07016 Ju, Q. Zhou, and J. Wang, Is [485] M. J. Buehler, MechGPT, language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines, and modalities, Applied Mechanics Reviews, vol. 76, no. 2, p. 021001, 2024. [486] S. C. Tan and B. C. Yiap, Dna, rna, and protein extraction: the past and the present, BioMed Research International, vol. 2009, no. 1, p. 574398, 2009. [487] E. Nguyen, M. Poli, M. G. Durrant, B. Kang, D. Katrekar, D. B. Li, L. J. Bartie, A. W. Thomas, S. H. King, G. Brixi et al., Sequence modeling and design from molecular to genome scale with evo, Science, vol. 386, no. 6723, p. eado9336, 2024. [488] G. Brixi, M. G. Durrant, J. Ku, M. Poli, G. Brockman, D. Chang, G. A. Gonzalez, S. H. King, D. B. Li, A. T. Merchant et al., Genome modeling and design across all domains of life with evo 2, BioRxiv, pp. 202502, 2025. [489] A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin et al., Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences, Proceedings of the National Academy of Sciences, vol. 118, no. 15, 2021. [490] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido et al., Language models of protein sequences at the scale of evolution enable accurate structure prediction, bioRxiv, 2022. [491] T. Hayes, R. Rao, H. Akin, N. J. Sofroniew, D. Oktay, Z. Lin, R. Verkuil, V. Q. Tran, J. Deaton, M. Wiggert, R. Badkundri, I. Shafkat, J. Gong, A. Derry, R. S. Molina, N. Thomas, Y. A. Khan, C. Mishra, C. Kim, L. J. Bartie, M. Nemeth, P. D. Hsu, T. Sercu, S. Candido, and A. Rives, Simulating 500 million years of evolution with language model, Science, vol. 387, no. 6736, pp. 850858, 2025. [Online]. Available: https://www.science.org/doi/10.1126/science.ads0018 [492] R. Rao, J. Liu, R. Verkuil, J. Meier, J. F. Canny, P. Abbeel, T. Sercu, the 38th and A. Rives, Msa transformer, in Proceedings of International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 1824 Jul 2021, pp. 88448856. [Online]. Available: https://proceedings.mlr.press/v139/rao21a.html [493] J. Meier, R. Rao, R. Verkuil, Language models J. Liu, T. Sercu, effects of mutations on protein function, and zero-shot prediction of A. Rives, in Advances the in Neural 2021. [Online]. Available: https://proceedings.neurips.cc/paper/2021/hash/ f51338d736f95dd42427296047067694-Abstract.html"
        },
        {
            "title": "Information Processing",
            "content": "Systems, enable vol. 34, [494] C. Hsu, R. Verkuil, J. Liu, Z. Lin, B. Hie, T. Sercu, A. Lerer, and A. Rives, Learning inverse folding from millions of predicted structures, in Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 1723 Jul 2022, pp. 89468970. [Online]. Available: https://proceedings.mlr.press/v162/hsu22a.html [495] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. Dos Santos Costa, M. FazelZarandi, T. Sercu, S. Candido, and A. Rives, Evolutionary-scale prediction of atomic-level protein structure with language model, Science, vol. 379, no. 6637, pp. 11231130, 2023. [Online]. Available: https://www.science.org/doi/10.1126/science.ade2574 [496] N. Ferruz, S. Schmidt, and B. Hocker, Protgpt2 is deep unsupervised language model for protein design, Nature communications, vol. 13, no. 1, p. 4348, 2022. [497] A. Madani, B. McCann, N. Naik, N. S. Keskar, N. Anand, R. R. Eguchi, P.-S. Huang, and R. Socher, Progen: Language modeling for protein generation, arXiv preprint arXiv:2004.03497, 2020. [498] E. Nijkamp, J. A. Ruffolo, E. N. Weinstein, N. Naik, and A. Madani, Progen2: Exploring the boundaries of protein language models, Cell Systems, vol. 14, no. 11, pp. 968978, 2023. [499] A. Elnaggar, H. Essam, W. Salah-Eldin, W. Moustafa, M. Elkerdawy, C. Rochereau, and B. Rost, Ankh: Optimized protein language model unlocks general-purpose modelling, arXiv preprint, 2023. [500] L. Lv, Z. Lin, H. Li, Y. Liu, J. Cui, C. Y. Chen, L. Yuan, and Y. Tian, Prollama: protein language model for multi-task protein language processing, arXiv preprint, 2024. [501] S. Liu, Y. Li, Z. Li, A. Gitter, Y. Zhu, J. Lu, Z. Xu, W. Nie, A. Ramanathan, C. Xiao, J. Tang, H. Guo, and A. Anandkumar, text-guided protein design framework, arXiv preprint, 2023, v4, 2025. [502] C. Yuan, S. Li, G. Ye, Y. Zhang, L. Huang, W. Huang, W. Liu, J. Yao, and Y. Rong, Annotation-guided protein design with multilevel domain alignment, arXiv preprint, 2024. [503] F. Dai et al., Toward de novo protein design from natural language, bioRxiv, 2025. [Online]. Available: https://www.biorxiv.org/content/ 10.1101/2024.08.01.606258v4 [504] Y. Xiao, E. Sun, Y. Jin, Q. Wang, and W. Wang, Proteingpt: Multimodal llm for protein property prediction and structure understanding, arXiv preprint, 2024, v2, 2025. [505] C. Wang, H. Fan, R. Quan, and Y. Yang, Protchatgpt: Towards understanding proteins with large language models, arXiv preprint, 2024, v2, 2025. [506] H. Xu and S. Wang, Protranslator: Zero-shot protein function prediction using textual description, arXiv preprint, 2022. [507] H. Xu, A. Woicik, R. B. Altman, H. Poon, and S. Wang, Multilingual translation for zero-shot biomedical classification using biotranslator, Nature Communications, vol. 14, 2023. [Online]. Available: https://www.nature.com/articles/s41467-023-36476-2 [508] H. Abdine, M. Chatzianastasis, C. Bouyioukos, and M. Vazirgiannis, Prot2text: Multimodal proteins function generation with gnns and transformers, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 10, 2024, pp. 10 75710 765. [509] X. Zhou, C. Han, Y. Zhang, J. Su, K. Zhuang, S. Jiang, Z. Yuan, W. Zheng, F. Dai, Y. Zhou et al., Decoding the molecular language of proteins with evolla, bioRxiv, pp. 202501, 2025. [510] B. P. de Almeida, G. Richard, H. Dalla-Torre, C. Blum, L. Hexemer, P. Pandey, S. Laurent, C. Rajesh, M. Lopez, A. Laterre et al., multimodal conversational agent for dna, rna and protein tasks, Nature Machine Intelligence, pp. 114, 2025. [511] Y. Liu, S. Ding, S. Zhou, W. Fan, and Q. Tan, Moleculargpt: Open large language model (llm) for few-shot molecular property prediction, arXiv preprint arXiv:2406.12950, 2024. [512] H. Cui, C. Wang, H. Maan, K. Pang, F. Luo, N. Duan, and B. Wang, scgpt: toward building foundation model for single-cell multi-omics using generative ai, Nature methods, vol. 21, no. 8, pp. 14701480, 2024. [513] S. Chithrananda, G. Grand, and B. Ramsundar, ChemBERTa: largescale self-supervised pretraining for molecular property prediction, in Machine Learning for Molecules Workshop at NeurIPS 2020, 2020. [514] J. Li and X. Jiang, Mol-bert: An effective molecular representation with bert for molecular property prediction, Wireless Communications and Mobile Computing, vol. 2021, no. 1, p. 7181815, 2021. [515] K. Hatakeyama-Sato, N. Yamane, Y. Igarashi, Y. Nabae, and T. Hayakawa, Prompt engineering of gpt-4 for chemical research: what can/cannot be done? Science and Technology of Advanced Materials: Methods, vol. 3, no. 1, p. 2260300, 2023. [516] S. Jiang, Y. Wang, S. Song, Y. Zhang, Z. Meng, B. Lei, J. Wu, J. Sun, and Z. Liu, Omniv-med: Scaling medical vision-language model for universal visual understanding, arXiv preprint arXiv:2504.14692, 2025. [517] Y. Labrak, A. Bazoge, E. Morin, P.-A. Gourraud, M. Rouvier, and R. Dufour, Biomistral: collection of open-source pretrained large language models for medical domains, 2024. [518] E. Bolton, A. Venigalla, M. Yasunaga, D. Hall, B. Xiong, T. Lee, R. Daneshjou, J. Frankle, P. Liang, M. Carbin, and C. D. Manning, Biomedlm: 2.7b parameter language model trained on biomedical text, 2024. [Online]. Available: https://arxiv.org/abs/2403.18421 [519] A. Toma, P. R. Lawler, J. Ba, R. G. Krishnan, B. B. Rubin, and B. Wang, Clinical camel: An open expert-level medical language model with dialogue-based knowledge encoding, arXiv preprint arXiv:2305.12031, 2023. [520] T. Han, L. C. Adams, J.-M. Papaioannou, P. Grundmann, T. Oberhauser, A. Loser, D. Truhn, and K. K. Bressem, Medalpacaan opensource collection of medical conversational ai models and training data, arXiv preprint arXiv:2304.08247, 2023. [521] D. Jin, E. Pan, N. Oufattole, W. Weng, H. Fang, P. Szolovits, What disease does scale open domain question answering dataset exams, https://arxiv.org/abs/2009.13081 abs/2009.13081, arXiv, 2020. and this patient have? largefrom medical [Online]. Available: [522] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, Pretrain, prompt, and predict: systematic survey of prompting methods in natural language processing, ACM Comput. Surv., vol. 55, no. 9, Jan. 2023. [Online]. Available: https://doi.org/10.1145/3560815 [523] X. Wang, N. Chen, J. Chen, Y. Hu, Y. Wang, X. Wu, A. Gao, X. Wan, H. Li, and B. Wang, Apollo: Lightweight multilingual medical llms towards democratizing medical ai to 6b people, 2024. [524] H. Zhang, J. Chen, F. Jiang, F. Yu, Z. Chen, J. Li, G. Chen, X. Wu, Z. Zhang, Q. Xiao, X. Wan, B. Wang, and H. Li, Huatuogpt, towards taming language models to be doctor, arXiv preprint arXiv:2305.15075, 2023. [525] C. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, Pmcllama: Towards building open-source language models for medicine, 2023. [Online]. Available: https://arxiv.org/abs/2304.14454 [526] Y. Tian, R. Gan, Y. Song, J. Zhang, and Y. Zhang, Chimed-gpt: chinese medical large language model with full training regime and better alignment to human preferences, arXiv preprint arXiv:2311.06025, 2023. [527] J. Zhang, R. Gan, J. Wang, Y. Zhang, L. Zhang, P. Yang, X. Gao, Z. Wu, X. Dong, J. He, J. Zhuo, Q. Yang, Y. Huang, X. Li, Y. Wu, J. Lu, X. Zhu, W. Chen, T. Han, K. Pan, R. Wang, H. Wang, X. Wu, Z. Zeng, and C. Chen, Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence, CoRR, vol. abs/2209.02970, 2022. [528] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan et al., Training helpful and harmless assistant with reinforcement learning from human feedback, arXiv preprint arXiv:2204.05862, 2022. [529] S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y. Jia, and H. Zan, Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 17, 2024, pp. 19 36819 376. [530] Q. Xie, Q. Chen, A. Chen, C. Peng, Y. Hu, F. Lin, X. Peng, J. Huang, J. Zhang, V. Keloth, X. Zhou, L. Qian, H. He, D. Shung, L. Ohno-Machado, Y. Wu, H. Xu, and J. Bian, Me llama: Foundation large language models for medical applications, 2024. [Online]. Available: https://arxiv.org/abs/2402.12749 [531] B. Wang, H. Zhao, H. Zhou, L. Song, M. Xu, W. Cheng, X. Zeng, Y. Zhang, Y. Huo, Z. Wang et al., Baichuan-m1: Pushing the medical capability of large language models, arXiv preprint arXiv:2502.12671, 2025. [532] J. Liu, Y. Wang, J. Du, J. T. Zhou, and Z. Liu, Medcot: Medical chain of thought via hierarchical expert, arXiv preprint arXiv:2412.13736, 2024. [533] G. Reale-Nosei, E. Amador-Domınguez, and E. Serrano, From vision to text: comprehensive review of natural image captioning in medical diagnosis and radiology report generation, Medical Image Analysis, vol. 97, p. 103264, 2024. [534] Z. Lin, D. Zhang, Q. Tao, D. Shi, G. Haffari, Q. Wu, M. He, and Z. Ge, Medical visual question answering: survey, Artificial Intelligence in Medicine, vol. 143, p. 102611, 2023. [535] Y. Wang, J. Liu, S. Gao, B. Feng, Z. Tang, X. Gai, J. Wu, and Z. Liu, V2t-cot: From vision to text chain-of-thought for medical reasoning and diagnosis, arXiv preprint arXiv:2506.19610, 2025. 80 [536] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, Llava-med: Training large languageand-vision assistant for biomedicine in one day, arXiv preprint arXiv:2306.00890, 2023. [537] S. Lee, J. Youn, H. Kim, M. Kim, and S. H. Yoon, Cxr-llava: multimodal large language model for interpreting chest x-ray images, 2024. [Online]. Available: https://arxiv.org/abs/2310.18341 [538] Z. Liu, Y. Li, P. Shu, A. Zhong, L. Yang, C. Ju, Z. Wu, C. Ma, J. Luo, C. Chen, S. Kim, J. Hu, H. Dai, L. Zhao, D. Zhu, J. Liu, W. Liu, D. Shen, T. Liu, Q. Li, and X. Li, Radiology-llama2: Best-in-class large language model for radiology, 2023. [Online]. Available: https://arxiv.org/abs/2309. [539] M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec, Med-flamingo: multimodal medical few-shot learner, arXiv preprint arXiv:2307.15189, 2023. [540] J. Chen, C. Gui, R. Ouyang, A. Gao, S. Chen, G. H. Chen, Ji, G. Yu, X. Wan, and X. Wang, R. Zhang, Z. Cai, K. B. Wang, Huatuogpt-vision, injecting medical visual knowledge into multimodal llms at scale, 2024. [Online]. Available: https://arxiv.org/abs/2406.19280 towards [541] T. Li, Y. Su, W. Li, B. Fu, Z. Chen, Z. Huang, G. Wang, C. Ma, Y. Chen, M. Hu, Y. Li, P. Chen, X. Hu, Z. Deng, Y. Ji, J. Ye, Y. Qiao, and J. He, GMAI-VL & GMAI-VL-5.5m: large visionlanguage model and comprehensive multimodal dataset towards general medical ai, arXiv preprint arXiv:2411.14522, 2025. [542] A. Sellergren, S. Kazemzadeh, T. Jaroensri, A. Kiraly, M. Traverse, T. Kohlberger, S. Xu, F. Jamil, C. Hughes, C. Lau et al., Medgemma technical report, arXiv preprint arXiv:2507.05201, 2025. [543] J. Chen, Z. Cai, K. Ji, X. Wang, W. Liu, R. Wang, J. Hou, and B. Wang, Huatuogpt-o1, towards medical complex reasoning with llms, arXiv preprint arXiv:2412.18925, 2024. [544] H. Xu, Y. Nie, H. Wang, Y. Chen, W. Li, J. Ning, L. Liu, H. Wang, L. Zhu, J. Liu et al., Medground-r1: Advancing medical image grounding via spatial-semantic rewarded group relative policy optimization, arXiv preprint arXiv:2507.02994, 2025. [545] Y. Su, T. Li, J. Liu, C. Ma, J. Ning, C. Tang, S. Ju, J. Ye, P. Chen, M. Hu et al., GMAI-VL-R1: Harnessing reinforcement learning for multimodal medical reasoning, arXiv preprint arXiv:2504.01886, 2025. [546] F. Yang, H. Kong, J. Ying, Z. Chen, T. Luo, W. Jiang, Z. Yuan, Z. Wang, Z. Ma, S. Wang et al., Seedllm rice: large language model integrated with rice biological knowledge graph, Molecular Plant, 2025. [547] G. Penedo, H. Kydlıˇcek, L. B. allal, A. Lozhkov, M. Mitchell, C. Raffel, L. Von Werra, and T. Wolf, The FineWeb datasets: Decanting the web for the finest text data at scale, in Advances in Neural Information Processing Systems, vol. 37. Curran Associates, Inc., 2024, pp. 30 81130 849. [548] S. Huang, T. Cheng, J. K. Liu, W. Xu, J. Hao, L. Song, Y. Xu, J. Yang, J. Liu, C. Zhang, L. Chai, R. Yuan, X. Luo, Q. Wang, Y. Fan, Q. Zhu, Z. Zhang, Y. Gao, J. Fu, Q. Liu, H. Li, G. Zhang, Y. Qi, X. Yinghui, W. Chu, and Z. Wang, OpenCoder: The open cookbook for toptier code large language models, in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025, pp. 33 16733 193. [549] Z. Chen, W. Jiang, J. Li, Z. Yuan, H. Kong, W. Ouyang, and N. Dong, Graphgen: Enhancing supervised fine-tuning for llms with knowledge-driven synthetic data generation, arXiv preprint arXiv:2505.20416, 2025. [550] X. Yang, J. Gao, W. Xue, and E. Alexandersson, Pllama: An open-source large language model for plant science, arXiv preprint arXiv:2401.01600, 2024. [551] M. Awais, A. H. S. A. Alharthi, A. Kumar, H. Cholakkal, and R. M. Anwer, Agrogpt: Efficient agricultural vision-language model with expert tuning, in 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 2025, pp. 56875696. [552] X. Luo, A. Rechardt, G. Sun, K. K. Nejad, F. Yanez, B. Yilmaz, K. Lee, A. O. Cohen, V. Borghesani, A. Pashkov et al., Large language models surpass human experts in predicting neuroscience results, Nature human behaviour, vol. 9, no. 2, pp. 305315, 2025. [553] J. W. Kim, A. Alaa, and D. Bernardo, EEG-GPT: Exploring capabilities of large language models for EEG classification and interpretation, arXiv preprint arXiv:2401.18006, 2024. [554] W.-B. Jiang, Y. Wang, B.-L. Lu, and D. Li, Neurolm: universal multi-task foundation model for bridging the gap between language and eeg signals, arXiv preprint arXiv:2409.00101, 2024. [555] W. Xia, R. de Charette, C. Oztireli, and J.-H. Xue, Umbrae: Unified multimodal brain decoding, in European Conference on Computer Vision. Springer, 2024, pp. 242259. [556] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 1824 Jul 2021, pp. 87488763. [Online]. Available: https://proceedings.mlr.press/v139/radford21a.html [557] J. Chen, Y. Qi, Y. Wang, and G. Pan, Mindgpt: Interpreting what you see with non-invasive brain recordings, IEEE Transactions on Image Processing, 2025. [558] W. Qiu, Z. Huang, H. Hu, A. Feng, Y. Yan, and R. Ying, Mindllm: subject-agnostic and versatile model for fmri-to-text decoding, arXiv preprint arXiv:2502.15786, 2025. [559] W. Lu, C. Song, J. Wu, P. Zhu, Y. Zhou, W. Mai, Q. Zheng, and W. Ouyang, Unimind: Unleashing the power of llms for unified multi-task brain decoding, arXiv preprint arXiv:2506.18962, 2025. [560] W. Cui, W. Jeong, P. Tholke, T. Medani, K. Jerbi, A. A. Joshi, and R. M. Leahy, Neuro-gpt: Towards foundation model for eeg, in 2024 IEEE International Symposium on Biomedical Imaging (ISBI). IEEE, 2024, pp. 15. [561] T. D. Nguyen, Y.-S. Ting, I. Ciuca, C. ONeill, Z.-C. Sun, M. Jabłonska, S. Kruk, E. Perkowski, J. Miller, J. Li et al., Astrollama: Towards specialized foundation models in astronomy, arXiv preprint arXiv:2309.06126, 2023. [562] S. Zaman, M. J. Smith, P. Khetarpal, R. Chakrabarty, M. Ginolfi, M. Huertas-Company, M. Jabłonska, S. Kruk, M. L. Lain, S. J. R. Mendez et al., Astrollava: towards the unification of astronomical data and natural language, arXiv preprint arXiv:2504.08583, 2025. [563] T. de Haan, Y.-S. Ting, T. Ghosal, T. D. Nguyen, A. Accomazzi, A. Wells, N. Ramachandra, R. Pan, and Z. Sun, Achieving gpt-4o level performance in astronomy with specialized 8b-parameter large language model, Scientific Reports, vol. 15, no. 1, p. 13751, 2025. [564] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. The International Conference on Learning Representations (ICLR), vol. 1, no. 2, p. 3, 2022. [565] H. W. Leung and J. Bovy, Deep learning of multi-element abundances from high-resolution spectroscopic data, Monthly Notices of the Royal Astronomical Society, vol. 483, no. 3, pp. 32553277, 2019. [566] T. de Haan, Y.-S. Ting, T. Ghosal, T. D. Nguyen, A. Accomazzi, E. Herron, V. Lama, R. Pan, A. Wells, and N. Ramachandra, Astromlab 4: Benchmark-topping performance in astronomy q&a with 70b-parameter domain-specialized reasoning model, arXiv preprint arXiv:2505.17592, 2025. [567] C. Deng, T. Zhang, Z. He, Q. Chen, Y. Shi, Y. Xu, L. Fu, W. Zhang, X. Wang, C. Zhou et al., K2: foundation language model for geoscience knowledge understanding and utilization, in Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024, pp. 161170. [568] Z. Chen, X. Wang, Y. Liao, M. Lin, and Y. Bai, Climatechat: Designing data and methods for instruction tuning llms to answer climate change queries, arXiv preprint arXiv:2506.13796, 2025. [569] Z. Bi, N. Zhang, Y. Xue, Y. Ou, D. Ji, G. Zheng, and H. Chen, Oceangpt: large language model for ocean science tasks, arXiv preprint arXiv:2310.02031, 2023. [570] H. Jiang, J. Yin, Q. Wang, J. Feng, and G. Chen, Eaglevision: Objectlevel attribute multimodal llm for remote sensing, arXiv preprint arXiv:2503.23330, 2025. [571] W. Zhang, M. Cai, T. Zhang, Y. Zhuang, J. Li, and X. Mao, Earthmarker: visual prompting multi-modal large language model for remote sensing, IEEE Transactions on Geoscience and Remote Sensing, 2024. [572] A. Shabbir, M. Zumri, M. Bennamoun, F. S. Khan, and S. Khan, Geopixel: Pixel grounding large multimodal model in remote sensing, arXiv preprint arXiv:2501.13925, 2025. [573] F. Wang, M. Chen, Y. Li, D. Wang, H. Wang, Z. Guo, Z. Wang, B. Shan, L. Lan, Y. Wang et al., Geollava-8k: Scaling remote-sensing multimodal large language models to 8k resolution, arXiv preprint arXiv:2505.21375, 2025. [574] S. Soni, A. Dudhane, H. Debary, M. Fiaz, M. A. Munir, M. S. Danish, P. Fraccaro, C. D. Watson, L. J. Klein, F. S. Khan et al., Earthdial: Turning multi-sensory earth observations to interactive dialogues, 81 in Proceedings of Conference, 2025, pp. 14 30314 313. the Computer Vision and Pattern Recognition [575] D. Wang, M. Hu, Y. Jin, Y. Miao, J. Yang, Y. Xu, X. Qin, J. Ma, L. Sun, C. Li et al., Hypersigma: Hyperspectral intelligence comprehension foundation model, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [576] F. Wang, H. Wang, D. Wang, Z. Guo, Z. Zhong, L. Lan, W. Yang, and J. Zhang, Harnessing massive satellite imagery with efficient masked image modeling, arXiv preprint arXiv:2406.11933, 2024. [577] F. Wang, H. Wang, Y. Wang, D. Wang, M. Chen, H. Zhao, Y. Sun, S. Wang, L. Lan, W. Yang et al., Roma: Scaling up mamba-based foundation models for remote sensing, arXiv preprint arXiv:2503.10392, 2025. [578] J. A. Irvin, E. R. Liu, J. Kim, S. Khanna, Z. Zheng, and S. Ermon, Teochat: large visionlanguage assistant for temporal earth observation data, arXiv preprint arXiv:2410.06234, 2024. J. C. Chen, I. Dormoy, [579] E. Angeloudi, J. Audenaert, M. Bowles, B. M. Boyd, D. Chemaly, B. Cherinka, I. Ciuca, M. Cranmer, A. Do, M. Grayling et al., The multimodal universe: enabling large-scale machine learning with 100 tb of astronomical scientific data, Advances in Neural Information Processing Systems, vol. 37, pp. 57 84157 913, 2024. [580] C. O. de Burgh-Day and T. Leeuwenburg, Machine learning for numerical weather and climate modelling: review, Geoscientific Model Development, vol. 16, no. 22, pp. 64336477, 2023. [581] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., LLaMA 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [582] Q. Team, Qwen2 technical report, arXiv preprint arXiv:2407.10671, 2024. [583] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez et al., Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality, See https://vicuna. lmsys. org (accessed 14 April 2023), vol. 2, no. 3, p. 6, 2023. [584] H. Xiong, S. Wang, Y. Zhu, Z. Zhao, Y. Liu, L. Huang, Q. Wang, and D. Shen, Doctorglm: Fine-tuning your chinese doctor is not herculean task, arXiv preprint arXiv:2304.01097, 2023. [585] A. English and C. A. Ford, The hipaa privacy rule and adolescents: legal questions and clinical challenges, Perspectives on sexual and reproductive health, vol. 36, no. 2, pp. 8086, 2004. [586] M. Reichstein, G. Camps-Valls, B. Stevens, M. Jung, J. Denzler et al., Deep learning and process understanding for data-driven earth system science, Nature, vol. 566, no. 7743, pp. 195204, 2019. [587] Y. He, F. Huang, X. Jiang, Y. Nie, M. Wang, J. Wang, and H. Chen, Foundation model for advancing healthcare: Challenges, opportunities and future directions, IEEE Reviews in Biomedical Engineering, 2024. [588] K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and F. S. Khan, Geochat: Grounded large vision-language model for remote sensing, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 27 83127 840. [589] L. L. Wang, K. Lo, Y. Chandrasekhar, R. Reas, J. Yang, D. Burdick, D. Eide, K. Funk, Y. Katsis, R. M. Kinney, Y. Li, Z. Liu, W. Merrill, P. Mooney, D. A. Murdick, D. Rishi, J. Sheehan, Z. Shen, B. Stilson, A. D. Wade, K. Wang, N. X. R. Wang, C. Wilhelm, B. Xie, D. M. Raymond, D. S. Weld, O. Etzioni, and S. Kohlmeier, CORD-19: The COVID-19 open research dataset, in Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Online: Association for Computational Linguistics, Jul. 2020. [590] A. Algaba, V. Holst, F. Tori, M. Mobini, B. Verbeken, S. Wenmackers, and V. Ginis, How deep do large language models internalize scientific literature and citation practices? arXiv preprint arXiv:2504.02767, 2025. [591] X. Guan, Y. Liu, H. Lin, Y. Lu, B. He, X. Han, and L. Sun, Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 18 12618 134. [592] H. Zhang, R. Li, Y. Zhang, T. Xiao, J. Chen, J. Ding, and H. Chen, The evolving role of large language models in scientific innovation: Evaluator, collaborator, and scientist, arXiv preprint arXiv:2507.11810, 2025. [593] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang et al., Yi: Open foundation models by 01. ai, arXiv preprint arXiv:2403.04652, 2024. [594] Common Crawl Foundation, Common crawl: Open repository of web crawl data, https://commoncrawl.org/, 2008, accessed 2025-0828; large longitudinal web crawl (monthly snapshots) used widely in research. [595] A. P. Thompson, H. M. Aktulga, R. Berger, D. S. Bolintineanu, W. M. Brown, P. S. Crozier, P. J. in Veld, A. Kohlmeyer, S. G. Moore, T. D. Nguyen, R. Shan, M. J. Stevens, J. Tranchida, C. Trott, and S. J. Plimpton, LAMMPS - flexible simulation tool for particlebased materials modeling at the atomic, meso, and continuum scales, Comp. Phys. Comm., vol. 271, p. 108171, 2022. [596] M. Vogelsberger, S. Genel, V. Springel, P. Torrey, D. Sijacki, D. Xu, G. Snyder, D. Nelson, and L. Hernquist, Introducing the illustris project: simulating the coevolution of dark and visible matter in the universe, Monthly Notices of the Royal Astronomical Society, vol. 444, no. 2, pp. 15181547, 2014. [597] A. A. Klypin, S. Trujillo-Gomez, and J. Primack, Dark matter halos in the standard cosmological model: Results from the bolshoi simulation, The Astrophysical Journal, vol. 740, no. 2, p. 102, 2011. [598] G. L. Bryan, M. L. Norman, B. W. OShea, T. Abel, J. H. Wise, M. J. Turk, D. R. Reynolds, D. C. Collins, P. Wang, S. W. Skillman et al., Enzo: An adaptive mesh refinement code for astrophysics, The Astrophysical Journal Supplement Series, vol. 211, no. 2, p. 19, 2014. [599] CERN Open Data team, Cern open data portal, https://opendata. cern.ch, European Organization for Nuclear Research (CERN), 2014, petabyte-scale collider data with documentation. [600] LHCb Collaboration, The large hadron collider beauty (lhcb) experiment, https://home.cern/science/experiments/lhcb, March 2022, accessed 2025-08-28. [601] A. Damascelli, Z. Hussain, and Z.-X. Shen, Angle-resolved photoemission studies of the cuprate superconductors, Reviews of Modern Physics, vol. 75, no. 2, p. 473, 2003. [602] Alma science archive, https://almascience.nrao.edu/alma-data/ archive, 2025, accessed 2025-08-28. [603] D. Chapon and P. Hennebelle, The galactica database: an open, generic and versatile tool for the dissemination of simulation data in astrophysics, arXiv preprint arXiv:2411.08647, 2024. [604] L. Li, Y. Wang, R. Xu, P. Wang, X. Feng, L. Kong, and Q. Liu, Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models, arXiv preprint arXiv:2403.00231, 2024. [605] J. J. Irwin and B. K. Shoichet, Zinca free database of commercially available compounds for virtual screening, Journal of chemical information and modeling, vol. 45, no. 1, pp. 177182, 2005. [606] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy, M. Veselov et al., Molecular sets (MOSES): benchmarking platform for molecular generation models, Frontiers in pharmacology, 2020. [607] B. Xu, Y. Lu, C. Li, L. Yue, X. Wang, N. Hao, T. Fu, and J. Chen, Smiles-mamba: Chemical mamba foundation models for drug admet prediction, arXiv preprint arXiv:2408.05696, 2024. [608] T. Fu, W. Gao, C. Xiao, J. Yasonik, C. W. Coley, and J. Sun, Differentiable scaffolding tree for molecular optimization, International Conference on Learning Representations, 2022. [609] T. Fu, C. Xiao, X. Li, L. M. Glass, and J. Sun, MIMOSA: Multiconstraint molecule sampling for molecule optimization, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, 2021, pp. 125133. [610] K. Huang, T. Fu, L. M. Glass, M. Zitnik, C. Xiao, and J. Sun, DeepPurpose: deep learning library for drugtarget interaction prediction, Bioinformatics, vol. 36, no. 22-23, pp. 55455547, 2020. [611] T. Fu, W. Gao, C. W. Coley, and J. Sun, Reinforced genetic algorithm for structure-based drug design, in Annual Conference on Neural Information Processing Systems (NeurIPS), 2022. [612] A. C. Marco, A. Myers, S. J. Graham, P. DAgostino, and K. Apple, The USPTO Patent Assignment Dataset: Descriptions and Analysis, ser. USPTO Economic Working Paper. SSRN, 2015. [Online]. Available: https://books.google.com.hk/books?id=THPfzwEACAAJ [613] K. Huang, T. Fu, W. Gao, Y. Zhao, Y. Roohani, J. Leskovec, C. W. Coley, C. Xiao, J. Sun, and M. Zitnik, Artificial intelligence foundation for therapeutic science, Nature Chemical Biology, pp. 1 4, 2022. [614] J. E. Saal, S. Kirklin, M. Aykol, B. Meredig, and C. Wolverton, Materials design and discovery with high-throughput density functional theory: The open quantum materials database (oqmd), 82 JOM, vol. 65, no. 11, pp. 15011509, 2013. [Online]. Available: https://doi.org/10.1007/s11837-013-0755-4 [615] D. Zagorac, H. Muller, S. Ruehl, J. Zagorac, and S. Rehme, Recent developments in the Inorganic Crystal Structure Database: theoretical crystal structure data and related features, Journal of Applied Crystallography, vol. 52, no. 5, pp. 918925, Oct 2019. [Online]. Available: https://doi.org/10.1107/S160057671900997X [616] Y. G. Chung, E. Haldoupis, B. J. Bucior, M. Haranczyk, S. Lee, H. Zhang, K. D. Vogiatzis, M. Milisavljevic, S. Ling, J. S. Camp, B. Slater, J. I. Siepmann, D. S. Sholl, and R. Q. Snurr, Advances, updates, and analytics for the computation-ready, experimental metalorganic framework database: Core mof 2019, Journal of Chemical & Engineering Data, vol. 64, no. 12, pp. 59855998, 2019. [Online]. Available: https://doi.org/10.1021/acs.jced.9b [617] A. S. Rosen, S. M. Iyer, D. Ray, Z. Yao, A. Aspuru-Guzik, L. Gagliardi, J. M. Notestein, and R. Q. Snurr, Machine learning the quantum-chemical properties of metalorganic frameworks for accelerated materials discovery, Matter, vol. 4, no. 5, pp. 15781597, May 2021. [Online]. Available: https://doi.org/10.1016/j.matt.2021. 02.015 [618] K. Gubsch, R. Bence, L. Glasby, and P. Z. Moghadam, Digimof: database of mof information generated via text mining, ChemRxiv, 2023, this content is preprint and has not been peer-reviewed. [Online]. Available: https://doi.org/10.26434/ chemrxiv-2022-41t70 synthesis [619] M. Scheidgen, L. Himanen, A. N. Ladines, D. Sikter, M. Nakhaee, A. Fekete, T. Chang, A. Golparvar, J. A. Marquez, S. Brockhauser et al., NOMAD: distributed web-based platform for managing materials science research data, Journal of Open Source Software, vol. 8, no. 90, p. 5388, 2023. [620] B. Deng, Materials Project Trajectory (MPtrj) Dataset, 7 2023. [Online]. Available: https://figshare.com/articles/dataset/Materials Project Trjectory MPtrj Dataset/23713842 [621] K. Choudhary, K. F. Garrity, A. C. E. Reid, B. DeCost, A. J. Biacchi, A. R. H. Walker, Z. Trautt, J. Hattrick-Simpers, A. G. Kusne, A. Centrone, A. Davydov, J. Jiang, R. Pachter, G. Cheon, E. Reed, A. Agrawal, X. Qian, V. Sharma, H. Zhuang, S. V. Kalinin, B. G. Sumpter, G. Pilania, P. Acar, S. Mandal, K. Haule, D. Vanderbilt, K. Rabe, and F. Tavazza, The joint automated repository for various integrated simulations (jarvis) for data-driven materials design, npj Computational Materials, vol. 6, no. 1, p. 173, November 2020. [Online]. Available: https://doi.org/10.1038/s41524-020-00440-1 [622] J. J. Irwin, K. G. Tang, J. Young, C. Dandarchuluun, B. R. Wong, M. Khurelbaatar, Y. S. Moroz, J. Mayfield, and R. A. Sayle, Zinc20a free ultralarge-scale chemical database for ligand discovery, Journal of Chemical Information and Modeling, vol. 60, [Online]. Available: no. 12, pp. 60656073, December 2020. https://doi.org/10.1021/acs.jcim.0c00675 [623] O. P. Pfeiffer, H. Liu, L. Montanelli, M. I. Latypov, F. G. Sen, V. Hegadekatte, E. A. Olivetti, and E. R. Homer, Aluminum alloy compositions and properties extracted from corpus of scientific manuscripts and us patents, Scientific Data, vol. 9, no. 1, p. 128, March 2022. [Online]. Available: https://doi.org/10.1038/ s41597-022-01215-7 [624] V. Tshitoyan, J. Dagdelen, L. Weston, A. Dunn, Z. Rong et al., Unsupervised word embeddings capture latent knowledge from materials science literature, Nature, vol. 571, no. 7763, pp. 9598, 2019. [625] P. Eastman, P. K. Behara, D. L. Dotson, R. Galvelis, J. E. Herr, J. T. Horton, Y. Mao, J. D. Chodera, B. P. Pritchard, Y. Wang et al., Spice, dataset of drug-like molecules and peptides for training machine learning potentials, Scientific Data, vol. 10, no. 1, p. 11, 2023. [626] Z. Zeng, Y. Yao, Z. Liu, and M. Sun, deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals, Nature Communications, vol. 13, no. 1, p. 862, 2022. [627] S. Liu, W. Nie, C. Wang, J. Lu, Z. Qiao, L. Liu, J. Tang, C. Xiao, and A. Anandkumar, Multi-modal molecule structuretext model for text-based retrieval and editing, Nature Machine Intelligence, vol. 5, no. 12, pp. 14471457, 2023. [628] B. Su, D. Du, Z. Yang, Y. Zhou, J. Li, A. Rao, H. Sun, Z. Lu, and J.-R. Wen, molecular multimodal foundation model aslanguage, arXiv preprint sociating molecule graphs with natural arXiv:2209.05481, 2022. [629] J. Li, Y. Lu, R. Akbani, Z. Ju, P. L. Roebuck, W. Liu, J.-Y. Yang, J. N. Weinstein, I. Shmulevich, and G. B. Mills, TCPA: resource for cancer functional proteomics data, Nature Methods, vol. 10, no. 11, pp. 10461047, 2013. [630] B. E. Suzek, Y. Wang, H. Huang, P. B. McGarvey, C. H. Wu, and U. Consortium, Uniref clusters: comprehensive and scalable alternative for improving sequence similarity searches, Bioinformatics, vol. 31, no. 6, pp. 926932, 2015. [631] Y. Ji, Z. Zhou, H. Liu, and R. V. Davuluri, Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome, Bioinformatics, vol. 37, no. 15, pp. 2112 2120, 2021. [632] R. Consortium, Rnacentral: hub of information for non-coding rna sequences, Nucleic Acids Research, vol. 47, no. D1, pp. D221D229, 2019. [633] C. G. Cole, O. T. McCann, J. E. Collins, K. Oliver, D. Willey, S. M. Gribble, F. Yang, K. McLaren, J. Rogers, Z. Ning et al., Finishing the finished human chromosome 22 sequence, Genome Biology, vol. 9, no. 5, p. R78, 2008. [634] N. Siva, 1000 genomes project, 2008. [635] Z. Li, V. Subasri, Y. Shen, D. Li, Y. Zhao, G.-B. Stan, and C. Shan, Omni-dna: unified genomic foundation model for cross-modal and multi-task learning, arXiv preprint arXiv:2502.03499, 2025. [636] S. Dhanasekar, A. Saranathan, and P. Xie, Genechat: multi-modal large language model for gene function prediction, bioRxiv, pp. 202506, 2025. [637] W. Liang, Dnahlmdna sequence and human language mixed large language model, arXiv preprint arXiv:2410.16917, 2024. [638] F. Yang, W. Wang, F. Wang, Y. Fang, D. Tang, J. Huang, H. Lu, and J. Yao, scbert as large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data, Nature Machine Intelligence, vol. 4, no. 10, pp. 852866, 2022. [639] Y. Shi, J. Yang, C. Nai, S. Li, J. Fang, X. Wang, Z. Liu, and Y. Zhang, Language-enhanced representation learning for single-cell transcriptomics, arXiv preprint arXiv:2503.09427, 2025. [640] Y. Xiao, E. Sun, Y. Jin, and W. Wang, Rna-gpt: Multimodal generative system for rna sequence understanding, arXiv preprint arXiv:2411.08900, 2024. [641] B. Chen, X. Cheng, P. Li, Y.-a. Geng, J. Gong, S. Li, Z. Bei, X. Tan, B. Wang, X. Zeng et al., xTrimoPGLM: unified 100b-scale pretrained transformer for deciphering the language of protein, arXiv preprint arXiv:2401.06199, 2024. [642] Y. Shen, Z. Chen, M. Mamalakis, L. He, H. Xia, T. Li, Y. Su, J. He, and Y. G. Wang, fine-tuning dataset and benchmark for large language models for protein understanding, in 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 2024, pp. 23902395. [643] G. Stelzer, N. Rosen, I. Plaschkes, S. Zimmerman, M. Twik, S. Fishilevich, T. I. Stein, R. Nudel, I. Lieder, Y. Mazor et al., The genecards suite: from gene data mining to disease genome sequence analyses, Current protocols in bioinformatics, vol. 54, no. 1, pp. 1 30, 2016. [644] J. S. Amberger, C. A. Bocchini, F. Schiettecatte, A. F. Scott, and A. Hamosh, Omim. org: Online mendelian inheritance in man (omim), an online catalog of human genes and genetic disorders, Nucleic acids research, vol. 43, no. D1, pp. D789D798, 2015. [645] P. W. Harrison, M. R. Amode, O. Austine-Orimoloye, A. G. Azov, M. Barba, I. Barnes, A. Becker, R. Bennett, A. Berry, J. Bhai et al., Ensembl 2024, Nucleic acids research, vol. 52, no. D1, pp. D891 D899, 2024. [646] W.-B. Jiang, L.-M. Zhao, and B.-L. Lu, Large brain model for learning generic representations with tremendous eeg data in bci, arXiv preprint arXiv:2405.18765, 2024. [647] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, Chatdoctor: medical chat model fine-tuned on large language model meta-ai (llama) using medical domain knowledge, arXiv, abs/2303.14070, 2023. [Online]. Available: https://arxiv.org/abs/2303.14070 [648] A. B. Abacha, E. Agichtein, Y. Pinter, and D. Demner-Fushman, trec 2017 Overview of the medical question answering task at liveqa. in TREC, 2017, pp. 112. [649] S. Chen, Z. Ju, X. Dong, H. Fang, S. Wang, Y. Yang, J. Zeng, R. Zhang, R. Zhang, M. Zhou et al., Meddialog: large-scale medical dialogue dataset, arXiv preprint arXiv:2004.03329, vol. 3, 2020. [650] L. Luo, P.-T. Lai, C.-H. Wei, C. N. Arighi, and Z. Lu, BioRED: rich biomedical relation extraction dataset, Briefings in Bioinformatics, vol. 23, no. 5, p. bbac282, 07 2022. [651] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al., The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027, 2020. 83 [652] S. Wang, M. B. McDermott, G. Chauhan, M. Ghassemi, M. C. Hughes, and T. Naumann, Mimic-extract: data extraction, preprocessing, and representation pipeline for mimic-iii, in Proceedings of the ACM conference on health, inference, and learning, 2020, pp. 222235. [653] S. Li, V. Balachandran, S. Feng, J. Ilgen, E. Pierson, P. W. W. Koh, and Y. Tsvetkov, Mediq: Question-asking llms and benchmark for reliable interactive clinical reasoning, Advances in Neural Information Processing Systems, vol. 37, pp. 28 85828 888, 2024. [654] J. W. Smith, J. E. Everhart, W. C. Dickson, W. C. Knowler, and R. S. Johannes, Using the adap learning algorithm to forecast the onset of diabetes mellitus, in Proceedings of the annual symposium on computer application in medical care, 1988, p. 261. [655] G. Zheng, X. Wang, J. Liang, N. Chen, Y. Zheng, and B. Wang, Efficiently democratizing medical llms for 50 languages via mixture of language family experts, arXiv preprint arXiv:2410.10626, 2024. [656] P. Chambon, J.-B. Delbrouck, T. Sounack, S.-C. Huang, Z. Chen, M. Varma, S. Q. Truong, C. T. Chuong, and C. P. Langlotz, Chexpert plus: Augmenting large chest x-ray dataset with text radiology reports, patient demographics and additional image formats, arXiv preprint arXiv:2405.19538, 2024. [657] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, Towards generalist foundation model for radiology by leveraging web-scale 2d & 3d medical data, arXiv preprint arXiv:2308.02463, 2023. [658] J.-H. Huang, C.-H. H. Yang, F. Liu, M. Tian, Y.-C. Liu, T.-W. Wu, I. Lin, K. Wang, H. Morikawa, H. Chang et al., DeepOpht: medical report generation for retinal images via deep models and visual explanation, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2021, pp. 24422452. [659] F. Bai, Y. Du, T. Huang, M. Q. H. Meng, and B. Zhao, M3d: Advancing 3d medical image analysis with multi-modal large language models, 2024. [660] J. Ruckert, L. Bloch, R. Brungel, A. Idrissi-Yaghir, H. Schafer, C. S. Schmidt, S. Koitka, O. Pelka, A. B. Abacha, A. G. Seco de Herrera et al., ROCOv2: Radiology objects in context version 2, an updated multimodal image dataset, Scientific Data, vol. 11, no. 1, p. 688, 2024. [661] S. Subramanian, L. L. Wang, S. Mehta, B. Bogin, M. Van Zuylen, S. Parasa, S. Singh, M. Gardner, and H. Hajishirzi, Medicat: dataset of medical images, captions, and textual references, arXiv preprint arXiv:2010.06000, 2020. [662] R. Wu, N. Su, C. Zhang, T. Ma, T. Zhou, Z. Cui, N. Tang, T. Mao, Y. Zhou, W. Fan et al., MM-retinal v2: Transfer an elite knowledge spark into fundus vision-language pretraining, arXiv preprint arXiv:2501.15798, 2025. [663] R. Wang, J. Chen, K. Ji, Z. Cai, S. Chen, Y. Yang, and B. Wang, Medgen: Unlocking medical video generation by scaling granularlyannotated medical videos, arXiv preprint arXiv:2507.05675, 2025. [664] M. Hu, K. Yuan, Y. Shen, F. Tang, X. Xu, L. Zhou, W. Li, Y. Chen, Z. Xu, Z. Peng et al., Ophclip: Hierarchical retrieval-augmented learning for ophthalmic surgical video-language pretraining, arXiv preprint arXiv:2411.15421, 2024. [665] B. Huang, H. Yuan, M. Xiang, Y. Huang, K. Xiao, S. Xu, R. Zhang, L. Yang, Z. Niu, and H. Gu, comprehensive correction of the gaia dr3 xp spectra, The Astrophysical Journal Supplement Series, vol. 271, no. 1, p. 13, 2024. [666] Astrophysics Data System, HarvardSmithsonian CfA, Nasa ads developer api, https://ui.adsabs.harvard.edu/help/api, 2025, programmatic access to 15+ million astronomy & physics records. [667] F. Grezes, S. Blanco-Cuaresma, A. Accomazzi, M. J. Kurtz, G. Shapurian, E. Henneken, C. S. Grant, D. M. Thompson, R. Chyla, S. McDonald et al., Building astrobert, language model for astronomy & astrophysics, arXiv preprint arXiv:2112.00590, 2021. [668] X. Zhao, Y. Huang, G. Xue, X. Kong, J. Liu, X. Tang, T. C. Beers, Y.-S. Ting, and A.-L. Luo, Specclip: Aligning and translating spectroscopic measurements for stars, arXiv preprint arXiv:2507.01939, 2025. [669] M. J. Smith, R. J. Roberts, E. Angeloudi, and M. Huertas-Company, Astropt: Scaling large observation models for astronomy, arXiv preprint arXiv:2405.14930, 2024. [670] W. Xu, X. Zhao, Y. Zhou, X. Yue, B. Fei, F. Ling, W. Zhang, and L. Bai, Earthse: benchmark evaluating earth scientific exploration capability for large language models, arXiv preprint arXiv:2505.17139, 2025. [671] V. V. Manivannan, Y. Jafari, S. Eranky, S. Ho, R. Yu, D. WatsonParris, Y. Ma, L. Bergen, and T. Berg-Kirkpatrick, Climaqa: An automated evaluation framework for climate foundation models, arXiv preprint arXiv:2410.16701, 2024. [672] A. E. W. Johnson, L. Bulgarelli, L. Shen, A. Gayles, A. Shammout, S. Horng, T. J. Pollard, S. Hao, B. Moody, B. Gow, L.-w. H. Lehman, L. A. Celi, and R. G. Mark, MIMIC-IV, freely accessible electronic health record dataset, Scientific Data, vol. 10, no. 1, 2023. [673] M. Guevara, S. Chen, S. Thomas, T. L. Chaunzwa, I. Franco, B. H. Kann, S. Moningi, J. M. Qian, M. Goldstein, S. Harper et al., Large language models to identify social determinants of health in electronic health records, NPJ digital medicine, vol. 7, no. 1, p. 6, 2024. [674] T. Porian, M. Wortsman, J. Jitsev, L. Schmidt, and Y. Carmon, Resolving discrepancies in compute-optimal scaling of language models, in Advances in Neural Information Processing Systems, A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, Eds., vol. 37, 2024, pp. 100 535100 570. [Online]. Available: https://proceedings.neurips.cc/paper files/paper/2024/file/ b6341525cd84f3be0ef203e4d7cd8556-Paper-Conference.pdf [675] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. Liang, Q. V. Le, T. Ma, and A. W. Yu, DoReMi: Optimizing data mixtures speeds up language model pretraining, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. [Online]. Available: https://openreview.net/forum?id=lXuByUeHhd [676] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. CallisonBurch, and N. Carlini, Deduplicating training data makes language models better, arXiv preprint arXiv:2107.06499, 2021. [677] H. Shi, Z. Xu, H. Wang, W. Qin, W. Wang, Y. Wang, Z. Wang, S. Ebrahimi, and H. Wang, Continual learning of large language models: comprehensive survey, ACM Computing Surveys, 2025. [Online]. Available: https://doi.org/10.1145/3735633 [678] Y. Guo, J. Fu, H. Zhang, D. Zhao, and Y. Shen, Efficient continual pre-training by mitigating the stability gap, arXiv preprint arXiv:2406.14833, 2024. [679] P. Tong, E. Brown, P. Wu, S. Woo, A. J. V. IYER, S. C. Akula, S. Yang, J. Yang, M. Middepogu, Z. Wang et al., Cambrian-1: fully open, vision-centric exploration of multimodal llms, Advances in Neural Information Processing Systems, vol. 37, pp. 87 31087 356, 2024. [680] Y. Bisk, R. Zellers, R. Le Bras, J. Gao, Y. Choi et al., PIQA: Reasoning about physical commonsense in natural language, in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 74327439. [681] S. Dai, Y. Yan, J. Su, D. Zihao, Y. Gao, Y. Hei, J. Li, J. Zhang, S. Tao, Z. Gao, and X. Hu, PhysicsArena: The first multimodal physics reasoning benchmark exploring variable, process, and solution dimensions, May 2025. [682] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, Lets verify step by step, arXiv preprint arXiv:2305.20050, 2023. [683] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, Measuring mathematical problem solving with the math dataset, arXiv preprint arXiv:2103.03874, 2021. [684] A. Bercovich, I. Levy, I. Golan, M. Dabbah, R. El-Yaniv, O. Puny, I. Galil, Z. Moshe, T. Ronen, N. Nabwani et al., Llama-nemotron: Efficient reasoning models, arXiv preprint arXiv:2505.00949, 2025. I. Kulikov, J. Yu, S. K. Cho, D. Wang, Y. Tian, and X. Li, Naturalreasoning: Reasoning in the wild with 2.8m challenging questions, ArXiv, vol. abs/2502.13124, 2025. [Online]. Available: https://api.semanticscholar.org/CorpusID:276421963 Jiang, K. Padthe, Y. Li, [685] W. Yuan, J. E. Weston, [686] D. Lu, X. Tan, R. Xu, T. Yao, C. Qu, W. Chu, Y. Xu, and Y. Qi, Scp-116k: high-quality problem-solution dataset and generalized pipeline for automated extraction in the higher education science domain, 2025. [687] P. G. Francoeur, T. Masuda, J. Sunseri, A. Jia, R. B. Iovanisci, I. Snyder, and D. R. Koes, Three-dimensional convolutional neural networks and cross-docked data set for structure-based drug design, Journal of chemical information and modeling, vol. 60, no. 9, pp. 42004215, 2020. [688] R. Wang, X. Fang, Y. Lu, and S. Wang, The pdbbind database: Collection of binding affinities for proteinligand complexes with known three-dimensional structures, Journal of medicinal chemistry, vol. 47, no. 12, pp. 29772980, 2004. [689] B. Yu, F. N. Baker, Z. Chen, X. Ning, and H. Sun, Llasmol: Advancing large language models for chemistry with large-scale, comprehensive, high-quality instruction tuning dataset, arXiv preprint arXiv:2402.09391, 2024. 84 [690] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy, M. Veselov, A. Kadurin, S. Johansson, H. Chen, S. Nikolenko, A. Aspuru-Guzik, and A. Zhavoronkov, Molecular Sets (MOSES): Benchmarking Platform for Molecular Generation Models, Frontiers in Pharmacology, 2020. language queries, in Proceedings of [691] C. Edwards, C. Zhai, and H. Ji, Text2Mol: Cross-modal molecule the retrieval with natural 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 595607. [Online]. Available: https://aclanthology.org/2021.emnlp-main.47/ [692] P. Liu, J. Tao, and Z. Ren, quantitative analysis of knowledgelearning preferences in large language models in molecular science, 2025. [Online]. Available: https://arxiv.org/abs/2402.04119 [693] J. M. Ede, Warwick electron microscopy datasets, Machine Learning: Science and Technology, vol. 1, no. 4, p. 045003, Sep. 2020. [Online]. Available: http://dx.doi.org/10.1088/2632-2153/ab9c3c [694] M. Xu, Z. Zhang, J. Lu, Z. Zhu, Y. Zhang, M. Chang, R. Liu, and J. Tang, Peer: comprehensive and multi-task benchmark for protein sequence understanding, Advances in Neural Information Processing Systems, vol. 35, pp. 35 15635 173, 2022. [695] Y. Ren, Z. Chen, L. Qiao, H. Jing, Y. Cai, S. Xu, P. Ye, X. Ma, S. Sun, H. Yan, D. Yuan, W. Ouyang, and X. Liu, Beacon: Benchmark for comprehensive rna tasks and language models, Advances in Neural Information Processing Systems, vol. 37, pp. 92 89192 921, 2024. [696] Y. Fang, X. Liang, N. Zhang, K. Liu, R. Huang, Z. Chen, X. Fan, and H. Chen, Mol-instructions: large-scale biomolecular instruction dataset for large language models, arXiv preprint arXiv:2306.08018, 2023. [697] H. Xiao, W. Lin, H. Wang, Z. Liu, and Q. Ye, OPI: An open instruction dataset for adapting large language models to proteinrelated tasks, in Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges, 2024. [Online]. Available: https://openreview.net/forum?id=I4bA7ekJGh [698] M. and Y. Zhang, Jin, H. Xue, Z. Wang, B. Kang, R. Ye, K. Zhou, M. Du, ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction, in First Conference on Language Modeling, 2024. [Online]. Available: https://openreview.net/forum?id=2nTzomzjjb [699] M. Wesney, Tot-biology, https://huggingface.co/datasets/moremilk/ ToT-Biology, 2025, version v1.0.0; MIT License; accessed 2025-0817. [700] H. He, Y. Ren, Y. Tang, Z. Xu, J. Li, M. Yang, D. Zhang, D. Yuan, T. Chen, S. Zhang et al., Biology instructions: dataset and benchmark for multi-omics sequence understanding capability of large language models, arXiv preprint arXiv:2412.19191, 2024. [701] X. He, S. Chen, Z. Ju, X. Dong, H. Fang, S. Wang, Y. Yang, J. Zeng, R. Zhang, R. Zhang, M. Zhou, P. Zhu, and P. Xie, Meddialog: Two large-scale medical dialogue datasets, arXiv, abs/2004.03329, 2020. [Online]. Available: https://arxiv.org/abs/2004.03329 [702] J. Wang, Z. Yao, Z. Yang, H. Zhou, R. Li, X. Wang, Y. Xu, and H. Yu, Notechat: dataset of synthetic doctor-patient conversations conditioned on clinical notes, arXiv preprint arXiv:2310.15959, 2023. [703] S. Bae, D. Kyung, J. Ryu, E. Cho, G. Lee, S. Kweon, J. Oh, L. Ji, E. Chang, T. Kim et al., EHRXQA: multi-modal question answering dataset for electronic health records with chest x-ray images, Advances in Neural Information Processing Systems, vol. 36, pp. 38673880, 2023. [704] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman, dataset of clinically generated visual questions and answers about radiology images, Scientific Data, vol. 5, no. 1, pp. 110, 2018. [705] T. Grootswagers, I. Zhou, A. K. Robinson, M. N. Hebart, and T. A. Carlson, Human eeg recordings for 1,854 concepts presented in rapid serial visual presentation streams, Scientific Data, vol. 9, no. 1, p. 3, 2022. [706] A. T. Gifford, K. Dwivedi, G. Roig, and R. M. Cichy, large and rich eeg dataset for modeling human visual object recognition, NeuroImage, vol. 264, p. 119754, 2022. [707] E. J. Allen, G. St-Yves, Y. Wu, J. L. Breedlove, J. S. Prince, L. T. Dowdle, M. Nau, B. Caron, F. Pestilli, I. Charest et al., massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence, Nature neuroscience, vol. 25, no. 1, pp. 116126, 2022. [708] M. N. Hebart, O. Contier, L. Teichmann, A. H. Rockter, C. Y. Zheng, A. Kidder, A. Corriveau, M. Vaziri-Pashkam, and C. I. Baker, Things-data, multimodal collection of large-scale datasets for investigating object representations in human brain and behavior, Elife, vol. 12, p. e82580, 2023. [709] R. Kneeland, P. S. Scotti, G. St-Yves, J. Breedlove, K. Kay, and T. Naselaris, Nsd-imagery: benchmark dataset for extending fmri vision decoding methods to mental imagery, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 28 85228 862. [710] Z. Guo, J. Wu, Y. Song, J. Bu, W. Mai, Q. Zheng, W. Ouyang, and C. Song, Neuro-3d: Towards 3d visual decoding from eeg signals, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 23 87023 880. [711] N. Hollenstein, J. Rotsztejn, M. Troendle, A. Pedroni, C. Zhang, and N. Langer, Zuco, simultaneous eeg and eye-tracking resource for natural sentence reading, Scientific data, vol. 5, no. 1, pp. 113, 2018. [712] N. Hollenstein, M. Troendle, C. Zhang, and N. Langer, Zuco 2.0: dataset of physiological recordings during natural reading and annotation, arXiv preprint arXiv:1912.00903, 2019. [713] D. Alvarez-Estevez and R. M. Rijsman, Inter-database validation of deep learning approach for automatic sleep scoring, PloS one, vol. 16, no. 8, p. e0256111, 2021. [714] K. A. I. Aboalayon, M. Faezipour, W. S. Almuhammadi, and S. Moslehpour, Sleep stage classification using eeg signal analysis: comprehensive survey and new investigation, Entropy, vol. 18, no. 9, p. 272, 2016. [715] S. F. Quan, B. V. Howard, C. Iber, J. P. Kiley, F. J. Nieto, G. T. OConnor, D. M. Rapoport, S. Redline, J. Robbins, J. M. Samet et al., The sleep heart health study: design, rationale, and methods, Sleep, vol. 20, no. 12, pp. 10771085, 1997. [716] A. Harati, M. Golmohammadi, S. Lopez, I. Obeid, and J. Picone, Improved eeg event classification using differential energy, in 2015 IEEE Signal Processing in Medicine and Biology Symposium (SPMB). IEEE, 2015, pp. 14. [717] E. von Weltin, T. Ahsan, V. Shah, D. Jamshed, M. Golmohammadi, I. Obeid, and J. Picone, Electroencephalographic slowing: primary source of error in automatic seizure detection, in 2017 IEEE signal processing in medicine and biology symposium (SPMB). IEEE, 2017, pp. 15. [718] J. Ma, B. Yang, W. Qiu, Y. Li, S. Gao, and X. Xia, large eeg dataset for studying cross-session variability in motor imagery braincomputer interface, Scientific Data, vol. 9, no. 1, p. 531, 2022. [719] W.-L. Zheng and B.-L. Lu, Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks, IEEE Transactions on autonomous mental development, vol. 7, no. 3, pp. 162175, 2015. [720] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, Emotionmeter: multimodal framework for recognizing human emotions, IEEE transactions on cybernetics, vol. 49, no. 3, pp. 11101122, 2018. [721] I. Zyma, S. Tukaev, I. Seleznov, K. Kiyono, A. Popov, M. Chernykh, and O. Shpenkov, Electroencephalograms during mental arithmetic task performance, Data, vol. 4, no. 1, p. 14, 2019. [722] H. Zhang, J. Sun, R. Chen, W. Liu, Z. Yuan, X. Zheng, Z. Wang, Z. Yang, H. Yan, H. Zhong et al., Empowering and assessing the utility of large language models in crop science, Advances in Neural Information Processing Systems, vol. 37, pp. 52 67052 722, 2024. [723] E. Perkowski, R. Pan, T. D. Nguyen, Y.-S. Ting, S. Kruk, T. Zhang, C. ONeill, M. Jablonska, Z. Sun, M. J. Smith et al., Astrollama-chat: Scaling astrollama with conversational and diverse datasets, Research Notes of the AAS, vol. 8, no. 1, p. 7, 2024. [724] R. Pan, T. D. Nguyen, H. Arora, A. Accomazzi, T. Ghosal, and Y.- S. Ting, Astromlab 2: Astrollama-2-70b model and benchmarking specialised llms for astronomy, in SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2024, pp. 8796. [725] J. Evans, S. Sadruddin, and J. DSouza, Astro-nerastronomy named entity recognition: Is gpt good domain expert annotator? arXiv preprint arXiv:2405.02602, 2024. [726] M. Rahnemoonfar, T. Chowdhury, A. Sarkar, D. Varshney, M. Yari, and R. R. Murphy, Floodnet: high resolution aerial imagery dataset for post flood scene understanding, IEEE Access, vol. 9, pp. 89 644 89 654, 2021. [727] J. Wang, Z. Zheng, Z. Chen, A. Ma, and Y. Zhong, Earthvqa: Towards queryable earth via relational reasoning-based remote sensing visual question answering, in Proceedings of the AAAI conference on artificial intelligence, vol. 38, no. 6, 2024, pp. 54815489. 85 [728] C. Ma, Z. Hua, A. Anderson-Frey, V. Iyer, X. Liu, and L. Qin, Weatherqa: Can multimodal language models reason about severe weather? arXiv preprint arXiv:2406.11217, 2024. [729] K. X. Nguyen, F. Qiao, A. Trembanis, and X. Peng, Seafloorai: large-scale vision-language dataset for seafloor geological survey, Advances in Neural Information Processing Systems, vol. 37, pp. 22 10722 123, 2024. [730] Z. Zhang, T. Zhao, Y. Guo, and J. Yin, Rs5m and georsclip: large scale vision-language dataset and large vision-language model for remote sensing, IEEE Transactions on Geoscience and Remote Sensing, 2024. [731] Z. Wang, R. Prabha, T. Huang, J. Wu, and R. Rajagopal, Skyscript: large and semantically diverse vision-language dataset for remote sensing, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 6, 2024, pp. 58055813. [732] J. Wu, W. Deng, X. Li, S. Liu, T. Mi, Y. Peng, Z. Xu, Y. Liu, H. Cho, C.-I. Choi, Y. Cao, H. Ren, X. Li, X. Li, and Y. Zhou, MedReason: Eliciting factual medical reasoning steps in LLMs via knowledge graphs, arXiv preprint arXiv:2504.00993, 2025. [733] Y. Sun, X. Qian, W. Xu, H. Zhang, C. Xiao, L. Li, Y. Rong, W. Huang, Q. Bai, and T. Xu, ReasonMed: 370k multi-agent generated dataset for advancing medical reasoning, arXiv preprint arXiv:2506.09513, 2025. [734] C. Qin, X. Chen, C. Wang, P. Wu, X. Chen, Y. Cheng, J. Zhao, M. Xiao, X. Dong, Q. Long et al., Scihorizon: Benchmarking aifor-science readiness from scientific data to large language models, arXiv preprint arXiv:2503.13503, 2025. [735] A. Anand, J. Kapuriya, A. Singh, J. Saraf, N. Lal, A. Verma, R. Gupta, and R. Shah, MM-PhyQA: Multimodal physics question-answering with multi-image cot prompting, in Advances in Knowledge Discovery and Data Mining, D.-N. Yang, X. Xie, V. S. Tseng, J. Pei, J.-W. Huang, and J. C.-W. Lin, Eds. Singapore: Springer Nature, 2024, pp. 5364. [736] C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun, Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems, Jun. 2024. [737] S. Aroca-Ouellette, C. Paik, A. Roncone, and K. Kann, PROST: Physical reasoning of objects through space and time, Jun. 2021. [738] L. Wang, E. Su, J. Liu, P. Li, P. Xia, J. Xiao, W. Zhang, X. Dai, X. Chen, Y. Meng, M. Ding, L. Bai, W. Ouyang, S. Tang, A. Wang, and X. Ma, PhysUniBench: An undergraduate-level physics reasoning benchmark for multimodal models, Jun. 2025. [739] X. Zhang, Y. Dong, Y. Wu, J. Huang, C. Jia, B. Fernando, M. Z. Shou, L. Zhang, and J. Liu, PhysReason: comprehensive benchmark towards physics-based reasoning, May 2025. [740] X. Xu, Q. Xu, T. Xiao, T. Chen, Y. Yan, J. Zhang, S. Diao, C. Yang, and Y. Wang, UGPhysics: comprehensive benchmark for undergraduate physics reasoning with large language models, arXiv preprint arXiv:2502.00334, 2025. [741] H. Shen, T. Wu, Q. Han, Y. Hsieh, J. Wang, Y. Zhang, Y. Cheng, Z. Hao, Y. Ni, X. Wang et al., PhyX: Does your model have the wits for physical reasoning? arXiv preprint arXiv:2505.15929, 2025. [742] K. Feng, Y. Zhao, Y. Liu, T. Yang, C. Zhao, J. Sous, and A. Cohan, PHYSICS: Benchmarking foundation models on university-level physics problem solving, arXiv preprint arXiv:2503.21821, 2025. [743] K. Xiang, H. Li, T. J. Zhang, Y. Huang, Z. Liu, P. Qu, J. He, J. Chen, Y.-J. Yuan, J. Han, H. Xu, H. Li, M. Sachan, and X. Liang, Seephys: Does seeing help thinking? benchmarking vision-based physics reasoning, Jun. 2025. [744] D. J. Chung, Z. Gao, Y. Kvasiuk, T. Li, M. Munchmeyer, M. Rudolph, F. Sala, and S. C. Tadepalli, Theoretical physics benchmark (TPBench)-a dataset and study of ai reasoning capabilities in theoretical physics, Machine Learning: Science and Technology, 2025. [745] S. Qiu, S. Guo, Z.-Y. Song, Y. Sun, Z. Cai, J. Wei, T. Luo, Y. Yin, H. Zhang, Y. Hu, C. Wang, C. Tang, H. Chang, Q. Liu, Z. Zhou, T. Zhang, J. Zhang, Z. Liu, M. Li, Y. Zhang, B. Jing, X. Yin, Y. Ren, Z. Fu, J. Ji, W. Wang, X. Tian, A. Lv, L. Man, J. Li, F. Tao, Q. Sun, Z. Liang, Y. Mu, Z. Li, J.-J. Zhang, S. Zhang, X. Li, X. Xia, J. Lin, Z. Shen, J. Chen, Q. Xiong, B. Wang, F. Wang, Z. Ni, B. Zhang, F. Cui, C. Shao, Q.-H. Cao, M.-x. Luo, Y. Yang, M. Zhang, and H. X. Zhu, PHYBench: Holistic evaluation of physical perception and reasoning in large language models, May 2025. [746] W. La Cava, P. Orzechowski, B. Burlacu, F. O. de Franca, M. Virgolin, Y. Jin, M. Kommenda, and J. H. Moore, Contemporary symbolic regression methods and their relative performance, Jul. 2021. [747] P. Shojaee, N.-H. Nguyen, K. Meidani, A. B. Farimani, K. D. Doan, and C. K. Reddy, Llm-srbench: new benchmark for scientific equation discovery with large language models, Jun. 2025. [748] F. Bordes, Q. Garrido, J. T. Kao, A. Williams, M. Rabbat, and E. Dupoux, Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environments, Jun. 2025. [749] B. Krojer, M. Komeili, C. Ross, Q. Garrido, K. Sinha, N. Ballas, and M. Assran, shortcut-aware video-qa benchmark for physical understanding via minimal video pairs, Jun. 2025. [750] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo, L. Wang, and Y. Qiao, MVBench: comprehensive multi-modal video understanding benchmark, May 2024. [751] Y. Huang, R. Zhang, X. He, X. Zhi, H. Wang, X. Li, F. Xu, D. Liu, H. Liang, Y. Li et al., Chemeval: comprehensive multilevel chemical evaluation for large language models, arXiv preprint arXiv:2409.13989, 2024. [752] H. Zhao, X. Tang, Z. Yang, X. Han, X. Feng, Y. Fan, S. Cheng, D. Jin, Y. Zhao, A. Cohan et al., Chemsafetybench: benchmarking llm safety on chemistry domain, arXiv preprint arXiv:2411.16736, 2024. [753] J. Chen, Y. Hu, Y. Wang, Y. Lu, X. Cao, M. Lin, H. Xu, J. Wu, C. Xiao, J. Sun et al., Trialbench: Multi-modal artificial intelligenceready clinical trial datasets, Nature Scientific Data, 2024. [754] J. Xie, W. Wang, B. Gao, Z. Yang, H. Wan, S. Zhang, T. Fu, and Y. Li, Qcbench: Evaluating large language models on domainspecific quantitative chemistry, arXiv preprint arXiv:2508.01670, 2025. [755] Z. Yang, J. Xie, S. Shen, D. Wang, Y. Chen, B. Gao, S. Sun, B. Qi, D. Zhou, L. Bai, L. Chen, S. Zhang, J. Jiang, T. Fu, and Y. Li, Spectrumworld: Artificial intelligence foundation for spectroscopy, arXiv preprint, 2025. [756] A. N. Rubungo, K. Li, J. Hattrick-Simpers, and A. B. Dieng, Llm4mat-bench: Benchmarking large language models for materials property prediction, 2024. [Online]. Available: https://arxiv.org/abs/ 2411.00177 [757] V. Mishra, S. Singh, M. Zaki, H. S. Grover, S. Miret, M. ., and N. M. A. Krishnan, LLamat: Large language models for materials science, in AI for Accelerated Materials Design - Vienna 2024, 2024. [Online]. Available: https://openreview.net/forum?id=ZUkmRy6SqS [758] N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher, Guacamol: Benchmarking models for de novo molecular design, Journal of Chemical Information and Modeling, vol. 59, no. 3, pp. 10961108, 2019. [Online]. Available: https://doi.org/10.1021/acs.jcim.8b00839 [759] J. Zhou and O. G. Troyanskaya, Predicting effects of noncoding variants with deep learningbased sequence model, Nature methods, vol. 12, no. 10, pp. 931934, 2015. [760] K. L. Howe, P. Achuthan, J. Allen, J. Allen, J. Alvarez-Jarreta, M. R. Amode, I. M. Armean, A. G. Azov, R. Bennett, J. Bhai et al., Ensembl 2021, Nucleic acids research, vol. 49, no. D1, pp. D884 D891, 2021. [761] E. Trop, Y. Schiff, E. M. Marroquin, C. H. Kao, A. Gokaslan, I. long-range benchmark: [Online]. Available: M. Polen, M. Shao, B. P. de Almeida, T. Pierrot, Y. Li, and V. Kuleshov, The genomics Advancing DNA language models, 2024. https://openreview.net/forum?id=Cdc90HKs1I [762] J. Li, J. Li, Y. Liu, D. Zhou, and Q. Li, Tomg-bench: Evaluating llms on text-based open molecule generation, arXiv preprint arXiv:2412.14642, 2024. [763] X. Lu, H. Cao, Z. Liu, S. Bai, L. Chen, Y. Yao, H.-T. Zheng, and Y. Li, Moleculeqa: dataset to evaluate factual accuracy in molecular comprehension, arXiv preprint arXiv:2403.08192, 2024. [764] J. M. Laurent, J. D. Janizek, M. Ruzo, M. M. Hinks, M. J. Hammerling, S. Narayanan, M. Ponnapati, A. D. White, and S. G. Rodriques, Lab-bench: Measuring capabilities of language models for biology research, arXiv preprint arXiv:2407.10362, 2024. [765] W. Hou and Z. Ji, Geneturing tests gpt models in genomics, BioRxiv, pp. 202303, 2023. [766] M. Yin, Y. Qu, D. Liu, L. Yang, L. Cong, and M. Wang, Genomebench: scientific reasoning benchmark from real-world expert discussions, bioRxiv, pp. 202506, 2025. [767] Y. Hasson, P. Luc, L. Momeni, M. Ovsjanikov, G. L. Moing, A. Kuznetsova, I. Ktena, J. J. Sun, S. Koppula, D. Gokay et al., Scivid: Cross-domain evaluation of video models in scientific applications, arXiv preprint arXiv:2507.03578, 2025. [768] A. Krithara, A. Nentidis, K. Bougiatiotis, and G. Paliouras, Bioasqqa: manually curated corpus for biomedical question answering, Scientific Data, vol. 10, no. 1, p. 170, 2023. 86 [769] M. Liu, J. Ding, J. Xu, W. Hu, X. Li, L. Zhu, Z. Bai, X. Shi, B. Wang, H. Song, P. Liu, X. Zhang, S. Wang, K. Li, H. Wang, T. Ruan, X. Huang, X. Sun, and S. Zhang, Medbench: comprehensive, standardized, and reliable benchmarking system for evaluating large language models, arXiv, abs/2407.10990, chinese medical 2024. [Online]. Available: https://arxiv.org/abs/2407.10990 [770] Y. Zuo, S. Qu, Y. Li, Z. Chen, X. Zhu, E. Hua, K. Zhang, N. Ding, and B. Zhou, MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding, Jun. 2025. [771] R. K. Arora, J. Wei, R. S. Hicks, P. Bowman, J. Quinonero-Candela, F. Tsimpourlas, M. Sharman, M. Shah, A. Vallone, A. Beutel et al., Healthbench: Evaluating large language models towards improved human health, arXiv preprint arXiv:2505.08775, 2025. [772] B. Liu, L.-M. Zhan, L. Xu, L. Ma, Y. Yang, and X.-M. Wu, Slake: semantically-labeled knowledge-enhanced dataset for medical visual question answering, in 2021 IEEE 18th international symposium on biomedical imaging (ISBI). IEEE, 2021, pp. 16501654. [773] Q. Zheng, W. Zhao, C. Wu, X. Zhang, L. Dai, H. Guan, Y. Li, Y. Zhang, Y. Wang, and W. Xie, Large-scale long-tailed disease diagnosis on radiology images, Nature Communications, vol. 15, no. 1, p. 10147, 2024. [774] X. Chen, X. Mao, Q. Guo, L. Wang, S. Zhang, and T. Chen, RareBench: can LLMs serve as rare diseases specialists? in Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining, 2024, pp. 48504861. [775] Y. Jiang, K. C. Black, G. Geng, D. Park, J. Zou, A. Y. Ng, and J. H. Chen, MedAgentBench: virtual ehr environment to benchmark medical llm agents, NEJM AI, p. AIdbp2500144, 2025. [776] S. Schmidgall, R. Ziaei, C. Harris, E. Reis, J. Jopling, and M. Moor, Agentclinic: multimodal agent benchmark to evaluate ai in simulated clinical environments, arXiv preprint arXiv:2405.07960, 2024. [777] J. Ying, Z. Chen, Z. Wang, W. Jiang, C. Wang, Z. Yuan, H. Su, H. Kong, F. Yang, and N. Dong, SeedBench: multi-task benchmark for evaluating large language models in seed science, in Proceedings of the 63rd Annual Meeting of the Association for Computational (Volume 1: Long Papers), W. Che, Linguistics J. Nabende, E. Shutova, and M. T. Pilehvar, Eds. Vienna, Austria: Association for Computational Linguistics, Jul. 2025, pp. 31 39531 449. [Online]. Available: https://aclanthology.org/2025.acl-long.1516/ [778] Y.-S. Ting, T. D. Nguyen, T. Ghosal, R. Pan, H. Arora, Z. Sun, T. de Haan, N. Ramachandra, A. Wells, S. Madireddy et al., Astromlab 1: Who wins astronomy jeopardy!? Astronomy and Computing, vol. 51, p. 100893, 2025. [779] J. Li, F. Zhao, P. Chen, J. Xie, X. Zhang, H. Li, M. Chen, Y. Wang, and M. Zhu, An astronomical question answering dataset for evaluating large language models, Scientific Data, vol. 12, no. 1, p. 447, 2025. [780] F. Zhao, Y. Li, Y. Wang, H. Li, M. Chen, P. Chen, N. Sun, C. Wang, and J. Liu, Pulsar candidate classification with multimodal large language models, in Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges, 2024. [Online]. Available: https://openreview.net/forum?id=8SKgWpZiDL [781] S. Mishra-Sharma, Y. Song, and J. Thaler, Paperclip: Associating language with multi-modal astronomical observations and natural models, arXiv preprint arXiv:2403.08851, 2024. [782] K. G. Iyer, M. Yunus, C. ONeill, C. Ye, A. Hyk, K. Mccormick, I. Ciuca, J. F. Wu, A. Accomazzi, S. Astarita et al., pathfinder: semantic framework for literature review and knowledge discovery in astronomy, The Astrophysical Journal Supplement Series, vol. 275, no. 2, p. 38, 2024. [783] N. Webersinke, M. Kraus, J. A. Bingler, and M. Leippold, Climatebert: pretrained language model for climate-related text, arXiv preprint arXiv:2110.12010, 2021. [784] Y. Hu, J. Yuan, C. Wen, X. Lu, Y. Liu, and X. Li, Rsgpt: remote sensing vision language model and benchmark, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 224, pp. 272286, 2025. [785] F. Wang, H. Wang, Z. Guo, D. Wang, Y. Wang, M. Chen, Q. Ma, L. Lan, W. Yang, J. Zhang et al., Xlrs-bench: Could your multimodal llms understand extremely large ultra-high-resolution remote sensing imagery? in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 14 32514 336. [786] F. Wang, M. Chen, X. He, Y. Zhang, F. Liu, Z. Guo, Z. Hu, J. Wang, J. Xu, Z. Li et al., Omniearth-bench: Towards holistic evaluation of earths six spheres and cross-spheres interactions with multimodal observational earth data, arXiv preprint arXiv:2505.23522, 2025. [787] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He, C-eval: multi-level multi-discipline chinese evaluation suite for foundation models, Nov. 2023. [788] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, Agieval: human-centric benchmark for evaluating foundation models, arXiv preprint arXiv:2304.06364, 2023. [789] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen, Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, Jun. 2024. [790] X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, Y. Su, W. Chen, and G. Neubig, MMMUPro: More Robust Multi-discipline Multimodal Understanding Benchmark, May 2025. [791] X. Du, Y. Yao, K. Ma, B. Wang, T. Zheng, K. Zhu, M. Liu, Y. Liang, X. Jin, Z. Wei et al., Supergpqa: Scaling llm evaluation across 285 graduate disciplines, arXiv preprint arXiv:2502.14739, 2025. [792] L. Sun, Y. Han, Z. Zhao, D. Ma, Z. Shen, B. Chen, L. Chen, and K. Yu, Scieval: multi-level large language model evaluation benchmark for scientific research, arXiv preprint arXiv:2308.13149, 2023. [793] A. Anand, J. Kapuriya, A. Singh, J. Saraf, N. Lal, A. Verma, R. Gupta, and R. Shah, Mm-phyqa: Multimodal physics question-answering with multi-image cot prompting, in Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 2024, pp. 53 64. [794] S. Lobry, D. Marcos, J. Murray, and D. Tuia, Rsvqa: Visual question answering for remote sensing data, IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no. 12, pp. 85558566, 2020. [795] X. Hu, L. Gu, Q. An, M. Zhang, L. Liu, K. Kobayashi, T. Harada, R. M. Summers, and Y. Zhu, Expert knowledge-aware image difference graph representation learning for difference-aware medical visual question answering, in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Association for Computing Machinery, 2023, p. 41564165. [Online]. Available: https://doi.org/10.1145/3580305.3599819 [796] Y. Hu, T. Li, Q. Lu, W. Shao, J. He, Y. Qiao, and P. Luo, OmniMedVQA: New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM, arXiv preprint arXiv:2402.09181, 2024. [797] X. Li, J. Ding, and M. Elhoseiny, Vrsbench: versatile visionlanguage benchmark dataset for remote sensing image understanding, Advances in Neural Information Processing Systems, vol. 37, pp. 32293242, 2024. [798] M. Savery, A. B. Abacha, S. Gayen, and D. Demner-Fushman, Question-driven summarization of answers to consumer health questions, Scientific Data, vol. 7, no. 1, p. 322, 2020. [799] B. S. Bloom, M. D. Engelhart, E. J. Furst, W. H. Hill, D. R. Krathwohl et al., Taxonomy of educational objectives: The classification of educational goals. Handbook 1: Cognitive domain. Longman New York, 1956. [800] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi, BERTScore: Evaluating text generation with bert, in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr [801] W. Yan, H. Liu, T. Wu, Q. Chen, W. Wang, H. Chai, J. Wang, W. Zhao, Y. Zhang, R. Zhang et al., Clinicallab: Aligning agents for multidepartmental clinical diagnostics in the real world, arXiv preprint arXiv:2406.13890, 2024. [802] S. Pletscher-Frankild, A. Pallej`a, K. Tsafou, J. X. Binder, and L. J. Jensen, Diseases: Text mining and data integration of diseasegene associations, Methods, vol. 74, pp. 8389, 2015. [803] A. S. Brown and C. J. Patel, standard database for drug repositioning, Scientific data, vol. 4, no. 1, pp. 17, 2017. [804] N. Bogard, J. Linder, A. B. Rosenberg, and G. Seelig, deep neural network for predicting and engineering alternative polyadenylation, Cell, vol. 178, no. 1, pp. 91106, 2019. [805] Y. Zhan, Z. Xiong, and Y. Yuan, Rsvg: Exploring data and models for visual grounding on remote sensing data, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 113, 2023. [806] J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, W. Li, Y. Shen, S. Ma, H. Liu et al., survey on llm-as-a-judge, arXiv preprint arXiv:2411.15594, 2024. [807] M. Zhuge, C. Zhao, D. Ashley, W. Wang, D. Khizbullin, Y. Xiong, Z. Liu, E. Chang, R. Krishnamoorthi, Y. Tian et al., Agent-as-ajudge: Evaluate agents with agents, arXiv preprint arXiv:2410.10934, 2024. 87 [808] F. Zhang, S. Tian, Z. Huang, Y. Qiao, and Z. Liu, Evaluation agent: Efficient and promptable evaluation framework for visual generative models, arXiv preprint arXiv:2412.09645, 2024. [809] Z. Yang, W. Liu, B. Gao, T. Xie, Y. Li, W. Ouyang, S. Poria, E. Cambria, and D. Zhou, Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses, arXiv preprint arXiv:2410.07076, 2024. [810] Z. Yang, W. Liu, B. Gao, Y. Liu, W. Li, T. Xie, L. Bing, W. Ouyang, E. Cambria, and D. Zhou, Moose-chem2: Exploring llm limits in fine-grained scientific hypothesis discovery via hierarchical search, arXiv preprint arXiv:2505.19209, 2025. [811] J. Hu, Z. Zhang, G. Chen, X. Wen, C. Shuai, W. Luo, B. Xiao, Y. Li, and M. Tan, Test-time learning for large language models, arXiv preprint arXiv:2505.20633, 2025. [812] W. Shi, R. Xu, Y. Zhuang, Y. Yu, H. Sun, H. Wu, C. Yang, and M. D. Wang, MedAdapter: Efficient test-time adaptation of large language models towards medical reasoning, in Proceedings of in Natural Language Processing, Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 22 29422 314. [Online]. Available: https://aclanthology.org/2024.emnlp-main.1244/ the 2024 Conference on Empirical Methods [813] M. Thomas, A. Bou, and G. De Fabritiis, Test-time training scaling for chemical exploration in drug design, arXiv e-prints, pp. arXiv 2501, 2025. [814] Z. Gao, T. Li, Y. Kvasiuk, S. C. Tadepalli, M. Rudolph, D. J. Chung, F. Sala, and M. Munchmeyer, Test-time scaling techniques in theoretical physicsa comparison of methods on the tpbench dataset, arXiv preprint arXiv:2506.20729, 2025. [815] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang, J. Li, X. Wan, B. Wang et al., Cmb: comprehensive medical benchmark in chinese, arXiv preprint arXiv:2308.08833, 2023. [816] R.-R. Griffiths, P. Schwaller, and A. A. Lee, Dataset bias in the natural sciences: case study in chemical reaction prediction and synthesis design, arXiv preprint arXiv:2105.02637, 2021. [817] Y.-N. Huang, V. Munteanu, M. I. Love, C. F. Ronkowski, D. Deshpande, A. Wong-Beringer, R. Corbett-Detig, M. Dimian, J. H. Moore, L. X. Garmire et al., Perceptual and technical barriers in sharing and formatting metadata accompanying omics studies, Cell Genomics, vol. 5, no. 5, 2025. [818] K. Dwan, C. Gamble, P. R. Williamson, J. J. Kirkham, and R. B. Group, Systematic review of the empirical evidence of study publication bias and outcome reporting biasan updated review, PloS one, vol. 8, no. 7, p. e66844, 2013. [819] C. Tardy, The role of english in scientific communication: lingua franca or tyrannosaurus rex? Journal of English for academic purposes, vol. 3, no. 3, pp. 247269, 2004. [820] M. Graziani, A. Foncubierta, D. Christofidellis, I. Espejo-Morales, M. Molnar, M. Alberts, M. Manica, and J. Born, We need improved data curation and attribution in ai for scientific discovery, arXiv preprint arXiv:2504.02486, 2025. [821] S. Gao, A. Fang, Y. Huang, V. Giunchiglia, A. Noori, J. R. Schwarz, Y. Ektefaie, J. Kondic, and M. Zitnik, Empowering biomedical discovery with ai agents, Cell, vol. 187, no. 22, pp. 61256151, 2024. [822] S. Ren, P. Jian, Z. Ren, C. Leng, C. Xie, and J. Zhang, Towards scientific intelligence: survey of llm-based scientific agents, arXiv preprint arXiv:2503.24047, 2025. [823] Y. Huang, Y. Chen, H. Zhang, K. Li, M. Fang, L. Yang, X. Li, L. Shang, S. Xu, J. Hao et al., Deep research agents: systematic examination and roadmap, arXiv preprint arXiv:2506.18096, 2025. [824] Anthropic, Introducing the model context protocol, https://www. anthropic.com/news/model-context-protocol, Nov. 2024, accessed: 2025-08-11. [825] OpenAI, Computer-using agent, https://openai.com/zh-Hans-CN/ index/computer-using-agent/, Jan. 2025, accessed: 2025-08-11. [826] M. Mudryi, M. Chaklosh, and G. Ałjcik, The hidden dangers of browsing ai agents, arXiv preprint arXiv:2505.13076, 2025. [827] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, Reflexion: Language agents with verbal reinforcement learning, Advances in Neural Information Processing Systems, vol. 36, pp. 86348652, 2023. [828] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang et al., Self-refine: Iterative refinement with self-feedback, Advances in Neural Information Processing Systems, vol. 36, pp. 46 53446 594, 2023. [829] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, Voyager: An open-ended embodied agent with large language models, Transactions on Machine Learning Research, 2024. [Online]. Available: https://openreview.net/forum?id=ehfRiF0R3a [830] E. Zelikman, Y. Wu, J. Mu, and N. Goodman, Star: Bootstrapping reasoning with reasoning, Advances in Neural Information Processing Systems, vol. 35, pp. 15 47615 488, 2022. [831] W. Yuan, R. Y. Pang, K. Cho, S. Sukhbaatar, J. Xu, and J. Weston, Self-rewarding language models, arXiv preprint arXiv:2401.10020, vol. 3, 2024. [832] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, and Y. Yang, Evoprompt: Connecting llms with evolutionary algorithms yields powerful prompt optimizers, in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=ZG3RaNIsO8 [833] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom, Toolformer: Language models can teach themselves to use tools, Advances in Neural Information Processing Systems, vol. 36, pp. 68 53968 551, 2023. [834] R. Jin, Z. Zhang, M. Wang, and L. Cong, STELLA: Selfresearch, arXiv preprint for biomedical evolving LLM agent arXiv:2507.02004, 2025. [835] Z. Zhang, Z. Qiu, Y. Wu, S. Li, D. Wang, Z. Zhou, D. An, Y. Chen, Y. Li, Y. Wang et al., Origene: self-evolving virtual disease biologist automating therapeutic target discovery, bioRxiv, pp. 2025 06, 2025. [836] X. Tang, T. Hu, M. Ye, Y. Shao, X. Yin, S. Ouyang, W. Zhou, P. Lu, Z. Zhang, Y. Zhao et al., Chemagent: Self-updating memories in large language models improves chemical reasoning, in The Thirteenth International Conference on Learning Representations, 2025. [837] P. Jansen, M.-A. Cˆote, T. Khot, E. Bransom, B. Dalvi Mishra, B. P. Majumder, O. Tafjord, and P. Clark, Discoveryworld: virtual environment for developing and evaluating automated scientific discovery agents, Advances in Neural Information Processing Systems, vol. 37, pp. 10 08810 116, 2024. [838] T. Chen, S. Anumasa, B. Lin, V. Shah, A. Goyal, and D. Liu, Autobench: An automated benchmark for scientific discovery in llms, arXiv preprint arXiv:2502.15224, 2025. [839] T. Ossowski, J. Chen, D. Maqbool, Z. Cai, T. Bradshaw, and J. Hu, Comma: communicative multimodal multi-agent benchmark, arXiv preprint arXiv:2410.07553, 2024. [840] M. Mohammadi, Y. Li, J. Lo, and W. Yip, Evaluation and benchmarking of llm agents: survey, in Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, 2025, pp. 61296139. [841] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [842] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [843] Z. Luo, Z. Yang, Z. Xu, W. Yang, and X. Du, Llm4sr: survey on large language models for scientific research, arXiv preprint arXiv:2501.04306, 2025. [844] T. Zheng, Z. Deng, H. T. Tsang, W. Wang, J. Bai, Z. Wang, and Y. Song, From automation to autonomy: survey on large language models in scientific discovery, arXiv preprint arXiv:2505.13259, 2025. [845] J. Yuan, X. Yan, B. Shi, T. Chen, W. Ouyang, B. Zhang, L. Bai, Y. Qiao, and B. Zhou, Dolphin: Closed-loop open-ended autoresearch through thinking, practice, and feedback, arXiv e-prints, pp. arXiv2501, 2025. [846] X. Yan, S. Feng, J. Yuan, R. Xia, B. Wang, B. Zhang, and L. Bai, Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing, arXiv preprint arXiv:2503.04629, 2025. [847] C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, and D. Ha, The ai scientist: Towards fully automated open-ended scientific discovery, arXiv preprint arXiv:2408.06292, 2024. [848] Y. Qu, K. Huang, M. Yin, K. Zhan, D. Liu, D. Yin, H. C. Cousins, W. A. Johnson, X. Wang, M. Shah et al., CRISPR-GPT for agentic automation of gene-editing experiments, Nature Biomedical Engineering, pp. 114, 2025. 88 [849] S. Jia, C. Zhang, and V. Fung, Llmatdesign: Autonomous materials discovery with large language models, arXiv preprint arXiv:2406.13163, 2024. [850] N. Singh, S. Lane, T. Yu, J. Lu, A. Ramos, H. Cui, and H. Zhao, generalized platform for artificial intelligence-powered autonomous enzyme engineering, Nature Communications, vol. 16, no. 1, p. 5648, 2025. [851] N. Team, B. Zhang, S. Feng, X. Yan, J. Yuan, Z. Yu, X. He, S. Huang, S. Hou, Z. Nie et al., Novelseek: When agent becomes the scientist building closed-loop system from hypothesis to verification, arXiv preprint arXiv:2505.16938, 2025. [852] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White, and P. Schwaller, Augmenting large language models with chemistry tools, Nature Machine Intelligence, vol. 6, no. 5, pp. 525535, 2024. [Online]. Available: [853] Shanghai AI Lab, Intern-discovery, 2025. https://discovery-invitecode.intern-ai.org.cn/ [854] Institute of Automation, Chinese Academy of Sciences, Scienceone, 2025. [Online]. Available: https://scienceone.ia.ac.cn [855] G. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz et al., Augmented language models: survey, arXiv preprint arXiv:2302.07842, 2023. [856] P. Ohm, Broken promises of privacy: Responding to the surprising failure of anonymization, UCLA l. Rev., vol. 57, p. 1701, 2009. [857] L. Sweeney, k-anonymity: model for protecting privacy, International journal of uncertainty, fuzziness and knowledge-based systems, vol. 10, no. 05, pp. 557570, 2002. [858] R. V. Atreya, J. C. Smith, A. B. McCoy, B. Malin, and R. A. Miller, Reducing patient re-identification risk for laboratory results within research datasets, Journal of the American Medical Informatics Association, vol. 20, no. 1, pp. 95101, 2013. [859] A. Narayanan and V. Shmatikov, Robust de-anonymization of large sparse datasets, in 2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE, 2008, pp. 111125. [860] P. Regulation, General data protection regulation, Intouch, vol. 25, pp. 15, 2018. [861] G. Chassang, The impact of the eu general data protection regulation on scientific research, ecancermedicalscience, vol. 11, p. 709, 2017. [862] H.-D. Jacobsen, Us export control and export administration legislation, in Economic Warfare Or Detente. Routledge, 2019, pp. 213225. [863] A. Albalak, Y. Elazar, S. M. Xie, S. Longpre, N. Lambert, X. Wang, N. Muennighoff, B. Hou, L. Pan, H. Jeong et al., survey on data selection for language models, arXiv preprint arXiv:2402.16827, 2024. [864] B. Messmer, V. Sabolˇcec, and M. Jaggi, Enhancing multilingual llm pretraining with model-based data selection, arXiv preprint arXiv:2502.10361, 2025. [865] J. T. Wang, T. Wu, D. Song, P. Mittal, and R. Jia, Greats: Online selection of high-quality data for llm training in every iteration, Advances in Neural Information Processing Systems, vol. 37, pp. 131 197131 223, 2024. [866] A. Wettig, A. Gupta, S. Malik, and D. Chen, Qurating: Selecting high-quality data for training language models, arXiv preprint arXiv:2402.09739, 2024. [867] M. Xia, S. Malladi, S. Gururangan, S. Arora, and D. Chen, Less: Selecting influential data for targeted instruction tuning, in International Conference on Machine Learning. PMLR, 2024, pp. 54 10454 132. [868] N. Baghbanzadeh, S. Ashkezari, E. Dolatabadi, and A. Afkanpour, Open-pmc-18m: high-fidelity large scale medical dataset for multimodal representation learning, arXiv preprint arXiv:2506.02738, 2025. [869] A. Pal, J.-O. Lee, X. Zhang, M. Sankarasubbu, S. Roh, W. J. Kim, M. Lee, and P. Rajpurkar, ReXVQA: Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding, Jun. 2025. [870] X. Zhang, J. N. Acosta, J. Miller, O. Huang, and P. Rajpurkar, ReXGradient-160K: Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports, May 2025. [871] C. Liu, H. Wang, J. Pan, Z. Wan, Y. Dai, F. Lin, W. Bai, D. Rueckert, and R. Arcucci, Beyond distillation: Pushing the limits of medical llm reasoning with minimalist rule-based RL, arXiv preprint arXiv:2505.17952, 2025. [872] S. Yan, M. Hu, Y. Jiang, X. Li, H. Fei, P. Tschandl, H. Kittler, and Z. Ge, Derm1m: million-scale vision-language dataset aligned with clinical ontology knowledge for dermatology, arXiv preprint arXiv:2503.14911, 2025. [873] B. Liu, K. Zou, L. Zhan, Z. Lu, X. Dong, Y. Chen, C. Xie, J. Cao, X.-M. Wu, and H. Fu, GEMeX: Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis, Mar. 2025. [874] C. Ma, Y. Ji, J. Ye, L. Zhang, Y. Chen, T. Li, M. Li, J. He, and H. Shan, Towards interpretable counterfactual generation via multimodal autoregression, arXiv preprint arXiv:2503.23149, 2025. [875] T. Lin, W. Zhang, S. Li, Y. Yuan, B. Yu, H. Li, W. He, H. Jiang, M. Li, X. Song, S. Tang, J. Xiao, H. Lin, Y. Zhuang, and B. C. Ooi, HealthGPT: Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation, Feb. 2025. [876] T. Olatunji, C. Nimo, A. Owodunni, T. Abdullahi, E. Ayodele, M. Sanni, C. Aka, F. Omofoye, F. Yuehgoh, T. Faniran et al., Afrimed-qa: pan-african, multi-specialty, medical questionanswering benchmark dataset, arXiv preprint arXiv:2411.15640, 2024. [877] W. Sun, X. You, R. Zheng, Z. Yuan, X. Li, L. He, Q. Li, and L. Sun, Bora: Biomedical generalist video generation model, arXiv preprint arXiv:2407.08944, 2024. [878] G. Kumichev, P. Blinov, Y. Kuzkina, V. Goncharov, G. Zubkova, N. Zenovkin, A. Goncharov, and A. Savchenko, Medsyn: Llm-based synthetic medical text generation framework, in Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2024, pp. 215230. [879] G. Kell, A. Roberts, S. Umansky, Y. Khare, N. Ahmed, N. Patel, C. Simela, J. Coumbe, J. Rozario, R.-R. Griffiths, and I. J. Marshall, Realmedqa: pilot biomedical question answering dataset containing realistic clinical questions, 2024. [Online]. Available: https://arxiv.org/abs/2408.08624 [880] Y. Xie, Z. Wang, Y. Shen, Z. Zhuang, Y. Wang, Z. Zhang, Y. Wu, Y. Liu, Z. Li, M. Yuan, Z. Yan, Y. Chen, G. Qi, Z. Chen, J. Li, Y. Zhu, J. Liu, Y. Wang, Y. Shen, and C. Xie, Medtrinity-25m: large-scale multimodal dataset with multigranular annotations for medicine, arXiv preprint arXiv:2408.02900, 2024. [Online]. Available: https://arxiv.org/abs/2408.02900 [881] I. Siragusa, S. Contino, M. La Ciura, R. Alicata, and R. Pirrone, Medpix 2.0: comprehensive multimodal biomedical data set for advanced ai applications, arXiv preprint arXiv:2407.02994, 2024. [882] Y. Chen, C. Liu, X. Liu, R. Arcucci, and Z. Xiong, BIMCV-R: Landmark Dataset for 3D CT Text-Image Retrieval, Jul. 2024. [883] S. Bae, D. Kyung, J. Ryu, E. Cho, G. Lee, S. Kweon, J. Oh, L. Ji, E. Chang, T. Kim, and E. Choi, MIMIC-Ext-MIMIC-CXR-VQA: Complex, Diverse, And Large-Scale Visual Question Answering Dataset for Chest X-ray Images (version 1.0.0), https://physionet. org/content/mimic-ext-mimic-cxr-vqa/1.0.0/, 2024. [884] J. Zhou, L. Sun, Y. Xu, W. Liu, S. Afvari, Z. Han, J. Song, Y. Ji, X. He, and X. Gao, Skincap: multi-modal dermatology dataset annotated with rich medical captions, arXiv preprint arXiv:2405.18004, 2024. [885] R. Wu, C. Zhang, J. Zhang, Y. Zhou, T. Zhou, and H. Fu, MM-retinal: Knowledge-enhanced foundational pretraining with fundus image-text expertise, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 722732. [886] X. Zhang, C. Wu, Z. Zhao, J. Lei, Y. Zhang, Y. Wang, and W. Xie, RadGenome-Chest CT: Grounded Vision-Language Dataset for Chest CT Analysis, Apr. 2024. [887] Y. Tan, M. Li, Z. Huang, H. Yu, and G. Fan, Medchatzh: better medical adviser learns from better instructions, 2023. [Online]. Available: https://arxiv.org/abs/2309. [888] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, Benchmarking retrievalaugmented generation for medicine, in Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 62336251. [889] M. S. Seyfioglu, W. O. Ikezogwo, F. Ghezloo, R. Krishna, and L. Shapiro, Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos, Jan. 2025. [890] S. Lyu, C. Chi, H. Cai, L. Shi, X. Yang, L. Liu, X. Chen, D. Zhao, Z. Zhang, X. Lyu et al., Rjua-qa: comprehensive qa dataset for urology, arXiv preprint arXiv:2312.09785, 2023. [891] L. Luo, J. Ning, Y. Zhao, Z. Wang, Z. Ding, P. Chen, W. Fu, Q. Han, G. Xu, Y. Qiu, D. Pan, J. Li, H. Li, W. Feng, S. Tu, Y. Liu, Z. Yang, J. Wang, Y. Sun, and H. Lin, Taiyi: bilingual fine-tuned large language model for diverse biomedical tasks, arXiv, abs/2311.11608, 2023. [Online]. Available: https://arxiv.org/abs/2311.11608 [892] A. B. Abacha, W.-w. Yim, Y. Fan, and T. Lin, An empirical study of clinical note generation from doctor-patient encounters, in 89 Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023, pp. 22912302. [893] S.-C. Huang, Z. Huo, E. Steinberg, C.-C. Chiang, M. P. Lungren, C. P. Langlotz, S. Yeung, N. H. Shah, and J. A. Fries, INSPECT: Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis, arXiv preprint arXiv:2311.10798, 2023. [894] K.-H. Støverud, D. Bouget, A. Pedersen, H. O. Leira, T. Langø, and E. F. Hofstad, Aeropath: An airway segmentation benchmark dataset with challenging pathology, arXiv preprint arXiv:2311.01138, 2023. [895] Y. Labrak, M. Rouvier, and R. Dufour, Morfitt: Un corpus multilabels darticles scientifiques francais dans le domaine biomedical, in 18e Conference en Recherche dInformation et Applications16e Rencontres Jeunes Chercheurs en RI30e Conference sur le Traitement Automatique des Langues Naturelles25e Rencontre des Etudiants Chercheurs en Informatique pour le Traitement Automatique des Langues. ATALA, 2023, pp. 6670. [896] J. Liu, Z. Wang, Q. Ye, D. Chong, P. Zhou, and Y. Hua, Qilin-med-vl: Towards chinese large vision-language model for general healthcare, arXiv preprint arXiv:2310.17956, 2023. [897] S. Chen, M. Guevara, S. Moningi, F. Hoebers, H. Elhalawani, B. H. Kann, F. E. Chipidza, J. Leeman, H. J. W. L. Aerts, T. Miller, G. K. Savova, R. H. Mak, M. Lustberg, M. Afshar, and D. S. Bitterman, The effect of using large language model to respond to patient messages, 2023. [898] A. D. Lelkes, E. Loreaux, T. Schuster, M.-J. Chen, and A. Rajkomar, Sdoh-nli: dataset for inferring social determinants of health from clinical notes, 2023. [899] S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y. Jia, and H. Zan, Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue, arXiv, abs/2308.03549, 2023. [Online]. Available: https: //arxiv.org/abs/2308. [900] Z. Bao, W. Chen, S. Xiao, K. Ren, J. Wu, C. Zhong, J. Peng, X. Huang, and Z. Wei, Disc-medllm: Bridging general large language models and real-world medical consultation, arXiv preprint arXiv:2308.14346, 2023. [901] F. Remy and T. Demeester, Automatic glossary of clinical terminology: large-scale dictionary of biomedical definitions generated from ontological knowledge, arXiv preprint arXiv:2306.00665, 2023. [902] W. H. Pinaya, M. S. Graham, E. Kerfoot, P.-D. Tudosiu, J. Dafflon, V. Fernandez, P. Sanchez, J. Wolleb, P. F. Da Costa, A. Patel et al., Generative ai for medical imaging: extending the monai framework, arXiv preprint arXiv:2307.15208, 2023. [903] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, LLaVA-Med: Training large language-andvision assistant for biomedicine in one day, Advances in Neural Information Processing Systems, vol. 36, pp. 28 54128 564, 2023. [904] Wei Zhu and Wenjing Yue and Xiaoling Wang, ShenNong-TCM: Traditional Chinese Medicine Large Language Model, https://github. com/michael-wzhu/ShenNong-TCM-LLM, 2023. [905] X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie, PMC-VQA: Visual instruction tuning for medical visual question answering, arXiv preprint arXiv:2305.10415, 2023. [906] Wei Zhu and Xiaoling Wang, ChatMed: Chinese Medical Large Language Model, https://github.com/michael-wzhu/ChatMed, 2023. [907] CMKRG, QiZhenGPT: An Open Source Chinese Medical Large Language Model, https://github.com/CMKRG/QiZhenGPT, 2023. [908] X. Wang, J. Li, S. Chen, Y. Zhu, X. Wu, Z. Zhang, X. Xu, J. Chen, J. Fu, X. Wan et al., Huatuo-26m, large-scale chinese medical qa dataset, in Findings of the Association for Computational Linguistics: NAACL 2025, 2025, pp. 38283848. [909] H. Wang, C. Liu, S. Zhao, B. Qin, and T. Liu, Med-ChatGLM: instruction-tuned chatglm-6b, https://github.com/ Chinese medical SCIR-HI/Med-ChatGLM, 2023. [910] D. Sileo, K. Uma, and M.-F. Moens, Generating multiple-choice questions for medical question answering with distractors and cue-masking, in Proceedings of International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). Italia: ELRA and ICCL, May 2024, pp. 76477653. [Online]. Available: https://aclanthology.org/2024.lrec-main. the 2024 Joint Torino, [911] N. T.-H. Nguyen, P. P.-D. Ha, L. T. Nguyen, K. Van Nguyen, and N. L.-T. Nguyen, Spbertqa: two-stage question answering system based on sentence transformers for medical texts, in International Conference on Knowledge Science, Engineering and Management. Springer, 2022, pp. 371382. [912] Z. Zhao, Q. Jin, F. Chen, T. Peng, and S. Yu, Pmc-patients: largescale dataset of patient summaries and relations for benchmarking retrieval-based clinical decision support systems, arXiv preprint arXiv:2202.13876, 2022. [913] Zhao, Zhengyun and Jin, Qiao and Chen, Fangyuan and Peng, Tuorui and Yu, Sheng, large-scale dataset of patient summaries for retrieval-based clinical decision support systems, Scientific data, vol. 10, no. 1, p. 909, 2023. [914] Xia, Fei and Li, Bin and Weng, Yixuan and He, Shizhu and Liu, Kang and Sun, Bin and Li, Shutao and Zhao, Jun, MedConQA: Medical Conversational Question Answering System based on Knowledge Graphs, in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Abu Dhabi, UAE: Association for Computational Linguistics, Dec. 2022, pp. 148158. [Online]. Available: https://aclanthology.org/2022.emnlp-demos. [915] Zhang, Qi and others, framework for automatic medical consultation: Dialogue understanding and task-oriented interaction, Bioinformatics, vol. 39, no. 1, p. btac817, 2023. [916] J. Li, S. Zhong, and K. Chen, Mlec-qa: chinese multi-choice biomedical question answering dataset, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 88628874. [917] A. Ben Abacha, M. Sarrouti, D. Demner-Fushman, S. A. Hasan, and H. Muller, Overview of the VQA-Med task at ImageCLEF 2021: Visual question answering and generation in the medical domain, in Proceedings of the CLEF (Conference and Labs of the Evaluation Forum) 2021 Working Notes, 2021. [918] R. Yermakov, N. Drago, and A. Ziletti, Biomedical datato-text generation via fine-tuning transformers, in Proceedings the 14th International Conference on Natural Language of Generation. Aberdeen, Scotland, UK: Association for Computational Linguistics, Aug. 2021, pp. 364370. [Online]. Available: https: //aclanthology.org/2021.inlg-1. [919] N. Zhang, M. Chen, Z. Bi, X. Liang, L. Li, X. Shang, K. Yin, C. Tan, J. Xu, F. Huang, L. Si, Y. Ni, G. Xie, Z. Sui, B. Chang, H. Zong, Z. Yuan, L. Li, J. Yan, H. Zan, K. Zhang, B. Tang, and Q. Chen, CBLUE: chinese biomedical language understanding evaluation benchmark, https://github.com/CBLUEbenchmark/CBLUE, 2021. [920] Liu, Wenge and Tang, Jianheng and Cheng, Yi and Li, Wenjie and Zheng, Yefeng and Liang, Xiaodan, MedDG: An Entity-Centric Medical Consultation Dataset for Entity-Aware Medical Dialogue Generation, arXiv preprint arXiv:2010.07497, 2022, version 2: submitted Jul 31, 2022. [Online]. Available: https://arxiv.org/abs/ 2010.07497 [921] Toyhom, Chinese Medical Dialogue Data, https://github.com/ Toyhom/Chinese-medical-dialogue-data, 2024. [922] P. Soda, N. C. DAmico, J. Tessadori, G. Valbusa, V. Guarrasi, C. Bortolotto, M. U. Akbar, R. Sicilia, E. Cordelli, D. Fazzini et al., Aiforcovid: Predicting the clinical outcomes in patients with covid19 applying ai to chest-x-rays. an italian multicentre study, Medical image analysis, vol. 74, p. 102216, 2021. [923] A. Ben Abacha, V. Datla V, S. A. Hasan, D. Demner-Fushman, and H. Muller, Overview of the VQA-Med task at ImageCLEF 2020: Visual question answering and generation in the medical domain, in Proceedings of the CLEF (Conference and Labs of the Evaluation Forum) 2020 Working Notes, 2020. [924] A. Ben Abacha and D. Demner-Fushman, question-entailment approach to question answering, BMC bioinformatics, vol. 20, no. 1, p. 511, 2019. [925] A. Ben Abacha, S. A. Hasan, V. V. Datla, D. Demner-Fushman, and H. Muller, Vqa-med: Overview of the medical visual question answering task at ImageCLEF 2019, in Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes, 2019. [926] He, Junqing and Fu, Mingming and Tu, Manshu, Applying deep matching networks to Chinese medical question answering: study and dataset, BMC Medical Informatics and Decision Making, vol. 19, no. 2, p. 52, 2019. [927] Zhang, S. and Zhang, X. and Wang, H. and Guo, L. and Liu, S., Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection, IEEE Access, vol. 6, pp. 74 06174 071, 2018. [928] O. Pelka, S. Koitka, J. Ruckert, F. Nensa, and C. M. Friedrich, Radiology objects in context (ROCO): multimodal image dataset, in Intravascular Imaging and Computer Assisted Stenting and LargeScale Annotation of Biomedical Data and Expert Label Synthesis. Springer International Publishing, 2018, pp. 180189. 90 [929] A. Pampari, P. Raghavan, J. Liang, and J. Peng, emrqa: large corpus for question answering on electronic medical records, arXiv preprint arXiv:1809.00732, 2018. [930] S. A. Hasan, Y. Ling, O. Farri, J. Liu, H. Muller, and M. Lungren, Overview of ImageCLEF 2018 medical domain visual question answering task, Proceedings of CLEF 2018 Working Notes, 2018. the medical question answering task at [931] A. Ben Abacha, E. Agichtein, Y. Pinter, and D. Demner-Fushman, trecOverview of 2017 liveqa, in Proceedings of The Twenty-Sixth Text REtrieval Conference (TREC-2017), ser. NIST Special Publication 500-324. National Institute of Standards and Technology (NIST), 2017, liveQA track overview for medical QA task. [Online]. Available: https://trec.nist.gov/pubs/trec26/papers/OverviewQA.pdf [932] E. Guidotti and D. Ardia, Covid-19 data hub, Journal of Open Source Software, vol. 5, no. 51, p. 2376, 2020. [933] J. Abreu-Vicente, H. Sonntag, T. Eidens, C. S. Mitchell, and T. Lemberger, Integrating curation into scientific publishing to train ai models, arXiv preprint arXiv:2310.20440, 2023. [934] R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, and T.-Y. Liu, Biogpt: generative pre-trained transformer for biomedical text generation and mining, Briefings in bioinformatics, vol. 23, no. 6, p. bbac409, 2022. [935] N. Wang, J. Bian, Y. Li, X. Li, S. Mumtaz, L. Kong, and H. Xiong, Multi-purpose rna language modelling with motif-aware pretraining and type-guided fine-tuning, Nature Machine Intelligence, vol. 6, no. 5, pp. 548557, 2024. [936] E. Boutet, D. Lieberherr, M. Tognolli, M. Schneider, P. Bansal, A. J. Bridge, S. Poux, L. Bougueleret, and I. Xenarios, Uniprotkb/swissprot, the manually annotated section of the uniprot knowledgebase: how to use the entry view, in Plant bioinformatics: methods and protocols. Springer, 2016, pp. 2354. [937] K. Greˇsova, V. Martinek, D. ˇCechak, P. ˇSimeˇcek, and P. Alexiou, Genomic benchmarks: collection of datasets for genomic sequence classification, BMC Genomic Data, vol. 24, no. 1, p. 25, 2023. [938] J. Chen, H. Xu, W. Tao, Z. Chen, Y. Zhao, and J.-D. J. Han, Transformer for one stop interpretable cell type annotation, Nature Communications, vol. 14, no. 1, p. 223, 2023. [939] M. Hao, J. Gong, X. Zeng, C. Liu, Y. Guo, X. Cheng, T. Wang, J. Ma, X. Zhang, and L. Song, Large-scale foundation model on single-cell transcriptomics, Nature methods, vol. 21, no. 8, pp. 14811491, 2024. [940] L. F. Camarillo-Guerrero, A. Almeida, G. Rangel-Pineros, R. D. Finn, and T. D. Lawley, Massive expansion of human gut bacteriophage diversity, Cell, vol. 184, no. 4, pp. 10981109, 2021. [941] S. Cheng, Z. Li, R. Gao, B. Xing, Y. Gao, Y. Yang, S. Qin, L. Zhang, H. Ouyang, P. Du et al., pan-cancer single-cell transcriptional atlas of tumor infiltrating myeloid cells, Cell, vol. 184, no. 3, pp. 792809, 2021. [942] S. Lukassen, R. L. Chua, T. Trefzer, N. C. Kahn, M. A. Schneider, T. Muley, H. Winter, M. Meister, C. Veith, A. W. Boots et al., Sarscov-2 receptor ace 2 and tmprss 2 are primarily expressed in bronchial transient secretory cells, The EMBO journal, vol. 39, no. 10, p. e105114, 2020. [943] A. C. Gregory, O. Zablocki, A. A. Zayed, A. Howell, B. Bolduc, and M. B. Sullivan, The gut virome database reveals age-dependent patterns of virome diversity in the human gut, Cell host & microbe, vol. 28, no. 5, pp. 724740, 2020. [944] L. Schirmer, D. Velmeshev, S. Holmqvist, M. Kaufmann, S. Werneburg, D. Jung, S. Vistnes, J. H. Stockley, A. Young, M. Steindel et al., Neuronal vulnerability and multilineage diversity in multiple sclerosis, Nature, vol. 573, no. 7772, pp. 7582, 2019. [945] O. Franzen, L.-M. Gan, and J. L. Bjorkegren, Panglaodb: web server for exploration of mouse and human single-cell rna sequencing data, Database, vol. 2019, p. baz046, 2019. [946] G. X. Zheng, J. M. Terry, P. Belgrader, P. Ryvkin, Z. W. Bent, R. Wilson, S. B. Ziraldo, T. D. Wheeler, G. P. McDermott, J. Zhu et al., Massively parallel digital transcriptional profiling of single cells, Nature communications, vol. 8, no. 1, p. 14049, 2017. [947] G. Shen, T. Horikawa, K. Majima, and Y. Kamitani, Deep image reconstruction from human brain activity, PLoS computational biology, vol. 15, no. 1, p. e1006633, 2019. [948] M. Amiri and T. Bocklitz, Chemrxivquest: curated chemistry question-answer database extracted from chemrxiv preprints, arXiv preprint arXiv:2505.05232, 2025. [949] K. Choudhary and M. L. Kelley, Chemnlp: natural languageprocessing-based library for materials chemistry text data, The Journal of Physical Chemistry C, vol. 127, no. 35, pp. 17 54517 555, 2023. [950] J. Xie and T. Fu, Deepprotein: Deep learning library and benchmark for protein sequence learning, Bioinformatics, p. btaf165, 2025. [951] A. Velez-Arce, K. Huang, M. M. Li, X. Lin, W. Gao, T. Fu, M. Kellis, B. L. Pentelute, and M. Zitnik, Tdc-2: Multimodal foundation for therapeutic science, bioRxiv, pp. 202406, 2024. [952] T. Fu, W. Gao, C. Coley, and J. Sun, Reinforced genetic algorithm for structure-based drug design, Advances in Neural Information Processing Systems, vol. 35, pp. 12 32512 338, 2022. [953] T. Fu, K. Huang, C. Xiao, L. M. Glass, and J. Sun, HINT: Hierarchical interaction network for clinical-trial-outcome predictions, Patterns, vol. 3, no. 4, p. 100445, 2022. [954] D. S. Wishart, Y. D. Feunang, A. C. Guo, E. J. Lo, A. Marcu, J. R. Grant, T. Sajed, D. Johnson, C. Li, Z. Sayeeda et al., Drugbank 5.0: major update to the drugbank database for 2018, Nucleic acids research, vol. 46, no. D1, pp. D1074D1082, 2018. [955] O. Ursu, J. Holmes, J. Knockel, C. G. Bologa, J. J. Yang, S. L. Mathias, S. J. Nelson, and T. I. Oprea, Drugcentral: online drug compendium, Nucleic acids research, p. gkw993, 2016. [956] V. Sotnikov and A. Chaikova, Language models for multimessenger astronomy, Galaxies, vol. 11, no. 3, p. 63, 2023. [957] N. S. Bobbitt, K. Shi, B. J. Bucior, H. Chen, N. Tracy-Amoroso, Z. Li, Y. Sun, J. H. Merlin, J. I. Siepmann, D. W. Siderius, and R. Q. Snurr, Mofx-db: An online database of computational adsorption data for nanoporous materials, Journal of Chemical & Engineering Data, vol. 68, no. 2, pp. 483498, 2023. [Online]. Available: https://doi.org/10.1021/acs.jced.2c00583 [958] C. Pang, X. Weng, J. Wu, J. Li, Y. Liu, J. Sun, W. Li, S. Wang, L. Feng, G.-S. Xia et al., Vhm: Versatile and honest vision language model for remote sensing image analysis, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 6, 2025, pp. 6381 6388. [959] J. Luo, Z. Pang, Y. Zhang, T. Wang, L. Wang, B. Dang, J. Lao, J. Wang, J. Chen, Y. Tan et al., Skysensegpt: fine-grained instruction tuning dataset and model for remote sensing vision-language understanding, arXiv preprint arXiv:2406.10100, 2024. [960] W. Zhang, M. Cai, T. Zhang, Y. Zhuang, and X. Mao, Earthgpt: universal multimodal large language model for multisensor image comprehension in remote sensing domain, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 120, 2024. [961] Z. Yuan, Z. Xiong, L. Mou, and X. X. Zhu, Chatearthnet: globalscale image-text dataset empowering vision-language geo-foundation models, Earth System Science Data Discussions, vol. 2024, pp. 124, 2024. [962] D. Muhtar, Z. Li, F. Gu, X. Zhang, and P. Xiao, Lhrs-bot: Empowering remote sensing with vgi-enhanced large multimodal language model, in European Conference on Computer Vision. Springer, 2024, pp. 440457. [963] Y. Zhan, Z. Xiong, and Y. Yuan, Skyeyegpt: Unifying remote sensing vision-language tasks via instruction tuning with large language model, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 221, pp. 6477, 2025. [964] R.-Z. Fan, Z. Wang, and P. Liu, MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning, arXiv e-prints, p. arXiv:2507.16812, Jul. 2025. [965] X. Liu and D. Song, Constructing ophthalmic mllm for positioningdiagnosis collaboration through clinical cognitive chain reasoning, arXiv preprint arXiv:2507.17539, 2025. [966] Y. Li, J. Liu, T. Zhang, T. Zhang, S. Chen, T. Li, Z. Li, L. Liu, L. Ming, G. Dong, D. Pan, C. Li, Y. Fang, D. Kuang, M. Wang, C. Zhu, Y. Zhang, H. Guo, F. Zhang, Y. Wang, B. Ding, W. Song, X. Li, Y. Huo, Z. Liang, S. Zhang, X. Wu, S. Zhao, L. Xiong, Y. Wu, J. Ye, W. Lu, B. Li, Y. Zhang, Y. Zhou, X. Chen, L. Su, H. Zhang, F. Chen, X. Dong, N. Nie, Z. Wu, B. Xiao, T. Li, S. Dang, P. Zhang, Y. Sun, J. Wu, J. Yang, X. Lin, Z. Ma, K. Wu, J. li, A. Yang, H. Liu, J. Zhang, X. Chen, G. Ai, W. Zhang, Y. Chen, X. Huang, K. Li, W. Luo, Y. Duan, L. Zhu, R. Xiao, Z. Su, J. Pu, D. Wang, X. Jia, T. Zhang, M. Ai, M. Wang, Y. Qiao, L. Zhang, Y. Shen, F. Yang, M. Zhen, Y. Zhou, M. Chen, F. Li, C. Zhu, K. Lu, Y. Zhao, H. Liang, Y. Li, Y. Qin, L. Sun, J. Xu, H. Sun, M. Lin, Z. Zhou, and W. Chen, Baichuan-Omni-1.5 Technical Report, Jan. 2025. [967] W. Wang, Y. Su, J. Huan, J. Liu, W. Chen, Y. Zhang, C.-Y. Li, K.- J. Chang, X. Xin, L. Shen et al., Asclepius: spectrum evaluation benchmark for medical multi-modal large language models, arXiv preprint arXiv:2402.11217, 2024. [968] C. Chen, J. Yu, S. Chen, C. Liu, Z. Wan, D. Bitterman, F. Wang, and K. Shu, Clinicalbench: Can llms beat traditional ml models in clinical prediction? arXiv preprint arXiv:2411.06469, 2024. 91 [969] J. Matos, S. Chen, S. Placino, Y. Li, J. C. C. Pardo, D. Idan, T. Tohyama, D. Restrepo, L. F. Nakayama, J. M. M. Pascual-Leone, G. Savova, H. Aerts, L. A. Celi, A. I. Wong, D. S. Bitterman, and J. Gallifant, WorldMedQA-V: multilingual, multimodal medical examination dataset for multimodal language models evaluation, Oct. 2024. [970] I. Ziegler, A. Koksal, D. Elliott, and H. Schutze, Craft your dataset: Task-specific synthetic dataset generation through corpus retrieval and augmentation, arXiv preprint arXiv:2409.02098, 2024. [971] P. Chen, J. Ye, G. Wang, Y. Li, Z. Deng, W. Li, T. Li, H. Duan, Z. Huang, Y. Su, B. Wang, S. Zhang, B. Fu, J. Cai, B. Zhuang, E. J. Seibel, J. He, and Y. Qiao, GMAI-MMBench: Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI, Oct. 2024. [972] Y. He, J. Chen, H. Dong, E. Jimenez-Ruiz, A. Hadian, and I. Horrocks, Machine learning-friendly biomedical datasets for equivalence and subsumption ontology matching, in International semantic web conference. Springer, 2022, pp. 575591. [973] H. Chen, Z. Fang, Y. Singla, and M. Dredze, Benchmarking large language models on answering and explaining challenging medical questions, arXiv, abs/2402.18060, 2024. [Online]. Available: https://arxiv.org/abs/2402.18060 [974] C. Royer, B. Menze, and A. Sekuboyina, MultiMedEval: Benchmark and Toolkit for Evaluating Medical Vision-Language Models, Feb. 2024. [975] S. Wu, M. Koo, L. Blum, A. Black, L. Kao, F. Scalzo, and I. Kurtz, comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology, arXiv preprint arXiv:2308.04709, 2023. [976] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P.-C. Chang, A. Carroll, C. Lau, R. Tanno, I. Ktena et al., Towards generalist biomedical ai, NEJM AI, vol. 1, no. 3, p. AIoa2300138, 2024. [977] T. Zack, E. Lehman, M. Suzgun, J. A. Rodriguez, L. A. Celi, J. Gichoya, D. Jurafsky, P. Szolovits, D. W. Bates, R.-E. E. Abdulnour et al., Coding inequity: assessing gpt-4s potential for perpetuating racial and gender biases in healthcare, MedRxiv, pp. 202307, 2023. [978] P. Hosseini, J. M. Sin, B. Ren, B. G. Thomas, E. Nouri, A. Farahanchi, and S. Hassanpour, benchmark for long-form medical question answering, arXiv preprint arXiv:2411.09834, 2024. [979] F. Gaschi, X. Fontaine, P. Rastin, and Y. Toussaint, Multilingual clinical ner: Translation or cross-lingual transfer? in 5th Clinical Natural Language Processing Workshop. Association for Computational Linguistics, 2023, pp. 289311. [980] O. Kovaleva, C. Shivade, S. Kashyap, K. Kanjaria, J. Wu, D. Ballah, A. Coy, A. Karargyris, Y. Guo, D. B. Beymer et al., Towards visual dialog for radiology, in Proceedings of the 19th SIGBioMed workshop on biomedical language processing, 2020, pp. 6069. [981] B. Yu, Y. Li, and J. Wang, Detecting causal language use in science findings, in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 46644674. [982] Q. Jin, Y. Yang, Q. Chen, and Z. Lu, GeneGPT: Augmenting large language models with domain tools for improved access to biomedical information, Bioinformatics, vol. 40, no. 2, p. btae075, 2024. [983] A. F. Villaverde, D. Henriques, K. Smallbone, S. Bongard, J. Schmid, D. Cicin-Sain, A. Crombach, J. Saez-Rodriguez, K. Mauch, E. BalsaCanto et al., Biopredyn-bench: suite of benchmark problems for dynamic modelling in systems biology, BMC systems biology, vol. 9, no. 1, p. 8, 2015. [984] Y. Liu, L. Lv, X. Zhang, L. Yuan, and Y. Tian, Bioprobench: Comprehensive dataset and benchmark in biological protocol understanding and reasoning, arXiv preprint arXiv:2505.07889, 2025. [985] L. Mitchener, J. M. Laurent, B. Tenmann, S. Narayanan, G. P. Wellawatte, A. White, L. Sani, and S. G. Rodriques, Bixbench: comprehensive benchmark for llm-based agents in computational biology, arXiv preprint arXiv:2503.00096, 2025. [986] W. Cheng, Z. Song, Y. Zhang, S. Wang, D. Wang, M. Yang, L. Li, and J. Ma, Dnalongbench: benchmark suite for long-range dna prediction tasks, bioRxiv, 2025. [987] V. Sarwal, S. Lee, R. He, A. Kattapuram, E. Eskin, W. Wang, S. Mangul et al., Bioinformaticsbench: collaboratively built large language model benchmark for bioinformatics reasoning, in ICML 2024 Workshop on Efficient and Accessible Foundation Models for Biological Discovery, 2024. [988] Q. Chen and C. Deng, Bioinfo-Bench: simple benchmark framework for llm bioinformatics skills evaluation, bioRxiv, 2023. [Online]. Available: https://www.biorxiv.org/content/early/2023/10/ 21/2023.10.18. [989] E. Nguyen, M. Poli, M. Faizi, A. Thomas, M. Wornow, C. BirchSykes, S. Massaroli, A. Patel, C. Rabideau, Y. Bengio et al., Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution, Advances in neural information processing systems, vol. 36, pp. 43 17743 201, 2023. [990] R. K. Umarov and V. V. Solovyev, Recognition of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks, PloS one, vol. 12, no. 2, p. e0171410, 2017. [991] E. Z. Kvon, T. Kazmar, G. Stampfel, J. O. Yanez-Cuna, M. Pagani, K. Schernhuber, B. J. Dickson, and A. Stark, Genome-scale functional characterization of drosophila developmental enhancers in vivo, Nature, vol. 512, no. 7512, pp. 9195, 2014. [992] J. Wu, Z. Ren, J. Wang, P. Zhu, Y. Song, M. Liu, Q. Zheng, L. Bai, W. Ouyang, and C. Song, Adabrain-bench: Benchmarking brain foundation models for brain-computer interface applications, arXiv preprint arXiv:2507.09882, 2025. [993] J. Kim, M. Hur, and M. Min, From rag to qa-rag: Integrating generative ai for pharmaceutical regulatory compliance process, in Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing, 2025, pp. 12931295. [994] H. Yang, J. Cole, Y. Li, R. Chen, G. Min, and K. Li, Omnigenbench: modular platform for reproducible genomic foundation models benchmarking, arXiv preprint arXiv:2505.14402, 2024. [995] M. Nakata and T. Shimazaki, PubChemQC project: large-scale first-principles electronic structure database for data-driven chemistry, Journal of chemical information and modeling, vol. 57, no. 6, pp. 13001308, 2017. [996] S. Axelrod and R. Gomez-Bombarelli, GEOM, energy-annotated molecular conformations for property prediction and molecular generation, Scientific Data, vol. 9, no. 1, p. 185, 2022. [997] S. A. Joseph, S. M. Husain, S. S. Offner, S. Juneau, P. Torrey, A. S. Bolton, J. P. Farias, N. Gaffney, G. Durrett, and J. J. Li, Astrovisbench: code benchmark for scientific computing and visualization in astronomy, arXiv preprint arXiv:2505.20538, 2025. [998] N. Alampara, S. Miret, and K. M. Jablonka, Mattext: Do language models need more than text & scale for materials modeling? 2024. [Online]. Available: https://arxiv.org/abs/2406.17295 [999] Y. Song, S. Miret, and B. Liu, Matsci-nlp: Evaluating scientific language models on materials science language tasks using text-toschema modeling, 2023. [Online]. Available: https://arxiv.org/abs/ 2305.08264 [1000] Y.-F. Zhang, H. Zhang, H. Tian, C. Fu, S. Zhang, J. Wu, F. Li, K. Wang, Q. Wen, Z. Zhang et al., Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. [1001] Q. Cheng, H. Huang, Y. Xu, Y. Zhou, H. Li, and Z. Wang, Nwpucaptions dataset and mlca-net for remote sensing image captioning, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 119, 2022. [1002] X. Lu, B. Wang, X. Zheng, and X. Li, Exploring models and data for remote sensing image caption generation, IEEE Transactions on Geoscience and Remote Sensing, vol. 56, no. 4, pp. 21832195, 2017. [1003] W. Chen et al., Theoremqa: Evaluating large language models on theorem-based scientific reasoning, https://arxiv.org/abs/2305.12524, 2023. [1004] A. Agrawal et al., Jeebench: Iit-advanced exam problems for evaluating llms, https://arxiv.org/abs/2305.15074, 2023. [1005] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, Measuring massive multitask language understanding, Jan. 2021. [1006] Z. Gu, X. Zhu, H. Ye, L. Zhang, J. Wang, Y. Zhu, S. Jiang, Z. Xiong, Z. Li, W. Wu et al., Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 18 09918 107. [1007] Z. Xi, G. Li, Y. Fan, H. Guo, Y. Liu, X. Fan, J. Liu, J. Ding, W. Zuo, Z. Yin, L. Bai, T. Ji, T. Gui, Q. Zhang, P. Torr, and X. Huang, Bmmr: large-scale bilingual multimodal multi-discipline reasoning dataset, arXiv preprint arXiv:2507.03483, 2025. [1008] J. Yin, S. Dash, F. Wang, and M. Shankar, Forge: Pre-training open foundation models for science, in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2023, pp. 113. 92 [1009] C. Heneka, F. Nieser, A. Ore, T. Plehn, and D. Schiller, Large language modelsthe future of fundamental physics? arXiv preprint arXiv:2506.14757, 2025. [1010] Y. Ruan, C. Lu, N. Xu, Y. He, Y. Chen, J. Zhang, J. Xuan, J. Pan, Q. Fang, H. Gao et al., An automatic end-to-end chemical synthesis development platform powered by large language models, Nature communications, vol. 15, no. 1, p. 10160, 2024. [1011] Y. Zhang, Y. Han, S. Chen, R. Yu, X. Zhao, X. Liu, K. Zeng, M. Yu, J. Tian, F. Zhu et al., Large language models to accelerate organic chemistry synthesis, Nature Machine Intelligence, pp. 113, 2025. [1012] Z. Cao, R. Magar, Y. Wang, and A. B. Farimani, Moformer: Self-supervised transformer model for metal-organic framework property prediction, 2022. [Online]. Available: https://arxiv.org/abs/ 2210. [1013] R. Ghugare, S. Miret, A. Hugessen, M. Phielipp, and G. Berseth, Searching for high-value molecules using reinforcement learning and transformers, 2023. [Online]. Available: https://arxiv.org/abs/ 2310.02902 [1014] N. Gruver, A. Sriram, A. Madotto, A. G. Wilson, C. L. Zitnick, and Z. Ulissi, Fine-tuned language models generate stable inorganic materials as [Online]. Available: https://arxiv.org/abs/2402.04379 text, 2025. [1015] N. Alampara, S. Miret, and K. M. Jablonka, Less can be more for predicting properties with large language models, 2025. [Online]. Available: https://arxiv.org/abs/2406.17295 [1016] Y. Kang and J. Kim, Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models, Nature communications, vol. 15, no. 1, p. 4705, 2024. [1017] M. Vaˇskeviˇcius and J. Kapoˇciute-Dzikiene, Language models for predicting organic synthesis procedures, Applied Sciences, vol. 14, no. 24, 2024. [Online]. Available: https://www.mdpi.com/2076-3417/ 14/24/11526 [1018] A. N. Rubungo, C. Arnold, B. P. Rand, and A. B. Dieng, Llmprop: Predicting physical and electronic properties of crystalline [Online]. Available: text descriptions, 2023. solids from their https://arxiv.org/abs/2310.14029 [1019] J. Chen, Z. Cai, Z. Liu, Y. Yang, R. Wang, Q. Xiao, X. Feng, Z. Su, J. Guo, X. Wan, G. Yu, H. Li, and B. Wang, Shizhengpt: Towards multimodal llms for traditional chinese medicine, arXiv preprint arXiv:2508.14706, 2025. [1020] W. Gao, Z. Deng, Z. Niu, F. Rong, C. Chen, Z. Gong, W. Zhang, D. Xiao, F. Li, Z. Cao et al., Ophglm: Training an ophthalmology large language-and-vision assistant based on instructions and dialogue, arXiv preprint arXiv:2306.12174, 2023. [1021] C. Wu, W. Lin, X. Zhang, Y. Zhang, W. Xie, and Y. Wang, Pmcllama: toward building open-source language models for medicine, Journal of the American Medical Informatics Association, vol. 31, no. 9, pp. 18331843, 2024. [1022] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin, L. Hou, K. Clark, S. R. Pfohl, H. Cole-Lewis et al., Toward expertlevel medical question answering with large language models, Nature Medicine, vol. 31, no. 3, pp. 943950, 2025. [1023] C. Peng, X. Yang, M. Lyu, K. E. Smith, A. Costa, M. G. Flores, J. Bian, and Y. Wu, Gatortron and gatortrongpt: large language models for clinical narratives, in AAAI 2024 Spring Symposium on Clinical Foundation Models, 2024. [1024] K. Zhang, R. Zhou, E. Adhikarla, Z. Yan, Y. Liu, J. Yu, Z. Liu, X. Chen, B. D. Davison, H. Ren, J. Huang, C. Chen, Y. Zhou, S. Fu, W. Liu, T. Liu, X. Li, Y. Chen, L. He, J. Zou, Q. Li, H. Liu, and L. Sun, generalist visionlanguage foundation model for diverse biomedical tasks, Nature Medicine, vol. 30, no. 11, p. 31293141, Aug. 2024. [Online]. Available: http://dx.doi.org/10.1038/s41591-024-03185-2 [1025] G. Wang, G. Yang, Z. Du, L. Fan, and X. Li, Clinicalgpt: large language models finetuned with diverse medical data and comprehensive evaluation, arXiv preprint arXiv:2306.09968, 2023. [1026] V. Fishman, Y. Kuratov, A. Shmelev, M. Petrov, D. Penzar, D. Shepelin, N. Chekanov, O. Kardymon, and M. Burtsev, Gena-lm: family of open-source foundational dna language models for long sequences, Nucleic Acids Research, vol. 53, no. 2, p. gkae1310, 01 2025. [Online]. Available: https://doi.org/10.1093/nar/gkae1310 [1027] L. Y. Jiang, X. C. Liu, N. P. Nejatian, M. Nasir-Moin, D. Wang, A. Abidin, K. Eaton, H. A. Riina, I. Laufer, P. Punjabi et al., Health system-scale language models are all-purpose prediction engines, Nature, vol. 619, no. 7969, pp. 357362, 2023. [1028] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, Chatdoctor: medical chat model fine-tuned on large language model meta-ai (llama) using medical domain knowledge, Cureus, vol. 15, no. 6, 2023. [1029] Y. Chen, X. Xing, J. Lin, H. Zheng, Z. Wang, Q. Liu, and X. Xu, Soulchat: Improving llms empathy, listening, and comfort abilities through fine-tuning with multi-turn empathy conversations, arXiv preprint arXiv:2311.00273, 2023. [1030] D. Zhang, W. Zhang, B. He, J. Zhang, C. Qin, and J. Yao, DNAGPT: generalized pretrained tool for multiple dna sequence analysis tasks, bioRxiv, pp. 202307, 2023. [1031] R. Wang, Y. Duan, C. Lam, J. Chen, J. Xu, H. Chen, X. Liu, P. C.- I. Pang, and T. Tan, Ivygpt: Interactive chinese pathway language model in medical domain, in CAAI International Conference on Artificial Intelligence. Springer, 2023, pp. 378382. [1032] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data, arXiv preprint arXiv:2308.02463, 2023. [1033] O. Ben Shoham and N. Rappoport, Cpllm: Clinical prediction with large language models, PLOS Digital Health, vol. 3, no. 12, p. e0000680, 2024. [1034] H. Wang, C. Gao, C. Dantona, B. Hull, and J. Sun, Drg-llama: tuning llama model to predict diagnosis-related group for hospitalized patients, NPJ digital medicine, vol. 7, no. 1, p. 16, 2024. [1035] R. K. Luu and M. J. Buehler, Bioinspiredllm: Conversational large language model for the mechanics of biological and bio-inspired materials, Advanced Science, vol. 11, no. 10, p. 2306724, 2024. [1036] Q. Ye, J. Liu, D. Chong, P. Zhou, Y. Hua, F. Liu, M. Cao, Z. Wang, X. Cheng, Z. Lei et al., Qilin-med: Multi-stage knowledge injection advanced medical large language model, arXiv preprint arXiv:2310.09089, 2023. [1037] Z. Wang, Q. Zhang, K. Ding, M. Qin, X. Zhuang, X. Li, and H. Chen, Instructprotein: Aligning human and protein language via knowledge instruction, arXiv preprint arXiv:2310.03269, 2023. [1038] L. Luo, J. Ning, Y. Zhao, Z. Wang, Z. Ding, P. Chen, W. Fu, Q. Han, G. Xu, Y. Qiu et al., Taiyi: bilingual fine-tuned large language model for diverse biomedical tasks, Journal of the American Medical Informatics Association, vol. 31, no. 9, pp. 18651874, 2024. [1039] S. L. Hyland, S. Bannur, K. Bouzid, D. C. Castro, M. Ranjit, A. Schwaighofer, F. Perez-Garcıa, V. Salvatelli, S. Srivastav, A. Thieme et al., Maira-1: specialised large multimodal model for radiology report generation, arXiv preprint arXiv:2311.13668, 2023. [1040] S. Bannur, K. Bouzid, D. C. Castro, A. Schwaighofer, A. Thieme, S. Bond-Taylor, M. Ilse, F. Perez-Garcıa, V. Salvatelli, H. Sharma et al., Maira-2: Grounded radiology report generation, arXiv preprint arXiv:2406.04449, 2024. [1041] P. Qiu, C. Wu, X. Zhang, W. Lin, H. Wang, Y. Zhang, Y. Wang, language model for and W. Xie, Towards building multilingual medicine, Nature Communications, vol. 15, no. 1, p. 8384, 2024. [1042] L. Lv, Z. Lin, H. Li, Y. Liu, J. Cui, C. Y.-C. Chen, L. Yuan, and Y. Tian, Prollama: protein large language model for multitask protein language processing, IEEE Transactions on Artificial Intelligence, 2025. [1043] L. Zhuo, Z. Chi, M. Xu, H. Huang, H. Zheng, C. He, X.-L. Mao, and W. Zhang, Protllm: An interleaved protein-language llm with proteinas-word pre-training, arXiv preprint arXiv:2403.07920, 2024. [1044] K. Saab, T. Tu, W.-H. Weng, R. Tanno, D. Stutz, E. Wulczyn, F. Zhang, T. Strother, C. Park, E. Vedadi et al., Capabilities of gemini models in medicine, arXiv preprint arXiv:2404.18416, 2024. [1045] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick, S. W. Bodenstein, D. A. Evans, C.-C. Hung, M. ONeill, D. Reiman, K. Tunyasuvunakool, Z. Wu, A. ˇZemgulyte, E. Arvaniti, C. Beattie, O. Bertolli, A. Bridgland, A. Cherepanov, M. Congreve, A. I. CowenRivers, A. Cowie, M. Figurnov, F. B. Fuchs, H. Gladman, R. Jain, Y. A. Khan, C. M. R. Low, K. Perlin, A. Potapenko, P. Savy, S. Singh, A. Stecula, A. Thillaisundaram, C. Tong, S. Yakneen, E. D. Zhong, M. Zielinski, A. ˇZıdek, V. Bapst, P. Kohli, M. Jaderberg, D. Hassabis, and J. M. Jumper, Accurate structure prediction of biomolecular interactions with alphafold 3, Nature, vol. 630, no. 8016, pp. 493 -500, 2024. [1046] R. Wang, R. Zhou, H. Chen, Y. Wang, and T. Tan, CareGPT: Medical llm, open source driven for healthy future, https://github.com/ WangRongsheng/CareGPT, 2023. [1047] Z. Liu, A. Zhang, H. Fei, E. Zhang, X. Wang, K. Kawaguchi, and T.-S. Chua, Prott3: Protein-to-text generation for text-based protein understanding, arXiv preprint arXiv:2405.12564, 2024. 93 [1049] [1048] J. Pan, C. Liu, J. Wu, F. Liu, J. Zhu, H. B. Li, C. Chen, C. Ouyang, and D. Rueckert, Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning, arXiv preprint arXiv:2502.19634, 2025. ˇZ. Avsec, N. Latysheva, J. Cheng, G. Novati, K. R. Taylor, T. Ward, C. Bycroft, L. Nicolaisen, E. Arvaniti, J. Pan, R. Thomas, V. Dutordoir, M. Perino, S. De, A. Karollus, A. Gayoso, T. Sargeant, A. Mottram, L. H. Wong, P. Drotar, A. Kosiorek, A. Senior, R. Tanburn, T. Applebaum, S. Basu, D. Hassabis, and P. Kohli, AlphaGenome: advancing regulatory variant effect prediction with unified DNA sequence model, bioRxiv, 2025. [1050] W. Xu, H. P. Chan, L. Li, M. Aljunied, R. Yuan, J. Wang, C. Xiao, G. Chen, C. Liu, Z. Li et al., Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning, arXiv preprint arXiv:2506.07044, 2025. [1051] S. Jia, S. Bit, E. Searls, M. V. Lauber, L. A. Claus, P. Fan, V. H. Jasodanand, D. Veerapaneni, W. M. Wang, R. Au et al., Podgpt: An audio-augmented large language model for research and education, medRxiv, pp. 202407, 2024. [1052] Z. Xiang, GeoGPT: Transforming Paleontology with AI-Powered Data Extraction and Analysis, EGU General Assembly 2025, online, 14239, 2025. [Online]. Available: https://meetingorganizer. copernicus.org/EGU25/EGU25-14239.html?pdf"
        }
    ],
    "affiliations": [
        "Beijing Institute of Heart, Lung and Blood Vessel Diseases",
        "China Pharmaceutical University",
        "Chinese Academy of Sciences",
        "Fudan University",
        "Fuzhou University",
        "Monash University",
        "Purdue University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "South China University",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "The Hong Kong Polytechnic University",
        "The Hong Kong University of Science and Technology",
        "The University of Hong Kong",
        "UNC-Chapel Hill",
        "University College Dublin",
        "University College London",
        "Virginia Tech"
    ]
}