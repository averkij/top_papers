{
    "paper_title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
    "authors": [
        "Ye Tian",
        "Ling Yang",
        "Xinchen Zhang",
        "Yunhai Tong",
        "Mengdi Wang",
        "Bin Cui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 6 4 1 2 1 . 2 0 5 2 : r Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Ye Tian 1 * Ling Yang 2 * Xinchen Zhang 3 Yunhai Tong 1 Mengdi Wang 2 Bin Cui 1 1Peking University 2Princeton University 3Tsinghua University Code: https://github.com/Gen-Verse/Diffusion-Sharpening"
        },
        {
            "title": "Abstract",
            "content": "We propose Diffusion-Sharpening, fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering scalable and efficient solution for future diffusion model fine-tuning. 1. Introduction Diffusion models have emerged as cornerstone of modern generative modeling, achieving state-of-the-art performance in tasks such as text-to-image synthesis and video generation (Ho et al., 2020; Song et al., 2020; Sohl-Dickstein et al., 2015; Ramesh et al., 2022; Rombach et al., 2022; Ho et al., 2022; Blattmann et al., 2023; Zhang et al., 2024a). Despite their success, fine-tuning these models to align with diverse and nuanced user preferences remains fundamental challenge, particularly in domains requiring fine-grained or domain-specific control over generated outputs. *Equal contribution . Correspondence to: Ling Yang <yangling0818@163.com>. Preprint. Figure 1. Comparison of Three Diffusion-Based Methods for Reward-Driven Optimization: (i) Diffusion Reinforcement Learning, (ii) Diffusion Sampling Trajectory Optimization, and (iii) Diffusion Sharpening. Fine-tuning diffusion models to align with predefined evaluation criteria or human preferences remains key challenge. promising approach involves fine-tuning these models using reinforcement learning (RL) through gradient-based optimization during training to optimize reward signals that reflect user-defined objectives (Black et al., 2024; Wallace et al., 2024; Prabhudesai et al., 2023; Xu et al., 2024; Zhang et al., 2024b), as shown in Figure 1 (i).While effective with large-scale curated datasets, these methods focus on optimizing single timesteps output and overlook the potential for optimizing the entire sampling trajectory. Recent approaches extend optimization to the backward 1 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening denoising process, enabling real-time adjustments during diffusion sampling and performing progressive trajectory refinement. As illustrated in Figure 1 (ii), these sampling trajectory optimization methods (Kim et al., 2024; Yeh et al., 2024; Ma et al., 2025) demonstrate that intermediate states along the trajectory can guide generative improvements. However, these methods incur significant computational overhead, with high-quality generation taking up to 40 minutes per image (Yeh et al., 2024), making them impractical for real-world use. To address these limitations, we propose DiffusionSharpening, fine-tuning framework that enhances diffusion model alignment by optimizing the sampling trajectory, as shown in Figure 1(iii). During training, we sample multiple trajectories and compute rewards through path integration, guiding the model to optimize towards the best trajectory. We introduce two implementations: (1) SFT-Diffusion-Sharpening, which uses pre-existing image-text dataset for supervised fine-tuning, enabling optimization with any reward model; and (2) RLHF-DiffusionSharpening, which uses online methods to generate positive and negative samples from denoising outputs, achieving selfguided learning and improved alignment with any reward model through DPO loss. We experimentally demonstrate the effectiveness of our method, showing its efficient convergence during training compared to standard fine-tuning, as well as its high efficiency during inference without the need for additional search costs. Furthermore, Diffusion-Sharpening consistently outperforms RL-based fine-tuning methods and sampling trajectory optimization methods across range of image generation metrics, including text alignment, compositional abilities, and human preferences. Our contributions are summarized as follows: We introduce Diffusion-Sharpening, fundamental and effective trajectory-level optimization-based finetuning method that aligns diffusion models with arbitrary pre-defined rewards. We develop SFT-Diffusion-Sharpening and RLHFDiffusion-Sharpening, with the former providing more efficient SFT pipeline, while the latter eliminates the need for dataset curation in DPO training. Compared to previous fine-tuning-based and sampling trajectory optimization methods, our approach achieves the best training and inference efficiency, while setting state-of-the-art performance across diverse metrics, including text alignment, compositional capabilities, and human preferences. 2 2. Related Work 2.1. Diffusion Alignment Diffusion alignment aims to align model outputs with user preferences by integrating reinforcement learning (RL) into diffusion models to enhance generative controllability (Wallace et al., 2024; Xu et al., 2024; Zhang et al., 2025; Uehara et al., 2024; Yang et al., 2024a). DDPO (Black et al., 2024) uses predefined reward functions to fine-tune diffusion models for specific tasks, such as compressibility. In contrast, DPOK (Fan et al., 2024) utilizes feedback from AI models trained on large-scale human preference datasets. An alternative to predefined rewards is direct preference optimization (DPO). Diffusion-DPO (Wallace et al., 2024) extends DPO (Clark et al., 2024) to diffusion models by directly utilizing preference data for fine-tuning, thereby eliminating the need for predefined reward functions. Despite its potential, Diffusion-DPO relies on large-scale preference datasets and still fails to handle complex generation scenarios. Recent IterComp (Zhang et al., 2024b) address these challenges by gathering composition-aware preference data from set of open-sourced models and aligning with the collected preferences iteratively. 2.2. Diffusion Trajectory Forward Optimization Forward optimization in diffusion trajectories focuses on refining the forward process through carefully designed transition kernels or data-dependent initialization distributions (Liu et al., 2022; Hoogeboom & Salimans, 2022; Dockhorn et al., 2021; Lee et al., 2021; Karras et al., 2022; Yang et al., 2024b). For instance, Rectified Flow (Liu et al., 2022) and Consistency Flow Matching (Yang et al., 2024c) learns straight path connecting the data distribution and the prior distribution, effectively simplifying the denoising process. Grad-TTS (Popov et al., 2021) and PriorGrad (Lee et al., 2021) introduce conditional forward processes with data-dependent priors, specifically designed for audio diffusion models. Other methods like ContextDiff (Yang et al., 2024b) focus on parameterizing the forward process with additional neural networks. For example, Diffusion Models for Video Generation (Zhang & Chen, 2021), Maximum Likelihood Training for Score-based Diffusion Models (Kim et al., 2022), and Variational Diffusion Models (VDM) (Kingma et al., 2021) employ neural architectures to enhance the forward trajectory. 2.3. Diffusion Trajectory Sampling Optimization Beyond forward optimization, recent research has explored real-time optimization during the sampling process, incorporating stochastic optimization techniques to guide the backward sampling trajectory. For instance, MBD (Pan et al., 2024) utilizes score functions to direct the sampling Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening path in the backward process. Similarly, in music generation tasks, SCG (Huang et al., 2024b) employs stochastic optimization to leverage non-differentiable reward functions. Demon (Yeh et al., 2024) focuses on optimizing the sampling process to concentrate sampling density in regions with high rewards during inference. Free2Guide (Kim et al., 2024) uses path integral control to provide gradient-free, non-differentiable reward guidance, enabling the alignment of generated videos with textual prompts without requiring additional model training. Inference-Scaling (Ma et al., 2025) employs verifier and search algorithm to scale diffusion inference beyond NFEs. While these approaches demonstrate significant potential, they often incur substantial computational overhead due to the extra steps required for calculating intermediate rewards during inference. For example, Demon (Yeh et al., 2024) and Inference-Scaling (Ma et al., 2025) may require up to 1000x the inference cost per image to achieve optimal performance. This significant increase in computational cost considerably slows down the generation process, limiting their practicality for real-world applications. 3. Method 3.1. Preliminaries Diffusion Probabilistic Models (Sohl-Dickstein et al., 2015; Song et al., 2020; Ho et al., 2020) learns stochastic process by iteratively denoising random noise generated by the forward diffusion process. Specifically, for any (0, ], the transition distribution is defined as: p(xtx0, c) = p(xtx0) = (αtx0, σ2 I), (1) where x0 RD is D-dimensional data signal variable with an unknown distribution p0(x0c), q(c) is the given condition, and αt, σt R+ are noise scheduler. Foundational works (Kingma et al., 2021; Song et al., 2020) have analyzed the underlying stochastic differential equation (SDE) and ordinary differential equation (ODE) formulations for DPM. The forward and reverse dynamics are given for any [0, ] as: dxt = (xt)dt + g(t)dwt, x0 p0(x0c), dxt = (cid:2)f (xt) g2(t)xt log pt(xtc)(cid:3)dt + g(t)d wt, (2) (3) where wt and wt are standard Wiener processes in forward and reverse time, respectively, and and are functions defined in terms of αt and σt. Practically, DPM performs sampling by solving either the reverse SDE or ODE backward from to 0. To facilitate this, neural network ϵθ(xt, c, t), known as the noise prediction model, is introduced to approximate the conditional score function based on xt and at time t. Specifically, ϵθ(xt, c, t) = σtxt log pt(xtc), and its parameters θ are optimized via the objective: Ex0,ϵ,c,t (cid:2)ωtϵθ(xt, c, t) ϵ2 2 (cid:3) , (4) where ωt is weighting function, ϵ (0, I), q(c), xt = αtx0 + σtϵ, and U[0, ]. 3.2. Diffusion Sharpening In autoregressive language models, performance can be improved through self-improvement, where the model itself acts as validator. Specifically, base model πbase : (Y ), representing conditional distribution, evaluates generated sequences. We refer to sharpening as training the model to produce outputs with higher conditional probabilities shifts the models distribution towards more confident and higher-quality responses. Formally, sharpening model ˆπ(x) is one that (approximately) maximizes the self-reward toward responses that maximize self-reward rself: ˆπ(x) arg max yY rself(y x; πbase). While sharpening in language models focuses on sequencelevel optimization, diffusion alignment typically fine-tunes individual trajectory points, which may lead to suboptimal results. The lack of trajectory-level feedback exposes the generative process to stochastic noise and inconsistencies along the sampling path. To address these challenges, we propose DiffusionSharpening, which leverages online alignment techniques for fine-tuning diffusion models. First, we approximate x0 from intermediate states to evaluate the reward for xt. Then, we perform reward evaluation along the sampling trajectory. To implement this, we introduce two fine-tuning strategies: SFT-Diffusion-Sharpening and RLHF-DiffusionSharpening. Approximate x0 for Reward Evaluation We leverage techniques from EDM (Karras et al., 2022) to approximate the posterior distribution of x0 given an intermediate state xt. Starting from the reverse-time SDEEquation (3), we have x0 = xt + (cid:90) 0 (xc)du + g(u)d wu, (5) where (xc) = (xt) g2(t)xt log pt(xtc) represents the drift term , while wu denotes the stochastic noise component. To simplify this estimation, as shown in (Song et al., 2020; Karras et al., 2022), the reversed-time SDE reduces to PF-ODE when β 0. For each t, diffeomorphic relationship exists between noisy sample xt and clean sample 3 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Figure 2. Overview of Our Diffusion Sharpening Framework: (i) Training, (ii) Inference, and (iii) Reward Model Selection x0 generated by PF-ODE (Yeh et al., 2024): c(xt, t) := x0 = xt + (cid:90) 0 (uxu log p(xuc)) du. (6) For any timestep xt in the diffusion process and given condition c, the reward R(xt, c) is defined as: R(xt, c) = Reward(c(xt, t)), (7) where Reward() represents any reward model, which can be implemented using various forms, such as differentiable neural network, human feedback-based scoring function, or even non-differentiable external model like multimodal LLM. Trajectory-Level Reward Aggregation Fine-tuning based on single trajectory point is often insufficient, as it is highly sensitive to stochastic perturbations in the noise distribution. To address this limitation, we computed and aggregated rewards over selected diffusion sampling trajectories τ rather than individual xt. Specifically, this involves evaluating the reward for different sampled trajectories and selecting the optimal one based on cumulative feedback: ˆτ = arg max τ (cid:88) tτ R(xt, c), (8) where denotes the set of possible trajectories. This approach ensures that the diffusion model learns to generate sampling paths with consistently high rewards, leading to improved sample quality and more robust generative behavior. 3.3. Algorithms for Diffusion Sharpening In this section, we present two families of self-improvement algorithms for diffusion sharpening: SFT Diffusion Sharpening, which filters high-reward responses and performs online fine-tuning using standard supervised learning pipelines, and RLHF Diffusion Sharpening, which refines the sampling trajectory by online optimizing winning and losing sets through reinforcement learning techniques, such as Diffusion-DPO (Wallace et al., 2024). SFT Diffusion Sharpening In the language model framework, SFT-Sharpening (Huang et al., 2024a) filters responses with large self-reward values and applies standard supervised fine-tuning to the resulting high-quality samples. Similarly, in the pretraining or supervised fine-tuning (SFT) of text-to-image diffusion models, large image-text dataset is filtered through selected reward models, retaining the highest-scoring image-text pairs for training. However, this direct fine-tuning process only captures the preferences of the final output generated by the diffusion model, relying solely on single random timestep and backpropagating with v-loss (Salimans & Ho, 2022) or ϵ-loss (Ho et al., 2020). We argue that this approach fails to fully exploit the potential of each sample (image, text), as one timesteps v-prediction or ϵ-prediction cannot represent the entire denoising trajectory. To address this limitation, as discussed in Section 3.2, we propose redesigned SFT Diffusion Sharpening process that fully utilizes the sampling trajectory for each sample (image, text), prestented in Algorithm 1. Specifically, consider collection of image-text pairs (x, c) from dataset D. For each prompt (x, c), we randomly sample noise vectors z1, . . . , zn (0, 1), and then randomly select timestep t. Noise is added to the image to generate xi t, where {1, . . . , }. We then perform sampling on the noisy images xi for steps and collect the corresponding sampling trajectories. Afterward, we select the optimal trajectory based on reward feedback. Finally, we backpropagate the gradients using the loss from the m-step path = ExT ,ϵ,c,T [t,tm] (cid:2)ωT ϵθ(xT , c, ) ϵ2 (cid:3) . (9) RLHF Diffusion Sharpening RLHF Diffusion Sharpening Algorithm 2 aims to optimize conditional distribution pθ(xtc) such that the reward model R(xt, c) defined on it is maximized while regularizing the KL-divergence from reference distribution pref 4 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Algorithm 1 SFT Diffusion Sharpening Algorithm 2 RLHF Diffusion Sharpening Input: dataset D, number of samples n, number of steps m, reward model R, diffusion model Mθ, learning rate η for each image-text pair (x, c) in do Sample random noise vectors z1, . . . , zn (0, 1) Randomly select timestep Add noise to the image to generate xi {1, . . . , } for each noisy image xi for do Perform steps of sampling from xi Calculate R(xt, c) in Equation (7) Collect the sampling trajectory τi = {xtk }m k=1 end for Select the optimal trajectory ˆτ with Equation (8) Compute the loss in Equation (9) Mθ Mθ ηθL end for max θ EcDc,x0pθ(x0c) [r(c, x0)] βDKL [pθ(x0c) pref(x0c)] (10) For its efficiency, we adopt Diffusion-DPO (Wallace et al., 2024) to implement RLHF Diffusion Sharpening, aiming to fully leverage the models self-evolution capabilities. Instead of relying on predefined image-text pairs, we construct the dataset online by generating latent samples and applying noise perturbations during training. Similar to SFT Diffusion Sharpening, after selecting set of noisy samples xi and their corresponding trajectories, we use reward model to identify the best and worst trajectories, τw and τl, respectively. To maximize the use of prior reward information, we optimize the model using the reward-modulated DPO loss (Gao et al., 2024). LRLHF(θ) = Exwτw,xlτl,c (cid:20) (cid:18) log σ β log pθ(xw c) pref(xw c) (R(xw, c) R(xl, c)))] β log (cid:19) pθ(xl c) pref(xl c) (11) 4. Experiments 4.1. Implemention Details Baseline Models We conduct diffusion sharpening finetuning on SDXL (Podell et al., 2023) for fair comparison, using the default configuration with DDIM Scheduler, = 50 steps, and classifier-free guidance with scale of = 5. For comparison with fine-tuning methods, we select 5 Input: prompt dataset D, number of samples n, number of steps m, reward model R, diffusion model Mθ, learning rate η for each training iteration do Sample prompt and generate latents with Mθ Sample random noise vectors z1, . . . , zn (0, 1) Randomly select timestep and add noise to generated latents for noisy latent samples xi for each noisy sample xi do Perform steps of sampling from xi Evaluate reward R(xt, c) using the reward model Collect the sampling trajectory τi = {xtk }m k=1 end for Identify best and worst trajectories: (cid:80) tτ R(xt, c), τw = arg maxτ (cid:80) τl = arg minτ tτ R(xt, c), Compute LRLHF(θ) using Equation (11) Update model parameters: Mθ Mθ ηθLDPO end for five established approaches: (1) Standard Fine-tuning1, traditional fine-tuning using predefined image-text dataset; (2) Diffusion-DPO (Wallace et al., 2024), fine-tuning based on human preference datasets; (3) DDPO (Black et al., 2024), reward model-based reinforcement fine-tuning; (4) D3PO (Yang et al., 2024a), fine-tuning using human feedback without reward model; and (5) IterPO (Zhang et al., 2024b), iterative alignment of composition-aware model preferences introduced in IterComp (Zhang et al., 2024b). For comparison with sampling trajectory optimization methods, we select: (1) Demon (Yeh et al., 2024), which recalculates the optimal noise at each denoising timestep to optimize inference; (2) Free2Guide (Kim et al., 2024), an inference optimization method for video generation that searches for optimal noise over 1/10 of the timesteps; and (3) Inference Scaling (Ma et al., 2025), which performs inference using search and verifier mechanism. These methods are adapted to SDXL with default settings as described in their respective papers. More Details are provided in Appendix A.1. Datasets For SFT Diffusion-Sharpening, we use two highquality text-to-image datasets: JourneyDB (Pan et al., 2023) and Text-to-Image-2M (zk, 2024), which contain large number of image-text pairs, ideal for evaluating the benefits of sharpening over baseline SFT methods. Additionally, we employ the domain-specific dataset Pokemon-BlipCaption (lambdalabs, 2023) to assess sharpenings effectiveness in personalized scenarios, measuring its adaptabil1https://github.com/huggingface/ diffusers/blob/main/examples/text_to_image/ train_text_to_image_sdxl.py Model SDXL Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Table 1. Comparison of Model Performance across Multiple Metrics CLIP Score T2I-Compbench Color Shape Texture Spatial Non-Spatial Complex Aesthetic ImageReward MLLM 0.322 0.6369 0.5408 0.5637 0.2032 0. 0.4091 5.531 Standard Fine-tuning Diffusion DPO (Wallace et al., 2024) DDPO (Black et al., 2024) D3PO (Yang et al., 2024a) IterPO (Zhang et al., 2024b) Free2Guide (Kim et al., 2024) Demon (Yeh et al., 2024) Inference Scaling (Ma et al., 2025) SFT Diffusion Sharpening RLHF Diffusion Sharpening 0.325 0.334 0.324 0.328 0. 0.325 0.325 0.328 0.334 0.338 Fine-tuning based Methods 0.6437 0.6602 0.6435 0.6434 0.6637 0.5771 0.5553 0.5365 0.5435 0.5593 0.5692 0.5640 0.5531 0.5657 0. 0.2084 0.2112 0.2030 0.2114 0.2128 0.3147 0.3180 0.3142 0.3153 0.3207 Sampling Trajectory Optimization Methods 0.6321 0.6502 0.6550 0.6578 0.6841 0.5386 0.5507 0. 0.5692 0.5680 0.5548 0.5602 0.5700 0.5733 0.6401 0.2050 0.2150 0.2204 0.2120 0.2134 0.3125 0.3158 0. 0.3185 0.3220 0.4100 0.4055 0.4024 0.4102 0.4377 0.4082 0.4070 0.4265 0.4125 0.4498 5.556 5.754 5.640 5.528 5.923 5.560 5.630 5. 5.785 5.956 0.780 0.791 1.352 0.910 0.982 1.408 0.873 1.243 1.329 1.301 1.445 0. 0.784 0.864 0.791 0.785 0.884 0.786 0.300 0.872 0.864 0.921 ity while preserving output quality. For RLHF DiffusionSharpening, no image data is required during training as online optimization relies solely on prompts. We randomly sample 10,000 prompts from DrawBench (Saharia et al., 2022), DiffusionDB (Wang et al., 2022), and prompts from the SFT datasets for fine-tuning. More details are included in Appendix A.2 Reward Models We evaluate the performance of various reward models in diffusion sharpening, analyzing their effectiveness and efficiency across tasks: (1) CLIP Score (Radford et al., 2021), used to evaluate text-image alignment, (2) Compositional rewards from IterComp (Zhang et al., 2025), which assess the models ability to handle compositional prompts such as object relationships and attributes, (3) MLLM grader (Ma et al., 2025), specifically prompted GPT-4o, detailed in Appendix C, which provide holistic image scoring across multiple dimensions to improve overall quality, and (4) Human Preferences. We employ ImageReward (Xu et al., 2024), reward model trained to align with human preferences, to evaluate satisfaction with text-image alignment, aesthetic quality, and harmlessness. Evaluation Metrics We use several key metrics to evaluate the performance of our models: (1) CLIP Score (Radford et al., 2021), which measures text-image alignment, (2) Aesthetic Score from DrawBench (Saharia et al., 2022), assessing the visual appeal and quality of the generated image (3) T2I-Compbench (Huang et al., 2023), to evaluate the compositional capabilities and (4) ImageReward (Xu et al., 2024), which evaluates how well the generated images align with human preferences, including text-image consistency, aesthetic quality, and overall satisfaction. We also report scores used in the MLLM grader for overall evalution. Additionally, due to the inherent subjectivity in evaluating image generation tasks, we conducted an extensive user study to complement our quantitative metrics in Appendix B. 4.2. Main Results Comparison with Fine-tuning based Methods In the quantitative analysis, we compare various methods, as shown in Table 1. We train Diffusion-Sharpening on different reward models and report the corresponding evaluation metrics. Notably, the Aesthetic score is derived as the average result across 4 rewards corresponding model. Our approach outperforms Diffusion-DPO and D3PO in human preference evaluations and generalizes to any reward model. Compared to DDPO, which also uses reward modelbased fine-tuning, our sharpening method optimizes the most relevant reward path, further improving overall performance. Compared to IterPO, our method achieves improved image compositionality, further enhancing models alignment with complex compositional prompts. As seen in the table, RLHF-Diffusion-Sharpening consistently achieves top results across all evaluation metrics, demonstrating exceptional generalization and adaptability to diverse reward models. Qualitative results, presented in Figure 3, show that our model leverages multiple reward models tailored to specific needs, improving text-image alignment, compositional abilities, human preferences, and MLLM assessments. RLHF-Diffusion-Sharpening, in particular, excels in both qualitative and quantitative performance. These improvements stem from the base models extensive pretraining on large datasets. In SFT-Sharpening, the standard epsilon-loss converges quickly, leaving little room for further enhancement. However, RLHF-Diffusion-Sharpening, through DPO loss, better separates good and bad trajectories, offering greater optimization potential. Comparison with Sampling Trajectory Optimization Methods We also compare with sampling trajectory optimization methods. As shown in Table 1, Free2Guide provides slight improvements in image generation, but its performance is limited. Demon and Inference Scaling improve by increasing inference steps (NFE), but our method achieves superior quantitative results while effectively amor6 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Figure 3. Qualitative results comparing Diffusion Sharpening methods using different reward models. The images show the generated results with CLIP Score, Compositional Reward, MLLM, and Human Preferences as reward models, showcasing the effectiveness of SFT Diffusion Sharpening and RLHF Diffusion Sharpening in diffusion finetuning. Figure 4. SDXL Finetuning Loss across Difference Datasets. Here Diffusion-Sharpening represents SFT Diffusion-Sharpening specifically. 7 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening tizing inference costs, demonstrating efficiency and validity. 4.3. Model Efficiency Training Efficiency Diffusion Sharpening significantly enhances model efficiency through trajectory-level optimization. During the training phase, we set τ = 3 and = 3 random noise vectors, with learning rate of 1 106, comparing it to the standard SDXL fine-tuning pipeline. We reported the fitted loss curve in Figure 4. As is shown, Diffusion-Sharpening leads to faster convergence, typically within 500 to 1000 steps, whereas the baseline requires 1000 to 1500 steps to achieve similar results. The training curve for Diffusion Sharpening is smoother and achieves better final convergence with lower final loss. These results demonstrate that Diffusion Sharpening enables faster, more stable, and superior fine-tuning compared to standard diffusion pipelines. Figure 5. Inference Performance of Diffusion Sharpening. Inference Efficiency Beyond training efficiency, our method also achieves optimal inference performance. Using CLIP Score as the reward model during inference, we evaluate SDXL with the default 100 NFE. As shown in Figure 5, all sampling trajectory optimization methods improve performance as NFE increases. However, the computational cost rises sharply, with methods like Demon and Inference Scaling requiring over 10,000 NFE, leading to inference times of several hours per imagerendering them impractical for real-world use. In contrast, our method integrates inference optimization into training, focusing on refining the sampling trajectory. This allows it to achieve superior performance within the same inference time as the baseline SDXL, demonstrating its efficiency. 4.4. Ablation Study Effect of Sampling Trajectory Optimization To validate the optimization process along the sampling trajectory, we conducted an ablation study focusing on Sampling Trajec8 Figure 6. Diffusion Sharpening Fine-tuning Reward Curve. tory Optimization. During training, we log reward results for both SFT and RLHF Diffusion Sharpening. As shown in Figure 6, we track reward scores over the first 1000 SDXL fine-tuning steps, with the shaded region representing the standard deviation across multiple sampled trajectories at each step. The results show steady increase in average reward and decrease in variance as training progresses, indicating the models convergence toward more optimal paths. This confirms the effectiveness of our approach in enhancing both stability and performance during training. Analysis of the Number of Samples We analyze the effect of the sampling number of samples during training and set the number of steps = 1 for comparison. As is shown in Table 2, number of samples of 1 corresponds to standard DPO finetuning pipeline and we find an optimal number of samples = 3 for the final training configuration. Table 2. Performance of Different Number of Samples in Training Number of Steps CLIP Score 1 2 3 4 8 0.334 0.336 0.338 0.336 0. ImageReward MLLM 0.864 0.891 0.921 0.911 0.919 1.352 1.355 1.445 1.446 1.444 Analysis of the Number of Steps We also analyze the effect of the sampling number of steps during training in Table 3 after choosing the number of samples n. number of steps of 1 corresponds to standard end-to-end finetuning baseline. The results show that increasing the number of steps leads to improved model performance. We set the number of steps to 3 for balancing cost and performance. Table 3. Performance of Different Number of Steps in Training Number of Steps CLIP Score 1 2 3 4 0.322 0.328 0.338 0.334 0.321 ImageReward MLLM 0.897 0.902 0.921 0.923 0.912 1.321 1.357 1.445 1.442 1.376 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening 5. Conclusion In this work, we propose Diffusion-Sharpening, novel fine-tuning approach that optimizes diffusion model performance by refining sampling trajectories. Our method addresses the limitations of existing approaches by enabling trajectory-level optimization through alignment with arbitrary reward models, while effectively amortizing the high inference costs. We introduce two variants: SFTDiffusion-Sharpening, which leverages supervised finetuning for efficient backward trajectory optimization, and RLHF-Diffusion-Sharpening, which eliminates the need for curated datasets and performs online trajectory optimization. Through extensive experiments, we demonstrate superior training efficiency as well as inference efficiency. Across diverse metrics, our Diffusion-Sharpening consistently outperforms existing fine-tuning methods and sampling trajectory optimization approaches."
        },
        {
            "title": "References",
            "content": "Black, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. Align your latents: Highresolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023. Clark, K., Vicol, P., Swersky, K., and Fleet, D. J. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. Dockhorn, T., Vahdat, A., and Kreis, K. Score-based generative modeling with critically-damped langevin diffusion. In International Conference on Learning Representations, 2021. Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P., Ghavamzadeh, M., Lee, K., and Lee, K. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Gao, Z., Chang, J. D., Zhan, W., Oertell, O., Swamy, G., Brantley, K., Joachims, T., Bagnell, J. A., Lee, J. D., and Sun, W. Rebel: Reinforcement learning via regressing relative rewards. arXiv preprint arXiv:2404.16767, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Hoogeboom, E. and Salimans, T. Blurring diffusion models. In The Eleventh International Conference on Learning Representations, 2022. Huang, A., Block, A., Foster, D. J., Rohatgi, D., Zhang, C., Simchowitz, M., Ash, J. T., and Krishnamurthy, A. Self-improvement in language models: The sharpening mechanism. arXiv preprint arXiv:2412.01951, 2024a. Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2icompbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. Huang, Y., Ghatare, A., Liu, Y., Hu, Z., Zhang, Q., Sastry, C. S., Gururani, S., Oore, S., and Yue, Y. Symbolic music generation with non-differentiable rule guided diffusion. arXiv preprint arXiv:2402.14285, 2024b. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 2656526577, 2022. Kim, D., Na, B., Kwon, S. J., Lee, D., Kang, W., and Moon, I.-c. Maximum likelihood training of implicit nonlinear diffusion model. Advances in Neural Information Processing Systems, 35:3227032284, 2022. Kim, J., Kim, B. S., and Ye, J. C. Free2Guide: Gradient-free path integral control for enhancing text-to-video generation with large vision-language models. arXiv preprint arXiv:2411.17041, 2024. Kingma, D., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. lambdalabs. pokeman-blip-captions, 2023. URL https://huggingface.co/datasets/ lambdalabs/pokemon-blip-captions. Lee, S.-g., Kim, H., Shin, C., Tan, X., Liu, C., Meng, Q., Qin, T., Chen, W., Yoon, S., and Liu, T.-Y. Priorgrad: Improving conditional denoising diffusion models with data-dependent adaptive prior. In International Conference on Learning Representations, 2021. Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022. 9 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Ma, N., Tong, S., Jia, H., Hu, H., Su, Y.-C., Zhang, M., Yang, Inference-time X., Li, Y., Jaakkola, T., Jia, X., et al. scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Pan, C., Yi, Z., Shi, G., and Qu, G. Model-based diffusion for trajectory optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Pan, J., Sun, K., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z., Wang, Y., Dai, J., Qiao, Y., and Li, H. Journeydb: benchmark for generative image understanding, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-tts: diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pp. 85998608. PMLR, 2021. Prabhudesai, M., Goyal, A., Pathak, D., and Fragkiadaki, K. Aligning text-to-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494, 2022. Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022. Progressive distillation for arXiv preprint Uehara, M., Zhao, Y., Black, K., Hajiramezanali, E., Scalia, G., Diamant, N. L., Tseng, A. M., Levine, S., and Biancalani, T. Feedback efficient online fine-tuning of diffusion models. In Forty-first International Conference on Machine Learning, 2024. Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Purushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik, N. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Wang, Z. J., Montoya, E., Munechika, D., Yang, H., Hoover, B., and Chau, D. H. Diffusiondb: large-scale prompt gallery dataset for text-to-image generative models. arXiv preprint arXiv:2210.14896, 2022. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Yang, K., Tao, J., Lyu, J., Ge, C., Chen, J., Shen, W., Zhu, X., and Li, X. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024a. Yang, L., Zhang, Z., Yu, Z., Liu, J., Xu, M., Ermon, S., and Bin, C. Cross-modal contextualized diffusion models for text-guided visual generation and editing. In The Twelfth International Conference on Learning Representations, 2024b. Yang, L., Zhang, Z., Zhang, Z., Liu, X., Xu, M., Zhang, W., Meng, C., Ermon, S., and Cui, B. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024c. Yeh, P.-H., Lee, K.-H., and Chen, J.-C. Training-free diffusion model alignment with sampling demons. arXiv preprint arXiv:2410.05760, 2024. Zhang, Q. and Chen, Y. Diffusion normalizing flow. In NeurIPS, volume 34, pp. 1628016291, 2021. 10 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Zhang, X., Yang, L., Cai, Y., Yu, Z., Wang, K.-N., Tian, Y., Xu, M., Tang, Y., Yang, Y., Bin, C., et al. Realcompo: Balancing realism and compositionality improves textto-image diffusion models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. Zhang, X., Yang, L., Li, G., Cai, Y., Xie, J., Tang, Y., Yang, Y., Wang, M., and Cui, B. Iterative composition-aware feedback learning from model arXiv preprint gallery for text-to-image generation. arXiv:2410.07171, 2024b. Itercomp: Zhang, X., Yang, L., Li, G., Cai, Y., Xie, J., Tang, Y., Itercomp: Iterative Yang, Y., Wang, M., and Cui, B. composition-aware feedback learning from model gallery for text-to-image generation. In International Conference on Learning Representations, 2025. zk. text-to-image-2m, 2024. URL https: //huggingface.co/datasets/jackyhate/ text-to-image-2M. 11 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening A. Implemantation Details A.1. Baseline Models Configuration In this section, we describe the configurations of different baseline models used in our study. We adopt the original model implementations whenever possible. For models that are not open-sourced or not directly compatible with SDXL, we perform minimal adaptations based on the original papers. Diffusion-DPO (Wallace et al., 2024):Re-formulate Direct Preference Optimization (DPO) for diffusion models by incorporating likelihood-based objective, utilizing the evidence lower bound to derive differentiable optimization process. Using the Pick-a-Pic dataset containing 851K crowdsourced pairwise preferences, they fine-tuned the base SDXL-1.0 model with Diffusion-DPO. We directly use the pretrained Diffusion-DPO on SDXL. DDPO (Black et al., 2024): This method optimizes diffusion models directly on downstream objectives using reinforcement learning (RL). By framing denoising diffusion as multi-step decision-making process, it enables policy gradient algorithms referred to as Denoising Diffusion Policy Optimization. We adapt the original implementation to SDXL and use aesthetic quality as the optimization metric for fine-tuning. D3PO (Yang et al., 2024a): This approach omits the training of reward model and instead functions as an optimal reward model trained using human feedback data to guide the learning process. By eliminating the need for explicit reward model training, D3PO proves to be more direct and computationally efficient solution. IterPO (Zhang et al., 2024b): This method is the alignment framework of IterComp (Zhang et al., 2024b), which collects composition-aware model preferences from multiple models and employ an iterative feedback learning approach to enable the progressive self-refinement of both the base diffusion model and reward models. Demon (Yeh et al., 2024): This method guides the denoising process at inference time without backpropagation through reward functions or model retraining. We adapt the original method to SDXL using the EDM scheduler with the tanh-demon configuration, setting fixed inference cost of five minutes per image generation. Free2Guide (Kim et al., 2024): gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free2Guide approximates guidance for diffusion models using non-differentiable reward functions. Since the original work focuses on video models, we directly adapt the provided pseudo-code to SDXL with DDIM scheduler. Experiments are conducted on randomly selected = 5 inference steps, maintaining the same reward model settings. Inference-Scaling (Ma et al., 2025): This method formulates search problem to identify better noise initializations for the diffusion sampling process. The design space is structured along two axes: the verifiers providing feedback and the algorithms searching for optimal noise candidates. While the original paper evaluates this approach on FLUX.1-DEV, we adapt the pseudo-code to SDXL, maintaining fixed inference cost of five minutes and using the same verifier configurations for evaluation. A.2. Datasets We utilize multiple datasets for training and evaluation, covering diverse range of text-to-image tasks. Below, we describe each dataset used in our experiments: JourneyDB (Pan et al., 2023): large-scale collection of high-resolution images generated by Midjourney. This dataset contains diverse and detailed text descriptions that capture wide range of visual attributes, enabling robust multi-modal training. Text-to-Image-2M (zk, 2024): curated text-image pair dataset designed for fine-tuning text-to-image models. The dataset consists of approximately 2 million samples, carefully selected and enhanced to meet the high demands of text-to-image model training. Pokemon-Blip (lambdalabs, 2023): dataset containing unique Pokemon images labeled with BLIP-generated captions. It is specifically designed to evaluate adaptation to seen data and assess the models convergence capabilities. Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening DiffusionDB (Wang et al., 2022): The first large-scale text-to-image prompt dataset, containing 14 million images generated by Stable Diffusion using user-specified prompts and hyperparameters. The unprecedented scale and diversity of this human-actuated dataset provide valuable research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to improve model usability. DrawBench (Saharia et al., 2022): comprehensive and challenging benchmark for text-to-image models, introduced by the Imagen research team. It consists of 200 prompts spanning 11 diverse categories. The benchmark evaluates text-to-image models ability to handle complex prompts and generate realistic, high-quality images. During evaluation, we generate one image per prompt. A.3. Training Settings We train our models with carefully optimized settings to ensure stable and efficient training. We use the AdamW optimizer without weight decay, configured with beta parameters (β1 = 0.0, β2 = 0.99). The learning rate is set to 5 106, reflecting the distinct requirements of each modality. Both diffusion sharpening models are trained with batch size of 8. A.4. Evaluation Settings For MLLM Grader, we prompt the GPT-4o model to assess synthesized images from five different perspectives: Accuracy to Prompt, Originality, Visual Quality, Internal Consistency, and Emotional Resonance following (Ma et al., 2025). Each perspective is rated from 0 to 100, and the averaged overall score is used as the final metric. In Figure 8 we present the detailed prompt. We observe that search can be beneficial to each scoring category of the MLLM Grader. T2I-CompBench. For each prompt we search for two noises and generate two samples. During evaluation, the samples are splitted into six categories: color, shape, texture, spatial, numeracy, and complex. Following Huang et al. (2023), we use the BLIP-VQA model for evaluation in color, shape, and texture, the UniDet model for spatial and numeracy, and weighted averaged scores from BLIP VQA, UniDet, and CLIP for evaluating the complex category. B. User Study Figure 7. User Study about Comparision with Other Methods To verify the effectiveness of our proposed Diffusion-Sharpening, we conduct an extensive user study across various scenes and models. Users compared model pairs by selecting their preferred video from three options: method 1, method 2, and comparable results. As presented in Figure 7, our method (orange in left) obtains more user preferences than others (blue in right), which further proving its effectiveness. Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening C. MLLM Grader Design Consider For each score, include Your goal is to assess each generated Key Evaluation Aspects and Scoring Criteria: \"You are multimodal large-language model tasked with evaluating images generated by text-to-image model. image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. Below is comprehensive guide to follow in your evaluation process: 1. For each aspect, provide score from 0 to 10, where 0 represents poor performance and 10 represents excellent performance. short explanation or justification (1-2 sentences) explaining why that score was given. The aspects to evaluate are as follows: a) Accuracy to Prompt Assess how well the image matches the description given in the prompt. whether all requested elements are present and if the scene, objects, and setting align accurately with the text. Score: 0 (no alignment) to 10 (perfect match to prompt). b) Creativity and Originality Evaluate the uniqueness and creativity of the generated image. Does the model present an imaginative or aesthetically engaging interpretation of the prompt? Is there any evidence of creativity beyond literal interpretation? (lacks creativity) to 10 (highly creative and original). c) Visual Quality and Realism Assess the overall visual quality, including resolution, detail, and realism. Look for coherence in lighting, shading, and perspective. stylized or abstract, judge whether the visual elements are well-rendered and visually appealing. Score: 0 (poor quality) to 10 (high-quality and realistic). d) Consistency and Cohesion Check for internal consistency within the image. aligned with the prompt? For instance, does the perspective make sense, and do objects fit naturally within the scene without visual anomalies? (inconsistent) to 10 (fully cohesive and consistent). e) Emotional or Thematic Resonance Evaluate how well the image evokes the intended emotional or thematic tone of the prompt. For example, if the prompt is meant to be serene, does the image convey calmness? If its adventurous, does it evoke excitement? resonance) to 10 (strong resonance with the prompts theme). 2. After scoring each aspect individually, provide an overall score, representing the models general performance on this image. average based on the importance of each aspect to the prompt or an average of all aspects.\" Are all elements cohesive and This should be weighted Even if the image is Overall Score Score: Score: Score: 0 (no 0 Figure 8. The detailed prompt for evaluation with the MMLLM Grader. Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening D. More Qualitative Results Figure 9. More Qualitative Results for SFT Diffusion-Sharpening. 15 Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening Figure 10. More Qualitative Results for RLHF Diffusion-Sharpening."
        }
    ],
    "affiliations": [
        "Peking University",
        "Princeton University",
        "Tsinghua University"
    ]
}