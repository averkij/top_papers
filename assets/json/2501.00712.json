{
    "paper_title": "Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding",
    "authors": [
        "Jiajun Zhu",
        "Peihao Wang",
        "Ruisi Cai",
        "Jason D. Lee",
        "Pan Li",
        "Zhangyang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset. To address this, we propose con$\\textbf{T}$extualized equivari$\\textbf{A}$nt $\\textbf{P}$osition $\\textbf{E}$mbedding ($\\textbf{TAPE}$), a novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving robustness and adaptability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments shows that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 2 1 7 0 0 . 1 0 5 2 : r a"
        },
        {
            "title": "RETHINKING ADDRESSING IN LANGUAGE MODELS\nVIA CONTEXTUALIZED EQUIVARIANT POSITIONAL\nENCODING",
            "content": "Jiajun Zhu1,2, Peihao Wang1*, Ruisi Cai 1, Jason D. Lee3, Pan Li4, Zhangyang Wang1 1University of Texas at Austin, 2Zhejiang University, 3Princeton University, 4Georgia Tech junnian@zju.edu.cn, {peihaowang,ruisi.cai,atlaswang}@utexas.edu, jasonlee@princeton.edu, panli@gatech.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within dataset. To address this, we propose conTextualized equivariAnt Position Embedding (TAPE), novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving robustness and adaptability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments shows that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques."
        },
        {
            "title": "INTRODUCTION",
            "content": "Attention mechanisms are core component of many modern deep learning architectures, enabling models to selectively focus on relevant information within given context. Transformer models (Vaswani et al., 2017) and their numerous variants (Carion et al., 2020; Dosovitskiy et al., 2021; Zhao et al., 2021), which are fundamentally driven by attention, have revolutionized tasks involving sequential and spatial data, such as text (Kitaev et al., 2020), image (Dosovitskiy et al., 2021), and point cloud (Zhao et al., 2021). More recently, large transformer models have become dominant in natural language understanding, language generation, and complex reasoning (Brown et al., 2020). Delving into attentions underlying paradigm, the prediction made for each token is expressed as weighted aggregation over the representations of other tokens. Due to the softmax function, attention often generates sparse mask, extracting limited subset of tokens for interaction. Through this interpretation, attention can be understood as an addressing mechanism that searches the context, locating and retrieving token representations deemed most relevant or important (Hopfield, 1982; Pagiamtzis & Sheikholeslami, 2006). Since the attention score is computed upon token features and positions (see Section 2), transformers addressing ability can be further decomposed into two fundamental mechanisms: content-based addressing and position-based addressing. Content-based addressing recognizes relevant tokens through feature similarity, while position-based addressing is facilitated by positional encoding techniques, designed to (ideally) enable random access along the sequence via indexing. It is important to let the two mechanisms cooperate to tackle more complex tasks, such as in-context retrieval (Hinton & Anderson, 2014; Ba et al., 2016), arithmetic (Lee et al., 2023; McLeish et al., 2024b), counting Equal contribution. Work done while J. Zhu was interning at UT Austin. 1Code is available at https://github.com/VITA-Group/TAPE 1 (Golovneva et al., 2024), logical computation (Liu et al., 2024), and reasoning (Wei et al., 2022; Rajani et al., 2019; Dziri et al., 2024). However, we contend that the role of position-based addressing is limited, if not diminishing, in modern transformer architectures (Ebrahimi et al., 2024). It has not escaped our notice that most existing positional encodings weakens the position-based addressing capability. Recent works (Press et al., 2021b; Su et al., 2024; Chi et al., 2022b; Sun et al., 2022) impose fixed and somewhat artisanal pattern on attention maps, typically adopting decaying pattern in relation to relative distances, thereby enforcing locality bias. This rigidity limits the ability of positional encodings to model long-range dependencies and makes it challenging to attend to distant query-key pairs. Although some positional encodings have trainable parameters (Vaswani et al., 2017; Shaw et al., 2018; Chi et al., 2022a; Li et al., 2023), the hypothesis space is often excessively constrained. Perhaps more crucially, most existing positional encodings are designed and learned as general bias across the entire dataset, lacking specialization and adaptability to specific instances informed by the context. The interplay between context and positional embeddings has proven essential in LLMs for various compositional tasks such as algorithmic (McLeish et al., 2024a), language modeling and coding tasks (Golovneva et al., 2024). Recent studies indicate that token indices can be reconstructed through causal attention, suggesting the elimination of positional encoding (Haviv et al., 2022; Wang et al., 2024b; Kazemnejad et al., 2024). However, their arguments require specific configuration of transformer weights, which may not be achievable. To unleash the power of position-based addressing, we endeavor to design more universal and generic position encoding for language transformers. We introduce Contextualized Equivariant Positional Encoding (TAPE), novel framework designed to contextualize positional embeddings by incorporating sequence content. TAPE continually progresses information flow between positional embeddings and token features via specialized attention and MLP layers. To ensure the stability during model updates, we enforce permutation and orthogonal group equivariance properties on attention and MLP layers. This approach is inspired from the studies for geometric deep learning which processes graphs and point clouds by integrating token features with their geometric properties while preserving inherent physical symmetries (Wang et al., 2024c; Huang et al., 2024). By enforcing these properties, TAPE ensures robustness to input permutations and translations in sequences, while maintaining the relative relationships between encoded positions. This design greatly enhances the models capacity to generalize across diverse domains. Technically, we extend conventional vectorized positional embeddings into multi-dimensional tensor, which enriches interactions between positional embeddings and token features. In the attention mechanism, TAPE incorporates the pairwise inner product between positional encodings, allowing attention values to be computed based on not only token similarities but also positional proximities. We adiditionaly customize an MLP layer that directly mixes token features with positional encodings, while preserving orthogonal equivariance. We demonstrate the superior performance of TAPE on arithmetic reasoning tasks (McLeish et al., 2024a), which require LLMs to effectively locate/address and retrieve specific tokens, as well as on representative natural language tasks, including SCROLLS (Shaham et al., 2022) and passkey retrieval (Mohtashami & Jaggi, 2023), to validate the generalizability of the framework. Our contributions are summarized as follows: We introduce TAPE, novel framework to contextualize positional embeddings with sequence content across layers to enhance the position-addressing ability of transformers. We further enforce TAPE with permutation and orthogonal equivariance to guarantee the stability of positional encodings during the update. We propose practical implementations for our TAPE, which extends conventional positional embeddings into multi-dimensional and facilitates attention and MLP in transformers with two levels of equivariance. We also show that TAPE can be used as drop-in component into extant pre-trained models for parameter-efficient fine-tuning. Extensive experiments showcase TAPEs superioroty in both training from scratch and parameter-efficient fine-tuning scenarios for language modeling as well as downstream tasks such as arithmetic reasoning and long-context retrieval. TAPE achieves state-of-theart performance in language modeling, surpassing baselines in perplexity reduction for long sequences. We also report the state-of-the-art performance of TAPE in long-context tasks such as passkey retrieval tasks with LLM fine-tuning, and in arithmetic learning."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In this work, we aim to design expressive and generalizable positional embeddings for transformers to address complex language tasks. Let = [x1 xN ] RN represent the input sequence of tokens, where is the context length and is the feature dimension. Transformers learn token representations using the attention mechanism (Vaswani et al., 2017), which propagates information across tokens by computing pairwise correlations. Since pure attention is inherently permutationequivariant, language models integrate positional information into the attention computation to differentiate tokens based on their positions."
        },
        {
            "title": "2.1 HIGH-DIMENSIONAL FEATURES AS POSITIONAL ENCODING",
            "content": "One common approach is to leverage high-dimensional features to represent positions. Positional encoding can be formulated as series of embeddings attached to each token index e1 eN , with the shape of ei determined by the specified positional encoding schemes. When computing the attention value, the pre-softmax attention value can be in general formulated as 2: αi,j = q(xi, ei)k(xj, ej), (1) where q(, ) and k(, ) are generalized query and key transformations that incorporate positional features. The original transformer paper (Vaswani et al., 2017) assigns each absolute token index vector of length identical to token embeddings, either learnable or fixed as sinusoidal waves: ei RC. The query and key transformations directly add the positional information into token features at the first layer: q(xi, ei) = WQ(xi +ei) and k(xj, ej) = WK(xj +ej) for some query and key matrices WQ, WK RCC. Shaw et al. (2018) introduces learnable embeddings for relative distances, which are applied to the key vector during attention computation. More recently, Rotary Position Encoding (RoPE) (Su et al., 2024) has gained widespread adoption in modern LLMs (Touvron et al., 2023a;b; Biderman et al., 2023; Chowdhery et al., 2023; Jiang et al., 2023). RoPE encodes absolute positions using series of block-wise rotation matrices RN C/222, while implicitly capturing relative distances during dot-product attention. Formally, the positional embeddings and the transformation q(, ) are defined as shown below, with k(, ) adhering to similar formulation: q(xi, ei) = RiWQxi, Ri = diag(ei,1, , ei,C/2), ei,m = (cid:21) (cid:20)cos(θmi) sin(θmi) cos(θmi) sin(θmi) , (2) where diag() constructs block-diagonal matrix by concatenating the arguments on the diagonal. In RoPE, the hyper-parameters θm ranges from θm = 100002m/C, [C/2]. Subsequent works explore methods to extend the context length for RoPE-based LLMs through the adoption of damped trigonometric series (Sun et al., 2022), positional interpolation (Chen et al., 2023a) and adjustments to coefficients{θm}m[C/2] (r/LocalLLaMA, 2023; Peng et al., 2023; Liu et al., 2023). 2.2 ATTENTION BIAS AS POSITIONAL ENCODING An alternative method for encoding positional information involves applying bias to the attention map, conditioned on the relative distances between tokens during the attention computation. The pre-softmax attention value with bias can be formulated as: αi,j = (WQxi)(WKxj) + b(i, j), (3) where b(i, j) : is bias regarding the token indices and j. Many existing positional encoding methods can be interpreted as various instantializations of b(i, j). We follow Li et al. (2023) to summarize few examples: (i) In T5 (Raffel et al., 2020), b(i, j) = rmin{ij,Lmax}, where Lmax denotes the maximal relative distance considered, and {ri : [0, Lmax]} are learnable scalars. (ii) Alibi (Press et al., 2021b) simplifies the bias term to b(i, j) = ri j, where > 0 is hyperparameter that acts as the slope, imposing linear decay pattern based on the relative distance. (iii) Kerple (Chi et al., 2022a) enforces logarithmic or power decay rate: b(i, j) = r1 log(1 + r2i j) and b(i, j) = r1i jr2 respectively, where r1, r2 > 0 are hyperparameters. (iv) FIRE (Li et al., 2023) learns neural network with parameters θ to model the bias: b(i, j) = fθ(ψ(i j)/ψ(max{i, L})), where ψ(x) = log(cx + 1), and > 0 is hyperparameter. 2For simplicity, we ignore the denominator by default. 3 Figure 1: Overview of our proposed TAPE in standard decoder-only Transformer architecture."
        },
        {
            "title": "3 OUR APPROACH",
            "content": "3.1 MOTIVATIONS AND DESIGN PRINCIPLES FOR POSITIONAL ENCODING In the paper, we interpret the attention mechanism as an addressing system, where row-wise attention logits can be viewed as an indicator vector locating important tokens in the context to inform predictions for the current token. The underlying addressing mechanisms include both content-based addressing, which locates tokens via feature similarity, and position-based addressing, which leverages positional encodings to extract location-based information. Content-based addressing is often prioritized in language modeling which is evidenced by series of simplifications on positional encoding in the literature (Press et al., 2021b; Haviv et al., 2022; Wang et al., 2024b; Kazemnejad et al., 2024) due to the fact that natural language semantics primarily depend on the meaning of constituent words rather than their arrangement order (Sinha et al., 2021). However, position-based addressing can sometimes be crucial for many advanced tasks. Ebrahimi et al. (2024) demonstrates that in arithmetic tasks (Lee et al., 2023), tokens position is as important as its value. Specifically, an ideal attention map for performing addition needs to exclusively rely on token indices. Moreover, we observe that the interaction between token features and positional embeddings is lacking in current transformer. Golovneva et al. (2024) demonstrate that incorporating the interplay between context and positional information allows for more flexible addressing, leading to improvements in complex compositional tasks such as algorithm execution and logical reasoning (Liu et al., 2024). In domains with data characterized by intricate geometries, such as graphs and point clouds, capturing the interaction between node or point features and their geometric locations is essential for effectively learning structural patterns relevant to tasks (Wang et al., 2024c; Huang et al., 2024). Based on above arguments, we aim to establish more expressive family of positional encoding, which can be effectively informed by the context to facilitate position-based addressing in LLMs. The main idea is to customize attention and MLP modules in transformers such that they can iteratively update positional embeddings at each layer with sequence content, and use the updated embeddings as the positional encoding for the next layer. Let tuple (X, E) represent language sequence, where RN are the token features, and RN are the positional embeddings. We define transformer block consisting of two separate operations: token mixing and position contextualization. The token mixing is formulated as function : RN C, which combines token features and positional embeddings to represent each token. The position contextualization : RN encodes the context information into the positional embeddings. 4 Tensorial Positional Encoding. Our first enhancement extends positional encodings to multidimensional format and diversifies their coupling with token features to allow for richer interactions among token and position representations, drawing inspiration from positional encodings used in geometric learning (Deng et al., 2021; Wang et al., 2024c; Huang et al., 2024). We first divide the hidden dimension into blocks, resulting = [x1, , xN ] RN B with xi RM and = C/M . We propose to assign each block L-many R-dimension positional embeddings. Therefore, we reorganize positional embeddings as = [e1, , eN ] RN LR with ei RM LR and = R. Equivariance Principles. Second, we establish two fundamental criteria for the design of functions and g. Conceptually, by representing each token as tuple comprising its token and positional embedding, the entire sequence can be viewed as an unordered set. This implies that permuting these tuples arbitrarily will not alter the outputs of and g, aside from corresponding change in order (Zaheer et al., 2017; Lee et al., 2019). We note that this is an intrinsic property of standard attention. Furthermore, we aim for the positional embeddings to effectively model relative distances, necessitating that remains invariant to translations in the token positions (Sun et al., 2022). This invariance can be achieved by structuring and to depend on the positional embeddings in manner invariant and equivariant, respectively, to orthogonal transformations along the last dimension (Villar et al., 2021). Formally, let us denote Π(N ) as permutation group over elements, and O(R) as an orthogonal group over the R-dimension Euclidean space. The two aforementioned criteria require and to satisfy that: for Π(N ), O(R), (P X, ER) = (X, E) (4) g(P X, ER) = g(X, E)R (5) where left-multiplication of Π(N ) permutes on the first dimension of and E, while rightmultiplication of O(R) applies to the last dimension of tensor E. We note that attention with RoPE inherently satisfies Eq. 4, with the invariant orthogonal group being O(2). We formalize the merits of orthogonal group invariance in Proposition 1 (proved in Appendix A). Proposition 1 (Relativity of position encoding). Suppose transformer consists of f, satisfying Eqs. 4 and 5. Given inputs (X, E) with position encoding initialized by RoPE or random Fourier features, then the transformer is invariant to shift on token indices. Proposition 1 indicates that even though positional encodings are updated among intermediate layers, the attention is computed based on relative distances between token positions (Sinha et al., 2022). This property is crucial for stability and generalization to varying sequence lengths. 3.2 CONTEXTUALIZED POSITIONAL ENCODING WITH EQUIVARIANCE In this section, we instantiate design principles discussed in Sec. 3.1 as practical neural architecture. We note that although there are lots of ways to achieve conditions in Eq. 4 and 5 (Dym & Maron, 2020; Bogatskiy et al., 2020; Yarotsky, 2022), the proposed method focuses on enhancing existing components used in standard transformers with consideration of computational efficiency. We term our proposed approach of informing positional encoding with context through enforcing equivariance as ConTexturalized EquivAriant Positional Encoding (TAPE). Model Structure and Initialization. We adhere to the conventional architecture of the standard transformer (Vaswani et al., 2017), wherein each layer comprises an attention module for token mixing and Multi-Layer Perceptron (MLP) for channel mixing. Both the attention and MLP components are tailored to update positional embeddings at each layer. We depict the overall architecture in Fig. 1. The initial positional features may encompass variety of representations, including but not limited to learnable features (Vaswani et al., 2017), sinusoidal series (Vaswani et al., 2017; Su et al., 2024; Sun et al., 2022), or random Fourier features (Rahimi & Recht, 2007; Yu et al., 2016). Among these, we select the widely-used sinusoidal series embedding, RoPE (Su et al., 2024), as our initialization, as detailed in Sec. 3.3. O(R)-Invariant Token Mixing. In each transformer block, updates token features through attention and MLP following the principles of permutation-equivariance and O(R)-invariance. We define pre-softmax attention value between the i-th and j-th tokens as: αi,j = (cid:88) m=1 αi,j,m, αi,j,m = (WQxj) mϕ(ej,me i,m)(WKxi)m, (6) where ϕ() : RLL RBB can be any function. Permutation-equivariance is inherently preserved in pairwise attention, regardless of the method used to derive attention values. O(R)-invariance is achieved by computing the inner product of positional embeddings (Villar et al., 2021; Wang et al., 2022a; 2024a). We note that O(R)-invariance stems from the separation of the inner product calculations between features and positional embeddings, in contrast to Vaswani et al. (2017). In practice, we can let = and ϕ be an identity mapping, which simplifies Eq. 6 to series of tensor multiplications. After applying attention, standard MLP layer is employed to transform token embeddings without using positional embeddings. O(R)-Equivariant Position Contextualization. The primary contribution of this work is the introduction of method to condition positional embeddings on sequence content. We employ an O(R)-equivariant function to ensure structure conservation of this update. key insight is that linearly combining positional coordinates preserves O(R)-equivariance, provided the weights are invariant to the orthogonal group (Villar et al., 2021; Wang et al., 2022a; Huang et al., 2024). This observation leads us to leverage attention maps, which capture content-based token relationships, to integrate positional embeddings. Hence, the attention layer can update positional embedding via: (cid:101)ej,m = (cid:88) i=1 exp(αi,j,m) i=1 exp(αi,j,m) (cid:80)N ei,m, [N ], [M ], (7) where ej,m denotes an intermediate output of the attention layer. In practice, we share the attention map between Eq. 6 and 7. We can re-use αi,j,m computed in Eq. 6 because we have shown that attention weights αi,j,m are O(R)-invariance. We further propose layer similar to the function of MLP, which directly transform matrix-form positional embeddings with token features incorporated. Specifically, we first flatten the first two dimensions of (cid:101)ej RM LR to the shape RM LR, then apply linear transformation constructed by token features, and finally unflatten the first dimension of the resultant matrix to the (cid:98)ej RM LR: (8) where we denote (cid:101)xj RC as the output of attention after token mixing. Let be the intermediate dimension, W1 and W2 have shape I. We further define ψ : RC RII as mapping between token features to linear transformations. To reduce computation overhead, we adopt an MLP to transform xj into an I-dimensional vector and form diagonal matrix with the resultant vector as its diagonal. The detailed computational flow is illustrated in Fig. 4 in Appendix B. By applying linear transformations only to the first two dimensions, this layer maintains O(R)-equivariance. (cid:98)ej = unflatten (cid:0)W2ψ((cid:101)xj)W 1 flatten((cid:101)ej)(cid:1) , [N ], We summarize the overall geometric properties of our architecture in Proposition 2 below. Proposition 2. The proposed TAPE including: (i) attention in Eq. 6 with normal MLP for token mixing, and (ii) attention in Eq. 7 with MLP defined in Eq. 8 for position contextualization, satisfies Eq. 4 and Eq. 5. 3.3 PARAMETER-EFFICIENT FINE-TUNING WITH TAPE In this section, we demonstrate that our TAPE can be seamlessly integrated into pre-trained models, enabling parameter-efficient fine-tuning to enhance position-based addressing in existing architectures. Notably, the widely adopted RoPE (Su et al., 2024) can be considered special case of TAPE. This can be seen by letting = = 2 and ei,m,1 = [cos(θmi) sin(θmi)] , ei,m,2 = cos(θmi)]. With this configuration, Eq. 6 becomes equivalent to Eq. 2. As result, [sin(θmi) RoPE can serve as the initialization for TAPE, while the model is further enhanced by incorporating the contextualization component specified in Eq. 7 and 8. This initialization is applied across all experiments, encompassing both pre-training and fine-tuning stages. Specifically, during finetuning, to ensure that the augmented model remains identical to the original at initialization, we 6 follow Hu et al. (2021) by setting the initialization of W2 in Eq. 8 to zero such that all updates to the positional encoding inside the block will then be reset via residual connection. To allow for parameter-efficient fine-tuning, we enable gradients to update the weights for position encoding contextualization including W1, W2 and the weights for the post-attention linear layer, while keeping all other parameters frozen."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we first validate our method on arithmetic tasks, which explicitly rely on absolute positions for prediction (Sec. 4.1). We also show our effectiveness in natural languages, in both pre-training (Sec. 4.2) and fine-tuning case (Sec. 3.3)."
        },
        {
            "title": "4.1 ARITHMETIC LEARNING",
            "content": "As demonstrated by prior research (Lee et al., 2023; Zhou et al., 2024), even large transformer models struggle with arithmetic tasks. Recent studies suggest that this limitation may stem from their constrained position-addressing capabilities (Ebrahimi et al., 2024). In particular, arithmetic tasks treat every digit as equally important to the equation, regardless of its distance from the output. In contrast, traditional positional embeddings for language tasks often assume distance-decay effect, where words farther apart have less significance in the output. Positional contextualization potentially addresses this by dynamically reweighting positional importance based on the task context. To evaluate the ability of LLMs of performing arithmetic tasks with our position embedding, we use the Addition Bucket 40 dataset (McLeish et al., 2024a) which contains 20 million samples with ( < 40) operand lengths. We train transformers from scratch using the arthimetic data, and during evaluation, we sample 100 samples for each pair of operand lengths. Following the existing attempt (McLeish et al., 2024a), the operands in the training set are not necessary to have the same length, but the maximum length of two operands are the same. We then report model accuracy for each (i, j) length pair. Note that accuracy is measured strictly, counting only exact matches of all output digits as correct. The transformers are standard decoder-only architecture with config detailed in Appendix B. We compare our method with four baselines, including RoPE (Kitaev et al., 2020), RandPE (Ruoss et al., 2023) NoPE (Kazemnejad et al., 2024), and FIRE (Li et al., 2023). Figure 2: Accuracy on addition task between different methods on 2 context length. Models are trained on sequence with length up to 40 while test on sequence with length up to 80. The average accuracy across the heatmap is 26.32%, 26.56%, 22.45%, 26.98% and 32.82% respectively for RoPE, RandPE, NoPE, FIRE and TAPE. The heatmaps further demonstrate TAPEs superior generalization to longer sequences, as indicated by the concentrated dark-colored regions representing higher accuracy across wider range of operand lengths. TAPE outperforms other methods with the highest average accuracy of 32.82%. Compared to FIRE, which achieves 26.98% and previously held the strongest length generalization in arithmetic tasks (McLeish et al., 2024a; Zhou et al., 2024), TAPE shows remarkable 21.6% relative improvement. This shows TAPEs effectiveness in maintaining accuracy as sequence lengths increase, making it particularly suitable for long-range dependency tasks. 4.2 PRE-TRAINING FROM SCRATCH Pre-training language model on corpus followed by fine-tuning on downstream tasks is the standard methodology for evaluating the performance of positional embeddings in prior studies (Li et al., 2023; He et al., 2024). Similarly, we first pre-train transformers with 1024 context window from Table 1: Performance comparison on seven datasets from SCROLLS benchmark. Metric (%) Median length RoPE (Kitaev et al., 2020) ALiBi (Press et al., 2021a) RandPE (Ruoss et al., 2023) FIRE (Li et al., 2023) xPos (Sun et al., 2022) TAPE (ours) QAS CNLI NQA QuAL QMS SumS GovR F1 8.39 8.25 13.44 3.41 9.02 11.52 EM 2148 65.00 69.62 62.01 71.26 71.75 72.80 F1 57829 1.77 4.11 4.63 0.48 4.83 6.79 EM 0.04 0.0 0.38 1.25 0.24 11.60 Rgm 14197 6.34 9.92 8.43 8.78 10.73 12.42 Rgm 9046 5.63 9.78 8.31 7.42 9.38 10.34 Rgm 9.71 18.81 8.93 11.03 16.38 15.18 scratch, using C4 dataset (Raffel et al., 2020), and then fine-tune those models in long-context benchmark SCROLLS (Shaham et al., 2022). We report three evaluation metrics for seven different tasks: unigram overlap (F1) for Qasper and NarrativeQA, and exact match (EM) for QuALITY (QAS) and ContractNLI (CNLI), and Rgm score (the geometric mean of ROUGE-1,2,L) for the three summarization tasks: GovReport (GovR), QMSum (QMS), and SummScreenFD (SumS). We compare our methods with RoPE (Kitaev et al., 2020), ALiBi (Press et al., 2021a), RandPE (Ruoss et al., 2023), FIRE (Li et al., 2023) and xPos (Sun et al., 2022), and report the results in Tab. 1. Our method consistently outperforms all baselines, demonstrating significant improvements, particularly in scenarios with longer context lengths, as observed in QuAL and NQA. In terms of overall performance, xPos closely follows TAPE. While FIRE, RandPE, and ALiBi exhibit strong results on one or two datasets, they generally lag behind across the board. RoPE struggles with the longcontext task across all datasets. 4.3 CONTEXT WINDOW EXTENSION BY PARAMETER-EFFICIENT TUNING We extend the context window of the pre-trained Llama2 7B model (GenAI, 2023) from 4096 to 8192, using the Redpajama (Computer, 2023). For validation, we then compare the perplexity on sequence of length 8192, on the cleaned ArXiv Math proof-pile dataset (Azerbayev et al., 2022; Chen et al., 2023a) and the book corpus dataset PG19 (Rae et al., 2019). To further evaluate the models performance of long context understanding, we report the accuracy of fine-tuned models on passkey retrieval task which has been adopted by many literature (Chen et al., 2023b;a; Tworkowski et al., 2024). We choose popular open-sourced LLM Llama2 7B (Touvron et al., 2023b), which uses RoPE, as the base model and extend it to the 8192 context length. Three baselines are selected to compare to our TAPE method: vanilla LoRA (Hu et al., 2022), LongLoRA (Chen et al., 2023b), Theta Scaling (Liu et al., 2023). Table 2: Evaluation on perplexity across different context lengths. Method Proof-pile PG19 1024 2048 8192 1024 2048 4096 8192 LoRA LongLoRA Theta Scaling TAPE 3.828 3.918 3.864 3.641 3.369 3.455 3.415 3.196 3.064 3.153 3.121 2.901 2.867 2.956 2.934 2.708 9.791 9.989 9.257 8.226 9.098 9.376 8.640 7. 8.572 8.948 8.241 7.278 8.199 8.645 7.999 7.063 As shown in Tab. 2, TAPE consistently outperforms the other methods across all context lengths on both the Proof-pile and PG19 datasets. On Proof-pile, TAPE achieves perplexity of 3.641 at 1024 tokens, improving over LoRA (3.828), LongLoRA (3.918), and Theta Scaling (3.864). At 8192 tokens, TAPEs advantage grows, reaching 2.708, surpassing LongLoRA (2.956), LoRA (2.867), and Theta Scaling (2.934). Similarly, on PG19, TAPE achieves 8.226 at 1024 tokens, improving up to 18.3% over competitors. At 8192 tokens, TAPE reaches 7.063, further showing superiority, especially at longer context lengths. 8 Figure 3: Accuracy on passkey retrieval from 1k to 8k context length between Llama2 7B with different fine-tuning methods. We also evaluate the passkey retrieval accuracy of our model, following Landmark Attention (Mohtashami & Jaggi, 2023), which has also been adopted by other literature (Chen et al., 2023a; Tworkowski et al., 2024; Chen et al., 2023b). In this task, the models are required to locate and retrieve random passkey hidden in long document. We test the passkey retrieval accuracy ranging from 1k to 8k. The results of long-context passkey retrieval task is presented in Fig. 3. As shown, TAPE consistently achieves near-perfect accuracy across all context lengths, outperforming other methods. Theta Scaling shows relatively stable performance while LoRA and LongLoRA exhibit fluctuating and lower accuracy. Notably, Theta Scaling is widely employed in popular opensource long-context models like Llama3 8B Instruct 262k (AI@Meta, 2024) and MistralLite (AWS, 2024). TAPE demonstrates similar superior capability to be applied in long-context tasks. 4.4 EFFICIENCY ANALYSIS In this subsection, we analyze the complexity of our methods in comparison to traditional position embedding techniques. Using the models from the pretraining experiment in Sec. 4.2, we report three key metrics: FLOPs, MACs, and the number of parameters. The metrics are evaluated with batch size of 1 and sequence length 1024. As shown in Tab. 3, our architectural modifications introduce negligible increase in FLOPs, MACs and number of parameters, compared to the standard Transformer with RoPE. Moreover, our TAPE is fully compatible with Flash Attention (Dao et al., 2022; Dao, 2024a), widely adopted accelerated attention mechanism with IO-awareness, which introduces extra efficiency. Table 3: Comparison of FLOPS, MACs, and the number of parameters for models with different position embeddings. Method TAPE RoPE FIRE T5s relative bias FLOPS (G) MACs (G) Params. (M) 365.65 180.69 155.33 321.10 160.46 154.89 331.97 165.69 154. 321.10 160.46 154.90 Table 4: System measurement. We report execution time per step in the Time row and iteration per second in the Throughput row. The values are averaged over 100 inference steps. Method TAPE w/ Fusion w/o Fusion RoPE FIRE T5s relative bias Time (104) Throughput Flash Attention 2.56 3910 5.63 1775 2.08 4810 5.56 1799 6.90 1449 9 For simplicity, we evaluate the running time of attention layers with different position embedding methods on single A100 GPU. We run 100 inference steps and report the average execution time. Both RoPE and TAPE leverage the acceleration provided by Flash Attention (Dao, 2024b), whereas FIRE and T5s relative bias are not fully compatible with Flash Attention, as it currently lacks support for gradient computation in relative bias. In contrast, we observe that the computations for position embeddings and token features in TAPE are highly parallelizable, making it suitable for further acceleration using kernel fusion techniques. To capitalize on this, we implemented version of TAPE with kernel fusion, referred to as TAPE w/ Fusion. As shown in Tab. 4, the efficiency of the original TAPE (w/o Fusion) already surpasses T5s relative bias and is comparable to FIRE. With additional kernel fusion applied, TAPE achieves 2.2 speedup, approaching the efficiency of RoPE with Flash Attention."
        },
        {
            "title": "5 MORE RELATED WORK",
            "content": "Context Length Extrapolation. The length extrapolation ability of Transformers are limited mainly in two aspects: (1) the high memory usage caused by quardratic memory usage; and (2) the poor generalizability to unseen sequence length during inference. To address the memory usage during long sequences training, LongLoRA (Chen et al., 2023b) introduced shifted sparse attention and leveraged parameter-efficient tuning. LoCoCo (Cai et al., 2024) introduce KV cache compression mechanism. To help generalizability of positional embedding to unseen sequence length, (Chen et al., 2023a) explores zero-shot linear interpolation on rotary embedding; (r/LocalLLaMA, 2023; Peng et al., 2023) enhance simple interpolation by retaining high-frequency encoding ability; (Liu et al., 2023) investigate the relationship between rotary base and extrapolation ability. While the previously mentioned methods focus primarily on extending rotary positional embeddings, Li et al. (2023) introduced functional relative position encoding framework that enhances generalization to longer contexts. However, these methods generally impose fixed pattern on attention maps, often adopting decaying pattern based on distance. In contrast, we propose learnable and generic position encoding framework that primarily focus on arithmetic reasoning ability. Equivariant Learning. Equivariant machine learning is broad field that leverages task-specific symmetries to introduce inductive biases into neural networks, reducing learning complexity and improving generalization. Here, we focus on foundational works in this domain that are highly relevant and provide key motivation for our study. Prior research has primarily focused on data representations with intrinsic symmetries, such as graphs (Satorras et al., 2021; Schutt et al., 2021; Batzner et al., 2022; Maron et al., 2018), hyper-graphs (Wang et al., 2022b; Chien et al., 2021), and point clouds (Zaheer et al., 2017; Fuchs et al., 2020; Thomas et al., 2018; Hoogeboom et al., 2022), which primarily require permutation equivariance. Recent progress models graph representation learning as joint invariance of permutation and orthogonal groups (Wang et al., 2022a; Huang et al., 2023; Wang et al., 2024c). Beyond geometric data, another stream of work (Worrall et al., 2017; Zhang, 2019; Weiler & Cesa, 2019; Cohen et al., 2019) ensures symmetric inputs yield consistent outputs, reflecting the same label under symmetric transformations. For instance, Worrall et al. (2017) introduce rotation-equivariant feature transformations after mapping images to the continuous fourier domain, while Zhang (2019) enhance translation invariance in CNNs by incorporating low-pass filter in the pooling layer. To the best of our knowledge, we are the first to introduce equivariance in language models, recognizing the symmetry in positional embeddings."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduced TAPE, framework that enhances transformer models by contextualizing positional embeddings with sequence content across layers. Through incorporating permutation and orthogonal equivariance, we ensured stability and adaptability in positional encoding updates. TAPE can also be easily integrated into existing models, introducing negligible computation and inference overhead. Extensive experiments confirmed TAPEs effectiveness in both arithmetic reasoning and long context language modeling tasks. One current limitation lies in our exclusive focus on decoderonly models, with limited training scale."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We are pleased to acknowledge that the work reported in this paper was substantially performed using the Princeton Research Computing resources at Princeton University which is consortium of groups led by the Princeton Institute for Computational Science and Engineering (PICSciE) and Research Computing. We also would like to thank Yinan Huang and Siqi Miao for discussing positional encoding for geometric data. PL is supported by NSF awards IIS-2239565 and IIS-2428777 for this project."
        },
        {
            "title": "REFERENCES",
            "content": "AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. AWS. Mistrallite model card. 2024. URL https://github.com/awslabs/ extending-the-context-length-of-open-source-llms/blob/main/ MistralLite/README.md. Zhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https:// github.com/zhangir-azerbayev/proof-pile. Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016. Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess Smidt, and Boris Kozinsky. (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1): 2453, 2022. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. Alexander Bogatskiy, Brandon Anderson, Jan Offermann, Marwah Roussi, David Miller, and Risi Kondor. Lorentz group equivariant neural network for particle physics. In International Conference on Machine Learning, pp. 9921002. PMLR, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Ruisi Cai, Yuandong Tian, Zhangyang Wang, and Beidi Chen. Lococo: Dropping in convolutions for long context compression. arXiv preprint arXiv:2406.05317, 2024. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213229. Springer, 2020. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023b. Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:83868399, 2022a. Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. arXiv preprint arXiv:2212.10356, 2022b. 11 Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: multiset function framework for hypergraph neural networks. arXiv preprint arXiv:2106.13264, 2021. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral cnn. In International conference on Machine learning, pp. 13211330. PMLR, 2019. Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024a. Tri Dao. Flash attention. 2024b. URL https://github.com/Dao-AILab/ flash-attention. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas Guibas. Vector neurons: general framework for so (3)-equivariant networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1220012209, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proceedings of ICLR, 2021. Nadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. arXiv preprint arXiv:2010.02449, 2020. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. MohammadReza Ebrahimi, Sunny Panchal, and Roland Memisevic. Your context is not an array: Unveiling random access limitations in transformers. arXiv preprint arXiv:2408.05506, 2024. Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d rototranslation equivariant attention networks. Advances in neural information processing systems, 33:19701981, 2020. Meta GenAI. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Olga Golovneva, Tianlu Wang, Jason Weston, and Sainbayar Sukhbaatar. Contextual position encoding: Learning to count whats important. arXiv preprint arXiv:2405.18719, 2024. Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022. Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang, Di He, Jingjing Xu, Zhi Zhang, Hongxia Yang, and Liwei Wang. Two stones hit one bird: Bilevel positional encoding for better length extrapolation. arXiv preprint arXiv:2401.16421, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Geoffrey Hinton and James Anderson. Parallel models of associative memory: updated edition. Psychology press, 2014. Emiel Hoogeboom, Vıctor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pp. 8867 8887. PMLR, 2022. John Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):25542558, 1982. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan Li. On the stability of expressive positional encodings for graph neural networks. arXiv preprint arXiv:2310.02579, 2023. Yinan Huang, Siqi Miao, and Pan Li. What can we learn from state space models for machine learning on graphs? arXiv preprint arXiv:2406.05815, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024. Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: framework for attention-based permutation-invariant neural networks. In International conference on machine learning, pp. 37443753. PMLR, 2019. Nayoung Lee, Kartik Sreenivasan, Jason Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023. Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023. Bingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing attention glitches with flip-flop language modeling. Advances in Neural Information Processing Systems, 36, 2024. Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023. Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In International Conference on Learning Representations, 2018. Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, and Tom Goldstein. Transformers can do arithmetic with the right embeddings. arXiv preprint arXiv:2405.17399, 2024a. 13 Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, et al. Transformers can do arithmetic with the right embeddings. arXiv preprint arXiv:2405.17399, 2024b. Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. Kostas Pagiamtzis and Ali Sheikholeslami. Content-addressable memory (cam) circuits and architectures: tutorial and survey. IEEE journal of solid-state circuits, 41(3):712727, 2006. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021a. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021b. Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, et al. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21(140):167, 2020. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019. r/LocalLLaMA. Ntk-aware scaled rope. https://www.reddit.com/r/LocalLLaMA/ comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_ have/, 2023. Anian Ruoss, Gregoire Deletang, Tim Genewein, Jordi Grau-Moya, Robert Csordas, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In 61st Annual Meeting of the Association for Computational Linguistics, 2023. Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. (n) equivariant graph neural networks. In International conference on machine learning, pp. 93239332. PMLR, 2021. Kristof Schutt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pp. 93779388. PMLR, 2021. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language sequences. arXiv preprint arXiv:2201.03533, 2022. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018. Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. arXiv preprint arXiv:2104.06644, 2021. Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke Hupkes, and Adina Williams. The curious case of absolute position embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 44494472, 2022. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 14 Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav arXiv preprint Chaudhary, Xia Song, and Furu Wei. length-extrapolatable transformer. arXiv:2212.10554, 2022. Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłos. Focused transformer: Contrastive training for context scaling. Advances in Neural Information Processing Systems, 36, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In Proceedings of NeurIPS, Łukasz Kaiser, and Illia Polosukhin. Attention is All You Need. 2017. Soledad Villar, David Hogg, Kate Storey-Fisher, Weichi Yao, and Ben Blum-Smith. Scalars are universal: Equivariant machine learning, structured like classical physics. Advances in Neural Information Processing Systems, 34:2884828863, 2021. Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024a. Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more powerful graph neural networks. arXiv preprint arXiv:2203.00199, 2022a. Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. arXiv preprint arXiv:2404.12224, 2024b. Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, and Pan Li. Equivariant hypergraph diffusion neural operators. arXiv preprint arXiv:2207.06680, 2022b. Xiyuan Wang, Pan Li, and Muhan Zhang. Graph as point set. In Forty-first International Conference on Machine Learning, 2024c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Maurice Weiler and Gabriele Cesa. General (2)-equivariant steerable cnns. Advances in neural information processing systems, 32, 2019. Daniel Worrall, Stephan Garbin, Daniyar Turmukhambetov, and Gabriel Brostow. Harmonic networks: Deep translation and rotation equivariance. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 50285037, 2017. Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive Approximation, 55(1):407474, 2022. Felix Xinnan Yu, Ananda Theertha Suresh, Krzysztof Choromanski, Daniel Holtmann-Rice, and Sanjiv Kumar. Orthogonal random features. Advances in neural information processing systems, 29, 2016. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ Salakhutdinov, and Alexander Smola. Deep sets. Advances in neural information processing systems, 30, 2017. 15 Richard Zhang. Making convolutional networks shift-invariant again. In International conference on machine learning, pp. 73247334. PMLR, 2019. Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1625916268, 2021. Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024. PROOF OF PROPOSITION 1 We first formulate the computational paradigm of transformers with TAPE. Inputs. Let (0) RN be the input sequence of token embeddings. We consider two types of initialization of positional encoding: RoPE. Construct position embeddings as tensor: E(0) RN C/222. For every (cid:21) (cid:20)cos(θmi) sin(θmi) , where θm = 100002m/C. cos(θmi) sin(θmi) [N ], [C/2], we let e(0) i,m = Reweighted Random Fourier Features. Construct position embeddings as tensor: E(0) RN LR. For every [N ], [M ], [L] e(0) i,m,l = (cid:114) 2 [ wm,l,r cos(θm,ri) wm,l,r sin(θm,ri) ] , [R/2] where {θm,r}m[M ],r[R/2] are often chosen as (quasi-)Monte-Carlo samples from distribution, and {wm,l,r}m[M ],l[L],r[R/2] are collection of coefficients. Model. We consider transformer : RN RN LR RN consisting of transformer blocks. For the t-th block, we consider it employs function (t) : RN RN LR RN to update features, and function g(t) : RN RN LR RN LR to update position embeddings: (t) = (t)(X (t1), E(t1)), E(t) = g(t)(X (t1), E(t1)), [T ]. We denote (T ) as the final output of (X (0), E(0)). We assume (t) and g(t) jointly satisfies our invariance properties Eqs. 4 and 5. Phase Shift. We say transformer is invariant to translation if the final outputs (T ) remains unchanged when the input token indices are shifted by an offset. This implies the attention mechanism inside depends on positional information based on the relative distances instead of absolute indices. Formally, when an offset is applied to all positions, the initial positional embeddings undergo phase shift. We denote the positional embeddings with translation as (cid:101)E(0), whose per-token representations {(cid:101)e(0) }i[N ] can be written as: (cid:20)cos(θm(i + )) sin(θm(i + )) cos(θm(i + )) sin(θm(i + )) i,m = RoPE. (cid:101)e(0) Random Fourier Features. For every [N ], [M ], [L] (cid:21) for every [N ], [C/2]. (cid:101)e(0) i,m,l = (cid:114) 2 [ wm,l,r cos(θm,r(i + )) wm,l,r sin(θm,r(i + )) ] . We denote the intermediate outputs resultant by shifted positional embeddings as ( (cid:102)X (t), (cid:101)E(t)), for every [T ]. Main Result. We provide formal version and proof of Proposition 1 as below: Proposition 3 (Formal version of Proposition 1). Assume (t) and g(t) satisfies Eqs. 4 and 5 for every [T ]. Then for any shift R, we have (X (0), E(0)) = (X (0), (cid:101)E(0)). Proof. First, we observe that shift on the token indices translates to an orthogonal transformation on the embedding space for both RoPE and random Fourier features. For RoPE, this can be seen by: (cid:101)e(0) i,m = (cid:20)cos(θmi) sin(θmi) cos(θmi) sin(θmi) (cid:21) (cid:20)cos(θm) sin(θm) cos(θm) sin(θm) (cid:21) , (cid:124) (cid:123)(cid:122) ORoP E,,m (cid:125) 17 where the extracted matrix ORoP E,,m is an orthogonal matrix. For random Fourier features, we observe that (cid:101)ei,m = e(0) i,mORF F,,m, where ORF F,,m = diag(ORF F,,m,1, ORF F,,m,R/2), and each ORF F,,r R22 is defined as: ORF F,,m,r = (cid:21) (cid:20)cos(θm,r) sin(θm,r) cos(θm,r) sin(θm,r) , which shows the orthogonality of ORF F,,m. Now we apply the orthogonal invariance in an inductive argument. We make hypothesis that (t) = (cid:102)X (t) and E(t) = (cid:101)E(t)O for some [T ] {0} with corresponding to the initialization specific orthogonal transformation. It is obvious that this holds for = 0. Then by the orthogonal invariance and equivariant of (t+1) and g(t+1), we have that for every [T 1] {0}: (cid:102)X (t+1) = (t+1)( (cid:102)X (t), (cid:101)E(t)) = (t+1)(X (t), E(t)O) = (t+1)(X (t), E(t)) = (t+1) (cid:101)E(t+1) = g(t+1)( (cid:102)X (t), (cid:101)E(t)) = g(t+1)(X (t), E(t)O) = g(t+1)(X (t), E(t))O = E(t+1)O, which concludes the proof by induction."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "Experiment Settings. In Sec.4.1, the model architecture includes 16 layers, hidden dimension of 1024, an intermediate dimension of 2048, and 16 attention heads, resulting in approximately 120M parameters. In Sec.4.2, the architecture features 12 layers, hidden dimension of 768, an intermediate dimension of 3072, and 12 attention heads, totaling approximately 155M parameters. The training recipe in three experiments are presented in Tab. 5. Table 5: Training recipe for language model pre-training and fine-tuning in experiments. Arithmetic (4.1) C4 Pre-training (4.2) SCROLLS (4.2) Context Extension (4.3) Sequence length Batch size Number of iterations Attention dropout prob. Optimizer Learning rate 40 + 40 512 20k 0.0 AdamW 1 10 1024 512 10k 0.0 AdamW 1 104 1024 64 1k 0.0 AdamW 1 105 8096 64 1k 0.0 AdamW 2 105 Masked Multi-Head Mechanism. The masked multi-head attention is key design in the original Transformer and is well compatible with our method. To enforce causality in language generation, the Transformer masks out (sets to ) all values in the input to the softmax that correspond to illegal connections from future tokens to current tokens. This is similarly implemented in our enhanced Transformer for language modeling. To allow the model to jointly attend to information from different representation subspaces at different positions, multiple attention outputs are computed in parallel with multiple attention heads, and then mixed through concatenation and linear transformation. In our enhanced Transformer, the head dimension is added to both token embeddings and positional embeddings, resulting in RN HM and RN HM LR, where denotes the number of heads. Parameterization in Position Contextulization. The shapes of W1 and W2 allow for considerable flexibility. Given ei RHM LR. To achieve maximal expressiveness, ei can be flattened into RHM LR, with W1 and W2 RHM LI . Alternatively, to minimize parameter usage, we set W1 and W2 as RHI , with weights shared across the and dimensions. ψ is implemented through standard MLP and the dimension is set to 4H in all experiments. Visualization of Tensor Operations. To provide clearer understanding of TAPE and the operation within the attention and feed-forward layers, we visualize the process in Fig. 4. 18 Figure 4: Visualization of TAPEs operations. The channel dimension is omitted for simplicity as all operations can be channel-wise. In the attention layer, the input token embeddings have shape of B, and the position embeddings have shape of R. For the feed-forward layer, the dimension is omitted as its operations are position-wise. The input token embeddings then have shape of (or 1), and the position embeddings have shape of R."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTS",
            "content": "Ablation Study on Architecture. We ablate our architecture design for both attention layer and MLP layer in position contextualization. We conduct ablation studies on our architectural design for both the attention layer and the MLP layer in position contextualization. Additionally, we ablate two aspects of the design: rotation equivariance, by setting W1, W2 RHRI , which disrupts the O(R)-equivariance; the use of tensorial embeddings, by flattening = = 2 into = 1 and = 4; and both properties simultaneously, by setting = 4 and = 1. We use the same pretraining setting as Sec. 4.2 and directly report its perplexity in the test dataset of Github following He et al. (2024). Table 6: Ablation study on TAPE architecture. We evalute pre-trained models perplexity across varying sequence lengths on the GitHub test set. Architecture Perplexity Attention Feed Forward Rotation Equivariance Tensorial Embedding 128 139.2 143.3 142.7 132.0 140.7 138.4 132.9 132. 256 92.8 95.0 94.3 86.6 92.1 91.3 87.8 86.6 512 69.3 70.7 70.1 63.9 68.2 67.8 65.4 63. 1024 57.2 58.4 57.6 52.2 56.2 55.7 54.1 52.2 As shown in Tab. 6 , incorporating position contextualization in both the attention layer and the MLP layer results in the lowest perplexity across different positions within the training sequence length. Removing position contextualization from either layer increases perplexity, even exceeding that of the traditional positional embedding without any architectural modifications. This outcome is reasonable, as applying position contextualization to only one component introduces an architectural inconsistency. Furthermore, ablating rotation equivariance allows all neurons in the positional embedding to undergo linear transformations, increasing the number of parameters but leading to 19 worse results compared to TAPE. Similarly, reducing the tensorial embedding to vector embedding leads to higher perplexities and decline in performance. Ablation Study on TAPE Hyperparameter. We aim to investigate the impact of varying on learning performance. Using the same pre-training settings as described in Section 4.2, we directly report the perplexity on the GitHub test dataset. As shown in Tab. 7, there is no significant difference when using different values of I, although trend of first decreasing and then increasing can be observed. This suggests that range of values from 2H = 24 to 3H = 48 may yield better performance compared to other settings. Therefore, as general guideline, we recommend considering {2, 3, 4}H to optimize TAPEs performance. Table 7: Ablation study on TAPE hyperparameter I. We evalute pre-trained models perplexity across varying sequence lengths on the GitHub test set."
        },
        {
            "title": "TAPE",
            "content": "Added Params. (M) 0.11 0.22 0.44 0.88 1.76 12 24 48 96 192 128 133.2 133.0 132.0 133.2 133."
        },
        {
            "title": "Perplexity",
            "content": "256 87.9 86.1 86.6 87.5 87.3 512 65.2 63.2 63.9 64.5 64.5 1024 53.6 51.8 52.2 52.7 53. Stability of TAPE under Positional Shifts. Stability in this context refers to the consistency of sequences representation under positional shifts (Sun et al., 2022). To evaluate the stability of TAPE, we examine two types of positional shifts: (1) appending [BOS] tokens at the beginning of the sequence and (2) initializing positional indices with non-zero values to simulate phase shift (Sinha et al., 2022). We analyze two aspects of the representation: the attention weights and the dot product of positional embeddings, quantifying their changes after applying positional shifts. For comparison, we include RoPE, which also exhibits O(R)-equivariance (R = 2) and remains consistent across layers, as well as TAPE without equivariance, as explored in previous ablations. Table 8: Comparison of RoPE, TAPE, and TAPE without equivariance (w/o EQ) under positional shifts. The table shows differences in attention weights (top) and positional embedding dot products (bottom) across layers for two shift methods: adding three [BOS] tokens (Add Tokens) and starting position IDs at 3 (Shift IDs). Atten. Diff. (102) Add Tokens Shift IDs Layer 1 Layer 2 Layer 4 Layer 8 Layer 1 Layer 2 Layer 4 Layer RoPE TAPE w/o EQ 8.93 9.08 11.30 8.51 11.24 11.38 12.29 12.23 13.32 11.46 13.78 14.55 0.01 0.01 0. 0.02 0.02 0.24 0.02 0.04 0.37 0.03 0.04 0.51 PE Dot Prod. Diff. (%) Add Tokens Shift IDs Layer 1 Layer 2 Layer 4 Layer 8 Layer 1 Layer 2 Layer 4 Layer 8 RoPE TAPE w/o EQ 0.03 0.03 0.03 0.03 0.37 2.29 0.03 2.75 3.34 0.03 6.62 6. 0.03 0.03 0.03 0.03 0.02 0.54 0.03 0.03 0.44 0.03 0.04 0.86 As shown in Tab. 8, TAPE demonstrates stability comparable to RoPE, maintaining consistent attention weights and positional embedding dot products across different layers. Among these, the near-zero change (not exactly zero, attributable to numerical error observed in RoPE as well) in the dot-product when shifting IDs serves as empirical evidence for Proposition 1. However, when equivariance is removed from TAPE, the differences increase significantly, especially in deeper layers, highlighting the importance of equivariance in preserving stability. Additional Evaluation on Fine-tuned Llama-7b. Modern benchmarks provide comprehensive means to assess large language models advanced capabilities in language understanding and reasoning. Accordingly, we further evaluate our fine-tuned Llama-7b (Sec. 4.3) on standard benchmarks, including ARC (Clark et al., 2018) and MMLU (Hendrycks et al., 2021). Table 9: Accuracy in Percentage Across Methods and Benchmarks Method Humanities Social Sciences STEM Other Challenge Easy MMLU (%) ARC (%) LoRA LongLoRA ThetaScaling TAPE 39.09 0.69 37.53 0.69 37.45 0.69 37.96 0.69 46.47 0.88 43.55 0.88 43.16 0.88 45.40 0.88 33.65 0.83 32.54 0.83 33.05 0.83 33.27 0.83 45.83 0.89 43.84 0.88 44.64 0.88 45.06 0.88 45.31 1.45 45.31 1.45 45.65 1.46 46.25 1.46 74.28 0.90 74.16 0.90 74.24 0.90 74.16 0. As Tab. 9 shows, TAPE demonstrates notable performance compared to other methods on MMLU and ARC benchmarks. While TAPEs accuracy on MMLU is slightly lower than that of LoRA, it consistently outperforms others. On the ARC benchmark, TAPE performs comparably to other methods on the Easy subset but exhibits an advantage on the Challenge subset, underscoring its potential in complex reasoning tasks. Remarkably, these results are achieved using only fine-tuning, without pretraining TAPE, despite the presence of certain degree of architectural shift. Additional Evaluation in Arithmetic Learning We also evaluate the effectiveness of TAPE in Sec. 4.1 using different training and testing length: 20/40 instead of 40/80. This setup is easier for the model to learn, with convergence achieved in less than half the steps. As shown in Fig. 5, TAPE outperforms FIRE with marginal improvement of 5%. However, this improvement is less pronounced compared to the case with train/test length of 40/80, suggesting that TAPE may be more effective in tackling complex and challenging tasks than simpler ones. Figure 5: Accuracy on addition task trained with length 20 test on 2 context length. The average accuracy across the heatmap is 26.12%, 26.12%, 39.44% and 41.42% respectively for RoPE, RandPE, FIRE and TAPE. Figure 6: Accuracy on addition task on 2 context length. The average accuracy is 26.98%, 32.82% and 33.92% respectively for FIRE, TAPE and TAPE + YaRN. Integration with Extrapolation Technique. Inspired by the demonstrated potential of NTKbased methods (Peng et al., 2023) to enhance the length extrapolation ability of RoPE, we have explored integrating TAPE with such techniques when initialized as RoPE. Specifically, we selected the most recent method, YaRN (Peng et al., 2023), and implemented its integration with TAPE to evaluate its performance in length extrapolation. The experiments were conducted under the same settings as described in Sec. 4.1. As shown in Fig. 6, the diagonal region exhibits darker colors, indicating higher accuracies. Quantitatively, YaRN effectively enhances the length extrapolation performance of TAPE with RoPE initialization, achieving modest relative improvement of 3.4%. However, it still struggles to generalize to unseen sequences with significantly longer digit lengths."
        },
        {
            "title": "D FURTHER ILLUSTRATIONS",
            "content": "Visualization of PE Patterns. To better understand the impact of TAPE, we analyze its attention and positional embedding (PE) dot-product patterns. Fig. 7 compares the patterns of TAPE and RoPE in the last layer, while Fig. 8 illustrates the evolution of TAPEs dot-product patterns from shallow to deeper layers. The x-axis and y-axis correspond to the token positions of sampled input sequence. As shown in Fig. 7, TAPE demonstrates more evenly distributed long-range attention patterns, whereas RoPE tends to emphasize token locality. In Fig. 8, TAPE behaves similarly to RoPE in the first layer but gradually reduces the dominance of diagonal patterns as the depth increases. This transition results in the formation of grid-like patterns, indicating that the model starts to focus on distant tokens in structured and periodic manner. (a) Dot-product patterns of positional embeddings of TAPE and RoPE. (b) Difference between TAPE and RoPE Figure 7: Comparison of TAPE and RoPE methods in terms of positional embedding dot-product patterns and their resulting attention differences. (a) TAPE demonstrates systematic attention to surrounding tokens with relatively small dynamic ranges, whereas RoPE exhibits highly significant diagonal pattern with distinctively black regions. (b) TAPE effectively attends to longer-range tokens, avoiding excessive attention to the self-token, in contrast to RoPE. Figure 8: Dot-product patterns of positional embeddings in layers 1, 4, 8, and 12 (last) of TAPE. 22 Examples on QuALITY. To further validate TAPEs superior performance on the SCROLLS benchmark, we present two example questions from the QuALITY dataset within the SCROLLS benchmark. As shown in Tab. 10 and the detailed questions in Tab. 11, TAPE consistently generates either the correct answer or response similar to the correct answer, even if not an exact match. In contrast, xPos and RandPE produce meaningful sentences that are unrelated to the specific question. RoPE and ALiBi, however, generate incoherent outputs: RoPE tends to repeat certain phrases, while ALiBi fails to recognize the presence of question, producing the same irrelevant answer regardless of the input. Table 10: Comparing answers of different methods on example questions in QuALITY. Question Question Method"
        },
        {
            "title": "Ground\nTruth\nTAPE\nxPos",
            "content": "Answer"
        },
        {
            "title": "The secret service budget was small\nThey were all they were waiting for",
            "content": "RoPE RandPE Their human opinion was trusted by others who have trust the services of their people Their orless them together with their repories did not only they didns never done was never done was never done... (repeating) Jimmy Carter is the presidents de facto president ALiBi EM Answer"
        },
        {
            "title": "Only the private quarters or\nthe office restroom\nOnly the private quarters\nOnly a tiny part of the right of\nthe right to leave foreverish\nOnly a handsome man",
            "content": "The/O only the full-College All of the full-College All of the full-College... (repeating) Jimmy Carter is the presidents de facto president EM Table 11: Example Questions in QuALITY Qu. (ID: 20007 RZDMZJYW 2) What made it easier for previous presidents to get away with adultery? Qu. (ID: 20007 RZDMZJYW 4) Where in the White House is it feasible for the president to meet woman? (A) Only the East Wing (B) Only the private quarters (C) Only the oval office, bowling alley, or East Wing (D) Only the private quarters or the office restroom (A) Their staff did not know (B) They always tried to hide it well (C) The secret service budget was small (D) The reporters never found out Article Content: The logistics of presidential adultery. The Washington Times could hardly contain its excitement: former FBI agent assigned to the White House describes in new book how President Clinton slips past his Secret Service detail in the dead of night, hides under blanket in the back of dark-colored sedan, and trysts with woman, possibly celebrity, at the JW Marriott Hotel in downtown Washington. For Clinton-haters, Gary Aldrichs tale sounded too good to be true. And it was. The not-so-Secret-Service agents source turned out to be thirdhand rumor passed on by Clinton scandalmonger David Brock. Those who know about White House securityClinton staffers, the Secret Service, former aides to Presidents Reagan and Bushdemolished Aldrichs claims. Clinton couldnt give his Secret Service agents the slip (they shadow him when he walks around the White House), couldnt arrange private visit without tipping off hotel staff, and couldnt re-enter the White House without getting nabbed. (Guards check all cars at the gateespecially those that arrive at 4 a.m.) Even so, the image resonates. For some Americans, it is an article of faith: Bill Clinton cheated on his wife when he was governor, and he cheats on her as president. But can he? Is it possible for the president of the United States to commit adultery and get away with it? Maybe, but its tougher than you think. Historically, presidential adultery is common. Warren Harding cavorted with Nan Britton and Carrie Phillips. Franklin Roosevelt entertained Lucy Rutherford at the White House when Eleanor was away. America was none the wiser, even if White House reporters were. Those who know Clinton is cheating often point to the model of John F. Kennedy, who turned presidential hanky-panky into science. Kennedy invited mistresses to the White House for afternoon (and evening, and overnight) liaisons. Kennedy seduced women on the White House staff (including, it seems, Jackies own press Continued on next page... 23 secretary). Kennedy made assignations outside the White House, then escaped his Secret Service detail by scaling walls and ducking out back doors. If Kennedy did it, so can Clinton. Well, no. Though Clinton slavishly emulates JFK in every other way, hed be fool to steal Kennedys MO damour. Heres why: 1) Too many people would know. Kennedy hardly bothered to hide his conquests. According to Kennedy mistress (and mob moll) Judith Campbells autobiography, those who knew about their affair included: Kennedys personal aides and secretary (who pandered for him), White House drivers, White House gate guards, White House Secret Service agents, White House domestic staff, most of Campbells friends, lot of Kennedys friends, and several Kennedy family members. Such broad circulation would be disastrous today because: 2) The press would report it. Kennedy conducted his affairs brazenly because he trusted reporters not to write about them. White House journalists knew about, or at least strongly suspected, Kennedys infidelity, but never published story about it. Ask Gary Hart if reporters would exercise the same restraint today. Clinton must worry about this more than most presidents. Not only are newspapers and magazines willing to publish an adultery story about him, but many are pursuing it. For the same reason, Clinton would find it difficult to hire mistress. lovely young secretary would set off alarm bells in any reporter investigating presidential misbehavior. Says former Clinton aide, There has been real tendency to have no good-looking women on the staff in order to protect him. 3) Clinton cannot avoid Secret Service protection. During the Kennedy era, the Secret Service employed fewer than 500 people and had an annual budget of about $4 million. Then came Lee Harvey Oswald, Squeaky Fromme, and John Hinckley. Now the Secret Service payroll tops 4,500 (most of them agents), and the annual budget exceeds $500 million (up 300 percent just since 1980). At any given time, more than 100 agents guard the president in the White House. Top aides from recent administrations are adamant: The Secret Service never lets the president escape its protection. So whats randy president to do? Any modern presidential affair would need to meet stringent demands. Only tiny number of trusted aides and Secret Service agents could know of it. They would need to maintain complete silence about it. And no reporters could catch wind of it. Such an affair is improbable, buttake heart, Clinton-hatersits not impossible. Based on scuttlebutt and speculation from insiders at the Clinton, Bush, Reagan, and Ford White Houses, here are the four likeliest scenarios for presidential adultery. 1) The White House Sneak. This is discreet variation of the old Kennedy/Campbell liaison. Its late at night. The presidents personal aides have gone home. The family is away. He is alone in the private quarters. The private quarters, a.k.a. the residence, occupy the second and third floors of the White House. Secret Service agents guard the residences entrances on the first floor and ground floors, but the first family has privacy in the quarters themselves. Maids and butlers serve the family there, but the president and first lady ask them to leave when they want to be alone. The president dials friend on his private line. (Most presidents placed all their calls through the White House operators, who kept record of each one; the Clintons installed direct-dial line in the private quarters.) The president invites the friend over for cozy evening at the White House. After he hangs up with the friend, he phones the guard at the East Executive Avenue gate and tells him to admit visitor. He also notifies the Secret Service agent and the usher on duty downstairs that they should send her up to the residence. taxi drops the woman near the East gate. She identifies herself to the guard, who examines her ID, runs her name through computer (to check for outstanding warrants), and logs her in database. White House usher escorts her into the East Wing of the White House. They walk through the East Wing and pass the Secret Service guard post by the White House movie theater. The agent on duty waves them on. The usher takes her to the private elevator, where another Secret Service agent is posted. She takes the elevator to the second floor. The president opens the door and welcomes her. Under no circumstances could she enter the living quarters without first encountering Secret Service agents. Let us pause for moment to demolish two of the splashier rumors about White House fornication. First, the residence is the only place in the White House where the president can have safe (i.e., uninterrupted) sex. He can be intruded upon or observed everywhere elseexcept, perhaps, the Oval Office bathroom. Unless the president is an exhibitionist or lunatic, liaisons in the Oval Office, bowling alley, or East Wing are unimaginable. Second, the much-touted tunnel between the White House and the Treasury Department is all-but-useless to the presidential adulterer. It is too well-guarded. The president could smuggle mistress through it, but it would attract far more attention from White House staff than straightforward gate entry would. Meanwhile, back in the private quarters, the president and friend get comfortable in one of the 14 bedrooms (or, perhaps, the billiard room). After pleasant 15 minutes (or two hours?), she says goodbye. Depending on how long she stays, she may pass different shift of Secret Service agents as she departs. She exits the White House grounds, unescorted and unbothered, at the East gate. The Risks: gate guard, an usher, and handful of Secret Service agents see her. All of them have very good idea of why she was there. The White House maid who changes the sheets sees other suspicious evidence. And the womansrealname is entered in Secret Service computer. None of this endangers the president too much. The computer record of her visit is private, at least for several decades after he leaves office. No personal aides know about the visit. Unless they were staking out the East gate, no journalists do either. The Secret Service agents, the guard, the steward, and the maid owe their jobs to their discretion. Leaks get them fired. That said, the current president has every reason not to trust his Secret Service detail. No one seriously compares Secret Service agents (who are pros) to Arkansas state troopers (who arent). But Clinton might not trust any Continued on next page... 24 security guards after the beating he took from his Arkansas posse. Also, if other Secret Service agents are anything like Aldrich, they may dislike this president. One Secret Service leakthe lamp-throwing storyalready damaged Clinton. Agents could tattle again. 2) The Off-the-Record Visit. Late at night, after his personal aides and the press have gone home, the president tells his Secret Service detail that he needs to take an off-the-record trip. He wants to leave the White House without his motorcade and without informing the press. He requests two agents and an unobtrusive sedan. The Secret Service shift leader grumbles but accepts the conditions. Theoretically, the president could refuse all Secret Service protection, but it would be far more trouble than its worth. He would have to inform the head of the Secret Service and the secretary of the Treasury. The president and the two agents drive the unmarked car to woman friends house. Ideally, she has covered garage. (An apartment building or hotel would raise considerably the risk of getting caught.) The agents guard the outside of the house while the president and his friend do their thing. Then the agents chauffeur the president back to the White House, re-entering through the Southwest or Southeast gate, away from the press station. The Risks: Only two Secret Service agents and their immediate supervisor know about the visit. It is recorded in the Secret Service log, which is not made public during the administrations tenure. Gate guards may suspect something fishy when they see the car. reporter or passer-by could spy the presidenteven through tinted windowsas the car enters and exits the White House. The friends neighbors might spot him, or they might notice the agents lurking outside her house. neighbor might call the police to report the suspicious visitors. All in all, risky, though not unthinkable, venture. 3) The Camp David Assignation. bucolic, safer version of the White House Sneak. The president invites group of friends and staffersincluding his paramour but not his wifeto spend the weekend at Camp David. The girlfriend is assigned the cabin next to the presidents lodge. Late at night, after the Hearts game has ended and everyone has retired to their cabins, she strolls next door. There is Secret Service command post outside the cabin. The agents on duty (probably three of them) let her enter. few hours later, she slips back to her own cabin. The Risks: Only few Secret Service agents know about the liaison. Even though the guest list is not public, all the Navy and Marine personnel at Camp David, as well as the other guests, would know that the presidential entourage included an attractive woman, but not the first lady. That would raise eyebrows if it got back to the White House press room. 4) The Hotel Shuffle. The cleverest strategy, and the only one that cuts out the Secret Service. The president is traveling without his family. The Secret Service secures an entire hotel floor, reserving elevators and guarding the entrance to the presidents suite. The presidents personal aide (a man in his late 20s) takes the room adjoining the presidents. An internal door connects the two rooms, so the aide can enter the presidents room without alerting the agents in the hall. This is standard practice. Late in the evening, the aide escorts comely young woman back to the hotel. The Secret Service checks her, then waves her into the aides room. She emerges three hours later, slightly disheveled. She kisses the aide in the hall as she leaves. Someone got luckybut who? The Risks: The posted Secret Service agents might see through the charade. More awkwardly, the aide would be forced to play the seamy role of procurer. (He would probably do it. Kennedys assistants performed this task dutifully.) In short, presidential adultery is just barely possible in 1996. But it would be extremely inconvenient, extremely risky, and potentially disastrous. It seems, in fact, lot more trouble than its worth. president these days might be wiser to imitate Jimmy Carter, not Jack Kennedy, and only lust in his heart."
        }
    ],
    "affiliations": [
        "Georgia Tech",
        "Princeton University",
        "University of Texas at Austin",
        "Zhejiang University"
    ]
}