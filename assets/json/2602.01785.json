{
    "paper_title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding",
    "authors": [
        "Yuling Shi",
        "Chaoxiang Xie",
        "Zhensu Sun",
        "Yeheng Chen",
        "Chenxu Zhang",
        "Longfei Yun",
        "Chengcheng Wan",
        "Hongyu Zhang",
        "David Lo",
        "Xiaodong Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 5 8 7 1 0 . 2 0 6 2 : r CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding Yuling Shi1, Chaoxiang Xie2, Zhensu Sun3, Yeheng Chen4, Chenxu Zhang5, Longfei Yun6, Chengcheng Wan7,8, Hongyu Zhang9, David Lo3, Xiaodong Gu1 1Shanghai Jiao Tong University, 2Hohai University, 3Singapore Management University, 4Beijing Institute of Technology, Zhuhai, 5Imperial College London, 6UC San Diego, 7East China Normal University, 8Shanghai Innovation Institute, 9Chongqing University"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become critical bottleneck. Currently, these models rely on text-based paradigm that treats source code as linear sequence of tokens, which leads to linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8 compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4 compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out shift toward image-modality code representation as pathway to more efficient inference1."
        },
        {
            "title": "Introduction",
            "content": "LLMs have established dominant paradigm in software engineering (Chen et al., 2021; Rozi`ere et al., 2023; Fan et al., 2023; Shi et al., 2025b; Jiang et al., 2024). Currently, these models primarily operate on text-based paradigm, where source code is treated as linear sequence of tokens (Zhang et al., 2024). However, as software systems grow in scale and complexity, the resulting linear increase in context length and its associated computational overhead has become significant efficiency bottleneck (Guo et al., 2023; Bogomolov et al., 2024; Shi et al., 2026; Wang et al., 2026a; 2025c). The rapid advancement of multimodal LLMsparticularly Vision Language Models (VLMs) that integrate visual understanding capabilities (OpenAI, 2023; Gemini Team et al., 2023; Liu et al., 2023; Yin et al., 2023)presents promising opportunity to mitigate this limitation. To be specific, many popular LLMs, such as GPT-5 (OpenAI, 2025c) and Gemini-3 (Google Correspondence: xiaodong.gu@sjtu.edu.cn 1Code and data available at https://github.com/YerbaPage/CodeOCR. 1 Figure 1: An Example of Code Representations across Different Modalities. DeepMind, 2025a), now natively support multimodal inputs, enabling the processing of text and visual data within unified architecture. This capability motivates us to reconsider how source code can be represented. Compared with text, image modality exhibits key advantage in terms of compressibility (Li et al., 2025b). Image data can be scaled by simply adjusting resolution (Wei et al., 2025; Li et al., 2025b), whereas compressing code text for LLMs is discrete and often lossy process involving token pruning or semantic rewriting.As result, representing source code as rendered images (i.e., code images) could provide more scalable and computationally efficient alternative to traditional text representations. Consider the code snippet in Figure 1 for example. This snippet, when represented as text (left), costs approximately 110 text tokens. When rendered into code image (middle), its resolution is calibrated to occupy an equivalent budget of 110 visual tokens (image tokens are priced at standard text token rates for vision-capable models like GPT-5 and Gemini-3 (OpenAI, 2025a; Google, 2025)). However, this code image can be further compressed by reducing its resolution, yielding 75.5% reduction to just 27 visual tokens while maintaining clarity in readability. In contrast, text-based compression methods that achieve similar reduction ratios typically rely on token pruning, which often result in significant information loss (Shi et al., 2025a; Zhang et al., 2022; Pan et al., 2025). This flexibility highlights the potential of code images to alleviate the high inference costs and context-window constraints faced by current LLMs (Jiang et al., 2023; Shi et al., 2025a). To leverage this advantage, fundamental question naturally arises: how effectively can LLMs understand and reason over code images? The answer to this question may signal paradigm shift in how source code should be represented for AI to understand. However, to the best of our knowledge, this question remains largely unexplored within the research community. Existing research (You et al., 2024; Baechler et al., 2024; Chen et al., 2025a; Yang et al., 2025b) has largely focused on GUI understanding and UI-to-code generation, where the visual inputs are graphical user interfaces rather than code images. While these works demonstrate that LLMs can exhibit strong coding capabilities when processing visual inputs, they do not investigate whether code images themselves constitute viable or effective representation of source code. To address this question, large-scale, comprehensive experimental evaluation of LLMs performance on code images is necessary. To fill this knowledge gap, we conduct comprehensive empirical study on image-based code representation with LLMs guided by five research questions. In the study, seven widely used LLMs, all of which support multimodal inputs, are evaluated across 4 downstream tasks (code completion, code summarization, clone detection, and code question answering), while systematically investigating compression ratios (18) and rendering strategies (plain, highlighted, bolded). Next, we introduce the five RQs and summarize essential findings. RQ1: How effective are LLMs in visual code understanding compared with textual code? As the first step, we investigate whether LLMs can process visualized code as effectively as traditional text. Specifically, for each sample in all benchmarks, we derive variant by rendering it into images that contain the same number of tokens. We then compare model performance when the input is provided as raw text versus as code image Findings. For all the four downstream tasks, LLMs with visualized code input can achieve comparable or even superior performance to textual input. For example, GPT-5-mini 2 achieves 42% F1 improvement with code images over raw text for clone detection, and Gemini-3-Pro demonstrates comparable or superior performance across all four tasks. These results indicate that replacing textual code with visual representations is both viable and promising for code understanding, demonstrating the feasibility of leveraging the multimodal capabilities of modern LLMs. However, performance improvements are not uniform across models and tasks, suggesting that current LLMs are not yet fully optimized for this paradigm. Bridging this gap remains an important direction for future research. RQ2: How resilient are LLMs to visual compression across different coding tasks? Building on the feasibility established in RQ1, we further explore key advantage of visual representations, i.e., their compressibility. Specifically, we vary the compression ratios of code images from 1 to 8 and systematically evaluate LLM performance under each compression ratio. Findings. LLMs can exhibit exceptional compression resilience across tasks, with multiple models exceeding raw text baseline even at 8 compression ratio, i.e., costing only 12.5% of tokens for raw text input. For example, Gemini-3-Pro achieves 79.5% accuracy on code question answering at 8 compression, surpassing its 74.8% raw text baseline. These results effectively demonstrate remarkable robustness of LLMs understanding of code images, highlighting key advantage of image-based representations over linear text. Consistent with RQ1, compression resilience varies across tasks and models, with state-of-the-art models like Gemini-3 and GPT-5 series maintaining stable or improved performance under nearly all compression settings. This suggests that robust visual compression understanding is an achievable capability, pointing to clear opportunities for future model development. RQ3: Can visual enhancements (e.g., syntax highlighting, bold rendering) further improve LLMs understanding of code images? In the previous RQs, we established that code images are viable and compressible medium. However, another key advantage of the visual modality is the ability to incorporate visual cues that are absent in raw text. In this RQ, we investigate whether visual enhancements, specifically syntax highlighting and bolding, provide benefits to LLMs. Findings. Visual enhancements improve model performance primarily at moderate compression levels (14), with diminishing returns at higher ratios. Both syntax highlighting and bold rendering provide consistent gains when the underlying visual signal remains legible at 12 compression, multiple models show 13% improvements in Edit Similarity and accuracy. However, at 8 compression, these enhancements offer limited benefit, as reduced resolution obscures the visual distinctions they introduce. Notably, bold rendering can exacerbate degradation at extreme compression ratios by further reducing character clarity. These findings indicate that visual enhancements are most effective within compression sweet spot, motivating future work on adaptive rendering strategies. RQ4: Can LLMs understanding of visualized code generalize across programming languages? To ensure our findings are not Python-specific, we replicate the experiments for RQ1RQ3 on Java. Findings. The core trends remain consistent across languages. The Gemini family achieves up to 12% ES improvement in Java code completion, and clone detection shows 620% ACC gains with visual inputs across multiple models. Model-specific strengths and compression resilience patterns also hold: models that performed well under compression in Python maintain their relative advantages in Java. These results support that the core findings generalize across programming languages. RQ5: How does visual compression degrade code information, and what error types emerge across compression ratios? To better understand how information is lost under visual compression, we conduct detailed degradation analysis. Specifically, we perform OCR-style code reconstruction experiments, in which LLMs are required to reproduce the code content from compressed code images across compression ratios ranging from 1 to 8. We then analyze the errors between the original and reconstructed code. Findings. Information degradation follows clear hierarchical pattern. Token-level errors emerge first at low compression (12), followed by line-level errors at moderate compres3 Figure 2: Multimodal Processing Pipeline for Visualized Code Understanding in MLLMs. sion (24), while block-level errors dominate under high compression (48). The 48 range represents critical thresholdmost models experience significant degradation, while the Gemini-3 family maintains stability with high CodeBLEU even at 8 compression. Crucially, we found that the token-level errors do not always impair downstream semantic performance, suggesting that LLMs can often infer the correct logic even when the visual signal is slightly blurred. These empirical results provide valuable insights for developing code image understanding systems. To this end, we implement CODEOCR, practical tool for rendering source code into images with configurable visual enhancements and compression ratios for researchers or developers to use. To sum up, this paper makes the following contributions: We perform the first comprehensive empirical study on visual code understanding, evaluating seven state-of-the-art MLLMs across four downstream tasks with systematic analysis of compression ratios and rendering strategies. We empirically demonstrate that image-based code representation is viable technical direction, where, without any targeted optimization, multiple existing LLMs can achieve comparable or even superior performance to text-based baselines on code understanding tasks. We propose and implement practical tool called CODEOCR for rendering source code into image, supporting LLMs to process code in more token-efficient manner."
        },
        {
            "title": "2 Background",
            "content": "Multimodal capability has become native feature in state-of-the-art LLMs like GPT-5 and Gemini-3, enabling them to process both text and images within unified architecture (OpenAI, 2025c; Google DeepMind, 2025b; Bai et al., 2025; Team et al., 2026). But how do these models process visual inputs? We illustrate the pipeline in Figure 2, which consists of four stages. Stage 1: Inputs. The code is rendered as an image RHW3 with visual cues like syntax highlighting and indentation (left panel). Alongside, text prompt provides the instruction. While text-based models directly tokenize raw code strings, MLLMs treat code as 2D visual artifact. Stage 2: Encoding & Tokenization. The rendered image is divided into fixed-size patches (e.g., 1414 pixels). visual encoder (typically Vision Transformer) converts these patches into visual embeddings: = Encoder(I) = {v1, v2, . . . , vN} (1) where each vi captures the visual features of patch. In parallel, the text prompt is tokenized into sequence of text tokens (words or subwords). Stage 3: Alignment & Fusion. The visual and text tokens are processed through separate alignment modules before fusion. For visual tokens, V-L Adapter applies pooling and 4 Figure 3: Overview of the Empirical Study Design and Core Findings. projection to compress adjacent patches into aligned visual embeddings. For instance, 2 2 pooling operation merges four patches: Tv = MLP(Concat(vi,j, vi+1,j, vi,j+1, vi+1,j+1)) (2) This reduces the number of visual tokens while preserving semantic density. For text tokens, lookup table maps each token to its corresponding text embedding. The aligned visual embeddings and text embeddings are then concatenated to form unified input sequence. Stage 4: Multimodal Modeling & Output. The MLLM backbone (self-attention layers) processes the unified sequence: Input = [Tv; Ttext] (3) Unlike text models that rely on discrete vocabulary and syntax rules, MLLMs learn to interpret continuous visual patternssuch as color-coded keywords, indentation depth, and bracket alignmentdirectly from pixel data. This enables them to understand code structure without explicit parsing, leveraging the same spatial reasoning used for natural images. Importantly, this multimodal capability is additive rather than trade-offmultimodal variants maintain comparable performance to text-only counterparts (e.g. Qwen-3-VL vs Qwen-3) on NLP and coding benchmarks (Bai et al., 2025; Team et al., 2026). This visual processing capability opens new possibilities for representing structured contentincluding source codeas images rather than text tokens, which we systematically investigate in this paper."
        },
        {
            "title": "3 Experimental Setting",
            "content": "In this section, we introduce the experimental setting for this study, including the task, models, benchmark, evaluation metrics, and implementation details. Our study is guided by five research questions: RQ1: How effective are LLMs in visual code understanding compared with textual code? RQ2: How resilient are LLMs to visual compression across different coding tasks? 5 RQ3: Can visual enhancements (e.g., syntax highlighting, bold rendering) further improve LLMs understanding of code images? RQ4: Can LLMs ability of visual code understanding generalize across programming languages? RQ5: How does visual compression degrade code information, and what error types emerge across compression ratios? An overview of our experimental design is presented in Figure 3. Our investigation follows progressive logic: we begin by establishing the fundamental feasibility of visual code understanding compared to text (RQ1). Building on this baseline, we explore the core advantage of the visual modalityoptical compressionby systematically varying resolution (RQ2). To further optimize performance, we examine whether visual cues like syntax highlighting can mitigate compression loss (RQ3). Finally, we validate the robustness of our findings across different programming languages (RQ4) and conduct microscopic analysis of information degradation patterns (RQ5). 3.1 Benchmark and Metrics Table 1: Summary of Tasks Used in Our Evaluation. Task Language # Examples Avg. Context Len. Avg. GT Len. Code Summarization Code Completion Code Clone Detection Python Python Java Python Java Code Question Answering Python 200 200 200 200 200 6184.1 6138.6 5653.5 124.9 215. 1316.9 1481.8 12.3 11.9 1.0 1.0 1.0 To comprehensively evaluate visual code understanding, we select four representative tasks spanning different levels of code comprehension. All tasks require models to process code as input, aligning with our focus on code understanding capability. We primarily evaluate on Python and extend our analysis to Java in RQ4. Dataset statistics are summarized in Table 1 and the lengths are computed with the tokenizer of Qwen-3-VL (Bai et al., 2025). Code Completion tests fine-grained syntactic understanding. We adopt the LongCodeCompletion dataset (Guo et al., 2023) and randomly sample 200 Python and 200 Java samples from the challenging subset curated by Shi et al. (2025a). We apply Retrieval-Augmented Generation (RAG) to provide relevant code context (details in Section 3.5). The average context lengths are 6,139 tokens for Python and 5,654 tokens for Java. We use Exact Match (EM) and Edit Similarity (ES) (Guo et al., 2023) for evaluation: EM measures whether the generated code exactly matches the ground truth, while ES captures partial correctness via token-level Levenshtein distance. Code Summarization evaluates high-level semantic extraction. We use the LongModuleSummarization dataset (Bogomolov et al., 2024) following (Shi et al., 2025a), containing 109 examples with an average of 6,184 tokens per sample. We adopt CompScore (Bogomolov et al., 2024), an LLM-as-judge metric where DeepSeek-V3.2 (DeepSeek-AI, 2024) compares generated documentation against ground truth with bidirectional averaging to mitigate ordering bias (scores range 0100, where 50 indicates parity). Code Clone Detection assesses semantic similarity recognition. We employ GPTCloneBench (Alam et al., 2023), focusing on Type-4 (semantic) clonescode pairs implementing identical functionality with different syntax and structure. For each language (Python and Java), we randomly sample balanced dataset of 200 pairs (100 positive, 100 negative). The average context lengths are 125 tokens for Python and 216 tokens for Java. We report Accuracy (ACC) and F1 score, where F1 provides more balanced assessment given the class imbalance. 6 Table 2: Summary of Evaluated MLLMs with Release Information and API Pricing (per 1M tokens). Model Qwen-3-VL GLM-4.6v GPT-5-mini GPT-5.1 Gemini-2.5-Pro Gemini-3-Flash Gemini-3-Pro Release Knowledge Date 2025-09 2025-12 2025-08 2025-11 2025-06 2025-12 2025-11 Cut-off 2024-06 2025-07 2024-05 2024-09 2025-01 2025-01 2025-01 Type Open-weight Open-weight Proprietary Proprietary Proprietary Proprietary Proprietary Multimodal Ability Price ( 200k) Input Output Price (> 200k) Input Output $0.40 $0.30 $0.25 $1.25 $1.25 $0.50 $2.00 $1.60 $0.90 $2.00 $10.00 $10.00 $3.00 $12. $0.40 $0.30 $0.25 $1.25 $2.50 $0.50 $4.00 $1.60 $0.90 $2.00 $10.00 $15.00 $3.00 $18.00 Code Question Answering examines code comprehension through question answering, where models must select the correct answer from multiple choices based on the provided code context. We construct dataset following the format of LongCodeQA (Rando et al., 2025), as our preliminary experiments revealed severe data leakage in the original dataset GPT-5.1 achieved 81.5% accuracy when given only questions and options without any code context, indicating that models could answer correctly through memorization rather than code understanding. To address this, we crawled 35 Python repositories from GitHub (created after August 2025, 10+ stars) and used DeepSeek V3.2 (DeepSeek-AI, 2024) to generate an initial pool of 1,000 candidate (context, question) pairs following the original datasets exact format. Three PhD students unaffiliated with the authors, each with 3+ years of programming experience, then validated each question one by one, ensuring: (1) the question is meaningful and valuable for evaluating code comprehension, (2) the question is answerable from the provided code context, (3) the context is necessary to determine the correct answer, and (4) exactly one answer is unambiguously correct. Only questions receiving unanimous approval from all three validators were retained, and annotation continued until 200 validated samples were collected. Finally, the authors shuffled answer option orders to avoid positional bias. This curated dataset is publicly available for research use. We evaluate performance using Accuracy (ACC). 3.2 Studied Large Language Models To ensure the generalizability of our findings, we evaluate seven state-of-the-art LLMs with multimodal capability spanning both proprietary and open-weight categories. Table 2 summarizes model details and official pricing as of January 30, 2026 (OpenRouter, 2025). The proprietary models include GPT-5-mini and GPT-5.1 (OpenAI, 2025c;b) from OpenAI, and Gemini-2.5-Pro, Gemini-3-Flash, and Gemini-3-Pro (Comanici et al., 2025; Google DeepMind, 2025a;b) from Google. For open-weight models, we include Qwen-3-VL with 235B parameters (Bai et al., 2025) and GLM-4.6v with 108B parameters (V Team et al., 2026), enabling reproducible research and architectural analysis. Importantly, these proprietary models have multimodal capability natively integrated, while open-weight models have been officially benchmarked to match their text-only counterparts (Bai et al., 2025; Team et al., 2026). It ensures that our experimental setup does not introduce confounding factors from degraded baseline capability. 3.3 Visual Rendering of Source Code Code Rendering. We render source code into images at high base resolution of 22402240 pixels, following prior work (Liang et al., 2026). This resolution is selected for compatibility with modern MLLMs, as it is divisible by common image patch sizes (e.g., 14 and 16 pixels) used in visual encoders (Bai et al., 2025), ensuring that no partial patches are created during tokenization. By default, we use plain renderingblack monospace text on white backgroundwhich serves as the baseline configuration throughout our experiments. To investigate the effect of visual enhancements (RQ3), we additionally support two variants: bold rendering with increased stroke width, and syntax highlighting following Visual Studio Codes (Microsoft Corporation, 2024) Default Light theme (Figure 4). When the code exceeds single page, we split it into multiple consecutive images while preserving line boundaries. Modern MLLMs natively support multi-image inputs and can process them in the provided order (Bai et al., 2025; Team et al., 2026). 7 Figure 4: Examples of Visual Rendering Strategies: Plain, Bold, and Highlight. Resolution Compression. MLLMs process images by dividing them into fixed-size patches and encoding each patch as visual tokens (Section 2). For an image of resolution with patch size p, the visual token count is (W/p) (H/p). We define compression ratio such that the visual token count equals exactly 1/k of the original text token count; thus, at 1 compression, the visual token count matches the text token count. And since providers typically price visual and text tokens at similar rates, this also results in comparable cost (OpenRouter, 2025). To generate images at any compression level, we start from the code image at the high base resolution (22402240), which produces more visual tokens than the equivalent text tokens, ensuring sufficient visual fidelity as the starting point. We then apply bilinear downsampling to reach the exact target resolution corresponding to the desired compression. In our experiments, we evaluate compression ratios of 1, 2, 4, and 8 to investigate the trade-off between visual fidelity and token efficiency. 3.4 Baselines and Input Design Input Modality. Following the established paradigm in visual text understanding research (Wei et al., 2025; Liang et al., 2026; Zhao et al., 2025), we decouple code content from task instructions: code is rendered as images while instructions are provided in text form. This design enables us to isolate and evaluate the visual code understanding capability of MLLMs. Specifically, for code summarization, the source code to be summarized is rendered as an image, accompanied by text instruction requesting documentation generation; for code completion, the RAG-retrieved relevant code from the codebase is rendered as images while the incomplete code prefix and completion instruction remain in text; for clone detection, the two code snippets to be compared are rendered as separate images, with text instruction asking the model to classify whether they are clones; for question answering, the code context is rendered as images while the question itself and answer options are provided in text. Baseline. We establish two baselines: (1) NoCtx (No Context), where code context is removed and only the task instruction is kept to measure the lower bound and detect potential data leakage; and (2) Text, where code is provided as plain text tokens, representing the standard text-based approach. The NoCtx baseline is not applicable for Code Summarization and Clone Detection, as these tasks require source code to be summarized or compared. 3.5 Implementation Details We implement our experiments in Python using custom rendering pipeline built on Pygments (Brandl et al., 2006) for syntax tokenization and Pillow (Clark & Contributors, 2010) for image generation and processing. The base images are rendered with the default monospaced font from Visual Studio Code (Microsoft Corporation, 2024), at font size of 40 pixels as suggested by prior work (Liang et al., 2026; Zhao et al., 2025), line height of 1.0, and margin of 1% of the page width. For syntax highlighting, we adopt the Default Light theme from Visual Studio Code (Microsoft Corporation, 2024). For bold rendering, following the font synthesis definitions in W3C CSS standards (Maxfield et al., 2024) and the FreeType engine (Fre, 2024), we render each glyph multiple times with +1 pixel horizontal 8 Table 3: Overall Performance of MLLMs on Downstream Tasks with Different Inputs. Model Qwen-3-VL GLM-4.6v GPT-5-mini GPT-5.1 Gemini-2.5-Pro Gemini-3-Flash Gemini-3-Pro Code Summarization CompScore (%) Code Completion ES / EM (%) Code Clone Detection ACC / F1 (%) Code Question Answering ACC (%) NoCtx Text Image NoCtx Text 56.6 2. 55.4 2.0 57.1 0.6 56.6 0.4 54.5 0.7 55.2 1.0 56.0 1. 56.4 4.1 54.6 1.6 56.5 2.1 55.9 1.6 55.2 2.1 55.5 1. 56.8 1.5 45.0/12.8 0.4/0.6 49.7/21.6 0.2/0.9 39.9/9.5 0.8/0.3 49.8/21.0 0.4/1.2 42.6/10.7 0.8/1. 51.3/25.3 1.1/1.9 41.8/10.7 0.4/1.3 51.6/24.6 0.9/1.2 46.7/15.5 0.8/0.6 54.9/28.8 0.8/0.5 49.7/15.8 0.4/0. 55.1/26.5 0.4/0.8 50.3/16.2 0.4/0.7 55.8/27.6 0.5/0.8 Image 35.5/8.0 0.4/1.1 50.8/17.2 0.7/0.5 51.6/24.7 1.8/1.7 47.9/18.6 1.4/2.0 54.5/25.7 1.2/1.8 57.1/29.2 0.6/1.0 57.7/29.2 0.6/0.9 NoCtx Text Image NoCtx Text 67.8/52.2 0.4/0.7 81.6/78.4 0.5/1. 59.4/33.2 0.5/1.3 65.8/46.8 0.4/1.0 65.4/46.4 1.1/2.5 70.0/59.4 1.1/1.5 71.0/60.8 0.9/1.8 67.2/51.2 0.4/0.9 69.6/58.2 0.5/1.9 64.8/47.0 0.4/1.4 71.6/62.4 0.5/1. 67.0/50.6 1.2/2.6 67.8/55.4 1.2/1.4 70.2/58.8 1.0/1.6 45.6 0.2 37.2 2.5 46.3 1.9 44.7 1. 40.2 3.0 47.9 2.6 46.8 2.2 84.0 0.0 78.5 1.6 82.0 1. 79.3 1.2 82.2 1.4 73.4 0.4 74.8 1.2 Image 58.1 1.2 72.6 1.3 77.5 1.4 68.6 1.5 71.2 0.6 74.8 1.2 77.2 1.4 Green : Image outperforms Text. : p-value < 0.05 : p-value < 0.01 and vertical offsets to simulate increased stroke width. Compression is achieved through bilinear downsampling with Pillow (Clark & Contributors, 2010). For task-specific input preparation, most tasks directly use the code context with corresponding instructions according to Section 3.1. And code completion employs function-level Retrieval-Augmented Generation (RAG) (Shi et al., 2025a) using UniXcoder (Guo et al., 2022) to retrieve the top-5 most similar code snippets as context. In our experiments, all models are accessed through OpenRouter (OpenRouter, 2025), unified API gateway that provides standardized access to multiple LLM providers. During inference, we use the default sampling parameters provided by the API provider and repeat all experiments for 5 times to report the average performance along with standard deviation."
        },
        {
            "title": "4 Results and Analysis",
            "content": "In this section, we report our experimental results and answer the five research questions. 4.1 RQ1: How Effective are LLMs in Understanding Visualized Code vs. Textual Code? In this RQ, we systematically evaluate whether LLMs can effectively understand code through visual representations. To investigate this, we evaluate all seven models on the four Python tasks described in Section 3.1, comparing their performance between raw text input and code image input. We also include the No Context baseline (NoCtx) defined in Section 3.4 to rule out the possibility that models answer correctly through memorization rather than genuine code understanding. We use the Wilcoxon signed-rank test (Wilcoxon, 1945) to assess statistical significance between Text and Image inputs. The null hypothesis is that they exhibit no significant difference. The results are presented in Table 3. We highlight cells where visualized input achieves better performance than textual input. 4.1.1 Feasibility of Code Image Understanding For all four downstream tasks, LLMs with code images as input can achieve comparable or even superior performance to raw text, indicating that replacing textual code with visual representations is both viable and promising. In code summarization, the Gemini family achieves slightly higher CompScore with code images (e.g., Gemini-3-Pro: 56.0 56.8), with no statistically significant difference from textindicating that visual representations preserve high-level semantic information. In code completion, Gemini-3-Flash (55.1 57.1) and Gemini-3-Pro (55.8 57.7) achieve significantly higher ES with code images (p < 0.05). In clone detection, GPT-5-mini and GPT-5.1 show significant improvements (p < 0.01): F1 increases by 42% (33.2 47.0) and 33% (46.8 62.4), respectively. In code question answering, Gemini-3-Flash (73.4 74.8) and Gemini-3-Pro (74.8 77.2) achieve significant gains (p < 0.05). 9 Notably, Gemini-3-Pro demonstrates comparable or superior performance across all four tasks, suggesting that state-of-the-art LLMs can effectively leverage image-based code representations. One possible explanation is that visual representations enable models to perceive code structure holisticallycapturing indentation patterns, block boundaries, and longrange dependencies in single glancerather than processing tokens sequentially (Storey, 2006; Busjahn et al., 2015). Finding #1 For all four tasks, LLMs with code images can achieve comparable or even superior performance to raw text (e.g., GPT-5-mini achieves 42% F1 improvement in clone detection), demonstrating the feasibility and promise of image-based code representation."
        },
        {
            "title": "4.1.2 Model-Specific Variation",
            "content": "However, performance improvements are not uniform across models and tasks. We observe that stronger models tend to achieve better code image understanding effectiveness. The Gemini-3 family demonstrates the most consistent results across tasks. GPT-5-mini and GPT-5.1 show strong performance in clone detection, where visual inputs provide substantial improvements over text baselines. In contrast, models such as Qwen-3-VL and GLM-4.6v exhibit significant degradation (p < 0.01): Qwen-3-VLs ES in code completion drops from 49.7 to 35.5, while GLM-4.6vs clone detection accuracy decreases from 81.6 to 69.6. This variation reveals that visual code understanding is not yet uniformly developed across model families, with significant optimization potential remaining for open-weight models. To assess that models are genuinely leveraging visual code information instead of memorizing its training data, we compare against No Context baselines. For models showing improvement, image performance substantially exceeds the No Context baseline. For example, GLM-4.6v achieves 72.6% accuracy in code QA with images, far above its No Context baseline of 37.2%. This confirms that these models are extracting meaningful information from visual code representations. We also observe task-specific patterns. Clone detection shows the most pronounced visual advantage, with GPT-5-mini and GPT-5.1 achieving statistically significant improvements (p < 0.01). We attribute this to the pairwise comparison nature of the task: visual representations may help models focus on high-level semantic patterns rather than being distracted by syntactic differences in token sequences. Code summarization results show no significant differences between modalities, confirming that visual representations preserve high-level semantic information. Code completion and question answering show greater model-dependent variation, reflecting the different demands these tasks place on code image understanding. These observations suggest that current LLMs are not yet fully optimized for code image understanding. Bridging this gap remains an important direction for future research. Finding # Code image understanding effectiveness varies by model, with state-of-the-art models (Gemini-3 family) showing consistent results across tasks. This variation suggests that current LLMs are not yet fully optimized for this paradigm, and bridging this gap remains an important direction for future research. 4.2 RQ2: How Resilient are LLMs to Visual Compression Across Different Coding Tasks? Building on the feasibility established in RQ1, we further explore key advantage of visual representations, i.e., their compressibility. Specifically, we vary the compression ratios of code images from 1 to 8 and systematically evaluate LLM performance under each compression level. We apply the Wilcoxon signed-rank test (Wilcoxon, 1945) to assess whether 10 Figure 5: Performance under Varying Remaining Tokens across Different Tasks. performance under compression differs significantly from the uncompressed baseline (1), with the null hypothesis that there is no significant difference. 4.2.1 Compression Effects Across Tasks We observe that compression resilience varies across tasks, with some tasks tolerating higher compression ratios than others. In code summarization, several models maintain or even improve performance under compression. GPT-5-mini peaks at 4 compression with significant improvement (58.4 vs. 57.1 raw text, < 0.05), and Gemini-3-Pro improves from 56.0 (raw text) to 58.2 at 8. In contrast, weaker models such as Qwen-3-VL and GLM-4.6v show consistent degradation as compression increases. In clone detection, GPT-5-mini shows significant improvement with compressed images (p < 0.01): F1 increases by 75% from 33.2 (raw text) to 58.2 at 2 compression. One possible explanation is that moderate compression acts as denoising mechanism, blurring syntactic details and encouraging models to focus on semantic equivalence rather than surface-level differences. In code completion, performance varies substantially across models. The Gemini-3 family significantly outperforms raw text across all compression levels (p < 0.05; Gemini-3-Flash: ES 57.158.8 vs. 55.1 raw text), while other models show significant degradation. Notably, Qwen-3-VLs ES increases from 35.5 at 1 to 41.1 at 8. We attribute this to the models limited code image understanding capability: as compression reduces image clarity, the visual input provides less interference, and performance converges toward the no-context baseline (ES 45.0). In code question answering, the Gemini-3 family demonstrates significant improvements under compression (p < 0.05; Gemini-3-Pro: 77.2 at 1 to 79.5 at 8 vs. 74.8 raw text), while other models exhibit significant degradation at higher compression ratios. This resilience may stem from two factors: (1) modern MLLMs are trained on diverse image resolutions, developing inherent robustness to visual degradation (Bai et al., 2025); and (2) LLMs strong language priors enable them to infer missing details from partial visual signals, similar to how humans read blurred text by leveraging contextual expectations. Finding #3 Compression resilience varies by task. Code summarization and clone detection tolerate higher compression ratios, with some models maintaining performance at 48. Code completion and question answering are more sensitive, with most models showing degradation beyond 24 compression. Table 4: Impact of Visual Rendering Strategies (Plain, Bold, Highlight) on Code Understanding Tasks. Image (1x) 100% tokens Image (2x) 50% tokens Image (4x) 25% tokens Image (8x) 12.5% tokens Plain Bold Highlight Plain Bold Highlight Plain Bold Highlight Plain Bold Highlight Code Completion (ES/EM, %) Model Qwen-3-VL GLM-4.6v GPT-5-mini GPT-5.1 35.5/8.0 0.4/1.1 50.8/17.2 0.7/0.5 51.6/24.7 1.8/1. 34.5/9.2 0.7/0.9 52.3/18.0 1.0/1.3 52.5/25.2 1.1/1.0 47.9/18.6 50.1/18.3 1.4/2.0 Gemini-2.5-Pro 54.5/25.7 Gemini-3-Flash 57.1/29.2 1.2/1. Gemini-3-Pro 0.6/1.0 57.7/29.2 0.6/0.9 1.4/1.4 55.2/27.1 0.8/1.6 58.2/28.6 0.4/1.1 58.3/29.4 0.5/0.8 56.4 4.1 Qwen-3-VL 54.6 1.6 GLM-4.6v 56.5 2.1 GPT-5-mini 55.9 1.6 GPT-5.1 Gemini-2.5-Pro 55.2 2.1 Gemini-3-Flash 55.5 1.8 56.8 1.5 Gemini-3-Pro 55.2 2.0 53.5 1.8 56.8 1.5 56.2 1.2 54.8 1.8 55.8 1.2 57.4 1.2 Qwen-3-VL GLM-4.6v GPT-5-mini GPT-5.1 67.2/51.2 0.4/0.9 69.6/58.2 0.5/1.9 64.8/47.0 0.4/1.4 71.6/62.4 0.5/1.2 Gemini-2.5-Pro 67.0/50.6 1.2/2. 65.4/47.4 0.5/1.0 68.4/55.0 1.4/3.3 64.4/46.4 1.6/3.1 68.2/55.6 1.7/3.8 64.8/46.0 1.3/2.6 Gemini-3-Flash 67.8/55.4 68.6/57.8 Gemini-3-Pro 1.2/1.4 70.2/58.8 1.0/1.6 0.5/1.5 70.8/59.5 0.9/1.5 57.4/28.3 58.7/29.8 0.4/1.0 58.1/29.4 0.5/0.8 0.5/0.7 58.7/29.9 0.5/0.8 36.2/10.5 0.7/0.9 35.7/9.2 1.0/0.8 53.2/20.7 46.3/11.8 1.3/1.9 49.1/21.9 1.6/1.5 48.0/19.7 1.7/1.3 53.9/26.7 0.8/1.3 58.3/29.7 0.2/0.8 58.4/29.6 0.4/0. 56.0 2.2 52.9 1.6 57.4 0.9 55.5 1.0 56.0 0.8 56.2 1.0 57.2 1.4 67.0/50.6 0.6/1.6 69.2/56.0 0.7/1.7 64.0/45.2 1.1/3.0 71.0/61.6 1.1/2.4 65.4/47.4 1.6/3.2 67.8/55.6 0.7/0.5 71.2/60.0 1.1/1.9 0.4/0.9 45.6/16.7 1.0/1.2 46.5/13.1 1.7/1.8 56.2/27.5 0.9/1.9 54.5 2.8 54.0 2.8 57.2 0.6 55.5 1.6 54.0 2.8 55.6 2.0 57.0 1.6 68.8/56.0 0.4/1.0 66.4/51.2 1.6/3.3 69.6/58.2 1.1/2.9 69.2/58.0 0.7/1.9 65.8/48.4 1.3/2.1 69.4/60.0 0.8/2.2 71.4/60.8 1.0/2.0 36.0/9.9 0.5/0.5 47.3/12.5 0.9/0.7 47.3/17.3 1.8/1.8 47.0/14.5 0.6/0.6 56.1/27.7 0.8/1. 33.8/8.2 0.6/0.7 38.5/9.7 0.8/0.7 47.7/13.6 44.1/10.5 0.5/0.6 47.1/17.7 1.3/1.3 0.7/1.2 44.4/14.1 1.7/1.7 47.4/16.0 46.6/13. 1.0/0.8 54.1/25.9 1.2/1.9 58.3/28.6 0.6/0.7 58.6/29.7 0.5/0.7 0.8/1.8 54.8/24.7 1.6/1.3 58.8/27.8 0.5/0.7 58.5/29.0 0.5/0.8 Code Summarization (CompScore, %) 54.0 2.5 53.2 2.2 57.5 1.2 55.8 1.4 54.5 2.2 56.0 1.5 57.8 1.3 54.8 2.4 53.5 1.9 58.0 1.1 55.2 1.4 56.0 1.9 56.5 1.5 57.6 1.5 52.0 1.8 53.3 1.3 58.4 0.2 56.1 1.2 55.3 1.3 56.0 1.4 57.6 1. Code Clone Detection (ACC / F1, %) 38.8/10.1 0.3/0.4 44.1/10.2 0.8/1.2 43.7/12.8 0.5/0.9 46.8/12.1 0.9/1.2 55.6/25.0 1.4/2.3 58.0/27.8 0.3/1.0 58.3/29.1 0.6/0.8 51.5 2.0 52.5 2.0 58.2 1.5 56.4 1.0 55.8 1.6 56.4 1.2 58.5 1.5 37.7/10.7 0.5/0.8 46.0/12.0 0.6/0.6 44.2/14.9 0.8/0.7 47.4/16.0 0.2/1.3 55.5/26.4 1.6/2.0 58.5/28.6 0.9/1.0 58.5/29.2 0.5/0.7 41.1/12.8 0.4/0.8 42.9/8.9 0.3/0.2 43.3/12.3 1.0/1.4 45.5/13.1 0.3/0.4 53.5/22.4 1.5/1.2 58.3/27.7 0.7/0.7 58.0/28.3 0.6/0.9 52.2 1.9 52.8 2.3 58.8 2.0 56.5 0.5 57.2 2.3 57.0 1.8 58.2 1. 51.2 2.2 52.5 1.2 58.0 1.2 55.9 0.8 56.0 1.2 56.5 1.6 58.2 1.4 41.1/12.4 0.6/0.5 42.8/8.9 0.6/0.8 42.6/11.6 1.2/1.7 45.0/12.8 1.1/0.9 54.5/22.0 0.8/1.4 58.1/27.4 0.2/0.7 57.9/28.2 0.6/0.9 50.8 2.5 51.8 1.5 57.6 1.8 55.5 1.2 55.5 1.5 56.8 1.4 59.0 1.6 68.0/54.6 0.6/1.4 63.2/42.8 1.9/4.3 69.4/57.8 1.0/2.1 69.6/58.8 0.5/1.0 67.0/51.8 1.3/2.9 68.6/58.8 0.5/1.5 71.8/61.2 1.2/1.8 66.6/51.6 0.5/0.7 66.4/51.2 1.7/3.4 69.0/56.4 1.7/4.0 69.2/58.0 1.3/2.3 60.2/33.8 0.7/1.6 70.8/61.6 1.3/2.3 67.8/54.8 1.8/3. 60.8/34.6 0.8/1.4 69.8/67.0 2.5/3.3 68.4/58.4 1.0/1.5 69.4/58.2 72.4/64.6 59.2/31.8 0.7/1.5 69.4/59.4 1.5/2.8 68.8/56.8 1.5/3.1 69.0/57.8 0.6/1.2 59.7/32.8 0.5/1.2 75.4/71.4 0.8/0.5 63.2/45.6 2.9/6.9 68.8/56.8 1.3/2.3 1.0/2.3 1.4/2.4 68.2/54.0 64.6/44.8 67.2/52.0 66.2/49.8 68.0/52.8 2.3/4.0 68.2/57.8 1.2/2.1 71.2/60.2 1.0/1. 71.8/61.2 72.0/61.5 72.0/61.6 70.8/59.6 0.9/2.2 67.8/56.0 1.2/2.6 1.7/2.1 67.6/55.4 0.5/1.7 1.0/1.4 68.4/57.0 0.5/1.4 1.1/1. 1.4/3.3 1.2/2.1 0.8/1.7 1.3/2.1 1.0/1.7 60.4/33.6 0.6/1.2 57.8/62.8 1.9/1.3 60.6/52.0 2.1/3.0 71.2/61.8 1.5/2.3 67.4/53.4 1.6/3. 1.1/1.4 72.2/62.0 1.1/1.9 69.6/59.0 72.0/65.4 41.0/11.9 0.3/0.7 44.0/10.2 0.6/1.4 44.1/13.5 1.0/1.1 46.3/13.8 1.6/1.0 55.3/23.7 1.7/1.0 57.8/25.3 0.4/0.4 58.0/28.4 0.5/0.8 51.5 2.1 52.0 1.3 57.1 2.7 55.3 1.4 55.7 1.3 57.2 2.0 58.0 1.8 60.8/35.4 0.7/2.2 71.4/66.2 2.1/3.5 64.0/47.0 3.0/6.8 69.2/57.4 1.3/2.4 67.4/52.0 1.2/2.3 70.2/60.8 0.7/1.2 71.5/60.8 1.4/2.3 58.1 1.2 Qwen-3-VL 72.6 1.3 GLM-4.6v 77.5 1.4 GPT-5-mini 68.6 1.5 GPT-5.1 Gemini-2.5-Pro 71.2 0.6 Gemini-3-Flash 74.8 1.2 77.2 1.4 Gemini-3-Pro 58.5 1.0 71.5 1.5 76.2 0.8 69.0 0.5 70.7 1.2 76.8 1.4 78.0 1.2 58.5 1.2 72.2 1.3 76.4 1.3 68.1 1.4 71.7 0.9 76.7 1.0 78.2 1.5 49.2 0.6 63.0 2.7 74.3 1.5 61.9 1.5 69.6 1.2 74.2 1.0 77.8 1.3 48.4 1.3 61.2 2.2 67.9 1.7 58.7 1.4 68.3 2.4 77.3 1.0 78.6 1.1 49.7 1.0 61.9 1.7 75.1 1.4 61.7 1.8 68.1 2.9 76.4 1.2 78.8 1.4 51.8 1.0 43.3 2.5 56.8 2.0 63.5 1.7 69.8 1.3 75.6 0.9 78.4 1. 51.0 0.9 42.0 1.8 50.3 2.1 64.2 2.8 66.9 1.4 76.3 0.7 78.8 1.3 Code Question Answering (ACC, %) Green : Bold/Highlight outperforms Plain. 50.6 0.5 43.7 1.4 57.9 2.0 63.9 0.9 70.4 0.4 77.2 2.5 78.6 1.5 49.8 0.7 39.7 1.5 51.6 1.6 63.9 1.2 70.3 1.7 77.8 0.9 79.5 1.0 : p-value < 0.05 51.3 0.7 42.9 1.2 52.5 2.9 63.9 1.9 68.1 0.7 76.3 1.2 78.2 1. 45.8 1.0 38.5 1.2 47.5 1.7 57.2 1.4 63.6 2.0 75.2 0.8 79.2 1.2 : p-value < 0.01 4.2.2 Compression Effects Across Models Model capability strongly influences compression resilience. Across all four tasks, the Gemini-3 family (Gemini-3-Flash and Gemini-3-Pro) demonstrates remarkable compression resilience, with no significant degradation and even significant improvements in code completion and question answering at 8. In code completion, Gemini-3-Pro achieves ES 58.0 at 8 compared to 55.8 with raw text. In code question answering, Gemini-3-Pro reaches 79.5% accuracy at 8 versus 74.8% with raw text. In contrast, models with weaker visual understanding capabilities show more pronounced degradation. GLM-4.6vs accuracy in code question answering drops significantly from 72.6% at 1 to 39.7% at 8 (p < 0.01). GPT-5-mini and GPT-5.1 exhibit moderate resilience in some tasks but inconsistent performance in others. These observations suggest that compression resilience correlates with overall model capability in code image understanding, and that state-of-the-art models are better equipped to handle compressed visual inputs. Finding # Compression resilience also varies by model. Gemini-3-Pro maintains or improves performance at 8 compression (only 12.5% of text tokens), while models with weaker visual capabilities show pronounced degradation at higher ratios. 12 4.3 RQ3: Can Visual Enhancements Improve Code Image Understanding? In RQ1RQ2, we used plain rendering (the default configuration). Here, we investigate whether enhanced rendering strategiesbold and syntax highlightingcan further improve model performance. To assess statistical significance, we apply the Wilcoxon signed-rank test (Wilcoxon, 1945) to compare Plain rendering against Bold and Highlight variants, with the null hypothesis that there is no significant difference. The results are presented in Table 4."
        },
        {
            "title": "4.3.1 Enhancement Effectiveness at Low-to-Moderate Compression",
            "content": "Visual enhancementsincluding syntax highlighting and bold renderingcan significantly improve LLMs code image understanding, particularly at compression ratios of 14 where the visual signal remains legible. In code completion, both bold rendering and syntax highlighting provide significant improvements. At 1 compression, GLM-4.6v improves significantly from ES 50.8 (plain) to 53.2 with highlighting (p < 0.01), and GPT-5.1 benefits significantly from bold rendering (ES: 50.1 vs. 47.9, < 0.01). The Gemini family demonstrates particularly strong responsiveness: Gemini-3-Flash achieves significant improvements across both strategies at 12 compression (p < 0.05). In clone detection, GPT-5.1 shows significant improvement with bold rendering at 4 compression (F1: 64.6 vs. 58.2, +11%, < 0.01), and Gemini-2.5-Pro benefits significantly from both strategies at moderate compression levels. For code question answering, Gemini-3-Flash achieves 76.8% accuracy with bold rendering at 1 compression, compared to 74.8% with plain rendering (p < 0.01). Finding #5 Visual enhancements are most effective at 14 compression, with state-of-the-art models like the Gemini family showing consistent improvements of 25% across tasks. 4.3.2 Diminishing Returns at High Compression At 8 compression, visual enhancements generally offer limited additional benefit, as reduced resolution obscures the visual distinctions they introduce. However, some modeltask combinations still show significant improvements: Gemini-3-Pro with bold rendering in code summarization (p < 0.01) and Gemini-3-Flash with bold rendering in clone detection (p < 0.01). Bold rendering can even introduce slight degradation at extreme compression for some models, as thicker strokes may reduce character distinguishability. The varying effectiveness across models suggests that visual enhancement responsiveness depends on model-specific factors, representing an optimization opportunity for future work. These findings suggest that enhancement strategies should be adapted to compression level meaningful at moderate compression, but potentially unnecessary at high compression ratios. Finding #6 At 8 compression, visual enhancements provide diminishing returns. Adaptive rendering strategies that adjust enhancement based on target compression ratio represent promising direction for future optimization. 4.4 RQ4: Can Code Image Understanding Generalize to Other Languages? To validate the generalizability of our findings from RQ1RQ3 beyond Python, we extend key experiments to Javaa language with fundamentally different syntactic characteristics (explicit braces vs. whitespace indentation). We evaluate code completion and clone detection using Java benchmarks detailed in Section 3.1. Following the same statistical 13 Table 5: Performance on Java Code Across Compression Ratios and Rendering Strategies. Model Baseline Image 1x 100% tokens Image 2x 50% tokens Image 4x 25% tokens Image 8x 12.5% tokens Text NoCtx Plain Bold Highlight Plain Bold Highlight Plain Bold Highlight Plain Bold Highlight Qwen-3-VL 50.7/23.0 49.0/15. GLM-4.6v 0.5/0.7 0.3/0.2 52.3/24.6 42.6/9.0 0.8/1.0 0.8/1.4 GPT-5-mini 54.7/28.4 46.0/14. GPT-5.1 2.1/1.8 0.9/0.7 54.3/29.9 44.3/14.5 35.4/9.1 0.5/0.4 47.9/18.0 1.1/1.1 54.5/26.7 1.4/1.5 53.8/24.6 0.7/1.0 1.1/1. 0.7/0.5 Gemini-2.5-Pro 58.7/33.6 52.2/20.4 63.3/34.3 Gemini-3-Flash 56.1/28.7 53.0/18.8 62.7/36.1 0.4/1.6 0.4/0.9 0.7/1. 0.6/0.6 0.3/0.4 1.1/1.5 35.6/8.3 0.8/0.6 48.8/18.5 1.1/0.7 53.5/26.2 1.2/0.8 54.2/24.5 1.5/2.3 62.9/33.1 0.4/1.6 62.5/37.1 0.1/0.4 38.1/11.6 0.6/0.9 49.5/19.0 0.8/1.3 54.4/26.4 0.7/0.8 54.0/24.3 0.5/1.0 62.7/35.3 0.4/0.2 63.2/38.0 0.2/0.3 Code Completion (ES / EM, %) 39.7/12.4 0.2/0.7 44.6/13.9 1.4/1.3 51.3/20.4 0.9/1.1 53.0/20.9 1.4/1.9 63.7/32.5 0.7/1. 39.0/10.8 0.6/0.2 44.3/13.8 0.8/1.1 52.5/22.7 0.7/0.9 51.6/18.4 0.9/1.8 63.4/32.5 1.4/1.4 38.7/11.7 0.5/0.5 43.2/12.6 2.0/1.6 51.7/20.8 1.4/2.0 53.3/21.1 1.1/1.6 63.3/33.2 0.4/1.4 42.3/13.2 0.5/0.7 42.2/11.2 1.5/1.2 48.9/17.3 0.5/0.9 50.6/19.4 0.9/0.7 64.0/33.0 1.2/1.1 60.7/34.3 61.5/33.9 62.3/36.9 63.2/35.5 0.4/0.7 0.5/0. 0.7/1.1 0.5/0.8 41.2/12.0 0.2/0.6 41.7/10.5 0.8/0.5 48.4/17.3 1.4/1.2 50.8/18.0 0.7/1.1 62.2/31.0 0.9/0.4 62.9/35.1 0.8/0.5 43.0/12.3 0.4/0.2 42.8/11.4 1.0/1.2 48.7/16.5 1.0/1.3 51.8/18.5 1.1/1.0 64.4/33.9 1.1/2.0 63.5/37.2 0.4/0.8 45.9/13.6 0.6/0.6 42.0/9.5 0.4/0.8 48.6/16.2 0.7/1.1 49.4/17.7 1.4/1.4 59.7/27.5 1.0/0.8 62.3/32.9 0.3/0.7 45.1/13.3 0.7/0. 45.5/13.1 0.4/0.4 42.5/10.7 42.3/10.3 1.1/1.0 48.2/17.0 0.6/1.7 50.0/18.0 1.0/1.1 59.5/27.5 0.7/1.1 61.2/31.1 0.5/0.4 0.5/1.0 48.8/16.1 1.5/1.5 49.6/15.9 1.2/0.5 60.2/28.8 0.5/1.1 61.7/32.0 0.6/0.3 Gemini-3-Pro 57.4/37.4 51.9/24.3 63.5/44.7 65.1/44.7 64.9/43.2 66.8/46.6 68.2/45.9 68.0/44.5 67.2/46.2 68.8/44.1 68.4/44.2 64.4/39.4 66.2/40.5 65.9/39. 0.9/1.4 1.0/0.2 0.5/0.8 0.6/1.0 1.3/0.9 0.8/1. 0.9/0.9 1.4/1.7 1.0/1.6 1.1/0.7 0.8/1.0 0.9/1. 1.1/0.5 1.0/1.2 Code Clone Detection (ACC / F1, %) Qwen-3-VL GLM-4.6v GPT-5-mini GPT-5.1 Gemini-2.5-Pro Gemini-3-Flash Gemini-3-Pro 56.6/24.2 0.5/1.0 72.2/63.6 1.2/1.5 66.5/51.8 0.5/1.2 61.0/40.0 0.6/1.9 64.4/46.0 1.4/3.4 62.6/50.0 1.4/1.1 65.2/52.4 0.5/1.4 / / / / / / / 60.4/38.2 0.5/0.7 70.6/59.6 0.5/1.4 69.3/57.8 0.9/1.9 67.2/51.2 1.6/3.4 64.6/44.8 0.5/1.2 68.6/57.2 0.5/0.4 64.6/51.4 0.8/1.6 59.8/36.6 0.7/1.5 70.4/59.2 1.5/2.8 69.0/57.2 0.9/2.0 67.0/50.8 1.7/3.5 64.4/44.4 0.8/2.2 68.4/56.8 0.8/1.7 65.4/52.6 1.0/1.8 60.8/38.8 0.7/1.2 69.6/57.8 1.4/2.0 69.4/57.6 0.5/1.6 66.6/50.2 0.8/1.6 61.0/37.6 0.9/2.6 69.0/58.0 0.6/1.3 65.8/53.2 0.9/1.5 66.6/50.8 0.5/0.7 69.2/57.4 1.0/2.2 69.7/57.2 1.6/3.5 64.8/46.2 0.7/1.5 60.6/37.0 0.8/1.9 68.4/57.4 0.8/0.8 65.0/52.0 0.7/1.4 66.8/51.2 0.7/1.5 68.4/55.6 1.6/3.7 70.8/61.0 1.2/2.6 65.4/47.2 1.0/2.6 62.2/40.8 1.5/3.8 68.6/57.2 0.8/1.2 65.6/53.2 1.1/2.0 65.4/47.8 1.5/2.8 69.8/58.8 1.0/2.5 70.2/59.2 1.0/2.5 65.8/49.4 0.4/1.2 60.4/36.0 1.2/3.3 69.2/59.0 0.4/1.3 66.0/53.8 0.8/1. 64.8/47.4 1.0/2.1 70.2/61.2 1.0/1.5 68.8/56.3 1.6/3.9 66.8/51.8 0.7/1.3 63.4/43.0 2.2/4.6 68.8/58.2 0.4/0.7 64.8/51.8 0.9/1.7 65.6/49.4 0.8/1.6 68.6/59.8 1.5/2.7 71.6/62.4 1.4/2.0 67.6/53.2 1.0/2.0 63.2/42.0 1.0/1.8 68.6/57.4 0.8/1.4 66.2/54.0 1.2/1.9 64.6/46.4 0.5/1.6 70.2/61.6 1.3/3.9 70.8/60.2 1.9/4.0 67.8/54.4 1.2/2.2 64.0/44.0 0.6/1.4 68.4/57.8 0.5/1.0 66.6/54.6 1.0/1.8 67.8/53.0 0.7/1.4 67.6/69.2 2.1/2.6 72.0/63.7 3.6/5.9 65.0/47.0 1.5/2.8 64.6/44.8 1.0/2.3 68.8/57.0 0.4/1.1 65.4/52.4 1.1/2.0 68.0/54.4 0.6/1.0 67.2/71.2 0.7/1.3 69.6/61.8 2.3/3.5 64.8/46.8 0.7/1.5 63.8/44.0 1.2/2.4 69.2/58.0 0.4/1.5 65.8/53.6 0.9/1.7 67.6/53.0 0.8/1.5 66.4/68.2 1.7/1.6 70.0/61.0 1.7/2.9 63.6/43.8 1.0/1.7 65.0/46.6 1.1/2.1 68.6/57.2 0.5/1.6 66.4/54.4 1.0/1. Green : Image outperforms Text, or Bold/Highlight outperforms Plain. : p-value < 0.05 : p-value < 0.01 testing protocol as RQ1RQ3, we apply the Wilcoxon signed-rank test (Wilcoxon, 1945) to assess significance. The results are presented in Table 5. The fundamental patterns observed in Python experiments replicate consistently in Java. In code completion, the Gemini family significantly outperforms raw text across all compression levels (p < 0.01), demonstrating strong visual code understanding. In clone detection, visual inputs provide significant improvements across multiple models (p < 0.01). Qwen-3-VL shows particularly large gains under compression (F1: 24.2 53.0 at 8, +119%, < 0.01), suggesting that compression may blur syntactic details and encourage the model to focus on higher-level semantic patterns rather than surface-level differences. The compression resilience patterns also hold: models that performed well under compression in Python maintain their relative advantages in Java. Finding # Core findings generalize from Python to Java. The patterns of code image understandingincluding model-specific strengths and compression resilienceremain consistent across languages."
        },
        {
            "title": "4.5 RQ5: How Does Visual Compression Degrade the Information in Code?",
            "content": "RQ1RQ4 revealed that compression affects different tasks differentlysummarization and clone detection remain resilient while code completion shows more variation. This raises key question: how does compression degrade the information in code, and why does this impact tasks differently? To answer this, we design code reconstruction experiment that directly measures information preservation, where models are instructed to transcribe the code from compressed code images. The errors between the original code and reconstructed code thereby reveal what visual information is lost. To ensure uncontaminated evaluation, we utilize the GitHub REST API (GitHub, 2025) to fetch fresh Python repositories created strictly after August 1, 2025 (after the knowledge cutoff of all studied models), filtering for repositories with 10+ stars and file lengths between 50120 lines to ensure code quality while maintaining suitability for code image generation. To ensure diversity, we selected top 100 repositories (excluding those used in Code Question Answering) and randomly selected one code snippet per repository that meets the target length criteria, resulting in 100 code snippets with an average length of 473.1 tokens. We evaluate all seven models across four compression ratios (1, 2, 4, 8), using strict OCR prompt: Transcribe the code in the image exactly. Reconstruction quality is measured using 14 Figure 6: Code Reconstruction Performance across Different Remaining Token Ratios. Character Error Rate (CER) (Thennal et al., 2025), CodeBLEU (Ren et al., 2020), and Exact Match (EM). We further categorize errors using rule-based three-level taxonomy: Token Error (non whitespace tokens that differ from the ground truth), Line Error (a line where 50% of tokens differ), and Block Error (three or more consecutive Line Errors). We quantify these errors by measuring their Prevalence: the percentage of samples containing at least one instance of specific error type. The results are presented in Figure 6. 4.5.1 Visual Information Loss Patterns Compression degrades code reconstruction quality in predictable patterns, with model capability determining resilience. At 1 compression, Gemini-3-Pro achieves the highest Exact Match and lowest CER, followed by Gemini-3-Flash and GPT-5.1. Under compression, we observe two distinct patterns. The Gemini-3 family demonstrates graceful degradation, maintaining high CodeBLEU even at 8 compressionlikely due to training objectives that emphasize visual document understanding (Google DeepMind, 2025b). Other models exhibit performance cliff pattern, maintaining reasonable accuracy until 4 compression before rapid decline at 8. These reconstruction quality differences directly predict downstream task performance: models with graceful degradation (Gemini3) excel across all tasks in RQ1RQ4, while models with performance cliffs show taskdependent results. 4.5.2 Error Type Analysis The results reveal clear degradation hierarchy. Token Errors emerge firsteven at 1 compression, most models show token errors (e.g., confusing 1 vs l, 0 vs O, missing punctuation), with error rates increasing under compression. Line Errors remain relatively stable from 1 to 4, then surge dramatically at 8 for most models. Block Errors dominate at aggressive compression for weaker models, indicating that they begin hallucinating code rather than transcribing. Notably, the Gemini-3 family maintains low block error rates even at 8 compression, which directly explains their consistent performance on downstream tasks across all compression levels observed in RQ1RQ4. The error hierarchy directly explains the task-dependent patterns observed in RQ1RQ4. First, some downstream tasks do not require perfect reconstruction: even when Token Error prevalence is high, models can still achieve competitive performance on summarization 15 and clone detection, because these tasks rely on high-level semantic patterns rather than character-level precision. This explains the apparent paradox where models with substantial reconstruction errors still perform well on downstream tasksthe OCR task demands exact transcription, while downstream tasks only require sufficient semantic understanding. Second, detail-sensitive tasks degrade with error accumulation: code completion and question answering benefit from low error rates, explaining why models with graceful degradation (Gemini-3) excel while others show more variation. This causal linkfrom visual information loss patterns to downstream task performancevalidates our reconstruction analysis as diagnostic tool for understanding code image understanding capabilities. Finding #8 Information degradation follows predictable hierarchy: Token Errors emerge first (1 2), Line Errors at moderate compression (24), and Block Errors at high compression (48) for most models. However, the Gemini-3 family maintains low block error rates even at 8 compression, explaining their stable downstream performance across all compression levels."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Inference Latency key question for practical deployment is whether visual code processing introduces prohibitive latency overhead compared to text-based approaches. While commercial API providers typically charge the same rate for visual and text tokens (OpenAI, 2025a; Google, 2025), the actual computational cost may differ due to the additional visual encoder and alignment stages (Figure 2). Since API latency is heavily influenced by network conditions and server load, we benchmark locally on two open-weight MLLMs: Qwen-3-VL (235B)2 and GLM-4.6v (108B)3, measuring Time to First Token (TTFT)the latency from input submission to the first generated token, encompassing both prefill and initial decoding (Agrawal et al., 2024), which is widely used measure that reflects the perceived responsiveness for interactive developer tools (Agarwal et al., 2023; Zhong et al., 2024; Agrawal et al., 2025). Experiments are conducted on machine with 8NVIDIA A100-80G GPUs, dual-socket AMD EPYC 7763 CPUs (128 cores), and 1.8TB system memory. We use PyTorch (Paszke et al., 2019) with the HuggingFace Transformers library (Wolf et al., 2020) for inference. Each measurement consists of 2 warmup iterations followed by 10 timed iterations, with the average execution time reported. Figure 7: Time-to-First-Token (TTFT) Comparison: Text vs. Image Inputs As shown in Figure 7, the latency curves for images and text are comparable at identical token scales, indicating that visual encoding introduces minimal overhead. With per-token latency parity established, the 24 compression ratios demonstrated in RQ23 directly translate to equivalent inference speedupprocessing 4 compressed images is approximately 4 faster than processing raw text. Interestingly, Qwen-3-VL shows slightly lower latency for images than text at small token counts (< 29), likely due to the parallel processing efficiency of vision encoders on small inputs compared to sequential text tokenization overhead (Wolf et al., 2020). These results suggest that code image understanding is not only viable in terms of task performance but also practically deployable without latency penalties, paving the way for vision-first code intelligence systems. 2https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct 3https://huggingface.co/zai-org/GLM-4.6V 16 5.2 Threats to Validity Internal Validity. The primary internal threat is data contaminationcommercial MLLMs may have seen benchmark data during pre-training. We address this through multiple strategies: (1) No Context baselines that remove retrieved context while preserving taskspecific inputs, allowing us to isolate the contribution of visual context; (2) constructing our CodeQA dataset exclusively from GitHub repositories created after August 2025, well beyond model training cutoffs; and (3) crawling 100 fresh repositories for RQ5s code reconstruction experiments (Section 3.1). To ensure annotation quality, three independent researchers validated each CodeQA question-answer pair, with unanimous agreement required for inclusion; samples with any disagreement were discarded. For statistical reliability, we repeat all experiments five times and apply Wilcoxon signed-rank tests to assess significance (Section 3.5). External Validity. For programming language coverage, while our primary experiments focus on Python, we replicate key findings on Java in RQ4, demonstrating consistent performance patterns across languages with different syntax characteristics. For model coverage, we evaluate seven representative MLLMs from diverse familiesincluding open-weight models (Qwen-3-VL, GLM-4.6v) and proprietary systems (GPT-5-mini, GPT-5.1, Gemini-2.5-Pro, Gemini-3-Flash, Gemini-3-Pro)to capture the spectrum of current capabilities (Section 3.2). For rendering configuration, rather than exploring exhaustive visual parameters, we adopt VSCodes default syntax highlighting themea widely-used IDE configurationto maximize practical relevance (Section 3.3)."
        },
        {
            "title": "6 CODEOCR: Code Transformation Tool",
            "content": "Our experiments reveal that visual code representation offers promising paradigm for MLLM-based code understanding, achieving comparable or improved performance at significant compression ratios. Building on these findings, we developed CODEOCR, practical middleware for rendering source code into images with configurable visual enhancements and compression ratios. Workflow. As illustrated in Figure 8, users provide code and instructions as input; CODEOCR renders the code into compact image, which is then passed to the MLLM along with the instructions, and the output is returned to the user. Internally, the transformation comprises two stages: (1) Visual Rendering converts source code into syntax-highlighted images, and (2) Dynamic Compression adjusts resolution to achieve target compression ratios based on user-specified token budgets. The tool leverages Pygments (Brandl et al., 2006) for syntax analysis and Pillow (Clark & Contributors, 2010) for image rendering, currently supporting six languages (Python, Java, JavaScript, C/C++, Go, TypeScript) with native extensibility to 500+ languages via Pygments lexer ecosystem. Figure 8: CodeOCR Workflow Usage Scenarios. CODEOCR serves as an efficient middleware for both LLM service providers and end-users. By converting code into compact images, it significantly reduces the computational overhead and financial costs associated with API usage. This transformation is applicable to code of any scalefrom individual functions to entire projectsenabling users to trade visual fidelity for token savings based on their specific needs. Performance Testing. We evaluated the middlewares efficiency using over 1,000 samples across four benchmarks. Performance tests demonstrate that CODEOCR achieves high transformation throughput of 6.9k token/s, making it sufficiently fast for real-time applications or on-the-fly processing in IDE plugins. We further validated the tools reliability by confirming 100% consistency in token estimation and compression ratio accuracy across repeated runs."
        },
        {
            "title": "7 Related Work",
            "content": "Large Language Models for Code. Recent years have witnessed rapid advancement in LLMs for code (Fan et al., 2023; Jiang et al., 2024; Zhang et al., 2024; Wang et al., 2025b). Starting from Codex (Chen et al., 2021), series of code LLMs including Code Llama (Rozi`ere et al., 2023), StarCoder (Lozhkov et al., 2024), DeepSeek-Coder (Guo et al., 2024; Zhu et al., 2024), and Qwen2.5-Coder (Hui et al., 2024) have achieved strong performance across diverse tasks such as code generation (Zhuo et al., 2025; Hu et al., 2026; Zeng et al., 2026), repair (Muennighoff et al., 2023; Shi et al., 2024; Li et al., 2025a; Chen et al., 2025b;c; Chen & Jiang, 2025; Wang et al., 2026b), translation (Khan et al., 2024; Ahmad et al., 2023; Wang et al., 2025a), and reasoning (Gu et al., 2024; Zeng et al., 2025; Peng et al., 2025). While these models have achieved remarkable success, they process code as linear token sequences, facing scalability challenges as context length grows (Guo et al., 2023; Bogomolov et al., 2024). Text-based compression methods (Zhang et al., 2022; Wang et al., 2024b; Shi et al., 2025a; Yang et al., 2025a; Pan et al., 2025; Sun et al., 2024) alleviate this through selective token retention, i.e., each token is either kept or dropped. This inevitably leads to certain degree of information loss, and those kept key tokens cannot be further compressed (Wang et al., 2026b; Shi et al., 2025a; Sun et al., 2025). Our work explores complementary paradigm: representing code as images enables continuous compression via resolution scaling rather than discrete selection. Empirically, we find MLLMs achieve comparable performance at up to 8 compression, highlighting visual representation of code as promising research direction. Visual Document Understanding. OCR and visual document understanding have evolved from traditional digitization (Alaei et al., 2023; Smith, 2007; Jaderberg et al., 2014; Baek et al., 2019; Long et al., 2020; Katti et al., 2018; Xu et al., 2020; Rausch et al., 2021) to endto-end neural approaches. Early systems like TrOCR (Li et al., 2022) and Nougat (Blecher et al., 2023) demonstrated direct transcription without separate detection stages, while GOTOCR2.0 (Wei et al., 2024) enhanced structure recovery for charts and tables. General-purpose MLLMs (OpenAI, 2023; Gemini Team et al., 2023; Liu et al., 2023; Chen et al., 2024; Kim et al., 2022; Lee et al., 2023; Bai et al., 2023) have advanced high-resolution visual understanding, with specialized models for document comprehension (Wang et al., 2024a; Hu et al., 2024) and GUI understanding (You et al., 2024; Baechler et al., 2024). DeepSeek-OCR (Wei et al., 2025) introduced optical compression for documents, achieving up to 20 ratios. However, these works focus on natural documents or UI screenshots, where visual layouts are loosely structured. Code presents unique challenges with dense symbolic content and strict syntactic constraints (Buse & Weimer, 2010; Storey, 2006). Our study systematically evaluates how MLLMs handle code-specific visual fidelity under compressionrevealing task-dependent resilience patterns not explored in prior OCR research."
        },
        {
            "title": "8 Conclusion and Future Directions",
            "content": "This paper presents the first comprehensive empirical study exploring visual code representation as new paradigm for code understanding. Through systematic evaluation of state-of-the-art MLLMs across four representative tasks, we provide empirical evidence that this paradigm is both viable and practically beneficial. Our findings offer actionable insights for future research and practice. First, we observe that image compression can achieve competitive or even superior performance while using only 25% or fewer tokensthis suggests that for practitioners, visual representation can substantially reduce API costs without sacrificing quality, and motivates the design of code-specific compression techniques. Second, we find that syntax highlighting improves model robustness, indicating opportunities for task-adaptive rendering strategies. Third, we identify significant OCR capability gaps across models, pointing to the need for code-specific visual pre-training. These findings establish visual code representation as promising research direction and motivate future work on task-adaptive rendering, aggressive compression techniques, and code-specialized multimodal models."
        },
        {
            "title": "References",
            "content": "Megha Agarwal, Asfandyar Qureshi, Nikhil Sardana, Linden Li, Julian Quevedo, Best pracURL https://www.databricks.com/blog/ and Daya Khudia. tices. llm-inference-performance-engineering-best-practices. Accessed: 2025-01-24. Llm inference performance engineering: Databricks Blog, 2023. Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, and Alexey Tumanov. Metron: Holistic performance evaluation framework for llm inference systems. arXiv preprint arXiv:2407.07000, 2024. Amey Agrawal, Nitin Kedia, Anmol Agarwal, Jayashree Mohan, Nipun Kwatra, Souvik Kundu, Ramachandran Ramjee, and Alexey Tumanov. On evaluating performance of llm inference serving systems, 2025. URL https://arxiv.org/abs/2507.09019. Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. In Findings of the AVATAR: parallel corpus for java-python program translation. Association for Computational Linguistics: ACL 2023, pp. 22682281, Toronto, Canada, 2023. Association for Computational Linguistics. Alireza Alaei, Vinh Bui, David Doermann, and Umapada Pal. Document image quality assessment: survey. ACM Comput. Surv., 56(2), September 2023. ISSN 0360-0300. doi: 10.1145/3606692. URL https://doi.org/10.1145/3606692. Ajmain Inqiad Alam, Palash Ranjan Roy, Farouq Al-omari, Chanchal Kumar Roy, Banani Roy, and Kevin Schneider. Gptclonebench: comprehensive benchmark of semantic clones and cross-language clones using gpt-3 model and semanticclonebench. In Proceedings of the 39th International Conference on Software Maintenance and Evolution (ICSME), pp. 112, Bogota, Colombia, 2023. IEEE. Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. Screenai: vision-language model for ui and visually-situated language understanding, 2024. URL https://arxiv.org/abs/2402.04615. Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis, 2019. URL https://arxiv.org/abs/1904. 01906. Shuai Bai, Jinze Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. Shuai Bai, Yuxuan Cai, Ruizhe Chen, et al. Qwen3-vl technical report, 2025. URL https: //arxiv.org/abs/2511.21631. Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents, 2023. Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, and Timofey Bryksin. Long code arena: set of benchmarks for long-context code models, 2024. Georg Brandl et al. Pygments: Python syntax highlighter. https://pygments.org/, 2006. Accessed: 2025-01-01. Raymond P.L. Buse and Westley R. Weimer. Learning metric for code readability. IEEE Transactions on Software Engineering, 36(4):546558, 2010. Teresa Busjahn, Roman Bednarik, Andrew Begel, Martha Crosby, James H. Paterson, Carsten Schulte, Bonita Sharif, and Sascha Siebert. Eye movements in code reading: Relaxing the linear order. In Proceedings of the 23rd IEEE International Conference on Program Comprehension (ICPC), pp. 255265. IEEE, 2015. Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, and Lichao Sun. Gui-world: video benchmark and dataset for multimodal gui-oriented understanding, 2025a. URL https://arxiv.org/abs/2406.10819. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code, 2021. Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, and Qianxiang Wang. Swe-exp: Experience-driven software issue resolution, 2025b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks, 2024. URL https://arxiv.org/abs/2312.14238. Zhi Chen and Lingxiao Jiang. Evaluating software development agents: Patch patterns, code quality, and issue complexity in real-world github scenarios. In 2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), pp. 657668, Montreal, Canada, 2025. IEEE. Zhi Chen, Wei Ma, and Lingxiao Jiang. Unveiling pitfalls: Understanding why ai-driven code agents fail at github issue resolution, 2025c. Alex Clark and Contributors. Pillow: The friendly pil fork. https://python-pillow.org/, 2010. Accessed: 2025-01-01. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412. 19437. Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M. Zhang. Large language models for software engineering: Survey and open problems. In Proceedings of the 45th International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE), pp. 3153, Melbourne, Australia, 2023. IEEE. FreeType 2 Documentation: Glyph Styling. The FreeType Project, 2024. URL https://freetype. org/freetype2/docs/reference/ft2-bitmap handling.html#ft bitmap embolden. Accessed: 2025-01-01. Google Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models, 2023. GitHub. GitHub REST API documentation. https://docs.github.com/en/rest, 2025. Accessed: January 2025. Google. Gemini developer API pricing. https://ai.google.dev/gemini-api/docs/pricing, 2025. Accessed: January 2025. Google DeepMind. Gemini-3-flash model card. https://storage.googleapis.com/ deepmind-media/Model-Cards/Gemini-3-Flash-Model-Card.pdf, 2025a. Official model specification and capabilities document. Google DeepMind. Gemini-3-pro model card. https://storage.googleapis.com/ deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf, 2025b. November 2025. Documents Gemini 3 Pros training on document understanding and OCR tasks. Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: benchmark for code reasoning, understanding and execution. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1656816621, Vienna, Austria, 2024. PMLR. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. Unixcoder: Unified cross-modal pre-training for code representation. arXiv preprint arXiv:2203.03850, 2022. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: long-range pre-trained language model for code completion. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1196911984, Honolulu, Hawaii, USA, 2023. PMLR. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. DeepSeek-Coder: When the large language model meets programming the rise of code intelligence, 2024. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, 2024. Chao Hu, Wenhao Zeng, Yuling Shi, Beijun Shen, and Xiaodong Gu. In line with context: Repository-level code generation via context inlining, 2026. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-Coder technical report, 2024. Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Reading text in the wild with convolutional neural networks, 2014. URL https://arxiv.org/abs/1412. 1842. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation, 2024. Anoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes ohne, and Jean Baptiste Faddoul. Chargrid: Towards understanding 2d documents, 2018. URL https://arxiv.org/abs/1809.08799. Mohammad Abdullah Matin Khan, M. Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. XCodeEval: An execution-based large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 67666805, Bangkok, Thailand, 2024. Association for Computational Linguistics. Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer, 2022. URL https://arxiv.org/abs/2111.15664. Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding, 2023. URL https://arxiv.org/abs/2210.03347. Han Li, Yuling Shi, Shaoxin Lin, Xiaodong Gu, Heng Lian, Xin Wang, Yantao Jia, Tao Huang, and Qianxiang Wang. Swe-debate: Competitive multi-agent debate for software issue resolution, 2025a. 21 Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. Trocr: Transformer-based optical character recognition with pre-trained models, 2022. URL https://arxiv.org/abs/2109.10282. Yanhong Li, Zixuan Lan, and Jiawei Zhou. Text or pixels? evaluating efficiency and understanding of llms with visual text inputs. In Findings of the Association for Computational Linguistics: EMNLP 2025, pp. 1056410578, 2025b. Yunhao Liang, Ruixuan Ying, Bo Li, Hong Li, Kai Yan, Qingwen Li, Min Yang, Okamoto Satoshi, Zhe Cui, and Shiwen Ni. Visual merit or linguistic crutch? close look at deepseek-ocr. arXiv preprint arXiv:2601.03714, 2026. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, volume 36, pp. 3489234916. Curran Associates, Inc., 2023. Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He, Wenhao Wu, and Cong Yao. Textsnake: flexible representation for detecting text of arbitrary shapes, 2020. URL https://arxiv.org/abs/1807.01544. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. StarCoder 2 and The Stack v2: The next generation, 2024. Myles C. Maxfield, John Daggett, and Tab Atkins Jr. Css fonts module level 4. W3c working draft, World Wide Web Consortium (W3C), 2024. URL https://www.w3.org/TR/ css-fonts-4/#font-synthesis-style-prop. Microsoft Corporation. Visual studio code documentation: Color themes. https://code. visualstudio.com/docs/getstarted/themes, 2024. Accessed: 2025-01-01. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models, 2023. OpenAI. Gpt-4 technical report, 2023. OpenAI. OpenAI API pricing. https://openai.com/api/pricing/, 2025a. Accessed: January 2025. Image tokens are priced at standard text token rates for vision-capable models. OpenAI. Gpt-5.1 model documentation. https://platform.openai.com/docs/models/ gpt-5.1, 2025b. Accessed via OpenAI API. OpenAI. Gpt-5-mini model documentation. https://platform.openai.com/docs/models/ gpt-5-mini, 2025c. Accessed via OpenAI API. OpenRouter. OpenRouter: unified API for LLMs. https://openrouter.ai/, 2025. Accessed: January 2026. Provides unified API access to multiple LLM providers. Dangfeng Pan, Zhensu Sun, Cenyuan Zhang, David Lo, and Xiaoning Du. The hidden cost of readability: How code formatting silently consumes your LLM budget. arXiv preprint arXiv:2508.13666, 2025. URL https://arxiv.org/abs/2508.13666. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, volume 32, pp. 80248035, 2019. Weihan Peng, Yuling Shi, Yuhang Wang, Xinyun Zhang, Beijun Shen, and Xiaodong Gu. Swe-qa: Can language models answer repository-level code questions?, 2025. Stefano Rando, Luca Romani, Alessio Sampieri, Luca Franco, John Yang, Yuta Kyuragi, Fabio Galasso, and Tatsunori Hashimoto. Longcodebench: Evaluating coding llms at 1m context windows. arXiv preprint arXiv:2505.07897, 2025. 22 Johannes Rausch, Octavio Martinez, Fabian Bissig, Ce Zhang, and Stefan Feuerriegel. Docparser: Hierarchical structure parsing of document renderings, 2021. URL https: //arxiv.org/abs/1911.01702. Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. CodeBLEU: method for automatic evaluation of code synthesis, 2020. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code Llama: Open foundation models for code, 2023. Yuling Shi, Songsong Wang, Chengcheng Wan, Min Wang, and Xiaodong Gu. From code to correctness: Closing the last mile of code generation with hierarchical debugging, 2024. Yuling Shi, Yichun Qian, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. Longcodezip: Compress long context for code language models, 2025a. Yuling Shi, Hongyu Zhang, Chengcheng Wan, and Xiaodong Gu. Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pp. 16281639, Los Alamitos, CA, USA, 2025b. IEEE Computer Society. doi: 10.1109/ICSE55347.2025.00005. Yuling Shi, Maolin Sun, Zijun Liu, Mo Yang, Yixiong Fang, Tianran Sun, and Xiaodong Gu. Reasoning in trees: Improving retrieval-augmented generation for multi-hop question answering. arXiv preprint arXiv:2601.11255, 2026. R. Smith. An overview of the tesseract ocr engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), volume 2, pp. 629633, Curitiba, Brazil, 2007. IEEE. doi: 10.1109/ICDAR.2007.4376991. Margaret-Anne Storey. Theories, tools and research methods in program comprehension: Past, present and future. Software Quality Journal, 14(3):187208, 2006. Zhensu Sun, Xiaoning Du, Zhou Yang, Li Li, and David Lo. Ai coders are among us: Rethinking programming language grammar towards efficient code generation. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 11241136, 2024. Zhensu Sun, Chengran Yang, Xiaoning Du, Zhou Yang, Li Li, and David Lo. Token sugar: Making source code sweeter for llms through token-efficient shorthand. arXiv preprint arXiv:2512.08266, 2025. DK Thennal, Jesin James, Deepa Padmini Gopinath, et al. Advocating character error rate for multilingual asr evaluation. In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 49264935, 2025. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, et al. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2026. URL https://arxiv.org/abs/2507.01006. Chaofan Wang, Tingrui Yu, Chen Xie, Jie Wang, Dong Chen, Wenrui Zhang, Yuling Shi, Xiaodong Gu, and Beijun Shen. Evoc2rust: skeleton-guided framework for project-level c-to-rust translation. arXiv preprint arXiv:2508.04295, 2025a. Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. DocLLM: layout-aware generative language model for multimodal document understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 85298548, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.463. URL https://aclanthology.org/2024.acl-long.463/. 23 Yaqin Wang, Xiaoyu Li, Tung Thanh Nguyen, Shuai Wang, Chenchao Ni, and Li Ding. Natural is the best: Model-agnostic code simplification for pre-trained large language models. Proceedings of the ACM on Software Engineering, 2024b. Yifei Wang, Yu Sheng, Linjing Li, and Daniel Dajun Zeng. Uncertainty unveiled: Can exposure to more in-context examples mitigate uncertainty for large language models? In Findings of the Association for Computational Linguistics: ACL 2025, pp. 2065920678, 2025b. Yifei Wang, Feng Xiong, Yong Wang, Linjing Li, Xiangxiang Chu, and Daniel Dajun Zeng. Position bias mitigates position bias: Mitigate position bias through inter-position knowledge distillation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 14951512, 2025c. Yifei Wang, Yueqi Wang, Zhenrui Yue, Huimin Zeng, Yong Wang, Ismini Lourentzou, Zhengzhong Tu, Xiangxiang Chu, and Julian McAuley. FASA: FREQUENCY-AWARE SPARSE ATTENTION. In The Fourteenth International Conference on Learning Representations, 2026a. URL https://openreview.net/forum?id=FnSgecCEwg. Yuhang Wang, Yuling Shi, Mo Yang, Rongrui Zhang, Shilin He, Heng Lian, Yuting Chen, Siyu Ye, Kai Cai, and Xiaodong Gu. Swe-pruner: Self-adaptive context pruning for coding agents. arXiv preprint arXiv:2601.16746, 2026b. Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model, 2024. Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression, 2025. Frank Wilcoxon. Individual comparisons by ranking methods. Biometrics bulletin, 1(6):8083, 1945. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845. Association for Computational Linguistics, 2020. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 11921200, Virtual Event, 2020. ACM. doi: 10.1145/3394486.3403172. URL http: //dx.doi.org/10.1145/3394486.3403172. Guang Yang, Yu Zhou, Wei Cheng, Xiangyu Zhang, Xiang Chen, Terry Yue Zhuo, Ke Liu, Xin Zhou, David Lo, and Taolue Chen. Less is more: Docstring compression in code generation, 2025a. URL https://arxiv.org/abs/2410.22793. Zhen Yang, Wenyi Hong, Mingde Xu, Xinyue Fan, Weihan Wang, Jiele Cheng, Xiaotao Gu, and Jie Tang. Ui2code n: visual language model for test-time scalable interactive ui-to-code generation, 2025b. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models, 2023. Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In Proceedings of the 18th European Conference on Computer Vision (ECCV), pp. 234251, Milan, Italy, 2024. Springer. Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, and Xiaodong Gu. Pruning the unsurprising: Efficient code reasoning via first-token surprisal, 2025. 24 Wenhao Zeng, Xuteng Zhang, Yuling Shi, Chao Hu, Yuting Chen, Beijun Shen, and Xiaodong Gu. Glimprouter: Efficient collaborative inference by glimpsing one token of thoughts. arXiv preprint arXiv:2601.05110, 2026. Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. Diet code is healthy: Simplifying programs for pre-trained models of code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 10731084, 2022. Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. Unifying the perspectives of nlp and software engineering: survey on language models for code, 2024. URL https://arxiv.org/abs/2311.07989. Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, and Zhaoxiang Zhang. Vtcbench: Can vision-language models understand long context with vision-text compression? arXiv preprint arXiv:2512.15649, 2025. Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving, 2024. URL https://arxiv.org/abs/2401.09670. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. DeepSeek-Coder-V2: Breaking the barrier of closed-source models in code intelligence, 2024. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro von Werra. BigCodeBench: Benchmarking code generation with diverse function calls and complex instructions. In Proceedings of the 13th International Conference on Learning Representations, Singapore, 2025. OpenReview.net."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "Chongqing University",
        "East China Normal University",
        "Hohai University",
        "Imperial College London",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "Singapore Management University",
        "UC San Diego"
    ]
}