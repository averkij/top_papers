{
    "paper_title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
    "authors": [
        "Qingyuan Wu",
        "Yuhui Wang",
        "Simon Sinong Zhan",
        "Yanning Dai",
        "Shilong Deng",
        "Sarra Habchi",
        "Qi Zhu",
        "Matthias Gallé",
        "Chao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization."
        },
        {
            "title": "Start",
            "content": "Qingyuan Wu 1 2 * Yuhui Wang 3 * Simon Sinong Zhan 4 Yanning Dai 3 Shilong Deng 5 Sarra Habchi 2 Qi Zhu 4 Matthias Galle 2 Chao Huang"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verified Reward (RLVR) has emerged as critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces unified clipping framework that characterizes existing methods via general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, variancereduced Monte Carlo estimator of the KL divergence, as key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization. 6 2 0 2 5 ] . [ 1 4 9 4 5 0 . 2 0 6 2 : r 1. Introduction Reinforcement Learning (RL) (Kaelbling et al., 1996; Sutton & Barto, 2018) has served as pivotal training paradigm in decision-making problems (Tesauro, 1994; Silver et al., 2016; Mnih et al., 2013; Berner et al., 2019), and has re- *Equal contribution 1University of Southampton 2Cohere 3KAUST 4Northwestern University 5University of Liverpool. Correspondence to: Qingyuan Wu <qingyuan.wu@soton.ac.uk>. Preprint. 1 cently been playing central role in advancing Large Language Models (LLMs) (Ouyang et al., 2022; Lambert et al., 2024). RL provides an efficient and general training framework for LLMs, enabling optimization over complex, nondifferentiable objectives that extend beyond direct supervised learning from human data. This capability is particularly critical for real-world tasks such as code generation (Jain et al., 2024), mathematical reasoning (Zhang et al., 2024; Cobbe et al., 2021), and dialogue alignment (Chiang et al., 2024). Current RL methodologies for LLMs, particularly in RL with Verified Reward (RLVR) (Lambert et al., 2024) settings, predominantly rely on Proximal Policy Optimization (PPO) (Schulman et al., 2017). PPO ensures training stability through the ratio-based clipping mechanism, aiming to approximate the trust-region constraint of Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a). Recently, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) has emerged as memory-efficient alternative for training large-scale LLMs by using group-normalized returns as the advantage baselines, thus eliminating the need to maintain separate value function. Like PPO, GRPO and its variants (Yu et al., 2025; Yang et al., 2025b) rely on the ratio-based clipping to ensure stable policy updates. Despite their success, the reliance on ratio-based clipping constitutes specific and potentially restrictive design choice within the broader landscape of policy optimization. While these methods aim to ensure stable updates by constraining policy divergences through clipping likelihood ratios, recent studies (Cui et al., 2025; Park et al., 2025) reveal that both training exploration and evaluation performance are highly sensitive to the specific definition and implementation of policy divergence constraints. Although different variants of the ratio-based clipping mechanism (Yu et al., 2025; Yang et al., 2025b) have been proposed, principled understanding of how different policy divergence measures and corresponding constraints affect the trade-off between exploration and stability remains largely unexplored. To address this issue, this paper first introduces unified clipping framework that characterizes existing clipping methods under general notion of policy divergence. This unified clipping framework provides foundational perspective Unified Framework for Rethinking Policy Divergence Measures in GRPO for analyzing various policy divergence constraints, encompassing both likelihood ratios and KullbackLeibler (KL) divergences. Furthermore, we identify that the KL3 estimator (Schulman, 2020) serves as the pivotal policy divergence constraint under our framework. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping. Based on these observations, we propose Approximate Trust Region-based GRPO (ATR-GRPO). Unlike standard symmetric ratio-based clipping, ATR-GRPO leverages the KL3 estimator to actively steer exploration by reallocating probability mass toward high-confidence actions, all while maintaining the computational efficiency of GRPO. Comprehensive experiments on mathematical reasoning benchmarks demonstrate that KL3 estimator consistently improves training stability and final performance compared to existing state-of-the-art (SOTA) baselines. Our contributions can be summarized as follows: We introduce unified clipping framework for policy optimization that unifies existing policy divergence constraints and can be extended to arbitrary alternative measures. We identify the KL3 estimator as an effective policy divergence constraint, establish its connection to ratiobased constraints, and theoretically show that it promotes stronger exploration than existing alternatives. Building on these insights, we develop ATR-GRPO, which exhibits improved exploration dynamics while retaining the simplicity of existing methods. We empirically show that ATR-GRPO enhances learning stability and achieves performance competitive with various SOTA baselines. 2. Related Work RL for LLMs. Reinforcement Learning (RL) has been established as the standard paradigm for advancing Large Language Models (LLMs), such as RLVR (Lambert et al., 2024). PPO (Schulman et al., 2017) utilizes clipped surrogate objective to ensure stable policy updates. However, PPO requires training separate critic model, which can be prohibitive for large-scale reasoning tasks. To mitigate these memory constraints, RLOO (Ahmadian et al., 2024) eliminates the critic by employing the Leave-One-Out baseline to reduce variance by averaging rewards across other samples within the batch. GRPO (Shao et al., 2024) calculates advantages relative to sampled group of outputs for each prompt, effectively optimizing memory usage for reasoning tasks. GSPO (Zheng et al., 2025) further extends this paradigm by elevating the optimization granularity to the sequence level, aligning updates with reward signals. Building on GRPO or GSPO, some recent works such as GTPO (Tan et al., 2025) and EMPO (Zhang et al., 2025a) incorporate semantic entropy into reward shaping to address the persistent challenge of sparse credit assignment. Trust Region Methods in RL. Trust-region methods underpin stable policy optimization in RL, from NPG (Kakade, 2001) to the explicit KL-constrained formulation of TRPO (Schulman et al., 2015a). However, TRPO relies on computationally expensive second-order optimization, making it impractical for large-scale RL tasks. To address this scalability issue, PPO (Schulman et al., 2017) was introduced as first-order approximation that replaces the hard KL constraint with ratio-based clipping, aiming to implicitly constrain policy updates. While PPO has become the de facto choice for large-scale training due to its computational efficiency, its reliance on ratio-based clipping does not explicitly enforce trust-region constraint. Prior work has shown that this approximation can fail to properly control policy updates, leading to potential optimization instability, as demonstrated by TRGPPO and Truly PPO (Wang et al., 2019; 2020). In this work, we propose unified clipping framework that generalizes different policy divergence constraints, including likelihood ratios and KL divergences. Clipping Mechanisms in RL. TRGPPO (Wang et al., 2019) shows that ratio-based clipping in PPO overly constrains low-likelihood probabilities, and introduces dynamic clipping to relax this effect. Truly PPO (Wang et al., 2020) further exposes the divergence between ratio-based and KLbased constraints, and proposes KL-based clipping method as an alternative. DAPO (Yu et al., 2025) introduces an asymmetric clip-higher mechanism that increases the upper clipping range, mitigating entropy collapse and facilitating probability increases for low-likelihood exploratory tokens. DCPO (Yang et al., 2025b) proposes the dynamic ratiobased clipping mechanism, which can adaptively adjust the clipping ranges based on the prior probabilities to strengthen exploration ability. Aside from exploration purposes, the dual clipping mechanism (Ye et al., 2020) is proposed to ensure the convergence and stability in large-scale distributed RL training of MOBA games, which is also adopted as the default technique in the LLM training framework (Sheng et al., 2025). SAPO (Gao et al., 2025) presents the soft gate operation to replace the hard clipping, stablizing the optimization. These ratio-based clipping mechanisms, however, designed for stability, exert profound and often detrimental influence on policy entropy (Cui et al., 2025; Park et al., 2025). Different from previous approaches, this paper identifies the KL3 estimator (Schulman, 2020) as the pivotal policy divergence constraint within our proposed framework, demonstrated by our theoretical analysis and empirical evaluation. 2 Unified Framework for Rethinking Policy Divergence Measures in GRPO Table 1. Comparative analysis of policy divergence constraints. Unlike previous methods that rely on either heuristic symmetric ratio-based clipping (PPO, GRPO), asymmetric ratio-based clipping (DAPO) or computationally expensive full expectation of the KL divergence (TRPO, Truly PPO), our ATR-GRPO achieves principled, approximate trust-region constraint with low computational cost. Clipping / Constraint Criterion Method wt(θ) [1 ϵ, 1 + ϵ] PPO (Schulman et al., 2017), GRPO (Shao et al., 2024) wt(θ) [1 ϵl, 1 + ϵu] DAPO (Yu et al., 2025) DCPO (Yang et al., 2025b) Computational Cost Low Low Low (cid:113) (cid:113) ] wt(θ) [0.5 + 1 2 wt(θ) (cid:2)min(cid:0)lKL ), 0.5 + 1 max(1 4ϵl πθold KLt(θ) δ δ , 1 ϵ(cid:1), max(cid:0)uKL KLt(θ) δ δ , 1 + ϵ(cid:1)(cid:3) 1 + 4ϵu πθold KL3t(θ) δ or wt(θ) [lKL3 δ , uKL3 δ ] Trust-Region Constraint? (approximated by KL3 ) High Medium Medium (for large action space A) Low TRPO (Schulman et al., 2015a) TRGPPO (Wang et al., 2019) Truly PPO (Wang et al., 2020) ATR-GRPO (ours) 3. Preliminaries Language Model Generation as MDP. Language generation process can be formulated as Markov Decision Process (MDP), denoted by the tuple (S, A, , r). Here, denotes the state space, where the state at timestep is defined as st (x, y<t), corresponding to the concatenation of the input query and the partially generated response y<t. The action space is defined over the token vocabulary, represents the transition dynamics induced by autoregressive token generation, and the reward function is defined over concatenations of the input query and the partially generated response. The policy πθ parameterized by θ aims to maximize the objective J(θ) defined as: J(θ) = xD (cid:34) (cid:104)(cid:88) r(st, yt); πθ (cid:105) (cid:35) β KL(cid:0)πθ(st)πref (st)(cid:1) , (1) where denotes the query distribution, and is the response generated by the language model πθ. πref is reference policy, KL denotes the KullbackLeibler (KL) divergence between two policies (e.g., π1 and π2), defined as follows: KL(cid:0)π1(s)π2(s)(cid:1) (cid:88) π1(as) log aA π1(as) π2(as) , (2) and β controls the KL regularization to avoid policy drift (Shao et al., 2024). Clipping in Policy Optimization Algorithms. Given query and generated response y, the clipped surrogate objective for the policy πθ is defined as: L(θ) = yπθ (x) xD [min (wt(θ)At, clip (wt(θ), ) At)] , where wt(θ) = πθ(ytst) πθold (ytst) represents the token-level likelihood ratio between πθ and old policy πθold . Here, the term At denotes the advantage estimate, which corresponds to group-normalized score in GRPO (Shao et al., 2024) and the Generalized Advantage Estimator (Schulman et al., 2015b) in PPO (Schulman et al., 2017). The clipping function clipratio (wt(θ), lt, ut) = clip() constrains the likelihood ratio, thereby preventing excessively large policy updates and ensuring training stability. Without loss of generality, we adopt the generalized clipping formulation proposed by (Wang et al., 2019): wt(θ), lt, ut, where lt and ut denote the lower and upper clipping ranges, respectively. PPO and GRPO both use the symmetric clipping ranges (lt = 1 ϵ, ut = 1 + ϵ), while DAPO (Yu et al., 2025) uses the asymmetric clipping ranges (lt = 1 ϵl, ut = 1 + ϵu). lt wt(θ) ut wt(θ) lt wt(θ) ut (3) Despite the popularity of ratio-based clipping, Wang et al. identified mismatch between the ratio-based and the KLbased constraints, and accordingly proposed an alternative KL-based clipping function: (cid:40) clipKL(wt(θ), δ) = wt(θ), wt(θold), KLt(θ) δ, otherwise, (4) where δ is the trust-region threshold and KLt(θ) (cid:0)πθ(st)πθold(st)(cid:1) involves an expectation over the KLt full action space (Eq. (2)). We distinguish loss functions induced by different clipping functions using subscripts. For instance, the objectives corresponding to ratio-based and KL-based clipping are denoted by Lratio and LKL, respectively. 4. Method 4.1. Unified Clipping Framework We first propose unified framework that subsumes both ratio-based and KL-based policy constraints, while naturally accommodating more general constraint variants. Specifically, we define general clipping operator as clipgeneral(wt(θ), ) = (cid:40) wt(θ), wt(θold), if Ct(θ) is true otherwise (5) where the constraint function Ct(θ) encodes prescribed feasibility condition on the policy corresponding to the sample (st, at). This formulation provides flexible abstraction 3 Unified Framework for Rethinking Policy Divergence Measures in GRPO of policy divergence constraints: rather than committing to specific measure, the operator admits arbitrary constraint functions that define how policy divergences are measured and restricted at the sample level. Under this unified clipping framework, existing methods can be recovered as special cases by specifying different choices of the constraint function, as illustrated in Table 1. In particular, when the constraint is set as CKL (θ) := KLt(θ) δ, the resulting surrogate objective exactly recovers the original KL-constrained formulation, i.e., LKL general(θ) = LKL(θ), corresponding to an explicit trust-region constraint. When the constraint is set as Cratio (θ) := lt wt ut, the resulting surrogate objective yields the same gradient as the standard ratio-based clipping objective. As result, this choice implicitly constrains policy updates without explicitly enforcing trust-region constraint. We formalize this equivalence in the following theorem. Theorem 4.1 (Gradient Equivalence). Let the constraint be defined as Cratio (θ) := lt wt(θ) ut. Then, for any parameter θ where the objective is differentiable, the gradient of the general objective is equivalent to that of the ratio-based objective: Lratio(θ) = Lratio t general(θ). 4.2. Analysis of KL3 Estimator and ATR-GRPO We have shown that both ratio-based and KL-based objectives are special cases of the unified formulation in Eq. (5). Next, we investigate other policy divergence constraints that can be incorporated within this framework. Previous literature has shown that the KL-based constraint KL(θ) δ aligns with the original trust-region theory, which enforces monotonic policy improvement (Schulman et al., 2015a; Wang et al., 2020; 2019). However, in the context of LLMs, enforcing such KL-based constraint is often intractable, as it requires computing the full expectation of the KL divergence over an extremely large action space. To address this limitation, Schulman proposed several Monte-Carlo estimators to approximate the KL divergence. Among them, the most widely used estimator is the KL3t operator, which admits lightweight surrogate expression: KL3t(θ) := wt(θ) 1 log wt(θ). (6) This estimator offers several appealing properties. First, it can be computed at the sample level without requiring an explicit expectation over the entire action space, which is crucial for large-scale policies such as LLMs. Second, KL3t is non-negative and exhibits substantially lower variance than naive Monte-Carlo KL estimators. Finally, KL3t provides local approximation to the KL divergence near the identity ratio, and thus serves as principled trust-region surrogate (Schulman, 2020). Below, we analyze the relationship between different policy divergence constraints within the unified framework. As implied by Eq. (6), KL3t(θ) can be viewed as function of the ratio wt(θ), and we show that the two constraints (likelihood ratio and KL divergence) are equivalent under appropriate hyperparameter choices. Theorem 4.2 (Equivalence and Asymmetry). Define lower and upper clipping ranges for given threshold δ > 0 as: lKL3 δ = min θ wt(θ) s.t. KL3t(θ) δ, uKL3 δ = max θ wt(θ) s.t. KL3t(θ) δ. , where 0 < lKL3 (1) The constraint KL3t(θ) δ is equivalent to lKL3 wt(θ) uKL3 (2) These ranges satisfy the asymmetry property: 1lKL3 uKL3 δ 1. δ < 1 < uKL3 δ δ δ . δ < The proof of Theorem 4.2 is provided in Appendix A.1. Theorem 4.2 reveals an explicit connection between the KL3-based and ratio-based constraints by characterizing the admissible likelihood ratio ranges under the KL3 constraint, in the spirit of TRGPPO (Wang et al., 2019). Importantly, our goal here is to reveal this underlying relationship between these two types of constraints, instead of how to compute these clipping ranges explicitly. In practice, it is not necessary to explicitly compute the clipping ranges, but can directly restrict policy divergences through the specified clipping function Ct(θ) in Eq. (5). Figure 1. Illustration of the KL3-based constraint. As implied by Theorem 4.2 (1), the KL3-based constraint is equivalent to ratio-based constraint with specific choice of hyperparameters. Furthermore, as implied by Theorem 4.2 (2), the KL3-based constraint always induces an asymmetric clipping range, where the upper clipping range deviates more than the lower range does (1 lKL3 δ < uKL3 δ 1). Also, as illustrated in the Figure 1, the constraint KL3t(θ) δ results in larger upper clipping range compared to the symmetric clipping range (1 ϵ, 1 + ϵ), where we set ϵ = 1 lKL3 . This property is consistent with prior work, such as Clip-Higher (Yu et al., 2025). However, unlike heuristic asymmetric clipping rules of Clip-Higher, the KL3-based constraint provides an exact and principled δ 4 Unified Framework for Rethinking Policy Divergence Measures in GRPO characterization of this asymmetry, thereby offering theoretically grounded mechanism for tuning the clipping range. Building upon these observations, we formally propose Approximate Trust Region-based GRPO (ATR-GRPO), which integrates the KL3 constraint directly into the unified framework(Eq. (5)) by setting Ct(θ) := KL3t(θ) δ. Different from GRPO, ATR-GRPO enforces the approximate trust-region constraint through the ATR-based clipping (wt(θ) [lKL3 ]) within the unified framework (Eq. (5)), enabling stable policy updates without explicitly computing the full KL expectations. , uKL3 δ δ 5. Theoretical Analysis In this section, we present comprehensive theoretical analysis of ATR-based clipping and ratio-based clipping, building on the frameworks developed in prior works (Cui et al., 2025; Park et al., 2025). We formally characterize the induced policy logit differences (Theorem 5.1) and analyze their impact on policy entropy difference (Theorem 5.2). 5.1. Setup and Assumptions We assume that GRPO (symmetric ratio-based clipping) and ATR-GRPO (ATR-based clipping) both utilize the fullbatch gradients under the standard policy gradient algorithm (Williams, 1992). Furthermore, the policy πθ is modeled as softmax policy πθ(as) = A(s) exp(θs,a) . We mainly consider two representative cases, and we denote the corresponding probabilistic events as: exp(θs,a) (cid:80) X(s) := {a A(s)wt(θs,a) [1 ϵ, lKL3 X+(s) := {a A(s)wt(θs,a) [1 + ϵ, uKL3 δ ], 1 + ϵ = uKL3 ], 1 ϵ = lKL3 δ }, }, δ δ where X(s) and X+(s) represent the event of unsatisfying and satisfying the KL3 constraint (approximate trustregion constraint), respectively. We define IX(s)(a) and IX+(s)(a) to be the indicator function for X(s) and X+(s), respectively. 5.2. Policy Logits Difference Analysis s,a s,a and θATR,k+1 s,a θratio,k+1 s,a Let θk s,a denote the policy logits at the k-th step. We denote the updated policy logits for the ratio-based and ATRbased clipping methods as θratio,k+1 , respectively. We define θs,a := θATR,k+1 as the policy logits difference between ATR-based clipping and the ratio-based clipping. We characterize θs,a under different events in the following Theorem 5.1 and provide proof in Appendix A.2. Theorem 5.1 (Policy Logits Difference). Consider the policy gradient algorithm with learning rate η and given the state visitation distribution dπθold (s) induced by πθold. Let θs,a denote the policy logits difference between the ATR-based and ratio-based clipping methods. Under event X(s), we have (cid:20) AIX(s)(a)) θs,a = ηdπθold (s)πθk (as) aπθk (s) (cid:21) [AIX(s)(a)] . Under event X+(s), we have (cid:20) AIX+(s)(a) θs,a = ηdπθold (s)πθk (as) aπθk (s) (cid:21) [AIX+(s)(a)] . 5.3. Entropy Difference Analysis Building upon Theorem 5.1, we characterize the exploration behaviours of ATR-based clipping by deriving the entropy difference := H(θATR,k+1s) H(θratio,k+1s) in the following theorem. The proof is provided in Appendix A.3. Theorem 5.2 (Entropy Difference). The entropy difference between the ATR-based and ratio-based clipping methods is related to the covariance between the advantage and the log-likelihood. Under event X(s), we have = ηdπθold (s) Cov aπθk (s) (cid:0)AIX(s)(a), log πθk (as)(cid:1) . Under event X+(s), we have = ηdπθold (s) Cov aπθk (s) (cid:0)AIX+(s)(a), log πθk (as)(cid:1) . Theorem 5.2 implies that the entropy difference depends on the covariance between the advantage under the specific event and the log-likelihood of πθk . Crucially, this dependency reveals that ATR-based clipping exhibits exploration behavior that is better aligned with the approximate trust-region constraint than ratio-based clipping. We identify this as the dynamic exploration mechanism: it enforces conservative exploration when updates are large and potentially risky (X(s)) while permitting expansive exploration when updates are within the approximate trust-region (X+(s)). Specifically, when the approximate trust-region constraint is unsatisfied (X(s)), ATR-based clipping maintains higher entropy than ratio-based clipping when there is an alignment between the advantage and policy likelihood (i.e., advantageous and high-probability actions). By preserving entropy in these high-confidence regions, our ATR-based clipping method prioritizes stability, anchoring the policy to highconfidence actions, preventing the premature amplification of low-confidence tails that lead to instability. 5 Unified Framework for Rethinking Policy Divergence Measures in GRPO Table 2. Performance comparison on the AMC2023, AIME2024, and AIME2025 benchmarks. We report both the final and best evaluation performance (formatted as final (best)). The best performance is highlighted. Method AMC AIME2024 AIME2025 Average Mean@8 (%) Pass@8 (%) Mean@8 (%) Pass@8 (%) Mean@8 (%) Pass@8 (%) Mean@8 (%) Pass@8 (%) Qwen3-1.7B Base Model Clip Clip-Higher Dual Clip Dynamic Clipping Clip-Cov Soft Gate ATR-based Clipping (ours) 28.61 40.21 (40.21) 37.65 (37.80) 42.02 (42.02) 41.72 (41.72) 33.58 (36.14) 42.92 (42.92) 41.72 (45.03) 48.19 60.24 (67.47) 66.27 (66.27) 65.06 (68.67) 63.86 (67.47) 61.45 (67.47) 68.67 (68.67) 69.88 (72.29) 6.67 10.00 (10.00) 10.42 (10.83) 10.83 (11.67) 9.17 (11.67) 7.92 (9.58) 8.33 (9.58) 13.33 (13.33) 13.33 23.33 (26.67) 23.33 (30.00) 30.00 (33.33) 20.00 (30.00) 20.00 (23.33) 26.67 (30.00) 26.67 (36.67) 4.17 10.42 (12.50) 6.25 (10.42) 12.50 (13.33) 11.67 (12.92) 7.50 (10.83) 13.33 (13.33) 13.75 (14.58) 20.00 20.00 (26.67) 16.67 (26.67) 23.33 (26.67) 23.33 (26.67) 16.67 (23.33) 23.33 (26.67) 30.00 (30.00) 13.15 20.21 (20.41) 18.11 (18.68) 21.78 (21.78) 20.85 (21.56) 16.33 (18.80) 21.53 (21.53) 22.93 (23.07) 27.18 34.52 (36.93) 35.42 (37.15) 39.46 (39.56) 35.73 (38.35) 32.70 (34.93) 39.56 (39.87) 42.18 (44.00) Qwen3-8B Base Model Clip Clip-Higher Dual Clip Dynamic Clipping Clip-Cov Soft Gate ATR-based Clipping (ours) 26.05 45.78 (48.34) 53.77 (56.33) 55.57 (55.72) 53.16 (54.97) 50.75 (55.12) 53.01 (53.01) 56.02 (56.02) 50.60 72.29 (74.70) 73.49 (78.31) 77.11 (78.31) 78.31 (78.31) 69.88 (75.90) 74.70 (74.70) 80.72 (80.72) 5.42 12.92 (18.33) 20.83 (24.17) 21.25 (24.17) 23.75 (24.58) 21.25 (22.50) 20.42 (22.92) 25.42 (25.83) 16.67 26.67 (36.67) 43.33 (46.67) 36.67 (46.67) 50.00 (50.00) 46.67 (46.67) 43.33 (43.33) 50.00 (50.00) 1.25 15.83 (15.83) 19.58 (21.67) 19.58 (22.50) 20.83 (22.92) 20.83 (22.08) 19.17 (19.17) 19.58 (22.92) 6.67 30.00 (33.33) 26.67 (30.00) 26.67 (33.33) 33.33 (36.67) 33.33 (33.33) 26.67 (30.00) 30.00 (36.67) 10.91 24.84 (26.81) 31.39 (33.36) 32.14 (33.34) 32.58 (32.93) 30.95 (32.48) 30.87 (31.17) 33.67 (33.67) 24.65 42.99 (46.01) 47.83 (51.66) 46.81 (49.75) 53.88 (53.88) 49.96 (50.05) 48.23 (48.23) 53.57 (53.57) Conversely, when the approximate trust-region constraint is satisfied (X+(s)), our method increases entropy when there is misalignment between the advantage and policy likelihood (i.e., advantageous yet low-probability actions). This regime drives aggressive exploration, actively reallocating probability mass toward promising but underexplored parts of the action space, improving efficiency without compromising the approximate trust-region constraints. iments to explicitly highlight our primary distinction. We train each method for 1,000 gradient steps, evaluating performance every 50 steps on the AMC2023 (Li et al., 2024), AIME2024 (Veeraboina, 2023), and AIME2025 (AIME, 2025) benchmarks. The implementation details are provided in Appendix B. The full codebase used to reproduce our experimental results is included in the supplementary material. 6. Experiments 6.1. Experiment Setting all using implement experiments We the Unsloth (Daniel Han & team, 2023) and TRL (von Werra et al., 2020) frameworks. We fine-tune the Qwen3-1.7B and Qwen3-8B models (Yang et al., 2025a) on the DAPO-Math17k dataset (Yu et al., 2025), employing sparse binary reward function, where the model receives reward of +1 upon generating the correct final answer, and 0 otherwise. To mitigate computational overhead, we employ Low-Rank Adaptation (LoRA) (Schulman & Lab, 2025), which injects low-rank adapter matrices to optimize minimal set of parameters, greatly reducing memory requirements without compromising performance (Schulman & Lab, 2025). 6.2. Performance Comparison As shown in Table 2, we conduct comprehensive evaluation of the proposed ATR-based clipping method against SOTA baselines on the AMC2023, AIME2024, and AIME2025 benchmarks. We evaluate the performance of both Qwen3-1.7B and Qwen3-8B using Mean@8 and Pass@8. Mean@8 measures the average accuracy across 8 sampled responses, while Pass@8 indicates the success rate where at least one of the 8 samples is correct. We primarily report the evaluation performance of the final checkpoint, and additionally present the best evaluation performance achieved during training. We compare our approach against SOTA clipping methods, including Clip-Higher (Yu et al., 2025), Dynamic Clipping (Yang et al., 2025b), Clip-Cov (Cui et al., 2025), and Soft Gate (Gao et al., 2025). To ensure fair comparison, we standardize Dr.GRPO (Liu et al., 2025c) as the backbone RL algorithm across all baselines, varying only the clipping mechanism. Consistent with this setup, we denote our method (ATR-GRPO) as ATR-based clipping in the experPerformance on Qwen3-1.7B. Our proposed method achieves substantial improvements over the base model and consistently outperforms existing baselines on Qwen3-1.7B. Specifically, our method achieves the highest scores on the final evaluation performance (Average), with the Mean@8 of 22.93% and Pass@8 of 42.18%, surpassing the best baseline (Dual Clip). Specifically, for challenging AIME2025, our method achieves the final Mean@8 of 13.75% compared to 13.33% for Soft Gate, the best baseline. Unified Framework for Rethinking Policy Divergence Measures in GRPO (a) Return. (b) Entropy. (c) Completion Length. (d) Mean@8 (Average). Figure 2. Comparison of Clip, Clip-Higher, and ATR-based clipping on Qwen3-1.7B. The training curves for (a) return, (b) entropy, and (c) completion length are smoothed with 100-step moving average window. (d) Evaluation performance of Mean@8 (Average). Performance on Qwen3-8B. Our method continues to exhibit superior performance on Qwen3-8B, particularly on the AMC2023 and AIME2024. On AMC2023, our method achieves final Mean@8 of 56.02% and Pass@8 of 80.72%. Furthermore, on AIME2024, our approach demonstrates superior robustness, achieving final Mean@8 of 25.42% and Pass@8 of 50.00%, outperforming Dynamic Clipping. While Dynamic Clipping proves competitive on AIME2025, our method yields the highest final Mean@8 of 33.67% (Average). 6.3. Efficiency and Stability Analysis We now analyze the learning stability and efficiency of ATRbased clipping, comparing it primarily against the Clip and Clip Higher methods, the two baselines most closely related to and as computationally simple as ours. We present the training curves for return, entropy and completion length alongside evaluation performance (Mean@8) in Figure 2. Specifically, as shown in Figure 2(a), while the returns of all methods are initially similar, ATR-based clipping surpasses the baselines around 400 gradient steps, continuing to improve to the return of 0.36, whereas the baselines both plateau near 0.28, demonstrating that our modification yields efficient performance improvements. This trend is mirrored in the evaluation tasks (Figure 2(d)), where our method consistently outperforms baselines, confirming the superior sample efficiency of ATR-based clipping. Moreover, our method does not sacrifice the learning stability. As presented in Figure 2(b), unlike the Clip method, ATR-based clipping maintains steady, moderate entropy level, indicating stable exploration strategy that avoids premature convergence or policy collapse. In terms of the completion length (Figure 2(c)), ATR-based clipping exhibits stable, monotonic decrease. In contrast, the baselines show erratic oscillations (Clip) or sharp, potentially premature drops (Clip-Higher), showing our proposed method effectively stabilizes the learning process. 6.4. Ablation Studies Trust Region Threshold δ. Specifically, we show Mean@8 and Pass@8 with varying δ in Figure 3(a) and Figure 3(b), respectively. Both metrics exhibit similar trend, steadily increasing from δ = 0.05 and peaking at δ = 0.07. However, further increasing δ results in sharp performance degeneration. The results indicate that an overly small δ imposes stronger constraints that hinder performance improvement, while an excessively large δ leads to training instability or policy collapse, thereby degrading performance. (a) Mean@8 with varying δ. (b) Pass@8 with varying δ. Figure 3. Ablation experiments of ATR-GRPO on Qwen3-1.7B. The performance for (a) Mean@8 and (b) Pass@8 with varying δ. Fine-grained Tuning Clip and Clip-Higher. To ensure that our performance gains stem from fundamental algorithmic improvements rather than sub-optimal baseline configurations, we conducted extensive hyperparameter tuning on Clip and Clip-Higher baselines. Specifically, we evaluated Clip with ϵ {0.1, 0.2, 0.3, 0.4, 0.5}, reporting the performance achieved by the optimal configuration. Additionally, for the trust region threshold δ = 0.07, we have corresponding ATR-based clipping range of [lKL3 ] = [0.671, 1.422]. We specifically configured δ Clip and Clip-Higher using these two ranges, including Clip (ϵ {0.329, 0.422}) and Clip-Higher (ϵl = 0.329, ϵu {0.5, 0.6}). As shown in Table 3, our method achieves the highest performance of Mean@8 and Pass@8 (Average) compared to these meticulously tuned baselines. , uKL3 δ 7 Unified Framework for Rethinking Policy Divergence Measures in GRPO Table 3. Performance comparison on Qwen3-1.7B across various KL estimators and fine-grained hyperparameter configurations for Clip and Clip-Higher. Each method is independently trained using 3 separate runs. We report the mean and standard deviation of the performance based on the final checkpoints from these 3 runs. The best performance is highlighted. Method Clip (ϵ = 0.2) Clip (ϵ {0.1, 0.2, 0.3, 0.4, 0.5}) Clip (ϵ = 0.329) Clip (ϵ = 0.422) Clip-Higher (ϵl = 0.2, ϵu = 0.28) Clip-Higher (ϵl = 0.329, ϵu = 0.5) Clip-Higher (ϵl = 0.329, ϵu = 0.6) KL1(δ = 0.07) KL2(δ = 0.07) Full KL-Guided Clipping (δ = 0.07) (Wang et al., 2019) IS-weighted KL3 (δ = 0.07) KL3 (δ = 0.07, lKL δ = 0.671, uKL3 δ = 1.422) Qwen3-1.7B AMC2023 AIME2024 AIME Average Mean@8 (%) Pass@8 (%) Mean@8 (%) Pass@8 (%) Mean@8 (%) Pass@8 (%) Mean@8 (%) Pass@8 (%) 39.810.26 41.970.14 40.161.60 40.160.14 36.750.56 39.660.61 42.720.68 41.471.40 43.221.23 38.700.44 39.260.40 61.452.60 65.861.50 64.263.98 63.052.27 64.663.16 61.852.05 66.671.50 65.862.27 67.071.14 62.650.98 64.661.50 8.470.86 11.671.02 11.811.87 8.611.19 9.170.59 7.640.39 10.830.34 8.190.20 11.111.37 8.471.37 8.751. 26.670.00 27.784.16 32.221.57 24.443.14 21.111.57 22.223.14 27.781.57 24.443.14 26.672.72 23.332.72 26.674.71 10.970.39 14.171.23 11.250.68 10.140.39 8.470.86 11.810.20 12.220.39 12.781.19 13.330.90 10.140.86 11.670.68 22.221.57 28.891.57 22.221.57 23.330.00 20.002.72 23.332.72 23.332.72 24.445.67 24.443.14 21.111.57 24.441. 19.750.10 22.600.63 21.070.95 19.640.38 18.130.64 19.700.24 21.930.21 20.810.77 22.560.66 19.110.65 19.890.38 36.781.27 40.841.53 39.572.33 36.940.29 35.262.10 35.802.31 39.260.60 38.253.67 39.391.75 35.700.29 38.591.98 43.171.17 68.670. 13.061.71 27.784.16 14.031.71 28.891.57 23.421.04 41.782. Crucially, our method outperforms Clip (ϵ = 0.329 and ϵ = 0.422), validating our previous statement and theoretical analysis (Theorem 5.2). KL Estimators. Furthermore, we evaluate alternative KL estimators (KL1, KL2, Full KL-guided clipping, and ISweighted KL3) under the same trust region threshold (δ = 0.07), as presented in Table 3. The results demonstrate that KL3 consistently yields the performance gains. While recent studies (Shah et al., 2025; Liu et al., 2025b; Zhang et al., 2025b; Liu et al., 2025a) have explored KL estimators primarily as loss regularization terms or auxiliary reward signals, we investigate different application, utilizing the KL estimator to guide the clipping criterion. We empirically demonstrate that KL3 is the best choice among these KL estimators for this specific purpose. Test-time Sample Budget. Then, we further report Mean@K (Figure 4(a)) and Pass@K (Figure 4(b)) for ATRGRPO on Qwen3-1.7B with varying K. As increases, Mean@K remains stable around 0.23. In contrast, Pass@K exhibits an increasing trend, scaling with the test-time sample budget from 24.36% at Pass@1 to 56.29% at Pass@128. 6.5. Limitations and Future Works Adaptive Trust Region Threshold. Employing static trust region threshold throughout the entire training process may be suboptimal. Inspired by existing works (Wang et al., 2019; Yang et al., 2025b), we will investigate mechanisms to adaptively adjust δ based on the policy probability and entropy for more stable and efficient learning in the future. Sequence-level Integration. While this work primarily focuses on the token-level objective, we recognize the limitation posed by the mismatch between token-level importance sampling and sequence-level rewards, leading to high-variance gradients and unstable training (Zheng et al., 8 (a) Mean@K with varying K. (b) Pass@K with varying K. Figure 4. The performance (Average) of ATR-GRPO on Qwen31.7B for (a) Mean@K and (b) Pass@K with varying K. 2025). We aim to explore the extension of our framework to sequence-level objectives in future work. 7. Conclusion In this work, we introduce unified clipping framework for policy optimization that generalizes the notion of policy divergence, encompassing both ratiobased and KL-based constraints. From this foundational perspective, we analyze and identify the KL3 estimator as particularly effective policy divergence measure. Based on this insight, we proposed ATR-GRPO that actively reallocates probability mass toward promising actions rather than passively truncating updates through ratio-based clipping. Our theoretical analysis demonstrates that ATR-GRPO enables more principled and effective exploration than standard symmetric ratio-based clipping, while preserving the computational efficiency of GRPO. Empirically, comprehensive experiments across multiple verifiable mathematical reasoning benchmarks demonstrate that ATR-GRPO yields performance improvements in both training stability and final performance compared to existing baselines. These results underscore the central role of the policy divergence constraint in policy optimization and suggest that exploring alternative policy divergence measures is promising direction for advancing LLMs. Unified Framework for Rethinking Policy Divergence Measures in GRPO"
        },
        {
            "title": "References",
            "content": "Ahmadian, A., Cremer, C., Galle, M., Fadaee, M., Kreutzer, Ustun, A., and Hooker, S. Back J., Pietquin, O., to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. AIME. AIME Problems and Solutions, 2025. URL https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhu, B., Zhang, H., Jordan, M., Gonzalez, J. E., et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Daniel Han, M. H. and team, U. Unsloth, 2023. URL http://github.com/unslothai/unsloth. Gao, C., Zheng, C., Chen, X.-H., Dang, K., Liu, S., Yu, B., Yang, A., Bai, S., Zhou, J., and Lin, J. Soft adaptive policy optimization. arXiv preprint arXiv:2511.20347, 2025. Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H.-Y. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. Kakade, S. M. natural policy gradient. Advances in neural information processing systems, 14, 2001. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025a. Liu, K., Liu, J. K., Chen, M., and Liu, Y. Rethinking kl regularization in rlhf: From value estimation to gradient optimization. arXiv preprint arXiv:2510.01555, 2025b. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025c. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Moshkov, I., Hanley, D., Sorokin, I., Toshniwal, S., Henkel, C., Schifferer, B., Du, W., and Gitman, I. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Park, J. R., Kim, J., Kim, G., Jo, J., Choi, S., Cho, J., and Ryu, E. K. Clip-low increases entropy and clip-high decreases entropy in reinforcement learning of large language models. arXiv preprint arXiv:2509.26114, 2025. Schulman, J. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-approx.html, 2020. Schulman, J. and Lab, T. M. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml. 20250929. https://thinkingmachines.ai/blog/lora/. 9 Unified Framework for Rethinking Policy Divergence Measures in GRPO Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, In International P. Trust region policy optimization. conference on machine learning, pp. 18891897. PMLR, 2015a. Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shah, V., Obando-Ceron, J., Jain, V., Bartoldson, B., Kailkhura, B., Mittal, S., Berseth, G., Castro, P. S., Bengio, Y., Malkin, N., et al. comedy of estimators: On kl regularization in rl training of llms. arXiv preprint arXiv:2512.21852, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 1279 1297, 2025. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018. Tan, H., Pan, J., Lin, J., Chen, T., Zheng, Z., Tang, Z., and Yang, H. Gtpo and grpo-s: Token and sequencelevel reward shaping with policy entropy. arXiv preprint arXiv:2508.04349, 2025. Tesauro, G. Td-gammon, self-teaching backgammon program, achieves master-level play. Neural computation, 6 (2):215219, 1994. Veeraboina, H. Aime 1983problem set URL https://www.kaggle. 2023. 2024, com/datasets/hemishveeraboina/ aime-problem-set-1983-2024. von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouedec, Q. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. 10 Wang, Y., He, H., Tan, X., and Gan, Y. Trust region-guided proximal policy optimization. Advances in Neural Information Processing Systems, 32, 2019. Wang, Y., He, H., and Tan, X. Truly proximal policy optimization. In Uncertainty in artificial intelligence, pp. 113122. PMLR, 2020. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, S., Dou, C., Guo, P., Lu, K., Ju, Q., Deng, F., and Xin, R. Dcpo: Dynamic clipping policy optimization. arXiv preprint arXiv:2509.02333, 2025b. Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., Yang, S., Wu, X., Guo, Q., et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 66726679, 2020. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zhang, H., Da, J., Lee, D., Robinson, V., Wu, C., Song, W., Zhao, T., Raja, P., Zhuang, C., Slack, D., et al. careful examination of large language model performance on grade school arithmetic. Advances in Neural Information Processing Systems, 37:4681946836, 2024. Zhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025a. Zhang, Y., Liu, Y., Yuan, H., Yuan, Y., Gu, Q., and Yao, A. C. On the design of kl-regularized policy gradient algorithms for llm reasoning. arXiv preprint arXiv:2505.17508, 2025b. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Unified Framework for Rethinking Policy Divergence Measures in GRPO A. Proof Details A.1. Equivalence and Asymmetry Analysis Theorem A.1 (Equivalence and Asymmetry). Define the lower and upper clipping ranges for given threshold δ > 0 as: lKL3 δ = min θ wt(θ) s.t. KL3t(θ) δ, uKL3 δ = max wt(θ) s.t. KL3t(θ) δ. θ (1) The constraint KL3t(θ) δ is equivalent to lKL3 (2) These ranges satisfy the asymmetry property: 1 lKL3 δ wt(θ) uKL3 δ < uKL3 δ 1. δ , where 0 < lKL3 δ < 1 < uKL3 δ . Proof. For given trust region threshold δ > 0, we aim to find the area (0 < lKL3 δ, wt(θ) [lKL3 ]. δ , uKL3 δ δ < 1 < uKL3 δ ) such that KL3t(θ)"
        },
        {
            "title": "Recall that",
            "content": "We have KL3t(θ) := wt(θ) 1 log wt(θ) wt(θ) 1 log wt(θ) = δ log wt(θ) = wt(θ) 1 δ wt(θ) = ewt(θ)1δ wt(θ)ewt(θ) = e1δ wt(θ)ewt(θ) = e1δ The Lambert function is defined as (z)eW (z) = z. Then, let wt(θ) = (z) and = e1δ, we have wt(θ) = (e1δ) The equation has two real-valued Lambert solutions corresponding to the two real branches W0 and W1 for lKL3 uKL3 δ , respectively. Specifically, we have δ lKL3 δ = W0(e1δ) (0, 1) uKL3 δ = W1(e1δ) (1, + inf) Then, we show that we always have 1 lKL3 δ < uKL3 δ 1. We have Specifically, we have KL3t(θ) =1 dwt(θ) d2 , 1 wt(θ) d(wt(θ))2 KL3t(θ) = (wt(θ))2 > 0. (cid:12) (cid:12) (cid:12) (cid:12) dwt(θ) (cid:12) (cid:12) KL3t(θ) (cid:12) (cid:12) = (cid:40) 1 wt(θ) 1, wt(θ) < 1, 1 1 wt(θ) , wt(θ) > 1, (7) (8) (9) and (10) (11) (12) We can observe that as wt(θ) 0+, the slope magnitude uniformly bounded by 1. This implies that kl3(wt(θ)) increases more rapidly for wt(θ) < 1 than for wt(θ) > 1. (cid:12) (cid:12) (cid:12) diverges to infinity, whereas for wt(θ) > 1 it is dwt(θ) KL3t(θ) (cid:12) (cid:12) (cid:12) 11 Unified Framework for Rethinking Policy Divergence Measures in GRPO Recall that we have lKL3 (cid:12) as δ wt(θ) 0+ while (cid:12) (cid:12)KL3 (cid:12) < 1 for wt(θ) > 1, the function grows faster for wt(θ) < 1 than for wt(θ) > 1. Therefore, to reach the same divergence level δ, the solution above 1 must deviate farther from 1 than the solution below 1, which implies that we always have . Since KL3t(θ) is strictly convex on (0, ) and satisfies (cid:12) < 1 < uKL3 t(θ)(cid:12) (cid:12)KL3 t(θ)(cid:12) δ 1 lKL3 δ < uKL3 δ 1. (13) A.2. Policy Logits Difference Analysis Theorem A.2 (Policy Logits Difference). Consider the policy gradient algorithm with learning rate η and given the state visitation distribution dπθold (s) induced by πθold. Let θs,a denote the policy logits difference between the ATR-based and ratio-based clipping methods. Under event X(s), we have θs,a = ηdπθold (s)πθk (as) (cid:34) AIX(s)(a) (cid:35) [AIX(s)(a)] . aπθk (s) Under event X+(s), we have θs,a = ηdπθold (s)πθk (as) AIX+(s)(a) (cid:34) (cid:35) [AIX+(s)(a)] . aπθk (s) Proof. Consider the clipped surrogate objective of ratio-based clipping: ratio(θ) := (cid:20) 1 y (cid:88) t=1 yπθold (x) xD clipratio (wt(θ), 1 ϵ, 1 + ϵ) At (cid:21) . Consider the clipped surrogate objective of ATR-based clipping: ATR(θ) := (cid:20) 1 y (cid:88) t=1 yπθold (x) xD clipratio (cid:16) wt(θ), lKL δ , uKL3 δ (cid:17) (cid:21) . At (14) (15) Following the similar proof sketch of (Park et al., 2025), for given trust region threshold δ and the corresponding ATR-based 12 Unified Framework for Rethinking Policy Divergence Measures in GRPO clipping range (lKL3 δ , uKL3 δ ), we have 1 dπθold (s) θs,a ATR(θ) =P(A > 0, lKL δ wt(θs,a) uKL3 δ ) aπθold (s) [ θs,a wt(θs,a)AA > 0, lKL3 δ wt(θs,a) uKL3 δ ] + P(A > 0, wt(θs,a) lKL3 δ ) aπθold (s) [ + P(A > 0, wt(θs,a) uKL3 δ ) aπθold (s) [ θs,a θs,a + P(A < 0, lKL3 δ wt(θs,a) uKL3 δ ) lKL3 δ AA > 0, wt(θs,a) lKL3 δ ] uKL3 δ AA > 0, wt(θs,a) uKL3 δ ] [ θs,a wt(θs,a)AA < 0, lKL3 δ wt(θs,a) uKL3 δ + P(A < 0, wt(θs,a) lKL3 δ ) δ AA < 0, wt(θs,a) lKL3 lKL3 δ ] + P(A < 0, wt(θs,a) uKL3 δ ) δ AA < 0, wt(θs,a) uKL3 uKL3 δ ] [ aπθold (s) θs,a θs,a [ aπθold (s) aπθold (s) =P(A > 0, lKL δ wt(θs,a) uKL3 δ ) aπθold (s) [ wt(θs,a)AA > 0, lKL δ wt(θs,a) uKL3 δ ] + P(A < 0, lKL3 δ wt(θs,a) uKL3 δ ) wt(θs,a)AA < 0, lKL3 δ wt(θs,a) uKL3 δ θs,a θs,a [ aπθold (s) Similarly, for the ratio-based clipping (1 ϵ, 1 + ϵ), we have ] (16) ]. 1 dπθold (s) θs,a ratio(θ) =P(A > 0, 1 ϵ wt(θs,a) 1 + ϵ) aπθold (s) [ θs,a θs,a [ aπθold (s) + P(A < 0, 1 ϵ wt(θs,a) 1 + ϵ) wt(θs,a)AA < 0, 1 ϵ wt(θs,a) 1 + ϵ]. wt(θs,a)AA > 0, 1 ϵ wt(θs,a) 1 + ϵ] Since we apply the policy gradient algorithm, and we have that θATR,k+1 s,a θk s,a =η θratio,k+1 s,a θk s,a =η θs,a θs,a ATR(θ) ratio(θ) Then, we have 1 ηdπθold (s) (θATR,k+1 s,a θratio,k+1 s,a ) =P(A > 0, lKL3 δ wt(θs,a) uKL3 δ ) aπθold (s) [ θs,a wt(θs,a)AA > 0, lKL3 δ wt(θs,a) uKL3 δ ] (17) (18) P(A > 0, 1 ϵ wt(θs,a) 1 + ϵ) aπθold (s) [ + P(A < 0, lKL3 δ wt(θs,a) uKL3 δ ) aπθold (s) [ P(A < 0, 1 ϵ wt(θs,a) 1 + ϵ) aπθold (s) [ θs,a θs,a θs,a 13 wt(θs,a)AA > 0, 1 ϵ wt(θs,a) 1 + ϵ] (19) wt(θs,a)AA < 0, lKL3 δ wt(θs,a) uKL3 δ ] wt(θs,a)AA < 0, 1 ϵ wt(θs,a) 1 + ϵ] Unified Framework for Rethinking Policy Divergence Measures in GRPO Now, we consider the first case: uKL3 δ = 1 + ϵ and 1 ϵ < lKL3 δ . In this case, we have P(A > 0, lKL δ wt(θs,a) uKL3 δ ) aπθold (s) [ θs,a wt(θs,a)AA > 0, lKL3 δ wt(θs,a) uKL3 δ ] P(A > 0, 1 ϵ wt(θs,a) uKL3 δ ) wt(θs,a)AA > 0, 1 ϵ wt(θs,a) uKL3 δ ] =P(A > 0, lKL3 δ wt(θs,a) uKL δ ) wt(θs,a)AA > 0, lKL3 δ wt(θs,a) uKL3 δ ] (20) aπθold (s) θs,a [ [ aπθold (s) θs,a θs,a θs,a θs,a aπθold (s) aπθold (s) aπθold (s) [ [ [ P(A > 0, 1 ϵ wt(θs,a) lKL3 δ P(A > 0, lKL3 δ wt(θs,a) uKL3 δ = P(A > 0, 1 ϵ wt(θs,a) lKL δ Similarly, we have ) ) ) wt(θs,a)AA > 0, 1 ϵ wt(θs,a) lKL δ wt(θs,a)AA > 0, lKL3 δ wt(θs,a) uKL3 δ wt(θs,a)AA > 0, 1 ϵ wt(θs,a) lKL3 δ ] ] ] P(A < 0, lKL3 δ wt(θs,a) uKL3 δ ) aπθold (s) [ θs,a wt(θs,a)AA < 0, lKL3 δ wt(θs,a) uKL δ ] P(A < 0, 1 ϵ wt(θs,a) uKL3 δ ) aπθold (s) [ wt(θs,a)AA < 0, 1 ϵ wt(θs,a) uKL3 δ ] (21) θs,a θs,a wt(θs,a)AA < 0, 1 ϵ wt(θs,a) lKL3 ] δ = P(A < 0, 1 ϵ wt(θs,a) lKL3 δ ) aπθold (s) [ Therefore, we have 1 ηdπθold (s) (θATR,k+1 s,a θratio,k+1 s,a ) = P(A > 0, 1 ϵ wt(θs,a) lKL3 δ P(A < 0, 1 ϵ wt(θs,a) lKL3 δ ) ) aπθold (s) [ aπθold (s) [ θs,a θs,a wt(θs,a)AA > 0, 1 ϵ wt(θs,a) lKL3 δ wt(θs,a)AA < 0, 1 ϵ wt(θs,a) lKL3 δ ] ] (22) = P(1 ϵ wt(θs,a) lKL3 δ ) aπθold (s) [ θs,a wt(θs,a)A1 ϵ wt(θs,a) lKL3 δ ] Due to we have We assume the event θs,a wt(θs,a) =I{a=a} πθk (as) πθold (as) πθk (as) πθk (as) πθold (as) X(s) ={a A(s)wt(θs,a) [1 ϵ, lKL δ ], 1 + ϵ = uKL3 δ } 14 (23) (24) Unified Framework for Rethinking Policy Divergence Measures in GRPO"
        },
        {
            "title": "Then",
            "content": "θATR,k+1 s,a θratio,k+1 s,a = ηdπθold (s)P(1 ϵ wt(θs,a) lKL3 δ ) aπθold (s) [ θs,a wt(θs,a)A1 ϵ wt(θs,a) lKL3 δ ] = ηdπθold (s) aπθold (s) [ θs,a wt(θs,a)AIX(s)(a)] = ηdπθold (s) (cid:88) (cid:2)(I{a=a}πθk (as) πθk (as)πθk (as))AIX(s)(a)(cid:3) (25) = ηdπθold (s) aA(s) (cid:34) πθk (as)AIX(s)(a) πθk (as) (cid:35) [AIX(s)(a)] aπθk (s) = ηdπθold (s)πθk (as) (cid:34) AIX(s)(a) (cid:35) [AIX(s)(a)] aπθk (s) Then, we consider the second case: lKL3 δ = 1 ϵ and 1 + ϵ < uKL3 δ . We assume the event X+(s) ={a A(s)wt(θs,a) [1 + ϵ, uKL3 δ ], 1 ϵ = lKL3 δ Similarly, in this case, we have θATR,k+1 s,a θratio,k+1 s,a =ηdπθold (s)P(1 + ϵ wt(θs,a) uKL3 δ ) aπθold (s) [ θs,a wt(θs,a)A1 + ϵ wt(θs,a) uKL3 δ ] =ηdπθold (s)πθk (as) AIX+(s)(a) (cid:34) (cid:35) [AIX+(s)(a)] aπθk (s) (26) (27) A.3. Entropy Difference Analysis Theorem A.3 (Entropy Difference). Let := H(θATR,k+1s) H(θratio,k+1s) denote the entropy difference between the ATR-based and ratio-based clipping methods. Under event X(s), we have = ηdπθold (s) Covaπθk (s) Under event X+(s), we have = ηdπθold (s) Covaπθk (s) (cid:0)AIX(s)(a), log πθk (as)(cid:1) . (cid:0)AIX+(s)(a), log πθk (as)(cid:1) . Proof. The first-order Taylor expansion of policy entropy H(θk+1s) H(θks) = aπθk (s) [(θk+1 s,a θk s,a)(log πθk (as) + H(θks))] + O((θ)2) (28) 15 Unified Framework for Rethinking Policy Divergence Measures in GRPO Consider that we have two different clipping methods with corresponding θratio,k+1 and θATR,k+1 H(θATR,k+1s) H(θratio,k+1s) = (cid:0)H(θATR,k+1s) H(θks)(cid:1) (cid:0)H(θratio,k+1s) H(θks)(cid:1) s,a)(log πθk (as) + H(θks))] + = [(θATR,k+1 s,a θk aπθk (s) aπθk (s) aπθk (s) = = aπθk (s) [(θratio,k+1 s,a θk s,a)(log πθk (as) + H(θks))] [(θATR,k+1 s,a θratio,k+1 s,a )(log πθk (as) + H(θks))] [(θATR,k+1 s,a θratio,k+1 s,a )(log πθk (as) aπθk (s) [log πθk (as)])] (29) For the event X(s), we have H(θATR,k+1s) H(θratio,k+1s) (cid:34)(cid:32) = aπθk (s) =ηdπθold (s) Cov aπθk (s) ηdπθold (s)πθk (as) (cid:34) AIX(s)(a) aπθk (s) (cid:35)(cid:33) (cid:32) (cid:33)(cid:35) [AIX(s)(a)] log πθk (as) aπθk (s) [log πθk (as)] (cid:0)AIX(s)(a), log πθk (as)(cid:1) (30) For the event X+(s), we have H(θATR,k+1s) H(θratio,k+1s) (cid:34)(cid:32) ηdπθold (s)πθk (as) = aπθk (s) = ηdπθold (s) Cov aπθk (s) (cid:34) AIX+(s)(a) (cid:35)(cid:33) (cid:32) (cid:33)(cid:35) aπθk (s) [AIX+(s)(a)] log πθk (as) aπθk (s) [log πθk (as)] (cid:0)AIX+(s)(a), log πθk (as)(cid:1) (31) Unified Framework for Rethinking Policy Divergence Measures in GRPO B. Implementation Details The hyper-parameter setting used in this work is provided in Table 4. Following recent works (Hu et al., 2025; Liu et al., 2025c), we set β = 0 for all methods. We adopt the official implementation from Unsloth (Daniel Han & team, 2023) and TRL (von Werra et al., 2020). For all methods, we perform the supervised fine-tuning on the OpenMathReasoning dataset (Moshkov et al., 2025) for formatting before the RLVR process. We provide the full codebase used to reproduce our experimental results in the supplementary material. On one single NVIDIA A100 GPU, ATR-GRPO requires an average training time of approximately 7 and 16 hours per run for Qwen3-1.7B and Qwen3-8B, respectively. For the implementation of baselines, we adopt the recommended value settings from the original papers, including DAPO (Yu et al., 2025), DCPO (Yang et al., 2025b), Clip-Cov (Cui et al., 2025), and SAPO (Gao et al., 2025). Table 4. Hyper-parameter Setting. Hyper-Parameter"
        },
        {
            "title": "Train",
            "content": "Maximum Sequence Length Lora Rank Lora Alpha Temperature Learning Rate Weight Decay Optimizer Batch Size Gradient Accumulation Steps Top-p Top-k Group Size KL coefficient β (reference policy) 2048 32 64 1.0 5e-6 1e-3 AdamW (Loshchilov & Hutter, 2017) 8 4 1.0 -1 8 0 Evaluation Temperature Max Tokens Top-p Top-k 0.3 32768 0.95 -"
        }
    ],
    "affiliations": [
        "Cohere",
        "KAUST",
        "Northwestern University",
        "University of Liverpool",
        "University of Southampton"
    ]
}