{
    "paper_title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks",
    "authors": [
        "Keanu Nichols",
        "Nazia Tasnim",
        "Yan Yuting",
        "Nicholas Ikechukwu",
        "Elva Zou",
        "Deepti Ghadiyaram",
        "Bryan Plummer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Object orientation understanding represents a fundamental challenge in visual perception critical for applications like robotic manipulation and augmented reality. Current vision-language benchmarks fail to isolate this capability, often conflating it with positional relationships and general scene understanding. We introduce DORI (Discriminative Orientation Reasoning Intelligence), a comprehensive benchmark establishing object orientation perception as a primary evaluation target. DORI assesses four dimensions of orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. Through carefully curated tasks from 11 datasets spanning 67 object categories across synthetic and real-world scenarios, DORI provides insights on how multi-modal systems understand object orientations. Our evaluation of 15 state-of-the-art vision-language models reveals critical limitations: even the best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments, with performance deteriorating for tasks requiring reference frame shifts or compound rotations. These findings demonstrate the need for dedicated orientation representation mechanisms, as models show systematic inability to perform precise angular estimations, track orientation changes across viewpoints, and understand compound rotations - suggesting limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for orientation awareness in multimodal systems, DORI offers implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments. DORI data: https://huggingface.co/datasets/appledora/DORI-Benchmark"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 9 4 6 1 2 . 5 0 5 2 : r Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks Keanu Nichols,1 Nazia Tasnim,1 Yuting Yan1 Nicholas Ikechukwu1 Elva Zou1 Deepti Ghadiyaram1 Bryan A. Plummer1 1Boston University {kmn5409, nimzia, ytyan, ncholas, elzou030, dghadiya, bplum}@bu.edu Equal contribution Figure 1: DORI captures four core dimensions of orientation reasoning intelligence: (1) objects directional alignment, (2) its orientation relative to viewers, scenes, and other objects, (3) required rotational transformation for different objectives, and (4) its natural/canonical orientation in the world. Each dimension evaluates specific perceptual abilities through visual tasks in varying settings. DORI provides holistic understanding of object orientation reasoning."
        },
        {
            "title": "Abstract",
            "content": "Object orientation understanding represents fundamental challenge in visual perception that underpins critical real-world applications like robotic manipulation and augmented reality. However, current vision-language benchmarks fail to isolate and evaluate this core capability, often conflating it with positional relationships (such as above/below or proximity between objects) and general scene understanding. To address this, we introduce DORI (Discriminative Orientation Reasoning Intelligence), comprehensive hierarchical benchmark that establishes object orientation perception as primary evaluation target. DORI rigorously assesses four essential dimensions of object(s) orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. DORI provides valuable insights on how existing multi-modal systems process and understand object orientations through carefully curated tasks from 11 datasets that spans 67 object categories across synthetic and real-world scenarios. Our evaluation of 15 state-of-the-art vision-language models using DORI reveals critical limitations: even the best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments, with performance deteriorating substantially for tasks requiring reference frame shifts or compound Preprint. Under review. rotations. These findings demonstrate the urgent need for dedicated orientation representation mechanisms in future architectures, as models show systematic inability to perform precise angular estimations, track orientation changes across multiple viewpoints, and understand compound rotationssuggesting fundamental limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for advancing orientation awareness in multimodal systems, DORI offers immediate implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments1."
        },
        {
            "title": "Introduction",
            "content": "Object orientation understanding demands complex, multi-stage processing of intrinsic object features, viewer perspective, angular relationships, and reference frame transformations [46, 54, 99, 14]. Humans master this fundamental aspect of visual cognition [21, 3] through various inherent sensorymotor experiences, proprioceptive integration and neural formation [95]. In the human cognitive system, these mechanisms develop progressively from basic frontal orientation recognition to complex rotational transformations [96, 74, 98]. This equips us with crucial aptitude for real-world visual tasks that require precise spatial interaction with the environment, such as tool manipulation and navigation. Many artificial intelligence applications still require human-like sophisticated orientation understanding capabilities. For example, autonomous vehicles must determine which way objects are facing for effective navigation [37, 50]. Similarly, augmented reality applications require alignment of virtual objects with physical ones [78, 82, 104], and robotic grasping requires understanding an objects orientation to determine approach angles [22, 17]. Multimodal large language models (MLLMs) [7, 103, 94] are attractive for many of these applications as they perform well across various vision-language tasks [44, 65, 82, 78, 108, 39]. Prior works have extensively studied this capability and found that existing MLLMs, in general, do not perform well on object orientation tasks [92, 58, 18, 15, 70, 57]. However, these benchmarks only evaluate limited set of orientation questions. For example, some focus only on simple directional judgments without testing fine-grained rotational understanding [109, 87], do not represent the naturality or nuances of real-life [106, 62], some do not present task context in clear, unambiguous way [77, 76], others fail to systematically evaluate orientation across different frames of reference [53], or have extremely few samples [35]. These limitations lead to an incomplete assessment of models orientation reasoning abilities, potentially resulting in over-optimistic performance estimates, failure to identify critical weaknesses in real-world scenarios, and inability to distinguish between true geometric understanding versus memorized patterns or statistical shortcuts. To address these shortcomings, we introduce DORI (Discriminative Orientation Reasoning Intelligence), human cognitive study-informed benchmark to evaluate object orientation understanding in multimodal language models. As summarized in Fig. 1, we decompose evaluation of this critical ability into four fundamental dimensions with progressive complexity: (1) frontal alignment perception, (2) rotational transformations, (3) ego and allocentric relative orientation understanding, and (4) natural or canonical orientation of objects. Furthermore, for holistic view of given models performance, we employ two-tiered assessment framework coarse-grained questions to evaluate basic categorical understanding (e.g., is the car facing toward or away from the camera?\") and fine-grained questions to probe precise metric relationships (e.g., at what angle is the car oriented relative to the camera?). DORI leverages 13652 images from 11 existing computer vision datasets to generate 33,656 multiple-choice questions, combining real-world images (37%) with simulated renders (63%) to ensure we have large dataset with varying levels of visual complexity. We generate clear, unambiguous prompt-answer pairs through rigorous three-step process: (1) isolating objects in question with bounding boxes to tackle cluttered scenes, (2) employing standardized orientation terminology (e.g., frontal alignment) with explicit spatial frames of reference (e.g., egocentric, allocentric, and object-centric), examples, and task descriptions, and (3) ensuring difficulty progression from simple categorical judgments to precise angular measurements across all orientation dimensions. This systematic approach isolates orientation perception from broader scene perception skills, minimizes confounding factors such as object recognition difficulty, scene clutter, linguistic ambiguity, and contextual distractions that plague existing benchmarks [19, 15, 59]. 1DORI data: https://huggingface.co/datasets/appledora/DORI-Benchmark Table 1: Characteristics of different datasets for orientation reasoning Asterisks (*) indicate that the reported number of classes, sources, or samples refers specifically to orientation-related questions or tasks, rather than the full dataset. DORI provides large and diverse study of orientation. Dataset Obj. Cls. Type Granularity View Sources Images VQA Pairs Spatial-MM [88] ScanQA [4] BLINK [36] KiVA [120] EgoOrientBench [53] EmbSpatial-Bench [30] CLEVR-3D [114] 3DSRBench [73] PO3D-VQA [105] SR-Bench [90] Spatial457 [107] DORI (Ours) 342 20 71* 62* 197 294 160 7* 5 197 5*"
        },
        {
            "title": "Coarse\nCoarse Allocentric\nBoth\nCoarse\nAllocentric\nFine\nEgocentric\nCoarse\nCoarse\nEgocentric\nCoarse Allocentric\nCoarse Allocentric\nCoarse Allocentric\nEgocentric\nCoarse\nCoarse Allocentric\nBoth",
            "content": "Both 1 1 2 2* 5 3 2 1* 1 1* 1 11 692 695 7,445* 800 552* 552* 600* 400* 33,460 8365 2,435* 2181 2320* 1044* 2073* 6,188* 30k 300k* 400* 992* 13,652 400* 4,555* 33,656 Our extensive experiments with 15 state-of-the-art MLLMs on DORI reveal several key findings: Models perform on average 30% worse on complex, dynamic rotational tasks that require mental tracking of object rotations between images, compared to simple, static orientation tasks (e.g., identifying current object poses) Models particularly struggle on tasks requiring perspective shifts (e.g., adapting viewpoints different from the camera, such as determining if two objects are facing each other when viewed from their own frame of reference), showing 25% drop in accuracy compared to egocentric frame tasks Token-based integration approaches (like Mantis-Idefics2-8B) consistently outperform linear projection methods in orientation tasks, indicating that architectural design significantly impacts orientation reasoning capabilities Model scale alone does not guarantee better orientation understanding; smaller, dialogue-tuned variants (e.g., DeepSeek-1.3B-Chat) often outperform larger base models (e.g., DeepSeek-7B-Base), highlighting the importance of training objectives over parameter count"
        },
        {
            "title": "2 Related Work",
            "content": "Tab. 1 compares our proposed benchmark, DORI, to existing datasets for evaluating MLLMs with orientation questions (e.g., BLINK [36], 3DSRBench [73], ScanQA [4], Spatial457 [107], SpatialEval [102], and SpatialRGPT-Bench [19]). These datasets only provide limited evaluation of orientation with small scales or only basic questions (e.g., is the object facing left or right?), with most of their questions focusing on spatial reasoning tasks instead. In contrast, DORI introduces comprehensive framework that isolates orientation perception through carefully designed tasks spanning four core aspects. In particular, DORI contains larger number of samples from at least twice as many sources, and has large variety of objects, question granularity, and views. For example, PO3D-VQAs [105] evaluates MLLMs ability to understand eight cardinal directions with tolerance, and EgoOrientBenchs [53] discrete orientation classes discard crucial continuous rotational information, resulting in identical scoring for models with significantly different error margins. DORI addresses this by introducing hierarchical evaluation through coarse to granular QA pairs. Unlike some datasets that rely only on simulated data in their benchmarks [105, 120], DORI contains simulated and natural images to provide diverse dataset. Another closely related topic is pose estimation [26, 29, 122], where models are often tasked with identifying set of keypoints to provide detailed accounting of an objects orientation. However, these do not generalize well to MLLMs as they are expensive to annotate (e.g., for every object you have to decide on set of keypoints and then annotate many examples of each), and efficiently predicting keypoints requires specialized models and techniques [24, 32, 83]. In addition, having keypoints does not directly translate into an easily human-interpretable format. For example, the upper-left question of Fig. 1, where model is asked if the object is facing the camera, keypoint estimation model would require complex function to map the pose into the direction it is facing as 3 there are many poses that human annotators would consider is facing forward. This is important since MLLM may be given language-based instruction rather than detailed pose information at test time. There are also different 3D poses that map into the same 2D positions [6, 8], and thus require even more expensive 3D keypoint information to disambiguate. Instead, for orientation questions we use natural image datasets like COCO [63] and KITTI [40], and annotate the correct answer to better evaluate the desired capabilities of MLLMs with more scalable processes."
        },
        {
            "title": "3 DORI: Discriminative Orientation Reasoning Intelligence",
            "content": "Evaluating models understanding of object orientation mandates closer inspection of several fundamental research questions: what core orientation perception capabilities do current MLLMs possess (or lack) compared to humans? Do MLLMs follow similar progression patterns to human cognitive development in orientation understanding, i.e., first master basic frontal object recognition before developing the complex ability to mentally rotate objects and transform frame of reference [9, 98, 89]? Which architectural components or training paradigms in current MLLMs contribute to their object orientation reasoning gaps? What specific training strategies might bridge these gaps to make MLLMs approach human-like orientation understanding? These critical questions guide the task structure, data sample selection, and evaluation probe development in DORI to ensure that we effectively dissect the orientation reasoning capabilities of MLLMs. In this work, we aim to evaluate MLLMs understanding of object orientation across both static scenarios and dynamic manipulations in both natural and simulated settings. This distinction is crucial as it mirrors the progression from simple to complex spatial reasoning observed in human cognitive development. Below, we first describe the four aspects of object orientation comprehension, and then proceed to detail our benchmark creation process. 3.1 Core Capabilities Drawing on the established frameworks in cognitive neuroscience (e.g., [100, 47, 46]), DORI decomposes object orientation comprehension into four fundamental dimensions that reflect distinct neural and cognitive processes humans employ when reasoning about an objects orientation. 1. Frontal Alignment evaluates the fundamental ability to perceive how an objects front-facing surface is oriented relative to the viewera prerequisite for any orientation-based reasoning. Humans rapidly identify which way an object is facing (e.g., deciding if vehicle is approaching or departing) by recognizing structural and functional features such as faces, headlights, or entrances. However, we require additional reasoning steps for orientation interpretation [55], such as assessing objects angular relationship with the viewing plane. For MLLMs, we assess this frontal alignment capability through two complementary tasks: view parallelism analysis, which quantifies the degree to which an objects frontal surface deviates from being parallel to the image plane, providing angular measurements (e.g., is chair directly facing the camera or at 45-degree angle). Directional facing perception asks the cardinal direction an objects front is oriented relative to the camera position (e.g., whether desk is facing toward, away, leftward, or rightward from the viewers perspective). This dual assessment is supported by research that reports viewpoint-invariant recognition of frontal features operates through different neural mechanisms than precise angular estimation [46]. Prior work also demonstrates MLLMs inability to perceive object frontality (e.g., confusing left/right perspectives) even when provided with bounding boxes [93, 88], often being on par with random predictions [119]. 2. Rotational Transformation examines the ability of an MLLM to comprehend orientation changes through rotation, reflecting requirements such as embodied object manipulation (e.g., fitting key into lock), and viewpoint-dependent navigation (e.g., reorienting rotated map during wayfinding). Mental rotation capabilities allow humans to predict how objects align when rotated [79], with neuroimaging evidence showing premotor and parietal activation during both physical and imagined rotations [28, 79]. Both processes trigger similar neural activations [112], highlighting the inherent complexity of spatial transformation tasks that MLLMs must simulate. Inspired by this, we design the rotational tasks in our benchmark to progress from simple to complex levels mirroring human cognitive processing demands [60, 91, 112]. Thus, the first subtask examines single-axis rotation and evaluates basic angular transformations rotated along one spatial dimension (e.g., determining the shortest rotation to face chair toward the camera). This establishes baseline capabilities before progressing to the more cognitively demanding compound rotation [91], which involves multipleaxes rotation (e.g., aligning objects through sequence of horizontal/vertical rotations). These subtasks represent common scenarios in assembling products through item/tool manipulation and real-world scene planning by embodied agents, where object orientation understanding directly impacts overall task performance. 3. Relative Orientation examines the understanding of how objects are oriented in relation to each other and with respect to the viewer. Humans navigate complex visual world by seamlessly tracking orientation changes across scenes and viewpoints, which is crucial for scene comprehension and geometric coordination. The human brain contains specific interconnected region facilitating mental orientation -the ability to effectively spatially orient an object to different viewpoints and perspectives [81, 72, 43, 121]. In contrast, studies have shown that MLLMs struggle significantly with questions posed from non-egocentric perspectives [88, 104, 118] and with maintaining consistent dimensional relationships between multiple objects across time and viewpoints [116] We systematically probe this aspect of objects relative orientation via the following sub-tasks: (1) inter-object directional relationships, to assess the relative facing directions of objects (e.g., determine if two cars are facing the same or opposite directions), and (2) image-pair rotational relationships, to measure the ability to track orientation changes between two images (e.g., identify the degree of rotation between two views of the same object). 4. Canonical Orientation Perception evaluates the ability to recognize when objects deviate from their expected orientations, and to determine what transformations would restore them to their canonical state (e.g., identifying that flipped image of building is upside-down and needs 180degree rotation to appear normal). Humans possess an innate ability to predict and understand the physical properties of objects and their interaction through specialized neural processing [5, 75, 46] and use functional cues for inferring canonical orientation, such as gravity alignment [33], functional feature positioning, and ecological validity [10, 45]. Prior work shows that MLLMs struggle to reason with intuitive physics [11, 51]. In our benchmark, we break this complex task into two sub-tasks: first, assess the ability to identify deviations from canonical orientation across object categories, then evaluate models ability to determine the specific geometric operations (rotation, flipping, or combinations) required to restore the object to its canonical orientation. 3.2 Benchmark Creation Process As illustrated in Fig. 1, DORI contains seven carefully designed orientation-reasoning tasks. Each multiple-choice question has two assessment level: coarse-grained questions for basic categorical judgments and fine-grained questions for precise angular measurements. This hierarchical approach enables systematic evaluation from fundamental perception to advanced orientation reasoning. Our data was collected via two primary means: converting existing 3D information to orientation questions (sampled from JTA [31], 3D-Future [34], Get3D [38], ShapeNet [13], OmniObject3D [111], NOCS REAL [101], Objectron [1], COCO Space SEA [63, 97, 42, 69, 110, 80], and the OminNOCS [56] subsets of KITTI [40] and Cityscapes [23]) or manually annotating samples (collected for COCO [63]). Each question type only contains some of the dataset sources (discussed further in Sec. 3.3). For example, Inter-object direction perception requires at least two objects in scene, so we did not create questions using ShapeNet [13], which contains single objects, and instead created questions with 3D-Future [34] and NOCS REAL [101]. NOCS REAL itself contains 6 DoF information about objects which we converted into our questions. We provide detailed description of the process used to create questions for each dataset in the supplementary. We designed DORIs evaluation prompts using systematic, human-centered approach to isolate orientation perception from confounding factors (e.g. object recognition difficulty, scene clutter, linguistic ambiguity, and contextual distractions) [12]. As highlighted in Fig. 2, the prompts follow carefully structured format with five key components: (1) concise task description specifying the orientation dimension being tested, (2) contextual information explaining relevant orientation concepts (e.g., \"An object is considered front facing when its inherent structural features are visible from the camera\"), (3) step-by-step analysis instructions (e.g. \"1. Identify Object and its key structural features 2. Determine the orientation...\")(4) multiple-choice options, and (5) concrete examples illustrating expected reasoning (e.g. \"A person whose body is directed towards the camera would be facing toward the camera\"). This structured approach was inspired by effective instruction-tuning datasets like LLaVAs, which demonstrate that explicit task framing and example-driven guidance significantly improve model comprehension [48]. We iteratively refine our prompts through multiple 5 Figure 2: Structured Prompt Design in DORI. The five key components are: informatask description, contextual tion, step-by-step instructions, multiplechoice options, and examples. The structured format ensures consistent evaluation of orientation perception abilities. Figure 3: (a) The DORI benchmark embodies seven distinct orientation tasks (inner circle) with balanced and systematic distribution of samples across both natural and simulated images. (b) DORI captures diverse objects commonly encountered in day-to-day life, supporting comprehensive analysis of models orientation understanding capabilities. cycles of human feedback from non-expert annotators, address ambiguities, clarify terminology, and improve the task specificity [64]. For instance, early versions of rotational transformation prompts yielded inconsistent interpretations of rotation axes. Human evaluators helped us incorporate more precise language and visual references (e.g., \"like ballerina spinning clockwise\" to illustrate vertical axis rotation versus abstract directional descriptions clearly). This iterative refinement process ensured that the MLLMs performance differences genuinely reflect their object orientation understanding capabilities rather than prompt interpretation challenges. Additionally, we standardized response formats (requiring explicit answer selections and reasoning explanations) to facilitate consistent evaluation while providing insight into models reasoning processes. We provide comprehensive examples of each of our 14 question prompts and more details of how we created questions for each task in the supplementary. To ensure comprehensive coverage of object orientation reasoning, we developed two-tiered question framework across four orientation dimensions. Coarse-grained questions: These evaluate basic categorical understanding (e.g., \"Has the object rotated between the two images?\" for rotational transformation, or \"Are objects and facing the same direction?\" for relative orientation) and provide foundation for assessing fundamental orientation perception. Fine-grained questions: These probe precise quantitative estimations (e.g., \"How many degrees clockwise did the object rotate?\" for rotational transformation, or \"What specific transformations would restore this image to its canonical orientation?\" for canonical orientation perception), requiring detailed metric understanding of angular relationships. 3.3 DORI Statistics Collectively, DORI encompasses 13, 652 images spanning both natural (37%) and simulated (63%) environments, has 33, 656 carefully constructed multiple-choice questions. The benchmark covers 67 object categories ( 31 household and 36 outdoor item categories) across 11 diverse computer vision datasets including KITTI [40], Cityscapes [23], COCO [63], JTA [31], 3D-FUTURE [34], Objectron [1], ShapeNet [13], and OmniObject3D [111], amoung others. Fig. 3 illustrates the object 6 Table 2: Performance of several open-source MLLMs on DORI. Most models perform poorly, particularly on granular questions. These experiments reveal systemic gap in object orientation understanding across all four dimensions studied in DORI. C: coarse, G: granular."
        },
        {
            "title": "Frontal\nAlignment",
            "content": "View Parallel. Dir. Facing Rotational Transformation Singleaxis Rot. Compound Rot. Relative Orient. InterObj. Dir. Viewerscene Dir."
        },
        {
            "title": "Canonical",
            "content": "Orient. Avg."
        },
        {
            "title": "C G C G C G C G C G C G C",
            "content": "G"
        },
        {
            "title": "C G",
            "content": "35.7 25.5 20.2 19.8 20.7 17.4 20.4 6.6 15.0 10.1 33.1 20.3 34.8 16.0 25.7 16.5 Random LLaVA-v1.6-13B [67] 55.9 26.8 22.6 19.8 23.4 12.9 32.8 10.6 7.2 12.4 39.8 20.1 16.7 11.3 28.3 16.7 LLaVA-v1.6-34B [67] 52.8 35.3 32.6 26.4 22.1 26.2 13.0 4.3 16.2 14.8 30.5 25.4 9.9 11.6 25.3 20.6 LLava-Next-8B [68] 33.2 9.0 21.3 21.0 20.5 17.9 40.6 6.3 10.3 12.0 61.5 25.2 10.5 9.2 28.3 14.4 33.7 31.0 16.9 25.2 28.5 14.7 3.8 3.0 15.2 12.3 15.8 10.8 6.0 14.4 17.1 15.9 Yi-VL-6B [2] 53.1 35.1 23.3 24.0 28.1 21.2 25.9 4.4 13.4 14.5 44.7 19.7 11.3 12.5 28.5 18.7 Yi-VL-34B [2] 60.1 24.3 26.1 16.3 15.4 20.9 9.1 5.8 13.2 10.1 37.7 17.9 23.7 34.5 23.8 15.0 Mantis-CLIP [52] 57.8 33.0 22.5 12.7 25.7 23.4 25.9 6.6 17.6 9.0 55.4 24.5 48.8 41.0 34.5 17.5 Mantis-Idfs-8B [52] 58.1 24.8 15.0 19.4 21.1 15.2 17.0 5.3 17.0 11.9 61.3 26.0 2.5 2.3 29.3 14.7 DS-1.3B-Base [7] 58.1 24.8 22.6 22.0 21.2 15.5 33.9 5.8 15.3 10.5 47.6 18.0 29.2 17.3 33.0 14.1 DS-1.3B-Chat [7] 26.3 31.1 14.6 16.4 18.3 14.3 35.5 2.8 3.6 4.1 35.7 6.8 31.3 10.7 24.7 9.6 DS-7B-Base [7] 59.4 31.3 31.3 24.5 28.2 17.8 35.8 6.0 20.2 14.9 18.5 32.2 15.2 32.2 29.4 18.6 DS-7B-Chat [7] category and dataset distribution. This multi-source approach allows us to balance complex, realworld environments (37% of images) with controlled synthetic environments (63%) where orientation parameters are precisely known. Knowing precise orientation parameters in synthetic data is crucial as it provides ground truth angular measurements with known accuracy, eliminating visual ambiguity or occlusion that might confound assessment. Meanwhile, real-world images introduce natural complexity and diversity while maintaining clear ground truth through expert annotation. Additional examples and discussion can be found in the supplementary."
        },
        {
            "title": "4 Experiments and Results",
            "content": "We evaluate 15 state-of-the-art multimodal models spanning diverse architectures, parameter scales, and pretraining methodologies across both open-source and proprietary systems. The LLaVA family [67] employs CLIP ViT visual encoders [84] connected to Vicuna language models [20] via linear projections, offering strong vision-language capabilities , while LLaVA-Next [68] extends this architecture with mixture-of-experts techniques. Yi-VL [2] adopts similar approach with modified pretraining objectives. On the other hand, the Mantis family of models [52] represent distinct tokenbased integration approach with variants using either lightweight CLIP-based systems (Mantis-CLIP) or sophisticated Idefics architectures (Mantis-Idefics2-8B). DeepSeek variants (1.3B/7B in both base and chat-tuned versions) [7] utilize hybrid SigLIP and ViTDet encoder with DeepSeek-MoE language models, allowing us to isolate the effects of both scale and dialogue tuning on orientation reasoning. Among proprietary systems, we evaluated Gemini (1.5 Pro, 2.0 Flash) and GPT-4 variants (One and Omni), which implement advanced but undocumented multimodal fusion techniques. We evaluate the proprietary models via their official APIs and open-source models on single NVIDIA RTX A6000 GPU, with the 34B parameter models being evaluated on single NVIDIA A100-80G. Overall results: We evaluate models on accuracythe percentage of correct answers chosen from multiple optionsacross both coarse and granular question types. All models were prompted with the standardized task descriptions and prompted to provide both an answer selection and reasoning explanation. We report accuracy performance on open-source MLLMs on DORI in Tab. 2. We note that some models, especially smaller ones, perform worse than random. Overall, as expected, most models struggle more with the challenging granular questions than with coarse ones. This indicates that current models cannot perform precise angular reasoning, instead relying on coarse categorical judgments. Larger models of each model family generally perform better on average, particularly on coarse questions. Larger models of each model family generally perform better on average, particularly on coarse questions. Closed-source models in Tab. 3 obtain 13-26% better performance than the best open-source models. However, particularly on rotational transformation and 7 Table 3: Performance of proprietary vs. open-source MLLMs on 100 randomly sampled questions across all four dimensions and granularities. Commercial models also struggle with complex orientation tasks, particularly with angular precision and reference frame shifts. C: coarse, G: granular."
        },
        {
            "title": "Frontal\nAlignment",
            "content": "View Parallel. Dir. Facing Rotational Transformation Singleaxis Rot. Compound Rot. Relative Orient. InterObj. Dir. Viewerscene Dir."
        },
        {
            "title": "Canonical",
            "content": "Orient. Avg."
        },
        {
            "title": "C G C G C G C G C G C G C",
            "content": "G"
        },
        {
            "title": "C G",
            "content": "LLaVA-v1.6-34B 45.5 30.0 22.5 23.5 15.0 15.5 19.3 7.6 15.0 10.5 37.3 34.3 17.0 19.0 24.5 20.0 LLava-Next-8B 34.0 13.0 18.0 19.0 12.5 17.5 40.3 7.0 12.0 13.5 64.0 25.0 17.0 16.0 28.2 15.8 52.5 28.0 24.5 24.5 21.0 15.0 35.3 5.0 24.0 14.0 23.6 32.0 16.0 2.0 28.1 19.7 DS-7B-Chat 68.5 53.0 43.5 39.0 24.0 37.5 65.3 12.3 25.0 14.5 91.0 47.3 22.0 28.0 54.2 33.0 Gemini 1.5 Pro Gemini 2.0 Flash 67.0 35.0 37.5 33.5 28.5 27.0 52.0 14.3 23.0 17.0 92.0 43.6 5.0 29.0 49.9 28.5 44.5 45.0 30.5 35.5 29.5 34.0 39.3 15.3 23.5 13.5 88.3 53.3 19.0 45.0 19.0 45.0 GPT-4 48.5 41.0 39.0 40.5 32.5 42.0 31.6 14.0 22.5 8.5 87.6 44.6 20.0 46.0 40.2 33.8 GPT-4-1 relative orientation questions, all closed-source models continue to struggle significantly, highlighting substantial room for improvement even in state-of-the-art commercial systems. These results suggest that current architectures lack robust continuous geometric representations and instead rely on discretized categorical approximations of orientation. This is evidenced by the consistent performance gap between coarse categorical questions (where models perform relatively better) and granular questions requiring precise angular estimations (where performance drops significantly). We hypothesize this limitation stems from how MLLMs are pretrained - most models being evaluated use CLIP-style contrastive objectives that optimize for high-level image-text semantic alignment rather than core geometric understanding [123]. Previous studies [93] also support this claim, that pretraining creates dimensional collapse in the embedding space, where continuous orientation variations become compressed into discrete semantic clusters (e.g., treating left and right as opposing categorical concepts rather than points along continuous angular spectrum). This explains why models can often distinguish between categorical extremes but fail on tasks that require precise angular discrimination. While this limitation may be slightly mitigated by using generative objectives [61], MLLMs lack the necessary equivalent neural inductive biases utilized by humans [85, 66, 25, 41]. This disembodied training regime forces MLLMs to approximate the neural mechanisms through suboptimal attention patterns leading to hallucinations [49, 113, 27, 115]. Architecture impact. Among open-source systems in Tab. 2, clear architectural pattern emerges: token-based approaches like Mantis-Idefics2-8B outperform linear projection methods, suggesting that token-level integration preserves richer dimensional information. Notably, our results contradict the assumption that larger models necessarily perform better on orientation tasks. DeepSeek-1.3BChat (33.0% average accuracy across all coarse tasks) significantly outperforms DeepSeek-7B-Base (24.7% average coarse accuracy) despite having fewer parameters. This pattern highlights that architectural choices and training objectives play more critical role in orientation reasoning than parameter count alone. We observe that dialogue-tuned variants consistently outperform their base counterparts (e.g., DeepSeek-7B-Chat vs. DeepSeek-7B-Base), suggesting that instruction tuning, which emphasizes structured, logical reasoningenhances models ability to follow multi-step orientation reasoning instructions and produce coherent judgments [86]. Overall, performance clusters tend to align with model families, with Gemini models consistently outperforming others, suggesting architectural advantages in their design. These findings also indicate that improving orientation reasoning requires careful consideration of how to combine information from multiple modalities, instead of just scaling models. Performance across data sources and object categories: Fig. 4 reveals that models generally perform better on orientation tasks involving people and animals, presumably because these categories have clear front/back distinctions through faces compared to more ambiguous objects like furniture or containers. This pattern may also suggest that current models rely heavily on semantic features (e.g., recognizing faces) rather than fundamental geometric understanding when determining object orientation. Common datasets like COCO [63] and Cityscapes [23] are large scale annotated datasets widely used to train and evaluate performance of models [71, 16, 117]. Thus, it would be expected 8 Figure 4: Our evaluation on 15 MLLMs across (a) 8 broad categories and (b) 11 datasets suggest that while some models perform modestly on \"Person\" and \"Animals\" broad categories, and up to 70% on datasets like KITTI [40], MLLMs struggle with several objects, revealing limitations in robust orientation understanding. Table 4: Comparison of Human performance vs. MLLMs on 30 randomly sampled questions of each type. We find significant performance gap exists between our expert annotators and MLLMs Frontal Alignment View Parallel. Dir. Facing Rotational Transformation Singleaxis Rot. Compound Rot. Relative Orient. InterObj. Dir. Viewerscene Dir. Canonical Orient. Avg. G G G G GPT-4 43.3 56.6 46.6 26.6 33.3 23.3 69.9 46.6 36.6 43.3 89.9 70.0 20.0 46.6 48.5 44.7 Gemini 1.5 Pro 73.7 70.0 73.0 86.5 49.9 60.0 96.6 53.0 56.6 33.3 100.0 63.3 26.6 66.6 68.0 61.8 83.3 86.6 93.3 76.6 70.0 70.0 76.6 83.3 83.3 70.0 100.0 93.3 100.0 86.7 86.6 80.9 HUMAN that model performance is bit higher on these datasets due to potential train/test distribution overlap, as these images may be similar to those seen during pretraining. However, we see that COCOs performance is much higher compared to Cityscapes, despite COCO having higher number of object classes. This observation may be attributed to the noise introduced into Cityscapes since it is typically used for self-driving cars for tasks like object detection and depth estimation in urban scenes. This can involve several objects being present in scene, ranging from cars to persons to bikes."
        },
        {
            "title": "5 Conclusion",
            "content": "DORI reveals critical limitations in current MLLMs orientation understanding capabilities, with even state-of-the-art models achieving only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments and perform significantly below human performance. Our comprehensive evaluation demonstrates that MLLMs systematically falter when tasks require precise angular estimations, multi-axis rotational transformations, or perspective shifts beyond egocentric frames. These deficiencies likely stem from fundamental architectural constraints that compress continuous geometric information into discrete semantic categories rather than developing true geometric representations. As the first diagnostic framework specifically targeting orientation awareness, DORI provides clear metrics for measuring progress and establishes specific pathways for advancing multimodal systems toward the robust orientation reasoning capabilities essential for real-world applications in robotics, autonomous navigation, augmented reality, and embodied AI. The results strongly suggest that future architectures must develop specialized mechanisms for continuous geometric representation to bridge this critical gap in machine perception."
        },
        {
            "title": "References",
            "content": "[1] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron: large scale dataset of object-centric videos in the wild with pose annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 78227831, 2021. [2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. [3] M. Alomari, F. Li, D. Hogg, and A. G. Cohn. Online perceptual learning and natural language acquisition for autonomous robots. Artificial Intelligence, 303:103637, 2022. [4] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Cécile Ballaz, Luc Boutsen, Carole Peyrin, Glyn Humphreys, and Christian Marendaz. Visual search for object orientation can be modulated by canonical orientation. Journal of Experimental Psychology: Human Perception and Performance, 31(1):20, 2005. [6] Axel Barroso-Laguna, Sowmya Munukutla, Victor Adrian Prisacariu, and Eric Brachmann. Matching 2d images in 3d: Metric relative pose from metric correspondences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 48524863, 2024. [7] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [8] Benjamin Biggs, Sébastien Ehrhart, Hanbyul Joo, Benjamin Graham, Andrea Vedaldi, and David Novotny. 3D multibodies: Fitting sets of plausible 3D models to ambiguous image data. In NeurIPS, 2020. [9] Mark Blades and Christopher Spencer. The development of childrens ability to use spatial representations. Advances in child development and behavior, 25:157199, 1994. [10] Neil Bramley, Tobias Gerstenberg, Joshua Tenenbaum, and Todd Gureckis. Intuitive experimentation in the physical world. Cognitive psychology, 105:938, 2018. [11] L. M. S. Buschoff, E. Akata, M. Bethge, and E. Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, 7:96106, 2025. [12] Courtney Castle. Using design thinking to create human-centered assessments. In NERA Conference Proceedings 2024, 2024. [13] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. [14] Robert Chavez, Taylor Guthrie, and Jack Kapustka. Independent allocentric and egocentric reference frame encoding of person knowledge within the brains of social groups. bioRxiv, pages 202309, 2023. [15] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1445514465, 2024. [16] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. [17] Ya-Ling Chen, Yan-Rou Cai, and Ming-Yang Cheng. Vision-based robotic object graspinga deep reinforcement learning approach. Machines, 11(2):275, 2023. [18] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. 10 [19] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2025. [20] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. [21] A. G. Cohn. Qualitative spatial representation and reasoning techniques. KI-97: Advances in Artificial Intelligence, pages 130, 1997. [22] Yang Cong, Ronghan Chen, Bingtao Ma, Hongsen Liu, Dongdong Hou, and Chenguang Yang. comprehensive study of 3-d vision-based robot manipulation. IEEE Transactions on Cybernetics, 53(3): 16821698, 2021. [23] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. [24] Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, and Fabio Poiesi. Open-vocabulary object 6d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1807118080, 2024. [25] Benoit Delhaye, Katie Long, and Sliman Bensmaia. Neural basis of touch and proprioception in primate cortex. Comprehensive Physiology, 8(4):1575, 2018. [26] Qianyi Deng, Oishi Deb, Amir Patel, Christian Rupprecht, Philip Torr, Niki Trigoni, and Andrew Markham. Towards multi-modal animal pose estimation: survey and in-depth analysis, 2025. [27] Xinke Deng, Yu Xiang, Arsalan Mousavian, Clemens Eppner, Timothy Bretl, and Dieter Fox. Selfsupervised 6d object pose estimation for robot manipulation. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 36653671. IEEE, 2020. [28] Naz Doganci, Giannina Rita Iannotti, Sélim Yahia Coll, and Radek Ptak. How embodied is cognition? fmri and behavioral evidence for common neural resources underlying motor planning and mental rotation of bodily stimuli. Cerebral Cortex, 33(22):1114611156, 2023. [29] Guoguang Du, Kai Wang, Shiguo Lian, and Kaiyong Zhao. Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: review. Artif. Intell. Rev., 54(3):16771734, 2021. [30] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. EmbSpatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 346355, Bangkok, Thailand, 2024. Association for Computational Linguistics. [31] Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea Palazzi, Roberto Vezzani, and Rita Cucchiara. Learning to detect and track visible and occluded body joints in virtual world. In Proceedings of the European conference on computer vision (ECCV), pages 430446, 2018. [32] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael J. Black. Chatpose: Chatting about 3d human pose. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20932103, 2024. [33] Hongbo Fu, Daniel Cohen-Or, Gideon Dror, and Alla Sheffer. Upright orientation of man-made objects. In ACM SIGGRAPH 2008 Papers, New York, NY, USA, 2008. Association for Computing Machinery. [34] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:33133337, 2021. [35] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [36] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [37] Manikandan Ganesan, Sivanathan Kandhasamy, Bharatiraja Chokkalingam, and Lucian Mihet-Popa. comprehensive review on deep learning-based motion planning and end-to-end learning for self-driving vehicle. IEEE Access, 2024. [38] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems, 35:3184131854, 2022. [39] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1246212469, 2024. [40] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 33543361. IEEE, 2012. [41] Daniel Goble, James Coxon, Annouchka Van Impe, Monique Geurts, Wim Van Hecke, Stefan Sunaert, Nicole Wenderoth, and Stephan Swinnen. The neural basis of central proprioceptive processing in older versus younger adults: an important sensory role for right putamen. Human brain mapping, 33(4): 895908, 2012. [42] Camille Gontier, Jakob Jordan, and Mihai Petrovici. Delaunay: dataset of abstract art for psychophysical and machine learning research. arXiv preprint arXiv:2201.12123, 2022. [43] Klaus Gramann, Julie Onton, Davide Riccobon, Hermann Mueller, Stanislav Bardins, and Scott Makeig. Human brain dynamics accompanying use of egocentric and allocentric reference frames during navigation. Journal of cognitive neuroscience, 22(12):28362849, 2010. [44] Tianrui Guan, Yurou Yang, Harry Cheng, Muyuan Lin, Richard Kim, Rajasimman Madhivanan, Arnie Sen, and Dinesh Manocha. LOC-ZSON: language-driven object-centric zero-shot object retrieval and navigation. CoRR, abs/2405.05363, 2024. [45] Jessica Hamrick, Peter Battaglia, and Joshua Tenenbaum. Internal physics models guide probabilistic judgments about object dynamics. In Proceedings of the 33rd annual conference of the cognitive science society. Cognitive Science Society Austin, TX, 2011. [46] Irina Harris. Interpreting the orientation of objects: cross-disciplinary review. Psychonomic Bulletin & Review, 31(4):15031515, 2024. [47] I. M. Harris, J. A. Harris, and M. C. Corballis. Binding identity and orientation in object recognition. Attention, Perception, & Psychophysics, 82:153167, 2019. [48] Michael Hewing and Vincent Leinhos. The prompt canvas: literature-based practitioner guide for creating effective prompts in large language models. arXiv preprint arXiv:2412.05127, 2024. [49] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Gong. Visual hallucinations of multi-modal large In Findings of the Association for Computational Linguistics: ACL 2024, pages language models. 96149631, Bangkok, Thailand, 2024. Association for Computational Linguistics. [50] Joel Janai, Fatma Güney, Aseem Behl, Andreas Geiger, et al. Computer vision for autonomous vehicles: Problems, datasets and state of the art. Foundations and trends in computer graphics and vision, 12 (13):1308, 2020. [51] Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, and Elia Bruni. Grasp: novel benchmark for evaluating language grounding and situated physics understanding in multimodal In Proceedings of the Thirty-Third International Joint Conference on Artificial language models. Intelligence, IJCAI-24, pages 62976305. International Joint Conferences on Artificial Intelligence Organization, 2024. Main Track. [52] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. Transactions on Machine Learning Research, 2024, 2024. [53] Ji Hyeok Jung, Eun Tae Kim, Seo Yeon Kim, Joo Ho Lee, Bumsoo Kim, and Buru Chang. Is right right? enhancing object orientation understanding in multimodal language models through egocentric instruction tuning, 2024. [54] Aylin Kallmayer, Melissa L-H Võ, and Dejan Draschkow. Viewpoint dependence and scene context effects generalize to depth rotated three-dimensional objects. Journal of Vision, 23(10):99, 2023. 12 [55] Zoe Kourtzi and Ken Nakayama. Distinct mechanisms for the representation of moving and static objects. Visual Cognition, 9(1-2):248264, 2002. [56] Akshay Krishnan, Abhijit Kundu, Kevis-Kokitsi Maninis, James Hays, and Matthew Brown. Omninocs: unified nocs dataset and model for 3d lifting of 2d objects. In European Conference on Computer Vision, pages 127145. Springer, 2024. [57] Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, and Yang Liu. Scaffolding coordinates to promote vision-language coordination in large multi-modal models. In Proceedings of the 31st International Conference on Computational Linguistics, pages 28862903, Abu Dhabi, UAE, 2025. Association for Computational Linguistics. [58] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. [59] Fangjun Li, David C. Hogg, and Anthony G. Cohn. Reframing spatial reasoning evaluation in language models: real-world simulation benchmark for qualitative reasoning. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, 2024. [60] Hui Li, Nan Liu, You Li, Ralph Weidner, Gereon Fink, and Qi Chen. The simon effect based on allocentric and egocentric reference frame: Common and specific neural correlates. Scientific reports, 9 (1):13727, 2019. [61] Siting Li, Pang Wei Koh, and Simon Shaolei Du. Exploring how generative mllms perceive more than clip with the same vision encoder, 2025. [62] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning, 2023. [63] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [64] Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang Low. Prompt optimization with human feedback, 2025. [65] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023. [66] Sam Ling, Joel Pearson, and Randolph Blake. Dissociation of neural mechanisms underlying orientation processing in humans. Current Biology, 19(17):14581462, 2009. [67] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306, 2024. [68] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [69] Ruihao Liu, Zhenzhong Yu, Qigao Fan, Qiang Sun, and Zhongsheng Jiang. The improved method in fabric image classification using convolutional neural network. Multimedia Tools and Applications, 83(3): 69096924, 2024. [70] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. [71] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [72] J. E. Loy and V. Demberg. Individual differences in spatial orientation modulate perspective taking in listeners. Journal of Cognition, 6, 2023. 13 [73] Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Alan Yuille, and Jieneng Chen. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [74] Hanspeter A. Mallot. From geometry to behavior : an introduction to spatial cognition. The MIT Press, Cambridge, 1st ed. edition, 2023. [75] Elad Mezuman and Yair Weiss. Learning about canonical views from internet image collections. Advances in neural information processing systems, 25, 2012. [76] Roshanak Mirzaee and Parisa Kordjamshidi. Transfer learning with synthetic corpora for spatial role In Proceedings of the 2022 Conference on Empirical Methods in Natural labeling and reasoning. Language Processing, pages 61486165, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. [77] Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. SPARTQA: textual question answering benchmark for spatial reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45824598, Online, 2021. Association for Computational Linguistics. [78] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: vision-language pre-training via embodied chain of thought. In Proceedings of the 37th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2023. Curran Associates Inc. [79] H. Muto. Correlational evidence for the role of spatial perspective-taking ability in the mental rotation of human-like objects. Experimental Psychology, 68:4148, 2021. [80] neelgajare. Rock images, 2022. https://www.kaggle.com/datasets/neelgajare/ rocks-dataset [Accessed: (01-29-2025)]. [81] Michael Peer, Roy Salomon, Ilan Goldberg, Olaf Blanke, and Shahar Arzy. Brain system for mental orientation in space, time, and person. Proceedings of the National Academy of Sciences, 112(35): 1107211077, 2015. [82] Jiahuan Pei, Irene Viola, Haochen Huang, Junxiao Wang, Moonisa Ahsan, Fanghua Ye, Yiming Jiang, Yao Sai, Di Wang, Zhumin Chen, Pengjie Ren, and Pablo César. Autonomous workflow for multimodal finegrained training assistants towards mixed reality. In Annual Meeting of the Association for Computational Linguistics, 2024. [83] Tessa Pulli, Stefan Thalhammer, Simon Schwaiger, and Markus Vincze. From words to poses: Enhancing novel object pose estimation with vision language models, 2024. [84] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [85] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? In ICLR, 2025. [86] Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael Ryoo, and Tsung-Yu Lin. Learning to localize objects improves spatial reasoning in visual-llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1297712987, 2024. [87] Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. Stepgame: new benchmark for robust multi-hop spatial reasoning in texts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1132111329, 2022. [88] Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2144021455, 2024. [89] Donatella Spinelli, Gabriella Antonucci, Roberta Daini, Maria Luisa Martelli, and Pierluigi Zoccolotti. Hierarchical organisation in perception of orientation. Perception, 28(8):965979, 1999. [90] Ilias Stogiannidis, Steven McDonagh, and Sotirios Tsaftaris. Mind the gap: Benchmarking spatial reasoning in vision-language models. arXiv preprint arXiv:2503.19707, 2025. 14 [91] Arjan Ter Horst, Rob Van Lier, and Bert Steenbergen. Mental rotation task of hands: differential influence number of rotational axes. Experimental Brain Research, 203:347354, 2010. [92] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [93] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [94] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. [95] John Tuthill and Eiman Azim. Proprioception. Current Biology, 28(5):R194R203, 2018. [96] B. Tversky and M. Suwa. Thinking with sketches. Tools for Innovation, pages 7584, 2009. [97] unsplash. The unsplash dataset, 2020. https://github.com/unsplash/datasets?tab= readme-ov-file [Accessed: (01-29-2025)]. [98] Marina Vasilyeva and Stella Lourenco. Development of spatial cognition. Wiley Interdisciplinary Reviews: Cognitive Science, 3(3):349362, 2012. [99] Simone Viganò, Rena Bayramova, Christian Doeller, and Roberto Bottini. Mental search of concepts is supported by egocentric vector representations and restructured grid maps. Nature Communications, 14 (1):8132, 2023. [100] G. Wang and D. Alais. Tactile adaptation to orientation produces robust tilt aftereffect and exhibits crossmodal transfer when tested in vision. Scientific Reports, 14, 2024. [101] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26422651, 2019. [102] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. Advances in Neural Information Processing Systems, 37:7539275421, 2025. [103] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [104] Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, et al. Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2027020281, 2023. [105] Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, and Alan Yuille. 3d-aware visual question answering about parts, poses and occlusions. arXiv preprint arXiv:2310.17914, 2023. [106] Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. Pulsecheck457: diagnostic benchmark for 6d spatial reasoning of large multimodal models, 2025. [107] Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. Spatial457: diagnostic benchmark for 6d spatial reasoning of large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [108] Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, and Yanfeng Wang. Editable scene simulation for autonomous driving via collaborative llm-agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [109] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards ai-complete question answering: set of prerequisite toy tasks. arXiv: Artificial Intelligence, 2015. 15 [110] Kyle Willett, Chris Lintott, Steven Bamford, Karen Masters, Brooke Simmons, Kevin RV Casteels, Edward Edmondson, Lucy Fortson, Sugata Kaviraj, William Keel, et al. Galaxy zoo 2: detailed morphological classifications for 304 122 galaxies from the sloan digital sky survey. Monthly Notices of the Royal Astronomical Society, 435(4):28352860, 2013. [111] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803814, 2023. [112] Jiguo Xue, Chunyong Li, Cheng Quan, Yiming Lu, Jingwei Yue, and Chenggang Zhang. Uncovering the cognitive processes underlying mental rotation: An eye-movement study. Scientific Reports, 7(1):10076, 2017. [113] Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluating spatial understanding of large language models. Transactions on Machine Learning Research, 2024. [114] Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Shuguang Cui, and Zhen Li. Comprehensive visual question answering on point clouds through compositional scene manipulation. IEEE Transactions on Visualization & Computer Graphics, pages 113, 2023. [115] Cheng Yang, Rui Xu, Ye Guo, Peixiang Huang, Yiru Chen, Wenkui Ding, Zhongyuan Wang, and Hong Zhou. Improving vision-and-language reasoning via spatial relations modeling. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 769778, 2024. [116] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171, 2024. [117] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [118] Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Rouyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, and Yi Ma. Seeing from another perspective: Evaluating multi-view understanding in mllms. arXiv preprint arXiv:2504.15280, 2025. [119] Hang Yin, Zhifeng Lin, Xin Liu, Bin Sun, and Kan Li. Do multimodal language models really understand direction? benchmark for compass direction reasoning. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [120] Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate Saenko. KiVA: Kid-inspired visual analogies for testing large multimodal models. In The Thirteenth International Conference on Learning Representations, 2025. [121] Tino Zaehle, Kirsten Jordan, Torsten Wüstenberg, Jürgen Baudewig, Peter Dechent, and Fred Mast. The neural basis of the egocentric and allocentric spatial frame of reference. Brain research, 1137:92103, 2007. [122] Ce Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Sijie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak Shah. Deep learning-based human pose estimation: survey. ACM Comput. Surv., 56(1), 2023. [123] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1679316803, 2022."
        },
        {
            "title": "6 Task-wise Dataset Creation",
            "content": "View parallelism perception task evaluates models ability to determine whether an objects frontfacing surface is oriented toward, away from, or perpendicular to the camera plane. We constructed this dataset using images from the JTA [31] and KITTI[40] datasets (specifically the subset used in OmniNOCS [56]). For JTA, which contains 3D human pose annotations, we calculated orientation by analyzing shoulder positions relative to the camera and head angle to precisely determine facing direction. For KITTI, we leveraged the available rotation matrices to categorize vehicles and pedestrians based on their orientation relative to the camera. This task is critical for fundamental scene understanding, where determining which objects are facing an agent is essential for interaction and navigation decisions. Frontal alignment perception task extends orientation assessment to cardinal directions, requiring models to identify if objects are facing toward, away, leftward, or rightward relative to the camera. We developed this dataset using images from COCO [63] and Cityscapes [23] (from the OmniNOCS [56] subset). For COCO images, which lack orientation annotations, we conducted expert manual labeling of object orientations. For Cityscapes, we utilized rotational matrices to precisely determine directional orientation, limiting images to contain at most three objects to ensure assessment clarity. This directional understanding is vital for spatial navigation and object manipulation tasks where agents must understand not just if objects face them, but their specific directional orientation. Single-axis rotation task assesses understanding of rotational transformations around vertical axis by asking models to determine the optimal rotation direction and precise angular adjustment needed for objects to face the camera. We constructed this dataset using 3D-Future [34], which provides highresolution 3D furniture models with known 6-DoF parameters. We focused primarily on chair variants with distinctive front/back features, calculating the exact rotational adjustment needed for the object to face the camera directly. This capability forms the foundation for computational manipulation planning and scene reconfiguration understanding. Furthermore, we utilized the Objectron [1] subset of the OmniNOCS [56] dataset. This also included 6 DoF information which we used to determine the angle of the object with respect to the camera, for this dataset we utilized bikes, chairs and bottles. Compound rotation task evaluates comprehension of complex rotations involving sequential transformations around multiple axes, where the rotation order impacts the final orientation. We developed this dataset using 3D-rendered objects from Get3D [38], ShapeNet [13], and OmniObject3D, [111] implementing controlled rendering pipeline in Blender. For each object, we rendered an initial thirdperson view, then applied precise rotations along horizontal and vertical axes in varying sequences, rendering the transformed state. This task assesses the sophisticated mental rotation capabilities required for complex object manipulation and orientation reasoning across multiple dimensions. Inter-object direction perception task evaluates understanding of relative orientation between multiple objects from their own perspectives rather than the camera view. Using the 3D Future [34] and NOCS REAL [101] datasets, we leveraged 6 DoF parameters to calculate precise angular relationships between object pairs. The task requires models to determine if objects face the same direction, opposite directions, or have perpendicular orientations, progressing to granular assessment of the exact rotation needed for objects to align. This capability is essential for understanding agent-object and object-object relationships in complex scenes, particularly for collaborative robotic tasks or scene arrangement planning. Viewer-scene direction perception task evaluates perception of rotational changes between two images of the same object. Using Get3D [38], ShapeNet [13], and OmniObject3D [111] datasets, we rendered objects with ground plane reference, then created corresponding images with the object rotated by specific angles around vertical axis. Models must determine whether rotation occurred and, at the granular level, specify the exact degree of rotation. This assessment examines the ability to track orientation changes across viewsa crucial capability for video understanding, temporal reasoning, and object tracking applications. Canonical orientation reasoning task evaluates models understanding of normal object orientations and their ability to identify when objects appear in non-canonical positions. Using subset of COCO images with clear orientation expectations (e.g., people standing upright, vehicles with wheels on the ground), we created variations with systematic flips and rotations. Models must first identify whether images appear in their canonical orientation, then determine the specific transformations (rotation, flipping, or both) needed to restore proper orientation. This capability assesses world 17 knowledge about typical object positioning, which is critical for anomaly detection, image correction, and understanding intentional vs. unintentional orientation deviations."
        },
        {
            "title": "7 Category Distribution Across Orientation Tasks",
            "content": "Fig. 5 presents the distribution of the top 25 most frequent object categories in DORI, annotated with their corresponding orientation task types. This visualization highlights not only which categories are most prevalent, but also how they are utilized across the various tasks in DORI. While commonly occurring objects such as chair, car, and sofa span several task types, others are more narrowly concentrated. This overlap and separation across tasks reflect the intentional diversity of DORI, designed to evaluate model capabilities across both general-purpose and category-specific orientation challenges. The distribution also underscores the need for models to generalize effectively across unevenly represented categories and task combinations. Figure 5: Sample category distribution of top 25 object categories showing their associated orientation tasks"
        },
        {
            "title": "8 Mapping Object Categories to Broad Classes",
            "content": "To enable category-level analysis, we group the fine-grained object labels in DORI into smaller set of semantically meaningful broad classes. This abstraction facilitates more interpretable evaluation across heterogeneous datasets and reduces sparsity in underrepresented categories. The mapping spans common categories such as Vehicle, Person, Animals, Food, Containers, Furniture, Electronics, and Miscellaneous, and is illustrated in Tab. 5 and Fig. 6. Table 5: Mapping from object categories to broad semantic classes. Object Category car, truck, tram, van, bus, train, airplane, boat, motorcycle, bicycle, coach, bike, motorbike, trailer pedestrian, cyclist, person zebra, dog, elephant, giraffe, cat, horse, sheep, bird, cow, bear, starfish pizza, broccoli, donut, orange, apple, tomato, potato cake, egg, wine glass, bowl, bottle, onion bottle, cup, wine glass, bowl, medicine_bottle, box chair, sofa, table, lamp, bench, sofa-chair, soft-sofa laptop, tv, display, camera, laptop-camera clock, vase, toilet, umbrella, teddy bear, rifle stop sign, toy_train, toy_bus, house, ball scene, pattern Broad Class"
        },
        {
            "title": "Vehicle\nVehicle\nPerson\nAnimals\nAnimals\nFood\nFood\nContainers\nFurniture\nElectronics\nMiscellaneous\nMiscellaneous\nMiscellaneous",
            "content": "Figure 6: Distribution of fine-grained object categories mapped to broader semantic classes. Most categories fall under Vehicle, Miscellaneous, and Animals, reflecting common object types in the evaluated datasets."
        },
        {
            "title": "9 Dataset Composition and Visual Diversity",
            "content": "To support comprehensive evaluation of Multimodal Large Language Models (MLLMs) on orientation reasoning and object-centric understanding, we construct our dataset by curating and aligning images from diverse computer vision sources. Specifically, DORI incorporates scenes from diverse computer vision datasets including KITTI [40], Cityscapes [23], COCO [63], JTA [31], 3D-FUTURE [34], Objectron [1], ShapeNet [13], OmniObject3D [111], and COCO Space SEA [63, 97, 42, 69, 110, 80]. 19 These datasets span various natural and simulated domains, capturing varied object instances, backgrounds, occlusion levels, lighting conditions, and viewpoints. Fig. 7 showcases representative image samples from each source, illustrating the diversity of environments, object types, and scene structures. This breadth of distribution ensures that the evaluation probes both the generalization ability of MLLMs and their robustness to contextual, visual, and category shifts across domains. Fig. 8 further complements this by presenting the distribution of the top 25 most frequent object categories across the datasets. While few object classes, such as car, person, camera, and chair dominate in frequency, the distribution spans broad range of object types and source domains. This heterogeneity plays critical role in evaluating the performance of models not only on popular categories but also on long-tail classes, thereby encouraging more balanced and comprehensive assessment. Figure 7: Representative image samples drawn from each constituent dataset. Natural datasets include Kitti, Cityscapes, Coco, Coco-Space-Sea, Nocs-real, and Objectron. In the figure, we also show simulated dataset samples including: JTA, 3D-Future, Get-3D, Omniobject3D, and Shapenet. 20 Figure 8: Sample category distribution of top 25 object categories showing their sources"
        },
        {
            "title": "10 Task Complexity Hierarchy",
            "content": "Our multi-dimensional analysis reveals clear hierarchy of difficulty in orientation reasoning tasks that closely mirrors human cognitive development. Infants first master basic frontal orientation recognition before developing the neural machinery for complex mental rotation operations [96, 74, 98]. Similarly, models perform most competently on frontal alignment tasks (particularly view parallelism), where the top model (Gemini 1.5 Pro in Table 3 in the main paper) achieves 68.5% coarse accuracy. Performance systematically deteriorates as tasks require more complex transformations, with compound rotation proving exceptionally challenging (best granular performance of only 15.3% achieved by GPT-4 in Table 3 in the main paper). The models difficulty with compound rotations strongly suggests they lack the neural inductive biases that allow humans to mentally simulate and track objects through complex transformations. The most striking performance gap appears in tasks requiring perspective shiftsspecifically, i.e. viewer-scene direction perception and inter-object direction perception in Tables 2 and 3 in the main paper. While viewer-scene direction perception shows relatively strong performance (Gemini models reaching 91-92% coarse accuracy), inter-object direction tasks reveal fundamental weakness across all models (best performance of only 25% coarse). This disparity demonstrates that current MLLMs struggle to mentally adopt perspectives different from the allocentric viewpointa cognitive ability that humans develop through perspectivetaking experiences. This limitation is particularly concerning for embodied AI applications like robotics and navigation, which require reasoning about object relationships from multiple viewpoints. Canonical orientation understanding (determining if objects appear in their natural orientation) shows highly variable performance across models (Tab. 2 in the main paper). Some systems perform exceptionally poorly (DeepSeek-1.3B-base at 2.5% coarse) while others demonstrate relatively robust capabilities (GPT-4 at 45% and GPT-4-1 at 46% granular, as shown in Table 3 in the main paper). This variability suggests that recognizing natural object orientations depends on world knowledge that is inconsistently encoded across different architectural approaches and training regimes."
        },
        {
            "title": "11 Additional Analyses of Model Performance",
            "content": "To deepen our understanding of current Multimodal Large Language Models (MLLMs) on DORI, we present series of additional analyses that dissect performance across key axes of task structure and model behavior. Fig. 9 examines the relationship between answer set size and model accuracy. We observe peak in performance at 3-option questions (42.5%), with accuracy declining markedly as the number of candidate answers increases. At 5 and 6 options, performance drops to 26% and 19%, respectively, with further decline to 13% at 9 options, and minimum of 6% at 16 options. This degradation 21 illustrates the difficulty MLLMs face when navigating more complex decision spaces, suggesting that increasing output space size strains their orientation reasoning capabilities. In Fig. 10, we break down model performance across simulated vs. natural imagery and annotation granularity levels. Gemini and GPT variants dominate across the board. Yet, all models exhibit significantly higher accuracy on simulated datasets and coarse-type questions, revealing persistent challenge in transferring orientation understanding to more fine-grained and realistic visual contexts. Finally, Fig. 11 explores performance along two orthogonal decompositions: four cognitive-relevant orientation reasoning dimensions (left) and seven distinct orientation tasks (right). Strikingly, models consistently struggle with complex relational and rotational reasoning, particularly on inter-object direction, single-axis rotation, and compound transformation tasks. These persistent errors underscore the difficulty of encoding and reasoning over structured spatial transformations in orientation reasoning, indicating key limitations in current MLLMs geometric and physical reasoning faculties. Figure 9: Model accuracy as function of answer set size. Accuracy peaks at 42.5% for 3-option questions, then declines steadily, dropping to about 26% at 5 options, 19% at 6, about 13% at 9 options, and reaching low of about 6% at 16 options, indicating increasing difficulty with larger candidate sets. (a) Performance grouped by data type. (b) Performance grouped by question granularity. Figure 10: Performance of 15 leading Multimodal Large Language Models (MLLMs) across data types (a) and annotation granularity levels (b). Gemini and GPT models lead overall. Models perform noticeably better on simulated datasets and coarse-type questions, revealing gap in generalizing to natural images and more fine-grained questions. 22 Figure 11: We evaluate 15 leading models on DORIs decomposed object orientation comprehension across four fundamental neural and cognitive dimensions (left) and seven distinct orientation tasks (right). The consistently low performance on the rotational transformation dimension, inter-object direction, single-axis, and compound rotation tasks highlights the limitations of current MLLMs in understanding complex spatial relationships and orientation dynamics."
        },
        {
            "title": "12 VQA Examples in DORI",
            "content": "To further contextualize model behavior, we present curated selection of Visual Question Answering (VQA) examples drawn from the DORI benchmark, covering each of the seven distinct orientation reasoning tasks. These qualitative illustrations shed light on the nuanced challenges faced by state-ofthe-art Multimodal Large Language Models (MLLMs), beyond aggregate metrics. Each example is carefully chosen to represent either canonical failure or, in rarer cases, surprising success. The samples span both coarse-level and granular-level questions, reflecting the dual axes of abstraction and visual complexity within DORI. Despite confident language in many model predictions, we observe frequent dissonance between answer correctness and the underlying rationale, especially for tasks requiring precise spatial alignment or inter-object reasoning. For instance, in the View Parallelism and Directional Facing tasks, models often misjudge subtle orientation cues, such as limb articulation or gaze direction as we show in Figures 12, 13, 14, and 15, leading to confidently incorrect predictions. Likewise, for Single-Axis and Compound Rotation scenarios, we show in Figures 16, 17, 18, and 19, that even top-performing models struggle with mentally simulating object motion, resulting in angular miscalculations or overly generic justifications. Notably, Inter-object Direction and Viewer-Scene Direction tasks as we show in Figures 20, 21, 22, and 23, expose limitations in relational orientation reasoning, with models frequently underestimating angular disparities or misrepresenting the directional frame of reference. The final task, Canonical Orientation as seen in Fig. 24, underscores broader epistemic gap: models often assert certainty in inherently ambiguous scenarios, revealing an overconfidence not grounded in the visual evidence. Together, these examples highlight persistent limitations in visual-spatial grounding, even among the most capable contemporary MLLMs. They underscore the need for further architectural innovations and training strategies to imbue models with deeper, more structured understanding of object orientation dynamics. Figure 12: An example VQA from the View Parallelism orientation task illustrating failure case on coarse-level question in DORI, with prediction from Gemini 1.5 Pro. Many models failed this question, highlighting common challenge among MLLMs in understanding fundamental scene geometryspecifically, in reasoning whether an objects front-facing surface is oriented toward, away from, or perpendicular to the camera plane. Figure 13: An example VQA from the View Parallelism orientation task highlighting failure case on granular-level question in DORI, with response from Gemini 2.0 Flash. Although the model confidently selects an answer, its reasoning reflects fundamental misunderstanding of the objects orientation relative to the camera. While the ground truth indicates the object is turned between 135 and 180 away, the model incorrectly infers near-parallel alignment, underscoring limitations in fine-grained spatial perception among current MLLMs. 24 Figure 14: coarse-level VQA from the Directional Facing task in DORI, illustrating failure case by GPT-4-1. While the ground truth indicates leftward orientation, the model predicts away, misreading body posture and head direction. Notably, 13 out of 15 models failed this question, underscoring widespread struggle with coarse directional inference. Figure 15: coarse-level VQA from the Directional Facing task in DORI, showing failure case by Gemini-2.0-Flash. While the ground truth identifies the giraffes orientation as rightward, the model incorrectly infers leftward direction. This highlights the difficulty in interpreting animal pose and orientation cues, challenge shared by the majority of models in this example. Figure 16: granular-level VQA from the Single-Axis Rotation task in DORI, featuring case where GPT-4o selects the correct answer (180 rotation) but provides flawed reasoning. While the model concludes with the correct choice, its explanation suggests uncertainty due to perceived ambiguity in object orientation. This mismatch between prediction and rationale underscores gaps in visual reasoning, with only 2 out of 15 models answering correctly. Figure 17: granular-level VQA from the Single-Axis Rotation in DORI, showing failure case with prediction from Gemini 2.0 Flash. While the correct answer is 180 rotation, the model incorrectly selects 90 (option B), suggesting misjudgment of the objects current orientation relative to the camera. This highlights challenges models face in reasoning about precise rotational alignment. 26 Figure 18: coarse-level VQA from the Compound Rotation task in DORI, with correct prediction from Gemini 2.0 Flash. The model identifies horizontal axis rotation, but its reasoning overstates the change, describing the car as upside down when it is only partially inverted. Figure 19: granular-level VQA from the Compound Rotation task in DORI showing failure case from Gemini 2.0 Flash. While the model selects the correct option, its reasoning misidentifies the vertical rotation, describing 180 flip when the ground truth indicates 90 transformation. Figure 20: coarse-level VQA from the Inter-object Direction task in DORI showing failure case with GPT-4-1. Although the correct answer is Partially facing the same direction, the model incorrectly selects Partially facing opposite directions, misjudging the relative orientations. Notably, 13 out of 15 models failed this example, highlighting shared difficulty in reasoning about partially aligned object directions. Figure 21: granular-level VQA from the Inter-object Direction task in DORI illustrating failure case with Gemini 1.5 Pro. While the ground truth indicates required clockwise rotation between 46 and 90 for alignment, the model underestimates this angle. Its reasoning misinterprets the spatial alignment between the chair and sofa. All 15 models failed this example, underscoring the challenge of inter-object directional understanding. 28 Figure 22: Viewer-Scene Direction Example success case on granular-level DORI question with Gemini-2.0-Flash. While the model correctly selects the 90-degree rotation, it inaccurately describes the directional shift as right to bottom instead of the more precise bottom-right to bottom-left, reflecting partial misunderstanding of fine-grained orientation. Figure 23: Viewer-Scene Direction Example failure case on granular-level DORI question with GPT-4-1. Although the model selects the correct answer of 180 degrees, its reasoning mistakenly describes 90-degree rotation, highlighting disconnect between answer selection and spatial understanding. Notably, 13 out of 15 models failed this case. Figure 24: Canonical Orientation Example failure case on coarse-level DORI question with Gemini-2.0-Flash. While the ground truth is Cannot be determined, the model incorrectly selects definitive orientation. Its reasoning contradicts the inherent ambiguity it acknowledges, exposing uncertainty in handling objects with no fixed canonical pose. 30 Human evaluation. We recruited seven experts with experience in complex annotation procedures to assess orientation perception abilities. Each participant evaluated 30 examples for both coarse and granular tasks using identical images, with three answer options per example (correct answer, \"Cannot be determined,\" and incorrect answer). Tab. 4 in the main paper reports that humans achieved 80-87%, but the best closed-source model is almost 30% lower, suggesting significant room for improvement. An example of the high level instructions shown for this task can be seen in Fig. 25, examples of questions are shown in Fig. 26 and Fig. 27. Error Analysis Qualitative analysis of model errors reveals consistent failure patterns that transcend specific architectures. All models show disproportionate performance degradation when tasks require: (1) tracking orientation changes across multiple reference frames, (2) understanding compound rotations involving multiple axes, and (3) making precise angular judgments beyond basic categorical distinctions. These patterns strongly suggest that current MLLMs lack robust internal 3D spatial representations that would enable human-like mental simulation of object rotations and perspective shifts. Figure 25: An example of the high level instructions shown for the Human Evaluation"
        },
        {
            "title": "13 Enumerated List of Questions",
            "content": "Coarse Questions Q1 - View Parallelism: Determine which way Object As front is facing relative to the camera. Q2 - Directional Facing: Determine which direction Object As front-facing surface is oriented from the cameras viewpoint. Q3 - Single-axis Rotation: Determine the shortest direction of rotation for Object to face the camera. Q4 - Compound Rotation: Determine what type of rotation the object has undergone between the two images. Q5 - Inter-object Direction: Determine if objects and are facing each other from their own perspectives. Q6 - Viewer-Scene Direction: Determine if the object has rotated between the two images. Q7 - Canonical Orientation: Determine if the image is in its canonical orientation. 31 Figure 26: An example of sample for the coarse-level VQA for the Directional Facing task in DORI shown for the Human Evaluation Fine-grained/Granular Questions Q1 - View Parallelism: Determine how much Object As front surface deviates from being parallel to the camera plane. Q2 - Directional Facing: Identify the precise orientation of Object As front-facing surface from the cameras viewpoint Q3 - Single-axis Rotation: Determine the closest clockwise rotation needed for Object to face the camera. Q4 - Compound Rotation: Determine the exact rotation angles the object has undergone between the two images. Q5 - Inter-object Direction: Determine how much Object would need to rotate to face Object A. Q6 - Viewer-Scene Direction: Determine how many degrees clockwise the object has rotated between the two images. Q7 - Canonical Orientation: Determine how the image can be restored to its canonical orientation."
        },
        {
            "title": "14 Performance Error Bar",
            "content": "The error bars in Figs. 28, 29, 30, 31, 32, 33, and 34 report the mean and standard deviation of various models on DORI questions. For each model and question type combination, the formula error = std_accuracy/sqrt(seed_count) was applied. We used 3 different seeds: [42, 1998, 107983]. Looking at the View Parallelism task  (Fig. 28)  , we observe relatively narrow error bars for most models, indicating consistent performance across different seeds. However, both DeepSeek base models and the LLaVa-13 B-base models display wider error bars on coarse questions (approximately 3 4%), suggesting that their performance comes with greater variability. Despite LLaVa-13B-base demonstrating better performance than the other 2 base models, its clear that all three base models performance is more sensitive to initialization conditions. The Directional Facing task  (Fig. 29)  reveals generally smaller error bars across all models, with most variations under 2%, indicating that performance on cardinal direction assessment remains relatively stable regardless of initialization. However, all models perform notably worse on this task 32 Figure 27: An example of sample for the coarse-level VQA for the Viewer-Scenen Direction task in DORI shown for the Human Evaluation compared to View Parallelism, with even the best model (DeepSeek-7B-Chat) achieving only 32.5% accuracy on coarse questions The Single-axis Rotation task  (Fig. 30)  is more uniform across models, with most showing variations of 2 3%. The comparable error bar sizes across models suggest that this task presents similar levels of difficulty for all architectures, with no model demonstrating significantly more stable performance than others. This uniformity in variability indicates that improvements in this task may require fundamental architectural innovations rather than just parameter tuning. The Compound Rotation task  (Fig. 31)  exhibits the most dramatic performance gap between coarse and granular questions, with granular accuracy dropping below 15% for all models. The error bars for granular questions are relatively tight (except in LLaVa-13B-base), suggesting that models consistently struggle with this task rather than showing initialization-dependent variability. The narrow error bands on poor performance indicate systematic limitation in the models ability to track complex multi-axis rotations. Overall, this plot demonstrates noticeable variability in the error bands across both coarse and granular questions. For Inter-object Direction  (Fig. 32)  , we observe slightly asymmetric performance patterns between coarse and granular questions. While DeepSeek-7B-Chat achieves the highest coarse accuracy (39.2%), all models show substantially lower performance on granular questions (generally below 15%). The error bars appear to be tight for coarse and granular questions, indicating consistent performance across different trials. The performance gap between coarse and granular questions suggests that while models can sometimes succeed at basic relational orientation tasks (determining if objects face the same/opposite directions), they systematically fail when asked to make precise angular judgments about inter-object relationships. Interestingly, for LLaVA-13B-base model, the granular performance (14.0%) slightly exceeds its coarse performance (11.1%), running counter to the typical pattern observed in other tasks and models. This anomaly may indicate that LLaVAs training regime potentially encodes some specific features that assist with fine-grained angular estimations 33 between objects, though its overall performance remains well below human capabilities on these tasks. Similarly, the Viewer-Scene Direction task  (Fig. 33)  reveals intriguing performance inversions between coarse and granular questions for certain models. LLaVA-13B-Base shows higher coarse accuracy (40.1%) but lower granular accuracy (21.6%), while DeepSeek-7B-Chat demonstrates the opposite pattern (17.0% coarse, 29.9% granular). These inversions, coupled with wide error bars, indicate that different model architectures encode rotation perception in fundamentally different ways. This task exposes fundamental inconsistencies in how current MLLMs process orientation changes, suggesting that rotation tracking may rely on different computational mechanisms than static orientation perception, with these mechanisms developing unevenly across model architectures and training regimes. The Canonical Orientation task  (Fig. 34)  doesnt exhibit any high variability among the error bars across coarse and granular performance. The error bars remain relatively narrow for most models (1 3%), indicating that performance limitations on this task are consistent across initialization seeds rather than highly variable. This consistency, coupled with generally poor performance, suggests that canonical orientation understanding, which requires both world knowledge about natural object positions and spatial transformation reasoning, represents fundamental capability gap in current MLLMs. The results indicate that models lack robust internal representations of how objects \"should\" appear in the world, crucial component for embodied navigation and manipulation tasks where recognizing and correcting non-canonical orientations is essential. Across all tasks, our error bar analysis reveals several critical insights about MLLMs orientation reasoning capabilities. First, model performance stability varies substantially across tasks, with simpler perception tasks (View Parallelism, Directional Facing) showing more consistent performance across initializations compared to complex reasoning tasks (Compound Rotation, Viewer-scene direction, Canonical Orientation). Moreover, the consistently tight error bars on poor-performing granular questions, particularly for rotational tasks, indicate systematic limitations rather than chance variability, suggesting architectural rather than parametric constraints. We also observe that Chattuned models generally demonstrate more stable performance than their base counterparts, suggesting that instruction tuning not only improves accuracy but also reduces initialization sensitivity. Finally, the performance inversions observed between coarse and granular questions for some models highlight the disconnect between categorical and precise metric orientation understanding, fundamental challenge that persists across all model families and architectures evaluated. Figure 28: Mean and Standard Deviation of various models on View Parallelism task"
        },
        {
            "title": "15 Soft Accuracy Calculation",
            "content": "Soft accuracy metrics were introduced to provide more nuanced evaluation of models orientation understanding capabilities and help distinguish between models that are completely wrong versus 34 Figure 29: Mean and Standard Deviation of various models on Directional Facing task Figure 30: Mean and Standard Deviation of various models on Single-axis Rotation task those that have an approximate understanding of orientation concepts. Unlike standard binary accuracy that only awards points for exact matches, soft accuracy awards half points (0.5) for answers that are partially correct or adjacent to the ground truth. To calculate such accuracies, we implement carefully designed spatial tolerance thresholds and logical equivalences. Soft accuracy is only calculated for the fine-grained questions that typically demand precise metric response. For View Parallelism, Predictions within 45 of the ground truth angle receive partial credit. This threshold captures predictions in adjacent sectors while excluding opposed orientations. For Directional Facing, half points are awarded exclusively for mirror-image confusions between \"30 degrees left\" and \"30 degrees right\". We intentionally do not extend partial credit to other angular errors, preserving the specificity of directional understanding assessment. For Single-axis Rotatoin, to predicted rotations (0, 45, 90, 135, 180). This allows credit for adjacent discrete positions while maintaining distinction between major orientation categories. For instance, predicting 45 when the correct answer is 90 would not qualify, but 135 prediction for 180 ground truth would receive partial credit. tolerance window applies 45 In Compound Rotation, partial credit is awarded if either the horizontal or vertical rotation component is correct in multi-axis transformations. In this \"X then Y\" rotation sequence response, we parse both components separately. For example, prediction of \"90 horizontal then 0 vertical\" 35 Figure 31: Mean and Standard Deviation of various models on Compound Rotation task Figure 32: Mean and Standard Deviation of various models on Inter-object direction. would receive 0.5 points for either correct component when compared to the ground truth \"90 horizontal then 180 vertical\" For Inter-object Direction, half points are given for adjacent magnitude ranges, but only if the direction (clockwise vs. counterclockwise) matches. For instance, if the ground truth was \"0 to 45 degrees clockwise\" and the prediction was \"46 to 90 degrees clockwise\", we award 0.5 points. However, if the prediction was \"46 to 90 degrees counterclockwise,\" it would earn 0 points despite similar magnitude For Viewer-scene direction, the soft accuracy specifically addresses confusion between opposite rotational directions. For example, if an object has rotated 90 clockwise between images, but the model reports 270 clockwise (which is equivalent to 90 counterclockwise), it receives 0.5 points. No partial credit is given for other angle confusions. For Canonical Orientation, the soft accuracy addresses confusion in the order of operations. For example, if an image requires rotation followed by flipping to restore its canonical orientation, but the model suggests flipping followed by rotation, it receives 0.5 points. Results. Figs. 35, 36, 37, 38, 39, 40, 41, compares standard (hard) vs. soft accuracy on DORI questions. Figures 35 and 36 reveal that for View Parallelism and Directional Facing tasks, soft accuracy provides no benefit over standard accuracy, with identical performance metrics across all models. This indicates that when models err on these fundamental orientation tasks, they tend to make categorical 36 Figure 33: Mean and Standard Deviation of various models on Viewer-Scene direction task. Figure 34: Mean and Standard Deviation of various models on Canonical Orientation task. mistakes rather than near-miss approximations. The lack of improvement suggests that errors in these tasks stem from fundamental misunderstandings rather than subtle misjudgments. In contrast, the Single-axis Rotation task (Figure 37) shows the most substantial gains under soft accuracy metrics, with improvements ranging from 8.6% to 17.7% across models. DeepSeek-7B-Chat achieves the most dramatic improvement, with accuracy increasing from 18.6% (standard) to 36.3% (soft). This 17.7% gain suggests that while models often fail to identify the exact rotational angle, they frequently select adjacent angular categories, demonstrating partial understanding of rotational relationships. For Compound Rotation (Figure 38), all models except DeepSeek-7B-Base show notable improvements under soft accuracy metrics. LLaVA-13B-Base improves from 12.1% to 23.2%, indicating that models often correctly identify one of the two rotation components (horizontal or vertical) while missing the other. This partial success highlights both the inherent complexity of multi-axis rotations and the models fragmentary grasp of compound transformations. The Inter-object Direction task (Figure 39) shows similar amount of soft accuracy gains across all models, with improvements ranging from 7.7% to 17.1%. DeepSeek-7B-Chat improves from 12.7% to 29.8%, more than doubling its effective performance. This suggests that models often select directionally appropriate answers that fall into adjacent angular ranges, indicating coarse understanding of relative orientations despite lacking precise angular discrimination. 37 For Viewer-scene Direction (Figure 40), soft accuracy provides moderate improvements (1.2% to 5.9%), with DeepSeek-7B-Chat showing the largest gain. The comparatively smaller improvements here suggest that models correctly identify rotational changes, with fewer \"near miss\" responses than in other tasks. Canonical Orientation (Figure 41) shows minimal improvement under soft accuracy metrics (0.7% to 3.3%). The small gains observed suggest that models are rarely confused in the order of operations. When they fail on canonical orientation tasks, they typically misidentify the necessary operations entirely rather than simply reversing their order. This indicates more fundamental gap in understanding canonical object positioning rather than mere sequencing errors. Fig. 42 summarizes the relative gains of soft accuracy across all tasks and models. Notably, this more lenient metric only helps some questions (Fig. 37 to 41). The heatmap reveals that Single-axis Rotation, Compound Rotation, and Inter-object Direction tasks benefit most from soft accuracy metrics, with improvements frequently exceeding 10%. This pattern suggests that rotational and relational orientation understanding in current MLLMs exists on spectrum rather than in binary states of correctness. In addition, the highest gain reported was for Single-axis Rotation, but is significantly below human performance using standard accuracy (18% vs. avg of about 30%). This shows that even when given an advantage, these models still fall significantly below human ability. Figure 35: Comparing the mean and standard deviation of soft vs. standard (hard) accuracy on View Parallelism task. See discussion Sec. 15. Figure 36: Comparing the mean and standard deviation of soft vs. standard (hard) accuracy on Directional Facing task. See discussion Sec. 15. 38 Figure 37: Comparing the mean and standard deviation of soft vs. standard (hard) accuracy on Single-axis Rotation task. See discussion Sec. 15. Figure 38: Comparing the mean and standard deviation of soft vs. standard (hard) accuracy on Compound Rotation task. See discussion Sec. 15. Figure 39: Comparing the mean and standard deviation of soft vs. standard (hard) accuracy on Inter-object Direction task. See discussion Sec. 15. 39 Figure 40: Comparing the mean and standard deviation of soft vs. standard (hard) accuracy on Viewer-scene Direction task. See discussion Sec. 15. Figure 41: Comparing the mean and standard deviation of soft vs. standard (hard) accuracy on Inter-object Direction task. See discussion Sec. 15. 40 Figure 42: Relative gain from using Soft accuracy rather than Standard (hard) accuracy per question. See 15 for discussion"
        }
    ],
    "affiliations": [
        "Boston University"
    ]
}