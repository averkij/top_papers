{
    "paper_title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason",
    "authors": [
        "Ang Lv",
        "Ruobing Xie",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Rui Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 3 5 6 2 2 . 5 0 5 2 : r The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason Ang Lv1 Ruobing Xie2 Xingwu Sun2,3 Zhanhui Kang2 Rui Yan1,4 1 GSAI, Renmin University of China 2 Large Language Model Department, Tencent 3 University of Macau 4 School of Computer Science, Wuhan University {anglv, ruiyan}@ruc.edu.cn {ruobingxie, sammsun, kegokang}@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward functions outputs in math tasks still allows Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as first, need towithout verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLMs performance on open-ended tasks. These findings suggest the importance of improving models foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https: //github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement learning (RL) applied to post-training large language models (LLMs) has led to significant advancements in enhancing their thinking and reasoning abilities [6, 29], resulting in improved performance on many challenging downstream tasks. Most current research focuses on math tasks [13, 23, 33, 10, 3], as these can be easily verified as correct or incorrect by simple rule-based reward functions. However, in many real-world applications, such as preference alignment [22, 38] and open question-answering [14, 21], responses cannot be easily quantified with simple rule-based functions and instead require evaluation by neural reward models. These models, being imperfect, often introduce noise even resulting in opposite rewards. In this study, we studied scenarios in which Corresponding authors Preprint. Under review. the reward, whether derived from neural reward model or rule-based function, contains noise, aiming to gain more practical understanding of noisy rewards in teaching LLMs to reason. We made an unexpected discovery: despite the presence of substantial noise in the rewards, the model can still be effectively trained and achieve fast convergence. For example, when training on math problems and introducing noise by randomly flipping 40% of the reward functions outputs (i.e., assigning positive rewards for incorrect answers), Qwen-2.5-7B [32] model still improved its MATH-500 [12] score from an initial 5% to surprising peak of 72.02%, close to the 75.85% achieved using noiseless reward function. Training collapses when the flip rate reaches 50%, where the reward becomes entirely random. The models ability to tolerate substantial noisy rewards suggests that, although outputs with incorrect answers are mistakenly rewarded, they still exhibit valuable logical reasoning processes. These reasoning patterns are also valuable. If this were not the case, high frequency of incorrect rewards would likely hinder performance or, at the very least, slow convergence. Thus, we hypothesize that the effectiveness of RL in enhancing LLMs primarily stems from the exploration of appropriate reasoning patterns during rollouts. Through this exploration, the model can more effectively leverage its pretrained knowledge by adopting strong reasoning patterns, which increases the likelihood of arriving at the correct answer.2 Figure 1: All of (1) standard RL, (2) RL with 40% of the rewards manually flipped to the opposite, and (3) RL with only Reasoning Pattern Rewards (RPR) (i.e., rewards are given whenever key reasoning phrases appear, without verifying the final answer)can improve Qwen-2.5-7Bs accuracy on MATH-500 from an initial 5% to over 70%. The performance gap between these three setups is minimal compared to the overall improvements. To support this hypothesis, we conducted another experiment in which the model was rewarded whenever key reasoning phrases, such as first, need to or let me first, appeared in the outputs, without verifying the correctness of the final answer. We name this strategy as Reasoning Pattern Reward (RPR). Using only RPR, the model achieved peak task performance (70.21% for Qwen-2.5-7B) comparable to the performance achieved when the correctness of the solution was strictly verified. This provides strong evidence suggesting that LLMs can reason through RL because they have already learned to reason during pretraining, as no correctness supervision signals were given, meaning no new knowledge was learned. LLMs robustness to noisy rewards extends to open-ended NLP tasks as well. We conducted experiments using the NVIDIA HelpSteer3 dataset [30], which consists of broad range of challenging open-ended questions requiring AI assistance. We varied reward model accuracy by adjusting training set size and found that LLMs trained with 75% accurate reward model performed comparably to those using our best model (85% accuracy). These insights motivate our proposal of simple yet effective method to improve LLM performance on open-ended NLP tasks post-trained with noisy reward models. We use RPR to calibrate the reward models by compensating for potential false negative signals, resulting in up to 30% net win rate over LLMs post-trained with vanilla reward models. RPR-calibrated reward models also enable smaller models, such as Qwen-2.5-3B, to demonstrate strong reasoning capabilities on complex open-ended tasks, where vanilla reward models lead to training collapse. In summary, we provide some insights into training LLMs to reason via RL with noisy rewards: 1. We demonstrate that LLMs with strong inherent reasoning abilities are surprisingly robust to reward noise. 2A premise of this hypothesis is that the model must have acquired substantial reasoning capabilities during pretraining. As we show later, Qwen significantly outperforms Llama [19] not only in downstream performance after RL but also in robustness to noisy rewardsa difference that aligns with Llamas widely recognized weakness in reasoning [23, 33, 10]. 2 2. We present direct evidence showing that after RL, models enhanced performance on challenging tasks primarily stems from adapted output patterns, which are more likely to lead to correct answers, rather than from learning much new knowledge during RL. 3. We propose simple yet effective method to calibrate noisy reward models by rewarding reasoning patterns, leading to improved LLM performance on open-ended NLP tasks and unlocking smaller models reasoning capabilities."
        },
        {
            "title": "2 Noisy verification rewards in math tasks",
            "content": "Mathematics is one of the most commonly studied domains in LLM reasoning, due to its straightforward rule-based reward and evaluation. To explore the RL performance under noisy rewards, we begin by manually introducing noise to RL rewards in math tasks. While math tasks are typically assumed to be noiseless and verification is considered accurate, our noisy scenario is practical for two reasons: (1) false negative rewards are common in math tasks, where correct answer may be flagged as incorrect due to formatting issues, and (2) many proof-based questions in math tasks are difficult to verify accurately."
        },
        {
            "title": "2.1 Settings",
            "content": "Training. Most of the training setups follow the approach in [13], which provides simplified framework designed to help LLMs learn to reason. The experiments are based on VeRL [28] framework by Volcengine. Figure 2: The prompt used in math training, where the question placeholder will be replaced with specific question. Specifically: the dataset includes 57K high-quality, source-mixture math problems spanning various difficulty levels. The prompts used in these tasks are shown in Figure 2. The outputs are extracted from the box tag, normalized for format (e.g., converting fractions to decimals and translating LaTeX answers to plain text), and compared to the ground truth. In the absence of manual reward noise, the model is given reward of 1 if the output matches the ground truth and 0 otherwise. For training, we employ vanilla PPO [27] with GAE [26], using λ = 1 and γ = 1, with no KL-regularization. The training batch size is 128, with maximum response length of 4096. The learning rate for the actor is 106, and for the critic, it is 5 106. To ensure stable training, we apply critic warmups for 20 steps, initially training the critic before training the actor model. We set the rollout number to 4. The primary focus of the main text is the Qwen-2.5-7B [32], which has demonstrated strong reasoning potential [10, 33], while experimental results on other models are provided in the appendix. Evaluation. We use three datasetsMATH-500 [12], GPQA [25], and AIME 2024 [2]to assess the models reasoning ability on challenging tasks. We report the Pass@1 accuracy dynamics throughout the training. Random reward flip. We train the model by randomly flipping the reward with probability p, where reward of 1 is transformed to 0, and vice versa. This flip is applied on question-wise basis, meaning that if reward flip occurs for given question, the rewards for all rollout outputs corresponding to that question will be flipped. Note that flipping rewards on an output-wise basis does not effectively introduce noise. For instance, when an LLM generates multiple correct outputs, some of which are rewarded correctly while others are not, it results in sparse reward distribution. Such sparsity can only slow convergence and has minimal impact on the models final performance. Figure 3: Accuracy on three test sets during training. Due to critic warmup, the actor model is not updated during the first 20 steps; thus, the x-axis begins at step 20."
        },
        {
            "title": "2.2 Experiments",
            "content": "Experiment 1. We train the model with the probability of noise increasing from 0% to 50%, with intervals of 10%, corresponding to increasingly random reward flips. The results are shown in Figure 3. We only display the first 150 steps, as the performance has already plateaued. In MATH-500, it is evident that even with high flip rate of 40%, the final performance is only slightly lower than that of the model trained with no noise (peak score of 72.02% versus 75.85%). For lower noise levels, the final performance is comparable, and the convergence also occurs at similar rate. Only when we increase to 50%, leading the reward entirely random, the training collapse. In the other two tasks, similar trends are observed, though fluctuations are more pronounced. In particular, for p-20%, the peak performance is even higher than when p=0. We attribute such fluctuations to the inherent difficulty of the datasets, which cause larger variability in model performance. In Appendix A, we also demonstrate that Qwen-2.5-3B exhibits strong robustness to noise, comparable to the 7B models. In contrast, Llama-3.1-8B shows considerably weaker reasoning capabilities and underperforms even in noiseless settings. This underperformance aligns with prior findings that highlight the limited foundational reasoning abilities and RL improvements in Llama models [23, 33, 10]. These limitations result in poor noise tolerance. Based on the overall performance of the three models analyzed in this study, we carefully formulate Takeaway 1 with an important caveat: while large language models can exhibit robustness to reward noise, this property holds primarily for models with strong reasoning capabilities. This is both practical and necessary conditionmodels lacking foundational reasoning abilities, such as Llama3.1-8B, are unlikely to be effectively used in real-world applications, regardless of the level of reward noise, and are therefore of limited interest for our study. Takeaway 1. For models with strong reasoning potential, even with significant opposite noise in the verification rewards, the model can still be effectively trained during RL. Experiment 2. Given the surprising result from Experiment 1, the key question is why assigning reward of 1 to outputs with genuinely incorrect answers does not have significant detrimental effect. Since the answer is incorrect, we hypothesize that the reasoning process itself might still be valuable and worth rewarding. If this were not the case, it would be challenging to achieve performance comparable to noiseless setups, and convergence would likely be slower at the very least. To test this hypothesis, we conducted the experiment 2: We first identified high-frequency phrases that imply certain desired reasoning patterns, such as We know that and First need to, in the outputs of model trained with p=0.3 Next, we designed rule-based reward function: instead of verifying the correctness of the answer, the model would receive reward of value each time pre-identified reasoning phrase appeared in the output. The total reward is clipped to 1, creating simple keyword-matching reward that ranged from 0 to 1. We name this strategy as Reasoning Pattern Reward (RPR). 3These phrases also frequently appear in model outputs trained with higher levels of noise. 4 Figure 4 illustrates how RPR works, and an example code is provided in Figure 9. To prevent the model from hacking the reward by outputting repeated reasoning phrases (e.g., We know that We know that We know that...), repetition penalty [33] is used. Since our goal was to validate the hypothesis rather than achieving stable stateof-the-art accuracies, was somewhat arbitrarily set to 40 without value tuning, and for simplicity, we set = 1/n = 0.025. The results are presented in Figure 3 for ease of comparison. Remarkably, even without verifying the correctness of reasoning during the training process, the model demonstrates strong reasoning capabilities in the early stages, achieving peak performance of 70.21% on the MATH-500 task. The peak performance on other tasks also shows minimal gap compared to models trained without noise. However, as training progresses, the performance eventually declines. Upon analyzing the outputs, we found that this decline was due to overthinking. Specifically, after reasoning through few steps, the model revisits its prior thoughts, and this reasoning-then-revisiting cycle continues for too many iterations, resulting in an excessively long chain of reasoning that exceeds the context limit and is truncated before the final answer can be generated. The model produces these long reasoning steps from multiple viewpoints, effectively escaping the repetition penalty. An example of this is shown in Figure 10, where the model has already arrived at the correct answer but continues to reason, preventing the extraction of the final answer. Figure 4: An illustration of how the reasoning pattern reward works through two example outputs. Suppose the red text represents highfrequency phrases that we have pre-identified as indicating key reasoning processes. In the first output, five key phrases are present, so the reward is 5r. Similarly, the second output contains four key phrases, so the reward is 4r. We do not verify the correctness of the answer. The lack of answer verification and supervision, combined with strong peak performance, suggests that the model does not require substantial new knowledge through supervision to final answers in RL post-training. Instead, most of the improvements in solving challenging tasks were already learned during pretraining and are activated by RL through rewarding effective reasoning patterns that can lead to correct answers. This experiment provides direct evidence illustrating the role of RL in LLMs during post-training. Takeaway 2. Training LLMs solely based on reasoning pattern rewards, without any correctness check, can develop powerful, though transient, reasoning abilities. This is direct evidence that LLMs do not require much new knowledge during RL; instead, RL explores outputs that are likely to lead to correct answers and reinforces those reasoning patterns. Remark. Key reasoning phrases we collected are broadly applicable across tasks. As shown in Experiment 4 (Section 3.3), RPR patterns from math tasks remain effective in diverse, open-ended domains. While there may be other effective keywords or even implicit hidden states [11] that trigger reasoning, pursuing more refined design is not the focus of this study."
        },
        {
            "title": "3 Noisy reward models in open NLP tasks",
            "content": "Now we turn to the open NLP tasks requiring reward models (RMs). Different from manually flipping rewards in math tasks, the noise level in open NLP tasks can be approximately reflected in the varying evaluation accuracies of RMs. We first introduce the data, RM training details, and then introduce experiments with noisy RL rewards and corresponding findings. 5 Figure 6: The prompt used in the HelpSteer3 task, where the question and chat history placeholders are filled accordingly."
        },
        {
            "title": "3.1 Preliminaries: Training reward models with varying accuracy",
            "content": "Dataset. We use the NVIDIA HelpSteer3 [30] dataset, which contains 40.5K multi-domain openended questions that require helpful assistance. Each question is paired with two responses, evaluated by multiple annotators for helpfulness, categorized into seven fine-grained levels. There is also chat history preceding the current question, providing context for the question. The dataset is split into training set of 38.5K samples and validation set of 2K samples. Training. Our reward model is built on Qwen-2.5-7B model with an added prediction head. We simplify the original seven-level helpfulness scale into binary classification task: the more helpful response in each pair is labeled as 1, and the less helpful one as 0. For each response, we concatenate it with the chat history as the input to the reward model. The prediction head produces scalar output s, and we optimize the model using the MSE between and the corresponding binary label [16, 35, 7]. The model learns to predict the absolute helpfulness, facilitating further RL, instead of using contrastive learning to compare the relative helpfulness of paired responses. The learning rate is 106, and the RM is trained for 25,000 steps. The evaluation accuracy dynamics of RMs are shown in Figure 5. The best-performing model achieved an evaluation accuracy of 85%. Different RM models with varying accuracies are used in subsequent experiments to simulate the scenarios with different levels of reward noises similarly in practical usages."
        },
        {
            "title": "3.2 Learning to reason using reward\nmodels of varying accuracy",
            "content": "Figure 5: Reward models accuracy across the training. Checkpoints at specific steps are used for RL experiments. Training. The hyperparameters used in this section basically follow those employed in previous math experiments. Training is conducted with Qwen-2.5-7B on the HelpSteer3 dataset, lasting total of 200 steps. Figure 6 shows the prompt template, which instructs the model to first carefully consider how to provide useful assistance. It then asks the model to summarize its reasoning and present the final response within the <answer> tag. Importantly, the RM only evaluates the text within the <answer> tag, not the entire output. This approach ensures that the reward pipeline aligns with the one used in the mathematical experiments. Evaluation. Evaluating open-ended tasks during training presents much greater challenge than evaluating math problems, due to the lack of objective criteria and the absence of reliable, efficient evaluators. Because our most accurate reward model (RM) is used during training, it cannot be employed for evaluation at test time, as LLMs may learn to hack its preferences. It is also prohibitive to ask more advanced LLMs like ChatGPT or human evaluators to frequently evaluate models during training. As result, we perform evaluation only after training, using subset of 200 samples from the evaluation set, assessed by both GPT-4o and human evaluators. Specifically, we compare two models by having GPT-4o and human evaluators assess their responses to the same question. Only text in <answer> tags is used for evaluation. 6 The prompt used for GPTs evaluation is shown in Figure 16. The evaluation considers factors including helpfulness, informativeness, reasoning, and coverage of user needs. To avoid bias from positional preferences [17, 5, 36] in language models, ChatGPT-4o evaluates an output pair twice for the same question, each time with different order. models response may result in win, loss, or tie relative to the other models response, with the results presented in pie charts. In the main text, we report GPT evaluation scores, as they are more reproducible for the community. Details on human evaluationguidelines, results, and inter-evaluator agreement measured by Fleiss Kappa [8]are provided in Appendix B. There, we show that human evaluation aligns with GPT assessments, with evaluators demonstrating moderate to substantial agreement. Experiment 3. We compare the performance of the Qwen-2.5-7B model trained with reward models (RMs) of varying accuracies: 85%, 75%, and 65%. The results are presented in Figure 7. The 85%- accurate RM yields only modest 4% net win-rate advantage over the 75%-accurate RM, suggesting that their performances are similar. Notably, the LLM trained using the 65%-accurate RM shows significant decline in downstream performance. This decline can be attributed to multiple factors: First, while one might expect the 85% RM to be just 1.31 times (i.e., 85/65) better than the 65% RM based on raw accuracy, the actual difference in the number of misclassified labels is more than double (35% vs. 15%). This nonlinear increase in noise significantly impacts the quality of the training signal. Figure 7: Qwen-2.5-7B trained with an 85%- accurate RM performs similarly to using 75%-accurate RM, but significantly better than using 65%-accurate RM. The Net Win refers to the performance advantage of the former RM over the latter. Second, beyond accuracy, the magnitude and distribution of reward scores are also critical. In contrast to domains like math problem solvingwhere rewards are typically binary (e.g., 0 or 1)RMs, especially less accurate ones, tend to produce scores clustered around 0.5, even when they make correct classifications. This clustering reflects underlying model uncertainty. This effect is exhibited in the reduced variance of reward outputs from lower-accuracy RMs: on validation set, the score variances are 0.1937, 0.1161, and 0.0672 for the 85%, 75%, and 65%-accurate RMs, respectively. more accurate RM pushes scores further from the decision boundary, helping to avoid both overand under-estimated rewards. These observations align with findings by [24], who emphasized that higher variance is also key factor in RM effectiveness. In summary, both the lower accuracy and lower variance of the 65%-accurate RM likely contribute to its weak downstream performance. However, disentangling the individual effects of these factors remains challenge, as training RMs with both targeted accuracy and targeted variance for clean ablations is difficult. Nonetheless, our results demonstrate that the Qwen-2.5-7B model performs comparably when trained with reward models that are 75% and 85% accurate, indicating degree of robustness to reward noisethough this robustness is less pronounced than what has been observed in mathematical tasks. Takeaway 3. While reward noise in neural reward models arises from multiple factors, the robustness to such noise observed in mathematical tasks persistsalbeit to different extentin open-ended tasks. We now understand that effective reasoning does not necessarily require using reward models (RMs) with the highest possible accuracy. This insight may offer some relief for real-world applications, where researchers often worry that their RMs are not sufficiently accurate. However, as demonstrated by our RM with 65% accuracy, some noisy RMs are indeed inadequate for practical use as the sole source of reward. Recognizing the importance of reasoning patterns, we propose method for calibrating RMs with RPR (Section 2). This approach overcomes the performance ceiling imposed by the limitations of the reward models at hand."
        },
        {
            "title": "3.3 Calibrating noisy RMs with reasoning pattern reward",
            "content": "Method. Considering that (1) it is impossible to train perfect RM, and (2) exploring effective reasoning patterns is crucial for LLMs, we wonder whether reasoning pattern reward (RPR, Section 2) could help calibrate noisy RMs and thus obtain better performance. There are two potential ways to calibrating noisy rewards based on the value of reasoning patterns: 1. Compensatory reward for underestimated responses. When an RM produces false negatives, assigning low score to an objectively good response, we give it some compensation. 4 We assume that responses that display better reasoning patterns are likely to be closer to the objectively good ones. Therefore, we reuse the RPR as the compensation reward. 2. Discounting for overestimated responses. Conversely, when an RM provides false positive results, that is, it incorrectly assigns high score to objectively poor response, we can apply discount to RM scores. However, this situation is more complex than discounting false negatives. The main challenge is determining the appropriate discount factor. For instance, if response receives full score but lacks key reasoning phrases, should its reward be near zero? Setting it too low could overemphasize reasoning pattern rewards, leading to overthinking and performance collapse, as discussed in Section 2. This remains an open research question: how can we effectively calibrate an RM when the noisy reward is false positive? Given these considerations, we only introduce the first method to calibrate the RM model: When the RM outputs low score (as determined by threshold τ ), we calculate an RPR score only for the thought text (enclosed in <think> tags), while text in <answer> tags is not considered. This RPR score is added to the RM output, scaled by weight α. This calibration incurs no additional time or memory costs. Note that in our approach, RPR compensates not only for false negatives but also for true negatives. This is not problematic, as we demonstrated in Section 2 that true negative responses still contain valuable reasoning patterns and are therefore worth rewarding. Another potential concern is that using RPR as the sole reward signal might lead to performance collapse (Figure 3). However, when RPR is used as an auxiliary signal rather than the sole reward, LLMs are trained effectively without such collapse in the following experiments. Figure 8: Reward noise calibration effectively enhances downstream performance. Experiment 4. We use reward models with accuracies of 65% and 85% to conduct several comparisons: (1) Calibrating the RMs using RPR, applying it to post-training Qwen models, and comparing their performance with models trained solely with the original RMs. (2) Comparing Qwen model trained with 65%-accurate RM calibrated by RPR to model trained with an 85%-accurate RM. We set the threshold τ to 0.5 and α to 0.1. The choice of α is discussed in Appendix E. The results in Figure 8 demonstrate the effectiveness of RPR calibration: 1. The calibrated 65%-accurate reward model lags only 8% behind the 85%-accurate modelan improvement from an initial 25% gap, highlighting the substantial gains achieved through calibration. 2. Calibrating noisy RMs boosts downstream LLM performance, outperforming LLMs trained with original RMs. Even the 85%-accurate RM continues to improve after RPR calibration. RPR calibration addresses the limitations of RMs at hand. Notably, the improvements observed are not due to an increase in reward score variance (0.1889 and 0.0653 for the 85% and 65%-accurate RMs post-calibration), as variance actually decreases 4We acknowledge that there are no objective rules in this task; By objectively, we refer to whether rewarding the response eventually improves performance on the test set. If it does, the response should be considered good, at least from deep learning perspective. slightly. We provide output examples from models trained with both the original and calibrated RMs in Appendix C. Furthermore, in Appendix A, we show that the Qwen-2.5-3B model, which fails to emerge strong reasoning capabilities under the original RMs, demonstrates such abilities when trained with calibrated RMs. This suggests that reasoning pattern rewards not only raise the performance ceiling constrained by reward models in post-trained language models, but also lower the requirements for pre-trained models, enabling less capable models to exhibit reasoning behavior. Takeaway 4. RPR can calibrate noisy reward models, particularly when they provide potentially false negative rewards. Its downstream success highlights the importance of strong reasoning patterns in improving reasoning ability. Remark. While our RPR is straightforward and could potentially benefit from more refined design, our primary goal is to demonstrate the importance of reasoning patterns and their effectiveness in real-world scenarios. For this purpose, the current simple RPR is sufficient to convey both points through Experiment 4."
        },
        {
            "title": "4 Related works",
            "content": "Reward model accuracy. An accurate reward model was considered crucial for successful RL [9, 15, 18, 37]. Even in math tasks where rewards are calculated by verification functions, Yeo et al. [33] proposed that it is beneficial to refine the reward function with fine-grained approach for accurately evaluating math answers, considering factors such as output length, correctness, and repetition. However, Chen et al. [4] found that more accurate reward models do not necessarily lead to stronger LLMs in downstream tasks. Razin et al. [24] argues that high reward variance is also important for making the reward model good teacher. Additionally, Wen et al. [31] suggested that relying solely on accuracy does not fully capture the impact of reward models on policy optimization. Studying the accuracy of reward models from noise perspective offers some new insights. We argue that an accurate reward model is not always necessary in practice, though calibration to noisy rewards improves evaluation. We provide the first evidence of LLMs robustness to significant reward noise. The role of RL in post-training LLMs. This paper aligns with recent studies suggesting that pre-trained models already possess the fundamental reasoning abilities needed for complex tasks. Yeo et al. [33] found that pre-training data often includes long chain-of-thought patterns, establishing foundation for reasoning. Similarly, Yue et al. [34] noted that base models can perform similarly to RL-post-trained models after multiple attempts at difficult tasks. Gandhi et al. [10] showed that Qwen models outperform Llama [19] models in downstream tasks post-RL, with Qwen models exhibiting natural reasoning behavior. Prior works [1] demonstrated that reasoning can emerge during pre-training, with models using reasoning trigger token like wait to activate chain-of-thoughts and arrive at the correct answer. We provide strong evidence that models can achieve peak performance, comparable to those trained with strict verification, by rewarding key reasoning patterns instead of requiring correctness verification. While RL post-training has seen significant progress, our findings highlight the continued importance of pre-training in building advanced LLMs. From post-training perspective, this also explains why small amount of high-quality data [20] can enhance reasoning abilities, as the foundational capabilities are already present and need effective triggers."
        },
        {
            "title": "5 Conclusions",
            "content": "We studied the reward noise, practical consideration for real-world post-training of LLMs. Our findings show that LLMs are highly robust to significant reward noise. Surprisingly, when trained solely with reasoning pattern rewards (RPR), the model achieved peak downstream performance on par with models trained with strict correctness verification and accurate rewards. Recognizing the importance of reasoning processes over final answers, we use RPR to calibrate noisy reward models. RPR reduces false negative rewards and improves LLM performance on open-ended tasks. In the future, enhancing foundational ability during pre-training continues to be promising, and our findings also provide insights for improving post-training techniques."
        },
        {
            "title": "References",
            "content": "[1] Essential AI, :, Darsh Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Anthony Polloreno, Ashish Tanwer, Burhan Drak Sibai, Divya Mansingka, Divya Shivaprasad, Ishaan Shah, Karl Stratos, Khoi Nguyen, Michael Callahan, Michael Pust, Mrinal Iyer, Philip Monk, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, and Tim Romanski. Rethinking reflection in pre-training, 2025. [2] Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. [3] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. [4] Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, and Xiaoyu Shen. The accuracy paradox in RLHF: When better reward models dont yield better language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 29802989, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [5] Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan. Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1116011174, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [6] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [7] Yanchen Deng, Chaojie Wang, Zhiyi Lyu, Jujie He, Liang Zeng, Shuicheng YAN, and Bo An. Q*: Improving multi-step reasoning for LLMs with deliberative planning, 2024. [8] J.L. Fleiss et al. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378382, 1971. [9] Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. How to evaluate reward models for rlhf, 2024. [10] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. [11] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space, 2024. [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [13] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. [14] Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39854003, Online, November 2020. Association for Computational Linguistics. 10 [15] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Lester James Validad Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating reward models for language modeling. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, pages 17551797, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. [16] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms, 2024. [17] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. [18] Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. RM-bench: Benchmarking reward models of language models with subtlety and style. In The Thirteenth International Conference on Learning Representations, 2025. [19] AI @ Meta Llama Team. The llama 3 herd of models, 2024. [20] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [21] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2022. [22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. [23] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [24] Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason D. Lee, and Sanjeev Arora. What makes reward model good teacher? an optimization perspective, 2025. [25] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. [26] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation, 2018. [27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [28] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [29] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [30] Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, and Oleksii Kuchaiev. Dedicated feedback and edit models empower inferencetime scaling for open-ended general-domain tasks, 2025. [31] Xueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, XingYu, Xinyu Lu, Ben He, Xianpei Han, Debing Zhang, and Le Sun. Rethinking reward model evaluation: Are we barking up the wrong tree? In The Thirteenth International Conference on Learning Representations, 2025. [32] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [33] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. [34] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. [35] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReSTMCTS*: LLM self-training via process reward guided tree search. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [36] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and Zhangyang Wang. Found in the middle: How language models use long contexts better via plug-and-play positional encoding, 2024. [37] Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Rmb: Comprehensively benchmarking reward models in llm alignment, 2025. [38] Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 4303743067. PMLR, 2329 Jul 2023. 12 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 from collections import Counter def c t _ a _ e i _ a ( text , ) : words = text . split () ngrams = [ tuple ( words [ : + ]) for in range ( len ( words ) - + 1) ] ngram_counts = Counter ( ngrams ) total_ngrams = len ( ngrams ) repeated_ngrams = sum (1 for count in ngram_counts . values () if count > 1) repetition_penalty = repeated_ngrams / total_ngrams if total_ngrams > 0 else 0 return re petition_pe nalty def ea i _p e _r ar ( solution ) : reason_pos = solution . find ( \" Assistant : < think > \" ) solution_str = solution [ think_pos + len ( \" Assistant : < think > \" ) :] score = 0 solution_str = solution . lower () score += float ( \" need to \" in solution_str ) score += float ( \" we need to \" in solution_str ) score += float ( \" wait \" in solution_str ) score += float ( \" alternatively \" in solution_str ) score += float ( \" let me check \" in solution_str ) score += float ( \" let me see \" in solution_str ) score += float ( \" let focus on \" in solution_str ) score += float ( \" we know that \" in solution_str ) score += float ( \" we can observe \" in solution_str ) score += float ( \" we can see \" in solution_str ) score += float ( \" let me try \" in solution_str ) score += float ( \" let try \" in solution_str ) score += float ( \" let us try \" in solution_str ) score += float ( \" first , \" in solution_str ) score += float ( \" firstly , \" in solution_str ) score += float ( \" next , \" in solution_str ) score += float ( \" finally , \" in solution_str ) score += float ( \" let us first \" in solution_str ) score += float ( \" let first \" in solution_str ) score += float ( \" let me first \" in solution_str ) score += float ( \" try again \" in solution_str ) score += float ( \" still not \" in solution_str ) score += float ( \" not working \" in solution_str ) score += float ( \" not correct \" in solution_str ) score += float ( \" does not work \" in solution_str ) score += float ( \" doesn work \" in solution_str ) score += float ( \" makes sence \" in solution_str ) score += float ( \" since we \" in solution_str ) score += float ( \" because we \" in solution_str ) score += float ( \" consequently \" in solution_str ) score += float ( \" as result \" in solution_str ) score += float ( \" thus \" in solution_str ) score += float ( \" therefore \" in solution_str ) score += float ( \" hence \" in solution_str ) score += float ( \" so that \" in solution_str ) score += float ( \" thereby \" in solution_str ) score += float ( \" if we \" in solution_str ) score += float ( \" given there \" in solution_str ) score += float ( \" for instance \" in solution_str ) score += float ( \" for example \" in solution_str ) score /= 40 score -= c t _ a _ e i _ a ( solution_str , 20) score = max (0 , score ) return score Figure 9: An example code of reasoning pattern reward without checking answer correctness. Figure 10: An example of output in the later stage of RL math training, where only the reasoning pattern reward is used without correctness verification. The model has arrived at the correct answer A, but due to the ongoing reasoning process, the <think> tag remains open, causing the output length to reach the limit and preventing the correct answer from being generated in answer tags. 14 Experiments on Qwen-2.5-3B and Llama-3.1-8B In Figure 11, we demonstrate that Qwen-2.5-3B exhibits strong robustness to significant reward noise (Experiment 1). Specifically, it can tolerate up to 40% of rewards being flipped while still achieving final performance comparable to the noiseless setup. However, its convergence under noisy rewards is noticeably slower than that of the 7B models. Additionally, performance on the AIME tasks continues to fluctuate, consistent with the behavior observed in the 7B model. For Experiment 2, we find that using RPR as the sole reward signal effectively enables the model to reach peak performance comparable to the noiseless baseline. Notably, on the most challenging AIME tasks, RPR yields the highest peak performance across all setups. Figure 11: Qwen-2.5-3B results for Experiments 1 and 2: Accuracy on three test sets during training. In the HelpSteer3 task, vanilla RL fails to enable Qwen-2.5-3B to perform effective reasoning. We observe that response lengths initially increase but then rapidly collapse to just few tokens. This pattern of first-reason-then-collapse has also been observed in [23], where an LLM is trained to reason on tasks beyond its initial capabilities. However, the underlying mechanisms driving this length dynamics remain understudied. In contrast, when trained with RPR-calibrated RMs (accuracy > 75%), Qwen-2.5-3B exhibits clear reasoning behaviors. As shown in Figure 12(a), the response lengths differ significantly between models trained with original versus calibrated RMs. These results echo the core insight of Experiment 4: calibrated RMs more effectively evoke reasoning abilities in large language models. Subfigure (b) shows that using an 85%-accurate RM yields only 5-point improvement in net win rate over the 75%-accurate RMmirroring observations in 7B models in Experiment 3. Figure 13 and Figure 14 present two sample outputs from Qwen-2.5-3B trained with the 85%-accurate RM, demonstrating that we have successfully elicited the basic reasoning capabilities of this smallscale model, despite some imperfections in these outputs. In Figure 13, the Chinese query asks for the creation of PowerPoint file to teach primary school students about statistical charts. The 3B model engages in step-by-step reasoning to generate coherent PowerPoint structure and follows through on its plan. In Figure 14, the model processes complex chat history and solves physics problem, despite not being explicitly trained for mathematics or physics. Figure 12: (a) Average response length of Qwen-2.5-3B during training with original vs calibrated RMs. The calibrated RMs successfully enable this small-scale model to perform reasoning, whereas the original RMs fail. (b) Experiment 3 using Qwen-2.5-3B models trained with calibrated RMs. 15 Figure 13: Our calibrated RMs successfully elicit Qwen-2.5-3Bs reasoning ability, whereas the original RM fails to do so. This figure presents 1 of 2 output cases. The Chinese question translates to: Please design teaching PowerPoint for teaching elementary school students about [statistical charts]. The overall structure of the PPT should include three parts: introductory activity, developmental activity, and summary activity. Each of these activities must include: question that triggers student inquiry, along with an image illustrating the context of the problem. concept explanation (detailed). math hands-on activity using multiple representations, with clearly listed step-by-step process. At the end of the PPT, include formative assessment checklist. Each checklist item should start with the symbol \"\" aligned to the left and should address three aspects: knowledge, skills, and attitudes. Please present the entire PPT in Markdown format with three-level heading structure. For fractions, use the a/b format as it is easier for me to understand. As for the images, first translate the QUERY into English, and then use the following website with an English query to retrieve the images:https://source.unsplash.com/960x640/?QUERY Use Markdown syntax to display the images. Llama models are widely recognized to exhibit inherent weaknesses in reasoning capabilities and limited potential for improvement through reinforcement learning [23, 33, 10]. As shown in Figure 15, LLaMA-3.1-8B performs significantly worse than the Qwen models under noiseless conditions and suffers marked degradation in performance as noise increases. The accuracy on MATH-500 drops to 0 at = 30%. Due to its limited foundational capabilities, LLaMA-3.1-8B is not suitable for effective training on the HelpSteer3 task, and thus we omit its results. 16 Figure 14: This figure presents the second of two output cases from Qwen-2.5-3B, trained with the calibrated 85%-accurate RM. The model demonstrates reasoning to solve physics problem. Figure 15: Llama-3.1-8B results for Experiments 1 and 2: Accuracy on three test sets during training (Llama-3.1-8B)."
        },
        {
            "title": "B Human evaluation",
            "content": "B.1 Guidelines We recruited three graduate students with expertise in model evaluation. Each evaluator spent approximately 8 hours completing all tasks and earned $70 USD. The human evaluation was granted by our institute, with the payment slightly above the standard wage for graduate students working in AI companies in our country. Below is the guideline for human evaluators: Guideline for Evaluating Responses: 17 Figure 16: The evaluation prompt for GPT, designed according to the core guidelines for human annotators. The placeholders will be replaced with user-assistant chat history and two models responses. Your task is to determine which of the two responses better addresses the users latest request. Steps to Follow: Review the Conversation History: Carefully read the conversation history provided. The users most recent question will be the last message, and that is the request you need to evaluate the responses against. Examine the Two Responses: You will be presented with two possible replies from two AI assistants (Response #1 and Response #2). Criteria for Evaluation: Evaluate each response based on the following factors: Helpfulness: Does the response directly answer the users request? Is it practical and useful? Amount of Information: Does the response provide sufficient details to address the request thoroughly? Clarity and Coherence: Is the response easy to understand, and does it present information logically? Thoroughness: Does the response cover all aspects of the users request? Is anything missing or incomplete? Avoid Quick Judgment: We will randomize the response order from two models. You cannot infer which one is always better based on the order. Also, dont assume one response is better simply because its shorter or longer. After evaluating both responses, decide which one is more helpful overall. You can choose #1 is better, #2 is better, or they tie with each other. Write the evaluation, as well as reasons. B.2 Results and inter-annotator agreement In Figure 17, we present the averaged human evaluation results for Experiments 3 and 4 in Section 3. Each figure also reports inter-evaluator agreement κ, with all experiments demonstrating moderate (0.4 < κ 0.6) to substantial (0.6 < κ 0.8) consistency among evaluators. key distinction between human evaluations and those from GPT-4o is that human judges exhibit stronger discriminatory ability, resulting in fewer comparisons marked as ties. Nonetheless, the overall conclusionssuch as the net win ratios and the impact of calibrationalign closely with the GPT-based evaluations. Therefore, we do not repeat Takeaways and conclusions here. Figure 17: Human evaluation results and agreements."
        },
        {
            "title": "C Case studies",
            "content": "Figures 19 and 20 show outputs from Qwen-2.5-7B trained with the calibrated and original 85%- accurate RMs, respectively. With RPR, the generated code and comments are more detailed, and both the main function and interaction loop are more comprehensive compared to the single test case produced by models trained without RPR. The reasoning process is also more thoroughly articulated. Figures 21 and 22 illustrate Qwen-2.5-7B trained with calibrated and original 65%-accurate RMs, respectively. Compared to models trained with 85%-accurate RMs, both outputs here fail to explicitly move the model to the GPU. However, the model trained with the calibrated 65%-accurate RM correctly implements chatbot using the transformers pipeline API, which implicitly moves the model to the GPU. As result, the model trained with the original 65%-accurate RM performs slightly worse in terms of helpfulness. It is uncommon for an assistant to build chatbot using the transformers pipelinean approach that is both concise and effectivesuggesting that Qwen models have acquired substantial knowledge during pretraining. Limitations, broader impacts and safety issues Due to resource limitations, we limit our experiments to models with up to 78 billion parameters. However, because our results are consistent even for smaller Qwen-2.5-3B models, and given that the 14B and 70B Qwen models exhibit stronger inherent reasoning abilities, the scalability of our takeaways might not be concern, as the core premise of these findings relies on strong reasoning capabilities in pre-trained models. Regarding broader impacts, we hope this paper will inspire future advancements in post-training techniques. It is possible that efficient tuning methods could yield similar outcomes with RL. Additionally, we highlight the importance of continuing efforts to enhance fundamental reasoning abilities during the pre-training stage. This paper presents findings and insights into post-training LLMs using RL with noisy rewards, which do not raise safety concerns."
        },
        {
            "title": "E RPR weight in calibration",
            "content": "In Section 3.3, we set τ = 0.5 since the RMs are binary classifiers. For the scaling factor α, we initially used value of 1, but observed little difference in training dynamicssuch as average response lengthcompared to using the original RMs. We then experimented with values of α ranging from 1 to 0.1. Notably, reducing α to 0.1 led to significant increase in average response length (see Figure 18), suggesting shift in the models reasoning behavior. Based on these findings, we set α = 0.1. 19 Figure 18: α = 0.1 enables Qwens effective reasoning in HelpSteer3 task. These experiments use the 85%-accurate RM. Figure 19: How to create chatbot using an LLM: the answer from Qwen-2.5-7B trained with the calibrated 85%-accurate RM. 20 Figure 20: How to create chatbot using an LLM: the answer from Qwen-2.5-7B trained with the original 85%-accurate RM. 21 Figure 21: How to create chatbot using an LLM: the answer from Qwen-2.5-7B trained with the calibrated 65%-accurate RM. 22 Figure 22: How to create chatbot using an LLM: the answer from Qwen-2.5-7B trained with the original 65%-accurate RM."
        }
    ],
    "affiliations": [
        "GSAI, Renmin University of China",
        "Large Language Model Department, Tencent",
        "School of Computer Science, Wuhan University",
        "University of Macau"
    ]
}