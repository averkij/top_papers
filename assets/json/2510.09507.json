{
    "paper_title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
    "authors": [
        "Zixin Zhang",
        "Kanghao Chen",
        "Xingwang Lin",
        "Lutao Jiang",
        "Xu Zheng",
        "Yuanhuiyi Lyu",
        "Litao Guo",
        "Yinchuan Li",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 0 5 9 0 . 0 1 5 2 : r PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs PHYSTOOLBENCH: BENCHMARKING PHYSICAL"
        },
        {
            "title": "TOOL UNDERSTANDING FOR MLLMS",
            "content": "Zixin Zhang1,, Kanghao Chen1,, Xingwang Lin3, Lutao Jiang1, Xu Zheng1, Yuanhuiyi Lyu1, Litao Guo1, Yinchuan Li4, Ying-Cong Chen1,2, 1HKUST(GZ) 2HKUST 3Beihang University 4Knowin Figure 1: For an Embodied Agent, using physical tools is crucial in many tasks. The understanding of physical tools significantly impacts the tasks success rate and execution efficiency (Top). PhysToolBench (Bottom) systematically evaluates the understanding of physical tools of multimodal LLMs. The benchmark is designed with three progressive levels of difficulty and employs Visual Question Answering (VQA) format. Notice that in the actual benchmark, tools in the images are numerically labeled, and images here are for illustrative purposes only."
        },
        {
            "title": "ABSTRACT",
            "content": "The ability to use, understand, and create tools is hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any generalpurpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: 1) Tool Recognition: Requiring the recognition of tools primary function. 2) Tool Understanding: Testing the ability to grasp the underlying principles of tools operation. 3) Tool Creation: Challenging the model to fashion new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMsspanning proprietary, open-source, specialized embodied, and backbones in VLAsreveals significant deficiency in the tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available at PhysToolBench Repository. 1 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs"
        },
        {
            "title": "INTRODUCTION",
            "content": "Man is tool-using animal. Without tools, he is nothing; with tools, he is all. Thomas Carlyle key factor in humanitys success throughout natural evolution is the ability to create and utilize vast array of tools to enhance survival and prosperity. With the advancement of technology, humans continuously reshape the physical world, inventing diverse instruments to extend the boundaries of their capabilities. For an embodied intelligent agent designed to complete physical tasks, the use of tools is prerequisite for achieving success and efficiency. For instance, as illustrated in Fig. 1 (Top), robot must use hammer to drive nail into walla task it cannot accomplish with its bare manipulators. Arguably, profound understanding of physical tools is fundamental precondition for Artificial General Intelligence (AGI). (Bai et al., 2025; Hurst et al., 2024; OpenAI, Multimodal Large Language Models (MLLMs) 2025b; Anthropic, 2025; Comanici et al., 2025), which can process inputs from both vision and language modalities, have acquired substantial common-sense knowledge from being trained on massive datasets. They show great promise for evolving into AGI and have been the focus of numerous studies for deployment in robotics. Some studies employ MLLMs as high-level planners (Yuan et al., 2025; Team et al., 2025a; Driess et al., 2023), while others utilize them for low-level control as the backbone of Vision-Language-Action (VLA) models (Black et al., 2024; Kim et al., 2024; Wen et al., 2025; Black et al.). In either case, interaction with the physical world is fundamental, which inevitably involves the use of physical tools. Although some research has demonstrated that MLLMs possess preliminary understanding of tools (Gao et al., 2025; Tang et al., 2025; Trupin et al., 2025), the true depth of physical tool comprehension remains largely unexplored. Based on these considerations, we propose PhysToolBench, benchmark for evaluating an agents understanding of physical tools. To the best of our knowledge, this is the first benchmark specifically designed for this purpose. To evaluate an agents practical capabilities, we designed Visual Question Answering (VQA) benchmark that simulates robotic workflow. Presented with task and an image of objects, the agent must select the appropriate tool(s). As shown in Fig. 1 (Bottom), the benchmark features three difficulty levels to progressively assess the agents depth of understanding: 1) Easy (Recognizing Tools). This fundamental level assesses whether an agent can identify conventional tool and its primary function. 2) Medium (Understanding Tools). This intermediate level probes the agents comprehension through three distinct challenges: optimal tool selection from functionally similar options, selection of all tools required for multi-tool task, and assessment of tools operational viability based on its physical state. 3) Hard (Creating Tools). This advanced level evaluates an agents inventive capabilities. Faced with task and no standard tools, the agent must fashion solution by repurposing or combining available objects, which requires an understanding of the physical principles underlying the required tool. We evaluate the performance of 32 MLLMs on PhysToolBench, spanning four distinct classes: general-purpose proprietary MLLMs, general-purpose open-source MLLMs, MLLMs tailored for embodied AI, and those functioning as backbones in VLAs. The results demonstrate clear performance ceiling, with even the most advanced proprietary models scoring no higher than 63%, revealing profound disparity with human proficiency in tool understanding (over 90%). Furthermore, our analysis uncovers several critical weaknesses in current MLLMs: (1) failure of small MLLMs, including those within VLA models, to exhibit an emergent ability of tool understanding; (2) long-tail distribution issue in recognizing and understanding wide array of tools; (3) tendency to hallucinate tool affordances and their availability; and (4) inadequate visual reasoning skills. We further propose vision-centric reasoning framework to bolster the visual reasoning of MLLM agents. We hope our work will inspire future research on physical tool understanding."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 MLLM AND ITS APPLICATION IN EMBODIED AI Recent years have witnessed remarkable advancements in Multimodal Large Language Models (MLLMs). Building on the significant success of Large Language Models (LLMs), these models effectively process visual information by leveraging modality alignment techniques (Li et al., 2 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs 2022; Radford et al., 2021). Typically, visual encoder and connector are employed to link visual data to the LLM, enabling reasoning at the language level and granting Vision-Language Models (VLMs) sophisticated image comprehension capabilities. To date, numerous impressive MLLMs have emerged, including proprietary models (Hurst et al., 2024; OpenAI, 2025b; Comanici et al., 2025; Anthropic, 2025; xAI, 2025), as well as open-source alternatives (Bai et al., 2025; Zhu et al., 2025; Wang et al., 2025b; Wu et al., 2024; Beyer et al., 2024; Wu et al., 2024; Lu et al., 2025). These models have demonstrated powerful visual understanding across diverse range of tasks. Beyond general-purpose domains, MLLMs are also finding significant applications in embodied intelligence. On one hand, they are being utilized as the high-level brain for task planning in embodied agents, as exemplified by PaLM-E (Driess et al., 2023), RoboBrain (Team et al., 2025a), and Embodied-R1 (Yuan et al., 2025). On the other hand, research has also capitalized on the inherent common-sense knowledge within MLLMs. By adding an action head and fine-tuning on robotic data, they can be transformed into end-to-end Vision-Language-Action (VLA) models capable of directly outputting robot actions. Notable examples of this approach include π0 (Black et al., 2024), π0.5 (Black et al.), and OpenVLA (Kim et al., 2024). 2.2 PHYSICAL TOOL USE IN EMBODIED AI These advancements in foundation models have empowered robots with the ability to perform fundamental tasks when these models are embodied. For instance, embodied models such as π0, π0.5, and OpenVLA can successfully accomplish basic household chores like folding clothes and tidying desktops. However, while these tasks can be efficiently completed using only the robots own manipulators, many higher-level, real-world tasks are difficult and even impossible to achieve with robot manipulators alone. Consequently, teaching robots how to use tools to effectively accomplish complex objectives is of critical importance. Initial research has begun to explore endowing robots with tool-using capabilities. For example, VLMgineer (Gao et al., 2025) employs VLM agent to assist robots in crafting simple tools to complete tasks. Similarly, Trupin et al. (2025) leverages vision foundation models to enable tool use during task planning. MimicFunc (Tang et al., 2025) establishes an imitation learning framework that allows robots to learn tool manipulation by observing human demonstration videos. Leveraging the common-sense knowledge inherent in MLLMs, these approaches have demonstrated rudimentary ability to use physical tools. Nevertheless, the depth of physical tool understanding that their brainthe MLLMpossesses remains largely unexplored. The primary motivation for our work is to clarify this question by creating benchmark designed specifically to evaluate the understanding of physical tools within Multimodal Large Language Models. 2.3 RELATED BENCHMARKS For Large Language Models (LLMs), multitude of benchmarks (Wang et al., 2024a; Huang et al., 2023; 2024; Lu et al., 2024a; Ye et al., 2025) have been developed to evaluate their ability to utilize digital tools, such as search engines, translation services, booking systems, etc.. These benchmarks have catalyzed the rapid development of modern LLM Agents, equipping them with the capability to invoke external APIs to accomplish complex tasks. However, significant gap exists when it comes to physical tools, as there is currently no corresponding benchmark for MLLMs. We argue that such benchmark is crucial for advancing MLLMs toward becoming true Embodied Agents capable of meaningful interaction with the physical world. Among existing benchmarks for MLLMs, A4Bench (Wang et al., 2025a) is the most relevant to our research. It operates in VQA format, presenting an image of tool and asking the MLLM to identify its function from set of multiple-choice options. While this can, to some extent, reflect the MLLMs understanding of object affordances, we contend that this question-answering format lacks practical applicability. Our work, therefore, aims to establish more application-oriented evaluation. We provide the MLLM with specific task requirement and an image containing several tools, compelling it to answer the question based on the observation. This approach more rigorously assesses whether the MLLM can apply genuine knowledge and reasoning to find the optimal tool, rather than merely relying on the rote memorization of tool-function associations. 3 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 2: Statistics of PhysToolBench. (a) is the distribution of the category. (b) is the distribution of the difficulty level. (c) is the word cloud of the task description given to MLLMs."
        },
        {
            "title": "3 THE PHYSTOOLBENCH",
            "content": "3.1 OVERVIEW PhysToolBenchis VQA benchmark comprising over 1,000 text-image pairs designed to evaluate an MLLMs understanding of physical tools. Each pair consists of text prompt outlining specific task and corresponding 1024 1024 image displaying several numerically labeled tools and objects. core design constraint is that the MLLM is explicitly instructed that the items depicted in the image are the only available things, simulating realistic robotics scenario with limited resources. The MLLMs objective is to analyze the task and visual information, then output the numerical label(s) of the required tool(s), or None if no suitable tool is available. PhysToolBenchspans four major domains: Daily Life, Industrial, Outdoor Activities, Professional Settings, and three difficulty levels: Easy, Medium, Hard. Detailed statistics are shown in Fig. 2. 3.2 DESIGN PRINCIPLES To progressively evaluate the depth of an MLLMs understanding, we designed PhysToolBenchwith three distinct difficulty levels: Easy, Medium, and Hard, each demanding more profound comprehension of tool properties and functionality. The Easy level assesses fundamental tool recognition. Questions are answerable with basic tool identification and common-sense knowledge. Task prompts are straightforward, and the image always contains tool whose primary function directly matches the task. For example, to cut vegetables, the image will include kitchen knife. The Medium level requires deeper understanding of tools, necessitating reasoning based on specific task constraints. This tier is subdivided into three challenges: 1) M.1. Attribute Understanding, requiring comprehension of tools specific attributes (e.g., selecting cast-iron skillet for its high heat tolerance); 2) M.2. Tool Combination, evaluating the ability to combine tools to unlock new affordances (e.g., inserting batteries into remote); and 3) M.3. Availability Understanding, testing the recognition of non-functional tools (e.g., identifying cracked plunger as unusable). The Hard level assesses higher-order reasoning and creativity. The model must work backwards from task requirements to innovatively utilize surrounding objects. For instance, if tasked to tighten flat-head screw without suitable screwdriver, the MLLM must identify that coin can serve as substitute. We propose these difficulty levels as tiered evaluation standard. The Easy score serves as prerequisite for basic tool-use planning, Medium benchmarks potential in complex scenarios, and Hard presents forward-looking challenge for AGI research. 4 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 3: MLLM Leaderboard on our PhysToolBench, ranked by overall performance. 3.3 DATASET COLLECTION PROCESS The collection of test samples for PhysToolBenchwas conducted in three phases to ensure quality. Phase 1: Conceptualization. Human experts designed task-scene pairs, consisting of task requirement and detailed scene description, meticulously aligning each scenario with our Easy, Medium, and Hard difficulty criteria. Phase 2: Image Generation. Scene descriptions were transformed into visual images primarily using GPT-4o-image (OpenAI, 2025a)(approximately 90%), process closely supervised by human experts who vetted for quality and realism. For complex objects that the generative model struggled with, we resorted to physical staging and photography(approximately 10%). Phase 3: Annotation and Verification. Experts used custom software tool to apply numerical labels to objects in each image. The entire dataset then underwent final, thorough review and revision by separate team to verify its integrity and ensure reliability. More details are provided in Appendix. A."
        },
        {
            "title": "4 EXPERIMENTS ON PHYSTOOLBENCH",
            "content": "4.1 BENCHMARK CANDIDATES We conducted comprehensive evaluation across four distinct categories of state-of-the-art Multimodal Large Language Models (MLLMs), encompassing 32 models in total: a) GeneralPurpose Proprietary MLLMs: GPT-5 (2025-08-17) (OpenAI, 2025b), o3 (2025-04-16) (OpenAI, 2025c), ChatGPT-4o-latest (2025-01-29) (Hurst et al., 2024), Claude-3-7-Sonnet-thinking (Anthropic, 2025), Gemini-2.5-pro (2025-05-06) (Comanici et al., 2025), Grok-4 (xAI, 2025). b) General-Purpose Open-Source MLLMs: Qwen-2.5-VL-72B-Instruct (Bai et al., 2025), Qwen-2.5VL-32B-Instruct, Qwen-2.5-VL-7B-Instruct, Qwen-2.5-VL-3B-Instruct, InternVL-3.5-38B (Wang et al., 2025b), InternVL-3.5-30B-A3B, InternVL-3.5-14B, InternVL-3.5-1B, InternVL-3-78B (Zhu et al., 2025), InternVL-3-38B, GLM-4.5V-108B (Team, 2025), Ovis-2-34B (Lu et al., 2024b), Ovis2.5-9B (Lu et al., 2025), DeepSeek-VL-2 (Wu et al., 2024), DeepSeek-VL-2-small, DeepSeekVL-2-tiny, Kimi-VL-A3B-thinking-2506 (Team et al., 2025b). c) Embodied-Specific MLLMs: RoboBrain-2-32B (Team et al., 2025a), RoboBrain-2-7B, RoboBrain-2-3B, Embodied-R1-3B (Yuan et al., 2025), Magma-8B (Yang et al., 2025) d) MLLM Backbones of Vision-Language-Action (VLA) models: Prismatic-7B (Karamcheti et al., 2024) in OpenVLA (Kim et al., 2024), PaliGemma3B (Beyer et al., 2024) in π0 (Black et al., 2024), Qwen-2-VL-2B (Wang et al., 2024b) in DexVLA (Wen et al., 2025), Phi-3-Vision-4B (Abdin et al., 2024) in TraceVLA (Zheng et al., 2024). The first category of proprietary models was evaluated via their respective APIs. For the latter three categories, the models were downloaded and deployed locally for testing. To ensure fair comparison, we used consistent text prompt for all models. The system prompt was designed to encourage 5 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Overall m1 m3 87.5% 80.5% Scene Category 93.52% 85.02% 58.5% 64.0% 62.5% 67.5% 50.5% 53.5% Hard Professional Industrial Outdoor Daily 91.17% 96.71% 93.19% 87.65% 93.42% 87.85% Difficulty Level Easy m2 96.19% 93.61% 90.78% 93.97% 89.10% 91.74% 87.77% 85.11% 90.36% 81.68% 61.54% 46.47% 51.39% 54.45% 68.02% 61.18% 56.46% 61.56% 59.41% 59.75% 61.26% 63.97% 58.82% 57.97% 62.15% 66.8% 43.53% 52.15% 52.27% 59.92% 45.88% 47.85% 50.59% 55.87% Table 1: Benchmark results on the PhysToolBench. For each difficulty level and scene category, the best performance was marked in bold and the second best was marked underline. *Prismatic7B achieves an unusually high score on the Medium-M3 difficulty. Upon inspecting its reasoning process, we discovered that the model does not generate sound reasoning but instead exhibits strong tendency to output None in all case. Categories MLLM HUMAN(BEST) HUMAN(WORST) General-Purpose Proprietary MLLMs: 78.10% 48.40% 46.10% 45.78% 36.14% GEMINI-2.5-PRO 93.02% 67.02% 46.81% 22.89% 49.50% O3 86.03% 70.74% 48.23% 35.54% 44.06% GPT-4O 90.16% 63.83% 50.35% 36.75% 46.04% GPT-5 73.65% 46.28% 30.50% 52.41% 39.60% GROK-4 CLAUDE-3-7-SONNET-THINKING 74.60% 58.51% 35.46% 27.11% 35.64% General-Purpose Open-Source MLLMs: QWEN-2.5-VL-72B QWEN-2.5-VL-32B QWEN-2.5-VL-7B QWEN-2.5-VL-3B GLM-4.5V-108B GEMMA-3-27B INTERNVL-3.5-38B INTERNVL-3.5-30B-A3B INTERNVL-3.5-14B INTERNVL-3.5-1B INTERNVL-3-78B INTERNVL-3-38B OVIS-2.5-9B OVIS-2-34B DEEPSEEK-VL2-27B DEEPSEEK-VL2-SMALL-16B DEEPSEEK-VL2-TINY-3B KIMI-VL-30B-A3B-THINKING Embodied-Specified MLLMs: ROBOBRAIN-2-32B ROBOBRAIN-2-7B ROBOBRAIN-2-3B EMBODIED-R1-3B MAGMA-8B MLLM backbones in VLAs: PALIGEMMA-3B PHI-3-VISION-4B QWEN-2-VL-2B PRISMATIC-7B 75.56% 55.85% 35.46% 31.93% 27.23% 67.62% 43.09% 30.5% 22.29% 19.31% 71.43% 51.6% 20.57% 21.08% 12.87% 36.51% 10.64% 6.38% 13.86% 9.9% 90.48% 65.43% 36.88% 16.27% 35.15% 68.57% 57.45% 31.91% 19.88% 27.72% 70.79% 50.53% 29.08% 18.67% 27.72% 66.03% 37.77% 20.57% 15.06% 20.79% 66.03% 40.43% 21.99% 21.08% 21.29% 38.73% 19.68% 4.26% 3.61% 8.42% 79.05% 53.72% 39.01% 21.08% 27.23% 77.78% 44.68% 31.91% 16.87% 27.72% 80.63% 55.85% 42.55% 17.47% 21.29% 83.17% 45.21% 35.46% 15.66% 24.75% 71.75% 39.89% 19.86% 6.63% 17.33% 64.44% 28.19% 10.64% 10.24% 9.9% 7.62% 2.13% 2.84% 4.22% 1.98% 79.05% 45.21% 31.21% 18.67% 13.37% 44.71% 46.84% 49.51% 37.06% 36.46% 40.81% 38.24% 34.68% 40.71% 15.88% 16.2% 18.48% 56.47% 47.85% 55.14% 42.94% 41.52% 45.26% 37.65% 39.75% 44.07% 31.18% 33.67% 37.06% 31.76% 35.44% 38.83% 20.0% 16.2% 18.58% 42.94% 45.32% 48.91% 41.18% 39.24% 45.26% 44.12% 41.27% 48.52% 40.0% 41.27% 46.74% 35.88% 30.38% 37.06% 25.88% 25.06% 30.43% 2.94% 2.78% 4.25% 40.59% 38.99% 43.08% 75.87% 49.47% 19.86% 6.63% 19.31% 66.03% 44.68% 13.48% 10.84% 11.88% 46.35% 18.62% 3.55% 11.45% 6.93% 38.41% 6.38% 4.96% 4.22% 6.93% 3.01% 20.3% 46.35% 29.26% 0% 7.94% 10.11% 0% 1.49% 33.97% 12.77% 4.26% 3.01% 9.41% 19.37% 1.6% 0.71% 7.83% 2.97% 6.98% 4.26% 1.42% *56.02% 0.99% 55.47% 49.39% 49.39% 21.46% 59.92% 48.99% 49.8% 43.32% 44.94% 18.22% 56.28% 53.04% 56.28% 52.23% 42.91% 37.65% 5.26% 48.58% 39.41% 32.66% 40.51% 34.71% 29.87% 34.88% 18.24% 16.71% 21.64% 11.76% 11.39% 15.91% 25.88% 23.29% 24.41% 51.5% 42.0% 44.0% 21.5% 62.5% 50.0% 51.0% 41.0% 44.0% 22.5% 52.0% 51.0% 57.0% 56.5% 44.0% 36.0% 7.0% 46.5% 4.12% 4.05% 4.64% 11.18% 13.42% 15.91% 4.12% 10.13% 8.3% 8.24% 14.43% 12.55% 47.37% 41.7% 28.74% 20.24% 29.55% 48.5% 36.5% 25.5% 23.0% 19.0% 4.86% 19.43% 9.31% 13.77% 6.0% 20.5% 7.0% 11.0% 0% Chain-of-Thought (Wei et al., 2022), explicitly asking the models to reason before providing their final answer. The only exception was for models that feature native, built-in thinking mode, in which case we allowed them to utilize their default inference process without modification. We also recruited 5 human participants as testers to serve as reference. 4.2 OVERALL RESULTS As shown in Tab. 1, MLLMs generally underperform, with most scoring below 60%a result far inferior to human performance, which consistently achieves at least 87.85% overall accuracy. This indicates that contemporary MLLMs have superficial understanding of tool usage. Among the models evaluated, proprietary general-purpose MLLMs performed best. The OpenAI series (o3, gpt-4o, and gpt-5) all exceeded the 60% threshold, with gpt-5 leading the group. Open-source general-purpose MLLMs followed, typically scoring above 40%. GLM-4.5V was notable exception, achieving 55.14% and outperforming not only its open-source peers but also some proprietary models, highlighting its significant potential. Embodied-specific MLLMs demonstrated some capability but lagged behind the general-purpose open-source models. Lastly, MLLM backbones within VLA frameworks exhibited the weakest performance, likely due to their limited number of parameters. An overall leaderboard of MLLMs is shown in Fig. 3. We provide set of complete VQA results in Appendix. B. 6 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 4: Overall performance v.s. model size for open-source MLLMs. significant correlation is observed between performance and model size. Figure 5: Performance comparison between the embodied models and their base model. 4.3 FINDINGS ON PHYSTOOLBENCH F.1. foundational ability to understand tools emerges in large models with sufficient scale. As shown in Fig. 4, our evaluation of numerous open-source models reveals that theres significant correlation between the understanding of physical tool and the size of the model. Furthermore, for the easy difficulty setting in Tab. 1, we also observe that foundational understanding of tool usage emerges once model reaches certain scale, which we preliminarily identify as approximately 10 billion parameters. Most models exceeding this 10B threshold achieve an accuracy of 60-70% on easy-level tasks. In contrast, performance drops significantly for smaller models; those with fewer than 5B parameters generally score below 50% on easy tasks and have an overall accuracy below 25%. Consequently, we recommend selecting MLLMs with more than 10 billion parameters for applications in embodied intelligence. F.2. long-tail problem persists in tool recognition and understanding, even for the most advanced MLLMs. Although top-tier MLLMs are proficient at identifying common objects, their performance diminishes for less common items, creating long-tail effect. notable finding is the models pronounced weakness in the subcategory of digital products. They frequently fail to distinguish between visually similar items, such as HDMI versus DP cables and Type-C versus Lightning charging ports. This deficiency is widespread in open-source models, where even the highly capable GLM-4V shows errors in basic recognition, as in Fig. 6. (a). Closed-source models offer marginal improvement but still demonstrate only shallow comprehension. As an example in Fig. 6. (c), most top-tier proprietary models do not grasp the functional requirement that monitor must be connected to laptop using an HDMI cable and an adapter if the laptop only has Type-C port. F.3. Embodied-specific MLLMs show no significant advantage on PhysToolBench. Models specifically fine-tuned for embodied tasks, such as RoboBrain2 and Embodied-R1, do not exhibit notable performance improvement on our benchmark. RoboBrain2s parameters were initialized from Qwen2.5VL and subsequently fine-tuned on combination of general vision and robotic datasets. Nevertheless, as shown in Fig. 5, its 32B, 7B variants all performed slightly below their Qwen2.5VL backbone of equivalent scale. similar trend was observed with Embodied-R1-3B, which, despite being fine-tuned from Qwen-2.5-VL-3B, also achieved marginally lower score than the original model. These findings indicate that the fine-tuning process did not confer an enhanced understanding of tools. We hypothesize that current robotic datasets may require more high-quality data centered on tool comprehension to advance these models physical tool understanding. F.4. MLLMs exhibit critical deficiency in comprehending tool availability, failing to grasp the fundamental principles of their utility. The M3 difficulty tier of our benchmark was specifically designed to probe this issue by incorporating simple traps: presenting the correct tool for task but in damaged or non-functional state. Counter-intuitively, as shown in Tab. 1, models found this task more difficult than the Hard tier, which requires complex reasoning for tool creation. For instance, in the selected four cases in Fig. 6. (d), none of the MLLMs could identify when the tools are unavailable. This outcome strongly suggests that the models comprehension of tools is shallow and relies on surface-level common sense associations rather than robust understanding of their core functionality, leading them to hallucinate the tools usability. 7 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 6: Some results of PhysToolBench. We showcase illustrative examples for each difficulty level, along with the answers from several top-tier models and human participants. Note that the markers are intentionally enlarged for visualization purposes. The implications of this hallucination for embodied agents are severe. An agent that cannot recognize tool as non-functional may attempt to use it, resulting in mission failure and significant safety hazardsfor instance, fueling tractor with gasoline, drawing blood sample with damaged syringe. We contend that addressing this issue is critical for advancing embodied AI. F.5. The MLLM backbones in current VLAs are extremely weak. Our evaluation revealed that the MLLM backbones of the contemporary VLA models exhibit exceptionally poor performance on PhysToolBench, with overall scores universally below 15%. This result calls into question the prevailing assumption that VLAs can effectively inherit common sense from their base MLLM and then achieve generalization through fine-tuning on robotic action datasets. Our findings suggest that the foundational common sense of these MLLMs is profoundly insufficient for general-purpose intelligence. We posit that this fundamental limitation cannot be rectified through fine-tuning on robotic datasets of modest scale. Consequently, we conclude that advancing the VLA paradigm will require two-pronged approach: first, leveraging significantly larger and more capable MLLMs as backbones; and second, substantial expansion in the size and diversity of robotic action datasets. F.6. Reasoning ability is important and useful, but still insufficient. The capability for reasoning is crucial. In our experiments, we evaluated subset of models under two conditions: one with Chainof-Thought (CoT) prompting and one without. As shown in Tab. 2, the models prompted with CoT demonstrated significantly higher accuracy. Furthermore, models that are natively optimized for reasoning exhibit superior performance. For instance, GLM-4.5V, the top-performing opensource model, was trained with strong emphasis on reasoning. Its training regimen included not only Supervised Fine-Tuning (SFT) on high-quality CoT datasets but also reinforcement learning to bolster its reasoning skills further. When utilizing its built-in thinking mode, GLM-4.5Vs overall score was markedly higher than other open-source models and even surpassed some proprietary ones. Similarly, Ovis-2.5-9B, through specialized reasoning op timizations, achieved total score of 48.52% with just 9B parametersa performance comparable to that of 72B model (49.51%). These results underscore the significant impact of reasoning. 8 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 7: Comparison Between (a) Text-Level Reasoning and (b) Our proposed Vision-Centric Reasoning. Table 2: Influence of reasoning MLLM QWEN-2.5-VL-72B + CoT QWEN-2.5-VL-32B + CoT QWEN-2.5-VL-7B + CoT GPT-4O + CoT + VCR GPT-5 (W/ THINKING) + VCR Easy 79.68% 75.56% 71.75% 67.62% 75.24% 71.43% 88.25% 86.03% 90.16% Difficulty Level Medium-M3 25.30% 31.93% 11.45% 22.29% 11.45% 21.08% 35.54% 35.54% 45.78% 36.75% 54.81% Hard 24.26% 27.23% 10.89% 19.31% 13.37% 12.87% 42.57% 44.06% 46.04% Overall Total 46.64% 49.51% 34.88% 40.81% 37.94% 40.71% 60.77% 61.26% 62.15% However, current reasoning abilities remain inadequate. Models are prone to generating hallucinations in certain tasks. Moreover, their spatial reasoning is deficient; for instance, as depicted in Fig. 6. (b), none of the models realized that flathead screwdriver of the right size could also unscrew this Phillips screw. We contend that greater focus on visual-centric reasoning is essential for models to effectively undertake high-level planning tasks. 4.4 PRELIMINARY SOLUTION We here further introduce preliminary method aiming at improving the reasoning process. Current MLLMs often exhibit modality bias, where reasoning occurs predominantly at the text level while frequently overlooking crucial visual information, as shown in Fig. 7. (a). To mitigate this, we propose an approach that emphasizes vision-centric reasoning. As shown in Fig. 7. (b), we developed Vision-Centric Reasoning Agent with an MLLM as its backbone and decomposed the answering process into three distinct steps. First, in the Global Analysis stage, the agent forms holistic understanding of the users query in the context of the image. Second, it invokes an object detection tool (DINOX (Ren et al., 2024), formatted as an MCP tool for agent use) to identify and crop objects based on their bounding boxes. These crops then undergo secondary, more In-depth Analysis. Finally, the agent performs Multi-level Evidence Integration and Reasoning, synthesizing the initial global understanding with the detailed analysis of the cropped objects to formulate the final answer. We evaluated our approach on the M3 difficulty level, where existing models perform the worst. As shown in Tab. 7, our method leads to substantial performance gains when using the same backbone MLLM. Specifically, GPT-4o and GPT-5 achieved performance boosts of 10.24% and 18.06%, respectively, highlighting the critical importance of vision-centric reasoning. Although this approach is relatively straightforward and shares conceptual similarities with some concurrent work (Man et al., 2025), we aim to demonstrate the significance of vision-centric reasoning in the context of embodied intelligence. We hope our findings will inspire further research in Embodied Agents."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We present PhysToolBench, novel benchmark for evaluating the understanding of physical tools in MLLMs. This VQA benchmark comprises 1,000 image-text pairs, spanning broad spectrum of scenarios and features three fine-grained difficulty tiers to probe the depth of model comprehension. We evaluated 32 MLLMs, including closed-source, open-source, embodied-specific models, and MLLM backbones used in VLA models. Our findings reveal that all tested models fall significantly short of human performance, highlighting critical gap in their ability to reason about physical tools. Through an extensive analysis, we identify the key weaknesses of current MLLMs and outline promising directions for future research. We propose PhysToolBenchas tiered evaluation standard to systematically measure the capability frontiers of embodied agents and road map for more general intelligence. 9 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs SUPPLEMENTARY MATERIALS OF PHYSTOOLBENCH: BENCHMARKING PHYSICAL TOOL UNDERSTANDING FOR MLLMS"
        },
        {
            "title": "A MORE DETAILS ABOUT BENCHMARK CONSTRUCTION",
            "content": "A.1 DATASET CONSTUCTION Here, we provide more detailed introduction to the construction details of our benchmark. The entire benchmark and evaluation code will be open-sourced. Phase 1: Conceptualization. In this phase, we invited 5 experts (all are co-authors) to conceptualize task-scene pairs through manual brainstorming to obtain high-quality data. Continuous discussions were conducted throughout this process, which lasted three weeks and resulted in an initial collection of 1,500 cases. The intermediate results of this phase are presented in CSV files, as shown in Fig. 8. Figure 8: Task-Scene Pair Brainstorming Phase 2: Image Generation. We took the scene descriptions brainstormed in the previous step and fed them into GPT-4o for image generation. To better approximate real-world use cases, we added an additional prompt to most cases: photo taken with smartphone, slightly cluttered arrangement. This process was closely supervised by human experts who vetted the generated images for quality, realism, and accuracy. While significant number of initial generations contained inaccuracies, most images met our stringent criteria after 1 to 3 iterations of refinement through regeneration or prompting to modify the inaccurate parts, achieving level of realism nearly indistinguishable from actual photographs. For the small subset of cases where the generative model consistently failedparticularly with complex objects such as digital products, which GPT-4o struggled to render correctlywe resorted to physical staging and photography based on the original scene descriptions. We present here some examples of generation failure cases alongside the final corrected images in Fig. 9. Phase 3: Human-in-the-Loop Annotation and Quality Review. During the annotation process, we developed annotation software that retained an Abnormal Annotation function, enabling annotators to flag cases with problematic images or tasks while conducting annotations. Subsequently, after completing batch of data annotation, we assigned another group of reviewers to re-examine 10 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 9: Example failure cases and the final revised images Figure 10: UI demonstration of our annotation app the images and regenerate problematic images as needed. demonstration of the UI of the annotation app is provided in Fig. 10. Through these three rigorous rounds of benchmark construction with continuous review processes, we ultimately filtered out 1,000 high-quality cases. We also conducted simple analytical experiment to demonstrate the authenticity of the generated images in the next section. 11 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs A.2 REALISM EVALUATION OF GENERATED IMAGES To quantitatively assess the realism of the generated images in PhysToolBench, we first utilize GPT4o as an evaluator and also conduct user study. The prompt provided to GPT-4o is shown in Fig. 11. We randomly select 100 images from PhysToolBenchand ask GPT-4o to rate their realism on scale of 0 to 2, where 2 represents highly realistic, 1 denotes somewhat realistic, and 0 indicates unrealistic. The average score obtained from GPT-4o is 1.92, suggesting that most images in PhysToolBenchare realistic. Additionally, we perform user study with 10 participants, who also rate the same 100 images on the same scale. The average score from the user study is 1.78, which aligns closely with the GPT-4o evaluation. These findings indicate that the images in PhysToolBenchare generally realistic and appropriate for evaluating the physical tool comprehension of MLLMs. COMPLETE DEMONSTRATION OF IMAGEQUESTIONANSWER TRIPLETS Since the examples in Fig. 6 are different from the actual ones for illustrative purposes, we provide the full, verbatim materials, including the original input images and text prompts, the corresponding ground-truth answers, and GPT-4os outputs for each instance here, as in Figures 12 to 20. 12 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 11: We utilize GPT-4o to evaluate the realism of the final images. Above is the system prompt we provided to GPT-4o. 13 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 12: Examples of data in PhysToolBenchwith GPT-4o predictions. Figure 13: Examples of data in PhysToolBenchwith GPT-4o predictions. Figure 14: Examples of data in PhysToolBenchwith GPT-4o predictions. 14 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 15: Examples of data in PhysToolBenchwith GPT-4o predictions. Figure 16: Examples of data in PhysToolBenchwith GPT-4o predictions. Figure 17: Examples of data in PhysToolBenchwith GPT-4o predictions. 15 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Figure 18: Examples of data in PhysToolBenchwith GPT-4o predictions. Figure 19: Examples of data in PhysToolBenchwith GPT-4o predictions. Figure 20: Examples of data in PhysToolBenchwith GPT-4o predictions. 16 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs"
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin et al. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024. URL https://doi.org/10.48550/arXiv. 2404.14219. Anthropic. Claude 3.7 sonnet and claude code, Feb 2025. URL https://www.anthropic. com/news/claude-3-7-sonnet. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: Vision-Language-Action Model with Open-World Generalization. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. Pi0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023. George Jiayuan Gao, Tianyu Li, Junyao Shi, Yihan Li, Zizhe Zhang, Nadia Figueroa, and Dinesh Jayaraman. Vlmgineer: Vision language models as robotic toolsmiths. arXiv preprint arXiv:2507.12644, 2025. Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, et al. Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios. arXiv preprint arXiv:2401.17167, 2024. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv preprint arXiv:2310.03128, 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 17 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, et al. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682, 2024a. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024b. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, et al. Ovis2. 5 technical report. arXiv preprint arXiv:2508.11737, 2025. Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, YuXiong Wang, and Zhiding Yu. Argus: Vision-centric reasoning with grounded chain-of-thought. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1426814280, 2025. OpenAI. Addendum to gpt-4o system card: 4o image generation. Online supplement, March 2025a. Accessed September 2025. OpenAI. Gpt-5 system card, Aug 2025b. URL https://cdn.openai.com/ gpt-5-system-card.pdf. OpenAI. Openai o3 and o4-mini system card, Apr 2025c. URL https: //cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, and Hong Zhang. Mimicfunc: Imitating tool manipulation from single human video via functional correspondence. arXiv preprint arXiv:2508.13534, 2025. BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025a. GLM-V Team. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. URL https://arxiv.org/abs/2507.01006. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025b. Noah Trupin, Zixing Wang, and Ahmed Qureshi. Dynamic robot tool use with vision language models. arXiv preprint arXiv:2505.01399, 2025. Junying Wang, Wenzhe Li, Yalun Wu, Yingji Liang, Yijin Guo, Chunyi Li, Haodong Duan, Zicheng Zhang, and Guangtao Zhai. Affordance benchmark for mllms. arXiv preprint arXiv:2506.00893, 2025a. 18 PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs Pei Wang, Yanan Wu, Zekun Wang, Jiaheng Liu, Xiaoshuai Song, Zhongyuan Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, et al. Mtu-bench: multi-granularity tool-use benchmark for large language models. arXiv preprint arXiv:2410.11710, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Internvl3. 5: Advancing open-source multimodal Linglin Jing, Shenglong Ye, Jie Shao, et al. models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. xAI. Grok 4 model card, Aug 2025. 2025-08-20-grok-4-model-card.pdf. URL https://data.x.ai/ Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1420314214, 2025. Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, et al. Toolhop: query-driven benchmark for evaluating large language models in multi-hop tool use. arXiv preprint arXiv:2501.02506, 2025. Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. arXiv preprint arXiv:2508.13998, 2025. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "Beihang University",
        "HKUST",
        "HKUST(GZ)",
        "Knowin"
    ]
}