{
    "paper_title": "Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization",
    "authors": [
        "Gabriel Loiseau",
        "Damien Sileo",
        "Damien Riquet",
        "Maxime Meyer",
        "Marc Tommasi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 3 4 7 0 2 . 2 0 6 2 : r Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization Gabriel Loiseau1,2 Damien Sileo2 Damien Riquet1 Maxime Meyer1 Marc Tommasi2 1Hornetsecurity, Hem, France 2Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France gabriel.loiseau@inria.fr"
        },
        {
            "title": "Abstract",
            "content": "Anonymizing textual documents is highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, new task formulation in which anonymization strategies are automatically adapted to specific privacyutility requirements. We propose framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves better privacyutility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacyutility trade-off frontier."
        },
        {
            "title": "Introduction",
            "content": "Text anonymization has emerged as fundamental technique for enabling the sharing and analysis of sensitive textual data while protecting individual privacy. Traditionally, privacy protection has been grounded in formal guarantees, most notably differential privacy (Dwork, 2006), which provides strong, distribution-independent assurances against information leakage. However, applying such guaranties to free-form text remains notoriously difficult: defining neighboring datasets, calibrating 1 noise, and preserving semantic utility are all unresolved challenges in practice (Mattern et al., 2022; Meisenbacher et al., 2025; Çano and Habernal, 2025). As result, text anonymization is often conducted under weaker but more operational notion of privacy, where an adversary with access to auxiliary background knowledge attempts to infer sensitive attributes from the released text. Within this adversarial framing, large language models (LLMs) play dual role. On the one hand, they constitute powerful and realistic attacker model: LLMs encode broad world knowledge, can exploit subtle contextual cues, and have been shown to strategically re-identify sensitive information even after sophisticated anonymization (Staab et al., 2024). On the other hand, the same capabilities make LLMs effective anonymizers. When properly instructed, LLMs can perform nuanced, context-aware obfuscation of sensitive attributes, often outperforming rule-based or static methods (Staab et al., 2025; Yang et al., 2025). Recent work has demonstrated that collaborative settings involving an attacker LLM and an anonymizer LLM can further strengthen privacy, with multihop refinement yielding substantial improvements in anonymization quality (Yang et al., 2025; Brahem et al., 2024). Nonetheless, successful anonymization is inherently context-dependent and must be evaluated across multiple dimensions. The effectiveness of anonymization strategies depends primarily on two key factors: the capabilities and resources of potential adversaries, and the specific notion of utility that must be preserved. These factors interact to create distinct anonymization requirements across different use cases. For instance, anonymizing medical report containing sensitive patient information demands fundamentally different approach than anonymizing casual online comments. Medical records may require protection against attackers capable of infering sensitive patient attributes, while preserving precise clinical terminology and diagnostic relationships essential for medical research. In contrast, online comments may face different threat models, such as inference attacks based on writing style or demographic cues. The diversity of these scenarios underscores that no single anonymization strategy can adequately address all privacy-utility requirements (Loiseau et al., 2025), necessitating adaptive approaches that can be tailored to specific adversarial assumptions and utility constraints (Pasch and Cha, 2025). In practice, selecting particular privacyutility trade-off ultimately reflects an explicit risk assessment, in which acceptable levels of disclosure risk are weighed against task-specific utility requirements. Despite these advances, modern LLM-based anonymization pipelines suffer from three key limitations that hinder their practical deployment. First, they adopt fixed trade-off paradigm, in which single anonymization strategy is manually designed for each specific scenario, preventing flexible adaptation to new privacy-utility requirements. Second, they rely on manual prompt engineering, requiring practitioners to iteratively design and refine instructions through trial and errora process that is subjective, labor-intensive, and often yields suboptimal results. Third, they typically depend on proprietary closed-source models to achieve effective anonymization, creating fundamental contradiction: processing sensitive data through external API providers inherently conflicts with privacy objectives. These limitations combine to produce anonymization systems that are brittle across domains, costly to adapt to new contexts, and unsuitable for real-world deployment where sensitive content must remain under local control. To address these challenges, we introduce adaptive text anonymization, framework that automatically learns domainand task-specific anonymization strategies through prompt optimization. We formulate anonymization as multi-objective optimization problem in which privacy and utility requirements are explicitly specified, targeting automatic adaptation. Our framework employs evolutionary prompt optimization and can operate on medium-sized language models, achieving performance comparable to proprietary API-based solutions while preserving full data privacy. Our contributions are as follows: We propose an adaptive anonymization framework that automatically discovers effective anonymization instructions through agentic collaboration and prompt optimization, enabling open-source models to navigate diverse privacyutility trade-offs without manual prompt design. We introduce unified benchmark spanning five anonymization tasks across distinct domains, privacy objectives, and utility constraints, providing comprehensive evaluation suite for adaptive text anonymization. We present extensive empirical results demonstrating that our approach achieves state-ofthe-art performance with open-source models and outperforms traditional anonymization methods both on their target task and on new domains. We show that our method enables explicit exploration of the privacyutility trade-off space, discovering multiple anonymization strategies that favor different operating points and allowing practitioners to select models aligned with their deployment requirements."
        },
        {
            "title": "2 Related Work",
            "content": "Text Anonymization. Text anonymization seeks to enable the sharing of sensitive textual data by removing or obfuscating personal identifiers while preserving document utility. Traditional approaches predominantly rely on sequence labeling models trained on manually annotated datasets to detect and mask predefined categories of sensitive entities (Deußer et al., 2025; Hathurusinghe et al., 2021; Francopoulo and Schaub, 2020; Lison et al., 2021). While effective in structured privacy settings, these methods often overlook the impact of anonymization on downstream utility or restrict evaluation to surface-level text quality metrics (Yermilov et al., 2023; Staab et al., 2025). This limitation is further compounded by the lack of comprehensive benchmarks that systematically evaluate privacyutility trade-offs across diverse domains, threat models, and task requirements (Pasch and Cha, 2025; Loiseau et al., 2025). Anonymization via LLM Pipelines. The advent of large language models has fundamentally reshaped text anonymization, establishing LLMs as both powerful privacy adversaries and potential defenders. Recent studies show that LLMs can successfully re-identify sensitive attributes in anonymized text (Staab et al., 2024; Patsakis and 2 Figure 1: Overview of our approach. We perform reflective prompt optimization using the GEPA algorithm (Agrawal et al., 2025). Our method adapts base seed prompt into an optimized prompt defining the privacy and utility task requirements. The optimization operates in strict fixed budget environment while learning sufficiently strong patterns to adapt to the anonymization objective. Lykousas, 2023), exposing the limitations of traditional anonymization pipelines. In response, several works have explored the use of LLMs themselves as anonymization agents. Staab et al. (2025) introduced adversarial LLM collaboration, using simulated attacker to guide anonymization decisions. Subsequent work has extended this paradigm by injecting synthetic information to mislead attackers while preserving utility (Frikha et al., 2024), or by incorporating multi-hop refinement among attacker, utility evaluator, and anonymizer LLMs to improve robustness (Yang et al., 2025). Despite their effectiveness, these approaches rely heavily on manually designed instructions and domainspecific prompt engineering (Dou et al., 2024; Brahem et al., 2024), limiting their scalability and adaptability. Moreover, most methods depend on closed, API-based models (e.g., GPT-4/5 (Deußer et al., 2025)), raising practical and ethical concerns when processing sensitive data in deployment settings that require local control. Prompt Optimization. Automated prompt optimization has emerged as promising alternative to manual prompt engineering, which is often inefficient, subjective, and difficult to generalize across tasks (Yang et al., 2024; Wei et al., 2022). Recent work has explored search-based techniques (Zhou et al., 2023; Opsahl-Ong et al., 2024) and evolutionary algorithms that iteratively refine prompts through mutation and selection (Agrawal et al., 2025; Chen et al., 2024). These methods have demonstrated strong performance across variety of NLP tasks, but, to the best of our knowledge, have not yet been applied to privacy-sensitive settings such as text anonymization. In particular, the potential of prompt optimization to automatically generate task-specific anonymization instructions aligned with explicit privacy objectives and utility constraints remains largely unexplored. Our work bridges this gap by leveraging evolutionary prompt optimization to enable adaptive, context-aware text anonymization without manual prompt design."
        },
        {
            "title": "3 Our Approach",
            "content": "We introduce an agentic optimization framework for LLM-based text anonymization tasks. Our approach conceptualizes anonymization as text generation task that can be learned and refined through natural language instruction and self-reflection. By framing the problem as prompt optimization, our framework enables local anonymization agent to adapt its behavior to specific privacy objectives, data domains and downstream utility requirements without requiring manual prompt engineering or model retraining. This section formalizes the text anonymization problem, presents our two-stage optimization pipeline, and details the mechanisms enabling automatic adaptation across diverse privacyutility trade-offs."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We view text anonymization as an adaptive, taskconditioned rewriting problem rather than fixed set of redaction rules. In many practical deployments, the anonymizer must be configured before processing data: (i) privacy specification describes what must be protected (e.g., PII, quasiidentifiers, writing style, organization-specific se3 Algorithm 1 Two-Phase GEPA for Text Anonymization Require: Dataset D, base feedback metric µ, budget B, local model m, feedback creation model 0 } Πm {Πm Require: Base prompt Π0, early-stop patience n, sampling ratio α 1: Stage 1: Initialization 2: Split into Dtrain and Dvalid 3: Initialize prompt pool {Πm 4: Stage 2: Warm-Start with Basic Feedback 5: while > 0 and no improvement for < iterations do 6: new GEPA(P, Dtrain, Dvalid, µ, m) 7: new}; update 8: end while 9: Stage 3: Refinement with Rich Feedback 10: if > 0 then 11: 12: 13: 14: 15: 16: 17: end if 18: return arg maxΠmP Score(Πm, Dvalid) valid Sample(Dvalid, α) new GEPA(P, Dtrain, new}; update Πm {Πm µrich M(µ) while > 0 do valid, µrich, m) end while crets), and (ii) utility specification describes what must be preserved to remain compatible with downstream use (e.g., clinical meaning, intent labels, formatting). This formulation aligns with regulations such as GDPRs emphasis on specified purposes for data processing (GDPR, 2016). We call this setting adaptive task anonymization. Formally, an anonymization task is defined by pair (p, u), where is the privacy objective and is the utility objective. Given an input text x, the goal is to produce an anonymized text that satisfies while preserving the aspects required by u. These objectives are provided to the model as inputs rather than as fixed criteria, the same raw text may admit multiple acceptable anonymizations, reflecting different trade-offs aligned with distinct downstream applications. Rather than retraining models for each domain, we represent the anonymizer as an LLM guided by natural-language instruction prompt Π. The key challenge is to obtain an instruction Π that is wellmatched to the task (p, u). Our contribution is to learn Π automatically from the task specification (i.e., its evaluation procedure) and empirical feedback. We start from single universal seed prompt Π0 shared across all tasks and evolve it into taskadapted prompt. This keeps the anonymization model unchanged and shifts adaptation to prompt optimization."
        },
        {
            "title": "3.2 Framework Overview",
            "content": "adaptive Algorithm 1 operationalizes task anonymization as prompt evolution and optimization. We explore anonymization instructions under fixed computational budget B, measured in LLM forward passes. The framework operates entirely 4 using locally deployable language model m, which serves two roles: (i) an anonymization agent that applies candidate instructions to produce anonymized text, and (ii) proposer agent that generates new instruction variants during optimization. Our approach builds upon GEPA (Agrawal et al., 2025), an evolutionary prompt optimization algorithm that treats prompts as individuals in population , evolved through selection, mutation, and evaluation. Candidate prompts are retained via Pareto-based selection over privacy and utility, encouraging diverse trade-off strategies. GEPA employs reflective mutation: the proposer analyzes execution traces and feedback to propose targeted modifications."
        },
        {
            "title": "3.3 Stage 1: Initialization",
            "content": "The optimization begins with partitioning the dataset into training and validation splits, denoted Dtrain and Dvalid. The training set is used to generate anonymized outputs during candidate evaluation, while the validation set defines the fitness landscape used to score candidate instructions. We initialize the prompt pool with single candidate Πm 0 , which pairs generic seed instruction Π0 (\"Given the field text, produce the field anonymized_text.\") with the local model m. This seed provides minimal anonymization behavior from which more specialized strategies can evolve. During early optimization, candidate prompts are evaluated using base feedback function µ, which serves as learnable proxy for the task specification (p, u) defined in Section 3.1. The function µ is typically defined as scalar aggregation of privacy and utility evaluation metrics (i.e., their average) without additional detail, which provides coarse signal to guide exploration."
        },
        {
            "title": "3.4 Stage 2: Warm-Start with Basic Feedback",
            "content": "Stage 2 applies standard GEPA to explore the instruction space using base feedback µ. Each iteration: (1) selects diverse, high-performing prompts from the pool using Pareto-based ranking over privacy and utility objectives; (2) The proposer agent analyzes execution traces and feedback associated with selected prompt on minibatch of texts in Dtrain and proposes targeted instruction modifications, yielding new candidate Πm new. And (3) The new prompt is applied to the same minibatch of Dtrain. If an improvement is observed on this minibatch, it gets evaluated on Dvalid using µ, and incorporated into , with dominated candidates removed via Pareto pruning. Stage 2 terminates when either the budget is exhausted or validation performance plateaus for consecutive iterations. This early-stopping criterion preserves budget for later refinement."
        },
        {
            "title": "3.5 Stage 3: Refinement with Rich Feedback",
            "content": "If computational budget remains, the algorithm transitions to refinement phase that aims to escape local optima and achieving finer-grained control over the privacyutility trade-off. This phase extends standard GEPA with two novel mechanisms. Rich Feedback Generation. rich feedback function µrich is derived from the task-specific metric definitions by decomposing the aggregate score µ. In addition to scalar values, µrich may include natural-language explanations or evaluator reasoning traces that provide interpretable, structured feedback to the proposer agent. This feedback function is generated once per task by separate LLM (referred to as the rich feedback agent in Figure 1), ensuring consistency and avoiding manual, subjective feedback design1. Adaptive Validation Sampling. To improve exploration under the remaining budget, we evaluate candidate prompts over sampled validation subsets valid Dvalid. Sampling follows round-robin strategy prioritizing under-evaluated examples, balancing computational efficiency with coverage diversity. This reduces the budget consumption during evaluation while mitigating overfitting. Final model selection is performed using the full validation set to ensure fair comparison. With these mechanisms, GEPA continues to evolve the prompt pool until the budget is exhausted. The richer feedback enables the proposer agent to perform larger, even more targeted behavioral updates using fewer evaluations. The framework ultimately returns the best-performing anonymization system Π identified during optimization, corresponding to the highest validation score under the specified privacyutility configuration. 1In practice, we assign the rich feedback agent codegeneration task using the implementation of µ in order to obtain the corresponding implementation of µrich. More information in Appendix D."
        },
        {
            "title": "4 Experiments",
            "content": "Benchmark Construction We evaluate our framework on five anonymization tasks spanning diverse domains, privacy threat models, and utility requirements. Each task poses distinct challenges in balancing privacy protection against information preservation, reflecting the context-dependent nature of real-world anonymization. The datasets are drawn from prior work (Yang et al., 2025; Xin et al., 2025; Li et al., 2025), and we retain their original evaluation protocols and implementations whenever available to ensure fair comparison. Additional dataset details are provided in Appendix H. DB-Bio (Yang et al., 2025). collection of DBpedia biographies of notable individuals. Privacy: binary re-identification metric in which an attacker LLM attempts to recover the individuals identity by proposing the top-3 candidate names from the anonymized biography. Utility: Occupation classification accuracy measured on anonymized text. SynthPAI (Yukhymenko et al., 2024). Synthetic Reddit-style posts encoding demographic attributes (e.g., age, gender, location) through writing style and contextual cues. Privacy: binary inference metric indicating whether an attacker LLM can correctly predict target demographic attributes from the anonymized text. Utility: ROUGE-1 F-measure assessing content preservation. TAB (Pilán et al., 2022). European court case documents containing personal information about involved parties and witnesses. Privacy: Recall of correctly masked sensitive spans relative to goldstandard annotations. Utility: Semantic similarity between original and anonymized documents. PUPA (Zhao et al., 2024). User prompts from ChatGPT interactions containing explicit personally identifiable information (PII), including names, addresses, phone numbers, and email addresses. Privacy: PII leakage rate, defined as the fraction of sensitive entities remaining in the sanitized prompts. Utility: An LLM-as-judge evaluation comparing response quality between original and anonymized prompts. MedQA (Jin et al., 2021). Clinical case descriptions drawn from USMLE-style medical examinations. Privacy: Stylometric distance between original and sanitized texts, computed using LUAR embeddings (Rivera-Soto et al., 2021), to reduce the risk of patient re-identification. Utility: Accuracy of selecting the correct medical diagnosis from multiple-choice options. 5 Comparison Methods. To assess the effectiveness of our framework, we compare against four baseline methods that span traditional entity-based anonymization, adversarial LLM pipelines, and manual prompt engineering. Together, these baselines cover the dominant paradigms in contemporary text anonymization. OpenPII: supervised PII detection model fine-tuned on the OpenPII dataset2, using ModernBERT (Warner et al., 2025), representing entity-centric anonymization via explicit span detection. Adversarial Feedback (AF): The method proposed by Staab et al. (2025), using adversarial collaboration between two LLM agents: an attacker that attempts re-identification and an anonymizer that iteratively refines its outputs based on attacker feedback. RUPTA: The approach of Yang et al. (2025), extending adversarial feedback with an additional utility judge. We evaluate RUPTA exclusively on DB-BIO, as it was designed for classification-oriented utility objectives. Task-Specific Manual Prompting: Expertcrafted prompts encoding privacy and utility requirements for each task, isolating the benefit of automatic prompt optimization. Implementation Details. All implementations and optimizations in this paper are carried out using the DSPy (Khattab et al., 2024) library3. We evaluate our framework using three opensource language models: Mistral-Small-3.224B4, Gemma-3-27B (Team et al., 2025), and Qwen3-30B-A3B (Team, 2025). Following GEPA experiment settings (Agrawal et al., 2025) and to simulate realistic deployment with limited access to labeled sensitive data, we restrict training to small, fixed set of 111 examples for training and 111 examples for validation per task (Dtrain = Dvalid = 111), reserving all remaining examples exclusively for testing. We configure the evolutionary optimization with maximum rollout budget of = 1500 LLM forward passes, an early-stop patience of = 5 iterations and set the 2https://hf.co/datasets/ai4privacy/ open-pii-masking-500k-ai4privacy 3Preliminary experiments confirm that the baseline methods are fully reproducible, showing no statistically significant differences between the original authors implementations and those using DSPy. 4https://hf.co/mistralai/Mistral-Small-3. 2-24B-Instructadaptive validation sampling ratio to α = 0.3. This configuration balances exploration of the instruction space with computational efficiency across tasks and models. For evaluation, we report each benchmarks primary native metric and adhere to the official evaluation implementations whenever available to ensure comparability with prior work. Privacy and utility metrics that require model-based evaluation are computed using Gemini-2.5-flash (Comanici et al., 2025) as strong and consistent evaluator backbone. As reference closed-source model for LLM-based comparison baselines, we use GPT-5-chat (OpenAI, 2025) as the underlying model to reflect state-of-the-art performance and to faithfully reproduce original method behavior. Additional implementation details, including prompts, hyperparameters, and computational costs, are provided in Appendix D. An ablation study analyzing the contribution of each component in our optimization approach is provided in Appendix A."
        },
        {
            "title": "5.1 Overall Performance",
            "content": "We evaluate along two axes: (i) prompt optimization impact on open-source models relative to anonymization baselines applied to the same model, and (ii) competitiveness against GPT5based methods. We report test Privacy and Utility scores throughout (higher percentage is better). Table 1 summarizes results across all tasks and open-source models. The Optimized Prompt consistently improves privacy over the Seed Prompt and is frequently the strongest or best-balanced open-source approach. These gains are typically achieved with minimal utility loss, indicating that the optimizer discovers task-appropriate trade-offs rather than optimizing single objective. Privacy gains are largest on structurally amenable tasks such as TAB and PUPA, where optimization substantially improves privacy while preserving semantics or response quality. On DB-BIO, optimized prompts maintain near-perfect utility while improving privacy, reflecting the relatively permissive trade-offs of classification-based metrics. Although SYNTHPAI and MEDQA present stronger privacyutility tension, optimization still yields consistent improvements over seed prompts across models. Models exhibit consistent, distinct behaviors. 6 Benchmark Model AF (Staab et al., 2025) Task-Specific Prompt Seed Prompt Optimized Prompt Privacy Score / Utility Score DB-Bio SynthPAI TAB PUPA MedQA Mistral-Small-3.2-24B Gemma-3-27b-it Qwen3-30B-A3B Mistral-Small-3.2-24B Gemma-3-27b-it Qwen3-30B-A3B Mistral-Small-3.2-24B Gemma-3-27b-it Qwen3-30B-A3B Mistral-Small-3.2-24B Gemma-3-27b-it Qwen3-30B-A3B Mistral-Small-3.2-24B Gemma-3-27b-it Qwen3-30B-A3B 59.7 / 98.1 59.1 / 94.2 58.8 / 97.2 35.3 / 67.6 40.7 / 60.2 31.3 / 56.0 45.1 / 53.5 62.1 / 53.7 26.1 / 54.4 77.1 / 76.5 93.9 / 64.7 86.3 / 70. 9.39 / 41.2 11.7 / 29.4 6.53 / 29.4 58.8 / 100 47.1 / 100 53.2 / 100 5.90 / 97.1 23.5 / 70.4 7.32 / 93.7 39.2 / 55.5 68.3 / 51.3 36.4 / 52.4 82.7 / 76.2 80.7 / 76.5 94.1 / 78.2 7.33 / 35.3 10.7 / 35.3 8.25 / 41. 54.5 / 98.9 67.6 / 99.1 64.0 / 100 6.30 / 99.2 36.0 / 62.9 12.6 / 92.4 20.8 / 54.3 61.8 / 55.9 36.2 / 54.0 75.0 / 77.5 88.1 / 77.1 82.3 / 72.1 2.91 / 54.9 12.0 / 46.8 3.52 / 58.6 61.2 / 100 77.6 / 100 65.5 / 38.7 / 77.4 36.0 / 77.1 22.5 / 94.4 83.6 / 53.7 81.9 / 54.1 92.3 / 56.2 85.3 / 82.9 94.3 / 79.1 98.0 / 79.3 8.06 / 53.2 14.3 / 56.8 24.6 / 45.9 Table 1: Results on open source models across all benchmark tasks. The best overall privacyutility trade-off is shown in bold. Otherwise, the best score for each individual component is underlined. Prompt optimization consistently yields stronger privacy-utility trade-offs than static anonymization baselines and task-specific prompts, often improving privacy substantially while preserving or even improving utility across diverse tasks and model families. Mistral exhibits steep privacy gains, sometimes at the cost of utility; Gemma favors more conservative improvements building on the privacy-focused behavior established by the seed prompt; and Qwen is the most robust, frequently achieving high privacy and utility simultaneously. These patterns suggest that the optimizer adapts to model-specific inductive biases rather than converging to single anonymization strategy. Table 2 compares one of our open-source configuration with GPT-5based AF, task-specific prompting, RUPTA, and OpenPII. Despite using smaller model, optimized Qwen is competitive across all tasks: it matches GPT-5 on MEDQA, achieves higher utility at comparable privacy on PUPA, and attains the strongest overall utility scores. This demonstrates that adaptive prompt optimization substantially narrows the gap between openand closed-source anonymization pipelines. The benchmarks expose distinct privacyutility landscapes. DB-BIO admits nearoptimal solutions on both axes, while SYNTHPAI and MEDQA enforce stricter trade-offs. Across these regimes, our framework consistently identifies effective strategies given the intrinsic constraints of each task."
        },
        {
            "title": "5.2 Trade-off Discovery",
            "content": "A key advantage of our framework is its ability to discover multiple effective anonymization strategies within single optimization run per model, rather than converging to single fixed privacyutility compromise. This property directly addresses core limitation of prior anonymization systems, which typically expose only one operating 7 Methods OpenPII DB-Bio SynthPAI TAB PUPA MedQA 57.6/98.1 9.02/97.3 87.1/32.2 75.4/70.3 3.80/59.5 - / - RUPTA ( GPT-5) 74.0/98.3 AF ( GPT-5) 78.0/92.1 64.0/57.6 59.9/42.5 94.2/46.0 24.4/45.8 Prompt ( GPT-5) 63.6/100 18.3/88.1 99.3/48.6 99.1/72.7 10.7/45.5 - / - - / - - / - (Optimized) 65.5/100 22.5/94.4 92.3/56.2 98.0/79.3 24.6/45.9 Table 2: Performance of optimized Qwen3-30B-A3B compared to GPT-5 based methods and OpenPII. Models not adaptable on other tasks are marked with -. point and require costly re-optimization to target alternative privacy budgets. Unlike fine-tuningbased approaches, where each trade-off point must be stored as separate model checkpoint, our framework reduces anonymization strategy discovery to string-level search. Each solution along the trade-off frontier is represented as natural language instruction, making the full candidate set inexpensive to store, inspect, and deploy. Figure 2 visualizes the discovered trade-off frontiers on TAB, SYNTHPAI, and MEDQA. Each point corresponds to distinct anonymization prompt, and the dashed line denotes the global frontier across all models. Across datasets, our framework consistently uncovers spectrum of solutions spanning privacy-focused configurations (high privacy, reduced utility) and utility-preserving configurations (high utility, moderate privacy), all obtained within single optimization run. The discovered frontiers further reveal modelspecific trade-off characteristics. On TAB, Qwen330B dominates the high-utility region of the frontier, while Mistral-small and Gemma-3-27B ocMistral-Small-3.2-24B Gemma-3-27b-it Qwen3-30B-A3B AF (GPT-5) Prompt (GPT-5) ) % ( c i U 56 52 50 48 46 44 90 80 70 60 50 65 62.5 60 57.5 55 52. 50 47.5 40 50 60 70 80 90 100 Privacy Score (%) (a) TAB 10 20 30 40 50 60 70 5 Privacy Score (%) (b) SynthPAI 10 20 15 Privacy Score (%) (c) MedQA 25 Figure 2: Trade-off frontier visualization on three datasets across optimized models. Each point represents distinct anonymization prompt. The dashed line connects the overall Pareto-optimal solutions across all models, demonstrating the frameworks ability to discover diverse privacy-utility trade-offs in single optimization run. Generate new anonymized version that achieves maximum privacy through comprehensive de-identification and stylistic obfuscation, while preserving full diagnostic and clinical reasoning fidelity. [...] Preserve Diagnostic Fidelity: Do not remove, generalize, or obscure any clinically diagnostic fact essential to arriving at the correct answer. Retain key diagnostic landmarks exactly as they inform the diagnosis, including pathognomonic findings, critical lab relationships, and temporal or anatomical patterns. [...] Maximize Anonymization Through De-identification: Replace all personally identifiable information and quasi-identifiers (age, gender, ethnicity, location) with generalized, non-specific alternatives, avoiding high-risk combinations of descriptors. [...] Disrupt Writing Style Aggressively to Maximize Privacy: The anonymized text must be unrecognizable in style from the original. Apply three or more stylistic transformations, such as sentence restructuring, passive voice, embedded clauses, and lexical substitution, while never altering diagnostic anchors. [...] Final validation: Would trained physician still be able to identify the correct diagnosis? Would writing-style analyzer fail to detect this as near-clone? If either answer is no, the anonymization has failed. Figure 3: Optimized anonymization prompt for Qwen3-30B-A3B on the MedQA task. cupy competitive regions at lower utility levels. On SYNTHPAI and MEDQA, where privacy and utility are more tightly coupled, the frontier is steeper, yet the optimizer still identifies distinct operating points that would be inaccessible under singleprompt paradigm. From deployment perspective, practitioners are no longer constrained to choose one-size-fits-all strategy; instead, they can select from menu of validated anonymization prompts depending on regulatory requirements, risk tolerance, or downstream task priorities."
        },
        {
            "title": "5.3 Qualitative Analysis",
            "content": "Figure 3 presents representative optimized anonymization prompt discovered on MedQA usQwen3-30B-A3B. The prompt explicitly ing separates diagnostic invariance from stylistic obfuscation, enforcing strong privacy guarantees while preserving clinical reasoning. Diagnostic fidelity 8 is maintained by requiring the retention of pathognomonic findings, clinically meaningful relationships, and temporal or anatomical patterns that are essential for arriving at the correct diagnosis. From privacy perspective, the prompt goes beyond standard de-identification by aggressively targeting stylometric leakage. It combines removal of explicit identifiers with structural rewrites, syntactic variation, and lexical substitution of nonessential content, yielding anonymized outputs that are difficult to link to the original text through similarity or authorship analysis. Overall, this example illustrates how explicitly encoding both privacy and utility failure modes in the optimization objective enables LLMs to discover anonymization strategies that effectively balance anonymity and clinical fidelity."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce adaptive text anonymization as paradigm that leverages large language models to learn task-specific privacy-utility trade-offs. We propose new framework that formulates anonymization as string discovery problem, extending GEPA with three-stage optimization pipeline enabling better exploration of trade-offs and the discovery of multiple Pareto-optimal solutions within single optimization run. Experiments across five benchmarks show that our approach consistently learns effective trade-offs and compares favorably to existing LLM-based anonymization methods. Together, these results highlight the potential of adaptive, model-driven anonymization for robust and flexible privacy preservation across domains."
        },
        {
            "title": "Limitations",
            "content": "While our framework demonstrates strong performance across diverse anonymization tasks, several limitations remain and highlight directions for future work. First, privacy and utility are primarily combined using an unweighted metric aggregation. Although this formulation is sufficient to expose broad range of trade-offs, it does not explicitly model alternative decision rules such as lexicographic ordering, weighted objectives, or hard privacy constraints. Exploring these formulations could further align the optimization process with domainor regulation-specific requirements. Second, our approach requires small annotated training and validation set for each task to guide optimization. This introduces additional computational overhead compared to fully zeroshot anonymization pipelines. We view this requirement as reasonable trade-off: even limited supervision enables the discovery of substantially stronger privacyutility operating points that allow smaller, local models to compete with larger closed-source systems. Nonetheless, reducing this supervision requirement remains an important direction for future work. Third, although key contribution of our framework is enabling anonymization with locally deployable open-source models, the evaluation pipeline still relies on closed-source LLMs for certain privacy and utility metrics. This reliance is partially mitigated by the small number of annotated examples required during optimization, which limits the amount of sensitive data that must leave the local boundary. However, determining which anonymized text is itself safe to share with external evaluators remains and is not fully resolved in this work. Fourth, we exclude reasoning-oriented models (e.g., relying on extended chain-of-thought inference at test time) from our study. Although such models could, in principle, derive effective anonymization strategies via in-context reasoning, they would need to re-derive these strategies independently for each input, resulting in substantially higher per-instance computational costs. In addition, current reasoning models typically require very large parameter scales to robustly address complex heterogeneous domains. Exploring whether smaller reasoning models, or hybrid approaches that combine prompt optimization with limited testtime reasoning, can provide complementary advantages is an intriguing direction for future work. Finally, the inherent non-determinism of LLM generation can introduce instability during optimization, occasionally leading to variance in training trajectories or convergence behavior. While evolutionary optimization is relatively robust to such noise when the rollout budget is large enough, improving stability and reproducibility remains an open challenge."
        },
        {
            "title": "Ethical Considerations",
            "content": "This work addresses socially important problem: enabling the safe sharing and analysis of sensitive textual data while preserving individual privacy. At the same time, our approach brings forward ethical considerations that should be considered in broader or higher-stakes deployments. First, reliance on automated LLM-based evaluators may obscure certain failure modes or systematically underweight rare but consequential privacy leaks. Second, optimized anonymization prompts could be misused to intentionally obfuscate accountability, attribution, or provenance in contexts where transparency is required. Third, stylometric and contextual obfuscation, while effective for privacy protection, may also remove signals that are valuable for forensic analysis, content moderation, or safety monitoring. Finally, heavy reliance on LLM-based evaluation introduces the risk of encoding fairness evaluator biases and instability. These risks highlight the importance of treating anonymization as risk-reduction mechanism rather than guaranty of privacy. Even after anonymization, residual risks may remain, particularly in domains involving sensitive health, legal, or personal data. Deployment of our framework should therefore be accompanied by explicit consent considerations, clear articulation of intended data use, and alignment with applicable regulatory frameworks such as GDPR or HIPAA."
        },
        {
            "title": "References",
            "content": "Lakshya Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, and Omar Khattab. 2025. Gepa: Reflective prompt evolution can outperform reinforcement learning. Preprint, arXiv:2507.19457. 9 Mariem Brahem, Jasmine Watissee, Cédric Eichler, Adrien Boiret, Nicolas Anciaux, and Jose Maria de Fuentes. 2024. retellme: Design rules for using large language models to protect the privacy of individuals in their textual contributions. In DPM 2024 - International Workshop on Data Privacy Management @ ESORICS, Barcelona, Spain. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2024. Instructzero: instruction optimization for black-box Efficient large language models. In Forty-first International Conference on Machine Learning. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, and 201 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Preprint, arXiv:2507.06261. Tobias Deußer, Lorenz Sparrenberg, Armin Berger, Max Hahnbück, Christian Bauckhage, and Rafet Sifa. 2025. survey on current trends and recent advances in text anonymization. In 2025 IEEE 12th International Conference on Data Science and Advanced Analytics (DSAA), pages 19. Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, and Wei Xu. 2024. Reducing privacy risks in online selfdisclosures with language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1373213754, Bangkok, Thailand. Association for Computational Linguistics. Cynthia Dwork. 2006. Differential privacy. In Automata, Languages and Programming, pages 1 12, Berlin, Heidelberg. Springer Berlin Heidelberg."
        },
        {
            "title": "Gil",
            "content": "Léon-Paul"
        },
        {
            "title": "Francopoulo",
            "content": "and Anonymization for Schaub. 2020. the GDPR in the Context of Citizen and Customer Relationship Management In workshop on Legal and Ethical Issues (Legal2020), pages 914, Marseille, France. LREC2020, ELRA. and NLP. Ahmed Frikha, Nassim Walha, Krishna Kanth Nakka, Ricardo Mendes, Xue Jiang, and Xuebing Zhou. 2024. Incognitext: Privacy-enhancing conditional text anonymization via LLM-based private attribute In Neurips Safe Generative AI randomization. Workshop 2024. GDPR. 2016. Article 5 principles relating to processing of personal data. Accessed: 2025-12-16. Rajitha Hathurusinghe, Isar Nejadgholi, and Miodrag Bolic. 2021. privacy-preserving approach to extraction of personal information through automatic annotation and federated learning. In Proceedings of 10 the Third Workshop on Privacy in Natural Language Processing, pages 3645, Online. Association for Computational Linguistics. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2024. DSPy: Compiling declarative language model calls into stateof-the-art pipelines. In The Twelfth International Conference on Learning Representations. Siyan Li, Vethavikashini Chithrra Raghuram, Omar Khattab, Julia Hirschberg, and Zhou Yu. 2025. PAPILLON: Privacy preservation from Internetbased and local language model ensembles. In Proceedings of the 2025 Conference of the Nations the Association of for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3371 3390, Albuquerque, New Mexico. Association for Computational Linguistics. the Americas Chapter of for"
        },
        {
            "title": "State of",
            "content": "text data: Pierre Lison, Ildikó Pilán, David Sanchez, Montserrat Batet, and Lilja Øvrelid. 2021. Anonymithe sation models In challenges and future directions. art, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 41884203, Online. Association for Computational Linguistics. Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, and Marc Tommasi. 2025. Taueval: unified evaluation framework for useful and private text anonymization. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 216227, Suzhou, China. Association for Computational Linguistics. Justus Mattern, Benjamin Weggenmann, and Florian Kerschbaum. 2022. The limits of word level difIn Findings of the Association ferential privacy. for Computational Linguistics: NAACL 2022, pages 867881, Seattle, United States. Association for Computational Linguistics. Stephen Meisenbacher, Maulik Chevli, and Florian Matthes. 2025. On the impact of noise in differentially private text rewriting. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 514532, Albuquerque, New Mexico. Association for Computational Linguistics. OpenAI. 2025. Gpt-5 system card. Technical report, OpenAI. Accessed: 2025-12-18. Krista Opsahl-Ong, Michael Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab. 2024. Optimizing instructions and demonstrations for multi-stage language model programs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 93409366, Miami, Florida, USA. Association for Computational Linguistics. Stefan Pasch and Min Chul Cha. 2025. Balancing privacy and utility in personal LLM writing tasks: An automated pipeline for evaluating anonymizations. In Proceedings of the Sixth Workshop on Privacy in Natural Language Processing, pages 3241, Albuquerque, New Mexico. Association for Computational Linguistics. Constantinos Patsakis and Nikolaos Lykousas. 2023. Man vs the machine in the struggle for effective text anonymisation in the age of large language models. In Scientific Reports, volume 13, page 16026. Nature Publishing Group UK London. Ildikó Pilán, Pierre Lison, Lilja Øvrelid, Anthi Papadopoulou, David Sánchez, and Montserrat Batet. 2022. The text anonymization benchmark (TAB): dedicated corpus and evaluation framework for text anonymization. Computational Linguistics, 48(4):10531101. Rafael A. Rivera-Soto, Olivia Elizabeth Miano, Juanita Ordonez, Barry Y. Chen, Aleem Khan, Marcus Bishop, and Nicholas Andrews. 2021. Learning uniIn Proceedings versal authorship representations. of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 913919, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Robin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. 2024. Beyond memorization: Violating privacy via inference with large language models. In The Twelfth International Conference on Learning Representations. Robin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. 2025. Language models are advanced anonymizers. In The Thirteenth International Conference on Learning Representations. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025. Gemma 3 technical report. Preprint, arXiv:2503.19786. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Griffin Thomas Adams, Jeremy Howard, and Iacopo Poli. 2025. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25262547, Vienna, Austria. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Rui Xin, Niloofar Mireshghallah, Shuyue Stella Li, Michael Duan, Hyunwoo Kim, Yejin Choi, Yulia Tsvetkov, Sewoong Oh, and Pang Wei Koh. 2025. false sense of privacy: Evaluating textual data sanitization beyond surface-level privacy leakage. In ICLR 2025 Workshop on Building Trust in Language Models and Applications. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. 2024. Large language models as optimizers. In The Twelfth International Conference on Learning Representations. Tianyu Yang, Xiaodan Zhu, and Iryna Gurevych. 2025. Robust utility-preserving text anonymization based on large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2892228941, Vienna, Austria. Association for Computational Linguistics. Oleksandr Yermilov, Vipul Raheja, and Artem Chernodub. 2023. Privacyand utility-preserving NLP with anonymized data: case study of pseudonymization. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 232241, Toronto, Canada. Association for Computational Linguistics. Hanna Yukhymenko, Robin Staab, Mark Vero, and Martin Vechev. 2024. synthetic dataset for personal In Thirty-eighth Conference attribute inference. on Neural Information Processing Systems Datasets and Benchmarks Track. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations. Erion Çano and Ivan Habernal. 2025. Differentiallyprivate text generation degrades output language quality. Preprint, arXiv:2509.11176. 11 GEPA combines the fast initial gains of warm-start with decisive refinement jump, ultimately reaching 0.70 and maintaining that advantage over the remainder of the budget. This pattern suggests the warm-start stage efficiently finds strong region of the prompt space, while refinement is necessary to escape the early plateau and reach higher-quality trade-offs. On TAB, the separation between components is even clearer. GEPA SIMPLE FEEDBACK again improves quickly (reaching 0.63 early and later 0.69), whereas GEPA RICH FEEDBACK alone lags substantially and only recovers late in the budget (ending around 0.56). Our method steadily accumulates improvements throughout optimization and achieves the best final score (about 0.74), outperforming both single-component variants and MIPROV2 (which plateaus around 0.62). Overall, these results highlight that warm-start is important for sample-efficient early gains, refinement provides the additional progress needed to surpass strong plateaus, and combining the two stages yields robust optimizer that dominates competitive baseline under relatively small and fixed compute budget."
        },
        {
            "title": "B Backbone Evaluation Robustness",
            "content": "On our paper, all evaluations that require an LLMbased judge are conducted using Gemini-2.5flash. To contextualize this choice and sanitycheck that our reported improvements are not an artifact of unusually permissive judging, we run the same evaluation pipeline on the original, nonanonymized inputs. Table 3 reports these baseline privacy/utility scores. As expected, the unmodified texts exhibit nearmaximal privacy leakage across tasks: privacy is close to zero for SYNTHPAI, TAB, and MEDQA, indicating that the corresponding privacy objectives are achieved almost systematically without any protective rewriting. DB-BIO and PUPA show slightly higher privacy values, but remain very low, reflecting that these datasets still contain substantial identifying content even before any adversarial probing. In contrast, utility remains high on tasks where the metric measures task performance or content fidelity without requiring anonymization (e.g. DB-BIO and PUPA), confirming that the raw texts are well-formed for downstream use and that the evaluation is capable of recognizing strong task signal. Figure 4: comparison of learning behavior of our modified GEPA implementation against each separated component and state-of-the-art prompt optimizer reference (MIPROv2). Results are measures with Gemma-327b-it on SynthPAI (top) and Mistral-Small-3.2-24B on TAB (bottom)."
        },
        {
            "title": "A Prompt Optimization Ablation Study",
            "content": "Figure 4 ablates the main components of our GEPA-based optimizer by comparing: (i) our full two-stage GEPA pipeline (warm-start + refinement), (ii) warm-start-only variant using simple scalar feedback (GEPA SIMPLE FEEDBACK), (iii) refinement-only variant driven by rich structured feedback (GEPA RICH FEEDBACK), and (iv) the state-of-the-art prompt optimizer MIPROV2 (Opsahl-Ong et al., 2024). All methods are run under the same rollout budget and evaluated using the same task score as in the main paper. We report learning curves on two representative tasks/backbones: SYNTHPAI with Gemma-327B-it (top) and TAB with Mistral-Small-3.2-24B (bottom). Across both tasks, the full two-stage method is consistently the best final performer and the most reliable. On SYNTHPAI, GEPA SIMPLE FEEDBACK makes rapid early progress but quickly saturates (plateauing around 0.62), while GEPA RICH FEEDBACK improves more gradually and reaches similar plateau. In contrast, two-stage 12 Methods DB-Bio SynthPAI TAB PUPA MedQA Methods DB-Bio SynthPAI TAB PUPA MedQA No Anonymization 6.29/100 0/100 0/100 5.62/99.2 0/64.0 Qwen3-30B-A3B Table 3: Evaluation results on original texts (encoded as privacy/utility). Tasks requiring backbone LLM ( Gemini-2.5-flash) are marked with ."
        },
        {
            "title": "C Model Scaling",
            "content": "We study whether the benefits of adaptive text anonymization persist on smaller language models by repeating the same prompt-optimization procedure on substantially smaller one ( Qwen-2.57B) compared to the mid-size model Qwen330B-A3B used in our core paper. Table 4 reports privacy/utility for the Seed Prompt and the corresponding Optimized Prompt on all five tasks. Overall, optimization yields consistent privacy gains at both scales, indicating that instruction learning transfers beyond large backbones. For Qwen-2.5-7B: optimization delivers substantial privacy improvements on SYNTHPAI (5.8817.6) and TAB (34.390.6), and moderate gains on DBBIO and PUPA. While the smaller model typically starts from weaker seed prompt (lower privacy and sometimes lower utility than the 30B model), the optimized prompts recover fraction of the privacy gains observed at 30B: most strikingly on TAB. That said, the 7B model more often exhibits stronger privacyutility coupling (e.g., SYNTHPAI and PUPA), suggesting that smaller models have less headroom to simultaneously perform complex rewriting and strict content preservation. One possible hypothesis is that using the same backbone model both to propose/refine prompts during optimization increase the task difficulty which can favor larger models compared to smaller versions. Taken together, these results support that adaptive prompt optimization is effective across model scales, while larger backbones provide more favorable privacyutility trade-offs on the hardest tasks which can compete with closed-models."
        },
        {
            "title": "D Implementation Details",
            "content": "D.1 Our Framework Rich Feedback Generation. We generate rich feedback for each task automatically using Qwen3-Next-80B-A3B-Instruct, though we observed similar results with other closed and open models. The rich feedback agent transforms simple scalar metrics into detailed, actionable feedback strings that guide the evolutionary prompt optiSeed Prompt Optimized Prompt 64.0/100 12.6/92.4 36.2/54.0 82.3/72.1 3.52/58.6 65.5/100 22.5/94.4 92.3/56.2 98.0/79.3 24.6/45.9 Qwen-2.5-7b Seed Prompt Optimized Prompt 41.2/97.3 5.88/91.5 34.3/55.3 79.7/87.3 5.18/47.1 47.1/98.0 17.6/86.0 90.6/53.2 88.4/79.2 7.29/41.2 Table 4: Performance of Qwen-2.5-7b compared to Qwen3-30B-A3B and their seed prompt version (encoded as privacy/utility). the gold, task is comtrace=None, single method: pred, You are an expert Python developer specializing in the DSPy framework. will provide you with Python class definition used for benchmarking text anonymization. The class typically initializes datasets and dspy modules, and defines two evaluation methods: compute_utility and compute_privacy. Your for code to write pute_overall_score_with_rich_feedback(self, pred_name=None, pred_trace=None). This method should implement the following logic: 1. Execute Metrics: Run the logic found in compute_utility and compute_privacy. If the metric relies on dspy.ChainOfThought module or similar, do not just get the final boolean/float result. You must capture the reasoning or specific outputs (e.g., the specific attribute the attacker guessed, the list of missing entities, or the logic the judge used) to provide context. 2. Compute Overall Score: Calculate the average of the utility and privacy scores (unless the logic implies different weighting). 3. Construct Feedback: Build detailed string (feedback_string) that acts as prompt for an optimizer. This string must: - State the specific values for Utility and Privacy. - Include the qualitative details captured in step 1 (e.g., \"The attacker successfully identified [Attribute] because [Reasoning]\"). - Conclude with one-sentence actionable instruction (e.g., \"Try to mask more entities while preserving utility.\"). 4. Return Output: Return dspy.Prediction object containing the calculated score and the generated feedback. Output only the Python code for this method. Figure 5: Instruction for the Rich Feedback Agent. This prompt is used to automatically generate detailed feedback functions from base evaluation metrics. mization process. Figure 5 shows the instruction provided to the rich feedback agent. Base vs. Rich Feedback Examples. To illustrate the difference between base and rich feedback, consider the TAB dataset. The base feedback simply states: \"The overall score is 0.65, which is the arithmetic mean of the utility score (0.75) and the privacy score (0.55). Try to improve both scores.\" In contrast, the rich feedback provides: \"Overall Score: 0.650 (out of 1.0). Score Breakdown: Utility (semantic similarity): 0.750. Privacy (entity masking rate): 0.550. Remaining Sensitive Entities (3): John Smith, 555-1234, john@example.com. Try to mask more entities while preserving the utility of the text.\" This additional context enables more targeted instruction refinement. We present example feedback result implementation in Listing 1. The rich feedback includes detailed analysis, specific entities or errors, and actionable guidance. This additional context enables more targeted instruction refinement. 1 def compute_overall_score_with_rich_feedback ( 2 3 self , gold , pred , trace = None , pred_name = None , pred_trace = None 13 Task Domain Privacy Threat Privacy Metric Utility Metric Test Size DB-Bio SynthPAI TAB PUPA MedQA Biographies Social media Legal documents User prompts Medical QA Identity re-identification Attribute inference Sensitive span leakage PII exposure Patient re-identification LLM attacker (top-3 accuracy) Occupation classification LLM attacker accuracy Span masking recall PII detection recall Stylistic similarity ROUGE-1 Semantic similarity LLM-as-judge quality Medical QA 239 617 555 350 125 Table 5: Overview of the five anonymization tasks in our benchmark. Each task presents distinct challenges across domain, threat model, and utility requirements, enabling comprehensive evaluation of adaptive anonymization approaches. 4 ): 5 6 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Compute individual metrics utility = self . compute_utility ( gold , pred , trace ) privacy = self . compute_privacy ( gold , pred , trace ) overall_score = ( utility + privacy ) / 2.0 # Gather remaining entities for context remaining_entities = [] for entity in gold . entity_mentions : if ( entity [\" identifier_type \"] != \" NO_MASK \" and entity [\" span_text \"] not in pred . anonymized_text ): remaining_entities . append ( entity [\" span_text \" ]) remaining_entities = set ( remaining_entities ) # Construct detailed feedback string feedback_parts = [] feedback_parts . append ( f\" Overall Score : { overall_score :.3 f}\" ) feedback_parts . append (\" nScore Breakdown :\") feedback_parts . append ( f\" - Utility : { utility :.3 f}\") feedback_parts . append ( f\" - Privacy : { privacy :.3 f}\") feedback_parts . append ( f\" nRemaining Sensitive Entities \" f\" ({ len ( remaining_entities ) }) : \" f\"{', '. join ( remaining_entities )}\") feedback_parts . append ( \" Try to mask more entities while \" \" preserving the utility of the text .\") feedback_string = \"n\". join ( feedback_parts ) return dspy . Prediction ( score = overall_score , feedback = feedback_string ) Listing 1: Generated rich feedback implementation for TAB. Optimization Hyperparameters. All experiments use fixed rollout budget of = 1500 LLM forward passes. During Stage 2 (warm-start), we use early stopping with patience = 5 iterations to preserve budget for refinement. In Stage 3, we set the adaptive validation sampling ratio to α = 0.3, evaluating candidate prompts on 30% of the validation set in round-robin fashion. The reflection minibatch size is set to 3 examples across all tasks. We use 111 examples for training and 111 examples for validation, with all remaining data reserved exclusively for testing. The seed prompt is the default instruction generated by DSPy for minimal anonymization signature (text -> anonymized_text), without any task-specific enGiven legal case document in the field text, anonymize all personally identifiable information including names, addresses, phone numbers, email addresses, dates of birth, and any other sensitive identifiers. Replace each entity with generic placeholder (e.g., [PERSON], [LOCATION], [DATE]). Preserve the semantic meaning and legal context of the document. Output the anonymized text in the field anonymized_text. Figure 6: Task-specific prompt for TAB. Given user query in the field text, remove or generalize all personally identifiable information (PII) including names, addresses, phone numbers, email addresses, social security numbers, and credit card numbers. The anonymized query should preserve the users intent and allow for generating comparable response. Output the sanitized text in the field anonymized_text. Figure 7: Task-specific prompt for PUPA. Given Reddit comment in the field text, rewrite it to prevent inference of the authors demographic attributes (age, gender, location, occupation, education, relationship status) while preserving the core content and message. Generalize or remove identifying markers without inventing new information. Output the rewritten comment in the field anonymized_text. Figure 8: Task-specific prompt for SynthPAI. gineering. D.2 Comparison Methods Adversarial Feedback and RUPTA Configuration. To ensure fair comparison, we configure both Adversarial Feedback (Staab et al., 2025) and RUPTA (Yang et al., 2025) with = 3 refinement hops, matching the multi-turn interaction budget used in their original implementations. All methods use GPT-5-chat as the base model to reflect state-of-the-art closed-source performance. Other available GPT-5 variants did not yield meaningful or consistent improvements on our benchmarks, so we report results using GPT-5-chat for clarity and reproducibility. Task-Specific Manual Prompts. We design taskspecific prompts through careful manual engineering, explicitly encoding privacy and utility requirements for each benchmark. These prompts serve as upper bounds on what can be achieved through expert human effort without optimization. The prompts are shown in Figures 610. Given celebrity biography in the field text, anonymize the text to prevent re-identification of the individual while preserving information about their occupation and achievements. Remove or generalize names, specific locations, organization names, and other identifying details. Maintain the factual content necessary for occupation classification. Output the anonymized biography in the field anonymized_text. Mistral-Small and Qwen3 are distributed under Apache 2.0 and Gemma-3 under its dedicated terms of use https://ai.google.dev/gemma/terms. Figure 9: Task-specific prompt for DB-Bio. Given medical case description in the field text, rewrite it to obfuscate the authors writing style while preserving all clinical information necessary to answer the associated medical question. Vary sentence structure, word choice, and phrasing without altering diagnostic details, symptoms, lab values, or treatment information. Output the rewritten case in the field anonymized_text. Figure 10: Task-specific prompt for MedQA. Computational Costs. We report approximate API costs to facilitate reproducibility and cost-aware deployment decisions. For our framework, running the full optimization pipeline locally with open-source models (MistralSmall, Gemma-3, or Qwen3) incurs costs only for the external evaluation backbone. Using Gemini2.5-flash for privacy and utility evaluation during optimization and final testing results in approximately $1 per task per model, covering the 1,500 rollout budget plus validation and test set evaluations. In contrast, the GPT-5-based comparison methods require significantly higher expenditure due to the cost of closed-source inference. Running single GPT-5-based agent across all test examples costs approximately $8 per task. These estimates highlight practical advantage of our approach by shifting the computational burden to locally deployed open-source models and using affordable API-based evaluation only for metric computation. Moreover, even when local GPU resources are unavailable, outsourcing inference for open-source models via trustworthy providers incurs only minimal additional costs due to the competitive pricing of mid-sized models at the time of writing (< $0.10 per task)."
        },
        {
            "title": "F Hardware and Code",
            "content": "We conducted all experiments with two Nvidia Quadro RTX 6000 GPU cards with 24GB memory and Intel Xeon Silver 4114 CPU. The main libraries used include DSPy 3.0.4, HuggingFace transformers 4.57.1, datasets 4.4.1 and sentencetransformers 5.1.2."
        },
        {
            "title": "G Scientific Artifacts",
            "content": "We used DSPy, TAB, PUPA (MIT), DB-Bio, LUAR (Apache 2.0), SynthPAI (CC-BY-NC-SAFor models, 4.0), MedQA (CC-BY-SA 4.0)."
        },
        {
            "title": "H Task Details",
            "content": "H.1 DB-Bio The DB-Bio task uses biographical entries from the DBpedia Classes dataset, which contains short, factual descriptions of notable individuals including their profession, achievements, and life events. The privacy objective is to prevent re-identification: an adversarial LLM attempts to infer the identity of the person described in the biography by analyzing residual biographical signals. This represents realistic threat model where attackers possess extensive world knowledge and reasoning capabilities about celebrities. The utility objective requires preserving sufficient occupational information to maintain accuracy on occupation classification. This task tests the models ability to remove identifying details (names, specific achievements, temporal markers) while retaining abstract professional characteristics. The tension between identity protection and occupational signal preservation creates nuanced trade-off that requires careful semantic manipulation. Training Example DB-BIO Privacy Metric: Utility Metric: Data Sample: Optimized Prompt (Mistral-Small): Binary re-identification metric: an attacker LLM (Listing 2) proposes the top-3 most likely celebrity names based on the anonymized biography. Privacy score is 1.0 if the true identity is not in the top-3, 0.0 otherwise. Binary classification accuracy: classifier LLM (Listing 3) predicts the persons occupation from the anonymized text. Geoffrey Harber Diggle (6 December 1902 13 February 1993) was British chess player and writer. Diggle contributed articles to the British Chess Magazine (BCM) from 1933 to 1981, Alexander called Diggle \"one of the best writers on chess that know\". In his Book of Chess, Alexander reproduced in toto Diggles account, first published in the November and December 1943 BCM, of the de facto 1843 world championship match between Staunton and St. Amant. [...] Diggle later adopted the sobriquet as pseudonym, writing series of articles in Newsflash under that name between 1974 and 1986. Chess historian Edward Winter wrote the following in his remembrance of Diggle in CHESS magazine: Specializing in nineteenth-century chess history (particularly the Staunton period), he brought the old masters to life with rare wit and shrewdness. [...] [...] Anonymize the given text by redacting all personally identifiable information (PII) while ensuring: 1. The occupation or role of the individual(s) remains clear from the context. 2. The text remains useful and coherent for its intended purpose. 3. The anonymized text does not allow identification of the individual(s) through inference or remaining details. [...] Guidelines 1. Redaction Rules - Names: Replace all names (first, last, nicknames, titles) with placeholders (e.g., [Name], [Nickname], [Lastname]). - Dates: Replace specific dates, years, or ranges with placeholders. - Locations: Replace cities, states, countries, and specific addresses with placeholders. - Organizations: [...] 2. Preservation Rules - Occupation/Role: Ensure the individuals profession or role is still inferable [...] - Multiple Individuals: Redact all names and (e.g., \"cyclist,\" \"diplomat,\" \"racing driver\"). identifiers for all mentioned individuals. [...] - Unique Achievements: Redact specific achievements (e.g., \"gold medal in 2004 Olympics\") to prevent identification. - Scandals/Controversies: Redact details of scandals or controversies that could uniquely identify the individual. [...] Strategy 1. Scan for PII: Identify all names, dates, locations, organizations, and unique achievements. 2. Replace with Placeholders: Use generic placeholders while preserving context. 3. Validate: Check for identifiability and utility. \"\"\" You are very knowledgeable about celebrities and can identify them from the given anonymized biography . When asked to identify individuals , disregard any missing information and context , like placeholder '< PERSON >' or ' ORGANIZATION ', respond only with the names of 3 celebrity candidates most likely being talked by the provided anonymized biography text and sort the names from highest to lowest probability .\"\"\" 1 class Privacy ( dspy . Signature ): 2 3 4 5 6 7 8 9 10 11 biography : str = dspy . InputField () celebrity_candidates : list [ str ] = dspy . OutputField () Listing 2: Privacy attacker signature for DB-Bio. \"\"\" Classify the following biography text into one of the following occupations \"\" \" 1 class Utility ( dspy . Signature ): 2 3 4 5 biography : str = dspy . InputField () occupation : Literal [* self . _labels ] = dspy . OutputField () Listing 3: Utility evaluator signature for DB-Bio. 16 H.2 SynthPAI SynthPAI (Yukhymenko et al., 2024) addresses the challenging problem of mitigating inference attacks based on weak, distributed profiling signals. The dataset consists of synthetic Reddit-style posts where demographic attributes (age, gender, location) and other personal characteristics can be inferred from writing style, topic choices, and contextual clues rather than explicit mentions. Privacy is evaluated using an adversarial LLM that attempts to infer specific labeled attributes from sanitized text, representing sophisticated attacker capable of exploiting subtle linguistic patterns and background knowledge. Utility is measured through ROUGE-1 score to ensure the sanitized post retains its core content and readability. This task is particularly challenging because the sensitive information is implicit and diffuse throughout the text rather than localized to specific entities. Effective sanitization requires identifying and mitigating subtle stylistic and contextual cues while preserving the posts communicative value, testing the limits of LLM-based semantic rewriting capabilities. Training Example SYNTHPAI Privacy Metric: Binary re-identification metric: an attacker LLM attempts to infer the target demographic attribute from both original and anonymized text, and verifier LLM (Listing 4) checks if the inferences match. Privacy score is 1.0 if the inferences differ (successful anonymization), 0.0 otherwise. Utility Metric: ROUGE-1 F-measure between original and anonymized text. Data Sample (target attribute: Wellington, New Zealand): Optimized Prompt (Gemma 3): Heard rumors bout some cheeky taniwha messin with boaters out on oriental bay old stories die hard reckon. You are an expert at anonymizing text while preserving its meaning and utility. [...] The goal is to remove potentially identifying information about person, while maintaining the core meaning and sentiment of the original text as much as possible. high utility is crucial. Heres how to approach the anonymization: 1. Identify potentially identifying information: Look for details that could reveal someones occupation, location (city, country), or other private attributes. This includes: Specific job titles or roles (e.g., \"editing suite\") References to niche hobbies or cultural practices specific to region (e.g., \"kabuki\", \"kite battles\") prioritize these for generalization or removal. Details about frequented locations, especially local spots (e.g., \"local spots\", references to specific festivals). References to financial hardship or specific income levels (e.g. \"barely make ends meet\"). 2. Anonymize by generalization or removal: Generalization: Replace specific details with broader categories. For example: \"kabuki\" -> \"performance art\" \"editing suite\" -> \"workplace\" or simply work environment [...] Prioritize retaining the overall message. 3. Preserve Utility: Strive to maintain the original meaning and intent of the text as much as possible. Your goal is to reduce identifiability, not to rewrite the text. [...] 4. Avoid Over-Anonymization: [...] 5. Context Matters: Pay attention to the context. Combine seemingly innocuous details can become identifying if combined. Important Considerations: Retain conversational elements like \"lol\" and expressions of emotion unless specific detail is too revealing. [...] \"\"\" The following are public comments made on Reddit by one author . Your task is to guess the authors target_attribute \" \"\" comment : str = dspy . InputField () target_attribute : str = dspy . InputField () guess : str = dspy . OutputField () 1 class Privacy ( dspy . Signature ): 2 3 4 5 6 7 8 9 10 class Verif ( dspy . Signature ): 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 string_A : str = dspy . InputField () string_B : str = dspy . InputField () prediction : bool = dspy . OutputField () \"\"\" You are helpful assistant that decides whether prediction and target refer to the same thing or if the prediction is less precise . Below give you several two string pairs , the ground truth and prediction each , and you have to decide whether the prediction is correct . If it is return True . If not write False . One correct example is ' usa ' and ' United States '. In case the prediction is less precise version of the ground truth , e.g., ' Vancouver ' and ' Canada ', you should type True . For locations and only locations if the prediction is more accurate and also contains the ground truth , e.g., prediction is ' London , UK ' and ground truth is ' United Kingdom ', you should type True . For occupation unemployed and none should be considered the same . \"\"\" 17 Listing 4: Privacy attacker and verifier signatures for SynthPAI. H.3 TAB The Text Anonymization Benchmark (TAB) (Pilán et al., 2022) comprises sensitive text spans extracted from European court case documents. Legal texts contain personal information about parties, witnesses, and case-specific details that must be protected for publication or research use. Privacy is measured through recall of properly masked sensitive spans: the proportion of annotated sensitive information successfully removed or replaced. Utility is assessed via ROUGE-1 score, which measures lexical overlap between the sanitized and original text, serving as proxy for content preservation. This task presents different challenge: rather than full document rewriting, it requires precise identification and localized modification of sensitive spans while maintaining document coherence and legal semantic content. The structured nature of legal text and the availability of gold-standard sensitive span annotations make this task particularly suited for evaluating fine-grained anonymization precision. Training Example TAB Privacy Metric: Utility Metric: Data Sample: Optimized Prompt (Qwen3): Entity masking recall: the fraction of gold-annotated sensitive spans that no longer appear in the anonymized text. Semantic similarity using cross-encoder model (cross-encoder/stsb-roberta-base) that computes similarity scores between original and anonymized documents. PROCEDURE The case originated in an application (no. 38007/02) against the Republic of Poland lodged with the Court under Article 34 of the Convention for the Protection of Human Rights and Fundamental Freedoms (the Convention) by Polish national, Mr Artur Warsinski (the applicant), on 3 October 2002. The Polish Government (the Government) were represented by their Agent, Mr J. Woł asiewicz of the Ministry of Foreign Affairs. [...] THE CIRCUMSTANCES OF THE CASE The criminal proceedings against the applicant. The applicant, Mr Artur Warsinski, is Polish national who was born in 1976 and lives in Bytów. The applicant was charged with drug trafficking and detained on remand from 6 April 2002 to 21 May 2004. He was detained in the Słupsk Detention Centre. On 28 May 2003 the Słupsk Regional Court found the applicant guilty of drug trafficking and sentenced him to three years imprisonment. [...] The envelope also bears stamp of the Słupsk Regional Court. On 4 December 2002 the applicant sent another letter to the European Court of Human Rights. It was received by the Court on 17 December 2002. It bears stamp censored on, handwritten date 4 December and an illegible signature. The envelope also bears stamp of the Słupsk Regional Court. [...] On 30 June 2003 he received letter from the President of the Słupsk Regional Court informing him that, according to the information obtained from the Criminal Department of the Słupsk Regional Court, the applicants correspondence had not been censored. He further stated that the applicants correspondence had been stamped censored by mistake. You are to perform rigorous, context-aware, and deterministic anonymization of legal or judicial documents using multi-layered strategy derived from verified best practices and iteratively refined through real-world feedback. The goal is to produce an anonymized_text output that eliminates **all identifiable entities** while preserving exact structural, legal, factual, procedural, chronological, and linguistic fidelity** of the original. [...] The output is analytically useful for legal, human rights, academic, or policy research contexts without compromising privacy. [...] All replacements must be deterministic and consistent throughout the text: Individuals - Full names Always [REDACTED] if gender unspecified. [...] Legal professionals Replace with: - lawyer practising in [TOWN] - representative of [INSTITUTION] - judge at the [CITY] Court of Appeal - Do **not** use abbreviations (e.g., R.K. [REDACTED] only). [...] 1. Scan the document **once**, identifying all entities with precise type (name, date, place, number, etc.). 2. Apply **predefined, reusable replacements** using the exact templates above. [...] 7. **Never** replace named laws, international conventions, or standard legal instruments (e.g., the Convention, Rule 52 1) with placeholders unless the jurisdiction or location enables re-identification. [...] 10. Do not omit any temporal expressions: every date including in ranges, comparisons, or durations must be replaced. Output only the fully anonymized text in the anonymized_text field. No explanations. H.4 PUPA The PUPA task (Zhao et al., 2024) focuses on sanitizing user prompts submitted to conversational AI systems in different languages, using data from real ChatGPT interactions. Users often inadvertently include personally identifiable information (PII) in their promptsnames, addresses, phone numbers, 18 email addresses, and other explicit identifiers. The privacy objective is to detect and remove all labeled PII instances from the text. Utility is evaluated using an LLM-as-a-judge approach: we measure whether responses generated from sanitized prompts maintain comparable quality and relevance to responses from original prompts. This task evaluates the frameworks ability to perform entity-level redaction while preserving the semantic intent and context necessary for meaningful AI assistant responses. Unlike the other tasks, PUPA emphasizes explicit identifier removal rather than implicit signal obfuscation, representing more traditional anonymization scenario adapted to modern LLM interaction contexts. Training Example PUPA Privacy Metric: Utility Metric: Data Sample: Optimized Prompt (Qwen3): LLM judge (Listing 5) that counts PII units remaining in the anonymized query. We rely on an LLM judge following original work (Li et al., 2025) because ground truth labels contains annotated PII which correspond to celebrity names (which in this task is not considered private information). Generating responses to both original and anonymized queries using comparing response quality via an LLM judge (Listing 6). Gemini-2.5-flash, then Hi Salar/ Waqas, Well noted. We will arrange for testing once receive the board samples you sent. Would you please also advise the demand and delivery location for measure jig (one free jig per factory location), so that we can send them to you as soon as possible. Thanks for your attention to this matter 是什么意思 You are an assistant tasked with processing text and producing modified version of it based on specific instructions. Task Description: Given field called text, your goal is to produce [...] The tasks can vary widely, from simple redaction new field called anonymized_text. to complex rephrasing, translation, or code modification. The type of modification needed is determined by the nature of the input text. [...] PII Redaction/Generalization: Identify and protect PII. This may include, but is not limited to: Names of people and organizations (e.g., \"St. Marys Church\", \"Hermawan DataTables\", \"Lille Université Club\", \"Marcq-en-baroeul basketball club\"). Replace specific names with generic placeholders (e.g., \"Church\", \"DataTables library\", \"a university [...] Consider generalization (e.g., \"aged 18 to 45\" can often club\", \"a town basketball club\"). remain unchanged, but specific birthdates should be removed). Code snippets containing sensitive information (API keys, specific URLs). Any other data that could be used to identify an individual. [...] When the input is code (e.g., CodeIgniter 4 code snippet), maintain the codes functionality. Replace specific library names or project-specific details with generic equivalents. Do not alter the codes behavior unless explicitly requested. [...] Focus on anonymizing project-specific names (e.g. controller/method names) [...] The responses will be evaluated on the level of PII leakage. Aim for 0% leakage rate whenever possible. [...] 1 class JudgeLeakage ( dspy . Signature ): 2 3 \"\"\" Count the number information pieces in `pii ` that are simultaneously (i) forms of PII and ( ii ) are leaked into the prompt given . Don 't count the cases where the leaked information is just celebrity name . Don 't count the cases where the `pii ` information does not appear in the prompt . \"\"\" pii = dspy . InputField () prompt = dspy . InputField () num_pii_leaked : int = dspy . OutputField () Listing 5: Privacy evaluator signature for PUPA. 4 5 6 7 8 9 10 \"\"\" You are comparing the quality of two responses , given user query . Is response at least as good as response B? \"\"\" 1 class JudgeQuality ( dspy . Signature ): 2 3 4 5 6 7 8 9 10 user_query : str = dspy . InputField ( desc =\" The user 's request to be fulfilled .\") response_A : str = dspy . InputField () response_B : str = dspy . InputField () judgment : bool = dspy . OutputField () Listing 6: Utility evaluator signature for PUPA. H.5 MedQA Built on the MedQA dataset (Jin et al., 2021), which contains multiple-choice questions from the United States Medical Licensing Examination, this task addresses the sanitization of medical case descriptions containing patient information and clinical details. Privacy is evaluated by measuring the writing style 19 similarity between medical claims extracted from sanitized and original texts. Utility is assessed through the downstream performance of zero-shot medical question answering classifiers applied to sanitized case descriptions. The key challenge is to obfuscate patient-specific details and potentially identifying medical facts while preserving the clinical reasoning structure and diagnostic information necessary for accurate medical inference. This task reflects real-world healthcare data sharing scenarios where regulatory compliance (e.g., HIPAA) demands rigorous privacy protection without compromising clinical utility. Training Example MEDQA Privacy Metric: Utility Metric: Data Sample: Stylometric distance: we compute LUAR embeddings (Rivera-Soto et al., 2021) for both original and anonymized case descriptions using the LUAR model, and define privacy as 1 cosine_similarity. Higher distance indicates successful style obfuscation. Binary question-answering accuracy: an LLM (Listing 7) selects the correct diagnosis from multiplechoice options based on the anonymized case description. 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby? Optimized Prompt (Mistral-Small): Placing the infant in supine position on firm mattress while sleeping Keeping the infant covered and maintaining high room temperature Application of device to maintain the sleeping position Avoiding pacifier use during sleep You are an expert medical text anonymizer. [...] The ultimate goal is to maximize the Privacy Score (a measure of writing style dissimilarity higher is better) without sacrificing the ability to infer the original diagnosis or solution.[...] Preserve Diagnostic/Problem-Solving Information: This is the most important aspect. Do not remove details crucial for diagnosis or answering the question. This includes symptoms, lab values, imaging results, family history, incidence rates, and the context of the presentation. The goal is to make it harder to identify the individual, not harder to arrive at the correct medical conclusion. [...] For questions, ensure the question remains logical and solvable after anonymization. The correct answer should still be obtainable if it was obtainable prior to anonymization. [...] Consider, *cautiously* and *only if it doesnt impact medical accuracy*, replacing specific biological term with more general category (e.g., \"peripheral artery\" -> \"blood vessel,\" \"lymph node\" -> \"gland\"). [...] Specifically, avoid simply replacing 14-year-old boy with patient\". Try paraphrasing sentences to achieve higher Privacy Score. [...] \"\"\" You are an expert in answering medical exam questions .\"\"\" description =\" The medical question to answer .\" question : str = dspy . InputField ( 1 class MedQAAnswer ( dspy . Signature ): 2 3 4 5 6 7 8 9 10 11 ) answer : str = dspy . OutputField ( ) options : list = dspy . InputField ( additional text .\" ) 12 desc =\" List of multiple choice answer options to choose from .\" description =\" The correct answer choice . Make sure to only provide the correct answer , and no Listing 7: Implementation Code for the Utility Evaluator. 20 Task: DB-BIO Method Original OpenPII RUPTA ( GPT-5) AF ( GPT-5) Task-Specific Prompt ( GPT-5) Mistral-Small-3.2-24B Gemma-3-27B Qwen3-30B-A3B Output Robin McLaurin Williams (July 21, 1951 - August 11, 2014) was an American stand-up comedian, actor, director, producer, writer, singer and voice artist. Starting as stand-up comedian in San Francisco and Los Angeles in the mid-1970s, he is credited with leading San Franciscos comedy renaissance. After rising to fame as Mork in Mork & Mindy (1978-1982), Williams went on to establish career in both stand-up comedy and feature film acting. He was known for his improvisational skills. [PERSON_NAME] [PERSON_NAME] [PERSON_NAME] ([DATE] - [DATE]) was an American stand-up comedian, actor, director, producer, writer, singer and voice artist. Starting as stand-up comedian in [LOCATION_CITY] [LOCATION_STATE] and [LOCATION_CITY] [LOCATION_STATE] in the [TIME_PERIOD], he is credited with leading [LOCATION_CITY] [LOCATION_STATE] comedy renaissance. After rising to fame as [PERSON_NAME] in [PERSON_NAME] & [PERSON_NAME] ([YEAR] - [YEAR]), [PERSON_NAME] went on to establish career in both stand-up comedy and feature film acting. He was known for his improvisational skills. person (born and passed away during unspecified times) was notable figure in the entertainment industry, with roles spanning from various forms of artistry including acting, directing, producing, and voice artistry. Starting as stand-up comedian in an unspecified location during an unspecified time, this individual is credited with leading an unspecified cultural movement in that location. After gaining recognition in an unspecified role (during an unspecified period), they went on to establish career in stand-up comedy and feature film acting. An individual who was active from the late 20th century into the early 21st century was an entertainer known for work as comedian, performer, director, producer, writer, singer, and voice artist. Starting as stand-up comedian in region of the United States during the 1970s, this person is credited with contributing to local comedy renaissance. After rising to fame through popular television role around that time, they went on to establish career in both stand-up comedy and film acting and became known for their improvisational skills. [ANONYMIZED PERSON] was an American stand-up comedian, actor, director, producer, writer, singer and voice artist. Starting as stand-up comedian in San Francisco and Los Angeles in the mid-1970s, [HE/SHE/THEY] is credited with leading San Franciscos comedy renaissance. After rising to fame as television character in popular late-1970s sitcom, [HE/SHE/THEY] went on to establish career in both stand-up comedy and feature film acting. [HE/SHE/THEY] was known for exceptional improvisational skills. [Name] [Name2] (July [Date1], [Year1] - August [Date2], [Year2]) was an American stand-up comedian, actor, director, producer, writer, singer and voice artist. Starting as stand-up comedian in [City1] and [City2] in the mid-[Decade], he is credited with leading [City1]s comedy renaissance. After rising to fame as [Character] in [TV Show] ([Year3]-[Year4]), [Name] went on to establish career in both stand-up comedy and feature film acting. He was known for his improvisational skills. Born in July 1951 and passing away in August 2014, this individual was performer working in comedy, acting, directing, and writing. They began their career as stand-up comedian in California in the mid-1970s and were part of burgeoning comedy scene in major West Coast city. After gaining recognition on popular television sitcom in the late 1970s and early 1980s, they continued to work as comedian and established successful career in film. They were particularly well-regarded for their ability to improvise and think on their feet. [REDACTED PERSON] ([DATE] - [DATE]) was an [REDACTED NATIONALITY] stand-up comedian, actor, director, producer, writer, singer and voice artist. Starting as stand-up comedian in [REDACTED LOCATION] and [REDACTED LOCATION] in the mid-[YEAR], [REDACTED PERSON] is credited with leading [REDACTED REGION]s comedy renaissance. After rising to fame as [NICKNAME] in [REDACTED TV SHOW] ([YEAR][YEAR]), [REDACTED PERSON] went on to establish career in both stand-up comedy and feature film acting. [REDACTED PERSON] was known for [REDACTED PERSON]s improvisational skills. Table 6: Additional qualitative examples for each text anonymization method."
        }
    ],
    "affiliations": [
        "CNRS",
        "Centrale Lille",
        "Hornetsecurity",
        "Inria",
        "Univ. Lille"
    ]
}