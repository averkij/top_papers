{
    "paper_title": "TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis",
    "authors": [
        "Shunian Chen",
        "Hejin Huang",
        "Yexin Liu",
        "Zihan Ye",
        "Pengcheng Chen",
        "Chenghao Zhu",
        "Michael Guan",
        "Rongsheng Wang",
        "Junying Chen",
        "Guanbin Li",
        "Ser-Nam Lim",
        "Harry Yang",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit a critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age groups. We argue that this generalization gap is a direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, a new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through a principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, a stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https://github.com/FreedomIntelligence/TalkVid"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 8 1 6 3 1 . 8 0 5 2 : r TalkVid: Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis Shunian Chen1,, Hejin Huang1,2,, Yexin Liu3,*, Zihan Ye1, Pengcheng Chen1, Chenghao Zhu1, Michael Guan1, Rongsheng Wang1, Junying Chen1, Guanbin Li2, Ser-Nam Lim3,, Harry Yang3,, Benyou Wang1, 1The Chinese University of Hong Kong, Shenzhen 2Sun Yat-sen University 3The Hong Kong University of Science and Technology wangbenyou@cuhk.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age-groups. We argue this generalization gap is direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https:// github.com/FreedomIntelligence/TalkVid 1. Introduction Audio-driven talking head synthesis has achieved remarkable photorealism, with recent models generating outputs that are often indistinguishable from real video under con- *First three authors contributed to this work equally. Ser-Nam Lim, Harry Yang, and Benyou Wang are the corresponding authors. trolled conditions [7, 22]. Yet, this success masks critical fragility. SOTA models remain brittle; as our experiments will demonstrate, their performance degrades significantly when confronted with the full spectrum of human diversity. This generalization gapthe failure to handle varied ethnicities, unconstrained head poses, and diverse languages as shown in Figure 1is not minor flaw. It is the primary bottleneck preventing the widespread, reliable, and equitable application of this technology. We argue this brittleness is direct consequence of foundational bottleneck: the data upon which these models are trained. Existing datasets force difficult trade-off. On one hand, datasets like HDTF [26] offer high-resolution, front-facing videos but are narrowly curated, lacking the linguistic, demographic, and motion diversity needed for robust generalization. On the other hand, large-scale, in-thewild datasets like VoxCeleb2 [5] offer diversity but are rife with technical artifactsmotion blur, compression noise, and inconsistent framingthat compromise the training of high-fidelity generative models. Consequently, the field has lacked resource that is simultaneously large-scale, diverse, and technically pristine. This data gap directly translates into models that are biased and unreliable. This paper introduces unified solution to this datacentric challenge. We present TalkVid, new dataset designed from the ground up to eliminate the trade-offs of prior work. Its construction is guided by three core principles: 1) Scale and Diversity, sourcing over 6,000 hours of raw video to capture broad cross-section of speakers, languages, and contexts; 2) High-Quality, enforced via rigorous, multi-stage automated filtering pipeline that ensures technical excellence in motion, aesthetics, and facial detail; and 3) Reliability, validated through human verifications confirming our pipelines alignment with perceptual quality. 1 Figure 1. Examples from our TalkVid dataset, showcasing the diversity in identity, ethnicity, and head pose that SOTA models must generalize to. Existing datasets lack this combined diversity and technical quality, leading to generalization failures. Name Year Speaker Hours Resolution Language Age Body Included Caption Source GRID [6] Crema-D [2] LRW [4] VoxCeleb1 [12] VoxCeleb2 [5] MEAD [18] HDTF [26] Hallo3 [7] MultiTalk [14] TalkVid (Ours) 2006 2014 2017 2017 2018 2020 2021 2024 2024 2025 33 91 1k+ 1.2k 6.1k+ 60 362 - - 7,729 27.5 11.1 173 352 2.4k 39 15.8 70 243.2 1,244.33 288p, 576p 720p 120p 256p 256p 384p 512p 480p 512p 1080p,2160p English English English - - English - - 20 lang 15 lang 1849 2074 - - - 2035 - - - 0-60+ No No No No No No No Yes Yes Yes No No No No No No No No No Yes Lab Lab Wild Wild Wild Lab Wild Wild Wild Wild Table 1. Comparison of open-source datasets for audio-driven talking-head generation. Resolution abbreviations denote pixel dimensions (widthheight): 120p (120120), 256p (256256), 288p (360288), 384p (384384), 480p (480720), 512p (512512), 576p (720576), 720p (960720), 1080p (19201080), and 2160p (38402160). For our TalkVid dataset, the listed resolutions comprise 87.59% of the data. Crucially, better training requires better evaluation. We introduce TalkVid-Bench, dedicated 500-clip evaluation benchmark stratified across key demographic (age, gender, ethnicity) and linguistic dimensions. Current evaluation practices, which rely on aggregate metrics, obscure critical failure modes and fairness issues. TalkVid-Bench enables fine-grained analysis that reveals model-specific biases and quantifies true generalization. Our experiments, leveraging this benchmark, provide the definitive evidence that training on TalkVid yields superior performance and that TalkVid-Bench is essential for exposing robustness gaps invisible to previous evaluation protocols. Our contributions are threefold: 1) we introduce TalkVid, large-scale, high-quality dataset containing 1,244 hours of talking-head videos from 7,729 speakers, curated via rigorous, human-validated pipeline; 2) we build TalkVid-Bench, stratified evaluation benchmark balanced across demographic and linguistic dimensions to enable transparent assessment of model fairness and generalization; 3) we conduct comprehensive empirical validation demonstrating that models trained on TalkVid achieve state-of-the-art performance, superior cross-domain robustness, and reveal critical biases missed by existing methods, thereby providing the community with essential resources for developing equitable talking-head synthesis models. 2. Related Works Audio-Driven Talking Head Synthesis. The synthesis of talking heads from audio has rapidly evolved, moving from GAN-based architectures to the now-dominant diffusion models. Early methods, often leveraging Generative Adversarial Networks (GANs) [10], achieved high-resolution, efficient synthesis. For instance, StyleHEAT [23] enabled one-shot generation, while others like SadTalker [25] and LipSync3D [13] focused on improving motion dynamics and lip-sync accuracy, respectively. Despite these ad2 Figure 2. The TalkVid construction pipeline. The process starts with (1) video collection and clip segmentation. Each candidate clip then undergoes (2) multi-stage filtering cascade to enforce quality across aesthetics, motion, and facial detail. Finally, the pipelines effectiveness is (3) validated against human judgments. vances, GAN-based approaches often suffer from temporal inconsistencies and limited expressiveness, particularly with large pose variations. To address these limitations, recent work has overwhelmingly shifted towards Diffusion Models (DMs), which offer superior temporal stability and photorealism. Foundational models like VExpress [17] demonstrated the potential of DMs for this task. Subsequent works have focused on enhancing control and alignment; for example, AniPortrait [19] and Hallo [21] employ multistage pipelines and part-aware modules to improve audiovisual correspondence. The current SOTA, exemplified by VASA [22], Hallo3 [7], and EDTalk [15], introduces disentangled latent representations to generate highly expressive and controllable facial dynamics beyond simple lip movements. However, these methods often rely on complex pipelines and pre-trained priors, leaving gap for unified and efficiently controllable model. Datasets for Talking Head Generation. Progress in talking head synthesis is intrinsically tied to the available training data. Early datasets, such as GRID [6] and CREMAD [2], were collected in controlled laboratory settings, providing clean audio-visual pairs but lacking diversity and scale. The advent of large-scale, in-the-wild datasets like LRW [4] and VoxCeleb2 [5] was major step forward, offering real-world variability. However, these datasets often suffer from inconsistent video quality and lack of granular annotations necessary for modeling fine-grained expression. More recent high-quality datasets, including MEAD [18], HDTF [26], and MultiTalk [14], have improved upon resolution and speaker diversity. Nevertheless, critical bottleneck persists: the absence of truly largescale benchmark that pairs high-resolution video with rich, semantic annotations for detailed expressive and semantic control. Our dataset is designed to fill this void, providing over 1,244 hours of high-fidelity video with dense captions to foster the next generation of controllable talking head models. 3. The TalkVid Dataset This section details the construction methodology, human verification protocol, and presents the analysis of the datasets characteristics. 3.1. Construction Methodology The TalkVid construction pipeline is illustrated in Figure 2. We first describe each stage and then present quantitative analysis validating the effectiveness of our filtering process against human assessments. 3.1.1. Data Preprocess Stage 1: Video Collection. We begin by collecting over 30,000 videos from YouTube, totaling more than 6,000 hours of high-resolution (1080p or higher) content. To ensure high-quality starting point, we target genres known for clear audio and stable video, such as educational lectures, technical reviews, and professional vlogs. For each source, we download the video, audio, and available autogenerated transcripts through yt-dlp [24]. Complete sourcing criteria can be found in Appendix A. Stage 2: Clip Segmentation. The collected videos are then processed through segmentation pipeline. First, all videos are standardized by re-encoding them to the H.264 (MP4) format. We then use PySceneDetect [1] to detect shot boundaries. Segments shorter than 5 seconds are discarded, as they are typically too brief to contain complete thought or gesture. Finally, using transcript timings, we remove segments without speech events. 3 Figure 3. The data filtering cascade. This diagram quantifies the progressive refinement of the dataset, showing the hours of video retained and discarded at each preprocessing and content-based filtering stage. Figure 4. Statistical distributions of the TalkVid dataset. Top row: technical quality metrics for the final, filtered dataset. Bottom row: distributions of the high-level characteristics, including video categories, language, and speaker demographics. 3.1.2. Content-Based Filtering To ensure each clip meets the technical demands of modern generative modeling, we subject each candidate to content-based filtering cascade. clip is retained only if it satisfies all criteria across three key quality dimensions: aesthetic quality, motion dynamics, and head detail. The metrics and their corresponding thresholds are summarized in Table 2, while the rationale for each is elaborated below. Filter 1: Aesthetic Quality. To guarantee high visual fidelity, we employ DOVER [20], no-reference video quality assessment model. By enforcing minimum score, we filter out clips containing perceptible compression artifacts, noise, or excessive blur, retaining only those with clean, high-quality appearance. Filtering Stage Metric Criterion / Threshold Aesthetic Quality DOVER Score [20] 7.0 Motion Stability CoTracker Ratio [9] [0.85, 0.999] Head Detail Movement Score Rotation Score Orientation Score Resolution Score Completeness Score = 100 Avg 80, Min 60 Avg 70, Min 60 Avg 70, Min 30 Avg 50, Min 40 Table 2. Filtering criteria for video clip selection, each candidate clip must satisfy all listed conditions. purpose: the lower bound ( 0.85) removes clips with erratic motion or blur, which manifest as tracking failures, while the upper bound ( 0.999) crucially discards unnaturally static or frozen shots. This ensures the retention of subtle, natural movements characteristic of live speaker. Filter 2: Motion Dynamics. We select for clips with natural motion characteristics using the point tracking stability ratio from CoTracker [9]. The specified range serves dual Filter 3: Head-Detail Filtering. Finally, we perform fine-grained assessment of the subjects head using suite 4 Stage CoT. Dov. Comp. Move. Orient. κ 0.74 0.90 0.80 0.66 0. Res. 0.72 Rot. 0.90 Avg. 0. Table 3. Cohens Kappa, κ for quality filtering stages. Abbreviations are: CoTracker (CoT.), Dover (Dov.), Head-Completeness (Comp.), Head-Movement (Move.), Head-Orientation (Orient.), Head-Resolution (Res.), and Head-Rotation (Rot.). of five metrics designed to ensure stability and clarity. These metrics collectively ensure the temporal stability of facial keypoints (Movement Score), smoothness of head orientation transitions (Rotation Score), largely frontal view without extreme angles (Orientation Score), sufficient face resolution for detail (Resolution Score), and the consistent visibility of all facial parts (Completeness Score). candidate must meet all five criteria, with further formulation details in Appendix B. 3.1.3. Human Validation Protocol. To confirm our automated filters serve as reliable proxy for human quality judgments, we conduct human evaluation study. For each of our seven filter criteria, we sample 100 borderline clips: 50 that marginally pass the filter and 50 that marginally fail. This focus on the decision boundary provides stringent test of our thresholds. Two trained annotators, blind to the filters decision, assign binary label (e.g., Acceptable/Unacceptable) to each clip based on detailed rubric defining each quality attribute, more details can be found in Appendix C.2. Results. The evaluation confirms the reliability of our pipeline. First, we measure high inter-annotator agreement (IAA), achieving an average Cohens Kappa (κ) of 0.79 across all criteria  (Table 3)  . This indicates our quality standards are well-defined and consistently interpreted. Second, our automated filters demonstrate strong performance against the human-annotated ground truth, reaching an average accuracy of 95.1% and an F1-score of 95.3%. This result validates our automated pipeline as reliable proxy for human quality assessment. Detailed per-criterion metrics are in the Appendix C.1. 3.2. Quantitative Analysis The final TalkVid dataset contains 1,244 hours of video from 7,729 unique speakers. Figure 3 provides holistic overview of the data attrition throughout our pipeline, while Figure 4 details the statistical properties of the final dataset. 3.2.1. Composition Clips average 17.93s in duration. The dataset is compositionally diverse, led by Personal Experience (474.0h) and Vlogger/Creator (357.6h) content. It spans over 15 Figure 5. Qualitative examples from TalkVid. The sequences illustrate demographic diversity and key technical challenges for synthesis: varied lighting, complex backgrounds, and occlusions. languages, with English (867.1h) and Chinese (248.9h) being the most prominent. Speaker demographics are varied across age, gender, and ethnicity, with the 31-45 year group being the largest (814.8h). 3.2.2. Technical Quality Our filtering ensures high technical quality. mean DOVER score of 8.55 confirms strong visual fidelity. The mean CoTracker ratio of 0.92 validates our selection for natural motion, successfully culling both overly static and erratic shots. Head detail scores (Movement, Rotation, Orientation) are sharply skewed towards their maxima, indicating stable, consistently trackable faces. This technical profile makes the dataset highly suitable for generative tasks. 3.2.3. TalkVid-Core We introduce TalkVid-Core, high-purity and diverse subset comprising 160 hours of content. This subset is derived by applying stringent set of thresholds to quality metrics. Importantly, the data is uniformly sampled across ethnicity, gender, and age categories to ensure balanced representation. Following the selection of this high-quality video set, we generate annotations for each clip using Gemini 1.5 Pro2. Further details and qualitative examples of these annotations are provided in Appendix E. 3.3. Qualitative Analysis Figure 5 illustrates the datasets breadth. The examples confirm the demographic diversity (age, gender, ethnicity) and show difficult capture conditions. These include in-thewild lighting (Row 1), complex indoor backgrounds (Row 4), and accessories such as headwear and glasses (Rows 1, 5). The sequences also contain significant challenges for synthesis models, including occlusions from hands and microphones (Row 5) and full range of motion dynamics, 1Data characteristics labeled as Unknown are not taken into account. 2gemini-1.5-pro-"
        },
        {
            "title": "Polish",
            "content": "FID FVD Sync-C Sync-D FID FVD Sync-C Sync-D FID FVD Sync-C Sync-D"
        },
        {
            "title": "Language",
            "content": "HDTF Hallo3 TalkVid 60.817 443.202 59.842 387.507 59.562 357.603 4.000 4.753 4.567 10.403 9.536 9.867 48.561 415.564 52.514 342.062 47.509 306.131 3.285 4.005 4. 10.058 9.450 9.521 39.231 321.261 38.458 343.553 39.271 288.178 2.654 3.424 3.695 10.368 9.690 9."
        },
        {
            "title": "Ethnicity",
            "content": "HDTF Hallo3 TalkVid 46.589 305.284 40.927 267.492 40.740 274.226 3.587 4.218 4.176 9.997 9.292 9.587 50.807 376.161 48.218 350.025 44.373 326.840 3.746 4.296 4. 10.203 9.636 9.724 53.163 302.214 52.492 303.478 48.511 303.997 3.453 4.076 4.056 10.198 9.517 9."
        },
        {
            "title": "Gender",
            "content": "HDTF Hallo3 TalkVid 46.525 306.947 41.549 299.984 39.398 294.709 3.540 3.935 3.984 9.965 9.496 9.639 46.173 297.840 42.659 258.583 41.967 241.920 3.489 4.034 4. 10.223 9.704 9.788 19-30 31-45 60+"
        },
        {
            "title": "Age",
            "content": "HDTF Hallo3 TalkVid 45.591 283.927 41.501 272.912 37.879 253.698 3.679 4.214 4.329 10.078 9.565 9.605 58.843 295.236 44.493 253.756 43.702 222.202 3.592 4.380 4. 10.448 9.581 9.626 53.192 350.580 53.854 332.383 51.141 321.556 3.630 3.748 3.942 10.018 9.741 9.804 Table 4. Comparison with other baseline training datasets, including HDTF [26] and Hallo3 [7] on TalkVid-bench across four dimensions, showing subgroup-level performance. from large-amplitude expressions (Row 1) to subtle conversational movements (Row 2). The inclusion of diverse subjects under these conditions makes TalkVid valuable resource for training and evaluating generative models. 4. Experiments We conduct experiments to validate the benefits of TalkVid as training corpus and to demonstrate the utility of TalkVid-Bench for revealing model-specific biases. We compare SOTA model trained on TalkVid against the same model trained on prior datasets. 4.1. Experimental Setup 4.1.1. Model and Baselines We train the open-source V-Express [17] model, SOTA diffusion-based architecture for talking-head synthesis. We evaluate its performance when trained under three distinct dataset conditions: 1) HDTF [26]: high-resolution talking-head dataset. 2) Hallo3 [7]: curated dataset with clean motion conditions. 3) TalkVid-Core (Ours): 160hour subset of our proposed TalkVid dataset. 4.1.2. Implementation Details For all conditions, we adhere strictly to the original threestage training protocol of V-Express (40k, 75k, and 50k steps) and its hyperparameters. We use the AdamW optimizer [11] with learning rate of 1e-6 and global batch sizes of 8, 4, and 2 for each stage, respectively. Input video frames are preprocessed by cropping facial regions and resizing to 512512. Training for each condition requires 3 days on 4 NVIDIA A100 GPUs. 4.1.3. Evaluation datasets Evaluation is conducted on three test sets. First, we use 100-clip subset of the HDTF test set. Second, we use 167-clip subset from the Hallo3 test set to evaluate performance on cleaner motion and larger pose variations. Third, we introduce TalkVid-Bench, our primary benchmark of 500 five-second clips designed for robust and fair evaluation. Drawn from the TalkVid corpus but held-out from training, the benchmark is stratified and balanced across four dimensions: language, ethnicity, gender, and age. Whereas HDTF and Hallo3 only support aggregate scores, TalkVid-Bench enables granular analysis of model performance across subgroups, making it the definitive tool for the cross-domain and fairness experiments in this paper. Subgroup distributions are detailed in Appendix C.4. 4.1.4. Evaluation metrics We employ set of metrics to evaluate performance: Visual Quality. We report Frechet Inception Distance (FID) [8] for per-frame realism and Frechet Video Distance (FVD) [16] for temporal coherence and video-level fidelity. Audio-Visual Synchronization. Following SyncNet [3], we measure audio-lip sync confidence (Sync-C) and the distance between audio-visual embeddings (Sync-D)."
        },
        {
            "title": "Ethnicity",
            "content": "Training Dataset FID FVD Sync-C Sync-D FID FVD Sync-C Sync-D HDTF Hallo3 TalkVid 31.385 29.721 28.686 205.990 184.465 178. 3.109 3.849 3.842 10.567 9.887 10.064 36.381 34.650 32.588 214.488 194.847 187.368 3.605 4.204 4.205 10.135 9.488 9."
        },
        {
            "title": "Age",
            "content": "Training Dataset FID FVD Sync-C Sync-D FID FVD Sync-C Sync-D HDTF Hallo3 TalkVid 38.950 35.260 34.347 225.611 208.815 199. 3.513 3.987 4.019 10.100 9.605 9.717 34.257 32.934 30.790 166.276 158.799 151.580 3.542 4.052 4.112 10.168 9.596 9. Table 5. Comparison with other baseline training datasets, including HDTF [26] and Hallo3 [7] on TalkVid-bench across four dimensions in general. 4.2. Quantitative Results HDTF test set 4.2.1. Fine-grained Results on TalkVid-Bench Training Dataset FID FVD Sync-C Sync-D The fine-grained evaluation on TalkVid-Bench, shown in Table 4, confirms that TalkVid produces models with superior generalization and reduced bias. HDTF Hallo3 TalkVid 19.963 188.657 24.484 183.172 21.772 175.122 3.523 3.615 3.707 10.254 9.966 9.970 Cross-lingual Generalization While baselines perform well on English, our model achieves the best visual quality (FID/FVD) across English, Chinese, and Polish, significantly outperforming on non-English languages. This result validates that TalkVids linguistic breadth directly remedies the brittleness of models trained on narrower data. Mitigating Ethnic Bias The Hallo3-trained model is competitive for White speakers but falters for African speakers, where our model is clearly superior. This performance delta, revealed by TalkVid-Bench, shows that TalkVids inclusive data fosters more equitable models. Robustness Across Gender and Age Our model achieves the most consistent high performance for both male and female subjects and shows marked improvements for challenging age groups, particularly for speakers aged 60+. This underscores the value of TalkVids comprehensive demographic coverage. 4.2.2. Overall Results on TalkVid-Bench Table 5 shows the overall performance for each dimension of TalkVid-Bench. Three trends stand out. Visual fidelity leads across the board TalkVid consistently records the lowest FID/FVD in all four dimensions, confirming that its higher-quality training data translates into universally sharper and more stable videos. Hallo3 test set Training Dataset FID FVD Sync-C Sync-D HDTF Hallo3 TalkVid 16.690 114.891 19.745 106.009 18.367 101.819 4.464 4.941 4. 9.901 9.404 9.530 Table 6. Comparison with other baseline training datasets on HDTF and Hallo3 test sets. Synchronisation remains competitive Hallo3 consistently achieves the lowest Sync-D, but the gaps are small. TalkVid matches or surpasses Hallo3 on Sync-C in three of the four dimensions and is only marginally lower on Language, showing that visual improvements do not come at meaningful cost to lip-sync quality. Balanced demographic performance From language and ethnicity to gender and age, TalkVid delivers the most uniform improvements, reinforcing the conclusion that diverse training set yields model that generalises well without introducing new biases. 4.2.3. Comparison on Standard Benchmarks We further assess generalization on the canonical HDTF and Hallo3 test sets  (Table 6)  . The TalkVid-trained model demonstrates superior cross-domain robustness, achieving the best temporal coherence (FVD) on both benchmarks In contrast, modand strong lip-sync (Sync-C) scores. els trained on HDTF and Hallo3 exhibit significant performance degradation when evaluated out-of-domain, high4.3. Qualitative Results 4.3.1. Diversity coverage and naturalness Figure 6 showcases the performance of our model on diverse identities from TalkVid-Bench. The model accurately preserves identity and background across speakers. Crucially, it synthesizes natural, non-verbal behaviors often absent in prior work: subtle head motion (row 2) and realistic eye blinks (row 4) are generated in sync with speech. This confirms that the model generalizes well beyond nearfrontal, static poses. 4.3.2. Comparison with baseline datasets The frame-by-frame comparison in Figure 7 reveals clear deficiencies in the baseline models. Both the HDTF and Hallo3-trained models produce static expressions with muted lip motion, failing to match the audio or replicate In contrast, our model natural behaviors like eye blinks. reproduces the ground-truths dynamic expression, including correctly timed eye blinks and larger, more articulate lip shapes. This qualitative evidence corroborates our quantitative findings, confirming that TalkVids rich motion diversity leads to more lifelike and accurate synthesis. 5. Ethical Considerations While generative models pose significant risks of misuse, we contend that an equally critical ethical failure is the status quo: creating biased technology from non-diverse data that systematically fails for underrepresented groups. Our work directly confronts this harm. TalkVid provides the demographically rich data to train fairer models, while TalkVid-Bench offers standardized framework to audit and mitigate such algorithmic bias. To ensure responsible dissemination, we will distribute the dataset as source URLs and timestamps to verified researchers under strict license. This protocol respects creator copyright and explicitly prohibits all malicious applications, including defamation and non-consensual content generation, thereby balancing research accessibility with accountability. 6. Conclusion This paper addressed the critical brittleness of SOTA talking head models, direct consequence of inadequate training data. We introduced TalkVid, large-scale, diverse, and technically pristine dataset curated through rigorous, human-validated pipeline. To enable fair assessment, we also presented TalkVid-Bench, stratified benchmark that uncovers biases invisible to standard metrics. Our experiments show that training on TalkVid produces more robust and equitable models. By releasing this ecosystem, we hope it will spur further research into auditing and mitigating bias in generative video models. 8 Figure 6. Qualitative examples from our TalkVid-trained model, evaluated on diverse samples from TalkVid-Bench spanning language, ethnicity, gender, and age. Figure 7. Qualitative comparison on an unseen clip from TalkVid-Bench. From top: V-Express fine-tuned on HDTF, Hallo3, TalkVid-Core (ours), and Ground-Truth (GT). lighting their tendency to overfit. Notably, while the HDTF model secures the best FID on its own test set, it does so at the cost of the worst FVD. Our model makes more effective trade-off, indicating that TalkVids diversity promotes more balanced and generalizable synthesis that avoids overfitting to domain-specific visual artifacts."
        },
        {
            "title": "Training on TalkVid yields a model",
            "content": "is robust across evaluation domains and metrics. The quantitative results, particularly the stratified analysis on TalkVid-Bench, prove that TalkVid provides superior foundation for developing talking-head models that are both high-fidelity and equitable. that"
        },
        {
            "title": "References",
            "content": "[1] Breakthrough Apps. Pyscenedetect: Automated video scene detection. https://github.com/Breakthrough/ PySceneDetect, 2021. Version 0.6+. 3 [2] Houwei Cao, David Cooper, Michael Keutmann, Ruben Gur, Ani Nenkova, and Ragini Verma. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing, 5(4):377390, 2014. 2, 3 [3] Joon Son Chung and Andrew Zisserman. Out of time: auIn Workshop on Multi-view tomated lip sync in the wild. Lip-reading, ACCV, 2016. 6 [4] Joon Son Chung and Andrew Zisserman. Lip reading in the In Computer VisionACCV 2016: 13th Asian Conwild. ference on Computer Vision, Taipei, Taiwan, November 2024, 2016, Revised Selected Papers, Part II 13, pages 87103. Springer, 2017. 2, 3 [5] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. arXiv preprint Voxceleb2: Deep speaker recognition. arXiv:1806.05622, 2018. 1, 2, 3 [6] Martin Cooke, Jon Barker, Stuart Cunningham, and Xu Shao. An audio-visual corpus for speech perception and automatic speech recognition. The Journal of the Acoustical Society of America, 120(5):24212424, 2006. 2, 3 [7] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks. arXiv preprint arXiv:2412.00733, 2024. 1, 2, 3, 6, 7 [8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibIn Advances in Neural Information Processing Sysrium. tems, pages 66296640, 2017. 6 [9] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudoarXiv preprint arXiv:2410.11831, labelling real videos. 2024. 4, [10] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 2 [11] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 6 [12] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. Voxceleb: Large-scale speaker verification in the wild. Computer Speech & Language, 60:101027, 2020. 2 [13] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484492, 2020. 2 [14] Kim Sung-Bin, Lee Chae-Yeon, Gihun Son, Oh Hyun-Bin, Janghoon Ju, Suekyeong Nam, and Tae-Hyun Oh. Multitalk: Enhancing 3d talking head generation across languages with multilingual video dataset. arXiv:2406.14272, 2024. 2, 3 arXiv preprint [15] Shuai Tan, Bin Ji, Mengxiao Bi, and Ye Pan. Edtalk: Efficient disentanglement for emotional talking head synthesis. In European Conference on Computer Vision, pages 398 416. Springer, 2024. 3 [16] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2019. 6 [17] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. 3, 6 [18] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: large-scale audio-visual dataset for emotional In European conference on comtalking-face generation. puter vision, pages 700717. Springer, 2020. 2, 3 [19] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. 3 [20] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. 4 [21] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 3 [22] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2024. 1, [23] Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, and Yujiu Yang. Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan. In European conference on computer vision, pages 85101. Springer, 2022. 2 [24] yt-dlp contributors. yt-dlp. GitHub repository, 2025. Accessed: 2025-08-02, Version: 2025.01.01. 3 [25] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86528661, 2023. 2 [26] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with In Proceedings of high-resolution audio-visual dataset. the IEEE/CVF conference on computer vision and pattern recognition, pages 36613670, 2021. 1, 2, 3, 6, 7 A. Video Collection Criteria To ensure consistency and quality in the collected data, we establish standardized guidelines across three key dimensions: recording environment, speaker behavior, and identity diversity. Recording Environment Requirements. All videos are recorded indoors to avoid uncontrollable outdoor factors such as variable lighting or wind noise. Lighting conditions must be stable and evenly distributed, with strong side or backlighting strictly avoided. The background should be visually clean and preferably monochromatic to minimize distractions. Recording devices are required to support minimum resolution of 1080p and frame rate of at least 25 fps, and should be mounted stablypreferably on tripodto avoid camera shake or motion blur. Audio must be captured clearly, and free from background noise or interference (e.g., music, environmental sounds). The recorded audio should contain only single speaker, with no overlapping dialogue or ambient speech. Speaker Movement Requirements. During recording, speakers are instructed to face the camera directly, maintaining natural and relaxed facial expression. Excessive head motion, exaggerated gestures, or sudden movements are discouraged to preserve alignment quality. The speakers face should remain unobstructed throughout the recordingmasks, microphones, or large reflective glasses are not allowed, while standard eyeglasses are acceptable. The entire facial region, including the chin and forehead, must stay within the camera frame, with the face occupying approximately 3040% of the frame area. Appropriate headroom and consistent shooting distance (recommended 0.51 meter) should be maintained. Speech content must be delivered in clear, accent-neutral English at moderate pace, with well-pronounced articulation. Speaker Diversity Requirements. To promote fairness and generalizability in downstream applications, the dataset is curated to include diverse range of speakers. We ensure balanced representation across genders, age groups, ethnic backgrounds, and speaking styles. Collected samples vary in facial expressions, emotional tone, speaking speed, and prosody. Each video features unique spoken content between 10 to 30 seconds in duration, avoiding repetition or overly scripted delivery. For quality assurance, collectors verify facial visibility and audiovisual synchronization and record basic demographic metadata (e.g., gender, age group, race). tories on uniform grid within the central 256 256 crop. trajectory is deemed valid over the entire clip if: 1) its confidence remains 0.5 in every frame, 2) the per-frame displacement never exceeds 20 pixels (approx. 8% of the image diagonal), 3)it stays inside the frame boundaries. The stability ratio is then defined as: ρ = #valid trajectories [0, 1]. (1) high ρ indicates consistent tracking but not necessarily large motion; conversely, low ρ usually corresponds to motion blur or tracking failure. We retain clips whose stability ratio satisfies 0.85 ρ 0.999. (2) Lower bound (ρ 0.85). Clips with ρ < 0.85 exhibit > 10% trajectory loss, typically caused by severe motion blur or compression artifacts that would corrupt subsequent 3D landmark estimation. Upper bound (ρ 0.999). Clips with ρ > 0.999 contain almost no detectable micro-motion (empirical mean displacement < 0.3 px), resulting in unnaturally static frozen faces that degrade the perceived liveliness of generated talking heads. All videos are resampled to 25 fps and down-scaled so that the shorter side is 512 px. We extract 16-frame clips with sliding window stride of 8 frames. B.2. Head-detail Filtering Details To evaluate the facial quality in video clips, we define scoring system based on five dimensions. All scores range from 0 to 100, with the exception of the Resolution Score which ranges from 0 to 3000. Higher values indicate better quality for all scores. clip is retained only if all scores meet their respective thresholds. Movement Score. This metric measures the temporal stability of facial keypoints. We compute the average displacement of keypoints between adjacent frames, normalized by the smaller dimension of the image (height or width). The score is defined as 100 100 avg movement. Lower displacement leads to higher scores. Threshold: average 80, minimum 60. B. Video Filtering Details B.1. Motion Filtering Details We adopt the point-tracking stability ratio ρ provided by CoTracker [9] as proxy for natural facial motion. For each 16-frame clip, CoTracker initializes = 256 trajecOrientation Score. This score reflects how frontal the face is across frames. For each frame, we compute pitch, yaw, and roll scores as θ/180 100, and derive the fipitch2 + yaw2 + roll2. higher score nal score as 100 indicates better alignment with the camera. Threshold: average 70, minimum 30. (cid:112) Completeness Score. We assess whether key facial regions are fully visible within the frame. The score combines three regions: eyes (weight 0.3), nose (0.4), and mouth (0.3). For each region, the presence of all keypoints within image bounds contributes 1; otherwise, 0. The weighted sum gives the final score. Occlusions are tolerated as long as keypoints are within the visible area. Threshold: average = 100, minimum = 100. Resolution Score. This score quantifies how large the is calculated as 30 face appears in the frame. (face area/image area) 100. Larger face regions yield higher scores, which may exceed 100. Threshold: average 50, minimum 40. It (cid:112) Rotation Score. This metric evaluates the smoothness of head motion. It is defined as 100 avg rotation amplitude, where the amplitude is computed by the 3D orientapitch2 + yaw2 + roll2 between adjation change: cent frames. Smaller variations indicate higher stability. Threshold: average 70, minimum 60. All metrics must satisfy their corresponding thresholds for clip to pass the quality screening. C. Video Annotation This appendix provides supplementary details for the human verification study discussed in the main paper. C.1. Detailed Performance Metrics detailed breakdown of the evaluation results for each of the seven automated filtering stages is summarized in Table 7. For each category, the table presents the InterAnnotator Agreement (IAA) rate, which measures annotator consistency, alongside key classification metrics (Accuracy, Precision, Recall, and F1-score) for our automated filter when benchmarked against the Golden Standard. The consistently high scores reported in the table underscore the robustness and reliability of our data curation pipeline. C.2. Annotator Background and Training The human evaluation was conducted by team of five annotators with strong technical and scientific backgrounds. The team comprised two Ph.D. students in Computer Science, one Ph.D. student in Applied Mathematics, one undergraduate student in Computer Science, and one undergraduate student in Statistics. All evaluators possess substantial experience with rigorous scientific research methodologies. To ensure consistency and objectivity, strict evaluation protocol was enforced. For each of the seven filtering categories, pair of these annotators was assigned to evaluate the 100 sample clips independently. Prior to the main annotation task, all participants underwent dedicated calibration session. During this session, they were provided with detailed written guidelines and illustrative examples for each quality criterion (e.g., defining acceptable vs. unacceptable head movement). The goal of this phase was to establish shared and consistent understanding of the task requirements, which directly contributed to the high interannotator agreement rates observed in our results. Crucially, the entire evaluation process was blinded; the annotators had no knowledge of the decisions made by the automated filters, ensuring that their judgments remained completely unbiased. C.3. Annotation Guidelines for Human Evaluators To ensure all annotators applied the same criteria, we provided them with the following detailed instructions for each filtering category. These guidelines were also used during the calibration session to resolve ambiguities. C.3.1. General Rule critical edge case applied to all categories: clips where human face or head could not be reliably detected were generally to be labeled as negative, even if the clip otherwise met the quality criterion (e.g., high video quality or smooth motion). C.3.2. Filter-Specific Instructions Cotracker Filter The primary criterion is the spatiotemporal stability of the head. Clips were labeled as negative if they contained sudden, jerky movements resulting from large translations or rapid 3D rotations of the head. Dover Filter This filter assesses overall visual and technical quality. Clips were labeled as negative if they suffered from low resolution, significant compression artifacts, poor lighting, or motion blur. Head Detail Filters This is composite evaluation. clip must generally meet all five of the following subcriteria to be labeled as positive: Movement Stability: The head must remain relatively stationary. Clips with large, abrupt translations or occlusions were rejected. Frontal Orientation: The subjects face must be predominantly front-facing. Clips containing significant head turns, downward gazes, or profiles were rejected. Head Completeness: The entire facial region must be clearly visible and unobstructed. Clips showing only partial features or where the face was occluded were rejected. Facial Resolution: The face must occupy salient portion of the frame (heuristically, > 20%). Clips"
        },
        {
            "title": "Recall",
            "content": "F1-score"
        },
        {
            "title": "Cotracker\nDover\nHead Completeness\nHead Movement\nHead Orientation\nHead Resolution\nHead Rotation",
            "content": "86.87% 94.00% 90.00% 83.00% 90.00% 86.00% 95.00% 87.21% 96.81% 96.67% 96.39% 94.44% 97.67% 96.84% 87.64% 86.67% 88.64% 95.83% 96.84% 97.87% 93.48% 100.00% 96.63% 92.50% 100.00% 96.10% 97.78% 94.62% 91.67% 94.59% 100.00% 97.22% 96.84% 97.87% 95.83%"
        },
        {
            "title": "Average",
            "content": "89.3% 95.1% 95.5% 96.3% 95.3% Table 7. Detailed Human Evaluation Results for Each Filtering Stage. We report Inter-Annotator Agreement (IAA) and the classification performance (Accuracy, Precision, Recall, F1-score) of our automated filter against the Golden Standard. D. Computational Efficiency We quantify efficiency using the real-time factor (RTF), defined as the ratio between the input-video duration and the wall-clock processing time. An RTF greater than 1 indicates faster-than-real-time operation. Our pipeline comprises following sequential stages: 1. Rough segmentation + subtitle filtering (CPU-only, 96 cores) achieves an average RTF of 18.14. 2. Motion filtering (CoTracker) (96-core CPU with 8 NVIDIA A800 GPUs) reaches an average RTF of 64.21. 3. Quality filtering (DOVER) (96-core CPU with 8 NVIDIA A800 GPUs) reaches an average RTF of 87.36. 4. Head filtering (96-core CPU with 8 NVIDIA A800 GPUs) reaches an average RTF of 72.47. where the face was too small (e.g., < 10% of the frame area) were rejected. Rotational Stillness: This criterion is exceptionally strict. The head must maintain fixed orientation with minimal rotation. Even single, noticeable head turn, nod, or shake within the clip was sufficient for it to be labeled as negative. Annotators were instructed to watch the entire clip before making final judgment. C.3.3. Visual Examples To further clarify the annotation criteria, this section provides visual examples. We first show an ideal positive case that passes all filters. Subsequently, we present seven negative cases, each illustrating failure for specific filtering criterion. Annotators were instructed to label the following types of clips as negative. C.4. Benchmark Design TalkVid-Bench comprises 500 carefully sampled and stratified video clips along four critical demographic and language dimensions: age, gender, ethnicity, and language. This stratified design enables granular analysis of model performance across diverse subgroups, mitigating biases hidden in traditional aggregate evaluations. Each dimension is divided into balanced categories: Age: 019, 1930, 3145, 4660, 60+, with total of 105 samples. Gender: Male, Female, with total of 100 samples. Ethnicity: Black, White, Asian, with total of 100 samples. Language: English, Chinese, Arabic, Polish, German, Russian, French, Korean, Portuguese, Japanese, Thai, Spanish, Italian, Hindi, and Other languages, with total of 195 samples. 12 Figure 8. Distribution of TalkVid-bench across the language dimension (15 languages, 195 samples). The left panel shows the number of samples per language, while the right panel shows the distribution of these samples by language. Language abbreviations: ar (Arabic), pl (Polish), de (German), ru (Russian), fr (French), ko (Korean), pt (Portuguese), other (other languages), ja (Japanese), th (Thai), es (Spanish), it (Italian), hi (Hindi), en (English), zh (Chinese). Figure 9. Distribution of TalkVid-bench across three demographic dimensions. These statistics illustrate the diversity of TalkVid-bench in terms of participant demographics, providing comprehensive benchmark for evaluating models under varied demographic conditions."
        },
        {
            "title": "Quality\nLevel",
            "content": "Poor"
        },
        {
            "title": "Good",
            "content": "Table 8. Examples of Sample Quality Based on Filter Cotracker"
        },
        {
            "title": "Quality\nLevel",
            "content": "Poor"
        },
        {
            "title": "Sample Example",
            "content": "Table 9. Examples of Sample Quality Based on Filter Dover"
        },
        {
            "title": "Quality\nLevel",
            "content": "Poor"
        },
        {
            "title": "Good",
            "content": "Table 10. Examples of Sample Quality Based on Filter Head Movement"
        },
        {
            "title": "Quality\nLevel",
            "content": "Poor"
        },
        {
            "title": "Good",
            "content": "Table 11. Examples of Sample Quality Based on Filter Head Orientation"
        },
        {
            "title": "Quality\nLevel",
            "content": "Poor"
        },
        {
            "title": "Good",
            "content": "Table 12. Examples of Sample Quality Based on Filter Head Completeness"
        },
        {
            "title": "Quality\nLevel",
            "content": "Poor"
        },
        {
            "title": "Good",
            "content": "Table 13. Examples of Sample Quality Based on Filter Head Resolution"
        },
        {
            "title": "Quality\nLevel",
            "content": "Poor"
        },
        {
            "title": "Good",
            "content": "Table 14. Examples of Sample Quality Based on Filter Head Rotation 20 E. Annotation Visualization This section provides visual overview of the generated annotations for the TalkVid-Core dataset. E.1. Annotation Details We prompt the model to perform detailed analysis focusing on anatomical movement patterns and behavioral dynamics, returning its findings as structured JSON object. This process yields rich set of 180,860 structured annotations. To quantify their richness, the descriptions have an average length of 84.6 tokens (σ = 28.4) and draw from diverse vocabulary of over 18,000 unique tokens, underscoring the detail and variety of the generated text. E.2. Annotation Length Distribution Figure 10. Distribution of caption lengths by token count across all 180k annotations in TalkVid-Core. The mean length is 84.6 tokens, with standard deviation of 28.4, indicating consistent descriptive depth. E.3. Qualitative Examples F. Details of Annotation Generation Process This section provides the full prompt used for annotation generation and representative example of the structured JSON output. F.1. Full Generation Prompt The following prompt was provided to large multimodal model to instruct it on the analysis task and the required output format. Analyze the video with precise focus on anatomical movement patterns and behavioral dynamics. Prioritize detailed descriptions of body part trajectories and their temporal relationships. IMPORTANT OUTPUT REQUIREMENTS: 1. Provide your analysis in clean, minified JSON format without any line breaks or escape characters 2. Do not include any explanatory text before or after the JSON 21 3. Ensure the JSON is valid and properly formatted 4. Use single-line format for the entire output 5. Do not include any comments or additional formatting Expected JSON structure: { \"scene_context\": { \"background\": [\"Setting description\", \" Environmental elements\", \"Lighting details\", \"Camera angle and position\"], \"subject\": [\"Physical appearance\", \"Clothing description\", \"Notable features\", \" Demographic attributes\"] }, \"movement_analysis\": { \"head_movements\": [\"Sequential head rotations with angle measurements (in degrees)\", \"Tilt progression with directional markers\", \" Orientation changes relative to initial position\", \"Facial expressions and changes\"], \"hand_actions\": { \"left\": [\"Trajectory patterns with spatial coordinates\", \"Gesture type classification\", \"Interaction duration and objects\", \"Force/ intensity of movements\"], \"right\": [\"Trajectory patterns with spatial coordinates\", \"Gesture type classification\", \"Interaction duration and objects\", \"Force/ intensity of movements\"] }, \"torso_movements\": [\"Rotation angles and direction\", \"Flexion/extension patterns\", \" Lateral movements\", \"Weight distribution shifts\", \"Postural stability assessment\"], \"body_posture\": [\"Postural transition timeline\", \"Spinal alignment changes\", \" Weight distribution shifts\", \"Balance and stability patterns\"], \"confidence_scores\": { \"head_tracking\": 0.0, \"hand_tracking\": 0.0, \"torso_tracking\": 0.0, \"posture_analysis\": 0.0, \"overall_confidence\": 0.0 } }, \"interactions\": { \"environmental\": [\"Object interactions\", \" Space utilization\", \"Environmental adaptations\"], \"social\": [\"Interpersonal distances\", \"Social gestures\", \"Interactive behaviors\"], \"object_handling\": [\"Object types\", \" Manipulation patterns\", \"Duration of interactions\"] }, \"audio_behavioral_analysis\": { \"speech\": [\"Voice characteristics\", \"Speech patterns\", \"Verbal expressions\"], \"non_verbal_sounds\": [\"Types of sounds\", \" Timing\", \"Context\"] }, \"movement_metrics\": { \"speed\": {\"unit\": \"meters/second\", \" measurements\": []}, \"acceleration\": {\"unit\": \"meters/secondˆ2\", \" measurements\": []}, \"angular_velocity\": {\"unit\": \"degrees/second \", \"measurements\": []} }, \"anomaly_detection\": { \"unusual_movements\": [], \"irregular_patterns\": [], \"potential_concerns\": [] 0:05 seconds.\", \"Performs similar pointing gesture as the left hand, rising in the sagittal plane and extending forward in the frontal plane, but with more pronounced movement.\", \"The right hand is more active, continuing to gesture with varied movements in all three planes (sagittal, frontal, and transverse) throughout the video to emphasize }, \"description_summary\": \"Comprehensive summary describing the complete action sequence\" } points.\" ] }, \"body_posture\": [ RESPONSE FORMAT: - Output must be single JSON object without any additional text - Do not include any markdown formatting - Do not include any explanatory text - Provide direct values without any placeholder text - The entire response should be valid JSON that can be parsed directly F.2. Example of Generated JSON Output This is representative example of the structured JSON data produced by the model for single video clip. { \"scene_context\": { \"background\": [ \"Indoor setting\", \"Plain, light-colored wall\", \"Even, bright lighting\" ], \"subject\": [ \"Middle-aged woman, light skin tone\", \"Dark pink/maroon long-sleeved top\", \"Shoulder-length blonde hair\", \"Presumably Caucasian\" } \"The subject maintains stationary, standing posture throughout the video.\", \"Upright posture with minimal spinal curvature changes.\", \"Shoulders remain relaxed and relatively still.\", \"Weight distribution appears evenly balanced.\" ] }, \"description_summary\": \"The video shows woman standing against plain background, delivering short explanation. dark pink top. From medium close-up shot framing her from the chest up, she speaks directly to the camera. Throughout the video, She wears her head remains relatively still with subtle nodding for emphasis. Her hands, initially at her sides, rise into the frame to perform illustrative pointing gestures. The right hand exhibits more dynamic movement , emphasizing key points with variety of gestures in multiple planes. Her overall body posture remains static, standing upright and facing forward, with weight evenly distributed. Her torso and shoulders show minimal movement.\" ] }, \"movement_analysis\": { \"head_movements\": [ \"Slight head nodding throughout the video, particularly emphasizing certain words. Vertical rotation within approximately 5 degrees.\", \"Minimal head tilt, maintaining neutral position relative to the camera.\" ], \"hand_actions\": { \"left\": [ \"Initially rests at her side, out of frame.\", \"At approximately 0:06 seconds, her left hand rises into the frame and executes pointing gesture, moving upward in the sagittal plane and extending forward in the frontal plane.\", \"The hand remains raised, with slight movements emphasizing speech.\" ], \"right\": [ \"Initially at her side and enters the frame slightly before the left hand, around 22 The image collage shows man sitting in studio setting... He maintains consistent seated posture... no significant body movement is observed. The subject, young woman, sits at table... her primary movements are hand gestures The subject, an adult female, is seated... She engages with the camera using combination of hand gestures... She holds small red box and gestures with it... Figure 11. Qualitative examples from the TalkVid-Core dataset. Each example displays sampled frames from video clip, paired with its corresponding descriptive caption generated by Gemini 1.5 Pro. For brevity, captions are truncated. Figure 12. Positive Example (Passes All Filters). This visual guideline shows an ideal case that meets all quality criteria. The subject is consistently front-facing, stable, well-lit, and occupies significant portion of the frame. This type of clip should be labeled as positive. 23 (a) Fails the Cotracker Filter due to it remains still. (b) Fails the Head Movement Filter due to abrupt scene changes. (c) Fails the Head Rotation Filter due to distinct head turns. (d) Fails the Head Orientation Filter as the face is not front-facing. (e) Fails the Head Completeness Filter due to facial occlusion. (f) Fails the Head Resolution Filter as the face is too small. (g) Fails the Dover Filter due to motion blur and low quality. Figure 13. Visual guidelines illustrating negative examples for all seven filtering stages. Each sub-figure demonstrates specific failure criterion, instructing evaluators to label such clips as negative."
        }
    ],
    "affiliations": [
        "Sun Yat-sen University",
        "The Chinese University of Hong Kong, Shenzhen",
        "The Hong Kong University of Science and Technology"
    ]
}