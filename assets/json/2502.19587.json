{
    "paper_title": "NeoBERT: A Next-Generation BERT",
    "authors": [
        "Lola Le Breton",
        "Quentin Fournier",
        "Mariam El Mezouar",
        "Sarath Chandar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 8 5 9 1 . 2 0 5 2 : r NeoBERT: Next-Generation BERT Lola Le Breton1,2,3 Quentin Fournier2 Mariam El Mezouar Sarath Chandar1,2,3,5 1Chandar Research Lab 4Royal Military College of Canada 2Mila Quebec AI Institute 3Polytechnique Montr√©al 5Canada CIFAR AI Chair"
        },
        {
            "title": "Abstract",
            "content": "Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERTlarge, RoBERTalarge, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption1,2."
        },
        {
            "title": "1 Introduction",
            "content": "Auto-regressive language models have made tremendous progress since the introduction of GPT (Radford et al., 2018), and modern large language models (LLMs) such as LLaMA 3 (Dubey et al., 2024), Mistral (Jiang et al., 2023), OLMo (Groeneveld et al., 2024), and DeepSeek-r1 (DeepSeek-AI et al., 2025) now exhibit remarkable reasoning and in-context learning capabilities. These improvements result from scaling both the models and the web-scraped text datasets they are trained on, as well as from innovations in architecture and optimization techniques. However, while decoders have continuously evolved, encoders have not seen the same level of progress. Worse, their knowledge has become increasingly outdated despite remaining critical for wide range of downstream NLP tasks that depend on their embeddings, notably for retrieval-augmented generation (Ram et al., 2023) and toxicity classification (Hartvigsen et al., 2022). Despite being five years old, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) remain widely used, with more than 110 million combined downloads from Hugging Face as of the writing of this paper. Similar to decoders, which undergo multi-stage processes of pre-training, instruction-tuning, and alignment, encoders also require successive training phases to perform well on downstream tasks. First, encoders go through self-supervised pre-training on large corpora of text with the masked language modeling objective. By predicting masked or replaced tokens, this stage enables models to learn the structural patterns of language and the semantics of words. However, the pre-training task is disconnected from downstream applications, and models require additional training to achieve strong performance in clustering or retrieval. Thus, second fine-tuning phase is often achieved through multiple stages of semi-supervised contrastive learning, where models learn to differentiate between positive and negative sentence pairs, refining their embeddings in the latent space. 1https://huggingface.co/chandar-lab/NeoBERT 2https://github.com/chandar-lab/NeoBERT 1 Recently, substantial progress has been made in improving the fine-tuning stage of pre-trained encoders, with models like GTE (Li et al., 2023b), jina-embeddings (Sturua et al., 2024), SFR-embeddings (Meng et al., 2024), and CDE (Morris & Rush, 2024) significantly outperforming previous encoders on the MTEB leaderboard, recent and challenging benchmark spanning 7 tasks and 56 datasets. However, all these approaches focus on proposing complex fine-tuning methods and do not address the inherent limitations of their pre-trained backbone models. As result, there is dire need for new generation of BERT-like pre-trained models that incorporate up-to-date knowledge and leverage both architectural and training innovations, forming stronger backbones for these more advanced fine-tuning procedures. In response, we introduce NeoBERT, next-generation BERT that integrates the latest advancements in architecture, data, and pre-training strategies. The improvements are rigorously validated on GLUE by fully pre-training 10 different models that successively incorporate the modifications. This validation ensures that the improvements benefit encoder architectures and highlights how some design choices drastically affect the models abilities. Additionally, we design and experimentally validate two-stage training procedure to increase NeoBERTs maximum context window from 1, 024 to 4, 096. To ensure fair evaluation of NeoBERT against existing baselines and to isolate the impact of fine-tuning procedures, we propose model-agnostic and systematic fine-tuning strategy with straightforward contrastive learning. All models are fine-tuned using this standardized approach and subsequently evaluated on the MTEB benchmark. On MTEB, NeoBERT consistently outperforms all competing pre-trained models while being 100M parameters smaller than the typical large-sized encoders. With context window of 4,096 tokens, it processes sequences 8 longer than RoBERTa (Liu et al., 2019) and two times longer than NomicBERT (Nussbaum et al., 2024). It is also the fastest encoder of its kind, significantly outperforming ModernBERT base and large in terms of inference speed. Despite its compact 250M parameter size, NeoBERT is trained for over 2T tokens, prioritizing training over scale to maximize accessibility for both academic researchers and industry users without requiring large-scale compute resources. This makes NeoBERT the most extensively trained model among modern encoders, ensuring robust generalization and superior downstream performance. Furthermore, NeoBERT maintains the same hidden size as base models, allowing for seamless plug-and-play adoption without modifications to existing architectures. As the only fully open-source model of its kind, we release the code, data, training scripts, and model checkpoints, reinforcing our commitment to reproducible research."
        },
        {
            "title": "2 Related work",
            "content": "In 2019, Devlin et al. (2019) introduced BERT, novel approach to embedding text using bi-directional Transformers pre-trained without supervision on large corpora. Shortly after, Liu et al. (2019) improved over BERTs pre-training by removing the next-sentence prediction objective and drastically increasing the amount of data, leading to RoBERTa. Since then, the primary focus of the community has shifted towards optimizing the fine-tuning phase of these models through contrastive learning, where the model is trained to maximize the similarity between positive text pairs while pushing them apart from negative samples. Among the earliest contrastive learning approaches designed for encoders, SimCSE (Gao et al., 2022) demonstrated that sentence pairs could be easily generated by feeding the same input to the model twice and applying dropout to introduce noise. However, this simple approach was soon outperformed by models like GTE (Li et al., 2023b), which introduced more advanced contrastive learning techniques. GTE employed weakly supervised stage that takes advantage of the vast number of successive sentence pairs available in traditional unlabeled datasets, followed by semi-supervised stage incorporating labeled sentence pairs from high-quality datasets such as NLI (Bowman et al., 2015) and FEVER (Thorne et al., 2018). Recently, fine-grained strategies have emerged to better adapt models to both task and context. For instance, Jinaembeddings (Sturua et al., 2024) introduced task-specific Low-Rank Adaptation (LoRA) adapters. As of January 2025, CDE (Morris & Rush, 2024) ranks at the top of the MTEB leaderboard for models under 250M parameters thanks to two key innovations: grouping samples with related contexts into the same batch and providing contextual embeddings for the entire corpus in response to individual queries. 2 However, pre-training has not seen the same level of effort, and thus progress, most likely due to its prohibitively high computational cost. RoBERTa, for instance, required total of 1, 024 V100 days for its pretraining. As result, GTE, Jina-embeddings, and CDE all rely on pre-trained BERT, XLM-RoBERTa (Conneau et al., 2020), and NomicBERT (Nussbaum et al., 2024) to initialize their respective models. The latter, NomicBERT, represents recent effort to refine BERTs architecture and pre-training. NomicBERT incorporates architectural improvements such as SwiGLU and RoPE, utilizes FlashAttention, and extends the context length to 2, 048 tokens. Despite these innovations, NomicBERT still relied on sub-optimal choices, as discussed in section 3. In parallel with the development of NeoBERT, Warner et al. (2024) released ModernBERT with the goal of further refining the pre-training of NomicBERT. Although we share some of the modifications, we make distinct design choices and conduct thorough ablations that ultimately lead to greater performance on MTEB."
        },
        {
            "title": "3 NeoBERT",
            "content": "The following section describes NeoBERTs improvements over BERT and RoBERTa, as well as the recent NomicBERT and ModernBERT models. Since GTE and CDE use BERT and NomicBERT as their pretrained backbone, they inherit their respective characteristics. Table 1 summarizes the modifications. Table 1: Comparison of Model Architectures, Training Data, and Pre-Training Configurations. BERT base large RoBERTa large base NomicBERT base ModernBERT base large NeoBERT medium Layers Hidden Size Attention Heads Parameters Activation Function Positional Encoding Normalization Data Sources Dataset Size Dataset Year Tokenizer Level Vocabulary Size Sequence Length Objective Masking Rate Masking Scheme Optimizer Scheduler Batch Size Tokens Seen 12 768 12 24 1, 024 16 120M 350M 125M 12 768 24 1, 024 16 355M GeLU Positional Embeddings Post-LayerNorm BooksCorpus Wikipedia 13GB 2019 Character 30K 512 MLM + NSP 15% 80/10/10 Adam - 131k tokens 131B BooksCorpus OpenWebText Stories / CC-News 160GB 2019 Byte 50K 512 MLM 15% 80/10/10 Adam - 131k 131B 12 768 12 137M SwiGLU RoPE Post-LayerNorm 22 768 12 149M 28 1, 024 16 395M GeGLU RoPE Pre-LayerNorm 28 768 12 250M SwiGLU RoPE Pre-RMSNorm BooksCorpus Wikipedia Undisclosed RefinedWeb 13GB 2023 Character 30K 2, 048 MLM 30% - AdamW - 8M - - - Character 50K 1, 024 8, 192 MLM 30% - StableAdamW WSD 448k to 5M 2T Alternate Attention Unpadding FlashAttention 2.8TB 2023 Character 30K 1, 024 4, 096 MLM 20% 100 AdamW CosineDecay 2M 2.1T DeepSpeed FlashAttention Training DDP DDP DeepSpeed FlashAttention"
        },
        {
            "title": "3.1 Architecture",
            "content": "The Transformer architecture has been refined over the years and has now largely stabilized, with models like LLaMA 3 being essentially the same as the original LLaMA. NeoBERT integrates the latest modifications that have, for the most part, become standard. Depth-to-Width The concept of depth efficiency has long been recognized in neural network architectures. In the case of Transformers, stacking self-attention layers is so effective that it can quickly saturate the networks capacity. Recognizing this, Levine et al. (2020) provided theoretical and empirical evidence for an optimal depth-to-width ratio in Transformers. Their findings suggested that most language models were operating in depth-inefficiency regime, where allocating more parameters to width rather than depth would have improved performance. In contrast, small language models like BERT, RoBERTa, and NomicBERT are instead in width-inefficiency regime. To maximize NeoBERTs parameter efficiency while ensuring it remains seamless plug-and-play replacement, we retain the original BERTbase width of 768 and instead increase its depth to achieve this optimal ratio. Positional Information Transformers inherently lack the ability to distinguish token positions. Early models like BERT and RoBERTa addressed this by adding absolute positional embeddings to the token embeddings before the first Transformer block. However, this approach struggles to generalize to longer sequences and requires the positional information to be propagated across layers. To overcome these limitations, Su et al. (2023) proposed Rotary Position Embeddings (RoPE), which integrate relative positional information directly into the self-attention mechanism. RoPE has quickly become the default in modern Transformers due to its significant improvements in performance and extrapolation capabilities. NeoBERT, like all newer encoders, integrates RoPE. Nevertheless, degradation still occurs with sequences significantly longer than those seen during training. As solution, Peng et al. (2023) introduced Yet Another RoPE Extension (YaRN), which allows to efficiently fine-tune models on longer contexts. NeoBERT is readily compatible with YaRN, making it well-suited for tasks requiring extended context. Layer Normalization Consistent with standard practices in modern Transformer architectures, we move the normalization layer inside the residual connections of each feed-forward and attention block, technique known as Pre-Layer Normalization (Pre-LN). Pre-LN improves stability, allows for larger learning rates, and accelerates model convergence (Xiong et al., 2020). While all newer encoder models adopt Pre-LN, they typically continue to use the classical LayerNorm rather than Root Mean Square Layer Normalization In NeoBERT, we substitute the classical LayerNorm with RMSNorm (Zhang & Sennrich, (RMSNorm). 2019), which achieves comparable training stability while being slightly less computationally intensive, as it requires one fewer statistic. Activations BERT and RoBERTa utilize the standard Gaussian Error Linear Unit (GELU) activation function. However, Shazeer (2020) demonstrated the benefits of the Gated Linear Unit in Transformer architectures. These activation functions have since been adopted in several language models, including the LLaMA family. Following previous works, NeoBERT incorporates the SwiGLU activation function, and to keep because it introduces third weight matrix, we scale the number of hidden units by factor of 2 3 the number of parameters constant. 3.2 Data Data has emerged as one of the most critical aspects of pre-training, and datasets with increasing quantity and diversity are frequently released. NeoBERT takes advantage of the latest datasets that have proven to be effective. Dataset BERT and NomicBERT were pre-trained on two carefully curated and high-quality datasets: Wikipedia and BookCorpus (Zhu et al., 2015). As Baevski et al. (2019) demonstrated that increasing data size can improve downstream performance, Liu et al. (2019) pre-trained RoBERTa on 10 times more data from BookCorpus, CC-News, OpenWebText, and Stories. However, RoBERTas pre-training corpus has 4 become small in comparison to modern web-scraped datasets built by filtering and deduplicating Common Crawl dumps. Following the same trend, we pre-trained NeoBERT on RefinedWeb (Penedo et al., 2023), massive dataset containing 600B tokens, nearly 18 times larger than RoBERTas. Although RefinedWeb does not have strict high-quality constraints, we believe that exposing the model to such large and diverse dataset will improve its real-world utility. Sequence Length BERT and RoBERTa were pre-trained on sequences up to 512 tokens, which limits their downstream utility, especially without RoPE and YaRN. NomicBERT increased the maximum length to 2, 048 and employed Dynamic NTK interpolation at inference to scale to 8192. To further broaden NeoBERTs utility, we seek to increase the context length. However, due to the computational cost associated with pre-training, we adopt two-stage pre-training procedure similar to LLMs like LLaMA 3. In the first stage, we train the model for 1M steps (2T tokens) using sequences truncated to maximum length of 1, 024 tokens, referring to this version as NeoBERT1024. In the second stage, we extend the training for an additional 50k steps (100B tokens), increasing the maximum sequence length to 4, 096 tokens. We refer to this final model as NeoBERT4096. To ensure the model encounters longer sequences during this stage, we create two additional sub-datasets, Refinedweb1024+ and Refinedweb2048+, containing only sequence lengths greater than 1, 024 and 2, 048 tokens, respectively, alongside the original Refinedweb dataset. Each batch is sampled from Refinedweb, Refinedweb1024+ and Refinedweb2048+ with probabilities 20%, 40%, and 40%. Since longer sequences tend to represent more complex or academic content, this strategy helps mitigate the distribution shift typically observed when filtering for longer sequences. We explore the benefits of this two-stage training strategy in subsection 5.3. 3.3 Pre-Training Encoder pre-training has received less attention than the data and architecture. However, many improvements made to decoders also apply to encoders. NeoBERT combines encoder-specific modifications with widely accepted decoder improvements. Objective In light of RoBERTas findings that dropping the next-sentence prediction task does not harm performance, both NomicBERT and NeoBERT were only pre-trained on masked language modeling. Moreover, Wettig et al. (2023) challenged the assumption that the 15% masking rate of BERT and RoBERTa is universally optimal. Instead, their findings suggest that the optimal masking rate is actually 20% for base models and 40% for large models. Intuitively, model learns best when the difficulty of its training tasks aligns with its capabilities. Based on their insight, we increase the masking rate to 20%, while NomicBERT exceeds it by opting for 30%. Optimization Following standard practice, we use the AdamW optimizer (Loshchilov & Hutter, 2019) with the same hyperparameters as LLaMA 2: Œ≤1 = 0.9, Œ≤2 = 0.95, and œµ = 108. In preliminary experiments, we also considered SOAP (Vyas et al., 2025), recent extension of the Shampoo optimizer, but it failed to outperform Adam and AdamW and has been omitted from the list of ablations. We employ linear warmup for 2, 000 steps to reach peak learning rate of 6 104, followed by cosine decay to 10% of the peak learning rate over 90% of the training steps. Once fully decayed, the learning rate remains constant for the last 100k steps at sequence length of 1, 024 and 50k steps at sequence length of 4, 096. We use weight decay of 0.1 and apply gradient clipping with maximum norm of 1.0. Scale Recent generative models like the LLaMA family (Touvron et al., 2023; Dubey et al., 2024) have demonstrated that language models benefit from being trained on significantly more tokens than was previously standard. Recently, LLaMA-3.2 1B was successfully trained on up to 9T tokens without showing signs of saturation. Moreover, encoders are less sample-efficient than decoders since they only make predictions for masked tokens. Therefore, it is reasonable to believe that encoders of similar sizes can be trained on an equal or even greater number of tokens without saturating. For NeoBERTs pre-training, we use batch sizes of 2M tokens over 1M steps in the first stage and 50k steps in the second, resulting in theoretical total of 2.1T tokens. Note that because sequences are padded to the maximum length, this represents theoretical number of tokens. In terms of tokens, this is comparable to RoBERTa and represents 2x increase over 5 Table 2: Modifications between successive ablations. The initial 0 baseline corresponds to model similar to BERT, while 9 corresponds to NeoBERT."
        },
        {
            "title": "Before",
            "content": "M 1 Embedding Activation Pre-LN 2 Dataset 3 Tokenizer Optimizer Scheduler 4 Positional GELU LayerNorm"
        },
        {
            "title": "After",
            "content": "RoPE SwiGLU RMSNorm Wiki + Book RefinedWeb Google WordPiece LLaMA BPE Adam Linear AdamW Cosine 15% (80 / 10 / 10) 20% (100) Sequence packing False 5 Masking Scheme 6 7 Model Size 8 Depth - Width Batch size Context length 120M 16 - 1056 131k 512 True 250M 28 - 2M 4, 096 NomicBERT. In terms of training steps, this amounts to 2x increase over RoBERTa and 10x increase over NomicBERT. Efficiency We improve efficiency by parallelizing the model across devices using DeepSpeed (Aminabadi et al., 2022) with the ZeRO (Rajbhandari et al., 2020) optimizer, reducing memory usage by eliminating data duplication across GPUs and increasing the batch size. We further optimize the GPU utilization by employing fused operators from the xFormers library to reduce overhead, selecting all dimensions to be multiples of 64 to align with GPU architectures, and removing biases to simplify computations without sacrificing performance. To address the quadratic demands of attention, we integrate FlashAttention (Dao, 2023), which computes exact attention without storing the full matrices."
        },
        {
            "title": "4 Effect of Design Choices",
            "content": "We conduct series of ablations in controlled settings to evaluate our improvements to the original BERT architecture. We fully train each model for 1M steps, controlling for the seed and dataloader states to ensure successive models are trained with identical setups. These resource-intensive ablations were crucial to confirm our design choices, as they are based on the literature of pre-training decoder models. The baseline model, referred to as 0, is similar to BERTbase but includes pre-layer normalization. Following RoBERTa, 0 also drops the next sentence prediction objective. We introduce modifications iteratively, resulting in total of ten different models, as detailed in Table 2. To mitigate computational costs, the ablations are evaluated on the GLUE benchmark with limited hyperparameter grid search of batch sizes {16, 32} and learning rates {1e 5, 2e 5, 3e 5}. For the final model 10, we extend the grid search, as detailed in Appendix C. Results are in Figure 1. Key Performance-Enhancing Modifications As expected, the two modifications that had the greatest impact on the average GLUE score were related to scale. In 2, replacing Wikitext and BookCorpus with the significantly larger and more diverse RefinedWeb dataset improved the score by +3.6%, while increasing the model size from 120M to 250M in 7 led to +2.9% relative improvement. Note that to assess the impact of the depth-to-width ratio, we first scale the number of parameters in 7 to 250M while maintaining 6 Figure 1: GLUE ablation scores on the development set. Modifications in grey are not included in the subsequent models. Increasing data size and diversity leads to the highest relative improvement (M 2, +3.6%), followed by the model size (M 7, +2.9%). Packing the sequences and using the LLaMA 2 tokenizer cause the largest relative drops (M 6, 2.9%, 3, 2.1%). similar ratio to BERTbase, resulting in 16 layers of dimension 1056. In 8, the ratio is then adjusted to 28 layers of dimension 768. Modifications That Were Discarded In 3, replacing the Google WordPiece tokenizer with LLaMA BPE results in 2.1% performance decrease. We hypothesize that while the heterogeneous and multilingual nature of the LLaMA 2 vocabulary enhances broader applicability in decoders, it trades off performance for more compact encoder representations. In 6, we un-pad the sequences by concatenating samples of the same batch. While this removes unnecessary computation on padding tokens, packing sequences without accounting for cross-sequence attention results in relative performance drop of 2.8%. Although this modification was discarded from our subsequent ablations, we incorporate methods of un-padding with accurate cross-attention in our released codebase, following Kundu et al. (2024). Modifications Retained Despite Performance Trade-offs Unexpectedly, using AdamW (Loshchilov & Hutter, 2019) and cosine decay (Loshchilov & Hutter, 2017) in 4 decreases performance by 0.5%. As AdamW introduces additional regularization with weight decay, we expect that it will become beneficial when extending training by mitigating overfitting. Similarly, increasing the masking ratio from 15% to 20% in 5 decreases performance by 0.7%. We hypothesize that increasing the task difficulty initially hinders downstream task performance but is likely to become advantageous when training larger models on more tokens. Consequently, we retain both modifications despite being unable to verify these hypotheses at scale due to the computational costs."
        },
        {
            "title": "5 Experiments",
            "content": "Selecting appropriate metrics and benchmarks is crucial for properly assessing the downstream performance and utility of language models. Following both early and recent studies, we include the GLUE and MTEB benchmarks in our evaluations."
        },
        {
            "title": "5.1 GLUE",
            "content": "The GLUE benchmark (Wang et al., 2019) is cornerstone of language modeling evaluations and has played significant role in the field. Although GLUE is now 6 years old and the community has long recognized its limitations, we report the GLUE score due to its widespread adoption and to facilitate the comparison of NeoBERT with existing encoders. Following standard practices, we fine-tune NeoBERT on the development set of GLUE with classical hyperparameter search and introduce transfer learning between related tasks. Complete details of the fine-tuning and best hyperparameters are presented in Appendix C. NeoBERT achieves score of 89.0% comparable to previous large models while being 100M to 150M parameters smaller. We present the results in Table 3. Table 3: GLUE scores on the development set. Baseline scores were retrieved as follows: BERT from Table 1 of Devlin et al. (2019), RoBERTa from Table 8 of Liu et al. (2019), DeBERTa from Table 3 of He et al. (2023), NomicBERT from Table 2 of Nussbaum et al. (2024), GTE from Table 13 of Zhang et al. (2024), and ModernBERT from Table 5 of Warner et al. (2024). Size Model MNLI QNLI QQP RTE SST MRPC CoLA STS Avg. Base ( 150M ) Medium 250M Large ( 340M ) BERT RoBERTa GTE-en-8192 NomicBERT2048 ModernBERT NeoBERT1024 NeoBERT4096 BERT RoBERTa DeBERTaV3 GTE-en-8192 ModernBERT 84.0 87.6 86.7 86.0 89.1 88.9 89. 86.3 90.2 91.9 89.2 90.8 90.5 92.8 91.9 92.0 93.9 93.9 93.7 92.7 94.7 96.0 93.9 95.2 71.2 91.9 88.8 92.0 92.1 90.7 90. 72.1 92.2 93.0 89.2 92.7 66.4 78.7 84.8 82.0 87.4 91.0 91.3 70.1 86.6 92.7 88.1 92.1 93.5 94.8 93.3 93.0 96.0 95.8 95. 94.9 96.4 96.9 95.1 97.1 88.9 90.2 92.1 88.0 92.2 93.4 93.4 89.3 90.9 91.9 93.5 91.7 52.1 63.6 57.0 50.0 65.1 64.8 66. 60.5 68.0 75.3 60.4 71.4 85.8 91.2 90.2 90.0 91.8 92.1 91.8 86.5 92.4 93.0 91.4 92.8 79.6 86.4 85.6 84.0 88.5 88.8 89. 82.1 88.9 91.4 87.6 90.5 5.2 MTEB Beyond the GLUE benchmark, we consider the more recent and challenging MTEB benchmark (Muennighoff et al., 2023), which has quickly become standard for evaluating embedding models, with wide coverage of 7 tasks and 56 datasets in its English subset. MTEB tasks rely on the cosine similarity of embeddings pooled across tokens in sentence. The most common and straightforward pooling strategy is computing the average of each tokens final hidden representation. However, because out-of-the-box encoders are trained with the masked language modeling objective, they provide no guarantee that mean embeddings will produce meaningful representations without further fine-tuning. As result, models require expensive fine-tuning strategies crafted for MTEB to achieve strong scores. For instance, GTE (Li et al., 2023b) with multi-stage contrastive learning, Jina-embeddings (Sturua et al., 2024) with task-specific Low-Rank Adaptation (LoRA) adapters, and CDE (Morris & Rush, 2024), with batch clustering and contextual corpus embeddings all pushed the limits of the leaderboard in their respective categories. These coupled stages make it challenging to disentangle the respective impacts of pre-training and fine-tuning on the final models performance. To isolate and fairly evaluate the improvements introduced during pretraining, we implemented an affordable, model-agnostic fine-tuning strategy based on classical contrastive learning. This fine-tuning approach was designed in accordance with established methods in the literature. Its controlled settings ensured that all models were fine-tuned and evaluated under identical conditions. Method Given dataset of positive pairs = {qi, d+ œÑ , and set of negative documents Nq for each query q, the contrastive loss is defined as: , similarity metric s, temperature parameter }n i=1 = log es(q,d+)/œÑ es(q,d+)/œÑ + dNq es(q,d)/œÑ Negative documents can be either generic samples of the same format or tailored hard negatives, which exhibit high degree of similarity to the contrasted sample in their original representation. We constructed dataset of positive query-document pairs with optional hard negatives based on open-source datasets previously employed in the literature (Li et al., 2023a) for total of nine million documents. In addition to the optional hard negatives, we also leverage in-batch, task-homogeneous negatives. In line with prior research (Li et al., 2023a), we employ task-specific instructions and temperature-scaled sampling of the datasets. Complete details about the data, training, and evaluation can be found in Appendix D. Results We found that training for more than 2,000 steps resulted in minimal performance gains. Table 4 presents the complete MTEB-English evaluation of all fine-tuned models. Although NeoBERT is 100M parameters smaller than all large baselines, it is the best model overall with +4.5% relative increase over the second best model, demonstrating the benefits of its architecture, data, and pre-training improvements. Table 4: MTEB scores on the English subset after 2,000 steps of fine-tuning with contrastive learning. Size Model Class. Clust. PairClass. Rerank. Retriev. STS Summ. Avg. Base BERT RoBERTa DeBERTaV3 NomicBERT2048 ModernBERT Medium NeoBERT4096 Large BERT RoBERTa DeBERTaV3 ModernBERT 60.6 58.7 45.9 55.0 58.9 61.6 59.8 57.1 45.9 62.4 37.0 36.7 15.2 35.3 38.1 40. 39.3 39.3 16.4 38.7 71.5 71.2 44.3 69.0 63.8 76.2 70.9 72.5 45.0 65.5 48.9 49.8 39.0 48.8 48.5 51. 49.7 51.3 40.8 50.1 28.3 26.9 3.5 30.5 21.0 69.9 71.8 42.2 70.1 66.2 31.6 74.8 29.6 30.0 4.0 23. 71.4 71.7 40.1 68.3 31.1 29.1 25.0 30.1 30.1 30.7 31.2 31.1 29.9 27.8 48.1 47.7 26.9 47.1 45.0 51. 49.1 48.9 27.1 46.9 5.3 Sequence Length Following previous literature, NeoBERT underwent an additional 50k pre-training steps, during which it was exposed to extended sequences of up to 4,096 tokens. To assess the impact of this additional training, we randomly sampled 2,467 long sequences from the English subset of Wikipedia. For each sequence, we independently masked each token at position and computed its cross-entropy loss, li. The pseudo-perplexity of the entire sentence is then defined as = exp (cid:0) 1 Pn Although NeoBERT1024 was trained exclusively on sequences of up to 1, 024 tokens, it generalizes effectively to context lengths approaching 3,000 tokens. This demonstrates the robustness of RoPE embeddings to outof-distribution inputs. Moreover, after an additional 50k training steps with sequences up to 4, 096 tokens, NeoBERT4096 successfully models longer sequences. This approach provides compute-efficient strategy for extending the models maximum context window beyond its original length. (cid:1). We present the results in Figure 2. i=1 li 5.4 Efficiency To assess model efficiency, we construct synthetic dataset consisting of maximum-length sequences of sizes {512, 1024, 2048, 4096, 8192}. For each sequence length, we scale the batch size from 1 to 512 samples or until encountering out-of-memory errors. Inference is performed for 100 steps on single A100 GPU, and we report the highest throughput achieved for each model and sequence length. Figure 3 presents the results. 9 Figure 2: Pseudo-Perplexity in function of the sequence length for NeoBERT1024 (left) and NeoBERT4096 (right). This validates the effectiveness of the final pre-training stage on NeoBERTs ability to model long sequences. Due to their low parameter count and relatively simple architecture, BERT and RoBERTa are the most efficient for sequences up to 512 tokens. However, their use of positional embeddings prevents them from further scaling the context window. For extended sequences, NeoBERT significantly outperforms ModernBERTbase, despite having 100M more parameters, achieving 46.7% speedup on sequences of 4, 096 tokens. Figure 3: Model throughput (tokens per second) as function of sequence length ( is better). Above 1, 024 in sequence length, NeoBERT surpasses ModernBERTbase despite having 100M more parameters."
        },
        {
            "title": "6 Discussion",
            "content": "Encoders are compact yet powerful tools for language understanding and representation tasks. They require fewer parameters and significantly lower training costs compared to their decoder counterparts, making them strong alternatives for applications that do not involve text generation. Traditionally, the representational capacity of these models has been assessed through downstream tasks such as classification, in particular through the GLUE benchmark. 10 While GLUE has played pivotal role in guiding model adoption, it includes only nine sequence classification datasets, four of which are entailment tasks. Moreover, its small dataset sizes and occasionally ambiguous labeling make it prone to overfitting, with models long surpassing human performance on the benchmark. Although DeBERTa-v3 achieves state-of-the-art performance on GLUE by significant margin, our finetuning experiments reveal its comparatively poor performance on the more recent MTEB benchmark. MTEB encompasses broader range of datasets and tasks, but attaining high performance on its leaderboard necessitates carefully crafted fine-tuning strategies with costly training requirements. As more complex fine-tuning strategies emerge, it becomes unclear what the source of score improvements is. Moreover, these strategies are not easily reproducible or accessible, limiting the possibility of fair comparison between pre-trained backbones. This underscores the limitations of current evaluation paradigms and highlights the need for more affordable and standardized frameworks. We advocate for future research to focus on the development of standardized fine-tuning protocols and the exploration of new zero-shot evaluation methodologies to ensure more comprehensive and unbiased assessment of encoder-only models."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced NeoBERT, state-of-the-art encoder pre-trained from scratch with the latest advancements in language modeling, architecture, and data selection. To ensure rigorous validation, we systematically evaluated every design choice by fully training and benchmarking ten distinct models in controlled settings. On GLUE, NeoBERT outperforms BERTlarge and NomicBERT and is comparable with RoBERTalarge despite being 100M parameters smaller and supporting sequences eight times longer. To further validate its effectiveness, we conducted comprehensive evaluation on MTEB, carefully isolating the effects of pretraining and fine-tuning to provide fair comparison against the best open-source embedding models. Under identical fine-tuning conditions, NeoBERT consistently outperforms all baselines. With its unparalleled efficiency, optimal depth-to-width, and plug-and-play compatibility, NeoBERT represents the next generation of encoder models. To foster transparency and collaboration, we release all code, data, model checkpoints, and training scripts, making NeoBERT the only fully open-source model of its kind. Broader Impact Statement Despite its improvements, NeoBERT inherits the biases and limitations of its pre-training data. Moreover, the greatest jump in performance stems from the pre-training dataset, and as newer, larger, and more diverse datasets become available, retraining will likely be needed to further improve its performance. Nonetheless, NeoBERT stands today as an affordable state-of-the-art pre-trained encoder with great potential for downstream applications."
        },
        {
            "title": "Acknowledgements",
            "content": "Sarath Chandar is supported by the Canada CIFAR AI Chairs program, the Canada Research Chair in Lifelong Machine Learning, and the NSERC Discovery Grant. Quentin Fournier is supported by the Lambda research grant program. The authors acknowledge the computational resources provided by Mila and the Royal Military College of Canada."
        },
        {
            "title": "References",
            "content": "Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 115. IEEE, 2022. Simran Arora, Patrick Lewis, Angela Fan, et al. Reasoning over public and private data in retrieval-based systems, 2022. URL https://arxiv.org/abs/2203.11027. 11 Alexei Baevski, Sergey Edunov, Yinhan Liu, et al. Cloze-driven pretraining of self-attention networks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 53605369, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1539. URL https://aclanthology.org/D19-1539. Payal Bajaj, Daniel Campos, Nick Craswell, et al. Ms marco: human generated machine reading comprehension dataset, 2018. URL https://arxiv.org/abs/1611.09268. Samuel R. Bowman, Gabor Angeli, Christopher Potts, et al. large annotated corpus for learning natural language inference, 2015. URL https://arxiv.org/abs/1508.05326. Daniel Cer, Mona Diab, Eneko Agirre, et al. Semeval-2017 task 1: Semantic textual similarity multilingual In Proceedings of the 11th International Workshop on Semantic and crosslingual focused evaluation. Evaluation (SemEval-2017). Association for Computational Linguistics, 2017. doi: 10.18653/v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, et al. Unsupervised cross-lingual representation learning at scale, 2020. URL https://arxiv.org/abs/1911.02116. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. URL https://arxiv.org/abs/2307.08691. DeepSeek-AI, Daya Guo, Dejian Yang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv.org/abs/1810.04805. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Katja Filippova and Yasemin Altun. Overcoming the lack of parallel data in sentence compression, October 2013. URL https://aclanthology.org/D13-1155. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings, 2022. URL https://arxiv.org/abs/2104.08821. Dirk Groeneveld, Iz Beltagy, Pete Walsh, et al. Olmo: Accelerating the science of language models, 2024. URL https://arxiv.org/abs/2402.00838. Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, et al. Amazonqa: review-based question answering task, 2019. URL https://arxiv.org/abs/1908.04364. Moonsu Han, Minki Kang, Hyunwoo Jung, et al. Episodic memory reader: Learning what to remember for question answering from streaming data, 2019. URL https://arxiv.org/abs/1903.06164. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, et al. ToxiGen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 33093326, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.234. URL https://aclanthology.org/2022.acl-long.234. Pengcheng He, Jianfeng Gao, and Weizhu Chen. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing, March 2023. URL http://arxiv.org/ abs/2111.09543. arXiv:2111.09543 [cs]. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, et al. Mistral 7b, 2023. URL https://arxiv. org/abs/2310.06825. 12 Qiao Jin, Bhuwan Dhingra, Zhengping Liu, et al. Pubmedqa: dataset for biomedical research question answering, 2019. URL https://arxiv.org/abs/1909.06146. Daniel Khashabi, Amos Ng, Tushar Khot, et al. Gooaq: Open question answering with diverse answer types, 2021. URL https://arxiv.org/abs/2104.08727. Mahnaz Koupaee and William Yang Wang. Wikihow: large scale text summarization dataset, 2018. URL https://arxiv.org/abs/1810.09305. Achintya Kundu, Rhui Dih Lee, Laura Wynter, et al. Enhancing training efficiency using packing with flash attention, 2024. URL https://arxiv.org/abs/2407.09105. Yoav Levine, Noam Wies, Or Sharir, et al. Limits to Depth Efficiencies of Self-Attention. In Advances in Neural Information Processing Systems, volume 33, pp. 2264022651. Curran Associates, Inc., 2020. URL https://papers.nips.cc/paper/2020/hash/ff4dfdf5904e920ce52b48c1cef97829-Abstract.html. Xianming Li and Jing Li. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871, 2023. Zehan Li, Xin Zhang, Yanzhao Zhang, et al. Towards general text embeddings with multi-stage contrastive learning, 2023a. URL https://arxiv.org/abs/2308.03281. Zehan Li, Xin Zhang, Yanzhao Zhang, et al. Towards General Text Embeddings with Multi-stage Contrastive Learning, August 2023b. URL http://arxiv.org/abs/2308.03281. arXiv:2308.03281 [cs]. Yinhan Liu, Myle Ott, Naman Goyal, et al. Roberta: robustly optimized bert pretraining approach, 2019. URL https://arxiv.org/abs/1907.11692. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017. URL https://arxiv.org/abs/1608.03983. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/ abs/1711.05101. Rui Meng, Ye Liu, Shafiq Rayhan Joty, et al. Sfr-embedding-2: Advanced text embedding with multi-stage training, 2024. URL https://huggingface.co/Salesforce/SFR-Embedding-2%5FR. John X. Morris and Alexander M. Rush. Contextual document embeddings, 2024. URL https://arxiv. org/abs/2410.02525. Niklas Muennighoff, Nouamane Tazi, Lo√Øc Magne, et al. MTEB: Massive Text Embedding Benchmark, March 2023. URL http://arxiv.org/abs/2210.07316. arXiv:2210.07316 [cs]. Zach Nussbaum, John X. Morris, Brandon Duderstadt, et al. Nomic embed: Training reproducible long context text embedder, 2024. URL https://arxiv.org/abs/2402.01613. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa : large-scale multisubject multi-choice dataset for medical domain question answering, 2022. URL https://arxiv.org/ abs/2203.14371. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, et al. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023. URL https://arxiv.org/abs/2306. 01116. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, et al. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309.00071. Alec Radford, Karthik Narasimhan, Tim Salimans, et al. Improving language understanding by generative pre-training, 2018. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, et al. Zero: Memory optimizations toward training trillion parameter models, 2020. URL https://arxiv.org/abs/1910.02054. Ori Ram, Yoav Levine, Itay Dalmedigos, et al. In-context retrieval-augmented language models, 2023. URL https://arxiv.org/abs/2302.00083. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, et al. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URL https://arxiv.org/abs/2409.10173. Jianlin Su, Yu Lu, Shengfeng Pan, et al. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104.09864. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, et al. Fever: large-scale dataset for fact extraction and verification, 2018. URL https://arxiv.org/abs/1803.05355. Hugo Touvron, Louis Martin, Kevin Stone, et al. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Nikhil Vyas, Depen Morwani, Rosie Zhao, et al. Soap: Improving and stabilizing shampoo using adam, 2025. URL https://arxiv.org/abs/2409.11321. Alex Wang, Amanpreet Singh, Julian Michael, et al. Glue: multi-task benchmark and analysis platform for natural language understanding, 2019. URL https://arxiv.org/abs/1804.07461. Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, et al. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference, 2024. URL https://arxiv.org/abs/2412.13663. Alexander Wettig, Tianyu Gao, Zexuan Zhong, et al. Should You Mask 15% in Masked Language Modeling?, February 2023. URL http://arxiv.org/abs/2202.08005. arXiv:2202.08005 [cs]. Adina Williams, Nikita Nangia, and Samuel R. Bowman. broad-coverage challenge corpus for sentence understanding through inference, 2018. URL https://arxiv.org/abs/1704.05426. Ruibin Xiong, Yunchang Yang, Di He, et al. On layer normalization in the transformer architecture, 2020. URL https://arxiv.org/abs/2002.04745. Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019. URL https://arxiv.org/ abs/1910.07467. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification, 2016. URL https://arxiv.org/abs/1509.01626. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, et al. mGTE: Generalized long-context text representation and reranking models for multilingual text retrieval. In Franck Dernoncourt, Daniel Preo≈£iuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 13931412, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.103. URL https: //aclanthology.org/2024.emnlp-industry.103/. Yukun Zhu, Ryan Kiros, Richard Zemel, et al. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015. URL https://arxiv.org/abs/1506.06724."
        },
        {
            "title": "A Training details",
            "content": "NeoBERT was trained on 8 H100 for 1,050,000 steps, for total of 6,000 GPU hours. In the first stage of training, we used local batch size of 32, 8 gradient accumulation steps, and maximum sequence length of 1, 024, for total batch size of 2M tokens. In the second stage of training, we keep the theoretical batch size constant and increase the maximum sequence length to 4, 096."
        },
        {
            "title": "B Ablations",
            "content": "Our first model, 0 is modeled after BERTbase in terms of architecture. The only two differences are the absence of the next-sentence-prediction objective, as well as Pre-Layer Normalization. Each successive model, up until 8 is identical to the previous one on every point except for the change reported in Table 2."
        },
        {
            "title": "C GLUE",
            "content": "We perform classical parameter search with learning rates in {5e 6, 6e 6, 1e 5, 2e 5, 3e 5}, batch sizes in {4, 8, 16, 32} and weight decay in {1e 2, 1e 5}. In addition, we start training from the best MNLI checkpoint for RTE, STS, MRPC, and QNLI. We fine-tune on the training splits of every glue dataset for 10 epochs, with evaluation on the validation splits every steps, being defined as min(500, len(dataloader) // 10) with early stopping after 15 evaluation cycles if scores have not improved. Following BERT, we exclude WNLI from our evaluation3. For tasks with two scores and for MNLI matched and mismatched, we report the average between the two metrics. Model Task Batch Size Learning Rate Weight Decay NeoBERT1024 NeoBERT4096 CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B 4 16 8 8 32 8 16 8 16 2 8 32 32 32 32 6e-6 6e-6 2e-5 5e-6 1e-5 6e-6 1e-5 1e-5 8e-6 5e-6 1e-5 5e-6 8e-6 5e-6 8e-6 2e-5 1e-5 1e-2 1e-5 1e-5 1e-2 1e-5 1e-5 1e-2 1e-5 1e-5 1e-5 1e-5 1e-5 1e-5 1e-2 1e-5 Table 5: Optimal hyperparameters for GLUE tasks. The grid search was conducted over batch sizes {2, 4, 8, 16, 32}, learning rates {5e6, 6e6, 8e6, 1e5, 2e5, 3e5}, and weight decay values {1e2, 1e5}. 3See 12 in https://gluebenchmark.com/faq"
        },
        {
            "title": "D MTEB",
            "content": "D.1 Evaluation of pre-trained models As demonstrated in Figure 4, evaluating out-of-the-box pre-trained models on MTEB is inconclusive. In that setting, BERTbase outperforms both BERTlarge and RoBERTalarge, highlighting the importance of fine-tuning to ensure representative evaluation on the MTEB benchmark. Figure 4: Zero-shot evaluation of BERT and RoBERTa on the English subset of MTEB. D.2 Contrastive learning Following the existing literature, we designed simple fine-tuning strategy entirely agnostic to the models evaluated. We used cosine similarity and œÑ = 0.07 as temperature parameter in the contrastive learning with loss. Additionally, we sampled datasets with multinomial distribution based on their sizes (nj)m Œ± = 0.5: j= œÄ = nŒ± j=1 nŒ± Pm We trained on the following fully-open datasets: AG-News (Zhang et al., 2016), All-NLI (Bowman et al., 2015; Williams et al., 2018), AmazonQA (Gupta et al., 2019), ConcurrentQA (Arora et al., 2022), GitHub Issues (Li & Li, 2023), GooAQ (Khashabi et al., 2021), MedMCQA (Pal et al., 2022), NPR4, PudMedQA (Jin et al., 2019), SentenceCompression (Filippova & Altun, 2013) StackExchange5, TriviaQA (Han et al., 2019), Wikihow (Koupaee & Wang, 2018), Yahoo! Answers (Zhang et al., 2016) as well as the available training splits of MTEB datasets (StackOverFlowDupQuestion, Fever (Thorne et al., 2018), MS MARCO (Bajaj et al., 2018), STS12, and STSBenchmark (Cer et al., 2017)). We fine-tune every model for 2,000 steps and evaluate on MTEB in float16. The complete results are presented in Table 4. D.3 Task instructions We provide the set of instructions used for fine-tuning in Table 6 and evaluation in Table 7 and Table 8. 4https://huggingface.co/datasets/sentence-transformers/npr 5https://huggingface.co/datasets/sentence-transformers/stackexchange-duplicates 16 Table 6: Instructions for fine-tuning on the different contrastive learning datasets. Dataset Instruction AGNEWS ALLNLI AMAZONQA CONCURRENTQA Given multi-hop question, retrieve documents that can help answer the Given news title, retrieve relevant articles. Given premise, retrieve hypothesis that is entailed by the premise. Given question, retrieve Amazon posts that answer the question. FEVER GITHUBISSUE GOOAQ MEDMCQA MEDMCQACLU ST MSMARCO NPR PAQ PUBMEDQA QQP question. Given claim, retrieve documents that support or refute the claim. Given question, retrieve questions from Github that are duplicates to the given question. Given question, retrieve relevant documents that best answer the question. Given medical question, retrieve relevant passages that answer the question. Identify the main category of medical exams based on their questions and answers. Given web search query, retrieve relevant passages that answer the query. Given news title, retrieve relevant articles. Given question, retrieve Wikipedia passages that answer the question. Given medical question, retrieve documents that best answer the question. Given question, retrieve questions from Quora forum that are semantically equivalent to the given question. SENTENCECOMP Given sentence, retrieve semantically equivalent summaries. STACKEXCHANGE Given Stack Exchange post, retrieve posts that are duplicates to the given post. STACKOVERFLOW Retrieve duplicate questions from StackOverflow forum. Retrieve semantically similar text. STS12 STSBENCHMARK Retrieve semantically similar text. TRIVIAQA WIKIHOW YAHOOCLU ST Given question, retrieve documents that answer the question. Given Wikihow post, retrieve titles that best summarize the post. Identify the main topic of Yahoo posts based on their titles and answers. Task name Table 7: Instructions for evaluation on the different MTEB tasks. Instruction DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval Given question, retrieve questions that are semantically equivalent to the Given query, retrieve relevant entity descriptions from DBPedia. Given claim, retrieve documents that support or refute the claim. Given financial question, retrieve user replies that best answer the question. Given multi-hop question, retrieve documents that can help answer the question. Given web search query, retrieve relevant passages that answer the query. Given question, retrieve relevant documents that best answer the question. Given question, retrieve Wikipedia passages that answer the question. SCIDOCS SciFact Touche2020 given question. Given scientific paper title, retrieve paper abstracts that are cited by the given paper. Given scientific claim, retrieve documents that support or refute the claim . Given question, retrieve detailed and persuasive arguments that answer the question. TRECCOVID Given query on COVID-19, retrieve documents that answer the query. SICK-R STS BIOSSES SummEval Retrieve semantically similar text. Retrieve semantically similar text. Retrieve semantically similar text from the biomedical field. Given news summary, retrieve other semantically similar summaries. Task name Instruction Table 8: Instructions for evaluation on the different MTEB tasks. AmazonCounterfactualClass. AmazonPolarityClass. AmazonReviewsClass. Banking77Class. EmotionClass. ImdbClass. MassiveIntentClass. MassiveScenarioClass. MTOPDomainClass. MTOPIntentClass. Given an Amazon customer review, classify it as either counterfactual or not-counterfactual. Given an Amazon review, classify its main sentiment into positive or negative. Given an Amazon review, classify it into its appropriate rating category. Given online banking query, find the corresponding intents. Given Twitter message, classify the emotion expressed into one of the six emotions: anger, fear, joy, love, sadness, and surprise. Given an IMDB movie review, classify its sentiment into positive or negative. Given user utterance, find the user intents. Given user utterance, find the user scenarios. Given user utterance, classify the domain in task-oriented conversation. Given user utterance, classify the intent in task-oriented conversation. Given comments, classify them as either toxic or not toxic. ToxicConversationsClass. TweetSentimentExtractionClass. Given tweet, classify its sentiment as either positive, negative, or <dataset>ClusteringP2P <dataset>ClusteringS2S <dataset>Clustering TwentyNewsgroupsClustering SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverflowDupQuestions ArguAna ClimateFEVER CQADupstackRetrieval neutral. Identify the main and secondary category of <dataset> papers based on their titles and abstracts. Identify the main and secondary category of <dataset> papers based on their titles. Identify the topic or theme of <dataset> posts based on their titles. Identify the topic or theme of the given news articles. Retrieve duplicate questions from Sprint forum. Given tweet, retrieve tweets that are semantically similar. Given tweet, retrieve tweets that are semantically similar. Retrieve duplicate questions from AskUbuntu forum. Given user browsing history, retrieve relevant news articles. Given title of scientific paper, retrieve the relevant papers. Retrieve duplicate questions from StackOverflow forum. Given claim, find documents that refute the claim. Document Given claim about climate change, retrieve documents that support or refute the claim. Given question, retrieve detailed question descriptions from Stackexchange that are duplicates to the given question. 18 Figure 5: Average MTEB scores of fine-tuned encoders grouped by task type. The average score is computed across the 56 tasks of MTEB-English. NeoBERT is the best model on five out of seven task types and the best model overall. See Table 4 for complete scores."
        },
        {
            "title": "E Efficiency",
            "content": "Table 9 presents the complete results of model efficiency evaluations. Table 9: Throughput (103 tokens / second) in function of the sequence length, with optimal batch size. Size Model Base BERTbase RoBERTabase ModernBERTbase Medium NeoBERT Large BERTlarge RoBERTalarge ModernBERTlarge 512 27.6 3.6 24.9 3.0 25.4 2.3 24.5 1.4 19.5 0.6 15.9 0.3 13.4 0.2 1024 2048 8192 - - 22.6 2.7 22.2 1.7 - - 11.4 1.1 - - 17.2 1.7 20.5 1.6 - - 9.2 0.7 - - 11.7 0.8 17.2 1.2 - - 6.5 0.3 - - 6.8 0.2 13.0 0.2 - - 3.8 0."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Chandar Research Lab",
        "Mila Quebec AI Institute",
        "Polytechnique Montr√©al",
        "Royal Military College of Canada"
    ]
}