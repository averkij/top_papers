{
    "paper_title": "Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting",
    "authors": [
        "Jason Stock",
        "Troy Arcomano",
        "Rao Kotamarthi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models offer a physically grounded framework for probabilistic weather forecasting, but their typical reliance on slow, iterative solvers during inference makes them impractical for subseasonal-to-seasonal (S2S) applications where long lead-times and domain-driven calibration are essential. To address this, we introduce Swift, a single-step consistency model that, for the first time, enables autoregressive finetuning of a probability flow model with a continuous ranked probability score (CRPS) objective. This eliminates the need for multi-model ensembling or parameter perturbations. Results show that Swift produces skillful 6-hourly forecasts that remain stable for up to 75 days, running $39\\times$ faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with the numerical-based, operational IFS ENS. This marks a step toward efficient and reliable ensemble forecasting from medium-range to seasonal-scales."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 1 3 6 5 2 . 9 0 5 2 : r Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting Jason Stock1, Troy Arcomano1,2, Rao Kotamarthi1 1Argonne National Laboratory 2Allen Institute for AI"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models offer physically grounded framework for probabilistic weather forecasting, but their typical reliance on slow, iterative solvers during inference makes them impractical for subseasonal-to-seasonal (S2S) applications where long lead-times and domain-driven calibration are essential. To address this, we introduce Swift, single-step consistency model that, for the first time, enables autoregressive finetuning of probability flow model with continuous ranked probability score (CRPS) objective. This eliminates the need for multi-model ensembling or parameter perturbations. Results show that Swift produces skillful 6-hourly forecasts that remain stable for up to 75 days, running 39 faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with the numerical-based, operational IFS ENS. This marks step toward efficient and reliable ensemble forecasting from medium-range to seasonal-scales. Code and model weights are available at (cid:135) https://github.com/stockeh/swift PF-ODE x0 ˆx noise + 75 days (δi = 24) 0 S s g t fθ([xT , ˆx], ) data t2m z500 q700 v10 t850 msl . . . (a) Conceptual diagram (b) 24h forecast error vs compute (c) Long forecast stability Figure 1: Overview of our approach. (a) methodological flow diagram of generating single-member rollout from noise and an updated conditional state; (b) 39 faster inference over diffusion baselines by using single function evaluation with comparable skill to the IFS ENS; and (c) output over subset of variables shown for single-member forecast at 24h intervals that is 75 days into the future."
        },
        {
            "title": "Introduction",
            "content": "Recent machine learning models have achieved skill comparable to, and in some cases exceeding, operational weather prediction systems [14]. These deterministic approaches operate at fraction of the computational cost compared to their numerical counterparts, creating an opportunity to allow for large ensemble systems [5]. Yet, models such as GraphCast and FourCastNet suffer from number of problems, including spectral biases [6, 7] and insensitivity to initial condition perturbations [8], Corresponding author: jason.stock@anl.gov limiting their ability to create reliable and calibrated ensembles. Advances in generative modeling, particularly with diffusion models [912], offer grounded formulation to address these problems. Diffusion-based weather models [1317] have shown promise in improving small-scale variability, with competitive performance to numerical ensemble systems. However, as they effectively solve differential equations, each forecast step typically makes 2040 neural function evaluations (NFE), making autoregressive forecasts computationally costly. This compounds with long-lead, seasonalscale forecasts. To improve efficiency, some works increase their time interval to 12or 24-hours [13, 14] at the expense of temporal fidelity. Another class of probabilistic weather models combines noise perturbations and multi-model approaches to generate calibrated ensembles [18, 19]. For instance, [18] relies on four independent models. While effective, this adds maintenance overhead, limits scalability, and has forecasts that potentially remain prone to instability and artifacts by day 15. To address these limitations, we introduce Swift, probability flow generative model finetuned autoregressively with CRPS objective. Our approach builds on single-step consistency model [20, 21], which preserves many of the favorable properties of diffusion (e.g., learns conditional probabilities, does not require complete state information, and can quantify uncertainty) while being significantly faster. This efficiency not only makes finetuning possible, but consequently yields accurate, well-calibrated ensembles as result. Swift remains effective on medium-range tasks, but extends further into seasonal-scales while preserving temporal fidelity. In what follows, we summarize the underlying algorithms used to train our models in Section 2 (including our own baselines). Our method is presented in Section 3, where we describe the data and modeling task, our multi-stage training process, network used to achieve stable consistency training, and specific implementation details. This is followed by results in Section 4, where we evaluate medium-range forecast skill, assess long-term stability, and present case studies of extreme events and seasonal trends. An overview of our contributions are shown in Figure 1."
        },
        {
            "title": "2 Background",
            "content": "We first detail the necessary background and diverging modifications of our diffusion (Section 2.1) and consistency model (Section 2.2), then detail the continuous ranked probability score (Section 2.3). 2.1 Diffusion Models and Flow Matching (cid:2)w(t)fθ(xt, t) x02 Given clean data x0 pd sampled from our data distribution, diffusion models [911] diffuse this to noise along xt = αtx0 + σtz for [0, ] with Gaussian (0, I). While there are methods to directly predict the noise, EDM [12] sets αt = 1 and σt = to opti- (cid:3) with neural network Fθ preconditioned by fθ(xt, t) = mize Ex0,z,t cskip(t)xt + cout(t)Fθ (cin(t)xt, cnoise(t)) and weighting function w(t). The coefficients ensure the objective has unit variance across noise levels. Generating clean data involves iteratively solving the probability flow ODE (PF-ODE) [11] dxt dt = [xt fθ(xt, t)] /t from pure noise xT (0, 2I). Flow matching [2225] (also known as rectified flows or stochastic interpolants) considers linear interpolant with αt = 1 and σt = t, where the objective is to learn the velocity vθ using Ex0,z,t dt = vθ(xt, t) from x1 (0, I) with classical numerical solver (e.g., Runge-Kutta methods) from = 1 to = 0. (cid:3). Sampling involves solving the PF-ODE dxt (cid:2)vθ(xt, t) (z x0) 2 2 TrigFlow [20] unifies EDM and flow matching under simpler, v-prediction parameterization that is theoretically supported for both diffusion and consistency models (Section 2.2). It satisfies the above unit variance principle and maps arbitrary noise schedules onto common trigonometric trajectory. Concretely, we considers spherical interpolant with αt = cos(t) and σt = sin(t) and noisy sample xt = cos(t)x0 + sin(t)z, where (0, σ2 dI) with the data standard deviation σd, and = arctan(eτ /σd) [0, π/2] with τ drawn from prior distribution, such as log-normal or log-uniform. We use the latter in training our TrigFlow models, given our data distribution. We define our diffusion model fθ(xt, t) = Fθ(xt/σd, t) with no further preconditioning, and have the corresponding PF-ODE dxt dt = σdFθ(xt/σd, t). The training objective under TrigFlow for diffusion estimates the velocity with ℓDiff (θ) = Ex0,z,t (cid:20)(cid:13) (cid:13) (cid:13)σdFθ (cid:16) xt σd (cid:17) , (cos(t)z sin(t)x0) (cid:13) 2 (cid:13) (cid:13) 2 (cid:21) . (1) 2 Table 1: ERA5 surface-level and atmospheric variables (501000 hPa) with input forcings. Type Variable Description surface surface surface surface atmos. atmos. atmos. atmos. atmos. static static clock t2m 10u 10v mslp t v lsm n/a 2 meter temperature 10 meter u-wind component 10 meter v-wind component Mean sea level pressure Geopotential Temperature u-wind component v-wind component Specific humidity Geopotential at surface Land-sea mask TOA incident solar radiation Figure 2: Our proposed network architecture. These formulations, at high level, are all continuous-time generative models that aim to achieve the same result; while they differ in their parameterizations, they are theoretically equivalent under correct assumptions. However, these methods all share significant expense in their sampling techniques, whether that be simpler first-order Euler solver or higher-order method as used in related approaches [12, 13]. We consider the latter, though without stochastic Langevin-like churn due to the trigonometric schedule complexity. 2.2 Continuous-time Consistency Models Unlike diffusion, consistency models [21, 26, 20] learn to predict clean data x0 at the origin of the PF-ODE for any noisy sample xt along the trajectory. Intuitively, this means we need not rely on iterative ODE solvers, and instead can directly estimate data in one (or few) steps. Importantly, this process satisfies the boundary condition fθ(x, t) when parameterized by fθ(xt, t) = cskip(t)xt + cout(t)Fθ (cin(t)xt, cnoise(t)) with cskip(0) = 1 and cout(0) = 0. Under TrigFlow we parameterize the consistency model as single step solution to our PF-ODE with simplified arithmetic coefficients as fθ(xt, t) = cos(t)xt sin(t)σdFθ , . (2) (cid:19) (cid:18) xt σd To learn in the discrete case, the training objective is defined at two adjacent steps with finite distance as Ext,t[w(t)d(fθ(xt, t), fθ (xtt, t)], with weighting function w(t), stop gradient θ, finite distance > 0, and loss measure d(, ), e.g., ℓ2 or Pseudo-Huber [26]. In the continuous case, [21] show when d(x, y) = y2 2 and we take the limit as 0, the gradient of the discrete objective is θExt,t[w(t)f ]. Observing that θE[F 2, using an θ arbitrary vector independent of θ, we arrive at the following (prior weighting removed) objective 2 [Fθ Fθ + y]2 θ y] = 1 dfθ dt ℓsCM (θ) = Ext,t (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) Fθ (cid:18) xt σd (cid:19) , Fθ (cid:18) xt σd (cid:19) , cos(t) dfθ (xt, t) dt (cid:35) . (cid:13) 2 (cid:13) (cid:13) (cid:13) This function depends on the time derivative, or tangent along our trajectory, of Equation (2) as dfθ (xt, t) dt (cid:18) = cos(t) σdFθ (cid:19) dxt dt (cid:18) sin(t) xt + σd (cid:19) , dFθ dt (3) (4) where is tangent warmup that linearly increases (over the first 3 million training images) to stabilize training and the PF-ODE is approximated by an unbiased estimate dxt dt = cos(t)z sin(t)x0. We compute dFθ dt by means of forward-mode automatic differentiation via the Jacobian-vector product in PyTorch and the stopgrad θ by simply detaching the gradient flow. Additionally, following [20], we normalize the tangent by the factor (y + c)1 with = 0.1, but scale to remain invariant to the spatial dimensions. Note that pretrained diffusion model defined by Equation (1) could be distilled into the same or smaller sized model by replacing the PF-ODE with dxt , t). However, this is out of scope of the current work and we instead consolidate end-to-end training with the consistency model. dt = σdFpretrain( xt σd 3 Figure 3: Loss weights used during training. (left) pressure weights applied to atmospheric variables; (middle) surface level weighting to specific variables; and (right) clipped latitude weighting. 2.3 Continuous Ranked Probability Score (CRPS) Our consistency model is parameterized such that single-step predictions simplify Equation (2) and , t(cid:1), assuming we set = π are given by fθ(xt, t) = σdFθ dI). The objective in Equation (3) has no notion of forecast uncertainty and therefore cannot, on its own, ensure ensembles are physically calibrated. However, this simplified parameterization allows us to resample noise to produce diverse ensemble members, which we then can calibrate with the right, domain-driven metric. 2 and sample π (0, σ (cid:0) xt σd 2 CRPS is strictly proper scoring rule for univariate distributions that quantifies the discrepancy between forecast cumulative distribution function (CDF) and an observed value R. This is achieved by integrating (cid:82) (F (z) 1[z y])2 dz, which measures the squared difference between the forecast CDF and step function at the observation. When the forecast distribution is represented with finite ensemble members, we can approximate with an empirical CDF. fair estimate (with the self-comparison removed) [2729] provides the following unbiased estimate given predictions ˆy CRPS (cid:0)ˆy1:N , y(cid:1) = 1 (cid:88) ˆyn 1 2N (N 1) (cid:88) n=n ˆyn yn . (5) In this formulation, the first term encourages ensemble members to remain close to the observation, while the second accounts for ensemble dispersion by correcting for self-similarity. This ultimately balances both forecast accuracy and sharpness with ensembles that are well calibrated."
        },
        {
            "title": "3 Methodology",
            "content": "Our method connects the concepts in Section 2 for the task described in Section 3.1. We then introduce our physically motivated training objectives in Section 3.2, followed by our network architecture and modeling specifics in Section 3.3 before summarizing implementation details in Section 3.4. 3.1 Task and Dataset We model the global evolution of the atmosphere by learning (xi+1 xi) with the temporal atmospheric states indexed by i. These states come from four decades of ERA5 reanalysis data [30], as provided by WeatherBench 2 (WB2) [31]. Data are downsampled to 1.40625 resolution (128 256 pixels) at 6-hour intervals, which differs from the native 0.25 resolution of our baseline, GenCast [13]. Our models use data from 19792018 for training, 2019 for validation, and 2020 for testing. We predict four surface-level variables (t2m, u10, v10, mslp) and five atmospheric variables (z, t, u, v, q), each at 13 pressure levels ({50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000} hPa), with additional forcings of top-of-atmosphere solar radiation, surface geopotential, and land-sea mask to stabilize phase shifts during long horizons and to simplify orographic representations (see Table 1 for more details). To reduce error accumulation, we follow [4] and incorporate random dynamic intervals δi U{6, 12, 24} to capture diurnal and synoptic-scale dynamics while preserving temporal fidelity. Each autoregressive step then estimates the residual xδi xi from the initial state xi and input forcings xf . forecast rollout is made for autoregressive steps at any time delta. During training and inference, data are z-score standardized using per-variable and per-level statistics from the training set. Residual targets rely on time delta statistics for each interval (approximately Figure 4: Learning curves for Swift. (left) learning rate schedule with Muon η during pretraining and AdamW during finetuning (in red); and (right) training loss with 3M tangent warmup (Equation (4), in blue) before multi-step finetuning with = 18 autoregressive steps from 1520M images. Gaussian), while conditional states use the full-field statistics. At each autoregressive step in finetuning and inference, predictions are unstandardized with the correct delta statistics and added back to the initial state xi to reconstruct the full field xδi. This becomes the subsequent initial condition. Additional details on the incorporation of conditional states, including the noise level (diffusion time) and data delta time, are provided in Section 3.3. 3.2 Training Objectives Our training proceeds in two stages, yielding our final model and two baseline models for comparison. The first stage consists of pretraining baseline diffusion model alongside base consistency model, Swift-B. The second stage applies multi-step finetuning to the consistency model, producing Swift, our final calibrated model. Model Pretraining. We train and formulate both our diffusion and consistency models under TrigFlow (Section 2). This v-prediction parameterization allows for simpler, few step samplers and opens the possibility of distilling large-scale diffusion models (e.g. [17]) into smaller, more efficient models (not shown herein). Such capabilities are not possible with GenCast [13] and its EDM formulation. Our pretraining loss includes latitudeand pressure-dependent functions to account for the non-uniform spherical grid and to emphasize near-surface variables [13, 25]. Specifically, the latitude and variable weights, denoted α(s) and κ(v) for each variable (Figure 3), yield L(θ) = 1 (cid:88) (cid:88) sS vV κ(v)α(s)ℓv,s(θ), (6) where is the set of spatial indices over all batches and ℓ is either the diffusion or consistency loss in Equations (1) and (3). Extensive hyperparameter searches led us to adopt Muon [32] for consistency pretraining and AdamW [33] for diffusion; see Section 3.4 for additional details. Multi-step Finetuning. Our base, pretrained consistency model (Swift-B) is not explicitly optimized for long-term atmospheric dynamics, leading to greater error accumulation when applied autoregressively. Considering our v-prediction loss regresses trajectory-based target at short leadtimes, we posit it is misaligned with producing faithful and physically calibrated forecast ensembles. We therefore exploit Swifts probabilistic nature and efficient sampling by finetuning on proper scoring rule, specifically optimizing the unbiased and fair CRPS (Equation (5)) with weights as LCRPS = 1 (cid:88) (cid:88) sS vV κ(v)α(s)CRPS (cid:0) ˆy1:2 v,s, yv,s (cid:1) , (7) where ˆy is computed in single-step from the initial condition and varying Gaussian noise for = 2 ensembles and per-batch δi. We parallelize CRPS with vector map over batch elements and use sequential gradient checkpointing [34] over = 2 autoregressive steps to allow for higher step counts when needed. Following curriculum schedule, we train with = {1, 2} for 1.5 million images (mimg), = 3 for 1 mimg, and = {4, 8} for 0.5 mimg, backpropogating in time through with the loss computed on the final step. This would otherwise be prohibitively costly in time and space for diffusion models, thereby positioning this work among the first to make multi-step finetuning tractable for generative forecasting. 5 Figure 5: Global forecast skill (on subset of initials with δi = 6). (a) latitude-weighted ensemble RMSE and spread/skill compared to baselines; and (b) close view benefit of multi-step finetuning. 3.3 Model Architecture We introduce conditional non-hierarchical Swin transformer (Figure 2), extending [35, 36] that empirically yields stable training for both diffusion and consistency models (Figure 4). Inputs are the noisy samples (drawn from standard Gaussian at inference) concatenated channel-wise with the spatial conditions, along with the auxiliary scalar conditions (i.e., noise level and data time deltas). Predicting 69 channels means we have 141 input channels with forcings  (Table 1)  . We also find the dynamic time intervals help regularize the models training dynamics. Every other layer implements shifting windows in xand y-directions to increase the effective receptive field. We remove the relative position biases from the scaled cosine attention block and adopt adaptive LayerNorm (adaLN) [37] and SwiGLU [38] for the post-normalization layers and fully-connected layers, respectively. Each adaLN normalizes its input tensor modulated by the shared latent embedding t, which is the combination of the noise level encoded by sinusoidal transformation and the embedded predicted time delta, δi. Two layers with silu nonlinearities create the embedding. The output decoder layer is simple linear reshape. Specific to this work, the spatial inputs are embedded with linear projection of 2 2 patches. We use hidden dimension of 1,056, with 12 attention blocks, each using 12 attention headsresulting in 225M parameter model. Layer biases and modulation weights are zero-initialized; all other weights follow the initialization in [4] with truncated normal distribution (zero mean and standard deviation of 0.02). Other weight initializations we tested caused our model to diverge during training. 3.4 Implementation Details Our models are trained in two phases, pretraining with both the diffusion and consistency model, then finetuning of just our consistency model (Section 3.2). All our models are developed in PyTorch and trained on cluster of 120 64GB Intel Max 1550 GPUs (tiles) for maximum of 20 million images (15M for pretraining and 5M for finetuning) over 3 days per model; using local batch size of 1 and distributed data parallelism, this equates to 167K training/update steps. We also use bfloat16 in training and float32 during inference. We had explored many optimizers and learning rate schedules, finding AdamW [33] to work best with diffusion and finetuning, whereas Muon [32] was the most effective and stable with our consistency model. For AdamW we use maximum learning rate of η = 5e-4, β1,2 = [0.9, 0.95], ϵ = 1e-6, and weight decay λ = 1e-5 for all parameters except for the positional embedding and modulation layer. With Muon we still use AdamW for 1D parameters and embedding layers with η = 3e-4, β1,2 = [0.9, 0.95], ϵ = 1e-10, and λ = 0.01. All 2D transformer weights use η = 0.02, λ = 0.01, and momentum = 0.95 with default of 5 NewtonSchulz iterations. Training stability and performance is further improved by using cosine learning rate schedule for both optimizers, starting with linear warmup (a minimum value set by scaling η by 1e-4) for 2M images, cosine annealing until 15M images, and constant η = 1e-5 during finetuning (Figure 4). We also apply an exponential moving average (EMA) with 500K image halflife and use the EMA parameters during inference. Lastly, while our data (target residuals) is normal in its first two 6 moments, it has variables with heavy tails. We therefore find that log-uniform noise schedule, τ = (1 u) log(σmin) + log(σmax), where U(0, 1) and noise levels σmin = 0.02 and σmax = 200 empirically provide the best results. This only pertains to training as during inference and finetuning, we sample single step from = π 2 , i.e., the upper bound of as τ +."
        },
        {
            "title": "4 Experimental Results",
            "content": "We first assess global, medium-range forecast skill against baselines (Section 4.1), then examine long-term stability for subseasonal-to-seasonal forecasting (Section 4.2), and finally present case studies of extreme events and seasonal trends (Section 4.3). 4.1 Global Forecast Skill We compare forecast performance with the state-of-the-art data-driven model GenCast [13] and numerical forecasting system (IFS ENS), in addition to our own diffusion model and non-finetuned Swift-B (base) variant. All medium-range forecasts are made on δi = 6 intervals with uniformly distributed test samples in 2020. Skill is measured by latitude-weighted ensemble mean rootmean-squared error (RMSE) (Figure 12), per-location continuous ranked probability score (CRPS) (Figure 13), and the models spread to skill ratio (SSR) (Figure 14) relative to target ERA5. With our diffusion model and limited compute, we generates 12, 15 day forecasts in 7.6 min/forecast with 24 ensembles, using 39 NFEs per autoregressive step with the DPMSolver++ 2S solver [39, 12], albeit without stochastic churn due to the trigonometric complexity. In contrast, with Swift we generate 64 forecasts in 15 sec/forecast with 12 ensembles and only 1 NFE (30 empirically faster wall-clock time). As shown in Figure 5, Swift remains competitive with the IFS ENS for most variables despite being underdispersive (having an SSR < 1) and only using 12 ensemble members (versus 50 as in GenCast), and finetuning substantially improves stability over Swift-B. Concretely, Swift performs competitively with our diffusion model while running at fraction of the cost, and it consistently outperforms the pretrained baseline, though it marginally underperforms GenCast. Additional results for other variables are provided in Section B. We illustrate 7 day forecasts errors in Figure 10, where we see the greatest errors in the ensemble mean nearest the poles across variables. These align with persistent polar forecasting challenges, amplified by higher reanalysis uncertainties in ERA5 (e.g., from observation sparsity) and model uncertainty in our latitude weighted losses. While not qualitatively visible, we find Swifts predictions of smoother fields, i.e., geopotential and mean sea level pressure, drift at high zonal wavenumbers to larger scales proportional to lead-time. Other forecast fields maintain accurate spectra and are stable over time (Figure 11). This drift is known limitation that is addressable with field truncation [19], and is not inherently detrimental to model performance. (a) Hovmöller diagram (b) Global forecast states (c) Power spectra Figure 6: Long-term stability (with δi = 24). (a) u850 anomalies (climatology removed) Hovmöller [40] averaged between 10 N/S; (b) single-member 14and 45-day predicted fields of q700 initialized on Jan 1, 2020; and (c) power spectra of q700 forecasts averaged over 32 initials and 12 ensembles. 7 4.2 Long-term Stability We evaluate the potential for subseasonal-to-seasonal (S2S) forecasting by assessing long-term stability (qualitatively only to ERA5), which is essential for climate modeling and decision-making under uncertainty. The Hovmöller diagram [40] in Figure 6a shows u850 anomalies from singlemember 60 day forecast, where Swift resembles ERA5 by reproducing realistic eastwardand westward-propagating equatorial wave modes, preserving landocean boundaries (e.g., around 80 and 45 E), and correctly disperses high-amplitude equatorial wind events at subseasonal-scales. Additional Hovmöller diagrams supporting stability out to 75 days are shown in Figure 15. Deterministic data-driven weather models [14], while computational efficient, tend to blur sharp features at extended lead-timesan artifact of squared-error objectives converging toward the ensemble mean [6, 7], among other influences. In contrast, Swift enjoys the efficiency of inference while preserving sharpness, result of modeling the data distribution as consistency model during pretraining and calibrating it through finetuning. Figures 6b and 6c illustrates this with global forecasts of q700 and the respective power spectra averaged over 32 initial conditions made on δi = 24 intervals. We find our forecasts retain coherent structures while having realistic power spectra, further supporting stable long-term behavior. Additional fields on seasonal-scales are visualized in Figure 9, with single-member forecast out to 14and 45-days over all variables. In both cases, the forecasts remain realistically sharp. (a) Tropical cyclone tracks (b) Ensemble forecasts Figure 7: Probabilistic weather forecasting (initialized 2020-08-22T06z; δi = 6). (a) predicted tracks of Hurricane Laura depicted over 48 ensembles; and (b) individual ensembles at increasing lead-time, highlighting areas of tropical cyclone, an atmospheric river, and the intertropical convergence zone. 4.3 Case Studies While many potential case studies could be examined, we focus on two that highlight Swifts ability to: (a) generate diverse and realistic ensembles under extreme weather conditions, and (b) capture seasonal cycles throughout the year from given initial condition. Extreme Weather Events. On August 27, 2020, Hurricane Laura made landfall near Cameron, Louisiana, resulting in $19 billion in damage and 47 direct deaths [41]. Considering its impact, we study this storm and generate 48 forecasts from Swift, initialized 5 days before landfall, and superimpose the predicted tracks on the target ERA5 track, computed at 1.4 resolution (Figure 7a). Ensemble uncertainty remains low until the track turns northward, when ensemble spread increases, though several members capture the observed track. Individual realizations over North America are shown in Figure 7b, where in q700 we find the tropical cyclone, an atmospheric river in the Gulf of Alaska, and the Intertropical Convergence Zone, consistent with [42]. These qualitative fields further highlight the realistic atmospheric dynamics and diversity among ensemble members. Extratropical Seasonal Trends. Data-driven models often struggle to generate physically consistent behavior, particularly in reproducing slow, forced responses of the atmosphere rather than merely propagating variability. To assess this, we initialize Swift from several distinct initial conditions over 2020 and examine the evolution of extratropical 2 meter temperature over 75 day periods (Figure 8). Both individual ensemble members and the mean follow the expected response to the changing in the 8 Figure 8: Seasonal cycle forecasts (with δi = 24). Area mean 2 meter temperature for the Northern (top) and Southern (bottom) Hemisphere extratropics for three, seasonally spread initial conditions compared to ERA5 (black) with the closest forecast member. forcing conditions on seasonal time-scales. That is, the Northern Hemisphere warms through boreal spring and cools into autumn, while the Southern Hemisphere exhibits the opposite progression. The seasonal transition initialized in June is also captured in the rollout. Across initials, Swift remains stable and phase-aligned with ERA5, though with tendency to slightly understate the amplitude."
        },
        {
            "title": "5 Conclusion",
            "content": "Our analyses demonstrate that our generative, autoregressively finetuned consistency model, Swift, achieves skillful 6-hourly forecasts that remain stable for up to 75 days while being significantly more computationally efficient than diffusion baselines (up to 39 faster). Although more exploration is needed, preliminary results suggest that Swift has the potential to improve subseasonal-to-seasonal (S2S) forecasting. Case studies further demonstrate the ability to capture diverse, realistic ensembles during extreme events and model seasonal cycles. Taken together, these results mark significant step toward skillful machine learning-based ensemble weather forecasting that spans medium-range to seasonal-scales, without requiring substantially more compute compared to deterministic models. Despite these achievements, several limitations remain. Comparisons used reduced number of initial conditions and ensemble members due to computational and storage constraints; we expect improved performance by scaling both. Additionally, forecast results are underdispersive with finetuning leading to better long term spread at the expense of short term SSR (days 0-4), which could be addressed with better learning rate schedule during finetuning. Lastly, our medium-range and long-term forecasts used δi = 6and δi = 24-hourly intervals, respectively, leaving the potential to improve stability at higher temporal resolution with Pangu-Weather style [2] greedy schedule. Looking ahead, we aim to explore consistency distillation with larger, high-resolution model (e.g., [17]) to make operational deployment more accessible. Moreover, we can leverage methods such as classifier-free guidance to study improvements to forecast performance [43]. Finally, considering our method is agnostic to architecture, it would be insightful to benchmark alternative backbones."
        },
        {
            "title": "Acknowledgments",
            "content": "This work has been supported by the U.S Department of Energy (DOE) Office of Cybersecurity, Energy Security, and Emergency Response (CESER) and also by the Laboratory Directed Research and Development (LDRD) Program at Argonne National Laboratory through the U.S. Department of Energy (DOE) contract DE-AC02-06CH11357. Computing resources come from the Argonne Leadership Computing Facility, U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory."
        },
        {
            "title": "References",
            "content": "[1] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022. [2] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Pangu-weather: 3d high-resolution model for fast and accurate global weather forecast. arXiv preprint arXiv:2211.02556, 2022. [3] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Learning skillful medium-range global weather forecasting. Science, 382(6677):14161421, 2023. [4] Tung Nguyen, Rohan Shah, Hritik Bansal, Troy Arcomano, Romit Maulik, Rao Kotamarthi, Ian Foster, Sandeep Madireddy, and Aditya Grover. Scaling transformer neural networks for skillful and reliable medium-range weather forecasting. Advances in Neural Information Processing Systems, 37:6874068771, 2024. [5] Simon Lang, Mihai Alexe, Matthew Chantry, Jesper Dramsch, Florian Pinault, Baudouin Raoult, Mariana C. A. Clare, Christian Lessig, Michael Maier-Gerber, Linus Magnusson, Zied Ben Bouallègue, Ana Prieto Nemesio, Peter D. Dueben, Andrew Brown, Florian Pappenberger, and Florence Rabier. Aifs ecmwfs data-driven forecasting system, 2024. [6] Ashesh Chattopadhyay, Y. Qiang Sun, and Pedram Hassanzadeh. Challenges of learning multi-scale dynamics with ai weather models: Implications for stability and one solution, 2024. [7] Imme Ebert-Uphoff, Lander Ver Hoef, John Schreck, Jason Stock, Maria Molina, Amy McGovern, Michael Yu, Bill Petzke, Kyle Hilburn, David Hall, et al. Measuring sharpness of ai-generated meteorological imagery. Artificial Intelligence for the Earth Systems, 2025. [8] T. Selz and G. C. Craig. Can artificial intelligence-based weather prediction models simulate the butterfly effect? Geophysical Research Letters, 50(20):e2023GL105747, 2023. e2023GL105747 2023GL105747. [9] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [11] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [12] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [13] Ilan Price, Alvaro Sanchez-Gonzalez, Ferran Alet, Tom Andersson, Andrew El-Kadi, Dominic Masters, Timo Ewalds, Jacklynn Stott, Shakir Mohamed, Peter Battaglia, et al. Gencast: Diffusion-based ensemble forecasting for medium-range weather. arXiv preprint arXiv:2312.15796, 2023. [14] Guillaume Couairon, Renu Singh, Anastase Charantonis, Christian Lessig, and Claire Monteleoni. Archesweather & archesweathergen: deterministic and generative model for efficient ml weather forecasting. arXiv preprint arXiv:2412.12971, 2024. 10 [15] Jason Stock, Jaideep Pathak, Yair Cohen, Mike Pritchard, Piyush Garg, Dale Durran, Morteza Mardani, and Noah Brenowitz. Diffobs: Generative diffusion for global forecasting of satellite observations. arXiv preprint arXiv:2404.06517, 2024. [16] Morteza Mardani, Noah Brenowitz, Yair Cohen, Jaideep Pathak, Chieh-Yu Chen, Cheng-Chin Liu, Arash Vahdat, Mohammad Amin Nabian, Tao Ge, Akshay Subramaniam, et al. Residual corrective diffusion modeling for km-scale atmospheric downscaling. Communications Earth & Environment, 6(1):124, 2025. [17] Väinö Hatanpää, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman, Chunyong Jung, Sandeep Madireddy, Tung Nguyen, Varuni Sastry, Ray AO Sinurat, et al. Aeris: Argonne earth systems model for reliable and skillful predictions. arXiv preprint arXiv:2509.13523, 2025. [18] Ferran Alet, Ilan Price, Andrew El-Kadi, Dominic Masters, Stratis Markou, Tom Andersson, Jacklynn Stott, Remi Lam, Matthew Willson, Alvaro Sanchez-Gonzalez, et al. Skillful joint probabilistic weather forecasting from marginals. arXiv preprint arXiv:2506.10772, 2025. [19] Simon Lang, Mihai Alexe, Mariana CA Clare, Christopher Roberts, Rilwan Adewoyin, Zied Ben Bouallègue, Matthew Chantry, Jesper Dramsch, Peter Dueben, Sara Hahner, et al. Aifs-crps: ensemble forecasting using model trained with loss function based on the continuous ranked probability score. arXiv preprint arXiv:2412.15832, 2024. [20] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. [21] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. [22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [24] Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. [25] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid RectorBrooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. [26] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. [27] Christopher AT Ferro, David Richardson, and Andreas Weigel. On the effect of ensemble size on the discrete and continuous ranked probability scores. Meteorological Applications: journal of forecasting, practical applications, training techniques and modelling, 15(1):1924, 2008. [28] CAT Ferro. Fair scores for ensemble forecasts. Quarterly Journal of the Royal Meteorological Society, 140(683):19171923, 2014. [29] Martin Leutbecher. Ensemble size: How suboptimal is less than infinity? Quarterly Journal of the Royal Meteorological Society, 145:107128, 2019. [30] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín MuñozSabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. Quarterly journal of the royal meteorological society, 146(730):19992049, 2020. [31] Stephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter Battaglia, Tyler Russell, Alvaro Sanchez-Gonzalez, Vivian Yang, Rob Carver, Shreya Agrawal, et al. Weatherbench 2: benchmark for the next generation of data-driven global weather models. Journal of Advances in Modeling Earth Systems, 16(6):e2023MS004019, 2024. [32] Keller Jordan, Yuchen Jin, Vlado Boza, You Jiacheng, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [34] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [35] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1200912019, 2022. [36] Jared Willard, Peter Harrington, Shashank Subramanian, Ankur Mahesh, Travis OBrien, and William Collins. Analyzing and exploring training recipes for large-scale transformerbased weather prediction. Artificial Intelligence for the Earth Systems, 4(2):240061, 2025. [37] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [38] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [39] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pages 122, 2025. [40] Ernest Hovmöller. The trough-and-ridge diagram. Tellus, 1(2):6266, 1949. [41] Pasch, Berg, Roberts, and Papin. Hurricane laura (al 132020). National Hurricane center tropical cyclone report, 2021. [42] Dmitrii Kochkov, Janni Yuval, Ian Langmore, Peter Norgaard, Jamie Smith, Griffin Mooers, Milan Klöwer, James Lottes, Stephan Rasp, Peter Düben, et al. Neural general circulation models for weather and climate. Nature, 632(8027):10601066, 2024. [43] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022."
        },
        {
            "title": "A Full Field Visualizations",
            "content": "Figure 9: Additional samples from single ensemble member (initialized on 01-01-2020T0z). 13 Figure 10: Forecast error at 7-day lead-time (initialized on 01-01-2020T0z). 14 Additional Medium-Range Results Figure 11: Power spectra compared to target ERA5 from generated (δi = 6) 15 day forecasts, averaged over 32 initial conditions and 12 ensembles shown for each variable between 6360h. Figure 12: Latitude-weighted ensemble mean root-mean-squared error (RMSE). 15 Figure 13: Latitude-weighted continuous ranked probability score (CRPS). Figure 14: Latitude-weighted spread to skill ratio (SSR). 16 Additional Hovmöller Diagrams (a) 2020-04-12T0z (b) 2020-06-16Tz (c) 2020-08-10T18z (d) 2020-09-26Tz Figure 15: Additional u850 anomaly (climatology removed) Hovmöller diagrams highlighting the tropics with averages between 10 N/S over 75 days with initials over the year in (a)(d)."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "Argonne National Laboratory"
    ]
}