{
    "paper_title": "Variational Reasoning for Language Models",
    "authors": [
        "Xiangxin Zhou",
        "Zichen Liu",
        "Haonan Wang",
        "Chao Du",
        "Min Lin",
        "Chongxuan Li",
        "Liang Wang",
        "Tianyu Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning."
        },
        {
            "title": "Start",
            "content": "Variational Reasoning for Language Models"
        },
        {
            "title": "VARIATIONAL REASONING FOR LANGUAGE MODELS",
            "content": "Xiangxin Zhou*1,2,3, Zichen Liu1,4, Haonan Wang1,4, Chao Du1, Min Lin1, Chongxuan Li5, Liang Wang2,3, Tianyu Pang*1 1Sea AI Lab 5RUC (cid:135) Code Link 3CASIA 4NUS 2UCAS 5 2 0 2 6 ] . [ 1 7 3 6 2 2 . 9 0 5 2 : r ABSTRACT We introduce variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to multi-trace objective for tighter bounds and propose forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across wide range of reasoning tasks. Overall, our work provides principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reasoning has recently become central focus for large language models (LLMs), driving advances in tasks such as mathematics, coding, and scientific problem solving (Jaech et al., 2024; Comanici et al., 2025; Guo et al., 2025). common strategy is to let models generate explicit thinking traces before producing final answers. To train such reasoning abilities, two dominant approaches are widely used: supervised finetuning (SFT) (Guha et al., 2025; Muennighoff et al., 2025) and reinforcement learning (RL) (Yu et al., 2025a; Liu et al., 2025; Zeng et al., 2025), both showing strong empirical success. Despite this progress, each approach faces limitations. SFT often relies on curated long-thinking traces, which are costly to collect and, as an offline method, may struggle to generalize (Chu et al., 2025) or suffer from catastrophic forgetting (Shenfeld et al., 2025). Recent RL methods typically depend on verifiable rewards to mitigate reward hacking, yet training can be unstable and output diversity may collapse (Cheng et al., 2025; Cui et al., 2025b). As result, correct answers to harder questions become increasingly rare, leading to lower Pass@K accuracy than base models (Yue et al., 2025a). These challenges motivate the search for more principled objective for training reasoning models. To this end, we propose to view reasoning through the lens of probabilistic modeling, where thinking traces are treated as latent variables. Variational inference (Kingma & Welling, 2013) provides natural way to optimize the log-likelihood of producing correct answers. This perspective offers several advantages: it replaces the intractable marginalization over thinking traces with tractable lower bounds, enables multi-trace extensions that tighten the objective, and introduces variational posterior that can sample thinking paths more likely to yield correct answers. In this way, it provides principled objective for training reasoning models, while remaining compatible with verifiable rewards. Building on this perspective, we develop variational reasoning framework for language models in Section 2. The core idea is to decompose reasoning into thinking trace and an answer, leading to the maximum log-likelihood estimation (MLE) objective. To make this optimization tractable, we introduce an evidence lower bound (ELBO) and extend it to an IWAE-style multi-trace formulation (Burda et al., 2015), which tightens with more rollouts. To further stabilize the training of the variational posterior, we propose forward-KL objective that prevents collapse and makes better use of answer hints. Together, these components form unified training pipeline (as shown in Algorithm 1) that jointly improves the reasoning model and the variational posterior. Beyond the method itself, our framework also helps interpret existing approaches, as described in Section 3. Rejection-sampling finetuning (RFT) (Dong et al., 2023; Touvron et al., 2023) can *Equal contribution. Correspondence to Tianyu Pang. 1 Variational Reasoning for Language Models be re-expressed as forward-KL optimization weighted by model accuracy, and binary-reward RL, including GRPO (Shao et al., 2024), admits similar form. Our analysis shows that this weighting by accuracy arises implicitly and produces systematic bias toward easier questions, an effect that has not been explicitly recognized before. By placing these methods under shared probabilistic view, our framework provides principled objectives and clarifies the behavior of widely used methods. We validate our framework on the Qwen2.5 and Qwen3 model families (Yang et al., 2024; Team, 2025b) and observe consistent improvements over strong baselines across diverse reasoning benchmarks, including MATH500, AIME24&25, OlympiadBench, LiveCodeBench, GPQA-Diamond, and MMLU-Pro. Due to space constraints, detailed discussion of related work is deferred to Appendix B."
        },
        {
            "title": "2 VARIATIONAL REASONING",
            "content": "Let denote the set of all prompt strings over the vocabulary V. Given an input question , reasoning model πθ(z, yx) generates both thinking process and predicted answer . The joint probability can be written as πθ(z, yx) = πθ(yx, z) πθ(zx). Following standard format template (Guo et al., 2025), these two conditional terms are computed as1 πθ (zx) = πθ πθ(yx, z) = πθ (cid:0) [z, </think>] [x, <think>] (cid:1); (cid:0) [y, </answer>] [x, <think>, z, </think>, <answer>] (cid:1), (1) where </think> and </answer> serve as the end-of-sequence markers for and y, respectively. We define the marginal distribution Pθ(yx) = (cid:80) πθ(yx, z)πθ(zx), where the notation Pθ(yx) highlights that this distribution is induced by πθ. This is different from the non-thinking probability πθ(yx), which does not marginalize over possible thinking traces. πθ(z, yx) = (cid:80) 2.1 EVIDENCE LOWER BOUND Let Yx denote the oracle set (possibly infinite) of correct answers to the question x. The marginal probability that πθ generates correct answer is Pθ(Yxx) = (cid:80) Pθ(yx). Maximizing this probability gives the maximum log-likelihood estimation (MLE) objective: maxθ log Pθ(Yxx) . However, this MLE objective is intractable because computing Pθ(yx) requires summing over all possible thinking traces z. To make learning feasible, we apply variational inference (Kingma & Welling, 2013) to derive an evidence lower bound (ELBO): (cid:88) πθ(Yxx, z)πθ(zx) log Pθ(Yxx) = log yYx = log Eqϕ(y)Eqϕ(zx,y) (cid:20) πθ(Yxx, z)πθ(zx) qϕ(zx, y) (cid:21) Eqϕ(y) (cid:2)Eqϕ(zx,y) [log πθ(Yxx, z)] DKL (qϕ(zx, y)πθ(zx))(cid:3) (cid:123)(cid:122) (cid:125) (cid:124) LELBO(x, Yx, y; πθ, qϕ) . (2) In this expression, πθ(Yxx, z) denotes the probability of producing correct answer given the question and particular thinking trace z. The distribution qϕ(zx, y) is the variational posterior, which conditions not only on the question but also on an auxiliary answer hint y: qϕ(zx, y) = qϕ (cid:0)[z, </think>] [x, <hint>, y, </hint>, <think>](cid:1). (3) Here, <hint> and </hint> are shown as example delimiters; in experiments, we ablate different special tokens to wrap the hint and concatenate it after x. Conditioning on encourages the variational posterior to generate thinking traces that are more likely to yield correct answers. simple yet effective design choice is to let come directly from the oracle set, that is, supp[qϕ(y)] Yx. In practice, may be rephrasing of reference answer or any correct expression sampled from Yx. We can further show (detailed in Appendix A.1) that maximizing the ELBO objective w.r.t. qϕ in Eq. (2) is equivalent to minimizing the reverse KL divergence between qϕ(zx, y) and Pθ(zx, Yx): LELBO(x, Yx, y; πθ, qϕ) = log Pθ(Yxx) DKL (qϕ(zx, y)Pθ(zx, Yx)). (4) Here Pθ(zx, Yx) = πθ(Yxx,z)πθ(zx) is the true posterior. Compared with the prior distribution πθ(zx), this posterior distribution re-weights thinking traces by πθ(Yxx, z), thus favoring that are more likely to produce correct answers. According to Eq. (4), we know that the optimal solution for maxqϕ LELBO(x, Yx, y; πθ, qϕ) is: qϕ(y), there is ϕ(zx, y) = Pθ(zx, Yx) . Pθ(Yxx) 1We will omit special tokens such as </think> and </answer> in the formulas without ambiguity. 2 Variational Reasoning for Language Models 2.2 EXTENSION TO IWAE-STYLE LOWER BOUND In reinforcement learning (RL), it is now common practice to perform parallel rollouts of multiple thinking traces and answers for given question (Shao et al., 2024). This naturally motivates us to extend the single-trace ELBO in Eq. (4) to an importance-weighted autoencoder (IWAE) style bound (Burda et al., 2015). By leveraging multiple traces, this approach yields strictly tighter lower bound. Specifically, we obtain the following IWAE-style lower bound for log Pθ(Yxx): ELBO(x, Yx, y; πθ, qϕ) = Ez1:K qϕ(zx,y) LK log (cid:34) 1 (cid:88) k= πθ(zk, Yxx) qϕ(zkx, y) (cid:35) . (5) ELBO LK+1 ELBO log Pθ(Yxx) for any N+, which means These IWAE-style bounds satisfy LK the bound becomes tighter as increases (the proof is similar to that of Burda et al. (2015)). The single-trace ELBO objective in Eq. (4) corresponds to the special case of = 1, i.e., LELBO = L1 ELBO. ELBO(x, Yx, y; πθ, qϕ) w.r.t. the model Gradient estimation. We now derive the gradient of LK parameters θ (see Appendix A.2 for the gradient w.r.t. the variational parameters ϕ, i.e., ϕLK ELBO): θLK ELBO(x, Yx, y; πθ, qϕ) = Ez1:K qϕ(zx,y) where (cid:101)ρk = ρk (cid:80)K j=1 ρj and ρk = k=1 πθ(zk, Yxx) qϕ(zkx, y) . (cid:34) (cid:88) (cid:35) (cid:101)ρkθ log πθ(zk, Yxx) , (6) Estimating ρk. The weight ρk in Eq. (6) can be decomposed as ρk = πθ(zkx) qϕ(zkx,y) πθ(Yxx, zk) , where the first term, πθ(zkx) qϕ(zkx,y) , is the likelihood ratio of the thinking trace zk, and the second term, πθ(Yxx, zk), is the probability of producing correct answer given and zk. In reasoning models, single trace zk may contain thousands of tokens. Directly computing the likelihood ratio over such long sequences often leads to high variance, phenomenon also reported in concurrent studies (Cetin et al., 2025; Zheng et al., 2025). To mitigate this issue, we use the geometric mean (cid:0) πθ(zkx) (cid:1)1/zk qϕ(zkx,y) as surrogate for the likelihood ratio of zk. This per-token normalization reduces variance at the cost of introducing some bias, effectively spreading the ratio evenly across the thinking tokens. As for computing πθ(Yxx, zk), we consider two unbiased estimators: (i) likelihood-based estimator is πθ(Yxx, z) = YxEyU (Yx) [πθ(yx, z)], where Yx is cardinality of Yx and U(Yx) is the uniform distribution on Yx; (ii) accuracy-based estimator is πθ(Yxx, z) = Eyπθ(yx,z) [1(y Yx)], where 1() is the indicator function. When Yx = 1, i.e., there is unique correct answer expression y, Zhou et al. (2025) show that the likelihood-based estimator has lower variance (in fact, zero) compared to the accuracy-based one. We now extend this comparison to general cases when Yx > 1: Theorem 1. (Proof in Appendix A.3) For Yx > 1, the worst-case variances of the likelihood-based estimator and the accuracy-based estimator over all possible πθ (under fixed πθ(Yxx, z)) are Varacc = πθ(Yxx, z) (1 πθ(Yxx, z)) . Varlike = (Yx 1) πθ(Yxx, z)2; max πθ Therefore, the accuracy-based estimator has lower worst-case variance, i.e., maxπθ Varacc maxπθ Varlike, whenever the model accuracy (conditional on x, z) satisfies πθ(Yxx, z) 1 max πθ (7) Yx . Note that for many practical questions, the space of correct answers can be quite flexible, so typically Yx 1. In this regime, the accuracy-based estimator enjoys much lower worst-case variance. Based on this insight, in our experiments we estimate the weight ρk as (cid:1)1/zk = (cid:0) πθ(zkx) ρest qϕ(zkx,y) Eyπθ(yx,zk) [1(y Yx)] , (8) where the expectation Eyπθ(yx,zk) [1(y Yx)] is approximated by sampling multiple candidate answers for each thinking trace zk, similar to the implementation in Qi et al. (2025). Estimating θ log πθ(zk, Yxx). When evaluating θLK ELBO in Eq. (6), we need the gradient θ log πθ(zk, Yxx) = θ log πθ(zkx) + θ log πθ(Yxx, zk). The first term, θ log πθ(zkx), is straightforward to calculate. For the second term, θ log πθ(Yxx, zk), we also adopt an accuracybased estimator: θ log πθ(Yxx, zk) = . In practice, the expectations w.r.t. πθ(yx, zk) are approximated using the same samples drawn to estimate ρest . Eyπθ (yx,zk )[1(yYx)θ log πθ(yx,zk)] Eyπθ (yx,zk )[1(yYx)] 3 Variational Reasoning for Language Models Algorithm 1 Training pipeline of variational reasoning Inputs: An initial reasoning model πθ0(z, yx), variational posterior qϕ(zx, y), question-answer dataset {x, x} , where Yx is one of the reference answers corresponding to Inputs: Rollout numbers and , training rounds , steps per round Sθ and Sϕ, optimizer Outputs: The trained model parameters θT and variational parameters ϕT 1: Initialize qϕ0 (zx, y) 2: Construct Yx (or its subset by rephrasing 3: for = 1 to do (cid:12)[x, <hint>, y, </hint>, <think>](cid:1) x) and rule-based/model-based verifier 1(y Yx) (cid:0)[z, </think>](cid:12) ϕ0 copy θ0 πθ0 forward in Eq. (9); initializing ϕt ϕt1 ## Updating variational parameters ϕt with ϕLM for = 1 to Sϕ do Sample training batch of questions for all questions do Rollout z1:M πθt1(zx), qϕ(y) = U(Yx) Compute wm = Eyπθt1 (yx,zm) [1(y Yx)] and (cid:101)wm = wm (cid:80)M ## Collecting z1:M and compute weights (cid:101)wm for each Update ϕt O.step (cid:16) ϕt, 1 (cid:80) xB (cid:80)M ## Updating model parameters θt with θLK for = 1 to Sθ do Sample training batch of questions for all questions do j=1 wj m=1 (cid:101)wmϕt log qϕt(zmx, y) ELBO in Eq. (6); initializing θt θt1 (cid:17) Rollout z1:K qϕt(zx, y), qϕ(y) = U(Yx) Compute ρest = (cid:0) πθt (zkx) qϕt (zkx,y) (cid:1)1/zk ## Estimate ρest ## Collecting z1:K and compute weights (cid:101)ρk for each by Eq. (8) Eyπθt (yx,zk)[1(y Yx)] and (cid:101)ρk = ρest (yx,zk )[1(yYx)θt log πθt (yx,zk)] Eyπθt (yx,zk )[1(yYx)] k=1 (cid:101)ρkθt (log πθt(zkx) + log πθt(Yxx, zk)) j=1 ρest Eyπθt (cid:80)K (cid:80)K (cid:17) Compute θt log πθt(Yxx, zk) = (cid:16) θt, 1 Update θt O.step xB (cid:80) 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: return θT and ϕT 2.3 OPTIMIZING THE VARIATIONAL POSTERIOR VIA FORWARD KL DIVERGENCE While Eq. (5) provides IWAE-style bounds that yield tighter optimization of the MLE objective w.r.t. the model parameters θ (through θLK ELBO), our pilot experiments show unexpected behavior for the optimization of the variational parameters ϕ (through ϕLELBO or ϕLK ELBO). Recall from Eq. (4) that the ELBO objective minimizes the reverse KL divergence DKL (qϕ(zx, y)Pθ(zx, Yx)), where both the expectation and Monte Carlo samples are taken under qϕ(zx, y). In practice, however, the policy model πθ(zx) is often already well-trained due to pretraining of base LLMs (Liu et al., 2025), while the variational posterior qϕ(zx, y) may struggle to effectively use hints from correct answers without collapsing into shortcut reasoning (e.g., directly leaking answer tokens into the thinking trace). To address this imbalance, we propose to optimize qϕ(zx, y) using the forward KL divergence DKL(Pθ(zx, Yx)qϕ(zx, y)), whose gradient w.r.t. ϕ can be written as:2 ϕDKL(Pθ(zx, Yx)qϕ(zx, y)) Ez1:M πθ(zx) (cid:34) (cid:88) (cid:101)wmϕ log qϕ(zmx, y) (cid:35) where (cid:101)wm = wm j=1 wj (cid:80)M m=1 and wm = πθ(Yxx, zm) = Eyπθ(yx,zm) [1(y Yx)] . ϕLM forward, (9) This approximation, ϕLM forward, follows derivation similar to Bornschein & Bengio (2015) (see Appendix A.4), with the sample size not necessarily equal to used in θLK ELBO. Unlike IWAE, this objective is an approximation rather than lower bound. Optimizing Eq. (9) can be viewed as weighted supervised finetuning (SFT) for qϕ(zx, y), where training data is sampled from πθ(zx). We summarize the overall training pipeline of our variational reasoning method in Algorithm 1. In our experiments, we train for only single round (T = 1), leaving multi-round training as an interesting direction for future work. 2We assume that qϕ(y) is fixed prior distribution and does not involve gradients. 4 Variational Reasoning for Language Models"
        },
        {
            "title": "3 CONNECTION TO OTHER METHODS",
            "content": "In prior work, the thinking trace and the final answer are often treated together as the full output in the formulations (Shao et al., 2024; Guo et al., 2025; Liu et al., 2025; Wu et al., 2025). By explicitly decomposing the output into thinking process and an answer y, as we have done above, we can gain fresh perspectives on how our formulation relates to other mainstream methods. Connection to rejection sampling finetuning (RFT). RFT methods (Dong et al., 2023; Touvron et al., 2023) generate multiple candidate outputs for each input using reference model πref, and then select the candidate with the highest reward. The reference model may be strong teacher model or identical to the learner πθ (i.e., πref = πsg θ , with sg denoting stop-gradient). Formally, the gradient of RFT training objective, focusing only on the learning of the thinking trace z, can be written as: θLRFT(x, πθ) θEπref(zx)Eπref(yx,z) [1(y Yx) log πθ(z, yx)] = θEπref(zx) [πref(Yxx, z) (log πθ(zx) + log πθ(yx, z))] only w.r.t. ===== πθ(zx) θEπref(zx) [πref(Yxx, z) log πθ(zx)] = Pref(Yxx) θDKL(Pref(zx, Yx)πθ(zx)). (10) yYx Here Pref(Yxx) = (cid:80) Pref(yx) denotes the model accuracy on instruction x, and the true posterior of the reference model is Pref(zx, Yx) = πref(Yxx,z)πref(zx) . As seen, the RFT objective can be viewed as maximizing forward KL divergence weighted by Pref(Yxx), with the optimal solution π θ (zx) = Pref(zx, Yx) . In practice, this weighting downplays hard questions with small Pref(Yxx), biasing training toward easier ones. In contrast, our formulation in Eq. (9) treats all questions more evenly, ensuring that the objective remains attentive to difficult cases. Pref(Yxx) Connection to binary reward RL. In the case of RL training with 01 binary reward, the training objective can be written in form similar to Eq. (10), focusing only on the thinking trace z: θLbi-RL(x, πθ) θEπθ(zx)Eπθ(yx,z) [1(y Yx)] only w.r.t. ===== πθ(zx) θEπsg = sg θ (Yxx, z) log πθ(zx)] θ (zx) [πsg θ (Yxx) θDKL(P sg Thus, the local gradient for πθ(zx) in binary-reward RL is equivalent to minimizing the forward θ (zx, Yx) = πθ(Yxx,z)πθ(zx) KL divergence between πθ(zx) and the true posterior sg , with the update further weighted by the model accuracy sg θ (Yxx). Interestingly, the RL training objective itself is Lbi-RL(x, πθ) = Eπθ(zx)Eπθ(yx,z) [1(y Yx)] = Pθ(Yxx), whereas the MLE objective in Eq. (2) for our variational reasoning framework instead maximizes log Pθ(Yxx). θ (zx, Yx)πθ(zx)). Pθ(Yxx) (11) Furthermore, in Group Relative Policy Optimization (GRPO) (Shao et al., 2024), one of the most widely used RL training objectives, the reward is normalized by the standard deviation of rewards within group of rollouts. Under 01 binary reward, each rollout reward follows Bernoulli distribution with mean Pθ(Yxx) and standard deviation (cid:112)Pθ(Yxx) (1 Pθ(Yxx)). The gradient of the GRPO objective can therefore be derived as (we omit the min and clip operations for brevity) only w.r.t. ===== πθ(zx) (cid:115) θ (Yxx) sg 1 sg θLbi-GRPO(x, πθ) θDKL(P sg θ (zx, Yx)πθ(zx)), (12) θ (Yxx) which follows from θLbi-GRPO(x, πθ) = θLbi-RL(x, πθ)/(cid:112)Pθ(Yxx) (1 Pθ(Yxx)). Thus, the local gradient for πθ(zx) in GRPO is still equivalent to minimizing the forward KL divergence between πθ(zx) and the true posterior sg θ (zx, Yx). Note that the per-instruction weight becomes (cid:112)Pθ(Yxx)/(1 Pθ(Yxx)), which increases monotonically with model accuracy and therefore also emphasizes easier questions with higher Pθ(Yxx). In Appendix A.5, we extend these analyses and derive gradients for more general RL reward shaping, including cases with format reward."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Datasets. We train on the Bespoke-Stratos-17k dataset curated by Li et al. (2025a), which combines math problems from Numina-Math (Li et al., 2024) with code problems from APPS (Hendrycks et al., 2021a) and TACO (Li et al., 2023). The dataset contains 16,710 samples, each paired with 5 Variational Reasoning for Language Models Table 1: Performance of models trained from Qwen3-4B-Base. All models are trained on BespokeStratos-17k except for General-Reasoner-4B. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@2 Qwen3-4B-Base General-Reasoner-4B Bespoke-Stratos-4B Ours-PB-GML-4B Ours-PB-Acc-4B 45.30 71.70 84.70 87.30 88.30 4.79 19.06 27.29 33.54 31.67 5.73 16.77 24.17 26.77 27. 27.73 55.00 70.16 74.06 75.63 23.37 45.18 50.45 54.45 55.71 Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@8 Avg@8 Avg@8 Avg@8 Avg@ Qwen3-4B-Base General-Reasoner-4B Bespoke-Stratos-4B Ours-PB-GML-4B Ours-PB-Acc-4B 29.10 40.97 44.95 45.52 45.33 18.54 61.40 71.22 79.53 80.29 5.46 17.90 19.54 31.25 33.68 1.32 2.85 3.25 6.20 5.79 36.89 61.36 63.03 65.52 65. Avg 21.38 41.54 51.35 55.23 55.72 Avg 18.26 36.90 40.40 45.60 46.12 Table 2: Performance of models trained from Qwen3-8B-Base trained on Bespoke-Stratos-17k. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@2 Qwen3-8B-Base Bespoke-Stratos-8B Ours-PB-GML-8B Ours-PB-Acc-8B 65.20 89.70 91.60 91.80 11.46 39.58 44.06 45. 10.10 28.85 31.67 31.98 45.00 78.91 83.59 85.47 34.72 55.64 58.23 58.98 Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@8 Avg@8 Avg@8 Avg@ Avg@1 Qwen3-8B-Base Bespoke-Stratos-8B Ours-PB-GML-8B Ours-PB-Acc-8B 35.42 53.03 52.72 53.66 41.14 81.53 87.36 86.47 13.65 36.89 45.51 49.33 1.42 7.11 13.82 13. 45.62 68.74 70.76 70.76 Avg 33.30 58.54 61.83 62.77 Avg 27.45 49.46 54.03 54.69 long-thinking trace generated by DeepSeek-R1 (Guo et al., 2025) or QwQ-32B-Preview (Team, 2024). To ensure fair evaluation, all training data is strictly separated from the test sets. Baselines. We compare against broad set of baselines for rigorous evaluation. For Qwen2.5-Instruct models, we include Bespoke-Stratos (Labs, 2025) and RLT (Cetin et al., 2025) at the 7B and 32B scales, where RLT trains teachers via RL to improve distillation. For Qwen3-Base models, we compare with Bespoke-Stratos-4B/8B and General-Reasoner-4B (Ma et al., 2025), the latter trained with GRPO (Shao et al., 2024) and model-based verifier. Models marked are trained by us; others are official releases. All Bespoke-Stratos models are distilled on Bespoke-Stratos-17k. This ensures fair comparison, as all methods (except General-Reasoner) follow the same training recipes and datasets. Evaluation. We assess our models on broad set of challenging benchmarks: MATH500 (Hendrycks et al., 2021b), subset of competition math curated by Lightman et al. (2024); AIME24&25 (MAA, 2025); AMC23 (MAA, 2023); OlympiadBench (He et al., 2024); LiveCodeBench (Jain et al., 2025), with Easy, Medium, and Hard subsets (LCB-E, LCB-M, LCB-H) for fine-grained coding evaluation; GPQA-Diamond (Rein et al., 2024) (GPQA-D), graduate-level natural science questions; and MMLU-Pro (Wang et al., 2024), diverse multiple-choice benchmark. Among these, GPQA-D and MMLU-Pro are out-of-distribution (OOD) relative to our training data. Decoding and average accuracy. Following Hochlehnert et al. (2025), we sample responses with temperature=0.7 and report average accuracy over responses per question (Avg@k). To reduce randomness and ensure fair comparison, we use larger for smaller datasets: Avg@32 for AIME24, AIME25, and AMC23; Avg@8 for GPQA-Diamond, LCB-E, LCB-M, and LCB-H; Avg@2 for MATH500 and OlympiadBench; and Avg@1 for the large MMLU-Pro (12k+ questions). All evaluations are conducted with SkyThought (Team, 2025a), with additional details in Appendix D. 6 Variational Reasoning for Language Models Table 3: Performance of models trained from Qwen2.5-32B-Instruct trained on Bespoke-Stratos-17k. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@2 Qwen2.5-32B-Instruct Bespoke-Stratos-32B RLT-32B Ours-PA-GML-32B Ours-PA-Acc-32B 80.70 92.60 93.50 93.20 93.50 15.83 55.42 56.77 56.56 58.85 12.08 46.88 47.19 48.13 50.31 61.95 92.19 91.48 93.98 92.97 46.96 64.68 63.21 64.24 64.39 Method GPQA-D LCB-E LCB-M LCB-H Avg@8 Avg@8 Avg@8 Avg@8 MMLU-Pro Avg@1 Qwen2.5-32B-Instruct Bespoke-Stratos-32B RLT-32B Ours-PA-GML-32B Ours-PA-Acc-32B 46.28 57.57 59.09 60.92 60.73 79.88 94.78 93.20 95.19 94. 40.60 73.54 72.15 72.21 73.18 9.76 30.48 29.78 35.57 31.81 59.19 75.22 74.88 75.57 75.55 Avg 43.51 70.34 70.43 71.22 72.01 Avg 47.14 66.32 65.82 67.89 67.21 Figure 1: Training loss and gradient norm of different methods during Qwen3-Base model training. 4.1 TRAINING DETAILS FOR VARIATIONAL REASONING We conduct experiments on Qwen2.5-7B-Instruct, Qwen2.5-32B-Instruct (Yang et al., 2024), Qwen3-4B-Base, and Qwen3-8B-Base (Team, 2025b). Following Algorithm 1, we first train an initial reasoning model πθ0 on Bespoke-Stratos-17k using the recipe from Labs (2025), and then variational posterior qϕ with the forward KL divergence (Eq. (9)) on the same dataset. These models are later used to compute the weights (cid:101)ρk. For πθ0, we adopt the prompt template from Labs (2025); for qϕ, we test two alternative templates (-PA and -PB, see Appendix E). Both πθ0 and qϕ are finetuned independently from the same base model without weight sharing (Appendix C.1). Next, we use the trained qϕ to generate 8 responses (thinking traces and final answers) per training sample. For each response, the weight (cid:101)ρk in Eq. (6) is computed from qϕ, πθ0, and, when using the accuracy-based estimator, math/code verifiers from SkyThought. To estimate πθ(Yxx, z) in (cid:101)ρk, we compare three options: naive likelihood method (-L), an accuracy-based method (-Acc, Section 2.2), and geometric mean of token-level probabilities (-GML), as detailed in Appendix C.2. We train the final reasoning model πθ following Eq. (6) under two data settings. 17K: the full BespokeStratos-17k dataset. To enhance efficiency, we create mixed dataset containing, for each original sample, the qϕ-generated response with the highest (cid:101)ρk and the original sample itself. 1K: fixed 1,000sample subset uniformly drawn from the full dataset, where all 8 qϕ-generated responses per sample are used for weighted SFT with (cid:101)ρk. The same 1K subset is reused across related experiments. Main results are reported with 17K, while ablations use both 17K and 1K configurations (Appendix C.3). 4.2 MAIN RESULTS We evaluate our method across four model variants: Qwen3-4B/8B-Base (Tables 1 and 2) and Qwen2.5-7B/32B-Instruct (Tables 3 and 6). Extended results are provided in Appendix F.1. Variational reasoning performance. All methods substantially improve the reasoning ability of the base model, but our approach consistently achieves the best results. As shown in Tables 1 and 2, variational reasoning yields substantial improvements in math, code, and other general domains compared to the base model (e.g., over 160% improvement in math and over 152% in other domains). It also surpasses all baselines in average accuracy (e.g., over 8.5% higher than the strong baseline Bespoke-Stratos-4B that uses the same training data, and over 14% in other domains). 7 Variational Reasoning for Language Models Table 4: Ablation study on the effect of conditioning the proposal distribution on y. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@2 Qwen3-4B-Base Ours-4B w/o 45.30 88.30 81. 4.79 31.67 23.44 5.73 27.29 23.96 27.73 75.63 65.70 23.37 55.71 46.59 Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@ Avg@8 Avg@8 Avg@8 Avg@1 Qwen3-4B-Base Ours-4B w/o 29.10 45.33 40.53 18.54 80.29 67.93 5.46 33.68 16. 1.32 5.79 2.44 36.89 65.53 61.49 Avg 21.38 55.72 48.18 Avg 18.26 46.12 37. Figure 2: Pass@K comparison of baselines versus our method based on Qwen3-4B/8B-Base. Notably, GPQA-Diamond and MMLU-Pro can be considered out-of-distribution test sets, as our training data only cover math and code, whereas they are in-domain for General-Reasoner-4B. Despite this, our method significantly outperforms General-Reasoner on these benchmarks, suggesting that the reasoning improvements from variational reasoning generalize effectively. Additionally, our method demonstrates robustness across different prompt templates. Performance remains consistent between Prompt Template and (denoted as -PA and -PB in Table 6), with both outperforming baselines. Across four model scales, the accuracy-based estimator (-Acc) and the geometric mean of token likelihood estimator (-GML) exhibit similar performance, though the accuracy-based variant shows slight advantage in math-related benchmarks. Pass@K analysis. We report Pass@K results of experiments based on Qwen3-4B/8B-Base for different values of K. Figure 2 reveals two key trends: (1) Our methods advantage increases with larger on complex benchmarks (e.g., LiveCodeBench-Hard), and (2) Performance gaps diminish on simpler tasks (e.g., LiveCodeBench-Easy) and multiple-choice questions (e.g., GPQA-Diamond). This aligns with expectations, as simpler tasks offer limited room for improvement, and multiplechoice formats inherently allow high Pass@K with sufficiently large K. These results underscore the strong potential of variational reasoning in tackling complex tasks. 8 Variational Reasoning for Language Models Table 5: Ablation study on effects of different πθ(Yxx, zk) estimators. Experiments are done in data 1k setting. Acc: accuracy; GML: geometric mean of token likelihood; L: naive likelihood. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@2 Qwen2.5-7B-Instruct Bespoke-Stratos-7B-1K Ours-Acc-7B-1K Ours-GML-7B-1K Ours-L-7B-1K 75.60 77.20 81.30 81.30 79.90 10.94 16.25 19.69 19.27 17.81 7.40 13.96 18.44 18.33 14.17 51.10 53.75 61.64 62.50 59.53 39.91 40.88 45.99 45.48 43.62 Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@8 Avg@8 Avg@8 Avg@8 Avg@1 Qwen2.5-7B-Instruct Bespoke-Stratos-7B-1K Ours-Acc-7B-1K Ours-GML-7B-1K Ours-L-7B-1K 29.99 37.94 41.16 41.35 39.90 62.50 60.37 68.13 68.41 66. 18.20 13.59 21.42 23.30 19.90 3.35 1.22 1.42 2.74 1.93 48.20 56.07 60.94 61.31 58.61 Avg 36.99 40.41 45.41 45.38 43.01 Avg 32.45 33.84 38.62 39.42 37.35 Figure 3: Effects of scaling up the number of thinking traces (K in Algorithm 1) sampled from variational posterior qϕ on the performance of the final reasoning model πθ. Training dynamics. We monitor training loss and gradient norms during training for Qwen3-4B/8Bbase models (see Figure 1). Compared to Bespoke-Stratos-4B/8B, our method yields lower average training loss and fewer gradient norm spikes, indicating greater training stability. We attribute this stability to the πθ/qϕ ratio in (cid:101)ρk. Specifically, for reasoning trace zk, the weight (cid:101)ρk is large when the trace is both high-quality (high πθ(Yxx, zk)) and aligned with the reasoning policy (high likelihood ratio πθ(zkx)/qϕ(zkx, y)). This adaptive weighting promotes stable and effective training. 4.3 ABLATION STUDIES As mentioned in Section 4.1, we conduct ablation studies on both the 17k and 1k data settings to better analyze variational reasoning. Additional ablations are provided in Appendix F.2. Scaling the number of thinking traces zk. We investigate the effect of increasing the number of traces sampled from the variational posterior qϕ (i.e., in Algorithm 1) on the performance of the reasoning model πθ. Experiments are conducted under the 1k data setting and faithful to Algorithm 1. We scale exponentially from 1 to 32, adjusting the batch size to keep optimization steps consistent. Results in Figure 3 suggest that increasing can further enhance model performance. This implies practical trade-off between training computational cost and reasoning accuracy when selecting K. Conditioning on y. We ablate the necessity of conditioning on in data 17k setting. The variant w/o samples thinking traces by the initial reasoning model instead of the variational posterior. Results  (Table 4)  show that removing as the condition negatively affects the performance. Different πθ(Yxx, zk) estimators. We ablate different estimators for πθ(Yxx, zk) used in the weight (cid:101)ρk in data 1k setting  (Table 5)  . We find estimators based on accuracy or geometric mean of token likelihood outperform the naive likelihood by large margin, validating our analysis in Sec. 2.2."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced variational reasoning framework as principled and stable objective for training reasoning models, while clarifying biases in existing SFT/RFT and RL methods. Beyond consistent gains over strong baselines on diverse reasoning tasks, our analysis offers probabilistic perspective for interpreting current approaches. natural future direction is extending training beyond single round (T > 1 in Algorithm 1) and exploring richer posterior design for the answer hint y. 9 Variational Reasoning for Language Models"
        },
        {
            "title": "REFERENCES",
            "content": "Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research, 24(210):155, 2023. URL http: //jmlr.org/papers/v24/22-0364.html. Jörg Bornschein and Yoshua Bengio. Reweighted wake-sleep. In International Conference on Learning Representations (ICLR), 2015. Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015. Edoardo Cetin, Tianyu Zhao, and Yujin Tang. Reinforcement learning teachers of test time scaling. arXiv preprint arXiv:2506.08388, 2025. Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, et al. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. arXiv preprint arXiv:2411.04282, 2024. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025a. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025b. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Jiaxuan Gao, Shusheng Xu, Wenjie Ye, Weilin Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju Wang, and Yi Wu. On designing effective rl reward at training time for llm reasoning. arXiv preprint arXiv:2410.15115, 2024. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. 10 Variational Reasoning for Language Models Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding chalIn Thirty-fifth Conference on Neural Information Processing lenge competence with APPS. Systems Datasets and Benchmarks Track (Round 2), 2021a. URL https://openreview. net/forum?id=sD93GOzH3i5. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021b. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. Matthew Douglas Hoffman, Du Phan, david dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A. Saurous. Training chain-of-thought via latent-variable inference. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=a147pIS2Co. Edward Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=Ouj6p4ca60. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Openreasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, and Wenhu Chen. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Bespoke Labs. reasoning distillation. www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoningdistillation, 2025. Accessed: 2025-01-22. Bespoke-stratos: The unreasonable effectiveness of Variational Reasoning for Language Models Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025a. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. Codei/o: Condensing reasoning patterns via code input-output prediction. arXiv preprint arXiv:2502.07316, 2025b. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=v8L0pN6EOi. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. https: //github.com/ganler/code-r1, 2025. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023. MAA. American mathematics competitions - amc. https://maa.org/, 2023. MAA. American invitational mathematics examination - aime. https://maa.org/, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Optimizing anytime reasoning via budget relative policy optimization. arXiv preprint arXiv:2505.13438, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Yangjun Ruan, Neil Band, Chris Maddison, and Tatsunori Hashimoto. Reasoning to learn from latent thoughts. arXiv preprint arXiv:2503.18866, 2025. Variational Reasoning for Language Models Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259, 2025. Yunhao Tang, Sid Wang, and Rémi Munos. Learning to chain-of-thought with jensens evidence lower bound. arXiv preprint arXiv:2503.19618, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. NovaSky Team. Sky-t1: Train your own o1 preview model within $450. https://novaskyai.github.io/posts/sky-t1, 2025a. Accessed: 2025-01-09. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face, 2024. Qwen Team. Qwen3, April 2025b. URL https://qwenlm.github.io/blog/qwen3/. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629, 2025. Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought. arXiv preprint arXiv:2501.04682, 2025. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Yifei Xu, Tusher Chakraborty, Srinagesh Sharma, Leonardo Nunes, Emre Kıcıman, Songwu Lu, and Ranveer Chandra. Direct reasoning optimization: Llms can reward and refine their own reasoning for open-ended tasks. arXiv preprint arXiv:2506.13351, 2025. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. Variational Reasoning for Language Models An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. John Yang, Kilian Lieret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025b. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025a. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025b. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025. Variational Reasoning for Language Models"
        },
        {
            "title": "A DETAILED DERIVATIONS",
            "content": "In this section, we provide detailed derivations of the conclusions presented in the main text, along with some additional results. A.1 DERIVATION FOR EQ. (4) The ELBO objective induced from Eq. (2) can be rewritten as LELBO(x, Yx, y; πθ, qϕ) Eqϕ(zx,y) [log πθ(Yxx, z)] KL (qϕ(zx, y)πθ(zx)) = Eqϕ(zx,y) [log πθ(z, Yxx)] + (qϕ(zx, y)) = log Pθ(Yxx) KL (qϕ(zx, y)Pθ(zx, Yx)), (13) where H() is entropy function and Pθ(zx, Yx) = πθ(Yxx,z)πθ(zx) Pθ(Yxx) is the true posterior distribution. A.2 DERIVATION FOR EQ. (6) Given the IWAE-style lower bound LK w.r.t. model parameters θ as: ELBO(x, Yx, y; πθ, qϕ) in Eq. (5), we can derive its gradient θLK ELBO(x, Yx, y; πθ, qϕ) 1 log (cid:34) = θEz1:K qϕ(zx,y) (cid:88) k=1 (cid:35) πθ(zk, Yxx) qϕ(zkx, y) = Ez1:K qϕ(zx,y) = Ez1:K qϕ(zx,y) 1 1 (cid:34) (cid:88) (cid:80)K k=1 (cid:80)K k= θπθ(zk,Yxx) qϕ(zkx,y) πθ(zk,Yxx) qϕ(zkx,y) (cid:35) (cid:101)ρkθ log πθ(zk, Yxx) (14) where (cid:101)ρk = k=1 ρk (cid:80)K j=1 ρj and ρk = πθ(zk, Yxx) qϕ(zkx, y) . Using the notations of ρk and (cid:101)ρk, we can further derive the gradient w.r.t. ϕ as: ϕLK ELBO(x, Yx, y; πθ, qϕ) 1 log (cid:34) = ϕEz1:K qϕ(zx,y) (cid:88) k=1 = Ez1:K qϕ(zx,y) log (cid:34)(cid:32) 1 (cid:88) k=1 = Ez1:K qϕ(zx,y) (cid:32) (cid:101)ρk + log (cid:34) (cid:88) k=1 A.3 PROOF OF THEOREM 1 (cid:35) πθ(zk, Yxx) qϕ(zkx, y) (cid:33) (cid:88) k=1 1 (cid:88) k= ρk ϕ log qϕ(zkx, y) (cid:35) (cid:101)ρkϕ log qϕ(zkx, y) (cid:88) k= (cid:33) (cid:35) ϕ log qϕ(zkx, y) . ρk As to the computation of πθ(Yxx, z), there are two unbiased estimators: Likelihood-based estimator: πθ(Yxx, z) = Yx EyU (Yx) [πθ(yx, z)] ; Accuracy-based estimator: πθ(Yxx, z) = Eyπθ(yx,z) [1(y Yx)] , (15) where Yx is the cardinal (number of elements) of Yx, U(Yx) is the uniform distribution on Yx, and 1() is the indicator function. When Yx = 1, i.e., there is unique correct answer expression 15 Variational Reasoning for Language Models y, Zhou et al. (2025) show that the likelihood-based estimator has lower variance (in fact, zero) compared to the accuracy-based one. We now extend this comparison to general cases when Yx > 1: Varlike = Yx2 VaryU (Yx) [πθ(yx, z)] ; Varacc = πθ(Yxx, z) (1 πθ(Yxx, z)) . (16) Note that the variance Varacc of accuracy-based estimator is independent of Yx and the model distribution πθ over different elements in Yx. Assuming that in the worst case where only one element Yx has non-zero probability under πθ, i.e., πθ(yx, z) = πθ(Yxx, z), we have Varworst like Varworst acc max πθ max πθ Varlike = (Yx 1) πθ(Yxx, z)2; Varacc = πθ(Yxx, z) (1 πθ(Yxx, z)) . (17) Here we slightly abuse the notation of maxπθ , since the maximization is taken w.r.t. all πθ under fixed value πθ(Yxx, z). As seen, Varworst acc holds when the model accuracy (condition on x, z) satisfies like Varworst 1 Yx , (18) πθ(Yxx, z) which almost always holds for Yx 1. A.4 DERIVATION FOR EQ. (9) Now we derive the gradient of the forward KL divergence DKL(Pθ(zx, Yx)qϕ(zx, y)) w.r.t. ϕ: ϕDKL(Pθ(zx, Yx)qϕ(zx, y)) = ϕEPθ(zx,Yx) [log qϕ(zx, y)] = ϕEπθ(zx) (cid:20) πθ(Yxx, z) Pθ(Yxx) (cid:21) log qϕ(zx, y) Ez1:M πθ(zx) (cid:34) (cid:88) (cid:35) (cid:101)wmϕ log qϕ(zmx, y) ϕLM forward, (19) where (cid:101)wm = m=1 wm j=1 wj (cid:80)M and wm = πθ(Yxx, zm). A.5 CONNECTION TO MORE GENERAL RL REWARD SHAPING In the literature on reinforcement learning with verifiable rewards (RLVR), various strategies for reward shaping have been proposed, many of which can be expressed as R(x, y) = α if Yx; β if / Yx Yformat; γ otherwise, (20) where α, β, γ are hyperparameters, Yformat is the set of answers that correctly follow required format (e.g., boxed{}) and is typically independent of x. Apparently, Yx Yformat holds for any x. Then we can derive the gradient of training objective under the general reward shaping R(x, y) as θLgeneral-RL(x, πθ) θEπθ(zx)Eπθ(yx,z) [R(x, y)] = Eπθ(zx)Eπθ(yx,z) [R(x, y) (θ log πθ(zx)+θ log πθ(yx, z))] only w.r.t. ===== πθ(zx) Eπθ(zx)Eπθ(yx,z) [R(x, y) θ log πθ(zx)] (21) = Eπθ(zx) [((α β) πθ(Yxx, z) + (β γ) πθ(Yformatx, z)) θ log πθ(zx)] = (α β) Pθ(Yxx) θDKL(P sg θ (zx, Yx)πθ(zx)) + (β γ) Pθ(Yformatx) θDKL(P sg θ (zx, Yformat)πθ(zx)), 16 Variational Reasoning for Language Models where Pθ(Yformatx) is the probability that the output answers follow the required format. It is easy to know that the optimal solution for Eq. (21) can be written as: π θ (zx) = (α β) sg (α β) sg θ (z, Yxx) + (β γ) sg θ (Yxx) + (β γ) sg θ (z, Yformatx) θ (Yformatx) . (22) Remark. When β = γ, i.e., there is no format reward, the optimization problem in Eq. (21) degrades to Eq. (11). When α > β > γ, the model πθ(zx) will tend to hack reward function on hard problems (i.e., low Pθ(Yxx)) that can easily follow format (i.e., high Pθ(Yformatx)), where πθ(zx) will seek modes of sg θ (zx, Yformat). Besides, there may be an intuition that setting β < 0 could alleviate reward hacking, however, as shown in Eq. (21), the optimization only depends on the relative values of α β and β γ. Now we show that it is straightforward to debias Pθ(Yxx) and Pθ(Yformatx) in Eq. (21). Specifically, we can rewrite the reward function as (note that reward functions are equivalent up to any constant): R(x, y) = (α β) 1(y Yx) + (β γ) 1(y Yformat). (23) Then the debiased version of reward function is 1(y Yx) + (β γ) θ (Yformatx) sg 1(y Yformat), (24) where in practice sg θ (Yformatx) can be approximated by the ratio of correct answers (i.e., model accuracy) and the ratio of correct format for each batch of RL rollouts (larger rollout number could lead to more accurate estimation). After using the debiased reward function R(x, y), the optimal solution of π R(x, y) = (α β) sg θ (Yxx) θ (Yxx) and sg θ (zx) becomes (α β) sg π θ (zx) = θ (zx, Yx) + (β γ) sg α γ θ (zx, Yformat) . (25) A.6 SPECIAL CASES IN EQ. (6) Special case I: qϕ(zx, y) = πsg θ (zx). In this case, we can simplify the gradient estimation as θLK ELBO(x, Yx; πθ, πsg θ ) = Ez1:K πθ(zx) where (cid:101)wk = wk j=1 wj (cid:80)K and wk = πθ(Yxx, zk), (cid:101)wkθ log πθ(zk, Yxx) (cid:35) (cid:34) (cid:88) k=1 (26) which can be regarded as normalized version of VeriFree (Zhou et al., 2025). Special case II: = 1. In this case L1 ELBO = LELBO and we can simplify the gradient estimation as θLELBO(x, Yx, y; πθ, qϕ) = Ezqϕ(zx,y) [θ log πθ(z, Yxx)] ; (cid:19) (cid:20)(cid:18) ϕLELBO(x, Yx, y; πθ, qϕ) = Ezqϕ(zx,y) log πθ(z, Yxx) qϕ(zx, y) ϕ log qϕ(zx, y) (cid:21) . (27) A.7 MORE DERIVATIONS FOR EQ. (11) Now we investigate the gradient of binary reward RL w.r.t. πθ(yx, z): θLbi-RL(x, πθ) θEπθ(zx)Eπθ(yx,z) [1(y Yx)] = Eπθ(zx)Eπθ(yx,z) [1(y Yx) (θ log πθ(zx)+θ log πθ(yx, z))] only w.r.t. ====== πθ(yx,z) Eπθ(zx)Eπθ(yx,z) [1(y Yx) θ log πθ(yx, z)] (28) = Eπθ(zx) [θπθ(Yxx, z)] , where the optimal solution is straightforward that z, there is π θ (Yxx, z) = 1. However, this optimal solution is usually unachievable, since it requires the model to return 100% correct answers independent of the thinking process z. Variational Reasoning for Language Models A.8 CONNECTION TO REINFORCEMENT LEARNING TEACHERS In the derivation of our method, the ELBO objective in Eq. (4) minimizes reverse KL divergence DKL (qϕ(zx, y)Pθ(zx, Yx)). As analyzed in Section 2.3, we propose to optimize qϕ(zx, y) using the forward KL divergence DKL(Pθ(zx, Yx)qϕ(zx, y)), which shares the same optimal solution. Alternatively, we can also optimize the reverse KL divergence by policy gradient method as follows: ϕLELBO = ϕDKL (qϕ(zx, y)Pθ(zx, Yx)) = ϕEqϕ(zx,y)[log πθ(z, Yxx) log qϕ(zx, y)] = Eqϕ(zx,y) (cid:104) log πθ(z, Yxx)ϕ log qϕ(zx, y) log qϕ(zx, y)ϕ log qϕ(zx, y) (cid:105) (29) (cid:34)(cid:32) = Eqϕ(zx,y) log πθ(Yxx, z) log (cid:124) (cid:123)(cid:122) reward (cid:33) (cid:35) ϕ log qϕ(zx, y) . qϕ(zx, y) πθ(zx) (cid:125) More concisely, the reverse KL divergence can be alternatively minimized via reinforcement learning using log πθ(Yxx, z) log qϕ(zx,y) πθ(zx) as the reward function. This derivation establishes connection to Reinforcement Learning Teachers (RLTs) (Cetin et al., 2025), who focus on training reasoning LLMs to act as teachers for distilling new students. Their approach introduces RLTs optimized specifically for effective student distillation. RLTs are trained by GRPO using dense rewards obtained by feeding each explanation to the student and evaluating its understanding of the solution. Specifically, the dense reward in RLT combines two components: one measuring the students likelihood of reaching the correct solution (analogous to log πθ(Yxx, z)), and another regularizing the teachers explanation to remain coherent from the students perspective given only its prior knowledge and the question (analogous to log qϕ(zx,y) πθ(zx) ). While RLT employs an intuitively designed reward, our work provides rigorous theoretical justification from variational inference perspective. Furthermore, we enhance the method with tighter IWAEstyle lower bound and an accuracy-based estimator, as detailed in Section 2.2."
        },
        {
            "title": "B RELATED WORK",
            "content": "SFT and RL methods for reasoning. Reasoning has emerged as central capability of LLMs, driving advances in domains such as mathematics, programming, and scientific discovery (Jaech et al., 2024; Comanici et al., 2025; Team et al., 2025). Among the approaches developed to strengthen these abilities, SFT and RL have become the two dominant paradigms (Uesato et al., 2022; Rafailov et al., 2023; Guha et al., 2025; Hu et al., 2025; Hochlehnert et al., 2025). Building on the DeepSeek-R1 framework (Shao et al., 2024; Guo et al., 2025), range of new RL algorithms have been proposed, including Dr. GRPO (Liu et al., 2025), DAPO (Yu et al., 2025a), REINFORCE++ (Hu, 2025), VinePPO (Kazemnejad et al., 2024), and VAPO (Yue et al., 2025b). In parallel, extensive empirical studies have explored the design space of RL for reasoning (Zeng et al., 2025; Team et al., 2025), focusing on dimensions such as curriculum learning (Wen et al., 2025; Luo et al., 2025) and reward design (Gao et al., 2024; Cui et al., 2025a; Ma et al., 2023; Qi et al., 2025). While early progress has centered on mathematical reasoning, recent work has extended RL-based methods to code and software engineering tasks (Liu & Zhang, 2025; Xie et al., 2025; Wei et al., 2025; Yang et al., 2025; Chen et al., 2025; Li et al., 2025b), as well as to agentic problem-solving scenarios (Wang et al., 2025; Jin et al., 2025; Jiang et al., 2025; Xue et al., 2025). Decomposing thinking and answering processes. Traditional studies on LLM reasoning ability often treat model responses holistically. In contrast, recent line of research explicitly decomposes the LLM response into thinking trace and final answer y, given question (Chen et al., 2024; Xiang et al., 2025; Zhou et al., 2025). This decomposition offers several novel and useful perspectives. 18 Variational Reasoning for Language Models Zhou et al. (2025) propose VeriFree, which directly optimizes Pθ(yx) = Eπθ(zx)[πθ(yx, z)] using policy gradient. Their algorithm simultaneously optimizes πθ(zx) via policy gradient methods with πθ(yx, z) as reward, and performs weighted SFT on πθ(yx, z). This approach demonstrates strong performance in general domains where rule-based verifiers are typically unavailable. Subsequent works (Yu et al., 2025b; Xu et al., 2025) further improve upon this by reshaping the reward, e.g., intuitively replacing the product of token probabilities with the mean when computing πθ(yx, z). Chen et al. (2024) introduce LaTRO, formulating reasoning as sampling from latent distribution q(zx) and optimizing log Pθ(yx) via variational manner. Their derived lower bound is: log Pθ(yx) Eq(zx) [log πθ(yx, z)] DKL (q(zx) πθ(zx)) . They set the proposal distribution q(zx) to πθ(zx), resulting in reinforcement learning algorithm where log πθ(yx, z) serves as the reward. Tang et al. (2025) and Ruan et al. (2025) tighten this bound using ideas similar to IWAE (Burda et al., 2015). more natural choice for the variational distribution is the true posterior Pθ(zx, y), though it is intractable. Hoffman et al. (2023) use MCMC to sample from the posterior, while Hu et al. (2024) employ GFlowNets (Bengio et al., 2023) to fine-tune an LLM to approximate it. Both methods use an EM-like algorithm to optimize the ELBO of log Pθ(yx). Our approach uses forward KL divergence to train variational posterior and derives novel objective based on tighter IWAE-style bound. Additionally, we propose an accuracy-based estimator for πθ(Yxx, zk), instead of the likelihood-based estimator used in Zhou et al. (2025). We also build connections to other mainstream finetuning algorithms that enhance reasoning, such as RFT and GRPO."
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "In this section, we detail the training procedure used in our method. Our framework builds on LLaMA-Factory (Zheng et al., 2024). By default, SFT averages token-level cross-entropy over all valid tokens in batch. However, as shown in Section 2, our variables and are defined at the sentence level. To align with this, we modify the objective: instead of normalizing by the number of valid tokens, we sum the loss over all tokens and divide by constant equal to the average response length in the training set (precomputed offline). This change parallels the difference between GRPO (Shao et al., 2024) and Dr. GRPO (Liu et al., 2025), thus we name this slight modification as Dr. SFT. Both our models and Bespoke-Stratos-4B/8B are trained with this modified objective. We further extend the framework with weighted SFT, as the original LLaMA-Factory does not support weighting. This feature is essential for parts of our method that require weighted training. All experiments are conducted on NVIDIA H100 GPUs. C.1 TRAINING THE INITIAL REASONING MODEL AND VARIATIONAL POSTERIOR For training, we use the following settings: 1. Initial reasoning model πθ0 (following Labs (2025)): batch_size=96, cutoff_len=16384 Optimizer: AdamW (Loshchilov & Hutter, 2019) with adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1.0e-8, weight_decay=0 Learning rate schedule: cosine with warmup_ratio=0.1 learning_rate=1.0e-5, max_grad_norm=1.0 Training for 3 epochs Precision: bfloat16 Baselines (Bespoke-Stratos-7B/32B and Bespoke-Stratos-4B/8B) are trained with the same setup 2. Variational posterior qϕ: batch_size=16 Optimizer: adam_beta1=0.9, adam_epsilon=1.0e-8, weight_decay=1.0e-4 AdamW with adam_beta2=0.95, Variational Reasoning for Language Models warmup_ratio=0.05 Training for 10 epochs All other hyperparameters follow those used for πθ0 C.2 DETAILS OF SAMPLING FROM VARIATIONAL POSTERIOR Using the trained variational posterior qϕ, we sample 8 reasoning traces (including final answers) for each question in Bespoke-Stratos-17k with vLLM (Kwon et al., 2023), using temperature=0.7, top_p=1.0, top_k=-1, max_tokens=32764, and dtype=bfloat16. After obtaining the sampled reasoning traces, we compute the importance weight (cid:101)ρk for each question-thinking-answer triplet using the pre-trained initial reasoning model πθ0 and the variational posterior qϕ. This is done in forward mode by evaluating the log-likelihoods under both models, without requiring backpropagation, which is efficient. To estimate the term πθ(Yx x, z) used in (cid:101)ρk, we adopt an accuracy-based estimator. Specifically, for each question and each sampled thinking trace, we use πθ to generate 8 answers under the same sampling configuration as above. The correctness of these answers is evaluated using the math/code verifiers from SkyThought, and the average accuracy is taken as the estimate of πθ(Yx x, z). Experiments utilizing the accuracy-based estimator are labeled as -Acc. Additionally, we employ the geometric mean of token-level probabilities under πθ(Yx x, z) as an alternative and intuitive estimator. This approach mitigates the inherent length bias present in the strict definition of πθ(Yx x, z), which computes the product of token probabilities and consequently assigns excessively small values to longer reasoning traces. This estimator provides an intuitive approximation without requiring an external verifier. Experiments utilizing the estimator based on geometric mean are labeled as -GML, while those with the naive estimator are labeled as -L. This process results in weighted dataset where each sample consists of question-thinking-answer triplet along with its corresponding weight (cid:101)ρk, which will be utilized in subsequent training stages. C.3 DETAILS OF TRAINING FINAL REASONING MODEL To train the final reasoning model πθ, we adopt the following procedure. For the 17k data setting, we select, for each question, the reasoning trace with the highest importance weight (cid:101)ρk among the 8 samples sampled from the variational posterior. In experiments using the accuracy-based estimator, we pair the selected reasoning trace with randomly chosen verified answer generated by the initial reasoning model. For other estimators, the original answer from the dataset is retained. The resulting synthetic data is then mixed with the original Bespoke-Stratos-17k dataset. We maintain the same training configuration as used for the initial reasoning model, with one exception: the batch size is increased to batch_size=192. This adjustment ensures that the total number of optimization steps remains consistent with baseline models (e.g., Bespoke-Stratos-32B), as the mixed dataset is twice the size of the original. For the 1k data setting, the baseline model (e.g., Bespoke-Stratos-7B-1K) is trained with the following configuration: We adopt batch_size=16 and cutoff_len=32768. We use the AdamW optimizer (Loshchilov & Hutter, 2019) with parameters adam_beta1=0.9, adam_beta2=0.95, adam_epsilon=1.0e-8, and weight_decay=1.0e-4. cosine learning rate schedule is applied with warmup_ratio=0.1, alongside learning rate of 1.0e-5 and gradient clipping at max_grad_norm=1.0. Training is conducted for 5 epochs. In our method under the 1k setting, we do not combine with the original dataset. Instead, we use all 8 reasoning traces, weighted by (cid:101)ρk, which is faithful to Algorithm 1. To match the number of optimization steps in the baseline, we proportionally adjust the batch size while keeping all other hyperparameters unchanged. 20 Variational Reasoning for Language Models"
        },
        {
            "title": "D DETAILS OF EVALUATION",
            "content": "We conduct all evaluations using SkyThought, specifically at commit 0d190f1.3 Team (2025b) suggest avoiding greedy decoding for models with long thinking traces. Thus, responses are sampled from the models using temperature=0.7 and top_p=1.0. generous token budget of max_tokens=38912 is allocated to accommodate lengthy outputs. To maximize reproducibility, we perform inference using dtype=float32, accepting potential decrease in speed for improved consistency. For model parallelism, we configure tensor_parallel_size=4 for 4B/7B/8B models and tensor_parallel_size=8 for the 32B models. We choose vLLM (Kwon et al., 2023) as the inference backend. For models based on Qwen2.5, we use vllm==0.7.0, while for Qwen3-based models, we use vllm==0.8.4. Although we anticipate that these version differences have negligible impact on evaluation accuracy, we document them here to ensure full reproducibility. 3https://github.com/NovaSky-AI/SkyThought 21 Variational Reasoning for Language Models"
        },
        {
            "title": "E PROMPT TEMPLATES",
            "content": "In Section 2, we abstractly introduced how we define the prompt patterns used in the reasoning model πθ and variational posterior qϕ. In this section, we provide details of the prompt templates used in practice, as shown below. Prompt template (PA) for variational posterior qϕ Your role as an assistant involves providing precise and accurate solutions before providing detailed explanations with your full work showing your systematic thinking process leading to each solution. Your explanations should show how you engaged in comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Solution and Explanation. In the Solution section, present your well-thought solution that accurately answers the question. The solution should remain logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <begin_of_solution> {final formatted, precise, and clear solution} <end_of_solution>. In the Explanation section, comprehensively detail your reasoning process using the specified format: <begin_of_explanation> {explanation with steps separated with nn} <end_of_explanation> Each step should show detailed considerations leading to your solutions such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. Prompt template (PB) for variational posterior qϕ Your role as an assistant involves reconstructing the internal reasoning process that connects provided question to its correct answer. Your task is to methodically reverse-engineer the logical steps, demonstrating full cycle of analysis, summarization, idea generation, verification, error correction, and iterative refinement. Please structure your response into two distinct parts: Solution and Thought. In the Solution section, present the given correct answer in precise and clear format: <begin_of_solution> {provided correct solution} <end_of_solution>. In the Thought section, articulate the step-by-step cognitive journey that leads to the solution. Use the specified format: <begin_of_thought> {detailed thought process with steps separated by nn} <end_of_thought>. Each step should reflect analytical breakdowns, synthesis of key points, generation of logical pathways, validation of each steps accuracy, refinement of any missteps, and reassessment of previous conclusions. The focus is solely on depicting the internal, structured thinking that arrives at the provided solution. Prompt template for reasoning model πθ Your role as an assistant involves thoroughly exploring questions through systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought 22 Variational Reasoning for Language Models section, detail your reasoning process using the specified format: <begin_of_thought> {thought with steps separated with nn} <end_of_thought> Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <begin_of_solution> {final formatted, precise, and clear solution} <end_of_solution> Now, try to solve the following question through the above guidelines: 23 Variational Reasoning for Language Models Table 6: Performance of models trained from Qwen2.5-7B-Instruct trained on Bespoke-Stratos-17k. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@ Qwen2.5-7B-Instruct Bespoke-Stratos-7B RLT-7B Ours-PA-GML-7B Ours-PA-Acc-7B Ours-PB-GML-7B Ours-PB-Acc-7B 75.60 82.20 84.30 85.30 83.40 84.00 84.80 10.94 19.58 22.81 24.17 22.50 22.08 23.96 7.40 19.48 19.48 20.42 20.83 20.42 19.69 51.10 63.28 64.84 68.20 65.39 66.80 65.00 39.91 45.03 46.43 46.88 47.55 46.29 48. Method GPQA-D LCB-E LCB-M LCB-H Avg@8 Avg@8 Avg@8 Avg@8 MMLU-Pro Avg@1 Qwen2.5-7B-Instruct Bespoke-Stratos-7B RLT-7B Ours-PA-GML-7B Ours-PA-Acc-7B Ours-PB-GML-7B Ours-PB-Acc-7B 29.99 39.02 41.60 43.62 43.56 41.60 41. 62.50 69.30 72.32 74.52 74.45 74.73 75.07 18.20 23.06 25.06 27.79 29.43 28.46 29.49 3.35 2.95 3.66 4.78 4.37 4.57 4.68 48.2 60.59 61.28 60.88 60.72 61.14 61.28 Avg 36.99 45.91 47.57 48.99 47.94 47.92 48. Avg 32.45 38.98 40.78 42.32 42.51 42.10 42."
        },
        {
            "title": "F EXTENDED RESULTS",
            "content": "F.1 EXTENDED MAIN RESULTS Due to space constraints in the main paper, we present extended evaluation results in this section. We report the evaluation results for models fine-tuned from Qwen2.5-7B-Instruct on the BespokeStratos-7B dataset in Table 6. All variants of our method outperform all baselines in terms of average accuracy, demonstrating the superiority of the variational reasoning approach. Notably, the two prompt templates used for the variational posterior qϕ yield similar results, indicating that our method is robust to the choice of template. We attribute this robustness to the fact that the posterior is obtained by fine-tuning the model qϕ, rather than through prompt engineering alone, thereby reducing the sensitivity to specific prompt formulations. Additionally, we plot the distributions of the thinking token length versus the log-likelihood ratio log πθ(zkx) qϕ(zkx,y) and the answer token length versus the log-likelihood of the answer log πθ(Yx x, zk) in Figure 4. The results reveal strong correlations between these variables, indicating the presence of length biases. This observation further justifies the use of estimators based on accuracy or the geometric mean of token likelihood, rather than the naive likelihood. Another noteworthy observation is that our evaluation results for General-Reasoner-4B (see Table 1) differ from those reported by Ma et al. (2025) in their original paper, despite using their officially released checkpoints and provided prompt template. This discrepancy can be attributed to several factors: (1) different evaluation frameworks: we employ SkyThought whereas they utilize simple-evals4; (2) different sampling configurations: we use temperature=0.7 and max_tokens=38912, while they primarily employ greedy decoding (i.e., temperature=0) except for AIME24 and AIME25, along with more constrained token budget of max_tokens=8192. To facilitate fair comparison, we provide results on the common benchmarks that we and Ma et al. (2025) both utilize, comparing our model accuracy with their officially reported values (see Table 7). As demonstrated, our method continues to outperform General-Reasoner-4B by significant margin (where indicates their officially reported accuracy). 4https://github.com/openai/simple-evals 24 Variational Reasoning for Language Models Figure 4: Density maps of the thinking token length versus the log-likelihood ratio log πθ(zkx) qϕ(zkx,y) (left), and the answer token length versus the log-likelihood of the answer log πθ(Yx x, zk) (right). Table 7: Performance of models trained from Qwen3-4B-Base. All models are trained on BespokeStratos-17k except for General-Reasoner-4B. Here, denotes accuracy values officially reported by Ma et al. (2025), rather than results obtained through our own evaluation. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 Qwen3-4B-Base General-Reasoner-4B General-Reasoner-4B Bespoke-Stratos-4B Ours-PB-GML-4B Ours-PB-Acc-4B 45.30 71.70 80.6 84.70 87.30 88.30 4.79 19.06 20.0 27.29 33.54 31.67 5.73 16.77 15.4 24.17 26.77 27. 27.73 55.00 60.0 70.16 74.06 75.63 Method OlympiadBench GPQA-D MMLU-Pro Qwen3-4B-Base General-Reasoner-4B General-Reasoner-4B Bespoke-Stratos-4B Ours-PB-GML-4B Ours-PB-Acc-4B 23.37 45.18 47.7 50.45 54.45 55.71 29.10 40.97 42.9 44.95 45.52 45. 36.89 61.36 62.8 63.03 65.52 65.53 F.2 EXTENDED ABLATION STUDIES This section presents extended ablation studies analyzing: the impact of different training data sources and different ways of data usage (Appendix F.3); the effect of data overlap between variational posterior training and reasoning model training (Appendix F.4); the comparison between Dr. SFT and naive SFT (Appendix F.4); and the influence of cutoff length during training (Appendix F.6). F.3 EFFECTS OF DIFFERENT FINAL DATA SOURCES AND WAYS OF DATA USAGE In our main experiments, we prioritize training efficiency by using the 17k data setting, selecting the variational reasoning trace zk with the highest weight (cid:101)ρk and mixing it with the original BespokeStratos-17k data (which results in double data sizes), rather than using all eight traces sampled from the variational posterior. To evaluate the impact of this simplification, we conduct ablation studies under the 1k data setting. We compare variants that either mix or do not mix with the original data, and that use either single-best trace selection (-S) or weighted multiple traces (-M). Results are shown in Table 8. The best performance is achieved by the variant that uses weighted multiple reasoning traces without mixing with the original data. This suggests that, when computational cost is not constraint, the optimal approach is to utilize all reasoning traces from the 25 Variational Reasoning for Language Models Table 8: Ablation study on the effects of different final data sources (only sampled from variational posterior vs. mixed) and different ways to use samples from variational posterior to train reasoning models (single best reasoning trace selection (-S) vs. weighted multiple reasoning traces (-M)). This ablation is done in data 1k setting. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@ Avg@2 Qwen2.5-7B-Instruct Bespoke-Stratos-7B-1K Ours-M-7B-1K w/o Mix Ours-S-7B-1K w/o Mix 75.60 77.20 79.80 81.30 81.10 80.40 10.94 16.25 18.65 19.69 19.90 17.92 7.40 13.96 16.98 18.44 17.08 17.81 51.10 53.75 60.55 61.64 60.39 59. 39.91 40.88 44.81 45.99 43.92 43.18 Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@8 Avg@8 Avg@8 Avg@8 Avg@1 Qwen2.5-7B-Instruct Bespoke-Stratos-7B-1K Ours-M-7B-1K w/o Mix Ours-S-7B-1K w/o Mix 29.99 37.94 42.80 41.16 40.34 39.96 62.50 60.37 65.73 68.13 64.84 65.04 18.20 13.59 18.99 21.42 18.20 18.27 3.35 1.22 1.93 1.42 2.13 1.32 48.2 56.07 60.49 60.94 59.54 59.87 Avg 36.99 40.41 44.16 45.41 44.48 43.67 Avg 32.45 33.84 37.99 38.62 37.01 36.89 variational posterior, weighted by (cid:101)ρk, for training the final reasoning model. Another interesting observation is that for the single-trace method, data mixing improves performance, whereas for the weighted multi-trace method, mixing slightly degrades performance. This may indicate that the weighted ensemble of variational traces already provides sufficient information, making the original data redundant in this scenario. F.4 EFFECTS OF DATA OVERLAP In the 17k data setting, the variational posterior qϕ is trained on all 17k samples and generates thinking traces for the same set of 17k samples, which are subsequently used to train the final reasoning model πθ. In other words, both the variational posterior and the final reasoning model are trained on the same set of questionanswer pairs. An interesting question is how our method performs in the absence of data overlap. To investigate this, we design two experimental settings: the first is the same 1k data setting introduced earlier; the second is constructed by splitting Bespoke-Stratos-17k into two non-overlapping subsets: one contains 15,710 samples (approximately 16k) and the other contains 1k samples. In the latter setting, we train the variational posterior qϕ and the initial reasoning model πθ0 on the 16k subset. We then use the trained qϕ to sample thinking traces for the 1k subset, and employ both qϕ and πθ0 to compute (cid:101)ρk. Finally, the final reasoning model πθ is trained on the 1k subset using weighted multiple reasoning traces. This setting is referred to as w/o Overlap. The results are presented in Table 9. Both the overlap and non-overlap variants exhibit similar performance in terms of average accuracy, and both outperform the baseline, Bespoke-Stratos-7B1K. This suggests that the trained variational posterior generalizes reasonably well and can be applied to broader scenarios. F.5 COMPARING DR. SFT WITH NAIVE SFT As detailed in Appendix C, we employ slightly modified objective function, Dr. SFT, where the loss is defined as the sum of all valid token losses normalized by constant, rather than the mean loss across valid tokens in the batch. We conduct an ablation study comparing this Dr. SFT approach against naive SFT when training the final reasoning model πθ. 26 Variational Reasoning for Language Models Table 9: Ablation study on the effects of data overlap between variational posterior training and reasoning model training. This ablation is done in data 1k setting. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@2 Qwen2.5-7B-Instruct Bespoke-Stratos-7B-1K Ours-7B-1K w/o Mix w/o Overlap w/o Mix w/o Overlap 75.60 77.20 79.80 81.30 80.60 82.00 10.94 16.25 18.65 19.69 20.83 18.75 7.40 13.96 16.98 18.44 18.44 16.88 51.10 53.75 60.55 61.64 61.17 63.52 39.91 40.88 44.81 45.99 44.81 47.11 Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@8 Avg@8 Avg@8 Avg@8 Avg@1 Qwen2.5-7B-Instruct Bespoke-Stratos-7B-1K Ours-7B-1K w/o Mix w/o Overlap w/o Mix w/o Overlap 29.99 37.94 42.80 41.16 38.19 39.65 62.50 60.37 65.73 68.13 66.00 68. 18.20 13.59 18.99 21.42 19.84 19.96 3.35 1.22 1.93 1.42 1.12 2.44 48.2 56.07 60.49 60.94 60.65 60.36 Avg 36.99 40.41 44.16 45.41 45.17 45.65 Avg 32.45 33.84 37.99 38.62 37.16 38.25 The results are presented in Table 10. Both variants demonstrate comparable performance, with less than 2% difference in average accuracy, and both outperform the baseline. This allows us to conclude that the primary performance improvement stems from the variational reasoning mechanism rather than from this minor modification to the objective function. F.6 ABLATION STUDY ON THE EFFECTS OF CUTOFF LENGTH WHEN TRAINING In our main experiments (17k data setting), we use cutoff length of cutoff_len=16384. To investigate the impact of this hyperparameter, we conduct an ablation study comparing two variants: one using the default cutoff_len=16384 (denoted as -Len16k) and another with cutoff_len=32768 (denoted as -Len32k). The results are presented in Table 11. Both variants exhibit similar performance. We further analyze the average completion token lengths on several evaluation benchmarks (see Table 12). All methods produce significantly longer reasoning traces compared to the Qwen2.5-7B-Instruct, with our methods generating slightly longer thinking traces. Notably, the 16k and 32k cutoff variants result in similar generation lengths during inference. This indicates that increasing the cutoff length beyond 16k has minimal effect on the models output. Therefore, we can confidently use the 16k setting for better training efficiency without sacrificing performance. Variational Reasoning for Language Models Table 10: Ablation study comparing Dr. SFT and Naive SFT. This ablation is done in data 17k setting. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@32 Avg@ Qwen2.5-7B-Instruct Bespoke-Stratos-7B RLT-7B Ours-PA-GML-7B w/ naive SFT Ours-PA-Acc-7B w/ naive SFT 75.60 82.20 84.30 85.30 84.00 83.40 84.10 10.94 19.58 22.81 24.17 22.60 22.50 23.02 7.40 19.48 19.48 20.42 20.10 20.83 21.04 51.10 63.28 64.84 68.20 65.31 65.39 67.66 39.91 45.03 46.43 46.88 49.11 47.55 46. Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@8 Avg@8 Avg@8 Avg@8 Avg@1 Qwen2.5-7B-Instruct Bespoke-Stratos-7B RLT-7B Ours-PA-GML-7B w/ naive SFT Ours-PA-Acc-7B w/ naive SFT 29.99 39.02 41.60 43.62 42.49 43.56 42. 62.50 69.30 72.32 74.52 74.73 74.45 75.69 18.20 23.06 25.06 27.79 25.85 29.43 27.79 3.35 2.95 3.66 4.78 3.66 4.37 5.39 48.2 60.59 61.28 60.88 61.41 60.72 61.22 Avg 36.99 45.91 47.57 48.99 48.23 47.94 48. Avg 32.45 38.98 40.78 42.32 41.63 42.51 42.44 Table 11: Ablation study on effects of cutoff length used in training. This ablation is done in data 17k setting. Len16k: cutoff_len=16384; Len32k: cutoff_len=32768. The best and second-best results are highlighted using bold text and underlined text, respectively. Method MATH500 AIME24 AIME25 AMC23 OlympiadBench Avg@2 Avg@32 Avg@32 Avg@ Avg@2 Qwen2.5-7B-Instruct Bespoke-Stratos-7B RLT-7B Ours-PA-GML-7B-Len16k Ours-PA-GML-7B-Len32k 75.60 82.20 84.30 85.30 84.10 10.94 19.58 22.81 24.17 22.08 7.40 19.48 19.48 20.42 20.94 51.10 63.28 64.84 68.20 66. 39.91 45.03 46.43 46.88 48.37 Method GPQA-D LCB-E LCB-M LCB-H MMLU-Pro Avg@8 Avg@8 Avg@8 Avg@8 Avg@1 Qwen2.5-7B-Instruct Bespoke-Stratos-7B RLT-7B Ours-PA-GML-7B-Len16k Ours-PA-GML-7B-Len32k 29.99 39.02 41.60 43.62 42.49 62.50 69.30 72.32 74.52 74.93 18.20 23.06 25.06 27.79 28.58 3.35 2.95 3.66 4.78 4.37 48.2 60.59 61.28 60.88 61.64 Avg 36.99 45.91 47.57 48.99 48.46 Avg 32.45 38.98 40.78 42.32 42.40 Table 12: Average completion token length of models trained from Qwen2.5-7B-Instruct. Method MATH500 AIME24 AIME25 AMC23 MMLU-Pro Avg@32 Avg@32 Avg@ Avg@1 Avg@2 Qwen2.5-7B-Instruct Bespoke-Stratos-7B RLT-7B Ours-PA-GML-7B Ours-PA-GML-7B-Len32k Ours-PA-Acc-7B Ours-PB-GML-7B Ours-PB-Acc-7B 564 5801 5508 5677 5688 5688 5803 5787 1027 15769 15769 16471 16747 16747 16615 16696 849 10921 10986 11338 11531 11531 11080 531 3889 3942 3924 3965 3965 4052 3974 1270 18413 18143 18299 18170 18170 18479 18651 28 Variational Reasoning for Language Models"
        },
        {
            "title": "G EXAMPLES OF GENERATED RESPONSES",
            "content": "In this section, we provide examples of responses from the variational posterior qϕ and the final reasoning model πθ trained based on Qwen3-4B-Base. The provided example of the variational posterior qϕ is based on the variant with prompt template B, and the question-answer pair is one of the samples from the Bespoke-Stratos-17k dataset. The provided example of the final reasoning model πθ is based on one of the samples in MATH500. See the provided examples below. Example of variational posterior qϕ Prompt: <im_start>system Your role as an assistant involves reconstructing the internal reasoning process that connects provided question to its correct answer. Your task is to methodically reverse-engineer the logical steps, demonstrating full cycle of analysis, summarization, idea generation, verification, error correction, and iterative refinement. Please structure your response into two distinct parts: Solution and Thought. In the Solution section, present the given correct answer in precise and clear format: <begin_of_solution> {provided correct solution} <end_of_solution>. In the Thought section, articulate the step-by-step cognitive journey that leads to the solution. Use the specified format: <begin_of_thought> {detailed thought process with steps separated by nn} <end_of_thought>. Each step should reflect analytical breakdowns, synthesis of key points, generation of logical pathways, validation of each steps accuracy, refinement of any missteps, and reassessment of previous conclusions. The focus is solely on depicting the internal, structured thinking that arrives at the provided solution.<im_end> <im_start>user Return your final response within boxed{}. figure is an equiangular parallelogram if and only if it is $textbf{(A)} text{rectangle}qquad textbf{(B)} text{regular polygon}qquad textbf{(C)} text{rhombus}qquad textbf{(D)} text{square}qquad textbf{(E)} text{trapezoid}$<im_end> <im_start>assistant <begin_of_solution> To determine the correct answer, we need to analyze the definitions and properties of the given options in relation to an equiangular parallelogram. 1. **Definitions and Properties**: - **parallelogram** is quadrilateral with both pairs of opposite sides parallel. - **Equiangular** means all angles are equal. In quadrilateral, this means each angle is 90 degrees (since the sum of angles in quadrilateral is 360 degrees, and 360/4 = 90). - **rectangle** is parallelogram with all angles equal to 90 degrees. - **regular polygon** (for quadrilaterals) is square, which is both equiangular and equilateral. - **rhombus** is parallelogram with all sides equal, but not necessarily equiangular. - **square** is special case of both rectangle and rhombus, being both equiangular and equilateral. - **trapezoid** has at least one pair of parallel sides, but not necessarily two pairs. 2. **Analysis**: 29 Variational Reasoning for Language Models - An equiangular parallelogram must have all angles equal to 90 degrees, making it rectangle. - The \"if and only if\" condition requires that the converse is also true: rectangle is an equiangular parallelogram. - regular polygon (quadrilateral) is square, which is too specific since the problem does not mention sides being equal. - rhombus does not require all angles to be equal, so it is not necessarily equiangular. - square is special case of rectangle but requires both equiangular and equilateral properties, which are not both specified here. - trapezoid does not necessarily have two pairs of parallel sides, hence not parallelogram. 3. **Conclusion**: - The correct answer is rectangle, as it satisfies both the equiangular condition and the properties of parallelogram. boxed{A} <end_of_solution> <begin_of_thought> Response: Okay, let me try to figure out this problem. The question is: \"A figure is an equiangular parallelogram if and only if it is a...\" with options given. The answer choices are rectangle, regular polygon, rhombus, square, trapezoid. Hmm. So, first, lets break down the terms here. An equiangular parallelogram. Lets recall definitions. parallelogram is quadrilateral with both pairs of opposite sides parallel. Equiangular means that all the angles are equal. In quadrilateral, if all angles are equal, each angle must be 90 degrees because the sum of angles in quadrilateral is 360 degrees. So, 360 divided by 4 is 90. So, an equiangular quadrilateral has four right angles. Wait, but parallelogram by definition already has opposite angles equal. So, if parallelogram is equiangular, that forces all angles to be 90 degrees, right? So, that would make it rectangle. Because rectangle is parallelogram with all angles equal to 90 degrees. So, then, an equiangular parallelogram is rectangle. Then the \"if and only if\" part. So, the statement is saying that figure is an equiangular parallelogram exactly when it is [answer]. So, the converse would also have to be true. That is, if its [answer], then its an equiangular parallelogram, and vice versa. So, lets check the options. Option is rectangle. If figure is rectangle, then its parallelogram with all angles equal (to 90 degrees), so its an equiangular parallelogram. Conversely, if its an equiangular parallelogram, then as we saw, all angles are 90 degrees, so its rectangle. So, that seems to satisfy the \"if and only if\" condition. But wait, lets check the other options just to be thorough. Option is regular polygon. regular polygon is polygon that is both equiangular and equilateral. In the case of quadrilateral, regular polygon would be square. So, square is regular quadrilateral. But parallelogram that is equiangular is rectangle, which isnt necessarily square unless its also equilateral (all sides equal). So, regular polygon is too specific here. Moreover, the question is about 30 Variational Reasoning for Language Models parallelogram, which is quadrilateral, but \"regular polygon\" could refer to any regular polygon, not necessarily quadrilateral, but in the answer choices, its listed as an option. But since regular polygon (quadrilateral) is square, but the question is about an equiangular parallelogram, which is rectangle. So, unless rectangles are considered regular polygons, but rectangles are not regular polygons unless they are squares. So, regular polygon is not the correct answer here. So, is out. Option is rhombus. rhombus is parallelogram with all sides equal. rhombus doesnt have to be equiangular. In fact, rhombus can have angles of, say, 60 and 120 degrees. So, rhombus is not necessarily equiangular. So, an equiangular parallelogram is rectangle, not necessarily rhombus. So, is out. Option is square. square is parallelogram that is both equiangular (all angles 90 degrees) and equilateral (all sides equal). So, the problem here is that an equiangular parallelogram is rectangle, which is only square if the sides are equal. But the question doesnt specify that the sides are equal. So, an equiangular parallelogram is rectangle, but rectangle is square only if its equilateral. Since the problem is only specifying equiangular (all angles equal) and parallelogram, the figure is rectangle, which is type of parallelogram but not necessarily square. Therefore, the \"if and only if\" would require that being square is equivalent to being an equiangular parallelogram. But thats not true because square is special case. So, is not correct. Option is trapezoid. trapezoid is quadrilateral with at least one pair of parallel sides. However, parallelogram has two pairs of parallel sides, so parallelogram is special case of trapezoid (depending on the definition; in some definitions, trapezoids have exactly one pair). But even so, trapezoid isnt necessarily parallelogram. Moreover, an equiangular trapezoid would have all angles equal, but in trapezoid, which is not necessarily parallelogram, having all angles equal doesnt guarantee that the sides are parallel. For example, an isosceles trapezoid can be equiangular (with two pairs of equal angles), but thats different case. However, the question specifies an equiangular parallelogram. So, if its parallelogram, then its already quadrilateral with two pairs of parallel sides, and if its equiangular, all angles are 90 degrees, making it rectangle. trapezoid isnt required to have two pairs of parallel sides, so an equiangular trapezoid isnt necessarily parallelogram. Therefore, is out. So, the only remaining option is A, rectangle. Let me just verify again. If its rectangle, then its parallelogram with all angles equal, so equiangular. Conversely, if its an equiangular parallelogram, all angles are 90 degrees, hence rectangle. The \"if and only if\" holds here. So, the answer should be A. Wait, but hold on. The problem says \"a figure is an equiangular parallelogram if and only if it is [answer]\". So, this is biconditional statement. So, the statement and the statement \"it is [answer]\" must be equivalent. Therefore, the [answer] must be such that every equiangular parallelogram is [answer], and every [answer] is an equiangular parallelogram. But as thought earlier, rectangle is an equiangular parallelogram. However, the reverse - is every rectangle an equiangular parallelogram? Well, yes, because rectangle by 31 Variational Reasoning for Language Models definition has four right angles, so its equiangular. So, the equivalence holds. Therefore, the if and only if is satisfied. But wait, the answer options include square. So, maybe theres confusion here. Let me think again. The question is asking for the figure that is an equiangular parallelogram if and only if it is [answer]. So, the figure is parallelogram that is equiangular, and this happens exactly when its [answer]. So, since we concluded that this [answer] is rectangle, then is correct. But let me check if any of the other options could also satisfy the biconditional. For example, square. So, is square an equiangular parallelogram? Yes, because square is parallelogram with all sides equal and all angles equal (90 degrees). However, the problem states \"if and only if\". So, the figure is an equiangular parallelogram exactly when it is square. But thats not true. Because rectangle is also an equiangular parallelogram, but square is special case of rectangle. So, if the statement was \"if and only if it is square\", then it would be too restrictive, because there are rectangles that are equiangular parallelograms which are not squares. Therefore, the \"if and only if\" with square would not hold. Similarly, regular polygon is too broad or specific. So, the correct answer is rectangle. Therefore, the answer should be A. But wait, the original problem says \"equiangular parallelogram\". Wait, in some contexts, parallelogram being equiangular is the same as being rectangle, so equiangular parallelogram = rectangle. Then the question is asking, \"An equiangular parallelogram is [answer] if and only if it is a...\" Hmm, perhaps the wording is confusing. Let me parse the sentence again. \"A figure is an equiangular parallelogram if and only if it is [answer]\" So, the figure is defined as an equiangular parallelogram, and this is equivalent (if and only if) it is [answer]. So, the definition of an equiangular parallelogram is \"it is [answer]\". Therefore, the [answer] must be the name given to the figure that is exactly the equiangular parallelogram. Since weve concluded that an equiangular parallelogram is rectangle, then the definition would be \"a rectangle is an equiangular parallelogram\". But the answer options have rectangle as A. However, in logic, the if and only if is symmetric. So, saying \"a figure is an equiangular parallelogram if and only if it is rectangle\" is correct. So, that would be answer A. But then why is square an option? Maybe made mistake here. Lets consider that perhaps the question is from country where parallelogram definitions differ. For example, in some definitions, parallelogram is quadrilateral with both pairs of opposite sides parallel, and in those cases, parallelogram is only rectangle if it has right angles. But in standard definitions, rectangle is parallelogram with four right angles, so thats correct. Alternatively, maybe there is different interpretation. Wait, the problem is in the \"if and only if\" part. So, if we take \"equiangular parallelogram\" as the definition, then the \"if and only if\" it is [answer]. So, the [answer] must be co-extensive with the equiangular parallelogram. In other words, the set of equiangular parallelograms is the same as the set of [answer]. So, 32 Variational Reasoning for Language Models since we know that an equiangular parallelogram is rectangle, but rectangle is not necessarily square (since rectangles can have sides of different lengths), then the set of equiangular parallelograms is exactly the set of rectangles. Therefore, the answer is rectangle. But maybe the problem is trying to trick us into thinking that square is the answer because its special case. But since the problem doesnt mention sides being equal, just angles being equal, its rectangle. So, the answer should be A. Alternatively, perhaps theres different approach. Lets recall that in parallelogram, opposite angles are equal. If its equiangular, then all angles are equal. So, in parallelogram, this forces all angles to be 90 degrees. Therefore, its rectangle. Therefore, equiangular parallelogram is another name for rectangle. So, if the question is asking for the term that describes such figure, the answer is rectangle. So, A. But maybe the options are labeled differently. Let me check the options again: A) rectangle B) regular polygon C) rhombus D) square E) trapezoid Yes. So, given that, the answer is A. But let me verify once more. Suppose someone thinks that \"equiangular\" refers to both angles and sides. But no, equiangular just means all angles are equal. Equilateral would mean all sides are equal. So, the problem is only mentioning angles. So, parallelogram that is equiangular must have all angles 90 degrees, hence rectangle. Therefore, the answer is A. think thats solid. So, think the answer is A, rectangle. **Final Answer** boxed{A} Example of reasoning model πθ Prompt: <im_start>system Your role as an assistant involves thoroughly exploring questions through systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <begin_of_thought> {thought with steps separated with nn} <end_of_thought> Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. 33 Variational Reasoning for Language Models In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <begin_of_solution> {final formatted, precise, and clear solution} <end_of_solution> Now, try to solve the following question through the above guidelines:<im_end> <im_start>user If $f(x) = frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as common fraction.<im_end> <im_start>assistant <begin_of_thought> Response: Okay, so need to find the value of f(-2) + f(-1) + f(0) for the function f(x) = (3x - 2)/(x - 2). Hmm, let me start by recalling how to evaluate function at specific point. For each x-value given, substitute it into the function and simplify. Let me break this dow step by step. First, lets compute f(-2). Substituting = -2 into the function: f(-2) = [3*(-2) - 2]/[(-2) - 2] = [(-6) - 2]/[-4] = (-8)/(-4) = 2. Wait, that seems straightforward. The numerator becomes -8 and the denominator becomes -4, and negative divided by negative is positive 2. Okay, so f(-2) is 2. Next, lets find f(-1). Substituting = -1: f(-1) = [3*(-1) - 2]/[(-1) - 2] = [(-3) - 2]/[-3] = (-5)/(-3) = 5/3. Hmm, negative divided by negative is positive, so 5/3. That makes sense. Now, moving on to f(0). Substitute = 0: f(0) = [3*0 - 2]/[0 - 2] = [0 - 2]/[-2] = (-2)/(-2) = 1. Okay, so f(0) is 1. Now, adding these three results together: 2 + 5/3 + 1. Let me convert the whole numbers to fractions to make addition easier. 2 is 6/3, and 1 is 3/3. So adding them up: 6/3 + 5/3 + 3/3 = (6 + 5 + 3)/3 = 14/3. Wait, let me double-check each calculation to make sure didnt make any mistakes. Starting with f(-2): Numerator: 3*(-2) = -6, minus 2 is -8. Denominator: -2 - 2 = -4. So -8/-4 is indeed 2. Correct. For f(-1): Numerator: 3*(-1) = -3, minus 2 is -5. Denominator: -1 - 2 = -3. So -5/-3 is 5/3. Correct. For f(0): Numerator: 3*0 = 0, minus 2 is -2. Denominator: 0 - 2 = -2. So -2/-2 is 1. Correct. 34 Variational Reasoning for Language Models Adding them up: 2 + 5/3 + 1. Converting 2 and 1 to thirds: 2 is 6/3, 1 is 3/3. So 6/3 + 5/3 + 3/3 = 14/3. That seems right. Alternatively, maybe can add them in decimal form to verify. 2 is 2.0, 5/3 is approximately 1.666..., and 1 is 1.0. Adding those: 2.0 + 1.666... + 1.0 = 4.666..., which is 14/3 (since 14 divided by 3 is approximately 4.666...). So that checks out. dont think made any errors here. Each substitution was done correctly, and the arithmetic adds up. Therefore, the final answer should be 14/3. <end_of_thought> <begin_of_solution> To find ( f(-2) + f(-1) + f(0) ) for ( f(x) = frac{3x-2}{x-2} ): 1. **Calculate ( f(-2) ):** [ f(-2) = frac{3(-2) - 2}{-2 - 2} = frac{-6 - 2}{-4} = frac{-8}{-4} = 2 ] 2. **Calculate ( f(-1) ):** [ f(-1) = frac{3(-1) - 2}{-1 - 2} = frac{-3 - 2}{-3} = frac{-5}{-3} = frac{5}{3} ] 3. **Calculate ( f(0) ):** [ f(0) = frac{3(0) - 2}{0 - 2} = frac{-2}{-2} = 1 ] 4. **Sum the results:** [ 2 + frac{5}{3} + 1 = frac{6}{3} + frac{5}{3} + frac{3}{3} = frac{14}{3} ] The final answer is (boxed{dfrac{14}{3}}). <end_of_solution>"
        }
    ],
    "affiliations": [
        "CASIA",
        "NUS",
        "RUC",
        "Sea AI Lab",
        "UCAS"
    ]
}