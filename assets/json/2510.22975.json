{
    "paper_title": "VoMP: Predicting Volumetric Mechanical Property Fields",
    "authors": [
        "Rishit Dagli",
        "Donglai Xiang",
        "Vismay Modi",
        "Charles Loop",
        "Clement Fuji Tsang",
        "Anka He Chen",
        "Anita Hu",
        "Gavriel State",
        "David I. W. Levin",
        "Maria Shugrina"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is a feed-forward method trained to predict Young's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on a manifold of physically plausible materials, which we learn from a real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and a vision-language model, along with a new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 7 9 2 2 . 0 1 5 2 : r VoMP: Predicitng Volumetric Mechanical Property Fields VOMP: PREDICTING VOLUMETRIC MECHANICAL PROPERTY FIELDS Rishit Dagli1,2 Donglai Xiang1 Vismay Modi1 Charles Loop1 Clement Fuji Tsang1 Anka He Chen1 Anita Hu1 Gavriel State1 David I.W. Levin1,2 Maria Shugrina1 1NVIDIA 2University of Toronto https://research.nvidia.com/labs/sil/projects/vomp Figure 1: VoMP predicts physically accurate volumetric mechanical property fields across 3D representations in just few seconds (top), enabling their use in realistic deformable simulations (bottom)."
        },
        {
            "title": "ABSTRACT",
            "content": "Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is feed-forward method trained to predict Youngs modulus(E), Poissons ratio(ν), and density(ρ) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on manifold of physically plausible materials, which we learn from real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and vision-language model, along with new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed."
        },
        {
            "title": "INTRODUCTION",
            "content": "Accurate physics simulation is critical part of modern design and engineering, for example, in workflows like creating Digital Twins (virtual replicas of real systems) (Grieves & Vickers, 2017), Real-2-Sim (generating digital simulation from the real world) (NVIDIA, 2019), and Sim-2-Real (transfering policies trained in simulation to real-world deployment) (Rudin et al., 2021). However, setting up reliable simulations remains labor-intensive, partially due to the necessity to provide accurate mechanical properties throughout the volume of every object, namely the spatially-varying Youngs Modulus(E), Poissons ratio(ν), and density(ρ). Common 3D capture methods (Kerbl et al., 2023) and 3D repositories (Deitke et al., 2023) rarely contain such annotations, forcing artists and engineers to guess or copy-paste coarse material presets in subjective, time-consuming process. We focus on automatic prediction of these parameters, addressing important limitations of prior art. We propose VoMP, the first feed-forward model trained to estimate simulation-ready mechanical property fields (E, ν, ρ) within the volume of 3D objects across representations. Rather than specializing on inputs like Gaussian Splats (Shuai et al., 2025; Xie et al., 2024), our method works for any geometry that can be voxelized and rendered from turnaround views, including meshes, Gaussian Splats, NERFs and SDFs  (Fig. 1)  . Unlike virtually all prior works, VoMP is fully feedforward, requiring no per-object optimization of feature fields (Zhai et al., 2024; Shuai et al., 2025) or run-time aggregation of Vision-Language Model (VLM) (Lin et al., 2025a) or Video Model (Lin 1 VoMP: Predicitng Volumetric Mechanical Property Fields et al., 2025b) supervision. Uniquely among others, VoMP outputs true mechanical properties (a.k.a. material parameters), like those measured in the real world. Many existing pipelines target fast, approximate simulators, resulting in simulator-specific parameters (Zhang et al., 2025; Huang et al., 2024b) that may not transfer reliably across frameworks  (Fig. 2)  , whereas our result is directly compatible with any accurate simulator. Finally, unlike prior art, our method is designed to assign materials throughout the object volume, which is critical for simulation fidelity. To enable learning physically valid mechanical properties, we first train latent space on database of real-world values (E, ν, ρ) using variational auto-encoder MatVAE (3). To predict mechanical property fields for 3D objects, our method first voxelizes the input geometry and aggregates multiview image features across the voxels (4.1). This process accepts many representations1 and is fast, unlike optimization used in concurrent work (Le et al., 2025). We pass the voxel features through the Geometry Transformer (4.2), trained to output per-voxel material latents. The MatVAE latent space decouples learning material assignments for objects from learning what materials are valid, ensuring that the final volumetric properties (E, ν, ρ) decoded by MatVAE are physically valid, even in the case of interpolation. To create material property fields for training, we propose pipeline (5) combining the knowledge from part-segmented 3D assets, material databases, visual textures, and VLM. Our experiments(6) show that VoMP estimates simulation-ready spatially-varying mechanical properties across range of object classes and representations, resulting in realistic elastodynamic simulations. We evaluate our method on an existing mass prediction benchmark and contribute new material estimation benchmark (6.3), consistently outperforming prior art (Shuai et al., 2025; Lin et al., 2025a; Zhai et al., 2024). In summary, our contributions are: The first (to our knowledge) method to estimate object mechanical material property fields that (1) is trained feed-forward model with minimal preprocessing, (2) generalizes across 3D representations, (3) predicts physically valid properties that can be used with an accurate simulator, and (4) predicts mechanical properties within the volume of objects (4). The first (to our knowledge) mechanical properties latent space (3). An automatic data annotation pipeline and new benchmark for volumetric physics materials (5). Thorough evaluation through high-fidelity simulations and quantitative metrics on existing and new benchmarks, significantly outperforming the prior art (6)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 BACKGROUND All algorithms for continuum-based simulation of solids and liquids require material models as input. The material, or constitutive, model is the function that determines the force response of class of materials (e.g., rubbers, snow, water) to internal strains and strain rates. To produce the correct constitutive behavior for given material, the model requires an accurate set of corresponding material parameters for every point in the simulated volume. For locally isotropic material models, Youngs modulus (E, in the 1D linear regime, the proportionality constant between stress and strain), Poissons ratio (ν, the negative ratio of transverse to axial strain under uniaxial loading) and density (ρ, unit mass per volume) are ubiquitous. Given an accurate and valid triplet (E, ν, ρ) along with reasonable material model, consistent numerical simulation can produce accurate predictions of an objects behavior under load. Measured, real-world parameters are portable to any consistent simulation algorithm (we use high-resolution Finite Element Methods). Further, they are portable across any material model that relies on density, Youngs modulus and Poissons ratio, or derived quantities, such as shear or bulk modulus (e.g., Neo-Hookean, St. VenantKirchhoff, As-Rigid-As-Possible, Co-Rotated Elastic, MooneyRivlin, and Ogden models). Simulator differences Figure 2: when dropping solid sphere with (E, ν, ρ) = (104P a, 0.3, 103 kg/m3) with XPBD (Macklin et al., 2016) and MPM (Sulsky et al., 1994) vs. more accurate FEM. 1We describe available methods for meshes, SDFs, and NeRFs, and present method for Splats in 6.1. 2 VoMP: Predicitng Volumetric Mechanical Property Fields On the other hand, many physics simulation algorithms are not implemented or applied in consistent fashion, favoring speed over accuracy (Macklin et al., 2016; Sulsky et al., 1994). In these cases, material parameters must be modified to avoid inaccurate behavior  (Fig. 2)  . 2."
        },
        {
            "title": "INFERRING MECHANICAL PROPERTIES OF STATIC OBJECTS",
            "content": "Our goal is to predict volumetric mechanical properties given only shape and appearance, challenging inverse problem, which research suggests humans learn good intuition about (Adelson, 2001; Fleming, 2014; Fleming et al., 2013; Sharan et al., 2009). However, progress in learning-based approaches has been hampered by limited data. Existing datasets are small (Gao et al., 2022; Downs et al., 2022; Chen et al., 2025c), contain noisy labels (Lin et al., 2018), use simulator-specific parameters (Mishra, 2024; Xie et al., 2025; Belikov et al., 2015), provide only coarse annotations (Ahmed et al., 2025; Slim et al., 2023; Li et al., 2022) or are biased towards rigid or man-made objects (Cao et al., 2025). Worse, data collection is difficult, relying on rigorous physical experiments (ASTM Committee D20, 2022; ASTM Committee E28, 2024; Pai, 2000), and even then lacking spatial material fields (Loveday et al., 2004) due to digitization and annotation challenges. As result, works that infer physical properties from appearance often leverage knowledge from large pre-trained models. NeRF2Physics (Zhai et al., 2024) and PUGS (Shuai et al., 2025) optimize language-embedded feature fields for NeRF (Mildenhall et al., 2020) or 3D Gaussians (Kerbl et al., 2023), respectively, to predict coarse stiffness categories and density, but require per-object optimization and are limited in their ability to predict values inside objects due to the lack of meaningful features inside NeRFs or splats. Many approaches distill signals from Video Generation Model and optimize physics parameters by backpropagating through fast, approximate physics simulators, resulting in slow optimization process, yielding materials deviating from real-world values and overfit to specific simulation setup (Zhang et al., 2025; Huang et al., 2024b; Liu et al., 2025; Cleach et al., 2023; Liu et al., 2024a; Lin et al., 2025b) (2.1). Many methods are also tailored to specific 3D representation or real-time simulation implementations, such as Splats (Xie et al., 2024) or explicit Material Point Methods (Sulsky et al., 1994; Le et al., 2025), or work with coarse material categories (Fischer et al., 2024; Hsu et al., 2024; Lin et al., 2025a; Xia et al., 2025) that must be manually mapped to simulation parameters. Instead, we aim to augment objects across 3D representations with fine-grained spatially-varying mechanical properties that are physically accurate and compatible across accurate simulators. Like our method, many techniques leverage vision-language (VLM) models. PhysGen (Liu et al., 2024b) and PhysGen3D (Chen et al., 2025a) use VLM to infer mass, elasticity, and friction for segmented parts of single image. Phys4DGen (Lin et al., 2025a) uses VLM to annotate parts of 3D model with coarse material labels, which are then mapped to physical parameters, baseline used in our evaluation. Most works above rely on aggregation of large model outputs for every input shape, which can be brittle and time-consuming at run-time, and can only leverage external segmentation. Instead, our method uses VLM paired with other data sources to annotate training dataset for feed-forward model leveraging 3D data to annotate and learn internal material composition. Like our method, SOPHY (Cao & Kalogerakis, 2025), PhysX-3D (Cao et al., 2025), PhysSplat (Zhao et al., 2024a;b) (a.k.a. SimAnything) and the concurrent Pixie (Le et al., 2025) leverage pretrained models and 3D data to annotate training dataset with physical materials. PhysSplat trains network to predict spatially-varying simulator-specific material offset weights for MPM by using outputs from video distillation (Liu et al., 2024a), not focusing on material accuracy. SOPHY and PhysX-3D are 3D generative models, designed to generate new shapes augmented with physical attributes, and cannot augment existing assets, which is our goal. Still, we detail similar aspects of these works. Like these works, our method uses VLM to annotate 3D objects with Youngs Modulus, Poissons ratio, and density, but we do not rely on the human-in-the-loop and instead leverage multiple data sources, not just VLM knowledge, to ensure more accurate physical properties. As baseline, SOPHY does implement material decoder, but it has not been made available, and only considers object surface, while we aim to estimate volumetric properties. Like our method, PhysX3D adopts the structural latent space of TRELLIS, but trains joint generative model over these and learned shape-aware physical properties latents in order to generate physics-augmented shapes from scratch. In contrast, we treat material prediction as deterministic inference for simplicity, and further adjust the TRELLIS pipeline to facilitate accurate material prediction inside the object. Pixie (Le et al., 2025), concurrent work and the only other feed-forward approach, is trained on semanticallysegmented objects and uses points from filtering NeRF densities. Thus, Pixie is trained on segments 3 VoMP: Predicitng Volumetric Mechanical Property Fields Figure 3: VoMP Overview. For any input geometry, we aggregate multi-view DINOv2 features across its volumetric voxelization (4.1). trained GeometryTransformer (4.2) predicts per-voxel material latents, decoded by MatVAE (3) into mechanical properties (E, ν, ρ). biased toward the surface, as we show in Fig. 14, while we demonstrate being able to estimate volumetric properties with internal structures. Furthermore, unlike Pixie, we specifically focus on estimating physically plausible material properties, such as those measured in the real world."
        },
        {
            "title": "3 MECHANICAL PROPERTIES LATENT SPACE",
            "content": "To learn latent space of valid Youngs modulus, Poissons ratio, and density triplets (E, ν, ρ) (2.1), we propose MatVAE, variational autoencoder (VAE) trained on dataset of real-world values {mi := (Ei, νi, ρi)} (5.1). The models objective is to map these triplets into 2dimensional latent space, R2, from which they can be accurately reconstructed. While this offers only minor compression (R3 R2), this latent 2D space of material properties is now easy to visualize, sample, and interpolate within, and results in consistent distances between material triplets with disparate units (Fig. 7,6.4). MatVAE acts like continuous tokenizer that allows us to always ensure VoMP output properties that fall inside the range of some materials. We build on VAE (Kingma & Welling, 2022), with the reconstruction component of the loss defined as mean-squared error between the input (Ei, νi, ρi) and reconstructed material values ( ˆEi, ˆνi, ˆρi): LRecon = 1 (cid:88) (cid:13)((Ei, νi, ρi)N )T (( ˆEi, ˆνi, ˆρi)N )T(cid:13) (cid:13) 2 2 , (cid:13) i=1 (1) where denotes transpose and per-property normalization, where and ρ are first logtransformed (log10(E), log10(ρ)), then normalized to [0, 1], while ν is directly normalized to [0, 1]. We find other normalization schemes without log-transform or standard z-score normalization induce heavy-tailed feature distribution, which is poorly conditioned for learning (C). We make several modifications over standard VAE. First, to capture complex posterior beyond simple Gaussian, the encoders output is transformed by (radial) Normalizing Flow (Rezende & Mohamed, 2015), giving us more flexible variational distribution qϕ(zm) since we observe heavy-tailed distribution for Youngs Modulus and Density while Poissons Ratio concentrates near the boundaries after normalization. Second, we decompose the KL-divergence term of the ELBO following (Chen et al., 2018). This allows us to directly penalize the total correlation TC(z) = KL(qϕ(z) (cid:81) qϕ(zj)) where qϕ(z) is the aggregated posterior, zj is the {1, 2}-th coordinate of the latent vector z. Penalizing TC allowed us to reduce the high dependence between latent coordinates which caused MatVAE to encode density in both dimensions. Third, we observe imbalanced reconstruction, i.e. the latent space collapses to one property, giving us low reconstruction errors for one property and high reconstruction error for others (C). Thus, to ensure the 2 latent dimensions are actively utilized, we introduce capacity constraint (δ zdim) based on (Higgins et al., 2017), resulting in the following final objective: LMatVAE = LRecon (cid:124) (cid:123)(cid:122) (cid:125) MSE + (cid:122) Latent Space Regularization (cid:125)(cid:124) (cid:123) + β TC(z) (cid:123)(cid:122) (cid:125) (cid:124) Total Correlation γ MI(z) (cid:124) (cid:123)(cid:122) (cid:125) Mutual Information +α (cid:88) j= max(cid:0)δ, KL(qϕ(zj) p(zj))(cid:1) , (cid:125) (cid:123)(cid:122) (cid:124) Dimension-wise KL (2) where we set γ, β, α = (1.0, 2.0, 1.0), with free nats constraint δ = 0.1. See F.1 for more details. 4 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "4 PREDICTING MECHANICAL PROPERTY FIELDS",
            "content": "To predict volumetric mechanical properties across 3D representation, VoMP first aggregates volumetric features for the input geometry (4.1), which are then processed by trained feed-forward transformer model (4.2) that learns in the latent space of MatVAE (3). See 2.2."
        },
        {
            "title": "4.1 AGGREGATING FEATURES",
            "content": "Our method accepts any 3D representation that can be voxelized and rendered from multiple views. Following recent works (Wang et al., 2023; Dutt et al., 2024; Xiang et al., 2025), we compute rich DINOv2 (Oquab et al., 2024) image features across 3D views and lift them to 3D by projecting each voxel center into every view using the camera parameters to retrieve the corresponding image features. The retreived image features are then averaged to obtain feature for every voxel. critical difference with these prior works is that we also voxelize and process the interior of the objects and not just their surface, which allows us to learn and predict material properties inside the objects (See 6.1 for voxelization schemes and see F.3 for details on voxelization for training). Lets denote all active voxel center positions in 3D grid of size 3 as {pi}L i=1 where denotes the number of voxels, pi R3 denotes the voxel center, and Πj : R3 [1, 1]2 the camera projection for view where is the set of rendered views. Let the DINOv2 patch-token map be Tj R1024nn which is bilinearly sampled to get feature map Fj : [1, 1]2 R1024. Then for each voxel {1, 2, . . . , L}, we obtain feature fi: (3) This propagates multi-view information to the voxels in the interior of the object, encoding useful information that our model learns to process to predict internal material composition. (cid:0)Πj(pi)(cid:1) (cid:12) (cid:12) J(cid:9)) R1024 fi = Average(Ci = (cid:8)Fj 4.2 GEOMETRY TRANSFORMER The main component of VoMP is Transformer that maps voxelized image features to our trained material latent representation. The backbone of our model follows TRELLIS (Xiang et al., 2025) encoder/decoder, and the backbone layers of our model are initialized with TRELLIS weights. The encoder processes variable-length set of active voxels, represented by their positions and features = {(pi, fi)}L i=1. To make this data suitable for Transformer, we first serialize the voxel features into sequence and then inject spatial awareness by adding sinusoidal positional encodings derived from each voxels 3D coordinates. Similar to TRELLIS and state-of-the-art 3D Transformers, we adopt 3D shifted window attention mechanism (Liu et al., 2021; Yang et al., 2025). Contrary to TRELLIS (Xiang et al., 2025), to handle assets of various sizes, we define maximum sequence length of LN . For assets with fewer voxels LN , we use the complete set. However, for larger assets where > LN , we use stochastic sampling strategy, selecting random subset of LN voxels at the start of each training epoch. This dynamic resampling ensures the model is exposed to different parts of the asset over epochs and have larger number of \"effective\" max voxels. For each training asset, we first define as the set of voxel indices to be processed in the current iteration. The corresponding sequence of image features XS obtained from voxel features (4.1), is passed to F. The resulting latent representation is then fed into the frozen decoder of pre-trained MatVAE to predict material properties. The MatVAE is run times i.e. once per voxel, which gives us material triplets (E, ν, ρ) for each voxel. We train this transformer with the mean squared error between the predicted materials and the ground truth materials, averaged over all voxels in the set S, LF = 1 (cid:88) iS µθ(F(XS )i) ((Ei, νi, ρi)N )T2 2, (4) where µθ() denotes the output of the frozen MatVAE decoder, ((Ei, νi, ρi)N )T is the ground truth material vector for voxel i, and F(XS )i is the latent representation for voxel i. To transfer voxel materials back to the original representation (i.e. splat means, tets for FEM simulation, quadrature points for simulation, etc.), we use nearest neighhbour interpolation as outlined in G.1. The per-voxel latents are passed into the decoder model of MatVAE (3), which yields per-voxel material triplets, as shown in 2.2. 5 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "5.1 MATERIAL TRIPLETS DATASET (MTD)",
            "content": "To train MatVAE (3), we collect Material Triplet Dataset(MTD), containing 100,562 triplets (E, ν, ρ) for real-world materials. We first collect dataset of measured material properties from multiple online databases (MatWeb, LLC, 2025; Wikipedia contributors, 2024a;b;c; The Engineering Toolbox, 2024; Department of Engineering, University of Cambridge, 2011), containing values obtained experimentally, typically with valid ranges for all three properties for all materials. We sample numeric triplets from each material, with the number of samples proportional to the range size. Finally, we filter out duplicates resulting from overlapping ranges for some materials."
        },
        {
            "title": "5.2 GEOMETRY WITH VOLUMETRIC MATERIALS (GVM) DATASET",
            "content": "To train Geometry Transformer(4), we develop an automatic annotation pipeline to overcome the limited availability of detailed volumetric material datasets (2.2). Like prior works (Lin et al., 2025a; Cao & Kalogerakis, 2025; Le et al., 2025), we leverage pre-trained VLM, but overcome its limitations by introducing additional sources of knowledge present in our 3D dataset and the MTD(5.1). We collect high-quality 3D meshes from (NVIDIA Corporation, 2025a;c; NVIDIA Developer, 2025; NVIDIA Corporation, 2025d), containing 1624 partsegmentated 3D models, with total of 8089 parts, and treat each part as having isotropic material. Each part contains an English material name and its own realistic PBR texture, which can be used as additional cues to the VLM. For each part in each object, we pass the following information to the VLM: rendering of the full object, detail rendering of the parts visual material mapped onto sphere (showing visual aspects that tend to correlate with material composition), the material names, and the ranges of three closest real-world materials in the MTD (5.1) based on the material names (See Fig.4, detailed prompt in Fig. 22). The vision-language model then outputs material triplets for each part, and we map to all volumetric voxels within it, resulting in total of 37M voxels annotated with (E, ν, ρ). By guiding VLM with real-world material values and extra clues, we avoid inaccuracies and implausible material values. See additional details in 6.1, E. Figure 4: Training Data annotation leverages accurate 3D data labels together with VLM."
        },
        {
            "title": "6 EXPERIMENTS AND RESULTS",
            "content": "We evaluate VoMP end-to-end, showing diverse realistic simulations in 6.2. Quantitative results video and for many are presented in 6.3, with MatVAE evaluated separately in 6.4. See additional results, for extra comparisons with concurrent work and for ablations. 6.1 IMPLEMENTATION DETAILS Voxelization: For voxelizing 3D Gaussian splats (Kerbl et al., 2023), we present new voxelizer, that works in three phases: (1) 3D Gaussians are voxelized over 3D grid as solid ellipsoids defined by the 99th percentile iso-surface, (2) this set of voxels is rendered from several dozen viewpoints sampled over sphere to form depth maps, (3) these depthmaps are used to carve away empty space around the exterior of the object, but leaving unseen interior voxels to form solid approximation of the object. We then sample this solid at jittered sample points on regular grid. We employ octrees as acceleration structures and GPU implementations for both phases. Our test objects can be voxelized in 31 ms (see Tb. 1). To voxelize meshes and SDFs we use standard methods (see F.3). Data and Training: For material annotation, we experimentally choose Qwen 2.5 VL-72B VLM (Bai et al., 2023; 2025). We partition our MTD and GVM datasets (5) into 80-10-10 train, validation, and test sets. See for data details. For rendering we use Omniverse (NVIDIA Corporation, 2025b) and Blender (Blender Online Community, 2021), and for DINOv2 we use an optimized implementation (NVIDIA, 2025). During training and testing, we set the maximum number of nonVoMP: Predicitng Volumetric Mechanical Property Fields empty voxels per object LN = 32, 768 (sampled stochastically, 4.2), and sparse data structures for efficiency. See for more details. All experiments were performed on machine with four 80GB A100 GPUs, where training took about 12 hours for MatVAE and 5 days for the Transformer. Simulation: We used FEM simulator for meshes and sparse Simplicits (Modi et al., 2024; Fuji Tsang et al.) for our large-scale simulations combining splats and meshes. Details in G. Figure 5: Simulation-ready physics materials of VoMP enable realistic simulations for meshes and splats."
        },
        {
            "title": "6.2 END-TO-END QUALITATIVE EVALUATION",
            "content": "We qualitatively evaluate VoMP by using it to annotate volumetric mechanical fields for several meshes and 3D Gaussian Splats, and running physics simulation with these exact spatially varying (E, ν, ρ) values, resulting in realistic simulations without any hand-tweaks (Fig. 5, Fig. 8, : 0:36). We also show that our approach can work across more representations, including meshes, 3D Gaussian Splats, SDF, and NeRFs ( Fig. 8a, with additional results in A.2. 6.3 QUANTITATIVE EVALUATION Datasets and Metrics: The 10% hold-out test set of GVM (5) consists of 166 high-quality 3D objects with per-voxel mechanical properties for total of 4.9 million point annotations, significantly larger than previous works, e.g. 31 points across 11 objects (Zhai et al., 2024). We contribute this as new benchmark and use it for evaluation against baselines. We measure standard metrics, Average Log Displacement Error (ALDE), Average Displacement Error (ADE), Average Log Relative Error (ALRE), and Average Relative Error (ARE) for each mechanical property, further detailed in D.1. We provide additional intuition for interpreting these errors through targeted simulations in D.4. Table 1: Wall-clock comparisons and breakdown. Method NeRF2Physics PUGS Pixie Phys4DGen Ours Time (s) 1454.55 (1118) 1058.33 (6.94) 201.63 (27.74) 51.65 (4.07) 3.59 (1.36) Rendering Voxelization DINO-v2 Computation DINO-v2 Reconstruction Geometry Transformer MatVAE 2.11 (0.0540) 0.03 (0.0016) 0.86 (0.0020) 0.58 (0.0053) 0.0082 (0.0063) 0.00032 (0.00026) prior against Baselines: We art compare NeRF2Physics (Zhai et al., 2024) and PUGS (Shuai et al., 2025), where we look up material properties at the voxel locations (with proper scaling) using their optimized representations. Note that these techniques do not output Poissons ratio. Phys4DGen (Lin et al., 2025a) is an important baseline, aggregating VLM prediction directly, but does not provide code. We used our best effort to replicate their method and used prompts provided by the authors, designating this implementation Phys4DGen. More baseline details in F.5. We also include early comparisons against concurrent (and as yet unpublished) Pixie (Le et al., 2025), with additional explorations in B. Estimating Mechanical Properties: Quantitative evaluation of material estimates (E, ν, ρ) of our method against prior art on our new detailed benchmark shows dramatic quality boost across all properties and metrics (Fig. 6b). According to our explorations (D.4), ALRE under 0.05 for and ARE under 0.15 for other properties result in similar simulations, suggesting that our materials will lead to more faithful simulations than competitors when using an accurate simulator. Qualitatively (Fig. 6a), we observe that this performance difference may be due to baselines occasionally mislabeling segments (e.g. by Phys4DGen), due to noisy estimates (e.g. NeRF2Physics and PUGS), and less accurate values in the objects interior due to the baselines design. Run-Time: To show approximate speed difference, we report average material estimate speeds across 100 runs on objects with an average of 53.9K Gaussians for our method and the baselines in Tb. 1. To ensure fair compute between CPU and GPU heavy methods, we ran this experiment on machine with only one A100 GPU and 64 CPUs. While we do not provide timing breakdown of the 7 VoMP: Predicitng Volumetric Mechanical Property Fields (a) Qualitative Comparison: We show that qualitiative VoMP tends to provide less noisy volumetric? values compared to the baselines. We show the color coded fields and slice planes through the fields. Method Youngs Modulus Pa (E) Poissons Ratio (ν) ALDE () ALRE () ADE () ARE () Density kg ADE () m3 (ρ) ARE () NeRF2Physics PUGS Phys4DGen 2.8000 (1.05) 3.3942 (1.72) 4.8967 (3.17) 0.1346 (0.05) 0.1688 (0.10) 0.2227 (0.14) - - 0.0407 (0.04) - - 0.1467 (0.18) 1432.0343 (964.88) 3568.2150 (2839.13) 1865.5673 (2176.90) 1.0365 (0.63) 3.2429 (3.56) 1.4394 (2.35) Ours 0.3793 (0.29) 0.0409 (0.04) 0.0241 (0.01) 0.0818 (0.03) 142.6949 (166.90) 0.0921 (0.07) (b) Mechanical Property Estimates of our method significantly outperform the baselines on all metrics. Pervoxel error rate is first computed per object, then averaged across all objects in the test set to avoid weighing some objects more. Global voxel-level normalization yields similar results, see Supplement Tb. 2. Method ALDE () ADE () ARE () MnRE () NeRF2Physics PUGS Phys4DGen Ours 0.736 0.661 0.664 0.631 12.725 9.461 9.961 8.433 1.040 0.767 0.825 0. 0.564 0.576 0.566 0.576 Method log(E)() ν() ρ() NeRF2Physics PUGS Phys4DGen Pixie 1.62 (4.96) 1.87 (4.50) 1.77 (8.53) 11.90 (17.41) 0.85 (3.01) 3.46 (4.42) 19.75 (46.60) 13.24 (12.63) 39.49 (35.47) 46.58 (36.35) Ours 0.29 (1.23) 0.00 (0.00) 11.75 (4.02) (c) Mass Estimate: We show the errors for estimating mass of objects on the ABO-500 (Collins et al., 2022) dataset, the only existing benchmark, approximating the accuracy of our ρ estimates. Figure 6: Quantitative Results and Comparisons: We compare our method against prior art NeRF2Physics (Zhai et al., 2024), PUGS (Shuai et al., 2025) and Phys4DGen (Lin et al., 2025a), and include limited early results comparing with concurrent method Pixie (Le et al., 2025). (d) Material Validity: We report mean values and relative errors (in %) with the closest physically measured material range in MTD (5.1). other methods, this result suggests speed up of 5-100x achieved by our method, which is not surprising given that it is the only feed-forward model among previous work. Concurrent Pixie, which is also feed-forward, involves heavier pre-processing step, including per-object optimization, affecting its end-to-end time. In the timing breakdown of our method, rendering and pre-processing take the most time, and could be further optimized. Mass Estimation: Following NeRF2Physics (Zhai et al., 2024) and PUGS (Shuai et al., 2025), we also evaluate our dataset on the ABO-500 (Collins et al., 2022) object mass estimation benchmark, following the evaluation protocol of PUGS. We run our model to estimate density ρ for upto 32678 voxels per object, then average these values and multiply by the known object volume to obtain mass. While this is only an imperfect proxy for measuring the accuracy of volumetric density ρ, it is benchmark used by prior works, and we include it for completeness. We achieve better or on-par performance across most metrics (Fig. 6c), with qualitative results in A.3. Validity: To gauge how well different methods are at predicting realistic materials, such as those measured in the real world, which is our goal, we leverage our MTD dataset of real materials. First, we run all methods on GVM test set objects, and for each test voxel compute relative errors to 8 VoMP: Predicitng Volumetric Mechanical Property Fields log(E) () ν () ρ () log(E/ρ) () log(G) () log(K) () L.S. () E.A. () BrayCurtis () 0.0034 0.0426 0. 0.0054 0.0036 0.0036 0.0131 0.4439 0. (a) MatVAE shows excellent reconstruction errors on the MTD test set across all metrics. (c) Encoding real materials results in smoothly varying E, ν, ρ values throughout the 2D latent space. End Value Carbon Nitride Diamond Start Value Aerographite Carbon Fiber (high strength) 235.0 GPa 0.25 1775 kg/m3 MatVAE Interpolation Carbon Fiber (high modulus) 400.0 GPa 0.25 1800 kg/m3 Naive Interpolation Invalid () 610.0 GPa 0.23 1750 kg/m3 822.0 GPa 0.28 3200 kg/m3 0.001 GPa ν ρ 0.25 0.2 kg/m3 Start Value Aerographite 0.001 GPa ν ρ (d) Interpolating in latent space results in valid intermediate materials, unlike naive (E, ν, ρ) interpolation. Invalid () 915.0 GPa 0.21 2625 kg/m Invalid () 305.0 GPa 0.24 875 kg/m3 0.25 0.2 kg/m3 1220.0 GPa 0.20 3500 kg/m3 End Value Diamond 1220.0 GPa 0.20 3500 kg/m3 (b) Decoding latent samples leads to plausible (E, ν, ρ) values within real-world materials. Figure 7: Material Latent Space learned by MatVAE (3) ensures faithful(a), valid(b), smoothly varying(c), and interpolatable(d) materials. \"Invalid\" values (c) fall outside all material ranges in MTD(5.1). the nearest possible material range from MTD (error is 0 for estimates within an existing material range). These errors are averaged across all the voxels and reported in Fig. 6d. We observe that our method, on average, outputs much more realistic materials, as it was explicitly designed to do so. 6.4 RECONSTRUCTING AND GENERATING MATERIALS WITH MATVAE Given no prior works exploring latent space of material triplets (E, ν, ρ), we evaluate MatVAE on the MTD test set (6.1), achieving low reconstruction errors in Fig. 7a (See D.1, D.4) for metrics). Further, in Fig. 7 we show the desirable properties of this learned latent space. In (a), samples throughout the 2D latent space map to real-world material ranges in MTD. In (b), we show that (E, ν, ρ)values of real materials encoded to the latent space vary smoothly. Further, the latent space ensures valid interpolation points between materials (c), facilitating valid assignment from predicted voxel materials back to the original geometry. We include detailed ablations of MatVAE design (C), and additional latent space explorations (A.4)."
        },
        {
            "title": "7 DISCUSSION",
            "content": "We introduce representation-agnostic method that maps any 3D asset (mesh, SDF, Gaussian splat, or voxel grid) to volumetric field of physically valid mechanical properties (E, ν, ρ). We show that our method significantly outperforms prior art in accuracy and speed, lowering the barrier for integrating accurate physics into digital workflows across 3D representations, with potential impact across digital twins, robotics, and beyond. While we show important advances over existing works, our method is not without limitations, which we hope will open exciting avenues of future research. Due to fixed-grid voxelization, our output resolution is limited, causing oversmoothing in highly heterogeneous regions, and may result in approximation errors when transferring results to more detailed input geometry. During annotation, we assume part-level materials are isotropic, which is not true assumption for some common materials like wood. Further, future work could extend our method to predict additional properties like yield strength, shear modulus and thermal expansion, or to adapt true material properties output by our method to simulator-specific scales required for faster algorithms or implementations. We hope to support future directions in this area by releasing our material estimation benchmark, and trained models. 9 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "REFERENCES",
            "content": "Edward H. Adelson. On seeing stuff: the perception of materials by humans and machines. In Bernice E. Rogowitz and Thrasyvoulos N. Pappas (eds.), Human Vision and Electronic Imaging VI, volume 4299, pp. 1 12. International Society for Optics and Photonics, SPIE, 2001. doi: 10.1117/12.429489. URL https://doi.org/10.1117/12.429489. Mahmoud Ahmed, Xiang Li, Arpit Prajapati, and Mohamed Elhoseiny. 3dcompat200: LanguagearXiv preprint grounded compositional understanding of parts and materials of 3d shapes. arXiv:2501.06785, 2025. Michael Ashby and David Cebon. Materials selection in mechanical design. Le Journal de Physique IV, 3(C7):C71, 1993. ASTM Committee D20. Standard test method for tensile properties of plastics. Astm standard d638, ASTM International, West Conshohocken, PA, 2022. Approved June 30, 2022. ASTM Committee E28. Standard test methods for tension testing of metallic materials. Astm standard e8/e8m, ASTM International, West Conshohocken, PA, 2024. ANSI approved. ASTM International. ASTM Standard D2240-15, 2015. doi:10.1520/D2240-15. Standard Test Method for Rubber PropertyDurometer Hardness. URL https://doi.org/10.1520/D2240-15. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. VV Belikov, NP Vabishchevich, PN Vabishchevich, UV Katishkov, and NA Mosunova. Material property database. Mathematical Models and Computer Simulations, 7:95102, 2015. Jan Bender and contributors. Positionbaseddynamics: Physically-based simulation library. https: //github.com/InteractiveComputerGraphics/PositionBasedDynamics, 2015. Commit retrieved 3 Aug 2025. Kiran S. Bhat, Steven M. Seitz, Jovan Popovic, and Pradeep K. Khosla. Computing the physical parameters of rigid-body motion from video. In Anders Heyden, Gunnar Sparr, Mads Nielsen, and Peter Johansen (eds.), Computer Vision ECCV 2002, pp. 551565, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg. ISBN 978-3-540-47969-7. Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Blender Institute, Amsterdam, 2021. URL http://www.blender.org. J. Roger Bray and J. T. Curtis. An ordination of the upland forest communities of southern wisconsin. Ecological Monographs, 27(4):325349, 1957. doi: https://doi.org/10.2307/ 1942268. URL https://esajournals.onlinelibrary.wiley.com/doi/abs/ 10.2307/1942268. Marcus A. Brubaker, Leonid Sigal, and David J. Fleet. Estimating contact dynamics. In 2009 IEEE 12th International Conference on Computer Vision, pp. 23892396, 2009. doi: 10.1109/ICCV. 2009.5459407. Arthur Brussee. Brush: 3d reconstruction for all. https://github.com/ArthurBrussee/ brush, 2025. GitHub repository. Junyi Cao and Evangelos Kalogerakis. Sophy: Learning to generate simulation-ready objects with physical materials, 2025. URL https://arxiv.org/abs/2504.12684. 10 VoMP: Predicitng Volumetric Mechanical Property Fields Ziang Cao, Zhaoxi Chen, Liang Pan, and Ziwei Liu. Physx-3d: Physical-grounded 3d asset generation. arXiv preprint arXiv:2507.12465, 2025. Boyuan Chen, Hanxiao Jiang, Shaowei Liu, Saurabh Gupta, Yunzhu Li, Hao Zhao, and Shenlong Wang. Physgen3d: Crafting miniature interactive world from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 61786189, June 2025a. Chuhao Chen, Zhiyang Dou, Chen Wang, Yiming Huang, Anjun Chen, Qiao Feng, Jiatao Gu, and Lingjie Liu. Vid2sim: Generalizable, video-based reconstruction of appearance, geometry and physics for mesh-free simulation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025b. Ricky T. Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud. IsolatIn S. Bengio, H. Wal- (eds.), AdInc., URL https://proceedings.neurips.cc/paper_files/paper/2018/ ing sources of disentanglement lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, vances in Neural Information Processing Systems, volume 31. Curran Associates, 2018. file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf. in variational autoencoders. and R. Garnett Yunuo Chen, Tianyi Xie, Zeshun Zong, Xuan Li, Feng Gao, Yin Yang, Ying Nian Wu, and Chenfanfu Jiang. Atlas3d: Physically constrained self-supporting text-to-3d for simulation and fabrication, 2024. URL https://arxiv.org/abs/2405.18515. Yuzhen Chen, Hojun Son, and Arpan Kusari. Matpredict: dataset and benchmark for learning material properties of diverse indoor objects, 2025c. URL https://arxiv.org/abs/2505. 13201. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. Simon Le Cleach, Hong-Xing Yu, Michelle Guo, Taylor A. Howell, Ruohan Gao, Jiajun Wu, Zachary Manchester, and Mac Schwager. Differentiable physics simulation of dynamicsaugmented neural objects, 2023. URL https://arxiv.org/abs/2210.09420. Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F. Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2112621136, June 2022. Abe Davis, Katherine Bouman, Justin Chen, Michael Rubinstein, Fredo Durand, and William In Freeman. Visual vibrometry: Estimating material properties from small motion in video. Proceedings of the ieee conference on computer vision and pattern recognition, pp. 53355343, 2015. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsanit, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: Universe of Annotated 3D Objects . In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1314213153, Los Alamitos, CA, USA, Jun 2023. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.01263. URL https://doi.ieeecomputersociety.org/ 10.1109/CVPR52729.2023.01263. Department of Engineering, University of Cambridge. Materials Data Book. University of Cambridge, 2011. URL https://teaching.eng.cam.ac.uk/sites/teaching. eng.cam.ac.uk/files/Documents/Databooks/MATERIALS%20DATABOOK% 20(2011)%20version%20for%20Moodle.pdf. Data extracted from the Cambridge Engineering Selector (CES EduPack), courtesy of Granta Design Ltd. For educational use. Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items, 2022. URL https://arxiv.org/abs/2204.11918. VoMP: Predicitng Volumetric Mechanical Property Fields Niladri Shekhar Dutt, Sanjeev Muralikrishnan, and Niloy J. Mitra. Diffusion 3d features (diff3f): Decorating untextured shapes with distilled semantic features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 44944504, June 2024. Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, and Yin Yang. Pie-nerf: Physics-based interactive elastodynamics with nerf, 2024. URL https://arxiv.org/abs/ 2311.13099. Michael Fischer, Iliyan Georgiev, Thibault Groueix, Vladimir Kim, Tobias Ritschel, and Valentin Deschaintre. Sama: Material-aware 3d selection and segmentation. arXiv preprint arXiv:2411.19322, 2024. Roland W. Fleming. Visual perception of materials and their properties. Vision Research, 94:62 75, 2014. ISSN 0042-6989. doi: https://doi.org/10.1016/j.visres.2013.11.004. URL https: //www.sciencedirect.com/science/article/pii/S0042698913002782. Roland W. Fleming, Christiane Wiebel, and Karl Gegenfurtner. Perceptual qualities and material classes. Journal of Vision, 13(8):99, 07 2013. ISSN 1534-7362. doi: 10.1167/13.8.9. URL https://doi.org/10.1167/13.8.9. frankaemika. franka_description: Official models of franka robotics gmbh robots. https:// github.com/frankaemika/franka_description, 2025. GitHub repository, accessed June 2025. Clement Fuji Tsang, Maria Shugrina, Jean Francois Lafleche, Or Perel, Charles Loop, Towaki Takikawa, Vismay Modi, Alexander Zook, Jiehan Wang, Wenzheng Chen, Tianchang Shen, Jun Gao, Krishna Murthy Jatavallabhula, Edward Smith, Artem Rozantsev, Sanja Fidler, Gavriel State, Jason Gorski, Tommy Xiang, Jianing Li, Michael Li, and Rev Lebaredian. Kaolin: pytorch library for accelerating 3d deep learning research. URL https://github.com/ NVIDIAGameWorks/kaolin. Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. Objectfolder 2.0: multisensory object dataset for sim2real transfer, 2022. URL https://arxiv.org/abs/2204.02389. Michael Grieves and John Vickers. Digital Twin: Mitigating Unpredictable, Undesirable Emergent Behavior in Complex Systems, pp. 85113. Springer International Publishing, Cham, 2017. ISBN 978-3-319-38756-7. doi: 10.1007/978-3-319-38756-7_4. URL https://doi.org/ 10.1007/978-3-319-38756-7_4. Minghao Guo, Bohan Wang, Pingchuan Ma, Tianyuan Zhang, Crystal Elaine Owens, Chuang Physically compatiGan, Joshua B. Tenenbaum, Kaiming He, and Wojciech Matusik. In A. Globerson, L. Mackey, D. Belble 3d object modeling from single image. grave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 119260119282. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2024/ 2024. file/d7af02c8a8e26608199c087f50a21d37-Paper-Conference.pdf. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with constrained variational framework. In International conference on learning representations, 2017. Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, and Shenlong Wang. Autovfx: Physically realistic video editing from natural language instructions, 2024. URL https://arxiv.org/ abs/2411.02394. Yuanming Hu and contributors. taichi_mpm: High-performance mls-mpm solver. https:// github.com/yuanming-hu/taichi_mpm, 2018. Commit retrieved 3 Aug 2025. Kemeng Huang, Floyd M. Chitalu, Huancheng Lin, and Taku Komura. Gipc: Fast and stable gaussnewton optimization of ipc barrier energy. ACM Trans. Graph., 43(2), mar 2024a. ISSN 07300301. doi: 10.1145/3643028. 12 VoMP: Predicitng Volumetric Mechanical Property Fields Kemeng Huang, Xinyu Lu, Huancheng Lin, Taku Komura, and Minchen Li. Stiffgipc: Advancing ISSN gpu ipc for stiff affine-deformable simulation. ACM Trans. Graph., 44(3), May 2025. 0730-0301. doi: 10.1145/3735126. Tianyu Huang, Haoze Zhang, Yihan Zeng, Zhilu Zhang, Hui Li, Wangmeng Zuo, and Rynson W. H. Lau. Dreamphysics: Learning physics-based 3d dynamics with video diffusion priors, 2024b. URL https://arxiv.org/abs/2406.01476. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114. Jochen Lang, Dinesh Pai, and Hans-Peter Seidel. Scanning large-scale articulated deformations. In Graphics Interface, pp. 265272, 2003. Long Le, Ryan Lucas, Chen Wang, Chuhao Chen, Dinesh Jayaraman, Eric Eaton, and Lingjie Liu. Pixie: Fast and generalizable supervised learning of 3d physics from pixels, 2025. URL https: //arxiv.org/abs/2508.17437. Jinxi Li, Ziyang Song, Siyuan Zhou, and Bo Yang. Freegave: 3d physics learning from dynamic videos by gaussian velocity. CVPR, 2025. Minchen Li, Zachary Ferguson, Teseo Schneider, Timothy Langlois, Denis Zorin, Daniele Panozzo, Chenfanfu Jiang, and Danny M. Kaufman. intersectionand inversion-free, large-deformation dynamics. ACM Trans. Graph., 39(4), August 2020a. ISSN 0730-0301. doi: 10.1145/3386569.3392425. URL https://doi.org/10.1145/ 3386569.3392425. Incremental potential contact: Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, and Chuang Gan. Pac-nerf: Physics augmented continuum neural radiance fields for geometry-agnostic system identification, 2023. URL https://arxiv.org/abs/2303. 05512. Yuchen Li, Ujjwal Upadhyay, Habib Slim, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, and Mohamed Elhoseiny. 3d compat: Composition of materials on parts of 3d things. In Shai Avidan, Gabriel Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision ECCV 2022, pp. 110127, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-20074-8. Yunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel Yamins, Jiajun Wu, Joshua Tenenbaum, and In Hal Daumé III and Aarti Antonio Torralba. Visual grounding of learned physical models. Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 59275936. PMLR, 1318 Jul 2020b. URL https://proceedings.mlr.press/v119/li20j.html. Hubert Lin, Melinos Averkiou, Evangelos Kalogerakis, Balazs Kovacs, Siddhant Ranade, Vladimir Kim, Siddhartha Chaudhuri, and Kavita Bala. Learning material-aware local descriptors for 3d shapes. In 2018 International Conference on 3D Vision (3DV), pp. 150159. IEEE, 2018. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2024a. URL https://arxiv.org/abs/2312.07533. Jiajing Lin, Zhenzhong Wang, Yongjie Hou, Yuzhou Tang, and Min Jiang. Phy124: Fast physicsdriven 4d content generation from single image, 2024b. URL https://arxiv.org/abs/ 2409.07179. Jiajing Lin, Zhenzhong Wang, Dejun Xu, Shu Jiang, YunPeng Gong, and Min Jiang. Phys4dgen: Physics-compliant 4d generation with multi-material composition perception, 2025a. URL https://arxiv.org/abs/2411.16800. 13 VoMP: Predicitng Volumetric Mechanical Property Fields Yuchen Lin, Chenguo Lin, Jianjin Xu, and Yadong MU. OmniphysGS: 3d constitutive gausIn The Thirteenth International Confersians for general physics-based dynamics generation. ence on Learning Representations, 2025b. URL https://openreview.net/forum?id= 9HZtP6I5lv. Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, and Yueqi Duan. Physics3d: Learning physical properties of 3d gaussians via video diffusion. arXiv preprint arXiv:2406.04338, 2024a. Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physicsgrounded image-to-video generation. In European Conference on Computer Vision, pp. 360378. Springer, 2024b. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Zhuoman Liu, Weicai Ye, Yan Luximon, Pengfei Wan, and Di Zhang. Unleashing the potential of multi-modal foundation models and video diffusion for 4d dynamic physical scene simulation. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 11016 11025, June 2025. John Lloyd and Dinesh Pai. Robotic mapping of friction and roughness for reality-based modeling. In Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164), volume 2, pp. 18841890. IEEE, 2001. Malcolm Loveday, Tom Gray, and Johannes Aegerter. Tensile testing of metallic materials: review. Final report of the TENSTAND project of work package, 1, 2004. Miles Macklin. Warp: high-performance python framework for gpu simulation and graphics. https://github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC). Miles Macklin, Matthias Müller, and Nuttapong Chentanez. Xpbd: position-based simulation In Proceedings of the 9th International Conference on of compliant constrained dynamics. Motion in Games, MIG 16, pp. 4954, New York, NY, USA, 2016. Association for ComISBN 9781450345927. doi: 10.1145/2994258.2994272. URL https: puting Machinery. //doi.org/10.1145/2994258.2994272. MatWeb, LLC. Matweb: Online materials information resource. https://www.matweb.com/, 2025. Accessed: 2025-06-25. Mariem Mezghanni, Théo Bodrito, Malika Boulkenafed, and Maks Ovsjanikov. Physical simulation layer for accurate 3d modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1351413523, June 2022. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. Akshansh Mishra. Latticeml: data-driven application for predicting the effective young modulus of high temperature graph based architected materials, 2024. URL https://arxiv.org/ abs/2404.09470. Vismay Modi, Nicholas Sharp, Or Perel, Shinjiro Sueda, and David I. W. Levin. Simplicits: Meshfree, geometry-agnostic elastic simulation. ACM Trans. Graph., 43(4), July 2024. ISSN 07300301. doi: 10.1145/3658184. URL https://doi.org/10.1145/3658184. Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, and Ali Farhadi. Newtonian scene understanding: Unfolding the dynamics of objects in static images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, SongChun Zhu, and Siyuan Huang. Phyrecon: Physically plausible neural scene reconstruction, 2024. URL https://arxiv.org/abs/2404.16666. 14 VoMP: Predicitng Volumetric Mechanical Property Fields NVIDIA. Nvidia unveils omniverse - open, form for multi-tool workflows, 2019. omniverse-collaboration-platform/. NVIDIA Blog. interactive 3d design collaboration platURL https://blogs.nvidia.com/blog/ NVIDIA. Nv-dinov2. NVIDIA AI Foundation Models (Model card), 2025. NVIDIA Corporation. Commercial assets pack. https://docs.omniverse.nvidia. 2025a. https://docs.omniverse.nvidia.com/usd/latest/usd_content_ com/usd/latest/usd_content_samples/downloadable_packs.html, URL samples/downloadable_packs.html. Accessed: 2025-06-13. NVIDIA Corporation. Developer Documentation, 2025b. https://docs.omniverse.nvidia.com/extensions/latest/ext_ Nvidia omniverse replicator. URL replicator.html. NVIDIA Corporation. Residential assets pack. https://docs.omniverse.nvidia. 2025c. https://docs.omniverse.nvidia.com/usd/latest/usd_content_ com/usd/latest/usd_content_samples/downloadable_packs.html, URL samples/downloadable_packs.html. Accessed: 2025-06-13. NVIDIA Corporation. Vegetation assets pack. https://docs.omniverse.nvidia. com/usd/latest/usd_content_samples/downloadable_packs.html, 2025d. https://docs.omniverse.nvidia.com/usd/latest/usd_content_ URL samples/downloadable_packs.html. Accessed: 2025-06-13. NVIDIA Developer. Simready assets. https://developer.nvidia.com/omniverse/ simready-assets, 2025. URL https://developer.nvidia.com/omniverse/ simready-assets. Accessed: 2025-06-13. OpenAI and Josh Achiam et al. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/ 2303.08774. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. URL https://arxiv.org/abs/2304.07193. Dinesh Pai. Robotics in reality-based modeling. In Robotics Research: the Ninth International Symposium, pp. 353358. Springer, 2000. Dinesh K. Pai, Kees van den Doel, Doug L. James, Jochen Lang, John E. Lloyd, Joshua L. RichIn Proceedmond, and Som H. Yau. Scanning physical interaction behavior of 3d objects. ings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 01, pp. 8796, New York, NY, USA, 2001. Association for Computing Machinery. ISBN 158113374X. doi: 10.1145/383259.383268. URL https://doi.org/10.1145/ 383259.383268. Dinesh Pai, Jochen Lang, John Lloyd, and Robert Woodham. Acme, telerobotic active measurement facility. In Experimental Robotics VI, pp. 391400. Springer, 2008. Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, and Abhinav Gupta. The curious robot: Learning visual representations via physical interactions, 2016. URL https://arxiv.org/ abs/1604.01360. PlayCanvas contributors. SuperSplat: 3d gaussian splat editor. https://github.com/ playcanvas/supersplat, 2025. GitHub repository. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. 15 VoMP: Predicitng Volumetric Mechanical Property Fields Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 15301538, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/rezende15. html. Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In 5th Annual Conference on Robot Learning, 2021. URL https://openreview.net/forum?id=wK2fDDJ5VcF. Katalin Schäffer, Yasemin Ozkan-Aydin, and Margaret M. Coad. Soft wrist exosuit actuated by fabric pneumatic artificial muscles. IEEE Transactions on Medical Robotics and Bionics, 6(2): 718732, May 2024. ISSN 2576-3202. doi: 10.1109/tmrb.2024.3385795. URL http://dx. doi.org/10.1109/TMRB.2024.3385795. Lavanya Sharan, Ruth Rosenholtz, and Edward Adelson. Material perception: What can you see in brief glance? Journal of Vision, 9(8):784784, 2009. Nicholas Sharp et al. Polyscope, 2019. www.polyscope.run. Haochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li, and Jiajun Wu. Robocook: Long-horizon elasto-plastic object manipulation with diverse tools. arXiv preprint arXiv:2306.14447, 2023. Yinghao Shuai, Ran Yu, Yuantao Chen, Zijian Jiang, Xiaowei Song, Nan Wang, Jv Zheng, Jianzhu Ma, Meng Yang, Zhicheng Wang, Wenbo Ding, and Hao Zhao. Pugs: Zero-shot physical understanding with gaussian splatting, 2025. URL https://arxiv.org/abs/2502.12231. Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, et al. 3dcompat++: An improved large-scale 3d vision dataset for compositional recognition. arXiv preprint arXiv:2310.18511, 2023. Trevor Standley, Ozan Sener, Dawn Chen, and Silvio Savarese. image2mass: Estimating the In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg mass of an object from its image. (eds.), Proceedings of the 1st Annual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pp. 324333. PMLR, 1315 Nov 2017. URL https: //proceedings.mlr.press/v78/standley17a.html. D. Sulsky, Z. Chen, and H.L. Schreyer. particle method for history-dependent materials. ISSN doi: https://doi.org/10.1016/0045-7825(94)90112-0. URL https://www. Computer Methods in Applied Mechanics and Engineering, 118(1):179196, 1994. 0045-7825. sciencedirect.com/science/article/pii/0045782594901120. Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH 23, 2023. The Engineering Toolbox. Engineering materials properties. https://www. engineeringtoolbox.com/engineering-materials-properties-d_1225. html, 2024. Accessed: 2025-06-25. Yuang Wang, Xingyi He, Sida Peng, Haotong Lin, Hujun Bao, and Xiaowei Zhou. Autorecon: Automated 3d object discovery and reconstruction. In CVPR, 2023. Wikipedia contributors. Density. https://en.wikipedia.org/wiki/Density, 2024a. Accessed: 2025-06-25. Wikipedia contributors. Poissons ratio. https://en.wikipedia.org/wiki/Poisson% 27s_ratio, 2024b. Accessed: 2025-06-25. Wikipedia contributors. Youngs modulus. https://en.wikipedia.org/wiki/Young% 27s_modulus, 2024c. Accessed: 2025-06-25. 16 VoMP: Predicitng Volumetric Mechanical Property Fields Jiajun Wu, Galileo: Ilker Yildirim, Joseph Lim, Bill Freeman, and Josh Tenenbaum. Perceiving physical object properties by integrating physics engine with deep learnIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Ading. vances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2015/ 2015. file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf. Jiajun Wu, Joseph Lim, Hongyi Zhang, Joshua Tenenbaum, and William Freeman. Physics In BMVC, volume 2, pp. 7, 101: Learning physical object properties from unlabeled videos. 2016. Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, and Josh Tenenbaum. Learning to see physics via visual de-animation. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_ files/paper/2017/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf. Hongchi Xia, Zhi-Hao Lin, Wei-Chiu Ma, and Shenlong Wang. Video2game: Real-time interacIn Proceedings of the tive realistic and browser-compatible environment from single video. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 45784588, June 2024. Hongchi Xia, Entong Su, Marius Memmel, Arhan Jain, Raymond Yu, Numfor Mbiziwo-Tiapo, Ali Farhadi, Abhishek Gupta, Shenlong Wang, and Wei-Chiu Ma. Drawer: Digital reconstruction and articulation with environment realism. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 2177121782, June 2025. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation, 2025. URL https://arxiv.org/abs/2412.01506. Han Xie, Ru Jia, Yonglin Xia, Lei Li, Yue Hu, Jiaxuan Xu, Yufei Sheng, Yuanyuan Wang, and Hua Bao. An ab initio dataset of size-dependent effective thermal conductivity for advanced technology transistors. arXiv preprint arXiv:2501.15736, 2025. Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 43894398, 2024. Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B. Tenenbaum, and Shuran Song. Densephysnet: Learning dense physical object representations via multi-step dynamic interactions, 2019. URL https://arxiv.org/abs/1906.03853. Haotian Xue, Antonio Torralba, Joshua B. Tenenbaum, Daniel LK Yamins, Yunzhu Li, and HsiaoYu Tung. 3d-intphys: Towards more generalized 3d-grounded visual intuitive physics under challenging scenes, 2023. URL https://arxiv.org/abs/2304.11470. Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1626216272, June 2024. Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo. Swin3d: pretrained transformer backbone for 3d indoor scene understanding. Computational Visual Media, 11(1):83101, 2025. Shaoxiong Yao and Kris Hauser. Estimating tactile models of heterogeneous deformable objects in real time. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 1258312589, 2023. doi: 10.1109/ICRA48891.2023.10160731. Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, and Angjoo Kanazawa. gsplat: An open-source library for gaussian splatting, 2024. URL https://arxiv.org/abs/2409.06765. 17 VoMP: Predicitng Volumetric Mechanical Property Fields Ilker Yildirim, Jiajun Wu, Yilun Du, and Joshua B. Tenenbaum. Interpreting dynamic scenes by physics engine and bottom-up visual cues. arXiv preprint, 1605., 2016. arXiv:1605.02470. Samson Yu, Kelvin Lin, Anxing Xiao, Jiafei Duan, and Harold Soh. Octopi: Object property reasoning with large tactile-language models, 2024. URL https://arxiv.org/abs/2405. 02794. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639:609616, 2025. Albert J. Zhai, Yuan Shen, Emily Y. Chen, Gloria X. Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, and Shenlong Wang. Physical property understanding from language-embedded feature fields, 2024. URL https://arxiv.org/abs/2404.04242. Kaifeng Zhang, Baoyu Li, Kris Hauser, and Yunzhu Li. Adaptigraph: Material-adaptive graph-based neural dynamics for robotic manipulation. arXiv preprint arXiv:2407.07889, 2024. Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William T. Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol (eds.), Computer Vision ECCV 2024, pp. 388406, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72627-9. Haoyu Zhao, Hao Wang, Xingyue Zhao, Hao Fei, Hongqiu Wang, Chengjiang Long, and Hua Zou. Efficient physics simulation for 3d scenes via mllm-guided gaussian splatting. arXiv preprint arXiv:2411.12789, 2024a. Haoyu Zhao, Hao Wang, Xingyue Zhao, Hongqiu Wang, Zhiyu Wu, Chengjiang Long, and Hua Zou. Automated 3d physical simulation of open-world scene with gaussian splatting. arXiv e-prints, pp. arXiv2411, 2024b. 18 VoMP: Predicitng Volumetric Mechanical Property Fields Supplementary Material for VoMP: Predicting Volumetric Mechanical Property Fields"
        },
        {
            "title": "SUPPLEMENTARY CONTENTS",
            "content": "A Additional Results A.1 End-to-end Examples with Simulation . . . . . . . . . . . . . . . . . . . . . . . . A.2 More Mechanical Property Prediction Results . . . . . . . . . . . . . . . . . . . . A.3 Mass Estimation Example Results . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Additional MatVAE Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Comparison with Concurrent work Ablations Metrics D.1 Metrics for Mass and Field Estimation . . . . . . . . . . . . . . . . . . . . . . . . D.2 Metrics to Measure Differences in Mechanical Properties . . . . . . . . . . . . . . D.3 Metrics to Measure Differences in Distributions . . . . . . . . . . . . . . . . . . . D.4 Interpreting Errors for Material Property Estimation . . . . . . . . . . . . . . . . . Dataset Details E.1 Annotation with Vision-Language Model . . . . . . . . . . . . . . . . . . . . . . E.2 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Implementation Details F.1 Design of MatVAE . F.2 Network Design . F.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Simulation and Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Details on the Simulations G.1 Interpolation Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Preparing Scenes and Assigning Materials for the FEM Solver . . . . . . . . . . . G.3 Details of the FEM Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 Preparing Scenes and Assigning Materials for the Simplicits Solver . . . . . . . . G.5 Deforming Splats and Rebndering Deformed Splats . . . . . . . . . . . . . . . . . G.6 Details of the Simplicits Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.7 Preparing Scenes and Assigning Materials for the XPBD Solver . . . . . . . . . . 19 21 21 21 26 28 29 30 31 31 32 33 34 37 37 40 40 43 43 44 44 45 46 47 48 VoMP: Predicitng Volumetric Mechanical Property Fields G.8 Preparing Scenes and Assigning Materials for the MPM Solver . . . . . . . . . . . 48 Other Related Works 49 20 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "A ADDITIONAL RESULTS",
            "content": "A.1 END-TO-END EXAMPLES WITH SIMULATION We demonstrate the process that enables creating simulation-ready, realistic assets from Gaussian Splats and meshes in Fig. 8, demonstrating convincing simulations without any fine-tuning. For example, in Fig. 8c, we first capture video of an object, and then train 3D Gaussian Splatting model. Then, we pass it to VoMP which estimates the mechanical properties in couple of seconds. We then use these properties in simulator to produce realistic (see : 0:36), greatly reducing the barrier toward constructing realistic interactive digital worlds directly from our physical reality. A.2 MORE MECHANICAL PROPERTY PREDICTION RESULTS We show qualitative results for inferring mechanical property fields in Fig. 9 and 10. We notice from Fig. 9 row 1, column 2 that our model can pick up small details like the stem of the orange at the top of the object, which is given different Youngs modulus, though it only spans few voxels (see : 1:38). We notice from Fig. 9 row 2, column 2 that our model finds that the space isnide the pot should be made up of properties that fall in the range of dirt, even though the inside of the pot was not observed through external renders (see : 1:50). We notice from Fig. 9 row 3, column 2 that our model can tolerate some noise in assets such as the Gaussian splat of bowl with fruits segmented from larger Gaussian splat (see : 2:01). We notice from Fig. 10 row 1, column 1 that our model can accurately predict thing segments and thin boundaries, like for the seat of the : 2:10). We notice from Fig. 9 row 3, column 2 that our models can handle complex chair (see assets and complex materials like trees and accurately handle fine details, such as understanding : 2:16). We where all the leaves lie and giving them different material properties than wood (see notice from Fig. 9 row 4, column 1 that our models can handle complex volumetric materials, such as annotating the properties of wood under the flowers (see : 2:28). We notice from Fig. 9 row : 4, column 2 that our models can handle thin materials and identify the vein of the leaves (see 2:35). For completeness, we also include our metrics normalized by total test set voxels in Tb. 2 and observe the same performance boost compared to prior art as with the per-object normalization as presented in the main paper Fig. 6b. A.3 MASS ESTIMATION EXAMPLE RESULTS We show qualitative results from our model on mass estimation on the ABO-500 (Collins et al., 2022) dataset in Fig. 11. 21 VoMP: Predicitng Volumetric Mechanical Property Fields (a) Inference across representations: We show that VoMP can operate on any geometry that can be voxelized and rendered, including meshes, SDF, NeRFs, and Gaussian Splats ( : 1:30). (b) Simulating meshes: We show realistic simulations for meshes using predicted material values ( : 3:40). (c) Simulating captured Gaussian Splat model: In this example, we apply VoMP to this Gaussian Splat model that we captured using commercial app. Our method converts this model into simulation-ready asset ( : 0:36). (d) Simulating meshes and splats: We show realistic simulations for meshes (left) and splats (right) using predicted material values ( : 3:20). (e) Simulating Gaussian Splats at scale: an elastodynamic simulation of Gaussian Splat bulldozer going through forest of 100 Gaussian splat ficuses in the presence of wind; all materials predicted by VoMP( : 3:00). Figure 8: End To End Results: We test VoMP material estimates on variety of input representations (a), and show realistic simulations without any hand-tuning for meshes and splats across diverse scenarios. No handtuning of our predicted material parameters was performed, showing that VoMP directly predicts simulationready parameters. See our video for the simulation of these examples. 22 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "Object",
            "content": "Youngs Modulus (E, Pa) Poissons Ratio (ν) Density (ρ, kg m3 )"
        },
        {
            "title": "Object",
            "content": "Youngs Modulus (E, Pa) Poissons Ratio (ν) Density (ρ, kg m3 ) Figure 9: Inferred Mechanical Property Fields. We show additional mechnical property fields and slice planes through mechanical property fields estimated by VoMP ( : 1:40). Table 2: Voxel Mechanical Property Estimation. Errors for predicting mechanical properties from 3D objects averaged across all voxels in the test set. Method Youngs Modulus Pa (E) Poissons Ratio (ν) ALDE () ALRE () ADE () ARE () Density kg ADE () m3 (ρ) ARE () NeRF2Physics (Zhai et al., 2024) PUGS (Shuai et al., 2025) Phys4DGen (Lin et al., 2025a) 2.5719 (1.15) 3.8619 (2.01) 5.2977 (3.36) 0.4122 (0.08) 0.4512 (0.11) 0.4825 (0.14) - - 0.0394 (0.05) - - 0.1425 (0.21) 1354.9458 (1315.71) 3641.0715 (3320.78) 1285.9489 (1981.11) 1.1496 (0.67) 4.0413 (4.16) 1.0445 (2.53) Ours 0.3765 (0.39) 0.0421 (0.05) 0.0250 (0.01) 0.0837 (0.03) 113.3807 (301.90) 0.0908 (0.14) 23 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "Object",
            "content": "Youngs Modulus (E, Pa) Poissons Ratio (ν) Density (ρ, kg m3 )"
        },
        {
            "title": "Object",
            "content": "Youngs Modulus (E, Pa) Poissons Ratio (ν) Density (ρ, kg m3 ) Figure 10: Inferred Mechanical Property Fields. We show additional mechnical property fields and slice planes through mechanical property fields estimated by VoMP ( : 2:10). VoMP: Predicitng Volumetric Mechanical Property Fields Predicted: 1.58 kg Ground Truth: 1.58 kg Predicted: 0.01 kg Ground Truth: 0.01 kg Predicted: 0.02 kg Ground Truth: 0.01 kg Predicted: 0.71 kg Ground Truth: 0.70 kg Predicted: 9.09 kg Ground Truth: 9.07 kg Predicted: 0.37 kg Ground Truth: 0.34 kg Predicted: 7.52 kg Ground Truth: 7.54 kg Predicted: 0.14 kg Ground Truth: 0.10 kg Predicted: 0.38 kg Ground Truth: 0.32 kg Predicted: 0.25 kg Ground Truth: 0.19 kg Predicted: 0.05 kg Ground Truth: 0.11 kg Predicted: 0.19 kg Ground Truth: 0.12 kg Predicted: 7.68 kg Ground Truth: 7.60 kg Predicted: 4.56 kg Ground Truth: 4.65 kg Predicted: 1.23 kg Ground Truth: 1.12 kg Predicted: 0.80 kg Ground Truth: 0.91 kg Predicted: 4.47 kg Ground Truth: 4.36 kg Predicted: 0.42 kg Ground Truth: 0.29 kg Predicted: 2.40 kg Ground Truth: 2.27 kg Predicted: 2.85 kg Ground Truth: 2.72 kg Figure 11: Mass Estimation. We show qualitative results of estimating mass from the ABO-500 (Collins et al., 2022) dataset. 25 VoMP: Predicitng Volumetric Mechanical Property Fields A.4 ADDITIONAL MATVAE RESULTS A.4.1 DISTRIBUTION LEARNED BY MATVAE We measure standard metrics used for measuring the difference in the distribution learned by MatVAE and the distribution of the test set in Tb. 3, observing small errors which suggest that MatVAE learned good approximation of the true material distribution. Table 3: Distribution learned by MatVAE compared to the distribution of MTD test set. Youngs Modulus (E) Poissons Ratio (ν) Density (ρ) W1 () W2 () DKL () W1 () W2 () DKL () W1 () W2 () DKL () 0.0260 0.0437 0.0405 0.1379 0.0798 0.0342 0. 0.0317 0.0132 A.4.2 MOVING ACROSS MATVAE LATENT SPACE We show an example of moving across the latent space in Fig. 13. For each setting, we take point in our latent space and move across both of its dimensions to obtain multiple smoothly varying material properties. We apply these properties to bunny and simulate dropping it to the ground with FEM simulator and these various material properties. The color plots show the average displacements of the mesh from its rest state across simulation steps (e.g. Fig. 13b), demonstrating that even the actual physical behavior correlates with the dimensions of the latent space. Thus, we find that MatVAE learns rich, meaningful latent space with smooth interpolation and ensures generating physically valid material triplets. A.4.3 INTERPOLATION WITH MATVAE We show additional examples of interpolating in the MatVAE latent space (6.4) in Fig. 12. MatVAE Interpolation Start Value Styrofoam Vanadium Rhodium 275.0 GPa 0.30 Tungsten 411.0 GPa 0.28 0.37 0.002 GPa 128.0 GPa ν ρ 0.33 25 kg/m3 Start Value Styrofoam Invalid () 0.002 GPa 137.5 GPa ν ρ 0.33 25 kg/m3 0.31 6110 kg/m3 12450 kg/m3 19250 kg/m3 22570 kg/m3 End Value Osmium 550.0 GPa 0.25 Naive Interpolation Invalid () 275.0 GPa 0.29 Invalid () 412.5 GPa 0. 5661 kg/m3 11298 kg/m3 16934 kg/m3 22570 kg/m3 End Value Osmium 550.0 GPa 0.25 Figure 12: Interpolating in MatVAE latent space: an additional example of interpolation, complementary to Fig. 7d. 26 VoMP: Predicitng Volumetric Mechanical Property Fields Setting Position Material Youngs Modulus (Pa) Poissons Ratio Density (kg/m3) Fig. 13b Top-left () Fig. 13c Top-right () Bottom-left () Bottom-right () Top-left () Top-right () Bottom-left () Bottom-right () Aerographite Polyurethane Foam Rubber (soft) Styrofoam Aerogel Neoprene EPDM Rubber Flexible PVC (Plasticized) Fig. 13d Top-left () Top-right () Bottom-left () Bottom-right () Polystyrene Foam (EPS) Chloroprene Rubber (Neoprene) Polystyrene (Foam) Polybutylene (PB) Interpolated 4.4 105 4.8 106 3.1 106 1.6 106 4.4 106 1.0 107 6.6 106 4.8 107 2.6 106 5.0 106 5.8 106 2.5 108 True Range 1.0 105 1.0 106 1.0 105 5.0 106 3.0 106 5.0 106 1.0 106 3.0 106 1.0 106 1.0 107 1.0 106 1.0 107 5.0 106 1.0 107 2.0 107 1.0 108 1.0 106 5.0 106 5.0 106 5.0 106 2.5 106 7.0 106 2.5 108 3.0 108 Interpolated True Range Interpolated True Range 0.241 0.304 0.488 0.322 0.257 0.494 0.488 0.450 0.104 0.490 0.371 0.400 0.20.3 0.300.30 0.480.50 0.30.35 0.20.3 0.450.5 0.490.49 0.450.45 0.100.10 0.490.49 0.340.4 0.40. 0.2 298.2.0 952.0 22.6 1.0 1232.0 1100.9 1209.5 59.1 1200.8 34.8 932.0 0.20.2 50300 950950 1535 1.01.0 12301250 11001100 12001400 30100 12001200 1535 930 (a) Corner Materials for our experiments on moving across the latent space (Fig. 13b to 13d). Latent Dimension 1 Latent Dimension 1 Latent Dimension 1 Latent Dimension 1 Latent Dimension 1 Latent Dimension 1 2.23 101 1.37 101 1.14 101 1.00 101 6.85 102 1.66 101 1.11 101 1.10 101 1.10 101 1.08 10 2.25 101 2.25 101 1.56 101 1.51 101 1.49 101 1.65 101 2.06 101 1.29 101 1.08 101 9.86 102 1.29 101 1.39 101 1.84 101 1.22 101 1.04 101 1.15 101 1.23 101 1.33 101 1.49 101 1.17 101 2 2 o s e D t e a 2 2 o s e D t e a 1.13 101 1.07 101 9.85 102 9.75 102 9.65 102 1.77 101 1.20 101 1.10 101 7.01 102 3.30 102 1.13 101 1.06 101 9.57 102 8.76 102 8.19 102 1.11 101 1.04 101 9.19 102 7.99 102 3.97 102 2 2 o s e D t e a 1.56 101 1.07 101 2.38 102 2.37 102 2.27 102 1.33 101 4.18 102 2.37 102 1.82 102 1.80 102 1.05 101 1.11 101 1.17 101 1.26 101 1.37 101 1.10 101 9.78 102 9.13 102 4.35 102 2.21 10 1.27 101 2.44 102 1.89 102 1.73 102 1.70 102 (b) Setting 1. (c) Setting 2. Figure 13: Moving Across MatVAE latent space. We sample 3 different valid mechanical property triplets (E, ν, ρ) (Setting 1,2,3), corresponding to the middle square in the three color diagrams. We encode each of these triplets with MatVAE, and then traverse the 2D latent space to build 5 5 grid of latents around the starting value, which are each decoded to actual mechanical properties. To visualize if latent space dimensions correlate with actual simulation performance, we apply each meachnical property triplet to dropping bunny simulation and measure its mean displacement from rest, which is color coded in the graphs below. Each diagram (b, c, d) thus corresponds to 25 simulation runs with different parameters. We observe clear correlation between latent dimensions and simulation behavior. ( (d) Setting 3. : 4:40) VoMP: Predicitng Volumetric Mechanical Property Fields Figure 14: Data Annotation. We compare data annotation process of VoMP and Pixie. Table 4: Comparison of Mapped Material Properties between Pixies in-context physics examples (Le et al., 2025) and the datasets of known material properties. Item tree/pot tree/trunk tree/leaves flowers/vase flowers/flowers shrub/stems shrub/twigs shrub/foliage grass/blades soil (if visible) rubber_ducks_and_toys/toy sport_balls/ball soda_cans/can metal_crates/crate sand/sand jello_block/jello Mapped Materials Pixie ρ Dataset ρ Pixie Dataset Pixie ν Dataset ν Clay Brick Porcelain (Ceramic) Glass Ceramic Wood Oak (White) Maple Wood (Sugar) [400, 400] [400, 400] [1900, 1900] [2400, 2400] [2400, 2600] [700, 700] [770, 800] [630, 690] [2.000e+08, 2.000e+08] [2.000e+06, 2.000e+06] [2.000e+09, 6.000e+09] [7.000e+10, 7.000e+10] [9.000e+10, 1.100e+11] [8.000e+09, 1.100e+10] [1.200e+10, 1.500e+10] [1.000e+10, 1.300e+10] [0.40, 0.40] [0.40, 0.40] [0.20, 0.20] [0.20, 0.20] [0.24, 0.25] [0.30, 0.50] [0.30, 0.40] [0.30, 0.40] [200, 200] [2.000e+04, 2.000e+04] [0.40, 0.40] Glass (Soda-Lime) Glass (Borosilicate) [500, 500] [2500, 2500] [2300, 2300] [1.000e+06, 1.000e+06] [7.200e+10, 7.400e+10] [6.200e+10, 8.100e+10] [0.30, 0.30] [0.23, 0.23] [0.20, 0.20] Wood Wood Rubber (soft) EPDM Rubber Neoprene Sandy Loam [100, 100] [1.000e+04, 1.000e+04] [0.40, 0.40] [300, 300] [700, 700] [1.000e+05, 1.000e+05] [8.000e+09, 1.100e+10] [0.35, 0.35] [0.30, 0.50] [250, 250] [700, 700] [6.000e+04, 6.000e+04] [8.000e+09, 1.100e+10] [0.38, 0.38] [0.30, 0.50] [150, 150] [2.000e+04, 2.000e+04] [0.40, 0.40] [80, 80] [950, 950] [1100, 1100] [1230, 1250] [1.000e+04, 1.000e+04] [3.000e+06, 5.000e+06] [1.000e+07, 1.000e+07] [1.000e+06, 1.000e+07] [0.45, 0.45] [0.48, 0.50] [0.49, 0.49] [0.45, 0.50] [1200, 1200] [1600, 1800] [5.000e+05, 5.000e+05] [1.000e+08, 5.000e+08] [0.30, 0.30] [0.31, 0.31] Rubber (soft) EPDM Rubber Neoprene Flexible PVC (Plasticized) Rubber (soft) EPDM Rubber Neoprene Aluminium Aluminum 2024-T3 Aluminum 7075-T6 Steel Stainless Steel 17-7PH Stainless Steel 440A [80, 150] [80, 150] [2600, 2800] [2500, 2900] [950, 950] [1100, 1100] [1230, 1250] [1200, 1400] [950, 950] [1100, 1100] [1230, 1250] [2700, 2700] [2780, 2780] [2810, 2810] [7700, 7700] [7800, 7800] [7800, 7800] [3.000e+04, 5.000e+04] [3.000e+04, 5.000e+04] [5.000e+10, 8.000e+10] [8.000e+07, 1.200e+08] [3.000e+06, 5.000e+06] [1.000e+07, 1.000e+07] [1.000e+06, 1.000e+07] [2.000e+07, 1.000e+08] [3.000e+06, 5.000e+06] [1.000e+07, 1.000e+07] [1.000e+06, 1.000e+07] [7.000e+10, 7.000e+10] [7.240e+10, 7.240e+10] [7.100e+10, 7.100e+10] [2.000e+11, 2.000e+11] [2.040e+11, 2.040e+11] [2.000e+11, 2.000e+11] [0.40, 0.45] [0.40, 0.45] [0.25, 0.35] [0.25, 0.35] [0.48, 0.50] [0.49, 0.49] [0.45, 0.50] [0.45, 0.45] [0.48, 0.50] [0.49, 0.49] [0.45, 0.50] [0.35, 0.35] [0.33, 0.33] [0.33, 0.33] [0.31, 0.31] [0.30, 0.30] [0.30, 0.30] Sandy Loam [1800, 2200] [1600, 1800] [4.000e+07, 6.000e+07] [1.000e+08, 5.000e+08] [0.25, 0.35] [0.31, 0.31] [40, 60] [8.000e+02, 1.200e+03] [0.25, 0.35] snow_and_mud/snow_and_mud Sandy Loam [2000, 3000] [1600, 1800] [8.000e+04, 1.200e+05] [1.000e+08, 5.000e+08] [0.15, 0.25] [0.31, 0.31]"
        },
        {
            "title": "B COMPARISON WITH CONCURRENT WORK",
            "content": "Although Pixie (Le et al., 2025) is concurrent with us, we still compare aspects of our approach with Pixie. We discuss differences with Pixie in 2.2. Data Annotation Process. We compare our data annotation process with the annotation process of Pixie (Le et al., 2025) in Fig. 14. Our method performs annotation from meshes while Pixie (Le et al., 2025) gets points from training NeRF, which often produces noisy points, and the segmentation is performed based on CLIP features, which often produces noisy segmentation for difficult objects. Validity of Materials for Data Annotation. Although we do not have access to the Pixie (Le et al., 2025) dataset, Pixie uses in-context physics examples, which include material names and ranges of mechanical property triplets in the annotation process. We analyze these in-context physics examples and compare them with real material ranges from MTD (5.1) in Tb. 4. We find that some of these in-context properties might create pleasing simulations with particular simulator but can fall outside the range of real materials. 28 VoMP: Predicitng Volumetric Mechanical Property Fields Figure 15: Comparing Estimations. We compare property estimates between VoMP and Pixie for mesh. Comparison of Outputs. Although Pixie (Le et al., 2025) is concurrent with us, we still compare the outputs of Pixie with VoMP on one object in Fig. 15."
        },
        {
            "title": "C ABLATIONS",
            "content": "We provide an in-depth analysis motivating our MatVAE and Geometry Transformer training scheme by ablating each component in Tb. 5 and 6. Our ablations require changing the hyperparameters for fair comparisons; thus, for each ablation, we tune our hyperparameters within an identical compute budget. MatVAE vs Vanilla VAE. Technically, MatVAE (3) is built on top of the vanilla VAE (Kingma & Welling, 2022) and we can use vanilla VAE (Kingma & Welling, 2022) in its place. In Tb. 5, we show material property reconstruction and distributional metrics between vanilla VAE and our MatVAE. MatVAE outperforms the vanilla VAE in almost all metrics. We find that Vanilla VAE collapses to the Youngs Modulus property, giving us low reconstruction error for Youngs Modulus but significantly higher errors for other properties. Image Features. For aggregating image features (4.1), we experiment with using DINOv2 (Oquab et al., 2024), CLIP (Radford et al., 2021), and RGB colors by average pooling in the voxel. The results are shown in Tb. 6. Our model had many layers in the Geometry Transformer ( 4.2 and F.2) initialized from generation model (Xiang et al., 2025). This set of ablations was trained starting from random weights, due to the absence of generation weights for these settings. We find that using DINOv2 (Oquab et al., 2024) and CLIP (Radford et al., 2021) without initializing the weights from TRELLIS (Xiang et al., 2025) performs slightly worse, whereas simply using RGB colors performs significantly worse. MatVAE. While MatVAE acts as continuous tokenizer, it is possible to have the Geometry Transformer directly predict R3 vector i.e. directly predict the material triplets. We find this produces significantly worse results for Youngs Modulus estimation and Poissons Ratio estimation. Normalization Scheme. We experiment with different normalization schemes (3) like Z-score, and not using log-space trasnform for either Youngs Modulus or Density. All of these normalization schemes lead to significant degradation in performance. Most notably, removing the logarithmic scaling for Youngs Modulus (w/o log(E)) or using simple Z-score severely harms prediction accuracy. Loss. Our Geometry Transformer (4.2) is trained with ℓ2 loss for reconstruction (Equation (4)). We test the effect of replacing this with an ℓ1 loss. This change results in substantial drop in performance across all metrics, with errors increasing by factor of 2-3x for most properties. This indicates that the squared error penalty of the ℓ2 loss is more effective for this material property regression task. 29 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "D METRICS",
            "content": "We present an explanation of the metrics we use, and experiments on interpreting these metrics. D.1 METRICS FOR MASS AND FIELD ESTIMATION To evaluate the accuracy of predicted scalar quantities such as object mass, as well as continuous scalar fields like density or stiffness, we use several commonly adopted metrics. Let denote ground-truth scalar value or voxel-wise field (e.g., density), and ˆy its predicted counterpart. Absolute Difference Error (ADE). The average absolute error between predicted and groundtruth values: ADE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 yi ˆyi. (5) This metric is scale-sensitive and reports the error in physical units (e.g., kg/m3 for density, kg for mass). Absolute Log Difference Error (ALDE). The average absolute error in logarithmic space: (cid:88) ALDE = log yi log ˆyi. 1 i=1 (6) This metric captures multiplicative error and is particularly useful for quantities that vary over several orders of magnitude. Average Relative Error (ARE). The mean relative deviation between predictions and ground truth: ARE = 1 (cid:88) i=1 (cid:12) (cid:12) (cid:12) (cid:12) yi ˆyi yi (cid:12) (cid:12) (cid:12) (cid:12) . (7) This dimensionless metric penalizes overand under-estimates proportionally, making it appropriate for comparing across varying scales. Minimum Ratio Error (MnRE). symmetric and bounded measure of relative accuracy: MnRE = 1 (cid:88) i=1 min (cid:18) yi ˆyi , ˆyi yi (cid:19) . (8) This metric ranges from 0 to 1 and is maximized when predictions are perfectly accurate. As suggested in prior work (Standley et al., 2017), MnRE avoids bias toward systematic overor underestimation and reduces sensitivity to outliers, making it particularly effective for evaluating physical quantity predictions across heterogeneous samples. Table 5: MatVAE Ablation. We ablate MatVAE by comparing it against vanilla VAE and present reconstruction and distirbutional metrics. Model Youngs Modulus (E) Poissons Ratio (ν) Density (ρ) Vanilla VAE (Kingma & Welling, 2022) MatVAE w/o NF w/o TC penalty w/o free nats Vanilla VAE (Kingma & Welling, 2022) MatVAE w/o NF w/o TC penalty w/o free nats W1 () 0.0653 0.0405 (-0.025) 0.0339 0.0633 0.0749 W2 () DKL () 0.0547 0.0868 0.0798 0.1379 (+0.083) (-0.007) 0.0441 0.0447 0.0500 0.0855 0.1311 0.1168 W1 () 0.0849 0.0317 (-0.053) 0.0417 0.0844 0.2014 W2 () 0.1057 0.0437 (-0.062) 0.0504 0.1052 0. DKL () W1 () W2 () 0.0744 0.0547 0.0689 0.0172 0.0132 0.0342 (-0.057) (-0.042) (-0.035) 0.0819 0.0599 0.0848 0.0715 0.0525 0.0672 0.0507 0.0421 0.6376 DKL () 0.0175 0.0260 (+0.009) 0.0529 0.0109 0.0223 log(E) () ν () ρ () log(E/ρ) () log(G) () log(K) () L.S. () E.A. () BrayCurtis () 0.0512 0.0034 (-0.048) 0.0020 0.0499 0.0036 15366.8750 0.0426 (-15366.8) nan 15567.0986 16332.7829 0.8306 0.0330 (-0.798) 0.0160 0.8298 0. 0.0542 0.0054 (-0.049) 0.0033 0.0537 0.0053 0.0544 0.0036 (-0.051) 0.0021 0.0530 0.0037 0.0447 0.0036 (-0.041) 0.0021 0.0437 0.0041 0.2384 0.0131 (-0.225) 0.0086 0.2382 0.0116 2.1893 0.4439 (-1.745) 0.1729 2.1514 0.2966 0.4690 0.0411 (-0.428) 0.0234 0.4562 0. 30 VoMP: Predicitng Volumetric Mechanical Property Fields Table 6: Ablations. We ablate VoMP with choice of image features, using MatVAE, normalization scheme, and choice of loss function. We report the voxel-level mechanical property difference errors. Method Youngs Modulus Pa (E) Poissons Ratio (ν) ALDE () ALRE () ADE () ARE () Density kg ADE () m3 (ρ) ARE () Image Features (Geometry Transformer start from random weights, 4.1) w/ DINOv2 (Oquab et al., 2024) w/ CLIP (Radford et al., 2021) w/ RGB colors 0.2888 (0.41) 0.2695 (0.42) 1.2176 (0.88) 0.0536 (0.06) 0.0508 (0.06) 0.6593 (0.49) 0.0259 (0.02) 0.0250 (0.02) 0.1379 (0.06) 0.0803 (0.08) 0.0771 (0.07) 1.1642 (0.78) 373.5183 (675.90) 383.5844 (766.41) 3678.4451 (8421.17) 0.3126 (0.79) 0.3110 (0.85) 4.7430 (2.85) MatVAE (3) w/o MatVAE Normalization Scheme (3) w/ Z-score w/o log(ρ) w/o log(E) Loss w/ ℓ1 Ours 1.1284 (0.52) 0.1289 (0.08) 0.0480 (0.02) 0.1638 (0.08) 917.5879 (428.50) 0.8637 (0.61) 0.8838 (0.61) 0.6654 (0.54) 0.9033 (0.60) 0.0996 (0.08) 0.0748 (0.07) 0.1024 (0.09) 0.0814 (0.04) 0.0542 (0.03) 0.1182 (0.05) 0.2938 (0.20) 0.1806 (0.11) 0.4189 (0.25) 5269.2900 (946.03) 549.9512 (513.57) 4051.9121 (838.98) 6.4656 (4.00) 0.5976 (0.39) 5.0428 (3.22) 0.8947 (0.62) 0.1038 (0.09) 0.0468 (0.04) 0.1666 (0.16) 568.7543 (734.10) 0.6337 (0.88) 0.3765 (0.39) 0.0421 (0.05) 0.0250 (0.01) 0.0837 (0.03) 113.3807 (301.90) 0.0908 (0.14) D.2 METRICS TO MEASURE DIFFERENCES IN MECHANICAL PROPERTIES We use multiple commonly used metrics for measuring differences between mechanical properties. Relative Error in log(E). Relative error between predicted and true values of the logarithm of Youngs modulus reported in units of Pa. This captures relative error in material stiffness across several orders of magnitude. Relative Error in ν. Relative Error in linear space for Poissons ratio ν, dimensionless measure of lateral contraction under uniaxial loading. Relative Error in ρ. Relative Error between predicted and true values of material density ρ, reported in units of kg/m3. Relative Error in log(E/ρ). Relative Error in the logarithm of specific modulus, where is Youngs modulus and ρ is density. Reflects relative deviation in stiffness-to-weight efficiency. Relative Error in log(G). Relative Error in the logarithm of shear modulus = senting resistance to shear deformation. 2(1+ν) , repreRelative Error in log(K). Relative Error in the logarithm of bulk modulus = terizing resistance to uniform volumetric compression. 3(12ν) , characLightweight Stiffness Ashby Index (P = E1/2/ρ). The Relative Error in log(P ), where = E1/2/ρ, reflecting relative error in predicting material efficiency for maximizing stiffness per unit weight (Ashby & Cebon, 1993). Energy Absorption Ashby Index (P = E1/3/ρ). The Relative Error in log(P ), where = E1/3/ρ, quantifying relative deviation in predicted energy absorption efficiency (Ashby & Cebon, 1993). BrayCurtis dissimilarity. BrayCurtis dissimilarity (Bray & Curtis, 1957) between predicted and ground-truth property vectors and y: BC(x, y) = . (9) (cid:80) (cid:80) xi yi i(xi + yi) normalized, dimensionless measure in [0, 1] capturing overall distributional divergence across multiple material properties. D.3 METRICS TO MEASURE DIFFERENCES IN DISTRIBUTIONS We use multiple commonly-used metrics for measuring differences between distributions. 31 VoMP: Predicitng Volumetric Mechanical Property Fields time Figure 16: Simulations to Interpret Errors. We demonstrate simulations performed to show the relation between relative error and simulation error. Wasserstein1 Distance (W1). For probability measures µ, ν on space , W1(µ, ν) = inf γΠ(µ,ν) (cid:90) x dγ(x, y), (10) where Π(µ, ν) is the set of all couplings of µ and ν. W1 equals the minimum average work to move mass from µ to ν. Wasserstein2 Distance (W2). For probability measures µ, ν on space , W2(µ, ν) = (cid:16) (cid:90) inf γΠ(µ,ν) y2 dγ(x, y) (cid:17)1/2 , (11) where Π(µ, ν) is the set of all couplings of µ and ν. The root-mean-square transport cost between µ and ν. KullbackLeibler Divergence (DKL). For densities p, on , DKL(pq) = (cid:90) p(x) log p(x) q(x) dx. (12) DKL measures the expected extra log-likelihood of data drawn from when it is coded using instead of p. D.4 INTERPRETING ERRORS FOR MATERIAL PROPERTY ESTIMATION We experimentally demonstrate an interpretation of how relative changes in material properties affect simulations of the finite element method (FEM) solver. We do so by simulating the deformation of unit cubes under many different material properties and scenarios with an FEM solver. For each baseline triplet (E0, ν0, ρ0) representing Youngs modulus, Poissons ratio, and density, we introduce variations following the scaling laws: density variations follow linear scaling ρnew = ρ0(1 + ), Poissons ratio variations use the same linear relationship νnew = ν0(1 + ), while Youngs modulus variations use exponential scaling Enew = E0e to accommodate the wide range of stiffness values. We then apply every such unique material triplet to unit cube and perform simulation under some external forces. During each of these simulations, we measure the final volume and potential energy of the cube after the Newton iterations have converged. Measuring Volume. For body undergoing deformation, the deformation gradient = + maps material points from the reference to the current configuration, where represents the displacement field and is the 3 3 identity tensor. The local volume change is quantified by the Jacobian = det(F), which represents the ratio of deformed to reference volume at each material point. The total deformed volume is: Vdef = (cid:82) dV , where Ω0 denotes the reference configuration. The relative volume change, defined as /V = (Vdef V0)/V0, provides dimensionless measure of volumetric deformation. Ω0 Measuring Potential Energy. We compute the total potential energy by combining elastic strain energy and kinetic-potential contributions. We use corotated linear elasticity, where we calculate the deformation gradient = + and symmetric strain tensor = 1 2 (F + FT ) to obtain the energy density = µ tr(S2) + λ 2 (tr(S))2, with Lamé parameters µ and λ derived from the Youngs modulus and Poissons ratio. We use three distinct contributions in the kinetic-potential term: an 32 VoMP: Predicitng Volumetric Mechanical Property Fields Youngs Modulus (E) Poissons Ratio (ν) Density (ρ) Figure 17: Gripping Force by Robots. We demonstrate the relation between relative errors in materials and relative change in P.E. (top) and volume (bottom). We then show the confidence bounds in light shaded regions. inertial component Einertia = (cid:82) 2t2 un+1 un2 dV that captures displacement changes between iterations in our quasi-static solver, gravitational potential Egravity = (cid:82) Ω ρ dV accounting for body forces, and an external work term Eext = (cid:82) Ω fext dV representing the applied loads. We thus compute the total potential energy as Etotal = (cid:82) Ω dV + Einertia + Egravity + Eext, evaluated at the converged displacement field. Ω ρ We perform the simulations in the following scenarios, Gripping force by robots. We simulate 140 compressive force, which is common in robotic gripping applications, for example, the Franka Emika (frankaemika, 2025) Hand end effector applies maximum of 70 per finger with maximum clamping force of 140 N. We demonstrate the results from 486 simulations in this setting, all of which were run to convergence in Fig. 17. Impact Force on Dropping Objects. We simulate 120 impact force that simulates package drop scenarios, calculated from the impact dynamics of 1 kg package dropped from 0.6 height with 5 cm deformation distance. We demonstrate the results from 486 simulations in this setting, all of which were run to convergence in Fig. 18. Tensile Testing Machines. We simulate 330 force corresponding to standard tensile testing conditions employed in bench-top universal testing machines (ASTM Committee D20, 2022; ASTM Committee E28, 2024). We demonstrate the results from 486 simulations in this setting, all of which were run to convergence in Fig. 19. Tension. We simulate 200 force, which represents typical pretension in tendon-driven robotic systems, where continuum arms and wearable assistive devices maintain structural stiffness through cable tensions (Schäffer et al., 2024). We demonstrate the results from 486 simulations in this setting all of which were run to convergence in Fig. 20."
        },
        {
            "title": "E DATASET DETAILS",
            "content": "We present additional details about our datasets for training MatVAE (3) and Geometry Transformer (4.2). 33 VoMP: Predicitng Volumetric Mechanical Property Fields Youngs Modulus (E) Poissons Ratio (ν) Density (ρ) Figure 18: Impact Force on Dropping Objects. We demonstrate the relation between relative errors in materials and relative change in P.E. (top) and volume (bottom). We show the confidence bounds in light shaded regions. Youngs Modulus (E) Poissons Ratio (ν) Density (ρ) Figure 19: Tensile Testing Machine. We demonstrate the relation between relative errors in materials and relative change in P.E. (top) and volume (bottom). We show the confidence bounds in light shaded regions. E.1 ANNOTATION WITH VISION-LANGUAGE MODEL To create our training dataset (5) for the Geometry Transformer, we use Vision-Language Model (VLM) coupled with multiple other data sources like 3D assets, component-wise part segmentations, material databases (5.1), visual textures, and material names to annotate our dataset. We run the VLM on every segment of every object individually. We experiment with Qwen2.5-VL 7B, Qwen2.5-VL 32B, Qwen2.5-VL 72B (Bai et al., 2023; 2025), VL-Rethinker (Wang et al., 2023), SpatialRGPT (Cheng et al., 2024), and Cosmos Nemotron (Lin et al., 2024a). We experimentally choose Qwen2.5-VL 72B for data annotation. We show the system prompts and user prompts we use in Fig. 21 to 24. We find the best performing system prompts with TextGrad (Yuksekgonul et al., 34 VoMP: Predicitng Volumetric Mechanical Property Fields Youngs Modulus (E) Poissons Ratio (ν) Density (ρ) Figure 20: Tension. We demonstrate the relation between relative errors in materials and relative change in P.E. (top) and volume (bottom). We show the confidence bounds in light shaded regions. Table 7: VLM Annotation Errors. Errors for the VLM annotation for mechanical property annotation. log(E) () ν () ρ () log(E/ρ) () log(G) () log(K) () L.S. () E.A. () BrayCurtis () 0.0295 0.0426 0.1348 0. 0.0303 0.0330 0.2022 0.2162 0.2342 2025). We show response from the model for one of the segments from an object in our dataset in Fig. 25 and 26. We construct tiny dataset consisting of complex objects that are manually annotated and compare these properties with the outputs from the VLM in Tb. 7. We observe that the VLM, given significant additional information, performs very well in annotating complex objects. We report the commonly used metrics we list in D.2 for measuring differences in mechanical properties. E.2 DATASET STATISTICS Our dataset comprises diverse collection of 1,692 objects, sourced from four datasets: simready (NVIDIA Developer, 2025), residential (NVIDIA Corporation, 2025c), vegetation (NVIDIA Corporation, 2025d), and commercial (NVIDIA Corporation, 2025a). As shown in Tb. 8, the majority of objects belong to the simready and residential categories, with vegetation and commercial objects providing additional variety. Each object is decomposed into multiple segments, with total of 8,128 segments across the dataset. Most parts are labeled with English material names, and for few parts that do not have these material names, we infer these from the PBR texture names that were applied to these parts. To characterize the physical realism and diversity of the dataset, we analyze the distribution of key material properties for all segments in Tb. 9. The wide range of material properties highlights the heterogeneity of the dataset, which is essential for robust learning and evaluation of material-aware models. We summarize the most frequent material categories (e.g., metal, plastic, wood, cardboard) and object classes (e.g., residential, shelf, container), along with their respective counts and proportions in Tb. 10. We report the statistics of our Material Triplet dataset in Tb. 11. To train MatVAE, we use the \"Filtered Dataset\". 35 VoMP: Predicitng Volumetric Mechanical Property Fields System Prompt. You are materials science expert specializing in analyzing material properties from visual appearances and physical data. Your task is to provide precise numerical estimates for Youngs modulus, Poissons ratio, and density based on the images and context provided. Important Context: The material segment you are analyzing may be an internal component or structure that is not visible from the outside of the object. For example: - Internal support structures, frames, or reinforcements - Hidden layers or core materials - Components enclosed within the outer shell - Structural elements that are only visible when the object is disassembled When analyzing: - Consider that the material might be completely hidden from external view - Use the semantic usage and material type hints to infer properties of internal components - Internal structural components often have different properties than visible surfaces - For example, soft exterior might hide rigid internal frame Critical Instruction: You MUST provide numerical estimates for ALL materials, even organic, biological, or unusual materials like leaves, feathers, or paper. - For organic materials, estimate properties based on similar natural materials with known values - For leaves, consider them as thin plant fiber composites with values similar to paper or dried plant fibers - Never respond with \"N/A\" or any non-numeric value in your property estimates When analyzing materials, use step-by-step reasoning: 1. First identify the likely material class and subtype based on visual appearance (if visible) or contextual clues (if internal) 2. Consider how texture, color, and reflectivity inform your understanding of the material (when visible) 3. Incorporate the provided physical properties and contextual usage information 4. For each mechanical property, reason through how the visual and physical attributes lead to your estimate 5. Consider how the material compares to reference materials with known properties 6. If the material appears to be internal/hidden, use the object type and usage context to make informed estimates Important Formatting Requirements: - Youngs modulus must be provided in scientific notation followed by \"Pa\" (e.g., 2.0e11 Pa) - Poissons ratio must be simple decimal between 0.0 and 0.5 with no units (e.g., 0.34) - Density must be provided in kg/m^3 (e.g., 7800 kg/m^3) - Each property must be on its own line with exactly the label shown in the examples - Do not include explanatory text or parenthetical notes after the values - ALWAYS provide numerical values, never text like \"N/A\" or \"unknown\" Figure 21: System Prompt. The System Prompt we use for every segment of every object. Figure 27: Youngs Modulus Pa (E). Histogram of Youngs Modulus in our Geometry with Volumetric Materials Dataset (5). Figure 28: Poissons Ratio (ν). Histogram of Poissons Ratio in our Geometry with Volumetric Materials Dataset (5). Figure 29: Density kg m3 . Histogram of Density in our Geometry with Volumetric Materials Dataset (5). VoMP: Predicitng Volumetric Mechanical Property Fields Table 8: Dataset Statistics. Number of objects, total segments, total points, average segments per object (std. dev.), and average points per object (std. dev.) for each dataset. Dataset Total Objects Segments (%) Voxels (%) Avg. Segments/Object Avg. Voxels/Object commercial residential simready vegetation train validation test Total 82 449 1029 104 1333 165 166 650 (8.0) 4225 (52.2) 2544 (31.5) 670 (8.3) 6477 (80.1) 552 (6.8) 1060 (13.1) 1,812,064 (4.9) 9,109,380 (24.4) 24,148,660 (64.7) 2,267,848 (6.1) 28,709,190 (76.9) 3,719,996 (10.0) 4,908,766 (13.1) 8089 (100.0) 37,337,952 (100.0) 7.93 (7.19) 9.41 (21.82) 2.47 (1.33) 6.44 (4.53) 4.86 (12.69) 3.35 (3.19) 6.39 (11.33) 4.86 (11.97) 22,098 (22,774) 20,288 (21,714) 23,468 (25,032) 21,806 (19,428) 21,537 (23,431) 22,545 (23,095) 29,571 (25,987) 22,439 (23,786) Table 9: Material property statistics for all segments in the dataset. We report the minimum, maximum, mean, median, standard deviation, and outlier count (% of values outside 3σ) for Youngs modulus, Poissons ratio, and Density. Property Density (kg/m3) Youngs Modulus (Pa) Poissons Ratio Min 5.0 101 1.0 105 1.6 101 Max 1.93 104 2.8 1011 4.9 101 Mean 2.28 103 4.19 1010 3.36 101 Median 1.20 103 1.0 1010 3.5 10 Std Dev Outliers (%) 2.44 103 6.53 1010 4.36 102 25 (0.3) 165 (2.0) 88 (1.1)"
        },
        {
            "title": "F ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "We present additional implementation details. F.1 DESIGN OF MATVAE We explain the motivation behind the design of MatVAE. Normalizing Flow. Material triplets remain statistically non-Gaussian even after normalizalog10 E, log10 ρ; boundary-concentrated ν [0, 0.5)), so tion (heavy-tailed/multi-modal diagonal-Gaussian qϕ(z m) tends to mode-average and miscalibrate tails. We therefore parameterize the posterior with bijective normalizing flow (Rezende & Mohamed, 2015) fψ where ψ is the parameter of the flow network f, and Ψ is the space of all parameters ψ. This fψ applied to Gaussian base q0: sample q0(u m) = (u; µϕ(m), diag σ2 ϕ(m)), set = fψ(u), and compute the density by change of variables log qϕ(z m) = log q0 (cid:0)f 1 ψ (z) m(cid:1) (cid:123)(cid:122) (cid:125) base density + log(cid:12) (cid:124) (cid:12)det Jf 1 (cid:123)(cid:122) log-Jacobian ψ (z)(cid:12) (cid:12) (cid:125) (cid:124) = log q0(u m) (cid:123)(cid:122) (cid:125) base density (cid:124) log(cid:12) (cid:124) (cid:12)det Jfψ (u)(cid:12) (cid:12) (cid:123)(cid:122) (cid:125) log-Jacobian (cid:12) (cid:12) (cid:12)u=f 1 ψ (z) . (13) with standard normal prior p(z) = (0, I) and decoder likelihood pθ(m z). Table 10: Most frequent high-level material categories and object classes. We report the top high-level material categories (aggregated and deduplicated) and the most common object classes in the dataset, with their respective counts and percentages. Mat. Category Count (%) Object Class Count (%) Metal Plastic Wood Cardboard Leather Chrome Glass Fabric Rubber Stone 1434 (17.7) 553 (6.8) 707 (8.7) 315 (3.9) 140 (1.7) 171 (2.1) 85 (1.0) 60 (0.7) 55 (0.7) 40 (0.5) residential shelf container cardboard box vegetation crate commercial pallet barricade tape inclined plane 477 (28.2) 250 (14.8) 193 (11.4) 106 (6.3) 105 (6.2) 101 (6.0) 82 (4.8) 67 (4.0) 59 (3.5) 22 (1.3) 37 VoMP: Predicitng Volumetric Mechanical Property Fields Table 11: Dataset Details. We present statistics of our dataset for training MatVAE (3)."
        },
        {
            "title": "Size",
            "content": "249 105456 101517 This keeps the ELBO form unchanged while strictly enlarging the variational family (the identity map recovers the Gaussian case), allowing qϕ to match the true posterior and avoid mode-averaging on (E, ν, ρ). We instantiate fψ as single radial flow, displacement (cid:122) (cid:125)(cid:124) (cid:123) (u z0) , fψ(u) = + β h(u) (cid:124) (cid:123)(cid:122) (cid:125) radial scale 1 (cid:0)α + z02 h(u) = (cid:1) , (14) whose log-determinant has the closed form (substitute = h2 in the closed form from (Rezende & Mohamed, 2015)), log det Jfψ (u) = (D 1) log(cid:0)1 + βh(u)(cid:1) (cid:125) (cid:123)(cid:122) angular (cid:124) + log(cid:0)1 + βh(u) β h(u)2 r(u)(cid:1) , (cid:125) (cid:123)(cid:122) radial (cid:124) (15) r(u) = z02, where is the dimensionality of the latent space and z0 is trainable D-dimensional vector (one per flow layer) representing the centre of the deformation. Radial flows are invertible iff α > 0 and β > α, we satisfy these by α = softplus(α), β = α + softplus( β) (Rezende & Mohamed, 2015) with unconstrained trainable parameters α, β R. We show our implementation in Algorithm 1. Penalizing TC. We observed high dependence between latent coordinates in the aggregated posterior qϕ(z) (both dimensions tended to encode ρ). Thus, we decompose the KL-divergence term of the ELBO following (Chen et al., 2018). For MatVAE, this allows us to directly penalize the total correlation TC(z) = KL(qϕ(z) (cid:81) qϕ(zj)) where qϕ(z) is the aggregated posterior, zj is the {1, 2}-th coordinate of the latent vector z. This allowed us to reduce the high dependence between latent coordinates, causing both dimensions to encode density. During training, we follow (Chen et al., 2018) and approximate the aggregated posterior qϕ(z) = Empdata [qϕ(z m)] using samples from mini-batch where pdata is the empirical data distribution. Preventing Posterior Collapse. For fixed m, define per-dimension marginals qϕ(zj m) and KLj(z m) = KL (qϕ(zj m) p(zj)) . (16) Then, for each m, KL (qϕ(z m) p(z)) = (cid:88) KLj(m) + TC (qϕ(z m)) (cid:125) (cid:123)(cid:122) (cid:124) KL(qϕ(zm) (cid:81) qϕ(zj m)) . (17) So the total KL contains per-dimension rate term plus per-sample total correlation. In the Gaus- (cid:0)µj(m)2 + σj(m)2 log σj(m)2 1(cid:1), whose gradients sian (no-flow) case, KLj(z m) = 1 2 drive µj 0, σj 1 under posterior collapse. We therefore impose capacity constraint (free nats) on the dim-wise term to prevent collapse: (cid:88) j=1 max(cid:0)δ, EpdataKLj(z)(cid:1) , (cid:125) (cid:123)(cid:122) (cid:124) free-nats 38 (18) VoMP: Predicitng Volumetric Mechanical Property Fields Algorithm 1 MatVAE posterior update with radial normalizing flow. Require: Batch RB3 Encoder outputs µ(x) RBD Encoder outputs log σ2(x) RBD Flow param z0 R1D Flow param log α Flow param βraw Prior p(z) = (0, I) Ensure: Flowed latent RBD Post-flow log-density log q(z x) RB KL term denoted as KL Reconstruction loss Lrecon d)(cid:1) (α+r) 2 log σ2) ε d=1 log (cid:0)zbase,d; µd, exp(log σ2 1: Encode and reparameterize 2: µ Encoderµ(x) 3: log σ2 Encoderlog σ2 (x) 4: ε (0, I) 5: zbase µ + exp( 1 6: 7: Base posterior log-density 8: log q0 (cid:80)D 9: 10: Radial flow parameters (α > 0, β > α) 11: α softplus(log α) + εα 12: β α + softplus(βraw) 13: 14: Radial flow transform 15: diff zbase z0 16: diff2 + εr 17: 1 18: zbase + β diff 19: 20: Log-determinant of Jacobian 21: bh β 22: bhstab clamp(bh, c, c) 23: term1 (D 1) log(1 + bhstab) 24: term2 log(1 + bhstab β h2 r) 25: log term1 + term2 26: 27: Change of variables and KL pieces 28: log log q0 log 29: log pz (cid:80)D 30: 31: Losses (non-relevant details shown as . . .) 32: ( ˆE, ˆν, ˆρ, . . .) Decoder(z) 33: Lrecon MSE/NLL in transformed space (. . .) 34: KL log log pz 35: Lrecon + . . . 36: return (z, log q, KL, Lrecon) d=1 log (zd; 0, 1) which enforces minimum information budget δ = 0.1 per coordinate (zero subgradient below δ). This allows us to fix the empirically observed imbalance where one latent carried most information and the other collapsed. An aggregated alternative consistent with the KL decomposition is max(cid:0)ϕ d, (cid:80) KL(qϕ(zj) p(zj))(cid:1). VoMP: Predicitng Volumetric Mechanical Property Fields Table 12: Training Hyperparameters. We show the hyperparameters for the MatVAE and Geometry Transformer."
        },
        {
            "title": "Training Precision\nHidden Width\nNetwork Depth\nLatent Dimensions\nDropout Rate\nEpochs\nBatch Size\nOptimizer\nLearning Rate\nWeight Decay\nLR Scheduler\nFinal Learning Rate\nGradient Clipping",
            "content": "β-TC Loss Weights Free Nats KL Annealing Epochs Data Normalization F.2 NETWORK DESIGN FP-32 256 3 (2) 2 0.05 850 256 AdamW 104 104 Cosine Annealing 105 5.0 α = 1.0 (KL) β = 2.0 (TC) γ = 1.0 (MI) 0.1 200 Log Min-Max"
        },
        {
            "title": "Training Precision\nVoxel Grid Resolution\nInput Channels\nModel Channels\nLatent Channels\nTransformer Blocks\nAttention Heads\nMLP Ratio\nAttention Mode\nWindow Size\nMax Training Steps\nBatch Size per GPU\nTotal Batch Size\nOptimizer\nLearning Rate\nWeight Decay\nGradient Clipping\nLoss Function\nEMA Rate",
            "content": "FP-16 64³ 1024 768 2 12 12 4 Swin 8 200,000 4 16 AdamW 104 5 102 1.0 ℓ2 0.9999 We now present our network architecture. MatVAE. The encoder architecture begins by projecting the 3-dimensional material triplet through linear transformation into 256-dimensional hidden space, followed by SiLU activation. The resulting representation then passes through three \"ResidualBlocks\", each using bottleneck design that compresses the 256-dimensional vector to 128 dimensions via LayerNorm and SiLU activation, applies another linear transformation, and restores the original dimensionality through second LayerNorm-SiLU sequence. Each \"ResidualBlock\" maintains skip connection that adds the input directly to the final output. The encoder finally has separate linear heads that project the processed representation into the latent space parameters: one head predicts the posterior mean µϕ(m) and another predicts the log-variance log σ2 ϕ(m) for the 2-dimensional latent code z. The decoder mirrors this architecture in reverse, beginning with linear projection from the 2dimensional latent space back to the 256-dimensional hidden representation, followed by SiLU activation. The latent encoding then goes through three \"ResidualBlocks\" with an identical bottleneck structure and skip connections as the encoder. Finally, three separate linear heads decode the processed representation into the reconstructed material properties: Youngs modulus, Poissons ratio, and density, each predicted as scalar values in the normalized space. Geometry Transformer. Our model is based on TRELLIS (Xiang et al., 2025). We use transformer-based architecture specifically designed for processing sparse voxel representations with associated material properties. The model operates on 643 resolution voxel grid, accepting 1024-dimensional DINOv2 visual features as input and compressing them to compact 2dimensional latent representation through 12-layer transformer backbone. Each transformer block utilizes 12 attention heads with 4:1 MLP expansion ratio, using Swin attention with 88 local windows. During training, the Geometry Transformer operates in conjunction with frozen MatVAE that decodes the latent into material properties. F.3 TRAINING We present our voxelization scheme for training on meshes in Algorithms 2 and 3. We present the hyperparameters used for training MatVAE and Geometry Transformer in Tb. 12. 40 VoMP: Predicitng Volumetric Mechanical Property Fields Algorithm 2 Segment-aware volumetric voxelization for meshes. Require: Full-mesh vertices Vall RN 3, faces Fall Segments = {(Vi RNi3, Fi, sidi)}M i=1 Grid resolution (voxel pitch = 1/r) Per-segment cap Kseg Global cap Kall Ensure: Combined voxel centers Call RL3 within [0.5, 0.5]3 Segment identifiers sidall {str}L Discretized centers ˆCall RL3 on an r3 grid 1: Global normalization from the full mesh 2: vmin min(Vall) 3: vmax max(Vall) 4: (vmin + vmax)/2 5: max(vmax vmin) 6: ε 106 7: 8: Cacc [ ] 9: sidacc [ ] 10: for = 1 to do 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: choice(Ci, Kseg, without replacement) Ci Ci[I] if Ci = 0 then continue Cacc.append(Ci) sidacc.append([sidi]Ci) Voxelize segment and solid-fill (Algorithm 3) (Ci, Yi) VOXELIZESOLID(V , if Kseg is given and Ci > Kseg then , r) Normalize segment to [0.5, 0.5]3 and ensure triangles clip((Vi c)/s, 0.5 + ε, 0.5 ε) triangulate(Fi) return (, , ) 24: 25: if Cacc = 0 then 26: 27: Call concat(Cacc) 28: sidall concat(sidacc) 29: 30: Optional global subsampling 31: if Kall is given and Call > Kall then 32: 33: 34: choice(Call, Kall, without replacement) Call Call[I] sidall sidall[I] 35: 36: Discretize to an r3 grid aligned with [0.5, 0.5]3 37: clip((Call + 0.5) r, 0, 1) 38: ˆCall J/r 0.5 39: return (Call, sidall, ˆCall) Voxelization For Training. Our training dataset contains Universal Scene Description (USD) files with multi-segment meshes. Each mesh is normalized to the range [0.5, 0.5] using global bounding box computed across all segments to preserve relative spatial relationships. We use volumetric voxelization using regular 3D grid with resolution of 1 64 , where each voxel center is tested for interior containment within the mesh volume through point-in-polyhedron testing, followed by vol41 VoMP: Predicitng Volumetric Mechanical Property Fields Algorithm 3 Voxelization and flood fill primitives for meshes. Grid setup over padded mesh AABB 1/r amin min(V ); amax max(V ) bmin amin bmax amax + nx (bmax,x bmin,x)/h ny (bmax,y bmin,y)/h nz (bmax,z bmin,z)/h S[nx, ny, nz] false X[nx, ny, nz] false 1: procedure VOXELIZESOLID(V, F, r) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: Triangle rasterization: mark surface cells for each triangle = (v0, v1, v2) do for = kmin to kmax do for = jmin to jmax do S[i, j, k] true Compute triangle AABB [tmin, tmax] in world coordinates Convert to grid index ranges (imin : imax, jmin : jmax, kmin : kmax) for = imin to imax do Cell box = [bmin + (i, j, k)h, bmin + (i + 1, + 1, + 1)h] if TRIANGLEBOXINTERSECT(t, B) then 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: Exterior marking by flood fill on non-surface cells Initialize queue with boundary indices (i, j, k) where S[i, j, k] = false while not empty do Q.pop() if X[u] then continue X[u] true for each 6-neighbor of within bounds do if S[v] = false and X[v] = false then Q.push(v) Solid fill (interior) and center extraction S [ ] for all indices (i, j, k) where [i, j, k] = true do bmin + (i + 0.5, + 0.5, + 0.5) C.append(c) return (C, ) umetric filling to generate solid voxel representations rather than surface-only discretizations. All the voxels inside given segment receive the material properties of the segment they lie in. Rendering for Training. For multi-view image rendering of meshes, we use path-tracing renderer to produce photorealistic renderings of 3D objects. Camera viewpoints are sampled using quasi-random Hammersley sequence distributed uniformly across sphere. For training and testing, we render 150 views, though our method can work by rendering as many views as needed, with cameras positioned at fixed radius of 2 units from the object center and configured with 40-degree field of view. Images are rendered at 512512 pixel resolution. The rendering pipeline outputs both the RGB images and the corresponding camera extrinsics and intrinsics. For rendering splats, we simply replace the renderer with the 3D Gaussian Splat renderer (Kerbl et al., 2023) in our workflow. For rendering SDFs, we render meshes using many points collected from the SDF. For rendering NeRFs (Mildenhall et al., 2020), we simply replace the renderer with nerfstudio (Tancik et al., 2023) in our workflow. 42 VoMP: Predicitng Volumetric Mechanical Property Fields Feature Aggregation. For visual feature extraction, we employ DINOv2-ViT-L/14 (Oquab et al., 2024) with registers. We use patch size of 1414 pixels and process input images resized to 518518 pixels, resulting in 3737 patch. We use the nv-dinov22 (NVIDIA, 2025) implementation. F.4 SIMULATION AND RENDERING For our mesh simulations  (Fig. 5)  , we simulate with the finite-element method (FEM) using the libuipc (Huang et al., 2025; 2024a) implementation, and we render the simulations in pathtracing-based renderer. While comparing with other simulators  (Fig. 2)  we use MPM (Sulsky et al., 1994) using taichi-mpm (Hu & contributors, 2018), XPBD (Macklin et al., 2016) using PositionBasedDynamics (Bender & contributors, 2015), and FEM using Warp (Macklin, 2022). For our large-scale splat simulations or splat + mesh simulations, we use Simplicits (Modi et al., 2024) using the sparse simplicits implementation using Kaolin (Modi et al., 2024; Fuji Tsang et al.). For rendering our large-scale splat simulations or splat + mesh simulations, we use Polyscope (Sharp et al., 2019) and composite splat renders from gsplat (Ye et al., 2024). For these simulations, we apply material property tolerances to reduce numerical noise: voxels with Youngs modulus differing by less than 101 Pa, Poissons ratio by less than 103, or density by less than 101 kg/m³ are assigned identical values for the respective property. We present additional details for deforming and rendering deformed Gaussian Splats in G.5. F.5 BASELINES Converting Hardness to Youngs Modulus. NeRF2Physics (Zhai et al., 2024) does not estimate numerical value of Youngs Modulus, but instead predicts Shore A-Shore hardness. Thus, to compare our method with NeRF2Physics (Zhai et al., 2024) we convert these Shore hardness values to average Youngs Modulus values. Shore A. For Shore hardness, we follow (ASTM International, 2015) and use: EMPa = e(SA0.0235)0. where SA is the Shore hardness value and EMPa is Youngs modulus in megapascals. Shore D. For Shore hardness, we follow (ASTM International, 2015) and use: where SD is the Shore hardness value and EMPa is Youngs modulus in megapascals. EMPa = e((SD+50)0.0235)0.6403 (19) (20) Point or Voxel Sampling. The baselines NeRF2Physics (Zhai et al., 2024) and PUGS (Shuai et al., 2025) in their methods sample points from the NeRF or Gaussian splat, respectively, and predict mechanical properties at those points. To ensure fair comparisons in Tb. 2 and Fig. 6b, we explicitly make these methods work on the same set of points in the object on which our method is evaluated. Implementation details of Baselines. The baseline NeRF2Physics (Zhai et al., 2024) uses gpt-3.5-turbo for certain parts of their pipeline. We replace gpt-3.5-turbo in their pipeline with better performing model, GPT-4o (OpenAI & et al., 2024). The baseline Phys4DGen (Liu et al., 2024b) does not have code available. Thus, we faithfully reproduce the parts, \"Material Grouping and Internal Discovery\" and \"MLLMs-Guided Material Identification\". We reproduce these parts of their pipeline using GPT-4o (OpenAI & et al., 2024) for the MLLMs-Guided Material Identification. Furthermore, we obtained the prompts from the authors of Phys4DGen (Liu et al., 2024b) and use the same prompts."
        },
        {
            "title": "G ADDITIONAL DETAILS ON THE SIMULATIONS",
            "content": "We experiment with Simplicits (Modi et al., 2024), reduced-order simulator (Fig. 1, 5, 8c and 8e) and an accurate finite-element method (FEM) simulator (Fig. 1, 5 and 8b) with our material proper2https://build.nvidia.com/nvidia/nv-dinov2 43 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "Value",
            "content": "Table 13: Hyperparameters for FEM simulation."
        },
        {
            "title": "Time Integrator\nNonlinear Solver",
            "content": "Newton max iters. Velocity tol. CCD tol. Transform rate tol. dt Gravity Backward Euler Newtons w/ line search 1024 0.05 ms1 1.0 0.1/s 0.02 [0.0, 9.8, 0.0]"
        },
        {
            "title": "Linear tolerance",
            "content": "pre-conditioned CG"
        },
        {
            "title": "Line search\nmax iters",
            "content": "Collision Friction Contact Resistance ˆd 8 0.5 1.0 0.01 ties. We also use FEM simulator for our experiments on interpreting errors in properties (D.4). We use material point method (MPM) (Sulsky et al., 1994), and an Extended Position Based Dynamics (XPBD) (Macklin et al., 2016) simulator for our experiments to compare between simulators  (Fig. 2)  . We share details on these simulations. We also share details on the interpolation we use across all our simulations. We share the hyperparameters used for all the FEM simulations in Tb. 13. G.1 INTERPOLATION SCHEME Our simulations receive material field sampled on voxel grid predicted by VoMP, i.e., values m(Xi) given at lattice points {Xi} Ω. When the simulator needs material values at arbitrary query locations (e.g., element centroids or vertices), we evaluate nearest-neighbour interpolation of the voxel field: i(X) = arg min m(X) = m(cid:0)X i(X) (cid:1). Xi2, (21) We intentionally avoid higher-order interpolation of material fields since real objects are piecewiseconstant across label regions, and convex blending across parts of the objects invents intermediate materials. These intermediate materials might not be physically present or admissible, while our outputs fall into valid material due to the MatVAE (3). Nearest-neighbour preserves sharp interfaces and is usually robust for arbitrary query locations. G.2 PREPARING SCENES AND ASSIGNING MATERIALS FOR THE FEM SOLVER Mechanical properties are set either uniformly (like in D.4) or heterogeneously from voxel field. For uniform assignment, given and ν we compute Lamé parameters λ = ν (1 + ν)(1 2ν) , µ = 2(1 + ν) , (22) which are used elementwise together with constant mass density ρ. For heterogeneous assignment, voxel lattice provides E(X), ν(X), and ρ(X) at voxel centers. After applying the same rigid/scale transform as the mesh, each tetrahedron takes λ, µ from the nearest voxel to its centroid, and each vertex takes ρ from the nearest voxel to its position. This produces per-tetrahedron λ, µ and per-vertex ρ fields that are directly used in the elastic strain energy density per unit reference volume (W ), first variation of the incremental potential (R), and NewtonJacobian (K). During simulation, visual mesh is embedded into the physics mesh by assigning each visual vertex xv to containing (or nearest) tetrahedron with vertices {Xa}4 satisfying (cid:80) lation of current nodal positions {xa}4 a=1 wa Xa = xv; its deformed position is then the barycentric interpoa=1 and barycentric weights {wa}4 wa = 1 and (cid:80) a=1: def = 4 (cid:88) a=1 wa xa . (23) The state update in our simulation experiments is computed time-step by time-step, and we also deform and move the visual mesh according to the physics mesh at each time step. 44 VoMP: Predicitng Volumetric Mechanical Property Fields G.3 DETAILS OF THE FEM SOLVER For FEM simulations, we use simulator based on the libuipc (Huang et al., 2025; 2024a) implementation and the Warp (warp.fem) (Macklin, 2022) implementation. We first explain the details for our simulations in D.4. We consider deformable continuum body with reference configuration Ω R3 and boundary Ω = ΓD ΓN , where ΓD denotes boundary points with Dirichlet boundary conditions, and ΓN denotes boundary points with Neumann boundary conditions. The unknown to solve for is the displacement field : Ω R3. Time is discretized into frames with fixed step t. At each frame we compute an increment that advances the configuration + while enforcing Dirichlet constraints on ΓD. The deformation map is φ(X) = X+u(X), with deformation gradient F(u) = + u, Jacobian = det F, and isochoric invariant Ic = tr(FF). For corotational modeling, we use the stretch tensor from the polar/SVD decomposition: if = diag(σ) then = diag(σ) V. Given Youngs modulus and Poisson ratio ν, the Lamé parameters are λ = Eν/((1 + ν)(1 2ν)) and µ = E/(2(1 + ν)). Here denotes the gradient with respect to reference coordinates, A:B = tr(AB) is the Frobenius inner product, and is the Euclidean norm. The elastic response we use is the corotational Hookean model. Define the small strain ε = I. The strain energy density and Kirchhoff stress are WCR(S) = µ ε : ε (cid:124) (cid:123)(cid:122) (cid:125) shear (deviatoric) + λ 2 tr(ε)2 (cid:124) (cid:123)(cid:122) (cid:125) volumetric + λ tr(ε) , (cid:124) (cid:123)(cid:122) (cid:125) volumetric τ (S) = 2µ ε (cid:124)(cid:123)(cid:122)(cid:125) shear , (24) with consistent linearization obtained via the variation of with respect to and projected to maintain symmetry and positive semidefiniteness. Each frame solves an incremental variational problem. Given the previous increment n1, we seek that approximately minimizes the incremental potential (cid:19) (cid:18) (cid:90) Π(u) = ρ 1 u n12 t2 (cid:123)(cid:122) inertial regularization dV (cid:125) Ω (cid:124) (cid:90) + Ω (cid:124) (cid:90) Ω (cid:124) + (cid:0) ρ fext u(cid:1) dV (cid:125) (cid:123)(cid:122) body and external work WCR (cid:0)S(un1 + u)(cid:1) dV (cid:123)(cid:122) elastic energy (cid:125) (25) + Πint(u) (cid:125) (cid:123)(cid:122) (cid:124) interior/boundary regularization where ρ is the mass density, is the gravitational acceleration vector, fext denotes prescribed volumetric loads, and Πint denotes any interior/boundary regularization term (e.g., jump penalty in discontinuous settings). The admissible test function is any sufficiently smooth virtual displacement that vanishes on ΓD. The first variation δΠ(u; v) = 0 for all such yields the residual , 45 VoMP: Predicitng Volumetric Mechanical Property Fields functional R(u)[v] = (cid:90) ρ Ω (cid:124) (cid:90) + Ω (cid:124) (cid:90) Ω (cid:124) u n1 t2 (cid:123)(cid:122) inertia dV (cid:125) (cid:0) ρ fext v(cid:1) dV (cid:123)(cid:122) (cid:125) body/external τ (cid:0)S(un1 + u)(cid:1) : dV (cid:123)(cid:122) elastic (internal) virtual work (cid:125) , + Rint(u)[v] (cid:125) (cid:123)(cid:122) regularization (cid:124) (26) which is set to zero for all v. Newtons method is applied to R(u) = 0. At iterate (k) we assemble the consistent tangent operator = DR(cid:2)u (k)(cid:3) (the Gàteaux derivative of R) and solve the linear system δu = R(cid:0)u (k)(cid:1), (k+1) = (k) + α δu where α (0, 1] is chosen by backtracking Armijo rule to guarantee sufficient decrease of Π. The operator contains an inertial mass-like term (cid:82) Ω ρ t2 δu dV , the consistent elastic tangent from the linearization of τ (S()), and any interior/boundary penalty contributions. This procedure is repeated until the update norm or residual falls below prescribed tolerance. (27) For all our other simulations (i.e. except the simulations in D.4) we use closely related variant whose differences are in the constitutive law, material assignment, mesh preparation/interpolation, and contact handling. First, the stored energy and stress are taken to be compressible Neo-Hookean with volumetric regularization. Writing = FF and = F, the energy and Kirchhoff stress are WNH(F) = µ 2 τNH(F) = µ (B I) + λ ln I. (cid:0)tr 3 2 ln J(cid:1) + λ 2 (ln J)2, (28) This change only affects the elastic terms in Π, R, and K; the kinematics and inertial terms remain the same. For the simulation experiments, we also use IPC (Li et al., 2020a) for collision handling. G.4 PREPARING SCENES AND ASSIGNING MATERIALS FOR THE SIMPLICITS SOLVER Each object is specified by set of quadrature points = {Xq} that sample its volume (used for elasticity and inertia), set of collision particles = {Xc} for contact, and set of visual vertices for rendering. We position objects with rigid transform (origin and rotation) and an object scale; these transforms are applied consistently when evaluating kinematics, gravity, and material fields. We embed all objects into regular grid domain and attach to this grid low-dimensional Simplicits subspace. The displacement basis is the product of trilinear grid shape and per-object handle shape, with multiple duplicated handles per grid vertex. At each quadrature point, we evaluate and cache per-node subspace weights and their spatial gradients. These weights modulate the duplicated handle functions during assembly, instantiating the Simplicits subspace on the grid. Material parameters are assigned per quadrature point from voxel lattice providing E(X), ν(X), and ρ(X). After applying the same rigid/scale transform as the object, each quadrature location Xq takes its material from the nearest voxel to Xq. We compute Lamé parameters per point as λq = µq = E(Xq) ν(Xq) (1 + ν(Xq))(1 2ν(Xq)) , E(Xq) 2(1 + ν(Xq)) , (29) and use ρq = ρ(Xq) in the inertial terms. 46 VoMP: Predicitng Volumetric Mechanical Property Fields"
        },
        {
            "title": "The quadrature weights are set uniformly as",
            "content": "wq = , (30) where is the object volume estimate and is the number of quadrature points. This makes elastic and inertial energies invariant to the sampling density. Collision particles are used for detecting and resolving contact against other particles and registered kinematic triangle meshes (containers and obstacles). We use an IPC-style barrier with Coulomb friction, and scale the contact stiffness by object volume and the number of collision particles to obtain comparable penalties across scenes. G.5 DEFORMING SPLATS AND REBNDERING DEFORMED SPLATS We render each object as set of anisotropic Gaussian splats. At rest, splat is parameterized by its mean µ0 R3, unit quaternion q0 (with rotation R0 SO(3)), and axis scales s0 R3 >0. We define the rest-frame shape operator (cid:124)(cid:123)(cid:122)(cid:125) rest anisotropy = R0 diag(s0) . (31) During simulation, the displacement field yields world-space deformation gradient (cid:124)(cid:123)(cid:122)(cid:125) local deformation at the splat center and world-space position µ (obtained by evaluating the embedded deformation at the visual vertex). We map the rest anisotropy through the local deformation to obtain the worldspace covariance of the splat as Σ (cid:124)(cid:123)(cid:122)(cid:125) world covariance = (F L) (cid:124) (cid:123)(cid:122) (cid:125) deformed axes deformed axes (cid:122) (cid:125)(cid:124) (cid:123) (F L) + ε (cid:124)(cid:123)(cid:122)(cid:125) SPD padding . (32) Here ε > 0 is small scalar that guarantees positive-definiteness under extreme compression. For rasterization we pass µ and Σ to the Gaussian renderer. The renderer (gsplat (Ye et al., 2024)) expects symmetric 6-vector parameterization; we therefore pack the lower-triangular entries as (cid:124)(cid:123)(cid:122)(cid:125) packed covariance = (cid:2) Σ11, Σ12, Σ13, Σ22, Σ23, Σ33 (cid:3) . (33) Color appearance (spherical-harmonic coefficients) and opacity are carried from the rest representation; only the mean µ and covariance Σ change over time. We also support scalar scale multiplier applied to s0 for interactive control in qualitative visualizations. Given the view (extrinsic) matrix and vertical field-of-view, we synthesize camera intrinsics (cid:34)fx 0 0 (cid:35) , 0 fy 0 cx cy 1 = fx = 2 tan( fovx 2 ) , fy = fovy 2 ) 2 tan( , (34) cx = 2 , cy = 2 , with image size (W, H). We then render the set of splats {µ, c} under (V, K) to produce RGB (and depth) frames. At every frame, we interpolate µ and at the visual vertices, form Σ = LF + ε I, pack it as c, and feed the Gaussian rasterizer together with the stored colors and opacities. G.6 DETAILS OF THE SIMPLICITS SOLVER We use the Simplicits (Modi et al., 2024) simulator based on the implementation in Kaolin (Fuji Tsang et al.). Simplicits solves for displacement field represented in lowdimensional subspace attached to regular grid. This subspace is product basis between trilinear grid polynomials and duplicated per-vertex handle functions whose influence is modulated by per-point weights and weight gradients evaluated at quadrature points. We assemble inertia and compressible Neo-Hookean elasticity on this subspace, using the per-quadrature Lamé parameters 47 VoMP: Predicitng Volumetric Mechanical Property Fields (λq, µq) and measures wq. Each frame performs Newton steps on the incremental potential with backtracking line search. Linear systems are solved by preconditioned conjugate gradients. In the Kaolin implementation, splatsplat contact uses particle pairs: for pair (a, b) with current positions xa, xb and contact radius r, we set nc = (xa xb)/xa xb, rc = r, and use the relative offset oc(u) = ua ub, so that dc = nc (xa xb) and vt,c measures tangential slip between the two. However, in our implementation, splatmesh contact is handled differently than splat-splat contact. For splat-mesh contact, instead of using the collision points, we use triangle meshes as kinematic colliders: for particle at and its closest point on nearby triangle (with interpolated mesh normal/velocity nc, vm), we take rc = 2r and oc(u) = + (cid:0)x nc u(cid:1) nc vm , dc = p nc vm . (35) Only the simulated-object DOFs enter oc; mesh or splat motion appears through vm."
        },
        {
            "title": "These terms are used in the Newton system as",
            "content": "K + α HC (cid:124) (cid:125) (cid:123)(cid:122) contact stiffness , α Hg (cid:124) (cid:123)(cid:122) (cid:125) contact force , (36) where is the Jacobian of contact offsets, and g, are the per-contact gradient/Hessian with respect to oc. For splatmesh contacts only the simulated-object block of is present; for splatsplat contacts the two object blocks appear with opposite signs. G.7 PREPARING SCENES AND ASSIGNING MATERIALS FOR THE XPBD SOLVER We also use an Extended Position-Based Dynamics (XPBD) solver (Macklin et al., 2016) based on PositionBasedDynamics (Bender & contributors, 2015). Particles and initialization. Objects are represented by particles positioned at rest locations {Xi}. For each particle we initialize position xi Xi and previous position equal to Xi (Verlet), set mass mi and inverse mass wi = 1/mi (pinned points use wi = 0). soft sphere uses one center particle and set of surface particles sampled on UV sphere; the center forms simple tetrahedra with nearby surface points for volume preservation. Material parameters and compliance. To target an elastic behavior with Youngs modulus and Poisson ratio ν (bulk modulus = E/(3(1 2ν))), we choose distanceand volume-constraint compliances αdist, αvol inversely proportional to and K. XPBD uses the scaled compliance α = α/t2 so that smaller α yields stiffer response. Prediction, projections, and updates. Each frame predicts positions with Verlet integration, then iteratively projects distance, volume, and collision constraints. For pairwise distance constraint, the XPBD update displaces endpoints along the edge with factor γ = α α + wi + wj , xi γ Cij , xj = xi, (37) xi xj xi xj wi wj with the analogous formulation for volume constraints using their gradients. After projections, we update positions and apply ground-plane contact with Coulomb friction. Visualization. We track the evolving surface by updating triangular mesh whose vertices coincide with the surface particles. G.8 PREPARING SCENES AND ASSIGNING MATERIALS FOR THE MPM SOLVER We use material point method (MPM) simulator (Sulsky et al., 1994) based on taichi-mpm (Hu & contributors, 2018). Scenes are specified by uniform Cartesian grid, set of particles sampling each objects volume, and per-object mechanical properties. Domain, grid, and timestep. We embed all objects in unit cube domain discretized by an ngrid ngrid ngrid grid (cell size = 1/ngrid). We use fixed time step small enough for stability (e.g., = 5 105 in our experiments). 48 VoMP: Predicitng Volumetric Mechanical Property Fields Particles and initialization. Each object is sampled with material points (particles) positioned at rest locations {Xp}. For each particle we initialize position xp Xp, velocity vp 0, affine velocity field Cp 0, and deformation gradient Fp I. Mass is set as mp = ρ Vp with density ρ and particle volume Vp consistent with the grid resolution. In the simple drop test, we sample sphere at height and let gravity act. Material parameters and constitutive law. We assign per-object Youngs modulus and Poisson ratio ν, compute Lamé parameters µ = E/(2(1 + ν)) and λ = Eν/((1 + ν)(1 2ν)), and use fast corotated (FCR) elastic model. With polar/SVD decomposition = diag(σ) and rotation = UV, the Kirchhoff stress is τFCR(F) = 2µ (F R) + λ (J 1) I, = det F. (38) Transfers and updates. Each step performs Particle-to-Grid (P2G) transfers using quadratic Bspline weights: we scatter particle mass, momentum, and internal forces Vp τ to grid nodes. On the grid, we (i) convert momentum to velocity, (ii) add gravity, and (iii) enforce box boundary conditions by clamping outward normal velocities to zero. We then perform Grid-to-Particle (G2P) to interpolate grid velocities back to particles, update the affine field, and integrate xp xp + vp, Fp (I + v) Fp. (39) Visualization. We embed coarse surface at rest, align it to the particle center of mass, and update its vertices via interpolation of nearby particle displacements."
        },
        {
            "title": "H OTHER RELATED WORKS",
            "content": "For completeness, we include other tangentially related works here. different setting from ours is inferring physical properties given additional observations, such as video (Davis et al., 2015; Mottaghi et al., 2016; Bhat et al., 2002; Chen et al., 2025b; Liu et al., 2024a; Xue et al., 2023; Li et al., 2025; Brubaker et al., 2009; Yildirim et al., 2016; Li et al., 2023; 2020b; Wu et al., 2016; 2015; 2017; Xia et al., 2024; Xu et al., 2019; Feng et al., 2024; Lin et al., 2024b) or physical manipulation of real objects (Yu et al., 2024; Pai et al., 2001; Lang et al., 2003; Lloyd & Pai, 2001; Pai et al., 2008; Pai, 2000; Yao & Hauser, 2023; Pinto et al., 2016). Other related works focus on generating new physically plausible shapes, e.g. stable under gravity or other interactions, but cannot augment existing 3D assets with mechanical properties, which is our goal (Lin et al., 2025b; Guo et al., 2024; Chen et al., 2024; Ni et al., 2024; Yang et al., 2024; Mezghanni et al., 2022; Chen et al., 2025a; Cao et al., 2025; Cao & Kalogerakis, 2025). Other methods predict displacements (Zhang et al., 2024; Shi et al., 2023), bypassing mechanical properties, or focus on other aspects such as articulation (Xia et al., 2025). 49 VoMP: Predicitng Volumetric Mechanical Property Fields User Prompt. You are materials science expert analyzing two images: 1. photo of the full object (showing how the material appears in context). 2. sphere with the materials texture (showing color/roughness/reflectivity in isolation). Using both images and the information below, identify the real-world material and estimate its mechanical properties. Material context: *Material type: fill in from dataset *Opacity: fill in from dataset *Density: fill in from dataset kg/m^3 *Dynamic friction: fill in from dataset *Static friction: fill in from dataset *Restitution: fill in from dataset *Usage: fill in from dataset Your task is to provide three specific properties: 1. Youngs modulus (in Pa using scientific notation) 2. Poissons ratio (a value between 0.0 and 0.5) 3. Density (in kg/m^3 using scientific notation) Additional reference material property ranges to help you make accurate estimations: - closest match from material database : Youngs modulus range fill in , Poissons ratio range fill in , Density range fill in kg/m^3 - closest match from material database : Youngs modulus range fill in , Poissons ratio range fill in , Density range fill in kg/m^3 - closest match from material database : Youngs modulus range fill in GPa, Poissons ratio range fill in , Density range fill in kg/m^3 Example 1: Material: metal Opacity: opaque Density: 7800 kg/m^3 Dynamic friction: 0.3 Static friction: 0.4 Restitution: 0.3 Usage: structural component Analysis: Step 1: Based on the images, this appears to be standard structural steel with matte gray finish. Step 2: The surface has medium roughness with some subtle texture visible in the reflection pattern. Step 3: The physical properties (density, friction values, restitution) are consistent with carbon steel. Step 4: Considering the usage and measured properties: - High stiffness (Youngs modulus 200 GPa) based on typical steel values - Medium Poissons ratio typical of metals - High density matching the measured 7800 kg/m^3 Youngs modulus: 2.0e11 Pa Poissons ratio: 0.29 Density: 7800 kg/m^ Figure 22: User Prompt I. The User Prompt we use for every segment of every object. 50 VoMP: Predicitng Volumetric Mechanical Property Fields User Prompt Continued. Example 2: Material: plastic Opacity: opaque Density: 950 kg/m^3 Dynamic friction: 0.25 Static friction: 0.35 Restitution: 0.6 Usage: household container Analysis: Step 1: The material shows the characteristic smooth, uniform appearance of consumer plastic. Step 2: It has moderate gloss with some translucency and slight texture. Step 3: The physical properties (medium-low density, moderate friction, higher restitution) match polypropylene. Step 4: Based on these observations and measurements: - Medium-low stiffness typical of polyolefin plastics - Higher Poissons ratio indicating good lateral deformation - Density matching the measured 950 kg/m^ Youngs modulus: 1.3e9 Pa Poissons ratio: 0.42 Density: 950 kg/m^3 Example 3: Material: fabric Opacity: opaque Density: 300 kg/m^3 Dynamic friction: 0.55 Static friction: 0.75 Restitution: 0.2 Usage: furniture covering Analysis: Step 1: The material shows woven textile structure with visible fibers. Step 2: The surface has significant texture with matte appearance and no specular highlights. Step 3: The physical properties (low density, high friction, low restitution) match woven textile. Step 4: Based on these observations and measurements: - Low stiffness as expected for flexible textiles - Medium-high Poissons ratio from the woven structure - Density matching the measured 300 kg/m^3 Youngs modulus: 1.2e8 Pa Poissons ratio: 0.38 Density: 300 kg/m^3 Figure 23: User Prompt II. The User Prompt we use for every segment of every object. VoMP: Predicitng Volumetric Mechanical Property Fields User Prompt Continued. Example 4: Material: organic Opacity: opaque Density: 400 kg/m^3 Dynamic friction: 0.45 Static friction: 0.65 Restitution: 0.15 Usage: decorative element Analysis: Step 1: This is an organic material with the characteristic structure of natural fibers. Step 2: The surface shows natural pattern, matte finish, and relatively brittle structure. Step 3: The physical properties (low density, moderate-high friction, low restitution) align with plant-based materials. Step 4: Considering similar organic materials and the measured properties: - Low-medium stiffness in the fiber direction - Medium Poissons ratio reflecting the fibrous structure - Density matching the measured 400 kg/m^3 Youngs modulus: 2.5e9 Pa Poissons ratio: 0.30 Density: 400 kg/m^3 Based on the provided images and context information, analyze the material properties. Note: The material segment might be internal to the object and not visible from the outside. Respond using EXACTLY the following format (do not deviate from this structure): Analysis: Step 1: Identify the material class/type based on visual appearance Step 2: Describe the surface characteristics (texture, reflectivity, color) Step 3: Determine the specific material subtype considering its physical properties Step 4: Reason through each property estimate based on visual and measured data Youngs modulus: <value in scientific notation> Pa Poissons ratio: <single decimal value between 0.0 and 0.5> Density: <value in scientific notation> kg/m^3 Critical Instructions: 1. You MUST provide numerical estimates for ALL materials, including organic or unusual materials 2. For natural materials like leaves, wood, or paper, provide estimates based on similar materials with known properties 3. Never use \"N/A\", \"unknown\", or any non-numeric responses for the material properties 4. For Poissons ratio, provide simple decimal number (like 0.3 or 0.42) 5. Each property should be on its own line with exact formatting shown above Figure 24: User Prompt III. The User Prompt we use for every segment of every object. VoMP: Predicitng Volumetric Mechanical Property Fields Object. VLM Response. Step 1: Identify the material class/type based on visual appearance The material appears to be metallic, specifically aluminum, given the context of it being used in pallet and the provided density value which matches aluminum. Step 2: Describe the surface characteristics (texture, reflectivity, color) The surface is smooth with matte finish, showing minimal reflectivity. The color is uniform light gray, typical of aluminum. Step 3: Determine the specific material subtype considering its physical properties Considering the density of 2700 kg/m³, the material is most likely aluminum, common choice for pallets due to its lightweight yet durable properties. Step 4: Reason through each property estimate based on visual and measured data - **Youngs modulus**: Aluminum typically has Youngs modulus around 70 GPa. Given the usage as pallet, it needs to be stiff but not overly so, supporting the estimate of 70 GPa. - **Poissons ratio**: Aluminum generally has Poissons ratio around 0.33, which is typical for many metals. - **Density**: The provided density is 2700 kg/m^3, which is consistent with aluminum. Youngs modulus: 7.0e10 Pa Poissons ratio: 0.33 Density: 2.7e3 kg/m^3 Figure 25: Example Response I. We demonstrate an example response for segment from one of the objects from our dataset. The given object has one part. 53 VoMP: Predicitng Volumetric Mechanical Property Fields Object. VLM Response. Step 1: Identify the material class/type based on visual appearance The material appears to be type of rubber, specifically used for antiskid caps based on the context provided. The surface of the sphere suggests matte finish, which is common for rubber materials designed for grip and friction. Step 2: Describe the surface characteristics (texture, reflectivity, color) The surface of the sphere is matte with uniform gray color, indicating lack of significant reflectivity. There is slight texture visible, suggesting it is not perfectly smooth, which is consistent with rubber materials intended for anti-slip applications. Step 3: Determine the specific material subtype considering its physical properties Given the usage as an antiskid cap and the provided physical properties such as density (1100.0 kg/m^3), dynamic and static friction (both 0.7), and restitution (0.8), the material is likely type of rubber with good elasticity and frictional properties. The density matches well with EPDM rubber, which is commonly used for such applications due to its durability and resistance to environmental factors. Step 4: Reason through each property estimate based on visual and measured data - **Youngs modulus**: Considering the material is rubber used for antiskid purposes, it needs to be flexible yet provide sufficient stiffness to maintain shape under load. The given density and usage suggest medium-hard rubber, so an estimate within the upper range of rubbers Youngs modulus is appropriate. - **Poissons ratio**: Rubbers typically exhibit high Poissons ratios close to 0.5, indicating significant lateral expansion when compressed. Given the materials flexibility and the need for good grip, value near the upper end of the rubber range is suitable. - **Density**: The provided density is already specified as 1100.0 kg/m^3, which aligns well with EPDM rubber. Youngs modulus: 1.0e8 Pa Poissons ratio: 0.49 Density: 1.1e3 kg/m^ Figure 26: Example Response II. We demonstrate an example response for segment from one of the objects from our dataset. The given object has two parts, and we show the response for the \"rubber cap\" part."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of Toronto"
    ]
}