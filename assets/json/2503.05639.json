{
    "paper_title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "authors": [
        "Yuxuan Bian",
        "Zhaoyang Zhang",
        "Xuan Ju",
        "Mingdeng Cao",
        "Liangbin Xie",
        "Ying Shan",
        "Qiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model's learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter's superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 9 3 6 5 0 . 3 0 5 2 : r VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control YUXUAN BIAN, The Chinese University of Hong Kong, China ZHAOYANG ZHANG, Tencent ARC Lab, China XUAN JU, The Chinese University of Hong Kong, China MINGDENG CAO, The University of Tokyo, Japan LIANGBIN XIE, University of Macau, China YING SHAN, Tencent ARC Lab, China QIANG XU, The Chinese University of Hong Kong, China Fig. 1. VideoPainter enables plug-and-play text-guided video inpainting and editing for any video length and pre-trained Diffusion Transformer with masked video and video caption (user editing instruction). The upper part demonstrates the effectiveness of VideoPainter in various video inpainting scenarios, including object, landscape, human, animal, multi-region (Multi), and random masks. The lower section demonstrates the performance of VideoPainter in video editing, including adding, removing, changing attributes, and swapping objects. In both video inpainting and editing, we demonstrate strong ID consistency in generating long videos (Any Len.). Project Page: https://yxbian23.github.io/project/video-painter Video inpainting, crucial for the media industry, aims to restore corrupted content. However, current methods relying on limited pixel propagation or single-branch image inpainting architectures face challenges with generating fully masked objects, balancing background preservation with foreground generation, and maintaining ID consistency over long video. To address Authors addresses: Yuxuan Bian, The Chinese University of Hong Kong, China, yuxuanbian23@gmail.com; Zhaoyang Zhang, Tencent ARC Lab, China, zhaoyangzhang@link.cuhk.edu.hk; Xuan Ju, The Chinese University of Hong Kong, China, juxuan.27@gmail.com; Mingdeng Cao, The University of Tokyo, Japan, mingdengcao@gmail.com; Liangbin Xie, University of Macau, China, lb.xie@siat.ac.cn; Ying Shan, Tencent ARC Lab, China, yingsshan@tencent.com; Qiang Xu, The Chinese University of Hong Kong, China, qxu@cse.cuhk.edu.hk. these issues, we propose VideoPainter, an efficient dual-branch framework featuring lightweight context encoder. This plug-and-play encoder processes masked videos and injects background guidance into any pre-trained video diffusion transformer, generalizing across arbitrary mask types, enhancing background integration and foreground generation, and enabling user-customized control. We further introduce strategy to resample inpainting regions for maintaining ID consistency in any-length video inpainting. Additionally, we develop scalable dataset pipeline using advanced vision models and construct VPData and VPBenchthe largest video inpainting dataset with segmentation masks and dense caption (>390K clips) to support large-scale training and evaluation. We also show VideoPainters 2 Bian, Y. et al promising potential in downstream applications such as video editing. Extensive experiments demonstrate VideoPainters state-of-the-art performance in any-length video inpainting and editing across 8 key metrics, including video quality, mask region preservation, and textual coherence. CCS Concepts: Computing methodologies Computer vision. Additional Key Words and Phrases: Artificial Intelligence Generative Content, Computer Vision, Video Inpainting, Video Editing"
        },
        {
            "title": "1\nVideo inpainting [Quan et al. 2024], which aims to restore the\ncorrupted video while maintaining coherence, facilitates numer-\nous applications, including try-on [Fang et al. 2024], film produc-\ntion [Polyak et al. 2024], and video editing [Sun et al. 2024]. Recently,\nDiffusion Transformers (DiT) [OpenAI 2024; Peebles and Xie 2023]\nhave shown promise in video generation, leading to the exploration\nof generative video inpainting [Zhang et al. 2024b; Zi et al. 2024].",
            "content": "Existing approaches, as illustrated in Fig. 2, can be broadly categorized into two types: (1) Non-Generative methods [Lee et al. 2019; Li et al. 2022; Zhou et al. 2023] depend on limited pixel feature propagation (physical constraints or model architectural priors), which only take masked videos as inputs and cannot generate fully segmentation-masked objects. (2) Generative methods [Wang et al. 2024; Zhang et al. 2024b; Zi et al. 2024] extend single-branch image inpainting architectures [Rombach and Esser 2022] to video by incorporating temporal attention, which struggles to balance background preservation and foreground generation in one model and obtain inferior temporal coherence compared to native video DiTs. Moreover, both paradigms neglect long video inpainting and struggle to maintain consistent object identity with long videos. This motivates us to decompose video inpainting into background preservation and foreground generation and adopt dual-branch architecture in DiTs, where we can incorporate dedicated context encoder for masked video feature extraction while utilizing the pre-trained DiTs capabilities to generate semantic coherent video content conditioned on both the preserved background and text prompts. Similar observations have been made in image inpainting research, notably in BrushNet [Ju et al. 2024] and ControlNet [Zhang et al. 2023]. However, directly applying their architecture to video DiTs presents several challenges: (1) Given Video DiTs robust generative foundation and heavy model size, replicating the full/half-giant Video DiT backbone as the context encoder would be unnecessary and computationally prohibitive. (2) Unlike BrushNets pure convolutional control branch, DiTs tokens in masked regions inherently contain background information due to global attention, complicating the distinction between masked and unmasked regions in DiT backbones. (3) ControlNet lacks feature injection across all layers, hindering dense background control for inpainting tasks. To address these challenges, we introduce VideoPainter, which enhances pre-trained DiT with lightweight context encoder comprising only 6% of the backbone parameters, to form the first efficient dual-branch video inpainting architecture. VideoPainter features three main components: (1) streamlined context encoder with just two layers, which integrates context features into the pre-trained DiT in group-wise manner, ensuring efficient and dense background guidance. (2) Mask-selective feature integration to clearly distinguish the tokens of the masked and unmasked region. (3) novel inpainting region ID resampling technique to efficiently process videos of any length while maintaining ID coherence. By freezing the pre-trained context encoder and DiT backbone, and adding an ID-Adapter, we enhance the backbones attention sampling by concatenating the original key-value vectors with the inpainting region tokens. During inference, inpainting region tokens from previous clips are appended to the current key-value vectors, ensuring the long-term preservation of target IDs. Notably, our VideoPainter supports plug-and-play and user-customized control. For large-scale training, we develop scalable dataset pipeline using advanced vision models [OpenAI 2024; Ravi et al. 2024; Zhang et al. 2024a], constructing the largest video inpainting dataset, VPData, and benchmark, VPBench, with over 390K clips featuring precise segmentation masks and dense text captions. We further demonstrate VideoPainters potential by establishing an inpainting-based video editing pipeline that delivers promising results. To validate our approach, we compare VideoPainter against previous state-of-the-art (SOTA) baselines and single-branch finetuning setup that combines noisy latent, masked video latent, and mask at the input channel. VideoPainter demonstrates superior performance in both training efficiency and final results. In summary, our contributions are as follows: We propose VideoPainter, the first dual-branch video inpainting framework that supports plug-and-play background controls. We design lightweight context encoder for efficient and dense background control, and inpainting region ID resampling for ID consistency in any-length video inpainting and editing. We introduce VPData, the largest video inpainting datasets comprising over 390K clips (> 866.7 hours), and VPBench, both featuring precise masks and detailed video captions. Experiments show VideoPainter achieves state-of-the-art performance across 8 metrics including video quality, masked region preservation, and text alignment in video inpainting and editing."
        },
        {
            "title": "2 RELATED WORK\n2.1 Video Inpainting\nVideo inpainting approaches can be broadly classified into two\ncategories based on whether they possess generative capabilities:",
            "content": "Non-generative methods. These methods [Hu et al. 2020; Li et al. 2022; Zhang et al. 2022a,b; Zhou et al. 2023] leverage architecture priors to facilitate pixel propagation. This includes utilizing local perception of 3D CNNs [Chang et al. 2019a,b; Hu et al. 2020; Wang et al. 2019], and exploiting the global perception of attention to retrieve and aggregate tokens with similar texture for filling masked video [Lee et al. 2019; Liu et al. 2021; Zeng et al. 2020; Zhang et al. 2022a]. They also introduce various physical quantities, especially optical flow, as auxiliary conditions as it simplifies RGB pixel inpainting by completing less complex flow fields [Gao et al. 2020; Kim et al. 2019; Li et al. 2020; Xu et al. 2019; Zhang et al. 2022b,c; Zou et al. 2021]. However, they are only effective for partial object occlusions with random masks but face significant limitations when inpaint fully masked regions due to insufficient contexts. VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control 3 Fig. 2. Framework Comparison. Non-generative approaches, limited to pixel propagation from backgrounds, fail to inpaint fully segmentation-masked objects. Generative methods adapt single-branch image inpainting models to video by adding temporal attention, struggling to maintain background fidelity and generate foreground contents in one model. In contrast, VideoPainter implements dual-branch architecture that leverages an efficient context encoder with any pre-trained DiT, decoupling video inpainting to background preservation and foreground generation, and enabling plug-and-play video inpainting control. Table 1. Comparison of video inpainting datasets. Our VPData is the largest video inpainting dataset to date, comprising over 390K high-quality clips with segmentation masks, video captions, and masked region descriptions. Dataset #Clips Duration Video Caption Masked Region Desc. 0.1h 0.4K 5.6h 4.5K 4.2h 1.5K 7.4h 5.2K 1.0K 18.9h 642.6K 196.0h 390.3K 866.7h DAVIS [Perazzi et al. 2016] YouTube-VOS [Xu et al. 2018] VOST [Tokmakov et al. 2023] MOSE [Ding et al. 2023] LVOS [Hong et al. 2023] SA-V [Ravi et al. 2024] Ours Generative methods. Recent advances in generative foundation models [Guo et al. 2023; Rombach et al. 2022] have sparked numerous approaches that leverage additional modules or training strategies to extend backbones capabilities for video inpainting [Wang et al. 2024; Zhang et al. 2024b; Zi et al. 2024]. AVID [Zhang et al. 2024b] and COCOCO [Zi et al. 2024] represent the most related recent works. Both adopt similar implementation by augmenting Stable Diffusion Inpainting [Rombach and Esser 2022] with trainable temporal attention layers. This architecture includes per-frame region filling based on the image inpainting backbone and temporal smoothing with temporal attention. Despite showing promising results for both random and segmentation masks due to their generative abilities, they struggle to balance background preservation and foreground generation with text caption [Ju et al. 2024; Li et al. 2024] within the single backbone. AVID also explores any-length video inpainting by smoothing latent at segment boundaries and using the middle frame as the ID reference. In contrast, VideoPainter is dual-branch framework by decoupling video inpainting into foreground generation and background-guided preservation. It employs an efficient context encoder to guide any pre-trained DiT, facilitating plug-and-play control. Furthermore, VideoPainter also introduces novel inpainting region ID resampling technique that enables ID consistency in any-length video inpainting."
        },
        {
            "title": "2.2 Video Inpainting Datasets\nRecent advances in segmentation [Ravi et al. 2024] have created\nmany video segmentation datasets [Darkhalil et al. 2022; Ding et al.\n2023; Hong et al. 2023; Perazzi et al. 2016; Tokmakov et al. 2023; Xu\net al. 2018]. Among these, DAVIS [Perazzi et al. 2016] and YouTube-\nVOS [Xu et al. 2018] have become prominent benchmarks for video\ninpainting due to their high-quality masks and diverse object cate-\ngories. However, the existing datasets face two primary limitations:\n(1) insufficient scale to meet the data requirements of generative\nmodels, and (2) the absence of crucial control conditions necessary\nfor generating masked objects such as video captions. In contrast,",
            "content": "as shown in Tab. 1, we developed scalable dataset pipeline based on state-of-the-art vision understanding models [OpenAI 2024; Ravi et al. 2024; Zhang et al. 2024a], and constructed the largest video inpainting dataset to date with over 390K clips, each annotated with segmentation masks and dense video captions. Fig. 3. Dataset Construction Pipeline. It consists of five pre-processing steps: collection, annotation, splitting, selection, and captioning."
        },
        {
            "title": "3.1 VPData and VPBench Construction Pipeline\nTo address the challenges of limited size and lack of text annotations,\nwe present a scalable dataset pipeline leveraging advanced vision\nmodels [OpenAI 2024; Ravi et al. 2024; Zhang et al. 2024a]. This\nleads to VPData and VPBench, the largest video inpainting dataset\nand benchmark with precise masks and video/masked region cap-\ntions. As shown in Fig. 3, the pipeline involves 5 steps: collection,\nannotation, splitting, selection, and captioning.\nCollection. We chose Videvo and Pexels 1 as our data sources. We\nfinally obtained around 450ùêæ videos from these sources.\nAnnotation. For each collected video, we implement a cascaded\nworkflow for automated annotation:",
            "content": "1Videvo: https://www.videvo.net/, Pexels: https://www.pexels.com/ 4 Bian, Y. et al Fig. 4. Model overview. The upper figure shows the architecture of VideoPainter. The context encoder performs video inpainting based on concatenation of the noisy latent, downsampled masks, and masked video latent via VAE. Features extracted by the context encoder are integrated into the pre-trained DiT in group-wise and token-selective manner, where two encoder layers modulate the first and second halves of the DiT, respectively, and only the background tokens will be integrated into the backbone to prevent information ambiguity. The lower figure illustrates the inpainting ID region resampling with the ID Resample Adapter. During training, tokens of the current masked region are concatenated to the KV vectors, enhancing ID preservation of the inpainting region. During inference, the ID tokens of the last clip are concatenated to the current KV vectors, maintaining ID consistency with the last clip by resampling. We employ the Recognize Anything Model [Zhang et al. 2024a] for open-set video tagging to identify primary objects. Based on the detected object tags, we utilize Grounding DINO [Liu et al. 2023] to detect bounding boxes for objects at fixed intervals. These bounding boxes serve as prompts for SAM2 [Ravi et al. 2024], which generates high-quality mask segmentations. Splitting. Scene transitions may occur while tracking the same object from different angles, causing disruptive view changes. We utilize PySceneDetect [Castellano 2024] to identify scene transitions and subsequently partition the masks. Then we segmented the sequences into 10-second intervals and discarded short clips (< 6s). Selection. We employ 3 key criteria: (1) Aesthetic Quality, evaluated using the Laion-Aesthetic Score Predictor [Schuhmann et al. 2022]; (2) Motion Strength, predicted by optical flow measurements using the RAFT[Teed and Deng 2020]; and (3) Content Safety, assessed via the Stable Diffusion Safety Checker [Rombach et al. 2022]. Captioning. As Tab. 1 shows, existing video segmentation datasets lack textual annotations, primary conditions in generation [Betker et al. 2023; Chen et al. 2023], creating data bottleneck for applying generative models to video inpainting. Therefore, we leverage SOTA VLMs, specifically CogVLM2 [Wang et al. 2023] and GPT-4o [OpenAI 2024], to uniformly sample keyframes and generate dense video captions and detailed descriptions of the masked objects."
        },
        {
            "title": "3.2 Dual-branch Inpainting Control\nWe incorporate masked video features into the pre-trained diffusion\ntransformer (DiT) via an efficient context encoder, to decouple the\nbackground context extraction and foreground generation. This en-\ncoder processes a concatenated input of noisy latent, masked video\nlatent, and downsampled masks. Specifically, the noisy latent pro-\nvides information about the current generation. The masked video\nlatent, extracted via VAE, aligns with the pre-trained DiT‚Äôs latent\ndistribution. We apply cubic interpolation to downsample masks,\nensuring dimensional compatibility between masks and latents.",
            "content": "Based on DiTs inherent generative abilities [OpenAI 2024], the control branch only needs to extract contextual cues to guide the backbone in preserving background and generating foreground. Therefore, instead of previous heavy approaches that duplicate half or all of the backbone [Ju et al. 2024; Zhang et al. 2023], VideoPainter employs lightweight design by cloning only the first two layers of pre-trained DiT, accounting for merely 6% of the backbone parameters. The pre-trained DiT weights provide robust prior for extracting masked video features. The context encoder features are integrated into the frozen DiT in group-wise, token-selective manner. The group-wise feature integration is formulated as follows: the first layers features are added back to the initial half of the VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control 5 backbone, while the second layers features are integrated into the latter half, achieving lightweight and efficient context control. The token-selective mechanism is pre-filtering process, where only tokens representing pure background are added back, while others are excluded from integration, as shown in the upper right of Fig. 4. This ensures that only the background context is fused into the backbone, preventing potential ambiguity during backbone generation. The feature integration is shown in Eq. 1. ùúñùúÉ (ùëßùë° , ùë°, ùê∂)ùëñ indicates the feature of the ùëñ-th layer in DiT ùúñùúÉ with ùëñ [1, ùëõ], where ùëõ is the number of layers. The same notation applies to ùúñùëâ ùëñùëëùëíùëúùëÉùëéùëñùëõùë°ùëíùëü , which takes the concatenated noisy latent ùëßùë° , masked video latent ùëßùëöùëéùë†ùëòùëíùëë , and downsampled mask ùëöùëüùëíùë†ùëñùëßùëíùëë as input. The concate0 nation operation is denoted as []. is the zero linear operation. (1) ùúñùúÉ (ùëßùë° , ùë°, ùê∂)ùëñ = ùúñùúÉ (ùëßùë° , ùë°, ùê∂)ùëñ + , ùëöùëüùëíùë†ùëñùëßùëíùëë (cid:105) (cid:18) ùúñùëâ ùëñùëëùëíùëúùëÉùëéùëñùëõùë°ùëíùëü ùúÉ ùëßùë° , ùëßùëöùëéùë†ùëòùëíùëë 0 (cid:16) (cid:104) , ùë° ùúÉ (cid:19) (cid:17) ùëñ// ùëõ"
        },
        {
            "title": "3.3 Target Region ID Resampling\nWhile current DiTs show promise in handling temporal dynam-\nics [Bian et al. 2024; Kuaishou 2024], they struggle to maintain\nsmooth transitions and long-term identity consistency.\nSmooth Transition. Following AVID [Zhang et al. 2024b], we em-\nploy overlapping generation and weighted average to maintain\nconsistent transitions. Additionally, we utilize the last frame of the\nprevious clip (before overlap) as the first frame of the current clip‚Äôs\noverlapping region to ensure visual appearance continuity.\nIdentity Consistency. To maintain identity consistency in the long\nvideo, we introduce an inpainting region ID resampling method, as\nshown in lower Fig. 4. During training, we freeze both the DiT and\nthe context encoder. Then we add trainable ID-Resample Adapters\ninto the frozen DiT (LoRA), enabling ID resampling functionality.\nSpecifically, tokens from the current masked region, which contain\nthe desired ID, are concatenated with the KV vectors, thereby en-\nhancing ID preservation in the inpainting region through additional\nùëñ , Kùë£\nKV resampling. Specifically, given current Qùë£\nùëñ , we filter\nthe mask region tokens in current Kùë£\nùëñ and Vùë£\nùëñ , and concatenate them\nto Kùë£\nùëñ , forcing the model to resample these tokens that have\nthe needed ID. During inference, we prioritize maintaining ID con-\nsistency with the inpainting region tokens from the previous clip, as\nit represents the most temporally proximate generated result. There-\nfore, we concatenate the masked region tokens from the previous\nclip with the current key-value vectors, effectively resampling and\nmaintaining the identity information in long video processing.",
            "content": "ùëñ , and Vùë£ ùëñ and Vùë£"
        },
        {
            "title": "4 EXPERIMENTS\n4.1\nVideoPainter is built upon a pre-trained Image-to-Video Diffusion\nTransformer CogVideo-5B-I2V [Yang et al. 2024] (by default) and\nits Text-to-Video version. In training, we use VPData at a 480 √ó 720\nresolution, learning rate 1 √ó 10‚àí5, batch size 1 for both the context\nencoder (80, 000 steps) and the ID Resample Adapter (2, 000 steps)\nin two stages with AdamW on 64 NVIDIA V100 GPUs.",
            "content": "Benchmarks. In video inpainting, we employ Davis [Perazzi et al. 2016] as the benchmark for random masks and VPBench for segmentation-based masks. VPBench consists of 100 6-second videos for standard video inpainting, and 16 videos with an average duration of more than 30 seconds for long video inpainting. The VPBench includes diverse content including objects, humans, animals, landscapes, and multi-range masks. For video editing evaluation, we also utilize VPBench, which includes four fundamental editing operations (add, remove, swap, and change) and comprises 45 6-second videos and 9 videos with an average duration of 30 seconds. Metrics. We consider 8 metrics from three aspects: masked region preservation, text alignment, and video generation quality. Masked Region Preservation. We follow previous works using standard PSNR [Wikipedia contributors 2024c], LPIPS [Zhang et al. 2018], SSIM [Wang et al. 2004], MSE [Wikipedia contributors 2024b] and MAE [Wikipedia contributors 2024a] in the unmasked region among the generated video and the original video. Text Alignment. We employ CLIP Similarity (CLIP Sim) [Wu et al. 2021] to assess the semantic consistency between the generated video and its corresponding text caption. We also measure CLIP Similarity within the masked regions (CLIP Sim (M)). Video Generation Quality. Following previous methods, we use FVID [Wang et al. 2018] to measure the generated video quality."
        },
        {
            "title": "4.2 Video Inpainting",
            "content": "Quantitative comparisons. Tab. 2 shows the quantitative comparison on VPBench and Davis [Perazzi et al. 2016]. We compare the inpainting results of non-generative ProPainter [Zhou et al. 2023], generative COCOCO [Zi et al. 2024], and Cog-Inp [Yang et al. 2024], strong baseline proposed by us, which inpaint first frame using image inpainting models and use the I2V backbone to propagate results with the latent blending operation [Avrahami et al. 2023]. In the segmentation-based VPBench, ProPainter, and COCOCO exhibit the worst performance across most metrics, primarily due to the inability to inpaint fully masked objects and the single-backbone architectures difficulty in balancing the competing background preservation and foreground generation, respectively. In the random mask benchmark Davis, ProPainter shows improvement by leveraging partial background information. However, VideoPainter achieves optimal performance across segmentation (standard and long length) and random masks through its dual-branch architecture that effectively decouples background preservation and foreground generation. Qualitative comparisons. The qualitative comparison with previous video inpainting methods is shown in Fig. 5. VideoPainter consistently shows exceptional results in the video coherence, quality, and alignment with text caption. Notably, ProPainter fails to generate fully masked objects because it only depends on background pixel 6 Bian, Y. et al Fig. 5. Comparison of previous inpainting methods and VideoPainter on standard and long video inpainting. More visualizations are in the demo video. Table 2. Quantitative comparisons among VideoPainter and other video inpainting models in VPBench for segmentation mask (Standard (S) and Long (L) Video) and Davis for random mask: ProPainter [Zhou et al. 2023], COCOCO [Zi et al. 2024], and Cog-Inp [Yang et al. 2024]. Metrics include masked region preservation, text alignment, and video quality. Red stands for the best, Blue stands for the second best. Table 3. Quantitative comparisons among VideoPainter and other video editing models in VPBench (Standard and Long Video): UniEdit [Bai et al. 2024], DitCtrl [Cai et al. 2024], and ReVideo [Mou et al. 2024]. Metrics include masked region preservation, text alignment, and video quality. Red stands for the best, Blue stands for the second best. Metrics Masked Region Preservation Text Alignment Video Quality Metrics Models Masked Region Preservation Text Alignment Video Quality PSNR SSIM LPIPS102 MSE102 MAE102 CLIP Sim CLIP Sim (M) FVID ProPainter 20.97 - COCOCO 19.27 22.15 Cog-Inp 23.32 Ours ProPainter 20.11 - COCOCO 19.51 19.78 Cog-Inp 22.19 Ours i ProPainter 23.99 COCOCO 21.34 23.92 Cog-Inp 25.27 Ours 0.87 0.67 0.82 0.89 0.84 0.66 0.73 0. 0.92 0.66 0.79 0.94 9.89 14.80 9.56 6.85 11.18 16.17 12.53 9.14 5.86 10.51 10.78 4.29 1.24 1.62 0.88 0.82 1.17 1.29 1.33 0. 0.98 0.92 0.47 0.45 3.56 6.38 3.92 2.62 3.71 6.02 5.13 2.92 2.48 4.99 3.23 1.41 7.31 7.95 8.41 8.66 9.44 11.00 11.47 11. 7.54 6.73 7.03 7.21 17.18 20.03 21.27 21.49 17.68 20.42 21.22 21.54 16.69 17.50 17.53 18.46 0.44 0.69 0.18 0.15 0.48 0.62 0.21 0. 0.12 0.33 0.17 0.09 propagation instead of generating. While COCOCO demonstrates basic functionality, it fails to maintain consistent ID in inpainted regions ( inconsistent vessel appearances and abrupt terrain changes) due to its single-backbone architecture attempting to balance background preservation and foreground generation. Cog-Inp achieves basic inpainting results; however, its blending operations inability to detect mask boundaries leads to significant artifacts. Moreover, VideoPainter can generate coherent videos exceeding one minute while maintaining ID consistency through our ID resampling."
        },
        {
            "title": "4.3 Video Editing\nVideoPainter can be used for video inpainting by employing Vi-\nson Language Models [OpenAI 2024; Team et al. 2024] to generate\nmodified captions based on user editing instructions and source\ncaptions and apply VideoPainter to inpaint based on the modified\ncaptions. Tab. 3 shows the quantitative comparison on VPBench.\nWe compare the editing results of inverse-based UniEdit [Bai et al.",
            "content": "Models PSNR SSIM LPIPS102 MSE102 MAE102 CLIP Sim CLIP Sim (M) 11.08 12.73 3.49 1.02 9.96 UniEdit 9.30 DitCtrl ReVideo 15.52 22.63 Ours 56.68 57.42 27.68 7.65 25.78 27.45 11.14 2.90 14.23 15.59 20.01 20.20 8.46 8.52 9.34 8. 0.36 0.33 0.49 0.91 L 10.37 UniEdit 9.76 DitCtrl ReVideo 15.50 22.60 Ours 0.30 0.28 0.46 0.90 54.61 62.49 28.57 7.53 10.25 11.50 3.92 0. 24.89 26.64 12.24 2.76 10.85 11.78 11.22 11.85 15.42 16.52 20.50 19.38 FVID 1.36 0.57 0.42 0.18 1.00 0.56 0.35 0. 4. User Table VideoPainter with video inpainting and editing baselines. Study: User preference ratios comparing Task Ours Video Inpainting Video Editing Background Preservation Text Alignment 74.2% 82.5% Video Quality 87.4% Background Preservation Text Alignment 78.4% 76.1% Video Quality 81.7% 2024], DiT-based DiTCtrl [Cai et al. 2024], and end-to-end ReVideo [Mou et al. 2024]. For both standard and long videos in VPBench, VideoPainter achieves superior performance, even surpassing the end-to-end ReVideo. This success can be attributed to its dual-branch architecture, which ensures excellent background preservation and foreground generation capabilities, maintaining high fidelity in nonedited regions while ensuring edited regions closely align with editing instructions, complemented by inpainting region ID resampling that maintains ID consistency in long video. The qualitative comparison with previous video inpainting methods is shown in Fig. 5. VideoPainter demonstrates superior performance in preserving visual fidelity and text-prompt consistency. VideoPainter successfully generates seamless animation of futuristic spaceship traversing the sky, maintaining smooth temporal transitions and precise background boundaries throughout the removal process, without introducing artifacts that were observed in ReVideo. VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control 7 Fig. 6. Comparison of previous editing methods and VideoPainter on standard and long video editing. More visualizations are in the demo video. Table 5. Ablation Studies on VPBench. Single-Branch: We add input channels to adapt masked video and finetune the backbone. Layer Configuration (VideoPainter (*)): We vary the context encoder depth from one to four layers. w/o Selective Token Integration (w/o Select):: We bypass the token pre-selection step and integrate all context encoder tokens into DiT. T2V Backbone (VideoPainter (T2V)): We replace the backbone from image-to-video DiTs to text-to-video DiTs. w/o target region ID resampling (w/o Resample): We ablate on the target region ID resampling. (L) denotes evaluation on the long video subset. Red stands for the best result. Metrics Models Single-Branch VideoPainter (1) VideoPainter (4) w/o Select 21.92 22.86 0.81 0.85 20. 0.74 VideoPainter (T2V) 23.01 0.87 VideoPainter 23.32 0. w/o Resample (L) 21.79 0.84 VideoPainter (L) 22.19 0. Masked Region Preservation Text Alignment Video Quality PSNR SSIM LPIPS102 MSE102 MAE102 CLIP Sim CLIP Sim (M) 20.54 10.48 19. 0.79 8.19 0.94 4.16 8.78 6.51 7. 6.94 6.85 8.65 9.14 0.89 0.83 0. 0.89 0.82 0.81 0.71 3.26 2.86 3. 2.65 2.62 3.10 2.92 8.44 9.12 8. 9.41 8.66 11.35 11.52 20.79 20.49 17. 20.66 21.49 20.68 21.54 FVID 0. 0.17 0.16 0.25 0.16 0.15 0.19 0."
        },
        {
            "title": "4.5 Ablation Analysis\nWe ablate on VideoPainter in Tab .5, including architecture, context\nencoder size, control strategy, and inpainting region ID resampling.\nBased on rows 1 and 5, the dual-branch VideoPainter significantly\noutperforms its single-branch counterpart by explicitly decoupling",
            "content": "Fig. 7. Integrating VideoPainter to Gromit-style LoRA [Cseti 2024]. background preservation from foreground generation, thereby reducing model complexity and avoiding the trade-off between competing objectives in single branch. Row 2 to row 6 of Tab. 5 demonstrate the rationale behind our key design choices: 1 utilizing two-layer structure as an optimal balance between performance and efficiency for the context encoder, 2 implementing token-selective feature fusion based on segmentation mask information to prevent confusion from indistinguishable foreground-background tokens in the backbone, and 3 adapting plug-and-play control to different backbones with comparable performance. Furthermore, rows 7 and 8 verify the importance of employing inpainting region ID resampling for long videos, which maintains ID consistency by explicitly resampling inpainted region tokens from previous clips."
        },
        {
            "title": "5 DISCUSSION\nIn this paper, we introduce VideoPainter, the first dual-branch video\ninpainting framework with plug-and-play control capabilities. Our",
            "content": "8 Bian, Y. et al approach features three key innovations: (1) lightweight plug-andplay context encoder compatible with any pre-trained video DiTs, (2) an inpainting region ID resampling technique for maintaining long video ID consistency, and (3) scalable dataset pipeline that produced VPData and VPBench, containing over 390K video clips with precise masks and dense captions. VideoPainter also shows promise in video editing applications. Extensive experiments demonstrate that VideoPainter achieves state-of-the-art performance across 8 metrics in video inpainting and editing, particularly in video quality, mask region preservation, and text coherence. However, VideoPainter still has limitations: (1) Generation quality is limited by the base model, which may struggle with complex physical and motion modeling, and (2) performance is suboptimal with low-quality masks or misaligned video captions. REFERENCES Omri Avrahami, Ohad Fried, and Dani Lischinski. 2023. Blended latent diffusion. ACM transactions on graphics (TOG) 42, 4 (2023), 111. Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. 2024. Uniedit: unified tuning-free framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185 (2024). James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. 2023. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf 2, 3 (2023), 8. Yuxuan Bian, Xuan Ju, Jiangtong Li, Zhijian Xu, Dawei Cheng, and Qiang Xu. 2024. Multi-patch prediction: Adapting llms for time series representation learning. arXiv preprint arXiv:2402.04852 (2024). Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. 2024. DiTCtrl: Exploring Attention Control in MultiModal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation. arXiv preprint arXiv:2412.18597 (2024). Brandon Castellano. 2024. PySceneDetect: Intelligent Scene Cut Detection and Video Analysis Tool. https://github.com/Breakthrough/PySceneDetect Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston Hsu. 2019a. Free-form video inpainting with 3d gated convolution and temporal patchgan. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 90669075. Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston Hsu. 2019b. Learnable gated temporal shift module for deep video inpainting. arXiv preprint arXiv:1907.01131 (2019). Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. 2023. PixArt-ùõº: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. arXiv:2310.00426 [cs.CV] Cseti. 2024. CogVideoX-LoRA-Wallace_and_Gromit. Hugging Face. https://huggingface. co/Cseti/CogVideoX-LoRA-Wallace_and_Gromit Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen. 2022. EPIC-KITCHENS VISOR Benchmark: VIdeo Segmentations and Object Relations. In Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks. Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. 2023. MOSE: new dataset for video object segmentation in complex scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 20224 20234. Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, and Zheng-Jun Zha. 2024. ViViD: Video Virtual Try-on using Diffusion Models. arXiv preprint arXiv:2405.11794 (2024). Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf. 2020. Flow-edge guided video completion. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XII 16. Springer, 713729. Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. 2024. I2v-adapter: general imageto-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers. 112. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2023. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023). Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. 2023. Lvos: benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1348013492. Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grauman, and Alexander Schwing. 2020. Proposal-based video completion. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16. Springer, 3854. Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. 2024. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. arXiv preprint arXiv:2403.06976 (2024). Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. 2019. Deep video inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 57925801. Kuaishou. 2024. KLING SPARK YOUR IMAGINATION. https://kling.kuaishou.com/. Sungho Lee, Seoung Wug Oh, DaeYeun Won, and Seon Joo Kim. 2019. Copy-and-paste networks for deep video inpainting. In Proceedings of the IEEE/CVF international conference on computer vision. 44134421. Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong, Jianzhong Qi, Rui Zhang, Dacheng Tao, and Ramamohanarao Kotagiri. 2020. Short-term and long-term context aggregation network for video inpainting. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16. Springer, 728743. Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Ying Shan, and Qiang Xu. 2024. BrushEdit: All-In-One Image Inpainting and Editing. arXiv preprint arXiv:2412.10316 (2024). Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. 2022. Towards an end-to-end framework for flow-guided video inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1756217571. Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. 2021. Decoupled spatial-temporal transformer for video inpainting. arXiv preprint arXiv:2104.06637 (2021). Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023). Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. 2024. ReVideo: Remake Video with Motion and Content Control. arXiv preprint arXiv:2405.13865 (2024). NVIDIA. 2025. NVIDIA Cosmos: Accelerate Physical AI Development with World Foundation Models. https://www.nvidia.com/en-us/ai/cosmos/ OpenAI. 2024. Hello GPT-4. https://openai.com/index/hello-gpt-4o/ OpenAI. 2024. Video generation models as world simulators. https://openai.com/sora/. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. 2016. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 724732. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. 2024. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720 (2024). Weize Quan, Jiaxi Chen, Yanli Liu, Dong-Ming Yan, and Peter Wonka. 2024. Deep International Journal of learning-based image and video inpainting: survey. Computer Vision 132, 7 (2024), 23672400. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R√§dle, Chloe Rolland, Laura Gustafson, et al. 2024. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. Stable Diffusion 2 Inpainting. https: Robin Rombach and Patrick Esser. 2022. //huggingface.co/stabilityai/stable-diffusion-2-inpainting. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems 35 (2022), 2527825294. Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. 2024. Motioni2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers. 111. Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, and Dacheng Tao. 2024. Diffusion modelbased video editing: survey. arXiv preprint arXiv:2407.07111 (2024). Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control 9 IEEE/CVF International Conference on Computer Vision. 1047710486. Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong, and Lei Zhang. 2024. CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility. arXiv preprint arXiv:2403.12035 (2024). Xueyan Zou, Linjie Yang, Ding Liu, and Yong Jae Lee. 2021. Progressive temporal feature alignment network for video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1644816457. Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024). Zachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16. Springer, 402419. Pavel Tokmakov, Jie Li, and Adrien Gaidon. 2023. Breaking the Object in Video Object Segmentation. In CVPR. Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang. 2019. Video inpainting by jointly learning temporal structure and spatial details. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 52325239. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan In AdKautz, and Bryan Catanzaro. 2018. Information Processing Systems, S. Bengio, H. Wallach, vances in Neural H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/ d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf Video-to-Video Synthesis. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. CogVLM: Visual Expert for Pretrained Language Models. arXiv:2311.03079 [cs.CV] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. 2024. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems 36 (2024). Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13, 4 (2004), 600612. Wikipedia contributors. 2024a. Mean absolute error Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Mean_absolute_error Wikipedia contributors. 2024b. Mean squared error Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Mean_squared_error Wikipedia contributors. 2024c. Peak signal-to-noise ratio Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/w/index.php?title=Peak_signal-to-noise_ ratio&oldid=1210897995 [Online; accessed 4-March-2024]. Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. 2021. GODIVA: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806 (2021). Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. 2018. Youtube-vos: Sequence-to-sequence video object segmentation. In Proceedings of the European conference on computer vision (ECCV). 585601. Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy. 2019. Deep flow-guided video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 37233732. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. arXiv preprint arXiv:2408.06072 (2024). Yanhong Zeng, Jianlong Fu, and Hongyang Chao. 2020. Learning joint spatial-temporal transformations for video inpainting. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVI 16. Springer, 528543. Kaidong Zhang, Jingjing Fu, and Dong Liu. 2022a. Flow-guided transformer for video inpainting. In European Conference on Computer Vision. Springer, 7490. Kaidong Zhang, Jingjing Fu, and Dong Liu. 2022b. Flow-guided transformer for video inpainting. In European Conference on Computer Vision. Springer, 7490. Kaidong Zhang, Jingjing Fu, and Dong Liu. 2022c. Inertia-guided flow completion and style fusion for video inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 59825991. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 586595. Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. 2024a. Recognize anything: strong image tagging model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 17241732. Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. 2024b. AVID: Any-Length Video Inpainting with Diffusion Model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 71627172. Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy. 2023. Propainter: Improving propagation and transformer for video inpainting. In Proceedings of the 10 Bian, Y. et al Fig. 8. More video inpainting results. VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control 11 Fig. 9. More video editing results."
        }
    ],
    "affiliations": [
        "Tencent ARC Lab, China",
        "The Chinese University of Hong Kong, China",
        "The University of Tokyo, Japan",
        "University of Macau, China"
    ]
}