{
    "paper_title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
    "authors": [
        "Yu-Che Tsai",
        "Hsiang Hsiao",
        "Kuan-Yu Chen",
        "Shou-De Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 0 9 0 7 0 . 2 0 6 2 : r Published as conference paper at ICLR CONCEPT-AWARE PRIVACY MECHANISMS FOR DEFENDING EMBEDDING INVERSION ATTACKS Yu-Che Tsai1 Hsiang Hsiao1 Kuan-Yu Chen1 1Department of Computer Science and Information Engineering, National Taiwan University 2National Taiwan University AI Center of Research Excellence Taipei, Taiwan {f09922081,r12946003,d13922034,sdlin}@csie.ntu.edu.tw Shou-De Lin1,"
        },
        {
            "title": "ABSTRACT",
            "content": "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text embeddings are general representations of textual data that enable various downstream learning tasks without utilizing the raw text. Recent advances in pre-trained models like Sentence-T5 (Ni et al., 2022a) and SentenceBERT (Reimers & Gurevych, 2019) enable the generation of high-quality embeddings that power numerous NLP applications. prominent example is retrieval-augmented generation (RAG) systems (Lewis et al., 2020), which have led to the widespread adoption of online embedding database services such as Chroma1 and Faiss (Johnson et al., 2019). However, recent research has uncovered critical vulnerabilities in text embeddings through embedding inversion attacks (Huang et al., 2024; Pan et al., 2020; Song & Raghunathan, 2020). These attacks can extract sensitive attributes or even reconstruct the original text. For example, prior work (Coavoux et al., 2018) showed that demographic information can be inferred directly from embeddings, while GEIA (Li et al., 2023) demonstrated that full sentences can be recovered. Most strikingly, Vec2Text (Morris et al., 2023) reported that adversaries can reconstruct up to 92% of 32-token input from T5-based embeddings. Such vulnerabilities pose significant risks in domains handling sensitive data, such as patient notes in medical RAG system. Thus, developing robust defenses against embedding inversion has become critical challenge. Differential privacy (DP) (Dwork et al., 2006) is widely adopted framework for protecting sensitive information due to its rigorous guarantees. However, most existing DP-based defenses implicitly assume that all information in embeddings is equally privacy-sensitive. This assumption has two drawbacks. First, privacy concerns are inherently userand context-dependent (Brown et al., 2022): one individual may prioritize protecting health conditions, while another may care more about political views or personal relationships. Second, to cover all possible sensitive information, DP mechanisms typically inject substantial noise across all embedding dimensions, which inevitably leads to significant utility degradation. Therefore, it is crucial to develop defense mechanism that can provide concept-specific protectionallowing users to specify which attributes to protect while 1https://docs.trychroma.com/ 1 Published as conference paper at ICLR Figure 1: Illustration of embedding inversion attack and different defense strategies. (a) Sensitive information can be easily identified from non-protected text embeddings. (b) Adding spherical noise mitigates privacy leakage but harms textual semantics. (c) Our approach applies elliptical noise guided by user-defined privacy concept, selectively adding stronger perturbations to privacysensitive dimensions while preserving non-sensitive semantics. real-world case study is presented in Appendix J. preserving embedding quality for non-sensitive content. This work aims to address key research question: Research Question: Can we selectively obfuscate user-defined private concepts in embeddings while preserving non-sensitive semantics for downstream tasks? However, designing such defense mechanism is non-trivial. The central challenge lies in the mismatch between existing DP methods and the heterogeneous nature of embedding dimensions. Current approaches add the same level of noise to every embedding dimension, implicitly assuming that all dimensions carry equal amounts of sensitive information. However, our preliminary analysis (see Appendix A) reveals that embedding dimensions exhibit varying degrees of privacy sensitivity with respect to specific concepts. Some dimensions may be highly sensitive to particular privacy attributes (e.g., medical conditions), while others primarily encode non-sensitive semantic features. To address this challenge, an ideal defense mechanism should accomplish two key objectives: (1) identify which embedding dimensions are privacy-sensitive for given privacy concept, and (2) design differential privacy mechanism that calibrates noise injection based on dimension sensitivity while maintaining theoretical guarantees. We propose SPARSE (Sensitivity-guided Privacy-Aware Representations for better SEmanticpreserving), novel user-centric framework that improves privacy in text embeddings through sensitivity-guided perturbations. To achieve the first goal, we present differentiable mask learning framework to estimate the sensitivity of embedding dimensions with respect to user-defined privacy concept. To achieve the second goal, we introduce the Mahalanobis mechanism, an extension of the generalized Laplace mechanism, which injects elliptical noise calibrated by dimension sensitivity. As illustrated in Figure 1, while traditional methods apply spherical noise that uniformly perturbs all dimensions (panel b), our approach first identifies privacy-sensitive dimensions associated with user-specified concepts (e.g., symptom or age) and then applies elliptical noise with larger perturbations to these sensitive dimensions while minimally affecting others (panel c). We summarize our key contribution as follows: Novel defense paradigm. We introduce SPARSE, sensitivity-guided framework for user-defined privacy protection in embeddings, and introduce the Mahalanobis mechanisman extension of differential privacy that provides rigorous theoretical guarantees. Better privacy-utility tradeoffs. We evaluate SPARSE against two state-of-the-art differential privacy methods across six datasets. Experimental results show that SPARSE consistently reduces privacy leakage while achieving better downstream performance. Robust generalization. We assess the generalizability of SPARSE using three different embedding models and three attack models. Experimental results demonstrate that SPARSE remains consistently effective regardless of the specific embedding method or threat model used. Comparable performance to white-box defense. We further design white-box variant of SPARSE with full access to the threat model. Despite lacking prior knowledge of the attack 2 Published as conference paper at ICLR model, SPARSE achieves performance close to the white-box defense, demonstrating its ability to accurately identify privacy-sensitive dimensions."
        },
        {
            "title": "2.1 BACKGROUND ON DIFFERENTIAL PRIVACY",
            "content": "Differential Privacy (DP) (Dwork et al., 2006) is rigorous privacy guarantee that ensures randomized mechanism behaves similarly on any two inputs. There are two common models of DP: central and local. In this work, we focus on Local Differential Privacy (LDP)(Kasiviswanathan et al., 2011), where each user perturbs their data locally before sharing it. This approach offers stronger privacy guarantees in settings where the data collector cannot be trusted, as it removes the need for trusted aggregator. Definition 1 (Local Differential Privacy). randomized mechanism satisfies ϵ-local differential privacy if for all pairs of possible user inputs x, and any output set Range(M), Pr[M(x) O] eϵ Pr[M(x) O], where ϵ 0 is privacy parameter and Range(M) denotes the set of all possible outputs of M. The mechanism outputs random sample from probability distribution over possible outputs, rather than deterministic value. The ϵ parameter, termed the privacy budget, controls the similarity in the output, with smaller ϵ indicating higher privacy protection, and vice versa. Generalization with distance metrics. Local differential privacy (LDP) requires mechanism to produce nearly indistinguishable outputs for any two possible inputs, regardless of how different the inputs are. While this provides strong privacy guarantee, it often leads to significant utility loss, especially in continuous or semantic domains such as text embeddings (Feyisetan et al., 2019). To address this limitation, we adopt metric local differential privacy (metric LDP) (Chatzikokolakis et al., 2013; Alvim et al., 2018), generalization of LDP to metric spaces. Metric LDP relaxes the indistinguishability requirement by incorporating distance function over the input space. This allows the privacy guarantee to degrade gracefully as the dissimilarity between inputs increases. Definition 2 (Metric Local Differential Privacy). Let ϵ 0 be the privacy parameter, and be distance metric for the input space. mechanism satisfies ϵd-LDP, if for any two inputs x, and any output set Range(M), Pr[M(x) O] eϵd(x,x) Pr[M(x) O]. The key idea is that the privacy guarantee depends on how similar the inputs are: closer inputs must yield nearly indistinguishable outputs, while distant inputs may produce more distinguishable ones. Although the privacy budget ϵ remains fixed, the output bound varies with the input distance. To instantiate mechanism satisfying metric LDP under ℓ2 distance, we introduce the generalized Laplace mechanism, which is widely used for embedding sanitization against adversarial attacks. Definition 3 (Generalized Laplace Mechanism (Wu et al., 2017)). Let ϵ 0 be the privacy budget. The generalized Laplace mechanism MLap : Rn Rn perturbs any input Rn as MLap(x) = + ZLap, ZLap fZ(z) exp (ϵ z2) . We note two important properties of the generalized Laplace mechanism: (1) it satisfies ϵd-LDP with respect to the ℓ2 norm (Du et al., 2023); and (2) it adds isotropic (spherical) noise, implicitly assuming that privacy sensitivity is uniformly distributed across all embedding dimensions. 2.2 PROBLEM STATEMENT Attack Scenario. In this work, we focus on specific embedding inversion attack scenario where the adversary aims to reconstruct the input text from the corresponding text embedding. Formally, given sequence of tokens and the text embedding model Φ : Rn, where denotes the embedding dimension, the attacker seeks to find function to approximate the inversion function of Φ as: g(Φ(s)) Φ1(Φ(s)) = s. These inversion attacks can be classified into two categories based on 3 Published as conference paper at ICLR their target: (i) token-level inversion (Pan et al., 2020; Song & Raghunathan, 2020), which focuses on retrieving individual tokens from the original text, and (ii) sentence-level inversion (Li et al., 2023; Morris et al., 2023), which attempts to reconstruct the entire ordered sequence of text. Regardless of the attack model employed, our study prioritizes understanding whether private information (e.g., names, diseases) within the original text is revealed. Privacy Definition. Privacy is inherently context-dependent (Brown et al., 2022). While many prior works adopt narrow operational definition centered on personally identifiable information (PII) such as names or identification numbers (Sousa & Kern, 2023), such fixed notion is often insufficient. In practice, users may care about protecting different types of sensitive attributesfor instance, health conditions, political views, or personal relationships. To capture this variability, we adopt user-centric privacy definition, where the data owner specifies privacy concept representing the set of tokens or attributes to be protected. In our experiments, we instantiate primarily with named entities and PII tokens, but the framework naturally generalizes to other user-defined concepts. Defense Scenario. Our goal is to develop privacy-preserving embeddings that satisfy two objectives: Goal 1 (Defending against sensitive token inference attack): For the threat model and text embedding Φ(s), where is sentence that contains sensitive information. The data owner defines privacy concept = {t1, t2, . . . , tC}, which is set of sensitive tokens (e.g., names, medical conditions) that must be protected. The objective is to generate an obfuscated embedding Φ(s) that prevents the threat model from accurately reconstructing the tokens in C. Goal 2 (Maintaining downstream utility): The secondary objective is to ensure that the protective measures, while securing the embeddings from inversion attacks, do not compromise the utility of the embeddings in downstream tasks."
        },
        {
            "title": "3 SPARSE FRAMEWORK",
            "content": "3.1 IDENTIFYING PRIVACY-SENSITIVE DIMENSION THROUGH NEURON MASK LEARNING To quantify the sensitivity of individual dimensions with respect to privacy concept C, we propose neuron mask learning framework that estimates relaxed binary mask over the embedding dimensions. The goal is to learn mask vector [0, 1]n that approximates binary selection: assigning values close to 1 for dimensions relevant to C, and close to 0 otherwise. Given an embedding Φ(s), the masked representation is denoted by Φ(s) m, where indicates the Hadamard product. Differentiable Neuron Mask Learning. Although the ultimate goal is to approximate binary mask, direct optimization over discrete values is not feasible due to non-differentiability. Therefore, we resort to practical method that employs smoothing approximation of the discrete Bernoulli distribution (Maddison et al., 2017). Under this framework, we assume each mask mi follows hard concrete distribution HardConcrete(log αi, βi) with location αi and temperature βi (Louizos et al., 2018) as: si = σ (cid:18) log (cid:18) 1 βi µi 1 µi (cid:19)(cid:19) + log αi , mi = min (1, max (0, si (ξ γ) + γ)) , (1) where σ denotes the sigmoid function. ξ = 1.1 and γ = 0.1 are constants, and µi U(0, 1) is the random sample drawn from the uniform distribution. αi and βi are learnable parameters. The random variable si follows binary concrete (or Gumbel Softmax) distribution, which is an approximation of the discrete Bernoulli distribution. Samples from the binary concrete distribution are identical to samples from Bernoulli distribution with probability αi as βi 0, and the location αi allows for gradient-based optimization through reparametrization tricks (Jang et al., 2022). During the inference stage, the mask mi could be derived from hard concrete gate: mi = min (1, max (0, σ (log αi) (ξ γ) + γ)) . (2) Training Dataset Construction. We construct two datasets to identify the embedding dimensions most affected by the privacy concept C. The positive dataset D+ = {s1, . . . , sD+} consists of sentences that include tokens representing the concept C. For each sentence si D+, we construct corresponding negative sample by removing all tokens related to C, denoted as R(si, C). This yields Published as conference paper at ICLR 2026 the negative dataset = {R(si, C) si D+}, where each sentence is identical to its positive counterpart except for the absence of concept-specific tokens. Learning Objective. The neuron mask is trained to satisfy two key objectives: (i) The masked embedding Φ(s) should retain sufficient information to distinguish between the positive and negative datasets D+ and D, respectively; and (ii) the mask should be sparse, thereby isolating only the most relevant dimensions associated with the privacy-sensitive concept C. To achieve these objectives, we define composite loss function. The first term is discriminative loss that encourages separation between D+ and D: Lcls(m, θ) = (cid:88) log Pθ (cid:0)Φ(s+) m(cid:1) (cid:88) log (cid:0)1 Pθ (cid:0)Φ(s) m(cid:1)(cid:1) , (3) s+D+ sD where Pθ() denotes the probability predicted by MLP classifier parameterized by θ. To enforce sparsity in the learned mask, we add an L0-regularization term based on the expected number of active neurons under the hard concrete distribution: Lreg(m) = 1 m (cid:88) i= (cid:18) σ log αi βi log (cid:18) γ ξ (cid:19)(cid:19) . The final objective function jointly optimizes the classification performance and sparsity: min m,θ Lcls(m, θ) + λLreg(m), (4) (5) where the regularization coefficient λ controls the trade-off between predictive accuracy and the compactness of the neuron mask. For more implementation details, readers are referred to Appendix and Algorithm 2. 3.2 EMBEDDING PERTURBATION WITH MAHALANOBIS MECHANISM Having identified the privacy-sensitive embedding dimensions through the learned neuron mask m, we now describe how to perturb the embeddings in sensitivity-aware manner. Specifically, we extend the generalized Laplace mechanism by incorporating Mahalanobis norm-based perturbation scheme, thereby enabling elliptical noise calibrated by the neuron sensitivity of m. We begin by formally defining the Mahalanobis norm. Definition 4 (Mahalanobis Norm). For any vector Rn, and positive definite matrix Σ Rnn, its Mahalanobis norm is defined as vM = vΣ1v. Note that for any η > 0, the Euclidean ball {y Rn : x2 = η} defines sphere, implying isotropic noise in all directions. In contrast, the Mahalanobis ball {y Rn : xM = η} defines an ellipsoid. This distinction allows us to inject anisotropic noise whose spread adapts to the sensitivity of each embedding dimension. Definition 5 (Mahalanobis Mechanism). Let ϵ 0 be the privacy budget and let Σ Rnn be symmetric positive definite matrix. The Mahalanobis mechanism MMah : Rn Rn perturbs any input as MMah(x) = + ZMah, ZMah fZ(z) exp (ϵ zM ) . To calibrate noise based on the learned neuron sensitivity, we define Σ = diag(m1 + δ, . . . , mn + δ), where mi is the i-th entry of and δ = 1e6 is small constant ensuring positive definiteness. For scale compatibility with the isotropic Laplace mechanism, we normalize such that (cid:80) mi = (i.e., trace(Σ) = trace(In)). Algorithm 1 details how to sample ZMah. We now establish the privacy guarantee of this mechanism: Theorem 1. Given privacy parameter ϵ, the Mahalanobis mechanism outputting Φ(s) (Φ (s)) fulfills ϵd-LDP with respect to the Mahalanobis Norm. formal proof is provided in Appendix B.1. Below, we explain how the privacy guarantee of the Mahalanobis mechanism relates to that of the generalized Laplace mechanism. Connecting Privacy Guarantee to Generalized Laplace Mechanism. We now show that the privacy guarantee of the Mahalanobis mechanism is equivalent, up to constant factors, to that of 5 Published as conference paper at ICLR 2026 the generalized Laplace mechanism. Since the Mahalanobis and Euclidean norms are equivalent in finite-dimensional spaces, the Mahalanobis mechanism preserves the same asymptotic privacy guarantee, differing only by data-independent constants. Lemma 1. Let Σ Rnn be positivedefinite with trace(Σ) = n. Assume the smallest eigenvalue of Σ is bounded below by > 0. Then, for any vector Rn, v2 vM v2 . Building on this, the following lemma shows that the privacy-loss exponent under the Mahalanobis mechanism is bounded between two exponents based on the Euclidean norm: Lemma 2. Assume trace(Σ) = and that the smallest eigenvalue of Σ is bounded below by constant > 0. Then, for every input text s, and every ϵ 0, exp (cid:18) ϵ Φ(s) Φ(s)2 (cid:19) exp (ϵ Φ(s) Φ(s)M ) exp (cid:18) ϵ Φ(s) Φ(s) (cid:19) . Together, these lemmas show that the Mahalanobis mechanism achieves privacy guarantee comparable to that of the generalized Laplace mechanism under the same privacy budget ϵ. The detailed proof in the section is deferred to Appendix B."
        },
        {
            "title": "4 EXPERIMENTAL EVALUATION",
            "content": "4.1 EXPERIMENT SETUP Datasets. Following prior work on embedding inversion (Morris et al., 2023; Kim et al., 2022), We evaluate six benchmark datasets with downstream labels (for privacy-utility tradeoff) and two real-world datasets, PII-Masking-300K (Team, 2023) and MIMIC-III (Johnson et al., 2018), covering 27 PII types and clinical notes. We extract the named entities as sensitive information for these datasets using named entity recognition models (detailed in Appendix E). Attack models. Three attack models are employed to access the privacy risks of text embedding, including Vec2text (Morris et al., 2023), GEIA (Li et al., 2023), and MLC (Song & Raghunathan, 2020). Vec2text and GEIA are sentence-level attack methods that leverage pre-trained LLMs to reconstruct the input sentence. MLC utilizes three-layer MLP to predict the existence of individual words. Due to its superior performance, Vec2text serves as our default attack model in subsequent experiments. Defense methods. We compare our proposed SPARSE with two established differential privacy approaches: generalized Laplace mechanism (Wu et al., 2017) (LapMech) and Purkayastha mechanism (Du et al., 2023) (PurMech). LapMech introduces privacy by sampling noise from the Laplace distribution and adding it to the embedding vectors, while PurMech utilizes Purkayastha directional noise to perturb embeddings while preserving semantic meaning. These baselines represent the stateof-the-art in embedding privacy protection methods and provide strong comparisons for evaluating our approach. Evaluation Metrics. To quantify privacy risk, we use two measures: (1) Leakage: the attack models accuracy in predicting sensitive tokens (lower is better); (2) Confidence: the probability of the attack model to predict the sensitive tokens (lower indicates less exposure). For downstream utility, we report each datasets standard task metric (e.g., NDCG or correlation; see Appendix Table 6). Please refer to Appendix for detailed description of all the evaluation metrics. Embedding models. We evaluate three widely used embedding models: GTR-base (Ni et al., 2022b), Sentence-T5 (Ni et al., 2022a), and SBERT (Reimers & Gurevych, 2019). GTR-base is the default model due to its higher vulnerability to the Vec2text attack. 4.2 PRIVACY-UTILITY TRADE-OFF ANALYSIS We evaluate the privacy-utility trade-off across different defense methods and privacy budgets of ϵ using the STS12 and FIQA datasets. Note that we vary the values of ϵ {5, 10, 20, 30, 40} following 6 Published as conference paper at ICLR 2026 the settings of prior works (Feyisetan et al., 2020; 2019). The results are presented in Table 1. Here, ϵ = denotes the unprotected embedding. In comparison with the baseline methods (LapMech and PurMech), SPARSE demonstrates consistent superiority in minimizing privacy leakage while maintaining downstream utility. On the STS12 dataset at ϵ = 10, SPARSE reduces privacy leakage from 60% to 19%, whereas alternative methods achieve only 22% reduction. Meanwhile, SPARSE maintains 65% downstream utility while other methods decline to 60%. Although the marginal benefits diminish as ϵ increases, SPARSEs superior performance remains consistent across varying privacy budgets and datasets. We evaluate SPARSE on four more datasets and two real-world cases with sensitive attributes. As detailed in Appendix F.1 and 4.4, SPARSE consistently reduces privacy leakage and outperforms baseline methods. Table 1: Privacy-utility tradeoff across various defense methods. The mean and standard deviation of 5 runs are reported in percentages(%). Privacy Metrics Leakage Confidence Utility Metric Downstream Dataset STS12 FIQA ϵ 5 10 20 30 40 5 10 20 30 LapMech PurMech SPARSE LapMech PurMech SPARSE LapMech PurMech SPARSE 7.36 0.61 22.34 1.38 38.17 0.86 44.74 0.43 48.48 0.60 12.56 0.98 35.17 1.46 55.69 1.05 64.12 0.82 68.85 1. 7.42 0.49 22.66 1.15 38.04 0.71 44.76 0.49 48.34 0.57 60.09 13.01 1.40 35.31 0.86 55.38 1.26 64.13 0.85 68.63 1.36 77.35 4.34 0.51 19.31 0.21 36.98 0.45 43.81 0.24 47.54 0.44 6.70 0.32 9.39 0.17 24.70 0.75 34.59 0.32 38.75 0. 8.48 0.30 31.62 0.75 53.41 1.89 63.51 0.69 68.13 0.80 6.67 0.51 16.70 0.74 35.32 0.74 43.35 1.50 48.07 1.08 6.80 0.29 9.42 0.17 24.74 0.71 34.59 0.24 38.82 0.79 47.81 6.70 0.49 16.55 0.66 35.25 0.78 43.56 1.53 47.77 0.78 54. 6.41 0.23 8.91 0.12 23.85 0.43 34.16 0.76 38.49 0.76 29.28 0.00 60.72 0.00 72.47 0.00 73.68 0.00 73.98 0.00 6.18 0.25 13.45 0.38 33.77 0.73 42.21 0.91 46.65 0.55 10.64 0.24 21.74 0.36 32.22 0.14 33.24 0.03 33.50 0.14 29.31 0.00 60.72 0.00 72.47 0.00 73.68 0.00 73.98 0.00 74. 10.63 0.25 21.76 0.29 32.23 0.13 33.26 0.04 33.52 0.15 33.56 34.12 0.00 65.27 0.00 73.25 0.00 74.04 0.00 74.15 0.00 14.87 0.15 23.45 0.29 32.65 0.19 33.58 0.13 33.85 0.11 4.3 DEFENSE ROBUSTNESS AGAINST DIFFERENT THREAT MODELS While previous experiments focus on Vec2text, it is important to assess SPARSE under varied threat models. We evaluate privacy leakage under three embedding inversion attack models: MLC (Song & Raghunathan, 2020), GEIA (Li et al., 2023), and Vec2text (Morris et al., 2023). Since changing the attack model does not impact downstream utility, we report only the Leakage metric. As shown in Table 2, SPARSE consistently outperforms LapMech and PurMech across all attack models by significant margin. Additionally, we notice that complex attack models, such as Vec2text and GEIA, are more susceptible to embedding perturbation, exhibiting substantial leakage reductions of 92% and 72% respectively at ϵ = 5. In contrast, the shallow MLC model demonstrates less vulnerability to our defense method. The results suggest that SPARSE offers more resilient defense against diverse embedding inversion threats. Table 2: Defense performance with respect to different attack models. We report the Leakage metric in percentage (%) on the STS12 dataset. In addition, we highlight the relative performance compared to the non-protected embedding in red. Attack Models ϵ = LapMech PurMech SPARSE LapMech PurMech SPARSE Vec2text (Morris et al., 2023) GEIA (Li et al., 2023) MLC (Song & Raghunathan, 2020) 60.09 25.34 53.20 7.36 (-87.75%) 7.42 (-87.65%) 4.34 (-92.78%) 22.34 (-62.82%) 22.66 (-62.29%) 19.31 (-67.86%) 12.30 (-51.46%) 12.36 (-51.22%) 7.08 (-72.06%) 20.60 (-18.71%) 21.21 (-16.30%) 15.82 (-37.57%) 19.39 (-63.55%) 19.80 (-62.78%) 17.63 (-66.86%) 32.74 (-38.45%) 32.68 (-38.57%) 29.59 (-44.38%) ϵ = 5 ϵ = 10 4.4 EVALUATION ON REAL-WORLD PRIVACY THREATS We evaluated SPARSEs resilience to inversion attacks across various data domains and privacy categories. This evaluation used the PII-Masking 300K dataset (Team, 2023), and MIMIC-III clinical notes (Johnson et al., 2018). The results in Table 3 demonstrate significant privacy vulnerabilities in unprotected embeddings and the superior protection offered by our approach. In the MIMICIII dataset, unprotected models exhibited severe privacy leakage with attack models successfully extracting sensitive attributes at concerning rates: 88% for sex, 70% for diseases, and 82% for 7 Published as conference paper at ICLR 2026 symptoms. Under equivalent perturbation budgets of ϵ, SPARSE reduces sex attribute leakage from 88% to 28%, while both LapMech and PurMech achieve only modest reductions to 43%. This superior protection generalizes across all evaluated privacy categories. Table 3: Defense performance on different categories of sensitive information. We report the Leakage metric in percentage (%) with ϵ = 10. Dataset PII-300K MIMIC-III Category Sex City State Country Age Sex Disease Symptom Non-protected 86. 68.45 75.43 LapMech 42.35 33.39 36. PurMech 43.53 34.10 38.45 SPARSE 33. 28.76 33.62 84.07 40.37 41.45 35. 58.49 88.40 31.88 43.38 31.89 43. 28.98 28.45 70.43 23.32 22.86 18. 82.76 38.17 31.30 29."
        },
        {
            "title": "4.5 COMPARING SPARSE WITH WHITE-BOX DEFENSE",
            "content": "Our defense framework is predicated on the hypothesis that sensitive information is encoded within specific dimensions of the embedding space. Consequently, selectively perturbing these dimensions could effectively mitigate inversion attacks. This motivates two key questions: (i) How effective could SPARSE be under perfect knowledge of embedding sensitivity? and (ii) How closely can our black-box approach approximate this ideal? To answer these questions, we design SPARSE-WB, an empirical upper-bound defense assuming white-box access to the attack model. Extending SPARSE to White-Box Defense. For each sensitive token, we use Integrated Gradients (Sundararajan et al., 2017) to compute the gradient of the models output with respect to the input embedding, treating sensitivity estimation as feature attribution problem. Each dimensions attribution score reflects its influence on the prediction. Instead of applying the neuron mask as in the original SPARSE, the white-box method uses the attribution score for sampling noise from the Mahalanobis mechanism. Results. As shown in Table 4, SPARSE-WB consistently achieves the best privacy-utility tradeoff across different datasets and privacy budgets. The promising result of SPARSE-WB verifies our hypothesis and servers as strong upper bound. Importantly, we notice SPARSE closely approaches this white-box defense performance, especially at ϵ = 20, 30, 40, with only small gaps in both leakage and utility. This suggests that SPARSE is able to effectively approximate the white-box sensitivity estimation without access to the attack model, which is crucial in realistic threat settings. Table 4: Comparison of SPARSE with its white-box variant and LapMech to assess how well SPARSE approximates an ideal defense with perfect knowledge of sensitive dimensions. Results are reported in terms of privacy leakage and downstream utility under varying privacy budgets ϵ. Leakage (%) Downstream (%) Dataset Method ϵ = 5 10 20 40 STS12 FIQA LapMech SPARSE SPARSE-WB LapMech SPARSE SPARSE-WB 7.36 4.34 1. 12.56 8.48 3.03 22.34 19.31 12.01 35.17 31.62 22.35 38.17 36.98 33.67 55.69 53.41 51.27 44.74 43.81 42. 64.12 63.51 62.70 48.48 47.54 47.13 68.85 68.13 67.92 ϵ = 5 29.28 34.12 40.92 10.64 14.87 14. 10 20 30 40 60.72 65.27 67.45 21.74 23.45 26. 72.47 73.25 74.01 32.22 32.65 32.87 73.68 74.04 74.13 33.24 33.58 33.55 73.98 74.15 74.10 33.50 33.85 33. 4.6 QUALITATIVE ANALYSIS OF PRIVACY-SENSITIVE DIMENSIONS We present qualitative analysis to better understand the quality of the privacy-sensitive dimensions identified by SPARSE for specific privacy concepts. To enhance interpretability and visualization, we focus on individual words rather than aggregated token sets as in prior experiments. Figure 2 visualizes the learned neuron masks for six semantically coherent groups: weekdays, countries, 8 Published as conference paper at ICLR 2026 Table 5: Leakage mitigation rates achieved by SPARSE with ϵ = 10 compared to non-protected embeddings. Results are evaluated across three token types: target tokens, semantically similar tokens, and unrelated (other) tokens under different privacy categories. Target Similar Other Weekdays Country Months Gender City -76.2% -64.3% -72.5% -61.0% -70.2% -46.2% -36.2% -42.8% -40.5% -39.8% -11.7% -29.1% -12.6% -18.3% -14.7% Figure 2: Visualization of the learned neuron mask by SPARSE for individual tokens, where larger values represent higher privacy sensitivity. months, U.S.-related terms, gender-related terms, and numbers. The x-axis shows the union of the top-5 neuron indices most strongly associated with each word. We have the following two findings: 1) Semantically related words activate overlapping privacy-sensitive dimensions. As depicted in Figure 2, we found that words with similar semantics, such as weekdays or countries, tend to cluster around the similar embedding dimensions. The clustering behavior verifies the quality of our proposed neuron mask detection process, demonstrating that it effectively localizes meaningful, non-random privacy signals that align with linguistic structure. 2) SPARSE implicitly protects semantically similar tokens. We hypothesize that protecting tokens privacy-sensitive dimensions also benefits semantically similar tokens, as they often share overlapping dimensions. To test this, we apply the learned neuron mask for each target token and evaluate leakage reduction for three types: the target, semantically similar, and unrelated tokens. Leakage mitigation is quantified as the relative reduction of the Leakage metric compared to the non-protected embedding. As Table 5 shows, SPARSE substantially reduces leakage for similar tokens (e.g., 46.2% for Weekdays), even though only the target was protected. These results suggest that although our privacy-sensitive dimensions are identified based on explicitly defined tokens, it implicitly extends protection to broader, more generalizable privacy concept."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Inversion Attacks on Text Embeddings. Text embeddings have been shown to pose serious privacy risks, as they can unintentionally encode and expose sensitive attributes and content (Pan et al., 2020; Song & Shmatikov, 2019; Lyu et al., 2020b; Coavoux et al., 2018). For example, prior work (Pan et al., 2020) demonstrated that keywords can be partially recovered from text embeddings using annotated external datasets. Similarly, attribute inference and embedding inversion attacks have been used to extract unordered sets of words from sentence representations (Song & Raghunathan, 2020). GEIA (Li et al., 2023) extended these attacks by introducing generative approach that reconstructs entire input sequences. More recently, Vec2Text (Morris et al., 2023) showed that embeddings from commercial APIs (e.g., OpenAI) can be inverted with high accuracy. These findings underscore the need for robust privacy-preserving embedding methods. Privacy-preserving Text Embeddings. To mitigate privacy risks in textual representations, prior work has introduced various noise injection mechanisms for tokenand sentence-level embeddings. DPNR (Lyu et al., 2020b) randomly masks input tokens and adds Laplace noise to the resulting embeddings. Feyisetan et al. (Feyisetan et al., 2019) apply generalized Laplace mechanism to perturb token embeddings under metric local differential privacy (LDP). For sentence embeddings, Lyu et al. (Lyu et al., 2020a) directly inject Laplace noise into BERT-based vectors. Laplace-based mechanisms have also been employed to defend against inversion (Morris et al., 2023), membership inference (Song & Raghunathan, 2020), and attribute inference (Coavoux et al., 2018) attacks. Recent work such as the Purkayastha mechanism (Du et al., 2023) further refines Laplace perturbation for enhanced privacy guarantees. 9 Published as conference paper at ICLR"
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced SPARSE, framework that enhances privacy in text embeddings by selectively applying sensitivity-guided elliptical noise. By identifying and perturbing privacy-sensitive embedding dimensions, SPARSE resists embedding inversion attacks while preserving utility. Experiments across models, datasets, and threat scenarios demonstrate its effectiveness in improving the privacy-utility tradeoff. As embeddings become central to real-world systems, embedding-level privacy is essential. We see SPARSE as step toward controllable, concept-aware protection, and hope it encourages research into adaptive and accountable defenses for sensitive NLP."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This material is based upon work supported by National Science and Technology Council, ROC under grant number 114-2221-E-002-134-MY3 and by Taiwan Centers of Excellence (TCE) 10 Published as conference paper at ICLR"
        },
        {
            "title": "ETHICAL CONSIDERATIONS",
            "content": "While SPARSE is designed to enhance privacy in text embedding applications, its deployment must be guided by ethical considerations. First, although our method reduces the risk of embedding inversion, it does not eliminate all privacy threats, and may offer false sense of security if used without awareness of its limitations. Practitioners should carefully evaluate the privacy requirements of their specific context and avoid over-relying on embedding anonymization as substitute for broader data governance and access controls. Second, our framework is concept-driven and depends on predefining sensitive information categories. This raises fairness concerns: groups or attributes not explicitly included in the sensitive concept space may receive less protection, potentially reinforcing systemic biases or exposing vulnerable populations. Future implementations should strive for inclusiveness in concept selection and explore concept-agnostic sensitivity detection to mitigate this risk. Finally, as with any privacy-preserving technique, SPARSE could be misusedfor example, to evade moderation or mask malicious content. We encourage responsible use aligned with principles of transparency, accountability, and user consent, especially in high-stakes domains such as healthcare, education, or law."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "All essential details required to reproduce our main results are provided in this paper. Appendix offers comprehensive descriptions of the model architectures and training procedures, Appendix details the attack configurations used in our experiments, and Appendix presents the formal definitions of all evaluation metrics. In addition, we plan to publicly release our code in the near future to further facilitate reproducibility and future research."
        },
        {
            "title": "REFERENCES",
            "content": "Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: pilot on semantic textual similarity. In Eneko Agirre, Johan Bos, Mona Diab, Suresh Manandhar, Yuval Marton, and Deniz Yuret (eds.), *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 385393, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1051. Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2014 task 10: Multilingual semantic textual similarity. In Preslav Nakov and Torsten Zesch (eds.), Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pp. 8191, Dublin, Ireland, August 2014. Association for Computational Linguistics. doi: 10.3115/v1/S14-2010. URL https://aclanthology.org/S14-2010. Mário Alvim, Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Anna Pazii. Local differential privacy on metric spaces: optimizing the trade-off with utility. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pp. 262267. IEEE, 2018. Alexander Bondarenko, Maik Fröbe, Meriem Beloucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, et al. Overview of touché 2020: argument retrieval. In Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020, Thessaloniki, Greece, September 2225, 2020, Proceedings 11, pp. 384395. Springer, 2020. Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. full-text learning to rank dataset for medical information retrieval. In Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 2023, 2016. Proceedings 38, pp. 716722. Springer, 2016. Published as conference paper at ICLR 2026 Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr. What does it mean for language model to preserve privacy? In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pp. 22802292, 2022. Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens (eds.), Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 114, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://aclanthology.org/S17-2001. Konstantinos Chatzikokolakis, Miguel Andrés, Nicolás Emilio Bordenabe, and Catuscia Palamidessi. Broadening the scope of differential privacy using metrics. In Privacy Enhancing Technologies: 13th International Symposium, PETS 2013, Bloomington, IN, USA, July 10-12, 2013. Proceedings 13, pp. 82102. Springer, 2013. Maximin Coavoux, Shashi Narayan, and Shay Cohen. Privacy-preserving neural representations of text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 110, 2018. Minxin Du, Xiang Yue, Sherman SM Chow, and Huan Sun. Sanitizing sentence embeddings (and In Proceedings of the ACM Web Conference 2023, pp. labels) for local differential privacy. 23492359, 2023. Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pp. 265284. Springer, 2006. Oluwaseyi Feyisetan, Tom Diethe, and Thomas Drake. Leveraging hierarchical representations for preserving privacy and utility in text. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 210219. IEEE, 2019. Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and Tom Diethe. Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations. In Proceedings of the 13th international conference on web search and data mining, pp. 178186, 2020. Yu-Hsiang Huang, Yuche Tsai, Hsiang Hsiao, Hong-Yi Lin, and Shou-De Lin. Transferable embedding inversion attack: Uncovering privacy risks in text embeddings without model queries. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4193 4205, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.230. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2022. Alistair Johnson, David Stone, Leo Celi, and Tom Pollard. The mimic code repository: enabling reproducibility in critical care research. Journal of the American Medical Informatics Association, 25(1):3239, 2018. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535547, 2019. Shiva Prasad Kasiviswanathan, Homin Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793826, 2011. Donggyu Kim, Garam Lee, and Sungwoo Oh. Toward privacy-preserving text embedding similarity with homomorphic encryption. In Proceedings of the Fourth Workshop on Financial Technology and Natural Language Processing (FinNLP), pp. 2536, 2022. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. 12 Published as conference paper at ICLR 2026 Haoran Li, Mingshi Xu, and Yangqiu Song. Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1402214040, 2023. Christos Louizos, Max Welling, and Diederik Kingma. Learning sparse neural networks through l_0 regularization. In International Conference on Learning Representations, 2018. Lingjuan Lyu, Xuanli He, and Yitong Li. Differentially private representation for nlp: Formal guarantee and an empirical study on privacy and fairness. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 23552365, 2020a. Lingjuan Lyu, Yitong Li, Xuanli He, and Tong Xiao. Towards differentially private text representations. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 18131816, 2020b. Maddison, Mnih, and Teh. The concrete distribution: continuous relaxation of discrete random variables. In Proceedings of the international conference on learning Representations. International Conference on Learning Representations, 2017. Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pp. 19411942, 2018. Microsoft Corporation. Language service overview. https://learn.microsoft.com/ en-us/azure/ai-services/language-service/overview. John Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander Rush. Text embeddings reveal (almost) as much as text. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1244812460, 2023. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. doi: 10.48550/ARXIV.2210.07316. URL https://arxiv.org/abs/2210.07316. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 18641874, 2022a. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 98449855, 2022b. Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. Privacy risks of general-purpose language models. In 2020 IEEE Symposium on Security and Privacy (SP), pp. 13141331. IEEE, 2020. Shaina Raza, Deepak John Reji, Femi Shajan, and Syed Raza Bashir. Large-scale application of named entity recognition to biomedicine and epidemiology. PLOS Digital Health, 1(12):e0000152, 2022. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 39823992, 2019. Congzheng Song and Ananth Raghunathan. Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pp. 377390, 2020. Congzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 196206, 2019. 13 Published as conference paper at ICLR Samuel Sousa and Roman Kern. How to keep text private? systematic review of deep learning methods for privacy-preserving natural language processing. Artificial Intelligence Review, 56(2): 14271492, 2023. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International conference on machine learning, pp. 33193328. PMLR, 2017. AI4Privacy Team. Pii masking 300k dataset, 2023. URL https://huggingface.co/ datasets/ai4privacy/pii-masking-300k. Licensed under Apache License 2.0. Accessed on: Sep 30, 2024. Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the 2017 ACM International Conference on Management of Data, pp. 13071322, 2017. Published as conference paper at ICLR 2026 Figure 3: Sensitivity distribution comparison between the top and bottom 10% privacy neurons. The Wilcoxon Signed Rank Test indicates highly significant difference (p-value = 1.30 1021). EMPIRICAL VALIDATION OF PRIVACY-SENSITIVE DIMENSIONS In this section, we introduce the concept of privacy neurons and empirically validate their existence and relevance. We demonstrate that privacy-related information within text embeddings may be primarily concentrated in limited subset of dimensions. Definition 6 (Privacy Neurons). Consider an input text and an embedding model Φ : Rd. We assume there is subset of dimensions Nt = {1, . . . , d} that encapsulates the sensitive information associated with privacy concept C. Consequently, the embedding Φ(x) can be expressed as: Φ(s) = (ΦNC (s), ΦVNC (s)), (6) where ΦNC (s) represents the privacy-sensitive neuron activations and ΦVNC (s) the privacyinvariant neuron activations. Intuitively, dimensions identified as privacy neurons should exhibit higher sensitivity to the presence or absence of privacy-related tokens in the input text. To quantify how individual embedding dimensions respond to privacy-related information, we introduce the following measure: Definition 7 (Neuron Sensitivity). Let D+ and denote positive and negative datasets containing sentences with and without tokens related to the privacy concept C, respectively. For each embedding dimension i, the neuron sensitivity is defined as: = max (cid:0){Φ(s+)i Φ(s)i s+ D+, D}(cid:1) , (7) where Φ()i represents the activation value of the i-th embedding dimension. We assume high value of indicates that dimension is responsive and likely encodes privacyrelated information. Dataset Construction for Sensitivity Analysis To measure the embedding changes associated with the privacy concept C, we first construct dataset D+ = {s1, . . . , sD+}, containing sentences that include tokens from concept C. Correspondingly, we generate negative set = {R(si, C) si D+}, where R(si, C) denotes the operation of removing all tokens ti from the sentence si. Thus, consists of sentences identical to D+ except for the absence of tokens associated with the sensitive privacy concept. Results Figure 3 presents the distribution of sensitivity scores for dimensions identified as the top and bottom 10% privacy neurons based on the sensitivity vector v. Our pilot study clearly illustrates significant difference between the two groups. Specifically, the top-ranked privacy neurons demonstrate substantially higher sensitivity scores (mean sensitivity = 0.04) than the bottomranked neurons, which exhibit nearly zero sensitivity. Wilcoxon Signed Rank Test confirms the significance of this observation with p-value of 1.30 1021. These results empirically support the existence of privacy neurons, suggesting that embedding inversion attacks may be effectively mitigated by selectively manipulating only small subset of embedding dimensions. 15 Published as conference paper at ICLR 2026 MISSING PROOF IN SECTION 3. B.1 PROOF OF THEOREM 1 Proof of Theorem 1. Recall that the mechanism releases Φ(s) = Φ(s) + Z, where the noise density is fZ(z) = exp(εzM ) and the normalizing constant is independent of z. For any output Rd, we have: Pr[Φ(s) = y] Pr[Φ(s) = y] (8) = fZ(y Φ(s)) fZ(y Φ(s)) exp(ϵy Φ(s)M ) exp(ϵy Φ(s)M ) = (cid:16) = exp ϵy Φ(s)M + ϵy Φ(s)M (cid:17) By the triangle inequality for the Mahalanobis norm, we have: Φ(s)M Φ(s)M Φ(s) Φ(s)M Therefore: Pr[Φ(s) = y] Pr[Φ(s) = y] (cid:16) exp ϵΦ(s) Φ(s)M (cid:17) This precisely establishes ϵd-local differential privacy under the Mahalanobis norm. (9) (10) (11) (12) B.2 PROOF OF LEMMA Proof of Lemma 1. Because Σ is symmetric positivedefinite, it admits the spectral decomposition Σ = QΛQ, where is orthogonal (QQ = I) and Λ = diag(ξ1, . . . , ξn) collects the eigenvalues ξ1, . . . , ξn of Σ. Write := Qv; note that v2 = v2 because is orthogonal. Upper bound. By assumption ξi for every i, hence the eigenvalues of Σ1 satisfy ξ1 Therefore c1. v2 = vΣ1v = vΛ1v = which yields vM v2/ c. i=1 (cid:88) v2 ξi 1 (cid:88) i=1 v2 = v2 2 , Lower bound. Because trace(Σ) = n, (cid:80)n ξ1 1/n and i=1 ξi = n, implying ξi for every i. Consequently v2 = (cid:88) v2 ξi 1 (cid:88) i=1 v2 = v2 2 , so that vM v2/ n. i=1 Combining the two inequalities completes the proof. B.3 PROOF OF LEMMA 2 Proof of Lemma 2. Let := Φ(x) Φ(x) Rm. By Lemma 1 we have the deterministic bounds v2 vM v2 . Multiplying each term by the nonnegative scalar ϵ preserves the ordering, and applying the (strictly increasing) exponential map yields exp (cid:18) ϵ (cid:19) v2 exp (ϵvM ) exp (cid:18) ϵ (cid:19) v2 , which is precisely the desired statement. Published as conference paper at ICLR"
        },
        {
            "title": "C ALGORITHM FOR MAHALANOBIS NOISE SAMPLING",
            "content": "Algorithm 1 Sampling from fZ(z) exp(ϵzM) 1: Input: Privacy budget ϵ, dimension n, positive definite matrix Σ 2: Sample an n-dimensional random vector from multivariate normal distribution with mean zero and identity covariance matrix. 3: Normalize = N/N 2 4: Sample from Gamma distribution with shape parameter and scale parameter 1/ϵ 5: Return = Σ1/2X Lemma 3. The random variable returned from Algorithm 1 has probability-density function of the form fZ(z) exp(cid:0)ε zM (cid:1), zM = zΣ1z . Proof. Define = X. Note that conditional on = y, is uniformly distributed on the sphere of radius in Rm. Hence fU (u y) y(m1) whenever u2 = y, and zero otherwise. Using the Dirac delta function δ(), we write (cid:90) fU (u) = fU (u y) fY (y) δ (y u2) dy 0 (cid:90) 0 y(n1) ϵn Γ(n) eϵu2, n1eϵy δ (y u2) dy so fU (u) exp(ϵu2). Since Σ is positive definite, Σ1/2 exists and is invertible. Setting = Σ1/2 , the change-of-variables formula yields fZ(z) = fU (cid:16) Σ1/2z(cid:1) (cid:12) (cid:12)det(Σ1/2(cid:17) (cid:17) ϵ Σ1/2z = exp (cid:16) ϵ (cid:17) zΣ1z = exp(cid:0)ϵ zM (cid:1). (cid:16) exp This completes the proof. Table 6: Statistics of datasets. Dataset Downstream task Domain Sentences Average sentence length Unique named entities Evaluation metric STS12 STS SemEval 10684 14.53 123 FIQA Retrieval Financial 5500 10.80 Pearson Corr. NDCG@10 STSB STS14 Quora NFCorpus MIMIC-III PII-300K STS SemEval 17256 10.17 228 Pearson Corr. STS SemEval 3000 9.77 41 Retrieval QA 10000 9.53 90 Pearson Corr. NDCG@10 NDCG@10 Retrieval Medical 2590 3.31 13 - Medical 4244 15.03 290 - - PII 177677 47.12 491 -"
        },
        {
            "title": "D DATASET STATISTICS AND EVALUATION METRICS",
            "content": "Privacy Metrics. To quantify the privacy risk of our model, we adopt two complementary metrics: Leakage and Confidence. These metrics assess both the accuracy and certainty of an adversarial model attempting to infer sensitive information from the models outputs. (1) Leakage. Leakage measures the extent to which an attack model can recover sensitive tokens from an obfuscated embedding. Given sentence si containing sensitive tokens Ci C, the attacker 17 Published as conference paper at ICLR 2026 generates reconstructed sentence ˆsi = A(Φ(si)) based on the obfuscated embedding. The leakage is computed by checking whether any sensitive token appears in the reconstructed sentence: Leakage ="
        },
        {
            "title": "1\nT",
            "content": "N (cid:88) (cid:88) i=1 tCi 1 [t ˆsi] (13) where is the number of text samples, Ci is the set of sensitive tokens in sentence si, ˆsi is the reconstructed sentence from the attacker, and = (cid:80)N i=1 Ci is the total number of sensitive token instances across the dataset. lower Leakage score indicates better protection of sensitive content, as fewer sensitive tokens are successfully inferred by the attacker. (2) Confidence. Confidence quantifies how certain the attack model is when predicting sensitive tokens, regardless of whether the predictions are correct. It is defined as the average predicted probability assigned to the true sensitive tokens across all samples: Confidence ="
        },
        {
            "title": "1\nT",
            "content": "N (cid:88) (cid:88) i=1 tCi PA(t Φ(si)) (14) where Ci is the set of sensitive tokens in sentence si, Φ(si) is the obfuscated embedding, and = (cid:80)N i=1 Ci is the total number of sensitive token instances. The term PA(t Φ(si)) denotes the probability assigned by the attack model to sensitive token based on the obfuscated embedding. lower Confidence score indicates that the model is less certain in its inference, suggesting stronger privacy. Utility Metrics. To assess the utility of the learned representations, we follow the widely adopted evaluation framework provided by the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022). MTEB is standard benchmark for embedding models, covering diverse set of downstream tasks such as classification, clustering, retrieval, and semantic textual similarity. These tasks reflect the practical performance of embeddings in real-world applications. By using MTEB, we ensure that our utility evaluation is comprehensive, comparable, and aligned with established practices in the embedding research community."
        },
        {
            "title": "E SENSITIVE TOKEN EXTRACTION",
            "content": "We utilize the MIMIC-III clinical notes corpus (Johnson et al., 2018), de-identified electronic health record dataset comprising detailed clinical documentation from intensive care units. To extract privacy-sensitive information, we apply biomedical Named Entity Recognition (NER) model (Raza et al., 2022) specifically trained to identify medically relevant entities such as age, sex, diseases, and symptoms. For non-clinical datasets, named entities are extracted using the en_core_web_sm NER pipeline from the spaCy library2, which provides general-purpose entity recognition for categories such as persons, locations, and organizations."
        },
        {
            "title": "F ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "F.1 PERFORMANCE ON MORE DATASETS In addition to the STS12 (Agirre et al., 2012) and FIQA (Maia et al., 2018) datasets used in the main experiment, Table 6 also presents statistics of other datasets, including STSB (Cer et al., 2017), STS14 (Agirre et al., 2014), Quora (Bondarenko et al., 2020), and NFCorpus (Boteva et al., 2016). Table 7 shows the complete defense performance on all datasets. Besides using Leakage, we also utilize Confidence to assess the defense performance. This metric reflects the certainty of the attack models predictions. higher Confidence score indicates that the model is more confident in its prediction of the sensitive token. For the semantic textual similarity (STS) task, downstream 2https://github.com/explosion/spacy-models/releases/tag/en_core_web_ sm-3.7.0 18 Published as conference paper at ICLR performance is measured using the Pearson correlation of Cosine Similarity (Pearson corr.). In the context of information retrieval, we employ the ranking metric NDCG@10. As described in Section 4.2, SPARSE consistently demonstrates superior performance over LapMech and PurMech across all levels of perturbation and datasets, both in defense and downstream task metrics. Table 7: Privacy-utility tradeoff across different defense Methods. Privacy leakage is assessed using Leakage and Confidence metrics, where lower values indicate stronger privacy protection. Utility is measured by data-specific downstream performance. All metrics are presented as percentages (%). Privacy Metrics Leakage Confidence Utility Metric Downstream LapMech PurMech SPARSE LapMech PurMech SPARSE LapMech PurMech SPARSE 20.75 53.79 71.82 76.15 78. 1.03 4.04 8.64 11.22 13.67 25.96 57.44 75.56 81.65 83.69 7.77 29.73 56.76 69.93 78.72 19.03 49.95 69.15 74.98 77.40 86.75 1.55 4.13 8.77 11.26 13. 21.97 25.87 54.78 75.80 81.65 83.64 89.30 8.45 31.42 55.41 69.26 79.05 88.18 2.68 32.39 64.79 73.10 76. 0.30 2.41 9.46 14.70 16.12 2.85 33.67 68.00 76.75 79.79 0.68 12.16 46.96 57.77 66.55 1.89 15.40 41.76 52.36 56.84 20.42 21.86 28.05 30.38 32.09 2.71 18.62 50.87 58.99 62. 1.27 15.92 45.70 58.27 63.89 1.98 13.97 38.44 49.28 54.02 66.57 20.88 21.84 27.73 30.39 32.05 35.99 2.70 15.92 51.00 59.08 62. 68.30 1.06 15.51 46.26 58.00 63.83 75.54 1.78 8.07 36.33 48.63 53.98 18.05 21.10 28.56 32.95 34.81 1.57 9.94 41.21 53.43 57. 0.83 6.73 35.36 48.09 53.85 40.03 71.87 80.95 81.08 80.95 39.76 70.28 79.16 79.47 79.43 11.89 70.04 82.79 83.70 83.90 23.70 27.31 30.76 31.32 31.56 40.05 71.85 80.95 81.08 80. 80.64 39.71 70.25 79.16 79.47 79.43 79.25 11.78 70.19 82.75 83.72 83.91 84.01 23.61 27.38 30.75 31.32 31. 31.63 48.17 76.14 81.00 80.91 80.81 48.47 74.44 79.31 79.37 79.32 15.43 82.19 83.94 84.02 83.97 19.94 29.61 31.04 31.37 31.52 Dataset STSB STS14 Quora NFCorpus ϵ 5 10 20 30 5 10 20 30 40 5 10 20 30 40 5 10 20 30 F.2 DEFENSE PERFORMANCE ON MORE EMBEDDING MODELS To assess the generalizability of SPARSE, we evaluate its performance on three representative embedding models: GTR-base (Ni et al., 2022b), Sentence-T5 (Ni et al., 2022a), and SBERT (Reimers & Gurevych, 2019). As presented in Table 8, SPARSE consistently achieves low privacy leakage (e.g., 19% with GTR-base and 17% with SBERT), while preserving strong downstream utility. In contrast, baseline methods such as LapMech and PurMech not only suffer from higher leakage rates (2030%) but also incur greater utility degradation. These results support the generality of our approach and validate the effectiveness of detecting and perturbing privacy-sensitive dimensions across different embedding architectures. Table 8: Defense and downstream performance using different embedding models under ϵ = 10. We use STS12 dataset and report the mean and standard deviation of 5 runs for all evaluation metrics. Embedding Models GTR-base Sentence-T5 SBERT Metrics Leakage Downstream Leakage Downstream Leakage Downstream Non-protected 60. 74.25 43.83 86.79 42.11 81.36 LapMech PurMech SPARSE 22.34 0.62 60.72 0.00 31.71 0.62 63.16 0. 23.82 0.89 66.89 0.00 22.66 0.67 60.72 0.00 32.11 0.47 63.15 0. 23.59 0.78 65.89 0.00 19.31 0.21 65.27 0.00 22.38 0.44 74.45 0. 17.15 0.74 69.42 0.00 19 Published as conference paper at ICLR 2026 F.3 COMPARISON WITH PII-BASED DEFENSE METHODS Since the goal of SPARSE aims to mitigate the privacy leakage of sensitive tokens, it raises natural question: how does SPARSE compare to traditional PII removal or transformation methods? To answer this question, we evaluate three additional PII-based defense approaches: (1) PII removal via Azure Language Service (Microsoft Corporation), which replaces private tokens with *, (2) Random word replacement from the corpus, and (3) Semantic word replacement within the same named entity category. The results are presented in Table 9. We have the following key insights: PII transformation incurs significant information loss. All PII-based strategies lead to noticeable degradation in downstream performance. For instance, PII redaction reduces STS12 accuracy from 74% to 59%, and FIQA from 33% to 21%. Semantic replacement fares slightly better, with scores of 64% (STS12) and 18% (FIQA), but still underperforms relative to the original embeddings. Random replacement exhibits similar decline, indicating that simple token-level transformations often disrupt semantic integrity. SPARSE achieves better privacy-utility tradeoff. While PII transformations can obscure sensitive content, they often compromise task utility. To evaluate this tradeoff, we define tradeoff rate metric = Leakage Utility , where Leakage is the reduction in privacy leakage, and Utility is the drop in downstream performance relative to the unprotected embeddings. For simplicity and upper-bound estimation, we assume that PII-based methods reduce leakage to zero. As shown in Table 9, SPARSE achieves markedly higher tradeoff rates of 23.11 on STS12 and 26.30 on FIQA, compared to 46 for the PII-based approaches. These results verify the advantage of embedding-level defenses like SPARSE, which enable more nuanced and fine-grained privacy preservation without sacrificing utility. Table 9: Comparison of privacy-utility tradeoff between SPARSE and PII transformation methods. Dataset Defense Methods Leakage (%) Downstream (%) Tradeoff Rate STS12 FIQA Unprotected RemovePII Random-Replace Semantic-Replace SPARSE (ϵ = 20) Unprotected RemovePII Random-Replacement Semantic-Replacement SPARSE (ϵ = 20) 60.09 - - - 36. 77.35 - - - 53.41 74.25 59.47 60.50 64.46 73.25 33.56 21.24 19.20 18.37 32.65 - 4.12 4.42 6.22 23.11 - 6.27 5.38 5.09 26.30 F.4 HYPERPARAMETER ANALYSIS We analyze the impact of the regularization parameter λ on the tradeoff between privacy and utility. As shown in Table 10, increasing λ results in reduced leakage across all values of ϵ, confirming that stronger regularization suppresses sensitive information more effectively. However, this comes at the cost of reduced downstream performance, particularly under lower ϵ, where the noise becomes more dominant. Notably, moderate values such as λ = 1e3 strike balance, achieving significant privacy gains with tolerable performance degradation."
        },
        {
            "title": "G COMPUTATIONAL OVERHEAD",
            "content": "We provide an analysis of the computational overhead introduced by SPARSE, focusing on both inference-time noise sampling and offline neuron mask training. Inference Cost. During inference, the dominant overhead arises from sampling Mahalanobis noise, which involves lightweight matrix multiplication. To evaluate efficiency, we measured the average inference latency per sample over 10,000 runs and compared SPARSE with two representative 20 Published as conference paper at ICLR 2026 Table 10: Effect of the regularization hyperparameter λ on privacy leakage and downstream performance under different privacy budgets ϵ. Smaller λ values lead to stronger regularization. Leakage (%) Downstream (%) Dataset λ ϵ = 5 10 30 40 ϵ = 5 10 20 40 STS12 FIQA 1e2 5e3 1e3 5e4 1e4 1e2 5e3 1e3 5e4 1e4 0.52 1.36 4.34 7.62 9. 0.78 2.26 8.48 11.05 13.61 0.91 3.82 19.31 25.83 33.02 1.28 6.42 31.62 36.43 40.82 1.37 7.14 36.98 44.23 51.47 1.93 11.68 53.41 58.23 62.14 1.84 10.34 43.81 51.41 58. 2.71 17.23 63.51 67.28 70.25 2.15 13.76 47.54 55.27 62.90 3.26 21.14 68.13 71.83 74.44 22.14 27.61 34.12 31.33 28.40 8.71 11.38 14.87 13.72 12.45 36.87 48.05 65.27 59.78 52. 13.82 18.67 23.45 21.42 18.63 43.25 56.17 73.25 67.10 59.66 17.44 25.09 32.65 28.93 25.42 44.06 57.88 74.04 67.98 60.34 17.83 25.66 33.58 29.84 26.28 44.38 58.19 74.15 68.24 60. 18.14 25.94 33.85 30.11 26.50 baselines: the Laplace Mechanism and the Purkayastha Mechanism. The results are summarized in Table 11. Table 11: Average inference time per sample (in microseconds). Method Latency (µs/sample) Laplace Mechanism Purkayastha Mechanism SPARSE (ours) 39.8 33,200 48.4 As shown, SPARSE introduces only marginal overhead compared to the Laplace Mechanism (less than 25% increase), while being several orders of magnitude more efficient than the Purkayastha Mechanism. This confirms that SPARSE is suitable for real-time and low-latency applications. Training Cost. The training cost arises from learning the neuron mask used to identify privacysensitive dimensions. This is one-time offline process that can be precomputed and reused, and therefore does not affect inference efficiency. The training time scales linearly with dataset size and remains practical in common settings. For instance, training on 10,000 samples takes 25.3 minutes, and on 20,000 samples, it completes in under 45 minutes. Further acceleration can be achieved with larger batch sizes or distributed training."
        },
        {
            "title": "H IMPLEMENTATION DETAILS OF SPARSE",
            "content": "H.1 TRAINING ALGORITHM FOR NEURON-SENSITIVITY DETECTION Algorithm 2 details the training procedure used to learn neuron mask that identifies privacy-sensitive dimensions in the embedding space. The method jointly optimizes differentiable binary mask and classifier to distinguish between samples containing privacy concept and their perturbed counterparts. hard concrete distribution is used to approximate binary masking in differentiable manner, and the training objective combines classification loss with sparsity-inducing regularization term. H.2 TRAINING SETTINGS We train our privacy-sensitive dimension identification model using mini-batch gradient descent with the Adam optimizer. The model is trained for 100 epochs with batch size of 64 and learning rate of 1 104. The predictor Pθ is implemented as multi-layer perceptron (MLP) with two hidden layers of sizes 256 and 128, respectively, and ReLU activations. We conduct hyperparameter search over λ {0.01, 0.005, 0.001, 0.0005, 0.0001} and set λ = 0.001 as the default for all experiments unless stated otherwise. All implementations are based on PyTorch. 21 Published as conference paper at ICLR Algorithm 2 Training Neuron Mask for Privacy-Sensitive Dimension Detection 1: Input: Paired dataset D+, D, embedding function Φ(), learning rate η, temperature β, regularization coefficient λ, initialization of mask logits log α, constants ξ = 1.1, γ = 0.1 )} (D+, D) do 2: Initialize classifier parameters θ 3: for each epoch = 1 to do for each minibatch {(s+ 4: 5: 6: , 7: for each mask dimension do Sample µi U(0, 1) (cid:16) 1 Compute si = σ βi Compute mi = min (1, max (0, si(ξ γ) + γ)) log µi 1µi + log αi (cid:17)(cid:17) (cid:16) = Φ(s+) m, Φ end for Compute masked embeddings: Φ+ Compute classification loss Lcls(m, θ) using Eq. equation 3 Compute regularization loss Lreg(m) using Eq. equation 4 Compute total loss: Ltotal = Lcls + λLreg Update θ θ ηθLtotal Update log α log α ηlog αLtotal Update log β log β ηlog βLtotal 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for 19: Output: Trained classifier Pθ, optimized neuron mask end for = Φ(s) H.3 COMPUTING RESOURCES All experiments were performed on workstation with an Intel Core i9-10980XE CPU (18 cores, 36 threads, 3.00GHz) and an NVIDIA RTX 3090 GPU with 24GB of memory. The system runs on 64-bit x64 architecture. I"
        },
        {
            "title": "IMPLEMENTATION DETAILS OF ATTACK MODELS",
            "content": "To thoroughly evaluate the privacy risks associated with text embeddings, we adopt three representative attack models: Vec2text (Morris et al., 2023), GEIA (Li et al., 2023), and MLC (Song & Raghunathan, 2020). These models represent both sentence-level and word-level inference attacks, and are implemented or fine-tuned under controlled conditions to assess the effectiveness of various privacy-preserving mechanisms. I.1 VEC2TEXT Vec2text is sentence-level attack model designed to reconstruct input text directly from embeddings. We use the publicly available pre-trained version of Vec2text3, which is based on the GPT-2 architecture. To simulate realistic adversarial scenario, we fine-tune this model for 50 epochs individually on embeddings perturbed by each defense method (LapMech, PurMech, and SPARSE). The fine-tuning is performed using batch size of 32 and learning rate of 5e-5, optimized with Adam. I.2 GEIA GEIA is another sentence-level reconstruction model that inverts embeddings into textual sequences using fine-tuned GPT-2 decoder. Unlike Vec2text, GEIA employs mapping network to project embeddings into the GPT-2 latent space. We use GEIA based on the original paper4, using two-layer MLP as the projection module. The GPT-2 decoder is initialized from the HuggingFace Transformers library and fine-tuned for 30 epochs using embeddings from each defense method. The model is optimized using Adam with learning rate of 3e-5 and trained with batch size of 16. 3https://huggingface.co/ielabgroup/vec2text_gtr-base-st_inversion 4https://github.com/HKUST-KnowComp/GEIA 22 Published as conference paper at ICLR 2026 I.3 MLC MLC is word-level embedding inversion attack model that predicts whether specific sensitive tokens are present in the input text based on its embedding. The model consists of three-layer MLP with hidden sizes [512, 256, 128], ReLU activations, and sigmoid output layer. We train separate MLC for each perturbation method using binary cross-entropy loss function. Training is performed for 20 epochs using batch size of 64 and learning rate of 1e-4 with the Adam optimizer. CASE STUDY ON MIMIC-III DATASET To demonstrate the privacy risks in specific threat domain, we conducted case study using MIMICIII clinical notes (Johnson et al., 2018). Table 12 presents the results of embedding inversion attack on two types of sensitive tokens (\"age\" and \"disease name\") with different noise levels. We assessed the semantic fidelity of the reconstructed sentences by comparing their similarity to the original text using cosine similarity from an external embedding model. In Example 1, we applied strong perturbation level of ϵ = 5 to perturb the text embeddings. Under this condition, all three defense methods (LapMech, PurMech, and SPARSE) effectively prevented the leakage of sensitive age information. However, LapMech and PurMech significantly degraded the semantic quality of the embeddings with only 11% of the original semantic similarity. In contrast, SPARSE maintained 62% semantic similarity. In Example 2, we used lower perturbation level of ϵ = 10. Here, both LapMech and PurMech failed to protect against privacy leakage and further compromised the semantic integrity of the embeddings. Conversely, SPARSE successfully safeguarded the sensitive information while preserving semantic quality of the embeddings. Table 12: Case study on the MIMIC-III dataset with two sensitive words and perturbation level ϵ. We highlight the leakage of sensitive words and demonstrate the semantic similarity of the reconstructed sentence to the ground truth. Example 1: Protect age with strong noise ϵ = 5 Method Defense Semantic Reconstructed Sentence Ground truth Non-private LapMech PurMech SPARSE - Failed Success Success Success - 0.98 0.11 0.11 0.62 this 68-year-old white male has history of diabetes, hyperlipidemia and hypertension this 68-year-old white male has history of hypertension, hyperlipidemia, and diabetes. age (e.g., blood edemas in males of African PH whose history has been hyperesoteric age (e.g., blood edemas in males of African PH whose history has been hyperesoteric white male with diabetes has existing Hyperlipidemia history Example 2: Protect disease name with weak noise ϵ = Ground truth Non-private LapMech PurMech SPARSE - Failed Failed Failed Success - 0.95 0.23 0.18 0.54 this male has had known coronary disease and prior silent myocardial infarction. this male has known silent coronary disease and has had prior myocardial infarction. male has known coronary myopathy. Silent rib syndrome, white-fiddled gyne, and ca male has known coronary myopathy. Silent-fidged heart attacks. White-fidged-fid an active male with myocardial infarction, congestive heart disease."
        },
        {
            "title": "K LIMITATIONS",
            "content": "Limited Scope of Attack Scenario. Our method is explicitly tailored to mitigate embedding inversion attacks, in which an adversary seeks to reconstruct input data from text embeddings. However, it does not offer guarantees against other widely studied privacy attacks such as membership inference attacks. Although our approach is compatible with differential privacy mechanisms in principle, we leave the integration of comprehensive privacy protections to future work. Protecting Broader Privacy Concept. Our framework estimates privacy-sensitive dimensions based on predefined concepts, which works well for targeted protection but might not scale well with broader or abstract notions of privacy. As the definition of privacy becomes overly broad (e.g., \"any identifiable content\"), our method loses its specificity and utility. potential solution is to move toward concept-agnostic sensitivity estimation regardless of predefined labels. 23 Published as conference paper at ICLR 2026 USE OF LARGE LANGUAGE MODELS (LLMS) In this work, large language models (LLMs) were used in two ways. First, we employed pre-trained open-source LLMs as embedding generators to produce text representations, and also served as the foundation for conducting inversion attacks in our experiments. Second, an LLM-based assistant (OpenAI GPT-4) was used to improve the clarity and readability of the manuscript through grammar checking and minor language refinements. All decisions regarding research design, experimental setup, analysis, and interpretation were made solely by the authors."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Information Engineering, National Taiwan University",
        "National Taiwan University AI Center of Research Excellence"
    ]
}