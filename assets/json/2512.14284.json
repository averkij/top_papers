{
    "paper_title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
    "authors": [
        "Zhibing Li",
        "Mengchen Zhang",
        "Tong Wu",
        "Jing Tan",
        "Jiaqi Wang",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion"
        },
        {
            "title": "Start",
            "content": "SS4D: Native 4D Generative Model via Structured Spacetime Latents ZHIBING LI, The Chinese University of Hong Kong, China and Shanghai AI Laboratory, China MENGCHEN ZHANG, Shanghai AI Laboratory, China and Zhejiang University, China TONG WU, Stanford University, USA JING TAN, The Chinese University of Hong Kong, China JIAQI WANG, Shanghai AI Laboratory, China DAHUA LIN, The Chinese University of Hong Kong, China 5 2 0 2 6 1 ] . [ 1 4 8 2 4 1 . 2 1 5 2 : r Fig. 1. SS4D generates high-quality 4D content in 2 minutes. The monocular input videos, corresponding voxelized structure, and the final 4D content from an alternative viewpoint are presented. For additional dynamic results, please refer to our supplementary demo video. https://lizb6626.github.io/SS4D/ We present SS4D, native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that Equal contribution. Corresponding Authors. Authors Contact Information: Zhibing Li, The Chinese University of Hong Kong, China and Shanghai AI Laboratory, China, lz022@ie.cuhk.edu.hk; Mengchen Zhang, Shanghai AI Laboratory, China and Zhejiang University, China, zhangmengchen@zju.edu.cn; Tong Wu, Stanford University, USA, wutong16@stanford.edu; Jing Tan, The Chinese University of Hong Kong, China, tj023@ie.cuhk.edu.hk; Jiaqi Wang, wjqdev@gmail. com, Shanghai AI Laboratory, China; Dahua Lin, dhlin@ie.cuhk.edu.hk, The Chinese University of Hong Kong, China. construct 4D representations by optimizing over 3D or video generative models, we train generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ carefully designed training strategy to enhance robustness against occlusion This work is licensed under Creative Commons Attribution 4.0 International License. 2025 Copyright held by the owner/author(s). ACM 1557-7368/2025/12-ART243 https://doi.org/10.1145/3763302 ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. 243:2 Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, and Dahua Lin and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets. CCS Concepts: Computing methodologies Artificial intelligence. Additional Key Words and Phrases: 4D Generation, 3D Generation, Animation, Generative Model ACM Reference Format: Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, and Dahua Lin. 2025. SS4D: Native 4D Generative Model via Structured Spacetime Latents. ACM Trans. Graph. 44, 6, Article 243 (December 2025), 12 pages. https://doi.org/10.1145/"
        },
        {
            "title": "Introduction",
            "content": "Bridging the digital and physical worlds stands as one of the most profound frontiers in computer graphics and vision. While static 3D object creation has seen remarkable progress [Team 2025; Xiang et al. 2024; Zhang et al. 2024c], the synthesis of dynamic 3D content, also known as 4D, has lagged behind. 4D generation seeks to model not only how objects look but also how they move through time, holding the potential to redefine the fields of filmmaking, gaming, and virtual reality. In this work, we propose method for efficient and highquality 4D object generation from monocular videos, enabled by native 4D latent space that captures both spatial structure and temporal dynamics. Previous approaches to 4D content creation generally fall into two categories. The first line of works [Bahmani et al. 2024; Jiang et al. 2024; Ling et al. 2024; Zeng et al. 2024; Zhang et al. 2024b] leverages pre-trained multi-view and video generative models to optimize 4D representationssuch as 4D Gaussians and Dy-NeRFthrough intricate Score Distillation Sampling (SDS) [Poole et al. 2022] and its variants. While conceptually appealing, SDS-based methods often suffer from over-saturation artifacts and require hours of per-instance optimization. Another line of works[Sun et al. 2024; Yang et al. 2024a] refines video models for multi-view video generation and reconstructs 4D objects directly via photogrammetry losses. Though faster, these methods tend to yield coarse or noisy geometry and struggle to maintain spatio-temporal consistency, particularly under highly dynamic motion. Recent advances in native 3D generative models [Li et al. 2025; Team 2025; Xiang et al. 2024; Zhang et al. 2024c; Zhao et al. 2023] trained on large-scale 3D datasets [Deitke et al. 2023a,b] have demonstrated notable improvements in both quality and efficiency over SDS-based approaches [Chen et al. 2023; Liang et al. 2024; Lin et al. 2023; Poole et al. 2022; Qiu et al. 2024] and Large Reconstruction Models (LRMs) [Hong et al. 2023; Li et al. 2023; Zhang et al. 2024a]. Inspired by this evolutionary trajectory in 3D generation, we extend it into the 4D domain. Our method builds upon TRELLIS [Xiang et al. 2024], which encodes 3D objects as sparse voxel grids enriched with attribute channels, offering strong spatial inductive biases through the spatial locality inherent in sparse voxel structures. We propose SS4D, native 4D generative model capable of synthesizing dynamic 3D objects directly from monocular video input. Our approach extends TRELLIS into the spacetime domain by incorporating time as the fourth dimension in the latent space, while ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. retaining the strong spatial consistency provided by its structured latents representation. Specifically, we fine-tune TRELLISs autoencoder and generator with Temporal Layers and carefully designed 4D positional encodings to enforce consistency across frames. To improve long-term sequence generation, we apply temporal downsampling strategy via dedicated downsampling blocks and factorized 4D convolutions. Furthermore, we introduce progressive training schedule and random masking strategy to enhance robustness against occlusions and motion blur, both of which are common challenges in real-world video data. We conduct extensive experiments on synthetic benchmarks, including ObjaverseDy [Xie et al. 2024] and Consistent4D [Jiang et al. 2024], as well as real-world video sequences from DAVIS [Caelles et al. 2019]. Quantitative and qualitative results show that SS4D achieves state-of-the-art performance, generating high-quality, spatiotemporally consistent 4D objects with remarkable computational and memory efficiency. The generated 4D content and its voxelized structure by SS4D are visualized in Figure 1. We believe our model can serve as foundation for future work in 4D generation and unlock new possibilities in dynamic content creation."
        },
        {
            "title": "2 Related Work",
            "content": "SDS-based 4D Generation. prominent line of research in 4D generation [Bahmani et al. 2024; Jiang et al. 2024; Ling et al. 2024; Ren et al. 2023; Yu et al. 2024; Zeng et al. 2024; Zheng et al. 2024] leverages Score Distillation Sampling (SDS) with off-the-shelf pretrained models. These methods distill different types of priors: appearance priors: appearance priors [Ling et al. 2024] from text-to-image diffusion models, 3D priors [Ren et al. 2023; Zeng et al. 2024] from 3D-aware and multi-view diffusion models, and the motion priors [Ling et al. 2024] from video diffusion models. Consistent4D [Jiang et al. 2024] optimizes dynamic NeRF conditioned via SDS loss derived from 3D-aware image diffusion. 4D-fy [Bahmani et al. 2024] sequentially combines gradients from three distinct pretrained diffusion models to harness the strengths of each modality, thereby enhancing the quality of 4D generation. While SDS-based approaches effectively utilize diffusion priors to generate dynamic motion, they remain computationally expensive, often requiring several hours of optimization per object, and are prone to over-saturation artifacts. Feed-forward 4D Generation. Another line of work in 4D generation [Sun et al. 2024; Wu et al. 2024; Xie et al. 2024; Yang et al. 2024a; Yao et al. 2025] adopts feed-forward pipelines that avoid iterative optimization with SDS gradients. Diffusion2 [Yang et al. 2024a] and EG4D [Sun et al. 2024] leverage off-the-shelf video diffusion and multi-view diffusion to generate images at different viewpoints and timesteps using training-free techniques, followed by 4D reconstruction from the synthesized images. Other approaches [Wu et al. 2024; Xie et al. 2024; Yao et al. 2025] train multi-view multi-time diffusion with 4D datasets [Xie et al. 2024] or through combination of multi-view and video datasets. L4GM [Ren et al. 2024] extends LGM [Tang et al. 2024] to generate 3D Gaussians at each frame. However, most of these feed-forward methods rely on minimizing volumetric rendering losses on RGB images, which often leads to coarse or noisy geometry in the resulting 4D representations. SS4D: Native 4D Generative Model via Structured Spacetime Latents 243:3 Fig. 2. SS4D Overview. Left: SS4D pipeline. Our method takes monocular video as input. It extracts coarse voxel-based structure using 4D Flow Transformer, then generates spacetime latents through 4D Sparse Flow Transformer, both incorporating Temporal Layer to capture temporal consistency. These latents are subsequently decoded into sequence of 3D Gaussians, forming the final 4D content. Right: Temporal Layer. It combines temporal self-attention with shifted windows and hybrid 1D Rotary Position Embeddings to efficiently model dynamic 4D content and ensure consistency across frames. More recent methods aim to improve temporal consistency and geometric fidelity. V2M4 [Chen et al. 2025] first generates 3D meshes for individual frames, then enforces consistency through mesh registration and texture optimization. AnimateAnyMesh [Wu et al. 2025] and Puppeteer [Song et al. 2025] animate static 3D meshes using text or video prompts. The most relevant work, GVFDiffusion [Zhang et al. 2025] encodes canonical Gaussians and their temporal displacements into latent space using 4DMesh-to-GS Variation Field VAE. Gaussian Variation Field diffusion model is then trained on this latent space to generate 4D content. However, the canonical Gaussians are conditioned only on the first frame, the model may fail to match the appearance in subsequent frames, resulting in suboptimal performance. Our method directly synthesizes 4D objects using structured spacetime latent representation. By explicitly generating surfaces with geometric supervision, our framework produces more accurate 4D geometry that remains consistent across large viewpoint changes and aligns with the underlying motion."
        },
        {
            "title": "3 Background: Structured 3D Latents",
            "content": "Our approach is built upon TRELLIS [Xiang et al. 2024], model for generating 3D assets from single image. TRELLIS introduces structured latents that effectively capture both the geometry and appearance of 3D objects. TRELLIS first transforms 3D asset into voxelized feature representation, defined as: TRELLIS encodes the voxelized feature ùëì into structured latents ùëß = (ùëì ) = {(ùëßùëñ, ùëùùëñ )}ùêø , ùëßùëñ Rùê∑ , using 3D Variational Autoùëñ=1 Encoder (VAE) [Kingma and Welling 2014]. These latents can then be decoded into various 3D representations, including NeRF [Mildenhall et al. 2020], 3D Gaussians [Kerbl et al. 2023], and meshes. The generation of structured latents is decomposed into two stages: (1) generating the sparse structure {ùëùùëñ }ùêø ùëñ=1, followed by (2) generating the latent features {ùëßùëñ }ùêø ùëñ=1. Both stages employ generative models (GùëÜ for structure and Gùêø for latents) that leverage Transformer-based Diffusion model (DiT) [Peebles and Xie 2023] architecture and the Conditional Flow Matching (CFM) objective [Lipman et al. 2023]."
        },
        {
            "title": "4 Method\nGiven a monocular video of a dynamic object I = {ùêºùë° }ùëá\nùë° =1, where ùëá is\nnumber of frames and each ùêºùë° ‚àà R3√óùêª √óùëä represents an RGB image,\nour goal is to produce a high-quality 4D representation of the object.\nTo achieve this, we extend structured 3D latents into structured\nspacetime latents ùëç = {ùëßùë° }ùëá\nùë° =1. Through this extension, our model\ninherits spatial consistency from the pre-trained 3D backbones,\nalleviating the challenge posed by the scarcity of 4D data.",
            "content": "An overview of our model is illustrated in Figure 2. We begin by generating coarse voxel-based structure ùëÉ = {ùëùùë° }ùëá ùë° =1 from the input video. We then generate sparse structured spacetime latents ùëç , which are subsequently decoded into set of 3D Gaussians = {ùêæùë° }ùëá ùë° =1, representing the object at each time step. ùëì = {(ùëìùëñ, ùëùùëñ )}ùêø ùëñ=1 , ùëìùëñ Rùê∂, ùëùùëñ {0, 1, ..., ùëÅ 1}3, (1)"
        },
        {
            "title": "4.1 Temporal Alignment",
            "content": "where ùëùùëñ denotes the coordinate of an activated voxel in 3D grid, approximating the coarse surface of the 3D asset. The feature ùëìùëñ associated with each voxel is computed by aggregating DINOv2 [Oquab et al. 2023] features extracted from multi-view renderings. ùëÅ denotes the spatial resolution of the voxel grid, and ùêø is the total number of activated voxels. key insight in training native 4D generator is to leverage pretrained 3D generation model. However, extending 3D latent space to structured spacetime representation is non-trivial. While such models can produce high-quality 3D assets for individual frames, directly applying them to dynamic videos leads to noticeable temporal inconsistencies, as they lack temporal awareness. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. 243:4 Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, and Dahua Lin To address this, we extend the spatial self-attention layers of the pre-trained model into temporal self-attention layers. Specifically, we rearrange the temporal axis into the length dimension to perform attention, followed by reshaping to restore the original structure. The process, using einops [Rogozhnikov 2022] notation, is as follows: ùëß = rearrange(ùëß, (B T) (T M) C), ùëß = TemporalAttn(ùëß), ùëß = rearrange(ùëß, (T M) (B T) C), (2) where represents the batch size, is the number of temporal frames, is the attention length, and is the feature dimension. Given the local nature of temporal sequences and the quadratic complexity of full attention with increasing frame counts, we adopt shifted window attention [Liu et al. 2021] in temporal layers to improve efficiency. The temporal layers alternate between non-shifted and shifted window configurations, enabling both local context aggregation and global information exchange. An essential component of temporal attention is positional embedding. TRELLIS uses absolute position embedding [Vaswani et al. 2017] to encode 3D spatial coordinates. We retain this design to preserve spatial reasoning capabilities and further incorporate 1D Rotary Positional Embedding (RoPE) [Su et al. 2024] along the temporal axis to capture dynamic relationships between adjacent latents. This hybrid 4D positional encoding allows our model to generalize across varying sequence durations, despite being trained on fixed number of frames. The complete design of the temporal layer is shown on the right side of Figure 2. We find that temporal alignment of the diffusion model alone is insufficient. Since the VAE is trained exclusively on static 3D objects, it introduces flickering artifacts when encoding and decoding temporally coherent 4D features. To address this, we apply the same temporal alignment strategy to the VAE, enabling it to produce more coherent 4D reconstructions. As demonstrated in Section 5.3, this alignment significantly reduces jittering in the reconstructed Gaussians and is critical for achieving high-quality results."
        },
        {
            "title": "4.2 Long-Term Generation",
            "content": "Another major challenge in 4D generation is producing continuous 4D sequences from long videos. Previous works [Jiang et al. 2024; Ren et al. 2023; Zeng et al. 2024] often encounter issues such as quality degradation and inconsistency when generating long sequences, with optimization times increasing drastically. Given limited computational resources, generating spatiotemporally consistent 4D sequences remains significant challenge. To tackle this issue, SS4D incorporates the 1D Rotary Position Embedding (RoPE) [Su et al. 2024], which enhances the models ability to extrapolate and generate 4D objects that extend beyond the training dataset. More importantly, considering the significant overlap of information between frames, we improve efficiency by compressing 4D representations of long sequences. In the 4D Sparse Flow Transformer stage, we generate structured spacetime latents {ùëßùëñ }ùêø ùëñ=1. As shown in Figure 3, we explore 4D compression and convolutional network CompNet, which is specifically designed to compress and ùëñ=1 based on coarse structure {ùëùùëñ }ùêø ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. Fig. 3. Implementation of our 4D Compression Strategy. The strategy improve efficiency by compressing 4D representations for long-term sequence generation in our 4D Sparse Flow Transformer. exchange information across frames. The network captures longrange dependencies while preserving fine-grained spatial details and temporal consistency, ultimately enhancing the quality of the generated 4D sequences. Specifically, given the input structured spacetime latents ùëß Rùêøùê∑ , we first apply spatial 3D convolution and compression to each individual frame. sparse 3D convolution block [Contributors 2022; Wang et al. 2017] with space downsampling is employed to aggregate the latents within local region of size 23. Then, we apply sparse 1D convolutions [Contributors 2022] to facilitate information exchange across frames. Following this, temporal downsampling block is introduced to compress the latent features by packing two active voxels from the same ùë•ùë¶ùëß position across frames. This module effectively compresses the frame length and reduces the computational load in subsequent Transformer modules. After the above compression, the timesteps are integrated through AdaLN layers, and video conditions are injected via multiple Transformer blocks, each containing Temporal Layer. The subsequently compressed latent ùëß is decompressed in the reverse order of CompNet. Meanwhile, during the decompression process, the convolutional upsampling block is connected to the downsampling block from the compression process through skip connections, facilitating the flow of spatial information. Finally, we restore the latent ùëß to its original structure."
        },
        {
            "title": "4.3 Training SS4D",
            "content": "Data Curation. We curate dataset of 16,000 animated 3D objects from Objaverse [Deitke et al. 2023b] and ObjaverseXL [Deitke et al. 2023a], filtering out samples with low visual quality or minimal motion. During feature aggregation to activate voxels, we consider only the rendered views in which voxel is visible, rather than averaging across all views. Voxels that remain invisible in all views are discarded. This curation process not only reduces the length of the spacetime latents, enabling more efficient training, but also minimizes feature noise, leading to improved reconstruction quality. Progressive Learning. Animated objects from the Internet have different frame lengths. And directly training on long sequence causes slow to convergence. To fully utilize data and save training cost, we employ progressive learning strategy. That is, we first train on short animations to learn dynamic relationship among spacetime latents. Finally, we select subset of high-quality and long duration 4D data to finetune our model to cope with long sequences. Random Masking Augmentation. Real-world objects often appear in complex environments, where they may be partially occluded by surrounding elements or exhibit rapid motion that results in motion blursuch as during dancing or fast movement. To improve robustness to these real-world challenges, we introduce simple yet effective data augmentation strategy: randomly applying black masks to the conditioning video frames. This augmentation simulates common artifacts like occlusion and motion blur. Experimental results demonstrate that this approach mitigates the impact of such challenges on generation quality."
        },
        {
            "title": "5 Experiments",
            "content": "Implementation Details. We rendered our training data using Blender1 with the Cycles engine. For each animation, we use the first 36 frames if the sequence exceeds that length. Following TRELLIS, our pipeline consists of four models: the 4D Structure VAE, 4D Flow Transformer, 4D Sparse VAE, and 4D Sparse Flow Transformer. Among these, the 4D Structure VAE is frozen during training, as it achieves near-perfect reconstruction of the 4D structure sequence. The remaining three models are fine-tuned. Training is performed on 8A800 GPUs using the AdamW optimizer with learning rate of 1e4 and FP16 mixed precision. The batch size is set to 2 per GPU for the generator and 1 per GPU for the VAE. Training takes approximately 7 to 8 days in total. For the progressive learning strategy, we start with 8-frame sequences and gradually increase the sequence length to 16 and then 32 frames as training progresses. Datasets. We evaluate our method on two synthetic datasets: ObjaverseDy [Xie et al. 2024] and Consistent4D [Jiang et al. 2024]. For each object in these datasets, we uniformly sample 32 camera viewpoints and render up to 32 frames per object at resolution of 512 512. Additionally, we render front-view video per object to serve as input across all methods. To assess robustness and generalization to real-world scenarios, we also incorporate video sequences from DAVIS [Caelles et al. 2019] for test. Evaluation Metrics. To evaluate image quality, we compute Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) [Wang et al. 2004] between generated novel views and corresponding ground truth. We also report LPIPS [Zhang et al. 2018] and CLIP Similarity (CLIP-S) to assess perceptual and semantic similarity. Since our method focuses on object-centric generation, we crop all images using the ground truth bounding box to mitigate the influence of background ratio on metrics. For evaluating spacetime consistency of generated 4D data, we compute Frechet Video Distance (FVD) [Unterthiner et al. 2018] on novel view videos. 1https://www.blender.org/ SS4D: Native 4D Generative Model via Structured Spacetime Latents 243:5 Table 1. Quantitative Comparisons on the synthetic dataset ObjaverseDy. Bold denotes the best result, and underline the second-best. LPIPS CLIP-S PSNR SSIM FVD DG4D Consistent4D STAG4D L4GM 0.231 0.189 0.202 0.243 0.869 0.884 0.871 0.887 13.72 15.63 15.96 14.48 0.784 0.817 0.821 0.782 693 640 716 Ours 0.150 0.932 18.09 0.842 Table 2. Quantitative Comparisons on the synthetic dataset Consistent4D. Bold denotes the best result, and underline the second-best. LPIPS CLIP-S PSNR SSIM FVD DG4D Consistent4D STAG4D L4GM 0.215 0.228 0.204 0.219 0.896 0.886 0.890 0.908 15.47 13.66 16.89 15.69 0.785 0.768 0.821 0.803 641 674 740 596 Ours 0.149 0.947 18.90 0.843 455 Table 3. User Study Results on real-world videos from DAVIS, evaluating Geometry Quality, Texture Quality, and Motion Coherence of 4D generations. Bold denotes the best result, and underline the second-best. Geometry Quality Texture Quality Motion Coherence DG4D Consistent4D STAG4D L4GM Ours 2.870 2.703 1.913 3.017 4.497 3.077 2.837 1.627 3. 4.413 2.967 2.650 1.807 3.050 4.527 Baselines. We compare our results against several state-of-theart baselines, including three SDS-based methods and one feedforward method: DreamGaussian4D (DG4D) [Ren et al. 2023], Consistent4D [Jiang et al. 2024], STAG4D [Zeng et al. 2024] and L4GM [Ren et al. 2024]. DG4D and STAG4D both train deformable 3D Gaussians using SDS losses from multi-view diffusion models. Consistent4D employs cascaded DyNeRF architecture optimized with SDS losses from an image-to-image diffusion model. L4GM trains large reconstruction model that directly generates 3D Gaussians for each frame in the conditioning video."
        },
        {
            "title": "5.1 Quantitative Comparison",
            "content": "We present quantitative comparisons on the ObjaverseDy and Consistent4D datasets in Table 1 and Table 2, respectively. Our method consistently outperforms all baselines across all evaluation metrics by large margin. The results demonstrate the superior visual fidelity and quality of our method. In particular, our method achieves substantially lower FVD scores, highlighting improved spatio-temporal consistency in the generated 4D assets. Additionally, we present an inference time comparison in Table 4. Our method achieves significantly faster inference compared to optimizationbased approaches while being slower than L4GM. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. 243:6 Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, and Dahua Lin Fig. 4. Qualitative Comparisons on Synthetic Data. We show two frames from the conditioning video alongside two corresponding novel view renderings. Compared to the baselines, our method generates more accurate geometry and detailed textures that remain consistent over time. Table 4. Inference Time Comparison. Method DG4D Consistent4D STAG4D L4GM Ours Time 15 min 1.5 hr 1 hr 3.5s 2 min"
        },
        {
            "title": "5.2 Qualitative Comparison",
            "content": "We present visual comparisons on synthetic data in Figure 4 and real-world data in Figure 5. L4GM performs well on front views but exhibits noticeable distorted geometry when viewed from large elevation angles. Consistent4D tends to produce over-saturated textures, common artifact associated with SDS losses, while STAG4D often generates noisy and inconsistent textures. In contrast, our method reconstructs accurate and clean geometry, even under complex and high-speed motions (e.g., breakdancing), ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. while maintaining detailed and temporally consistent textures. Furthermore, SDS-based methods typically require several hours to process single dynamic object, whereas our method completes the same task in approximately 3 minutes. To further evaluate our performance on real-world data, we conduct user study comparing our method with the baselines. Following [Yao et al. 2025], we select 14 videos from the DAVIS dataset with relatively stable camera motion. We ask 25 users with background in 3D and video generation to evaluate the 4D generation quality in three aspects: geometry quality, texture quality, and motion coherence. Geometry quality refers to the plausibility of the 3D geometry, whether the generated 3D geometry aligns with the motion. Texture Quality refers to the alignment of texture with the condition video. Motion Coherence refers to the temporal consistency and stability SS4D: Native 4D Generative Model via Structured Spacetime Latents 243:7 Fig. 5. Qualitative Comparisons on real-world videos from DAVIS. For each method, we show two frames from the conditioning video under two novel viewpoints in 2 2 grid. Compared to the baseline methods, our model remains robust to real-world cases with more complex and high-speed motion. of the generated motion. We employed the Average User Ranking (AUR) metric to evaluate model performance. The users were asked to rate the 4D generations on scale of 1 to 5, and we report the average user rating in Table 3. Our model outperformed the baselines in all three aspects with large margin. STAG4D generally receives the weakest rating for its blurry generations and over-saturated texture. The rest of the methods receive similar scores, with L4GM slightly better in shape quality and motion coherence, and DG4D better in texture quality."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "Temporal Alignment. We ablate the effectiveness of temporal alignment in the VAE reconstruction task. As shown in Figure 6, Fig. 6. Temporal Alignment in VAE Reconstruction. Temporal alignment reduces flickering and improves consistency across frames. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. 243:8 Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, and Dahua Lin Table 5. Ablation of Temporal Alignment in VAE reconstruction. PSNR Flickering FVD w/o Temporal Align w. Temporal Align 27.11 30.58 2.99 2. 403.88 157.16 Fig. 7. Masking Augmentation enhances the models robustness to occlusions and improves overall geometry quality. without temporal alignment, the reconstructed sequence exhibits noticeable temporal flickering (e.g., the pattern on the dress), whereas incorporating temporal alignment leads to more coherent and stable textures across frames. Following the approach in [Yang et al. 2024b], we evaluate two variants using PSNR, FVD and flickering metric that quantifies video flickers by calculating L1 difference between adjacent frames. As reported in Table 5, temporal alignment significantly reduces flickering and improves the quality of reconstructed 4D assets. Masking Augmentation. As illustrated in Figure 7, the front leg of the rhino is occluded by rocks and the model without masking augmentation fails to reconstruct the occluded region, resulting in an incomplete front leg. In contrast, applying masking augmentation enables the model to infer and generate the complete shape. Table 6. Ablation of Feature Aggregation Methods. Aggregation Method PSNR Average Length Encode Speed Mean Visible 31.63 31.07 6605 5261 68.4 76. Visible Feature Aggregation. Averaging all features (as done in TRELLIS) may introduce noise, since occluded voxels can be affected by unrelated views. rigorous ablation study would require retraining from scratch, which exceeds our computational resources. Instead, we fine-tune 4D Sparse VAE for 5k steps using visibility-aware aggregation and evaluate reconstruction quality on the ObjaverseDy dataset. As shown in Table 6, our method achieves comparable reconstruction quality while reducing sequence length and increasing encoding speed (objects/sec), resulting in more efficient training and inference."
        },
        {
            "title": "6 Conclusions",
            "content": "We present SS4D, native 4D generation framework capable of producing high-quality 4D assets from single monocular video. At its core lies structured spacetime latent space, seamlessly extended from pre-trained 3D prior. By temporally aligning the 3D generator and autoencoder, our method ensures spatio-temporal consistency. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. Fig. 8. Failure Cases. Our method exhibits limitations in handling transparent layers, high-frequency details, and rapid motion. To address redundancy across time, we leverage factorized 4D convolutions for effective temporal compression. tailored training strategy further enhances robustness under real-world conditions. Extensive experiments show that SS4D significantly outperforms existing baselines. We hope this work contributes to the ongoing progress in 4D generation by introducing native model learned directly from spacetime data. Limitation and Future Work. While our method achieves strong performance in 4D generation, it still exhibits several limitations. First, it adopts two-stage pipeline inherited from TRELLIS, which results in less efficient training compared to fully end-to-end approaches. Second, as the model is trained primarily on synthetic data, it struggles to generate photo-realistic appearances when applied to real-world inputs, often producing over-simplified textures. Incorporating real-world video data during training could help bridge this gap and improve generalization. In addition to these structural limitations, our model also exhibits performance degradation in several practical scenarios, as illustrated in Figure 8: (a) Transparent layers: The model retains only the outermost voxels and discards internal ones, making it incapable of accurately reconstructing or generating transparent or multi-layered objects. For example, in the crystal ball case, the model produces overly blurred textures due to its inability to capture complex light refraction through multiple transparent surfaces. (b) High-Frequency Details: Despite employing temporal alignment, maintaining consistency in high-frequency details remains challenging. For instance, our model fails to preserve the fine patterns in camouflage pants, resulting in noticeable flickering. Introducing pixel-space losses may help retain finer details more effectively. (c) Rapid Movement: Scenarios involving fast motion and strong motion blur are particularly difficult. In the dog example, the model fails to reconstruct the correct shape under rapid movement. Ethical Issues. As with other generative models, our approach may inherit ethical and diversity-related biases from Objaverse and ObjaverseXL. Additionally, there are potential risks of misuse, particularly in generating deceptive or misleading content. Acknowledgement. This work was supported by National Key R&D Program of China 2022ZD0161600, Shanghai Artificial Intelligence Laboratory, Hong Kong RGC TRS T41-603/20-R, the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)s InnoHK. Dahua Lin is PI of CPII under the InnoHK."
        },
        {
            "title": "References",
            "content": "Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B. Lindell. 2024. 4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2024). Sergi Caelles, Jordi Pont-Tuset, Federico Perazzi, Alberto Montes, Kevis-Kokitsi Maninis, and Luc Van Gool. 2019. The 2019 DAVIS Challenge on VOS: Unsupervised MultiObject Segmentation. arXiv:1905.00737 (2019). Jianqi Chen, Biao Zhang, Xiangjun Tang, and Peter Wonka. 2025. V2M4: 4D Mesh Animation Reconstruction from Single Monocular Video. arXiv preprint arXiv:2503.09631 (2025). Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF international conference on computer vision. 2224622256. Spconv Contributors. 2022. Spconv: Spatially Sparse Convolution Library. https: //github.com/traveller59/spconv. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. 2023a. Objaverse-XL: Universe of 10M+ 3D Objects. arXiv preprint arXiv:2307.05663 (2023). Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2023b. Objaverse: Universe of Annotated 3D Objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1314213153. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. 2023. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023). Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, and Yao Yao. 2024. Consistent4D: Consistent 360 Dynamic Object Generation from Monocular Video. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum? id=sPUrdFGepF Bernhard Kerbl, Georgios Kopanas, Thomas Leimk√ºhler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics 42, 4 (July 2023). https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR. SS4D: Native 4D Generative Model via Structured Spacetime Latents 243:9 Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. 2023. Instant3d: Fast text-to3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214 (2023). Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. 2025. TripoSG: HighFidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models. arXiv preprint arXiv:2502.06608 (2025). Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. 2024. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 65176526. Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3d: Highresolution text-to-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 300309. Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. 2024. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 85768588. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. 2023. Flow Matching for Generative Modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision. 1001210022. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV. Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023). William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 41954205. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2022. DreamFusion: Text-to-3D using 2D Diffusion. arXiv (2022). Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. 2024. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 99149925. Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. 2023. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 (2023). Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, and Huan Ling. 2024. L4GM: Large 4D Gaussian Reconstruction Model. In Proceedings of Neural Information Processing Systems(NeurIPS). Alex Rogozhnikov. 2022. Einops: Clear and Reliable Tensor Manipulations with Einsteinlike Notation. In International Conference on Learning Representations. https:// openreview.net/forum?id=oapKSVM2bcj Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, and Jianfeng Zhang. 2025. Puppeteer: Rig and Animate Your 3D Models. arXiv preprint arXiv:2508.10898 (2025). Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. Qi Sun, Zhiyang Guo, Ziyu Wan, Jing Nathan Yan, Shengming Yin, Wengang Zhou, Jing Liao, and Houqiang Li. 2024. Eg4d: Explicit generation of 4d object without score distillation. arXiv preprint arXiv:2405.18132 (2024). Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. 2024. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision. Springer, 118. Tencent Hunyuan3D Team. 2025. Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation. arXiv:2501.12202 [cs.CV] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717 (2018). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. 2017. O-CNN: octree-based convolutional neural networks for 3D shape analysis. ACM Trans. Graph. 36, 4 (2017), 72:172:11. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. 243:10 Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, and Dahua Lin Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600612. Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. 2024. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613 (2024). Zijie Wu, Chaohui Yu, Fan Wang, and Xiang. Bai. 2025. AnimateAnyMesh: FeedForward 4D Foundation Model for Text-Driven Universal Mesh Animation. arXiv preprint arxiv:2506.09982 (2025). Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. 2024. Structured 3D Latents for Scalable and Versatile 3D Generation. arXiv preprint arXiv:2412.01506 (2024). Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. 2024. SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency. arXiv preprint arXiv:2407.17470 (2024). Zeyu Yang, Zijie Pan, Chun Gu, and Li Zhang. 2024a. Diffusion2: Dynamic 3D Content Generation via Score Composition of Video and Multi-view Diffusion Models. arXiv preprint arXiv:2404.02148 (2024). Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024b. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. arXiv preprint arXiv:2408.06072 (2024). Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, and Varun Jampani. 2025. SV4D2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation. arXiv preprint arXiv:2503.16396 (2025). Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, L√°szl√≥ Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 2024. 4real: Towards photorealistic 4d scene generation via video diffusion models. Advances in Neural Information Processing Systems 37 (2024), 4525645280. Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. 2024. STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians. (2024). arXiv:2403.14939 [cs.CV] Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, and Baining Guo. 2025. Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis. arXiv preprint arXiv:2507.23785 (2025). Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 2024b. 4Diffusion: Multi-view Video Diffusion Model for 4D Generation. arXiv preprint arXiv:2405.20674 (2024). Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. 2024a. GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting. European Conference on Computer Vision (2024). Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. 2024c. CLAY: Controllable Large-scale Generative Model for Creating High-quality 3D Assets. ACM Transactions on Graphics (TOG) 43, 4 (2024), 120. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586595. Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. 2023. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems 36 (2023), 7396973982. Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. 2024. unified approach for text-and image-guided 4d scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 73007309. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. SS4D: Native 4D Generative Model via Structured Spacetime Latents 243:11 Fig. 9. Qualitative Results of SS4D on our Test Set. Three frames from the input video are shown alongside three corresponding novel view renderings. Our method produces accurate geometry and detailed textures, maintaining consistency over time under significant motion. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025. 243:12 Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, and Dahua Lin Fig. 10. Qualitative Results of SS4D on synthetic data from the Internet. Fig. 11. Qualitative Results of SS4D on real-world data. ACM Trans. Graph., Vol. 44, No. 6, Article 243. Publication date: December 2025."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory, China",
        "Stanford University, USA",
        "The Chinese University of Hong Kong, China",
        "Zhejiang University, China"
    ]
}