{
    "paper_title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
    "authors": [
        "Tobias Sautter",
        "Jan-Niklas Dihlmann",
        "Hendrik P. A. Lensch"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements. Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 9 5 4 7 1 . 2 1 5 2 : r 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with Generative Framework Tobias Sautter tobias.sautter@student.uni-tuebingen.de Jan-Niklas Dihlmann jan-niklas.dihlmann@uni-tuebingen.de Hendrik Lensch hendrik.lensch@uni-tuebingen.de Figure 1. 3D-RE-GEN in action: single image is decomposed into clean background and complete 3D scene with individual 3D objects, creating production-ready asset for immediate use in VFX and games. Project page: https://3dregen.jdihlmann.com/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, compositional framework that reconstructs single image into textured 3D objects and background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists requirements. Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates comprehensive background that spatially constrains objects during optimization and provides foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3DRE-GEN achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization. https: //3dregen.jdihlmann.com/ 1. Introduction The demand for immersive 3D content is cornerstone of modern creative industries, most notably in Visual Effects (VFX) and game development. Traditionally, the creation of 3D scenes is significant production bottleneck, requiring highly specialized artists with extensive experience. To construct single scene, an environment artist must first model and texture the background. Then, every individual asset within that scene, from chairs to tables to small props, must be independently modeled and textured. Finally, layout artist must meticulously assemble all these components into coherent whole. Even for highly skilled professionals, this workflow is exceptionally time consuming and costly, scaling exponentially with scene complexity. Recent generative models for single image to 3D (e.g., [7, 46]) offer the potential to accelerate asset creation and democratize 3D modeling. However, while these models are effective at generating isolated objects, they generally fail when applied to complex, multi object scenes. Reconstructing 3D scene from single 2D image is an inherently ill-posed problem. 2D projection lacks true depth information, and objects mutually occlude one another, resulting in incomplete structural data. Furthermore, accurately inferring the spatial positioning of objects to produce credible layout remains significant challenge, as the 2D image provides no explicit information about object specific distances, scale, or contact points. Previous research has addressed this issue through various approaches. Early holistic methods attempted to reconstruct entire scenes in single pass [19, 44], while retrieval based methods matched image objects to databases of 3D models [15, 22]. More recently, compositional and diffusion based methods have become prevalent [4, 20, 31, 32, 45]. Despite strong emphasis on spatial relationships between objects, these methods share common critical gaps: the background environment is frequently neglected, and there is often no mechanism to enforce physical plausibility, such as aligning objects to common ground plane. This paper introduces 3D-RE-GEN, framework designed to address this gap and generate production ready 3D scenes from single image. The modular pipeline leverages large scale pretrained models and incorporates novel four degree of freedom (4-DoF) constrained optimization with differentiable rendering. Our method estimates precise camera poses and generates an accurate textured background mesh; creates high quality 3D assets using novel context aware inpainting technique; and aligns objects to the background through robust optimization strategy, alternating between five degree of freedom (5DoF) model for free floating objects and the new 4-DoF model for ground aligned objects. Although 3D-RE-GEN is developed with artists in mind, it demonstrates competitive performance and achieves state of the art results on comparative academic benchmarks. By automating the most laborious parts of scene reconstruction, 3D-RE-GEN allows artists to move from single concept image to fully editable 3D environment in minutes, not days, drastically accelerating the creative workflow and minimizing needed experience. 2. Related Works Reconstructing 3D scenes from single 2D images is fundamental challenge in computer vision, as it requires synthesizing complex geometric structures and spatial relationships from limited visual data [10, 25, 29, 42, 43]. In recent years, this task has attracted considerable research interest, with various approaches proposed to address the ambiguities and occlusions inherent in single-view observations [8, 16, 18, 20, 30, 33, 40, 41]. The field has progressed from early methods that utilized explicit geometric priors to contemporary approaches that employ deep learning and generative models [4, 20, 38]. This section reviews major developments, with particular emphasis on object level reconstruction 2.1, scene composition strategies 2.2, and diffusion-based techniques 2.3 that have emerged as promising solutions. 2.1. Monocular Object-Level 3D Reconstruction Recent advances in 3D representation learning have facilitated substantial progress in reconstructing individual objects from single images [7, 27, 46]. To extract the correct object from an image, models often leverage pretrained 2D object detectors or segmentation networks [28, 36]. Early approaches primarily utilized supervised learning frameworks that mapped image features to 3D geometry, often employing explicit representations such as point clouds or meshes [16, 19]. These methods generally required extensive 3D supervision and exhibited limited generalization to novel object categories. The introduction of diffusion models has transformed the field, enabling the generation of high fidelity 3D shapes with enhanced generalization [39, 47]. Diffusion based methods denoise latent representations of 3D geometry conditioned on image inputs, capturing complex shape priors without explicit 3D supervision. The availability of large scale 3D datasets has further accelerated progress, supporting the training of models capable of generating diverse and detailed 3D objects from single view observations [1214]. 2.2. Scene Composition Strategies Scene reconstruction introduces challenges beyond those encountered in object level reconstruction, as it necessitates modeling spatial relationships among multiple objects. Existing approaches are generally categorized by their strategies for composing individual objects into coherent scene. Holistic methods process the entire scene as single entity, predicting unified 3D representation from the input image that includes all objects [8, 10]. These methods often employ depth estimation as geometric prior, projecting image features back into 3D space before utilizing encoder and decoder architectures to recover scene geometry. Although effective for simple scenes, holistic approaches often struggle with complex scenes containing multiple obFigure 2. 3D-RE-GEN Framework Overview. Our framework converts single image into complete 3D scene. First, we segment the image: these masks provide the 2D silhouette loss, while our novel Application-Querying (A-Q) model generates clean, inpainted object images for 3D meshing. In parallel, the input and generated empty room image are used to extract the camera and scene point cloud, which is masked to create the 3D geometric loss. Finally, our Scene Positioning model assembles the 3D assets and background by minimizing both losses, using novel 4-DOF constrained workflow to ensure all ground based objects are physically aligned to the floor. jects of varying scales and orientations, and they frequently yield low resolution reconstructions that do not generalize well to real world images [8, 30]. 2.3. Diffusion-based Scene Generation Techniques Compositional approaches mitigate these limitations by decomposing scenes into individual objects, reconstructing each object independently, and subsequently assembling them into unified scene. Early compositional methods utilized feed-forward networks to reconstruct objects from segmented image regions, followed by spatial alignment optimization [32, 44]. More recent techniques leverage large scale, pretrained object reconstruction models to enhance the quality of individual reconstructions, followed by pose optimization to maintain spatial coherence [4, 18, 47]. Despite these advances, compositional pipelines face two key challenges: (1) error accumulation across multiple stages, where mistakes in intermediate steps significantly affect the final outcome; and (2) the absence of global scene context during object reconstruction, which often results in misaligned spatial relationships among objects [47]. Recent work extends single object diffusion models to multi object scenarios by modeling inter object relationships directly during generation [20, 31]. Depth-guided conditioning further enhances geometric consistency by integrating depth information throughout reconstruction [45]. To handle occlusions, these state of the art methods often employ attention or feature aggregation modules to fuse partial object features with global scene context [20, 31, 45]. However, these approaches still lack explicit mechanisms for robust scene level alignment and, critically, do not enforce strict physical alignment, which is essential for generating coherent scenes that preserve the input images spatial relationships. Our approach builds upon the compositional paradigm while addressing these limitations through novel mechanisms for scene alignment and object understanding. We introduce novel Application-Querying method to achieve global understanding of object properties, and 4-DoF ground alignment constraint to overcome the spatial misalignment and physical implausibility issues inherent in existing methods. This combination enables more accurate and coherent scene reconstruction that better matches the input images visual content. 3. Method This chapter details the complete framework of our proposed 3D-RE-GEN pipeline. We present step by step breakdown of our compositional diffusion approach, beginning with single 2D input image and concluding with fully reconstructed, physically plausible 3D scene. Figure 2 shows high level overview of this workflow. The entire pipeline commences with single input image, I. From this image, individual objects are identified and segmented (Section 3.1). These segmented objects are then processed by our novel context aware inpainting method (Section 3.2), which serves two purposes: first, to generate clean, isolated images of each object on white background, Iobj g, and second, to remove all objects from the original image, creating clean background plate, Ibg. The newly generated, isolated object images are then passed to 2D to 3D asset creation module (Section 3.3) to produce textured 3D meshes, M. Based on the clean background, our scene reconstruction module (Section 3.4) places each asset back into the scene using our novel 4-DoF 3 constrained optimization, ensuring physically correct pose by aligning it with both the 3D scene geometry and the 2D object masks. The final output is complete 3D scene, reconstructed from the initial 2D image, which is ready for direct use in applications such as game development, simulation, or visual effects. 3.1. Image Extraction and Mask Refinement The input to our system is single RGB image, I. In the first step, Grounded SAM [36] performs text based instance segmentation to identify all objects of interest. lightweight interactive tool enables further manual inspection and refinement of the generated masks, correcting any errors in the initial automated segmentation if needed. The quality of the resulting binary masks, Mobj, is fundamentally important to the success of the entire pipeline. They provide the precise 2D silhouette ground truth for the Lsilhouette loss during the 3D pose refinement (Section 3.4) and they define the exact pixels to be extracted and provided to our inpainting model. 3.2. Context Aware Object Inpainting primary challenge in compositional scene reconstruction is handling occluded objects. Simply extracting partial segment and tasking generative model with its completion often fails, as the model lacks the necessary scene context to resolve ambiguities, limitation noted in related generative assembly work [47]. To address this, we introduce Application Querying (A-Q), novel visual prompting technique that provides rich, structured representation of both the scene context and the generative task to large scale, pretrained image editing model. Rather than providing only the occluded object segment, our A-Q method constructs composite query image. This image is structured to mimic user interface, as shown in Figure 3. The query presents the model with two distinct panels: one panel displays the full, original input image with the target objects segmentation outline, providing complete scene context. The second panel displays the extracted, occluded object segment on neutral white background, clearly defining the generative task. Figure 3b is highlighting the importance of the correct UI image prompt for the model to correctly and consistently produce the correct result. We then prompt the generative model to complete this visual query, tasking it with returning an image in the identical UI-style format, but with the object segment in the second panel fully inpainted. This structured query compels the model to leverage the contextual cues from the first panel, such as perspective, lighting, and surrounding style, to inform the generative completion of the object in the second panel. The models output is then parsed to extract the completed object image, Iobj g. This resulting asset is an ideal input for 2D to 3D reconstruction models, as it combines clean, isolated depiction with the high fidelity, context aware details inferred from the original scene. (a) Input and output of the Application-Querying (A-Q) method. (b) Failure cases. The object image completion wont work correctly without proper UI image prompt, e.g., wrong materials and shapes. Figure 3. Application-Querying. Visual example of how we utilize GUI-style interface to provide better scene context to the image manipulation model. 3.3. Generation of 3D Scene Geometry and Assets The core of our reconstruction pipeline relies on aligning generated 3D assets to the input image. To achieve this, we must first establish set of 3D guidance points and constraints that define the scenes geometry and the cameras perspective. This process involves two parallel steps: (1) reconstructing the 3D scene geometry to derive camera pose and target point clouds, and (2) generating 3D asset for each segmented object. Scene and Camera Reconstruction. To acquire robust understanding of the 3D scene and to estimate camera that aligns with the input picture, we utilize geometry transformer model. Those models return an estimated scene camera and back projected 3D points. We employ novel strategy to enhance its alignment capabilities: instead of feeding only the original image I, we provide the model with both and the empty room background image Ibg generated during our inpainting phase (Section 3.2). The geometric model processes both images and produces two aligned point clouds and corresponding cameras, visualized 4 in our framework Figure 2. We retain only the main camera of the input image I, as the second camera is aligned to the background. We also now have the full scene point cloud Pscene and the aligned background point cloud Pbg. Background Creation. We can use the extracted background point cloud Pbg as base for our background creation. As there can be minor offsets between Pscene and Pbg, an iterative algorithm aligns both scenes by slightly shifting them in global 3D space. In the end, meshing algorithm turns the aligned point cloud into usable mesh for simulations and rendering. Object Point Cloud Extraction. To create specific 3D targets for our alignment loss, we use the 2D object masks Mobj from the segmentation step (Section 3.1). By back projecting these masks into the 3D scene, we effectively stencil Pscene and extract the specific points belonging to each object. This results in set of object specific point clouds {P 1 target} for objects. These point clouds serve as the ground truth for our 3D geometric loss term L3D during the pose refinement stage. Generative 3D Asset Creation. Running in parallel to the scene reconstruction, our modular pipeline generates 3D asset for each object. Using the inpainted, isolated object images {I1 obj g} from Section 3.2 as input, we pass each image to 2D to 3D generative model, generating separated and textured objects. target, ..., obj g, ..., IN 3.4. Differentiable Scene Reconstruction Pose Initialization and Model Selection. We first perform coarse initialization of the objects pose. This strategy is contingent on the objects placement, which we determine by computing the 2D Intersection over Union (IoU) between the object mask Mobj and the floor mask Mf loor. If there is no intersection, the object is assigned to 5-DoF Model (optimizing 3D translation, 1D yaw, and 1D scale) and is initialized by aligning its Oriented Bounding Box (OBB) with that of the target point cloud Ptarget. If the IoU is greater than zero, we assume the object is on the floor and assign it to our novel 4-DoF planar model. Its pose is initialized by projecting its bottom vertices onto the fitted floor plane (n, p0), which is extracted from Ptarget using robust RANSAC algorithm. Composite Loss Function. Both models are optimized by minimizing the same composite loss function Ltotal, which ensures alignment to both 2D and 3D priors: Ltotal = wsilLsilhouette + w3DL3D + wbboxLbbox, where wsil, w3D, and wbbox are scalar weights. 2D Silhouette Loss (Lsilhouette). This term enforces 2D alignment by minimizing the discrepancy between the rendered silhouette and the ground truth mask Mobj. It is combination of Dice and Focal loss to robustly handle class imbalance at object edges. 3D Geometric Loss (L3D). This term enforces 3D structural alignment by minimizing the point to mesh face distance from the target object point cloud Ptarget to the surface of the transformed object mesh M. Background Bounding Box Loss (Lbbox). This term acts as physical regularizer, preventing the object from penetrating static scene geometry. It applies penalty to any object vertex that lies outside precomputed background bounding box Bbg on the and axes, while ignoring the Y-axis to allow objects to rest on the floor. 4-DoF Constrained Planar Optimization. Our primary contribution for physically plausible scene assembly is the PlanarModel. This approach constrains the 5-DoF problem to 4-DoF problem defined in plane local coordinate system derived from the fitted ground plane (n, p0). The models four learnable parameters are defined in this local space: 2D translation (t y), and 1D uniform scale (s). The key physical constraint is enforced by applying the translation as 3D vector (t z) in this local space, explicitly restricting all movement to the 2D floor surface. In each step, the model applies these 4-DoF transformations locally, then projects the resulting vertices Vplane back to world space using fixed transformation matrix Tplaneworld to be evaluated by the composite loss. This reduction in dimensionality and enforcement of the strong physical planar placement constraint make the optimization highly robust. z), 1D yaw (r x, 0, x, 4. Experiments In this section, we conduct comprehensive evaluation of our proposed framework, 3D-RE-GEN, to validate its effectiveness in reconstructing high fidelity, physically plausible 3D scenes from single image. We first outline our implementation details and the standard metrics used for evaluation. We then present quantitative and qualitative results, comparing our method against SOTA compositional scene reconstruction pipelines. These comparisons demonstrate that our framework achieves SOTA performance, particularly in generating complete scenes with correctly aligned assets and fully modeled background. Finally, we perform ablation studies to assess the impact of our novel contributions. These studies isolate the components of our method, specifically validating the effectiveness of our Application-Querying inpainting technique and the crucial role of our 4-DoF constrained PlanarModel in achieving greater physical and spatial alignment. 4.1. Experimental Overview Implementation Details. 3D-RE-GEN integrates several off the shelf models: GroundedSAM [36] for object detection and segmentation; Googles Image Flash (NanoBanana) [9, 17] for image modification and inpainting, Hunyuan3D 2.0 [46] for 3D asset reconstruction, and VGGT [37] Figure 4. Qualitative comparison across different methods for different input scenes. Starting with 4 scenes based on synthetic datasets and two real images. In the bottom line, we even tested an outside image. for camera parameter estimation and point cloud reconstruction. Scene reconstruction is performed using differentiable renderer implemented with PyTorch3D [35] which is used for pose estimation per object based on loss. Blender [6] is used as physically based renderer for visualization and evaluation. Runtime depends on available hardware resources. The system supports multi GPU execution, distributing tasks such as object placement and 2D to 3D asset generation across devices. On single GPU, typical scene with approximately 10 objects requires 17 to 20 minutes, with 3D asset creation being the dominant factor. Using four GPUs reduces this to around 7 to 8 minutes. Experiments were conducted on an NVIDIA RTX 4090, 24GB VRAM and an NVIDIA A40, 40GB VRAM. The most memory intensive component is Hunyuan3D 2.0; replacing it with lighter models such as SPAR3D [21] would allow runtime with as 16GB VRAM or less. Datasets. Since 3D-RE-GEN is non trained, model it does not require large scale training based method, datasets. Instead, we evaluate it on set of hand picked scenes synthetic scenes from CGTrader [1], covering diverse conditions such as cluttered rooms, low light scenes, and monochromatic environments. We use royalty free images for our real world scenario cases and some self taken pictures for outside scene tests. These manually selected scenes provide strong generalization tests, as many modern approaches, such as MIDI [20], DepR [45], and SceneGen [31], are trained on large syn6 thetic datasets like 3D-Front [14]. Using unseen and varied CGTrader scenes helps ensure that evaluation metrics reflect realistic generalization rather than dataset specific bias. distance of our method indicates more stable reconstruction quality, with fewer outlier points and artifacts compared to all other approaches. (a) Overall reception (b) Reason for choice Figure 5. Quantitative survey with 59 participants on how people perceive scene reconstruction method outputs. Metrics. All 3D metrics are computed at the scene level. For evaluation, both the predicted and ground truth scenes are converted into point clouds and normalized to unit scale centered at the world origin [0, 0, 0]. The two point clouds are aligned using the ICP algorithm [5] before metric computation. Quantitative 3D evaluation includes commonly used metrics like Chamfer Distance (CD), F-score and Bounding Box IoU (BBOX-IOU) [20, 31, 45, 47]. Precision as part of F-Score allows for more intricate overview. BBOX-IOU as metric highlights if the objects are actually at the same relative scene position and size. The Hausdorff distance [3] was chosen to show the behavior of outliers, highlighting more consistent scenes that are spatially cohesive. Baselines. We compare 3D-RE-GEN against two SOTA scene generation methods, MIDI [20] and DepR [45]. Both are evaluated using their publicly available pretrained models. As both rely on segmentation masks as input, we utilize their respective automatic mask generation modules, each based on GroundedSAM derivatives. While MIDI produces textured outputs, DepR generates geometry only. As texture generation in MIDI currently faces known issues of reproducibility and doesnt contribute to 3D metrics we omit it. Metric 3D DepR [45] MIDI [20] Ours CD F-Score IOU Precision Recall Hausdorff 0.028 0.036 0.011 0.65 0. 0.12 0.18 0.61 0.70 0.57 0. 0.13 0.55 0.85 0.63 0.21 0. 0.33 Table 1. Quantitative comparison of different methods tested on our synthetic collected dataset, see implementation details. 4.2. Results Quantitative Results. Table 1 summarizes the quantitative evaluation on synthetic data of our final scene reconstructions. 3D-RE-GEN consistently and significantly outperforms competing methods such as MIDI and DepR across all major 3D metrics. While MIDI achieves respectable BBOX-IOU due to its multi instance attention, our method surpasses it by enforcing strict, physically based ground alignment. Furthermore, the significantly lower Hausdorff 7 Qualitative Results. Figure 4 illustrates representative qualitative comparisons, showcasing our methods robust generalization across diverse data, including synthetic, challenging real world, and even outdoor images; domain rarely tested by existing methods. 3D-RE-GEN uniquely produces coherent, physically plausible scenes. We note that for real world inputs, our method not only maintains correct spatial grounding and perspective alignment, but also achieves exceptionally high visual fidelity and texture quality. MIDI, while capturing spatial grouping, frequently exhibits severe mesh artifacts, such as merging distinct objects or duplicating geometry. DepR often fails to produce visually coherent scenes; its outputs, while scoring well on some metrics, often appear as misaligned, incorrectly rotated, or incomplete blobs that float in space. In contrast, our method consistently generates well structured layouts with sharply defined, complete geometry and fully reconstructed, textured background. The high quality background and the precise alignment of our assets are critical for our target applications, as they provide complete environment suitable for downstream VFX workflows, such as shadow casting, realistic light bounces, and physics based simulations. Our intermediate results also highlight the strength of our pipeline. Our Application-Querying method produces clean, complete object assets even from heavily occluded inputs. Figure 4 further shows our background reconstruction, which provides the essential physically grounded stage upon which our 4-DoF alignment can produce final, cohesive scene. We conducted small study with 59 participants 5. They had to chose, based on an input image, which output of scene reconstruction model they preferred most. Visually it resembled closely our qualitative table 4. 3D-RE-GEN achieved the best score of 81% of reception of quality. The most picked reason of choice was : Layout / Composition. Metric 3D No 4-DoF No A-Q Ours CD F-Score IOU Hausdorff 0.019 0.030 0.011 0. 0.52 0.51 0.68 0.51 0.52 0. 0.63 0.33 Metric 2D No 4-DoF No A-Q Ours SSIM LPIPS 0.27 0.46 0.13 0.66 0.29 0. Table 2. Ablation study. Comparing the complete 3D-REGEN model against 3D-RE-GEN without ground constraints (No 4-DoF); and against 3D-RE-GEN without Application-Querying model (No A-Q). (a) Input image (b) 3D-RE-GEN output (c) Without A-Q (d) Without 4-DoF Figure 6. Ablation examples. Output of 3D-RE-GEN without the use of our Application-Querying Model 6c and without the use of our ground constrain 4-DoF Model 6d. 4.3. Ablation Study To validate the effectiveness of our key contributions, we conduct an ablation study, with quantitative results presented in Table 2 and visually in Figure 6. We evaluate our full pipeline against two modified versions. Firstly, 3D-REGEN without the use of our 4-DoF model, so pose estimation without ground constraints. All objects are pose optimized using only the 5-DoF RegularModel. This tests how much objects would usually deviate from following camera perspective and how much our 4-DoF model ensures physically aligned scene creation. Secondly, 3D-RE-GEN without A-Q: This version disables our Application-Querying inpainting step. The 2D to 3D generator model is instead fed with only the unrepaired segmented object. We evaluate these versions using both 3D geometric metrics and 2D perceptual metrics (SSIM and LPIPS [20, 31]) to measure the visual quality of the final rendered image against the input. Our full, combined model shows the best performance, achieving the best scores across all metrics. The model without 4-DoF PlanarModel suffers from drops in 3D metrics, proving that our 4-DoF ground plane constraint is critical for geometric accuracy and well structured scenes follow camera perspective. Without the use of the 4DoF ground constraints, objects can float and dont need to follow correct ground alignment, see figure 6d. The version without Application-Querying 6c shows strong falloff in both 2D and 3D scores, demonstrating that generating assets from incomplete, occluded segments leads to poor geometry and visual incoherence and no background creation. Objects are recreated in 3D like the segmented input directs, so no scene context is given and incomplete objects are cre8 ated. These ablations confirm that our pipelines SOTA performance is direct result of our novel 4-DoF optimization and our context aware Application-Querying. 5. Conclusion In this work, we introduced 3D-RE-GEN, novel and robust framework for compositional reconstruction of complete 3D indoor scenes from single input image. Our method leverages large scale models to decompose an image, while two key novelties address the most significant challenges in scene reconstruction. First, our context aware Application-Querying (A-Q) inpainting technique allows generation of high quality and complete 3D assets, even from occluded inputs. Our 4-DoF PlanarModel ensures physical plausibility by correctly aligning all ground based objects to interpolated floor plane. Our pipeline successfully recovers the background, matched camera, and all generated objects, ensuring structural coherence and correct perspective alignment with the input view. Quantitative and qualitative evaluations demonstrate that 3D-REGEN achieves SOTA performance, producing cohesive 3D scenes free of the floating artifacts and geometric inconsistencies common in other methods. The reconstructed scenes, from single image, are readily applicable to downstream VFX and game development workflows, offering an efficient and powerful tool for easy usage. 6. Acknowledgements Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC number 2064/1 Project number 390727645. This work was supported by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP 02, project number: 276693517. This work was supported by the Tubingen AI Center. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Jan-Niklas Dihlmann."
        },
        {
            "title": "References",
            "content": "[1] CGTrader - 3D Model Store. https://www.cgtrader.com/. 6 [2] Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild, 2019. 3 [3] Ulugbek Alibekov, Vanessa Staderini, Philipp Schneider, and Doris Antensteiner. Advancing Precision in Multi-Point Cloud Fusion Environments, 2025. 7 [4] Andreea Ardelean, Mert and Bernhard Egger. Gen3DSR: Generalizable 3D Scene Reconstruction via Divide and Conquer from Single View, 2025. 2, 3 Ozer, [5] Paul J. Besl and Neil D. McKay. method for registration of 3-d shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2):239256, 1992. 7 [6] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2025. 6, 2 [7] Mark Boss, Zixuan Huang, Aaryaman Vasishta, and Varun Jampani. SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement, 2024. 2 [8] Tao Chu, Pan Zhang, Qiong Liu, and Jiaqi Wang. BUOL: Bottom-Up Framework with Occupancy-aware Lifting for Panoptic 3D Scene Reconstruction From Single Image, 2024. 2, 3 [9] Gheorghe Comanici, Eric Bieber, and et al. Schaekermann. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities, 2025. 5 [10] Manuel Dahnert, Ji Hou, Matthias Nie√üner, and Angela Dai. Panoptic 3D Scene Reconstruction From Single RGB Image, 2022. 2 [11] Haixing Dai, Chong Ma, Zhiling Yan, Zhengliang Liu, Enze Shi, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Fang Zeng, Dajiang Zhu, Wei Liu, Quanzheng Li, Lichao Sun, Shu Zhang Tianming Liu, and Xiang Li. SAMAug: Point Prompt Augmentation for Segment Anything Model, 2024. 3 [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: Universe of Annotated 3D Objects, 2022. [13] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3D-FUTURE: 3D Furniture shape with TextURE, 2020. [14] Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming Wang Cao Li, Zengqi Xun, Chengyue Sun, Rongfei Jia, Binqiang Zhao, and Hao Zhang. 3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics, 2021. 2, 7 [15] Daoyi Gao, David Rozenberszki, Stefan Leutenegger, and Angela Dai. DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and Alignment from an RGB Image, 2024. 2 [16] Georgia Gkioxari, Nikhila Ravi, and Justin Johnson. Learning 3D Object Shape and Layout without 3D Supervision, 2022. 2 [17] Google Developers. Introducing gemini 2.5 flash image, our state-of-the-art image model. https://developers. googleblog.com/en/introducinggemini25-flash-image/, 2025. Accessed: November 6, 2025. 5, 1, 3 [18] Haonan Han, Rui Yang, Huan Liao, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu Li, and Wanhua Li. REPARO: Compositional 3D Assets Generation with Differentiable 3D Layout Alignment, 2025. 2, [19] Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and Song-Chun Zhu. Holistic 3D Scene Parsing and Reconstruction from Single RGB Image, 2018. 2 [20] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, and Lu Sheng. MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation, 2024. 2, 3, 6, 7, 8 [21] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, and Varun Jampani. SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images, 2025. 6 [22] Hamid Izadinia, Qi Shan, and Steven M. Seitz. IM2CAD. 2 [23] Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, and Konrad Schindler. Marigold: Affordable Adaptation of DiffusionBased Image Generators for Image Analysis, 2025. 2 [24] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment Anything in High Quality. 3 [25] Muhammad Saif Ullah Khan, Alain Pagani, Marcus Liwicki, Didier Stricker, and Muhammad Zeshan Afzal. ThreeDimensional Reconstruction from Single RGB Image Using Deep Learning: Review. Journal of Imaging, 8(9):225, 2022. 2 [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment Anything, 2023. 3 [27] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, Sheng Zhang, Xin Huang, Di Luo, Fan Yang, Fang Yang, Lifu Wang, Sicong Liu, Yixuan Tang, Yulin Cai, Zebin He, Tian Liu, Yuhong Liu, Jie Jiang, Linus, Jingwei Huang, and Chunchao Guo. Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details, 2025. [28] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-SAM: Segment and Recognize Anything at Any Granularity, 2023. 2 [29] Feng Liu and Xiaoming Liu. Voxel-based 3D Detection and Reconstruction of Multiple Objects from Single Image, 2021. 2 [30] Haolin Liu, Yujian Zheng, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Towards High-Fidelity Single-view Holistic Reconstruction of Indoor Scenes, 2022. 2, 3 [31] Yanxu Meng, Haoning Wu, Ya Zhang, and Weidi Xie. SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass, 2025. 2, 3, 6, 7, 8 [32] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Jun Zhang. Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction for Indoor Scenes from Single Image, 2020. 2, 3 [33] Stefan Popov, Pablo Bauszat, and Vittorio Ferrari. CoReNet: Coherent 3D scene reconstruction from single RGB image, 2020. 2 [34] Yansong Qu, Shaohui Dai, Xinyang Li, Yuze Wang, You Shen, Liujuan Cao, and Rongrong Ji. DeOcc-1-to-3: 3D DeOcclusion from Single Image via Self-Supervised MultiView Diffusion, 2025. [35] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3D Deep Learning with PyTorch3D, 2020. 6 [36] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks, 2024. 2, 4, 5, 3 [37] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual Geometry Grounded Transformer. 5, 2 [38] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3D: High-Quality and Efficient 3D Mesh Generation from Single Image, 2024. 2 [39] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer, 2024. [40] Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, and Tat-Jen Cham. Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images, 2025. 2 [41] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 26902698, 2019. 2 [42] Yuan Yao, Nico Schertler, Enrique Rosales, Helge Rhodin, Leonid Sigal, and Alla Sheffer. Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction, 2020. 2 [43] Bo Zhang, Jiakang Yuan, Botian Shi, Tao Chen, Yikang Li, and Yu Qiao. Uni3D: Unified Baseline for Multi-dataset 3D Object Detection, 2023. 2 [44] Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys, and Shuaicheng Liu. Holistic 3D Scene Understanding from Single Image with Implicit Representation, 2021. 2, 3 [45] Qingcheng Zhao, Xiang Zhang, Haiyang Xu, Zeyuan Chen, Jianwen Xie, Yuan Gao, and Zhuowen Tu. DepR: Depth Guided Single-view Scene Reconstruction with Instancelevel Diffusion, 2025. 2, 3, 6, 7 [46] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Shaoxiong Yang, Song Zhang, Yang Liu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Jingwei Huang, and Chunchao Guo. Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation, 2025. 2, [47] Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Zero-Shot 10 Scene Reconstruction from Single Images with Deep Prior Assembly, 2024. 2, 3, 4, 7 [48] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: Modern Library for 3D Data Processing, 2018. 2 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with Generative Framework"
        },
        {
            "title": "Overview",
            "content": "B.1. Limitations"
        },
        {
            "title": "Supplementary Material",
            "content": "supplementary material complements This the main manuscript by providing extra background creation specifics, extended qualitative results, and critical discussion of the proposed 3D-RE-GEN framework. The document is organized as follows: Additional Videos (Section A): We introduce the accompanying video materials that demonstrate the pipelines efficacy and downstream utility in dynamic environments. Discussion (Section B): We provide critical analysis of the systems current limitations and outline prospective avenues for future research and development. Background Extraction and Texturing (Section C): We offer detailed visualization of the workflow used to extract geometry and generate textures for the scene background. Prompt Specifications (Section D): To ensure reproducibility, we provide the specific text and image prompts utilized in our Application-Querying module. User-Friendly Mask Refinement (Section E): We demonstrate our custom Gradio interface, designed to facilitate the manual fine tuning of segmentation masks for enhanced reconstruction precision. A. Additional Videos We provide two supplementary video files to visually validate the capabilities of our method: Downstream Application. 3D-RE-GEN vfx.mp4 demonstrates the practical utility of our reconstructed scenes in professional workflows. This sequence showcases visual effect applied directly to our generated output, highlighting the mesh quality and physical plausibility necessary for seamless integration into VFX pipelines. and Method Overview Comparisons. 3D-RE-GEN overview and results.mp4 proIt vides comprehensive summary of the framework. presents qualitative comparisons against state of the art methods using 360 degree turntable visualizations, allowing for rigorous assessment of geometric fidelity and texture consistency. B. Discussion In this section we highlight some issues that can occour or overall weaknesses. In the end we show what we plan on improvements that others can explore or we will implement in the future. 1 Mask Sensitivity. The proposed method exhibits sensitivity to inaccuracies in the initial segmentation masks, which are fundamental for guiding both inpainting and spatial positioning. Segmentation errors can propagate through the pipeline, providing erroneous context to the Application Querying module [17], resulting in the generation of incorrect geometry. Furthermore, inaccurate masks may cause the inadvertent culling of valid points from the target point cloud used for the 3D loss. This degradation of the optimization target can lead to suboptimal convergence, as the resulting 3D geometry fails to align with the intended 2D silhouette constraints. Geometric Estimation Uncertainty. The probabilistic nature of the geometry transformer introduces potential reconstruction artifacts. Points classified with low confidence are discarded during preprocessing, which can compromise the structural integrity of the reconstructed scene, manifesting as holes or discontinuities in the background mesh (see Figure 7). Additionally, because the geometry transformer is not explicitly trained to align background only images with original scene images under unified coordinate system, minor discrepancies in camera estimation can occur, potentially resulting in spatial misalignment between the generated background and the global scene context. Optimization Convergence. As with many learning based optimization frameworks, the differentiable rendering process is susceptible to the non convex nature of the loss optimizations. The pose refinement is liable to get trapped in local minima, particularly when the initial pose estimate deviates significantly from the ground truth. Silhouettes for example can be very close to the current perspective of an objects which is rotated 180 degrees. This can occasionally cause objects to converge to suboptimal positions or orientations within the scene. Object Granularity. To balance computational efficiency with reconstruction robustness, complex compound objects are currently generated as unified meshes rather than discrete assemblies (e.g., bookshelf and its books are treated as single entity). While this simplifies the reconstruction process, it limits the granularity of downstream interactions in VFX workflows, potentially necessitating manual geometry separation for fine grained manipulation. Generative Stochasticity. final limitation arises from the inherent stochastic nature of the generative components. The reliance on random seeds for both context aware inpainting and 2D to 3D asset creation means that identical inputs can yield perceptually distinct results, posing challenge for applications requiring strictly deterministic reproducibility. B.2. Future Work The modular architecture of 3D-RE-GENopens several promising avenues for future research and functional expansion. Hierarchical and Non Planar Constraints. While our current 4-DoF optimization strictly enforces ground plane alignment, future work will aim to generalize this to hierarchical constraint system. This would enable the placement of sub objects onto arbitrary planar surfaces, such as placing lamps on tables or books on shelves, facilitating the decomposition of clustered assets into granular, interactive components. Multi View. Although designed for single image reconstruction, the optimization framework is extensible to multi view inputs. Integrating constraints from multiple viewpoints would significantly reduce geometric ambiguity and improve occluded region fidelity. Advanced Rendering and Simulation. To further enhance utility for VFX pipelines, we plan to integrate advanced material estimation models capable of outputting high fidelity BRDF parameters. This would allow for photorealistic relighting and seamless integration of reconstructed scenes into varying lighting environments. Generalization to Unconstrained Environments. Finally, we demonstrate the extensibility of our framework beyond indoor scenes. By adapting our ground alignment logic to accommodate uneven terrain and leveraging the robustness of our 4-DoF optimization, 3D-RE-GEN successfully reconstructs complex, unconstrained outdoor environments (see Figure 4). We observe that our pipeline correctly extracts and aligns structured outdoor assets, such as vehicles, to the estimated ground plane. However, we note current limitation in the generative 2D to 3D backend: models trained primarily on synthetic or object centric datasets [46] struggle with the abstract, high frequency geometry of organic assets like trees and foliage. While our spatial positioning remains accurate, the geometric fidelity of these specific asset classes represents domain gap to be bridged by future generative models trained on diverse outdoor data. C. Background Extraction and Texturing The reconstruction of the background environment is still an important part of our pipeline, ensuring spatially correct stage for object placement (BBOX-Loss) and an object to bounce lights and act as collision for simulations. Due to space constraints in the main manuscript, we provide more detailed workflow for background creation here, as illustrated in Figure 7. Figure 7. Background extraction workflow. Extraction of pointclouds trough geometry transformer; pointclouds getting meshed with surfacing algorithm and then texture applied through projection. Geometry Extraction. We use VGGT [37] to extract the initial point cloud representation of the scene. key strategy in our approach is to process both the original input image and the inpainted empty room (background only) image simultaneously. This enforces shared coordinate system, ensuring the extracted background geometry is perfectly aligned with the original camera perspective. Meshing. To facilitate standard VFX workflows, we convert the raw point cloud into clean triangle mesh. We utilize the Poisson surface reconstruction algorithm implementation in Open3D [48] to generate surface from the point cloud data. Texturing and Material Generation. For texturing, the mesh is imported into Blender [6], where we perform camera based UV projection. We project the inpainted empty room image directly onto the geometry, effectively using it as baked light texture. While we also generate full suite of PBR maps including Albedo, Roughness, Metallic, and Normals; using Marigold [23]. We observed that the 2 ground prompt with the best working results was as follows: Remove ALL objects and furniture. want single empty room. No chairs, tables, lamps, dresser, kitchen parts etc. Just give me back the same room but EMPTY. Keep only canvas and rugs. Same light, same perspective, same walls, floor and ceiling. original baked light image yielded better visual quality and coherence for the background compared to the decoupled PBR maps. D. Prompts This chapter gives insight in how we utilized text and image prompts to guide the models to the best result. Prompts for GroundedSAM. Starting point for image segmentation is GroundedSAM [36]. We use diverse text prompts to cover the largest base, especially for bigger furniture. Most used prompts are: -furniture -table with decorations -chair -sideboard with decorations -shelf -bookshelf -dresser with decorations - cabinet with decorations -couch -bed with pillows -lamp - floor -kitchen counter Figure 8. Using just an inline approach can lead to wrong scene understanding. Prompts for nanoBanana. The inputs for nanoBanana [17] needed bit more refinement with lot of trail and error over time. Following are the prompts that worked overall the best. First was semi working inline prompt visualized in figure 8. It was starting point but some cases didnt work correctly. For example issues were that wrong object was picked, wrong object understanding, nothing at all happens (gave back the same image as the input was) and the inline color was mistakenly seen as part of the object : Extract this red marked object. Create single render of it with white background. Application-Querying prompt: [OBJECT EXTRACTION APPLICATION]: extract single 3D object out of scene. The extracted object should appear in the white box without background from frontal view. Only the single selected object with border should be extracted and repaired if parts are missing. No object occluding the selected object should be reconstructed. No accidental background leaking should be included. Use the scene as reference and extract the object. For the background, we used prompt with bit more details. Some backgrounds need more iterations with different seed to get the best possible outcome. The back3 Figure 9. Gradio App to finetune masks from GroundedSAM E. User-friendly mask generation Mask generation is still hard topic. Latest models like SAM [26] still struggle to correctly classify objects consistently. Research tries to negate the issues by improving with extra prompts [11] or trying to improve with better training [34]. We used SAM-HQ [24] for better mask initialization, which improved quality drastically. Even still, some objects where not covered good enough. As masks are fundamental for good result, we created gradio [2] app that lets the user add, manipulate and delete masks. For usage gradio UI can be opened up in browser locally 9."
        }
    ],
    "affiliations": [
        "University of T√ºbingen"
    ]
}