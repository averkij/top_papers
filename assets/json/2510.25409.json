{
    "paper_title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains",
    "authors": [
        "Vijay Devane",
        "Mohd Nauman",
        "Bhargav Patel",
        "Aniket Mahendra Wakchoure",
        "Yogeshkumar Sant",
        "Shyam Pawar",
        "Viraj Thakur",
        "Ananya Godse",
        "Sunil Patra",
        "Neha Maurya",
        "Suraj Racha",
        "Nitish Kamal Singh",
        "Ajay Nagpal",
        "Piyush Sawarkar",
        "Kundeshwar Vijayrao Pundalik",
        "Rohit Saluja",
        "Ganesh Ramakrishnan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 0 4 5 2 . 0 1 5 2 : r BHASHABENCH V1: Comprehensive Benchmark for the Quadrant of Indic Domains Vijay Devane, Mohd Nauman, Bhargav Patel, Aniket Mahendra Wakchoure, Yogeshkumar Sant, Shyam Pawar, Viraj Thakur, Ananya Godse, Sunil Patra, Neha Maurya, Suraj Racha, Nitish Kamal Singh, Ajay Nagpal, Piyush Sawarkar, kundeshwar Vijayrao pundalik, Rohit Saluja, Ganesh Ramakrishnan"
        },
        {
            "title": "BharatGen Team",
            "content": "The rapid advancement of large language models (LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domainspecific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides comprehensive dataset for evaluating large language models across Indias diverse knowledge domains. It enables assessment of models ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research: https://bharatgen-iitb-tih.github.io/bhashabenchv1/ Date: October 30, 2025 Correspondence: Kundeshwar.pundalik@tihiitb.org"
        },
        {
            "title": "1 Introduction",
            "content": "The rapid advancement of large language models (LLMs) has transformed artificial intelligence, extending their capabilities far beyond traditional natural language processing. Models such as GPT-4o [49], GPT-OSS-120B [52], DeepSeek-V3 [19], and Qwen-3 [72] excel across diverse domains, from code generation and mathematical reasoning to creative writing and scientific analysis [15, 63, 51], enabling applications in conversational AI, education, healthcare, finance, legal services, and agriculture [16, 68]. Platforms like Krishi Sathi [64] leverage LLMs for crop advisory and pest detection, improving agricultural productivity. Despite these advances, substantial performance gaps remain in multilingual and domain-specific contexts, particularly for non-Latin, low-resource languages [65, 77, 1]. Englishcentric training limits models ability to capture nuanced knowledge in specialized fields and India-specific domains, such as Ayurveda, indigenous agriculture, finance, and regional legal systems [69, 58, 35], highlighting the need for culturally and contextually aware evaluation. The scale of this problem demands urgent attention, as Indias diverse knowledge ecosystem affects millions of lives across multiple critical domains. In agriculture alone, Over 40 million farmers rely on farming-related activities [28], and access to accurate information on crop management, soil health, and sustainable practices can have direct impact on food security and livelihoods. The complexity is further magnified by the fact that each state in India has its own distinct agricultural methods, crop varieties, soil conditions, and traditional farming practices that have evolved over centuries to suit local climatic and geographical conditions. Similarly, Indias legal system processes millions of cases annually, requiring precise understanding of complex legal frameworks, precedents, and procedural nuances that vary across states and jurisdictions [44]. The healthcare sector, particularly 1 Figure 1 Overview diagram and statistics of BhashaBench V1. traditional medicine systems like Ayurveda, serves millions of patients who rely on practitioners knowledge of ancient texts, formulations, and treatment protocols. Furthermore, Indias financial ecosystem processes billions of transactions daily, including over 100 billion UPI transactions annually, where even minor misunderstandings in financial regulations or procedures can have cascading effects [45]. While existing benchmarks such as MMLU [26], HellaSwag [75], AGIEVAL [78], and more recent multilingual efforts like MEGA [1] attempt to assess model capabilities, they often focus primarily on English content and may not fully capture India-specific nuances, cultural contexts, and domain expertise that are essential for real-world applications in the Indian subcontinent. To address these critical gaps, we introduce BhashaBench V1, the first comprehensive domain-specific, multi-task, bilingual benchmark designed explicitly for evaluating large language models on India-centric knowledge systems. BhashaBench V1 encompasses four fundamental domains that form the backbone of Indian society and economy: Agriculture (BBK - BhashaBench Krishi), Legal (BBL - BhashaBench Legal), Finance (BBF - BhashaBench Finance), and Ayurveda (BBA - BhashaBench Ayurveda). The benchmark spans over 90 subdomains and covers more than 500 specific topics, reflecting the intricate complexity and diversity of Indian knowledge systems. This granular categorization enables fine-grained evaluation of model performance across specialized areas that require deep domain expertise and cultural understanding. The dataset has been meticulously curated from over 40 authentic government and professional examination papers, ensuring that the questions reflect real-world scenarios and ground-level challenges faced by practitioners in these domains [29, 79]. To maximize coverage across Indias linguistic landscape, BhashaBench V1 currently supports English and Hindi, the two most widely understood languages in the country, collectively enabling assessment of models capabilities for significant portion of Indias population while maintaining the cultural and contextual authenticity of the original knowledge systems. Our comprehensive evaluation of 29+ state-of-the-art language models on BhashaBench V1 reveals significant performance disparities across domains and languages, highlighting the urgent need for India-specific model development and evaluation. The results demonstrate substantial domain-specific performance gaps, with models showing varying 2 degrees of competency across different knowledge areas. For instance, GPT-4o, one of the top-performing models, achieved 76.49% accuracy in the Legal domain but only 59.74% in Ayurveda, illustrating the challenges models face with traditional Indian knowledge systems. Similarly, consistent language-specific performance gaps emerged, with models generally performing better on English content compared to Hindi across all domains. The subdomain-level analysis further reveals granular insights into model capabilities, showing that certain areas such as Cyber Law and International Finance demonstrate relatively strong performance, while traditional domains like Panchakarma, Seed Science, and Human Rights remain notably challenging for current LLMs. These findings underscore the critical importance of domain and language-specific evaluation frameworks for assessing model readiness for real-world deployment in diverse Indian contexts."
        },
        {
            "title": "2.1 Exploration of LLMs",
            "content": "The landscape of large language models has witnessed unprecedented growth, with both proprietary and open-source models achieving remarkable capabilities. Recent proprietary LLMs, including GPT-4o and GPT-4o-mini [51], Claude3.5 Sonnet [4], and the Gemini series [22], have demonstrated significant improvements across various benchmarks [18, 67]. The open-source ecosystem has flourished with models such as the Llama-3 series [23], Gemma [61], Qwen2.5 [56], and Mistral [31] achieving competitive performance while maintaining transparency and accessibility. While primarily trained on English-dominant corpora, many models incorporate substantial multilingual data during pretraining [61, 23, 81], enabling capabilities in hundreds of languages with varying proficiency [46]. Languagespecific models have gained momentum, particularly for underrepresented languages including Indic languages [21, 20]. Notable examples include Airavata [21], MuRIL [36], and recent generative models like Param-1 [55]. Domain-specific language models have emerged as critical research direction. Medical applications include MedPaLM [59] and BioBERT [40], while legal and financial domains have seen LegalBERT [17] and FinBERT [73] respectively. In the Indian context, domain-specific initiatives like Agri-Param [9], Ayur-Param [10], Finance-Param [11], and Legal-Param [12] address unique requirements of Indias diverse knowledge systems through continual pretraining [43] or instruction fine-tuning [5]. Despite these advances, comprehensive evaluation frameworks for culturally and linguistically diverse domains remain limited, particularly for traditional knowledge systems requiring nuanced understanding of local contexts. This work conducts comprehensive evaluation of 29+ state-of-the-art models on BhashaBench V1 to address these evaluation challenges."
        },
        {
            "title": "2.2 Evaluation of LLMs",
            "content": "Numerous benchmarks have been developed to assess large language model performance. General-purpose benchmarks such as MMLU [27], MMLU-Pro [66], AGIEval [78], BIG-Bench [60], and HellaSwag [75] evaluate LLMs across diverse tasks from commonsense reasoning to knowledge-intensive question answering. However, these remain largely Anglocentric with limited multilingual evaluation [8, 33]. To address domain-specific challenges, specialized benchmarks have emerged. In agriculture, benchmarks like AgriBench [80], BVL QA Corpus [3], AgXQA [38], AgEval [6], and SeedBench [74] cover crop disease identification to advisory support. The finance domain features FinGAIA [76], FinanceBench [30], MultiFin [54], InvestorBench [41], and MultiFinBen [54] for financial reasoning, fraud detection, and trading evaluation. Legal domain efforts include IL-TUR [32], IndicLegalQA [47], LegalBench [24], LEXTREME [48], and the CAIL series [70, 71] for legal question answering, case summarization, and judgment prediction. Traditional medicine resources such as MTCMB [37], Pratyaya-Kosh [57], Anveshana [62], and OpenTCM [25] provide task-specific evaluation datasets covering knowledge graphs, OCR correction, and dosha analysis. Despite this progress, key limitations persist. Many benchmarks are restricted to English or high-resource languages, limiting effectiveness for multilingual and Indic contexts. Others focus on narrow tasks, unable to capture full domain expertise. Evaluation methodologies vary widely from accuracy scores to human judgments, hindering standardized comparison across domains and languages. These gaps underscore the need for unified, multilingual, and domainaware evaluation framework."
        },
        {
            "title": "3.1 Design Principles",
            "content": "The primary motivation behind BhashaBench V1 is to comprehensively assess domain-specific knowledge and reasoning capabilities of large language models within Indias diverse and culturally rich knowledge ecosystems. Unlike existing benchmarks focusing on general or Western-centric domains, our benchmark evaluates specialized Indian knowledge systems requiring deep cultural understanding and contextual awareness. BhashaBench V1 adheres to seven core design principles: (1) Critical Indian Domains: Encompasses Agriculture, Legal systems, Finance, and Ayurveda with fine-grained subfields. (2) Diverse Task Formats: Includes multiple-choice, assertion-reasoning, fill-in-blanks, and comprehension tasks. (3) India-Specific Reasoning: Evaluates domain-specific reasoning incorporating cultural contexts and regional practices. (4) Bilingual Framework: Supports English and Hindi evaluation maintaining cultural authenticity. (5) Authentic Sources: Questions curated from government examinations and professional certifications. (6) Difficulty Assessment: Categorized into Easy, Medium, Hard levels. (7) Cultural Authenticity: Prioritizes traditional knowledge systems including Ayurvedic principles. 1 This framework spans 90+ subdomains covering 500+ topics, enabling comprehensive evaluation of model capabilities in India-centric contexts."
        },
        {
            "title": "3.2 Data Collection",
            "content": "The data collection process for BhashaBench V1 follows systematic approach similar to AGIEVAL [78], focusing on authentic examination materials from national and state-level assessments. We systematically gathered publicly available question papers from official online examination portals, which host previously released papers that are manually curated by subject matter experts, ensuring accurate topic tagging, language annotation, and validated answer keys. Our comprehensive collection encompasses over 40 different examination types across multiple categories: national competitive exams, domain-specific degree examinations, professional certification tests, and state-level civil services examinations. Regional state examinations proved particularly valuable as they incorporate state-specific topics, local knowledge systems, and cultural practices often overlooked in national assessments. These examinations are typically taken by individuals seeking higher education opportunities or career advancement, ensuring questions reflect practical, real-world knowledge requirements. The final dataset comprises 74,166 carefully curated questionanswer pairs spanning four core domains, with 52,494 questions in English (70.8%) and 21,672 questions in Hindi (29.2%), reflecting practical usage patterns in Indian educational and professional contexts. This approach ensures BhashaBench V1 captures the nuanced intersection between language, culture, and domain expertise essential for effective model deployment in Indian contexts."
        },
        {
            "title": "3.3 Data Processing",
            "content": "Our data processing phase focused on extracting structured question-answer pairs from PDF examination papers while preserving linguistic and formatting nuances essential for authentic evaluation. Most examination materials were available exclusively in PDF format, requiring sophisticated OCR processing pipelines to handle multilingual content and domain-specific terminology. OCR Pipeline Selection: Based on existing evaluations [53], Surya OCR demonstrated superior performance in handling Indic languages and domain-specific content. Reported results show 98.1% normalized text similarity for English and 98.9% for Hindi, with an average of 97.8%, outperforming alternatives such as Tesseract (88.0% overall) and Google Vision API (96.7%). Suryas architecture, designed for multilingual document understanding with enhanced Indic script support, makes it suitable choice for diverse examination materials. Question-Answer Extraction Pipeline: Following OCR processing, we developed an extraction pipeline leveraging GPT-OSS-120B [50] to structure raw text into formatted question-answer pairs. Key challenges included format variations across examination bodies, answer key alignment, multi-format questions (MCQ, assertion-reasoning, comprehension), and language-specific formatting conventions. The pipeline included: (1) Question Extraction using GPT-OSS-120B for boundary detection across different layouts; (2) Option Parsing to maintain original labeling 1More collection and processing procedures can be found in Appendix C. 4 Figure 2 Comparative performance of small models (4B) over BhashaBench V1. Figure 3 Comparative performance analysis of the GPT model family on BhashaBench V1. conventions; (3) Answer Key Alignment processing both inline and separate answer documents; and (4) Format Standardization into consistent JSON structure with domain metadata. Data Cleaning and Quality Control: Our multi-layered cleaning approach addressed noise and inconsistencies through systematic filtering. We excluded image-based questions, and questions with more than four options. Language verification used INDICLID [42] and Unicode-based filtering [34] for proper linguistic categorization. Approximately 30% of questions lacked subdomain classification, addressed using GPT-OSS-120B with domain-specific taxonomies. We classified questions into six categories: MCQ, assertion-reasoning, fill-in-the-blanks, match-the-column, reading comprehension, and sequence rearrangement. Duplicate detection employed both exact-match and semantic similarity measures. Manual Validation: Following methodology similar to [8], all extracted question-answer pairs underwent rigorous expert validation to ensure accuracy verification, cultural context preservation, ambiguity resolution, and consistency standardization. Additionally, domain experts reviewed the linguistic authenticity to maintain the natural flow and idiomatic expressions characteristic of each language. This comprehensive multi-stage validation approach ensured that BhashaBench V1 maintains the highest data quality standards while preserving the authentic complexity and cultural specificity of the original examination materials."
        },
        {
            "title": "3.4 Data Analysis",
            "content": "Figure 1 presents the comprehensive statistics of BhashaBench V1. Detailed exposition is provided in Appendix C.2. Of the total 74,166 questions, 70.8% are in English while 29.2% are in Hindi, reflecting practical bilingual usage patterns in Indian professional contexts. The dataset spans four specialized domains with varying complexity levels across 91 subdomains. Agriculture (BBK): This domain encompasses agricultural sciences relevant to Indian farming systems across 25 subdomains. Agronomy dominates with 5,078 questions, reflecting its foundational role in agricultural education. The domain covers traditional practices alongside emerging areas like Agricultural Biotechnology and IT solutions. Its balanced difficulty distribution (44% easy, 45% medium, 11% hard) ensures comprehensive skill assessment. Finance (BBF): Covers Indias complex financial ecosystem through 30 subdomains. Problem Solving leads with 5,686 questions, followed by Mathematics for Finance (4,845), emphasizing the quantitative nature of financial practice. The domain uniquely incorporates India-specific areas like Rural Economics and Environmental Finance while addressing modern fintech developments. Ayurveda (BBA): Represents traditional Indian medicine across 16 subdomains. Kayachikitsa (General Medicine) forms the core with 3,134 questions, while Dravyaguna covers pharmacology and therapeutics (2,972). This domain shows the highest proportion of accessible questions (53% easy), reflecting its foundational knowledge structure. Legal (BBL): Encompasses Indian jurisprudence through 20 subdomains. Civil Litigation & Procedure dominates with 7,126 questions, followed by Constitutional Law (3,609). The domain balances traditional legal areas with contemporary developments like Technology & Cyber Law, maintaining strong cultural relevance through Family & Personal Law. The predominantly MCQ format (>90%) ensures consistent evaluation methodology while supporting diverse cognitive assessment approaches across India-specific knowledge systems."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We evaluate multiple state-of-the-art models on BhashaBench V1, including large proprietary models, open-source multilingual models, and domain-specific fine-tuned variants. Both base versions and instruction fine-tuned models are assessed to measure the effectiveness of specialized training approaches across India-specific knowledge domains. All evaluations are conducted in zero-shot setting to assess the models inherent capabilities without taskspecific examples. For open-source models, we utilize the LM-EVALUATION-HARNESS library [13, 14] to ensure clean, reproducible, and standardized evaluations. We employ the log-likelihood method where the probability of given output string is computed by conditioning it on the provided input [15]. For multiple choice questions with possible answer choices, we select the answer string (ai) with the highest conditional log probability: arg max(log P(a1x), ..., log P(akx)). For closed-source and large-scale proprietary models, we utilize their respective APIs for evaluation due to computational constraints and access limitations. These API-based models are evaluated using generative approach and are prompted to generate responses in structured JSON format to facilitate automated response parsing. This comprehensive experimental framework enables systematic comparison across diverse model architectures while maintaining evaluation consistency across both open-source and proprietary systems. Additional details regarding model specifications, hyperparameters, and computational resources are provided in Appendix D."
        },
        {
            "title": "5 Results and Discussions",
            "content": "In this section, we discuss the results and our findings across all the experiments conducted."
        },
        {
            "title": "5.1 Zero-Shot Performance Across All Domains (EN + HI)",
            "content": "Table 1 shows the performance of various models in English and Hindi under the zero-shot setup. Among these, Qwen3-235B-A22B-Instruct emerges as the strongest model, consistently outperforming all competitors across both languages, with an average accuracy of 67.25%. This is followed by GPT-4O at 66.18% and gpt-oss-120b at 65.41%. Performance shows clear stratification across model sizes and types, with models exceeding 27B parameters demonstrating substantially higher accuracies compared to smaller variants. Among the 7B-27B range, gemma-2-27b leads with 53.11% average accuracy, followed by gemma-2-27b-it at 44.64%. In the mid-range category, gemma-2-9b shows impressive performance at 48.07%, with Pangea-7B achieving 41.54%. Smaller models under 4B parameters show more modest performance, with Qwen2.5-3B achieving the highest accuracy in this category at 39.68%. Models specifically designed for Indian languages include Param-1 (34.69%) and the Nemotron-4-Mini-Hindi variants (36.08% and 34.20%). Performance is notably higher in English compared to Hindi across most models, reflecting the typical pattern observed in multilingual language models, with models showing varying degrees of cross-lingual transfer capabilities."
        },
        {
            "title": "5.2 How do models perform in subdomains",
            "content": "We evaluate representative models across BBA, BBF, BBK, and BBL to capture performance within subdomains (see Figures 4 and 5). Qwen3-235B-A22B-Instruct-2507 achieves the strongest results, excelling in Research & Statistics (91.43%), Agricultural Biotechnology (91.6%), and Intellectual Property Law (87.91%). GPT-4o demonstrates robust performance, frequently scoring above 70% with peaks of 92% in Information Technology and Healthcare & Medical Law. GPT-oss-120b shows competitive performance, closely matching gpt-4o in domains like Agricultural Biotechnology (89.69%). Mid-sized models including Gemma-2-27b and Gemma-2-9b generally show moderate performance in the 5070% range, with the 27B variant consistently outperforming its smaller counterpart. Llama-3.1-8B demonstrates limited performance, typically scoring 3050% across domains. The compact Param-1 model shows consistent baseline performance, often matching Llama-3.1-8B despite requiring significantly fewer resources. Notable patterns 6 Table 1 Zero-shot scores (%) of LLMs across domains on BhashaBench V1 (EN + HI). The benchmark covers Agriculture (BBK), Finance (BBF), Legal (BBL), and Ayurveda (BBA). Avg denotes the overall average across that domain. Model BBA Eng Hin Avg Eng < 4B Models gemma-3-270m gemma-3-270m-it Param-1 gemma-2-2b gemma-2-2b-it Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct sarvam-2b-v0.5 sarvam-1 Nemotron-4-Mini-Hindi-4B-Base Nemotron-4-Mini-Hindi-4B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct granite-3.1-2b-instruct granite-3.1-3b-a800m-base Pangea-7B Indic-gemma-7b-finetuned-sft-Navarasa-2.0 aya-23-8B Llama-3.1-8B Llama-3.1-8B-Instruct gemma-2-9b gemma-2-9b-it gpt-oss-20b gemma-2-27b gemma-2-27b-it gpt-oss-120b Qwen3-235B-A22B-Instruct-25076 deepseek-v3 gpt-4o 28.08 26.23 41.12 36.80 29.38 29.17 26.77 31.62 35.31 26.79 29.70 34.76 33.38 40.61 35.22 33.39 31.75 40.69 37.12 33.84 35.48 36.86 48.16 36.22 38.30 50.70 40.45 55.62 60.25 51.38 62.75 28.25 25.77 38.04 30.61 26.79 26.30 25.82 28.05 29.67 27.07 28.41 32.82 33.82 31.90 28.46 27.30 26.18 31.93 31.83 28.87 29.17 31.26 37.92 31.18 33.09 42.26 33. 48.05 54.78 37.03 54.73 BBF Hin 25.06 23.84 29.56 27.50 27.93 25.61 26.04 27.92 29.09 25.31 27.27 31.41 30.06 32.13 29.17 27.11 24.17 28.14 26.06 39.97 34.48 28.40 28.10 26.41 30.28 33.20 26.89 29.21 34.03 33.54 37.34 32.68 31.10 29.66 24.98 24.13 32.24 34.20 31.26 28.24 26.28 33.04 32.94 26.42 29.66 34.95 31.98 39.54 34.84 32.82 29. 7B to 27B Models 37.41 35.13 31.97 33.12 34.76 44.32 34.33 36.34 47.53 37.99 41.71 37.00 35.25 36.20 35.68 42.73 38.85 37.11 47.79 42.47 > 27B Models 52.78 58.20 45.99 59.74 74.11 63.72 63.46 57. 33.73 30.47 30.88 30.61 30.27 36.91 32.03 32.61 41.24 34.29 64.16 56.27 57.04 49.82 BBK Avg Eng Hin Avg Eng 25.00 24.04 31.42 32.14 30.24 27.43 26.21 31.46 31.76 26.08 28.92 33.86 31.39 37.26 33.09 31.07 27.66 39.25 34.90 33.90 34.48 34.01 40.94 36.75 35.73 45.77 39.95 71.05 61.43 61.48 54.97 26.64 27.44 33.10 41.24 35.94 29.71 29.16 32.68 40.59 28.14 30.82 36.67 35.83 44.57 42.67 37.71 33. 47.16 42.31 37.09 39.52 47.14 55.23 48.92 46.58 59.84 54.95 71.40 74.57 62.93 75.31 24.45 25.35 27.97 27.49 27.71 25.21 26.33 28.69 29.09 25.57 27.57 36.49 35.33 32.72 27.20 27.86 26.70 34.71 33.44 33.22 31.41 35.07 43.89 36.45 36.27 50.38 41.24 60.25 64.13 45.01 65.18 26.24 27.06 32.18 38.78 34.47 28.91 28.65 31.96 38.53 27.68 30.24 36.64 35.74 42.45 39.90 35.95 32. 44.93 40.73 36.40 38.07 44.98 53.20 46.69 44.73 58.14 52.50 69.41 72.70 59.73 73.50 25.49 25.56 36.15 38.45 34.49 29.63 29.08 35.17 39.74 28.49 30.92 40.75 36.99 44.98 40.62 38.18 33.74 48.70 44.08 41.92 41.32 48.61 58.49 45.05 40.69 64.91 50.71 70.72 80.15 67.78 78.83 BBL Hin 25.54 27.26 32.89 29.61 30.25 25.88 27.04 28.53 30.13 25.95 26.66 37.55 34.11 33.97 29.89 27.30 24.01 34.95 34.09 33.01 31.76 36.47 42.96 38.66 35.24 51.83 42.02 62.94 68.60 46.78 71.02 Avg 25.51 26.07 35.17 35.79 33.22 28.52 28.47 33.17 36.86 27.72 29.64 39.79 36.12 41.67 37.39 34.91 30. 44.57 41.08 39.24 38.44 44.96 53.83 43.13 39.06 60.99 48.10 68.38 76.68 61.47 76.49 emerge: Finance and Legal domains show the highest performance ceiling, with top models regularly exceeding 80% in Business Management and Constitutional Law. Agricultural domains present moderate complexity, while Ayurveda proves most challenging, with even the best models rarely exceeding 80% in specialized areas like Panchakarma. Results highlight clear advantages for large models in knowledge-intensive tasks, while smaller models provide practical utility in resource-constrained scenarios for general applications."
        },
        {
            "title": "5.3 Performance Analysis Across Question Difficulty Levels",
            "content": "We evaluated model performance on Easy, Medium, and Hard questions across the four benchmark domains BBA, BBF, BBK, and BBL. In BBA, top-performing models such as GPT-4o and Qwen3-235B-A22B-Instruct-2507 achieved 66.4% and 65.18% on Easy questions, and 47.09% and 46.24% on Hard questions, while smaller models like gemma3-270m scored 28.1% on Easy and 26.81% on Hard. similar trend is observed in BBF, with Easy question scores ranging from 24.15% (gemma-3-270m) to 74.8% (gpt-oss-120b) and Hard questions from 21.22% to 62.61%. Mediumlevel questions show moderate differentiation, reflecting model reasoning capability. BBK and BBL follow the same pattern, with instruction-tuned and larger models consistently outperforming smaller models, particularly on Hard questions. Overall, model size, instruction tuning, and architecture significantly influence robustness to question difficulty and generalization across domains. See Appendix E.1. 7 Figure 4 Comparison of representative LLMs scores across different domains and subdomains. Figure 5 Comparison of representative LLMs scores across different domains and subdomains."
        },
        {
            "title": "5.4 Performance Analysis Across Question Types",
            "content": "We analyzed model performance on various question types including Assertion/Reasoning, Fill in the Blanks, MCQs, Match the Column, Reading Comprehension, and Rearrange the Sequence across the BBA, BBF, BBK, and BBL domains. In BBA, models like deepseek-v3 and GPT-4o achieved high scores of 66.67% and 62.96% on Assertion/Reasoning questions, whereas smaller models such as gemma-3-270m scored 28.09%. For Fill in the Blanks, scores ranged from 24.72% (gemma-3-270m-it) to 51.69% (Qwen3-235B-A22B-Instruct-2507). MCQ performance was moderate, between 26% and 59.95%. Match the Column and Reading Comprehension showed wider variation, with larger models consistently outperforming smaller or non-instruction-tuned models. Rearrange the Sequence proved challenging across domains, with top models reaching 71.43% in BBL. Overall, question type significantly affects performance, highlighting the importance of model size, instruction tuning, and reasoning capabilities in handling diverse formats."
        },
        {
            "title": "5.5 Performance Analysis of GPT Model Family",
            "content": "We evaluate the GPT model family across BBA, BBF, BBK, and BBL domains to understand scaling and architectural strengths (Figure 3). gpt-oss-20b demonstrates baseline performance with scores of 36.34% (BBA), 35.73% (BBF), 44.73% (BBK), and 39.06% (BBL). Scaling to gpt-oss-120b yields substantial improvements: 52.78% in BBA, 71.05% in BBF, 69.41% in BBK, and 68.38% in BBL, representing 16-35 percentage point gains. Despite gpt-4os larger parameter count, gpt-oss-120b significantly outperforms it in Finance (71.05% vs 54.97%), likely due to BBFs mathematical reasoning emphasis where gpt-oss-120bs training methodology excels [2]. Conversely, gpt-4o shows superior performance in Legal (76.49%) and Agriculture (73.5%) domains. This highlights that parameter size [7] alone doesnt guarantee performance; architectural choices and training approaches significantly influence domain-specific capabilities, with mathematical tasks favoring specific optimizations over raw parameter scaling."
        },
        {
            "title": "5.6 Performance Analysis of Small Models",
            "content": "We evaluate small models (4B parameters) across BBA, BBF, BBK, and BBL domains to assess efficiency-performance trade-offs (Figure 2). Param-1 and Qwen2.5-3B emerge as comparable top performers, with Param-1 achieving 39.97% in BBA while Qwen2.5-3B excels in BBK (42.45%). Both models demonstrate complementary strengths: Param-1 performs better in Ayurveda, while Qwen2.5-3B shows superior performance in Finance, Agriculture, and Legal domains. Instruction tuning effects vary significantly across architectures: Llama-3.2-3B-Instruct substantially outperforms its base version, whereas Qwen2.5-3B-Instruct shows mixed results. Nemotron-4-Mini-Hindi models achieve competitive performance in the 34-40% range, while the smallest models like gemma-3-270m struggle consistently below 28%. Results indicate that architectural efficiency and targeted optimization can achieve reasonable performance in resource-constrained scenarios, with Param-1 and Qwen2.5 leading the small model category through different domain specializations."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced BhashaBench V1, comprehensive, domain-specific, bilingual benchmark designed to evaluate large language models on India-centric knowledge systems across four critical domains: Agriculture (BBK), Legal (BBL), Finance (BBF), and Ayurveda (BBA). Our benchmark addresses significant gaps in existing evaluation frameworks by focusing on culturally relevant, domain-specific knowledge spanning over 90 subdomains and 500+ specialized topics curated from authentic government and professional examination papers. Our extensive evaluation reveals substantial performance disparities in current LLMs when applied to India-specific contexts, with models excelling in Legal contexts while struggling with traditional knowledge systems like Ayurveda and consistently performing better on English content compared to Hindi across all domains. These results highlight the urgent need for specialized model development strategies that incorporate India-specific knowledge, cultural contexts, and robust multilingual capabilities. To foster open research and accelerate progress toward more inclusive, culturally aware language models, we release BhashaBench V1 alongside all evaluation code and comprehensive documentation. We believe BhashaBench V1 offers foundational benchmark for developing culturally sensitive models that effectively serve Indias diverse linguistic and knowledge landscape."
        },
        {
            "title": "References",
            "content": "[1] Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. Megaverse: Benchmarking large language models across languages, modalities, models and tasks, 2024. URL https://arxiv.org/abs/2311.07463. [2] Artificial Analysis. gpt-oss-120b (high) vs gpt-4o: Model comparison. https://artificialanalysis.ai/models/ comparisons/gpt-oss-120b-vs-gpt-4o, 2025. Accessed: 2025-09-10. [3] AnhaltAI. Bvl q&a corpus 2024: German agricultural question-answer dataset. https://huggingface.co/ datasets/anhaltai/bvl-qa-corpus-2024, 2024. Accessed September 2025, Non-commercial use license. [4] Anthropic. Introducing the next generation of claude. https://www.anthropic.com/news/claude-3-family, March 2024. 9 [5] Rakshit Aralimatti, Syed Abdul Gaffar Shakhadri, Kruthika KR, and Kartik Basavaraj Angadi. Fine-tuning small language models for domain-specific ai: An edge ai perspective, 2025. URL https://arxiv.org/abs/2503.01933. [6] Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, and Soumik Sarkar. Leveraging vision language models for specialized agricultural tasks, 2025. URL https://arxiv.org/abs/2407.19617. [7] Sushant Babbar. Openai gpt-oss vs gpt-4o: Which one is better?, August 7 2025. URL https://blog.getbind.co/ 2025/08/07/openai-gpt-oss-vs-gpt-4o-which-one-is-better/. Accessed: October 30, 2025. [8] Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 749775. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.acl-long.44. URL http://dx.doi.org/10.18653/v1/2024.acl-long.44. [9] BharatGenAI. Agriparam, 2025. URL https://huggingface.co/bharatgenai/AgriParam. Accessed: September 2025. [10] BharatGenAI. Ayurparam, 2025. URL https://huggingface.co/bharatgenai/AyurParam. Accessed: September 2025. [11] BharatGenAI. Financeparam, 2025. URL https://huggingface.co/bharatgenai/FinanceParam. Accessed: September 2025. [12] BharatGenAI. Legalparam, 2025. URL https://huggingface.co/bharatgenai/LegalParam. Accessed: September 2025. [13] Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, François Yvon, and Andy Zou. Lessons from the trenches on reproducible evaluation of language models, 2024. URL https: //arxiv.org/abs/2405.14782. [14] Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, François Yvon, and Andy Zou. Lessons from the trenches on reproducible evaluation of language models, 2024. URL https: //arxiv.org/abs/2405.14782. [15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. [16] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. URL https://arxiv.org/abs/2303.12712. [17] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. Legal-bert: The muppets straight out of law school, 2020. URL https://arxiv.org/abs/2010.02559. [18] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL https://arxiv.org/abs/2403.04132. [19] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi 10 Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. [20] Jay Gala, Pranjal A. Chitale, Raghavan AK, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, and Anoop Kunchukuttan. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages, 2023. URL https://arxiv.org/abs/2305.16307. [21] Jay Gala, Thanmay Jayakumar, Jaavid Aktar Husain, Aswanth Kumar M, Mohammed Safi Ur Rahman Khan, Diptesh Kanojia, Ratish Puduppully, Mitesh M. Khapra, Raj Dabre, Rudra Murthy, and Anoop Kunchukuttan. Airavata: Introducing hindi instruction-tuned llm, 2024. URL https://arxiv.org/abs/2401.15006. [22] Google. Introducing gemini: our largest and most capable ai model. https://blog.google/technology/ai/ google-gemini-ai/, December 2023. [23] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres 11 Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [24] Neel Guha, Julian Nyarko, Daniel E. Ho, Christopher Ré, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N. Rockmore, et al. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models, 2023. URL https://arxiv.org/abs/2308.11462. [25] Jinglin He, Yunqi Guo, Lai Kwan Lam, Waikei Leung, Lixing He, Yuanan Jiang, Chi Chiu Wang, Guoliang Xing, and Hongkai Chen. Opentcm: graphrag-empowered llm-based system for traditional chinese medicine knowledge retrieval and diagnosis, 2025. URL https://arxiv.org/abs/2504.20118. [26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/2009.03300. [27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/2009.03300. [28] IndiaDataMap. Projected 2025: Number of farmers in every indian state, 2025. URL https://indiadatamap.com/ 2025/08/19/number-of-farmers-in-every-indian-state-2025/. [29] Indian Knowledge Systems Division, Ministry of Education, Government of India. Indian knowledge systems, 2025. URL https://iksindia.org/. Official website of the IKS Division under Ministry of Education at AICTE, New Delhi. [30] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering, 2023. URL https://arxiv.org/abs/2311.11944. [31] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. [32] Abhinav Joshi, Shounak Paul, Akshat Sharma, Pawan Goyal, Saptarshi Ghosh, and Ashutosh Modi. Il-tur: Benchmark for indian legal text understanding and reasoning, 2024. URL https://arxiv.org/abs/2407.05399. [33] Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational doi: Kumar. Indian languages. Linguistics: EMNLP 2020, pp. 49484961, Online, November 2020. Association for Computational Linguistics. 10.18653/v1/2020.findings-emnlp.445. URL https://aclanthology.org/2020.findings-emnlp.445/. [34] Mohammed Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad B, Varun G, Sparsh Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, and Mitesh Khapra. Indicllmsuite: blueprint for creating pre-training and fine-tuning datasets for indian languages. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1583115879. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.acl-long.843. URL http://dx.doi.org/10.18653/v1/2024.acl-long.843. [35] Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali, Vish Subramanian, and Partha Talukdar. Muril: Multilingual representations for indian languages, 2021. URL https://arxiv.org/abs/2103.10730. [36] Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali, Vish Subramanian, and Partha P. Talukdar. Muril: Multilingual representations for indian languages. CoRR, abs/2103.10730, 2021. URL https://arxiv. org/abs/2103.10730. [37] Shufeng Kong, Xingru Yang, Yuanyuan Wei, Zijie Wang, Hao Tang, Jiuqi Qin, Shuting Lan, Yingheng Wang, Junwen Bai, Zhuangbin Chen, Zibin Zheng, Caihua Liu, and Hao Liang. Mtcmb: multi-task benchmark framework for evaluating llms on knowledge, reasoning, and safety in traditional chinese medicine, 2025. URL https://arxiv.org/abs/2506.01252. [38] Josué Kpodo, Parisa Kordjamshidi, and A. Pouyan Nejadhashemi. Agxqa: benchmark for advanced agricultural extension question answering. Computers and Electronics in Agriculture, 225:109349, 2024. ISSN 0168-1699. doi: https: //doi.org/10.1016/j.compag.2024.109349. URL https://www.sciencedirect.com/science/article/pii/ S0168169924007403. [39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https: //arxiv.org/abs/2309.06180. [40] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240, doi: 10.1093/bioinformatics/btz682. URL http://dx.doi.org/10.1093/ September 2019. bioinformatics/btz682. ISSN 1367-4811. [41] Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, and Jordan W. Suchow. Investorbench: benchmark for financial decision-making tasks with llm-based agent, 2024. URL https://arxiv.org/ abs/2412.18174. [42] Yash Madhani, Mitesh M. Khapra, and Anoop Kunchukuttan. Bhasha-abhijnaanam: Native-script and romanized language In Findings of the Association for Computational Linguistics: ACL 2023, 2023. identification for 22 indian languages. URL https://github.com/AI4Bharat/IndicLID. Models and datasets available at https://github.com/ AI4Bharat/IndicLID and https://huggingface.co/datasets/ai4bharat/Bhasha-Abhijnaanam. [43] Arijit Nag, Soumen Chakrabarti, Animesh Mukherjee, and Niloy Ganguly. Efficient continual pre-training of llms for lowresource languages, 2024. URL https://arxiv.org/abs/2412.10244. [44] National Judicial Data Grid (NJDG), India. National judicial data grid - statistics 2023. https://njdg.ecourts.gov. in/, 2023. Accessed September 2025. [45] National Payments Corporation of India. Npci upi transaction statistics 2023. https://www.npci.org.in/, 2023. Accessed September 2025. 13 [46] Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and Lidong Bing. Seallms large language models for southeast asia, 2024. URL https://arxiv.org/abs/2312.00738. [47] Shubham Kumar Nigam, Shubham Kumar Mishra, Ayush Kumar Mishra, Noel Shallum, and Arnab Bhattacharya. Legal question-answering in the indian context: Efficacy, challenges, and potential of modern ai models, 2023. URL https: //arxiv.org/abs/2309.14735. [48] Joel Niklaus, Veton Matoshi, Pooja Rani, Andrea Galassi, Matthias Stürmer, and Ilias Chalkidis. Lextreme: multi-lingual In Findings of the Association for Computational Linguistics: EMNLP and multi-task benchmark for the legal domain. 2023, pp. 30163054. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.findings-emnlp.200. URL http://dx.doi.org/10.18653/v1/2023.findings-emnlp.200. [49] OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. [50] OpenAI. Introducing gpt-oss: Open-weight reasoning models from openai. https://openai.com/index/ introducing-gpt-oss/, 2024. Accessed September 2025. [51] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. [52] OpenAI, :, Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K. Arora, Yu Bai, Bowen Baker, Haiming Bao, Boaz Barak, Ally Bennett, Tyler Bertao, Nivedita Brett, Eugene Brevdo, Greg Brockman, Sebastien Bubeck, Che Chang, Kai Chen, Mark Chen, Enoch Cheung, Aidan Clark, Dan Cook, Marat Dukhan, Casey Dvorak, Kevin Fives, Vlad Fomenko, Timur Garipov, Kristian Georgiev, Mia Glaese, Tarun Gogineni, Adam Goucher, Lukas Gross, Katia Gil Guzman, John Hallman, Jackie Hehir, Johannes Heidecke, Alec Helyar, Haitang Hu, Romain Huet, Jacob Huh, Saachi Jain, Zach Johnson, Chris Koch, Irina Kofman, Dominik Kundel, Jason Kwon, Volodymyr Kyrylov, Elaine Ya Le, Guillaume 14 Leclerc, James Park Lennon, Scott Lessans, Mario Lezcano-Casado, Yuanzhi Li, Zhuohan Li, Ji Lin, Jordan Liss, Lily, Liu, Jiancheng Liu, Kevin Lu, Chris Lu, Zoran Martinovic, Lindsay McCallum, Josh McGrath, Scott McKinney, Aidan McLaughlin, Song Mei, Steve Mostovoy, Tong Mu, Gideon Myles, Alexander Neitz, Alex Nichol, Jakub Pachocki, Alex Paino, Dana Palmie, Ashley Pantuliano, Giambattista Parascandolo, Jongsoo Park, Leher Pathak, Carolina Paz, Ludovic Peran, Dmitry Pimenov, Michelle Pokrass, Elizabeth Proehl, Huida Qiu, Gaby Raila, Filippo Raso, Hongyu Ren, Kimmy Richardson, David Robinson, Bob Rotsted, Hadi Salman, Suvansh Sanjeev, Max Schwarzer, D. Sculley, Harshit Sikchi, Kendal Simon, Karan Singhal, Yang Song, Dane Stuckey, Zhiqing Sun, Philippe Tillet, Sam Toizer, Foivos Tsimpourlas, Nikhil Vyas, Eric Wallace, Xin Wang, Miles Wang, Olivia Watkins, Kevin Weil, Amy Wendling, Kevin Whinnery, Cedric Whitney, Hannah Wong, Lin Yang, Yu Yang, Michihiro Yasunaga, Kristen Ying, Wojciech Zaremba, Wenting Zhan, Cyril Zhang, Brian Zhang, Eddie Zhang, and Shengjia Zhao. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. [53] Vikram Paruchuri and DataLab Team. Surya: multilingual document ocr toolkit for indic and other languages. https: //github.com/datalab-to/surya, 2024. Software available at https://github.com/datalab-to/surya. [54] Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Guojun Xiong, Zhiyang Deng, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, and Qianqian Xie. Multifinben: multilingual, multimodal, and difficulty-aware benchmark for financial llm evaluation, 2025. URL https://arxiv.org/abs/2506.14028. [55] Kundeshwar Pundalik, Piyush Sawarkar, Nihar Sahoo, Abhishek Shinde, Prateek Chanda, Vedant Goswami, Ajay Nagpal, Atul Singh, Viraj Thakur, Vijay Dewane, Aamod Thakur, Bhargav Patel, Smita Gautam, Bhagwan Panditi, Shyam Pawar, Madhav Kotcha, Suraj Racha, Saral Sureka, Pankaj Singh, Rishi Bal, Rohit Saluja, and Ganesh Ramakrishnan. Param-1 bharatgen 2.9b model, 2025. URL https://arxiv.org/abs/2507.13390. [56] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412. 15115. [57] Shriram Shivajirao Ragad and Maya Vivek Gokhale. Ayurvedic concept of koshtha and its importance in panchkarma. International Journal of Research - Granthaalayah, 7(7):416421, 2019. doi: 10.5281/zenodo.3370488. URL https: //doi.org/10.5281/zenodo.3370488. [58] Anjali Sen and et al. Morphological understanding and retrieval in indic languages. Journal of Indic Computational Linguistics, 12(3):4567, 2023. [59] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather ColeLewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023. URL https: //arxiv.org/abs/2305.09617. [60] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke 15 Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023. URL https://arxiv.org/abs/2206.04615. [61] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, GeorgeChristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao 16 Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295. [62] Hrishikesh Terdalkar, Vishakha Deulgaonkar, and Arnab Bhattacharya. Ayurjñanam: Exploring Ayurveda using knowledge graphs, 2023. URL https://sanskrit.iitk.ac.in/ayurveda/. Presented at the National Youth Conference on Indian Knowledge Systems 2023. [63] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. [64] Abhay Vijayvargia, Ajay Nagpal, Kundeshwar Pundalik, Atharva Savarkar, Smita Gautam, Pankaj Singh, Rohit Saluja, and Intent aware context retrieval for multi-turn agricultural question answering, 2025. URL https: Ganesh Ramakrishnan. //arxiv.org/abs/2508.03719. [65] Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, Yihong Chen, Raphael Tang, and Pontus Stenetorp. Multilingual pretraining using large corpus machine-translated from single source language, 2024. URL https://arxiv.org/abs/ 2410.23956. [66] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. URL https://arxiv.org/abs/2406. 01574. [67] Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim Jyoti Das, and Preslav Nakov. Factuality of large language models: survey. arXiv preprint arXiv:2402.02420, 2024. doi: 10.48550/arXiv.2402.02420. URL https://arxiv.org/abs/2402.02420. [68] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. URL https://arxiv.org/abs/2206.07682. [69] Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, and Pascale Fung. Language models are few-shot multilingual learners, 2021. URL https://arxiv.org/abs/2109.07684. [70] Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, and Jianfeng Xu. Cail2018: large-scale legal dataset for judgment prediction, 2018. URL https: //arxiv.org/abs/1807.02478. [71] Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang, Xianpei Han, Zhen Hu, Heng Wang, and Jianfeng Xu. Cail2019-scm: dataset of similar case matching in legal domain, 2019. URL https: //arxiv.org/abs/1911.08962. [72] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [73] Yi Yang, Mark Christopher Siy UY, and Allen Huang. Finbert: pretrained language model for financial communications, 2020. URL https://arxiv.org/abs/2006.08097. [74] Jie Ying, Zihong Chen, Zhefan Wang, Wanli Jiang, Chenyang Wang, Zhonghang Yuan, Haoyang Su, Huanjun Kong, Fan Yang, and Nanqing Dong. Seedbench: multi-task benchmark for evaluating large language models in seed science, 2025. URL https://arxiv.org/abs/2505.13220. [75] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. [76] Lingfeng Zeng, Fangqi Lou, Zixuan Wang, Jiajie Xu, Jinyi Niu, Mengping Li, Yifan Dong, Qi Qi, Wei Zhang, Ziwei Yang, Jun Han, Ruilun Feng, Ruiqi Hu, Lejie Zhang, Zhengbo Feng, Yicheng Ren, Xin Guo, Zhaowei Liu, Dongpo Cheng, Weige Cai, and Liwen Zhang. Fingaia: chinese benchmark for ai agents in real-world financial domain, 2025. URL https: //arxiv.org/abs/2507.17186. [77] Tianyang Zhong, Zhenyuan Yang, Zhengliang Liu, Ruidong Zhang, Yiheng Liu, Haiyang Sun, Yi Pan, Yiwei Li, Yifan Zhou, Hanqi Jiang, Junhao Chen, and Tianming Liu. Opportunities and challenges of large language models for low-resource languages in humanities research, 2025. URL https://arxiv.org/abs/2412.04497. [78] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023. URL https://arxiv.org/abs/2304. 06364. [79] Yunshun Zhong and Sebastian D. Goodfellow. Domain-specific language models pre-trained on construction management systems corpora. Automation in Construction, 160:105316, 2024. ISSN 0926-5805. doi: https://doi.org/10.1016/j.autcon. 2024.105316. URL https://www.sciencedirect.com/science/article/pii/S0926580524000529. [80] Yutong Zhou and Masahiro Ryo. Agribench: hierarchical agriculture benchmark for multimodal large language models, 2024. URL https://arxiv.org/abs/2412.00465. [81] Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An instruction finetuned open-access multilingual language model, 2024. URL https://arxiv.org/abs/2402.07827."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank BharatGen for their generous support towards building this comprehensive benchmarking initiative for Indian languages and knowledge systems. We are also immensely grateful to our colleagues from the BharatGen team for their motivation and meticulous efforts in conducting manual validation and data sourcing. We acknowledge the significant contributions of BharatGens consortium members, including IIT Bombay and IIM Indore, for their expertise in data curation, validation, and domain-specific guidance that made this benchmark possible."
        },
        {
            "title": "B Towards Broader Impact",
            "content": "C More Details on BhashaBench V1 C.1 Details of Data Collection and Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.1 Examination Source Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.2 Processing Pipeline Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.3 Annotation Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.4 Data Processing Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Detailed Data Analysis of BhashaBench V1 . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D More Details on Experiment Setup",
            "content": "D.1 Task Formatting Template Used in LM Eval . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Task Formatting Template Used in API-Driven Evaluation . . . . . . . . . . . . . . . . . . . D.3 Details of Inference Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E More Details on Experiment",
            "content": "E.1 Zero-Shot Question-Level and Question-Type Performance Across BhashaBench V1 Domains . . . . . . . . . . E.2 Zero-Shot sub-domain wise Performance Across BhashaBench V1 Domains 1 3 3 3 4 4 4 4 5 6 6 6 6 7 8 9 9 20 20 21 21 21 22 25 26 36 38 38 39 39 39"
        },
        {
            "title": "A Limitations and Biases",
            "content": "In this paper, we introduce BhashaBench V1, providing comprehensive evaluation of LLMs on India-centric knowledge systems and exploring model capabilities across critical Indian domains. However, there are several limitations to acknowledge. (1) Language Coverage Limitations: Although BhashaBench V1 supports English and Hindi, covering significant portion of Indias population, India has 22 official languages and hundreds of regional dialects. Our current evaluation cannot capture the full linguistic diversity of Indian knowledge systems, particularly regional variations in agricultural practices, legal terminologies, and traditional medicine nomenclature that exist in languages like Tamil, Telugu, Bengali, and others. Future iterations will expand to include additional Indian languages to enhance coverage. (2) Domain Scope Limitations: While we cover four fundamental domains (Agriculture, Legal, Finance, and Ayurveda) representing core areas of Indian society, our assessment cannot encompass the entire breadth of India-specific knowledge systems. Areas such as traditional crafts, regional governance systems, indigenous engineering practices, and other vernacular knowledge traditions remain unexplored for future expansion. Our content spans from grassroots practical knowledge to professional examination standards, ensuring broad applicability across different expertise levels. (3) Evaluation Methodology Limitations: Our evaluation primarily uses structured question formats derived from authentic government and professional examinations. While this ensures real-world relevance and practical applicability, it may not fully capture all forms of contextual reasoning required in complex domain applications. The main biases in BhashaBench V1 can be categorized into three aspects: (1) Source Material Bias: Despite comprehensive curation from diverse authentic sources spanning grassroots to professional levels, certain regional practices and emerging contemporary developments may be underrepresented. (2) Language Resource Bias: The benchmark reflects the inherent resource disparity between English and Hindi, where Hindi content, while substantial, represents relatively lower-resource context compared to English. (3) Examination Framework Bias: Our reliance on established examination systems, while ensuring authenticity, may introduce institutional perspectives present in the original assessment frameworks. However, our extensive coverage across 90+ subdomains and 500+ topics from diverse sources mitigates this bias significantly. The impact of these limitations on LLM evaluation includes clear performance distinctions between models across domains and languages, as evidenced by the substantial score variations from 34.28% to 76.49%, demonstrating BhashaBench V1s effectiveness in distinguishing LLM capabilities while presenting meaningful challenges even for top-performing models in India-specific contexts."
        },
        {
            "title": "B Towards Broader Impact",
            "content": "Societal Impact. BhashaBench V1 is anticipated to play transformative role in bridging the digital divide for India-centric knowledge systems. LLMs trained and evaluated with BhashaBench V1 can significantly enhance accessibility to critical domain expertise across agriculture, legal services, finance, and traditional medicine, particularly benefiting underserved rural and semi-urban populations. In agriculture, improved LLM capabilities can democratize access to expert crop advisory, pest management, and sustainable farming practices, potentially impacting the livelihoods of over 40 million farmers dependent on agricultural activities. In the legal domain, enhanced models can assist with legal document comprehension, procedural guidance, and basic legal literacy, addressing the substantial access-to-justice challenges faced by millions in Indias complex legal system. For healthcare, particularly Ayurveda, better model performance can support practitioners and patients in understanding traditional treatment protocols and medicinal formulations, preserving and disseminating indigenous medical knowledge. In finance, improved model capabilities can enhance financial literacy and support the growing digital payment ecosystem processing billions of transactions annually. However, we acknowledge potential risks including over-reliance on automated systems for critical decisions, potential displacement of traditional knowledge practitioners, and the risk of perpetuating biases present in examination-based evaluation systems. The benchmarks focus on professional examination standards, while ensuring quality, may inadvertently favor formal educational backgrounds over experiential knowledge. Ethics Statement. We ensure strict adherence to applicable laws and ethical guidelines throughout our data collection, curation, and usage processes. All question-answer pairs are sourced exclusively from publicly available government and professional examination papers, respecting intellectual property rights and ensuring no unauthorized reproduction of copyrighted materials. Our curation process involved diverse teams to minimize cultural and regional biases, though we acknowledge the inherent limitations of our current English and Hindi coverage. The dataset contains no personally identifiable information, offensive content, or culturally insensitive material. All content has been thoroughly verified for authenticity and accuracy through multiple validation rounds involving domain experts. BhashaBench V1 20 is intended solely for academic research and educational purposes to advance inclusive AI development for Indian contexts. Any commercial use, misuse for harmful applications, or deployment without appropriate safeguards is strictly prohibited. We strongly urge all users to employ this resource responsibly, ensuring that any models developed or evaluated using BhashaBench V1 are deployed with appropriate human oversight, particularly in critical domains affecting public welfare, and with transparent disclosure of model limitations to end users. More Details on BhashaBench V1 C.1 Details of Data Collection and Processing This appendix provides comprehensive details on the data collection and processing methodology employed in BhashaBench V1, including systematic documentation of examination sources, processing pipelines, and quality validation procedures. C.1.1 Examination Source Documentation Our data collection strategy encompassed wide range of authoritative examination bodies across India, ensuring comprehensive coverage of national and regional assessment standards. Table 2 presents the complete list of examination organizations and the corresponding years from which question papers were collected. We systematically gathered question papers from official examination portals that host previously released materials, manually curated by subject matter experts with accurate topic tagging, language annotation, and validated answer keys. The temporal distribution of collected materials spans from 1995 to 2025, capturing evolving educational standards and assessment patterns while maintaining contemporary relevance. Table 3 provides detailed breakdown of specific examination types and their collection timeline, demonstrating the breadth and depth of our data sourcing strategy. Our collection process prioritized authentic examination materials from competitive examinations that directly assess knowledge in our target domains of Agriculture, Legal, Finance, and Ayurveda. Regional state examinations proved particularly valuable as they incorporate state-specific topics, local knowledge systems, and cultural practices often overlooked in national assessments. These examinations are typically taken by individuals seeking higher education opportunities or career advancement in business, finance, and legal sectors, ensuring questions reflect practical, real-world knowledge requirements essential for professional contexts in India. Table 2 Organizations and Their Examination Year Ranges"
        },
        {
            "title": "Organization",
            "content": "AIACAT (Private conducting body) Acharya N.G. Ranga Agricultural University (ANGRAU) Agricultural Scientists Recruitment Board (ASRB) All India Management Association (AIMA) Banaras Hindu University (BHU) Bank of Baroda Bank of India Bank of Maharashtra Bar Council of India (BCI) Bihar Public Service Commission (BPSC) Chhattisgarh Professional Examination Board (CG Vyapam) Consortium of National Law Universities (NLUs) ECGC Ltd. Employees Provident Fund Organisation (EPFO) Food Corporation of India (FCI) High Court of Delhi High Court/PSC (state-specific) ICMAB (as per exam title)"
        },
        {
            "title": "Year Range",
            "content": "20222023 20162024 20132024 20182025 20132017 20052023 2023 2021 20092021 19952024 20132019 20212025 20212022 20192023 2015 20112023 20012021"
        },
        {
            "title": "Continued on next page",
            "content": "21 Table 2 Continued from previous page"
        },
        {
            "title": "Organization",
            "content": "IDBI Bank Indian Council of Agricultural Research (ICAR) Indian Farmers Fertiliser Cooperative Limited (IFFCO) Indian Institutes of Management (IIMs) Institute of Banking Personnel Selection (IBPS) JNTU Kakinada on behalf of APSCHE Law School Admission Council (LSAC Global) MP Professional Examination Board (MPPEB/PEB) Maharashtra Agricultural Universities Examination Board (MAUEB) under MCAER Maharashtra Public Service Commission (MPSC) Narendra Deva University of Agriculture & Technology National Bank for Agriculture and Rural Development (NABARD) National Law University, Delhi (NLU Delhi) National Testing Agency (NTA) Reserve Bank of India (RBI) RVSKVV & JNKVV Small Industries Development Bank of India (SIDBI) State Bank of India (SBI) State Common Entrance Test Cell, Maharashtra SVKMs NMIMS The Institute of Chartered Accountants of India (ICAI) The Institute of Cost Accountants of India (ICMAI) The Nainital Bank Ltd. Union Public Service Commission (UPSC) University of Delhi University-specific (varies) Uttar Pradesh Public Service Commission (UPPSC)"
        },
        {
            "title": "Year Range",
            "content": "20142022 20172023 20192022 20172024 20162024 20122025 20102019 20162024 2024 20102025 20242025 20182023 20162025 20192025 20152025 2022 20162023 20182025 20142020 20192025 20182025 20222025 20192020 20022025 20152019 20202024 20192025 C.1.2 Processing Pipeline Architecture The comprehensive end-to-end pipeline developed for transforming raw examination materials into the structured BhashaBench V1 dataset incorporates multiple quality control checkpoints and validation stages to ensure data integrity and authenticity. The pipeline consists of seven major stages, each designed to address specific challenges encountered in multilingual examination material processing. Table 3 Examination Names and Their Year Ranges"
        },
        {
            "title": "Examination Name",
            "content": "AGRICET AIACAT - All India Agriculture Common Aptitude Test AIAPGET - All India AYUSH Post Graduate Entrance Test (Ayurveda) All India Bar Examination (AIBE) All India Law Entrance Test (AILET) Andhra Pradesh Judicial Service (Prelims) AP EAMCET ASRB NET Agriculture BHU PET BHU PG BHU RET"
        },
        {
            "title": "Year Range",
            "content": "20162024 20222023 20222025 20092021 20162025 2012 20122025 20132024 2017 20132017"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Year Range",
            "content": "Table 3 Continued from previous page BHU UET BPSC Bank of Baroda Bank of India Bank of Maharashtra CAT CG PAT Agriculture CMA CMAT Common Law Admission Test (CLAT) CUET Agriculture Previous Year Papers CUET PG (Law) Delhi Judicial Service DU LL.B Entrance ECGC PO EPFO Assistant EPFO SSA EPFO Stenographer FCI Agriculture Haryana Judicial Service (Prelims) Himachal Pradesh Judicial Service (Prelims) IBPS AFO Agriculture Field Officer IBPS AFO Mains IBPS Clerk IBPS PO IBPS RRB Officer Scale-I (merged) IBPS SO ICAI Final ICAI Foundation ICAI Intermediate ICAR AICE JRF/SRF (PHD) Agriculture ICAR AIEEA (PG) Agriculture ICAR AIEEA (UG) Agriculture ICMAB New Syllabus ICMAB Old Syllabus IDBI Assistant Manager IDBI Executive IFFCO AGT - Agriculture Graduate Trainee IPMAT Jharkhand Judicial Service (Prelims) JNKVV & RVSKVV Joint Entrance (M.Sc./Ph.D.) Karnataka Judicial Service (Prelims) LL.B. Admission Test LL.M. Admission Test LSAT - India Madhya Pradesh Judicial Service (Prelims) Maharashtra Judicial Service (Prelims) MAT MCAER-CET MH CET Law (3-year LL.B.) MH-CET 20162017 19952024 20052023 2023 2021 20172024 20132019 20222025 20222025 20212025 20222025 20232025 20112023 20152019 20212022 2019 20192023 2023 2015 20152021 20072019 20162024 20172023 20232024 20182024 20182024 2019 20182025 20182025 20182025 20202024 20192024 20172023 20162022 20162021 2021 20142022 20192022 20192023 20082019 2022 2012 20222024 20202024 20102019 20012018 20102019 20182025 2024 20162019"
        },
        {
            "title": "Continued on next page",
            "content": "23 Table 3 Continued from previous page"
        },
        {
            "title": "Examination Name",
            "content": "MP PAT Agriculture MPSC NABARD Agriculture Development Officer Nainital Bank Clerk Nainital Bank PO NPAT Odisha Judicial Service (Prelims) Rajasthan Judicial Service (Prelims) RBI Grade SBI Apprentice SBI CBO SBI Clerk SBI PO SIDBI Grade TANCET TG ICET (TS ICET) UGC NET (Law) UPCATET UPPSC Prelims UPSC EPFO UPSC EPFO APFC UPSC IFS - Indian Forest Service UPSC Prelims - Economy UPSC Prelims - Polity & Governance Uttarakhand Judicial Service (Prelims) West Bengal Judicial Service (Prelims)"
        },
        {
            "title": "Year Range",
            "content": "20162024 20102025 20182023 2019 2020 20192025 2011 20112021 20152025 20192023 2024 20222025 20182025 20162023 20242025 20222024 20142015 20242025 20192025 20132017 20022023 20232024 2025 2025 2011 2011 The data acquisition stage involved systematic collection from official portals with comprehensive metadata extraction including examination year, conducting body, subject classification, and language identification. This foundational step ensured proper provenance tracking and enabled systematic quality control throughout the processing pipeline. OCR processing utilized Surya OCR for multi-language document digitization, selected based on reported evaluations demonstrating superior performance in handling Indic languages and domain-specific content. Prior studies indicate 98.1% normalized text similarity for English and 98.9% for Hindi, with Surya significantly outperforming alternatives such as Tesseract and Google Vision API in multilingual contexts. Content extraction leveraged GPT-OSS-120B with the prompt strategies described in C.1.4, enabling intelligent text structuring that addressed key challenges such as format variations across examination bodies, answer key alignment complexities, multi-format question types, and language-specific formatting conventions. The extraction process maintained original question formatting while standardizing structural elements for consistency across the dataset. Quality filtering employed multi-layered approaches including language verification using INDICLID, duplicate detection through semantic similarity measures, and comprehensive content quality assessment. This stage excluded image-based questions requiring visual interpretation and questions with non-standard formatting that could compromise evaluation consistency. Subdomain classification addressed the challenge that approximately 30% of collected questions lacked explicit subdomain labels. We employed GPT-OSS-120B using few-shot prompts designed to extract missing key details, as described in Box C.1.4, and refined the outputs with domain-specific taxonomies in consultation with subject matter experts to ensure accurate categorization within the BBA, BBF, BBK, and BBL domains. In addition to subdomain classification, we employed GPT-OSS-120B with the same few-shot prompt setup described in Box C.1.4 to extract key details such as question type and question level. For both dimensions, domain-wise few-shot examples were manually curated to guide the model. For question level, the model was prompted to categorize items 24 Figure 6 Manual quality assessment of BhashaBench V1 domain questions. into three standard difficulty classes: Easy, Medium, and Hard, widely adopted practice in educational assessment. For question type, we guided the model to identify structural formats from six commonly used categories: Assertion/Reason (A/R), Fill in the Blanks (FIB), Multiple Choice Questions (MCQ), Match the Columns (MTC), Reading Comprehension (RC), and Rearrange the Sentence (RTS). These categories ensured consistent annotation of question properties across the dataset. Manual validation constituted the final stage of quality assurance, wherein all extracted question-answer pairs were subjected to meticulous expert review following comprehensive annotation guidelines. This rigorous process ensured verification of factual accuracy, preservation of cultural and contextual nuances, resolution of ambiguities, and standardization of consistency, all while maintaining the linguistic authenticity and natural flow characteristic of each target language. The detailed annotation guidelines, covering all domains, are summarized in Table 4. Figure 6 illustrates the outcomes of manual validation, showing the distribution of good, neutral, and bad samples. Bad and neutral samples identified in this process were subsequently reviewed and corrected manually. C.1.3 Annotation Guidelines Our annotation guidelines were meticulously designed to ensure consistency, accuracy, and cultural authenticity across all BhashaBench domains and languages. The guidelines established standardized protocols for answer verification, requiring annotators to cross-reference all responses against original source materials and validate factual correctness through domain-specific expertise. Special emphasis was placed on preserving linguistic nuances and cultural contexts inherent to each target language, while maintaining uniform quality standards across BBA, BBF, BBK, and BBL domains. 25 Table 4 Annotation Guidelines across Domains in BhashaBench V"
        },
        {
            "title": "Ayurveda",
            "content": "Answer Verification: Ensure that the provided answer key is correct. Cross-check against the original exam paper. Option Consistency: Verify that all answer options are present and plausible. Minor typographical or formatting errors may be corrected, but content must remain faithful. Preserve Original Meaning: Do not paraphrase unnecessarily; reflect the exact intent of the source item. Self-Contained Questions: Ensure questions are answerable solely from the original paper or passage. Clarity and Formatting: Correct minor OCR errors, formatting issues, or multi-language misalignments without introducing ambiguity. Avoid Bias or Modification: Do not alter numerical data, dates, or technical/domain-specific terms. Verify crop names, farming practices, and region-specific agricultural knowledge for accuracy and contextual relevance. Ensure legal terms, statutes, case references, and procedural knowledge are precise and jurisdictionally correct. Preserve numerical accuracy in calculations, financial formulas, market terminology, and regulatory compliance requirements. Maintain correctness of medicinal terms, herb names, therapeutic practices, and traditional knowledge references. C.1.4 Data Processing Prompts BBA Question-Answer Extraction Prompt Template You are an OCR forensic specialist for Ayurveda/Medical exams (BAMS, AIAPGET, UPSC Ayurveda optional). Extract questions and answers with surgical precision from corrupted text. CRITICAL MISSION: EXTRACT EVERYTHING - NEVER SKIP QUESTIONS PRIMARY EXTRACTION RULES 1. ZERO TOLERANCE FOR MISSING QUESTIONS - Scan text character by character - Look for question patterns: \"Q1\", \"1.\", \"(1)\", \"Question 1\", \"Que.1\", or ANY numbering - Extract PARTIAL questions with [INCOMPLETE] tag rather than skip - If options are corrupted beyond recognition, create synthetic placeholders 2. AYURVEDA DOMAIN OCR CORRECTIONS - Classical Texts: \"Charaka Samhita\" not \"Charak Samita\", \"Sushruta\" not \"Susrut \", \"Ashtanga Hridaya\" not \"Astanga Hridya\" - Terminology: \"Vata\" not \"Vatha\", \"Pitta\" not \"Pita\", \"Kapha\" not \"Kafa\" - Herbs: \"Ashwagandha\" not \"Ashwagonda\", \"Haritaki\" not \"Harithki\", \"Brahmi\" not \"Brahni\" - Therapy: \"Panchakarma\" not \"Panchkarma\", \"Rasayana\" not \"Rasayan\" - Institutions: \"CCRAS\" not \"CCR4S\", \"AYUSH\" not \"AYU5H\", \"NIA Jaipur\" not \"N1A Jeypur\" - Exams: \"AIAPGET\" not \"AIAPCET\", \"AIBE\" not \"A1BE\" - Units: \"ml\", \"g\", \"mg\", \"days\" preserved 3. AGGRESSIVE OPTION RECOVERY - If option starts with garbled text, extract the meaningful part - If missing, assign option letters a, b, c, - Example: \"aj Panchakarma\" becomes \"a) Panchakarma\" \"Harithki\" becomes \"c) Haritaki [OCR: truncated]\" 4. ANSWER DETECTION PATTERNS - Explicit: check, *, (Ans), [Answer] - Secondary: \"1. c\", \"Q1: b\", \"Ans: a\" - Tertiary: formatting cues - Last resort: pattern analysis 5. QUESTION BOUNDARY DETECTION - Start: number + punctuation (1., Q1:, (1), etc.) - End: next number or section break - Normalize multi-parts: 1.a, 1.i, 1.1 6. SELF-CONTAINED QUESTIONS - Each question MUST include context (passages, sutras, tables) - If questions refer to common passage, include passage in EACH - Never assume context from previous questions ENHANCED EXTRACTION LOGIC STEP 1: Preprocess text, fix OCR errors, detect boundaries STEP 2: Extract question, include passage, mark [INCOMPLETE] if needed STEP 3: Normalize options, recover corrupted, create placeholders STEP 4: Detect and embed answers directly in question JSON SCHEMA (STRICTLY ENFORCED) { \"exam_info\": { \"title\": \"Ayurveda Examination\", \"year\": null, \"paper\": null, \"total_questions_detected\": 50 }, \"metadata\": { \"ocr_quality\": \"poor\", \"common_errors\": [\"sanskrit_terms\",\"herb_names\",\"therapy_names\"], \"sections_detected\": [\"Dravyaguna\",\"Kayachikitsa\",\"Samhita\",\"Rachana Sharir \",\"Shalya\",\"Shalakya\"] }, \"questions\": [ { } \"number\": \"1\", \"section\": \"Dravyaguna\", \"question\": \"Passage: According to Charaka Samhita, Haritaki is considered one of the best Rasayanas.nnQuestion: Which property of Haritaki is described as Tridoshahara?\", \"options\": { \"a\": \"It balances Vata only\", \"b\": \"It balances Pitta only\", \"c\": \"It balances all three doshas\", \"d\": \"It has no effect on Kapha\" }, \"answer\": \"c\" ], \"extraction_summary\": { \"total_questions\": 50, \"questions_with_answers\": 48, \"questions_with_all_options\": } } 27 CRITICAL ERROR PREVENTION - NEVER skip questions - NEVER empty options - NEVER separate answer keys - ALWAYS preserve numbering - ALWAYS embed answers - ALWAYS self-contained questions --- BEGIN OCR TEXT --- {ocr_text} BBK Question-Answer Extraction Prompt Template You are an OCR forensic specialist for Agriculture/Agri-exams. Extract questions and answers with surgical precision from corrupted text. CRITICAL MISSION: EXTRACT EVERYTHING - NEVER SKIP QUESTIONS PRIMARY EXTRACTION RULES 1. ZERO TOLERANCE FOR MISSING QUESTIONS - Scan text character by character - Look for question patterns: \"Q1\", \"1.\", \"(1)\", \"Question 1\", \"Que.1\", or ANY numbering - Extract PARTIAL questions with [INCOMPLETE] tag rather than skip - If options are corrupted beyond recognition, create synthetic placeholders 2. AGRICULTURE DOMAIN OCR CORRECTIONS - Crop names: \"Wheat\" not \"Wheal\", \"Paddy\" not \"Pady\", \"Maize\" not \"Maiz\" - Fertilizers: \"Urea\" not \"Uiea\", \"DAP\" not \"DAF\", \"NPK\" not \"NPX\" - Units: \"kg/ha\", \"t/ha\", \"mm rainfall\" preserved, never corrupted - Pesticides: \"Carbendazim\", \"Malathion\", \"Glyphosate\" corrected - Institutions: \"ICAR\" not \"IC4R\", \"IARI\" not \"IAR1\", \"KVK\" not \"KVY\" - Schemes: \"PM-KISAN\" not \"PM-KISRN\", \"MSP\" not \"MS5P\", \"Kisan Credit Card\" not \" Cradit Gard\" 3. AGGRESSIVE OPTION RECOVERY - If option starts with garbled text, extract the meaningful part - If missing, assign option letters a, b, c, - Example: \"aj Wheat\" -> \"a) Wheat\"; \"Maiz\" -> \"c) Maize [OCR: truncated]\" 4. ANSWER DETECTION PATTERNS - Explicit: check, *, (Ans), [Answer] - Secondary: \"1. c\", \"Q1: b\", \"Ans: a\" - Tertiary: formatting cues - Last resort: pattern analysis 5. QUESTION BOUNDARY DETECTION - Start: number + punctuation (1., Q1:, (1), etc.) - End: next number or section break - Normalize multi-parts: 1.a, 1.i, 1.1 6. SELF-CONTAINED QUESTIONS - Each question MUST include context (passages, data, charts) - If questions refer to common passage, include passage in EACH - Never assume context from previous questions ENHANCED EXTRACTION LOGIC STEP 1: Preprocess text, fix OCR errors, detect boundaries STEP 2: Extract question, include passage, mark [INCOMPLETE] if needed STEP 3: Normalize options, recover corrupted, create placeholders STEP 4: Detect and embed answers directly in question JSON SCHEMA (STRICTLY ENFORCED) { \"exam_info\": { \"title\": \"Agriculture Examination\", 28 \"year\": null, \"paper\": null, \"total_questions_detected\": 50 }, \"metadata\": { \"ocr_quality\": \"poor\", \"common_errors\": [\"crop_names\",\"fertilizer_terms\",\"units\"], \"sections_detected\": [\"Agronomy\",\"Soil Science\",\"Plant Pathology\"] }, \"questions\": [ { } \"number\": \"1\", \"section\": \"Agronomy\", \"question\": \"Passage: farmer applies 120 kg N/ha to wheat using urea.n nQuestion: How much urea is required per hectare?\", \"options\": { \"a\": \"120 kg\", \"b\": \"261 kg\", \"c\": \"300 kg\", \"d\": \"520 kg\" }, \"answer\": \"b\" ], \"extraction_summary\": { \"total_questions\": 50, \"questions_with_answers\": 48, \"questions_with_all_options\": 47 } } CRITICAL ERROR PREVENTION - NEVER skip questions - NEVER empty options - NEVER separate answer keys - ALWAYS preserve numbering - ALWAYS embed answers - ALWAYS self-contained questions --- BEGIN OCR TEXT --- {ocr_text} BBL Question-Answer Extraction Prompt Template You are an OCR forensic specialist for legal examinations. Extract questions and answers with surgical precision from corrupted text. # CRITICAL MISSION: EXTRACT EVERYTHING - ZERO DEPENDENCIES BETWEEN QUESTIONS ## PRIMARY EXTRACTION RULES 1. **ABSOLUTE QUESTION COMPLETENESS** - SCAN ENTIRE TEXT character by character for any question patterns - Each question MUST be 100% self-contained and independently answerable - NEVER use references like \"above passage\", \"question 15\", \"as mentioned earlier\" - If questions share context, EMBED the full context in EACH question - Extract PARTIAL questions with [INCOMPLETE] tag rather than skip - Pattern recognition: \"Q1\", \"1.\", \"(1)\", \"Question 1\", \"Que.1\", roman numerals \"I.\", \"II.\" 2. **LEGAL DOMAIN OCR CORRECTIONS** 29 - Legal terms: \"Constitution\", \"Amendment\", \"Article\", \"Section\", \"Sub-section \" - Court names: \"Supreme Court\" not \"5upreme Court\", \"High Court\" not \"H1gh Court\" - Acts: \"IPC\", \"CrPC\", \"CPC\", \"Evidence Act\", \"Contract Act\" - Legal phrases: \"prima facie\", \"res judicata\", \"stare decisis\", \"ultra vires\" - Citations: \"AIR\", \"SCC\", \"All ER\" formatting preservation - Common OCR fixes: * \"Section\" not \"5ection\" or \"$ection\" * \"Article\" not \"Art1cle\" or \"Artic1e\" * \"Amendment\" not \"Arnendment\" or \"Amendrnent\" * \"Constitution\" not \"Con5titution\" or \"Const1tution\" * \"Parliament\" not \"Par1iament\" or \"Parliarnent\" * \"Judiciary\" not \"Judic1ary\" or \"jud1c1ary\" * \"vs.\" not \"v5.\" or \"v$.\" * \"Ltd.\" not \"1td.\" or \"Lte.\" 3. **CONTEXT EMBEDDING STRATEGY** - Identify shared contexts: case studies, legal scenarios, constitutional provisions, statutes - For each question referencing shared content, embed COMPLETE context within question text - Format: \"Context: [Full legal scenario/case/provision]nnQuestion: [actual question]\" - Never assume previous knowledge from other questions - Make every question standalone legal problem 4. **AGGRESSIVE OPTION RECOVERY (STRICTLY a, b, c, FORMAT)** - Legal options often contain complex phrases - recover aggressively - **MANDATORY**: All options must be normalized to exactly a, b, c, format - If option starts with corruption, extract meaningful legal content and assign proper letter - Pattern match: 4 consecutive lines that could be legal options (never more than 4) - Auto-assign missing option letters: first=a, second=b, third=c, fourth=d - **NEVER use option e** - if 5 options detected, merge weakest two or skip question - Examples: Corrupted: \"aj Constitutional Law\" Missing: \"Criminal Procedure\" Partial: \"c) Civil Procedur\" Garbled: \"d) Evidenc3 Act 187\" Extra: \"e) Fifth option\" 5. **ENHANCED ANSWER DETECTION** -> \"a) Constitutional Law\" -> \"a) Criminal Procedure\" -> \"c) Civil Procedure [OCR: truncated]\" -> \"d) Evidence Act 1872\" -> SKIP this question or merge with d) - Primary: Explicit markers (check, *, (Ans), [Answer], Bold, Correct option) - Secondary: Answer blocks (\"1. c\", \"Q1: b\", \"Ans: a\", \"Solution: d\") - Tertiary: Context clues (underlined, highlighted, different fonts) - Legal-specific: \"Held\", \"Ratio\", \"Decision\", \"Correct statement\" - Pattern analysis for similar legal questions - NEVER leave answer as null if ANY indication exists 6. **LEGAL QUESTION BOUNDARY DETECTION** - Start patterns: Number + punctuation (1., Q1:, (1), 1-, I., II.) - End: Next question number OR section break - Multi-part handling: \"1(a)\", \"1(i)\", \"Q1.1\" -> normalize to \"1.a\", \"1.i\", \"1.1\" - Legal instructions: \"Read the following case and answer\", \"Based on provisions\" - Fact patterns: Often lengthy - include completely in each question 30 7. **QUESTION QUALITY VALIDATION (MANDATORY)** - Apply 3-tier validation before including any question: **TIER 1 - BASIC STRUCTURE VALIDATION:** - Question must have clear interrogative structure - Must contain exactly 4 options (a, b, c, d) - skip if not achievable - Answer must be one of: a, b, c, or - Answer must be logically derivable from options - Question text must be grammatically coherent **TIER 2 - LEGAL COHERENCE VALIDATION:** - Legal concepts must be accurate and well-defined - Case references must be contextually appropriate - Statutory citations must make logical sense - Legal terminology must be used correctly - Question must test genuine legal knowledge, not gibberish **TIER 3 - LOGICAL CONSISTENCY VALIDATION:** - Options must be mutually exclusive where appropriate - Correct answer must be definitively better than other options - Question must be answerable based on provided context - No circular reasoning or impossible scenarios - Legal principles must align with established jurisprudence **SKIP CRITERIA - Only skip if question fails ANY of these:** - Question text is completely unintelligible after OCR correction attempts - Cannot recover exactly 4 coherent options (a, b, c, d) - No logical answer can be determined from the 4 options - Legal content is fundamentally nonsensical or contradictory - Question would mislead rather than educate (factually incorrect legal principles) ## ENHANCED EXTRACTION LOGIC **STEP 1: LEGAL TEXT PREPROCESSING** - Fix legal terminology OCR errors using domain dictionary - Identify question boundaries with legal-aware regex - Locate shared legal contexts (cases, statutes, provisions) - Mark potential option blocks with legal content validation **STEP 2: CONTEXT-EMBEDDED QUESTION EXTRACTION WITH VALIDATION** - Extract question with ALL necessary legal context embedded - **APPLY 3-TIER QUALITY VALIDATION:** * Tier 1: Verify basic question structure and coherence * Tier 2: Validate legal accuracy and terminology * Tier 3: Ensure logical consistency and educational value - **ONLY PROCEED if question passes validation tiers** - Include case facts, statutory provisions, legal scenarios within each question - Clean and validate legal terminology - Mark borderline questions with [REVIEW_NEEDED] but include if they pass basic validation - Preserve legal citations and case names - **SKIP ONLY** if question fails fundamental validation criteria **STEP 3: LEGAL OPTION PROCESSING (STRICT a,b,c,d FORMAT)** - **MANDATORY**: Normalize to exactly a, b, c, format only - Handle complex legal option text with recovery logic - **NEVER create option e** - questions must have exactly 4 options - If more than 4 options detected, either merge similar ones or skip the question - If fewer than 3 options recovered, skip the question - Create contextually appropriate placeholder options if missing (but only up to d) - Ensure options contain complete legal concepts - Validate legal terminology in options **STEP 4: COMPREHENSIVE ANSWER RESOLUTION** - Multi-pass answer detection with legal context awareness 31 - Look for legal reasoning indicators - Embed answers directly in questions - Cross-reference with legal principles if needed ## JSON SCHEMA (STRICTLY ENFORCED) {{ \"exam_info\": {{ \"title\": \"Legal Examination\", \"year\": null, \"paper\": null, // e.g., \"Constitutional Law\", \"Criminal Law\" \"total_questions_detected\": 0 // Actual count for validation // EXTRACT FROM TEXT - NEVER ASSUME }}, \"metadata\": {{ \"ocr_quality\": \"poor\", // excellent/good/fair/poor \"common_errors\": [\"legal_terms\", \"case_citations\", \"section_numbers\"], \"sections_detected\": [\"Constitutional Law\", \"Criminal Law\", \"Civil Law\"], \"shared_contexts_embedded\": 5 // Count of contexts embedded across questions }}, \"questions\": [ {{ \"number\": \"1\", \"section\": \"Constitutional Law\", \"question\": \"Context: The Supreme Court in Kesavananda Bharati v. State of Kerala (1973) established the basic structure doctrine, holding that Parliament cannot amend the Constitution to destroy its basic features like democracy, secularism, and federalism.nnQuestion: Which of the following is NOT considered part of the basic structure of the Constitution?\", \"options\": {{ \"a\": \"Judicial review\", \"b\": \"Parliamentary supremacy\", \"c\": \"Rule of law\", \"d\": \"Separation of powers\" }}, \"answer\": \"b\" }} ], \"extraction_summary\": {{ // Questions detected before validation \"total_questions_found\": 0, \"total_questions_extracted\": 0, // Questions that passed validation \"questions_skipped\": 0, \"questions_with_answers\": 0, \"questions_with_complete_context\": 0, \"questions_with_all_options\": 0, \"skip_reasons\": [] // Questions skipped due to quality issues // Array of reasons why questions were skipped }} }} ## CRITICAL SUCCESS FACTORS ### :white_check_mark: MUST DO: - Apply rigorous 3-tier validation to every question before extraction - Make every question completely independent and self-contained - Embed ALL necessary context within each question - Preserve legal terminology accuracy - Include questions that pass validation even if they have minor OCR issues - Include complete case facts, statutory provisions, legal scenarios in relevant questions - Normalize legal citations and references - Skip questions ONLY after thorough validation failure ### :x: NEVER DO: 32 - Create questions that reference other questions (\"as in question 15\") - Use phrases like \"above passage\", \"aforementioned case\", \"previously discussed\" - Skip questions due to OCR corruption - Create empty options arrays - Add confidence scores or OCR quality metadata to individual questions - Assume exam details not present in text - Leave questions dependent on external context ### :dart: LEGAL-SPECIFIC EXCELLENCE: - Recognize and preserve legal citation formats - Maintain accuracy of case names and statutory references - Handle complex legal fact patterns appropriately - Ensure constitutional provisions are correctly stated - Preserve legal Latin phrases and terminology - Maintain chronological accuracy of legal developments --- BEGIN OCR TEXT --- {ocr_text} BBF Question-Answer Extraction Prompt Template You are an OCR forensic specialist for financial/banking exams. Extract questions and answers with surgical precision from corrupted text. CRITICAL MISSION: EXTRACT EVERYTHING - NEVER SKIP QUESTIONS PRIMARY EXTRACTION RULES 1. ZERO TOLERANCE FOR MISSING QUESTIONS - SCAN ENTIRE TEXT character by character - Look for question patterns: \"Q1\", \"1.\", \"(1)\", \"Question 1\", \"Que.1\", or ANY numbering - Extract PARTIAL questions with [INCOMPLETE] tag rather than skip - If options are corrupted beyond recognition, create synthetic placeholders 2. FINANCIAL DOMAIN OCR CORRECTIONS - Currency: \"textrupee\" not \"Rs\" or \"Rupees\", \"$\" preservation - Percentages: \"%\" never \"per cent\" or missing - Financial terms: \"CAGR\", \"NPV\", \"IRR\", \"EBITDA\", \"P/E ratio\" - Numbers: \"10,000\" not \"10.000\", preserve commas in large numbers - Rates: \"7.5%\" not \"7.5 percent\" or \"7.5per cent\" - Common OCR fixes: * \"NIFTY\" not \"N1FTY\" or \"NJFTY\" * \"BSE\" not \"B5E\" or \"B$E\" * \"NSE\" not \"N5E\" or \"N$E\" * \"SEBI\" not \"5EBI\" or \"$EBI\" * \"RBI\" not \"RB1\" or \"R81\" * \"GDP\" not \"G0P\" or \"6DP\" 3. AGGRESSIVE OPTION RECOVERY - If option starts with garbled text, extract the meaningful part - Pattern match: Look for 4-5 consecutive lines that could be options - If missing option letters, assign them: first line=a, second=b, etc. - Examples of recovery: Corrupted: \"aj Fixed Deposit\" rightarrow \"a) Fixed Deposit\" Missing: \"Mutual Fund\" rightarrow \"a) Mutual Fund\" (assign letter) Partial: \"c) Equity Shar\" rightarrow \"c) Equity Share [OCR: truncated]\" 33 4. ANSWER DETECTION PATTERNS - Primary: Explicit markers (check, *, (Ans), [Answer], Bold text) - Secondary: Answer blocks (\"1. c\", \"Q1: b\", \"Ans: a\") - Tertiary: Context clues (underlined, different formatting) - Last resort: Pattern analysis of similar questions - NEVER leave answer as null if ANY indication exists 5. QUESTION BOUNDARY DETECTION - Start: Number + any punctuation (1., Q1:, (1), 1-, etc.) - End: Next question number OR distinctive break - Handle multi-part: \"1(a)\", \"1(i)\", \"Q1.1\" to normalize to \"1.a\", \"1.i\", \"1.1\" - Instructions/headers: Skip but note in metadata 6. SELF-CONTAINED QUESTIONS - Each question MUST include ALL necessary context (passages, data, charts) - If questions refer to common passage/data, include that passage in EACH question - Format: \"Passage: [full passage text]nnQuestion: [actual question]\" - Never assume context from previous questions - Make every question independently answerable ENHANCED EXTRACTION LOGIC STEP 1: TEXT PREPROCESSING - Fix obvious OCR errors in financial terms - Identify question boundaries using regex patterns - Mark potential option blocks - Identify shared passages/contexts STEP 2: QUESTION EXTRACTION - Extract question text, clean and validate - Include any relevant passage/context within the question - If question incomplete, note with [INCOMPLETE] tag - Preserve mathematical symbols and formulas - Only take question if complete with options - only meaningfull question. STEP 3: OPTION PROCESSING - Normalize labels to a, b, c, (and if exists) - Handle malformed options with recovery logic - Create placeholder options if completely missing - Ensure options are clearly defined and complete STEP 4: ANSWER RESOLUTION - Multi-pass answer detection - Embed answers directly in each question - No separate answer key needed JSON SCHEMA (STRICTLY ENFORCED) { \"exam_info\": { \"title\": \"Banking/Financial Examination\", \"year\": null, \"paper\": null, \"total_questions_detected\": 50 // EXTRACT FROM TEXT - NEVER ASSUME // NEW: Count for validation }, \"metadata\": { 34 \"ocr_quality\": \"poor\", // excellent/good/fair/poor \"common_errors\": [\"currency_symbols\", \"percentages\"], \"sections_detected\": [\"Quantitative Aptitude\", \"General Awareness\"] }, \"questions\": [ { \"number\": \"1\", \"section\": \"Quantitative Aptitude\", \"question\": \"Passage: bank offers different investment schemes with varying interest rates.nnQuestion: What is the compound interest on Rs.10,000 at 8% per annum for 2 years?\", \"options\": { \"a\": \"Rs.1,600\", \"b\": \"Rs.1,664\", \"c\": \"Rs.1,728\", \"d\": \"Rs.1,800\" }, \"answer\": \"b\" } ], \"extraction_summary\": { \"total_questions\": 50, \"questions_with_answers\": 48, \"questions_with_all_options\": 47 } } CRITICAL ERROR PREVENTION - NEVER skip questions due to poor OCR - NEVER output empty options array - NEVER create separate answer keys - NEVER assume exam details not in text - NEVER add confidence, ocr_issues, or extraction_notes fields - ALWAYS preserve original numbering scheme - ALWAYS include complete context in each question - ALWAYS embed answers directly in questions - ALWAYS make questions self-contained and independent --- BEGIN OCR TEXT --- {ocr_text}"
        },
        {
            "title": "Key Details Extraction Prompt Template",
            "content": "You are an expert in the {domain_name} domain. For each question, extract: 1. question_type: The format/structure of the question {question_type_examples} 2. question_level: The difficulty or complexity level {difficulty_levels_list} 3. topic: The academic topic or domain {human_annoted_topics_examples} 4. subdomain: The specific topic area within the main topic { human_annoted_subdomains_list} Respond only in this JSON format: { \"question_type\": \"\", \"question_level\": \"\", \"topic\": \"\", 35 \"subdomain\": \"\" } C.2 Detailed Data Analysis of BhashaBench V1 Table 5 Language distribution across domains in BhashaBench V"
        },
        {
            "title": "English\nHindi",
            "content": "12,648 2,757 13,451 5,"
        },
        {
            "title": "BBA",
            "content": "9,348 5,"
        },
        {
            "title": "Overall",
            "content": "17,047 7,318 52,494 21,"
        },
        {
            "title": "Total",
            "content": "15,405 19,433 14,963 24,365 74,166 Table 6 Difficulty distribution across domains in BhashaBench V"
        },
        {
            "title": "BBK",
            "content": "6,754 6,941 1,"
        },
        {
            "title": "BBF",
            "content": "7,111 9,348 2,"
        },
        {
            "title": "BBA",
            "content": "7,944 6,"
        },
        {
            "title": "Overall",
            "content": "13,913 9,405 1,047 35,722 32,008 6,"
        },
        {
            "title": "Total",
            "content": "15,405 19,433 14,963 24,365 74,166 Table 7 Question type distribution across domains in BhashaBench V"
        },
        {
            "title": "MCQ\nAssertion or Reasoning\nMatch the Column\nFill in the Blanks\nRearrange the Sequence\nReading Comprehension",
            "content": "13,550 648 949 49 209 0 18,019 215 119 286 708 86 14,717 27 41 178 0 0 21,566 430 495 1,402 147 325 67,852 1,320 1,604 1,915 1,"
        },
        {
            "title": "Total",
            "content": "15,405 19,433 14,963 24,365 74,166 Table 8 BBK Subject Domains and Question Counts"
        },
        {
            "title": "Subject Domain",
            "content": "Agri-Environmental & Allied Disciplines Agricultural Biotechnology Agricultural Chemistry & Biochemistry Agricultural Economics & Policy Agricultural Engineering & Technology Agricultural Extension Education Agricultural Microbiology Agriculture Communication Agriculture Information Technology Agronomy Animal Sciences Crop Sciences"
        },
        {
            "title": "Count",
            "content": "176 524 281 627 244 774 111 254 190 5078"
        },
        {
            "title": "Continued on next page",
            "content": "36 Table 8 Continued from previous page"
        },
        {
            "title": "Subject Domain",
            "content": "Dairy & Poultry Science Entomology Fisheries and Aquaculture General Knowledge & Reasoning Genetics and Plant Breeding Horticulture Natural Resource Management Nematology Plant Pathology Plant Sciences & Physiology Seed Science and Technology Soil Science Veterinary Sciences Table 9 BBF Subject Domains and Question Counts"
        },
        {
            "title": "Subject Domain",
            "content": "Problem Solving Mathematics for Finance Banking Services Governance & Policy Language & Communication Corporate Finance & Investment Commerce Accounting General Knowledge Information Technology Finance Economics & Development Studies Rural Economics Environmental Finance Taxation & Regulatory Compliance Interdisciplinary Finance Data & Analytics in Finance History, Sociology & Cultural Studies of Finance Finance Education Healthcare Economics Science and Technology in Finance International Finance & Trade Business Management Energy, Infrastructure & Finance Behavioral Finance Financial Markets Sports, Media & Finance Linkages Marketing Finance Insurance & Risk Management Legal Finance Financial Technology"
        },
        {
            "title": "Count",
            "content": "89 696 34 661 389 2070 193 184 397 129 202"
        },
        {
            "title": "Count",
            "content": "5686 4845 1171 1064 946 910 863 773 539 490 274 261 168 155 153 127 127 118 114 101 83 83 82 67 47 45 42 42 34 23 Table 10 BBA Subject Domains and Question Counts"
        },
        {
            "title": "Subject Domain",
            "content": "Kayachikitsa (General Medicine & Internal Medicine in Ayurveda) Dravyaguna & Bhaishajya Samhita & Siddhanta (Fundamentals) Sharir (Anatomy & Physiology) Panchakarma & Rasayana Stri Roga & Prasuti Tantra (Gynecology & Obstetrics) Shalakya Tantra (ENT, Eye, Dentistry) Kaumarbhritya & Pediatrics Agad Tantra & Forensic Medicine Shalya Tantra (Surgery) Swasthavritta & Public Health Research & Statistics Ayurvedic Literature & History Yoga & Psychology Administration, AYUSH & Miscellaneous Roga Vigyana (Diagnostics & Pathology) Table 11 BBL Subject Domains and Question Counts"
        },
        {
            "title": "Subject Domain",
            "content": "Civil Litigation & Procedure Constitutional & Administrative Law Criminal Law & Justice Corporate & Commercial Law General Academic Subjects Legal Theory & Jurisprudence Family & Personal Law International & Comparative Law Legal Skills & Communication Real Estate & Property Law Environmental & Energy Law Interdisciplinary Studies Tax & Revenue Law Employment & Labour Law Technology & Cyber Law Intellectual Property Law Consumer & Competition Law Media & Entertainment Law Healthcare & Medical Law Human Rights & Social Justice"
        },
        {
            "title": "Count",
            "content": "3134 2972 1541 1346 1308 847 734 714 587 526 453 210 204 188"
        },
        {
            "title": "Count",
            "content": "7126 3609 2769 2700 1756 1421 991 962 816 629 430 363 231 175 123 91 75 54"
        },
        {
            "title": "D More Details on Experiment Setup",
            "content": "D.1 Task Formatting Template Used in LM Eval This prompt format template is consistently applied across all task types, including Assertion or Reasoning, Fill in the Blanks, MCQs, Match the Column, Reading Comprehension, and Rearrange the Sequence tasks for BBF, BBK, and BBL domains. 38 Question: <question text> Choices: A. <option text> B. <option text> C. <option text> D. <option text> Answer: D.2 Task Formatting Template Used in API-Driven Evaluation This template is used when models are evaluated via API calls. It ensures consistent structure across all tasks, allowing the model to focus on producing the correct answer without additional explanation. The template separates the system prompt, which defines the models role and expected behavior, from the user/task prompt, which contains the question and options. This separation helps maintain clarity and consistency in responses across different multiple-choice and related tasks. SYSTEM PROMPT: You are helpful assistant for multiple-choice question answering. Respond with only the correct option letter: A, B, C, or D. Do not provide any explanation. USER PROMPT: Question: <question text> A. <option text> B. <option text> C. <option text> D. <option text> Please choose the correct option (A/B/C/D). D.3 Details of Inference Implementation For open-source models, inference is performed on cluster of 8 NVIDIA H200 GPUs using vLLM [39] for accelerated computation. The BhashaBench V1 tasks were integrated into the lm-eval library, and all evaluations used the default lm-eval parameters for consistency across tasks. For API-based models such as GPT-4o, inference is conducted via the Batch API with temperature set to 0, typically on CPU resources. Each evaluation is repeated three times and the average score is reported to minimize variability. Features like web search or external tool calls are disabled to maintain fair comparison across models."
        },
        {
            "title": "E More Details on Experiment",
            "content": "E.1 Zero-Shot Question-Level and Question-Type Performance Across BhashaBench V1 Domains E.2 Zero-Shot sub-domain wise Performance Across BhashaBench V1 Domains 39 Table 12 Zero-shot scores (%) of LLMs across domains on BhashaBench V1. The benchmark covers Ayurveda (BBA), Finance (BBF), Agriculture (BBK), and Legal (BBL) across Easy, Hard, and Medium difficulty levels. Model BBA BBF BBK BBL Easy Hard Med Easy Hard Med Easy Hard Med Easy Hard Med gemma-3-270m gemma-3-270m-it Param-1 gemma-2-2b gemma-2-2b-it Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct sarvam-2b-v0.5 sarvam-1 Nemotron-4-Mini-Hindi-4B-Base Nemotron-4-Mini-Hindi-4B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct granite-3.1-2b-instruct granite-3.1-3b-a800m-base Pangea-7B Indic-gemma-7b-finetuned-sft-Navarasa-2.0 aya-23-8B Llama-3.1-8B Llama-3.1-8B-Instruct gemma-2-9b gemma-2-9b-it gpt-oss-20b gemma-2-27b gemma-2-27b-it gpt-oss-120b Qwen3-235B-A22B-Instruct-2507 deepseek-v3 gpt-4o 28.1 25.89 43.93 38.27 29.96 28.52 27.44 31.63 36.42 27.08 30.94 37.01 36.08 41.18 35.55 33.9 31.45 41.45 38.54 35.51 35.99 39.43 51.12 38.91 42.03 55.35 43.47 60.62 65.18 52.44 66. 26.81 23.97 31.21 29.08 24.96 24.4 25.39 24.82 28.51 24.96 27.23 27.94 29.5 32.06 28.23 26.81 26.38 31.77 27.23 25.11 26.38 30.5 34.47 29.5 26.67 34.18 30.78 41.28 46.24 36.6 47.09 < 4B Models 28.35 26.5 35.95 30.31 26.83 27.97 25.23 29.19 29.66 26.88 27.26 30.96 30.8 33.1 29.57 28.06 27.78 24.15 25.38 38.31 39.76 36.55 30.5 28.72 36.75 39.73 28.18 32.2 41.95 39.21 45.34 39.91 36.68 31. 24.55 21.22 26.6 25.35 23.2 23.71 22.43 25.76 23.87 23.1 25.76 25.08 23.2 28.51 25.02 25.32 24.18 7B to 27B Models 32.94 31.72 28.29 30.25 29.36 36.85 29.11 30.27 39.18 31.9 49.33 43.68 41.2 42.92 44.24 55.32 47.03 46.77 60.92 51.03 > 27B Models 44.19 50.74 38.93 52. 74.8 72.52 73.49 69.13 28.72 26.8 25.62 26.93 22.19 27.44 24.78 24.61 30.09 26.93 62.61 41.49 40.55 36.35 25.8 23.92 27.71 28.5 27.67 26.27 25.5 29.26 28.2 25.43 27.43 30.5 28.05 33.9 30.48 28.63 25.77 34.94 30.99 30.98 30.46 30 34.3 32.74 30.86 39.24 35.67 70.88 59.33 59.01 50. 27.23 26.47 36.94 46.27 38.04 29.43 30.22 36.44 44.52 28.26 32.2 42.57 41.12 50.3 44.7 40.04 36.08 52.18 48.13 43.32 44.03 52.29 64.78 52.98 53.42 69.31 59.62 74.89 78.26 66.92 78.75 24.74 27.49 25.91 27.54 30.35 27.72 26.37 25.61 30.47 28.01 27.54 28.42 28.6 31.58 31.81 30.76 26.02 33.57 31.46 27.84 29.01 33.74 35.67 37.13 31.4 40.99 41.46 62.05 62.51 48.48 63. 25.66 27.53 29.09 34.26 32.01 28.68 27.69 29.17 34.69 27.03 28.99 32.89 32.27 37.49 37.23 33.25 29.88 40.69 35.8 31.77 34.51 40.63 46.26 42.93 39.56 51.51 48.28 65.88 69.79 55.5 70.84 27.23 26.47 36.94 46.27 38.04 29.43 30.22 36.44 44.52 28.26 32.2 42.57 41.12 50.3 44.7 40.04 36.08 52.18 48.13 43.32 44.03 52.29 64.78 52.98 53.42 69.31 59.62 74.89 78.26 66.92 78. 24.74 27.49 25.91 27.54 30.35 27.72 26.37 25.61 30.47 28.01 27.54 28.42 28.6 31.58 31.81 30.76 26.02 33.57 31.46 27.84 29.01 33.74 35.67 37.13 31.4 40.99 41.46 62.05 62.51 48.48 63.51 25.66 27.53 29.09 34.26 32.01 28.68 27.69 29.17 34.69 27.03 28.99 32.89 32.27 37.49 37.23 33.25 29.88 40.69 35.8 31.77 34.51 40.63 46.26 42.93 39.56 51.51 48.28 65.88 69.79 55.5 70. Table 14 Performance of GEMMA model family across sub-domains in BhashaBench v1, comparing base and instruction-tuned variants of different model sizes (270M, 2B, 9B, 27B) Subject Domain 270m 270m-it 2b 2b-it 9b 9b-it 27b 27b-it Tantra & Forensic Administration, AYUSH & Miscellaneous Agad Medicine Ayurvedic Literature & History Dravyaguna & Bhaishajya Kaumarbhritya & Pediatrics (General Kayachikitsa Medicine & Internal Medicine in Ayurveda) Panchakarma & Rasayana Research & Statistics Roga Vigyana (Diagnostics & Pathology) BBA 34.45 28.57 40.34 34.45 63.03 51. 60.5 57.14 25.89 27.94 31.18 27. 48.21 39.35 49.4 42.25 26.96 28.4 28.57 29.45 26.83 27.14 31. 23.53 26.35 27.03 25.72 23.7 25.24 38.75 31.37 30.08 38.8 36.76 28.92 27.79 28.15 29.1 46.08 38.43 46.22 47.16 31.86 32.74 31.65 34. 43.14 39.64 47.9 50.8 42.16 33.68 36.55 36.89 30.2 60 45 26.53 34.29 35 32.49 77.62 65 28.36 53.81 37.84 78.1 72.5 33.94 57.62 56.25 Continued on next page 40 Subject Domain 270m 270m-it 2b 2b-it 9b 9b-it 27b 27b-it Table 14 Continued from previous page Samhita & Siddhanta (Fundamentals) Shalakya Tantra (ENT, Eye, Dentistry) Shalya Tantra (Surgery) Sharir ogy) Stri Roga & Prasuti Tantra (Gynecology & Obstetrics) Swasthavritta & Public Health Yoga & Psychology (Anatomy & PhysiolInfrastructure & FiAccounting Banking Services Behavioral Finance Business Management Commerce Corporate Finance & Investment Data & Analytics in Finance Economics & Development Studies Energy, nance Environmental Finance Finance Education Financial Markets Financial Technology General Knowledge Governance & Policy Healthcare Economics History, Sociology & Cultural Studies of Finance Information Technology Finance Insurance & Risk Management Interdisciplinary Finance International Finance & Trade Language & Communication Legal Finance Marketing Finance Mathematics for Finance Problem Solving Rural Economics Science and Technology in Finance Sports, Media & Finance Linkages Taxation & Regulatory Compliance Agri-Environmental & Allied Disciplines Agricultural Biotechnology 30.89 29.07 33.29 28.42 37.7 30. 43.93 34.59 25.89 21.93 34.74 21. 44.69 31.2 45.78 34.88 26.0 24.59 23 26. 31.94 33.28 26.05 27.79 45.06 46.95 31.75 34.75 44.87 51.04 39.16 40. 24.68 24.09 34.59 29.87 46.99 40. 53.96 42.38 34.88 30.85 30.24 26.6 49.67 43.62 39.07 32. 67.33 57.45 49.01 37.77 71.52 61.7 59.82 46.81 BBF 26.78 23.4 31.34 26.51 28.04 25. 23.62 22.99 26 25.19 28.36 25.3 22.48 23.52 24.41 20.8 31.31 37.75 47.76 55.42 32.79 31.1 30.53 34.67 46.27 45.78 31.05 31.98 41.14 53.8 50.75 63.86 40.32 44. 38.03 47.82 59.7 50.6 39.17 39.56 44.11 60.8 52.24 75.9 48.78 50.55 32.28 37.96 27.56 41.24 38.58 62.41 30.71 45. 44.88 63.87 39.46 54.06 52.24 62.65 41.25 43.19 29.13 46.72 20.73 31.71 34. 28.05 43.9 50 51.22 42.68 22.02 26.27 31.91 34.78 24.3 26.69 27.19 18. 23.21 27.12 25.53 26.09 26.35 24.72 30.7 25.98 41.07 43.22 53.19 26.09 41.37 36.18 40.35 40.94 34.5 39.83 36.17 47.83 38.4 34.21 39.47 41.73 50 49.15 51.06 60.87 57.7 52.07 57.89 60.63 43.45 44.07 44.68 47.83 51.02 46.52 50 51.18 61.9 55.08 63.83 60.87 61.78 60.9 61.4 64. 54.76 49.15 55.32 47.83 52.5 51.13 51.75 57.48 23.06 28.57 55.31 44.49 63.47 83.27 67.14 16.67 25.49 21.69 22.73 32.35 26.19 24.83 25.08 25.67 26.73 33.33 20.92 16.87 23.04 29.41 26.19 23.76 23.11 29.89 19.8 38.1 35.95 42.17 39.43 35.29 47.62 28.96 26.28 39.46 31. 30.95 36.6 42.17 40.06 41.18 35.71 25.96 24.76 40.61 37.62 50 56.86 66.27 59.83 47.06 76.19 33.81 28.14 57.47 48.51 38.1 49.02 59.04 47.89 35.29 61.9 31 26.73 50.19 50.5 50 62.75 73.49 61.1 50 66.67 38.53 31.6 68.2 61.39 40.48 51.63 61.45 49.79 50 59.52 32.69 30.99 54.79 54.46 15. 20 37.78 48.89 62.22 62.22 66. 64.44 32.26 26.45 36.13 45.81 58. 51.61 64.52 52.9 BBK 26.14 26. 29.55 36.93 48.86 46.02 48.86 54. 26.15 29.77 54.2 43.13 75.19 63. 77.67 70.61 Continued on next page 41 Subject Domain 270m 270m-it 2b 2b-it 9b 9b-it 27b 27b-it Table 14 Continued from previous page Agricultural Chemistry & Biochemistry Agricultural Economics & Policy Agricultural Engineering & Technology Agricultural Extension Education Agricultural Microbiology Agriculture Communication Agriculture Information Technology Agronomy Animal Sciences Crop Sciences Dairy & Poultry Science Entomology Fisheries and Aquaculture General Knowledge & Reasoning Genetics and Plant Breeding Horticulture Natural Resource Management Nematology Plant Pathology Plant Sciences & Physiology Seed Science and Technology Soil Science Veterinary Sciences Civil Litigation & Procedure Constitutional & Administrative Law Consumer & Competition Law Corporate & Commercial Law Criminal Law & Justice Employment & Labour Law Environmental & Energy Law Family & Personal Law General Academic Subjects Healthcare & Medical Law Human Rights & Social Justice Intellectual Property Law Interdisciplinary Studies International & Comparative Law Legal Skills & Communication Legal Theory & Jurisprudence Media & Entertainment Law Real Estate & Property Law Tax & Revenue Law Technology & Cyber Law 23.84 24.2 40. 33.1 54.8 51.25 61.92 56.23 28. 25.36 43.06 38.76 56.3 49.6 62. 54.39 29.51 25 38.93 26.64 50. 34.02 58.61 41.8 27.13 28.68 37. 34.75 53.75 49.74 60.47 55.04 21.62 22.83 27. 26.47 31.08 24.95 34.83 27.16 32.35 26.32 25.96 25.56 27.98 26.09 23.17 28.68 22.28 25.0 39.58 25.26 25.27 32 25.33 25.57 24.57 21.63 25.83 29.27 32 5.26 25.27 20.39 24.22 27.7 25.4 16.67 24.8 23.81 28.46 25.23 22.44 28. 26.84 24.32 27.69 24.72 26.87 11.76 27.99 27.51 26.18 28.5 31.52 27.71 26.36 33.66 28 29.17 BBL 27.36 25.57 25.33 25.15 25.75 29.71 22.56 26.34 25.97 32 10.53 27.47 26.72 23.91 23.28 27.59 33.33 22.8 26.41 28. 48.65 38.19 39.47 38.64 52.7 38.43 46.07 38.36 35.29 39.18 39.85 36.28 38.34 28.8 36.27 45.74 35.64 35 60.42 35.14 33.86 43.16 33.56 43.24 37.34 32.58 34.63 38.24 32.83 36.25 32.42 33.68 32.07 34.51 29.46 32.18 35.08 35. 69.37 55.91 57.89 52.44 64.19 46.45 57.3 57.04 58.82 51.89 51.93 48.65 48.7 40.76 53.65 67.44 45.05 52.17 83.33 49.55 50.39 55.79 45.45 50.68 48.09 46.07 50.14 47.06 48.41 52.96 41.21 47.67 40.22 47.36 51.94 43.56 43.63 66. 75.68 64.57 61.05 57.33 66.22 51.73 66.29 61.21 73.53 56.58 58.61 53.67 52.33 48.91 55.67 71.32 47.52 56.6 85.42 33.6 37.55 32.33 33.75 49.61 58. 40.2 46.08 57.91 65.31 33.33 36.48 31.67 33.14 34.19 33.91 44.99 52 47.37 54.95 39.67 44.28 25.61 38.21 35.19 31 38.1 47.15 37.33 31.0 32.47 37.14 32.33 31.18 38.84 40 15.79 48.35 37.19 37.32 27.94 35.33 44.44 28.3 32.03 44. 57.33 53 50.31 54.29 53.26 47.83 67.94 72 47.37 72.53 61.98 65.49 36.76 57.49 61.11 47.54 51.52 64.23 53.33 39.81 42.9 44.57 41.4 37.74 53.76 52 26.32 56.04 49.86 52.18 32.35 48.06 51.85 34.34 38.1 59.35 69.33 60.04 57.39 60.57 61.16 57.62 73.52 76 42.11 70.33 70.8 70.17 39.46 64.6 72.22 53.42 65.37 75. 64.86 53.15 59.47 50.32 55.41 51.73 53.93 55.32 50 52.5 55.01 48.12 50.26 48.37 54.91 55.81 50.5 53.87 77.08 43.92 52.84 61.33 45.59 45.97 46.86 49.77 44.2 59.68 72 31.58 59.34 57.58 58.84 36.52 51.23 66.67 38 48.05 69. 42 Table 13 Zero-shot scores (%) of LLMs across question types on BhashaBench V1. Question types: A/R = Assertion/Reason, FIB = Fill in the Blanks, MCQ = Multiple Choice Questions, MTC = Match the Columns, RC = Reading Comprehension, RTS = Rearrange the Sentence. Model BBA BBF BBK BBL A/R FIB MCQ MTC A/R FIB MCQ MTC RC RTS A/R FIB MCQ MTC RTS A/R FIB MCQ MTC RC RTS gemma-3-270m gemma-3-270m-it Param-1 gemma-2-2b gemma-2-2b-it Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct sarvam-2b-v0.5 sarvam-1 Nemotron-4-Mini-Hindi-4B-Base Nemotron-4-Mini-Hindi-4B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct granite-3.1-2b-instruct granite-3.1-3b-a800m-base Pangea-7B Indic-gemma-7b-finetuned-sft-Navarasa-2.0 aya-23-8B Llama-3.1-8B Llama-3.1-8B-Instruct gemma-2-9b gemma-2-9b-it gpt-oss-20b gemma-2-27b gemma-2-27b-it gpt-oss-120b Qwen3-235B-A22B-Instruct-2507 deepseek-v3 gpt-4o 37.04 51.85 44.44 77.78 33.33 25.93 59.26 25.93 40.74 62.96 59.26 55.56 37.04 29.63 51.85 33.33 62. 62.96 59.26 18.52 25.93 29.63 33.33 48.15 25.93 29.63 55.56 62.96 62.96 66.67 62.96 28.09 24.72 29.78 36.52 32.02 32.02 26.97 29.21 34.83 25.84 30.9 32.02 30.34 26.97 29.21 21.35 25.28 24.16 35.39 30.9 29.78 26.97 35.39 29.21 32.02 39.89 35.96 46.07 51.69 38.2 47.19 28.1 26.02 40.12 34.4 28.33 28.06 26.34 30.28 33.17 26.81 29.14 34.01 33.6 37.5 32.7 31.22 29. 37.53 35.1 32.05 33.17 34.83 44.48 34.35 36.39 47.71 37.98 52.87 58.34 46.09 59.95 39.02 29.27 24.39 26.83 36.59 26.83 26.83 36.59 29.27 36.59 26.83 36.59 24.39 29.27 29.27 29.27 29.27 34.15 31.71 17.07 34.15 46.34 31.71 39.02 46.34 26.83 39.02 41.46 31.71 31.71 36.59 28.37 24.65 29.77 21.86 32.56 28.37 28.84 27.91 35.35 27.91 23.72 29.77 27.91 34.88 27.44 33.95 26. 34.88 27.91 33.95 31.16 38.6 35.35 36.74 30.7 42.33 39.53 66.05 67.91 63.26 63.72 24.13 23.78 44.76 41.26 35.66 27.62 27.97 36.71 38.11 29.02 38.81 43.36 38.81 50.7 44.06 33.92 33.57 52.8 43.36 41.96 47.55 44.41 61.89 52.1 47.9 61.89 55.24 100 77.27 81.82 100 < 4B Models 25.21 21.85 22.69 26.89 24.37 27.73 20.17 31.09 32.77 27.73 23.53 26.05 26.05 37.82 31.09 30.25 28.57 22.35 24.71 30.59 31.76 30.59 34.12 25.88 32.94 31.76 28.24 28.24 31.76 29.41 35.29 28.24 31.76 29.41 25.05 24.12 31.53 32.38 30.4 27.6 26.29 31.65 31.71 26.1 29.12 34.09 31.57 37.5 33.2 31.31 27.78 7B to 27B Models 39.44 35.35 34.13 34.74 34.18 41.26 36.88 36 46.36 40.15 76.22 61.65 61.7 75. 35.29 38.66 33.61 28.57 33.61 32.77 37.82 27.73 36.13 36.97 31.76 25.88 31.76 31.76 30.59 31.76 29.41 31.76 36.47 31.76 > 27B Models 71.3 69.75 65.55 54.82 68.07 51.76 41.18 63.87 23.45 22.18 25.14 26.13 24.29 21.75 23.59 25.42 29.1 23.16 22.32 26.98 25.99 26.41 28.39 22.88 22. 31.92 25.14 25.28 24.86 24.72 28.39 27.97 27.26 27.97 30.51 67.06 47.18 49.01 50.59 27.47 47.69 36.27 44.75 41.98 39.2 45.37 25.93 43.98 48.61 42.9 47.22 46.14 31.94 39.2 48.92 44.44 50.46 47.69 27.16 29.78 39.51 38.89 44.44 29.32 37.04 45.99 62.81 70.99 61.11 70.22 26.53 22.45 26.53 26.53 26.53 22.45 16.33 24.49 24.49 30.61 24.49 34.69 36.73 28.57 28.57 24.49 28. 32.65 30.61 30.61 34.69 28.57 40.82 24.49 26.53 40.82 38.78 40.82 59.18 46.94 57.14 26.21 26.37 32.61 39.51 34.6 28.53 28.24 32.73 39.11 26.83 30.08 37.01 35.68 44.08 40.61 35.92 32.24 45.69 41.63 37.99 39.46 46.07 55.95 47.12 46.74 61 53.28 70.14 73.14 60.71 74.06 26.24 22.97 24.34 27.4 28.98 28.66 24.03 26.45 28.03 24.55 25.61 26.77 30.56 28.13 30.87 28.66 24. 32.35 26.34 22.76 26.24 35.3 28.45 43.1 29.61 35.19 45.94 64.17 67.76 44.89 68.6 24.88 27.75 28.71 27.75 29.67 23.92 27.75 27.75 35.41 31.58 23.44 24.88 31.1 37.32 40.19 33.49 24.88 38.76 28.23 24.88 28.23 38.76 34.93 47.37 35.41 46.89 55.02 72.73 75.12 62.2 73.21 26.74 29.3 36.51 27.91 28.84 31.86 29.3 26.98 28.37 33.95 28.84 37.67 30.47 32.33 35.35 35.12 34. 39.3 40.93 31.4 28.6 34.19 34.88 42.56 24.65 43.49 39.77 62.09 73.49 55.58 69.07 24.82 22.11 35.45 40.51 33.38 28.32 32.17 35.66 37.8 26.75 29.32 43.51 35.16 46.36 38.45 37.09 31.53 47.65 42.51 43.01 42.08 46.43 58.42 44.15 45.58 65.34 50 71.61 75.82 61.98 74.96 25.44 26.21 35.26 35.82 33.55 28.47 28.2 33.33 37.1 27.47 29.81 40.02 36.43 41.8 37.63 34.97 30. 44.78 41.26 39.55 38.74 45.41 54.34 43.33 39.14 61.47 48.4 68.42 77.17 61.92 77.19 30.1 30.3 32.32 32.73 25.86 30.71 33.54 26.26 32.32 34.34 22.63 27.88 32.53 29.7 26.26 27.88 26.06 32.93 32.73 24.65 25.86 32.93 41.01 35.76 34.95 49.9 40 55.96 61.62 45.45 62.22 23.08 21.54 32.92 32.92 30.77 24.31 22.46 35.08 38.77 29.85 32.92 36.62 35.08 44 39.38 36.31 28. 46.77 41.23 40.31 41.54 44.92 53.54 40.62 31.08 58.77 45.23 78.77 77.54 66.15 74.46 27.89 29.93 30.61 25.85 25.85 27.89 26.53 23.81 27.89 28.57 27.21 23.13 30.61 40.14 31.29 25.85 24.49 34.69 29.25 28.57 25.17 36.73 33.33 36.05 37.41 42.18 44.9 69.39 71.43 51.7 61.9 Table 15 Performance of Llama model family across sub-domains in BhashaBench v1, comparing base and instruction-tuned variants (1B, 3B, 8B) Subject Domain 3.2-1B 3.2-1B-it 3.2-3B 3.2-3B-it 3.1-8B 3.1-8B-it Administration, AYUSH & Miscellaneous Agad Tantra & Forensic Medicine Ayurvedic Literature & History Dravyaguna & Bhaishajya Kaumarbhritya & Pediatrics Kayachikitsa (General Medicine & Internal Medicine in Ayurveda) Panchakarma & Rasayana Research & Statistics Roga Vigyana (Diagnostics & Pathology) Samhita & Siddhanta (Fundamentals) Shalakya Tantra (ENT, Eye, Dentistry) Shalya Tantra (Surgery) Sharir (Anatomy & Physiology) Stri Roga & Prasuti Tantra (Gynecology & Obstetrics) Swasthavritta & Public Health Yoga & Psychology Accounting Banking Services BBA 36.97 35.29 31.93 39.5 41. 44.54 35.09 29.9 26.95 29.41 31.33 27.06 40 45 31.28 29.43 28.33 29.49 31.88 40.62 32. 39.01 33.33 30.11 32.91 34.84 30.2 44.29 42.5 27.84 35.29 30.8 33.66 33.6 51.21 31.38 33.9 30.88 29.24 31.09 34. 29.05 47.62 50 33.55 31.61 35.17 32.76 34 47.46 43.62 35.6 36.27 31.53 35.71 34.78 28.75 54.76 61. 27.9 37.47 34.6 38.93 36.36 57.17 34.57 30.66 38.34 27.68 38.68 34.54 40. 30.66 42.36 Continued on next page 28.28 27.45 26.58 28.57 29.04 27.06 27.14 35 29.92 27.25 25.48 27.12 27. 34 26.6 27.3 30.49 27.09 30.88 26.92 25.63 24.92 25.76 29.5 25 26.15 26.84 25.48 25.19 28. 32.67 24.47 BBF 26.13 28.18 43 Subject Domain 3.2-1B 3.2-1B-it 3.2-3B 3.2-3B-it 3.1-8B 3.1-8B-it Table 15 Continued from previous page Behavioral Finance Business Management Commerce Corporate Finance & Investment Data & Analytics in Finance Economics & Development Studies Energy, Infrastructure & Finance Environmental Finance Finance Education Financial Markets Financial Technology General Knowledge Governance & Policy Healthcare Economics History, Sociology & Cultural Studies of Finance Information Technology Finance Insurance & Risk Management Interdisciplinary Finance International Finance & Trade Language & Communication Legal Finance Marketing Finance Mathematics for Finance Problem Solving Rural Economics Science and Technology in Finance Sports, Media & Finance Linkages Taxation & Regulatory Compliance Agri-Environmental & Allied Disciplines Agricultural Biotechnology Agricultural Chemistry & Biochemistry Agricultural Economics & Policy Agricultural Engineering & Technology Agricultural Extension Education Agricultural Microbiology Agriculture Communication Agriculture Information Technology Agronomy Animal Sciences Crop Sciences Dairy & Poultry Science Entomology Fisheries and Aquaculture General Knowledge & Reasoning Genetics and Plant Breeding Horticulture Natural Resource Management Nematology Plant Pathology Plant Sciences & Physiology 37.31 26.51 28.51 27.58 22.83 29.56 29.27 25 29.66 36.17 17.39 31.35 28.76 31.58 24. 31.63 19.05 26.14 27.71 32.45 26.47 23.81 27.31 24.67 27.97 21.78 33.33 36.13 28.36 26.51 27.46 26.37 18.11 32.85 28.05 29.76 25.42 29.79 13.04 28.94 27.63 31.58 30.71 35.51 26.19 30.72 34.94 29.18 20.59 38.1 24.91 23.65 30.65 30.69 28.89 31.61 BBK 35.82 43.37 32.1 29.56 31.5 36.13 32.93 39.29 49.15 57.45 21.74 37.48 34.3 38.6 37.01 46.33 30.95 37.25 36.14 35.62 29.41 38.1 28.96 27.08 33.33 31.68 48.89 43. 37.31 53.01 31.52 35.05 20.47 40.51 39.02 38.69 34.75 48.94 34.78 43.04 39.29 41.23 44.88 53.06 38.1 33.33 39.76 40.59 20.59 38.1 27.57 25.15 44.83 41.58 42.22 47.1 47.76 50.6 34.41 37.91 32.28 39.42 42.68 41.07 44.92 40.43 43.48 42.3 40.13 50.88 41.73 59.59 42.86 37.91 45.78 42.49 38.24 59.52 29.97 28.1 42.53 38.61 51.11 47.1 49.25 60.24 31.98 39.23 31.5 48.18 40.24 51.19 47.46 51.06 47.83 50.09 47.84 51.75 61.42 66.33 40.48 54.9 54.22 43.66 35.29 52.38 26.3 24.6 51.72 35.64 48.89 50. 31.82 32.95 25 36.36 30.68 47. 34.35 31.32 35.09 32.79 32.3 31.53 29.53 44.21 31.84 36.49 29.87 30.34 35.49 38.24 33.13 28.02 31.21 29.02 28.26 27.96 37.98 50.95 33.81 38.12 33.2 41.99 53.15 44.49 45.79 37.22 41.89 35.34 37.08 35.49 55.88 39.64 38.3 36.86 37.82 29.35 42.82 50. 48.85 38.79 40.35 38.93 40.31 38.74 36.61 46.32 37.2 46.62 38.25 41.57 38.79 38.24 38.88 40.62 35.89 33.16 35.33 34.01 43.41 58.78 48.75 46.73 41.8 48.19 54.95 49.21 45.79 43.34 45.95 40.8 44.94 47.7 52.94 42.66 43.19 43 44.56 41.3 44.84 54. Continued on next page 31.11 27.05 29.98 27.46 30.88 34.23 33.07 30.53 27.92 25.68 31.15 35.96 29.02 29.41 28.44 30.59 27.05 28.5 22.83 28.97 28.68 28.63 22.78 25.52 26. 29.46 36.04 28.35 31.58 28.77 34.46 26.41 31.46 27.59 41.18 27.53 30.08 28.6 26.42 28.26 30.48 31.78 44 Subject Domain 3.2-1B 3.2-1B-it 3.2-3B 3.2-3B-it 3.1-8B 3.1-8B-it Table 15 Continued from previous page Seed Science and Technology Soil Science Veterinary Sciences Civil Litigation & Procedure Constitutional & Administrative Law Consumer & Competition Law Corporate & Commercial Law Criminal Law & Justice Employment & Labour Law Environmental & Energy Law Family & Personal Law General Academic Subjects Healthcare & Medical Law Human Rights & Social Justice Intellectual Property Law Interdisciplinary Studies International & Comparative Law Legal Skills & Communication Legal Theory & Jurisprudence Media & Entertainment Law Real Estate & Property Law Tax & Revenue Law Technology & Cyber Law 29.7 31.25 27.08 29.32 29.54 28 27.7 27.09 23.43 27.67 24.12 29.21 40 21.05 30.77 33.33 30.87 25.74 29.63 33.33 23.53 27.71 30.89 28.71 29.92 14. BBL 28.18 28.15 22.67 28.63 26.98 25.71 24.42 28.86 32.52 20 42.11 31.87 28.1 30.35 27.33 28.36 35.19 25.91 31.6 41.46 27.72 31.69 37.5 32.4 36.22 28 29.78 30.01 28.57 33.49 29.06 37.47 68 36.84 46.15 38.57 40.02 28.68 33.92 42.59 29.89 40.26 48.78 37.13 38.84 47.92 34.97 40.62 34.67 34.67 33.66 29.1 37.91 31.69 43.91 40 26.32 45.05 41.32 45.22 30.15 39.69 51.85 31.96 38.1 49. 35.15 37.14 43.75 36.68 42.28 46.67 35.15 35.21 32 39.07 34.21 46.87 64 31.58 56.04 43.25 46.88 28.31 41.66 38.89 31.48 41.56 51.22 38.61 45.25 70.83 42.66 49.46 41.33 42.67 42.72 40 45.81 39.86 52.68 60 36.84 58.24 53.72 54.47 32.72 46.87 53.7 38.16 43.29 60.16 Table 16 Performance of Qwen model family across sub-domains in BhashaBench v1, comparing base and instruction-tuned variants (3B, 235B) Subject Domain 2.5-3B 2.5-3B-it 3-235B-A22B-it-2507 BBA Administration, AYUSH & Miscellaneous Agad Tantra & Forensic Medicine Ayurvedic Literature & History Dravyaguna & Bhaishajya Kaumarbhritya & Pediatrics Kayachikitsa (General Medicine & Internal Medicine in Ayurveda) Panchakarma & Rasayana Research & Statistics Roga Vigyana (Diagnostics & Pathology) Samhita & Siddhanta (Fundamentals) Shalakya Tantra (ENT, Eye, Dentistry) Shalya Tantra (Surgery) Sharir (Anatomy & Physiology) Stri Roga & Prasuti Tantra (Gynecology & Obstetrics) Swasthavritta & Public Health Yoga & Psychology Accounting Banking Services Behavioral Finance Business Management BBF 45 47.06 39.86 38.73 32.57 38.52 38.61 30.35 62.86 58.75 36.79 35.56 37.45 37.44 40.73 50.99 44.68 38.94 43.3 52.24 60.24 38.66 32.71 29.9 28.94 30.11 35.07 29.59 52.86 53.75 31.93 31.74 33.08 31.35 34.24 43.49 36. 31.82 36.89 44.78 40.96 73.11 63.88 55.88 49.43 55.32 59.48 49.54 91.43 82.5 55.22 59.67 60.46 60.1 66.82 82.56 75.53 63.52 71.22 71.64 84.34 Continued on next page Subject Domain 2.5-3B 2.5-3B-it 3-235B-A22B-it-2507 Table 16 Continued from previous page Commerce Corporate Finance & Investment Data & Analytics in Finance Economics & Development Studies Energy, Infrastructure & Finance Environmental Finance Finance Education Financial Markets Financial Technology General Knowledge Governance & Policy Healthcare Economics History, Sociology & Cultural Studies of Finance Information Technology Finance Insurance & Risk Management Interdisciplinary Finance International Finance & Trade Language & Communication Legal Finance Marketing Finance Mathematics for Finance Problem Solving Rural Economics Science and Technology in Finance Sports, Media & Finance Linkages Taxation & Regulatory Compliance BBK Agri-Environmental & Allied Disciplines Agricultural Biotechnology Agricultural Chemistry & Biochemistry Agricultural Economics & Policy Agricultural Engineering & Technology Agricultural Extension Education Agricultural Microbiology Agriculture Communication Agriculture Information Technology Agronomy Animal Sciences Crop Sciences Dairy & Poultry Science Entomology Fisheries and Aquaculture General Knowledge & Reasoning Genetics and Plant Breeding Horticulture Natural Resource Management Nematology Plant Pathology Plant Sciences & Physiology Seed Science and Technology Soil Science Veterinary Sciences 43.57 40.22 35.43 43.8 45.12 47.62 50.85 42.55 47.83 41.56 45.3 48.25 38.58 64.9 30.95 41.83 49.4 45.77 38.24 69.05 34.18 27.88 47.13 40.59 44.44 56.13 43.75 55.34 44.48 46.41 41.39 46.25 54.05 44.49 52.63 41.73 47.97 42.08 52.81 39.94 38.24 44.48 43.44 37.25 37.82 33.15 40.55 45.74 42.08 42 45.83 33.72 37.58 28.35 44.16 30.49 44.05 43.22 42.55 39.13 38.22 38.16 45.61 38.58 58.16 38.1 36.6 42.17 42.71 23.53 50 29.85 26.2 45.21 43.56 53.33 38.71 43.18 51.15 38.43 43.38 37.3 42.51 43.24 44.49 54.21 38.89 46.62 36.79 46.07 39.66 50 41.6 44.22 35.41 37.31 39.13 36.52 48.06 34.65 39.35 50 Civil Litigation & Procedure 38. 35.31 BBL 63.62 63.52 53.54 73.36 71.95 82.74 69.49 70.21 78.26 74.95 74.15 78.95 83.46 92.24 64.29 79.74 78.31 77.06 76.47 85.71 58.04 47.12 80.46 72.28 68.89 74.84 75.57 91.6 83.63 73.21 67.21 72.87 90.99 78.35 74.74 71.92 77.7 67.4 75.28 77.44 79.41 73.22 76.86 64.98 65.8 63.04 78.34 86.82 66.34 72.37 87.5 72.12 Continued on next page 46 Subject Domain 2.5-3B 2.5-3B-it 3-235B-A22B-it-2507 Table 16 Continued from previous page Constitutional & Administrative Law Consumer & Competition Law Corporate & Commercial Law Criminal Law & Justice Employment & Labour Law Environmental & Energy Law Family & Personal Law General Academic Subjects Healthcare & Medical Law Human Rights & Social Justice Intellectual Property Law Interdisciplinary Studies International & Comparative Law Legal Skills & Communication Legal Theory & Jurisprudence Media & Entertainment Law Real Estate & Property Law Tax & Revenue Law Technology & Cyber Law 43.67 36 40.74 38.21 39.43 44.65 38.35 53.82 56 47.37 60.44 49.31 47.51 32.35 46.45 42.59 36.09 39.83 58.54 37.93 46.67 37.7 34.45 37.14 38.84 32.8 45.44 40 31.58 54.95 44.08 43.76 31.74 40.04 33.33 33.55 37.66 59.35 82.65 82.67 77.11 75.44 71.43 76.74 74.37 85.82 88 73.68 87.91 84.85 83.89 61.27 79.38 79.63 71.7 74.03 86.18 Table 17 Performance of GPT model family across sub-domains in BhashaBench v1, comparing different model sizes (20B, 120B, GPT-4o) Subject Domain gpt-oss-20b gpt-oss-120b gpt-4o BBA Administration, AYUSH & Miscellaneous Agad Tantra & Forensic Medicine Ayurvedic Literature & History Dravyaguna & Bhaishajya Kaumarbhritya & Pediatrics Kayachikitsa (General Medicine & Internal Medicine in Ayurveda) Panchakarma & Rasayana Research & Statistics Roga Vigyana (Diagnostics & Pathology) Samhita & Siddhanta (Fundamentals) Shalakya Tantra (ENT, Eye, Dentistry) Shalya Tantra (Surgery) Sharir (Anatomy & Physiology) Stri Roga & Prasuti Tantra (Gynecology & Obstetrics) Swasthavritta & Public Health Yoga & Psychology Accounting Banking Services Behavioral Finance Business Management Commerce Corporate Finance & Investment Data & Analytics in Finance Economics & Development Studies Energy, Infrastructure & Finance BBF 47 53.78 39.52 33.82 30.75 35.99 39.06 28.36 70.95 66.25 30.63 38.15 35.36 39.75 35.18 56.51 41.49 35.45 42.53 50.75 53.01 37.89 37.25 34.65 46.72 39.02 79.83 60.14 51.47 44.48 51.4 54.69 41.44 86.67 82.5 46.07 54.9 55.13 57.06 59.03 76.6 70. 73.61 67.29 77.61 87.95 69.76 73.63 51.97 69.34 64.63 75.63 63.54 59.31 54.78 56.58 60.69 50.76 90 81.25 53.41 62.4 61.41 62.7 64.82 81.02 73.94 49.55 68.57 76.12 81.93 54.46 61.43 44.09 71.53 67.07 Continued on next page Subject Domain gpt-oss-20b gpt-oss-120b gpt-4o Table 17 Continued from previous page Environmental Finance Finance Education Financial Markets Financial Technology General Knowledge Governance & Policy Healthcare Economics History, Sociology & Cultural Studies of Finance Information Technology Finance Insurance & Risk Management Interdisciplinary Finance International Finance & Trade Language & Communication Legal Finance Marketing Finance Mathematics for Finance Problem Solving Rural Economics Science and Technology in Finance Sports, Media & Finance Linkages Taxation & Regulatory Compliance Agri-Environmental & Allied Disciplines Agricultural Biotechnology Agricultural Chemistry & Biochemistry Agricultural Economics & Policy Agricultural Engineering & Technology Agricultural Extension Education Agricultural Microbiology Agriculture Communication Agriculture Information Technology Agronomy Animal Sciences Crop Sciences Dairy & Poultry Science Entomology Fisheries and Aquaculture General Knowledge & Reasoning Genetics and Plant Breeding Horticulture Natural Resource Management Nematology Plant Pathology Plant Sciences & Physiology Seed Science and Technology Soil Science Veterinary Sciences Civil Litigation & Procedure Constitutional & Administrative Law Consumer & Competition Law Corporate & Commercial Law Criminal Law & Justice Employment & Labour Law BBK BBL 48 55.95 46.61 61.7 47.83 48.42 39.85 49.12 48.03 76.94 47.62 45.1 54.22 47.57 41.18 61.9 30.05 26.63 47.89 45.54 46.67 44.52 41.48 65.27 54.8 46.57 39.75 43.93 53.15 42.91 51.58 44.1 53.38 41.71 52.81 48.28 50 42.81 44.47 41.26 41.97 42.93 41.56 51.94 35.15 42.45 56. 34.63 41.06 33.33 37.48 35.14 33.14 73.21 73.73 59.57 73.91 77.18 69.36 78.07 68.5 90.82 57.14 73.2 75.9 74.42 64.71 85.71 76.16 64.14 75.86 77.23 75.56 68.39 73.86 89.69 80.43 71.77 62.7 69.25 89.19 73.23 75.26 68 69.59 64.66 75.28 72.84 64.71 69.59 74.04 61.88 64.77 64.13 71.03 82.17 64.85 70.67 87.5 59.01 75.56 72 69.59 65.11 62.86 77.98 74.58 72.34 78.26 77.18 78.29 80.7 87.4 92.04 64.29 75.82 85.54 77.48 76.47 78.57 41.28 42.65 82.76 73.27 73.33 73.55 74.43 89.31 81.14 73.68 66.8 75.19 94.59 81.1 68.42 72.43 76.35 68.85 78.65 77.87 73.53 68.38 75.84 70.14 65.8 64.67 78.34 88.37 65.84 73.18 93. 71.91 83.15 81.33 78.93 75.95 73.14 Continued on next page Subject Domain Environmental & Energy Law Family & Personal Law General Academic Subjects Healthcare & Medical Law Human Rights & Social Justice Intellectual Property Law Interdisciplinary Studies International & Comparative Law Legal Skills & Communication Legal Theory & Jurisprudence Media & Entertainment Law Real Estate & Property Law Tax & Revenue Law Technology & Cyber Law Table 17 Continued from previous page gpt-oss-20b gpt-oss-120b gpt-4o 41.4 37.03 56.49 60 15.79 53.85 43.25 48.86 32.84 42.08 50 32.59 42.86 56.91 69.3 63.87 83.14 92 73.68 85.71 82.64 79.42 69.12 75.16 83.33 59.62 67.53 86.18 73.26 72.86 84.79 92 68.42 90.11 83.75 81.7 53.43 81.21 85.19 71.7 69.26 86."
        }
    ],
    "affiliations": [
        "Indian Institute of Technology Bombay (IIT Bombay)",
        "Technology Innovation Hub (TIH) at IIT Bombay"
    ]
}