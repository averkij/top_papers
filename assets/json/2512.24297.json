{
    "paper_title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
    "authors": [
        "Meiqi Chen",
        "Fandong Meng",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning."
        },
        {
            "title": "Start",
            "content": "Figure It Out: Improve the Frontier of Reasoning with"
        },
        {
            "title": "Active Visual Thinking",
            "content": "Meiqi Chen, Fandong Meng, Jie Zhou WeChat AI, Tencent Inc {meiqiichen, fandongmeng, withtomzhou}@tencent.com 5 2 0 2 0 3 ] . [ 1 7 9 2 4 2 . 2 1 5 2 : r Figure 1: Comparison of four reasoning paradigms on geometry problem with complex spatial constraints. Text-only CoT struggles to capture spatial relations. Unified multimodal CoT often suffers from imprecise visual grounding. Tool-augmented LVLMs are limited by predefined operations. In contrast, FIGR generates executable code to construct figures, which enables geometric consistency and leads to correct reasoning."
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chainof-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning. Human reasoning is deeply intertwined with the ability to externalize information into visual form. When faced with complex constraints or multistep relationships, people naturally draw diagrams, sketch intermediate states, or construct spatial layouts to reduce cognitive load and clarify underlying structure (Tversky et al., 2019; Norman, 2024). Cognitive science further shows that such diagrammatic reasoning allows people to discover patterns that are difficult or even impossible to glean from text alone (Donald, 1993; Mandler, 2010). Figure 1 illustrates how different reasoning paradigms behave when solving geometry problem involving angle constraints, tangency conditions, and point intersections. text-only chainof-thought (CoT, (Wei et al., 2022)) model must implicitly track dense network of spatial relations through symbolic expressions alone. As shown in the leftmost column, this often leads to algebraic errors caused by subtle misinterpretations of positions or angles, since all geometric constraints must *https://github.com/chenmeiqii/FIGR. Corresponding author. 1 be internally maintained without explicit spatial grounding. To alleviate this, natural extension is to explore unified multimodal models (Team, 2024; Wu et al., 2025b; Deng et al., 2025; Li et al., 2025a) that directly generate images as part of the chain of thought. By externalizing intermediate visual representations, these models aim to make spatial relationships more explicit. However, as illustrated in Figure 1, such directly generated images are often noisy or spatially imprecise, making it difficult to reliably encode fine-grained geometric constraints. Even minor visual inaccuracies can accumulate during multi-step reasoning and critically impair downstream symbolic computation. In parallel, growing line of research investigates tool-augmented large vision-language models (LVLMs), which invoke external visual tools or predefined APIs to assist reasoning, spanning prompt engineering (Gupta and Kembhavi, 2023; Hu et al., 2024; Surís et al., 2023), fine-tuning (Wu and Xie, 2024; Wang et al., 2025), and reinforcementlearningbased approaches (Zheng et al., 2025; Zhang et al., 2025). By delegating visual processing to external tools, such methods improve controllability and enforce adherence to predefined operations. Nevertheless, as shown in Figure 1, toolaugmented LVLMs remain restricted to limited set of transformations applied to given images (e.g., zooming or cropping), and cannot autonomously generate task-specific visual representations from scratch. This restriction limits their expressive flexibility when reasoning requires constructing new diagrams to satisfy complex constraints. Motivated by these limitations, we introduce FIGR, which integrates visual construction directly into the reasoning loop through end-to-end reinforcement learning. Instead of generating images directly or relying on fixed toolsets, FIGR produces executable code that serves as bridge between symbolic reasoning and visual rendering. This design enables the model to actively construct and iteratively refine illustrative figures during multi-turn reasoning. The rendered figures then serve as dynamic, interpretable feedback that supports its reasoning process, much like how humans repeatedly sketch and update diagrams when solving complex problems. As illustrated in Figure 1, this figure-guided process enforces geometric consistency and leads to correct solutions in problems where other paradigms fail. Together, this comparison highlights key insight: effective visual reasoning requires not only access to visual representations, but also precise, controllable, and interpretable mechanisms for constructing them within the reasoning loop. To guide this iterative reasoningrendering process, we design an Adaptive Reward Mechanism that regulates when visual reasoning should be invoked, encouraging selective use of visual cues while discouraging unnecessary or spurious rendering behaviors. This mechanism serves as control signal that promotes appropriate engagement with figure construction, while accuracy rewards ultimately govern reasoning quality. Crucially, this design eliminates the need for supervised fine-tuning cold-start stage. Instead, FIGR can be initialized directly from an instruction-tuned model (e.g., Qwen3-VL-32B-Instruct (Bai et al., 2025a)) and refined solely through reinforcement learning. Through this process, FIGR autonomously learns when to invoke visual reasoning and how to integrate execution feedback into its reasoning trajectory, without relying on handcrafted heuristics or human-engineered demonstrations. Overall, our main contributions are as follows: We propose FIGR, which actively conducts visual thinking during multi-turn reasoning. We design novel adaptive reward mechanism that enables selective use of visual reasoning, without requiring supervised cold-start. We conduct extensive experiments across diverse mathematical reasoning benchmarks, demonstrating substantial gains over text-only baselines."
        },
        {
            "title": "2.1 Text-based Reasoning",
            "content": "A series of works improves upon chain-of-thought (CoT) prompting (Wei et al., 2022) by introducing structured reasoning strategies in large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023). Program-of-Thought (PoT) (Chen et al., 2022) and Chain-of-Code (Li et al., 2023) interleave natural language with executable code so that the models can offload arithmetic and symbolic manipulation to interpreters. Tree-of-Thought (ToT) (Yao et al., 2023) and its variants represent reasoning as branching search rather than single chain, allowing models to explore alternative solution paths and prune unpromising branches. DeepSeek-R1 (Guo et al., 2025) successfully applies RL to help models learn when to expand or truncate their reasoning paths. ReTool (Feng et al., 2025) further teaches models when and how to call 2 code interpreter during mathematical problem solving, enabling dynamic interleaving of textual reasoning and tool invocation. Nevertheless, even state-of-the-art text-only models struggle with geometry, kinematics, or combinatorial tasks requiring spatial inference. This limitation stems from their inability to externalize intermediate states visually, forcing all constraints to be encoded implicitly in textexactly the bottleneck illustrated in our introduction."
        },
        {
            "title": "2.2 Multimodal Reasoning",
            "content": "Large visionlanguage models (LVLMs) (Liu et al., 2023; Bai et al., 2025b) demonstrated that LLMs can interpret images through relatively shallow alignment layers, but most of them remain limited to text-only outputs. Recent unified architectures (Team, 2024; Wu et al., 2025b; Deng et al., 2025; Li et al., 2025a) extend this capability by generating both text and images. However, the quality of the generated images remains limited, preventing them from supporting fine-grained and precise relational depiction. These issues become especially evident in geometric and scientific reasoning tasks, where even minor visual inaccuracies can propagate into significant logical errors. parallel literature explores invoking external tools, rendering engines, or predefined APIs as part of the reasoning loop. Recent work demonstrates that LVLMs can decompose problem, apply visual operators (e.g., zooming, cropping, object detection), and interpret results to refine reasoning (Gupta and Kembhavi, 2023; Surís et al., 2023; Hu et al., 2024; Shao et al., 2024a; Wu and Xie, 2024; Li et al., 2025b; Shen et al., 2025; Wang et al., 2025). Reinforcement learningbased pipelines (Zheng et al., 2025; Zhang et al., 2025) further improve tool usage efficiency by rewarding correct invocation patterns. These methods offer strong interpretability and deterministic visual transformations, making them highly effective on perceptioncentric benchmarks. However, they are inherently restricted to limited operations on given images. They lack support for generating new diagrams, geometric constructions, or abstract visualizations that are not directly present in the input."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we describe the main components of FIGR. We first outline the overall architecture and reasoning loop in Section 3.1, then describe the reinforcement-learning (RL) algorithm in Section 3.2, and finally detail the adaptive reward mechanism in Section 3.3. Figure 2 gives an overview of our FIGR."
        },
        {
            "title": "3.1 Figure-Steered Reasoning Loop",
            "content": "Our core idea is to embed visual construction into models reasoning trajectory. Concretely, for each problem input, FIGR enters multi-turn reasoning loop in which it can interleave pure textual reasoning and executable code to generate diagrams - much like human drawing intermediate sketches while reasoning. Formally, at each turn t, FIGR maintains an internal context ht (e.g., textual history, prior code outputs, and rendered images) and optionally current figure/diagram It. The models policy πθ outputs an action: at πθ ( ht, It) . (1) Here, at may be: (1) text continuation (e.g., next reasoning step), or (2) An executable code snipper ct written in general-purpose programming language (e.g., Python). If at is code, the environment executes it via sandboxed interpreter, which may produce textual outputs and render figure: (cid:0)Tt+1, It+ (cid:1) = Interpreter (ct) , (2) where Tt+1, It+1 denotes the textual and visual feedback from code execution, respectively. The context is then updated as: ht+1 = UpdateContext (ht, ct, Tt+1, It+1) . (3) This multi-turn reasoning repeats until the model emits special end token (e.g., <End>), or maximum number of steps is reached. The produced final answer is then evaluated. Overall, the design enables both flexible visual construction and precise code-based control, providing reproducibility and interpretability."
        },
        {
            "title": "3.2 Reinforcement Learning Procedure",
            "content": "To train the model to decide when to construct figures and how to integrate them into the reasoning process, we employ reinforcement learning. Specifically, we adopt the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024b), together with multi-turn rollout framework implemented in VeRL (Sheng et al., 2024). 3 Figure 2: Overview of our proposed FIGR. FIGR alternates between textual reasoning and executable drawing steps with visual feedback. An adaptive reward mechanism dynamically regulates visual thinking based on task suitability and execution outcomes. For each input question, GRPO samples group of candidate trajectories (rollouts) under the current policy, computes their final rewards, and uses the group mean reward as baseline. Trajectories that yield above-average reward are reinforced; below-average ones are suppressed. This yields sample-efficient optimization. In our adaptation, state includes: question, previous reasoning and code outputs, all rendered images so far, and any interpreter feedback. Action at can be text continuation or code snippet. The environment handles code execution (if applicable), renders diagrams, and returns observations for the next turn. At the end of each rollout τi (i.e., after <End>), we compute the total reward: Ri = Racc (τi) + Rfmt (τi) + Rvis (τi) , (4) where Racc measures correctness of the final answer (e.g., 1 if answer matches the ground truth, else 0), Rfmt is the format reward, granted when the models output adheres to the required structural format (e.g., binary signal if the output includes separate reasoning and final answer sections), Rvis regulates when visual reasoning is invoked (details are introduce in Section 3.3). Within each training batch, for given prompt, 4 we sample group of candidate trajectories {τi}G i=1 using our policy πθ. For each trajectory, we compute Ri. We then compute the group baseline: ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 Ri, (5) and the relative advantage for each trajectory: ˆAi = Ri R. GRPO updates the policy by maximizing the (6) following surrogate objective (in expectation): JGRPO(θ) = qD,{τi}G i=1πθold"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 τi τi (cid:88) t=1 Mi,t(θ) β DKL(πθ πref ), (7) where (cid:105) πθold Mi,t(θ) = min (cid:104) ri,t(θ) ˆAi, clip(ri,t(θ), 1 ε, 1 + ε) ˆAi Here, ri,t(θ) = πθ(ai,tsi,t) . (8) (ai,tsi,t) is the probability ratio; ε is clipping hyperparameter; πref is reference policy (e.g., initial base policy) used for KL-regularization; β is regularization coefficient. Above all, this objective encourages the new policy to increase the probability of trajectories whose reward is above the group-average, while penalizing divergence from the reference policy to maintain stability."
        },
        {
            "title": "3.3 Adaptive Reward Mechanism",
            "content": "To encourage selective visual thinking while avoiding unnecessary drawing behaviors, we introduce an adaptive reward mechanism. Given an input question q, we prompt larger language model as classifier to predict suitability tag {0, 1}, indicating whether drawing-based visual thinking is likely to be beneficial for the question.* During reinforcement learning, the visual-invocation reward is defined by jointly considering (i) whether the final answer is correct, (ii) whether the problem is suitable for visual reasoning, and (iii) whether the generated code is successfully executed. Let Rvis denote the reward associated with diagram generation, we have: Rvis = 1.0, 0.2, 0, if = ˆy, = 1, exec = 1, if = ˆy, = 0, exec = 1, otherwise, (9) where and ˆy denote the ground-truth and predicted final answers, respectively, exec indicates whether the generated code is successfully executed and produces valid visual feedback. Under this formulation, full visual reward (1.0) is granted only when the model answers correctly and invokes diagram generation appropriately with executable code. reduced reward (0.2) is assigned when the answer is correct but diagram construction is deemed unnecessary, discouraging redundant visual invocations while still acknowledging successful visual thinking. All other cases, such as incorrect answers or failed executions, will receive zero reward. This design encourages selective and effective use of visual reasoning, while grounding its ultimate utility in final answer correctness. As result, the adaptive reward mechanism primarily serves as control signal that guides when visual reasoning should be invoked. This design reduces unnecessary or spurious figure construction while allowing the model to learn how to integrate diagrammatic feedback effectively into reasoning."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Implementation Details For the base model, we use Qwen3-VL-32B-Instruct (Bai et al., 2025a) to train FIGR. We set the maximum number of interaction rounds to 3, the maximum generation length to 32,768 tokens, and the sampling temperature to 0.7. Training Dataset We train our FIGR on DeepMath-103K (He et al., 2025), rigorously decontaminated, large-scale dataset of 103,000 challenging mathematical problems with verifiable answers, explicitly curated to avoid overlap with standard test benchmarks. In addition, as described in Section 3.3, we annotate each instance in DeepMath-103K with binary label indicating whether the current problem is suitable for figuresteered reasoning. The prompt template we use is shown in Appendix B.1. Evaluation Datasets To ensure comparability with prior work, we conduct evaluations on suite of mathematical reasoning datasets that are widely used as standard benchmarks. We use AIME 2024 (AIME, 2024), AIME 2025 (AIME, 2025), BeyondAIME (Seed et al., 2025), MATH 500 (Hendrycks et al., 2021; Lightman et al., 2023), AMC (AMC, 2023), MinervaMath (Lewkowycz et al., 2022), and the OE_TO_maths_en_COMP subset of OlympiadBench (He et al., 2024). The dataset details are introduced in Appendix A. Baselines We compare FIGR with the following competitive baselines: Large Language Models (LLMs). These models take textual inputs and generate textual outputs. We include Qwen3-235B-A22B (Thinking) 2025) et Qwen3-32B (Non-Thinking, and Thinking) (Yang et al., 2025). (Yang al., Large Vision-Language Models (LVLMs). These models could accept both text and images as inputs and produce textual outputs. We evaluate GLM-4.5V (108B) (Hong et al., 2025), MiMo-VL-7B-SFT-2508 (Yue et al., 2025), and Qwen3-VL-32B-Instruct. Unified Multimodal Models (UMMs). These models also take both text and images as inputs, but can generate visual outputs. We include Bagel-7B-MoT (Deng et al., 2025) and Bagel-Zebra-CoT (Li et al., 2025a). *In this work, we adopt Deepseek-V3 (Liu et al., 2024) as the classifier because it offers favorable balance between instruction-following capability and computational cost compared with other tested LLMs. Tool-Augmented Vision-Language Models (TAVLMs). These models enhance visual reasoning by invoking predefined tools or APIs to"
        },
        {
            "title": "Model",
            "content": "Large Language Models Qwen3-235B-A22B (Thinking) Qwen3-32B (Non-Thinking) Qwen3-32B (Thinking) Large Vision-Language Models GLM-4.5V (108B) MiMo-VL-7B-SFT-2508 Qwen3-VL-32B-Instruct Text-only RL FIGR (ours) Gains AIME 2024 AIME"
        },
        {
            "title": "Beyond\nAIME",
            "content": "MATH"
        },
        {
            "title": "Olymp\nBench",
            "content": "83.80 31.00 81.40 76.20 73.80 73.33 73.33 79.58 +6.25 80.78 20.20 72.90 67.34 68.07 66.20 69.22 79.32 +13. 52.00 16.00 40.00 47.00 39.00 43.00 46.00 54.00 +11.00 95.40 88.60 97.30 96.20 94.40 93.60 94.40 95.00 +1. 93.98 61.45 93.98 87.95 91.57 84.34 87.95 93.98 +9.64 46.69 39.34 45.22 38.60 36.40 37.87 43.38 41.92 +4. 74.04 52.08 71.96 70.03 68.40 67.51 66.18 70.33 +2.82 Avg. 75.24 44.10 71.82 69.62 67.38 66. 68.64 73.45 +6.90 Table 1: Main results (%) of FIGR on seven mathematical reasoning benchmarks compared with several competitive baselines. Green-colored font indicates improvement over the baseline Qwen3-VL-32B-Instruct. process given images. We include two RL-based models DeepEyes (Zheng et al., 2025) and Chain-of-Focus (Zhang et al., 2025). In addition, we provide text-only RL baseline, where the base model is trained with GRPO solely on the problemanswer pairs from DeepMath103K. This setting does not involve multi-turn rollout, image generation, or any external feedback, and the reward is computed only from the final textual outputcovering both answer correctness and output format. For all baseline models, we set the maximum generation length, sampling temperatures, and other hyperparameters follow the recommended configurations from their respective papers or reports. The prompt templates are shown in Appendix B.2. Due to space constraints, for UMMs and TAVLMs, we report results on subset of datasets in the main ablation study for clarity; full results are deferred to Appendix C. Evaluation Metrics To ensure stable evaluation, we default to pass@k evaluation (Chen, 2021) and report pass@1 metrics. Specifically, we generate 64 responses for each question of AIME 2024 and AIME 2025 (i.e., = 64), and = 1 for the remaining datasets. The pass@1 metric is then computed as: pass@1 = 1 i=1 pi, where pi dek notes the correctness of the i-th response. (cid:80)k"
        },
        {
            "title": "4.2 Main Results",
            "content": "As shown in Table 1, FIGR achieves substantial gains by incorporating visual cues through end-to-end reinforcement learning, reaching an average accuracy of 73.45% across seven mathematical benchmarks, exceeding the base model (Qwen3-VL-32B-Instruct) by 6.90% and the text-only RL baseline by 4.81%. Notably, the improvements are particularly pronounced on challenging benchmarks such as AIME 2025 and BeyondAIME. FIGR significantly outperforms equivalently sized text-only models (Qwen3-32B Thinking) as well as larger LVLM GLM-4.5V. These results indicate that purely textual chain-ofthought reasoning faces clear limitations on hard problems, whereas multimodal chain-of-thought effectively unlocks the reasoning capacity of FIGR."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "As shown in the Table 2, we conduct ablation studies on two representative datasets: AIME 2025 and BeyondAIME, to evaluate the contributions of different components in FIGR. Prompt Engineering (PE) on the Base Model. Prompt engineering introduces structured multimodal reasoning patterns without updating model parameters. While this leads to moderate performance gains on some datasets, the absence of learning signal prevents the model from internalizing when and how such behaviors should be applied. As result, the induced multimodal reasoning behaviors remain unstable, which inherently limits the attainable performance gains. Performance Comparison between SFT and RL. When fine-tuning the base model on DeepMath103K, supervised fine-tuning (SFT) leads to notable performance drop (e.g., on AIME 2025, accuracy decreases from 66.20% to 53.33%). In contrast, text-only reinforcement learning yields consistent performance improvements. This indicates that SFT tends to overfit to training trajectories and generalizes poorly to unseen problems, whereas 6 Figure 3: Ablation analysis across training steps. We report average response length, code count, code ratio, average code lines per executable block, and code pass rate under different settings. FIGR enables more frequent and structured visual reasoning while preserving stable execution behavior, compared to ablated settings."
        },
        {
            "title": "Model",
            "content": "Base Model + PE + SFT + Text-only RL + Bagel generated img + Qwen generated img Bagel-7B-MoT DeepEyes FIGR (ours) w/o ARM w/o img feedback AIME"
        },
        {
            "title": "Beyond\nAIME",
            "content": "66.20 63.33 53.33 69.22 64.32 62.93 10.00 2.34 79.32 70.00 73.33 43.00 46.00 38.00 46.00 41.00 41.00 1.00 0.00 52.25 49.00 47. Table 2: Ablation Study on FIGR. The base model is Qwen3-VL-32B-Instruct, PE denotes prompt engineering, SFT denotes supervised fine-tuning, and ARM denotes the adaptive reward mechanism. outcome-driven RL enables more robust and transferable problem-solving behavior. Injecting Visual Information without Active Visual Reasoning. We evaluate two baselines that directly inject visual information(+ Bagel img and + Qwen img), which generate diagram conditioned on the problem text by using Bagel-7B-MoT and Qwen-Image (Wu et al., 2025a), respectively. Although both baselines incorporate visual inputs, neither yields consistent improvements over the base model or text-only RL. These results indicate that passively injecting images fails to provide reliable structural guidance and may instead introduce noise. This contrast highlights the importance of active, feedback-driven visual reasoning in FIGR. Performance of UMMs and TAVLMs. Both unified multimodal models and tool-augmented vision-language models exhibit consistently weak performance. Unified models are vulnerable to cascading errors when tackling highly complex problems. Tool-augmented methods are restricted to predefined operations on given images and cannot flexibly construct task-specific diagrams. Consequently, neither paradigm provides reliable and informative visual feedback for complex mathematical reasoning. Due to space limitations, their full results are deferred to Appendix Ablations on FIGR Components. We further design two ablations to examine the contributions of FIGR components: (1) Removing the adaptive reward mechanism (ARM). We disable the visual-invocation reward, while keeping all other components unchanged. This ablation removes the explicit control signal that regulates when visual reasoning should be invoked. (2) Removing visual feedback. Building on (1), we further remove visual feedback during reinforcement learning. The model still performs multi-turn rollouts and executes code, but receives only textual feedback. This setting isolates the contribution of visual feedback in forming stable multimodal reasoning loop. Both ablations lead to substantial performance degradation compared to FIGR, demonstrating the effectiveness of the proposed components. Evolution of active visual thinking during training. We further analyze the emergence of visual-thinking behaviors on the BeyondAIME dataset, using four complementary metrics: response length (measured in tokens with the Qwen3-VL-32B-Instruct tokenizer), code ratio (fraction of samples that invoke code), average code lines (non-empty lines per code block), and code pass rate (fraction of code blocks that execute successfully). (1) When visual feedback is removed (w/o vis), the model exhibits persistently low code usage despite retaining textual execution feedback. This behavior highlights critical limitation: textual feedback fails to introduce structurally novel or 7 Figure 4: Case study of reasoning behaviors. FIGR demonstrates more effective integration of visual feedback into the reasoning process, resulting in clearer intermediate reasoning and improved final answers, while the baseline models rely primarily on textual reasoning. irreducible information beyond what the model can already generate in natural language. Consequently, the model learns to bypass code execution and instead relies on increasingly verbose textual reasoning, as reflected by longer responses but persistently low code ratio. (2) Removing the adaptive reward mechanism (w/o ARM) leads to transient increase in early code usage. However, this behavior rapidly collapses: both code ratio and code length drop to zero as the model learns that indiscriminate code invocation does not improve outcomes. This pattern reflects an overuseabandonment cycle in the absence of effective reward guidance. (3) In contrast, FIGR maintains consistently high code ratio throughout training, accompanied by stable and substantially longer code blocks with high execution pass rate. Visual feedback introduces an external representational modality that cannot be easily replicated within the language space, breaking the text-only reasoning loop. Consequently, code execution becomes necessary and reliable mechanism for constructing and validating global structure, rather than an opportunistic or exploratory behavior."
        },
        {
            "title": "4.4 Case Study",
            "content": "Figure 4 compares Qwen3-VL-32B-Instruct, the variant without visual feedback, and FIGR on representative example. The baseline model relies on textual reasoning and symbolic manipulation, requiring careful analytical handling of boundary cases, making it prone to subtle oversights. The variant without visual feedback introduces multiturn rollouts and intermediate code execution, but receives only textual feedback. While this allows the model to verify intermediate numerical values or symbolic expressions, it provides no access to visual cues that reveal global structural properties of the problem (e.g., the overall shape of function or the location of its zeros). Consequently, reasoning remains largely local and algebraic, and incorrect assumptions are difficult to detect or correct. In contrast, FIGR closes the reasoning loop by incorporating visual feedback, enabling it to observe global structural patterns and adjust subsequent reasoning steps accordingly. This leads to more robust and interpretable solution. Overall, this case study shows that the advantage of FIGR does not stem merely from executing intermediate programs, but from actively integrating visual feedback into the reasoning process. Compared with purely textual reasoning or execution with text-only feedback, active visual thinking provides complementary information that substantially improves reasoning reliability."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce FIGR, which integrates active visual thinking into multi-turn reasoning through end-toend reinforcement learning. By closing the loop between reasoning and visual evidence, FIGR enables models to reason over global structural properties that are difficult to infer from text alone. An adaptive reward mechanism regulates when visual reasoning should be invoked, promoting selective and reliable figure construction without unnecessary generation. Extensive experiments highlight the importance of actively incorporating visual feedback during reasoning and suggest promising direction for interpretable multimodal reasoning."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. AIME. 2024. American invitational mathematics examination (aime). AIME. 2025. American invitational mathematics examination (aime). AMC. 2023. American mathematics competitions (amc). Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, and 1 others. 2025a. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025b. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Mark Chen. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, and 1 others. 2025. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683. Merlin Donald. 1993. Origins of the modern mind: Three stages in the evolution of culture and cognition. Harvard university press. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1495314962. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828 3850. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, and 1 others. 2025. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, and 1 others. 2025. Glm-4.5v and glm-4.1vthinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. 2024. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and 1 others. 2022. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857. Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, and 1 others. 2025a. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746. tree-based image exploration. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 66136629. Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li FeiFei, Fei Xia, and Brian Ichter. 2023. Chain of code: Reasoning with language model-augmented code emulator. arXiv preprint arXiv:2312.04474. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. 2025b. Dyfo: training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 9098 9108. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Jean Mandler. 2010. The spatial foundations of the conceptual system. Don Norman. 2024. Things that make us smart. Diversion Books. ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, and 1 others. 2025. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024a. Visual cot: Advancing multimodal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024b. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. 2025. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11888 11898. Chameleon Team. 2024. Chameleon: Mixed-modal arXiv preprint early-fusion foundation models. arXiv:2405.09818. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Barbara Tversky, Julie Heiser, Paul Lee, and Jeffrey Zacks. 2019. Diagrams to augment cognition. In Proceedings of the Twenty-Fourth Annual Conference of the Cognitive Science Society, pages 5757. Routledge. Haozhe Wang, Alex Su, Weiming Ren, Fangzhen Lin, and Wenhu Chen. 2025. Pixel reasoner: Incentivizing pixel-space reasoning with curiosityarXiv preprint driven reinforcement arXiv:2505.15966. learning. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, and 1 others. 2025a. arXiv preprint Qwen-image technical report. arXiv:2508.02324. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and 1 others. 2025b. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977. Penghao Wu and Saining Xie. 2024. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1308413094. 10 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, and 1 others. 2025. Mimo-vl technical report. arXiv preprint arXiv:2506.03569. Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, and 1 others. 2025. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. 2025. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362. 11 AIME 2024 AIME"
        },
        {
            "title": "Beyond\nAIME",
            "content": "MATH"
        },
        {
            "title": "Model",
            "content": "Large Language Models Qwen3-235B-A22B (Thinking) Qwen3-32B (Non-Thinking) Qwen3-32B (Thinking) Large Vision-Language Models GLM-4.5V (108B) MiMo-VL-7B-SFT-2508 Qwen3-VL-32B-Instruct Unified Multimodal Models Bagel-7B-MoT Bagel-Zebra-CoT (7B) 83.80 31.00 81.40 76.20 73.80 73.33 10.00 0. Tool-Augmented Vision-Language Models 6.98 DeepEyes 1.30 Chain-of-Focus 80.78 20.20 72.90 67.34 68.07 66.20 3.33 5.55 2.34 0.00 52.00 16.00 40. 47.00 39.00 43.00 1.00 0.00 0.00 0.00 Text-only RL FIGR (ours) Gains 73.33 79.58 +6.25 69.22 79.32 +13. 46.00 54.00 +11.00 95.40 88.60 97.30 96.20 94.40 93.60 56.00 20.60 11.00 1.20 94.40 95.00 +1. 93.98 61.45 93.98 87.95 91.57 84.34 22.89 9.64 18.07 0.00 87.95 93.98 +9.64 46.69 39.34 45. 38.60 36.40 37.87 9.56 2.94 5.15 0.74 43.38 41.92 +4.05 74.04 52.08 71.96 70.03 68.40 67. 26.11 8.46 13.65 0.45 66.18 70.33 +2.82 Avg. 75.24 44.10 71.82 69.62 67.38 66. 18.70 6.74 8.17 0.53 68.64 73.45 +6.90 Table 3: Full experimental results (%) on seven mathematical reasoning benchmarks. Green-colored font indicates the improvement of FIGR over the baseline Qwen3-VL-32B-Instruct."
        },
        {
            "title": "A Dataset Details",
            "content": "We evaluate models on suite of challenging mathematical reasoning benchmarks, spanning competition problems, standardized mathematics datasets, and advanced reasoning tasks: (1) AIME 2024 (AIME, 2024) dataset contains 30 problems from the American Invitational Mathematics Examination 2024. Each problem requires multi-step reasoning across algebra, number theory, combinatorics, geometry, and probability. (2) AIME 2025 (AIME, 2025). Like the AIME 2024 dataset, the AIME 2025 dataset comprises the 30 problems from the 2025 AIME competitions. (3) Beyond AIME (Seed et al., 2025) is recently proposed benchmark of 100 problems. It is designed to push beyond standard AIME problems by emphasizing questions that reduce memorization and increase the complexity of reasoning. (4) MATH 500 (Hendrycks et al., 2021; Lightman et al., 2023). The MATH dataset contains 12,500 problems with step-by-step solutions spanning diverse mathematical topics. For evaluation focused on high-difficulty items, the MATH 500 subset (500 problems) is often used. (5) AMC (AMC, 2023). The American Mathematics Competitions (AMC) dataset consists of 83 problems from the AMC series, set of standardized mathematics contests administered annually. (6) MinervaMath (Lewkowycz et al., 2022) includes 272 undergraduate-level quantitative reasoning problems that require logical deductions and multi-step solutions, serving as more advanced testbed beyond high-school competition problems. (7) OlympiadBench (He et al., 2024) is an Olympiad-level bilingual multimodal scientific benchmark that includes thousands of challenging mathematics and physics problems compiled from international competitions, with detailed annotations for step-by-step reasoning. In this work, we specifically evaluate the math-focused English comprehensive subsets from OlympiadBench: OE_TO_maths_en_COMP. This subset is an English open-ended, text-only, comprehensive math subset containing 674 problems, emphasizing rigorous mathematical reasoning in text form."
        },
        {
            "title": "B Prompt Templates",
            "content": "B.1 Prompt Template for Suitability Classifier We provide dedicated prompt template for the figure-steered suitability classifier as introduced in Section 3.3 and Section 4.1, which is used to determine whether given problem is suitable for visual-guided reasoning. As shown in Figure 5, the template instructs the model to assess the necessity and usefulness of visual sketches based solely on the problem statement. B.2 Prompt Template for Reasoning We adopt three different categories of prompt templates corresponding to the reasoning settings. (1) As shown in Figure 6, multi-turn prompt with 12 Figure 5: Prompt template for the figure-steered suitability classifier. drawing feedback is used for FIGR, the variant without visual feedback, and the variant without ARM. Models can iteratively generate output and (2) As receive feedback from the interpreter. shown in Figure 7, standard single-turn prompt is adopted for text-only RL and other baselines, requiring the model to directly output the final answer without intermediate interactions. (3) For DeepEyes and Chain-of-Focus, we adopt the prompt templates recommended by the original papers."
        },
        {
            "title": "C Additional Experimental Results",
            "content": "As shown in Table 3, we present the full experimental results, including unified multimodal models and tool-augmented vision-language models that are partially reported in the main context. 13 Figure 6: Prompt template for multi-turn CoT. Figure 7: Prompt template for single-turn CoT."
        }
    ],
    "affiliations": [
        "WeChat AI, Tencent Inc"
    ]
}