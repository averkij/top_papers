{
    "paper_title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
    "authors": [
        "LG AI Research",
        "Soyoung An",
        "Kyunghoon Bae",
        "Eunbi Choi",
        "Kibong Choi",
        "Stanley Jungkyu Choi",
        "Seokhee Hong",
        "Junwon Hwang",
        "Hyojin Jeon",
        "Gerrard Jeongwon Jo",
        "Hyunjik Jo",
        "Jiyeon Jung",
        "Yountae Jung",
        "Hyosang Kim",
        "Joonkee Kim",
        "Seonghwan Kim",
        "Soyeon Kim",
        "Sunkyoung Kim",
        "Yireun Kim",
        "Yongil Kim",
        "Youchul Kim",
        "Edward Hwayoung Lee",
        "Haeju Lee",
        "Honglak Lee",
        "Jinsik Lee",
        "Kyungmin Lee",
        "Woohyung Lim",
        "Sangha Park",
        "Sooyoun Park",
        "Yongmin Park",
        "Sihoon Yang",
        "Heuiyeen Yeen",
        "Hyeongu Yun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 2 6 8 4 0 . 2 1 4 2 : r EXAONE 3.5: Series of Large Language Models for Real-world Use Cases LG AI Research"
        },
        {
            "title": "Abstract",
            "content": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai."
        },
        {
            "title": "Introduction",
            "content": "EXAONE 3.0 instruction-tuned large language model with 7.8B parameters [41] demonstrated strong bilingual capabilities in Korean and English with exceptional real-world performance and instruction-following proficiency. Since its release, we have received diverse feedback from both academic and industrial communities. For instance, academic researchers have emphasized the need for smaller models that can be trained and deployed on low-specification GPUs due to limited access to advanced computational infrastructure. The industry has expressed strong demand for larger models with enhanced performance that remain cost-effective, as well as smaller models suitable for on-device deployment. Additionally, with the increasing adoption of retrieval-augmented generation (RAG) techniques, which generate answers based on reference documents or web search results, there has been substantial demand for models capable of effectively handling longer contexts. In this report, we present EXAONE 3.5 language models, collection of instruction-tuned language models ranging from 2.4B to 32B parameters, developed to meet the diverse needs of users. EXAONE 3.5 language models include: 1) 2.4B model optimized for deployment on small or resource-constrained devices, 2) 7.8B model matching the size of its predecessor but offering improved performance, and 3) 32B model delivering exceptional performance. All models support long-context processing of up to 32K tokens. Each model demonstrates state-of-the-art performance in real-world use cases and long-context handling, while remaining competitive in general domains compared to recently released models of similar sizes. With the release of the EXAONE 3.5 language models, we hope to support researchers to push the boundaries of generative AI and inspire the development of innovative applications that enhance human life. This is in line with the mission of LG AI Research: ADVANCING AI FOR BETTER LIFE."
        },
        {
            "title": "2 Model Training",
            "content": "This section describes the detailed information on model configurations and the methods used for pre-training and post-training phases, along with the dataset construction process for each training phase. The complete list of authors who contributed to this work can be found in Appendix A. Model size d_model Number of layers Pre-normalization 32B 5,120 64 True 7.8B 4,096 32 True Non-linearity Feedforward dimension SwiGLU [44] 27,392 SwiGLU 14,336 Head type Number of heads Number of KV heads Head size Max sequence length RoPE theta [46] Tokenizer Vocab size Tied word embedding GQA [3] 40 8 128 32,768 1,000,000 BBPE [51] 102,400 False GQA 32 8 128 32,768 1,000,000 BBPE 102,400 False 2.4B 2,560 30 True SwiGLU 7,168 GQA 32 8 80 32,768 1,000,000 BBPE 102,400 True Table 1: Configurations of EXAONE 3.5 language models"
        },
        {
            "title": "2.1 Model Configurations",
            "content": "The EXAONE 3.5 language models are based on the latest decoder-only Transformer architecture, and detailed configurations are described in Table 1. These models are identical in structure to the EXAONE 3.0 7.8B model but mainly differ in their configurations related to sizes. Notably, the EXAONE 3.5 language models extend the maximum context length from 4,096 tokens in EXAONE 3.0 to 32,768 tokens by adopting the long-context fine-tuning [7]. All three models share the same vocabulary, which consists roughly of 50% Korean and 50% English."
        },
        {
            "title": "2.2 Pre-training",
            "content": "The amount of pre-training corpus data and computational resources are shown in Table 2. The approach to data construction and model training consists of two stages: 1) we perform first-stage pre-training based on the large training corpus, which is collected and processed from as diverse sources as possible aimed to increase the performance on general domains. After that, 2) we collect more data for the domains that need to be strengthened through evaluations and conduct second-stage of pre-training. For instance, we focus on enhancing long-context understanding capabilities in the second-stage. Model size 32B 7.8B 2.4B Training tokens Amount of computation (FLOPs) 6.5T 1.25 1024 9T 4.21 1023 6.5T 9.36 1022 Table 2: The sizes of the training data corpus along with the amounts of computation to build EXAONE 3.5 language models"
        },
        {
            "title": "2.2.1 Context Length Extension",
            "content": "To extend the context length, we utilize the long-context fine-tuning technique [7]. To mitigate the catastrophic forgetting problem [32], where the model forgets what it learned during the first pre-training stage, replay-based method [2] is applied. Specifically, during the second-stage pre-training, we reuse portion of the data used in the first-stage. While documents exceeding the maximum context length is split into smaller chunks in the first-stage, the original corpus are trained without being divided into chunks in the second-stage to extend the models context length."
        },
        {
            "title": "2.2.2 Decontamination",
            "content": "By the nature of massively web-crawled corpus, test-set examples often appear in the training corpus [43, 55]. These contaminated examples are likely to harm generalization performance and confuse test metrics, thus presenting unfair evaluations to users. To prevent the contaminated examples undermine the generalization performance of EXAONE 2 3.5 language models, we rigorously apply decontamination process for all targeted benchmark test data and remove contaminated examples from the training pipeline. We borrow simple yet powerful substring-level matching method [36] with stricter criteria. The entire decontamination process is described in Figure 4 in Appendix C. We first normalize all test-set examples by removing all other characters except alphabets and numbers, then we extract all unique substrings with sliding window size = 50 and stride of 1. To determine whether training example is contaminated, we randomly sample = 10 substrings from the normalized training example and check if they exist in the substring pools. Table 10 in Appendix provides examples of documents found in web corpora considered as contaminated."
        },
        {
            "title": "2.2.3 Training Cost",
            "content": "Considering the computational cost of pre-training large language model (LLM), it is necessary to make the training efficient by achieving as much high performance as possible with limited resources. Table 3 compares the total amounts of computations required for pre-training between the EXAONE 3.5 32B language model and others of similar size. When we simply approximate the total amounts of computations as the product of the model size and the number of training tokens [19, 24], Qwen 2.5 32B, for example, requires 2.77 times more computations than EXAONE 3.5 32B. One of the noticeable characteristics of the EXAONE 3.5 language models is that they demonstrate high performance despite being trained at lower costs than the other baseline models (see Section 3)."
        },
        {
            "title": "Model size",
            "content": "Training tokens Amount of computation (ratio) EXAONE 3.5 Qwen 2.5 Gemma 2 Yi 1.5 32B 32B 27B 34B 6.5T 18T 13T 3.6T 1.00 2.77 1.69 0.59 Table 3: Comparison of the total amounts of computations to build models. We approximate the amount of computations as the product of the model size and the number of training tokens. Although the EXAONE 3.5 32B model is behind in the computations compared to Qwen 2.5 and Gemma 2, it has shown competitive performances."
        },
        {
            "title": "2.3 Post-training",
            "content": "After pre-training, models go through further processes for strengthening their instruction-following capabilities and aligning with human preferences, which are well known as supervised fine-tuning (SFT) [53] and preference optimization."
        },
        {
            "title": "2.3.1 Supervised Fine-tuning",
            "content": "To perform well on new or unseen instructions, model needs to be trained on pairs of instruction-response datasets with varying difficulty from different domains. Hence, in order to build training data covering wide range of fields, we extract core knowledge from 8M web corpora using taxonomic system, as shown in Figure 1. We then generate an instruction-tuning dataset based on the extracted knowledge taxonomy. Finally, leveraging an instruction evolution method, which stems from the method proposed in [58], we diversify the complexity levels so that instructions with various complexities and difficulties can be produced."
        },
        {
            "title": "2.3.2 Preference Optimization",
            "content": "Direct alignment algorithms (DAAs) [38], such as DPO [39] and SimPO [33], are used to train models after supervised fine-tuning to align models with human preferences. We create preference data for training using synthetic data and pre-collected data. For response generation, we sample responses from multiple models for the prompt drawn from the preference data and select the best response as yw and the worst response as yl based on the scores of reward model to create preference data, {x, yw, yl}. To validate preference data, we use an additional reward model to calculate agreement based on the rankings of the two reward models and filter out data with agreement below the threshold. Our preference optimization comprises multiple stages to sequentially train models M1 and M2 through DAAs, where M0 is initialized from the SFT model. The staged pipeline enables us to mitigate over-optimization [38] that may occur during the DAAs training process. Figure 2 shows schematic diagram for constructing our preference dataset and training process. 3 Figure 1: procedure of instruction-tuning data construction. First, we extract the core knowledge from large-volume web corpora and classify it within the taxonomy we defined in advance. Next, instruction-tuning data is generated based on the knowledge. To construct additional training data that is more complex, we leverage an instruction-evolving method [58] that lets the final dataset cover various fields with varying levels of difficulty. Figure 2: Overview of the preference optimization pipeline. (Top) Preference Data Creation: It shows the process of constructing preference data {x, yw, yl} by scoring the responses generated from model for the prompt using reward model. (Bottom) Preference Optimization: Sequential training process where M0 initialized from the SFT model is trained through DAA to obtain M1 and M2."
        },
        {
            "title": "2.4 Data Compliance",
            "content": "Developing AI models requires large amount of data, and the acquisition and utilization of this data can lead to various legal issues, such as copyright infringement, intellectual property infringement, and personal information protection violations. To minimize these risks, LG AI Research conducts AI Compliance reviews throughout the entire process of data collection, AI model training, and information provision. For more detailed information, please refer to the EXAONE 3.0 Technical Report [41] and the LG AI Ethics Principles [28]."
        },
        {
            "title": "3 Evaluation",
            "content": "This section presents the evaluation settings and results of EXAONE 3.5 language models on various benchmark datasets. We select recently released open-sourced language models for baselines of our models to compare our performances on the benchmarks. All baselines and their detailed information are described in Appendix D.1."
        },
        {
            "title": "3.1 Benchmarks",
            "content": "Considering the diverse nature of user intents, it is crucial for an instruction-tuned model to generate response aligned to the users query, whatever it is. To evaluate our models in comprehensive and various scenarios, we select over 4 dozen evaluating benchmarks along with few in-house benchmarks. Table 4 summarizes all benchmarks, which can be grouped into three categories: Real-world Use Cases (Section 3.3): the benchmarks requiring the ability to understand and perform diverse user instructions. Long Context (Section 3.4): the benchmarks evaluating the ability to understand the long context. General Domain (Section 3.5): the benchmarks embracing general domain abilities that LLMs are expected to have. Specifically, this category includes benchmarks for measuring the ability to solve mathematical problems, the ability to write source codes, and the parametric knowledge embedded in an LLM. Category Benchmark Lang Evaluation Settings Real-world Use Cases MT-Bench [59] LiveBench [54] (v2024-08-31) Arena-Hard-v0.1 [29] AlpacaEval 2.0 LC [12] IFEval[61] KoMT-Bench [42] LogicKor [37] EN EN EN EN EN KO KO 1 LLM-as-a-judge (judge: gpt-4o-2024-08-06 ) Ground-truth match LLM-as-a-judge (judge: gpt-4-1106-preview) LLM-as-a-judge (judge: gpt-4-1106-preview) Prompt-level / strict accuracy LLM-as-a-judge (judge: gpt-4o-2024-08-06 ) LLM-as-a-judge (judge: gpt-4-1106-preview) Long Context General Domain Needle-In-A-Haystack [23] LongBench [5] LongRAG [21] (extended) Ko-LongRAG (In-house) Ko-WebRAG (In-house) EN/KO Ground-truth match Ground-truth match LLM-as-a-judge (judge: gpt-4o-2024-08-06 ) LLM-as-a-judge (judge: gpt-4o-2024-08-06 ) LLM-as-a-judge (judge: gpt-4o-2024-08-06 ) EN EN KO KO GSM8K [9] MATH [17, 27] HumanEval [6] MBPP [4] GPQA [40] ARC-C [8] BBH [47] MMLU [16] KMMLU [45] EN EN EN EN EN EN EN EN KO 0-shot / CoT 0-shot / CoT 0-shot 0-shot (Evalplus base)2 0-shot / CoT 0-shot 0-shot / CoT 0-shot / CoT 0-shot / CoT Metric LLM score Accuracy Win rate Win rate Accuracy LLM score LLM score Accuracy F1, Rouge LLM score LLM score LLM score Accuracy Accuracy pass@1 pass@1 Accuracy Accuracy Accuracy Accuracy Accuracy Table 4: The benchmarks used to evaluate the performance of EXAONE 3.5 language models along with their target languages, evaluation settings, and the metrics. LONGRAG is extended from the original, and KO-LONGRAG and KO-WEBRAG are in-house benchmarks (see Section 3.4)."
        },
        {
            "title": "3.2 Overall Performance",
            "content": "The results of overall performance against three categories are presented in Table 5. Our EXAONE 3.5 language models, with sizes 32B and 7.8B, perform best in Real-world Use Cases and Long Context categories compared to baseline models while showing competitive results in the General Domain category. Our smallest model, EXAONE 3.5 2.4B, outperforms baselines with similar sizes in all three categories, demonstrating strong performance. Surprisingly, our 2.4B model, despite its small size, has shown better performance compared to baselines even with larger size (< 9B) except for Qwen 2.5 7B in General Domain. Considering the recent surge in demand for smaller large language models (sLLM) [52], we believe that our EXAONE 3.5 2.4B model is well-positioned to be highly competitive in both academic and industrial use. In the following sections, we elaborate on detailed evaluation settings and the results for each category."
        },
        {
            "title": "3.3 Real-world Use Cases",
            "content": "For the Real-world Use Cases category, we have compiled seven benchmarks that represent real-world queries users might submit to chatbot model. In MT-BENCH, KOMT-BENCH, and LOGICKOR, models responses consisting of multi-turns are evaluated by judge model. For ARENA-HARD and ALPACAEVAL, responses of target language 1The separability of the original GPT-4 judge results is notably low, prompting the adoption of gpt-4o-2024-08-06 as judge. 2We choose the MBPP base from EvalPlus [31], which is subset of the original and consists of refined, high-quality problems. 5 Models Real-world Use Cases Long Context General Domain EXAONE 3.5 32B Qwen 2.5 32B [49] C4AI Command 32B [10] Gemma 2 27B [48] Yi 1.5 34B [2] EXAONE 3.5 7.8B Qwen 2.5 7B [49] Llama 3.1 8B [15] Gemma 2 9B [48] Phi 3 small (7B) [1] EXAONE 3.5 2.4B Qwen 2.5 3B [49] Qwen 2.5 1.5B [49] Llama 3.2 3B [34] Gemma 2 2B [48] 74.3 69.8 46.0 64.2 46.9 70.7 52.7 48.6 57.9 41. 61.1 44.5 30.1 36.7 41.7 71.1 66.9 63.4 - - 66.6 56.1 58.8 - 33.4 63.4 40.7 34.5 44.2 - 74.8 78.7 56.8 68.7 53.9 70.2 71.0 62.4 62.9 63. 63.3 62.1 47.9 54.9 42.2 Table 5: Overall comparison results of EXAONE 3.5 language models with similar-sized baseline language models. Here, dash (-) indicates the model does not support context lengths longer than 16K. Bold scores indicate the best performance, and underlined scores mean the second best. The detailed information for each baseline is described in Appendix D.1. model are compared with those of reference model (gpt-4-0314 and gpt-4-1106-preview, respectively) by judge model, recording the win rate. LIVEBENCH (ver. 2024-08-31) and IFEVAL (prompt-strict) assess how well the models responses align with user instructions by matching them to the ground-truth responses. Models MT-Bench LiveBench Arena-Hard AlpacaEval IFEval KoMT-Bench LogicKor Average EXAONE 3.5 32B Qwen 2.5 32B C4AI Command 32B Gemma 2 27B Yi 1.5 34B EXAONE 3.5 7.8B Qwen 2.5 7B Llama 3.1 8B Gemma 2 9B Phi 3 small (7B) EXAONE 3.5 2.4B Qwen 2.5 3B Qwen 2.5 1.5B Llama 3.2 3B Gemma 2 2B 8.51 8.49 7.38 8.28 7.64 8.29 6.48 7.59 7.64 7. 7.81 7.21 5.72 6.94 7.20 43.0 50.6 29.7 40.0 26.2 39.8 35.6 28.3 32.1 27.9 33.0 25.7 19.2 24.0 20.0 78.6 67.0 17.0 57.5 23.1 68.7 48.9 27.7 43.6 26. 48.2 26.4 10.6 14.2 19.1 60.6 41.0 25.9 52.2 34.8 54.2 31.7 25.7 47.3 29.2 37.1 17.4 8.4 18.7 29.1 81.7 78.7 26.1 59.7 55.5 78.9 72.5 74.5 54.7 59. 73.6 60.8 40.7 70.1 50.5 8.05 7.75 6.72 7.19 4.88 7.96 5.19 4.85 7.10 3.22 7.24 5.68 3.87 3.16 4.83 9.06 8.89 8.24 8.56 6.33 9.08 6.38 5.99 8.05 3. 8.51 5.21 3.60 2.86 5.29 74.3 69.8 46.0 64.2 46.9 70.7 52.7 48.6 57.9 41.7 61.1 44.5 30.1 36.7 41.7 Table 6: Performance comparison results of EXAONE 3.5 language models with similar-sized recently-released language models on seven benchmarks representing real-world use case scenarios. When calculating the macro average, the scores of MT-Bench, KoMT-Bench, and LogicKor are multiplied by 10 because they are scored out of 10 and the rest are scored out of 100. Bold scores indicate the best performance, and underlined scores mean the second best. As presented in Table 6, our three models have shown the best performance against baselines of similar size in all benchmarks, except for the 32B model in LIVEBENCH. Furthermore, by outperforming others in both English and Korean benchmarks, EXAONE 3.5 language models demonstrate their superior bilingual abilities."
        },
        {
            "title": "3.4 Long Context",
            "content": "The ability to process and understand long contexts is increasingly important for modern LLMs, as it enables their application in more complex scenarios. To demonstrate EXAONE language models long context performance, we 6 Figure 3: NIAH results of EXAONE 3.5 language models. The x-axis represents the token length of the input text, while the y-axis shows the relative position within the text, expressed as percentage (0% corresponds to the beginning, and 100% to the end). The results are represented using color-coded scheme: green indicates successful retrievals, and red represents unsuccessful ones. EXAONE 3.5 language models achieve near-perfect accuracy in retrieving information across various document depths and context lengths in English and Korean. evaluate our models using benchmarks designed for synthetic task with long context inputs, along with various retrieval-augmented generation (RAG) benchmarks."
        },
        {
            "title": "3.4.1 Needle-in-a-Haystack",
            "content": "Needle-in-a-Haystack (NIAH) [23] serves as benchmark to assess how effectively models can locate and retrieve information hidden at random locations within long documents. We comprehensively evaluate our models ability to process and retrieve information from long contexts, up to 32K tokens. Furthermore, we extend NIAH to Korean and employ it to evaluate our models long context processing ability across both English and Korean contexts. Figure 3 demonstrates that our models achieve near-perfect accuracy in retrieving targeted information across all tested document depths and context lengths in both English and Korean. These results highlight their robust long context processing capabilities, particularly in tasks demanding precise information retrieval and complex reasoning."
        },
        {
            "title": "3.4.2 Long Context Understanding",
            "content": "To assess long context understanding capabilities, we evaluate our models using benchmarks including LONGBENCH [5] and LONGRAG [21]. We expand unanswerable cases in LongRAG to make it more challenging. We also build KOLONGRAG, the Korean counterpart to LONGRAG, to evaluate long context understanding in Korean. For more realistic RAG scenario, requiring answers to difficult questions using actual web-searched results, we constructed KO-WEBRAG benchmark. We refer readers to the Appendix D.2 for more details. 7 Models LongBench LongRAG Ko-LongRAG Ko-WebRAG Average EXAONE 3.5 32B Qwen 2.5 32B C4AI Command 32B Gemma 2 27B Yi 1.5 34B EXAONE 3.5 7.8B Qwen 2.5 7B Llama 3.1 8B Gemma 2 9B Phi 3 small (7B) EXAONE 3.5 2.4B Qwen 2.5 3B Qwen 2.5 1.5B Llama 3.2 3B Gemma 2 2B 49.2 49.1 50.9 - - 46.0 47.2 44.6 - 40.6 42.7 42.0 37.1 41.7 - 67.6 63.6 55.3 - - 68.3 60.1 55.1 - 52.7 63.3 45.8 39.0 45.9 - 85.3 73.5 72.3 - - 71.7 55.3 64.8 - 7.7 74.7 40.5 33.8 39.3 - 82.3 81.3 75.0 - - 80.3 61.7 70.7 - 32.7 73.0 34.7 28.0 50.0 - 71.1 66.9 63.4 - - 66.6 56.1 58.8 - 33.4 63.4 40.7 34.5 44.2 - Table 7: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models across four benchmarks representing long context scenarios. dash (-) indicates that the model does not support context lengths longer than 16K. Context lengths for each model are detailed in Table 11. The average score in the rightmost is calculated as macro average across the benchmarks. Bold scores indicate the best performance, and underlined scores mean the second best. As shown in Table 7, EXAONE 3.5 language models have shown superior performance compared to other models3, except for the 32B and 7.8B models in LongBench. When averaged across the benchmarks, our three models outperform all baselines, confirming their capabilities to process complex, extended contexts effectively."
        },
        {
            "title": "3.5 General Domain",
            "content": "Language models are now expected to achieve human-level capabilities in various general domains, such as solving mathematical problems or writing source code programs. To evaluate overall performance in the general domains, we select nine benchmarks in three main domains: 1) GSM8K (CoT) and MATH (CoT) for mathematics, 2) HUMANEVAL (Evalplus base) and MBPP (Evalplus base) for coding, and 3) MMLU (CoT), KMMLU (CoT), GPQA (CoT), ARC-C, and BBH (CoT) for assessing the amount of knowledge embedded in an LLM. To better simulate the real-world scenarios where chatbot model usually receives single query from users, we evaluate all benchmarks in the General Domain category using the 0-shot setting. To achieve this, we prompt language models with instructions that require specific answer formats and parse the final answer from the responses. For fair comparison, we use the same prompts across all models. We make public all the prompts we used in Appendix D.3 for transparent reproducibility. Table 8 shows the results of EXAONE 3.5 language models and their baseline models on the benchmarks in General Domain. When averaged across the benchmarks, our EXAONE 3.5 language models with sizes 32B and 7.8B demonstrate competitive performance compared to baselines of similar size. The EXAONE 3.5 2.4B model, on the other hand, outperforms all baselines in the average score."
        },
        {
            "title": "4 Responsible AI",
            "content": "EXAONE 3.5 language models were developed in accordance with the Responsible AI Development Framework encompassing data governance, ethical considerations, and risk management as it would be made available to wide range of users. Given the nature of open models eventually leading to wide use in various domains we aim to maximize social benefits while ensuring humanity, fairness, safety, accountability, and transparency as mandated by the LG AI Ethics Principles [28]. 3Gemma models and Yi 1.5 34B model are excluded from evaluations due to their context limits ( 16k tokens), ensuring fair comparison. 8 Models GSM8K MATH HumanEval MBPP MMLU KMMLU GPQA ARC-C BBH Average EXAONE 3.5 32B Qwen 2.5 32B C4AI Command 32B Gemma 2 27B Yi 1.5 34B EXAONE 3.5 7.8B Qwen 2.5 7B Llama 3.1 8B Gemma 2 9B Phi 3 small (7B) EXAONE 3.5 2.4B Qwen 2.5 3B Qwen 2.5 1.5B Llama 3.2 3B Gemma 2 2B 91.9 92.0 56.5 84.2 83. 87.6 90.4 82.1 82.0 86.3 82.5 84.3 69.8 77.4 29.8 70.5 76.5 24.3 49.4 52.0 69.8 70.4 48.8 44.6 47.8 60.2 61.4 48.5 46.6 18.7 87.2 89.0 68.3 79.3 5. 84.2 82.3 67.7 68.3 72.6 76.2 72.6 55.5 54.9 45.7 81.8 88.9 78.8 80.7 35.7 79.4 78.8 70.6 75.1 72.0 74.3 72.5 65.6 60.6 55.0 78.3 81.4 71.1 74.8 75. 69.0 73.1 72.4 73.7 68.8 60.4 61.0 48.8 64.9 56.1 57.0 62.1 41.5 53.8 41.7 52.4 49.9 45.9 34.6 33.4 45.8 41.7 5.0 35.0 37.4 39.7 40.9 27.4 33.6 30. 32.5 33.1 27.4 27.9 25.3 28.4 25.8 23.1 23.2 22.6 91.7 95.1 88.0 92.9 93.9 87.6 90.6 83.7 90.5 90.4 79.2 82.1 72.4 78.0 76.3 75.3 82.7 55.7 69.7 67. 69.7 70.1 63.3 69.7 72.5 62.9 57.3 42.2 53.8 38.2 74.8 78.7 56.8 68.7 53.9 70.2 71.0 62.4 62.9 63.2 63.3 62.1 47.9 54.9 42.2 Table 8: Performance comparison results of EXAONE 3.5 models with similar-sized recently-released language models on nine benchmarks representing general scenarios. The macro average is used to evaluate the overall performance. Bold scores indicate the best performance, and underlined scores mean the second best."
        },
        {
            "title": "4.1 Benefits",
            "content": "EXAONE 3.5 language models are open for research purposes, aiming to advance AI research. Based on the feedback we have received since the release of the EXAONE 3.0 7.8B model, we now offer models of more diverse sizes: 2.4B, 7.8B, and 32B. This will allow researchers to select an optimal model for their research objectives and computing environment. We hope that this flexibility will support wide spectrum, ranging from foundational research to domainspecific applications. It is also expected to contribute positively to the advancement of generative AI, building upon the significant performance improvements over previous version. To ensure the reliability of the release, we have implemented standardized data compliance protocol, guaranteeing high-quality data. This standardized approach provides trustworthy foundation for researchers to use the model across various research areas in the future. While external users can employ EXAONE 3.5 language models in diverse domains, precisely identifying specific user needs has been challenging. To address this, we have conducted extensive reviews of its applicability across wide range of domains. Additionally, we have collaborated closely with LG affiliates, including business and research teams, to better align the model with specific user requirements."
        },
        {
            "title": "4.2 Risks and Mitigations",
            "content": "Open models can positively contribute to the AI community, but there are challenges in ensuring responsible use. We conducted an AI ethical impact assessment to identify potential risks such as unintended inequality and discrimination against socially disadvantaged groups, the generation of harmful content, and malicious misuse by users. We have adopted various policies and research initiatives to mitigate the potential risks identified through this assessment. First, on the data side, we conducted legal risk assessment on all candidate datasets to enhance privacy and security. Based on the outcomes, we determined the suitability of each dataset for training and performed de-identification process to remove sensitive data from qualified dataset. To minimize bias in the training data and ensure data quality, we documented all pre-processing steps and adopted standardized data processing protocol. Considering practical difficulties of verifying the representativeness of all data, we conducted qualitative evaluation of small sample of data. For quantitative evaluation, we endeavored to minimize data-related risks by verifying the data subsets through performance evaluation after the model training was completed. Also, we carefully reviewed the open-source libraries used in our model development. The levels of AI ethical considerations and regulatory requirements may vary across different user needs and characteristics (e.g., country of residence, age, etc.). To address this, we will continue to monitor global AI regulations and take immediate action as needed to avoid potential regulatory violations. lack of transparency in an AI models decision-making process can reduce trust among users and stakeholders. To address this limitation, we continuously analyze and evaluate our models performance to identify weaknesses and areas for improvement. While fully explaining 9 AI models decision-making process remains challenging, we are committed to to advancing explainability through ongoing research."
        },
        {
            "title": "4.3 Safety",
            "content": "We conducted comprehensive evaluations of EXAONE 3.5 language models ethics and security using third-party dataset: Korean Large Language Model Trustworthiness Benchmark Data [35], provided by the Ministry of Science and ICT of the Republic of Korea and the National Information Society Agency (NIA). This dataset is specifically designed to assess the harmlessness of language models. The evaluation results are presented in Table 9. To measure the performance, we asked model to choose one of five options. If the selected option is included in the set of correct answers, then it is scored as correct. In the provided dataset, the first two options were labeled False and the remaining three were labeled True. To mitigate potential bias from the order of options, we shuffled the order of options randomly for each evaluation. While the experimental results demonstrated effectiveness in filtering harmful reactions, there is still room for improvement. Category Bias Hate Illegal Sensitiveness Overall Subcategory Test Cases Accuracy Gender & Sexual Orientation Race & Ethnicity & Nationality Political Affiliation Region Job Miscellaneous Subtotal Gender & Sexual Orientation Race & Ethnicity & Nationality Political Affiliation Region Job Subtotal Illegal Contentious Ethical Predictive Subtotal 295 432 720 415 442 2,710 399 749 1,164 499 852 3,663 1,126 710 966 825 2, 10,000 32B 91.2% 86.8% 82.8% 87.7% 86.2% 85.2% 86.0% 95.2% 91.6% 85.7% 92.0% 91.0% 90.0% 92.9% 83.1% 81.2% 79.8% 81.2% 87.1% 7.8B 87.5% 85.0% 79.9% 84.6% 81.9% 86.5% 83.5% 92.2% 88.4% 83.4% 87.2% 87.8% 86.9% 89.6% 86.1% 83.7% 82.3% 83.9% 85.6% 2.4B 76.6% 72.2% 56.7% 69.2% 67.0% 73.2% 67.4% 83.5% 73.8% 66.2% 74.1% 72.3% 72.2% 80.3% 79.0% 72.8% 71.0% 74.0% 72.2% Table 9: Evaluation results of EXAONE 3.5 language models on the Korean Large Language Model Trustworthiness Benchmark Data [35] to assess the models harmlessness. The accuracy is determined by the number of times the model selects appropriate options when presented with questions involving various harmful and dangerous categories, such as illegal content."
        },
        {
            "title": "5 Limitations",
            "content": "EXAONE 3.5 language models, like all existing language models, have certain limitations and may occasionally generate inappropriate responses. The language model generates responses based on the output probability of tokens, and it is determined during learning from training data. While we have made every effort to exclude personal, harmful, and biased information from the training data, some problematic content may still be included, potentially leading to undesirable responses. Please note that the text generated by EXAONE 3.5 language models does not reflect the views of LG AI Research. Inappropriate answers may be generated, which contain personal, harmful or other inappropriate information. Biased responses may be generated, which are associated with age, gender, race, and so on. The generated responses rely heavily on statistics from the training data, which can result in the generation of semantically or syntactically incorrect sentences. 10 Since the model does not reflect the latest information, the responses may be false or contradictory. LG AI Research strives to reduce potential risks that may arise from EXAONE 3.5 language models. Users are not allowed to engage in any malicious activities (e.g., keying in illegal information) that may induce the creation of inappropriate outputs violating LG AIs ethical principles when using EXAONE 3.5 language models."
        },
        {
            "title": "6 Deployment",
            "content": "Section in Appendix provides license information for using the EXAONE 3.5 language models. Understanding the license information is essential for the legal utilization of the language model."
        },
        {
            "title": "7 Conclusion",
            "content": "In response to the growing interest from academia and industry, we are excited to release EXAONE 3.5 language models that excel in real-world use cases and long-context understanding. These models are available in three sizes (32B, 7.8B, and 2.4B). To validate performance of our models in the real-world use case scenarios, we evaluated our models on seven benchmarks requiring diverse instructions understanding. To assess long-context understanding, we evaluated our models on four benchmarks. Our models consistently outperformed in both categories. Additionally, our models exhibited competitive performance in general domains including solving mathematical problems and writing code. In particular, our 2.4B model ranked first in average scores across general domains. Our models are available to everyone for research purposes, and we welcome your feedback to help us improve the models. If you have any feedback or are interested in exploring commercial opportunities with our models, please reach out to contact_us@lgresearch.ai."
        },
        {
            "title": "A Contributors",
            "content": "All authors are listed in alphabetical order by last name. Core Contributors Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen, Hyeongu Yun Contributors Soyoung An, Kyunghoon Bae, Stanley Jungkyu Choi, Gerrard Jeongwon Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Youchul Kim, Edward Hwayoung Lee, Honglak Lee, Woohyung Lim, Sooyoun Park, Yongmin Park, Sihoon Yang"
        },
        {
            "title": "B Model License",
            "content": "EXAONE AI Model License Agreement 1.1 - NC This License Agreement (Agreement) is entered into between you (Licensee) and LG Management Development Institute Co., Ltd. (Licensor), governing the use of the EXAONE AI Model (Model). By downloading, installing, copying, or using the Model, you agree to comply with and be bound by the terms of this Agreement. If you do not agree to all the terms, you must not download, install, copy, or use the Model. This Agreement constitutes binding legal agreement between the Licensee and Licensor. 1. Definitions"
        },
        {
            "title": "1.5 Licensee: The individual, organization, corporation, academic institution, government agency, or other\nentity using or intending to use the Model under the terms and conditions of this Agreement. The Licensee is responsible\nfor ensuring compliance with the Agreement by all authorized users who access or utilize the Model on behalf of the\nLicensee.",
            "content": "2. License Grant"
        },
        {
            "title": "2.1 Grant of License: Subject to the terms and conditions outlined in this Agreement, the Licensor hereby\ngrants the Licensee a limited, non-exclusive, non-transferable, worldwide, and revocable license to:",
            "content": "a. Access, download, install, and use the Model solely for research purposes. This includes evaluation, testing, academic research, experimentation, and participation in competitions, provided that such participation is in non-commercial context. Notwithstanding Section 3.1, the Licensee may only provide the Model or Derivatives for competition if no commercial license is granted to the competition organizer or any third party. b. Publicly disclose research results and findings derived from the use of the Model or Derivatives, including publishing papers or presentations. c. Modify the Model and create Derivatives based on the Model, provided that such modifications and Derivatives are used exclusively for research purposes. The Licensee may conduct experiments, perform analyses, and apply custom modifications to the Model to explore its capabilities and performance under various scenarios. If the Model is modified, the modified Model must include EXAONE at the beginning of its name. d. Distribute the Model and Derivatives in each case with copy of this Agreement."
        },
        {
            "title": "2.2 Scope of License: The license granted herein does not authorize the Licensee to use the Model for any\npurpose not explicitly permitted under this Agreement. Any use beyond the scope of this license, including any com-\nmercial application or external distribution, is strictly prohibited unless explicitly agreed upon in writing by the Licensor.",
            "content": "13 3. Restrictions"
        },
        {
            "title": "3.4 Ethical Use: The Licensee shall ensure that the Model or Derivatives is used in an ethical and responsi-\nble manner, adhering to the following guidelines:",
            "content": "a. The Model and Derivatives shall not be used to generate, propagate, or amplify false, misleading, or harmful information, including fake news, misinformation, or disinformation. b. The Model and Derivatives shall not be employed to create, distribute, or promote content that is discriminatory, harassing, defamatory, abusive, or otherwise offensive to individuals or groups based on race, gender, sexual orientation, religion, nationality, or other protected characteristics. c. The Model and Derivatives shall not infringe on the rights of others, including intellectual property rights, privacy rights, or any other rights recognized by law. The Licensee shall obtain all necessary permissions and consents before using the Model and Derivatives in manner that may impact the rights of third parties. d. The Model and Derivatives shall not be used in way that causes harm, whether physical, mental, emotional, or financial, to individuals, organizations, or communities. The Licensee shall take all reasonable measures to prevent misuse or abuse of the Model and Derivatives that could result in harm or injury. 4. Ownership"
        },
        {
            "title": "4.3 Attribution: In any publication or presentation of results obtained using the Model, the Licensee shall\nprovide appropriate attribution to the Licensor, citing the Model’s name and version, along with any relevant\ndocumentation or references specified by the Licensor.",
            "content": "14 5. No Warranty 5.1 As-Is Basis: The Model, Derivatives, and Output are provided on an as-is and as-available basis, without any warranties or representations of any kind, whether express, implied, or statutory. The Licensor disclaims all warranties, including but not limited to, implied warranties of merchantability, fitness for particular purpose, accuracy, reliability, non-infringement, or any warranty arising from the course of dealing or usage of trade."
        },
        {
            "title": "5.3 No Endorsement: The Licensor does not endorse, approve, or certify any results, conclusions, or recom-\nmendations derived from the use of the Model. The Licensee is solely responsible for evaluating the accuracy, reliability,\nand suitability of the Model for its intended purposes.",
            "content": "6. Limitation of Liability"
        },
        {
            "title": "6.2 Indemnification: The Licensee agrees to indemnify, defend, and hold harmless the Licensor, its affili-\nates, officers, directors, employees, and agents from and against any claims, liabilities, damages, losses, costs, or\nexpenses (including reasonable attorneys’ fees) arising out of or related to the Licensee’s use of the Model, any\nDerivatives, or any Output, including any violation of this Agreement or applicable laws.",
            "content": "7. Termination"
        },
        {
            "title": "7.3 Survival: The provisions of this Agreement that by their nature should survive termination, including\nbut not limited to, Sections 4 (Ownership), 5 (No Warranty), 6 (Limitation of Liability), and this Section 7 (Termination),\nshall continue to apply after termination.",
            "content": "8. Governing Law"
        },
        {
            "title": "8.2 Arbitration: Any disputes, controversies, or claims arising out of or relating to this Agreement, includ-\ning its existence, validity, interpretation, performance, breach, or termination, shall be referred to and finally resolved\nby arbitration administered by the Korean Commercial Arbitration Board (KCAB) in accordance with the International\nArbitration Rules of the Korean Commercial Arbitration Board in force at the time of the commencement of the\narbitration. The seat of arbitration shall be Seoul, Republic of Korea. The tribunal shall consist of one arbitrator. The\nlanguage of the arbitration shall be English.",
            "content": "15 9. Alterations"
        },
        {
            "title": "9.2 Entire Agreement: This Agreement constitutes the entire agreement between the Licensee and Licensor\nconcerning the subject matter hereof and supersedes all prior or contemporaneous oral or written agreements,\nrepresentations, or understandings. Any terms or conditions of any purchase order or other document submitted by\nthe Licensee in connection with the Model that are in addition to, different from, or inconsistent with the terms and\nconditions of this Agreement are not binding on the Licensor and are void.",
            "content": "By downloading, installing, or using the EXAONE AI Model, the Licensee acknowledges that it has read, understood, and agrees to be bound by the terms and conditions of this Agreement."
        },
        {
            "title": "C Decontamination Details",
            "content": "As described in Section 2.2.2, we apply the decontamination process over our training data to remove any data instances that overlap with test sets, thus harming the generalization performance of our models. Figure 4 presents an overview of our decontamination process, and Table 10 shows examples of contaminated, therefore removed data. Figure 4: summary of the decontamination method employed to train EXAONE 3.5 language models. Adopting an approach borrowed from the GPT-4 method, we increase the number of random sample to = 10 for stricter decontamination. Benchmark Benchmark example Contaminated web corpus MMLU [16] teacher has three packages of stickers. One package contains 56 stickers, another package contains 48 stickers, and the third package contains 58 stickers. If the teacher divides all the stickers equally among 27 students, how many stickers will each student receive? A. 6 stickers B. 9 stickers C. 54 stickers D. 81 stickers Answer: (...truncated...) teacher has three packages of stickers. One package contains 56 stickers, another package contains 48 stickers, and the third package contains 58 stickers. If the teacher divides all the stickers equally among 27 students, how many stickers will each student receive? 6 stickers is correct #4 Last week Mario walked 7 3/4 miles. This week he walked 15 5/6 miles. What is the difference between the distance he walked this week and the distance he walked last week? (...truncated...) KMMLU [45] 국가가 국민의 생활안정과 복지증진을 위하여 보험의 원리를 도입하여 만든 사회보험의 일종으로 가입자, 사용자 및 국가로부터 일정한 보험료 를 받고 이를 재원으로 여러 가지 정형화된 보험금을 지급하는 사회보장 제도는? A. 국민건강보험 B. 국민연금 C. 고용보험 D. 산업재해보상보험 정답: [Translation] What is the social security system, which is type of social insurance created by the nation by introducing the principles of insurance to promote stability and welfare of citizens lives, and which receives certain premiums from subscribers, employers, and the nation and use these funds to provide various standardized insurance benefits. A. National Health Insurance B. National Pension C. Employment Insurance D. Industrial Accident Compensation Insurance Answer: (...중략...) 더군다나 개인주의의 확산, 핵가족화의 진전에 따라 전통적인 가족의 역할인 노인부양의 기능이 약화됨으로써 국가개입의 중요성은 더욱 증가하게 되었다. 따라서 국민연금제도는 국가가 국민의 생활안정 과 복지증진을 위하여 보험의 원리를 도입하여 만든 사회보험의 일종으 로 가입자, 사용자 및 국가로부터 일정한 보험료를 받고 이를 재원으로 여 러 가지 정형화된 보험금을 지급하는 사회보장제도이다. (...중략...) [Translation] (...truncated...) Moreover, with the spread of individualism and the rise of nuclear families, the traditional family role of supporting the elderly has weakened, thereby increasing the importance of nation intervention. Accordingly, the National Pension System is type of social insurance created by the nation by introducing the principles of insurance to promote stability and welfare of citizens lives, and which receives certain premiums from subscribers, employers, and the nation and use these funds to provide various standardized insurance benefits. (...truncated...) Table 10: Examples of contaminated web corpus. The text highlighted in grey is part of the text that exists in both benchmark test set and web corpus. The text underlined is corresponding golden answer."
        },
        {
            "title": "D Evaluation Details",
            "content": "D.1 Baseline Models We choose various open-source models as the baselines for our EXAONE 3.5 language models. We mainly utilize Huggingface library4 to access each checkpoint of baselines. The overall information of each model are presented in Table 11. Model Name Context Len. Link Qwen2.5 32B C4AI Command 32B Gemma 2 27B Yi 1.5 34B Qwen2.5 7B Llama 3.1 8B Gemma 2 9B Phi 3 small (7B) Qwen2.5 3B Qwen2.5 1.5B Llama 3.2 3B Gemma 2 2B 128k 128k 8k 16k 128k 128k 8k 128k 32k 32k 128k 8k https://huggingface.co/Qwen/Qwen2.5-32B-Instruct https://huggingface.co/CohereForAI/c4ai-command-r-08-2024 https://huggingface.co/google/gemma-2-27b-it https://huggingface.co/01-ai/Yi-1.5-34B-Chat-16K https://huggingface.co/Qwen/Qwen2.5-7B-Instruct https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct https://huggingface.co/google/gemma-2-9b-it https://huggingface.co/microsoft/Phi-3-small-128k-instruct https://huggingface.co/Qwen/Qwen2.5-3B-Instruct https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct https://huggingface.co/google/gemma-2-2b-it Release Sep., 2024 Aug., 2024 Jun., 2024 May, 2024 Sep., 2024 Jul., 2024 Jun., 2024 May, 2024 Sep., 2024 Sep., 2024 Sep., 2024 Jul., Table 11: The list of baseline models used for the evaluation along with their supported context length and released date D.2 Long Context D.2.1 Needle-In-A-Haystack The specific configurations used in the Needle-In-A-Haystack (NIAH) experiment are detailed in Table 12. Language Configuration Details English Haystack Needle Query Instruction Paul Graham essays [23] The best thing to do in San Francisco is eat sandwich and sit in Dolores Park on sunny day. What is the best thing to do in San Francisco? Analyze the content of the given document to locate the answer to the specified question. If found, provide the exact wording from the document without altering or summarizing it. Korean Haystack Needle Query Instruction AI-Hub5 대규모 구매도서 기반 한국어 말뭉치 데이터 (Large-scale Purchased Book-based Korean Language Corpus from AI-Hub) 광화문에서 가장 재미있는 일은 햇살 좋은 날에 샌드위치를 먹으며 청와대 안에 있는 공원에 앉아 있는 것입니다. (The best thing to do at Gwanghwamun is eat sandwich and sit in the park in the Blue House on sunny day.) 광화문에서 가장 재미있는 일이 무엇인가요? (What is the best thing to do at Gwanghwamun?) 주어진 문서를 읽고 질문에 대한 답을 확인하세요. 답을 찾으면, 문서의 원문을 그대로 유지하여 수정이나 해석 없이 반환하세요. (Identical to the English instruction) Table 12: Detailed configuration of the Needle-In-A-Haystack experiment. The Needle refers to specific text fragment embedded within the Haystack, which consists of long distractor texts. The task involves using Query as cue to identify the needle within the haystack and retrieve the associated values. 4https://huggingface.co/models 5https://www.aihub.or.kr 18 D.2.2 LongBench LONGBENCH has been suggested as bilingual benchmark to assess long context comprehension in English and Chinese. In this report, we focus on the English subsets, specifically Single-doc QA, Multi-doc QA, Summarization, and Few-shot Learning. The Single-doc QA task includes datasets such as NarrativeQA [25], Qasper [11], and MultiFieldQA-EN [5]. For the Multi-doc QA task, datasets like HotpotQA [56], 2WikiMultihopQA [18], and MuSiQue [50] are utilized. The Summarization task involves datasets such as GovReport [20], QMSum [60], and MultiNews [13], while the Few-shot Learning task relies on datasets from TREC [30] and TriviaQA [22]. All evaluation methods and metrics for these datasets adhere to the official LONGBENCH settings. Detailed task scores are presented in Table 13. Models Single-doc QA Multi-doc QA Summarization Few-shot Learning Average EXAONE 3.5 32B Qwen 2.5 32B C4AI Command 32B Gemma 2 27B Yi 1.5 34B EXAONE 3.5 7.8B Qwen 2.5 7B Llama 3.1 8B Gemma 2 9B Phi 3 small (7B) EXAONE 3.5 2.4B Qwen 2.5 3B Qwen 2.5 1.5B Llama 3.2 3B Gemma 2 2B 40.1 43.2 44.6 - - 38.4 40.8 39.8 - 33.2 35.0 35.5 29.9 33.9 - 52.9 54.9 48.9 - - 47.7 44.0 41.2 - 26.5 43.1 34.7 32.1 34.9 - 23.1 26.1 26.4 - - 22.6 26.5 27.6 - 26.3 20.1 24.7 22.3 25.8 - 80.1 72.4 83.6 - - 75.1 77.4 69.9 - 76.2 72.8 72.9 64.0 72.3 - 49.2 49.1 50.9 - - 46.0 47.2 44.6 - 40.6 42.7 42.0 37.1 41.7 - Table 13: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models across four benchmarks representing long context scenarios. Context lengths for each benchmark, as well as model limitations, are detailed in Table 11, where dash (-) indicates that the model does not support context lengths longer than 16k. The final overall score for each model is calculated as macro average across the benchmarks. Bold scores indicate the best performance, and underlined scores mean the second best. 19 D.2.3 LongRAG LONGRAG is RAG benchmark that focuses on long context retrieval and generation using large text chunks. We use Natural Questions [26] and HotpotQA [57] subsets from the original LONGRAG. To further evaluate the models capability to handle cases where the retrieved passage does not support valid answer, we extend the LONGRAG benchmark by incorporating unanswerable cases. The LONGRAG benchmark uses the open-sourced dense retrieval toolkit as its retriever. However, the retrieved context does not always include evidence that supports the correct answer. To address this limitation, we define unanswerable cases using the is_retrieval function in LONGRAG. The is_retrieval function takes the context and golden answer as inputs and determines whether the context contains sufficient evidence to extract the correct answer. It returns True if such evidence exists and False otherwise. When the return value of is_retrieval is False, indicating that the context does not contain the correct answer, we modify the ground-truth answer to [Unanswerable, No relevant information found., This question cannot be answered with the provided data.]. This modification allows the model to learn and appropriately handle unanswerable cases. Additionally, to ensure the model responds effectively to unanswerable cases, the following sentence is added to the existing prompt: If the answer cannot be found in the context, respond with Unanswerable. This prompt guides the model to respond explicitly with Unanswerable when it determines that no answer exists within the context. Through this extension, the LONGRAG benchmark gains an enhanced evaluation framework capable of handling unanswerable scenarios, enabling more comprehensive and nuanced performance assessments. Detailed task scores are presented in Table 14. Models EXAONE 3.5 32B Qwen 2.5 32B C4AI Command 32B Gemma 2 27B Yi 1.5 34B EXAONE 3.5 7.8B Qwen 2.5 7B Llama 3.1 8B Gemma 2 9B Phi 3 small (7B) EXAONE 3.5 2.4B Qwen 2.5 3B Qwen 2.5 1.5B Llama 3.2 3B Gemma 2 2B NQ Hotpot QA Average Answerable Unanswerable Total Answerable Unanswerable Total 73.6 62.3 64.0 - - 72.0 64.5 63.2 - 66.8 67.8 49.5 49.9 49.4 - 35.3 61.2 32.4 - - 41.0 51.1 15.1 - 13.7 25.9 34.5 18.0 41.7 - 68.3 62.1 59.6 - - 67.7 62.6 56.5 - 59.4 62.0 47.4 45.5 48.3 - 81.8 62.9 63.1 - - 74.3 61.8 67.4 - 60.2 73.1 52.5 43.6 53.6 - 26.4 70.6 18.2 - - 53.9 46.1 16.4 - 7.1 41.6 21.6 2.2 16.0 - 66.9 65.0 51.0 - - 68.8 57.6 53.7 - 45.9 64.6 44.2 32.5 43.5 - 67.6 63.6 55.3 - - 68.3 60.1 55.1 - 52.7 63.3 45.8 39.0 45.9 - Table 14: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models with LongRAG benchmarks. The benchmark is extended with the Unanswerable case, which requires models to respond as Unanswerable when the information cannot be found within the context. Bold scores indicate the best performance, and underlined scores mean the second best. Fig 5 shows the LLM-as-a-judge prompt used for LongRAG evaluation. In the LLM-as-a-judge evaluation setup, we incorporate short answer evaluation to align with the methodology used in LONGRAG, where short answers are extracted to calculate Exact Match (EM). We extend this approach to LLM-as-a-judge to ensure consistency across evaluation metrics. However, as the observed trends for short answers consistently align with those for long answers, we prioritize long answer evaluation in our final analysis to streamline the assessment process without compromising the robustness of the results. 20 LongRAG LLM-as-a-judge Prompt System: You are an expert evaluator of text answers. Your task is to compare the content of two answers, long answer (long_ans) and short answer (short_ans), with the provided correct answers (Answer), which may contain multiple correct options. Both the long answer and the short answer need to be checked for correctness. The long and short answers do not need to match any of the answers in the Answer list word-for-word but must convey the same key meaning or idea. If either the long or short answer matches any one of the correct answers in the Answer list, it should be considered correct. Focus only on the accuracy of the content and ignore style, tone, or extra information unless it introduces inaccuracies. For both the long and short answers, return only the evaluation result as Python dictionary object, and ensure the output is formatted as valid Python code. Here are two examples of how to evaluate answers: Example 1: Question: what does hp mean in war and order Answer: ['hit points', 'health points'] long_ans: HP stands for Health Points in video games and war, it is measure of an entitys ability to function and survive in combat situation. In video games, HP is often displayed as numeric value, and can be depleted by taking damage from enemies or other hazards. When an entitys HP reaches zero, it is often considered defeated or eliminated. In war, HP can refer to the physical and mental resilience of soldiers, and can be affected by factors such as injury, fatigue. short_ans: HP stands fer Health Points. Evaluation: {'long_ans': 'correct', 'short_ans': 'correct'} Example 2: Question: what is the capital of France Answer: ['Paris'] long_ans: The capital of France is Paris, major European city and global center for art, fashion, and culture. Paris is known for its cafe culture and landmarks like the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. short_ans: The capital of France is Lyon. Evaluation: {'long_ans': 'correct', 'short_ans': 'incorrect'} Now, proceed with your evaluation of the following question, answer, and responses, and return only the evaluation as valid Python dictionary. Ensure the response is valid Python dictionary object without any additional text. User: Evaluate the following long and short answers based on the provided correct answer. Your goal is to determine if the long and short answers are correct. Return the evaluation result in the form of Python dictionary: {'long_ans': 'correct 'or 'incorrect ', 'short_ans': 'correct'or 'incorrect'}. Question: {{question}} Answer: {{answer}} Long_ans: {{long_ans}} Short_ans: {{short_ans}} Return only the evaluation in the form of Python dictionary. Do not include any explanation or additional comments. Figure 5: LLM-as-a-judge prompt for evaluating LongRAG 21 D.2.4 Ko-LongRAG We construct Korean counterpart of LONGRAG, named KO-LONGRAG, to evaluate long-context reasoning and retrieval capabilities in Korean. KO-LONGRAG focuses on retrieval-augmented generation (RAG) tasks with an average context length of approximately 14,000 tokens, challenging models to process extensive Korean texts, extract relevant information, and reason effectively. Similar to LONGRAG, it includes 50 unanswerable cases among total of 300 queries. Models EXAONE 3.5 32B Qwen 2.5 32B C4AI Command 32B Gemma 2 27B Yi 1.5 34B EXAONE 3.5 7.8B Qwen 2.5 7B Llama 3.1 8B Gemma 2 9B Phi 3 small (7B) EXAONE 3.5 2.4B Qwen 2.5 3B Qwen 2.5 1.5B Llama 3.2 3B Gemma 2 2B Single-doc QA Multi-doc QA Average Answerable Unanswerable Total Answerable Unanswerable Total 92.4 90.0 85.6 - - 68.4 61.2 78.0 - 8.0 80.8 56.4 22.0 48.8 - 100.0 98.0 66.0 - - 100.0 98.0 76.0 - 14.0 100.0 98.0 96.0 12.0 - 93.7 91.3 82.3 - - 73.7 67.3 77.7 - 9.0 84.0 63.3 34.3 42.7 - 72.8 48.4 62.4 - - 64.0 33.2 56.8 - 4.8 61.6 2.4 21.6 40.0 - 98.0 92.0 62.0 - - 98.0 94.0 28.0 - 14.0 84.0 94.0 92.0 16.0 - 77.0 55.7 62.3 - - 69.7 43.3 52.0 - 6.3 65.3 17.7 33.3 36.0 - 85.3 73.5 72.3 - - 71.7 55.3 64.8 - 7.7 74.7 40.5 33.8 39.3 - Table 15: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language models with Ko-LongRAG benchmarks. The benchmark is extended with the Unanswerable case, which requires models to respond as Unanswerable when the information cannot be found within the context. Bold scores indicate the best performance, and underlined scores mean the second best. The detailed task scores are presented in Table 15. Similar to LONGRAG, the evaluation setup for KO-LONGRAG incorporates short answer evaluation to align with the methodology used in LONGRAG. However, as the trends for short answers are consistent with those observed for long answers, the final evaluation focuses solely on long answer correctness to streamline the analysis without compromising robustness. The detailed prompt and examples used for KO-LONGRAG evaluation are provided in Figure 6 and 7, respectively. The prompt used for KO-LONGRAG evaluation is illustrated in Figure 8. Ko-LongRAG Prompt [ Single-doc QA Prompt ] System: 당신은 도움이 되는 어시스턴트입니다. User: 다음 문서를 살펴보고, 질문에 대한 답을 추출하세요. 질문에 대한 답만 생성하세요. 답변은 매우 간결해야 합니다. 답변을 문서에서 찾을 수 없는 경우, 주어진 정보로 답할 수 없다로 응답하세요. 문서는 Title에 따라 정렬된 Wikipedia 문단 목록이며 제목은 다음과 같습니다: {{titles}}. 각 위키피디아 문단은 Title 필드와 Text 필드를 포함합니다. 문서는 다음과 같습니다: {{context}}. 질문은 다음과 같습니다: {{question}}. [ Multi-doc QA Prompt ] System: 당신은 도움이 되는 어시스턴트입니다. User: 다음 문서를 검토하고 질문에 답하세요. 문서는 Wikipedia 문단 목록이며 제목은 다음과 같습니다: {{titles}}. 질문에는 두 가지 유형이 있습니다: 예 또는 아니오로 답하거나 두 후보 중에서 선택해야 하는 비교 질문과, 단답형 형태의 일반 질문입니다. 문서는 다음과 같습니다: {{context}}. 문서에서 필요한 문단을 찾아 질문에 답하세요: {{question}}. 일반 질문의 경우 문단에서 정확한 단어를 찾아서 답변해야 합니다. 질문에 대한 답만 생성하고 다른 어떤 것도 생성하지 마세요. 답변을 문서에서 찾을 수 없는 경우, 주어진 정보로 답할 수 없다로 응답하세요. Figure 6: Prompt for evaluating Ko-LongRAG D.2.5 Ko-WebRAG KO-WEBRAG is real-world benchmark tailored to assess the performance of language models as generators within the Retrieval-Augmented Generation (RAG) framework, using web-search engine as fixed retriever. The benchmark comprises 300 RAG tasks, each featuring user query alongside documents retrieved by the simulated web-search engine. The retrieved documents in KO-WEBRAG are meticulously curated to ensure they provide sufficient supporting information for generating gold-standard answer. Context lengths vary from 4K to 32K tokens, with an average length of approximately 14,000 tokens. Each dataset instance includes user query, gold-standard answer, and the corresponding retrieved documents. The performance of the target LLM is evaluated based on its ability to generate answers that match the gold-standard answer, measuring its effectiveness as generator in RAG tasks. The evaluation involves assessing the responses of the LLM to each of the 300 tasks using GPT-4o. The percentage of tasks for which the LLMs responses pass this evaluation is reported as the final score. To align with the purpose of Korean-language benchmark, the GPT-4o LLM-as-a-judge prompt incorporates additional criteria beyond semantic alignment with the gold-standard answer; it also checks whether questions asked in Korean have been answered in Korean. Note that Qwen models smaller than 32B often fail to meet this language compliance criterion, leading to lower scores. Ko-LongRAG Examples [ Single-doc QA Answerable Case ] Context: ... Title: 박진우 (야구인) Text: 박진우(朴晋佑, 1990년 2월 12일 )는 전 KBO 리그 NC 다이노스의 투수이자, 현 KBO 리그 SSG 랜더스의 스카우트이다. ... 2019년 시즌 : 선발과 불펜을 가리지 않고 활약했다. 시즌 140.2이닝 3점대 평균자책점, 92탈삼진, 9승 7패, 5홀드를 기록했다. 이동욱 감독은 가장 MVP로 꼽고 싶은 선수라며 칭찬했다. ... Question: 박진우가 NC 다이노스에서 9승을 기록한 시즌은 언제인가요? Answer: 2019년 [ Single-doc QA Unanswerable Case ] Question: 인천남동소방서의 설립 연도는 무엇인가요? Answer: 주어진 문서내에서 답할 수 있는 정보가 충분하지 않습니다. [ Multi-doc QA Answerable Case ] Context: ... Title: 아스투리아스 공상 Text: 아스투리아스 공상은 스페인의 프린시페 데 아스투리아스 재단(Fundación Príncipe de Asturias) 이 주관하는 상이다. 1980년 9월 24일 스페인의 왕세자에 해당하는 호칭인 아스투리아스 공이었던 펠리페 (Felipe, 펠리페 6세)에 의해 제정되었으며 1981년에 첫 시상식이 열렸다. 총 9개 부문 (예술 부문, 커뮤니케이션 인문주의 부문, 국제 협력 부문, 문학 부문, 사회과학 부문, 체육 부문, 기술 과학 연구 부문, 화합 부문, 아스투리아스 모범상 부문)으로 나누어 시상한다. 시상식은 아스투리아스 지방의 오비에도에서 열린다. 수상자는 주안 미로가 제작한 조각, 상금 50,000 유로를 받게 된다. ... Title: 미겔 데 세르반테스 상 Text: 미겔 데 세르반테스 상(-賞, ) 또는 세르반테스 상은 스페인 작가 미겔 데 세르반테스의 이름이 붙은 스페인어 작가에게 수여되는 문학상으로, 영연방의 맨 부커 상과 유사한 스페인어권의 상이다. 그러나 맨 부커 상과는 다르게 일생 동안의 문학적 성취를 평가해서 단 한 번만 수여하므로 스페인어권에서 그 권위는 노벨 문학상에 버금간다. 1976년 제정되었다. 스페인 문화부가 수여하며 상금은 12만 5천 유로이다. ... Question: 아스투리아스 공상과 미겔 데 세르반테스 상 중 상금이 더 많은 것은 무엇인가요? Answer: 미겔 데 세르반테스 상 [ Multi-doc QA Unanswerable Case ] Question: 넬슨 록펠러와 노아 사이러스는 둘 다 정치 경력을 가지고 있었나요? Answer: 주어진 문서내에서 답할 수 있는 정보가 충분하지 않습니다. Figure 7: Examples of Ko-LongRAG Ko-LongRAG LLM-as-a-Judge Prompt System: You are an expert evaluator of text answers in Korean. Your task is to compare the content of two Korean answers, long answer (long_ans) and short answer (short_ans), with the provided correct answers (Answer), which may contain multiple correct options. Both the long answer and the short answer need to be checked for correctness. The long and short answers do not need to match any of the answers in the Answer list word-for-word but must convey the same key meaning or idea. If either the long or short answer matches any one of the correct answers in the Answer list, it should be considered correct. Focus only on the accuracy of the content and ignore style, tone, or extra information unless it introduces inaccuracies. For both the long and short answers, return only the evaluation result as Python dictionary object, and ensure the output is formatted as valid Python code. Here are two examples of how to evaluate answers: Example 1: Question: HP는 게임에서 무엇을 의미하나요? Answer: ['체력', '생명력'] long_ans: HP는 생명력 또는 체력을 의미하며, 게임에서 캐릭터의 생존력을 나타내는 지표입니다. HP 가 줄어들면 캐릭터는 점점 약해지며, 0이 되면 게임에서 탈락하거나 패배할 수 있습니다. short_ans: HP는 캐릭터의 체력입니다. Evaluation: {'long_ans': 'correct', 'short_ans': 'correct'} Example 2: Question: 프랑스의 수도는 어디인가요? Answer: ['파리'] long_ans: 프랑스의 수도는 파리로, 리옹의 오른쪽 아래에 위치하고, 문화와 예술의 중심지로 알려져 있습니다. 에펠탑, 루브르 박물관, 노트르담 대성당 등 유명한 관광지가 위치해 있습니다. short_ans: 프랑스의 수도는 리옹입니다. Evaluation: {'long_ans': 'correct', 'short_ans': 'incorrect'} Now, proceed with your evaluation of the following question, answer, and responses, and return only the evaluation as valid Python dictionary. Ensure the response is valid Python dictionary object without any additional text. User: Evaluate the following long and short answers based on the provided correct answer. Your goal is to determine if the long and short answers are correct. Return the evaluation result in the form of Python dictionary: {'long_ans': 'correct 'or 'incorrect ', 'short_ans': 'correct'or 'incorrect'}. Question: {{question}} Answer: {{answer}} long_ans: {{long_ans}} short_ans: {{short_ans}} Return only the evaluation in the form of Python dictionary. Do not include any explanation or additional comments. Figure 8: LLM-as-a-judge prompt for evaluating Ko-LongRAG 25 D.3 General Domain For all benchmarks in the General Domain category, we use 0-shot prompts and parse final answer from the generated model response. Greedy decoding is used and maximum length of generation is set to 2,048 for all tasks. From Figure 9 to 13, we present all prompts we use for the evaluation for each benchmarks. For BBH, we utilize 0-shot CoT prompts6 from Language Model Evaluation Harness [14]. GSM8K/MATH prompt (CoT) Given the following math problem, reason step-by-step and give final answer to the problem. Put your final answer within boxed{}. Problem: {{question}} Answer: Lets think step by step. Figure 9: Prompt for evaluating GSM8K (CoT) and MATH (CoT) benchmarks HumanEval/MBPP prompt User: Please provide self-contained Python script that solves the following problem in markdown code block: {{input}} Assistant: Below is Python script with self-contained function that solves the problem and passes corresponding tests: python Figure 10: Prompt for evaluating HUMANEVAL and MBPP benchmarks. We use the default prompt setting from the official Github repository7 of EvalPlus [31]. MMLU/GPQA prompt (CoT) Given the following question and candidate answers (A, B, and D), reason step-by-step and choose the best answer to the question. Question: {{question}} A. {{option }} B. {{option }} C. {{option }} D. {{option }} Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, or D. Answer: Lets think step by step. Figure 11: Prompt for evaluating MMLU (CoT) and GPQA (CoT) benchmarks 6https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/bbh/cot_zeroshot 26 KMMLU prompt (CoT) 다음 시험 문제에 대해서, 충분히 생각하고 추론하여, 4개의 보기(A, B, C, D) 중 정답을 고르세요. 문제: {{question}} A. {{option }} B. {{option }} C. {{option }} D. {{option }} 당신의 대답은 \"정답은 [정답 보기]입니다.\"로 끝나야하고, [정답 보기]는 A, B, C, 중 하나여야 합니다. 정답: 문제를 풀기 위해, 한 번 천천히 생각해봅시다. Figure 12: Prompt for evaluating KMMLU (CoT) benchmark ARC-C prompt Given the following question and candidate answers (A, B, and D), choose the best answer to the question. Question: {{question}} A. {{option }} B. {{option }} C. {{option }} D. {{option }} Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, or D. Answer: Figure 13: Prompt for evaluating ARC-C benchmark"
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open Foundation Models by 01.AI, 2024. [3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, 2024. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Cohere For AI. c4ai-command-r-08-2024 (Revision 280b5c1), 2024. [11] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of informationseeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45994610, Online, June 2021. Association for Computational Linguistics. [12] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [13] Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074 1084, Florence, Italy, July 2019. Association for Computational Linguistics. [14] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. [15] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, 29 Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. arXiv preprint arXiv:2009.03300, 2020. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. Advances in Neural Information Processing Systems, 2021. [18] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. [19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 30 [20] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14191436, Online, June 2021. Association for Computational Linguistics. [21] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs. arXiv preprint arXiv:2406.15319, 2024. [22] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. [23] Gregory Kamradt. LLMTest Needle In Haystack - Pressure Testing LLMs. https://github.com/gkamradt/ LLMTest_NeedleInAHaystack, 2023. [24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. [25] Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. [26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [27] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving Quantitative Reasoning Problems with Language Models. Advances in Neural Information Processing Systems, 2022. [28] LG AI Ethics Principles. https://www.lgresearch.ai/about/vision#ethics. [29] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline. arXiv preprint arXiv:2406.11939, 2024. [30] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [31] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [32] Michael McCloskey and Neal J. Cohen. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. Psychology of Learning and Motivation, 24:109165, 1989. [33] Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple Preference Optimization with Reference-Free Reward. arXiv preprint arXiv:2405.14734, 2024. [34] Meta. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models, 2024. [35] Korean Large Language Model Trustworthiness Benchmark Data. https://aihub.or.kr/aihubdata/data/ view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71760. [36] OpenAI. GPT-4 Technical Report, 2023. [37] Jeonghwan Park. LogicKor. 2024. [38] Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, W. Bradley Knox, Chelsea Finn, and Scott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 31 [39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model. Advances in Neural Information Processing Systems, 2024. [40] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: Graduate-Level Google-Proof Q&A Benchmark. arXiv preprint arXiv:2311.12022, 2023. [41] LG AI Research. EXAONE 3.0 7.8B Instruction Tuned Language Model. arXiv preprint arXiv:2408.03541, 2024. [42] LG AI Research. KoMT-Bench. https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench, 2024. [43] Martin Riddell, Ansong Ni, and Arman Cohan. Quantifying contamination in evaluating code generation capabilities of language models. arXiv preprint arXiv:2403.04811, 2024. [44] Noam Shazeer. GLU Variants Improve Transformer. arXiv preprint arXiv:2002.05202, 2020. [45] Guijin Son, Hanwool Albert Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. KMMLU: Measuring Massive Multitask Language Understanding in Korean. arXiv preprint arXiv:2402.11548, 2024. [46] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. Neurocomputing, 568:127063, 2024. [47] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. arXiv preprint arXiv:2210.09261, 2022. [48] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving Open Language Models at Practical Size. arXiv preprint arXiv:2408.00118, 2024. [49] Qwen Team. Qwen2.5: Party of Foundation Models, September 2024. 32 [50] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [51] Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural Machine Translation with Byte-Level Subwords. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 91549160, 2020. [52] Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, Tzuhao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu, Xianfeng Tang, Qi He, Yao Ma, Ming Huang, and Suhang Wang. Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness. arXiv preprint arXiv:arXiv:2411.03350, 2024. [53] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned Language Models Are Zero-Shot Learners. arXiv preprint arXiv:2109.01652, 2021. [54] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. LiveBench: Challenging, Contamination-Free LLM Benchmark, 2024. [55] Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824, 2024. [56] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. [57] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. [58] Weihao Zeng, Can Xu, Yingxiu Zhao, Jianguang Lou, and Weizhu Chen. Automatic Instruction Evolving for Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 69987018, 2024. [59] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [60] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: new benchmark for query-based multidomain meeting summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 59055921, Online, June 2021. Association for Computational Linguistics. [61] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-Following Evaluation for Large Language Models. arXiv preprint arXiv:2311.07911, 2023."
        }
    ],
    "affiliations": ["LG AI Research"]
}