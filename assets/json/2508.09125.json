{
    "paper_title": "Complex Logical Instruction Generation",
    "authors": [
        "Mian Zhang",
        "Shujian Liu",
        "Sixun Dong",
        "Ming Yin",
        "Yebowen Hu",
        "Xun Wang",
        "Steven Ma",
        "Song Wang",
        "Sathish Reddy Indurthi",
        "Haoyun Deng",
        "Zhiyu Zoey Chen",
        "Kaiqiang Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Code and Benchmark: https://github.com/mianzhang/LogicIF"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 5 2 1 9 0 . 8 0 5 2 : r a"
        },
        {
            "title": "COMPLEX LOGICAL INSTRUCTION GENERATION",
            "content": "Mian Zhangαβ Shujian Liuβ Sixun Dongβ Ming Yinβ Yebowen Huβ Xun Wangβ Steven Maβ Song Wangβ Sathish Reddy Indurthiβ Haoyun Dengβ Zhiyu Zoey Chenα Kaiqiang Songβ αUniversity of Texas at Dallas βZoom Video Communications"
        },
        {
            "title": "ABSTRACT",
            "content": "Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate collection of complex code functions and use LogicIFGen to construct LogicIFEval, benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Figure 1: (Left) Instruction Following Test: LLMs are required to follow only the natural language instruction to simulate every logic of code function via generating text. (Right) Overall instruction-following performance of evaluated models on LogicIFEval; NT denotes NoThinking. DS denotes DeepSeek. This work is done during an internship in Zoom Gen AI. Corresponding author"
        },
        {
            "title": "INTRODUCTION",
            "content": "Before the advent of ChatGPT (Ouyang et al., 2022), chatbots built on earlier models such as GPT-1 (Radford & Narasimhan, 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and other architectures struggled to generate coherent and contextually appropriate utterances. At that time, it was difficult to imagine that such models could assist with tasks in daily life. With the emergence of instruction-following capabilities, large language models (LLMs) are now able to accurately understand basic human intentions and even leverage tools to perform wide range of tasks that enhance productivity, such as deep research1, coding assistance2, and scientific discovery (AI4Science & Quantum, 2023). The instructions for these tasks could contain rich logic structures such as sequencing, loops, nesting, recursion, and backtracking. Previous instruction-following evaluations typically focus on instructions with constraints on the response format (e.g., fewer than 300 words) or content (e.g., in Shakespeares tone) (Zhou et al., 2023; Qin et al., 2024; Jiang et al., 2024) and seldom explore how well LLMs perform on the instructions with rich logic structures. To address this, we first propose LogicIFGen, scalable, automated framework for generating verifiable instructions from code functions, which can naturally contain rich logic structures. Given test input data, models are expected to rely solely on the natural language instruction to simulate every logic of the code function and produce the same results, analogous to being verbally guided by an examiner to process the input data step-by-step (see Figure 1 (Left)). The models are required to refrain from writing code or using external tools; instead, they must simulate and execute code logic only through text generation. This setting aligns more closely with instruction-following evaluation, as most tasks require the model to explicitly unfold the underlying logic. For example, when given an instruction such as repeat asking the user for clarification until you fully understand the intent of the user, the model must perform each logical step through natural language alone. LogicIFGen obtains the reference labels by executing the code function on the same inputs. By comparing model outputs with these reference labels, we can easily verify whether model follows the natural language instruction correctly. In addition, LogicIFGen incorporates state trackers to monitor intermediate logic flow, enabling us to double check whether models faithfully adhere to the instructions internal logic rather than hallucinating the final results. Second, we construct benchmark called LogicIFEval using LogicIFGen, which contains 426 verifiable, logic-rich instructions paired with associated test cases. The functions used to generate LogicIFEval are the solutions of challenging simulation problems from competitive programming platforms, CodeForces3 and POJ4. These simulation problem solutions are especially suitable for instruction-following evaluation because they require models to faithfully emulate complex, step-by-step processes and state transitions, often involving intricate control flow, edge case handling, and the coordination of multiple logic elements. Experimental results show that most popular LLMs are only able to correctly follow fewer than 60% of the instructions in LogicIFEval, revealing significant deficiency in their instruction-following ability (see Figure 1 (Right)). Open-source models continue to lag behind frontier models such as the OpenAI o-series and Anthropic Claude. As logical complexity increases, models find it increasingly difficult to accurately interpret and follow instructions. We also observe that incorporating explicit thinking before generating response can potentially enhance instruction-following performance for large LLMs, but not for smaller LLMs. Further error analysis and case studies reveal key failure modes and highlight promising directions for advancing LLMs ability to follow logic-rich instructions. We release LogicIFGen and LogicIFEval, as well as compute-friendly version of the benchmark, LogicIFEval-mini at: https://github.com/mianzhang/LogicIF. 1https://openai.com/index/introducing-deep-research/ 2https://cursor.com/en 3https://codeforces.com 4http://poj.org Figure 2: Pipeline of LogicIFGen. Given seed function and its corresponding test cases, LogicIFGen generates natural language instructions along with gold labels, which include both the function outputs and the values of state trackers. 1) The input function is first anonymized and augmented with state trackers. 2) The anonymized function is then translated into natural language description, producing instruction that precisely describes its logic and expected behavior with test cases verified to have no execution errors. 3) Finally, the test cases are executed on the anonymized function to obtain the gold labels."
        },
        {
            "title": "2 LO G I CIFGE N: VERIFIABLE INSTRUCTION GENERATION FROM CODE",
            "content": "We introduce LogicIFGen, framework that automatically generates verifiable, logic-rich instructions from code functions. Each instruction provides comprehensive, step-by-step natural language description of the functions behavior, clearly specifying input/output formats and detailing all relevant control flows and data processing steps. Models are then expected to follow these instructions without access to the source code to process inputs and reproduce the functions outputs. Figure 2 illustrates our instruction generation framework with running example. Additionally, LogicIFGen integrates Multi-turn Difficulty Evolution and Multi-turn Verification and Refinement modules, which dynamically adjust instruction complexity and verify correctness to ensure each instruction fully and accurately captures the functions logic. We provide more detailed explanation of each module in the remainder of this section. Anonymized Function with State Trackers The first step in LogicIFGen is to anonymize the original function so that only its data operations and control logic remains. LogicIFGen use an LLM to replace the function and variable names with simple, generic identifiers, single characters or common placeholders, thereby stripping away any semantic hints that could enable models to leverage domain-specific knowledge. Moreover, LogicIFGen asks an LLM to augment the function with state trackers, which are variables designed to log specific runtime states of the function, reflecting the functions internal logic and execution flow. These include, for example, the number of iterations of for loop, the number of times an if block is executed, or the maximum length reached by dynamically list. Models instructed via natural language are required to reproduce not only the functions outputs but also the values of state trackers. We consider 3 the instruction to be correctly followed only when both the final output and the state tracker values match the ground truth. This ensures that models truly comprehend and execute the intended logic, rather than relying on shortcuts or hallucinating plausible outputs. The prompt used for this step is shown in Figure 6. Furthermore, to increase the logical complexity of the function, the Multi-turn Difficulty Evolution module in LogicIFGen prompts an LLM to evolve the code function by introducing more diverse compositions of logic units, such as loops, function calls, and recursion (see Figure 9 for the full prompt). Natural Instruction Generation LogicIFGen uses an LLM (see Figure 7 for the prompt) to generate detailed natural language instructions based on the anonymized function with state trackers. The instructions adopt conversational style (e.g., Now you need to. . . , Next, go through. . . ) to guide step-by-step execution. Each instruction is crafted to be sufficiently precise so that an LLM can follow it to conduct all the data operations without access to the source code, producing identical outputs and state tracker values (as illustrated in Figure 1 (Left)). To ensure the instructions fully and correctly capture the sub-tasks and logic of the function, LogicIFGen incorporates the Multi-turn Verification and Refinement module. In this module, an LLM (see Figure 8 for the prompt) reviews the generated instructions, checking for comprehensive coverage of all code operations and flagging potential omissions, such as loop conditions, variable updates, or edge case handling. The instructions are iteratively refined based on this feedback until they accurately reflect the complete operations and logic of the function. The full function description of of the runing example in Figure 2 is shown below:"
        },
        {
            "title": "Function Description",
            "content": "INPUTS: You start with two pieces of data. First is single whole number called n, which tells you how many items you should expect. Second is list called A, which contains exactly numeric values. LOGICS: Okay, heres what you do step by step. First, take your list and sort it from largest down to smallest to make new list, which well call B. Now lay out four things on your workspace: 1. running total called val, which you set to 0. 2. An empty collection called you can think of it as bag where youll always be able to pull out the smallest number. 3. counter called rm for counting how many numbers you remove, which you set to 0. 4. tracker called max sz for the largest size ever reaches, which you also set to 0. Now youre going to process each number in in order, from the first (which is the largest) down to the last (the smallest). For each item, do the following: Let be the current item from B. Add to your running total val (so val = val + x). Drop into your bag h. Check how many items are now in h. If that count is bigger than max sz, then write down the new count as the new max sz. Next, look at your running total val. If val is now below zero (that means val < 0), you have to remove one item from h. To do that, find the smallest number currently sitting in h, call that y, and take it out of h. Then subtract from val (so val = val - y) and add 1 to your removal counter rm. After youve done this for every single in B, youre finished with the loop. No more items left to process. OUTPUTS: When youre all done, you end up with two things. First, the number of items left in your bag thats the first result youll report. Second, you have two statistics: the total rm, which tells you how many times you removed the smallest item, and max sz, which tells you the largest number of items that ever piled up in at once. Test Case Filtering and Gold Result Generation At this stage, we have two modalities representing the same operations and logic: the anonymized code function and the corresponding natural language instruction. Given the same input data, LLMs are expected to follow the instruction and produce results identical to those generated by the code function, including both the function outputs and the values of the state trackers. LogicIFGen generates these gold labels by executing the test cases on the anonymized function. Prior to 4 this, it performs preliminary execution step to filter out test cases that may trigger errors due to modifications introduced to function during the multi-turn difficulty evolution process. This ensures that only valid and executable test cases are retained for final gold label generation. Quantifiable Instruction Complexity One of the advantages of LogicIFGen is that the difficulty of the generated instructions is quantifiable: the anonymized function and natural language instruction describe the same operations and logic so we could use the code function as proxy to analyze the difficulty of the instruction. We use Pythons Abstract Syntax Tree (AST) package5 to traverse the syntactic structure of each function and calculates several key measures. Cyclomatic Complexity (C) quantifies the total number of control flow decision points (such as if/elif statements, for/while loops, and try/except blocks), representing the number of linearly independent execution paths through the code. Nesting depth (D) tracks the maximum depth of nested control structures. Function Call Count (F ) counts the number of function invocation, including both built-in and user-defined functions. Function length (L) is calculated as the number of lines spanned by the function definition. We adopt an intuitive weighting scheme for aggregating these ASTbased measures based on their cognitive complexity, which has been proven effective in measure the logical complexity of codes (Munoz Baron et al., 2020). Score = 3 + 2 + 1 + 0.5. (1) Our experiments in Section 4 also show the effectiveness of the intuitive weights. More advanced weighting methods like learning-based methods (Sepidband et al., 2025) could be explored in future work."
        },
        {
            "title": "3 LO G I CIFEV A L",
            "content": "In the previous section, we introduce LogicIFGen, which generates instructions and gold labels from code functions. However, we observed that the complexity of the seed function is crucial for producing challenging instructions. To this end, we curate collection of simulation problem solutions and corresponding test cases from competitive programming platforms, including Codeforces and POJ. Specifically, for Codeforces, we select problems tagged with implementation and difficulty score above 1700. For POJ, we include difficult simulation problems as identified by various online users. These functions require models to faithfully emulate complex, step-by-step processes and state transitions, involving intricate control flow, edge case handling, and the coordination of multiple logic elements, making them ideal sources for generating complex instructions used to evaluate the instruction-following ability. Then we construct LogicIFEval based on these functions as follows: Data Filtering We use two-stage filtering process to make the seed functions more diverse and remove test cases not suitable for the instruction-following evaluation: 1) Seed Function Filtering: We remove duplicate or highly similar functions to avoid redundancy. Specifically, we use OpenAIs text-embedding3-small model6 to compute embeddings of each function and calculate pairwise cosine similarities. If the similarity between two functions is greater than 0.7, we consider them near-duplicates. For each such pair, we keep only the longer function (measured by code length), since longer implementations often include richer logic and contribute to more challenging instruction-following tasks. 2) Test Case Filtering: We remove test cases with unusual or problematic outputs to keep the dataset clean and manually executable. Specifically, we discard cases where: I) The values of state trackers are greater than or equal to 50 (to avoid overly large internal states). II) The output values have excessive precision (more than six decimal places). III) The state tracker dictionaries are malformed or incorrectly formatted. IV) The input values are too large (magnitude exceeding 107). After this step, we remove functions with fewer than 3 test cases to ensure that each function has enough test coverage for reliable evaluation. These filtering steps are designed to prevent 5https://docs.python.org/3/library/ast.html 6https://platform.openai.com/docs/models/text-embedding-3-small 5 failures caused by large loops, deep recursion, or complex numerical computations that could overwhelm model capacity. This ensures that any model failures are attributable to weaknesses in instruction following, rather than the inherent difficulty of execution. The final dataset is easy to understand and execute step-bystep, staying consistent with our goal of generating clear and verifiable instructions. Originally, we collected 1,107 seed functions. After these filtering steps, 426 unique functions and 3,050 test cases remain. Data Generation and Human Verification We apply LogicIFGen using o4-mini as the generation LLM on the filtered seed functions and test cases, resulting in LogicIFEval, benchmark comprising 426 complex instruction-following tasks. The numbers of turns used for Multi-turn Difficulty Evolution and Multi-turn Verification and Refinement are 1 and 3, respectively. Instructions that still fail verification after 3 turns are discarded. To evaluate the quality of the generated instructions, we hired five PhD-level experts in computer science (our co-authors) to conduct manual verification. The annotators were instructed to verify whether each line of the anonymized function is accurately and completely described in the corresponding natural language instruction. According to their assessment, 97% of the instructions fully and correctly capture the underlying function logic and their agreement is 97.79%, demonstrating the effectiveness of LogicIFGen in transforming code functions into natural language instructions. More details human evaluation process could be found in Appendix D. We further categorize the functions into difficulty levels using tercile-based thresholds derived from the complexity scores computed by Equation 1. The final benchmark contains 142 easy, 145 medium, and 139 hard instructions. Benchmark Statistics and Release The instructions in LogicIFEval have average of 3,428 characters and 662 words. The functions used to generate instructions have an average cyclomatic complexity of 11.10 and maximum nesting depth of 3.16. In total, the benchmark includes 2,049 loops, 2,253 conditional statements, and 5,289 function calls, reflecting diverse control flow patterns. Each function is evaluated with an average of 7.2 test cases, resulting in 3,050 test cases overall. These statistics highlight the benchmarks scale and its focus on challenging instruction-following in LLMs. To support researchers with limited computational resources, we also release representative mini-benchmark, LogicIFEval-mini, which consists of 102 functions sampled in stratified manner based on the complexity scores. Our experimental results in Section 4 show that constructing the min-benchmark based on the complexity scores is effective."
        },
        {
            "title": "4 FRONTIER MODEL PERFORMANCE ON LO G I CIFEV A L",
            "content": "We test 21 LLMs on LogicIFEval: Frontier Thinking Models: Models incorporating explicit thinking process before generating response, including gpt-5, gpt-5-mini, o3, o4-mini, o3-mini, Claude-3.7-Sonnet and Claude-4Sonnet, Gemini-2.5-Flash, Qwen3-32B and Qwen3-8B, DeepSeek-R1-Distill(DS)-Llama-70B and DS-Qwen-32B. Frontier NoThinking (NT) Models: Models directly give response without explicit thinking: GPT-4.1, GPT-4.1-mini, GPT-4o, Claude-3.7-Sonnet-NT and Claude-4-Sonnet-NT, Gemma-3-27B, Qwen3-32B-NT and Qwen3-8B-NT, and Llama-3.3-70B(-Instruct). Inference Setting For closed-source models, we use the default temperature setting provided by the respective API. For open-source models, we adopt the official recommended inference settings; for example, for the Qwen3 series reasoning models, the temperature is set to 0.6. If no specific recommendation is available, we use temperature of 1.0 by default. For all models, we set the maximum number of generated tokens to 16k to ensure that the models have sufficient capacity to follow instructions and process the inputs."
        },
        {
            "title": "Output State Both Output State Both Output State Both Output State Both",
            "content": "Easy (142) Medium (145) Hard (139)"
        },
        {
            "title": "Average",
            "content": "Thinking Models gpt-5 o3 gpt-5-mini o4-mini o3-mini Claude-4-Sonnet Claude-3.7-Sonnet Gemini-2.5-Flash DS-Qwen-32B DS-Llama-70B Qwen3-32B Qwen3-8B 94.37 95.07 90.85 97.24 91.03 89.66 88.49 82.01 74.10 93.43 89.44 84.98 94.37 90.14 89.44 93.10 87.59 84.83 84.89 79.86 72.66 90.85 85.92 82.39 91.55 92.96 88.73 93.79 84.83 82.07 86.33 78.42 71.22 90.61 85.45 80.75 93.66 90.14 87.32 91.72 81.38 77.93 81.29 68.35 63.31 88.97 80.05 76.29 89.44 87.32 83.10 89.66 78.62 73.79 69.06 61.87 56.12 82.86 76.06 71.13 91.55 87.32 81.69 96.55 77.93 75.86 73.38 58.27 51.08 87.32 74.65 69.72 81.69 76.76 70.42 79.31 64.14 57.93 57.55 45.32 39.57 73.00 62.21 56.10 79.58 75.35 72.54 64.14 55.17 53.10 41.01 34.53 31.65 61.74 55.16 52.58 66.20 59.86 50.70 57.93 36.55 30.34 38.13 22.30 15.83 54.23 39.67 32.39 69.01 66.20 54.93 53.10 36.55 29.66 38.13 24.46 15.83 53.52 42.49 33.57 23.47 13.85 10.33 38.73 31.69 23.94 17.93 16.67 12.21 7.28 32.39 27.46 16.90 13.10 13.67 4.32 3.60 1.44 3.45 3.45 4.32 2. 5.52 6.21 NoThinking Models 85.92 81.69 74.65 82.07 64.83 60.00 71.22 49.64 41.73 79.81 65.49 58.92 GPT-4.1-mini 83.10 78.87 71.13 84.83 64.83 58.62 64.75 51.80 39.57 77.70 65.26 56.57 GPT-4.1 Claude-4-Sonnet-NT 80.28 76.06 69.01 63.45 46.21 37.93 44.60 30.22 24.46 62.91 50.94 43.90 Claude-3.7-Sonnet-NT 73.24 66.20 57.04 61.38 42.07 34.48 43.88 24.46 17.99 59.62 44.37 36.62 35.21 27.93 20.66 59.86 49.30 38.73 32.41 24.14 17.93 12.95 10.07 5.04 GPT-4o 20.42 14.55 8.92 2.88 9.66 33.10 29.58 19.72 18.62 Qwen3-32B-NT 30.28 25.35 15.49 10.34 Llama-3.3-70B 16.20 11.27 6.81 2.16 4.83 14.79 12.21 6.57 2.16 5.52 28.17 26.76 15.49 10.34 Gemma-3-27B 15.96 14.32 7.75 1.44 8.28 28.17 29.58 16.90 14.48 Qwen3-8B-NT 4.32 3.60 4.32 5.04 4.14 2.76 2.07 4.83 9.35 7.91 5.76 5.04 Table 1: Model performance (%) by complexity and average. Both denotes both the output and state trackers match. Best results in Average columns are in bold. Overall performance are highlighted in pink . Performance of each difficulty level are in blue . Main Results model is considered to successfully follow natural language instruction if it passes all associated test cases by producing both the correct outputs and accurate state tracker values. Figure 1 and the last column of Table 1 report the overall accuracies across evaluated models. The top-performing models are the OpenAI gpt-5, o-series and Claude-4-Sonnet, with the best-performing model, gpt-5, achieving an accuracy of 84.98%. These results highlight the strong instruction-following and logic execution capabilities of advanced proprietary LLMs. In contrast, GPT-4o achieves only 20.66%, underperforming relative to other OpenAI models. Besides, widely used open-source models still lag significantly behind. For example, Qwen3-32B, Gemma-3-27B, and Llama-3.3-70B all score below 11%, failing to correctly execute the multi-step logic and state tracking required by LogicIFEval, which highlights clear performance gap between commercial and open-source LLMs on this benchmark. We also notice that explicit thinking before response can potentially improve instruction following for large LLMs. For instance, Claude-4-Sonnet achieves 69.72% accuracy, notably outperforming Claude-4-Sonnet-NT (43.9%) and Claude-3.7-Sonnet (56.1%) outperforms Claude-3.7-Sonnet-NT (36.62%). Similarly, OpenAIs thinking models, gpt5 and o-series, perform substantially better than other non-thinking models from OpenAI. 7 However, the Qwen3 variants show little difference between thinking and non-thinking modes, suggesting that explicit thinking helps only when the underlying model has sufficiently strong capabilities. We also report model performance on LogicIFEval-mini in Figure 5. The results show that models exhibit nearly identical rankings compared to the full benchmark, indicating that LogicIFEval-mini effectively represents the whole benchmark and preserves similar distribution. Results Across Difficulty Levels Table 1 presents model performance across three difficulty levels in LogicIFEval, with separate evaluations for output correctness, state tracker correctness, and their intersection (Both). We could see that all models show clear degradation in performance as difficulty increases (see blue columns ), validating the effectiveness of our AST-based complexity scoring strategy. For instance, gpt-5 drops from 90.85% on Easy tasks to 74.10% on Hard ones, and GPT-4.1-mini from 82.07% to 41.73%. This trend confirms that our benchmarks stratification meaningfully reflects logical complexity of instructions. Second, output accuracy consistently exceeds state tracker accuracy across nearly all models, especially as complexity increases. For example, GPT-4.1-mini achieves 71.22% output accuracy on Hard instructions, but only 49.64% in state tracking. This implies that models may generate correct answers without strictly adhering to the intended logic steps or may follow alternative logic paths. These observations underscore the importance of adding state trackers to supervise the logic flow in complex instruction-following tasks."
        },
        {
            "title": "5 ANALYSIS",
            "content": "Failure Modes To understand the reasons of why models can not pass the test cases, we first go through some errors and summarize them as following five types: 1) Control Flow Misexecution: incorrect, incomplete, or inconsistent execution of core control structures (e.g., loops, branches, function calls), including wrong iteration counts, improper branching, or mishandled recursion/returns. 2) State Tracking Errors: failure to correctly maintain or update internal variables or data structures, such as counters, flags, arrays, stacks, or accumulated values. 3) Missing Logic Elements: omission of required components (e.g., loops, branches, edge case handling or initialization). 4) Misordered Execution: performing steps in the wrong sequence, such as using uninitialized variables, premature function calls, or out-of-order updates. 5) Instruction Misinterpretation: misunderstanding the instructions intent, leading to hallucinated steps, misapplied patterns, or ignored constraints. We map the error cases to these categorizes with the help of GPT-4.1. Figure 3 summarizes the error types made by some popular models. We could see that the most frequent error categories across models are Control Flow Misexecution, Instruction Misinterpretation, and State Tracking Errors. In contrast, Missing Logic Elements and Misordered Execution are consistently low (mostly under 10%), suggesting that most models can identify the required logic components (what to do) and their approximate ordering (when to do it). However, they often struggle with actually executing the logic elements correctly, either by mismanaging control structures (e.g., loop iterations, function calls), hallucinating or misinterpreting instruction details, or failing to track internal state variables accurately over time. In addition, open-source models like Qwen3-32B, DS-Llama-70B, and Gemma-3-27B exhibit especially high rates of Control Flow Misexecution, up to 53.5% in the case of Gemma-3-27B. This highlights their difficulty in faithfully reproducing the logic-heavy instruction steps, which require consistent handling of nesting, conditionals, and function boundaries. Besides, State Tracking Errors is also major issue across nearly all models. For instance, Claude-4-Sonnet and Gemini-2.5-Flash show 38.8% and 32.4% error rates respectively, indicating frequent failures in maintaining correct variable states when following the instructions. These include losing track of counters, failing to propagate updates through data structures, or resetting intermediate results incorrectly. This reinforces the importance of evaluating beyond output correctness. We show representative cases for each error type in Figure 4 and the full examples in Appendix B. It should be noted that the error types are not strictly mutually exclusive. For example, the missing logic 8 Figure 3: Error Type Distribution. elements error observed in the fourth example of Figure 4 can also be attributed to the incorrect execution of the while condition. Why Thinking Helps Large LLMs As indicated in Section 4, incorporating explicit thinking before generating response can potentially enhance instruction-following performance for large LLMs. To gain deeper insights into this effect, we conduct case analysis focusing on instances where Claude-4-Sonnet produces correct results, whereas Claude-4-Sonnet-NT fails. These cases can be categorized into two types: 1) the model arrives at the correct answer during the thinking process itself, and 2) the model articulates detailed, step-by-step plan to solve the task. Both scenarios suggest that explicit thinking encourages the model to slow down and solve the problem more deliberately, rather than relying on pattern matching or intuitive leaps. We provide two representative examples in the Appendix due to space limit."
        },
        {
            "title": "6 RELATED WORK",
            "content": "General instruction-following evaluation work typically focuses on instructions that impose constraints on the response format or content (Zhou et al., 2023; Qin et al., 2024; Wen et al., 2024; Pyatkin et al., 2025). These research centers on enabling models to follow instructions with multiple constraints (Jaroslawicz et al., 2025; Pyatkin et al., 2025). Verification methods generally fall into two categories: heuristic functions or LLM-as-judge. While LLM-as-judge has been shown to correlate highly with human judgments (Qin et al., 2024), there is still room for improvement (Zeng et al., 2023). In addition to general-purpose instructionfollowing benchmarks, other datasets target specific scenarios or constraints, such as length control (Zhang et al., 2025), long-context settings (Wu et al., 2024), or agentic scenarios (Qi et al., 2025). Yang et al. (2025) explore LLMs ability to adhere to user intent while producing functionally accurate code. In contrast, our work uses code as the source to generate instructions, requiring LLMs to generate text, rather than code, in response. Some studies have suggested that reasoning may decrease instruction-following performance (Li et al., 2025; Fu et al., 2025). However, our findings in Section 4 indicate that reasoning can actually enhance instruction-following for logic-rich instructions. Further research like Tam et al. (2024); Qin et al. (2025) is needed to clarify the relationship between reasoning and instruction following. To the best of our knowledge, we are the first to systematically investigate whether LLMs can precisely follow logic-rich instructions and how to generate such instructions in scale. 9 Figure 4: Error Cases: On the left are the excerpts from function codes where the model makes errors. On the right are excerpts from the LLMs responses, highlighting their failures across different modes. The explanations for model failures are indicated in red, and the corresponding code lines are highlighted . Please note that the model only has access to the natural language instruction, which could correctly describe the logic, when solving the tasks; the code is provided here solely to facilitate understanding of the errors."
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORK",
            "content": "In this paper, we introduce LogicIFGen, framework that automatically generates verifiable, taskintensive, logic-rich instructions from code functions. We also present LogicIFEval, challenging instruction-following evaluation benchmark constructed using LogicIFGen, which consists of 426 tasks featuring complex logic. Experiments show that most proprietary and open-source models struggle to solve these tasks, revealing significant deficiency in instruction-following capabilities. Future work includes exploring the use of LogicIFGen as verifiable instruction generator for model training and LogicIFEval for evaluation to develop models with generalized and robust instruction-following capabilities."
        },
        {
            "title": "REFERENCES",
            "content": "Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: preliminary study using GPT-4. arXiv [cs.CL], November 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv [cs.CL], May 2020. Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, and Yu Cheng. Scaling reasoning, losing control: Evaluating instruction following in large reasoning models. arXiv [cs.CL], May 2025. Daniel Jaroslawicz, Brendan Whiting, Parth Shah, and Karime Maamari. How many instructions can LLMs follow at once? arXiv [cs.AI], July 2025. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. FollowBench: multi-level fine-grained constraints following benchmark for In Proceedings of the 62nd Annual Meeting of the Association for Computalarge language models. tional Linguistics (Volume 1: Long Papers), pp. 46674688, Stroudsburg, PA, USA, 2024. Association for Computational Linguistics. Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, and Anurag Beniwal. When thinking fails: The pitfalls of reasoning for instruction-following in LLMs. arXiv [cs.CL], May 2025. Marvin Munoz Baron, Marvin Wyrich, and Stefan Wagner. An empirical validation of cognitive complexity In Proceedings of the 14th ACM / IEEE International as measure of source code understandability. Symposium on Empirical Software Engineering and Measurement (ESEM), New York, NY, USA, October 2020. ACM. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Welinder, Christiano, Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst., abs/2203.02155, March 2022. Valentina Pyatkin, Saumya Malik, Victoria Graf, Hamish Ivison, Shengyi Huang, Pradeep Dasigi, Nathan Lambert, and Hannaneh Hajishirzi. Generalizing verifiable instruction following. arXiv [cs.CL], July 2025. Yunjia Qi, Hao Peng, Xiaozhi Wang, Amy Xin, Youfeng Liu, Bin Xu, Lei Hou, and Juanzi Li. AGENTIF: Benchmarking instruction following of large language models in agentic scenarios. arXiv [cs.AI], May 2025. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. InFoBench: Evaluating instruction following ability in large language models. arXiv [cs.CL], January 2024. Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, and Xing Sun. Incentivizing reasoning for advanced instruction-following of large language models. arXiv [cs.CV], July 2025. 11 Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeff Wu, Child, Luan, Dario Amodei, and Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Melika Sepidband, Hamed Taherkhani, Song Wang, and Hadi Hemmati. Enhancing LLM-based code generation with complexity metrics: feedback-driven approach. arXiv [cs.SE], May 2025. Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-Yi Lee, and Yun-Nung Chen. Let me speak freely? study on the impact of format restrictions on performance of large language models. arXiv [cs.CL], August 2024. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, Yiming Liu, Jie Tang, Hongning Wang, and Minlie Huang. Benchmarking complex instruction-following with multiple constraints composition. arXiv [cs.CL], July 2024. Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, and Wei Zhang. LIFBench: Evaluating the instruction following performance and stability of large language models in long-context scenarios. arXiv [cs.CL], November 2024. Jian Yang, Wei Zhang, Shukai Liu, Linzheng Chai, Yingshui Tan, Jiaheng Liu, Ge Zhang, Wangchunshu Zhou, Guanglin Niu, Zhoujun Li, Binyuan Hui, and Junyang Lin. IFEvalCode: Controlled code generation. arXiv [cs.CL], August 2025. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. arXiv [cs.CL], October 2023. Wei Zhang, Zhenhong Zhou, Kun Wang, Junfeng Fang, Yuanhe Zhang, Rui Wang, Ge Zhang, Xavier Li, Li Sun, Lingjuan Lyu, Yang Liu, and Sen Su. LIFEBench: Evaluating length instruction following in large language models. arXiv [cs.CL], May 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv [cs.CL], November 2023. LO CIFEV L-M PERFORMANCE Figure 5: Overall instruction-following performance (%) of all evaluated models on LogicIFEval-mini, measured by the percentage of test cases where both the output and state trackers match the ground truth. Models are sorted by overall accuracy."
        },
        {
            "title": "B ERROR CASES",
            "content": "B.1 ERROR CASE: POJ-1852 Error Type: Control Flow Misexecution Test Input: 1 4, [[2, 1, 2], [2, 2, 3], [1, 1, 3], [2, 3, 1], [1, 4, 4]] LLM Results: 1 output: 1 2 stats: {find_calls: 12, unions: 2} Code Results: 1 output: 1 2 stats: {find_calls: 13, unions: 2} Function: 1 def f(n, L): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 else: = list(range(n+1)) = [0]*(n+1) find_calls = 0 unions = 0 def g(x): nonlocal find_calls find_calls += 1 if A[x] != x: = A[x] = g(p) B[x] = (B[x] + B[p]) % 3 A[x] = return A[x] = 0 for d, x, in L: if < 1 or > or < 1 or > n: += 1 continue = 0 if == 1 else 1 rx = g(x) ry = g(y) if rx == ry: if (B[x] - B[y] - r) % 3 != 0: += 1 unions += 1 delta = (B[y] - B[x] + r) % 3 A[rx] = ry B[rx] = delta return m, {find_calls: find_calls, unions: unions}"
        },
        {
            "title": "Function Description",
            "content": "INPUTS: You start with single integer and list L. The list contains multiple items, and each item is triple of integers written as (d, x, y). Those are all the pieces youll need to begin. LOGICS: First, what youre going to do is build two lists of length + 1. Call the first one and fill it so that A[0] = 0, A[1] = 1, A[2] = 2, all the way up to A[n] = n. Call the second one and fill every slot from B[0] through B[n] with zero. Then make two counters and set both to zero: one called find calls and the other called unions. Next you define little helper routine, lets say its called g(x). Heres exactly how you run on some index x: 14 1. Right when you enter g, add 1 to find calls. 2. Check if A[x] is the same number as x. If it is, youve found root and you just return immediately. 3. If A[x] is not x, that means points up to parent. So write down = A[x]. Then call g(p) to chase up the chainwhich itself bumps find calls again and eventually returns root r. 4. After g(p) returns r, you adjust B[x]: take the old B[x], add B[p], then reduce that sum modulo 3 (so you do (B[x] + B[p]) mod 3) and store it back into B[x]. 5. Also update A[x] so that it now directly equals r. 6. Finally return r. All set up now? Good. Next you create counter and set it to zero. This will count how many triples get tossed or prove contradictory. Then you go through each triple in your list in order. For each triple do this: First look at and and see if either one is below 1 or above n. If < 1 or > or < 1 or > n, add 1 to and skip the rest of these steps for that triple. (Thats just bad-index check.) Otherwise, you figure out small value r: if equals 1 then = 0, else = 1. Then call g(x) and call g(y). Store the results as rx and ry. Those are the roots of and y, and by now find calls has grown by two. Now check: if rx and ry are the same number, that means and are already in the same group. In that case you compute (B[x] B[y] r) mod 3; if that result is not zero, it means this triple contradicts existing info, so add 1 to m. If its zero, do nothing extrajust move on to the next triple. If rx and ry are different, you need to merge the two groups. So do unions + = 1. Then compute δ = (B[y] B[x] + r) mod 3. After that, link the root rx under ry by setting A[rx] = ry, and store B[rx] = δ. That finishes merging for this triple. Then move on to the next triple until you exhaust the list. When youve processed every triple in L, you stop. The number reflects how many triples were either invalid or contradictory, find calls tells you how many times you ran the helper g, and unions tells you how many merges you performed. OUTPUTS: At the end you get two things. First you get the integer m, which is the count of all the bad or contradictory triples you hit. Second you get small record or dictionary with two entries: find calls is the total number of times you entered the helper g, and unions is how many times you merged two distinct roots."
        },
        {
            "title": "LLM Reasoning",
            "content": "Reasoning: Let me work through this algorithm step by step. Initialization: = 4, = [[2, 1, 2], [2, 2, 3], [1, 1, 3], [2, 3, 1], [1, 4, 4]] = [0, 1, 2, 3, 4] (indices 0 through 4) = [0, 0, 0, 0, 0] (all zeros) find calls = 0, unions = 0, = 0 Processing Triple 1: [2, 1, 2] = 2, = 1, = 2. Both in bounds [1, 4]. = 1 (since = 1) g(1): find calls = 1, A[1] = 1 so return 1. rx = 1 g(2): find calls = 2, A[2] = 2 so return 2. ry = 2 rx = ry, so merge: unions = 1 delta = (B[2] B[1] + 1) mod 3 = (0 0 + 1) mod 3 = 1 A[1] = 2, B[1] = 1 Processing Triple 2: [2, 2, 3] = 2, = 2, = 3. Both in bounds. = 1 g(2): find calls = 3, A[2] = 2 so return 2. rx = 2 g(3): find calls = 4, A[3] = 3 so return 3. ry = 3 15 rx = ry, so merge: unions = 2 delta = (B[3] B[2] + 1) mod 3 = (0 0 + 1) mod 3 = 1 A[2] = 3, B[2] = Processing Triple 3: [1, 1, 3] = 1, = 1, = 3. Both in bounds. = 0 g(1): find calls = 5, A[1] = 2 = 1, so = 2 g(2): find calls = 6, A[2] = 3 = 2, so = 3 * g(3): find calls = 7, A[3] = 3 so return 3 B[2] = (1 + 0) mod 3 = 1, A[2] = 3, return 3 B[1] = (1 + 1) mod 3 = 2, A[1] = 3, return rx = 3 g(3): find calls = 8, A[3] = 3 so return 3. ry = 3 rx = ry = 3, check consistency: (B[1] B[3] 0) mod 3 = (2 0 0) mod 3 = 2 = 0 Contradiction! = 1 Processing Triple 4: [2, 3, 1] = 2, = 3, = 1. Both in bounds. = 1 g(3): find calls = 9, A[3] = 3 so return 3. rx = 3 g(1): find calls = 10, A[1] = 3 so return 3. ry = 3 (Control Flow Misexecution (code line 9): The model should enter the if block because A[x]=x.) rx = ry = 3, check consistency: (B[3] B[1] 1) mod 3 = (0 2 1) mod 3 = (3) mod 3 = 0 Consistent, no increment to Processing Triple 5: [1, 4, 4] = 1, = 4, = 4. Both in bounds. = 0 g(4): find calls = 11, A[4] = 4 so return 4. rx = 4 g(4): find calls = 12, A[4] = 4 so return 4. ry = 4 rx = ry = 4, check consistency: (B[4] B[4] 0) mod 3 = (0 0 0) mod 3 = Consistent, no increment to Output: 1 Statistics: {find calls: 12, unions: 2} 16 B.2 ERROR CASE: CODEFORCES-328A Error Type: Instruction Misinterpretation Test Input: 1 4, [1, 0, 1, 0] LLM Results: 1 output: 3 2 stats: {rec_calls: 7, max_depth: 5} Code Results: 1 output: 3 2 stats: {rec_calls: 7, max_depth: 3} Function: c1 = 0 c2 = 0 = 0 for val in arr: += val 1 def f(n, arr): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 = [1 if val == 0 else -1 for val in arr] def g(l, r, depth): nonlocal c1, c2 c1 += 1 if depth > c2: c2 = depth if == r: = A[l] return v, v, v, = (l + r) // 2 lt, lp, ls, lb = g(l, m, depth + 1) rt, rp, rs, rb = g(m + 1, r, depth + 1) total = lt + rt pref = max(lp, lt + rp) suf = max(rs, rt + ls) best = max(lb, rb, ls + rp) return total, pref, suf, best _, _, _, = g(0, - 1, 1) return + b, {rec_calls: c1, max_depth: c2}"
        },
        {
            "title": "Function Description",
            "content": "INPUTS: The inputs are number called and list called arr containing numbers. LOGICS: Start by writing down three counters named c1, c2, and and set them all to zero. Now go through each value in arr one at time and add that value to x. When you finish, holds the sum of arr. Next, build new list called A: begin with it empty, then for every element in arr in order, if the element is zero append 1 into A, otherwise append 1. Now you are going to define process named that takes three numbers l, r, and depth. Every time you start g, do the following: first add 1 to c1, then compare depth with c2 and if depth is larger replace c2 with depth. Then check if equals r. If they are equal, look up at index l, call that value v, and return four copies of v. If is less than r, compute as the floor of (l + r) divided by 2. Now call on the left half by using l, m, and depth plus 1, and write down its four outputs as lt, lp, ls, and lb in that order. Then call on the right half by using + 1, r, and depth plus 1, and write down its four outputs as rt, rp, rs, and rb. After you have those eight numbers, calculate total as lt plus rt, then pref as whichever is larger between lp and lt plus rp, then suf as whichever is larger between rs and rt plus ls, and finally best as the largest of lb, rb, and ls plus rp. Then return total, pref, suf, and best from this call. Once you have defined that, kick off the process by calling with = 0, = minus 1, and depth = 1. When it finishes, ignore the first three values it gives you and call the fourth value b. At this point c1 is the total count of calls, and c2 is the deepest level you reached. Finally, add to and that gives you the main result. OUTPUTS: You will end up with two things: number equal to plus b, and small object with two fields, rec calls set to c1 (the total number of calls) and max depth set to c2 (the deepest recursion level you reached)."
        },
        {
            "title": "LLM Reasoning",
            "content": "Reasoning: Let me work through this algorithm step by step. Step 1: Initialize variables = 4, arr = [1, 0, 1, 0] c1 = 0, c2 = 0, = 0 Step 2: Calculate sum of arr Going through each value in arr and adding to x: = 0 + 1 = = 1 + 0 = 1 = 1 + 1 = 2 = 2 + 0 = 2 So = 2 Step 3: Build array For each element in arr: if 0 append 1, otherwise append 1: arr[0] = 1 append arr[1] = 0 append 1 arr[2] = 1 append 1 arr[3] = 0 append 1 So = [1, 1, 1, 1] Step 4: Execute g(0, 3, 1) g(0, 3, 1): c1 = 1, depth= 1 > c2 = 0 so c2 = 1 = r, so = (0 + 3)/2 = 1 Call g(0, 1, 2) and g(2, 3, 3) (Instruction Misinterpretation (code line 18): The second function call should be g(2, 3, 2).) g(0, 1, 2): c1 = 2, depth= 2 > c2 = 1 so c2 = 2 = r, so = (0 + 1)/2 = Call g(0, 0, 3) and g(1, 1, 4) g(0, 0, 3): c1 = 3, depth= 3 > c2 = 2 so c2 = 3 = = 0, so = A[0] = 1 Return (1, 1, 1, 1) g(1, 1, 4): c1 = 4, depth= 4 > c2 = 3 so c2 = 4 = = 1, so = A[1] = 1 Return (1, 1, 1, 1) Back to g(0, 1, 2): (cid:40) lt = 1, rt = 1, lp = 1, ls = 1, lb = 1; rp = 1, rs = 1, rb = 1 total = 1 + 1 = 0 pref = max(1, 1 + 1) = 0 suf = max(1, 1 + (1)) = 1 best = max(1, 1, 1 + 1) = 1 Return (0, 0, 1, 1) g(2, 3, 3): c1 = 5, depth= 3 c2 = 4 so c2 stays 4 = r, so = (2 + 3)/2 = 2 Call g(2, 2, 4) and g(3, 3, 5) g(2, 2, 4): c1 = 6, depth= 4 = c2 = 4 so c2 stays = = 2, so = A[2] = 1 Return (1, 1, 1, 1) g(3, 3, 5): c1 = 7, depth= 5 > c2 = 4 so c2 = 5 = = 3, so = A[3] = 1 Return (1, 1, 1, 1) Back to g(2, 3, 3): Return (0, 0, 1, 1) Back to g(0, 3, 1): (cid:40) lt = 1, rt = 1, lp = 1, ls = 1, lb = 1; rp = 1, rs = 1, rb = 1 total = 1 + 1 = pref = max(1, 1 + 1) = 0 suf = max(1, 1 + (1)) = 1 best = max(1, 1, 1 + 1) = 1 (cid:40) lt = 0, rt = 0, lp = 0, rp = 0, ls = 1, rs = 1, lb = 1; rb = 1 total = 0 + 0 = 0 pref = max(0, 0 + 0) = 0 suf = max(1, 0 + 1) = 1 best = max(1, 1, 1 + 0) = Return (0, 0, 1, 1) Step 5: Final calculations = 1 (fourth value from g) c1 = 7 (total calls) c2 = 5 (max depth) Final result = + = 2 + 1 = Output: 3 Statistics: {rec calls: 7, max depth: 5} 19 B.3 ERROR CASE: CODEFORCES-2018B Error Type: Misordered Execution Test Input: 1 8, [13, 27, 35, ...] (len=8) LLM Results: 1 output: 21 2 stats: {xor_operations: 7, and_operations: 7} Code Results: 1 output: 24 2 stats: {xor_operations: 8, and_operations: 7} Function: = - 1 val = 0 = = 0 = 0 while True: 1 def f(n, arr): 2 3 4 5 6 7 8 9 10 11 12 13 14 val ˆ= arr[x] += 1 if == 0: break = (x - 1) & += 1 return val, {xor_operations: y, and_operations: z}"
        },
        {
            "title": "Function Description",
            "content": "INPUTS: Youre working with two pieces of data: number called n, and list called arr that contains exactly numeric entries, indexed from 0 up to minus one. LOGICS: Start by creating variable called by taking and subtracting 1. Next, create running total named val and set it to zero. Now set up another variable and give it the same value as m. Also prepare two counters, and z, and initialize both of them at zero. What youre going to do now is enter loop that keeps going until becomes zero. Every time you go through this loop, follow these exact steps: 1. First, look up the item at index in your arr list and apply bitwise exclusive OR between that item and your current val. To do that by hand, write both numbers in binary, compare each bit pair, write down 1 if the bits differ or 0 if they match, then turn that resulting bit pattern back into number and call it your new val. 2. Once youve done that, add 1 to your counter to record that XOR operation. 3. Next, check whether is equal to zero. If is zero, break out of the loop immediatelyyoure done. 4. If is not zero, you need to update for the next round: subtract 1 from your current x, then take that result and apply bitwise AND with (again, line up the binary bits of (x-1) and m, write down 1 only where both bits are 1, then convert that back into number). Store that outcome back into x, and because you just performed an AND, add 1 to your counter z. 5. Then go back to the top of the loop and repeat exactly those stepsXOR and increment y, check for == 0 and maybe break, AND and increment zuntil finally reaches zero and you exit. OUTPUTS: When youre finished, youll have two things: the first is the final number stored in val after all the XOR steps, and the second is small dictionary showing {xor operations: z} which equals your counter (the total number of XORs you performed), and counter (the total number of ANDs you performed). y, and operations:"
        },
        {
            "title": "LLM Reasoning",
            "content": "Reasoning: Given the input value: = 8, and arr = [13, 27, 35, 49, 59, 61, 73, 83]. Step 1: Initialization Compute = 1 = 8 1 = 7 (binary: 0111) Initialize val = 0 = = (XOR counter) = 0 (AND counter) = 0 Step 2: Loop until == 0 Iteration 1: = 7 arr[7] = 83 XOR: val = 0 XOR Convert to binary: 0 (00000000) XOR 83 (01010011) = 83 (01010011) += 1 = 1 is not 0 proceed = (x 1) = (7 1 = 6) 7 = 6 (binary: 0110 AND 0111 = 0110) += 1 = 1 Iteration 2: = arr[6] = 73 XOR: val = 83 XOR 73 83: 01010011 73: 01001001 XOR: 00011010 decimal: 26 += 1 = 2 is not 0 proceed = (6 1) 7 = 5 7 = 5 (binary: 0101 AND 0111 = 0101) += 1 = 2 Iteration 3: = 5 arr[5] = 61 XOR: val = 26 XOR 61 26: 00011010 61: 00111101 XOR: 00100111 decimal: 39 += 1 = is not 0 proceed = (5 1) 7 = 4 7 = 4 (binary: 0100 AND 0111 = 0100) += 1 = 3 Iteration 4: = 4 arr[4] = 59 XOR: val = 39 XOR 39: 00100111 59: 00111011 XOR: 00011100 decimal: 28 += 1 = 4 is not 0 proceed 21 = (4 1) 7 = 3 7 = 3 (binary: 0011 AND 0111 = 0011) += 1 = Iteration 5: = 3 arr[3] = 49 XOR: val = 28 XOR 49 28: 00011100 49: 00110001 XOR: 00101101 decimal: 45 += 1 = 5 is not 0 proceed = (3 1) 7 = 2 7 = 2 (binary: 0010 AND 0111 = 0010) += 1 = 5 Iteration 6: = 2 arr[2] = 35 XOR: val = 45 XOR 35 45: 00101101 35: 00100011 XOR: 00001110 decimal: += 1 = 6 is not 0 proceed = (2 1) 7 = 1 7 = 1 (binary: 0001 AND 0111 = 0001) += 1 = 6 Iteration 7: = 1 arr[1] = XOR: val = 14 XOR 27 14: 00001110 27: 00011011 XOR: 00010101 decimal: 21 += 1 = 7 is not 0 proceed = (1 1) 7 = 0 7 = 0 (binary: 0000 AND 0111 = 0000) += 1 = Iteration 8: = 0 Check if == 0 yes, exit loop (Misordered Execution (code line 8 9 10): The model forgets updating val (line 8) and increasing (line 9).) Step 3: Compute final results: Final val = 21 xor operations (y) = and operations (z) = 7 Output: 1 { 2 3 4 5 } val: 21, and_operations: 7, xor_operations: Statistics: 1 {xor_operations: 7, and_operations: 7} 22 B.4 ERROR CASE: CODEFORCES-1146E Error Type: Missing Logic elements Test Input: 1 5, [1, 2, 3, 4, 5], [[1, 5], [2, 4], [3, 3]] LLM Results: 1 output: [0, 1, 1] 2 stats: {stack_pops: 4, bit_ops: 9} Code Results: 1 output: [5, 3, 1] 2 stats: {stack_pops: 4, bit_ops: 11} Function: L.pop() pop_cnt += 1 pop_cnt = 0 bit_ops = 0 = [0] * = [] for in range(n): 1 def f(n, arr, lst): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def qry(i): return nonlocal bit_ops bit_ops += 1 while <= n: C[i] += 1 += & -i nonlocal bit_ops bit_ops += 1 = 0 while > 0: += C[i] -= & -i = [0] * len(lst) = 0 for x, y, idx in B: _, pos = A2[j] upd(pos) += 1 while and arr[L[-1]] < arr[i]: A[i] = L[-1] + 1 if else 0 L.append(i) A2 = sorted((A[i], + 1) for in range(n)) = sorted((x, y, idx) for idx, (x, y) in enumerate(lst)) = [0] * (n + 1) def upd(i): while < and A2[j][0] < x: D[idx] = qry(y) - qry(x - 1) return D, {\"stack_pops\": pop_cnt, \"bit_ops\": bit_ops}"
        },
        {
            "title": "Function Description",
            "content": "INPUTS: is an integer, arr is list of numbers, lst is list of pairs (x, y) LOGICS: Start by preparing your workspace: set pop cnt to 0 and bit ops to 0. Then make list with zeros and an empty list that well use like stack. 23 Now go index by index through arr: for from 0 up to 1, do this: while is not empty and arr[L[1]] < arr[i], remove that last index from and add 1 to pop cnt. Once that popping stage is done, if still has something, look at that last index (call it top) and set A[i] to top + 1; if is empty, set A[i] to 0. Then append to L. That completes filling A. Next what you do is build list A2 of pairs (A[i], + 1) for each from 0 to 1, and then sort A2 in ascending order by the first number in each pair (and by the second if theres tie). Then take your input lst and turn it into list of triples: for each pair (x, y) in lst, remember its original position idx, and form triple (x, y, idx); once you have all of those, sort in ascending order by x, then by y, then by idx. Now set up an array of length + 1 filled with zeros. Well use two helper actions: upd(i) and qry(i). When you call upd(i), add 1 to bit ops, then as long as n, add 1 to C[i] and then increase by the value of its lowest set bit (compute i& and add it to i). When you call qry(i), add 1 to bit ops, start local sum at 0, and while > 0, add C[i] to and then decrease by its lowest set bit (i& i); when that finishes return s. After that, create list of the same length as lst and fill it with zeros, and also set pointer to 0. Finally, go through each triple (x, y, idx) in in order: while < and A2[j][0] < x, take the second value from A2[j] (call it pos), call upd(pos), and add 1 to j; once that loop ends, call qry(y) to get the total up to y, call qry(x - 1) to get the total up to 1, subtract the two results, and store that number into at position idx. Once youve done that for every triple in B, youre done with the processing. OUTPUTS: You end up with D, list of numbers the same length as lst, where each entry is the count computed for the corresponding pair in lst. You also have dictionary of statistics: stack pops is the total number of times you removed an index from during that first scanning phase, and bit ops is the total number of times you entered either the upd or qry action."
        },
        {
            "title": "LLM Reasoning",
            "content": "Reasoning: We are given function to execute manually with the input values = 5, arr = [1, 2, 3, 4, 5], and lst = [(1, 5), (2, 4), (3, 3)]. Heres my step-by-step execution: STEP 1: INITIALIZE VARIABLES pop cnt = 0 bit ops = 0 = [0, 0, 0, 0, 0] (length n) = [] (empty stack) will be declared later STEP 2: SCAN THE ARRAY TO FILL LIST We process arr[i] for = 0 to 4. Lets track changes in and compute values for A: = 0: arr[0] = 1, is empty no popping A[0] = Append 0 to L: = [0] 24 = 1: = 2: = 3: = 4: arr[1] = 2, L[-1] = arr[0] = 1 arr[1] > arr[0], so pop 0 from L, = [] pop cnt + = 1 pop cnt = 1 is empty A[1] = 0 Append 1 to L: = [1] arr[2] = 3, L[-1] = arr[1] = 2 arr[2] > arr[1], so pop 1 from L, = [] pop cnt + = 1 pop cnt = is empty A[2] = 0 Append 2 to L: = [2] arr[3] = 4, L[-1] = arr[2] = 3 arr[3] > arr[2], so pop 2 from L, = [] pop cnt + = 1 pop cnt = 3 is empty A[3] = 0 Append 3 to L: = [3] arr[4] = 5, L[-1] = arr[3] = 4 arr[4] > arr[3], so pop 3 from L, = [] pop cnt + = 1 pop cnt = 4 is empty A[4] = 0 Append 4 to L: = [4] Now = [0, 0, 0, 0, 0], and = [4], and pop cnt = 4 STEP 3: BUILD LIST A2 A2 is list of pairs: (A[i], + 1), one for each i: A0 = 0 (0, 1) A1 = 0 (0, 2) A2 = 0 (0, 3) A3 = 0 (0, 4) A4 = 0 (0, 5) Sort A2 by first (A[i]) and tie-breaker (i + 1). Since all A[i] = 0, sort by + 1: A2 sorted = [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)] STEP 4: BUILD LIST Original lst = [[1, 5], [2, 4], [3, 3]] Assign index to each element: idx=0 (1, 5) idx=1 (2, 4) idx=2 (3, 3) Sort by x, then y, then idx: = [(1, 5, 0), (2, 4, 1), (3, 3, 2)] (1, 5, 0), (2, 4, 1), (3, 3, 2) 25 STEP 5: SETUP (BIT ARRAY) WITH = 5 Initialize = [0, 0, 0, 0, 0, 0] (1-based index: index 0 is unused) STEP 6: INITIALIZE AND = [0, 0, 0] (same length as B) = 0 Now process each element in in sorted order. Processing B[0] = (1, 5, 0) Phase 1: Process A2 while < and A2[j][0] < = 1, compare A2[j=0].0 = 0 < 1 break No movement of no calls to upd() (Missing Logic Elements (code line 33 34 35): The model omits the entire loop.) Phase 2: Query = 5, 1 = Query qry(5), then qry(0). The result is qry(5) qry(0) BIT: query(5) = 5 C[i] = 0, = 5 4 = 1 C[1] = 0 = 0 stop qry(5) = 0 bit ops+ = BIT: query(0) returns 0 (base case), bit ops+ = 1 qry(5) qry(0) = 0 0 = 0 D[0] = 0, bit ops = 2 Processing B[1] = (2, 4, 1) Phase 1: Process A2 while < and A2[j][0] < = 2 A2[j = 0].0 = 0 < 2 process A2[0] = (0, 1) Call upd(1): = 1 Low bit of 1 = 1 C[1]+ = 1 = [0, 1, 0, 0, 0, 0] = 1 + 1 = 2, low bit of 2 = 2 C[2]+ = 1 = 2 + 2 = 4, low bit of 4 = 4 C[4]+ = 1 = 4 + 4 = 8 exit loop bit ops + = 1 = 1 A2[j=1].0 = 0 < 2 process (0, 2), call upd(2): = [0, 1, 1, 0, 1, 0] = 2 +2 = 4 +4 = 8 done Count incremented at positions 2, 4 bit ops + = 1 = A2[j=2].0 = 0 < 2 process (0, 3), call upd(3): = [0, 1, 1, 1, 1, 0] = 3 +3 +4 = 7 stop at = 7 bit ops + = 1 = 3 26 A2[j=3].0 = 0 < 2 process (0,4), call upd(4): = [0, 1, 1, 1, 2, 0] = 4 +4 = 8 bit ops + = = 4 A2[j=4].0 = 0 < 2 process (0,5), call upd(5): = [0, 1, 1, 1, 2, 1] = 5 +1 = 6 0 end bit = 6 bit ops + = 1 = 5 end loop Phase 2: Query = 4, 1 = BIT: query(4) = 4 C[4] = 2 = 0 stop? No = 4 4 = 0 Total: 2 bit ops+ = 1 BIT: query(1) = 1 C[1] = = 1 1 = 0 done Total = 1 bit ops+ = 1 result = 2 1 = 1 D[1] = 1, bit ops = 7 Phase 1: Process A2 while < and A2[j][0] < = 3 Processing B[2] = (3, 3, 2) Currently = 5 (no more to process) Phase 2: Query = 3, 1 = 2 BIT: query(3) = 3 C[3] = 1 = 2 = 2 C[2] = 1 = 0 done Total = 1 + 1 = 2 bit ops+ = 1 BIT: query(2) = 2 C[2] = 1 = 0 done Total = 1 bit ops+ = 1 result = 2 1 = 1 D[2] = 1, bit ops = 9 FINAL AFTER ALL QUERIES: = [0, 1, 1] Statistics: pop cnt = 4 (from initial stack operation) bit ops = 9 (tracked step-by-step) Output: Final result: [0, 1, 1] Statistics: 1 {\"stack_pops\": 4, \"bit_ops\": 9} Answer: Reasoning: Step-by-step executed with all variables, logs, stack operations, and function calls to BIT including detailed motion and count of updates and queries. Output: [0, 1, 1] Statistics: {\"stack pops\": 4, \"bit ops\": 9} 28 B.5 ERROR CASE: CODEFORCES-1481D Error Type: State Tracking Error Test Input: 1 6 LLM Results: 1 output: [3, 4, 1, 5, 2, 6] 2 stats: {heap_pushes: 6, max_heap_size: 3} Code Results: 1 output: [3, 4, 1, 5, 2, 6] 2 stats: {heap_pushes: 6, max_heap_size: 2} Function: 1 def f(n): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import heapq = [0] * heap = [] heapq.heappush(heap, (-n, 0, - 1)) heap_pushes = 1 max_heap_size = len(heap) val = 1 while heap: neg_len, x, = heapq.heappop(heap) if len(heap) > max_heap_size: max_heap_size = len(heap) = (x + y) // 2 A[z] = val val += 1 if <= - 1: slen = (z - 1) - + 1 heapq.heappush(heap, (-slen, x, - 1)) heap_pushes += 1 if + 1 <= y: slen = - (z + 1) + 1 heapq.heappush(heap, (-slen, + 1, y)) heap_pushes += 1 return A, {\"heap_pushes\": heap_pushes, \"max_heap_size\": max_heap_size}"
        },
        {
            "title": "Function Description",
            "content": "INPUTS: One input: an integer n. From that youre going to make list called of length filled with zeros. Youll also use list called heap to store triples of numbers, and keep three counters: heap pushes, max heap size, and val. LOGICS: Start by creating list called with zeros in it, and an empty list called heap. Then push the triple (n, 0, 1) onto heap, set heap pushes to 1, set max heap size to 1, and set counter val to 1. Now youre going to repeat these steps as long as heap is not empty. First, remove from heap the triple that has the smallest first number; if theres tie, pick the one with the smallest second number. Call the three parts neg len, x, and y. Right after you take it out, check the current size of heap; if its bigger than max heap size, update max heap size to that size. Next, figure out by adding and y, dividing by 2 with integer division (discard any remainder). Then put val into at index z, and add 1 to val. What you do next is see if left piece remains: check if is less than or equal to 1, and if so compute slen as (z 1) + 1, push the triple (slen, x, 1) onto heap, and add 1 to heap pushes. Then check if right piece remains: see if + 1 is less than or equal to y, and if it does compute slen as (z + 1) + 1, push the triple (slen, + 1, y) onto heap, and add 1 to heap pushes. After you do both, go back to the start of these steps. When heap is empty, the loop ends. OUTPUTS: At the end, youll have your list filled with numbers from 1 up to in certain order, and two counters: heap pushes, which records how many times you pushed triple onto heap, and max heap size, which tells you the largest number of triples that ever sat in heap at once."
        },
        {
            "title": "LLM Reasoning",
            "content": "Reasoning: Given input: = 6 Step 1: Initialization Create = [0, 0, 0, 0, 0, 0] heap = [] (empty) Push (-6, 0, 5) onto heap (heap now: [(-6, 0, 5)]) heap pushes = 1 max heap size = 1 val = 1 Step 2: Main loop (while heap not empty) Iteration 1 heap: [(-6, 0, 5)] Pop (-6, 0, 5). (heap is now empty) After pop, heap size: 0 max heap size stays 1 neg len = -6, = 0, = 5 = (0 + 5) // 2 = 2 Set A[2] = 1 = [0, 0, 1, 0, 0, 0] val = Left piece: x=0, z-1=1 so 0 1 slen = (1) - 0 + 1 = 2 Push (-2, 0, 1) onto heap (heap pushes=2) heap: [(-2, 0, 1)] Right piece: z+1=3, y=5 so 3 5 slen = 5-(3)+1 = 3 Push (-3, 3, 5) onto heap (heap pushes=3) heap: [(-3, 3, 5), (-2, 0, 1)] max heap size now 2 Iteration heap: [(-3, 3, 5), (-2, 0, 1)] Pop (-3, 3, 5) After pop, heap size: 1 max heap size stays 2 neg len=-3, x=3, y=5 = (3+5)//2 = 4 Set A[4] = 2 = [0, 0, 1, 0, 2, 0] val = 3 Left piece: x=3, z-1=3, so 3 3 slen = (3)-(3)+1=1 Push (-1, 3, 3) (heap pushes=4) heap: [(-2, 0, 1), (-1, 3, 3)] 30 Right piece: z+1=5, y=5 so 5 5 slen = 5-(5)+1=1 Push (-1, 5, 5) (heap pushes=5) heap: [(-2, 0, 1), (-1, 3, 3), (-1, 5, 5)] max heap size now 3 (State Tracking Error: The model should not update max heap size here.) Iteration 3 heap: [(-2, 0, 1), (-1, 3, 3), (-1, 5, 5)] Pop (-2, 0, 1) After pop, heap size: 2 max heap size stays 3 neg len=-2, x=0, y= = (0+1)//2 = 0 Set A[0] = 3 [3, 0, 1, 0, 2, 0] val = 4 Left piece: x=0, z-1=-1 0 1 is False, no left piece Right piece: z+1=1, y=1 1 1 is True slen = 1-1+1=1 Push (-1, 1, 1) (heap pushes=6) heap: [(-1, 1, 1), (-1, 3, 3), (-1, 5, 5)] max heap size stays 3 Iteration 4 heap: [(-1, 1, 1), (-1, 3, 3), (-1, 5, 5)] Pop (-1, 1, 1) After pop, heap size: 2 neg len=-1, x=1, y= = (1+1)//2 = 1 Set A[1] = 4 [3, 4, 1, 0, 2, 0] val = 5 Left piece: 1 0? False Right piece: 2 1? False No pushes Iteration 5 heap: [(-1, 3, 3), (-1, 5, 5)] Pop (-1, 3, 3) After pop, heap size: 1 neg len=-1, x=3, y=3 = (3+3)//2 = Set A[3] = 5 [3, 4, 1, 5, 2, 0] val = 6 Left piece: 3 2? False Right piece: 4 3? False No pushes Iteration 6 heap: [(-1, 5, 5)] Pop (-1, 5, 5) After pop, heap size: 0 neg len=-1, x=5, y=5 = (5+5)//2 = Set A[5] = 6 [3, 4, 1, 5, 2, 6] val = 7 Left piece: 5 4? False Right piece: 6 5? False No pushes HEAP IS NOW EMPTY Final Results: A: [3, 4, 1, 5, 2, 6] heap pushes: 6 max heap size: 3 Output: [3, 4, 1, 5, 2, 6] Statistics: {heap pushes: 6, max heap size: 3}"
        },
        {
            "title": "C PROMPTS",
            "content": "You are an expert Python programmer. want to enhance function by adding meaningful, NONREDUNDANT execution statistics that capture the most crucial aspects of its computational behavior. **CRITICAL REQUIREMENTS:** 1. **Adaptive number of statistics (1-3)** - Choose the appropriate number based on function complexity 2. **No redundant statistics** - Each must measure something completely different 3. **Focus on crucial logic** - Statistics should reflect key computational steps that matter 4. **Generic variable names** - Convert all meaningful variable names to generic ones **Variable Naming Guidelines:** - Lists/arrays: Use \"L\", \"arr\", \"lst\", \"A\", \"B\" etc. - Matrices/2D arrays: Use \"M\", \"matrix\", \"grid\" etc. - Strings: Use \"s\", \"text\", \"str1\", \"str2\" etc. - Integers/numbers: Use \"n\", \"m\", \"x\", \"y\", \"val\" etc. - Dictionaries/maps: Use \"d\", \"mp\", \"cache\" etc. - Sets: Use \"st\", \"visited\", \"seen\" etc. - Keep parameter names generic but clear about data types **Guidelines for choosing the number of statistics:** - **1 statistic**: Simple functions with one main operation (e.g., single loop, basic calculation ) - **2 statistics**: Functions with two distinct computational aspects (e.g., nested structure with two key operations) - **3 statistics**: Complex functions with multiple distinct computational phases or operations **What makes good statistic:** - Counts operations that directly affect the algorithms behavior - Measures key computational steps that vary with different inputs - Reflects important decision points or iterations in the logic - Avoids counting trivial operations (simple assignments, basic comparisons) Original Function: python {function} **Requirements:** 1. Convert all meaningful variable names to generic ones (following naming guidelines above) 2. Change the function name to (regardless of original name) 3. Remove ALL comments from the function code 4. Return tuple: (original_output, statistics_dict) 5. Choose 1-3 statistics based on the functions complexity and distinct computational aspects 6. Each statistic measures unique computational aspect 7. Focus on operations that directly affect the functions core logic 8. Preserve the exact same logic and functionality, only change variable names and remove comments Return your response in JSON format: {{ \"function\": \"complete modified function code here with generic variable names, function name f, and no comments\", \"stats_keys\": [\"list\", \"of\", \"1\", \"to\", \"3\", \"unique\", \"keys\"] }} Figure 6: Prompt for Adding State Trackers and Anonymizing Functions 33 You are helping someone understand how to manually process data step-by-step. Ill give you function, and need you to explain it like youre talking to someone who needs to do this work by hand - as if youre giving them verbal instructions over the phone. Your explanation should be so clear and detailed that they can follow along exactly and get the same results . **CRITICAL REQUIREMENT**: Your instructions must be so complete and precise that someone could follow them step-by-step WITHOUT seeing any code and produce the exact same output and statistics as the function would. **IMPORTANT CONSTRAINTS:** - Dont explain what this is used for in the real world - just focus on the data processing steps - Dont mention specific problem names or applications - Treat this as pure data manipulation work - Use the generic variable names from the code (L, arr, n, m, etc.) **Natural Language Guidelines:** - Write like youre speaking to someone conversationally - Use natural transitions like \"Now you need to...\", \"Next, go through...\", \"At this point...\" - Include phrases like \"What youre going to do is...\", \"The way this works is...\" - Make it sound like verbal instructions, not formal manual - Still be extremely precise about every detail, but use conversational language - Use connecting words and phrases that make it flow naturally - Include every conditional check, loop, and decision point in natural speech - Be specific about indexing, bounds, and conditions, but explain them conversationally Function to describe: python {function} Return your response in the following JSON format with exactly three sections: {{ \"inputs\": \"Describe the data types and structure of what youre working with (like list of numbers or two text strings). Dont mention what these represent in real-world terms.\", \"logics\": \"Give detailed, conversational instructions for processing the data step-by-step. Use natural language like youre talking someone through it, with phrases like Now you need to..., Next, go through..., What you do is.... Be extremely precise about every step, condition, and operation, but explain it in flowing, conversational way. Include all loops , decisions, calculations, and counter updates. Use the generic variable names from the code .\", \"outputs\": \"Explain what youll end up with and what each number in your statistics dictionary represents from the work you did.\" }} Figure 7: Prompt for Generating Natural Language Description 34 You are helping someone check if set of conversational instructions are complete enough for manual data processing. need you to verify whether these step-by-step instructions would allow someone to manually work through the data and get the same results as the code - like youre checking if verbal instructions over the phone would be complete enough for someone to follow along exactly. **CRITICAL REQUIREMENT**: The instructions must be so complete and precise that someone could follow them step-by-step WITHOUT seeing any code and produce the exact same output and statistics as the function would. **VERIFICATION APPROACH:** Think about this like youre helping someone understand whether these conversational instructions are good enough for manual data processing. The instructions should be so clear and complete that someone could: 1. **Follow Every Step**: All the data processing steps are explained like youre talking someone through it 2. **Handle All Cases**: Every condition, loop, and decision point is covered in natural language 3. **Track Everything**: All variable updates, counters, and calculations are explained conversationally 4. **Get Same Results**: Following the instructions would produce identical output and statistics 5. **No Guessing**: Every significant operation is covered so no one has to guess what to do 6. **Natural Flow**: The instructions flow naturally like someone talking through the process **FUNCTION CODE:** python {function_code} **CONVERSATIONAL INSTRUCTIONS TO CHECK:** {description} **COMPLETENESS JUDGMENT:** - **COMPLETE**: The conversational instructions cover everything needed - someone could follow them and get identical results - **INCOMPLETE**: Some operations, conditions, or steps are missing or unclear - following the instructions wouldnt match the codes behavior Return your response in JSON format: {{ \"desc_is_complete\": true/false, \"reasoning\": \"Talk through your assessment conversationally, like youre explaining to someone whats working well in these instructions and what might be missing. Use natural language like What notice is..., The instructions do good job of..., Whats missing is..., Someone following these would probably get confused when...\", \"missing_aspects\": [\"List specific operations or steps that arent covered conversationally - describe them in natural language like explaining how to update the counter, walking through the loop condition, describing what to do when the list is empty\"], \"coverage_percentage\": \"estimated percentage (0-100) of code operations covered by the conversational instructions\" }} Figure 8: Prompt for Verifing Natural Language Description 35 You are an expert Python programmer. want you to evolve an existing function to make it MORE LOGICALLY COMPLICATED while maintaining the exact same input signature and core functionality. The evolved function should be significantly more sophisticated in its logic and computational approach. **CRITICAL REQUIREMENTS:** 1. **Same Input Signature**: The function must accept exactly the same parameters as the original 2. **Same Core Output**: The main result should be equivalent to the original functions output 3. **More Complex Logic**: Add sophisticated algorithmic patterns, advanced data structures, or multi-phase processing 4. **Enhanced Statistics**: Statistics can change to reflect the new complexity (1-3 meaningful stats) 5. **Preserve Function Name**: Keep the function name as 6. **Return Format**: Must return tuple (result, stats_dict) 7. **ABSOLUTELY NO COMMENTS**: Do NOT write any comments, docstrings, or explanations in the evolved function code. The function must be completely comment-free. **EVOLUTION STRATEGIES (choose the most appropriate):** - **Multi-phase processing**: Break the problem into sophisticated stages - **Advanced data structures**: Use heaps, trees, graphs, or complex mappings - **Optimized algorithms**: Replace naive approaches with efficient algorithms - **Dynamic programming**: Add memoization or tabulation for overlapping subproblems - **Divide and conquer**: Split problem into smaller, more complex subproblems - **State machines**: Add complex state tracking and transitions - **Mathematical optimization**: Add advanced mathematical techniques - **Sophisticated filtering/sorting**: Use multiple criteria or advanced comparison logic **STATISTICS GUIDELINES:** - Choose 1-3 statistics that reflect the NEW complexity - Track operations that highlight the sophisticated logic - Examples: phases_completed, recursive_calls, cache_hits, comparisons, transformations, iterations **ORIGINAL FUNCTION:** python {original_function} **REQUIREMENTS:** 1. Analyze the original functions core purpose and constraints 2. Design more sophisticated approach that achieves the same goal 3. Implement complex logic patterns while preserving correctness 4. Add meaningful statistics that capture the new complexity 5. Ensure the evolved function is significantly more algorithmically interesting 6. Test edge cases and maintain robustness 7. **ABSOLUTELY NO COMMENTS**: The evolved function code must be completely comment-free **OUTPUT FORMAT:** json {{ return result, {{stat1: stat1, stat2: stat2}}\", \"evolved_function\": \"def f(...):n \"stats_keys\": [\"stat1\", \"stat2\"], \"evolution_description\": \"Brief explanation of how the function was made more complex (e.g., Added multi-phase processing with dynamic programming, Implemented graph-based approach with state tracking)\" complex_logic_here stat2 = 0n stat1 = 0n }} Figure 9: Prompt for Function Evolution"
        },
        {
            "title": "D HUMAN VERIFICATION",
            "content": "To ensure the quality of the generated instructions, we conducted human verification study on 136 randomly sampled instances from LogicIFEval. The data were split evenly into four parts, each containing 34 instructions, and each part was assigned to two independent PhD-level annotators with expertise in computer science. Annotators were instructed to verify whether every line of the anonymized function was accurately and completely captured in the corresponding natural language instruction. For Batches 1 and 2, both annotators fully agreed on all items, yielding 100% agreement. In Batch 3, both annotators marked 32 out of 34 items as correct, with one identical negative case, resulting in an agreement rate of 33/34 (97.06%). In Batch 4, the two annotators agreed on 32 out of 34 items (94.12%). Overall, the human agreement rate across all 136 items was 133 136 97.79%, demonstrating high level of consistency and validating the reliability of the generated instructions."
        },
        {
            "title": "E CASES OF THINKING HELPS LARGE LLMS",
            "content": "E.1 CASE 1: CODEFORCES-818D Test Input: 1 6, [3, 1, 4, 1, 5, 9] (len=6) LLM Results: 1 output: 2 2 stats: {left_pops: 5, right_pops: 2, balanced_subarrays: 19} Code Results: 1 output: 2 2 stats: {left_pops: 5, right_pops: 2, balanced_subarrays: 19} Function: 1 def f(n, A): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 prev = [-1] * nxt = [n] * st = [] l_pops = 0 for in range(n): st.pop() l_pops += 1 while st and A[st[-1]] <= A[i]: prev[i] = st[-1] if st else -1 st.append(i) st.clear() r_pops = 0 for in range(n-1, -1, -1): while st and A[st[-1]] < A[i]: st.pop() r_pops += 1 nxt[i] = st[-1] if st else st.append(i) total = * (n + 1) // 2 balanced = 0 for in range(n): = - prev[i] = nxt[i] - balanced += + - return total - balanced, {left_pops: l_pops, right_pops: r_pops, balanced_subarrays: balanced}"
        },
        {
            "title": "LLM Thinking Process",
            "content": "Okay, need to execute this algorithm step-by-step manually. Let me break down whats happening: INPUT: = 6, = [3, 1, 4, 1, 5, 9] Step 1: Initialize data structures prev = [1, 1, 1, 1, 1, 1] (n slots, all 1) nxt = [6, 6, 6, 6, 6, 6] (n slots, all n) st = [] (empty stack) pops = 0 Step 2: Forward sweep (i from 0 to n-1) need to go through each from 0 to 5: = 0: A[0] = 3 st is empty, so no popping st is empty, so prev[0] = 1 (already is 1) push 0 onto st: st = [0] = 1: A[1] = st = [0], top is 0, A[0] = 3 A[0] = 3 > A[1] = 1, so no popping (condition is A[top] A[i]) st is not empty, top is 0, so prev[1] = 0 push 1 onto st: st = [0, 1] = 2: A[2] = st = [0, 1], top is 1, A[1] = 1 A[1] = 1 A[2] = 4, so pop 1, pops = 1, st = [0] st = [0], top is 0, A[0] = 3 A[0] = 3 A[2] = 4, so pop 0, pops = 2, st = [] st is empty, so prev[2] = 1 push 2 onto st: st = [2] = 3: A[3] = 1 st = [2], top is 2, A[2] = 4 A[2] = 4 > A[3] = 1, so no popping st is not empty, top is 2, so prev[3] = 2 push 3 onto st: st = [2, 3] = 4: A[4] = 5 st = [2, 3], top is 3, A[3] = 1 A[3] = 1 A[4] = 5, so pop 3, pops = 3, st = [2] st = [2], top is 2, A[2] = 4 A[2] = 4 A[4] = 5, so pop 2, pops = 4, st = [] st is empty, so prev[4] = 1 push 4 onto st: st = [4] = 5: A[5] = 9 st = [4], top is 4, A[4] = 5 A[4] = 5 A[5] = 9, so pop 4, pops = 5, st = [] st is empty, so prev[5] = 1 push 5 onto st: st = [5] After forward sweep: prev = [1, 0, 1, 2, 1, 1] pops = st = [] pops = 0 Stack cleared, preparing for backward sweep Backward sweep begins, starting from 1: = 5: A[5] = 9 Empty stack means no popping nxt[5] set to 6 (array end) 5 pushed onto stack: st = [5] = 4: A[4] = 5 Stack top is 5, A[5] = 9 No popping since A[top] < A[i] nxt[4] points to 5 4 added to stack: st = [5, 4] = 3: = 2: = 1: = 0: A[3] = 1 Stack top is 4, A[4] = 5 No popping occurs nxt[3] set to 4 3 pushed onto stack: st = [5, 4, 3] A[2] = Stack top is 3, A[3] = 1 Pop 3, pops increments, stack becomes [5, 4] Top now 4, A[4] = 5 No further popping nxt[2] points to 4 2 added to stack: st = [5, 4, 2] A[1] = 1 Stack top is 2, A[2] = 4 No popping nxt[1] set to 2 1 pushed onto stack: st = [5, 4, 2, 1] A[0] = Stack top is 1, A[1] = 1 Pop 1, pops increments, stack becomes [5, 4, 2] 39 Top now 2, A[2] = 4 No more popping nxt[0] points to 0 added to stack: st = [5, 4, 2, 0] Backward sweep calculates nxt = [2, 2, 4, 4, 5, 6] with 2 right pops. Total calculation yields 21. Balanced subarray computation follows, tracking incremental changes through each index. Final balanced value reaches 19. Result emerges as 2, with detailed statistics tracking left/right pops and balanced subarrays. 40 E.2 CASE 2: POJ-1516 Test Input: 1 [[0, 1], [1, 2], [2, 3], [3, 4]] LLM Results: 1 output: 4 2 stats: {update_calls: 20, pruned_calls: 0, full_cover_events: 4} Code Results: 1 output: 4 2 stats: {update_calls: 20, pruned_calls: 0, full_cover_events: 4} Function: 1 def f(A): 2 3 4 5 update_calls = 0 pruned_calls = 0 full_cover_events = 0 if not A: return 0, {\"update_calls\": update_calls, \"pruned_calls\": pruned_calls, \"full_cover_events\": 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 full_cover_events} = sorted({u for in for in v}) = {v: for i, in enumerate(B)} = len(B) - 1 if <= 0: return 0, {\"update_calls\": update_calls, \"pruned_calls\": pruned_calls, \"full_cover_events\": full_cover_events} arr1 = [False] * (4 * n) arr2 = [0] * (4 * n) def update(i, x, y, l, r): nonlocal update_calls, pruned_calls, full_cover_events update_calls += 1 if > or < x: return 0 if arr1[i]: pruned_calls += 1 return 0 if <= and <= r: full_cover_events += 1 length = B[y+1] - B[x] arr2[i] = length arr1[i] = True return length mid = (x + y) // 2 new_cov = update(i * 2, x, mid, l, r) + update(i * 2 + 1, mid + 1, y, l, r) arr2[i] = arr2[i * 2] + arr2[i * 2 + 1] if arr2[i] == B[y+1] - B[x]: arr1[i] = True return new_cov total = 0 for a, in A: if >= b: continue li = d[a] ri = d[b] - 1 if li <= ri: total += update(1, 0, - 1, li, ri) return total, {\"update_calls\": update_calls, \"pruned_calls\": pruned_calls, \"full_cover_events\": full_cover_events}"
        },
        {
            "title": "LLM Thinking Process",
            "content": "Let me carefully work through this algorithm step by step. Input: = [[0, 1], [1, 2], [2, 3], [3, 4]] Step 1: Initialize counters update calls = 0 pruned calls = 0 full cover events = Step 2: Check if is empty is not empty, so we continue. Step 3: Build set First, need to collect all numbers from pairs in A: From [0, 1]: 0, 1 From [1, 2]: 1, 2 From [2, 3]: 2, 3 From [3, 4]: 3, Set of all numbers: {0, 1, 2, 3, 4} Sorted list = [0, 1, 2, 3, 4] Step 4: Create mapping Walking through with index counter starting at 0: d[0] = 0 d[1] = 1 d[2] = 2 d[3] = d[4] = 4 So = {0 : 0, 1 : 1, 2 : 2, 3 : 3, 4 : 4} Step 5: Calculate = len(B) 1 = 5 1 = 4 Step 6: Check if is zero or negative = 4, which is positive, so we continue. Step 7: Prepare arrays arr1 of size 4 = 4 4 = 16, filled with false arr2 of size 4 = 4 4 = 16, filled with zeros Arrays represent grid of boolean and integer values, initialized to default states. Step 8: Main processing loop Ill track the total and process each pair systematically. The first pair [0, 1] requires careful examination of its components and their relationship. Checking the pairs conditions reveals sequence of logical checks. The update function gets called with specific parameters, incrementing tracking variables and performing boundary validations. The process involves examining array states and determining whether specific update conditions are met. The update method carefully checks segment boundaries and array flags, ensuring precise manipulation of the underlying data structure. Each iteration refines the computational state through targeted, incremental modifications. Recursive calls split the range systematically, progressively narrowing the search space and applying updates across different segments of the computational domain. 42 In the first recursive call to update(2, 0, 1, 0, 0), the algorithm checks range boundaries and recursively explores child nodes. When reaching update(4, 0, 0, 0, 0), complete coverage is detected, incrementing full coverage events and marking the corresponding array indices. The subsequent call to update(5, 1, 1, 0, 0) returns zero since the range falls outside the target interval. Back in the parent call, the coverage is calculated by aggregating child node results, though the computed coverage does not match the expected span length. Moving to the next pair [1, 2], the algorithm determines the left and right indices based on the dictionary values. It then initiates another update call with modified parameters, checking range overlaps and continuing the recursive exploration. The update process systematically tracks coverage, incrementing call counters and evaluating range intersections to determine potential full coverage scenarios. Splitting the range at the midpoint, the method recursively calls update on subdivided segments, carefully checking boundary conditions and maintaining state across different recursive levels. The process involves precise range comparisons, splitting intervals, and tracking potential coverage through strategic recursive calls with adjusted indices. At each step, the algorithm incrementally updates tracking variables like update calls and full cover events, methodically exploring the segment trees structure to determine event coverage and interval relationships. For the current update call, the process calculates new coverage by adding 1 to the existing coverage. The arr2 array is updated by summing previous segment values, and when the total matches the expected interval length, arr1 is marked as true. The subsequent update call checks interval boundaries, determining whether the current segment falls outside the target range and returning 0 if no overlap exists. This systematic approach ensures precise tracking of event intervals and their interactions. In the next update iteration, the method continues evaluating interval relationships, checking whether segments intersect or contain each other. The process involves incrementing tracking variables and performing detailed comparisons between different array segments. The algorithm methodically explores potential interval connections, using conditional checks to determine valid segment transformations and maintaining running total of processed intervals. When encountering fully contained interval at coordinates [2, 2], the method increments the full coverage event counter and calculates the span length by comparing boundary values. This precise segmentation allows for granular tracking of interval characteristics. The recursive calls progressively break down the problem space, systematically examining each potential interval configuration with careful boundary assessments and strategic splitting of ranges. At coordinate 6, the method marks the interval as covered and returns coverage value of 1, signaling successful interval identification within the specified constraints. The subsequent update process aggregates coverage information, comparing calculated values against predefined boundary markers to determine interval completeness and update tracking arrays accordingly. For the current pair [3, 4], the method evaluates interval relationships, checking relative positions and determining whether further recursive updates are necessary. The process involves comparing interval boundaries, accessing predefined mapping data, and conditionally triggering additional computational steps to track coverage progression. The recursive update mechanism continues to systematically explore and validate interval coverage, incrementing tracking variables and maintaining precise boundary tracking through strategic computational checks. During the nested update calls, the algorithm methodically splits intervals, examines their relationships, and updates coverage metrics when specific conditions are met. Each recursive call carefully assesses interval positioning, selectively updating event tracking and coverage information. The final update operation identifies complete interval coverage scenario, incrementing full coverage events and recording specific interval characteristics with precise computational precision. The process concludes by returning comprehensive result tuple capturing total coverage, update call statistics, and full coverage event count, reflecting the systematic interval processing strategy."
        }
    ],
    "affiliations": [
        "University of Texas at Dallas",
        "Zoom Video Communications"
    ]
}