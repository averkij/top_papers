{
    "paper_title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet",
    "authors": [
        "James Xu Zhao",
        "Bryan Hooi",
        "See-Kiong Ng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge"
        },
        {
            "title": "Start",
            "content": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet James Xu Zhao Bryan Hooi National University of Singapore See-Kiong Ng 5 2 0 2 ] . [ 1 1 6 8 6 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge."
        },
        {
            "title": "Introduction",
            "content": "Recent reasoning models, such as such as GPT-5 [20], Gemini 2.5 [9] and Qwen3 [28], have demonstrated impressive performance on many challenging tasks, like competition-level math problems [6, 12, 25, 26, 29, 37]. key technique behind these improvements is test-time scaling, where models generate long chain-of-thought (CoT) reasoning traces before producing an answer [19]. Despite these advances, frontier models still suffer from hallucinations, responses that contradict world knowledge [2, 11, 30]. This remains fundamental challenge, especially in knowledgeintensive tasks that require models to ensure factual accuracy and minimize hallucinations [14, 32]. Given that test-time scaling has shown promise across many domains, natural question arises: Is test-time scaling effective for knowledge-intensive tasks? To answer this question, we conduct comprehensive study of test-time scaling on two knowledgeintensive tasks. We evaluate 12 reasoning models by increasing their test-time computation (Section 3). Our findings, summarized in Table 1, challenge the common assumption that thinking more leads to better performance. Across models and tasks, increasing thinking time does not consistently improve accuracy, with Gemini 2.5 Flash being the only exception. Moreover, thinking more does not reduce hallucinations for most models and even increases them, such as in GPT-5 mini and Gemini 2.5 Flash. To understand hallucination changes with increased test-time computation, we analyze how model behavior shifts across different thinking levels (Section 4). We find that these changes are largely driven by the models willingness to answer. Specifically, reduced hallucinations are primarily due to Correspondence to: James Xu Zhao (xu.zhao@u.nus.edu) Preprint. Table 1: Summary of model behavior with test-time scaling. Increasing test-time computation does not consistently improve accuracy or reduce hallucinations for most models. It can even increase hallucinations for several models. For ACCURACY, denotes consistent improvement with >2% accuracy gains across consecutive reasoning levels, while indicates no consistent trend. For HALLUCINATION, denotes consistent reduction with >2% hallucination decrease, indicates degradation with >2% hallucination increase, and reflects inconsistent or fluctuating patterns. Gemini 2.5 Flash gpt-oss-20b Grok-3 mini GPT-5 mini ACCURACY HALLUCINATION o3-mini o4-mini Metric Metric Claude Sonnet 4 R1-DistillQwen-7B R1-DistillQwen-14B R1-DistillLlama-8B Qwen3-8B Qwen3-14B ACCURACY HALLUCINATION simple abstention, rather than improved factual accuracy. Conversely, when hallucinations increase, it is often because extended reasoning leads the model to attempt previously unanswered questions. Through case studies on gpt-oss-20b, we observe signs of confirmation bias [18], where the model fabricates details to support its prior belief, resulting in overconfident hallucinations. Given that increasing thinking time does not reliably improve factuality, we ask follow-up question: Is thinking helpful, compared to non-thinking? In Section 5, we evaluate models that natively support both thinking and non-thinking modes. Our results show that enabling the model to think before answering still offers benefits. Firstly, it improves accuracy on knowledge-intensive tasks, particularly those requiring multi-hop reasoning. Secondly, it reduces hallucinations for most models, with Gemini 2.5 Flash again being an exception. To summarize, while test-time scaling in reasoning models has led to strong performance in many domains, it is not yet effective for knowledge-intensive tasks. Increasing inference time does not consistently improve factual accuracy, and contrary to expectations, it can even increase hallucinations."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Test-Time Scaling Test-time scaling has emerged as promising strategy for enhancing the capabilities of large language models. It is typically categorized into two main paradigms [38]: (1) the parallel approach, which samples multiple outputs independently and aggregates them [4, 27]; and (2) the sequential approach, where the model generates long chain-of-thought (CoT) reasoning traces before producing an answer [17, 19, 31]. In this work, we focus on the sequential paradigm, which has become the dominant test-time scaling method for improving model performance. It is widely adopted in frontier reasoning models [1, 9, 10, 20, 28, 35], and has demonstrated strong performance across range of challenging tasks [12, 25, 26, 29]. However, recent studies suggest that in some tasks, increasing test-time computation does not necessarily bring better performance. Gema et al. [8] reveal inverse scaling effects and identify multiple failure modes, where longer reasoning may reinforce problematic patterns rather than improve accuracy. Liu et al. [16] show that extended reasoning in multimodal large language models often amplifies hallucinations by drifting away from visual grounding. Cuadron et al. [5] find that excessive internal reasoning reduces effectiveness in agentic tasks. Some works report that thinking can negatively affect models instruction-following capability [7, 15]. Our work shows that increasing test-time computation is not yet effective for knowledge-intensive tasks. We also find that compared to non-thinking, enabling the model to think is still beneficial for most models. 2.2 Factuality Hallucinations in LLMs Factuality hallucinations, where model generates content that contradict the world knowledge, have been long-standing issue in large language models [11, 13, 39]. These hallucinations reflect the models limitations in absorbing knowledge and its inability to recognize the knowledge boundaries [3]. Prior studies reveal that longer responses often lead to lower factual precision [33, 40], while our work focuses on the effect of reasoning length on hallucinations. Yao et al. [36] investigate whether reasoning models are prone to have more hallucinations, but they do not evaluate the effect of test-time scaling in reasoning models. recent study finds that Claude models have extremely low hallucination rates primarily because they frequently refuse to answer [23]. concurrent work shows that LLMs tend to hallucinate because current training and evaluation paradigms favor guessing over acknowledging uncertainty [24]. To the best of our knowledge, our study is the first to examine how increasing test-time computation affects factuality hallucinations. We show that increasing reasoning length does not reduce factuality hallucinations and can even increase them."
        },
        {
            "title": "3 How Does Test-Time Scaling Affect Accuracy and Hallucination Ratio?",
            "content": "To understand the impact of test-time scaling on knowledge-intensive tasks, we evaluate how increasing test-time computation affects accuracy and hallucination ratio across 12 reasoning models on two benchmarks. For each model, we analyze how accuracy and hallucination change as inference-time computation increases. 3.1 Experimental Setup Benchmarks. We evaluate on two knowledge-intensive benchmarks that involve answering shortform factual questions. SimpleQA [32]: benchmark of short, fact-seeking questions curated by human annotators. question example is: Who received the IEEE Frank Rosenblatt Award in 2010?. We randomly sample 800 questions for evaluation. FRAMES [14]: Questions in FRAMES tend to be more complex and often require multi-hop reasoning. An example is: What Pink Floyd album came out the year Pablo Picasso died? We use all 824 questions for evaluation. Models and Test-Time Scaling Settings. We evaluate 12 large reasoning models and group them into three categories, based on how they support test-time scaling. Models are not allowed to browse or access any external knowledge sources. Reasoning effort: Models such as GPT-5-mini [20], o3-mini, o4-mini [22], gpt-oss-20b [21], Grok-3 mini [34] accept reasoning effort parameter that adjusts the time of thinking. Thinking budget: Models like Gemini 2.5 Flash [9], Claude Sonnet 4 [1] provide thinking budget parameter that guides the model on the number of thinking tokens to use. larger number encourages the model to think for longer time. Budget forcing: For DeepSeek-R1-Distill models [10] and Qwen3 models [28], we adopt the budget forcing method [17]. Specifically, we extend the models thinking process by appending Wait when the model attempts to terminate its reasoning. Prompts. We use consistent prompting format for all models, except GPT-5-mini and Claude Sonnet 4, across both benchmarks: Give me the answer to the following question only when you are sure of it. Otherwise, say dont know. Put your answer on its own line after Answer:. 2 Evaluation and Metrics. Following Wei et al. [32], we prompt ChatGPT (gpt-4o-mini) as grader to evaluate model responses. For each question, the grader is provided with both the models predicted answer and the reference answer, and assigns one of three labels: correct, incorrect, or not attempted. Wei et al. [32] show that this automatic evaluation method closely aligns with human judgments. We report two metrics: (1) Accuracy, the percentage of all questions that were answered correctly. (2) Hallucination ratio, the percentage of all questions that were answered incorrectly. More details of our experimental setup are provided in Appendix A. 2For GPT-5-mini and Claude Sonnet 4, this prompt causes the model to abstain on over 80% of questions. Therefore, for GPT-5 mini, we instead use: Give me the answer to the following question. Put your answer on its own line after Answer:. For Claude Sonnet 4, we follow Gema et al. [8]. See Appendix A.3 for details. Figure 1: Accuracy with increased test-time computation across 12 reasoning models on SimpleQA and FRAMES. For most models, extended test-time reasoning does not consistently improve accuracy. While some models, such as GPT-5 mini, show initial accuracy gains, further increasing reasoning length brings little or no additional improvement. In many cases, such as Claude Sonnet 4 and Qwen3 models, accuracy plateaus or fluctuates with no clear upward trend. 3.2 Effect of Test-Time Scaling on Accuracy Thinking more does not consistently improve the accuracy for most models. As shown in Figure 1, increasing the reasoning length results in minimal or no accuracy gains across most models and both benchmarks. Higher reasoning effort leads to much longer thinking length, but does not consistently improve accuracy. For example, on FRAMES, GPT-5 mini and gpt-oss-20b show 5% accuracy increase when increasing effort from low to medium, but no further improvement at high effort, despite the average reasoning tokens tripling. For o4-mini, the reasoning length increases 10 times, but accuracy remains almost unchanged on both tasks. Similar patterns are observed in o3-mini and Grok-3 mini. This trend also holds on SimpleQA, which shows no or minimal accuracy improvements and inconsistent patterns. Increasing the thinking budget of Claude Sonnet 4 results in minimal accuracy gains on both benchmarks, less than 2% improvement, even as the average thinking length nearly triples. Among models using budget forcing, such as Qwen3-14B, accuracy fluctuates as the thinking length increases. In some cases, accuracy even decreases, as observed with DS-R1-Distill-Qwen-7B on FRAMES. For Gemini 2.5 Flash, low thinking budget limits accuracy due to incomplete reasoning. We evaluate Gemini 2.5 Flash with thinking budget of 256, 512, 1024 and 2048 tokens. Accuracy improves substantially as the budget increases from 256 to 1024 tokens. For example, on FRAMES, 4 Figure 2: Hallucination ratio with increased test-time reasoning across 12 models on SimpleQA and FRAMES. For most models, longer reasoning does not reduce hallucinations. In many cases, such as GPT-5 mini and gpt-oss-20b, hallucination increases with longer thinking length. Only Grok-3 mini and DS-R1-Distill-Qwen-14B exhibit reduced hallucinations with extended reasoning. accuracy improves by 18% as the average reasoning tokens grow from 200 to 610. However, further increasing the budget to 2048 (with an average reasoning length of 790 tokens) does not lead to more improvement. We observe that with low thinking budget, such as 256 tokens, the model cannot finish its thinking process, so it will abstain from answering the question, leading to low accuracy (see case studies in Section 4.4). Once the budget becomes sufficient (e.g., 1024 tokens), further increases, such as to 2048 tokens, do not lead to additional accuracy improvements. Takeaway 1: Test-time scaling does not consistently improve accuracy. Increasing test-time computation is not an effective strategy for enhancing factual accuracy in current LLMs. 3.3 Effect of Test-Time Scaling on Hallucination Ratio Thinking more does not reduce hallucinations and may even increase them. In Figure 2, for most models on two benchmarks, increasing test-time computation fails to reduce hallucinations and can make them worse. In OpenAI models, higher reasoning effort generally leads to more hallucinations, except for o4-mini on FRAMES. For example, on SimpleQA, the hallucination ratio of GPT-5-mini increases by over 15% as reasoning length increases from 300 to 3300 tokens. Similarly, o3-mini and gpt-oss-20b show increases of 12% and 25%, respectively, as their thinking length scales by nearly 10 times. On FRAMES, the same trend holds: hallucination ratio increases with longer 5 Figure 3: Changes in hallucination behavior with more thinking. We compare model responses at different reasoning levels, focusing on cases where one response is hallucination and the other is not. For the non-hallucinating responses, we compute the ratio of correct and not attempted. Results show that reduced hallucinations result from abstention, while more hallucinations stem from the model attempting previously unanswered questions. reasoning in GPT-5-mini, o3-mini, and gpt-oss-20b. Models with thinking budget settings show similar patterns. For Gemini 2.5 Flash, hallucination ratio increases by 10% on SimpleQA and 9% on FRAMES. Claude Sonnet 4 also shows no reduction in hallucinations despite longer reasoning. Among models using budget forcing, none except for DS-Distill-Qwen-14B benefits from longer thinking in terms of reducing hallucinations. In some cases, hallucination ratio even increases, as seen in DS-R1-Distill-Qwen-7B on SimpleQA. For Grok-3 mini and DS-R1-Distill-Qwen-14B, thinking more leads to fewer hallucinations. Increasing the reasoning effort reduces the hallucination ratio of Grok-3 mini by 4.4% on SimpleQA and 1.7% on FRAMES. However, considering that the thinking length nearly doubles on FRAMES, the reduction is relatively small. DS-R1-Distill-Qwen-14B shows more noticeable improvement. On SimpleQA, hallucinations drop by over 12% as the average reasoning length increases from 530 to 1580 tokens. On FRAMES, the hallucination ratio decreases by 8% as reasoning length increases from 850 to 1900 tokens. Despite these reductions, DS-R1-Distill-Qwen-14B still has higher hallucination ratio than other open-source models, such as DS-R1-Distill-Llama-8B and Qwen3-8B. Takeaway 2: Test-time scaling does not reduce hallucinations for most models. In some cases, extended thinking even increases hallucinations."
        },
        {
            "title": "4 Why Does Thinking More Sometimes Lead to Fewer or More",
            "content": "Hallucinations? As discussed in Section 3.3, thinking more sometimes leads to fewer hallucinations, and sometimes more. In this section, we examine the causes of these changes. We first compare model behavior across different thinking levels, then conduct case studies to better understand how extended reasoning affects model behavior. 6 4.1 Experimental Setup We compare model behavior across different levels of test-time reasoning. As illustrated in Figure 3, for each question, we obtain two responses from the same model under different reasoning levels. We focus on cases where the hallucination outcome changes between the two settings, that is, one response is hallucination while the other is not. We then examine the non-hallucinating responses and compute the ratio of correct versus not attempted. This helps us understand whether reduced hallucinations are due to improved knowledge retrieval or simply abstention, and conversely, whether increased hallucinations result from risky attempts at answering. As discussed in Section 3.3, Grok-3 mini and DS-R1-Distill-Qwen-14B show reduced hallucinations with longer thinking. For these models, we analyze cases where the model hallucinates at lower reasoning level but does not hallucinate at higher level. In contrast, GPT-5 mini, o3-mini, gpt-oss20b, and Gemini 2.5 Flash exhibit increased hallucinations. For these models, we examine cases where the model does not hallucinate at lower reasoning level but hallucinates at higher level. 4.2 Hallucination Changes Are Driven by the Models Willingness to Answer Fewer hallucinations are mostly due to abstention. As shown in Figure 3, for Grok-3 mini and DS-R1-Distill-Qwen-14B on both benchmarks, most cases of reduced hallucinations result from the model choosing not to answer, rather than providing correct answer. On SimpleQA, when increasing the reasoning effort of Grok-3 mini from low to high, 93.1% of the responses that are not hallucinating at higher effort are labeled as not attempted. Similarly, for DS-R1-Distill-Qwen14B, the not attempted ratio in these cases is 88.8%. These results suggest that in most cases where longer reasoning reduces hallucinations, the improvement is not due to better factual recall, but rather because the model chooses to abstain after thinking more. More hallucinations mostly come from previously unattempted questions. Figure 3 shows that the increase in hallucinations at higher reasoning levels is largely due to the model attempting questions it had previously abstained from. For example, on SimpleQA, when increasing the reasoning effort of gpt-oss-20b from low to high, 95% of new hallucinations come from questions that were not attempted at the lower effort level. similar trend is seen with Gemini 2.5 Flash: 76.5% of hallucinations under higher thinking budget correspond to questions the model had abstained from at the lower budget. This pattern also appears on FRAMES: for all four models, over 70% new hallucinations arise from the questions the model had not attempted before. These results suggest that longer thinking time encourages models to answer more questions. But for many of these newly attempted questions, the answers are incorrect, resulting in higher hallucinations. Takeaway 3: Test-time scaling sometimes leads to fewer hallucinations, and sometimes more. Reductions often result from the model choosing to abstain after thinking more, while increased hallucinations come from the model attempting previously unanswered questions with longer reasoning. 4.3 Case Study on gpt-oss-20b: Thinking More Leads to Confirmation Bias To better understand why longer thinking leads to more attempted answers, and thus more hallucinations, we conduct manual inspection of the reasoning traces from gpt-oss-20b. Longer reasoning leads to overconfident hallucinations. In the left example of Figure 4, gpt-oss20b initially engages with the question at low reasoning effort. It explores few possibilities, such as was named Fellow in 2012, but finally abstains due to uncertainty. Under higher effort, the model continues to search its memory. It begins by expressing uncertainty, for example, think he was made Fellow in 2008? But we need to confirm.. But as the reasoning progresses, the model gradually becomes more confident. The response shifts from tentative claims like maybe in 2005 to more confident statement: am fairly sure its 2005. Despite the increased confidence, the final answer remains incorrect. This example suggests that extended reasoning can inflate the models internal confidence, leading to overconfident hallucinations where the model chooses to answer despite still lacking accurate information. 7 Figure 4: Case studies illustrating how thinking more can lead to more hallucinations. (Left): gptoss-20b abstains under low reasoning effort, but produces an overconfident incorrect answer at high effort. (Right): Gemini 2.5 Flash abstains under low thinking budget due to incomplete reasoning, but hallucinates when given more budget. Full reasoning traces are provided in Appendix B. Longer reasoning induces confirmation bias, resulting in overconfident hallucinations. We observe signs of confirmation bias [18], where the model recalls or even makes up information to reinforce its initial belief. After tentatively proposing an answer, e.g., He has been recognized by AAAI maybe in 2005, the model attempts to justify this hypothesis by generating fabricated supporting details, such as We can check his CV... think that is correct and know of the AAAI Fellows list on the AAAI website. For 2005, the list includes John Mylopoulos indeed. These fabricated details build upon one another, reinforcing the models confidence in its incorrect belief and ultimately leading to overconfident hallucinations. 4.4 Case Study on Gemini 2.5 Flash: Incomplete Reasoning Leads to Abstention In Section 4.2, we find that thinking more causes Gemini 2.5 Flash to attempt more questions, leading to more hallucinations. We examine the thought summaries of Gemini 2.5 Flash to understand the underlying causes. Compared to responses with extended reasoning, incomplete reasoning leads to abstention and fewer hallucinations. In the right example of Figure 4, at low thinking budget (256 tokens), Gemini 2.5 Flash begins its reasoning process with will then attempt to pinpoint the specific bond amount from any accessible sources, but it is cut off before completing the process. As result, it abstains from answering, leading to relatively low hallucination ratio. With higher budget (512 tokens), the model is able to complete its reasoning, e.g., It appears Im getting confirmation, and proceeds to give confident yet incorrect answer: feel confident enough to provide definitive answer. Takeaway 4: Case studies reveal that extended reasoning can induce overconfident hallucinations, as the model reinforces incorrect beliefs through confirmation bias. For Gemini 2.5 Flash, incomplete reasoning leads to abstention. 8 Model Task Thinking Accuracy () Hallucination () GPT-5-mini Gemini 2.5 Flash Claude Sonnet 4 Qwen3-8B Qwen3-14B SimpleQA FRAMES SimpleQA FRAMES SimpleQA FRAMES SimpleQA FRAMES SimpleQA FRAMES minimal (cid:33) minimal (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) 15.0 22.1 (+7.1) 22.0 47.1 (+25.1) 12.6 28.6 (+16.0) 16.9 51.0 (+34.1) 23.0 27.8 (+4.8) 50.4 54.6 (+4.2) 3.8 3.3 (-0.5) 8.3 17.0 (+8.7) 4.0 3.4 (-0.6) 8.6 18.3 (+9.7) 82.4 40.6 (-41.8) 61.9 30.8 (-31.1) 29.1 59.3 (+30.2) 25.1 34.7 (+9.6) 37.5 46.0 (+8.5) 42.0 38.4 (-3.6) 77.1 30.0 (-47.1) 64.3 37.4 (-26.9) 48.6 33.1 (-15.5) 37.0 34.1 (-2.9) Table 2: Accuracy (%) and hallucination ratio (%) for models with and without thinking on SimpleQA and FRAMES. Green indicates improvement (higher accuracy or fewer hallucinations), while red indicates degradation (lower accuracy or more hallucinations). Enabling thinking improves accuracy and reduces hallucinations for most models."
        },
        {
            "title": "5 Thinking vs. Non-Thinking: Is Thinking Helpful?",
            "content": "In the previous sections, we examine test-time scaling within models thinking mode, where they generate reasoning chains before producing final answer. Our results show that increasing thinking length does not consistently improve accuracy or reduce hallucinations. In this section, we take step further: For models that support both thinking and non-thinking modes, is enabling thinking helpful? 5.1 Experimental Setup We focus on models that natively support both thinking and non-thinking modes for controlled comparison. Specifically, GPT-5 mini supports minimal reasoning effort setting, which produces very few or no reasoning tokens. For Gemini 2.5 Flash and Claude Sonnet 4, we disable thinking by setting the thinking budget to 0. Qwen3-8B and Qwen3-14B provide an explicit enable_thinking parameter that toggles reasoning behavior. We use the same prompts and evaluation methods as described in Section 3.1. 5.2 Thinking Can Be Helpful for Knowledge-Intensive Tasks Enabling thinking increases accuracy. As shown in Table 2, for most models, enabling the thinking mode leads to higher accuracy on both benchmarks. For example, GPT-5 mini shows 7.1% accuracy improvement on SimpleQA and substantial 25.1% gain on FRAMES. Gemini 2.5 Flash also benefits significantly from thinking, with accuracy increasing by 16.0% on SimpleQA and 34.1% on FRAMES. Claude Sonnet 4 also exhibits improvements across both tasks. For Qwen3 models, enabling thinking results in substantial gains on FRAMES. This suggests that thinking is particularly helpful for more complex tasks that often require multi-hop reasoning, such as FRAMES. For most models, enabling thinking leads to reduced hallucinations. Enabling the thinking mode often reduces hallucinations. GPT-5 mini shows significant drop, with hallucination ratio reduced by over 30% on both benchmarks. Qwen3-8B also shows substantial hallucination reduction, with hallucination ratio decreasing by 47.1% on SimpleQA and 26.9% on FRAMES. similar pattern is observed on Qwen3-14B. However, enabling thinking increases hallucinations for Gemini 2.5 Flash. This may be because thinking encourages the model to attempt more questions, resulting in higher hallucination ratio. For Claude Sonnet 4, thinking reduces hallucinations on FRAMES but not on SimpleQA. possible explanation is that Claude Sonnet 4 already has low hallucination ratio in non-thinking mode, leaving little room for further improvement. Takeaway 5: Compared to non-thinking, enabling thinking improves accuracy, especially on reasoning-intensive benchmarks like FRAMES. It also reduces hallucinations for most models, though not for Gemini 2.5 Flash."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present comprehensive study of test-time scaling in knowledge-intensive tasks, evaluating 12 large reasoning models across two benchmarks. We find that increasing test-time computation does not consistently improve factual accuracy and, in many cases, leads to more hallucinations. Our analysis shows that hallucination changes with extended reasoning are largely driven by the models willingness to answer: reductions in hallucinations often result from abstention, while increases stem from risky attempts on previously unanswered questions after thinking more. Case studies further show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. For Gemini 2.5 Flash, incomplete reasoning often results in abstention. These findings highlight the limitations of current test-time scaling approaches for knowledge-intensive tasks. While enabling thinking can be helpful, allocating more test-time computation is not yet reliable strategy to improve factual robustness in large language models."
        },
        {
            "title": "Limitation",
            "content": "Our study has several limitations. First, our evaluation focuses on two benchmarks that require shortform answers consisting of only few words. It remains unclear whether our findings generalize to open-ended or long-form generation tasks. Exploring how increased test-time computation affects such tasks is left for future work. Second, while our case studies reveal that extended reasoning can induce confirmation bias, we do not propose intervention strategies to mitigate this behavior. In particular, we do not attempt to constrain the models reasoning process to ensure that longer inference time does not worsen hallucinations. Developing such control mechanisms is promising area for future research."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, May 2025. [2] Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Y. Halevy, Eduard H. Hovy, Heng Ji, Filippo Menczer, Rubén Míguez, Preslav Nakov, Dietram A. Scheufele, Shivam Sharma, and Giovanni Zagni. Factuality challenges in the era of large language models and opportunities for fact-checking. Nat. Mac. Intell., 6:852863, 2024. URL https://www.nature.com/articles/s42256-024-00881-z. [3] Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. HalluLens: LLM hallucination benchmark. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of 10 the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2412824156, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1176. URL https: //aclanthology.org/2025.acl-long.1176/. [4] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. [5] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv: 2502.08235, 2025. [6] Long Phan et al. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. [7] Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, and Yu Cheng. Scaling reasoning, losing control: Evaluating instruction following in large reasoning models. arXiv preprint arXiv: 2505.14810, 2025. [8] Aryo Pradipta Gema, Alexander Hägele, Runjin Chen, Andy Arditi, Jacob Goldman-Wetzler, Kit Fraser-Taliente, Henry Sleight, Linda Petrini, Julian Michael, Beatrice Alex, Pasquale Minervini, Yanda Chen, Joe Benton, and Ethan Perez. Inverse scaling in test-time compute, 2025. URL https://arxiv.org/abs/2507.14417. [9] Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv: 2507.06261, 2025. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Trans. Inf. Syst., 43(2), January 2025. ISSN 1046-8188. doi: 10.1145/3703155. URL https://doi.org/10.1145/3703155. [12] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. [13] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12), March 2023. ISSN 0360-0300. doi: 10.1145/3571730. URL https://doi.org/10.1145/3571730. [14] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrievalaugmented generation. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 47454759, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.243. URL https://aclanthology. org/2025.naacl-long.243/. [15] Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, and Anurag Beniwal. When thinking fails: The pitfalls of reasoning for instructionfollowing in llms. arXiv preprint arXiv: 2505.11423, 2025. 11 [16] Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, and Sheng Liu. More thinking, less seeing? assessing amplified hallucination in multimodal reasoning models, 2025. URL https://arxiv.org/abs/2505.21523. [17] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv: 2501.19393, 2025. [18] Raymond Nickerson. Confirmation bias: ubiquitous phenomenon in many guises. Review of general psychology, 2(2):175220, 1998. [19] OpenAI. Learning to reason with large language models. https://openai.com/index/ learning-to-reason-with-llms/, September 2024. [20] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, August 2025. [21] OpenAI. Introducing gpt-oss. https://openai.com/index/introducing-gpt-oss/, August 2025. [22] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, April 2025. [23] OpenAI. Findings from pilot anthropicopenai alignment evaluation exercise: Openai safety tests. https://openai.com/index/openai-anthropic-safety-evaluation/, August 2025. [24] OpenAI. Why language models hallucinate. https://openai.com/index/ why-language-models-hallucinate/, September 2025. [25] Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovic, Nikola Jovanovic, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. [26] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level googleIn First Conference on Language Modeling, 2024. URL https: proof q&a benchmark. //openreview.net/forum?id=Ti67584b98. [27] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM testtime compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=4FWAwZtd2n. [28] Qwen Team. Qwen3 technical report. arXiv preprint arXiv: 2505.09388, 2025. [29] Hemish Veeraboina. Aime problem set 1983-2024, 2023. URL https://www.kaggle.com/ datasets/hemishveeraboina/aime-problem-set-1983-2024. [30] Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Nenkov Georgiev, Rocktim Jyoti Das, and Preslav Nakov. Factuality of large language models: survey. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1951919529, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.1088. URL https://aclanthology.org/2024.emnlp-main.1088/. [31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. [32] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv: 2411.04368, 2024. [33] Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc Le. Long-form factuality in large language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8075680827. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/file/937ae0e83eb08d2cb8627fe1def8c751-Paper-Conference.pdf. [34] xAI. Grok 3 beta the age of reasoning agents. https://x.ai/news/grok-3, February 2025. [35] xAI. Grok 4. https://x.ai/news/grok-4, July 2025. [36] Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. Are reasoning models more prone to hallucination?, 2025. URL https: //arxiv.org/abs/2505.23646. [37] Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Johannes Heidecke, and Amelia Glaese. Trading inference-time compute for adversarial robustness. arXiv preprint arXiv: 2501.18841, 2025. [38] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, and Chen Ma. survey on test-time scaling in large language models: What, how, where, and how well?, 2025. URL https://arxiv.org/abs/2503.24235. [39] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Sirens song in the ai ocean: survey on hallucination in large language models. arXiv preprint arXiv: 2309.01219, 2023. [40] James Xu Zhao, Jimmy Z.j. Liu, Bryan Hooi, and See-Kiong Ng. How does response length affect long-form factuality. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 31023125, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.161. URL https://aclanthology.org/2025.findings-acl.161/."
        },
        {
            "title": "A Experiment Details",
            "content": "A.1 Test-Time Scaling Settings We evaluate 12 large reasoning models under different test-time scaling strategies. The settings used to obtain the results in Figure 1 and Figure 2 are as follows: Reasoning effort: For models that support three levels of reasoning effort, i.e., GPT-5 mini, o3-mini, o4-mini, and gpt-oss-20b, we evaluate: low, medium, and high. For Grok-3 mini, only two levels are available: low and high. Thinking budget: For Gemini 2.5 Flash, we set thinking budget to 256, 512, 1024, and 2048 tokens. For Claude Sonnet 4, we set thinking budget to 1024, 2048, and 4096 tokens. Budget forcing: For DeepSeek-R1-Distill models and Qwen3 models, we use budget forcing by appending Wait multiple times. Specifically, we evaluate with 0 (default), 2, 4, 8, and 12 extension times. A.2 Parameter Settings and Hardware For all closed-source models, we use the default parameter setting. These experiments were conducted between June and August 2025. For gpt-oss-20b, we set the temperature to 0.7 and max_new_tokens to 20,000. For DeepSeek-R1Distill and Qwen3 models, we follow the usage recommendations3. We use consistent parameter settings for different levels of test-time reasoning. All open-source models are run without quantization using 8 NVIDIA A100-40GB GPUs. A.3 Prompt Details For all models except GPT-5 mini and Claude Sonnet 4, we use the following prompt on both benchmarks: Give me the answer to the following question only when you are sure of it. on its own line after Answer:. Otherwise, say dont know. Put your answer However, this prompt causes over-refusal for GPT-5 mini and Claude Sonnet 4, where the model refuses to answer in over 80% of all questions. This behavior is also reported in recent study [23]. Therefore, we for GPT-5 mini, we use the following prompt: Give me the answer to the following question. on its own line after Answer:. Put your answer For Claude Sonnet 4, we adopt the prompting approach from [8]: Give me the answer to the following question. thinking token budget of about <thinking_budget> tokens. MUST USE ALL OF YOUR THINKING TOKENS. Put your answer on its own line after Answer:. You have YOU A.4 Results Details For the results in Figure 3, we compare responses across different reasoning levels: Grok-3 mini: Compared at reasoning effort levels of low and high. DeepSeek-R1-Distill-Qwen-14B: Compared between 0 and 2 extension times. 3DeepSeek-R1-Distill models: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B Qwen3 models: https://huggingface.co/Qwen/Qwen3-8B 14 GPT-5 mini, o3-mini and gpt-oss-20b: compared at reasoning effort levels of low and high. Gemini 2.5 Flash: Compared at thinking budget of 256 and 512 tokens For the results in Table 2, we use the following results for the thinking mode: GPT-5 mini: reasoning effort set to low. Gemini 2.5 Flash: thinking budget set to 2048 tokens. Claude Sonnet 4: thinking budget set to 4096 tokens. Qwen3-8B and Qwen3-14B: Natural output (no extension) with enable_thinking enabled."
        },
        {
            "title": "B Full Reasoning Traces for Case Studies",
            "content": "We present the reasoning traces of gpt-oss-20b and Gemini 2.5 Flash for case studies in Section 4.3 and Section 4.4. These examples illustrate how models thoughts evolve with increased reasoning and help explain the causes of hallucinations. For each model, we show reasoning traces of both low and high reasoning levels. Specifically, we provide: Tables 3 and 4: Thoughs of gpt-oss-20b under low and high reasoning effort, respectively. Tables 5 and 6: Thought summaries of Gemini 2.5 Flash under thinking budget of 256 tokens and 512 tokens. Example of gpt-oss-20b with low reasoning effort Prompt Give me the answer to the following question only when you are sure of it. Otherwise, say dont know. Answer:. Put your answer on its own line after What year did John Mylopoulos (1943), professor at the University of Toronto, receive his AAAI Fellow award? Thought (with low reasoning effort) Need to know year John Mylopoulos received AAAI Fellow. He is computer scientist. think he was named AAAI Fellow in 2011? Wait AAAI Fellows list: 2009, 2010, etc. recall John Mylopoulos was named Fellow in 2012? Lets check memory: AAAI Fellows 2014 included John Mylopoulos? Actually 2014 fellows: Alan Turing Award? Hmm. think 2012. Im uncertain. Ill say dont know. Response Answer: dont know Table 3: gpt-oss-20b thought and response with low reasoning effort. Under low reasoning effort, the model abstains from answering the question. This example corresponds to the left case study in Figure 4. Example of gpt-oss-20b with high reasoning effort Prompt Give me the answer to the following question only when you are sure of it. Otherwise, say dont know. Answer:. Put your answer on its own line after What year did John Mylopoulos (1943), professor at the University of Toronto, receive his AAAI Fellow award? Thought (with high reasoning effort) We need to answer: \"What year did John Mylopoulos (1943), professor at the University of Toronto, receive his AAAI Fellow award?\" The user wants answer only if sure; else say \"I dont know\". Provide answer on its own line after Answer:. So we need to find the year he received AAAI Fellow award. Lets research mentally: John Mylopoulos is professor of computer science at University of Toronto, known for knowledge engineering, ontologies, etc. He is Fellow of AAAI. We need year: He was awarded AAAI Fellow in 2008? Wait, lets check. 16 We can recall that AAAI Fellow is recognition for contributions. Many scholars became Fellows in early 2000s. John Mylopoulos might have been recognized in 2004? But lets research. dont have internet but can recall known facts: Mylopoulos is co-author of the \"Knowledge Engineering Handbook\" and \"Conceptual Design of Information Systems\". He has been recognized by AAAI as Fellow in 2008? Actually AAAI Fellows started in 2001. The first group of Fellows in 2001 included people like John Platt, etc. But Im not sure. Better to search memory: recall reading about AAAI Fellows list: 2001, 2002, etc. John Mylopoulos appears in 2005? Lets think. The AAAI Fellows list includes for each year: 2001 (the first set), 2002, etc. The list likely includes Mylopoulos. think he was made Fellow in 2008? But we need to confirm. Ok, lets reason: AAAI Fellows are typically recognized for contributions to AI. Mylopoulos contributions to knowledge engineering, ontologies, knowledge representation. He published influential works like \"Knowledge Engineering: Conceptual Approach\" (1990). He has been recognized by AAAI maybe in 2005. Lets search memory: recall reading about \"John Mylopoulos, Professor at University of Toronto, was named an AAAI Fellow in 2005.\" That might be correct. But Id like to confirm. Alternatively, maybe 2006? Lets check AAAI Fellows list memory: 2001 Fellows: David B. Allen, John L. Holland, etc. 2002 Fellows: Andrew Yao, L.P. Chang? Not sure. 2003 Fellows: Some. But we can approximate: He might have been recognized in 2005. But we need to be certain. The user says \"Give me the answer to the following question only when you are sure of it. Otherwise, say dont know.\" So we need to determine if we are sure. Lets attempt to recall or deduce more precisely. John Mylopoulos was born 1943. He is professor at University of Toronto. The AAAI Fellow award: AAAI Fellows are selected at the AAAI conference. In 2005, there was group of Fellows. recall reading that \"John Mylopoulos\" was Fellow of AAAI in 2005. But Im not fully certain. Lets search memory: might have seen page: \"John Mylopoulos - AAAI Fellows\" maybe under \"John Mylopoulos (born 1943) University of Toronto\". On his CV maybe lists \"Fellow of AAAI (2005)\". Yes think that is correct: He was Fellow of AAAI in 2005. Lets see if there is any other year: 2010? 2011? He might have also been named Fellow of AAAI in 2005. Ok, lets examine: AAAI Fellows list by year (source memory). In 2003, there were 60 Fellows. In 2004 maybe 70. In 2005 maybe 90. In 2006 maybe 100. But we need to check if Mylopoulos appears. think he appears in 2005. Alternatively, maybe 2003. But we can check his CV: He might have \"Fellow of AAAI (2005)\". think that is correct. The reason: He has an influence on knowledge engineering. He might have been recognized in 2005. But lets double-check: AAAI Fellows list (2005) includes \"John Mylopoulos\" as Fellow. Could be correct. Lets recall that AAAI started awarding Fellows in 2001. The first group included people like \"John L. Holland\", \"Allen Newell\", etc. The next group 2001 included \"John M. Carmichael\" etc. But not Mylopoulos. But Im not entirely sure. 17 Lets search memory: Did AAAI have 2005 award? Many people were made Fellows in 2005: \"John Mylopoulos, University of Toronto, was named AAAI Fellow in 2005.\" Yes, think recall reading that. Ok, but we need to be sure. Lets cross-check with other sources: know of the AAAI Fellows list on the AAAI website. For 2005, the list includes \"John Mylopoulos\" indeed. recall reading that list. So think its safe. But we also need to consider that the user might expect specific year. If say \"2005\" and its wrong, that would be wrong answer. But we can also check if Mylopoulos might have received AAAI Fellow in 2014? But that seems unlikely. Lets search memory: The AAAI Fellows list includes many names: \"John Mylopoulos\" appears in \"2005 Fellows\" indeed. For example, \"John Mylopoulos\" appears in the AAAI Fellows 2005 list: \"John Mylopoulos, University of Toronto, Canada.\" think its correct. Therefore, answer: 2005. But we need to confirm. Lets think about his biography: He is leading figure in knowledge representation and ontologies. He has been recognized by AAAI. He was also recognized by the Canadian Academy of Engineering? He might have also been named Fellow of the Royal Society of Canada? But AAAI fellowship might be in 2005. Lets check memory of AAAI Fellows list: In 2005, the Fellows included \"John Mylopoulos, University of Toronto\" as well as others like \"Michael L. Littman\" etc. That seems plausible. Ok, Im fairly sure its 2005. So answer: 2005. But we need to provide answer on its own line after \"Answer:\". Thus: Answer: But we need to be sure. Lets double-check by reasoning: AAAI Fellows list 2005: can recall reading that \"John Mylopoulos\" is listed as Fellow in 2005. The AAAI website might have page listing all Fellows by year. Many individuals might have known names: \"John Mylopoulos\" appears in 2005. So its likely. Ok, answer: 2005. Lets produce the answer. Response Answer: 2005 (Incorrect. Reference Answer: 1993) Table 4: gpt-oss-20b thought and response with high reasoning effort. With high reasoning effort, the model shows confirmation bias, resulting in an overconfident but incorrect answer. This corresponds to the left case study in Figure 4. Example of Gemini 2.5 Flash with thinking budget of 256 tokens Prompt Give me the answer to the following question only when you are sure of it. Otherwise, say dont know. Answer:. Put your answer on its own line after 18 What was Jeffrey Epsteins released bond in dollars on July 27, 2006, at the Palm Beach County jail? Thought Summary (with thinking budget of 256 tokens) **Navigating Precision Information Request** Okay, so have user requesting very specific piece of information: Jeffrey Epsteins bond amount on July 27, 2006, at the Palm Beach County jail. The user seems to understand the nuances of this type of request, emphasizing need for surety. get it. Were dealing with very specific piece of data. My job is to provide either definitive \"Answer: [value]\" or clear \"Answer: dont know.\" No room for ambiguity here. My approach is straightforward. Ill need to formulate precise search query. Likely something like \"Jeffrey Epstein bond July 27 2006 Palm Beach County jail\" or perhaps \"Jeffrey Epstein bail 2006 Palm Beach.\" will then delve into my understanding of Epsteins legal history, relying on my existing knowledge base and recall. will quickly scan my mental database, looking for key events and dates. know he was arrested in Florida in 2006 and faced charges related to solicitation and lewd conduct. will then attempt to pinpoint the specific bond amount from any accessible sources. Response Answer: dont know Table 5: Gemini 2.5 Flash response with low thinking budget. Under low thinking budget, the model cannot complete its reasoning process and finally abstains from answering. This corresponds to the right case study in Figure 4. Example of Gemini 2.5 Flash with thinking budget of 521 tokens Prompt Give me the answer to the following question only when you are sure of it. Otherwise, say dont know. Answer:. Put your answer on its own line after What was Jeffrey Epsteins released bond in dollars on July 27, 2006, at the Palm Beach County jail? Thought Summary (with thinking budget of 512 tokens) **Analyzing and Responding to the Users Query** Okay, so the user needs very specific piece of information: Jeffrey Epsteins bond amount on July 27, 2006, in the Palm Beach County jail. The key here is that need to be *absolutely certain* before providing an answer. If Im unsure, have to say \"I dont know.\" My initial thought is that recall Epstein had multiple arrests and bond hearings. July 27, 2006, rings bell that sounds like significant arrest date in the early stages of his legal troubles. Im now going to mentally formulate my search query. Something like, \"Jeffrey Epstein bond July 27 2006 Palm Beach\" or maybe \"Epstein 2006 arrest bond\". Ill immediately access my knowledge base essentially mental index of reputable sources and my existing knowledge about Epsteins legal history. Now for the mental simulation of my search and evaluation of the results. Im accessing range of sources news archives, summaries of court documents, etc. looking for corroboration. 19 It appears Im getting confirmation. Several reputable sources (CNN, The New York Times, and summaries of court documents) *all* say that Epsteins bond on July 27, 2006, in Palm Beach County, was set at **$2 million**. The sources consistently use the same $2 million figure. Theres no conflicting information on *this specific arrest date* and *location*. feel confident enough to provide definitive answer. Im going to formulate the response in the requested format. Response Answer: $2,000,000 (Incorrect. Reference Answer: $3,000 bond) Table 6: Gemini 2.5 Flash response with high thinking budget. With 512 tokens of thinking budget, the model is able to complete its reasoning. But it hallucinates with confident yet incorrect answer. This corresponds to the right case study in Figure 4."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}