{
    "paper_title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
    "authors": [
        "Guochao Jiang",
        "Wenfeng Feng",
        "Guofeng Quan",
        "Chuzhan Hao",
        "Yuewei Zhang",
        "Guohua Liu",
        "Hao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 3 0 8 9 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "VCRL: VARIANCE-BASED CURRICULUM REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang Alibaba Cloud Computing anyue.jgc@alibaba-inc.com cashenry@126.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout groups reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have lower variance, while samples with moderate difficulty have higher variance. Based on this, we propose VCRL, curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines."
        },
        {
            "title": "INTRODUCTION",
            "content": "The new generation of large language models (LLMs) that use long Chain-of-Thoughts (CoTs) for reasoning (Xu et al., 2025a) have achieved remarkable results in information extraction (Zhang et al., 2025c; Jiang et al., 2024; 2025a), mathematics (Wang et al., 2025a), code (Yang et al., 2025b), and agent (Gao et al., 2025; Zhang et al., 2025a) fields, including GPT-51, GPT-OSS (Agarwal et al., 2025), DeepSeek-R1 (Guo et al., 2025), and Kimi k1.5 (Team et al., 2025). notable feature of this type of LLMs is the phenomenon called Test-Time Scaling (TTS) (Zhang et al., 2025b), which generates long CoTs to scale performance. Reinforcement Learning with Verifiable Rewards (RLVR) (Mroueh, 2025) has been proven to be an effective technique for achieving TTS in the post-training process. Recently, Reinforcement Learning (RL) methods have shown significantly better generalization performance in improving LLM reasoning capabilities compared to traditional Supervised Fine-Tuning (SFT) (Chu et al., 2025). SFT relies on high-quality, labeled data from human annotations or stronger model distillation, while RL relies primarily on the models own exploration. Rollout-based reinforcement learning methods represented by Group Relative Policy Optimization (GRPO) (Shao et al., 2024) require the model to generate multiple trajectories for each training sample and learn based on the rewards of the generated trajectories, which can continuously expand the boundaries of LLMs capabilities through the continuous RL process with diverse training samples. However, existing rollout-based RL methods do not consider how well the models current abilities match the difficulty of training samples. In human learning, people usually start with easy tasks and move to harder ones, an approach called Curriculum Learning (CL) (Wang et al., 2022; Soviany et al., 2022). Rollout-based RL methods have the model explore rollouts generated by the training samples, without considering if those samples are easy or hard. This does not help LLMs learn efficiently from samples with different levels of difficulty. Also, the models skills change during Corresponding author 1https://openai.com/index/gpt-5-system-card"
        },
        {
            "title": "Preprint",
            "content": "RL training, so the difficulty of training samples can vary for the model at different stages. Because of this, pre-sorting training samples by fixed difficulty is not effective. To address these limitations, we introduce curriculum reinforcement learning framework called VCRL. It dynamically adjusts the difficulty of training samples based on the variance of group rewards. We find that the variance in rollout group rewards in RLVR partly reflects how hard sample is for LLMs. With RLVRs current sparse reward system, samples that are too hard often get only 0 rewards, leading to low variance; this also happens with samples that are too easy. When samples are more uncertain, such as when half of the rollouts receive reward of 1 and the other half receive 0, the model is at key learning point for that sample. VCRL uses Variance-based Dynamic Sampling to select these samples for training, helping control the quality of the training batch. Group variance also gives way to measure sample difficulty for the current state of the model. Therefore, VCRL uses Replay Learning with memory bank to further boost training efficiency. Our contributions are as follows: We introduce VCRL, curriculum reinforcement learning framework that adjusts the difficulty of training samples based on the variance of group rewards. By focusing on samples with high reward variance, VCRL selects those most valuable for current model training. Building on group variance, we further introduce Replay Learning with memory bank to control training stability and improve training efficiency. By updating and utilizing the memory bank, VCRL ensures high variance of samples in the training batch, thus achieving higher training value. We conduct extensive experiments on five benchmark datasets to justify VCRLs advantage on LLMs efficient Test-Time Scaling over some SOTA RL methods. Our results show consistent performance gains across different models, validating the effectiveness and robustness of our VCRL."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In this section, we review the current policy-based reinforcement learning methods in LLM, especially the rollout-based like GRPO and some variants. 2.1 PROXIMAL POLICY OPTIMIZATION (PPO) PPO (Schulman et al., 2017) limits the update of the current policy to the proximal region of the old policy through the clipping mechanism. Specifically, give dataset D, is the query and is the response. For the policy model πθ parameterized by θ, the likelihood by the policy πθ is given by πθ(yx) = (cid:81)y t=1 πθ(ytx, y<t), where is the number of tokens in y. In RLVR, there is verifier that can score given query-response pair (x, y) and obtain reward r(x, y) [0, 1]. PPO optimizes the following objective for policy optimization to update the actor in the proximal region of the old policy πθold: JPPO(θ) = ExD,yπθold (x) 1 y (cid:88) t=1 (cid:16) min rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17) , (1) where the importance ratio of the token yt is given by rt(θ) = πθ(ytx,y<t) πθold (ytx,y<t) , ϵ is the clipping range of the importance ratio, and the advantage ˆAt of yt is estimated using value model by Generalized Advantage Estimator (GAE) (Schulman et al., 2016). PPO relies on the value model to evaluate the current state. Typically, the value model and the trained model have similar structures and parameters, resulting in significant computational and memory costs. Furthermore, the accuracy of the value model itself limits the effectiveness of the PPO algorithm, especially for long response and sparse reward in complex tasks for LLM."
        },
        {
            "title": "2.2 GROUP RELATIVE POLICY OPTIMIZATION (GRPO) AND VARIANTS",
            "content": "GRPO (Shao et al., 2024) calculates the relative advantages of each response within group of responses generated by LLM to the same query, eliminating the need to the value model. Specifically, GRPO optimizes the following objective for policy optimization to update the actor within the group of responses (we omit the KL regularization term for brevity): JGRPO(θ) = xD,{yi}G"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi i=1πθold(x) yi (cid:88) (cid:16) min t=1 ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) , (2) where is the number of generated responses to the same query x, the importance ratio ri,t(θ) and advantage ˆAi,t of token yi,t are given by ri,t(θ) = πθ(yi,tx, yi,<t) πθold(yi,tx, yi,<t) , ˆAi,t = r(x, yi) mean (cid:0){r(x, yi)}G std (cid:0){r(x, yi)}G (cid:1) i= i=1 (cid:1) . (3) Based on GRPO, Decoupled Clip and Dynamic sampling Policy Optimization (DAPO) (Yu et al., 2025) removes the KL divergence regularization and introduces the clip-higher and dynamic sampling with token-level loss, further improving the training stability and performance for LLMs. Specifically, DAPO optimizes the following objective for policy optimization: JDAPO(θ) = xD,{yi}G 1 i=1 yi (cid:80)G i=1πθold(x) yi (cid:88) (cid:88) min i=1 t=1 (cid:16) ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵlow, 1 + ϵhigh) ˆAi,t (cid:17) , (4) s.t. 0 < {yiis equivalent(y, yi)} < G, where ϵlow and ϵhigh are the low and high clipping range for the importance ratio respectively, and is the correct answer. Based GRPO, Group Sequence Policy Optimization (GSPO) (Zheng et al., 2025) uses sequencelevel importance ratio to replace the original token-level importance ratio to match the sentence-level reward in the generation task and optimization objective, thus achieving remarkable improvements. Specifically, GSPO optimizes the following objective for policy optimization: JGSPO(θ) = xD,{yi}G i=1πθold(x) (cid:34) 1 G (cid:88) i=1 (cid:16) min si(θ) ˆAi, clip(si(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:35) (cid:17) , (5) where the group-based advantage estimation and importance ratio are given by ˆAi = r(x, yi) mean (cid:0){r(x, yi)}G std (cid:0){r(x, yi)}G (cid:1) i=1 i=1 (cid:1) , si(θ) = (cid:18) πθ(yix) πθold(yix) (cid:19) 1 yi . (6)"
        },
        {
            "title": "3 VARIANCE-BASED CURRICULUM REINFORCEMENT LEARNING",
            "content": "In this section, we introduce Variance-based Curriculum Reinforcement Learning (VCRL), shown in Figure 1. First, we explain Variance-based Dynamic Sampling and how it helps identify the difficulty and value of training samples. Next, we combine Replay Learning with memory bank to focus training on high-value samples, which improves RL training efficiency and stability. 3.1 VARIANCE-BASED DYNAMIC SAMPLING As discussed above, existing rollout-based RL methods do not properly match model capabilities with sample difficulty during training. This problem mainly shows up in two ways:"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: An illustration of the proposed VCRL method. For rollout-based RL training, VCRL first calculates our proposed for each querys rollout results and filters queries based on their p. VCRL then uses the existing memory bank to update and add training samples. Finally, VCRL performs the standard RL update for this training batch. 1. Dynamic Model Parameters: During training, gradient backpropagation is performed using the objective function calculated from the training samples. This updates the model parameters to improve its performance on current samples. Model parameters keep changing, so the model may perform differently on the same samples at different stages of training. 2. Unordered Sample Difficulty: For most training datasets and algorithms, the difficulty of training samples is not considered. Some tasks, like search (Feng et al., 2025b; Hao et al., 2025) and tool use (Shen, 2024; Lu et al., 2025), are hard to define by difficulty. Also, sorting samples by difficulty requires lot of data preprocessing. As result, most datasets include samples that are not ordered by difficulty. Dynamic model parameters and unordered sample difficulty make it too expensive and hard to use ordered training samples based on predefined difficulty. Samples that are hard for the model early in training often become easier later. So, the indicator of training sample difficulty must be adjusted dynamically as the model changes. Multiple rollouts for the same query can help measure how hard training sample is for the current model. Formally, for query x, if it is too easy for model πθ, then Eyπθ(x) [r(x, y)] 1. If is too hard, then Eyπθ(x) [r(x, y)] 0. Both easy and hard samples have low group reward variance. So, we can use the variance of group rewards to pick samples that are better suited for the current model. Samples with higher variance are neither too easy nor too hard, meaning the difference between the probabilities of positive and negative outcomes is small: (cid:12) (cid:12)Pyπθ(x) [r(x, y) = 1] Pyπθ(x) [r(x, y) = 0](cid:12) (cid:12) < ε. In RLVR, for the binary reward distribution, the group variance for the query is given by Varyπθ(x)(r(x, y)) = Eyπθ(x) (cid:2)(r(x, y) Eyπθ(x) [r(x, y)])2(cid:3) . 4 (7) (8)"
        },
        {
            "title": "Preprint",
            "content": "If there are rollouts with reward of 1, the unbiased estimator of the group variance can be written as (cid:88) i=1 (cid:88) (cid:34) r(x, yi) (cid:20) r(x, yi) (cid:35)2 r(x, yi) (cid:88) i=1 (cid:21)"
        },
        {
            "title": "1\nG",
            "content": "k σ2 ="
        },
        {
            "title": "1\nG − 1",
            "content": "= ="
        },
        {
            "title": "1\nG − 1",
            "content": "i=1 k(G k) G(G 1) . When = (cid:4) 2 (cid:5), the maximum value of the estimator is σ2 max = (cid:26) 4(G1) , is even, G+1 is odd. 4G , (9) (10) Obviously, the group variance cannot exceed σ2 max in any case, so we can use the normalized group variance = σ2 to measure the value of the current query for the model πθ. Training with σ2 samples that have high helps the model learn areas where it is less skilled, which improves the model more effectively than using unordered samples. See Appendix Section for more discussion. max 3.2 REPLAY LEARNING Based on the normalization value discussed above, we can dynamically sample queries during training using threshold rules. This helps ensure that each training sample has high value for the model. For unordered training datasets, each sampled query can only obtain its value after long rollout, so we use variance-based dynamic sampling. Calculating for each training sample requires significant computational resources and time, which can be expensive if used only for sampling. To address this, we propose building high-value memory bank using and maintaining it with momentum update method. This lets us apply curriculum learning with data replay based on group variance, as shown in Algorithm 1. Specifically, each time we sample from the training set D, we get query batch {xj}B j=1, where is the batch size. First, we get the corresponding response set {yj,i}G i=1, then calculate pj for each query xj. If pj κ, where κ [0, 1] is predefined threshold, we keep the query xj. Otherwise, we remove it from the batch and perform variance-based dynamic sampling. i=1 and reward set {r(xj, yj,i)}G Suppose queries are removed from batch of B. To keep the batch size unchanged, we replace the missing queries by sampling queries from the memory bank M. The memory bank is implemented as priority queue, where each entry is query xj, and the priority (xj) is updated based on momentum and the number of steps since it was last accessed, β(xj): (xj) αP (xj) + (1 α)β(xj), (11) where α is the momentum constant and the (xj) is initialized using pj. The proposed VCRL based on GRPO optimizes the following objective for policy optimization: JVCRL(θ) = 1 xDM,{yi}G (cid:16) i=1πθold(x) (cid:17) (cid:88) i= pi = σ2 σ2 yi i,max κ (cid:16) min yi (cid:88) t=1 ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) , (12) where the calculation of pi and the memory bank mechanism are as described above, and I() is the indicator function. See Appendix Section for comparison of theoretical perspecitves on GRPO and VCRL."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: VCRL: Variance-based Curriculum Reinforcement Learning Require: Training Set D, Reward Verifier r, p-threshold κ, Policy Model πθ, Momentum Constant α, Training Batch Size B, Rollout Group Size i=1 Remove xj from Training Batch + 1 i=1 πθ(xj) j=1 D, 1: Initialize PriorityQueue() 2: while Training do Sample {xj}B 3: for = 1 to do 4: Sample {yj,i}G 5: Calculate Reward {r(xj, yi,j)}G 6: Calculate pj for xj 7: if pj < κ then 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end if 23: end for 24: 25: end while β(x) β(x) + 1 (x) αP (x) + (1 α)β(x) Calculate for if κ then end if end for Pop queries from and add them to the Training Batch for do end for Apply RL update using the Augmented Training Batch for do Push into with priority (x) = and β(x) ="
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Benchmarks. In this work, we focus specifically on mathematical reasoning tasks to evaluate our VCRL algorithm. For mathematical reasoning tasks, we use AIME-20242, AIME-20253, MATH500 (Lightman et al., 2024), OlympiadBench (He et al., 2024), and AMC234. Among them, AIME-2024 and AIME-2025 are used as high-difficulty benchmarks to effectively evaluate the performance of VCRL and other baseline RL methods in multiple difficulty levels. Implementation Details. For training dataset, we use DAPO-Math-17K5 to improve training stability, which consists of 17K prompts, each paired with an interger as the answer. We implement VCRL and conduct all experiments based on the verl (Sheng et al., 2025) framework. For hyperparameters, we utilize the AdamW (Loshchilov & Hutter, 2019) optimizer with constant learning rate of 1 106. For rollout, the prompt batch size is = 128 and we sample = 16 responses for each prompt. For training, we train 500 steps to ensure convergence. The maximum number of tokens for generation is set to 4,096 tokens. For evaluation on benchmarks, we repeat the evaluation set for 16 times and report avg@16 for the stability of the results. The inference hyperparameters of evaluation are set to temperature 0.6 and top-p 0.95. For VCRL, we set the variance threshold κ to 0.3 in first 20 steps and 0.8 in remaining steps, and the momentum constant α is set to 0.9. We implement VCRL based on GRPOs RL update. For memory bank, we allow up to 2 replays for the same sample to ensure the diversity of training sample. We conduct all experiments on server with 8NVIDIA H20-3e GPUs and an Intel Xeon Platinum 8575C CPU. 2https://huggingface.co/datasets/Maxwell-Jia/AIME_2024 3https://huggingface.co/datasets/yentinglin/aime_2025 4https://huggingface.co/datasets/AI-MO/aimo-validation-amc 5https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Main performance comparison of VCRL against other RL baselines on Qwen3 models. Method AIME-2024 AIME-2025 MATH500 OlympiadBench AMC23 Avg. Base Model + GRPO + DAPO + GSPO + VCRL Base Model + GRPO + DAPO + GSPO + VCRL 9.58 15.63 14.79 14.58 23. 10.83 23.13 22.08 27.29 34.38 Starting from Qwen3-4B-Base 4.79 12.92 12.29 10.42 22.71 56.69 80.78 79.86 79.90 86.48 Starting from Qwen3-8B-Base 10.00 21.88 20.42 22.92 27. 68.75 86.94 87.14 89.23 91.99 27.27 45.39 44.23 44.38 53.24 34.10 54.02 53.52 56.75 60.21 35.09 54.07 51.81 51.13 60.77 41.11 65.29 64.01 69.28 75.15 26.68 41.76 40.60 40.08 49. 32.96 50.25 49.43 53.09 57.76 Baselines and Models. We mainly use GRPO (Shao et al., 2024), DAPO (Yu et al., 2025) and GSPO (Zheng et al., 2025) as the baselines for our VCRL comparison. For Clip-Higher mechanism in DAPO, we set the clipping parameter ϵlow to 0.2 and ϵhigh to 0.28, which is aligned with the DAPO setting in the original paper. For GSPO, we set the clipping parameter ϵ to 0.0003. For models, we use the Qwen3 (Yang et al., 2025a) series models for training, including Qwen3-4B-Base and Qwen3-8B-Base. 4.2 MAIN RESULTS We conduct comprehensive evaluation of our proposed method, VCRL, against several strong LLM RL baselines on diverse suite of mathematical reasoning benchmarks. As detailed in Table 1, the experiments are performed on two models, Qwen3-4B-Base and Qwen3-8B-Base, to assess the scalability and generalizability of our method. The results unequivocally demonstrate the superiority of VCRL. Across all five benchmarks and on both model sizes, VCRL consistently achieves state-ofthe-art performance, outperforming all baseline methods, including GRPO, DAPO, and GSPO. This consistent dominance, indicated by the bolded scores, highlights the robustness and effectiveness of our proposed methodology. deeper analysis reveals the substantial performance gains enabled by VCRL. For instance, on the Qwen3-8B-Base model, VCRL achieves an average score of 57.76, significant margin of over 4.67 points above the strongest baseline, GSPO (53.09), and remarkable 24.8 points improvement over the base model. This trend holds for the Qwen3-4B-Base model, where VCRL elevates the average performance from 26.68 (Base Model) to 49.43, far surpassing the gains from other RL techniques. Notably, the performance leap is particularly pronounced on highly challenging, competition-level datasets such as AIME-2024 and AIME-2025, suggesting that VCRL is exceptionally proficient at unlocking the complex, multi-step reasoning capabilities essential for advanced mathematical problem-solving. These empirical findings strongly validate VCRL as superior alignment strategy for enhancing the mathematical reasoning prowess of LLMs. 4.3 PERFORMANCE TREND During RL training, the LLM starts with low ability and steadily improves, showing an upward trend on benchmark tests. To illustrate how VCRL compares to baseline methods during training, we show how model performance changes with training steps on each benchmark, as seen in Figure 2 for Qwen3-4B-Base and Figure 3 for Qwen3-8B-Base. For performance trend, the results clearly demonstrate that VCRL consistently and significantly outperforms all other baseline methods across all benchmarks. In terms of the speed of performance improvement, VCRL also has considerable advantage. In the first 100 training steps, VCRLs performance increases quickly, with its curve staying above the other methods. This is likely due to VCRLs control of high-p training samples in the early stages of model training, which improves training efficiency. Later in training, the performance of all methods generally converges, but VCRL"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: The performance curve of Qwen3-4B-Base on the five benchmarks using various RL methods over training steps. Figure 3: The performance curve of Qwen3-8B-Base on the five benchmarks using various RL methods over training steps. still achieves significantly better final results than the RL baselines. This demonstrates VCRLs strong competitiveness. More training dynamics are in Appendix Section A. 4.4 ABLATION STUDY To verify the effectiveness of the two core components of our proposed VCRL, we conduct the ablation study, as shown in Table 2. Starting from the Qwen3-4B-Base, our Naive GRPO baseline improves the average score from 26.68 to 41.76. The integration of Variance-based Dynamic Sampling further pushes this score to 44.73. Finally, the inclusion of Replay Learning achieves the best performance of 49.43, showing the largest marginal gain. This consistent trend on the larger Qwen3-8B-Base model robustly validates the positive impact of each component within our VCRL framework."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Ablation study of the key components of our proposed method VCRL. Starting from Naive GRPO baseline, we incrementally add Variance-based Dynamic Sampling and Replay Learning. The results on both Qwen3-4B-Base and Qwen3-8B-Base models show that each component contributes positively to the final performance, validating their effectiveness. Model Qwen3-4B-Base w/ Naive GRPO w/ Variance-based Dynamic Sampling w/ Replay Learning Qwen3-8B-Base w/ Naive GRPO w/ Variance-based Dynamic Sampling w/ Replay Learning Avg. 26.68 41.76 44.73 49.43 32.96 50.25 52.67 57."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Recent work on using RL methods with LLMs has greatly improved their ability to handle complex tasks. DeepSeek-R1 (Guo et al., 2025) introduces zero RL training framework, which directly trains the base LLM using simple rule-based reward model. Many RL methods have built on this idea to further boost LLM performance. Some approaches use novel RL mechanisms to make training more efficient and stable. DAPO (Yu et al., 2025) analyzes GRPOs training and applies four main techniques to improve RL efficiency. Dr. GRPO (Liu et al., 2025) removes the output length and standard deviation terms from GRPOs relative advantage, which increases token efficiency without hurting reasoning performance. SimpleRL-Zoo (Zeng et al., 2025) runs experiments on different base models and sizes to map out behavioral patterns and suggest future improvements. LUFFY (Yan et al., 2025) enhances RLVR with off-policy reasoning traces, helping to balance imitation and exploration by combining off-policy demonstrations with on-policy rollouts. VAPO (Yue et al., 2025) introduces the first value-model-based RL training framework built on PPO, with seven new techniques to improve training stability and performance. Yeo et al. (2025) investigates how RL helps models create longer reasoning chains, showing which factors matter most for extended CoT reasoning. PVPO (Feng et al., 2025c) presents an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Other work explores curriculum learning in LLM training for better results. Hammoud et al. (2025) improve GRPO with reward function that balances task correctness (via verifier feedback), length efficiency, and formatting (using structural tags), leading to higher accuracy and better token efficiency. Feng et al. (2025a) propose self-adaptive curriculum that picks fine-tuning examples based on difficulty scores predicted by pre-trained models. Shen et al. (2025) introduce TTI (Test-Time Interaction), an online RL method that adapts rollout lengths using curriculum approach. Parashar et al. (2025) provide convergence guarantees for easy-to-hard training within an approximate policy iteration framework. RAGEN (Wang et al., 2025b) introduces uncertainty-based filtering to maintain high training efficiency based on active learning (Settles, 2009). PODS (Xu et al., 2025b) generates numerous rollouts in parallel but updating only on informative subset. Curr-ReFT (Deng et al., 2025) explores the Out-of-Distribution generalization on small-scale Vision Language Models based on the curriculum learning framework. Xi et al. (2024) introduce novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models with step-wise curriculum. RLVR (Mroueh, 2025) is promising method for boosting reasoning in LLMs, especially in areas like math and programming (Jiang et al., 2025b). Gandhi et al. (2025) show that reasoning behaviorsnot just correct answersdrive RL performance gains. Li et al. (2025) find that the structure of long chains of thought is key for learning, while the details of each reasoning step matter less. Vassoyan et al. (2025) identify critical tokens in CoTs, which are decision points where models often make mistakes, and suggest increasing exploration around these tokens by changing the KL penalty. Lin et al. (2024) also find tokens that lead to errors and show that changing them can shift model behavior."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose VCRL, curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. By introducing Dynamic Variance Sampling, VCRL can filter out samples in the training batch that are moderately difficult for the current training model and remove samples that are too difficult or too easy, thereby improving training efficiency. By introducing Replay Learning, VCRL uses memory bank to maintain the high-p samples in the training batch, further improving training stability. By carefully controlling the difficulty of training samples, VCRL achieves state-of-the-art results on five math benchmarks compared to LLM RL baselines. Further analysis of training dynamics and ablation study also confirm VCRLs effectiveness."
        },
        {
            "title": "REFERENCES",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. arXiv preprint arXiv:2503.07065, 2025. Qi Feng, Yihong Liu, and Hinrich Schutze. Your pretrained model tells the difficulty itself: self-adaptive curriculum learning paradigm for natural language understanding. arXiv preprint arXiv:2507.09758, 2025a. Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Guochao Jiang, Jingyi Song, and Hao Wang. Airrag: Autonomous strategic planning and reasoning steer retrieval augmented generation. arXiv preprint arXiv:2501.10053, 2025b. Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang, and Hao Wang. arXiv preprint Pvpo: Pre-estimated value-based policy optimization for agentic reasoning. arXiv:2508.21104, 2025c. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, and Bernard Ghanem. Train long, think short: Curriculum learning for efficient reasoning. arXiv preprint arXiv:2508.08940, 2025. Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, and Hao Wang. Dynasearcher: Dynamic knowledge graph augmented search agent via multi-reward reinforcement learning. arXiv preprint arXiv:2507.17365, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual"
        },
        {
            "title": "Preprint",
            "content": "multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 38283850. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211. Guochao Jiang, Ziqin Luo, Yuchen Shi, Dixuan Wang, Jiaqing Liang, and Deqing Yang. Toner: Type-oriented named entity recognition with generative language model. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pp. 1625116262. ELRA and ICCL, 2024. URL https://aclanthology.org/2024.lrec-main.1412. Guochao Jiang, Ziqin Luo, Chengwei Hu, Zepeng Ding, and Deqing Yang. Mitigating out-ofentity errors in named entity recognition: sentence-level strategy. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert (eds.), Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pp. 77547765. Association for Computational Linguistics, 2025a. URL https://aclanthology.org/2025.coling-main.519/. Guochao Jiang, Guofeng Quan, Zepeng Ding, Ziqin Luo, Dixuan Wang, and Zheng Hu. Flashthink: An early exit method for efficient reasoning. arXiv preprint arXiv:2505.13949, 2025b. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan In The Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= v8L0pN6EOi. Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhances llms reasoning capability. arXiv preprint arXiv:2411.19943, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Haoping Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming Pang. Toolsandbox: stateful, conversational, interactive evaluation benchmark for LLM tool use capabilities. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 11601183. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.FINDINGS-NAACL.65. URL https://doi.org/10.18653/v1/2025.findings-naacl.65. Youssef Mroueh. Reinforcement learning with verifiable rewards: Grpos effective loss, dynamics, and success amplification. arXiv preprint arXiv:2503.06639, 2025. Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, et al. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632, 2025. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In Yoshua Bengio and"
        },
        {
            "title": "Preprint",
            "content": "Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http: //arxiv.org/abs/1506.02438. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Burr Settles. Active learning literature survey. 2009. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, et al. Thinking vs. doing: Agents that reason by scaling test-time interaction. arXiv preprint arXiv:2506.07976, 2025. Zhuocheng Shen. Llm with tools: survey. arXiv preprint arXiv:2409.18807, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, In ProHaibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. ceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pp. 12791297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https://doi.org/10.1145/3689031.3696075. Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: survey. Int. J. Comput. Vis., 130(6):15261565, 2022. doi: 10.1007/S11263-022-01611-X. URL https: //doi.org/10.1007/s11263-022-01611-x. Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. IEEE Trans. Neural Networks, 9(5):10541054, 1998. doi: 10.1109/TNN.1998.712192. URL https:// doi.org/10.1109/TNN.1998.712192. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Jean Vassoyan, Nathanael Beau, and Roman Plaud. Ignore the KL penalty! boosting exploration on critical tokens to enhance RL fine-tuning. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 61086118. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.FINDINGS-NAACL.340. URL https://doi.org/10. 18653/v1/2025.findings-naacl.340. Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, XuHui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, et al. survey on large language models for mathematical reasoning. arXiv preprint arXiv:2506.08446, 2025a. Xin Wang, Yudong Chen, and Wenwu Zhu. survey on curriculum learning. IEEE Trans. Pattern Anal. Mach. Intell., 44(9):45554576, 2022. doi: 10.1109/TPAMI.2021.3069908. URL https: //doi.org/10.1109/TPAMI.2021.3069908. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025b. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, and Xuanjing Huang. Training large language models for reasoning through reverse curriculum reinforcement learning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=t82Y3fmRtk."
        },
        {
            "title": "Preprint",
            "content": "Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025a. Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. Not all rollouts are useful: Down-sampling rollouts in llm reinforcement learning. arXiv preprint arXiv:2504.13818, 2025b. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, et al. Code to think, think to code: survey on code-enhanced reasoning and reasoning-driven code intelligence in llms. arXiv preprint arXiv:2502.19411, 2025b. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547, 2025a. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, et al. survey on test-time scaling in large language models: What, how, where, and how well? arXiv preprint arXiv:2503.24235, 2025b. Zikang Zhang, Wangjie You, Tianci Wu, Xinrui Wang, Juntao Li, and Min Zhang. survey of generative information extraction. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert (eds.), Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pp. 48404870. Association for Computational Linguistics, 2025c. URL https://aclanthology.org/2025.coling-main.324/. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025."
        },
        {
            "title": "A TRAINING DYNAMICS",
            "content": "Compared to GRPO, VCRL introduces two main techniques to improve training efficiency. To further understand their effects, we show the training dynamics shown in Figure 4, including reward score, response length, and entropy. For the reward score curve, in order to simultaneously measure their stability in training dynamics, we use moving average and rolling standard deviation with window size of 20 for visualization."
        },
        {
            "title": "Preprint",
            "content": "(a) Qwen3-4B-Base (b) Qwen3-8B-Base (c) Qwen3-4B-Base (d) Qwen3-8B-Base (e) Qwen3-4B-Base (f) Qwen3-8B-Base Figure 4: The metric curves of reward score, response length, and entropy of VCRL over GRPO based on Qwen3-4B-Base and Qwen3-8B-Base, which show the dynamics of RL training and serve as essential monitoring indicators to identify potential issues. Reward Score during training is closely linked to training stability and performance, as shown in Figure 4a and Figure 4b. For both VCRL and GRPO, the reward score rises quickly in the early stages and then slowly improves. For Qwen3-4B-Base, before about 270 training steps, VCRLs reward score is much higher than GRPOs. For Qwen3-8BBase, the reward score of VCRl is significantly higher than that of GRPO throughput the training process. Once the reward score stabilizes, VCRL shows much smaller fluctuations than GRPO, as seen in the shaded areas. This highlights VCRLs advantage in training stability. Response Length relates to how much the model can explore, as shown in Figure 4c and Figure 4d. Longer responses help the model develop more complex reasoning during training and boost performance. In the first 100 steps, VCRL and GRPO both show rapid increase in response length, then level off and fluctuate. VCRLs response length grows much faster early on, especially in first 50 steps, due to the training of high-p samples. Af-"
        },
        {
            "title": "Preprint",
            "content": "ter stabilizing, VCRL maintains noticeably longer responses, giving the model more room to explore and optimize its performance. Entropy shows how uncertain the model is in its generation ability, as seen in Figure 4e and Figure 4f. For efficient training, entropy should stay at reasonable level. If entropy is too low, the model becomes too deterministic and loses its ability to explore. For GRPO, entropy quickly drops below 0.1 within 50 steps and stays very low. In contrast, VCRL keeps entropy at reasonable level throughout training, which encourages the model to keep exploring."
        },
        {
            "title": "B VARIANCE AS A DIFFICULTY METRIC",
            "content": "Figure 5: The curve showing how = σ2 σ2 on group size = 16. max changes with the number of successful rollouts based Compared with generation entropy, it is more reasonable to use group variance to measure the difficulty of the current sample for the current training model in VCRL or GRPO. Group reward variance is grounded in its unique ability to identify samples at the cusp of the models current capabilities. For binary reward system (correct/incorrect), variance exhibits non-monotonic, U-shaped relationship with sample difficulty, as shown in Figure 5. Low variance occurs at two extremes. If sample is too easy, the model consistently succeeds (e.g., all 16 rollouts get reward of 1), leading to near-zero variance. If sample is too hard, the model consistently fails (all rewards are 0), also leading to near-zero variance. Peaks when the models success rate is approximately 50% (e.g., 8 rollouts succeed and 8 fail). This indicates maximum uncertainty and signifies that the sample is at the precise frontier of the models ability. While related to uncertainty, policy generation entropy measures the diversity of the models actions (tokens). High entropy could mean the model is exploring, but it does not directly map to task-level success. model could be highly uncertain (high entropy) while generating non-sensical responses that all lead to reward of 0. Variance, on the other hand, is directly related to the final outcome of the task (the reward), making it more direct measure of the difficulty relevant to learning. By using single indicator of group variance, it is possible to filter samples with high uncertainty results, while this task is difficult to accomplish based on the generation entropy."
        },
        {
            "title": "C POLICY GRADIENT REDUCTION",
            "content": "According to Equation 2 and Equation 12, we give the following theorem: Theorem 1. For policy gradient algorithm GRPO and VCRL, from the policy gradient norm perspective, the training of VCRL is more stable than that of GRPO in the expectation, that is, EVCRL [θ log πθ] EGRPO [θ log πθ]."
        },
        {
            "title": "Preprint",
            "content": "Proof. We first give the gradient form of the GRPO objective function (clipping is omitted for brevity) with Policy Gradient Theorem (Sutton & Barto, 1998): θJGRPO(θ) = θE xD,{yi}G i=1πθold(x)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 ri,t(θ) ˆAi,t = xD,{yi}G i=1πθold (x)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 ri,t(θ) ˆAi,tθ log πθ(yi,tx, yi,<t) , (13) where ri,t(θ) = πθ(yi,tx,yi,<t) πθold (yi,tx,yi,<t) is the importance sampling ratio. We can also derive the gradient of the VCRL objective as follows: θJVCRL(θ) = θE xDM,{yi}G i=1πθold(x) (cid:16)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 pi = σ2 σ2 yi i,max (cid:17) κ ri,t(θ) ˆAi,t yi (cid:88) t= = 1 xDM,{yi}G (cid:16) i=1πθold(x) (cid:17) G (cid:88) i=1 pi = σ2 σ2 yi i,max κ ri,t(θ) ˆAi,tθ log πθ(yi,tx, yi,<t) . (14) yi (cid:88) t=1 To align the gradients of the two, we use importance sampling to rewrite the gradient of VCRL to remove the term of memory bank M: θJVCRL(θ) = xD,{yi}G 1 (cid:88) i=1 1 yi i=1πθold(x) yi (cid:88) ri,t(θ) ˆAi,t t=1 P(x M) P(x D) I(pi κ)θ log πθ(yi,tx, yi,<t) . (15) Note that the blue part in the Equation 15 is the key to affecting the contribution of the policy gradient term to the overall gradient. We simplify the policy gradient terms in Equation 13 and Equation 15 into the following form for comparison: EGRPO [θ log πθ] = EVCRL [θ log πθ] = xD,{yi}G i=1πθold(x) [θ log πθ(yi,tx, yi,<t)] , (16) i=1πθold(x) xD,{yi}G P(x M) P(x D) (cid:20)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) I(pi κ)θ log πθ(yi,tx, yi,<t) (cid:13) (cid:13) (cid:21) . (17) For the training sample x, the sampling of the event is uniform, so P(x D) = 1 . And according to the nature of sampling probability, we can get P(x M) P(x D). Based on the value range of the indicator function, we can also get I(pi κ) 1. Using the homogeneity of the norm and above results: (cid:13) P(x M) (cid:13) (cid:13) P(x D) (cid:13) P(x M) P(x D) I(pi κ) θ log πθ(yi,tx, yi,<t) I(pi κ)θ log πθ(yi,tx, yi,<t) (cid:13) (cid:13) (cid:13) (cid:13) = I(pi κ) θ log πθ(yi,tx, yi,<t) θ log πθ(yi,tx, yi,<t) , which completes the proof."
        },
        {
            "title": "Preprint",
            "content": "(a) Qwen3-4B-Base (b) Qwen3-8B-Base Figure 6: The training dynamics of objective gradient norm θJ (θ) of VCRL over GRPO based on Qwen3-4B-Base and Qwen3-8B-Base. Theorem 1 provides theoretical guarantee for the training stability of VCRL compared to GRPO. To further illustrate the training stability of VCRL from the perspective of gradient norm, we show the training dynamics as shown in the Figure 6. Figure 6 provides an empirical validation of our proposed VCRLs stability by visualizing the norm of the objective functions gradient, θJ (θ), over the training steps. We compare VCRL against the GRPO baseline on two model scales: Qwen3-4B-Base and Qwen3-8B-Base. The empirical results unequivocally demonstrate the superiority of VCRL in maintaining well-behaved optimization trajectory. Specifically, VCRLs gradient norm remains consistently confined to lower and narrower band, indicating that the policy updates are more measured and stable. Furthermore, the VCRL curve is notably smoother, with significantly fewer and less pronounced transient spikes compared to the GRPO baseline. The frequent, high-magnitude oscillations observed in GRPOs gradient norm are indicative of more challenging optimization landscape, which can lead to inefficient and unstable training. We posit that the demonstrably smaller and more stable gradient norm engendered by VCRL is an important contributor to its enhanced training efficiency and robust performance."
        }
    ],
    "affiliations": [
        "Alibaba Cloud Computing"
    ]
}