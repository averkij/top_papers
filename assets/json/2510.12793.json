{
    "paper_title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
    "authors": [
        "Long Cui",
        "Weiyun Wang",
        "Jie Shao",
        "Zichen Wen",
        "Gen Luo",
        "Linfeng Zhang",
        "Yanting Zhang",
        "Yu Qiao",
        "Wenhai Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research."
        },
        {
            "title": "Start",
            "content": "ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution VICO: TRAINING STRATEGY TOWARDS SEMANTIC AWARE DYNAMIC HIGH-RESOLUTION Long Cui1,2, Weiyun Wang3,2, Jie Shao4,2, Zichen Wen1,2, Gen Luo2 Linfeng Zhang1, Yanting Zhang5, Yu Qiao2, Wenhai Wang6,2 1Shanghai Jiao Tong University 3Fudan University 6The Chinese University of Hong Kong 4Nanjing University 2Shanghai Artificial Intelligence Laboratory 5Donghua University 5 2 0 2 4 1 ] . [ 1 3 9 7 2 1 . 0 1 5 2 : r Model Weights: InternVL3.5-Flash Collection"
        },
        {
            "title": "ABSTRACT",
            "content": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the models perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in Multimodal Large Language Models (MLLMs) (Zhu et al., 2025; Chen et al., 2024c; Xiaomi, 2025; Kwai Keye et al., 2025; Anthropic, 2025; DeepMind, 2025; Wang et al., 2024b) have demonstrated remarkable performance across wide range of tasks, showing tremendous potential for real-world applications. Despite these advancements, MLLMs still suffer from the increased inference costs due to the additional vision tokens introduced by image inputs. Taking InternVL3.5 (Wang et al., 2025b) as an example, under its default configuration (Ye et al., 2023), each image is divided into up to 13 patches (including one thumbnail). Each patch is then represented by 256 visual tokens, resulting in maximum of 3,328 tokens per image. In real-world scenarios such as document understanding or video comprehension, models are often required to process multiple images simultaneously. In such cases, the visual component will serve as the main body of the token sequence and constitutes the primary source of inference cost. To address these challenges, we propose Visual Consistency Learning (ViCO), which introduces semantic-level adaptivity in the number of visual tokens. As shown in Figure 1(a), given an image feature map, we introduce Visual Resolution Router (ViR) that routes each patch to different compression rates: either high-resolution representation with 256 tokens or low-resolution representation with 64 tokens. These tokens are then concatenated with text tokens. The training procedure of ViCO consists of two stages: (1) Consistency Training: The model is trained to minimize the KL divergence between the responses conditioned on visual tokens with different compression rates. This Corresponding authors. 1 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution (a) (b) Figure 1: Overall view of the ViCO pipeline and model performance. (a) The ViCO inference pipeline, illustrating the image processing flow. (b) Bubble chart showing model performance on general benchmarks versus first token throughput of LLM. Bubble size is positively correlated with the number of model parameters. First token throughput is reported relative to InternVL3.5-38B, which is set as the baseline value of 1. Detailed experimental settings are provided in Section 4.6. encourages the model to generate accurate responses even when using highly compressed visual tokens, thereby improving its performance and robustness under compressed visual tokens. (2) Router Training: Although consistency training improves performance under high compression, inevitable information loss still causes drop in performance. To mitigate this, we introduce ViR automatically selects the appropriate compression rate for each image patch. Those containing complex semantic information are represented using more tokens, while simpler patches are represented using fewer tokens. Compared with existing dynamic high-resolution strategies (Chen et al., 2024d; Wang et al., 2024a), which adjust the number of visual tokens based on image resolutions, our method further determines the number of visual tokens allocated for each image patch at the semantic level. This fine-grained control enables substantial efficiency gains with minimal performance loss. To evaluate the effectiveness of our method, we conducted extensive experiments on benchmarks spanning OCR, document understanding, video understanding, and multi-image reasoning. Experimental results show that our method can reduce the number of visual tokens by up to 50% while preserving the models perception, reasoning, and OCR capabilities. As shown in Figure 1(b), our method maintains the original performance while improving the first token throughput of InternVL3.5 series (Wang et al., 2025b) across different model scales. Our main contributions are as follows: (1) We introduce novel training strategy, termed Visual Consistency Learning, which minimizes the response distribution gap of the model between different visual token compression rates. This enables the model to effectively utilize highly compressed visual representations without significant performance loss. (2) Building on ViCO, we develop Visual Resolution Router that dynamically allocates visual tokens to image patches based on their semantic complexity. This provides fine-grained control over image patch representation and achieves better trade-off between efficiency and accuracy. (3) We perform large-scale experiments on benchmarks covering diverse multimodal recognition and reasoning tasks. Our results demonstrate that our method can halve the number of visual tokens while maintaining strong performance, which almost doubles inference throughput."
        },
        {
            "title": "2.1 MULTIMODAL LARGE LANGUAGE MODELS.",
            "content": "With the development of large language models (Yang et al., 2025; OpenAI, 2025; Ouyang et al., 2022), multimodal large language models (MLLMs) have also made remarkable progress. To leverage LLMs and vision foundation models that have been pre-trained on unimodal datasets, series 2 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution of studies (Wang et al., 2024c; Liu et al., 2023a;d; Wang et al., 2025a; Li et al., 2023a) employ connector to align the representational spaces of vision and language. This approach allows visual feature maps to be flattened and fed into LLMs as soft prompts, achieving strong performance through relatively low-cost incremental training. In addition, some works (Dubey et al., 2024; Luo et al., 2025) extend pre-trained LLMs by incorporating additional vision-language fusion layers. These layers enable the model to process tokens from different modalities with separate parameters, reducing the gradient conflicts across different modalities. However, the introduction of large number of untrained parameters also brings additional training costs. More recently, several studies (Luo et al., 2024) have explored architectures without dedicated visual encoders. These models employ unified Transformer to jointly process visual and textual information, eliminating the need for separate vision encoder and fusion layer. Although MLLMs vary in their architectural designs, most of them adopt dynamic high-resolution strategy (Chen et al., 2024c;d), which segments images into patches based on their resolution to enhance the models perceptual capabilities. However, this approach requires large number of tokens to represent each image, thereby increasing the inference cost of MLLMs. In this paper, we propose dynamic resolution strategy that is compatible with this paradigm. Our method introduces semantic-level adaptivity in determining the number of visual tokens needed to represent each image patch. This reduces the number of visual tokens and consequently the inference cost, while maintaining nearly the same performance."
        },
        {
            "title": "2.2 EFFICIENT VISION LANGUAGE MODELS.",
            "content": "Improving the efficiency of large vision-language models (LVLMs) has drawn increasing attention, with visual token compression emerging as one of the most widely explored directions. Early approaches, such as LLaMA-VID (Li et al., 2024) and DeCo (Yao et al., 2024), aim to reduce redundancy in visual inputs through context tokens or adaptive pooling mechanisms, thereby lowering computational cost while retaining essential information. Similarly, MADTP (Cao et al., 2024) further enhances efficiency by identifying redundant tokens across different modalities and selectively removing them based on feature alignment, enabling more focused processing of relevant visual features. number of lightweight, training-free methods have been proposed to reduce token redundancy without additional training. For instance, FastV (Chen et al., 2024a) prunes tokens in the LLM based on attention scores, removing low-attention tokens, while similar approaches such as SparseVLM (Zhang et al., 2024) and ToMe (Bolya et al., 2022) also reduce redundancy through token merging or selection without requiring retraining. While these strategies provide tangible computational benefits, their effectiveness can be limited on vision-sensitive tasks such as OCR, where fine-grained spatial and textual details are crucial. In such scenarios, overly aggressive token compression may lead to loss of essential visual information, highlighting the need for methods that carefully balance efficiency with the preservation of detailed visual representations."
        },
        {
            "title": "3 VISUAL CONSISTENCY LEARNING",
            "content": "To enhance recognition and perception capabilities, most existing MLLMs adopt dynamic highresolution strategy, which introduces large number of visual tokens and greatly increases the inference cost of these models. In this work, we propose Visual Consistency Learning (ViCO), novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens."
        },
        {
            "title": "3.1 CONSISTENCY TRAINING",
            "content": "As shown in Figure 2, in the Consistency Training stage, the model is trained to generate consistent output conditioned on different patch compression rates. In practice, we introduce an extra reference model, which is frozen during training. The trained model is required to minimize the KL divergence between its response distribution and that of the reference model. The training objective is formulated as follows: LViCO = EξR (cid:34)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:16) KL πθref (yi y<i, I) (cid:13) (cid:13) (cid:13) πθpolicy (yi y<i, Iξ) (cid:17) (cid:35) , (1) where KL denotes the KL divergence and ξ denotes the compression ratio of the patches in each image, which is uniformly sampled from = [0, 1]. The corresponding ratio of patches in image 3 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Figure 2: Training procedure of Visual Consistency Learning (ViCO). During the Consistency Training stage, the model aligns outputs under different compression rates. During the Router Training, the Visual Resolution Router (ViR) is trained to determine the appropriate compression for each patch based on its effect on model predictions. Iξ is represented as 64 tokens, while the others are represented as 256 tokens. The reference model always performs inference with visual tokens without any compression."
        },
        {
            "title": "3.2 ROUTER TRAINING",
            "content": "As illustrated in Figure 2, the Router Training stage focuses on training the Visual Resolution Router (ViR), which is responsible for selecting an appropriate resolution for each input patch to balance efficiency and fidelity. ViR is implemented as binary classifier and trained with standard crossentropy loss, while the main MLLM backbone remains frozen throughout this stage. To generate supervision signals for ViR, we first measure the effect of patch compression on the models predictions. Concretely, for each patch, we calculate loss ratio that quantifies how much the models output degrades under compression. This ratio then serves as guide for the router, indicating which patches can be safely compressed without significantly affecting overall performance. Specifically, for each patch, we compute loss ratio defined as ri ="
        },
        {
            "title": "LViCO\nLViCO",
            "content": "(cid:0)yi 1 (cid:0)yi 1 16 (cid:1) (cid:1) , 4 (2) where LViCO denotes the consistency loss introduced in Section 3.1. This ratio captures the relative increase in loss caused by compressing the visual tokens, providing principled measure of each patchs sensitivity to compression. Patches with low loss ratios can be safely compressed with minimal impact on the models output, whereas patches with high loss ratios require higher resolution to preserve critical visual information. The binary ground-truth label for the router is then defined based on ri: yrouter = (cid:26)0, 1, ri < τ ri τ (compression has negligible impact) (compression has significant impact), (3) where yrouter = 0 indicates that the patch can be compressed with more aggressive strategy, and yrouter = 1 indicates that the patch can be compressed with more conservative strategy. To maintain balanced training signal, we store historical ri values in sliding window and dynamically set the threshold τ as the k-th percentile of these values. This approach typically results in roughly half of the patches being assigned to compression, which ensures balanced distribution of target labels across patches. Notably, for each training sample, we randomly select patch to compress and estimate its pseudo-label according to Equation 3. The loss is only computed on this patch. 4 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution In practice, the predicted router value for each patch is obtained by first extracting visual token features from the ViT backbone, aggregating them using attention pooling, and passing the resulting patch-level feature through lightweight MLP. This process can be defined as: pi = Softmax (cid:16) MLP(cid:0)AttnPool(cid:0)ViT(Ii)(cid:1)(cid:1)(cid:17) , (4) and p1 where p0 pression rate, respectively. The ViR is trained to match the ground-truth labels yrouter cross-entropy loss. denote the predicted probabilities for the patch being assigned high or low comusing standard i"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate ViCO on InternVL3.5 models of various sizes, measuring both patch compression and performance retention relative to the original model. To validate the effectiveness of our approach across wide range of domains, experiments are conducted on diverse set of benchmarks, including general multimodal tasks (Section. 4.2.1), OCR-related benchmarks (Section. 4.2.2), and multi-image benchmarks and video benchmarks (Section. 4.2.3). OCR-related benchmarks are particularly sensitive to visual tokens, requiring fine-grained understanding of local details. Therefore, in comparative (Section. 4.3) and ablation (Section. 4.4) studies, we focus primarily on these benchmarks to provide more discriminative evaluation."
        },
        {
            "title": "4.1 EXPERIMENTAL SETTINGS",
            "content": "Benchmarks. We evaluate our approach on diverse set of multimodal benchmarks. For OCR-related tasks, we use OCRBench (Liu et al., 2023c), ChartQA (Masry et al., 2022), and TextVQA (Singh et al., 2019), with InfoVQA (Mathew et al., 2022), DocVQA (Mathew et al., 2021), and AI2D (Kembhavi et al., 2016) additionally included in comparative and ablation studies. General multimodal tasks are evaluated on MMStar (Chen et al., 2024b), POPE (Li et al., 2023b), MME (Fu et al., 2023), MMBench V1.1 (Liu et al., 2023b), and RealWorldQA (Corp., 2024), while reasoning is assessed using the MMMU (Yue et al., 2023) benchmark. Multi-image understanding is measured on BLINK (Fu et al., 2024b), MMT-Bench (Ying et al., 2024), and MMIU (Ying et al., 2024), and video comprehension is tested on Video-MME (Fu et al., 2024a) under two settings: with subtitles and without subtitles. Training Details. Our experiments are conducted in two stages: consistency learning and router learning. In the consistency learning stage, we adopt maximum text length of 32K and use global batch size of 256. The learning rate follows cosine decay schedule from 4 106 to 1 107, and we use the AdamW optimizer. Additionally, we perform warm-up for the newly initialized 16 downsampling MLP. In the router learning stage, only the lightweight router is unfrozen for training, with the global batch size adjusted to 8. The routing threshold τ is set to the 60th percentile of the predicted scores. All experiments are conducted on H200 GPUs."
        },
        {
            "title": "4.2.1 RESULTS ON GENERAL BENCHMARKS",
            "content": "Table 1 presents the overall performance of InternVL3.5 models of different sizes (4B, 8B, 14B, 30B-MoE, 38B, and 241B-MoE) on wide range of benchmarks, including OCRBench, ChartQA, TextVQA, MMStar, POPE, MME, MMBench V1.1, RealWorldQA, and MMMU. We compare the original models with their ViCO counterparts. Across these benchmarks, each ViCO model retains over 99.6% of the original performance on average. Across all model sizes, the average retention is about 99.7%. For instance, the 8B model with ViCO achieves nearly identical scores to its baseline on POPE (88.7 vs. 88.4) with about 81% patch compression, together with strong results on MMStar, MMBench V1.1, and RealWorldQA, yielding an overall average of 99.6%. Similarly, the 38B and even the largest 241B-MoE models preserve strong performance across representative benchmarks, both maintaining around 99.6% relative to their original versions.Notably, for the MMMU, we adopt the thinking mode, which supports outputs up to 64K tokens. Even with such long outputs, the performance remains almost unchanged when using ViCO, further validating the methods stability and robustness. Overall, this high level of 5 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Table 1: Performance of ViCO on InternVL3.5 models across general benchmarks, including strongly OCR-related benchmarks. When calculating the overall score, MME is normalized from 02800 to 0100. RWQA refers to RealWorldQA. Model OCRBench ChartQA TextVQA MMStar POPE MME MMBench V1.1 RWQA MMMU Avg. InternVL3.5-4B + ViCO Ratio of Compressed Patches InternVL3.5-8B + ViCO Ratio of Compressed Patches InternVL3.5-14B + ViCO Ratio of Compressed Patches InternVL3.5-30B-MoE + ViCO Ratio of Compressed Patches InternVL3.5-38B + ViCO Ratio of Compressed Patches InternVL3.5-241B-MoE + ViCO Ratio of Compressed Patches 82.2 83.0 73% 84.0 83.9 71% 83.2 84.3 71% 88.1 87.7 71% 88.0 87.8 67% 91.1 90.7 73% 86.4 85.2 62% 86.5 86.7 47% 86.1 86.2 44% 87.5 87.4 62% 88.9 88.8 32% 88.6 88.3 55% 77.8 77.6 54% 77.6 77.8 58% 77.3 77.7 64% 80.2 79.6 55% 82.8 82.4 53% 84.6 84.2 55% 65.2 65.5 64% 68.5 67.5 67% 67.7 67.7 66% 71.6 70.6 69% 71.6 71.6 56% 74.3 73.7 60% 2272 88.9 88.6 2239 80% 76% 2380 88.7 2381 88.4 81% 77% 2398 87.7 87.7 2392 82% 76% 2461 89.6 89.7 2462 82% 76% 2492 90.4 91.4 2496 70% 70% 2525 90.7 90.5 2527 55% 62% 80.3 80.5 73% 79.5 79.8 75% 81.5 81.0 75% 84.8 83.8 75% 87.3 86.8 73% 87.4 87.2 65% 66.3 66.5 76% 67.5 66.8 75% 70.5 71.0 79% 72.3 71.8 64% 75.9 74.8 73% 75.2 74.6 77% 66.6 65.9 69% 73.4 71.9 71% 73.3 73.2 70% 75.6 75.9 74% 76.9 75.0 64% 77.7 76.9 78% 100.0% 99.7% 69.7% 100.0% 99.6% 69.1% 100.0% 100.2% 69.7% 100.0% 99.6% 69.8% 100.0% 99.6% 62.0% 100.0% 99.6% 64.4% performance retention holds consistently as the model scales from 4B to 241B-MoE, demonstrating that ViCO is both effective and scalable."
        },
        {
            "title": "4.2.2 RESULTS ON OCR-RELATED BENCHMARKS",
            "content": "As shown in Table 1, the 8B model achieves patch compression rate of 71% on OCRBench while retaining virtually the same performance (84.0 vs. 83.9). On ChartQA and TextVQA, the same model achieves 86.7 and 77.8, respectively, showing fluctuations compared to the original scores of 86.5 and 77.6. Across all OCR-related benchmarks, the performance remains largely consistent with the original models, demonstrating the robustness of our approach. This strong retention of performance can be attributed to ViCOs adaptive routing strategy. The model learns to compress simpler patches more aggressively, using lighter modeling, while preserving the original complex modeling for patches containing critical semantic information. As result, ViCO enables the model to focus on semantically important regions without losing essential details, allowing high-fidelity performance even under substantial token compression."
        },
        {
            "title": "4.2.3 RESULTS ON MULTI-IMAGE AND VIDEO UNDERSTANDING BENCHMARKS",
            "content": "As shown in Table 2, ViCO achieves substantial token compression while preserving performance. For example, on the Video-MME benchmark with subtitles, the largest 241B-MoE model compresses approximately 70% of the tokens but still achieves score of 76.6, slightly surpassing the original model. Similar trends are observed across other models: the 38B model compresses 63% of Video-MME tokens and maintains almost 100% of its performance, while smaller models retain over 99% of their original scores after being compressed. Across all multi-image and video benchmarks, the average performance remains consistently high, demonstrating that ViCO effectively accelerates token processing without sacrificing accuracy. This is achieved through our adaptive token compression strategy, which reduces computation on less informative patches while preserving complex modeling for semantically important regions. As result, the model efficiently handles long visual sequences and maintains robust performance on tasks that require understanding across multiple images or video frames."
        },
        {
            "title": "4.3 COMPARISON WITH EXISTING METHODS",
            "content": "As shown in Table 3, we evaluate our method against two recent token reduction approaches, FastV and SparseVLM, using comparable average compression ratio on general benchmarks. Unlike our approach, these baselines rely on manually pre-defined hyperparameters and cannot adapt compres6 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Table 2: Performance of ViCO on InternVL3.5 models across multi-image and video benchmarks. For Video-MME, both the subtitle and non-subtitle settings are evaluated using 64-frame inputs. Model BLINK MMT-Bench MMIU Video-MME no sub with sub InternVL3.5-4B + ViCO Ratio of Compressed Patches InternVL3.5-8B + ViCO Ratio of Compressed Patches InternVL3.5-14B + ViCO Ratio of Compressed Patches InternVL3.5-30B-MoE + ViCO Ratio of Compressed Patches InternVL3.5-38B + ViCO Ratio of Compressed Patches InternVL3.5-241B-MoE + ViCO Ratio of Compressed Patches 59.5 58.3 74% 59.5 58.9 74% 57.6 59.2 77% 60.0 60.2 72% 60.9 62.2 64% 61.4 64.8 61% 64.3 64.2 62% 66.7 67.1 61% 68.0 67.9 63% 66.6 66.9 60% 71.8 71.5 54% 72.7 72.0 58% 49.2 49.1 66% 49.4 50.7 56% 51.3 50.5 67% 55.1 54.5 56% 58.9 59.6 51% 61.3 61.4 55% 65.4 65.1 47% 66.0 66.1 40% 67.9 67.9 55% 68.7 69.0 42% 70.9 71.3 63% 72.9 73.7 70% 68.6 68.1 47% 68.6 68.7 40% 68.6 68.7 55% 71.8 71.4 42% 74.2 74.0 63% 76.5 76.6 70% Avg. 100.0% 99.2% 59.2% 100.0% 100.4% 54.2% 100.0% 100.3% 63.4% 100.0% 99.9% 54.4% 100.0% 100.6% 59% 100.0% 101.1% 62.8% Table 3: Ablation and comparative study of ViCO across general and strongly OCR-dependent benchmarks. Experiments are conducted on InternVL3.5-8B. For DocVQA and InfoVQA, the validation sets are used for performance evaluation."
        },
        {
            "title": "Model",
            "content": "DocVQA ChartQA InfoVQA TextVQA OCRBench AI2D MMStar BLINK Avg. Vanilla Vanilla + VIR Vanilla w/o dynamic res. FastV SparseVLM ViCO (All Compress) ViCO (Random Compress) ViCO (Image-level ViR) Ratio of Compressed Patches ViCO Ratio of Compressed Patches 91.3 57.9 56.2 87.5 75.9 85.8 88.8 89.2 69% 90.8 60% 86.5 78.0 75.0 84.0 84. 84.5 85.9 86.5 36% 86.7 47% 79.1 69.4 38.6 71.6 55.2 68.9 73.3 79.1 12% 78.6 21% 77.6 71.1 64.0 76.9 76.5 74.8 76.8 76.7 65% 77.8 58% 84.0 75.1 68.7 81.0 77. 80.6 83.0 82.9 78% 83.9 71% 84.0 83.5 83.0 83.0 84.1 83.2 83.3 84.2 32% 83.7 42% 68.5 64.7 64.1 64.2 66.4 66.3 66.6 67.4 75% 67.5 67% 59.5 54.7 56.0 56.8 54. 55.4 57.4 56.8 88% 58.9 74% 100.0% 87.9% 80.2% 95.9% 91.2% 95.1% 97.6% 98.8% 56.9% 99.6% 55.0% sion dynamically across tasks. Consequently, although their overall scores decrease only slightly, they exhibit substantial performance drops on vision-sensitive benchmarks such as OCR. In contrast, our method automatically assesses the semantic importance of tokens, allowing it to apply stronger compression on vision-insensitive tasks like BLINK while retaining more tokens on vision-critical benchmarks such as InfoVQA, thereby avoiding significant degradation in performance."
        },
        {
            "title": "4.4 ABLATION STUDY",
            "content": "Settings. We conduct ablation experiments on the InternVL3.5-8B model. To evaluate the consistency training stage, we construct the Vanilla with VIR variant by applying the router decisions from the fully trained ViCO model to the original model. For the router training stage, we test three variants: (1) All Compression: all patches are compressed indiscriminately; (2) Random Routing: patches are routed randomly at the same compression ratios as the ViCO model; (3) Image-Level Routing: the router operates at the image level instead of the patch level. Additionally, we perform an experiment on the original InternVL3.5-8B model without dynamic resolution. Effect of Consistency Training. As shown in Table 3, directly applying the final-stage ViCO router to the InternVL3.5 model causes substantial performance drop (e.g., OCRBench drops from 84.0 to 75.1), indicating that the InternVL3.5 model cannot handle interleaved visual tokens at different 7 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Figure 3: Visualization of Routing Results of the InternVL3.5-8B ViCO Model. White-shaded patches are compressed to 64 tokens, while unshaded patches retain 256 tokens. Each patch is annotated with the routers confidence score, where higher values indicate stronger tendency for compression. compression rates. Disabling dynamic resolution in InternVL3.5 also reduces visual tokens significantly but severely degrades performance on vision-sensitive benchmarks. These results highlight the necessity of Consistency Training, which allows the model to process interleaved tokens at varying downsampling rates while maintaining strong performance. Effect of Router Training. As shown in Table 3, to validate the effectiveness of the router, we first evaluate naive variant in which all visual tokens are compressed indiscriminately . This setting achieves performance retention of 95.1%, indicating that uniform compression alone can harm model performance. Next, we test variant where all patches are routed randomly to different compression rates while keeping the overall compression ratio identical to ViCO with VIR. This variant achieves 97.6% performance retention, still falling short of the 99.6% retained by our full ViCO with trained router. These results demonstrate that our router effectively learns to dynamically adjust patch-wise compression based on semantic content, with patches containing rich semantic information being lightly compressed, whereas patches with simpler content are more aggressively compressed. This learned adaptive routing enables balanced trade-off between efficiency and model performance. Effect of Compression Granularity. As shown in Table 3, we further investigate the impact of routing granularity by training router that performs image-level compression instead of patchlevel. In this variant, the router decides whether to compress all visual tokens of an image based on the overall image semantics. This image-level routing achieves 98.8% performance retention, slightly lower than the 99.6% obtained with patch-level routing. At the same time, image-level routing is overly coarse and lacks fine-grained semantic awareness. This causes the router to be highly sensitive to the dominant content of each image, leading to drastically different compression tendencies across datasets. For instance, only 12% of patches are compressed on InfoVQA, whereas 88% are compressed on BLINK. Such variability leads to unstable compression behavior. These results highlight the advantage of patch-level routing in ViCO, which achieves more balanced and semantically informed token compression."
        },
        {
            "title": "4.5 VISUALIZATION OF IMAGE ROUTING",
            "content": "Settings. We visualize the routing results of our proposed Visual Resolution Router on several representative examples from the benchmarks introduced earlier. The routing is performed using the InternVL3.5-8B ViCO model, following the same evaluation settings as in the performance tests.Patches with router scores above 0.5 are treated as less critical and routed to higher compression (64 tokens), while those below 0.5 are routed to lower compression rates. Results. As shown in Figure 3, our router is able to distinguish between semantically complex and simple patches. Regions containing salient objects, such as people, animals, or other key subjects, as 8 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Table 4: Throughput of LLM under different token compression rates. Ori. refers to the original non-compressed model. We evaluate token compression rates of 25%, 50%, and 75% to measure the speedup. All values are reported as speedup factors () relative to the non-compressed baseline."
        },
        {
            "title": "Input Type",
            "content": "LLM Throughput @ token compression Ori. 25% 50% 8B 30B 38B 10 patches + 64 text tokens 10 patches + 512 text tokens 1.00 1.29 1.97 1.00 1.30 1.76 10 patches + 64 text tokens 10 patches + 512 text tokens 1.00 1.30 1.99 1.00 1.29 1.80 10 patches + 64 text tokens 10 patches + 512 text tokens 1.00 1.33 1.99 1.00 1.30 1.77 241B 10 patches + 64 text tokens 10 patches + 512 text tokens 1.00 1.34 2.00 1.00 1.32 1.81 75% 3.76 2.77 3.77 2.72 3.80 2.76 3.76 2.87 well as areas with text carrying rich semantic information, are routed to lower compression rates and largely preserved at their original resolution. In contrast, relatively homogeneous background areas, which contain less critical information, are routed to higher compression rates. Importantly, being routed to higher compression does not imply that patch is unimportant. Instead, it indicates that such information can be adequately represented with fewer resources. This selective compression strategy effectively reduces computational cost while allowing the model to focus more on patches that carry significant semantic content, ensuring that key visual information is retained for the LLMs processing. Additional visualizations are provided in Appendix C."
        },
        {
            "title": "4.6 THROUGHPUT ANALYSIS",
            "content": "Settings. We deploy our ViCO models using the LMDeploy (Contributors, 2023) framework and evaluate different model scales, namely 8B, 30B, 38B, and 241B, under varying imagetext ratios and compression rates. Using real deployment framework ensures that the evaluation more accurately reflects practical inference performance. For throughput simulation, each request contains 10 visual patches with text inputs of length 512 or 64 tokens. The LLMs first-token throughput is measured over 2000 requests, which are sent concurrently to the API using 32 threads, and the results are reported as relative speedup compared to the non-compressed baseline. We set the tensor parallelism of the 241B model to 8, and to 1 for all other models. Results. As shown in Table 4, applying token compression substantially improves the LLMs processing efficiency across all model scales and input settings. Specifically, compressing 50% of visual tokens consistently achieves over 1.75 speedup, while further increasing the compression ratio leads to even larger gains. Notably, smaller text input lengths benefit more from high compression than longer text inputs, as the relative proportion of compressed visual tokens is higher. These results demonstrate that our method not only reduces computational overhead theoretically but also yields significant practical acceleration during model inference, confirming the effectiveness of visual token compression for large-scale multimodal models."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we propose Visual Consistency Learning (ViCO), which enables the model to represent images of varying semantic complexities using different numbers of vision tokens. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the models perception, reasoning, and OCR capabilities. Additional ablation studies further validate the effectiveness of each proposed module. By making decisions based on visual semantics, ViCO enables the model to efficiently focus computation on the most informative regions of an image, providing insights for future research on adaptive visual representations. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research. 9 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work is supported by the National Key R&D Program of China (NO. 2022ZD0161302)."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This study complies with the ICLR ethical guidelines and adheres to the principles of responsible research. We confirm that no personally identifiable, sensitive, or potentially harmful data were utilized. We have considered the potential impact of our methods and believe that they advance scientific understanding without causing any foreseeable harm."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We will open-source our code and model weights on platforms such as GitHub and Hugging Face, including methodological details and experimental settings, to ensure the reproducibility of our methods."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing claude 4: Claude sonnet 4 and claude opus 4, May 2025. URL https: //www.anthropic.com/news/claude-4. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, and Tao Chen. Madtp: Multimodal alignment-guided dynamic token pruning for accelerating vision-language transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1571015719, 2024. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models, 2024a. URL https://arxiv.org/abs/2403.06764. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024b. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024c. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024d. LMDeploy Contributors. Lmdeploy: toolkit for compressing, deploying, and serving llm. https://github.com/InternLM/lmdeploy, 2023. X.AI Corp. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model. https://x.ai/blog/grok-1.5v, 2024. DeepMind. Gemini 2.5 pro. https://deepmind.google/technologies/gemini/ pro/, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 10 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024a. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024b. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali In European Conference on Computer Vision, Farhadi. diagram is worth dozen images. pp. 235251, 2016. Team Kwai Keye, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, pp. 1973019742. PMLR, 2023a. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pp. 323340. Springer, 2024. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The Conference on Empirical Methods in Natural Language Processing, pp. 292305, 2023b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2023a. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023b. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023c. Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language. arXiv preprint arXiv:2305.05662, 2023d. Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Monointernvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024. Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, and Jifeng Dai. Mono-internvl-1.5: Towards cheaper and faster monolithic multimodal large language models. arXiv preprint arXiv:2507.12566, 2025. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 22632279, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 22002209, 2021. 11 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 16971706, 2022. OpenAI. Introducing gpt-oss. https://openai.com/index/introducing-gpt-oss/, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), NeurIPS 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. Conference on Computer Vision and Pattern Recognition, pp. 83178326, 2019. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024b. Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, Xizhou Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao, and Wenhai Wang. Needle in multimodal haystack. arXiv preprint arXiv:2406.07230, 2024c. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025a. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Internvl3. 5: Advancing open-source multimodal Linglin Jing, Shenglong Ye, Jie Shao, et al. models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025b. LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/ 2506.03569. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. Deco: Decoupling token compression from semantic abstraction in multimodal large language models, 2024. URL https://arxiv.org/abs/2405.20985. Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. 12 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 13 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution THE USE OF LARGE LANGUAGE MODELS (LLMS) Large language models (LLMs) were only employed for light linguistic refinement, such as polishing sentences and correcting grammatical errors. They were not involved in the formulation or development of the core ideas presented in this article. PERFORMANCE OF VICO ON SMALL-SCALE MODELS We also evaluate ViCO on smaller InternVL3.5 models 1B and 2B across range of benchmarks, including general multimodal, OCR-related, multi-image, and video benchmarks. Tables 5 and 6 show that the models retain nearly all of their original performance, demonstrating that our approach remains effective even on smaller-scale models. Table 5: Performance of ViCO on InternVL3.5 small-scale models across general benchmarks. When calculating the overall score, MME is normalized from 02800 to 0100. RWQA refers to RealWorldQA."
        },
        {
            "title": "Model",
            "content": "OCRBench ChartQA TextVQA MMStar POPE MME MMBench V1.1 RWQA Avg. InternVL3.5-1B + ViCO Ratio of Compressed Patches InternVL3.5-2B + ViCO Ratio of Compressed Patches 79.2 78.8 70% 83.7 83.3 68% 78.0 77.4 40% 80.8 79.8 36% 71.2 71.1 56% 76.7 76.2 60% 50.6 50.8 60% 57.5 57.7 61% 1910.2 86.8 86.2 1905.8 80% 73% 2123.3 87.2 2101.0 87.2 77% 73% 69.9 69.1 72% 76.6 76.7 71% 100.0% 57.6 99.4% 56.9 75% 65.8% 100.0% 62.0 99.4% 60.7 66% 64.0% Table 6: Performance of ViCO on InternVL3.5 small-scale models across multi-image and video benchmarks. For Video-MME, both the subtitle and non-subtitle settings are evaluated using 64-frame inputs."
        },
        {
            "title": "BLINK",
            "content": "MMT-Bench"
        },
        {
            "title": "MMIU",
            "content": "Video-MME no sub with sub InternVL3.5-1B + ViCO Ratio of Compressed Patches InternVL3.5-2B + ViCO Ratio of Compressed Patches 44.0 43.9 69% 51.3 51.3 69% 54.5 54.3 57% 58.5 58.7 58% 45.2 43.9 56% 44.9 45.5 48% 52.4 52.9 63% 58.3 59.1 73% 55.0 54.9 63% 61.2 61.4 73% Avg. 100.0% 99.5% 61.6% 100.0% 100.6% 64.2%"
        },
        {
            "title": "C VISUALIZATION OF ROUTER",
            "content": "As shown in Figures 4 and 5, we visualize the Visual Resolution Router from the InternVL3.5-8B ViCO model, demonstrating additional routing results across different patch layouts. 14 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Figure 4: Routing results of the InternVL3.5-8B ViCO model on images split into 33 patches. Grey-shaded patches are compressed to 64 tokens, while unshaded patches retain 256 tokens. Each patch is annotated with the routers confidence score, where higher values indicate stronger preference for compression. 15 ViCO: Training Strategy towards Semantic Aware Dynamic High-Resolution Figure 5: Routing results of the InternVL3.5-8B ViCO model on images split into 34 patches. Grey-shaded patches are compressed to 64 tokens, while unshaded patches retain 256 tokens. Each patch is annotated with the routers confidence score, where higher values indicate stronger preference for compression."
        }
    ],
    "affiliations": [
        "Donghua University",
        "Fudan University",
        "Nanjing University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}