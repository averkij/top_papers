{
    "paper_title": "Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning",
    "authors": [
        "Andrei Semenov",
        "Philip Zmushko",
        "Alexander Pichugin",
        "Aleksandr Beznosikov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vertical Federated Learning (VFL) aims to enable collaborative training of deep learning models while maintaining privacy protection. However, the VFL procedure still has components that are vulnerable to attacks by malicious parties. In our work, we consider feature reconstruction attacks, a common risk targeting input data compromise. We theoretically claim that feature reconstruction attacks cannot succeed without knowledge of the prior distribution on data. Consequently, we demonstrate that even simple model architecture transformations can significantly impact the protection of input data during VFL. Confirming these findings with experimental results, we show that MLP-based models are resistant to state-of-the-art feature reconstruction attacks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 9 8 6 1 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Just a Simple Transformation is Enough for Data\nProtection in Vertical Federated Learning",
            "content": "Andrei Semenov MIPT semenov.a@phystech.edu Philip Zmushko MIPT zmushko.fa@phystech.edu Alexander Pichugin MIPT pichugin.ad@phystech.edu Aleksandr Beznosikov ISP RAS, MIPT beznosikov.an@phystech.edu"
        },
        {
            "title": "Abstract",
            "content": "Vertical Federated Learning (VFL) aims to enable collaborative training of deep learning models while maintaining privacy protection. However, the VFL procedure still has components that are vulnerable to attacks by malicious parties. In our work, we consider feature reconstruction attacks common risk targeting input data compromise. We theoretically claim that feature reconstruction attacks cannot succeed without knowledge of the prior distribution on data. Consequently, we demonstrate that even simple model architecture transformations can significantly impact the protection of input data during VFL. Confirming these findings with experimental results, we show that MLP-based models are resistant to state-of-theart feature reconstruction attacks."
        },
        {
            "title": "Introduction",
            "content": "Federated Learning (FL) [33, 51] introduces revolutionary paradigm for collaborative machine learning, in which multiple clients participate in cross-device model training on decentralized private data. The key idea is to train the global model without sharing the raw data among the participants. Generally, FL can be divided into two types [82]: horizontal (HFL) [37, 51], when data is partitioned among clients by samples, and vertical (VFL) [35, 49, 77, 81] when the features of the data samples are distributed among clients. Both HFL and VFL train global model without sharing the raw data among participants. Since clients in HFL hold the same feature space, the global model is also the same for each participant. Consequently, the FL orchestrator (often termed the server) can receive the parameter updates from each client. In contrast, VFL implies that different models are used for clients, since their feature spaces differ. In this way, the participants communicate through intermediate outputs, called activations. The focus of this paper is on the privacy concepts of Vertical Federated Learning [61, 86, 49], namely in Two Party Split Learning (simply, SL) [22, 70], where the parties split the model in such way that the first several layers belong to the client, and the rest are processed at the master server. In SL the client shares its last layer (called Cut Layer) activations, instead of the raw data. As canonical use case [68] of SL, one can think of advertising platform and advertiser company B. Both parties own different features for each visitor: party can record the viewing history, while party has the visitors conversion rate. Since each participant has its own private information and they do not exchange it directly, the process of training recommender system with data from and can be considered as Split Learning. In our work, we consider setting where server holds only the labels, while data is stored on the client side. We discuss in depth the SL setting in Section 3.1. Preprint. With regard to practice, the types of attacks from an adversary party are divided into: label inference [42, 68, 47, 11, 34], feature reconstruction [50, 59, 32, 16, 23, 84, 29] and model reconstruction [41, 15, 12, 65, 5, 14, 11]. In particular, among all feature reconstruction attacks in Split Learning, we are interested in Model Inversion attacks (MI) [11, 12, 26, 27, 53, 54]: one that aims to infer and reconstruct private data by abusing access to the model architecture; and Feature-space Hijacking Attack (for simplicity, we call this type of attacks as Hijacking and the attack from this work we will also call FSHA) [56, 13]: when the malicious party with labels holds an auxiliary dataset from the same domain of the training data of the defending parties; thus, the adversary has prior knowledge of the data distribution. After revisiting all the attacks, to the best of our knowledge, we highlight that state-of-the-art (SOTA) MI and Hijacking attacks [11, 56] acquire knowledge of prior on data distribution (Section 2). Furthermore, these attacks are validated only on CNN-based models, bypassing MLP-based models, which also show promise in the same domains. This leads to further questions: 1. Is it that simple to attack features, or does the data prior knowledge give lot? 2. Does architectural design play crucial role in the effectiveness of the latter attacks? 3. Can we develop theoretical intuition that MLP-based models might be more privacy-preserving? In this work, we answer these question affirmatively. Following our theoretical justification from Section 3.3, by experimentally validating the proposed Hypothesis 1, we reveal that MI [11] and Hijacking [56] attacks fail on MLP-based client-side model. Thus, we neither consider specific defense framework nor propose novel method. In contrast, we demonstrate the failure of feature reconstruction attacks when the architecture is MLP-based. We summarize our contributions as follows: (Contribution 1) We prove that without additional information about the prior distribution on the data, the feature reconstruction attack in Split Learning cannot be performed even on one-layer (dense) client-side model. For MLP-based models we state the servers inability to reconstruct the activations in the hidden-space. Furthermore, we provably guarantee that (semi)orthogonal transformations in the client data and weights initialization do not change the transmitted activations during training under the GD-like algorithms (see Section 3.3 and Appendix A.4), and also do not affect convergence for Adam-like algorithms. (Contribution 2) We show that Hijacking and Model Inversion attacks fail on MLP-based models without any additional changes. We show the effectiveness of our approach against the UnSplit [11] and Feature-space Hijacking attacks [56] on popular community datasets [38, 39, 80] and argue that feature reconstruction attacks can be prevented without resorting to any of the defenses, while preserving the model accuracy on the main task. In addition, our findings can be combined with any of the defense frameworks covered in Appendix B. (Contribution 3) We reconsider the perception of defense quality from human-side perspective and evaluate resistance against an attacker using the Fréchet inception distance (FID) [28] between the true data and the reconstructed ones. And report the comparison with commonly used MSE in Sections 4 and 5. The code is available at https://github.com/Andron00e/JAST."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Recent feature reconstruction attacks show promising results. Meanwhile, these attacks sometimes require strong assumptions about the capabilities of the attacking side. For example, methods from [59, 32] assume access not only to the architecture, but also to the client-side model parameters during each step of the optimization process (White-Box). The above assumptions rarely occur in real-world applications, as such knowledge is not naturally aligned with the SL paradigm. Nevertheless, an adaptive obfuscation framework from [21] successfully mitigates the [32] attack. Moreover, the attackers setup from these works is more valid for the HFL case (see [16]), where the model is shared among clients and can be trained with [51, 43] algorithms, rather than for VFL. Therefore, such strong settings are not considered in our work. 2 2.1 Model Inversion attacks Model Inversion attack [12, 27, 26, 88, 89, 79] is common approach in machine learning, where an adversary party (server in our case) trains clone of the client-side model to reconstruct raw data given the client activations. Recent works [11, 40, 12] demonstrate that Split Learning is also vulnerable to MI attacks. Meanwhile, the most popular defense frameworks [40, 67], aiming to protect data from MI attack, are effective against the adversary with White-Box access, which does not hold in real-world, and require imitation of the attacker (called attacker-aware training) using client-side inversion models, which leads to 27% floating point operations (FLOPs) computational overhead(see Li et al. [40] Table 6). Next, we come to Unsplit, proposed in Erdogan et al. [11], the main MI attack aiming to reconstruct input image data by exploiting an extended variant of coordinate descent [78]. Given the client model , its clone (i.e., the randomly initialized model with the same architecture), the adversary server attempts to solve the two-step optimization problem: = arg min W = arg min W"
        },
        {
            "title": "LMSE",
            "content": "(cid:16) ( X, ), (X, ) (cid:16) ( X, ), (X, ) (cid:17) (cid:17) + λTV( X), . (1) (2) In this context, X, represent the client models private inputs and parameters; TV denotes the total variation distance [62] for image pixels (this term allows the attacker to use prior on data distribution); and , are the desired variables for the attackers reconstructed output and parameters, respectively. Whereas, λ is the coefficient to modify the impact of the total variation, e.g., minimizing TV( X) results in smoother images. At the beginning of the attack, mock features initializes as constant matrix. It should be noted that this optimization process can be applied both before and after training . The latter corresponds to feature reconstruction during the inference stage. The authors assume that the server is only aware of the architecture of the client model . 2.2 Hijacking attacks The Feature-space Hijacking Attack was initially proposed in Pasquini et al. [56]. The authors mention that the servers ability to control the learning process is the most pervasive vulnerability of SL, which is not used in UnSplit setting. Indeed, since the server is able to guide the client model towards the required functional states, it has the capacity to reconstruct the private features X. In hijacking attacks [13, 56, 85], the malicious server exploits an access to public dataset Xpub of the same domain as to subdue the training protocol. Specifically, in FSHA, the server initializes three additional models: encoder ψE, decoder ψD and discriminator D. While the client-side model : is initialized as mapping between the data distribution and hidden-space Z, the encoder network ψE : dynamically defines function to certain subset Z. Since the goal is to recover , to ensure the invertibility of ψE, the server trains the decoder model ψD : . To guide towards learning Z, server uses discriminator network trained to assign high probability to the ψE(Xpub) and low to the (X). The general scheme of the attack is the following: E, ψ ψ = arg min ψE,ψD LMSE (ψD(ψE(Xpub)), Xpub) , = arg min = arg min [log(1 D(ψE(Xpub))) + log(D(f (X)))] , [log (1 D(f (X)))] . And, finally, server recovers features with: = ψ (L(X)) . (3) (4) This paper has led to the creation of other works that study FSHA. Erdogan et al. [9] propose defense method SplitGuard in which the client sends fake batches with mixed labels with certain probability. Then, the client analyzes the gradients corresponding to the real and fake labels and computes 3 SplitGuard score to assess whether the server is conducting Hijacking Attack and potentially halt the training. In response to the SplitGuard defense, Fu et al. [13] proposed SplitSpy: where it is observed that samples from the batch with the lowest prediction score are likely to correspond to the fake labels and should be removed during this round of FSHA. Therefore, SplitSpy computes gradients from discriminator only for survived samples. We would like to outline that this attack uniformly weaker compared to the original FSHA [56] in the absence of the SplitGuard defense. Thus, we will only consider this attack later. 2.3 Quality of the defense In [69], authors study the faithfulness of different privacy leakage metrics to human perception. Crowdsourcing revealed that hand-crafted metrics [64, 58, 87, 76] have weak correlation and contradict with human awareness and similar methods[87, 30]. From this point of view, we reconsider the usage of the MSE metric for the evaluation of the defense against feature reconstruction attacks, i.e., the quality of reconstruction. Given that the main datasets contain images, we suggest to rely on Frechet Inception Distance (FID) [28]. Besides the fact that MSE metric is implied into the attacker algorithms Equations (1) to (3), most of works on evaluation of the images quality rely on FID. From the privacy perspective, the goal of the successful defense evaluation is to compare privacy risks of classification model under the reconstruction attack. This process can be formalized for Split Learning in the following way: let the attack mechanism aiming to reconstruct client model data given the Cut Layer outputs H, depending on the setup, can access the client model architecture (in other settings this assumption may differ), then the privacy leakage is represented as PrivacyLeak = InfoLeak (X, M(H, )) = PrivacyLeak (Xrec) , where InfoLeak stands for the amount of information leakage in reconstructed images Xrec. Note that, receives the Cut Layer outputs at every iteration; then, the PrivacyLeak can also be measured during every iteration of the attack. Generally, information leakage can be represented through the hand-crafted metric ρ: InfoLeak = ρ(X, Xrec)."
        },
        {
            "title": "3 Problem Statement and Theoretical Motivation",
            "content": "In this section, we: 1. Outline the (Two Party) Split Learning setting. (Section 3.1) 2. Demonstrate that (semi)orthogonally transformed data and weights result in an identical training process from the servers perspective. (Lemma 1) 3. Prove that in this scenario, even malicious server cannot reconstruct features without prior knowledge of the data distribution. (Lemma 2) 4. Show that similar reasoning applies to the distribution of activations before the Cut Layer. (Lemma 3) 5. Propose Hypothesis 1 explaining why SOTA feature reconstruction attacks achieve significant success and suggest potential remedies. 3.1 Problem Statement Notation. We denote the clients model in SL as , with the weights Rddh. Under X, we consider design matrix of shape Rnd. We denote activations that client transmits to the server as Rndh , while and are the hidden-space and the data distribution, respectively. corresponds to the number of samples in dataset X, stands for the features belonging to client and dh is hidden-size of the model. denotes the loss function of the entire model (both server and client). Next, we provide detailed description of our setup. Setup. From the perspective of one client, it cannot rely on any information about the other parties during VFL. Then, to simplify the analysis, we consider the Two Party Split Learning process. Server (label-party) holds vector of labels y, while the other data is located at the client-side matrix X. Server and client have their own neural networks. In each iteration, the non-label party computes activations = (X, ) and sends it to the server. Then, the remaining forward computation is performed only by server fs and results in the predictions = fs(H) and, consequently, to the loss L(p, y). In the backward phase, client receives H , and computes the W = H . 4 3.2 Motivation: Orthogonal transformation of data and weights stops the attack In this section, we consider client as one-layer linear model = XW with Rddh. Note that (semi)orthogonal transformations XU , W0 W0 preserve the outputs of at initialization. Turns out, that it also holds for subsequent iterations of (Stochastic) Gradient Descent: Lemma 1. For one-layer linear model trained using GD or SGD, there exist continually many pairs of client data and weights initialization that produce the same activations at each step. The complete proof of this lemma is presented in Appendix A.1. These pairs have the form { X, W0} = {XU, W0}, where arbitrary orthogonal matrix. The use of such pair within (S)GD induces the same rotation of the iterates: GD + = γL/ = (W γX L/H) = GD + . With such orthogonal transformation, the client produces the same activations, as if we had left and unchanged: W = = XW . The server cannot distinguish between the different data distributions that produce identical outputs; therefore, the true data also cannot be obtained. This results in: Remark 1. Under the conditions of Lemma 1, if the server has no prior information about the distribution of X, the label party cannot reconstruct initial data (only up to an arbitrary orthogonal transformation). Recent work [84] states similar considerations, but their remark about Adam [36] and RMSprop [19] not changing the Split Learning protocol is false. The use of Adam or RMSprop with (semi)orthogonal transformations changes the activations produced by the SL protocol because of the root-meansquare dependence of the denominator on gradients in their update rules. We delve into this question in Remark 4 (see Appendix for details). In fact, Lemma 1 holds only for algorithms whose update step is linear with respect to the gradient history. However, while Adam and RMSProp do not preserve the SL protocol in terms of exact matching of transmitted activations, we can relax the conditions and consider the properties of these algorithms from the perspective of protocol preservation in the sense of maintaining convergence to the same value. To begin with, let us note the following: Remark 2. The models optimal value after Split Learning is the same for any orthogonal data transformation. Indeed, = XU = : L( X, ) = = L(X, ). Thus, remains the same if we correspondingly rotate the optimal weights. In the case of convex or strongly-convex function (entire model) L(X, ), the optimal value is unique, and therefore any algorithm is guaranteed to converge to for any data transformation. Meanwhile, for general non-convex functions, convergence behavior becomes more nuanced: in fact, in Example 1, we present function on which the Adam algorithm converges before data and weight transformation and diverges after the transformation. However, the situation changes when we turn to functions satisfying the Polyak-Łojasiewicz-condition (PL)1, which is used as canonical description of neural networks [46], then, provably claim that the SL protocol is preserved for PL functions with orthogonal transformations of data and weights. We show that Adams preconditioning matrices can be bounded regardless of the initialization, and derive Descent Lemma 5 with the modification of bounded gradient Assumption 3 similar to prior works [63, 8]. The convergence guarantees are covered in Lemma 6 and the theoretical evidence can be found in Appendix A.4. According to Lemma 1, the desired result of the defending party can be achieved if we weaken the attackers knowledge of prior on data distribution (see Section 3.3). Actually, even knowledge of weights does not help the attacker: Corollary 1. Under the conditions of Lemma 1, assume that server knows the first layer W1 of , and let this layer be an invertible matrix. Then, the label party cannot reconstruct the initial data (only up to an arbitrary orthogonal transformation). Indeed, the activations send to the server in the first step: H1 = XW1, but if the client performs an , where H1 = XW1. orthogonal transformation leading to X, then, server can recover only H1W 1 1 1Note that PL-condition does not imply convexity, see footnote 1 from [44]. Meanwhile, the difference between and affects only the initialization of weights, and thus should not change the final model performance much. Next, we conclude that even malicious server cannot reconstruct the clients data without the additional prior on X. Lemma 2. Under the conditions of Lemma 1, assume training with the malicious server sending arbitrary vectors instead of real gradients = L/H. In addition, the server knows the initialization of the weight matrix W1.Then, tif the client applies non-trainable orthogonal matrix before W1, themalicious server cannot reconstruct initial data (only up to an arbitrary orthogonal transformation). Remark 3. With the same reasons as for Lemma 1, if even the malicious server from Lemma 2 has no prior information about the distribution of X, it is impossible for the label party to reconstruct the initial data X. We note that Lemmas 1 and 2 are correct under their conditions even if the update of the client-side model includes bias, see Corollary 2. 3.3 Motivation: You cannot attack the activations before \"Cut Layer\" Up until now, we considered the client-side model with one linear layer and proved that orthogonal transformation of data and weights lead to the same training protocol. The intuition behind Lemmas 1 and 2 suggests that in the client model, one should look for layers whose inputs cannot be given the prior distribution. Meanwhile, the activations after each layer are also data on which the servers model trains; however, this data become smashed after being processed by . This brings us to the consideration of Cut Layer, since this is bridge between the client and server. The closer Cut Layer is to the first layer of the clients model , the easier it is to steal data [11, 40]; the complexity of attack increase with the distance between Cut Layer and data. Consequently, we pose question: Does our intuition from Section 3.2 apply for the activations before Cut Layer? Lemma 3. [Cut Layer Lemma] There exist continually many distributions of the activations before the linear Cut Layer that produce the same Split Learning protocol. The results of Lemma 3 lead to promising remark. While server might have prior on original data distribution, acquiring prior on the distribution of the activations before Cut Layer is, generally, much more challenging. The absence of knowledge regarding the prior distribution of activations, combined with the assertion in Lemma 3, yields result for activations analogous to Remark 1. Specifically, even with knowledge of certain prior on the data, the server can, at best, reconstruct activations only up to an orthogonal transformation 2. 3.4 Should we use dense layers against feature reconstruction attacks? The findings from Section 3.3 indicate that the reconstruction of activations poses significant challenges for the server. However, many feature reconstruction attacks achieve considerable success. This raises the question: Does the servers inability to reconstruct activations before the Cut Layer not impede its capacity to reconstruct data features? Alternatively, Could it be that the conditions of Lemma 3 do not hold in practical scenarios? To investigate this matter more thoroughly, we examined outlined in Sections 2.1 and 2.2 feature reconstruction attacks. Specifically, we focused on UnSplit [11] and FSHA [56], which are SOTA representatives (to the best of our knowledge) of the Model Inversion and Feature Space Hijacking attack categories, respectively. UnSplit requires knowledge of the client-side model architecture, while FSHA should know the dataset Xpub of the same distribution as the original X. Assumptions are quite strong in the general case, but, we, in turn, argue that their attacks can be mitigated without any additional modifications in UnSplit [11] and FSHA [56] assumptions (see Section 4). 2Excluding degenerate cases, such as when the server knows that the clients network performs an identity transformation. 6 Figure 1: Results of UnSplit attack on MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): MLP-based client model. Figure 2: Results of UnSplit attack on F-MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): MLP-based client model. Both of these attacks are validated exclusively on image datasets, utilizing CNN architectures. Consequently, the client-side model architectures lack fully connected (dense) layers before Cut Layer and the conditions of Lemma 3 do not hold. While convolutional layer is inherently linear operation and can be represented as matrix multiplication where the inputs and weights can be flattened into 2D tensors the resulting matrix typically has very specific structure. In particular, all elements except for (kernel size) (kernel size) entries in each row are zero. Therefore, an inverse transform does not exist in general sense meaning not every matrix multiplication can be expressed as convolution, as the resultant matrix generally contains significantly more non-zero elements. As result, merging an orthogonal matrix into sequence convolutional layers and poolings by multiplying the convolution weights with an orthogonal matrix is impossible, since this would result in matrix with an excess of non-zero elements. Based on this observation we propose: Hypothesis 1. Could it be that the attacks are successful due to the lack of dense layers in the client architecture? Will usage of MLP-based architectures for , instead of CNNs, be more privacy preserving against Model Inversion attack and FSHA? We intend to experimentally test this conjecture in the following section."
        },
        {
            "title": "4 Experiments",
            "content": "This section is dedicated to the experimental validation of the concepts introduced earlier. To test our Hypothesis 1, we evaluate the effectiveness of UnSplit and FSHA on MNIST [39] and F-MNIST [80] in setting where at least one dense layer is present on the client side. It is important to note that although MLP-based architecture may not be conventional in the field of Computer Vision (where CNN usage is more prevalent), dense layers are the backbone of popular model architectures in many other Deep Learning domains, such as Natural Language Processing, Reinforcement Learning, Tabular Deep Learning, etc. In these domains, dense layers are commonly found at the very start of the architecture, and thus, when the network is split for VFL training, these layers would be contained in . Furthermore, even within the Computer Vision field, there is growing popularity of architectures like Vision Transformers (ViT) [4] and MLP-Mixer [71], which also incorporate dense layers at the early stages of data processing. Therefore, we contend that with careful architectural selection, integrating dense layers on the client side should not lead to significant deterioration in the models utility score. 4.1 UnSplit Before delving into the primary experiments of our study, we must note that unfortunately we were unable to fully reproduce the results of UnSplit using the code from their repository. Specifically, the images reconstructed through the attack were significantly degraded when deeper Cut Layers were used (see column Without Noise in 3). However, for the case where cut layer = 1 (i.e., when 7 there is only one layer on the client side), the images were reconstructed quite well. Therefore, we used this setup for our comparisons. As previously mentioned, to test Hypothesis 1, we utilized an MLP model with single or multiple dense layers on the client side (in the experiments below, clients part holds only one-layer model). For CIFAR-10, we use MLP-Mixer, which maintains the performance of CNN-based model while incorporating dense layers into the design. The results of the attack are shown in Figures 1 and 2. Despite our efforts to significantly increase the λ parameter in the TV in Equation (1) up to 100 thereby incorporating stronger prior about the data into the attackers model the attack failed to recover the images, thus supporting the assertion of Lemma 1. Table 1: UnSplit attack on MNIST, F-MNIST, and CIFAR-10 datasets. Dataset MNIST F-MNIST CIFARModel MLP-based CNN-based MLP-based CNN-based MLP-Mixer CNN-based MSE MSE 3 108 2 102 4 105 4 102 6 106 4 103 0.27 0.05 0.19 0.37 1.398 0.056 FID Acc% 98.42 394 98.68 261 88.31 361 89.23 169 89.29 423 93.61 455 Additionally, Table 1 presents the reconstruction loss values between normalized images. Here MSE and FID shows the difference between the original and reconstructed images, and MSE refers to the loss between the activations = (X, ) and = ( X, ). Acc% denotes final accuracy of the trained models. As we can see, the results of MLP-based model are very close to its CNN-based counterpart. In the image space , FID appears to be superior metric compared to MSE for accurately capturing the consequences of the attack. Furthermore, the tables show the MSE between activations before the Cut Layer for both the original and reconstructed images. These results indicate that in the case of the dense layer, the activations almost completely match, with significantly lower MSE than those even for well-reconstructed images. This implies that while the attack can perfectly fit = XW (in notation, when cut layer = 1), it fails to accurately recover X. In addition, we note that even one-layer linear model remains resistant to the UnSplit attack, and we provide the corresponding experiments in Appendix C. 4.2 FSHA Similarly to the previous subsections, we replaced the clients model in the FSHA attack[56] with an MLP consisting of one or multiple layers. The attackers models also varied, ranging from ResNet[25] architectures (following the original paper) to MLPs, ensuring that the attackers capabilities are not constrained by the limitations of any architectural design. The results, illustrated in Figures 3 and 4, consistently demonstrate that the malicious party fully reconstructs the original data in the case of the ResNet architecture and completely fails in the case of the Dense layer. Figure 3: Results of FSHA attack on MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): MLP-based client model. Figure 4: Results of FSHA attack on F-MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): MLP-based client model. In addition to the reconstructed data shown in Figure 5, we computed the Reconstruction error (Equation (4)) and Encoder-Decoder error (Equation (3)) for client using ResBlock architecture 8 (as in the original paper) and client employing an MLP-based architecture. These plots reveal that the Encoder-Decoder pair for both architectures is equally effective at reconstructing data from the public dataset on the attackers side. However, challenge arises on the attackers side with the training of GAN[18]. It is evident that in the presence of Dense layer on the client side, the GAN fails to properly align the clients model representation within the required subset of the feature space. Instead, it converges to mapping models of all classes into one or several modes within the activation space, corresponding to only few original classes. This phenomenon is particularly well illustrated for the F-MNIST dataset in Figure 4. Figure 5: Encoder-decoder error and Reconstruction error for FSHA attack 4.3 Evaluation with FID Inspired by prior works on GANs[18], we apply FID to the InfoLeak scheme for the next reasons: (1) FID measures the information leakage as the distribution difference between between original and reconstruction images, thus InfoLeak(X, Xrec) FID(X, Xrec). (2) Usage of FID is more common approach when dealing with images. (3) The widespread metric in reconstruction evaluation is MSE, that lacks an interpretation for complex images [69], at least from the CIFAR-10[38] dataset. However, we notice that the privacy evaluation of feature reconstruction attacks requires refined. The values of FID and MSE in Table 1 suggest that FID is more accurate reflection of the attacks outcomes than MSE in the image space . For instance, on the F-MNIST dataset, the MSE is higher for CNN architecture despite the better quality of the reconstructed images. This discrepancy appears to stem from differences in background pixel values compared to the original images."
        },
        {
            "title": "5 Discussions",
            "content": "With our work, we contribute to better understanding of the meaningfulness of feature reconstruction attacks. We show that the architectural design of client-side model reflects the attacks performance. Particularly, even the most powerful Black-Box feature reconstruction attacks fail when attempting to compromise clients data when its architecture is MLP. We observe our findings experimentally, and provide rigorous mathematical explanation of this phenomenon. Our study contributes to recent 9 Figure 6: Results of UnSplit attack on CIFAR-10. (Top): Original images. (Middle): CNN-based client model. (Bottom): MLP-Mixer client model. advances in privacy of VFL (SL) and suggest that novel Black-Box attacks should be revisited to address the challenges which occurs with MLP-based models. We note that our approach may not be impactful on NLP tasks, since the language models require discrete input instead of the continuous which we actively exploit during the theoretical justifications and experiments with MLP-based models. However, we note that UnSplit attack also cannot be efficiently performed against the transformer-based architectures due to the huge amount of computational resources for training multi-head attention and FFN layers with the coordinate descent."
        },
        {
            "title": "Acknowledgments",
            "content": "The work was done in the Laboratory of Federated Learning Problems of the ISP RAS (Supported by Grant App. No. 2 to Agreement No. 075-03-2024-214)."
        },
        {
            "title": "References",
            "content": "[1] Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In International Conference on Machine Learning, pages 394403. PMLR, 2018. [2] Kallista Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacypreserving machine learning. In Practical Secure Aggregation for Privacy-Preserving Machine Learning, pages 11751191, 10 2017. doi: 10.1145/3133956.3133982. [3] Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, Dimitrios Papadopoulos, and Qiang Yang. Secureboost: lossless federated learning framework, 2021. [4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [5] Ilias Driouich, Chuan Xu, Giovanni Neglia, Frederic Giroire, and Eoin Thomas. Local model reconstruction attacks in federated learning and their uses, 2023. [6] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9:211407, 2014. URL https://api.semanticscholar.org/ CorpusID:207178262. [7] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265284. Springer, 2006. 10 [8] Alexandre Défossez, Léon Bottou, Francis Bach, and Nicolas Usunier. simple convergence proof of adam and adagrad, 2022. [9] Ege Erdogan, Alptekin Küpçü, and Ercument Cicek. Splitguard: Detecting and mitigating training-hijacking attacks in split learning. In Proceedings of the 21st Workshop on Privacy in the Electronic Society, pages 125137, 2022. [10] Ege Erdogan, Unat Teksen, Mehmet Salih Celiktenyildiz, Alptekin Kupcu, and A. Ercument Cicek. Splitout: Out-of-the-box training-hijacking detection in split learning via outlier detection, 2023. [11] Ege Erdogan, Alptekin Küpçü, and A. Ercüment Çiçek. Unsplit: Data-oblivious model inversion, model stealing, and label inference attacks against split learning. In Proceedings of the 21st Workshop on Privacy in the Electronic Society, CCS 22. ACM, November 2022. doi: 10.1145/ 3559613.3563201. URL http://dx.doi.org/10.1145/3559613.3563201. [12] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, 2015. URL https: //api.semanticscholar.org/CorpusID:207229839. [13] Jiayun Fu, Xiaojing Ma, Bin B. Zhu, Pingyi Hu, Ruixin Zhao, Yaru Jia, Peng Xu, Hai Jin, and Dongmei Zhang. Focusing on pinocchios nose: gradients scrutinizer to thwart split-learning hijacking attacks using intrinsic attributes. In 30th Annual Network and Distributed System Security Symposium, NDSS 2023, San Diego, California, USA, February 27 - March 3, 2023. The Internet Society, 2023. URL https://www.ndss-symposium.org/ndss-paper/ focusing-on-pinocchios-nose-a-gradients-scrutinizer-to-thwart-split-learning-hijacking-attacks-using-intrinsic-attributes/. [14] Karan Ganju, Qi Wang, Wei Yang, Carl.A. Gunter, and Nikita Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations, pages 619633, 10 2018. doi: 10.1145/3243734.3243834. [15] Xinben Gao and Lan Zhang. PCAT: Functionality and data stealing from split learning by Pseudo-Client attack. In 32nd USENIX Security Symposium (USENIX Security 23), pages 52715288, Anaheim, CA, August 2023. USENIX Association. ISBN 978-1-939133-37-3. URL https://www.usenix.org/conference/usenixsecurity23/presentation/gao. [16] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting gradients how easy is it to break privacy in federated learning?, 2020. [17] Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, and Chiyuan Zhang. Deep learning with label differential privacy, 2021. [18] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. [19] Alex Graves. Generating sequences with recurrent neural networks, 2014. [20] Bin Gu, Zhiyuan Dang, Xiang Li, and Heng Huang. Federated doubly stochastic kernel learning for vertically partitioned data, 2020. [21] Hanlin Gu, Jiahuan Luo, Yan Kang, Lixin Fan, and Qiang Yang. Fedpass: Privacy-preserving vertical federated deep learning with adaptive obfuscation, 2023. [22] Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple agents, 2018. [23] Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. Recovering private text in federated learning of language models, 2022. [24] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption, 2017. [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. [26] Zecheng He, Tianwei Zhang, and Ruby B. Lee. Model inversion attacks against collaborative inference. Proceedings of the 35th Annual Computer Security Applications Conference, 2019. URL https://api.semanticscholar.org/CorpusID:208277767. [27] Zecheng He, Tianwei Zhang, and Ruby B. Lee. Attacking and protecting data privacy in edgecloud collaborative inference systems. IEEE Internet of Things Journal, 8:97069716, 2021. URL https://api.semanticscholar.org/CorpusID:226478768. [28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar.org/ CorpusID:326772. [29] Yuzheng Hu, Tianle Cai, Jinyong Shan, Shange Tang, Chaochao Cai, Ethan Song, Bo Li, and Dawn Song. Is vertical logistic regression privacy-preserving? comprehensive privacy analysis and beyond, 2022. [30] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics Letters, 44:800801, 2008. URL https://api.semanticscholar. org/CorpusID:62732555. [31] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation, 2024. URL https://arxiv.org/abs/2401.09603. [32] Xiao Jin, Pin-Yu Chen, Chia-Yi Hsu, Chia-Mu Yu, and Tianyi Chen. Cafe: Catastrophic data leakage in vertical federated learning, 2022. [33] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. DOliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Koneˇcný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning, 2021. [34] Sanjay Kariyappa and Moinuddin Qureshi. Exploit: Extracting private labels in split learning, 2022. [35] Afsana Khan, Marijn ten Thij, and Anna Wilbik. Vertical federated learning: structured literature review, 2023. [36] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. [37] Jakub Koneˇcný, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency, 2017. [38] Alex Krizhevsky. Learning multiple layers of features from tiny images. In Learning Multiple Layers of Features from Tiny Images, 2009. URL https://api.semanticscholar.org/ CorpusID:18268744. [39] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. doi: 10.1109/5.726791. [40] Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He, Deliang Fan, and Chaitali Chakrabarti. Ressfl: resistance transfer framework for defending model inversion attack in split federated In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern learning. Recognition (CVPR), pages 1019410202, June 2022. 12 [41] Jingtao Li, Adnan Siraj Rakin, Xing Chen, Li Yang, Zhezhi He, Deliang Fan, and Chaitali Chakrabarti. Model extraction attacks on split federated learning, 2023. [42] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, and Chong Wang. Label leakage and protection in two-party split learning, 2022. [43] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks, 2020. [44] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtárik. Page: simple and optimal probabilistic gradient estimator for nonconvex optimization, 2021. URL https://arxiv. org/abs/2008.10898. [45] Zitao Li, Tianhao Wang, and Ninghui Li. Differentially private vertical federated clustering. Proc. VLDB Endow., 16:12771290, 2022. URL https://api.semanticscholar.org/ CorpusID:251280212. [46] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in overparameterized non-linear systems and neural networks, 2021. URL https://arxiv.org/ abs/2003.00307. [47] Junlin Liu, Xinchen Lyu, Qimei Cui, and Xiaofeng Tao. Similarity-based label inference attack against training and inference of split learning. IEEE Transactions on Information Forensics and Security, 19:28812895, 2024. ISSN 1556-6021. doi: 10.1109/tifs.2024.3356821. URL http://dx.doi.org/10.1109/TIFS.2024.3356821. [48] Yang Liu, Tianyuan Zou, Yan Kang, Wenhan Liu, Yuanqin He, Zhihao Yi, and Qiang Yang. Batch label inference and replacement attacks in black-boxed vertical federated learning, 2022. [49] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and Qiang Yang. Vertical federated learning: Concepts, advances, and challenges. IEEE Transactions on Knowledge and Data Engineering, page 120, 2024. ISSN 2326-3865. doi: 10.1109/tkde.2024.3352628. URL http://dx.doi.org/10.1109/TKDE.2024.3352628. [50] Xinjian Luo, Yuncheng Wu, Xiaokui Xiao, and Beng Chin Ooi. Feature inference attack on model predictions in vertical federated learning. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 181192, 2021. doi: 10.1109/ICDE51399.2021.00023. [51] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera Arcas. Communication-efficient learning of deep networks from decentralized data, 2023. [52] Yuxi Mi, Hongquan Liu, Yewei Xia, Yiheng Sun, Jihong Guan, and Shuigeng Zhou. Flexible differentially private vertical federated learning with adaptive feature embeddings. ArXiv, abs/2308.02362, 2023. URL https://api.semanticscholar.org/CorpusID: 260611110. [53] Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and Ngai-Man Cheung. Label-only model inversion attacks via knowledge transfer, 2023. [54] Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and Ngai-Man Cheung. Re-thinking model inversion attacks against deep neural networks, 2023. [55] Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H. Pham, Khoa Doan, and Kok-Seng Wong. Backdoor attacks and defenses in federated learning: Survey, challenges and future research directions, 2023. [56] Dario Pasquini, Giuseppe Ateniese, and Massimo Bernaschi. Unleashing the tiger: Inference attacks on split learning, 2021. [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. URL https://arxiv.org/abs/1912.01703. 13 [58] Marius Pedersen and Jon Yngve Hardeberg. Full-Reference Image Quality Metrics. 10.1561/0600000037, 2012. doi: 10.1561/0600000037. [59] Xinchi Qiu, Ilias Leontiadis, Luca Melis, Alex Sablayrolles, and Pierre Stock. Evaluating privacy leakage in split learning, 2024. [60] Xinchi Qiu, Heng Pan, Wanru Zhao, Yan Gao, Pedro P. B. Gusmao, William F. Shen, Chenyang Ma, and Nicholas D. Lane. Secure vertical federated learning under unreliable connectivity, 2024. [61] Nuria Rodríguez-Barroso, Daniel Jiménez-López, M. Victoria Luzón, Francisco Herrera, and Eugenio Martínez-Cámara. Survey on federated learning threats: Concepts, taxonomy on attacks and defences, experimental study and challenges. Information Fusion, 90:148173, February 2023. ISSN 1566-2535. doi: 10.1016/j.inffus.2022.09.011. URL http://dx.doi. org/10.1016/j.inffus.2022.09.011. [62] Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica Nonlinear Phenomena, 60(1-4):259268, November 1992. doi: 10.1016/0167-2789(92)90242-F. [63] Abdurakhmon Sadiev, Aleksandr Beznosikov, Abdulla Jasem Almansoori, Dmitry Kamzolov, Rachael Tappenden, and Martin Takáˇc. Stochastic gradient methods with preconditioned updates. Journal of Optimization Theory and Applications, 201(2):471489, March 2024. ISSN 1573-2878. doi: 10.1007/s10957-023-02365-3. URL http://dx.doi.org/10.1007/ s10957-023-02365-3. [64] Umme Sara, Morium Akter, and Mohammad Shorif Uddin. Image quality assessment through fsim, ssim, mse and psnra comparative study. Journal of Computer and Communications, 2019. URL https://api.semanticscholar.org/CorpusID:104425037. [65] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models, 2017. [66] Adam D. Smith, Shuang Song, and Abhradeep Thakurta. The flajolet-martin sketch itself preserves differential privacy: Private counting with minimal space. In Neural Information Processing Systems, 2020. URL https://api.semanticscholar.org/CorpusID:227276564. [67] Jiankai Sun, Yuanshun Yao, Weihao Gao, Junyuan Xie, and Chong Wang. Defending against reconstruction attack in vertical federated learning, 2021. [68] Jiankai Sun, Xin Yang, Yuanshun Yao, and Chong Wang. Label leakage and protection from forward embedding in vertical federated learning, 2022. [69] Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, and Liang Zheng. Privacy assessment on reconstructed images: Are existing evaluation metrics faithful to human perception?, 2023. [70] Chandra Thapa, M. A. P. Chamikara, Seyit Camtepe, and Lichao Sun. Splitfed: When federated learning meets split learning, 2022. [71] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021. [72] Valeria Turina, Zongshun Zhang, Flavio Esposito, and Ibrahim Matta. Federated or split? performance and privacy analysis of hybrid split and federated learning architectures. In 2021 IEEE 14th International Conference on Cloud Computing (CLOUD), pages 250260, 2021. doi: 10.1109/CLOUD53861.2021.00038. [73] Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta, and Ramesh Raskar. Nopeek: Information leakage reduction to share activations in distributed deep learning, 2020. [74] Rui Wang, Oguzhan Ersoy, Hangyu Zhu, Yaochu Jin, and Kaitai Liang. Feverless: Fast and secure vertical federated learning based on xgboost for decentralized labels. IEEE Transactions on Big Data, pages 115, 2022. doi: 10.1109/TBDATA.2022.3227326. [75] Yulong Wang, Tong Sun, Shenghong Li, Xin Yuan, Wei Ni, Ekram Hossain, and H. Vincent Poor. Adversarial attacks and defenses in machine learning-powered networks: contemporary survey, 2023. [76] Zhou Wang and Alan Conrad Bovik. universal image quality index. IEEE Signal Processing Letters, 9:8184, 2002. URL https://api.semanticscholar.org/CorpusID:14488670. [77] Kang Wei, Jun Li, Chuan Ma, Ming Ding, Sha Wei, Fan Wu, Guihai Chen, and Thilina Ranbaduge. Vertical federated learning: Challenges, methodologies and experiments, 2022. [78] Stephen J. Wright. Coordinate descent algorithms, 2015. [79] Xi Wu, Matt Fredrikson, Somesh Jha, and Jeffrey F. Naughton. methodology for formalizing model-inversion attacks. 2016 IEEE 29th Computer Security Foundations Symposium (CSF), pages 355370, 2016. URL https://api.semanticscholar.org/CorpusID:5921778. [80] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: novel image dataset for benchmarking machine learning algorithms, 2017. [81] Liu Yang, Di Chai, Junxue Zhang, Yilun Jin, Leye Wang, Hao Liu, Han Tian, Qian Xu, and Kai Chen. survey on vertical federated learning: From layered perspective, 2023. [82] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications, 2019. [83] Ziqi Yang, Ee-Chien Chang, and Zhenkai Liang. Adversarial neural network inversion via auxiliary knowledge alignment, 2019. [84] Peng Ye, Zhifeng Jiang, Wei Wang, Bo Li, and Baochun Li. Feature reconstruction attacks and countermeasures of dnn training in vertical federated learning, 2022. [85] Fangchao Yu, Lina Wang, Bo Zeng, Kai Zhao, Zhi Pang, and Tian Wu. How to backdoor split learning. Neural Netw., 168(C):326336, nov 2023. ISSN 0893-6080. doi: 10.1016/j.neunet. 2023.09.037. URL https://doi.org/10.1016/j.neunet.2023.09.037. [86] Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, and Stacy Patterson. survey of privacy threats and defense in vertical federated learning: From model life cycle perspective, 2024. [87] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. [88] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients, 2020. [89] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients, 2019. [90] Tianyuan Zou, Yang Liu, Yan Kang, Wenhan Liu, Yuanqin He, Zhihao Yi, Qiang Yang, and YaQin Zhang. Defending batch-level label inference and replacement attacks in vertical federated learning. IEEE Transactions on Big Data, pages 112, 2022. doi: 10.1109/TBDATA.2022. 3192121. [91] Tianyuan Zou, Yang Liu, and Ya-Qin Zhang. Mutual information regularization for vertical federated learning, 2023. Appendix / supplemental material A.1 Orthogonal matrices give identical training for different data distributions The proof of Lemma 1. Proof. To proof this claim we will show that it is possible to obtain two completely identical model training procedures from the servers perspective with different pairs of the client data. Moreover, there are continually many such pairs. Let the clients data come from the distribution and the initialization of the matrix be W1. Then, consider an orthogonal matrix Rdd, distribution = and initialization W1 = W1. We now show by induction, that in each step of the model training protocol the client will obtain the same latents Hk, Hk, both with {X , W1} and { , W1} pairs, respectively, and, therefore, will transfer the same activations to the server. Note that we make no assumptions about the data Xk processed at each step of optimization. It can be either the entire dataset or mini-batch of arbitrary size. 1. Base case, = 1: H1 = X1W1 = X1U W1 = X1 W1 = H1 2. Induction step, + 1 > 1: Let Hk = Hk by induction hypothesis. Then L/Hk = L/ Hk = Gk Rndh. Recall, that Wk = Hk Hk Wk = Hk = Gk. Then the step of GD for the pairs {X , W1} and { , W1} returns Wk+1 = Wk γX Gk and respectively. Thus, at + 1 step Wk+1 = Wk γ Gk = Wk γU Gk Hk+1 = Xk+1Wk+1 = Xk+1Wk γXk+1X = Xk+1U Wk γXk+1U = Xk+1 Wk γ Xk+1 = Xk+1 Wk+1 = Hk+1, Gk = Gk = Gk = i.e., the activations sent to the server are identical for {X , W1}, { , W1} pairs. Since the above proof is true for any orthogonal matrix Rdd, there exist continually many different pairs of client data that produces the same model training protocol result. A.2 Fake gradients The proof of Lemma 2. Proof. For simplicity, assume training with full-batch gradient: Xk = X. It is noteworthy that the proof remains equivalent for gradients computed w.r.t. both batch and full-batch scenarios. Also let us write instead of the and denote an arbitrary vectors sent by server as Gfake. Consequently, client transmits Hk+1 = XWk+1 in each step. Then, the following is true for the weight matrix: 16 Wk+1 = Wk γX Gfake = = (cid:0)Wk1 γX Gfake k1 (cid:34) = = W1 γX (cid:1) γX Gfake (cid:35) = (cid:88)"
        },
        {
            "title": "Gfake\ni",
            "content": ". i=1 Hk+1 = XWk+1 = XW1 γXX (cid:35)"
        },
        {
            "title": "Gfake\ni",
            "content": ", (cid:34) (cid:88) i=1 Hk+1 = XW1 γ X (cid:35)"
        },
        {
            "title": "Gfake\ni",
            "content": "= XW1 γXX (cid:34) (cid:88) i=1 (cid:35)"
        },
        {
            "title": "Gfake\ni",
            "content": ". (cid:34) (cid:88) i=1 The only term that has changed is the first summand XW1. Similarly to the previous case, the server can recover X. But now it cannot recover the real data X. Indeed, the server can only build its attack based on the knowledge of = XU and X . This means that it cannot distinguish between two different pairs {X , } if they generate the same values X . Notice that, by simply multiplying given pair {X , } by another orthogonal matrix it is possible to construct continuum of other pairs { ˆX = V, ˆU = } that produce the same values X, X : {X , } { ˆX = V, ˆU = } = XU = (XV )(V ) = ˆX ˆV , X = (XU )(U ) = (XV )(U X ) = ( ˆX ˆU )( ˆU ˆX ) = ˆX ˆX , and correspondingly. Therefore, even malicious server that sabotages the protocol cannot distinguish between all the possible distributions. In addition we should note, that multiplying by some orthogonal matrix should not affect the quality of training, because the only term that contains in the final formula for activations is the first summand XW1 = XU W1. However we can rewrite this formula as XU W1 = W1, where W1 = W1. In this way, it is as if we have changed the initialization W1 of the weight matrix, that usually do not affect final model quality. Before we delve into the strongest lemmas, we also note, that the results of Lemmas 1 and 2 holds for the one-layer linear client-side model with bias, i.e., when Hk+1 = Xk+1Wk+1 + Bk+1. Corollary 2. Lemmas 1 and 2 are correct under their conditions even if the update of client-side model includes bias. 17 Proof. It is sufficiently to show only for Lemma 2. For simplicity, assume training with full-batch gradient: Xk = X, since that the proof remains equivalent. Let us denote an arbitrary vectors sent by server as Gfake. Client transmits Hk+1 = XWk+1 + Bk+1 in each step. Weights update: Wk+1 = Wk γX Gfake = = (cid:0)Wk1 γX Gfake k1 (cid:34) = = W1 γX (cid:1) γX Gfake (cid:35) = (cid:88)"
        },
        {
            "title": "Gfake\ni",
            "content": ". Biases update: i=1 Bk+1 = Bk γ Bk = Bk γ Hk Hk Bk = Bk γI Hk = = B1 γ (cid:34) (cid:88) i=1 (cid:35) , Hi recall that the serve transmits the fake gradient Gfake instead of Hi , thus Bk+1 = B1 γ (cid:35) Gfake . (cid:34) (cid:88) i=1 Activations update: Hk+1 = XWk+1 + Bk+1 = XW1 + B1 γ + XX (cid:34) (cid:35) (cid:88) i=1 Gfake = H1 γ (cid:34) + XX (cid:35) (cid:88) Gfake , i=1 Hk+1 = XW1 + B1 γ + X (cid:34) (cid:35) (cid:88) i=1 Gfake = XW1 + B1 γ + XX (cid:34) (cid:35) (cid:88) i=1 Gfake = H1 γ (cid:34) + XX (cid:35) (cid:88) Gfake . i=1 As in Lemma 2, only the first summand XW1 has changed. And the server cannot distinguish between two different pairs {X , } if they produce values X . Therefore, we can replay the conclusion of Lemma 2. A.3 Cut Layer The proof of Cut Layer Lemma 3. Proof. Given the client neural network we denote the last activations before the linear Cut Layer as Z. Then, we provide client with output activations = ZW and show that the model training protocol remains the same after transforming = ZU and W1 W1 = W1. 18 We define the clients previous parameters (before ) as θ and function of this parameters as fθ : fθ(θ, X) = Z."
        },
        {
            "title": "Then",
            "content": "f (X, θ, ) = = fθ(θ, X)W = ZW, = L(H) = L(ZW ). Let us consider the gradient of loss w.r.t. and θ: W θ = = H H W Z = , (cid:20) θ θ = (cid:21) H = H , where = θ Jacobian of fθ. Thus, after the first two iterations we conclude: H1 = Z1W1, θ2 = θ1 γJ 1 H 1 , W2 = W1 γZ 1 H1 , and H2 = Z2W2 = fθ(θ2, X)W2 = fθ(θ2, X)W1 γfθ(θ2, X)Z H1 . Adding the additional orthogonal matrix results in: W1 = W1, H1 = Z1 W1 = (Z1U ) W1 = H1, W2 = W1 γ L H1 In fact derivative w.r.t θ is obviously the same, because trick was constructed in such way that for any input both initial loss and loss after trick have the same value L(Z) = L(Z), i.e. it is the same function and has the same derivative w.r.t and therefore w.r.t θ. θ1 = H1 H1 Z1 Z1 = 1 H1 1 = Z1 θ θ . = 1 H1 1 = 1 H (U W1)U Then, for the activations obtained with and without we claim: H2 = Z2 W2 = (θ2, X)U W2 = (θ2, X)U W1 γf (θ2, X)U 1 H1 = H2. And that completes the proof. A.4 Convergence Additional notation. For matrices, . is spectral norm on the space Rnm, while the .F refers to the Frobenius norm. 19 A.4.1 Rotation example. Before we turn to the consideration of the PL-case, we present simple example for the general non-convex (even 2D) case. We deal with the function (y) = y2 + 6 sin2 y, where = X. We show that convergence of to the global minimum (y = 0, (y) = 0) breaks after the rotation of weights and data X, i.e., Adam converges to local minimum of . Example 1. Indeed, let the initial weight and data vectors equal: (cid:16) = 1.915 + (cid:17) 2 0.6, , = (1, 0) . We rotate these arguments by an angle of π 4 with: = (cid:21) (cid:20)1 1 1 1 . 1 2 In addition we pick the learning rate γ = 0.6. After that, the optimization algorithm stack in the local minima if starting from (U W, X) point. Which we show in Figure 7. Figure 7: While optimizing the non-convex function (x), Adam can get stuck in the local minima in depence on the initialization. A.4.2 Convergence for PL function. Adam. Next, we move to the PL case. In this part of the work we study the convergence of Adam on PL functions under the (semi)orthogonal transformation of data and weights {X, } { X, } = {XU, }. We need the following assumptions in our discourse. 20 Assumption 1 (L-smoothness). Function is assumed to be twice differentiable and L-smooth, i.e., W, dom we have L(W ) L(W ) LW . Assumption 2 (Polyak-Łojasiewicz-condition). Function satisfies the PL-condition if there exists µ > 0, such that for all dom holds where = L(W ) is optimal value. L(W ) 2µ(L(W ) L), In addition, we need the assumption on boundedness of gradient (a similar assumptions are made if [8]). Assumption 3. We assume boundedness of gradient, i.e., for all L(W ) Γ. In Adam[36], the bias-corrected first and second moment estimates, i.e. mk and ˆDk are: mk = 1 β1 1 βk 1 (cid:88) i=1 βki 1 L(Wi), ˆD = 1 β2 1 βk 2 (cid:88) i=1 βki 2 diag (L(Wi) L(Wi)) . And the starting m0, ˆD2 0 (with little abuse of notation W0 = W1 in this part only) are given by m0 = L(W0), ˆD2 0 = diag (L(W0) L(W0)) , with the following update rule: ˆmk+1 = β1 ˆmk + (1 β1)L(Wk), ˆD2 k+1 = β2 ˆD2 + (1 β2) diag(L(Wk) L(Wk)). Then, the iteration of Adam is simply: Wk+1 = Wk γ ˆD1 Despite the promising fact for (S)GD that continually many { X, } pairs consistently produce the same activations = at each step, we cannot make the same statement for Adam-like methods. mk. Remark 4. Let { X, } = {XU, } pairs are an orthogonal(semi-orthogonal) transformations of data and weights. Then, these pairs, in general, do not produce the same activations at each step of the Split Learning process with Adam. Proof. Indeed, assume that after k-th step of training, the client substitutes {X, Wk} pair with {XU, Wk}. Compare the discrepancy between the activations Hk+1 = Xk+1Wk+1 and Hk+1 = Xk+1 Wk+1. Since, with Adam, Wk+1 = Wk γ ˆD1 ˆmk, the difference arises in the updates of first and ˆmk. and second moment (biased) estimates: ˆD1 k"
        },
        {
            "title": "Recall",
            "content": "ˆD2 β2 ˆD2 k1 = (1 β2) diag = (1 β2) diag = (1 β2) diag ( Hk=Hk) = (1 β2) diag = (1 β2) diag the similar holds for ˆmk (cid:19) Wk (cid:32) Hk Wk (cid:18) Wk Hk (cid:18) L Hk (cid:18) L Hk (cid:18) L Hk (cid:33) Hk Wk (cid:19) Hk L Hk L Hk L Hk (cid:19) (cid:19) , ˆmk β1 ˆmk1 = (1 β1) = (1 β1) Wk (1 β1) L Hk = (1 β1) L Hk Hk Hk Wk = (1 β1)U L Hk . ( Hk=Hk) = Then, it is clear how to compare the activations at + 1-th step Hk+1 = Wk+1 = XWk γXU ˆD1 ˆmk, Hk+1 = XWk+1 = XWk γX ˆD1 ˆmk. The difference occurs in ˆmk and ˆD2 k. Let us specify ˆD ˆD2 = (1 β2) (cid:34) (cid:18) diag L Hk L Hk (cid:19) (cid:18) diag Hk Hk (cid:19) (cid:35) , for the squared preconditioner, and (cid:34) ˆmk ˆmk = (1 β1) L Hk Hk (cid:35) = (1 β1) (cid:2)U I(cid:3) Hk , for the first moment estimator. Therefore, descrepancy between Hk+1 and Hk+1 vanishes when ˆD1 ˆmk = ˆD1 ˆmk. The other thing we can do is analyze whether Adam, with the substitution of pairs {X, } { X, }, can converge to the same optimal value. Let W1 be an initial weight matrix of the one-layer client linear model from Lemma 1. Our goal is to prove that Adam with modifications = XU , W1 = W1, where is proper orthogonal matrix, converges to the same optimum as the original one. Considerations are made similar to [63]. 22 Remark 5. The difference occurs in the function to be optimized. When we deal with the data and weight transformation we consider the entire model (client-side with the servers top model and loss) as function of data and weights . As function of two variables L(X, ) = L( X, ), because the activations after the first dense layer of clients model are equal: W = XW . However, in the proof of convergence we are interested only in the dependence of on weights . To keep in mind that different functions are optimized, we denote: as function with input data X, and L(W ) = LX(W ), g(W ) = X(W ), as function with input data X. For this case, we require more stronger assumption than 3: Assumption 4 (Modified boundedness of gradient). X, LX(W ) Γ. Similarly, Assumption 1 and Assumption 2 are required quantifier, as we assume that depends on the data. We start with the next technical lemma: Lemma 4. For all 1, we have αI ˆDk ΓI, where 0 < α Γ. Proof. Firstly, note that ˆDk is diagonal matrix, where all elements are at least α. From this, the property αI ˆDk directly follows. Next, we prove the upper bound on ˆDk by induction: 1. Base case, = 0: Using the structure of diag (cid:16) (cid:17) g( W1) g( W1) , we obtain: ˆD2 0 = (cid:16) (cid:17)(cid:13) (cid:13) (cid:13) g( W1) g( W1) (cid:13) (cid:13) (cid:13)diag = (cid:13) (cid:13)diag (cid:0)U g(U W1) g(U W1)(cid:1)(cid:13) (cid:13) (cid:13)U g(U W1)(cid:13) (cid:13) (cid:13)U g(U W1)(cid:13) = (cid:13) 2 2 (cid:13) (cid:13) 2 (cid:13)g(U W1)(cid:13) = (cid:13) 2 2 Γ2. (cid:13) 2. Induction step, > 0: Let ˆDk1 ΓI by induction hypothesis. Then, k1 + (1 β2) diag (g(Wk1) g(Wk1)) k1 + (1 β2) diag (g(Wk1) g(Wk1)) ˆD2 = (cid:13) (cid:13)β2 ˆD2 (cid:13) (cid:13) (cid:13)β2 ˆD2 (cid:13) (cid:13) (cid:13)β2 ˆD2 (cid:13) (cid:13) (cid:13)β2 ˆD2 (cid:13) β2 diag (g(Wk1) g(Wk1))2 (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)2 k1 + diag (g(Wk1) g(Wk1))2 + (1 β2) diag (g(Wk1) g(Wk1))2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 β2Γ2 + Γ2 β2Γ2 = Γ2. The same is true for any other LX. In the next Lemmas 5 and 6 we omit the dependence on dataset X, since that analysis relies only on theresult from Lemma 4 and (modified) assumptions. 23 Lemma 5 (Descent Lemma). Suppose the Assumption 1 holds for function L. Then we have for all 0 and γ, it is true for Adam that L(Wk+1) L(Wk)+ γ 2α L(Wk)mk2 (cid:19) (cid:18) 1 2γ 2α Wk+1Wk2 ˆDk γ L(Wk)2 ˆD1 . Proof. To begin with, we use L-smoothness of the function and αI ˆDk L(Wk+1) L(Wk) + L(Wk), Wk+1 Wk + L(Wk) + L(Wk), Wk+1 Wk + Applying update Wk+1 = Wk γ ˆD1 mk, we obtain 2 2α Wk+1 Wk2 Wk+1 Wk2 ˆDk . L(Wk+1) L(Wk) + L(Wk) mk, γ ˆD1 mk + 1 γ ˆDk(Wk Wk+1), Wk+1 Wk + 2α Wk+1 Wk2 ˆDk =L(Wk) γL(Wk) mk, mk ˆD1 + 1 γ ˆDk(Wk Wk+1), Wk+1 Wk 2α + Wk+1 Wk2 ˆDk =L(Wk) + γL(Wk) mk, L(Wk) mk ˆD1 2α γL(Wk) mk, L(Wk) ˆD1 (cid:18) 1 γ (cid:19) Wk+1 Wk2 ˆDk =L(Wk) + γL(Wk) mk2 (cid:19) ˆD1 Wk+1 Wk2 ˆDk . (cid:18) 1 γ 2α γL(Wk) mk, L(Wk) ˆD1 Using denotation Wk+1 = Wk γ ˆD1 we have L(Wk) and once again update Wk+1 = Wk γ ˆD1 mk, L(Wt+1) L(Wk) + γL(Wk) mk ˆD1 1 γ Wk+1 Wk+1, Wk Wk+1 ˆDk (cid:19) (cid:18) 1 γ 2α Wk+1 Wk2 ˆDk =L(Wk) + γL(Wk) mk2 ˆD1 (cid:19) (cid:18) 1 γ 2α Wk+1 Wk2 ˆDk (cid:16) 1 2γ Wk+1 Wk+12 ˆDk =L(Wk) + γL(Wk) mk2 + Wk Wk+12 ˆDk (cid:19) (cid:18) 1 γ 2α ˆD1 Wk+1 Wk2 ˆDk (cid:17) Wk+1 Wk2 ˆDk (cid:16) 1 2γ γ2L(Wk) mk2 ˆD1 + γ2L(Wk)2 ˆD1 Wk+1 Wk2 ˆDk (cid:17) . Finally, the next lemma concludes our discourse. 24 Lemma 6. Suppose that the Assumptions 1, 2 for function hold. Then we have for all 0 and γ the following is true: L(Wk+1) (cid:16) 1 (cid:17) γµ Γ (L(Wk) L) γ 2α (cid:18) 1 2γ L(Wk) mk2 (cid:19) 2α Wk+1 Wk2 ˆDk . + Proof. According to Lemma 5, L(Wk+1) L(Wk)+ γ 2α Γ ˆD1 With PL-condition and 1 L(Wk)mk2 , we have γ 2α L(Wk)2 γ 2Γ L(Wk+1) L(Wk) + L(Wk) mk2 L(Wk) + γ 2α L(Wk) mk2 γµ Γ (L(Wk) L) . (cid:19) (cid:18) 1 2γ 2α Wk+1 Wk2 ˆDk γ 2 L(Wk)2 ˆD1 . (cid:19) (cid:18) 1 2γ 2α Wk+1 Wk2 ˆDk (cid:19) (cid:18) 1 2γ 2α Wk+1 Wk2 ˆDk A.5 Differentially Private defense In addition to obfuscation-based defenses, mentioned in Section 1, we consider the DP defense against UnSplit Model Inversion (MI) attack. Before further cogitations, we give several definitions. Definition 1. Two datasets and are defined as neighbouring if they differ at most in one row. Definition 2. Mechanism is (ε, δ)-DP (Differential Private) [7] if for every pair of neighbouring datasets and every possible output set the following inequality holds: [M(X) S] eεP [M(X ) S] + δ. If we call two datasets, and = XU , \"neighbouring\" (It should be noted that this is not the same as Definition 1), the standard Differential Privacy technique is not applicable in this case. Indeed, let us consider two \"neighbouring\" matrices and XU . We define subspace of Rnn as (cid:8)XX + α α [0, 1](cid:9). Then, for the protocol which does not include randomness itself, we conclude: P(A(X) S) = 1, P(A(XU ) S) = 0 which, in turn, corresponds to ε = , which does not correspond to Definition 2. Thus, including randomness inside the protocol is essential. Therefore, we use mechanism M(X) = A(X) + that should satisfy given differential privacy guarantee for each pair of two neighbouring datasets, already in accordance with the Definition 1. We consider is random variable. Let us explain the estimations on amount of noise required to ensure the mechanism M(X) = A(X) + (5) satisfies given differential privacy guarantee. We consider is zero mean isotopic Gaussian perturbation (0, σ2I). Definition 3. The global L2 sensitivity of function is following: = sup XX A(X) A(X )F. 25 To estimate the global L2 sensitivity, we plug A(X) = X, take as an average batch (to cope with the fact that there are often lots of zeroes in each batch), take as with replacement of random row with zeroes, and then perform the calculation of as in Definition 3. Theorem 1 (see Theorem 8 from [1]). Let be function with global L2 sensitivity . For and ε > 0 and δ (0, 1), the mechanism described in Algorithm 1 from [1] is (ε, δ)-DP. Thus, in our code we add implementation of Algorithm 1 from [1] to estimate σ, then apply the mechanism (5) and, finally, do experiments with Differential Privacy. We perform numerical experiments to test Differential Privacy defence against an attack from [11]. See results in Table 3. As one may note, in this case defence presents, but is far from perfect."
        },
        {
            "title": "B Discussion of the defense frameworks",
            "content": "The development of attacks and defensive strategies progresses in parallel. Corresponding defense strategies are broadly categorized into cryptographic [3, 48, 2, 82, 24, 66] and non-cryptographic defenses. The former technique provides strict privacy guarantee, but it suffers from computational overhead [11, 90, 55] during training which is undesirable in the context of SL. Consequently, cryptographic defenses are not considered in our work. Non-cryptographic defenses include: Differential Privacy (DP) mechanisms [6, 29, 45, 52], data obfuscation [21, 20, 67, 74, 60] and adversarial training [40, 75, 73, 67, 72, 10, 83, 15]. Since the DP solution does not always satisfy the desired privacy-utility trade-off for the downstream task [17, 91], the most common choice [86] is to use data obfuscation and adversarial training strategies to prevent the feature reconstruction attack."
        },
        {
            "title": "C Additional experiments",
            "content": "MSE across different classes. In addition to Figures 1 to 4 and 6, we also report the Cut Layer (Z-space) and Reconstruction (X -space) mean square error dependencies on the exact number of classes. Measure MSE w.r.t. each class is much more convenient and computationally faster than the analogues experiment for FID. We show those MSE between true and reconstructed features after the UnSplit attack on CIFAR-10, F-MNIST and MNSIT in Figure 8. As, generally, MSE do not describe the reconstruction quality by itself (unlike the FID, which can be validated with benchmarks [31]), we observe that MSE for MLPbased models are, approximately, 10 times largen than for CNN-based, meaning that the discrepancy between the original and reconstructed data should be more expressive in case of client-side model filled with dense layers. At the same time, an interesting thing we see in Figure 8: Cut Layer MSE is much more lower than Reconstruction MSE for the MLP-based model architecture, while these MSE values are approximately the same for CNNs. This fact reflects the Cut Layer Lemma 3. Provided figures contribute to the knowledge of which classes in the dataset are more sensitive to feature reconstruction attacks. We also highlight that combining these findings with any of the defense methods may significantly weaken the attackers side, or(and) the non-label party can concentrate on the defense of specific classes from the dataset. Architectural design. Since we claim that feature reconstruction attacks are not useful against MLP-based models, we aim to alleviate certain concerns about the experimental part of our work. In particular, we answer the following question: What is the exact architectural design of the MLP-based models? The exact architectures are available in our repository. Additionally, we specify that it is four-layer MLP with ReLU() activation functions for MNIST and F-MNIST datasets, and we use the PyTorch [57] MLP-Mixer implementation from 3 repository. All CNN-based models are listed in the original UnSplit and FSHA papers repositories 4 and 5. Nevertheless, we should stress that the main findings of our paper are independent of any specific architecture and work with any MLP-based model. 3https://github.com/omihub777/MLP-Mixer-CIFAR 4https://github.com/ege-erdogan/unsplit 5https://github.com/pasquini-dario/SplitNN_FSHA Figure 8: MSE across different classes for the UnSplit attack. (Top row): CIFAR-10 MLP-Mixer and CNN-based models. (Middle row): F-MNIST MLP and CNN-based models. (Bottom row): MNIST MLP and CNN-based models. The key requirement is that the architecture should consist mostly of linear layers and activation functions between them, thus, convolutional layers should be omitted. Experiments on small MLP model. In addition to our main experiments with MLP-based architectures Table 1 and Figures 1 to 4, 6 and 8, we also report results from experiments using small MLP-based model (SmallMLP). Unlike the models in Table 1,the SmallMLP architecture does not match the accuracy of the CNN-based model but is designed to match its number of parameters. Table 2: Number of parameters for different models across. # Parameters / Model # MLP 2,913, MLP-Mixer 146,816 CNN 45,278 SmallMLP 7,850 Specifically, the SmallMLP architecture consists of two-layer linear model with ReLU() activation function. The clients part of the model includes one linear layer, therefore, we follow the conditions of Lemma 1 in this case. As we observe in Figures 9 and 10, even small MLP-based model with less number of parameters than the CNN model remains resistant to the UnSplit attack. The same for the FSHA attack we conclude from Figures 11 and 12. However, the accuracy of SmallMLP is significantly lower compared to our four-layer MLP, with an average accuracy of 92.6% vs. 98.5% on MNIST, and 83.8% vs. 88.3% on F-MNIST. Figure 9: Results of UnSplit attack on MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): SmallMLP client model. Figure 10: Results of UnSplit attack on F-MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): SmallMLP client model. Figure 11: Results of FSHA attack on MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): SmallMLP client model. Figure 12: Results of FSHA attack on F-MNIST. (Top): Original images. (Middle): CNN-based client model. (Bottom): SmallMLP client model. 28 Table 3: Estimated inputs with and without adding noise for various Cut Layers for the MNIST, F-MNIST, and CIFAR-10 datasets. The \"Ref.\" row display the actual inputs, and the next rows display the attack results for different split depths. We took the following noise variance for different datasets: σ = 1.6 for MNIST, σ = 2.6 for F-MNIST, σ = 0.25 for CIFAR-10. Note that theoretical value of σ for CIFAR-10 is 7.1, but we decided to lower it due to neural network learning issues. Split Layer #"
        },
        {
            "title": "With Noise",
            "content": "Ref. 1 2 3 4 6 Ref. 1 2 3 5 6 Ref. 1 2 4 5"
        }
    ],
    "affiliations": [
        "ISP RAS, MIPT",
        "MIPT"
    ]
}