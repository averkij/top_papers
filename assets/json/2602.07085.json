{
    "paper_title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
    "authors": [
        "Jun Han",
        "Shuo Zhang",
        "Wei Li",
        "Zhi Yang",
        "Yifan Dong",
        "Tu Hu",
        "Jialuo Yuan",
        "Xiaomin Yu",
        "Yumo Zhu",
        "Fangqi Lou",
        "Xin Guo",
        "Zhaowei Liu",
        "Tianyi Jiang",
        "Ruichuan An",
        "Jingping Liu",
        "Biao Wu",
        "Rongze Chen",
        "Kunyi Wang",
        "Yifan Wang",
        "Sen Hu",
        "Xinbing Kong",
        "Liwen Zhang",
        "Ronghao Chen",
        "Huacan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts."
        },
        {
            "title": "Start",
            "content": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining Jun Han1*, Shuo Zhang2*, Wei Li1*, Zhi Yang1*, Yifan Dong1, Tu Hu2, Jialuo Yuan3, Xiaomin Yu2, Yumo Zhu1, Fangqi Lou1, Xin Guo1, Zhaowei Liu1, Tianyi Jiang4, Ruichuan An4, Jingping Liu5, Biao Wu2, Rongze Chen2, Kunyi Wang2, Yifan Wang2, Sen Hu2,4, Xinbing Kong6, Liwen Zhang1, Ronghao Chen2,4, Huacan Wang2 2026-02-10 1SUFE , 2QuantaAlpha , 3Stanford, 4PKU, 5SYSU, 6SEU *These authors contributed equally to this work. Correspondence: xinbingkong@126.com, zhang.liwen@shufe.edu.cn, chenronghao@alumni.pku.edu.cn, wanghuacan17@mails.ucas.ac.cn https://github.com/QuantaAlpha/QuantaAlpha"
        },
        {
            "title": "Abstract",
            "content": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poors 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts. 6 2 0 2 6 ] . - [ 1 5 8 0 7 0 . 2 0 6 2 : r Figure 1: Cumulative excess returns of different approaches on CSI 500 and S&P 500. AIFin Lab: aifinlab.sufe@gmail.com QuantaAlpha: quantaalpha.ai@gmail.com"
        },
        {
            "title": "Introduction",
            "content": "Financial markets are high-dimensional, non-stationary stochastic systems, where returns exhibit heavy tails (Fama, 1965), time-varying volatility (Engle, 1982), and cross-sectional dependence (Pesaran, 2021). Quantitative investment therefore relies on alpha mining to extract predictive signals from noisy observations. Recently, large language models (LLMs) (Lopez-Lira and Tang, 2023) and LLM-based agent frameworks (Zhang et al., 2024; Xiao et al., 2024) have been introduced into the factor research workflow. By leveraging advanced reasoning and code generation capabilities, these systems can automate factor construction and iteratively refine candidates through backtesting feedback. Figure 2: Comparison with existing methods: QuantaAlpha improves alpha discovery through trajectory-level self-evolution. Representative agent frameworks for alpha mining simulate the workflow of human quantitative researchers by iteratively performing hypothesis generation, executable factor construction, and backtestingbased refinement, as illustrated in Figure 2. Under this paradigm, AlphaAgent (Tang et al., 2025b) imposes explicit regularization during factor generation to curb factor crowding and mitigate alpha decay, while RD-Agent (Li et al., 2025d) targets full-stack automation with coordinated research and development agents, enabling joint factormodel co-optimization for robust strategy discovery. These frameworks reduce the trial-and-error costs of factor research while preserving interpretability relative to parameter-trained methods, making large-scale exploration practical. However, under real-market non-stationarity and low signal-to-noise ratios, existing systems still face three limitations. (i) Fragile controllability: Iterative refinement is driven by noisy backtest feedback, which can induce semantic drift and steer updates toward spurious correlations, gradually deviating from the intended economic mechanism. (ii) Limited trustworthiness: Many methods rely on stochastic re-generation conditioned on transient contexts, without explicitly inheriting validated rationales across iterations. As result, they lack traceable lineage and the produced factors are harder to audit and trust. (iii) Constrained exploration: Search often over-exploits local neighborhoods around initial seeds, leading to redundancy and factor crowding, while providing limited systematic coverage of the broader hypothesis space and weakening long-horizon discovery. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that improves factor quality through trajectory-level self-evolution. Inspired by (Lin et al., 2025), we view each end-to-end mining run as mining trajectory and explicitly evolve trajectories rather than relying on unconstrained re-generation from noisy feedback. First, to mitigate local optimization, we introduce Diversified Planning Initialization that generates multiple complementary research directions, yielding broad initial coverage in the hypothesis space. Second, to support controllable refinement and reliable reuse of validated experience, QuantaAlpha performs self-evolution via trajectory-level mutation and crossover. Mutation performs targeted revision by localizing the failure-causing step via self-reflection and rewriting only the corresponding segment, while keeping the rest of the trajectory intact. Crossover recombines complementary segments from high-reward parents to reuse validated hypothesis structures, construction patterns, and repair behaviors. We further apply generation constraints to enforce semantic consistency and constrain complexity and redundancy, preventing drift and crowding. Together, these mechanisms expand exploration while stabilizing refinement under real-market noise and non-stationarity. Extensive experiments on CSI 300 and cross-market transfer to CSI 500 and S&P 500 demonstrate consistent improvements in predictive power and strategy performance over strong baselines and prior agentic systems."
        },
        {
            "title": "2 Related Work",
            "content": "Agents in Finance Recent advances in financial LLMs (Liu et al., 2023, 2025) and evaluation benchmarks (Guo et al., 2025; Tang et al., 2025a) have substantially expanded the scope of automated financial reasoning. Building on these foundations, growing line of work explores how agentic systems (Li et al., 2025a; Yang et al., 2026) can move beyond financial understanding toward decision-oriented research workflows, including factor discovery and trading analysis. In this context, the pursuit of alpha factors has evolved from manual engineering and heuristic search toward LLM-driven closed-loop discovery (Li et al., 2024b; Shi et al., 2025b; Chen et al., 2025; Wang et al., 2025). Beyond search optimization, recent efforts emphasize workflow systematization. RD-Agent (Li et al., 2025d) proposes multi-agent framework that decouples the pipeline into research and development stages, enabling data-centric joint optimization of factors and models. To address market non-stationarity, AlphaForge (Shi et al., 2025a) focuses on the dynamic combination of mined factors, while Alphafin (Li et al., 2024a) and AlphaEval (Ding et al., 2025) establish standardized, task-oriented protocols for reproducible evaluation. Despite these advances, trading constraints (e.g., turnover, complexity) remain largely post-hoc filters rather than intrinsic objectives, leading to suboptimal generalization and interpretability in live trading. Self-Evolving Agents Self-evolving agents (Fang et al., 2025; Zhai et al., 2025; Lin et al., 2025; Zhang et al., 2026) represent paradigm shift from static instruction-following to autonomous learning through interacting with environment. AlphaEvolve (Novikov et al., 2025) demonstrates coding-centric evolution where the agent employs evolutionary resampling to autonomously generate algorithms for scientific discovery. Building on this, CSE (Hu et al., 2026) introduces controllable self-evolution, facilitating critical transition from stochastic generation to feedback-driven evolution. The financial domain adapts this self-evolutionary framework to handle non-stationary and high-dimensional data. Research has converged on multi-agent coordination and structured feedback to guide evolution (Li et al., 2025b). TradingAgents (Xiao et al., 2024) and FactorMAD (Duan et al., 2025) utilize institutional-style debates to refine trading hypotheses, while QuantAgents (Li et al., 2025c) and ATLAS (Papadakis et al., 2025) incorporate simulated trading performance as reward signal for dynamic prompt optimization. To ensure consistency over long horizons, FinMem (Yu et al., 2025) and FinCon (Yu et al., 2024) leverage hierarchical memory and conceptual reinforcement to retain and refine high-level trading experience. Despite these advancements, the transition of self-evolving agents to finance is hindered by the low signal-to-noise ratio and non-stationary. To mitigate this, our QuantaAlpha employs stable optimization units with mutation and crossover operators, ensuring robust, traceable evolution path through structured archiving of iteration logic."
        },
        {
            "title": "3 Problem Setup",
            "content": "Alpha Mining The alpha mining task aims to learn an alpha factor from market feature tensor RN D for stock universe = {s1, . . . , sN } and time horizon = {t1, . . . , tT }. At each 3 time , the factor produces signal from the feature slice Xt RN that is used to predict the cross-sectional return yt+1 RN . In notation, an alpha can be written as (Xt) yt+1, where yt+1 denotes the realized returns at time + 1. Accordingly, we formulate alpha mining as the following optimization problem: = arg max L(cid:0)f (X), y(cid:1) λR(f ), (1) where denotes the space of all possible factor expressions, is the ground-truth return target (e.g., next-day returns), L() measures predictive effectiveness, R() is regularization term that encourages simplicity and novelty of the expression, and λ balances utility and regularization. Alpha Mining Trajectory Distinct from purely data-driven pipelines, we introduce market hypotheses to guide LLM-based factor construction. Each alpha mining run follows an end-to-end workflow from hypothesis generation to factor generation and backtesting evaluation (see Section 4.1). We represent each run by mining trajectory, defined as an ordered sequence τ = (s0, a0, s1, a1, . . . , sn), where s0 denotes the initial mining context (e.g., market context and optional user-provided seeds), ai is the action taken at step by the multi-agent system, and sn is the terminal state containing the evaluated result of this run. The quality of trajectory is measured by its terminal reward: R(τ ) = L(cid:0)fτ (X), y(cid:1) λR(fτ ), (2) where fτ denotes the factor produced by trajectory τ . Objective Our objective is to find trajectory generation mechanism π for the multi-agent system that maximizes the expected terminal reward of the generated mining trajectory: π = arg max π Eτ π (cid:2)R(τ )(cid:3), (3) where τ π denotes the trajectory induced by repeatedly applying π from the initial state s0."
        },
        {
            "title": "4 Method",
            "content": "As illustrated in Figure 3, our approach treats alpha mining as an agentic research workflow rather than one-shot factor construction procedure. Given market context and user-provided seeds, multi-agent system produces complete mining trajectory that records hypothesis generation, controllable factor construction, code implementation, and backtesting-based evaluation. We first describe the end-to-end mining workflow that instantiates and evaluates single trajectory (Section 4.1). Building on these evaluated trajectories, we then introduce an evolutionary optimization scheme that iteratively improves mining quality via self-evolution process (Section 4.2). 4.1 Alpha Mining Process standalone alpha mining process involves sequential progression starting from hypothesis generation, followed by factor creation and concluding with backtesting-based evaluation. We translate this process into the collaborative efforts of multiple agents, each responsible for specific task at different stages of the workflow. 4.1.1 Hypothesis Generation We use the idea agent Ai to generate market hypotheses via structured knowledge integration. Conditioned on (i) observations from current market conditions or previous mining trajectories, (ii) domain priors from financial theories and empirical evidence, and (iii) parametric specifications (e.g., 10-day high/low), Ai produces actionable hypotheses that describe candidate market mechanisms and serve as the input to subsequent factor generation. Figure 3: Overview of the QuantaAlpha framework. Our approach consists of four core components: (A) Diversified Planning Initialization to generate candidate hypotheses, (B) Factor Realization that iteratively instantiates hypotheses into executable factors with constraint gating, (C) Self-Evolution that applies mutation and crossover over evaluated trajectories, and (D) Final Factor Pool that consolidates validated effective factors. 4.1.2 Controllable Factor Construction To effectively instantiate hypothesis-driven factors, we need robust mechanism to align implementations with domain hypotheses. However, directly generating factor code is brittle, often suffering from syntax errors, dependency mismatches, and semantic drift, causing mismatch between the code and the hypothesis.To address these limitations, We introduce an intermediate symbolic representation based on standardized operator library and an Abstract Syntax Tree (AST). This abstraction bridges high-level market intent and low-level implementation, enabling controllable construction, structural validation, and reliable compilation. Factor Generation Given hypothesis and raw features , the factor agent Af generates symbolic expression over the operator library and parses it into an AST as the intermediate representation. Concretely, it first maps to structured semantic description that formalizes the intended mechanism into discrete operators, while concurrently instantiating operational parameterssuch as look-back windows and thresholdsfrom either prescribed parameters in or heuristic defaults anchored in domain expertise. Conditioned on d, the agent then assembles an operator composition into symbolic expression and parses it into an AST, denoted as (f ). In (f ), leaf nodes bind to raw feature fields (e.g., $high, $volume) and internal nodes correspond to operator instances from (e.g., TS_MIN(), SMA(), RANK()), thereby rendering the computational dependencies and data flow fully transparent. Finally, the agent translates the validated symbolic form into executable code via compilation; when compilation fails due to implementation issues, an LLM-based repair step is triggered to regenerate consistent implementation while preserving the semantics of . This design ensures that code generation remains anchored to an explicit symbolic specification rather than unconstrained free-form generation. Consistency Verification We enforce semantic consistency across representations to prevent drift during generation. Specifically, we apply an LLM-based verifier to assess (i) alignment among the hypothesis h, semantic description d, and symbolic expression , and (ii) faithfulness between the symbolic definition and the generated code c. The verifier returns consistency decision under fixed criteria. If it fails, we rewrite the inconsistent component(s) by regenerating and or repairing until the check passes or maximum retry budget is reached. Complexity and Redundancy Control We further regularize factor generation with explicit structural constraints to promote parsimony and novelty. For complexity, we compute C(f ) = α1 SL(f ) + α2 C(f ) + α3 log(cid:0)1 + Ff (cid:1), where SL(f ) measures symbolic length, C(f ) counts free parameters (e.g., window sizes), and Ff is the set of raw features used by . For redundancy, we quantify structural similarity via AST matching. Given two factors fi and fj with ASTs (fi) and (fj), we define s(fi, fj) as the size of their largest common isomorphic subtree: (4) s(fi, fj) = max SiT (fi),Sj (fj ),Si=Sj (Si). (5) = Sj denotes subtree isomorHere, Si and Sj range over subtrees of (fi) and (fj), respectively, Si phism, and (Si) is the number of nodes in Si. For candidate factor , we compute its maximum similarity against an existing alpha zoo as S(f ) = max ϕZ s(f, ϕ). (6) We reject and rewrite any factor that violates the prescribed complexity or redundancy limits, promoting parsimonious generation while avoiding near-duplicate structures. 4.1.3 Factor Evaluation The evaluation agent Ae assesses factors via standardized backtesting system. The protocol covers three complementary aspects: predictive capability metrics for forecasting effectiveness, return performance metrics for profit-generating potential under fixed execution setting, and risk control metrics for stability and robustness across market conditions. Beyond factor scoring, the agent maintains an evaluation history that records outcomes and summarizes systematic patterns among successful and failed factors. 4.2 Evolutionary Alpha Mining We improve alpha discovery via iterative optimization over mining trajectories. Starting from userprovided seed factors, we generate diversified set of initial hypotheses. We then run the end-to-end mining workflow in Section 4.1 to turn each hypothesis into an evaluated mining trajectory, forming an initial trajectory pool T0 = {τ 0 }. We then apply an iterative evolution process to obtain progressively improved factors. 2 , . . . , τ 0 1 , τ Ninit 4.2.1 Diversified Planning Initialization Let Z0 denote the user-provided seed factors (or seed ideas). We employ an initialization agent A0 to propose diversified set of initial hypotheses H0 = {h0 }. The agent is instructed to maximize complementarity among hypotheses at both semantic and structural levels, e.g., varying signal sources (price vs. volume), time scales (short-term vs. long-term), and mechanism types (momentum vs. mean-reversion or regime-conditioned triggers). For each h0 H0, we run the mining workflow in Section 4.1 and obtain mining trajectory τ 0 . This diversified initialization expands the effective search frontier by providing broad starting coverage in the hypothesis space. As result, the subsequent evolutionary process can explore multiple promising regions in parallel, significantly reducing the risk of premature convergence to suboptimal local optimum. 1, . . . , h0 Ninit 4.2.2 Self-Evolution The goal of self-evolutionary updates is to guide the alpha mining system to search for better trajectories based on existing ones. Specifically, we apply trajectory-level operators to existing mining trajectories to generate improved trajectories as demonstrations. These demonstrations provide imitation learning priors that bias subsequent trajectory generation toward effective decisions. In practice, we instantiate the update with two evolutionary primitives, Mutation and Crossover, which revise single trajectory or recombine complementary sub-trajectories, respectively. Each iteration yields new trajectory generation Ti, and factor quality improves progressively across iterations (see Figure 7 in Appendix for case study). Mutation Given mining trajectory τ Ti1, the primitive first performs self-reflection to diagnose sub-optimal decision node that most significantly accounts for the low terminal reward, denoted by an index {0, . . . , 1}. We then rewrite only the localized action (or short local segment) while keeping the remaining steps unchanged: τchild = (cid:0)s0, a0, . . . , sk, Refine(ak), k+1, k+1, . . . , (cid:1), (7) where the prefix up to sk is frozen, and the remaining steps are regenerated by the agent conditioned on this fixed prefix to preserve trajectory coherence. Depending on the localized fault, the rewrite may update the hypothesis, the symbolic expression under the operator library, or the compiled code, and can introduce mechanism-level changes such as altering the time scale, or adding regime conditions. Crossover Crossover aims to exploit and inherit validated components by recombining complementary sub-trajectories from high-quality parents. At iteration 1, we select parent subset Pi1 Ti1 based on trajectory reward (Eq. 2). Given parent trajectories τ (1), . . . , τ (k) Pi1, Crossover synthesizes new child trajectory by composing high-performing segments from different parents: τchild = Crossover(cid:0)τ (1), . . . , τ (k)(cid:1). (8) Operationally, the primitive identifies specific trajectory segments that consistently contribute to high cumulative rewardssuch as hypothesis templates, factor construction patterns, or strategic repair actionsand merges them into single, highly coherent sequence. This recombination mimics the human practice of combining complementary strengths from different solutions, while providing more credible lineage by explicitly inheriting decisions that have been validated in previously successful trajectories."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Datasets and Metrics We conduct experiments on the CSI 300 dataset, which covers 300 large-cap A-share stocks in the Chinese market. We adopt chronological split with training (Jan. 1, 2016 to Dec. 31, 2020), validation (Jan. 1, 2021 to Dec. 31, 2021), and testing (Jan. 1, 2022 to Dec. 26, 2025). We evaluate performance from two complementary perspectives. Factor predictive power is measured by Information Coefficient (IC), IC Information Ratio (ICIR), Rank IC, and Rank ICIR. Strategy performance is measured by Annualized Return (ARR), Information Ratio (IR), Maximum Drawdown (MDD), and Calmar Ratio (CR). Further details are provided in Appendix A.1. Baselines We compare QuantaAlpha against four baseline categories: traditional machine learning models, deep learning time-series models, classical factor libraries, and LLM-based factor research agents, including RD-Agent and AlphaAgent. More details are provided in Appendix A.3. 5.2 Main Results Table 1 reports the main results on the CSI 300 market over four-year evaluation period. QuantaAlpha outperforms all baselines on both factor predictive power and strategy-level performance. Using GPT-5.2, QuantaAlpha achieves the best IC at 0.1501, delivers an ARR of 27.75%, and maintains low MDD of 7.98%. From real-world perspective, this performance indicates that the mined factors can support production trading under standard risk controls, rather than being artifacts of backtest noise. Similarly, we observe consistent gains across different base models, suggesting that the improvements are robust rather than model-specific. Compared with RD-Agent, QuantaAlpha and AlphaAgent both incorporate generation-stage regularization, and this is reflected in stronger predictive power and strategy performance. Under GPT-5.2, QuantaAlpha improves IC by 0.0970 and ARR by 17.84% relative to RD-Agent while reducing MDD by 6.84%, suggesting that constraining hypothesis-to-factor construction with explicit intermediate representations and verification effectively reduces semantic drift and improves implementation reliability. Beyond these constraints, QuantaAlpha further improves upon AlphaAgent by 0.0535 IC and 12.21% 7 Table 1: Performance comparison of different methods on CSI 300. We report both factor predictive power and strategy-level performance metrics, where higher values indicate better performance for all metrics (with the exception of MDD). For each method category, the best result in each column is highlighted in bold, while the second-best result is underlined. Across all methods, the overall best-performing method is highlighted with light blue background . Methods Factor Predictive Power Strategy Performance IC ICIR Rank IC Rank ICIR IR (SHR*) CR ARR (%) MDD (%) Machine Learning Deep Learning Factor Libraries Linear XGBoost CatBoost LightGBM MLP DoubleEnsemble GRU Transformer LSTM TRA Alpha158(20) Alpha158 Alpha360 0.0155 0.0175 0.0162 0.0247 0.0321 0.0213 0.0321 0.0331 0.0331 0.0421 0.0051 0.0131 0. Classical Factor Mining 0.1174 0.1336 0.1203 0.2055 0.2780 0.1670 0.0368 0.0420 0.0405 0.0423 0.0438 0.0408 0.2834 0.3417 0.3289 0.3726 0.4088 0.3372 0.2603 0.2702 0.2502 0.3402 0.0329 0.0817 0.0636 0.0442 0.0451 0.0451 0. 0.0184 0.0334 0.0306 0.3601 0.3801 0.3503 0.4203 0.1177 0.2119 0.1889 RD-Agent AlphaAgent QuantaAlpha 0.0352 Qwen3-235B Deepseek-V3.2 0.0401 Gemini-3-pro-preview 0.0481 0.0527 Claude-4.5-sonnet 0.0531 GPT-5.2 LLM-based Agentic Factor Mining 0.3950 0.4250 0.4750 0.5050 0.5150 0.0485 0.0522 0.0583 0.0623 0.0633 0.2850 0.3250 0.3900 0.4250 0.4300 0.0952 Qwen3-235B Deepseek-V3.2 0.0955 Gemini-3-pro-preview 0.0804 0.1092 Claude-4.5-sonnet 0.0966 GPT-5.2 Qwen3-235B 0.1252 0.1338 Deepseek-V3.2 Gemini-3-pro-preview 0.1124 0.1111 Claude-4.5-sonnet 0.1501 GPT-5. 0.6632 0.6637 0.5366 0.7718 0.6344 0.8672 0.8533 0.7391 0.6374 0.9110 0.1019 0.0919 0.0784 0.1062 0.0942 0.1226 0.1300 0.1086 0.1077 0.1465 0.6471 0.6450 0.5290 0.7596 0.6237 0.8577 0.8304 0.7205 0.6211 0. -0.3078 -0.5280 -0.2807 0.0092 0.1716 0.2490 0.5302 0.4502 0.6802 1.0502 0.5044 0.4099 0.6009 0.6502 0.8202 1.0202 1.2202 1.2502 2.1053 1.9230 1.7821 2.2739 1.9328 2.6114 2.8797 2.5189 2.6459 3. -0.1407 -0.1488 -0.1077 0.0032 0.0804 0.1233 0.2405 0.3773 0.4058 0.8002 0.2087 0.2620 0.3550 0.3591 0.4332 0.5499 0.6488 0.6687 1.5945 1.4746 1.4729 2.0246 1.2056 2.2859 2.6007 2.1503 3.2615 3. -2.67 -4.24 -2.30 0.07 1.46 1.85 3.61 5.21 6.01 6.81 4.63 2.66 4.09 7.20 7.81 8.81 9.81 9.91 15.10 14.51 14.11 16.48 15.54 20.55 23.77 20.32 22.70 27. 18.97 28.50 21.35 21.80 18.15 15.00 15.01 13.81 14.81 8.51 22.19 10.15 11.52 20.05 18.03 16.02 15.12 14.82 9.47 9.84 9.58 8.14 12.89 8.99 9.14 9.45 6.96 7. ARR while reducing MDD by 4.91%, highlighting the added value of trajectory-centric self-evolution: mutation broadens exploration via mechanism-level variations, while crossover reuses validated trajectory segments and repair strategies, jointly increasing the yield and stability of high-quality factors under non-stationary market dynamics. 5.3 Ablation Study Ablation of Evolutionary Mining Components We conduct an ablation study to quantify the contribution of three components in evolutionary alpha mining: diversified planning initialization, trajectory mutation, and trajectory crossover. The results are summarized in Table 2. Removing planning yields only marginal changes in IC and Rank IC, but substantially degrades strategy-level outcomes, with ARR decreasing by 7.78% and MDD increasing by 2.73%. This indicates that diversified initialization primarily improves the search frontier by providing broader and less correlated starting hypotheses, which stabilizes subsequent evolution. In contrast, removing mutation causes the largest drop in predictive power, decreasing IC by 0.0292 and Rank IC by 0.0284, and also leads to the largest reduction in ARR by 9.81%. This highlights mutation as the primary driver of effective exploration and trajectory repair, enabling the system to escape suboptimal regions and correct failure modes discovered during mining. Meanwhile, 8 Figure 4: Ablation study of semantic consistency, complexity, and redundancy controls. removing crossover leads to smaller but consistent degradation. This supports the role of crossover in exploiting and inheriting complementary high-performing trajectory segments, improving efficiency and stability by recombining validated patterns from successful trajectories. Table 2: Ablation study of evolutionary mining components. Method Key Metrics IC Rank IC ARR (%) MDD (%) QuantaAlpha - w/o Planning - w/o Mutation - w/o Crossover 0.1493 0.1488-0.0005 0.1201-0.0292 0.1423-0.0070 0.1458 0.1452-0.0006 0.1174-0.0284 0.1381-0.0077 28.99 21.21-7.78 19.18-9.81 26.17-2.82 9.42 12.15+2.73 9.85+0.43 10.63+1. Ablation of Consistency, Complexity, and Redundancy Controls We ablate three controls during factor generation that gate rewriting, including semantic consistency verification, complexity regularization, and redundancy filtering. As shown in Figure 4, disabling any single control consistently degrades performance, confirming that each contributes non-trivially to robust factor generation. The consistency control prevents semantic drift between the hypothesis, symbolic specification, and implementation. The complexity control improves robustness by discouraging overly complex expressions that generalize poorly, and its removal leads to the most pronounced degradation at the strategy level, with the annualized excess return dropping by 8.44% and the maximum drawdown increasing by 2.57%. The redundancy control preserves exploration by filtering near-duplicate structures and mitigating factor crowding. Disabling all three yields the largest degradation, indicating that these controls are complementary and jointly needed for reliable generation. 5.4 More Analysis Factor Generalizability To evaluate factor robustness under significant market distribution shifts, we conduct zero-shot transfer experiment where the factors mined and selected on CSI 300 are directly deployed on CSI 500 and the S&P 500 without any re-optimization or market-specific adaptation. As shown in Figure 1, QuantaAlpha demonstrates substantially stronger out-of-distribution generalization than all baselines. clear divergence emerges around December 2023, when competing methods begin to stagnate under shifts in market microstructure and volatility regimes, whereas QuantaAlpha sustains stable upward trajectory. By the end of the evaluation period, it reaches cumulative excess return of roughly 160% on CSI 500 and over 137% on the S&P 500. These results indicate that the discovered 9 factors transfer beyond the source market and retain effectiveness under distribution shifts, rather than relying on market-specific historical patterns. Alpha Decay Figure 5 reports annual IC and Rank IC on the CSI 300 universe from 2021 to 2025. pronounced performance collapse is observed for the baselines in 2023, coinciding with major regime shift in the A-share market. The pre-2023 period is dominated by large-cap core assets, where institutional trading supports relatively stable trends and makes classical momentum or mean-reversion signals effective. In 2023, the market rotates toward small-cap and thematic stocks, together with higher intraday noise, more frequent overnight gaps, and weaker trend persistence. Baseline methods, whose factor libraries implicitly assume smooth trends and regular reversal patterns, fail to transfer to this out-of-distribution environment. QuantaAlpha, by comparison, maintains strong IC and Rank IC through the regime transition. It discovers more structural factors, such as Mean-Reverting Range Deviation and Overnight Gap Structure, that reflect persistent microstructure effects including volatility clustering and overnight information incorporation. These signals are less sensitive to capitalization style, enabling better temporal generalization. Figure 5: Annual IC and Rank IC comparison on the CSI 300. Table 3 provides factor-level diagnosis of the alpha decay observed in 2023, grounding the analysis jointly in market style changes, factor semantics, and empirical factor statistics. Rather than enumerating factor formulas, we focus on which information channels remain predictive under the 20222023 regime transition and why. Market Style Transition and Its Implications The CSI 300 market undergoes pronounced style transition in 2023 relative to the training period (20162020). The earlier regime is dominated by large-cap core assets, with high institutional participation, smooth intraday dynamics, and persistent short-horizon trends. Under such conditions, classical momentum, mean-reversion, and exhaustion-based factors are effective.In 2023, leadership rotates toward small-cap and thematic stocks. This shift is accompanied by increased intraday noise, frequent overnight gaps driven by call auctions, and rapid cross-style liquidity re-allocation. These changes weaken trend persistence and amplify path-dependent noise, directly challenging factor constructions that rely on stable intraday structure or fast mean reversion. Factor Semantics Aligned with Market Microstructure The empirical contrast between QuantaAlpha (QA) and AlphaAgent (AA) in 2023 reflects differences in the semantic composition of their factor libraries. (i). Overnight and auction information. Gap-based factors aggregate information released during non-trading hours and price discovery in the opening auction. As shown in Table 3, multiple QA factors from this channel occupy the right tail of the 2023 Rank IC distribution, while maintaining near-complete coverage  (Table 4)  . This indicates that overnight information becomes dominant and stable signal when intraday predictability deteriorates. Table 3: Representative factors and their performance metrics on the CSI 300 index in 2023. Factor (representative) Rank IC IC Interpretation (short) QA strong performers (overnight/auction, trend-quality, and liquidity re-rating) GapZ10_Overnight_vs_TR 0.0793 0.0335 Normalized overnight gap magnitude relative to recent true range; captures auctionGap_IntradayAcceptanceScore_20D 0.0744 0. driven shocks and subsequent adjustment. Acceptance vs. rejection of an overnight gap using intraday direction, scaled by recent volatility. Gap_IntradayAcceptance_VolWeighted_20D 0.0606 0.0314 Gap acceptance score weighted by abnormal volume; emphasizes information-rich openings with high participation. CleanTrend_Continuation_Score_RS10_WVMA 0.0590 0.0267 Trend continuation conditioned on high trend quality (low residual noise) and muted OrderlyTrend_x_Absorption_10D_5D_20D 0.0465 0.0271 intraday/volume pressure. Orderly short-horizon trend cross-validated by liquidity absorption (high dollar volume with low price impact). QA weak performers (overly rigid gates or noisy-path proxies under rotation) KineticLength_AbsRetSum_Z_10D -0.0720 -0.0246 Path-length proxy (choppiness): can behave like noise detector, but may invert under fast style rotation. Drawdown_Gated_NegCorr_60D_20D_thr20pct -0.0282 -0.0095 Hard regime gate based on deep drawdown; brittle when drawdowns cluster and cross-sectional regimes shift quickly. HighClose_Shock_With_VolSync_60_20 -0.0274 -0.0090 Shock-day breakout quality (close-in-range, range shock, returnvolume sync); sensitive to regime-dependent follow-through. AA strong performers (exhaustion/climax-style reversals) Exhaustion_Intensity_Index_10D 0.0323 0.0159 Price displacement over 60D interacted with volume intensity ratio; targets potential exhaustion and reversal. Climax_Exhaustion_Intensity 0.0242 0.0160 Variant using short-horizon volume climax vs. long-horizon baseline; aims to identify capitulation-like turns. Exhaustion_Volume_Instability_Index 0. 0.0117 Trend deviation combined with volume instability; highlights fragile price levels AA weak performers (bottom-fishing under non-stationary liquidity) supported by unstable liquidity. Relative_Volume_Calm_Reversal -0.0279 -0.0188 Quiet-volume regimes multiplied by momentum divergence; may fail when liquidity conditions change abruptly. Volume_Stability_Momentum_Divergence_40D -0.0247 -0.0155 Robust volume-stability proxy (MAD) times momentum spread; sensitive to turnover regime changes. LVR_Bottom_Fishing_20D -0.0190 -0.0144 Bottom-fishing reversal with intraday rejection and volume surge; vulnerable when reversals are short-lived and crowded. Table 4: Summary statistics of annual factor predictability on the CSI 300 index in 2023. Metric Coverage ratio (valid metrics) Share with Rank IC > 0 Mean Rank IC Max Rank IC Min Rank IC Share with Rank IC > 0.03 Share with Rank IC > 0. Mean IC QA AA 0.98 0.626 0.0057 0.0793 -0.0720 0.102 0.0272 0.80 0.594 0.0012 0.0323 -0.0279 0.0156 0.0000 0. 0.0015 (ii). Volatility structure and range-based signals. Range deviation factors conditioned on volatility clustering capture abnormal variability rather than directional trends. In 2023, these signals remain predictive despite elevated noise, consistent with the persistence of volatility clustering across market styles. Their contribution is reflected in the heavier positive Rank IC tail of QA relative to AA. (iii). Trend quality and liquidity re-rating. QA emphasizes trend continuation only when supported by low residual volatility and improving liquidity (e.g., rising dollar volume with limited price impact). This conditioning filters out noise-driven pseudo-trends prevalent in small-cap rotation, explaining why QA retains higher fraction of strong-performing factors, whereas raw momentum proxies degrade. Factor Statistics as Supporting Evidence The factor-level statistics in 2023 provide quantitative support for the semantic interpretation above. QA achieves substantially higher coverage and heavier right tail of Rank IC, including non-trivial share of factors exceeding moderate predictability thresholds, whereas AA exhibits compressed tails and fewer robust signals. These differences are consistent with the dominance of overnight, volatility-structure, and trend-quality channels under the 2023 market style. key distinction is that QA explicitly encourages semantic diversity through its factor mutation mechanism. By generating and recombining heterogeneous primitives across multiple information channels, QA avoids concentration on single market hypothesis. This diversity increases the probability that subset of factors remains aligned with the prevailing market microstructure after style transition, thereby mitigating regime-specific alpha decay. Overall, the 2023 diagnosis indicates that alpha robustness under distribution shift is driven by alignment between market style and factor semantics, supported by sufficient diversity across information channels. QAs ability to maintain performance arises not from fitting specific regime, but from sustaining broad and adaptive factor population whose predictive subsets persist across market style transitions. Evolutionary Alpha Mining Efficiency We analyze the iterative dynamics of factor mining by tracking the IC distribution across generation rounds. As visualized in Figure 6, QuantaAlpha consistently maintains the highest IC across all five iterations, indicating that its evolutionary updates improve factor quality more effectively than both AlphaAgent and RD-Agent. Notably, QuantaAlpha exhibits rapid gain in the early rounds and then remains stable at high level, suggesting strong sample efficiency in converting early exploration into consistently predictive factors. In addition to the higher mean, QuantaAlpha shows moderate but persistent spread of IC across rounds, reflecting sustained diversity in the explored factor candidates rather than premature collapse to narrow region. By comparison, RDAgent remains lower with relatively homogeneous IC profiles, consistent with weaker exploration and higher risk of generating crowded signals. AlphaAgent improves over RD-Agent but remains consistently below QuantaAlpha, suggesting that our trajectory-level evolution more efficiently accumulates and reuses successful generation patterns across iterations. Figure 6: The evolution of IC over the first five iterations. 5.5 Case Study Case Study Setup To make the evolution process concrete, we present representative case study and trace how QuantaAlpha updates hypotheses and factors over five iterations. We run QuantaAlpha for total of 15 iterations on CSI300, using Deepseek-V3.2 as the backbone LLM for all agents. To improve mining throughput, we set the factor-complexity constraints to symbol length 200 and base features 4. Each iteration consists of two phases: Mutation phase followed by Crossover phase.At the end of each iteration, we maintain global high-quality factor pool using all factors generated up to that iteration. The maintenance rule is greedy and RankIC-driven: we sort all candidate factors by RankIC (descending) and add them into the pool in order, subject to redundancy constraint. factor is admitted only if its absolute correlation with every factor already in the pool is below 0.7. The pool size is capped at 50% of the total number of mined factors up to the current iteration. Factor Example Figure 7 illustrates how QuantaAlpha refines hypotheses and factor expressions across iterations via the mutation and crossover phases. The highlighted factors in the figure are therefore representative nodes on different traces across iterations. The first iteration yields interpretable shortterm reversal factors. The second broadens the mechanism via volatility-weighted momentum, but 12 Figure 7: case study of iterative hypothesis and factor updates in QuantaAlpha. increased structural complexity coincides with weaker generalization. Subsequent iterations simplify the expression into linear additive form, improving drawdown and stabilizing performance. The fifth iteration adds participant-differentiated behavioral signals, incorporating complementary information and further improving predictability. Throughout the evolution, QuantaAlpha maintains cumulative factor pool. Its predictive performance does not saturate within the first five iterations and only begins to decay after roughly 15 iterations, indicating that the evolution sustains effective improvement over substantial horizon before diminishing returns emerge. Iteration Convergence Figure 8 illustrates the predictive performance of the factor pool from the fifth iteration onwards. We observe that the predictive performance does not increase linearly with the number of iterations. The strategys return capacity and risk control ability are gradually optimized, reaching balanced high level between return and maximum drawdown around the 11th to 12th iterations. Subsequent additional iterations fail to bring significant performance improvement; instead, they may introduce redundant information through newly generated factors, leading to reduced strategy robustness and deteriorated drawdown performance. Overall, the 11th to 12th iterations (approximately 350 factors in total) represent the optimal trade-off point between return level and risk control effect under this experimental setup."
        },
        {
            "title": "6 Conclusion",
            "content": "We present QuantaAlpha, self-evolving framework for interpretable alpha mining that formulates factor discovery as constrained multi-agent research process. Extensive experiments across both Chinese and U.S. equity markets show that QuantaAlpha consistently produces more stable and generalizable factors than all baselines. Beyond empirical performance, QuantaAlpha emphasizes three properties critical for real-world quantitative research: diversity, controllability, and trustworthiness. Diversity is promoted by broad hypothesis exploration and redundancy-aware evolution, improving robustness under nonstationarity; controllability is enabled by symbolic representations and synthesis-time gates that enforce consistency and limit complexity. Trustworthiness is further strengthened by trajectory-level inheritance, where crossover recombines high-performing segments from previously successful trajectories, providing verifiable lineage of validated mechanisms and repair patterns. Future work will explore extending QuantaAlpha to multi-asset and cross-market settings, incorporating adaptive regime-aware evolution, and 13 Figure 8: Factor Pool Performance by Iteration. integrating the agentic discovery process more tightly with portfolio construction and risk management. More broadly, we believe that agentic evolution provides promising paradigm for discovery problems in high-noise, non-stationary domains beyond finance."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the National Social Science Fund of China Project under Grant No. 22BTJ031; and the Shanghai Engineering Research Center of Finance Intelligence under Grant No. 19DZ2254600. We acknowledge the technical support from the Qinghai Provincial Key Laboratory of Big Data in Finance and Artificial Intelligence Application Technology."
        },
        {
            "title": "References",
            "content": "Binqi Chen, Hongjun Ding, Ning Shen, Jinsheng Huang, Taian Guo, Luchen Liu, and Ming Zhang. 2025. Alphasage: Structure-aware alpha mining via gflownets for robust exploration. arXiv preprint arXiv:2509.25055. Hongjun Ding, Binqi Chen, Jinsheng Huang, Taian Guo, Zhengyang Mao, Guoyi Shao, Lutong Zou, Luchen Liu, and Ming Zhang. 2025. Alphaeval: comprehensive and efficient evaluation framework for formula alpha mining. arXiv preprint arXiv:2508.13174. Yitong Duan, Chuheng Zhang, and Jian Li. 2025. Factormad: multi-agent debate framework based on large language models for interpretable stock alpha factor mining. In Proceedings of the 6th ACM International Conference on AI in Finance, pages 605613. Robert Engle. 1982. Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation. Econometrica: Journal of the econometric society, pages 9871007. Eugene Fama. 1965. The behavior of stock-market prices. The journal of Business, 38(1):34105. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, and 1 others. 2025. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407. Xin Guo, Haotian Xia, Zhaowei Liu, Hanyang Cao, Zhi Yang, Zhiqiang Liu, Sizhe Wang, Jinyi Niu, Chuqi Wang, Yanhui Wang, and 1 others. 2025. Fineval: chinese financial domain knowledge evaluation benchmark for large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62586292. Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, and 1 others. 2026. Controlled self-evolution for algorithmic code optimization. arXiv preprint arXiv:2601.07348. 14 Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Kp Subbalakshmi, Jimin Huang, and 1 others. 2025a. Investorbench: benchmark for financial decision-making tasks with llm-based agent. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25092525. Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, and Jun Huang. 2024a. Alphafin: Benchmarking financial analysis with retrieval-augmented stock-chain framework. In Proceedings of the 2024 joint international conference on computational linguistics, language resources and evaluation (LREC-COLING 2024), pages 773783. Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, and Xiangmin Xu. 2025b. Hedgeagents: balanced-aware multi-agent financial trading system. In Companion Proceedings of the ACM on Web Conference 2025, pages 296305. Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, and Xiangmin Xu. 2025c. Quantagents: Towards multi-agent financial system via simulated trading. arXiv preprint arXiv:2510.04643. Yuante Li, Xu Yang, Xiao Yang, Minrui Xu, Xisen Wang, Weiqing Liu, and Jiang Bian. 2025d. R&d-agent-quant: multi-agent framework for data-centric factors and model joint optimization. arXiv preprint arXiv:2505.15155. Zhiwei Li, Ran Song, Caihong Sun, Wei Xu, Zhengtao Yu, and Ji-Rong Wen. 2024b. Can large language models mine interpretable financial factors more effectively? neural-symbolic factor mining agent model. In Findings of the Association for Computational Linguistics ACL 2024, pages 38913902. Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Hongzhang Liu, Ronghao Chen, Yangfan He, and 1 others. 2025. Se-agent: Self-evolution trajectory optimization in multi-step reasoning with llm-based agents. arXiv preprint arXiv:2508.02085. Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha. 2023. Fingpt: Democratizing internet-scale data for financial large language models. arXiv preprint arXiv:2307.10485. Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, and 1 others. 2025. Fin-r1: large language model for financial reasoning through reinforcement learning. arXiv preprint arXiv:2503.16252. Alejandro Lopez-Lira and Yuehua Tang. 2023. Can chatgpt forecast stock price movements? return predictability and large language models. arXiv preprint arXiv:2304.07619. Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, and 1 others. 2025. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131. Charidimos Papadakis, Angeliki Dimitriou, Giorgos Filandrianos, Maria Lymperaiou, Konstantinos Thomas, and Giorgos Stamou. 2025. Atlas: Adaptive trading with llm agents through dynamic prompt optimization and multi-agent coordination. arXiv preprint arXiv:2510.15949. Hashem Pesaran. 2021. General diagnostic tests for cross-sectional dependence in panels. Empirical economics, 60(1):1350. Hao Shi, Weili Song, Xinting Zhang, Jiahe Shi, Cuicui Luo, Xiang Ao, Hamid Arian, and Luis Angel Seco. 2025a. Alphaforge: framework to mine and dynamically combine formulaic alpha factors. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1252412532. Yu Shi, Yitong Duan, and Jian Li. 2025b. Navigating the alpha jungle: An llm-powered mcts framework for formulaic factor mining. arXiv preprint arXiv:2505.11122. Zichen Tang, Jiacheng Liu, Zhongjun Yang, Rongjin Li, Zihua Rong, Haoyang He, Zhuodi Hao, Xinyang Hu, Kun Ji, Ziyan Ma, and 1 others. 2025a. Finmmr: make financial numerical reasoning more multimodal, comprehensive, and challenging. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32453257. Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, Keze Wang, Jinrui Chen, and Liang Lin. 2025b. Alphaagent: Llm-driven alpha mining with regularized exploration to counteract alpha decay. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pages 28132822. Saizhuo Wang, Hang Yuan, Leon Zhou, Lionel Ni, Heung Yeung Shum, and Jian Guo. 2025. Alpha-gpt: Human-ai interactive alpha mining for quantitative investment. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 196206. 15 Yijia Xiao, Edward Sun, Di Luo, and Wei Wang. 2024. Tradingagents: Multi-agents llm financial trading framework. arXiv preprint arXiv:2412.20138. Zhi Yang, Runguo Li, Qiqi Qiang, Jiashun Wang, Fangqi Lou, Mengping Li, Dongpo Cheng, Rui Xu, Heng Lian, Shuo Zhang, and 1 others. 2026. Finvault: Benchmarking financial agent safety in execution-grounded environments. arXiv preprint arXiv:2601.07853. Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Jordan Suchow, Denghui Zhang, and Khaldoun Khashanah. 2025. Finmem: performance-enhanced llm trading agent with layered memory and character design. IEEE Transactions on Big Data. Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen, Jordan Suchow, Zhenyu Cui, Rong Liu, and 1 others. 2024. Fincon: synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Advances in Neural Information Processing Systems, 37:137010137045. Yunpeng Zhai, Shuchang Tao, Cheng Chen, Anni Zou, Ziqian Chen, Qingxu Fu, Shinji Mai, Li Yu, Jiaji Deng, Zouying Cao, and 1 others. 2025. Agentevolver: Towards efficient self-evolving agent system. arXiv preprint arXiv:2511.10395. Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, and 1 others. 2026. Evofsm: Controllable self-evolution for deep research with finite state machines. arXiv preprint arXiv:2601.09465. Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, and 1 others. 2024. multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. In Proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining, pages 43144325. 16 SUMMARY OF THE APPENDIX This appendix contains additional details for the QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining. The appendix is organized as follows:"
        },
        {
            "title": "A Experiment Settings",
            "content": "A.1 Evaluation Metrics A.2 Backtesting Setup . . A.3 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Algorithm Configuration",
            "content": "C Case Study: Factor Evolution Trajectory . . C.1 Factor Identity . . . C.2 Evolution Lineage . C.3 Synthesized Hypothesis . . C.4 Backtest Performance . . C.5 Trajectory Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 18 18 18 18 20 20 21"
        },
        {
            "title": "A Experiment Settings",
            "content": "This section provides details of our experimental setup, including computational infrastructure, evaluation metrics, backtesting setup, and baselines. A.1 Evaluation Metrics We evaluate predictive performance using two categories of metrics: factor predictive power and strategylevel performance. Without loss of generality, the bar notation denotes the mean, and σ() denotes the standard deviation. Information Coefficient (IC): Pearson correlation between factor values ft and future returns rt+1: ICt = (ft ft1)(rt+1 rt+11) ft ft12 rt+1 rt+112 , where 1 denotes column vector of ones. ICIR: Information ratio of IC, measuring consistency: ICIR = IC/σ(IC). Rank IC: Spearman correlation using rank vectors ft = rank(ft) and rt+1 = rank(rt+1): RankICt = (ft ft ft1)(rt+1 rt+11) ft12 rt+1 rt+112 , where rank() is the rank function applied element-wise to its input vector in ascending order. Rank ICIR: Information ratio of Rank IC: RankICIR = RankIC/σ(RankIC). All strategy metrics are computed on excess returns after transaction costs, where rexcess,t = rportfolio,t rbenchmark,t ctransaction,t. Here, rportfolio,t represents the return of the strategy portfolio, rbenchmark,t denotes the return of the market benchmark, and ctransaction,t accounts for the transaction costs incurred at time t. Information Ratio (IR): IR = (rexcess/σ(rexcess)) Annualized Return (ARR): Annualized excess return over benchmark. Maximum Drawdown (MDD): Largest peak-to-trough decline in cumulative excess returns. Calmar Ratio (CR): CR = ARR / MDD . 252. 17 A.2 Backtesting Setup Backtesting is conducted using the Qlib framework across the CSI 300, CSI 500, and S&P 500 indices, with the data split detailed in Table 5. Factor construction utilizes six basic featuresopen, high, low, t+1 1, where close close, volume, and vwapto predict the next-day return, defined as yt = close denotes the closing price at time t. To ensure robustness against outliers, the preprocessing pipeline includes forward-filling missing values, replacing infinite values, dropping samples with missing labels, and applying cross-sectional rank normalization (CSRankNorm) to both features and labels. t+2 /P close Table 5: Data Split Periods for Train, Validation, and Test Sets across All Markets Train Valid Test 2016-01-012020-12-31 2016-01-012020-12-31 2016-01-012020-122021-01-012021-12-31 2021-01-012021-12-31 2021-01-012021-12-31 2022-01-012025-12-26 2022-01-012025-12-26 2022-01-012025-12-26 Market CSI 300 CSI 500 S&P 500 A.3 Baselines We benchmark against four categories: (1) ML models: Linear Regression (Linear), Multi-Layer Perceptron (MLP), and gradient boosting decision trees including LightGBM, XGBoost, and CatBoost, along with DoubleEnsemble, an ensemble method for financial time series; (2) Deep learning: Recurrent networks such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), the attention-based Transformer, and Temporal Routing Adaptor (TRA); (3) Classical factors: Alpha158 and Alpha360, which are widely used sets of technical factors derived from price and volume; (4) LLM agents: RD-Agent and AlphaAgent, which utilize large language models for automated factor mining. For the LLM-agent baselines (RD-Agent and AlphaAgent), we evaluate multiple backbone LLMs, including Qwen3-235B, DeepSeek-V3.2, Gemini-3-Pro-Preview, Claude-4.5-Sonnet, and GPT-5.2. Unless otherwise specified, all other LLM-based experiments in this paper adopt DeepSeek-V3.2 for consistency and fair comparison."
        },
        {
            "title": "B Algorithm Configuration",
            "content": "This section details the evolution algorithm parameters, factor constraints, and trading strategy configuration. QuantaAlpha employs an evolutionary algorithm with mutation and crossover operations, with LightGBM used as the downstream model for factor-based prediction. In the Planning Phase of the algorithm, we set ten parallel exploration directions to broaden the initial coverage of research space. For the experimental setup, beyond the original round, the algorithm follows fixed iterative process: the main experiment consists of 5 total iterations, and each iteration comprises one Mutation phase followed by one Crossover phasealternating between these two phases to iteratively refine high-quality hypotheses. Additionally, we set rule that each hypothesis attempts to generate 3 factor expressions, ensuring focused yet sufficient exploration of factor candidates under each research direction. To prevent overfitting and ensure interpretability, factor expressionsbuilt using the operators listed in Table 6are restricted by the following constraints: symbol length 250 characters, base features 6, free arguments ratio < 50 We employ TopkDropout strategy for portfolio construction, as detailed in Table 7. On each trading day, stocks are ranked according to their predicted scores; the ndrop lowest-scoring holdings are liquidated and replaced with the highest-ranked candidates to maintain constant portfolio size with equal weighting. Case Study: Factor Evolution Trajectory This appendix presents detailed case study of factor evolution in QuantaAlpha. We trace the complete trajectory of representative factorInstitutional_Momentum_Score_20Dthrough the crossover phase, demonstrating how the evolutionary framework synthesizes complementary market hypotheses from parent trajectories. 18 Table 6: List of Supported Operators for Factor Construction Category Operators Description Time-Series DELTA, DELAY, TS_MEAN, TS_STD, TS_VAR, TS_MAX, TS_MIN, TS_SUM, TS_RANK, TS_CORR, TS_COVARIANCE, TS_ARGMAX, TS_ARGMIN, TS_SKEW, TS_KURT, TS_PCTCHANGE, TS_ZSCORE, TS_QUANTILE Cross-Sectional RANK, ZSCORE, SCALE, MEAN, STD, MEDIAN, MAX, MIN, SKEW, KURT Rolling statistics computed along time axis per instrument Statistics computed across stocks per datetime Mathematical ABS, SIGN, LOG, EXP, SQRT, POW, INV Element-wise mathematical functions Technical Logical Auxiliary SMA, EMA, WMA, MACD, RSI, BB_UPPER, BB_LOWER, DECAYLINEAR, REGBETA, REGRESI Common technical indicators GT, LT, GE, LE, AND, OR, WHERE Comparison and conditional operators COUNT, SUMIF, FILTER, PROD, HIGHDAY, LOWDAY Helper functions for complex expressions Table 7: Parameters for the TopkDropout Trading Strategy Parameter Value Description Portfolio topk n_drop 50 Number of stocks held Stocks dropped per rebalance Transaction Costs Buying Fee Selling Fee 0.05% 0.15% Commission Commission + stamp duty Execution Deal Price Limit Threshold Open 9.5% Next-day opening price Price limit for halt Benchmark China U.S. SH000300/SH000905 CSI 300/CSI500 SPX S&P 500 19 QuantaAlphas evolution process operates in three phases: (1) Original phase where initial hypotheses are generated, (2) Mutation phase where existing trajectories are perturbed to explore diversified strategies, and (3) Crossover phase where high-performing parent trajectories are combined to synthesize offspring with potentially superior predictive power. The following factor card illustrates Crossover operation. C.1 Factor Identity The factor card below presents the basic information of the evolved factor, including its unique identifiers, evolution lineage, and mathematical formulation. Institutional_Momentum_Score_20D c57cace576a95356 df5a496878f4 Factor ID: Trajectory ID: Evolution Round: Round 10 Evolution Phase: Direction ID: Crossover 6 Factor Expression: RANK(TS_CORR(DELTA(close, 1)/close, DELTA(volume, 1)/volume, 20) * TS_MEAN((close - open)/close, 5)) Mathematical Formulation: (cid:32) IMS20D = RANK ρ20 (cid:18) , V (cid:19) (cid:18) (cid:33) (cid:19) , 5 where ρ20(, ) denotes the 20-day rolling correlation, P/P is the daily return, /V is the volume change ratio, ()5 is the 5-day moving average, and and represent the closing and opening prices, respectively. Factor Interpretation: This factor captures institutional-driven momentum by measuring two key signals: (1) the correlation between price returns and volume changes, which indicates coordinated institutional trading when positive; and (2) the average intraday return pattern, reflecting institutional activity that typically influences closing prices. The cross-sectional ranking ensures comparability across stocks. C.2 Evolution Lineage The crossover operation combines insights from two parent trajectories with complementary market hypotheses. Parent 1 focuses on identifying fragile momentum driven by retail speculation, while Parent 2 targets sustainable momentum supported by institutional activity. The LLM synthesizes these complementary perspectives into unified framework. (cid:9) Evolution Information (cid:209) Parent Trajectories: 20 Parent 1: 1e6d57e38e89 Round: Phase: Rank IC: IC: IR: Round 9 Mutation 0.0216 0.0059 1.297 Core Hypothesis: When retail investors exhibit herd behavior and momentum chasing in stocks with high social media activity, but accompanied by declining institutional ownership and deteriorating fundamentals, the resulting price momentum is unsustainable and leads to mean reversion. Parent 2: 47e0f0e55382 Round: Phase: Rank IC: IC: IR: Round 8 Crossover 0.0246 0.0069 1.347 Core Hypothesis: regime-adaptive structural momentum factor combining institutional ownership-driven medium-term price trends with short-term microstructure regime validation, where coordinated accumulation/distribution patterns amplify momentum when confirmed by microstructure alignment. œ Evolution Path Diagram: Parent 1 Round 9 Mutation Rank IC: 0. Parent 2 Round 8 Crossover Rank IC: 0.0246 Offspring Factor Round 8 Crossover Rank IC: 0.0311 C.3 Synthesized Hypothesis Through crossover, the LLM generates new hypothesis that integrates the complementary insights from both parents, rather than simply averaging their factor expressions. This hypothesis-driven approach ensures that the offspring factor captures genuinely novel market dynamics. (cid:17) Hypothesis Core Hypothesis: regime-aware dual-source momentum factor that combines institutional-driven structural momentum (validated by healthy microstructure) and retail-driven speculative momentum (characterized by high attention and deteriorating fundamentals), dynamically weighted by market volatility: amplifying institutional signals in stable regimes and retail reversal signals in turbulent regimes, will generate superior predictive returns. 21 Component Observation Justification Domain Knowledge Description Parent strategies separately targeting institutional trends and retail herding show moderate predictive power (Rank IC 0.020.025), suggesting combined signals could capture complementary market dynamics. Sustainable price trends require institutional sponsorship and orderly trading, while retail-driven bubbles lack fundamental support and reverse under stress; hybrid model exploiting both can enhance robustness across market regimes. Institutional accumulation with strong price-volume correlation and low volatility indicates sustainable momentum; retail herding with declining institutional ownership and high volatility signals fragile momentum prone to reversal. C.4 Backtest Performance After factor construction, QuantaAlpha automatically backtests the generated factors using the Qlib framework. The results below compare the offspring factor against both parent trajectories and the baseline, demonstrating the effectiveness of the crossover operation. Backtest Metrics Metric IC Rank IC ARR (Excess) IR MDD (Excess) Offspring Factor Baseline 0.0126 0.0311 7.80% 0.963 11.37% 0.0058 0.0220 5.20% 0.973 7.30% Detailed Statistics: Metric Value Metric Daily Excess Return (w/o cost) Excess Return Std L2 Train Loss 0.0328% Daily Excess Return (w/ cost) Turnover (FFR) 0.52% L2 Valid Loss 0.9936 Value 0.0128% 100% 0.9962 C.5 Trajectory Summary After evaluating backtest results, the LLM provides structured summary. This trajectory summary loop enables continuous improvement by learning from both successes and failures. (cid:220) Evaluation & Summary (cid:219) Observations: The crossover operation demonstrates trade-off between enhanced predictive accuracy and increased risk exposure compared to the baseline: Significant improvement in annualized excess return and predictive metrics (IC and Rank IC), validating the effectiveness of synthesizing dual-source momentum signals. Increased maximum drawdown and marginal decline in the Information Ratio, suggesting that the offspring factor introduces higher volatility during certain market regimes. 22 8 Hypothesis Evaluation: Results partially support the hypothesis. Improved annualized return and IC suggest that combining institutional and retail momentum signals has merit. However, deterioration in risk metrics indicates that without proper regime-adaptive weighting, the combined signals may amplify risks during turbulent periods. The full hypothesis requires all three components (institutional momentum, retail herding reversal, volatility-adaptive weighting) to work effectively. Decision: REJECTED for direct deployment. . Recommendations: 1. Use 20-day price-volume correlation as institutional momentum proxy; 2. Use 5-day average intraday returns as retail attention proxy; 3. Add volatility regime indicator (recent/historical volatility ratio) for dynamic weighting. This summary will inform the next mutation round, guiding the LLM to simplify the factor expression while preserving the core dual-source concept."
        }
    ],
    "affiliations": [
        "PKU",
        "QuantaAlpha",
        "SEU",
        "SUFE",
        "SYSU",
        "Stanford"
    ]
}