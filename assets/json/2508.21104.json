{
    "paper_title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning",
    "authors": [
        "Wenfeng Feng",
        "Penghong Zhao",
        "Guochao Jiang",
        "Chuzhan Hao",
        "Yuewei Zhang",
        "Hao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 0 1 1 2 . 8 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "PVPO: PRE-ESTIMATED VALUE-BASED POLICY OPTIMIZATION FOR AGENTIC REASONING Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang, Hao Wang Alibaba Cloud Computing {wenfeng.fwf,zhaopenghong.zph}@alibaba-inc.com cashenry@126.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) is machine learning method for learning optimal policies through interaction with the environment. Policy optimization depends on accurately estimating the advantage function to improve the agents actions. In classic actor-critic frameworks, critic network predicts state-value (V ), which combines with action-value (Q) to compute the advantage and then guides policy updates. Recently, research has increasingly focused on more efficient critic-free architectures. These methods do not directly compute the absolute advantage. Instead, they build baselines for relative advantage, simplifying the training process and reducing resource consumption (Shao et al., 2024; Feng et al., 2025b). Grouping policies, as used in critic-free RL methods like GRPO (Shao et al., 2024), become an important research topic. This is not only because they demonstrate superior performance, but also because the removal of the value model saves training resources, enabling researchers to train largerscale models under limited hardware conditions. Although PPO and other actor-critic methods sometimes achieve higher accuracy, critic-free grouping policies are widely used for their practical efficiency. Some studies group by sample, running multiple trajectories within each group to compute relative advantage (Zuo et al., 2025; Lyu et al., 2025). Others group by action or timestep, enabling finer partitioning and more accurate baseline estimation (Feng et al., 2025b; Li et al., 2025a). These methods can improve baseline accuracy for similar trajectories. However, grouping policies usually require more rollouts to boost performance, which greatly increases computational cost. Methods such as DAPO (Yu et al., 2025) aim to mitigate this issue by prioritizing high-value data sampling. However, they primarily redistribute resource utilization rather than achieving genuine reduction in overall resource consumption. We still need to achieve an effective trade-off between training performance and computational cost. To construct the relative advantage, some methods use The first two authors contributed equally Corresponding author"
        },
        {
            "title": "Preprint",
            "content": "state-independent baselines to generate advantage values for each action (Williams, 1992; Ahmadian et al., 2024). GRPO (Shao et al., 2024) and GiGPO (Feng et al., 2025b) compare the rewards of actions or trajectories within groups. In these approaches, the evaluation criterion is derived from the policy itself, which may cause policy optimization to become confined to existing behavior patterns and lead to local optima. From human learning perspective, rollout can be seen as repeated practice. Grouping policies resemble trial-and-error learning, where individuals often compare outcomes to fixed Reference Anchor for more efficient learning. This anchor serves as an objective reference point, distinct from the idealized optimal solutions provided by critic or the dynamic relative performance within group, and establishes more general advantage baseline. In this paper, we introduce Pre-estimated Value-based Policy Optimization (PVPO), generalized RL method based on Proximal Policy Optimization (PPO) (Schulman et al., 2017). PVPO adopts critic-free architecture, is compatible with mainstream group policy RL methods, and maintains low computational cost for grouping, thus effectively combining the strengths of both approaches. Specifically, we use Reference Model (Ref) to run grouping reasoning and calculate task-based reward score as an anchor. This anchor serves as the estimate during RL training, helping to correct the cumulative bias in relative advantage calculations typically observed in large language models (LLMs). In essence, our method decouples and in the grouping policy advantage calculation. The reference anchor is computed in an unsupervised manner and acts as both supplement and an enhancement to the training dataset, without incurring additional time or memory overhead. In summary, our core contributions are as follows. We propose PVPO, an efficient and generalizable approach to critic-free reinforcement learning. PVPO provides stable, low-variance, and globally consistent advantage function, effectively mitigating concerns of error accumulation and policy drift during training. As result, PVPO enables more efficient and robust policy optimization while significantly reducing spatio-temporal overhead. We introduce group sampling strategy that offline filters data with unstable accuracy rates to construct high-quality batches, thereby enhancing convergence and learning efficiency. Furthermore, for samples with zero accuracy, we leverage large-scale LLM to generate ground-truth trajectories, facilitating more effective learning from sparse reward signals. PVPO achieves state-of-the-art performance on multi-step retrieval datasets and demonstrates strong generalization on mathematical reasoning benchmarks. Experimental results indicate that PVPO not only enhances multi-hop question answering and tool-use capabilities, but also improves the overall reasoning ability of LLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 AGENTIC REASONING Leveraging reinforcement learning to drive search represents an important direction in agentic reasoning (Jin et al., 2025; Jiang et al., 2025). Search-o1 (Li et al., 2025b) integrates an agentic search workflow into the reasoning trajectory. This achieves an elegant integration of search and reasoning, sparking wave of subsequent optimizations (Qian et al., 2025; Wang et al., 2025; Feng et al., 2025a). Moreover, numerous studies on Retrieval-Augmented Generation (RAG) (Li et al., 2025b; Feng et al., 2025c; Hao et al., 2025) have advanced the capabilities of LLM in tool use and information retrieval. However, existing studies often directly apply algorithms such as GRPO, which are intrinsically ill-suited to the sparse-reward setting of agentic search. These methods depend on dense token-level rewards, necessitating extensive rollouts to achieve stable advantage estimation. Consequently, the quality of the learning signal becomes tightly coupled with the sample size. Our PVPO framework is tailored for agentic search by decoupling the advantage function (A=QV ), thereby mitigating sample size dependency. While the actual return (Q) leverages the sample size, the advantage baseline (V ) remains independent of both the current and previous policies. This design ensures stable learning signal even under severe reward sparsity (e.g., Q=0), obviating the need for extensive rollouts."
        },
        {
            "title": "2.2 RL FOR LLMS",
            "content": "Recently, reward and advantage computation has been redefined through dynamic generation and iterative optimization, substantially enhancing the performance of critic-free RL methods. Some methods construct denser feedback signals by increasing the frequency of reward generation (Bensal et al., 2025; Chen et al., 2024), while others improve reward adherence by incorporating additional training phases into the learning process (Dong et al., 2025). These approaches often overlook the compounding hallucinations arising from repeated sampling and error accumulation from iterative policy updates. Each incremental policy change alters the rollout distribution, resulting in advantage estimates targeting continually shifting objective and potentially steering the policy toward suboptimal local minima. Moreover, these methods depend heavily on costly online sampling procedures. Another line of research seeks to recover endogenous rewards from the actor model via reverse engineering, process that has been mathematically substantiated (Li et al., 2025c; Zhao et al., 2025). This approach eliminates the need for additional training and enables adaptation to diverse evaluation preferences through prompt adjustment. However, the quality of the recovered reward is inherently limited by the base models capabilities, and consistently guiding reward signals through prompting remains significant challenge (Zhao et al., 2021; Lu et al., 2022; Liu et al., 2023). To address these challenges, the research community has investigated various static approaches. The most prominent is offline reinforcement learning, which optimizes policies using fixed datasets (Kumar et al., 2020; Kostrikov et al., 2022). Another notable class comprises Direct Preference Optimization (DPO) (Rafailov et al., 2023) and its variants (Ethayarajh et al., 2024), which reformulate the objective as direct fit to fixed preference pairs, reducing the reliance on online sampling but constraining generalization. Simpler static methods, such as weighted behavioral cloning (Xu et al., 2022a;b), offer limited expressive power and theoretical guarantees due to their parsimonious advantage estimation. To balance efficiency and adaptability in policy optimization, our approach integrates static with dynamic Q, ensuring stable advantage estimation and low computational overhead while maintaining responsive adaptation to policy updates."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "In this section, we review the fundamental concepts of policy optimization in RL, with particular focus on the role of the advantage function and its various estimation methods. 3.1 PROXIMAL POLICY OPTIMIZATION Actor-critic methods, such as PPO, train critic network Vϕ(s) to provide low-variance estimate of the state-value function π(s) of state s. The state-value function is used to compute the advantage at each time step t, typically via Generalized Advantage Estimation (GAE) (Schulman et al., 2015): ˆAGAE = (cid:88) l=0 (γλ)lδt+l, δt = rt + γVϕ(st+1) Vϕ(st), (1) where λ is hyper-parameter, δt is the temporal difference error at time step t, rt is the immediate reward received at time step t, γ is the discount factor. PPO then optimizes clipped surrogate objective to update the actor network in stable manner: PPO(θ) = EqP (D),oπθold (Oq) (cid:104) min (cid:16) rt(θ) ˆAGAE , clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAGAE (cid:17)(cid:105) , (2) where are questions sampled from the dataset D, are outputs sampled from the old policy πold, importance sampling ratio rt(θ) = πθ(otq,o<t) πθold (otq,o<t) , ϵ is the clipping range of rt(θ). 3.2 GROUP RELATIVE POLICY OPTIMIZATION Since the critic network is typically as large as the actor network, it adds substantial memory and computational burden. Critic-free methods, such as GRPO, eliminate this costly component by estimating the advantage directly from rewards."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The architecture of PVPO. Reference model updates Rref at fixed steps, maintaining value stability and improving the performance lower bound. Reward manager do not restrict the generation of reward. For each question, GRPO generates group of outputs {oi} from the old policy πθold. The advantage for each output oi is then calculated based on normalized reward relative to the group: ˆAi,t = ri mean(r) std(r) . This critic-free advantage estimate is then used to optimize PPO-like objective function: GRPO(θ) = EqP (D),{oi}πθold (Oq) (cid:26) (cid:34) (cid:27)(cid:35) min ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t βDKL[πθπref] , (cid:16) (cid:17) 1 (cid:88) i=1 1 oi oi (cid:88) t=1 (3) (4) where ri,t(θ) = πθ(oi,tq,oi,<t) reference policy πref, β is hyper-parameter. πθold (oi,tq,oi,<t) , DKL is the KL divergence between the trained policy πθ and the"
        },
        {
            "title": "4 METHODOLOGY",
            "content": "In this section, we will introduce our efficient and effective RL algorithm PVPO. The architecture is illustrated in Figure 1. PVPO optimizes the policy via the following objective: PVPO(θ) = EqP (D),{oi}πθold (Oq) (cid:26) (cid:34) 1 (cid:88) i= 1 oi oi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAPVPO i,t , clip (ri,t(θ), 1 ϵ, 1 + ϵ) ˆAPVPO i,t (cid:17) βDKL[πθπref] . (cid:27)(cid:35) where ri,t(θ) = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) , πθ(oi,tq,oi,<t) πθgt (oi,tq,oi,<t) , if oi / GT Traj. if oi GT Traj. 4 (5) (6)"
        },
        {
            "title": "4.1 STATIC V ESTIMATE",
            "content": "In actual policy optimization, the current method is to operate at the group level rather than through single sampling. For problem q, we use the current policy πθ to generate independent trajectories = {τ1, τ2, ..., τN } and obtain the corresponding rewards = {R(τ1), R(τ2), ..., R(τN )} = {r1, r2, ..., rN }. For any step (si,t, ai,t) in specific trajectory τi, the unbiased Monte Carlo estimate of the action value Qπ(si,t, ai,t) is the final reward ri observed in that trajectory. We refer to this as the Dynamic Estimate because it directly reflects the result of single rollout of the current policy: ˆQdyn(τi) = Eτ πθ [R(τi)] = ri. (7) Considering that reward ri is given after the generation of trajectory τi, the trajectory generation process is regarded as atomic actions ai = τi executed from si,0. This atomicity makes the reward distribution of the intermediate state si,t only depend on initial state si,0 (s0) and πi. Consequently, the expected return of the policy is equal to the state value of the initial state π(s0). natural estimation method is to approximate this expectation using the empirical mean of all rewards in the current group. This is the approach adopted by on-policy methods such as GRPO, which we refer to as Dynamic Estimate: ˆVdyn(s0) = ˆVdyn(T ) = 1 (cid:88) j= rj = mean(r). So we obtain the sparse advantage estimate for trajectory τi in the on-policy method: ˆAdyn(τi, s0) = ˆQdyn(τi) ˆVdyn(s0) = ri mean(r). (8) (9) This formula clearly shows that the advantage is calculated as the difference between the immediate reward and the average performance of the current policy πθ within the group. However, ˆVdyn fluctuates wildly with each sampling of the group and is directly affected by πθ, introducing significant instability, especially when the group size is not large enough. To more effectively mitigate the instability associated with dynamic estimation, we propose substituting it with more robust fixed estimate. The ideal baseline should represent Reference Anchor that does not change with current policy iterations. Therefore, we use the expected return of fixed reference policy πref (e.g., the initial policy model) as our Static Estimate ˆVsta. The baseline can be accurately estimated in advance by sampling the reference policy πref times, and update at fixed steps during training process: ˆVsta(s0) = 1 (cid:88) j= = mean(rref). rref (10) This stable static baseline replaces the unstable dynamic baseline in formula 8. We finally obtain the advantage function of PVPO, which is applicable to all sparse reward fields: ˆAPVPO(τi, s0) = ˆQdyn(τi) ˆVsta(s0) = ri mean(rref). For any step (si,t, ai,t) of any trajectory τi, the advantage can also be written as follows: ˆAPVPO i,t = ri mean(rref), (cid:26)max(0.1, racc), ri = 0, if format is correct. if format is incorrect. racc = (cid:26)F1(apred, agt), CEM(apred, agt), if Lpred Lgt. if Lpred < Lgt. (11) (12) (13) (14) where denotes the answer, denotes the length of the answer, pred denotes the prediction, and gt denotes the ground truth. rref is similar. F1 denotes the standard word-level F1 score and"
        },
        {
            "title": "Preprint",
            "content": "CEM denotes Cover Exact Match. represents the multiple of the text length, which we set to 3 by default. This reward is used for agentic search, following Hao et al. (2025). In summary, ˆQdyn(τi) is obtained from the immediate reward of on-policy πθ rollout. It reflects the current performance of the policy and is highly adaptive. The Static Estimate ˆVsta(s0) is obtained from the average reward of the reference policy πref pre-rollout. It provides stable and low-variance performance baseline."
        },
        {
            "title": "4.2 GROUP SAMPLING",
            "content": "Inspired by DAPOs dynamic sampling strategy, we also assess the accuracy of sample rollouts while continuing to utilize the reference model for offline rollouts. For each sample, the mean accuracy of the rollouts serves as the filtering criterion. Specifically, samples are categorized into three groups: Samples with mean accuracy of 1 are excluded from the training set, as they are considered too trivial to facilitate effective learning. Samples with mean accuracy strictly between 0 and 1 are retained, given their nonzero advantage. For samples exhibiting mean accuracy of 0, an additional rollout is conducted using larger LLM for further evaluation. The larger LLM can correctly answer some of these samples. We cache these Ground Truth Trajectories (GT Traj) and their probability distributions. During policy training, GT Traj is injected by replacing one of the generated rollouts for these specific samples. This method mitigates the sparse reward issue commonly encountered with complex samples. In the absence of guidance, the LLM may fail to obtain any positive feedback through unguided exploration. By providing reference trajectory, the model receives an explicit demonstration, which jumpstarts learning by offering clear example of successful reasoning process."
        },
        {
            "title": "5 EXPRIMENTS SETTING",
            "content": "5.1 METRICS For multi-hop question-answering tasks, we employ answer accuracy (Acc, %) and LLM-as-a-Judge (LasJ, %) (Song et al., 2025) as benchmarks. For mathematical reasoning tasks, we employ answer accuracy (Acc, %) as the evaluation metric, specifically reporting the mean accuracy across 32 independent rollouts for each sample (mean acc@32)."
        },
        {
            "title": "5.2 DATA COLLECTION AND PROCESSING",
            "content": "For multi-hop question-answering tasks, we conducted extensive experiments on four multi-step retrieval datasets, including Musique (Trivedi et al., 2022), 2WikiMultiHopQA (2Wiki) (Ho et al., 2020), HotpotQA (Yang et al., 2018), and Bamboogle (Bam) (Press et al., 2023). We trained on Musique, with the original dataset size of 20k and filtered size of 8k after Group Sampling. Evaluations were done on the full development or test sets of these datasets. For mathematical reasoning tasks, we trained on DAPO-Math-17k-Processed (Yu et al., 2025), with an original size of 17k and filtered size of 10k after Group Sampling. We evaluated on five test sets: DAPO-AIME-2024 (AI-MO, 2024; Bytedance & Tsinghua-SIA, 2025), AIME-2025 (Lin, 2025), MATH500 (Lightman et al., 2024; HuggingFaceH4, 2023), AMC23 (AI-MO, 2024), and Olympiad (He et al., 2024). 5.3 ENVIRONMENT AND HYPERPARAMETERS All experiments ran on server with an Intel(R) Xeon(R) Platinum 8369B CPU and eight NVIDIA A100-SXM4-80GB GPUs. We used Qwen2.5-7B-Instruct as the baseline model and Qwen2.5-72BInstruct as the large LLM to generate GT Traj. Rref was updated every 500 steps. The number of"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparisons between PVPO and the baselines on multi-step retrieval datasets. The best and second best results are bold and underlined, respectively. / represents in-domain/outof-domain datasets. Musique Acc LasJ Acc LasJ Acc LasJ Acc LasJ Acc LasJ HotpotQA Bamboogle Avg Acc 2Wiki Prompt Based Qwen2.5-7B-Instruct DeepSeek-R1 O4-mini GPT-4.1-global Gemini-2.5-pro 5.10 13.48 27.90 29.30 22.40 31.04 12.80 17.12 17.05 22.74 32.00 40.70 57.50 59.40 43.00 58.30 66.40 76.64 49.73 58.76 38.00 44.10 61.50 67.40 49.50 67.40 74.40 84.16 55.85 65.77 31.00 40.90 58.00 58.50 44.50 57.70 51.20 61.60 46.18 54.68 42.50 50.80 70.00 71.20 53.00 71.10 75.20 84.48 60.18 69.40 Train Based Qwen2.5-7B-Instruct 13.40 24.90 41.70 43.26 33.40 47.26 36.00 43.20 31.13 39.66 ReSearch GRPO on ReSearch 33.40 46.72 60.80 67.02 54.50 63.68 45.60 54.40 48.58 57.96 GRPO on DynaSearcher 38.90 52.04 74.30 76.77 62.70 68.32 51.20 58.72 56.78 63.96 PVPO on ReSearch 36.50 51.44 70.10 72.36 65.50 72.34 45.60 54.32 54.43 62.62 PVPO on DynaSearcher 46.90 59.44 77.70 80.62 69.00 78.44 50.40 59.68 61.00 69.55 pre-samples in the reference model was set equal to the number of samples in the policy model. Our multi-hop question answering task used the ReSearch (Chen et al., 2025) framework, with batch size 8, rollout count = 5, and 1,000 training steps. DynaSearcher added kg filter during inference, but we used direct inference throughout. The mathematical reasoning task used the veRL (Sheng et al., 2025) framework, with batch size 32, rollout count = 16, and 1,000 training steps. GRPOs loss agg mode was set to the default token-mean instead of seq-mean-token-mean. For evaluation, we set val temperature = 0.6 and val top = 0.95."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "In this section, we run series of experiments. First, we test our method on multi-hop question answering to validate its effectiveness in the agent domain. Then, we test it on mathematical reasoning to show its generalizability. These experiments help us answer the following questions: Q1: Compared to existing RL methods, can PVPO achieve better performance? Q2: Does PVPO show generalizability across different domains? Q3: Can PVPO reduce the temporal resource consumption required for training? Q4: Does PVPO have better stability than baseline methods during training? Q5: Does training with Static Estimate really help the model? 6.1 MAIN PERFORMANCE COMPARISON To answer Q1, we evaluated PVPO on multi-step retrieval datasets and compared the results with two types of benchmarks. The results are shown in Table 1. 6.1.1 PERFORMANCE OF LLMS We compared PVPO with the zero-shot inference performance of leading open-source and closedsource LLMs, including GPT-4.1-0414-global, DeepSeek-R1-0528, O4-mini-0416-global, and Gemini-2.5-pro-0325. The results show that the 7B model trained with PVPO improved significantly, reaching 3.6 times the accuracy of the original model. It outperforms all general LLMs on in-domain datasets. On the out-of-domain dataset, its performance is not as high as in zero-shot"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance comparisons between PVPO and GRPO under different scale models on mathematical reasoning datasets. MATH500 AMC23 Olympiad AIME-2024 AIME-2025 Avg Acc Qwen2.5-7B-Instruct GRPO PVPO Qwen2.5-14B-Instruct GRPO PVPO 78.66 80.30 82.22 83. 49.20 52.02 55.94 56.78 42.26 44.62 49.70 50.72 14.26 14.86 17.14 19. 12.83 14.70 16.90 17.74 39.44 41.30 44.38 45.62 settings. This may be due to differences in training data and model capacity, since these leading models might have seen similar data during pre-training or fine-tuning, while the 7B model has limited capacity. However, its average accuracy is 8 percentage points higher than the average of the four LLMs and about 1 percentage point higher than the best-performing LLM. 6.1.2 PERFORMANCE OF AGENTIC RETRIEVAL METHODS To enable more fair comparison, we also compared PVPO with agentic retrieval methods that require training. In this experiment, all methods were trained for 1,000 steps. We included the direct inference methods of ReSearch and DynaSearcher, as well as baseline that trained these two methods using RL with GRPO. We observed that the performance of PVPO improved by more than 5 percentage points on average compared to GRPO. The result demonstrates the consistent superiority of PVPO across agentic retrieval methods. Overall, this answers Q1: Compared to existing reinforcement learning methods, PVPO achieves state-of-the-art performance. 6.2 GENERALIZATION EVALUATION We shifted from multi-hop question answering to single-shot mathematical reasoning. We chose several math reasoning datasets with varying difficulty, from basic arithmetic to challenging olympiadlevel problems. The goal is to test general reasoning ability in different fields and situations. In the experiment, we compared PVPO with GRPO. The results are shown in Table 2. In terms of average performance, PVPO outperforms GRPO at both model scales. On the 7B model, PVPO scores 1.89 percentage points higher than GRPO. When scaled up to 14B, PVPO continues to lead. PVPO shows clear advantages on most datasets. Overall, PVPO achieved the best results on all five math reasoning datasets. This answers Q2: PVPO is generalizable and keeps stable performance across different fields and tasks. Figure 2: Training efficiency study of PVPO on mathematical reasoning datasets."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Group Sampling study on datasets from different fields. The Acc is the mean of the answer accuracies from trajectories rolled out by the reference model. M=5 in Figure (a) and M=16 in Figure (b)."
        },
        {
            "title": "6.3 TRAINING EFFICIENCY ANALYSIS",
            "content": "6.3.1 CONVERGENCE SPEED We compare PVPO and GRPO to show PVPOs faster convergence speed. During training, we tracked accuracy and rewards, as shown in Figure 2. Figure 2 (a) shows that after about 500 steps, PVPO reaches the accuracy level that GRPO gets after 1,000 steps. Figure 2 (b) shows that PVPO starts with higher reward than GRPO and grows at faster rate. Group Sampling provides highgain samples during training, and the static advantage baseline gives clear direction for policy optimization. Overall, PVPO reaches the target accuracy with fewer steps and converges faster. 6.3.2 TOTAL TIME CONSUMPTION We calculated the data filtering ratio on two training sets, as shown in Figure 3. Group Sampling removes samples with Acc = 1 or 0 before training, filtering out 40%-60% of the total dataset. This leads to 1.72.5 increase in training speed. This does not affect performance, since the model gets effective gradients each iteration after low-value samples are removed. Also, Group Sampling only runs inference once, so the extra overhead is much less than the training cost of keeping lowvalue samples. Experiments show that PVPO greatly reduces total training time. In summary, this answers Q3: PVPO can reduce the temporal consumption of training resources. 6.4 STABILITY EVALUATION We tracked PVPO training metrics to show its stability. Figure 4 (a) shows PVPO achieves much higher average reward than GRPO. With similar KL divergence in Figure 4 (b), this improvement comes not from more aggressive updates, but from better gradient direction estimates. As shown in Figure 4 (c), PVPO has much lower advantage variance, leading to more reliable and consistent update directions. PVPO also maintains exploration without losing stability. Figure 4 (d) shows it keeps higher policy entropy under similar KL constraint, which helps avoid premature convergence to local optimum. Overall, PVPO addresses key problems in RL: it supports high exploration, low variance, and high rewards. This evidence answers Q4: PVPO achieves more stable training than existing methods. 6.5 CASE STUDY: LOW SAMPLING BUDGET We ran case study to address Q5. We reduced the number of rollouts from 5 (used in the main experiments) to 2. For comparison, we report GRPOs performance with full budget. Figure 5 (a) shows that PVPO with low budget remains close to the fully budgeted GRPO. We calculate computational cost by multiplying the number of rollouts with the average number of tool calls in trajectories. As shown in Figure 5 (b), PVPOs average cost is only 4.3, which is much lower than GRPOs 11.7. PVPO achieves 97% of GRPOs performance (55.0% vs. 56.8%) while using less than 40% of the computational cost. This strong sample efficiency comes from the high-quality, low-"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Training stability study of PVPO on multi-step retrieval datasets. Figure 5: Low sampling budget study of PVPO on multi-step retrieval datasets. The denotes the number of trajectories in each single rollout. =5 is the full budget and =2 is the low budget. variance training signals provided by Static Estimate. The model can update its policy efficiently using fewer rollouts. This answers Q5: model training clearly benefits from Static Estimate."
        },
        {
            "title": "7 CONCLUSIONS",
            "content": "In this paper, we propose PVPO, an efficient critic-free reinforcement learning algorithm designed to optimize policy learning for complex tasks. By introducing Static Estimate as an external advantage reference and integrating it with group sampling for effective data filtering, PVPO addresses the limitations of extensive sampling and biased intra-group comparisons inherent in prior methods. Our approach yields stable, low-variance training signals, accelerates convergence, and significantly reduces computational costs. Extensive experiments across nine diverse benchmarks in multi-hop question answering and mathematical reasoning demonstrate that PVPO achieves state-of-"
        },
        {
            "title": "Preprint",
            "content": "the-art performance and strong generalization, even with small-scale models and limited resources. PVPO introduces substantial improvements in reasoning and tool use, supports scalable training, and ensures consistent performance, thereby demonstrating strong potential for widespread real-world application."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1224812267. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.662. URL https://doi.org/10.18653/v1/2024.acl-long.662. AI-MO. Aimo-validation-aime. aimo-validation-aime, 2024. https://huggingface.co/datasets/AI-MO/ AI-MO. AIMO Validation AMC Dataset. https://huggingface.co/datasets/AI-MO/ aimo-validation-amc, 2024. Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, and Waseem AlShikh. Reflect, retry, reward: Self-improving llms via reinforcement learning. arXiv preprint arXiv:2505.24726, 2025. Bytedance and Tsinghua-SIA. AIME-2024. https://huggingface.co/datasets/ BytedTsinghua-SIA/AIME-2024, 2025. Hugging Face Dataset. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=O4cHTxW9BS. Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410, 2025. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025a. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025b. Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Guochao Jiang, Jingyi Song, and Hao Wang. Airrag: Autonomous strategic planning and reasoning steer retrieval augmented generation. arXiv preprint arXiv:2501.10053, 2025c. Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, and Hao Wang. Dynasearcher: Dynamic knowledge graph augmented search agent via multi-reward reinforcement learning. arXiv preprint arXiv:2507.17365, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual"
        },
        {
            "title": "Preprint",
            "content": "multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 38283850. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multiIn Donia Scott, Nuria Bel, hop QA dataset for comprehensive evaluation of reasoning steps. and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp. 6609 6625. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020. COLING-MAIN.580. URL https://doi.org/10.18653/v1/2020.coling-main. 580. HuggingFaceH4. Math-500. https://huggingface.co/datasets/HuggingFaceH4/ MATH-500, 2023. Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. arXiv preprint arXiv:2503.00223, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit qlearning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/ forum?id=68n2s9ZJWF8. Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 0d2b2061826a5df3221116a5085a6052-Abstract.html. Siheng Li, Zhanhui Zhou, Wai Lam, Chao Yang, and Chaochao Lu. Repo: Replay-enhanced policy optimization. arXiv preprint arXiv:2506.09340, 2025a. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025b. Yi-Chen Li, Tian Xu, Yang Yu, Xuqin Zhang, Xiong-Hui Chen, Zhongxiang Ling, Ningjing Chao, Lei Yuan, and Zhi-Hua Zhou. Generalist reward models: Found inside large language models. arXiv preprint arXiv:2506.23235, 2025c. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan In The Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= v8L0pN6EOi. Yenting Lin. Aime 2025 dataset. https://huggingface.co/datasets/yentinglin/ aime_2025, 2025. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9):195:1195:35, 2023. doi: 10.1145/3560815. URL https://doi.org/10.1145/3560815."
        },
        {
            "title": "Preprint",
            "content": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 80868098. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.556. URL https://doi.org/10.18653/ v1/2022.acl-long.556. Shangke Lyu, Linjuan Wu, Yuchen Yan, Xingyu Wu, Hao Li, Yongliang Shen, Peisheng Jiang, Weiming Lu, Jun Xiao, and Yueting Zhuang. Hierarchical budget policy optimization for adaptive reasoning. arXiv preprint arXiv:2507.15844, 2025. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. MeaIn Houda Bouamor, Juan suring and narrowing the compositionality gap in language models. Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 56875711. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.378. URL https://doi.org/10. 18653/v1/2023.findings-emnlp.378. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint dimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, In ProHaibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. ceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pp. 12791297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https://doi.org/10.1145/3689031.3696075. Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher++: Incentivizing the dynamic knowledge acquisition of llms via reinforcement learning. arXiv preprint arXiv:2505.17005, 2025. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539 554, 2022. doi: 10.1162/TACL 00475. URL https://doi.org/10.1162/tacl_a_ 00475. Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. Stepsearch: Igniting llms search ability via step-wise proximal policy optimization. arXiv preprint arXiv:2505.15107, 2025."
        },
        {
            "title": "Preprint",
            "content": "Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8:229256, 1992. doi: 10.1007/BF00992696. URL https://doi. org/10.1007/BF00992696. Haoran Xu, Li Jiang, Jianxiong Li, and Xianyuan Zhan. policy-guided imitation apIn Sanmi Koyejo, S. Mohamed, A. Agarproach for offline reinforcement Informawal, Danielle Belgrave, K. Cho, tion Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022a. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 1a0755b249b772ed5529796b0a7cc9bd-Abstract-Conference.html. and A. Oh (eds.), Advances in Neural learning. Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imitation learning from suboptimal demonstrations. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 2472524742. PMLR, 2022b. URL https: //proceedings.mlr.press/v162/xu22l.html. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 23692380. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1259. URL https://doi.org/10.18653/v1/ d18-1259. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 1269712706. PMLR, 2021. URL http://proceedings.mlr.press/v139/zhao21c.html. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        }
    ],
    "affiliations": [
        "Alibaba Cloud Computing"
    ]
}