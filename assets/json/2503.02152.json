{
    "paper_title": "Tabby: Tabular Data Synthesis with Language Models",
    "authors": [
        "Sonia Cromp",
        "Satya Sai Srinath Namburi GNVV",
        "Mohammed Alkhudhayri",
        "Catherine Cao",
        "Samuel Guo",
        "Nicholas Roberts",
        "Frederic Sala"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, a simple but powerful post-training modification to the standard Transformer language model architecture, enabling its use for tabular dataset synthesis. Tabby enables the representation of differences across columns using Gated Mixture-of-Experts, with column-specific sets of parameters. Empirically, Tabby results in data quality near or equal to that of real data. By pairing our novel LLM table training technique, Plain, with Tabby, we observe up to a 44% improvement in quality over previous methods. We also show that Tabby extends beyond tables to more general structured data, reaching parity with real data on a nested JSON dataset as well."
        },
        {
            "title": "Start",
            "content": "Tabby: Tabular Data Synthesis with Language Models Sonia Cromp1 Satya Sai Srinath Namburi GNVV2 Catherine Cao1 Samuel Guo1 Nicholas Roberts Mohammed Alkhudhayri1 Frederic Sala1 cromp@wisc.edu 1University of Wisconsin-Madison 2GE HealthCare March 5, 2025 Abstract 5 2 0 2 M 4 ] . [ 1 2 5 1 2 0 . 3 0 5 2 : r While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, simple but powerful post-training modification to the standard Transformer language model architecture, enabling its use for tabular dataset synthesis. Tabby enables the representation of differences across columns using Gated Mixture-of-Experts, with column-specific sets of parameters. Empirically, Tabby results in data quality near or equal to that of real data. By pairing our novel LLM table training technique, Plain, with Tabby, we observe up to 44% improvement in quality over previous methods. We also show that Tabby extends beyond tables to more general structured data, reaching parity with real data on nested JSON dataset as well."
        },
        {
            "title": "1 Introduction",
            "content": "From spreadsheets to databases, much of our modern life is encoded in tables. Airplane black boxes, website visitor logs and hospital patient records are just few examples of this versatile modality. Despite widespread use of tabular data and many calls for improved tabular modeling approaches, this type of data has received less attention in deep learning research than images or text [5, 7, 31]. Progress towards realistic tabular data synthesis has encountered several key challenges. First, table columns often exhibit complex interdependencies. Second, many tabular datasets are in fact combination of various modalities, with text, numerical, and nested datatypes (such as JSON or dictionary) possible among the columns in one dataset. Third, although the order of tokens within one column is important, the order of columns with respect to each other is usually not meaningful and is potential source of spurious correlations during training. How to design and train models that address these issues remains an open question. There have been notable efforts to adapt several model architectures to tabular data, recently focusing on generative adversarial networks (GANs) [33], LLMs [3] and diffusion models [12]. However, because these architectures were each designed with images or text in mind, significant preprocessing must be made to tabular datasets in order to allow their use, likely resulting in lower performance than would be possible for an architecture designed specifically for tabular data. For these reasons, works including van Breugel and van der Schaar [31] have called for the development of pretrained Large Tabular Models (LTMs) to fill similar role to text and image foundation models, such as GPT [1] or DALL-E [18]. Unfortunately, the creation of an LTM would require (1) large and diverse tabular pretraining sets which have not yet been curated, (2) specialized tabular model architecture which has yet to be designed, and (3) substantial compute resources for pretraining. This work takes an initial step towards developing an LTM with Tabby, post-training modification to the standard transformer LLM architecture for enabling tabular data synthesis. After training on text databut before finetuning on tabular dataTabby replaces select LLM blocks with Mixture-of-Experts (MoE) layers [27], allowing each data column to be modeled by dedicated set of parameters in the LLM. The greater expressivity afforded by this change results in higher-fidelity synthetic data. Fine-tuning with our novel Plain technique results in still higher performance. We show that even small Tabby models are capable of outstripping large, non-Tabby LLMs with parameter counts that are orders of magnitude greater. To our knowledge, Tabby is the first architecture modification to make LLMs better-suited to table generation. Using pretrained language model as starting point allows Tabby to take advantage of its text pretraining, avoiding the 1 Figure 1: Tabby Multi-Head modifications (right side) compared to an original, Non-Tabby LLM on the left. logistical challenges of training LTM entirely from scratch. We find that, according to standard metrics, Tabby produces synthetic data nearor at-parity with real data on 4 out of 6 tabular datasets. Additionally, Tabby is not limited to tables and can be easily extended to other structured data. We validate this by synthesizing nested JSON data at-parity with real data as well. Our contributions are: We introduce Tabby1, the first architecture modification that allows transformer-based LLMs to synthesize more realistic tabular data. We demonstrate that Tabby produces higher-quality synthetic data for 4 out of 6 tabular datasets, and can be extended beyond tables to broader class of structured data modalities. We introduce our novel tabular training method for LLMs, Plain. Named after its surprising simplicity compared to prior LLM table training approaches, Plain increases data quality in 5/6 datasets when used together with or independently of Tabby."
        },
        {
            "title": "2 Tabby Architecture & Plain Train Method",
            "content": "We now formally introduce our two novel contributions for LLM table synthesis: Tabby is an architecture modification that may be applied to any transformer-based language model (LM) [32], and Plain is training technique for training any (Tabby or non-Tabby) LM on tabular data. Tabby and Plain are especially powerful together: Tabby increases model expressivity in way that is specially-suited to tabular data, while Plain allows the model to more easily focus on learning the critical features of dataset during training. In Section 2.1, we describe Tabby for tabular or other structured data. In Section 2.2 , we outline the process for training an LM on tabular data using our Plain training technique. Then, in Section 2.3, we provide additional insight into how Tabby models are trained (using Plain, or pre-existing LLM table training techniques such as GReaT [3]) by comparing the training processs forward pass and loss calculation for Tabby model with non-Tabby model."
        },
        {
            "title": "2.1 Architecture of Tabby Models",
            "content": "At its core, the Tabby modification is quite simple. The right side of Figure 1 depicts our best-performing Tabby model for tabular data: in this use case, Tabby modifies only the LMs language modeling head. 1Codebase: https://github.com/soCromp/tabby 2 Consider tabular dataset with columns. Let the order of blocks within an arbitrary transformer-based LM be represented as [L1, L2, . . . , LH ]. We apply the MoE technique by replacing an LM block La with vector Λa = [La,1, La,2, . . . , La,V ] of blocks. Thus, Tabby model with one MoE block Λa is represented [L1, L2, . . . , La1, [La,1, La,2, . . . , La,V ], La+1, . . . , LH ]. The datasets i-th column is modeled by La,i within Λa. This technique may be applied to any set of layers within the model. While we focus on the language modeling (LM) head 2 in Figure 1 and Section 3 evaluations, we also conduct extensive experiments applying Tabby to the transformer multi-layer perceptrons and attention blocks in Appendix E, visualized in Figure 8. We refer to Tabby models with MoE LM Heads as Tabby Multi-Head (MH) models."
        },
        {
            "title": "2.2 The Plain Technique for Fine-Tuning LLMs on Tabular Data",
            "content": "Suppose our training dataset contains rows and the datasets column names are denoted by v1, v2, . . . , vV , where the value of the j-th row in the i-th column is denoted as vj . To provide the LM with its expected text modality input, we convert the j-th row as follows, where <EOS> is the end-of-sequence token and <EOC> is specialized end-of-column token which we introduce to divide the text between columns: <BOS> v1 is vj 2 <EOC> vV is vj After converting the tabular dataset in this fashion, an LM is capable of fine-tuning on the dataset in normal sequence-to-sequence style. Because Plain encodes data in similar way to prior LLM table training techniques, GReaT and Tabula [3, 38], Plain training does not require more FLOPs than these methods. 1 <EOC> v2 is vj <EOS>\" During inference, the prompt for each row is the beginning-of-sequence token <BOS>. During generation, the LM will output text in similar format to the training data, which can then be parsed into tabular data as desired. We note that the simplicity of Plain is particularly impressive given its favorable performance compared to prior LLM table training methods, as we show in Section 3."
        },
        {
            "title": "2.3 Tabby Training",
            "content": "Now that we have introduced Tabby and the Plain method to fine-tune LMs on tabular data, we are able to provide further insight into aspects of the training process unique to Tabby. Suppose that we construct Tabby model from base LM by replacing one of its blocks La with an MoE set Λa. At the beginning of fine-tuning the Tabby model, weights for each block in Λa are initialized to equal the weights of La. The Tabby training process requires only slight modifications compared to other LMs for tabular data. Instead of representing each training row as one string, we convert each row into length-V list of strings as follows: [v1 is vj 1 <EOC>\", v2 is vj 2 <EOC>\", , vV is vj Internally, the Tabby model begins by training on column 1 with prompt <BOS>, attending to tokens 0 through 1 when predicting the k-th token. After computing the loss on column 1, this columns tokens are appended to the prompt used to train column 2. As such, the prompt when training on column is <BOS>v1 is vj 2 <EOC> vi1 is vj favorable side-effect of this training style is that we calculate the losses for each column separately, allowing the performances of each column to be monitored separately and compared, as demonstrated in Section 3.4. 1 <EOC> v2 is vj <EOS>\"] i1 <EOS>\""
        },
        {
            "title": "2.4 Extensions",
            "content": "We address two additional aspects of Tabby and Plain: (1) generalizations that go beyond tabular data and (2) optimizations for datasets with large numbers of columns. Synthesis for general structured modalities: The flexibility in Tabby MoE layer design enables extensions to variety of structured data, such as hierarchical data. For example, we create model for nested JSON data by applying Tabby recursively in Figure 7. The JSON structure is preserved inherently in the model: as we show in Section 3.4, this means that Plains method of representing data features does not need to be modified to indicate nested features in order to reach synthetic parity with real JSON data. 2Note: Here, LM head\" refers to the language model output layer, distinct from the attention heads in the models MLP blocks. 3 High-dimensional data: Because Tabby MoEs contain one block per dataset column, model parameter count is proportionate to the number of data features. In practice, however, techniques such as parameter sharing [23] can drastically reduce the number of parameters required to represent Tabby model. Additionally, Tabby may be implemented so that only one block in the MOE layer is in memory at time, resulting in memory requirements identical to non-Tabby model."
        },
        {
            "title": "3 Experimental Results",
            "content": "With our evaluations, we seek to assess the following claims: Claim 1: Plain-trained Tabby models generate higher-quality data than pre-existing tabular synthesis approaches. Claim 2: The Tabby architecture modification allows smaller LLMs to achieve similar or better synthetic data fidelity than LLMs with higher parameter counts. Claim 3: Tabby architecture modifications may also be applied to other structured data beyond tabular data, resulting in higher-quality synthetic data for these modalities as well. Claim 4: Tabbys loss formulation allows for convenient tracking of per-column performance at training time, leading to better understanding of model behavior. After providing key evaluation setup details in Section 3.0, we compare Tabby to broad array of prior works on diverse tabular datasets in Section 3.1 to evaluate Claim 1. As Tabby may be applied to any transformer-based LM, we explore Claim 2 for LMs of varying sizes in Section 3.2. To demonstrate Claim 3, we apply Tabby to nested (JSON) dataset in Section 3.3. Lastly, in Section 3.4, we investigate how Tabby adapts to individual columns within dataset during finetuning as demonstration of Claim 4."
        },
        {
            "title": "3.0 Setup",
            "content": "We detail here our experiments essential information, including baselines, evaluation datasets and metrics. Additional details are located in Appendix D. Baselines and Comparisons: We include variety of recent tabular synthesis approaches in our evaluations. LLM Approaches: Prior LLM table synthesis approaches are limited to the development of training techniques. We compare Tabby and Non-Tabby LLMs trained under three different paradigms: 1. Plain training, our lightweight and simple training paradigm. Plain training is detailed in Section 2.2. 2. GReaT [3], which encodes tabular data similarly to Plain, but permutes the orders in which columns are presented in training and imposes some conditional restrictions at sample time. For more detail, see Section 4. 3. GReaT, combined with two other prior techniques of TapTap [36] and Tabula [38]. We abbreviate this combination as GTT. TapTap involves pretraining the LLM on tabular data, while Tabula encodes each categorical column into an ordinal format by replacing each unique column value with an integer. To align with prior works, LLM methods use Distilled-GPT2 as base model where not otherwise stated. Non-LLM Approaches: To represent non-LLM tabular synthesis techniques, we include CTGAN [33] and TVAE [33], the leading GAN and VAE approaches, as well as diffusion model Tab-DDPM [12]. Although Tab-DDPM is SOTA approach to achieving high MLE scores, it does so under strong assumptionssee Figure 4 and Section 4. Additional details on how models are trained and sampled are available in Appendix D. Datasets: We evaluate Tabby on six common tabular datasets, which are summarized in Table 1. The majority of these datasets are standard for the evaluation of tabular synthesis techniques, allowing for easy comparison with prior approaches. For more information on these datasets, see Appendix B. Metrics: We focus on machine learning efficacy (MLE) [4], the standard metric essential to the quantitative evaluation of synthetic tabular data. In brief, MLE compares the performance of downstream classifiers that were trained using either real or synthetic data. Our MLE results in the following sections are interpreted as follows: the Original\" downstream classifier, trained using real data, is considered the upper bound and any MLE score higher than the Original\" score is considered the best. If no score surpasses the Original\" score, then any highest score is considered the best. 4 Table 1: Summary statistics of datasets. The first three columns list the number of rows in each data split, while the next two columns display the number of categorical versus numerical features, respectively. The rightmost column details whether the dataset is considered classification (C) or regression (R) task in downstream evaluations. Train Validation Test # Cat. # Num. Task Diabetes [11] Travel [30] Adult [2] Abalone [17] Rainfall [34] House [19] 576 715 36631 3132 12566 15480 57 71 3663 313 1256 1548 135 168 8548 732 2933 3612 0 4 8 1 2 0 8 2 6 7 1 C Figure 2: Process for calculating our primary metric, Machine Learning Efficacy (MLE). We train generative model, which produces synthetic dataset. Two downstream classifiers are trained: one on the generative models training data and the other on the synthetic data. Each downstream model is evaluated on real test data. MLE is the difference in downstream models test-time performance. Higher scores indicate better-quality synthetic data. visual summary of MLE calculation is provided in Figure 2. In Appendix D, we provide formal definition of MLE (as well as two more metrics deferred to the Appendices: Discrimination and the Distance to Closest Record). Aggregation of results: Our evaluation involves comparison between 9 synthesis methods (including Tabby) across 6 datasets. So, while we do report final scores on each task individually, we would also like to understand which method performs the best across all of the tasks in our evaluation. To do so, we aggregate MLE scores using performance profile curves [6], which are robust way to visually compare scores across noisy evaluations in large number of environments. Performance profiles improve over simpler aggregation techniques, such as averaging scores or computing the average rank of methods across tasks. Specifically, performance profiles are useful when scores for different tasks might be on different scales (which can be an issue with averaging scores), and can take into account methods that are extremely close to the best-performing method on task without dropping them full rank (which can be problematic when averaging ranks). To summarize these curves, we also calculate the area under the performance profile (AUP) scores [25], which serve as final ranking of methods. In short, the performance of synthesis method across all six datasets may be represented as just one performance profile curve. Methods with better performance will have higher curves and, therefore, higher AUP scores. As such, the method with highest AUP score is considered the best overall method. Details on performance profiles are in Appendix D."
        },
        {
            "title": "3.1 Tabby versus Baseline Synthesis Methods",
            "content": "We begin by validating our first claim. Claim 1: Plain-trained Tabby models generate higher-quality data than pre-existing tabular synthesis approaches. Setup: Table 2 contains MLE results for each dataset. For classification datasets (Diabetes, Travel, Adult), the reported metric is the accuracy of the downstream random forest classifier, while for regression datasets (Abalone, Rainfall, House), we report the coefficient of determination R2 of the downstream random forest regressor. The Original\" row corresponds to the performance achieved by training the downstream classifier or regressor on real instead of synthetic data. We consider this row to be performance ceiling for synthetic approaches. Any model and training technique that achieves MLE equal to or better than Original\" is considered to be top-performing approach and is presented in bold. Results: We find that Plain-trained Tabby models achieve the highest MLE in 4/6 datasets. Further, Tabby 5 Table 2: Machine Learning Efficacy (MLE, ). The Original\" row is upper-bound performance given by real, non-synthetic data . Top results (or any higher than upper-bound) are bolded. An asterisk indicates that at least one of three runs did not produce valid samples. Tabby models are presented in italic. The best-performing Tabby model, Plain Tabby MH DGPT2 is presented in purple and achieves best performance on 4/6 datasets. Terminology glossary in Appendix A. Diabetes Travel Adult Abalone Rainfall House Original (Upper Bound) 0.73 0.85 0.76 0.00 0.81 0.01 Tab-DDPM 0.75 0.02 0.87 0.01 0.84 0.00 CTGAN 0.39 0.00 TVAE 0.62 0.00 0.87 0.43 0.33 0.81 0. 0.61 0.54 0.45 0.00 0.00 0.00 0.00 0.01 0.01 0.07 0.03 0.05 0.09 0.00 0.00 0.41 0.01 0.54 0.01 0.43 0.01 Plain Base DGPT2 0.75 0.02 0.86 0.01 0.85 0.00 0.44 0.01 0.52 0.03 Plain Tabby MH DGPT2 0.74 0.00 0.88 0.01 0.85 0.00 0.43 0.01 0.55 0.08 0.49 0.00 0.60 0.00 GReaT Base DGPT2 GReaT Tabby MH DGPT2 0.62 0.01 0.64 0. 0.85 0.02 0.86 0.01 0.83 0.01 0.83 0.00 0.41 0.01 0.40 0.01 N/A* 0.00 0.00* 0.56 0.01 0.56 0.01 GTT Base DGPT2 GTT Tabby MH DGPT 0.72 0.06 0.87 0.02 0.83 0.01 0.76 0.07 0.85 0.01 0.62 0.00 0.40 0.01 0.37 0.02 0.05 0.01 0.26 0.37 0.55 0.02 0.55 0.00 Figure 3: Performance profile curves and AUP scores across computed using the MLE scores on our evaluation tasks. The top performing method is Tabby MH DGPT2 with plain training. reaches upper-bound performance on Diabetes, Travel and Adult, indicating that Tabby synthetic data is capable stand-in for real data in similar scenarios for these datasets. We also find that Plain is the best-performing technique for training tabular LLMs in almost all cases: for all six datasets, the highest-scoring LLM is trained using Plain. Plain-trained Tabby MH models demonstrate the highest MLE among all LLM architectures and training styles. For the Rainfall dataset, pre-existing LLM tabular training techinques introduce undesirable effects. Entries marked by an asterisk (*) for this dataset indicate that at least one of three runs were unsuccessful in synthesizing any valid samples. Particularly, the Non-Tabby GReaT model is unable to produce valid samples in any of the runs. Meanwhile, each Plain-trained model is successfully sampled and outperforms all GReaT or GTT-trained models in all three runs, indicating that Plain-trained Tabby models are capable of modeling complexities within the Rainfall dataset that pre-existing LLM-based tabular synthesis works are unable to capture. Performance Profile Analysis: The performance profile curves in Figure 5 support our findings. In particular, Plain-trained Tabby MH achieves the highest AUP score. This indicates that Plain-trained Tabby MH performs the best among all methods when comparing across all datasets. Further, we see that the top two synthesis approaches are the two plain-trained models, which surpass the prior SOTA method of Tab-DDPM. Given that these models rely on fewer assumptions than Tab-DDPM, and are simpler to train than the GTT or GReaT LLMs, we find that both Tabby MH and the Plain training technique are powerful advancements for the task of tabular data synthesis. 6 Figure 4: The House datasets target Median House Value column as function of its most-predictive feature, Median Income. Left to right: synthetic data from Tab-DDPM, the prior best LLM-based method and Plain Tabby MH, followed by the original data distribution. Table 3: MLE for Base and MH versions of 7 different LLMs with varying parameter counts, for the Travel dataset. Results higher than Original are presented in bold. Tabby improves or maintains upper-bound MLE for 6/7 models. MLE () Params Original (Upper Bound) 0.87 Base Pythia 14M 0.86 0.01 Tabby MH Pythia 14M 0.82 0. Base Distilled-GPT2 Tabby MH Distilled-GPT2 0.88 0.00 0.89 0.02 Base GPT2 Tabby MH GPT2 0.89 0.01 0.87 0.01 Base Pythia 160M 0.87 0.01 Tabby MH Pythia 160M 0.86 0.00 Base Pythia 410M 0.86 0.02 Tabby MH Pythia 410M 0.88 0. Base Llama 3.2 1B Tabby MH Llama 3.2 1B 0.82 0.01 0.84 0.02 Base Llama 3.1 8B Tabby MH Llama 3.1 8B 0.84 0.01 0.86 0.03 14M 53M 82M 310M 120M 360M 160M 390M 410M 710M 1.2B 2.8B 8.0B 11B Comparing Multivariate Modeling Capabilities Across Models: We further compare the multivariate modeling capabilities of Tab-DDPM, Plain-trained Tabby MH and the prior top-performing LLM-based approach of Greattrained Non-Tabby with TapTap and Tabula in Figure 4. We plot the House datasets target column (Median House Value) as function of its most predictive feature in the dataset (Median Income), for (left to right) real data, plain-trained Tabby MH, non-Tabby GTT and Dab-DDPM. Tab-DDPMs plot (leftmost) differs the most from the real data (rightmost) because this model only supports integer-valued regression targets. As result, both LLM-based approaches more accurately capture the target columns distribution than Tab-DDPM. Meanwhile, GReaT sampling (center left) ensures that the target column distribution in the training dataset is replicated in synthetic data. This constraint is enforced by prompting the model with target values selected randomly, based on their frequency in the training data. Accordingly, GReaT models will not generate target values outside those in the training data, which can be undesirable for datasets with few rows or limited target column coverage. In contrast, Plain training (center right) allows the model to generate previously unseen target values. The improved modeling capacity of Tabby over the Non-Tabby model allows Plains sampling approach to effectively capture the overall distribution of the target column. 7 Figure 5: Machine Learning Efficacy (MLE) as function of parameter count for 7 base LLMs, using Non-Tabby or Tabby MH architectures. Non-Tabby points displayed in blue; MH points in purple. Red line represents Original, upper-bound performance. 3."
        },
        {
            "title": "Investigating the Choice of Base Model",
            "content": "We now turn to our second claim. Claim 2: The Tabby architecture modification allows smaller LLMs to achieve synthetic data fidelity similar or better than LLMs with higher parameter counts. Comparisons: We compare tabular synthesis quality across LLMs of varying sizes. We consider seven LLMs, listed in Table 3 evaluating Non-Tabby and MH versions of each. Each model is plain-trained under the conditions provided in Section 3.0, then sampled 500 times. Results are averaged across two runs. Llama models use LoRA [10] on all linear transformer layers, with the LM head fully fine-tuned. Results: Table 3 and Figure 5 display results for the Travel dataset, with results for Diabetes and House (plus additional metrics and results for GReaT training) in Appendix E.3. We find that Tabby improves MLE or maintains upper-bound MLE for 6/7 models. Although higher-parameter models are generally correlated with greater generative abilities, Figure 5 demonstrates that this is not always the case: Interestingly, we find that the Llama models (1.2B and 8B parameters each), have lower average MLE than all smaller models. Meanwhile, Tabby offers favorable performance improvements relative to the scaling curve and allows even small models to better outperform large, resource-intensive models."
        },
        {
            "title": "3.3 Extending Tabby Beyond Tabular Data to General Structured Modalities",
            "content": "While tabular data is frequently overlooked in contemporary machine learning research, related structured modalities such as nested data receive even less attention. While GReaT, TapTap, Tabula, CTGAN and TVAE are focused solely on tabular data and do not clearly extend beyond tables, we demonstrate that Tabby can be generalized to address our third claim. Claim 3: Tabby architecture modifications may also be applied to other structured data, resulting in higher-quality synthetic data in these modalities as well. Comparisons: We plain-train non-Tabby and Tabby MH models on JSON dataset of patients being evaluated for Glaucoma [15]. Each datapoint has 10 features, organized in 3 groups: group of 7 columns representing qualitative aspects of the optic nerve, group of 2 columns corresponding to measurements between the optic nerve and eye, and standalone feature for the diagnosis (examples in Box C). The binary classification target is inside the first group and assesses whether the optic nerve is thinning. As with tabular datasets in Section 3.1 and 3.2, we train downstream classifiers to predict the target variable and then present the resulting MLE. Additionally, we consider the discrimination metric: given dataset containing equal numbers of real and synthetic samples, we measure the accuracy of discrimination classifier that is trained to distinguish real from synthetic 8 Table 4: MLE and Discrimination scores for Plain-trained Base and MH models on dataset of JSON records. Each record contains diagnostic information of glaucoma sufferer or healthy ophthalmic patient. MLE () Discrim. () Original (Upper Bound) Base DGPT2 Tabby MH DGPT2 0.97 0.93 0.97 0.06 0.01 Figure 6: Per-column validation loss across 10 epochs of training Tabby MH Distilled-GPT2 on subset of House, with average validation loss (black line). While the Occupancy column initially displays the highest loss, Median Income improves little throughout training and becomes the highest-loss column by step 32000. data. Because 50% accuracy would indicate that the classifier is fully unable to distinguish real from synthetic, we report the accuracys distance from 50% in Table 4 so that lower scores indicate higher-quality synthesis. Results: Table 4 demonstrates that Tabby MH improves MLE by 4%, to parity with real data. Additionally, Tabby MHs lower discrimination score signifies that this models samples are more realistic than the non-Tabby samples."
        },
        {
            "title": "3.4 Tracking the Adaptation to Individual Columns",
            "content": "We address our final claim by examining Tabbys progress while fine-tuning on tabular data. Claim 4: Tabbys loss formulation allows for convenient tracking of per-column performance at training time, leading to better understanding of model behavior. Setup: For three runs, we train Tabby MH model on subset of the House dataset containing 5160 rows and six columns. We log the individual columns losses on the evaluation dataset every 2500 steps while training for 10 epochs, then average across the runs. Results: Individual column losses are shown in Figure 6. This information can be vital to understanding model behavior and training progress, as elaborated in Section E.4."
        },
        {
            "title": "3.5 Discussion",
            "content": "We find that Tabby models synthesize high-quality data in variety of settings. In particular, Plain-trained Tabby MH consistently outperforms all prior LLM-based approaches and is comparable to or better than Tab-DDPM in most settings, despite Tabby enjoying greater flexibility under fewer assumptions. The Tabby architecture modification allows LLMs to better model univariate column distributions, as well as multivariate relationships across columns. This is evident across model sizes, shown in Section 3.2. Unusually, we find that the baseline Plain training technique with Distilled-GPT2 achieves near-optimal MLE on several datasets. The high performance of the Plain training technique compared to prior LLM works on tabular synthesis, which are more complex, is surprising. As of this writing, the Adult, House and Diabetes datasets have become quite prevalent for tabular synthesis evaluation. We hope that future research will begin to focus on more challenging datasets as well as extensions into other structured modalities."
        },
        {
            "title": "4 Related Work",
            "content": "Tabular data has played central role in machine learning since the fields early days. In particular, decision trees and relatives [29] are well-adapted to table classification or regression. Table synthesis, meanwhile, is frequently overlooked in favor of image and text synthesisthough still possesses growing body of work. Classical synthesis: Classic machine learning models, such as Bayesian networks, may be used to synthesize tabular data [24, 35]. However, these approaches are limited in the data types and distributions that may be represented. Generative Adversarial Networks (GANs): Many tabular synthesis methods rely on GANs [8, 33], but have encountered several limitations. In particular, distributions of ordinal columns are frequently imbalanced, leading GANs to undesirable phenomena such as mode collapse. Continuous columns may possess multiple modes and complex interactions with the other columns which GANs also struggle to capture. Diffusion Models: Tab-DDPM [12], current state-of-the-art table synthesis approach, is based on the diffusion model architecture. Tab-DDPM shows top performance on many standard tabular metrics and is highly reliable approach for certain applications. Unfortunately, this performance is achieved with strong assumptions on the nature of the data spacefor instance, numeric target variables may only assume integer values (see Figure 4) and diffusion models are unable to model non-categorical string columns such as addresses or telephone numbers. The ability to reach comparable performance to Tab-DDPM with fewer assumptions is as an area of active research. LLMs: small body of work has sought to apply LLMs demonstrated abilities of modeling complex relationships to tabular data. The landmark work in this area, GReaT [3], details methods to convert tabular data into sentence format which may be input to LLMs, then proposes training technique of shuffling\" the order in which columns occur for each row, which is reported to improve the modeling of inter-column dependencies. Two notable works have built off of GReaT: TapTap [36] pretrains full or Distilled-GPT2 [22] on variety of tabular data before fine-tuning on downstream tabular synthesis task, while Tabula [38] explores methods of preprocessing the training data to be more-easily modeled by LLMs. Other LLM-based works have adapted these recent advances to relational tables [28], or used the emergent abilities of very large models such as GPT-4 to generate synthetic data using In-Context Learning in place of fine-tuning [26]. Many of these prior LLM-based works are training techniques and may be applied in concert with Tabby: we demonstrate this using GReaT, TapTap and Tabula in Section 3. MoE Architectures: The key innovation of Tabby is the application of Gated Mixture of Expert (MoE) layers [16, 27] for LLM table synthesis. MoE layers have enjoyed utility in multitask [9, 14] and multimodal learning [20, 37], by creating sets of model parameters dedicated to specific task."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Tabby, an MOE-based architecture modification that allows LLMs generate realistic tabular data. Tabby reaches parity with real data in 3/6 datasets, according to machine learning efficacy. Given the promising performance of Tabby, we hope to spur future work in this area of architecture modifications that allow LLMs to better fit to tabular data and structured data. References [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] B. Becker and R. Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20. [3] V. Borisov, K. Sessler, T. Leemann, M. Pawelczyk, and G. Kasneci. Language Models are Realistic Tabular Data Generators. Sept. 2022. URL https://openreview.net/forum?id=cEygmQNOeI. [4] F. K. Dankar, M. K. Ibrahim, and L. Ismail. multi-dimensional evaluation of synthetic data generators. IEEE Access, 10:1114711158, 2022. [5] M. F. Davila, S. Groen, F. Panse, and W. Wingerath. Navigating Tabular Data Synthesis Research: Understanding User Needs and Tool Capabilities, May 2024. URL http://arxiv.org/abs/2405.20959. arXiv:2405.20959 [cs]. 10 [6] E. D. Dolan and J. J. Moré. Benchmarking optimization software with performance profiles. Mathematical Programming, 91(2):201213, Jan. 2002. ISSN 1436-4646. doi: 10.1007/s101070100263. URL https: //arxiv.org/abs/cs/0102001. [7] X. Fang, W. Xu, F. A. Tan, J. Zhang, Z. Hu, Y. Qi, S. Nickleach, D. Socolinsky, S. Sengamedu, and C. Faloutsos. Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding Survey, Feb. 2024. URL http://arxiv.org/abs/2402.17944. arXiv:2402.17944 [cs]. [8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [9] S. Gupta, S. Mukherjee, K. Subudhi, E. Gonzalez, D. Jose, A. H. Awadallah, and J. Gao. Sparsely activated mixture-of-experts are robust multi-task learners. arXiv preprint arXiv:2204.07689, 2022. [10] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [11] M. Kahn. Diabetes. UCI Machine Learning Repository, 1994. openml.org/search?type=data&sort=runs&id=37&status=active. https://doi.org/10.24432/C5T59G. URL https://www. DOI: [12] A. Kotelnikov, D. Baranchuk, I. Rubachev, and A. Babenko. TabDDPM: Modelling Tabular Data with Diffusion Models, Sept. 2022. URL http://arxiv.org/abs/2209.15421. arXiv:2209.15421 [cs]. [13] A. D. Lautrup, T. Hyrup, A. Zimek, and P. Schneider-Kamp. Syntheval: framework for detailed utility and privacy evaluation of tabular synthetic data. Data Mining and Knowledge Discovery, 39(1):6, Dec. ISSN 1573-756X. doi: 10.1007/s10618-024-01081-4. URL https://doi.org/10.1007/ 2024. s10618-024-01081-4. [14] J. Ma, Z. Zhao, X. Yi, J. Chen, L. Hong, and E. H. Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 19301939, 2018. [15] A. Manoj. Glaucoma Diagnosis JSON Analysis Dataset at Hugging Face, 2024. URL https:// huggingface.co/datasets/AswanthCManoj/glaucoma_diagnosis_json_analysis. [16] S. Masoudnia and R. Ebrahimpour. Mixture of experts: literature survey. Artificial Intelligence Review, 42: 275293, 2014. [17] W. J. Nash, T. L. Sellers, S. R. Talbot, A. J. Cawthorn, and W. B. Ford. The population biology of abalone (haliotis species) in tasmania. i. blacklip abalone (h. rubra) from the north coast and islands of bass strait. Sea Fisheries Division, Technical Report, 48:p411, 1994. [18] OpenAI. DALLE 2, 2021. URL https://openai.com/index/dall-e-2/. [19] R. K. Pace and R. Barry. Sparse spatial autoregressions. Statistics & Probability Letters, 33(3):291297, 1997. [20] D. K. Park, S. Yoo, H. Bahng, J. Choo, and N. Park. Megan: Mixture of experts of generative adversarial networks for multimodal image generation. arXiv preprint arXiv:1805.02481, 2018. [21] Z. Qian, B.-C. Cebere, and M. van der Schaar. Synthcity: facilitating innovative use cases of synthetic data in different data modalities, 2023. URL https://arxiv.org/abs/2301.07573. [22] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [23] S. Ravanbakhsh, J. Schneider, and B. Poczos. Equivariance through parameter-sharing. In International conference on machine learning, pages 28922901. PMLR, 2017. [24] J. P. Reiter. Using cart to generate partially synthetic public use microdata. Journal of Official Statistics - Stockholm, 21(3):441, 2005. [25] N. Roberts, S. Guo, C. Xu, A. Talwalkar, D. Lander, L. Tao, L. Cai, S. Niu, J. Heng, H. Qin, M. Deng, J. Hog, A. Pfefferle, S. A. Shivakumar, A. Krishnakumar, Y. Wang, R. Sukthanker, F. Hutter, E. Hasanaj, T.-D. Le, M. Khodak, Y. Nevmyvaka, K. Rasul, F. Sala, A. Schneider, J. Shen, and E. Sparks. Automl decathlon: Diverse tasks, modern methods, and efficiency at scale. In M. Ciccone, G. Stolovitzky, and J. Albrecht, editors, Proceedings of the NeurIPS 2022 Competitions Track, volume 220 of Proceedings of Machine Learning 11 Research, pages 151170. PMLR, 28 Nov09 Dec 2022. URL https://proceedings.mlr.press/ v220/roberts23a.html. [26] N. Seedat, N. Huynh, B. van Breugel, and M. van der Schaar. Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes, Feb. 2024. URL http://arxiv.org/abs/ 2312.12112. arXiv:2312.12112 [cs]. [27] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [28] A. V. Solatorio and O. Dupriez. REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers, Feb. 2023. URL http://arxiv.org/abs/2302.02041. arXiv:2302.02041 [cs]. [29] Y.-y. Song and Y. Lu. Decision tree methods: applications for classification and prediction. Shanghai Archives of Psychiatry, 27(2):130135, Apr. 2015. ISSN 1002-0829. doi: 10.11919/j.issn.1002-0829.215044. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466856/. [30] Tejashvi. Tour & Travels Customer Churn Prediction, 2023. URL https://www.kaggle.com/ datasets/tejashvi14/tour-travels-customer-churn-prediction. [31] B. van Breugel and M. van der Schaar. Why Tabular Foundation Models Should Be Research Priority, May 2024. URL https://arxiv.org/abs/2405.01147v2. [32] A. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [33] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni. Modeling Tabular data using Conditional GAN, Oct. 2019. URL http://arxiv.org/abs/1907.00503. arXiv:1907.00503 [cs, stat]. [34] Y. Zaman. Machine Learning Model on Rainfall - Predicted Approach for Bangladesh. 2018. [35] J. Zhang, G. Cormode, C. M. Procopiuc, D. Srivastava, and X. Xiao. Privbayes: Private data release via bayesian networks. ACM Transactions on Database Systems (TODS), 42(4):141, 2017. [36] T. Zhang, S. Wang, S. Yan, J. Li, and Q. Liu. Generative Table Pre-training Empowers Models for Tabular Prediction, May 2023. URL http://arxiv.org/abs/2305.09696. arXiv:2305.09696 [cs]. [37] X. Zhao, M. Wang, Y. Tan, and X. Wang. Tgmoe: text guided mixture-of-experts model for multimodal sentiment analysis. International Journal of Advanced Computer Science & Applications, 15(8), 2024. [38] Z. Zhao, R. Birke, and L. Chen. TabuLa: Harnessing Language Models for Tabular Data Synthesis, Oct. 2023. URL http://arxiv.org/abs/2310.12746. arXiv:2310.12746 [cs] version: 1."
        },
        {
            "title": "A Terminology Glossary",
            "content": "Distance to Closest Record (DCR): Metric for synthetic data quality and privacy, defined in Appendix D. Discrimination: Metric for synthetic data quality, defined in Appendix D. GReaT [3]: The landmark work on fine-tuning pre-existing LLMs to synthesize tabular data by encoding datapoints as text. Similar to Plain training, but includes train-time complications such as shuffling the order in which columns are encoded and sample-time complications such as the inability to generate label values that do not occur in the training dataset. Discussed in-detail in Section 4. GReaT+Tabula (GT): The combination of GReaT training plus Tabula [38] data encoding; see Section 4. GReaT+TapTap+Tabula (GTT): The combination of GReaT training plus Tabula encoding and TapTap [36] pre-training on tabular data (which is performed after the LLM is pre-trained on text data). Mixture-of-Experts (MoE): Architecture technique which replaces one block with set of specialized blocks; see Section 4. Multi-Head (MH): The best-performing variant of Tabby, which replaces the LLMs language model output layer with an MoE layer. Machine Learning Efficacy (MLE): Our primary evaluation metric, introduced in Section 3.0 and discussed in-detail in Appendix D. Multi-MMLP (MMLP): Tabby modification that applies MoE to the transformer blocks MLPs. Multi-MLP and LM Head (MMLP+MH): Tabby modification that applies MoEs to both the transformer blocks MLPs and to the language model output layers. Non-Tabby (NT): An LLM without the Tabby modification, also referred to as Base LLM. Plain: Our simple but high-performing technique for training LLMs on tabular data; introduced in Section 2. Tab-DDPM (TDDPM): state-of-the-art tabular synthesis technique based on the diffusion model architecture, which relies on several important assumptions; see Section 4."
        },
        {
            "title": "B Additional dataset information",
            "content": "We select variety of tabular datasets for our evaluations, with two goals in mind. First, the inclusion of the most standard tabular datasetsDiabetes, Adult and Houseallows for easy comparison with prior works. Second, we include classification and regression datasets from variety of domains, such as Earth science (Rainfall), business (Travel) and medicine (Diabetes). This diversity allows us to demonstrate that Tabby models achieve high performance across variety of real-world data types and distributions. Refer to Table 5 for download links to each dataset."
        },
        {
            "title": "Descriptions of Datasets",
            "content": "Diabetes [11] contains medical information on female hospital patients, including age, number of pregnancies and skin thickness. Downstream models learn to predict whether given patient suffers from diabetes. Apart from the label, all dataset columns are numerical, with some columns taking only integer values, while others are floats. Table 5: Download links for each dataset. Link https://www.openml.org/search?type=data&sort=runs&id=37&status=active https://www.kaggle.com/datasets/tejashvi14/tour-travels-customer-churn-prediction/data https://archive.ics.uci.edu/dataset/2/adult https://www.openml.org/search?type=data&sort=runs&id=183&status=active https://www.openml.org/search?type=data&status=active&id=41539&sort=runs https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html https://huggingface.co/datasets/AswanthCManoj/glaucoma_diagnosis_json_analysis Dataset Diabetes Travel Adult Abalone Rainfall House Glaucoma 13 Figure 7: An overview of the Tabby MH modifications for the nested Glaucoma dataset. Travel [30] was collected by travel agency wishing to predict customer churn. With the binary variable churn as the target, features include whether the traveler booked hotel, frequent flyer status and traveler age. While most features are categorical, there are two numerical columns: traveler age and the number of times that the customer has used the travel agency in the past. Adult [2] is dataset commonly used to benchmark tabular classification algorithms. Each row contains basic information on one American adult, such as their age, years of education and marital status. For each adult, the downstream task is to predict whether their annual income is above or below $50, 000. The features are mix of categorical and numerical columns, with each numerical column taking only integer values. Our first regression dataset is Abalone [17], which records the basic measurements of abalones, such weight and height. The target variable is the abalones age. The Rainfall [34] dataset, while challenging to many LLM-based synthesis methods, contains only four columns which record historical weather data in Bangladesh. Its target variable is the amount of rainfall recorded, and the features are the year, month and weather station location. House [19] is standard regression dataset. Each row represents block of houses in California during the 1990 census. The dataset records the number of households residing in the block, the blocks median building age, average number of bedrooms, and other basic information. The datasets target column is the blocks median house value, which is numerical and allows us to assess Tabbys synthetic data in regression task. Glaucoma [15] dataset consists of JSON records describing ophthalmic patients under consideration for glaucoma diagnosis. Each record contains various qualitative and quantitative information about the eye, as demonstrated by the examples in Box 1."
        },
        {
            "title": "C Tabby for Nested Data",
            "content": "Figure 7 provides visualization of the Tabby architecture used in Section E.4 to generate nested JSON data. 14 Box 1: Representative examples from Glaucoma [15] [ { \" u a \" , \" g i \" : \" c _ o \" : { \" c _ e \" : \" g \" , \" _ c _ i \" : 0 . 8 } , \" _ o \" : { e , \" _ l \" : \" e \" , \" _ o \" : e , \" o i \" : e , \" r _ e \" : \" i _ _ n \" : \" c g \" : \" _ n g \" : e , e e , } } , { \" m \" , \" g i \" : \" c _ o \" : { \" c _ e \" : \" _ c _ i \" : 0 . 4 \" m \" , } , \" _ o \" : { l , \" k \" , s , s , \" _ l \" : \" _ o \" : \" o i \" : \" r _ e \" : \" i _ _ n \" : \" c g \" : \" _ n g \" : s , s l , } } , { \" m \" , \" g i \" : \" c _ o \" : { \" c _ e \" : \" _ c _ i \" : 0 . 4 \" m \" , } , \" _ o \" : { s , \" k \" , s , s , \" _ l \" : \" _ o \" : \" o i \" : \" r _ e \" : \" i _ _ n \" : \" c g \" : \" _ n g \" : s , s l , } } ]"
        },
        {
            "title": "D Details on Experimental Setup",
            "content": "Calculation of results: The reported result for each model and training setup is the average across three training runs, where not otherwise stated. For each of the three trained models, we sample 10, 000 datapoints, compute evaluation metrics separately for the three resulting synthetic datasets, then calculate the average metric value across all runs. For LLM approaches, each model is trained for up to 50 epochs, using early stopping when the validation loss (assessed every 5000 steps) fails to improve twice in row. We perform grid search to select the 15 learning rate with lowest validation loss for each model and training setup, with selected learning rates reported in Appendix E.5. For non-LLM works, we follow the procedures detailed in each of these works. More detailed definition of Machine Learning Efficacy (MLE): Given synthetic dataset produced by generative model, we begin to calculate MLE by training one downstream classifier using the synthetic dataset. Then, we evaluate the performance of this downstream classifier on real test set, drawn from the same distribution as the generative models train set. We compare this classifiers performance to second classifier, which is trained on the same training data as the generative model. If the synthetically-trained classifier performs worse than the classifier trained on real data, then (intuitively) the synthetic data is of lower quality than the real data: for instance, the distributions of features in the real data are not well-reflected in the synthetic data. Put another way: given real dataset, we form disjoint training and test sets, denoted and respectively. generative model is trained on R, then generates synthetic dataset S. To calculate MLE, downstream classifier or regressor KR is trained using to predict predetermined label column, using all other columns as features. An additional classifier or regressor KS is similarly trained on S. Then, the performance of KS and KR on the real test dataset is evaluated: high-fidelity synthetic dataset will allow KS to exhibit similar performance to KR despite never encountering real datapoints before test-time. We report both KR and KS in our results, considering MLE to be the difference in performance between KR and KS. We use random forest classifier or regressor as our downstream model K. For classification datasets, we compare the accuracy of KR and KS, while for regression datasets, we compare the coefficient of determination R2. We define the coefficient of determination R2 as max(1 , 0), where and are the residual sum of squares and total sum of squares, respectively. This formulation means that if model performs worse than random guessing, its R2 value will be represented as 0. For both the accuracy and R2 coefficient metrics, higher score indicates higher-quality data. Information on Performance Profiles: For given method , its performance profile curve is defined as ρm(τ ) := 1 (cid:26) (cid:12) (cid:12) (cid:12) (cid:12) : st,m minmM {st,m} τ (cid:27)(cid:12) (cid:12) (cid:12) (cid:12) for set of tasks and scores st,m : , where lower values indicate better performance on each task. In order to satisfy the requirement that lower scores are better for the MLE metric, we set st,m = 1 MLEt,m. Then for each method, we obtain final score by taking the area under the curve ρm(τ ) to obtain the AUP score as AUPm = (cid:90) τ 0 ρm(τ )d log(τ ). with τ being the smallest τ such that ρm(τ ) = 1 for all methods , and where higher AUP score indicates better performance. Discrimination: Discrimination [21] quantifies the degree to which the generative model introduces spurious correlations or other patterns that differentiate synthetic from real data. Given the real training dataset and synthetic dataset S, we sample the same number of rows from each. Next, we train random forest classifier to discriminate between real and synthetic examples. Highest-quality synthetic data will result in 50% discrimination accuracy, indicating that is unable to distinguish between and S. For this reason, our reported discrimination scores are calculated as the absolute difference between 50% and the accuracy of discriminator C. Accordingly, lower discrimination scores represent better performance. Distance to Closest Record: Distance to Closest Record (DCR) [13] quantifies the distance between each synthetic datapoint and its most-similar example in the training set R. In addition to synthesis quality, this metric is an indication of the degree to which the model memorizes samples during training. Specifically, for each synthetic example S, we compute its distance to every training point (using L0 distance for categorical columns and L1 distance for numerical columns) and take the smallest of these distances. The overall DCR is then reported as the average of these minimum distances across all synthetic examples in SS. Lower DCR is associated with high-quality synthesis, but DCR score of 0 implies that most synthetic examples are merely copies of training dataset points memorized during training. As such, we consider the best DCR to be the lowest nonzero score. 16 Table 6: Discrimination metric (defined in Appendix D) for approaches compared in main results Section 3.1. Tabby produces data with better MLE without worsening the synthetic datas discrimination score, performing competitively with Tab-DDPM. CTGAN TVAE Diabetes 0.42 0.00 0.45 0.02 Tab-DDPM 0.11 0.00 0.04 0.01 0.06 0.02 0.28 0.01 0.29 0.02 0.27 0.02 0.28 0.02 Plain Base DGPT2 Plain Tabby MH DGPT2 GReaT Base DGPT2 GReaT Tabby MH DGPT2 GTT Base DGPT2 GTT Tabby MH DGPT2 Travel 0.27 0.01 0.50 0.00 0.05 0.03 0.03 0.02 0.02 0.01 0.06 0.01 0.08 0.03 0.07 0.01 0.07 0. Adult 0.48 0.00 0.46 0.01 0.01 0.01 0.09 0.01 0.10 0.01 0.20 0.01 0.20 0.01 0.20 0.02 0.13 0.05 Abalone 0.46 0.00 0.45 0.02 0.03 0.01 0.06 0.01 0.06 0.00 0.08 0.02 0.11 0.03 0.05 0.01 0.16 0.01 Rainfall 0.18 0.05 0.41 0.01 0.01 0.02 0.03 0.01 0.08 0.00 N/A* 0.45 0.09* 0.39 0.11 0.31 0.21 House 0.32 0.06 0.39 0.03 0.33 0.04 0.07 0.06 0.03 0.01 0.16 0.01 0.19 0.01 0.18 0.03 0.20 0.01 Table 7: Distance to Closest Record (DCR) metric (defined in Appendix D) for approaches compared in main results in Section 3.1. Tabby MH exhibits low, nonzero scores, indicating that its synthetic examples closely resemble real data without simply copying the training data points. CTGAN TVAE Diabetes 0.82 0.00 0.27 0.01 Tab-DDPM 0.63 0.04 0.01 0.00 0.02 0.00 0.33 0.00 0.36 0.00 0.31 0.01 0.37 0.01 Plain Base DGPT2 Plain Tabby MH DGPT2 GReaT Base DGPT2 GReaT Tabby MH DGPT2 GTT Base DGPT2 GTT Tabby MH DGPT2 Travel 0.59 0.03 0.10 0.06 0.00 0.00 0.01 0.00 0.01 0.00 0.02 0.01 0.01 0.00 0.02 0.00 0.02 0.00 Adult 1.70 0.09 0.16 0.03 0.31 0.03 0.33 0.15 0.25 0.03 0.12 0.03 0.17 0.08 0.14 0.01 0.16 0.07 Abalone 0.76 0.02 0.41 0.01 0.12 0.01 0.01 0.00 0.01 0.01 0.10 0.00 0.10 0.01 0.10 0.00 0.10 0.00 Rainfall 0.03 0.01 0.03 0.00 0.01 0.00 0.00 0.00 0.01 0.00 N/A* 0.01* 0.02 0.00 0.00 0. House 0.13 0.02 0.07 0.00 0.08 0.00 0.03 0.01 0.04 0.00 0.06 0.00 0.06 0.00 0.06 0.00 0.05 0."
        },
        {
            "title": "E Further Experimental Results",
            "content": "E.1 Additional Metrics for Main Results Tables For the experiment presented in Section 3.1, we include the two additional metrics of discrimination and distance to closest record (DCR) in Tables 6 and 7 respectively. These metrics largely corroborate our findings in Section 3.1. In particular, Plain Tabby MHs low DCR and discrimination scores indicate that this models synthetic data closely resembles that of real data. Additionally, the DCR scores are small but nonzero, which indicates that the model is generating novel datapoints rather than simply repeating datapoints memorized during training. E.2 Applying Tabby to Transformer MLPs or Attention Blocks We examine in-detail the performance of Tabby models with MoE applied to the transformer MLPs or attention blocks. We use the following terminology to refer to these architectures, which are visualized in Figure 8: Multi-MLP when each transformers MLP block is replaced with an MoE layer, Multi-MLP and Multi-Head (MMLP+MH) when each transformers MLP block is replaced with an MoE layer and the LM head is replaced with an MoE layer, Multi-Attention (MA) when each transformers attention block is replaced with an MoE layer. We focus on Tabby MH in Sections 3.1-3.4 because it demonstrates top performance in most settings. We display results for the MMLP and MMLP+MH architectures across all six datasets for MLE, discrimination and DCR in Tables 8, 9 and 10, respectively. All three metrics are displayed for the MA architecture on two datasets in Table 11. E.3 Additional Metrics for Experiment Applying Tabby MH to Models of Varying Sizes The results in Section 3.2 compare the MLE scores of Plain-trained models of varying sizes on the Travel dataset. Table 12 incorporates the results for the Diabetes and House datasets as well. Similarly, Table 13 presents results for models trained using GReaT and Tabula (TapTap is not included here, because TapTap-pretrained checkpoints are available only for Distill-DGPT2 and GPT2). Table 8: Machine Learning Efficacy (MLE), defined in Section 3.0, for Base non-Tabby GPT2 models, as well as Tabby models with MoE layers applied to the transformer MLPs, language modeling head, or both (notated as MMLP, MH, and MMLP+MH respectively). Original Plain Non-Tabby Plain Tabby MMLP Diabetes 0.73 0.75 0.02 0.75 0.03 Plain Tabby MH 0.74 0.00 0.68 0.02 0.62 0.01 0.74 0.01 0.64 0.01 0.69 0.04 0.72 0.06 0.69 0.04 0.62 0.00 0.70 0.04 Plain Tabby MMLP+MH GReaT Non-Tabby GReaT Tabby MMLP GReaT Tabby MH GReaT Tabby MMLP+MH GTT Non-Tabby GTT Tabby MMLP GTT Tabby MH GTT Tabby MMLP+MH Travel 0.87 0.86 0.01 0.83 0.02 0.88 0.01 0.83 0.01 0.85 0.02 0.85 0.03 0.86 0.01 0.83 0.02 0.87 0.02 0.87 0.01 0.85 0.01 0.85 0.02 Adult 0.85 0.85 0.00 0.77 0.01 0.85 0.00 0.76 0.01 0.83 0.01 0.84 0.01 0.83 0.00 0.83 0.01 0.83 0.01 0.84 0.00 0.76 0.07 0.84 0. Abalone 0.45 0.44 0.01 0.32 0.03 0.43 0.01 0.33 0.03 0.41 0.01 0.38 0.01 0.40 0.01 0.38 0.03 0.40 0.01 0.36 0.01 0.37 0.02 0.38 0.02 Rainfall 0.54 0.52 0.03 0.35 0.04 0.49 0.00 0.36 0.19 N/A* 0.24 0.25 0.00 0.00* 0.17 0.30 0.05 0.01 0.03 0.00* 0.26 0.37 0.09 0.13 House 0.61 0.55 0.08 0.00 0.00 0.60 0.00 0.02 0.03 0.56 0.01 0.56 0.02 0.56 0.01 0.57 0.01 0.55 0.02 0.56 0.01 0.55 0.00 0.57 0.00 Table 9: Discrimination metric, (defined in Appendix D), for Base non-Tabby GPT2 models, as well as Tabby models with MoE layers applied to the transformer MLPs, language modeling head, or both (notated as MMLP, MH, and MMLP+MH respectively). Plain Non-Tabby Plain Tabby MMLP Plain Tabby MH Plain Tabby MMLP+MH GReaT Non-Tabby GReaT Tabby MMLP GReaT Tabby MH GReaT Tabby MMLP+MH GTT Non-Tabby GTT Tabby MMLP GTT Tabby MH GTT Tabby MMLP+MH Diabetes 0.04 0.01 0.22 0.03 0.06 0.02 0.19 0.02 0.28 0.01 0.23 0.01 0.29 0.02 0.24 0.01 0.27 0.02 0.28 0.01 0.28 0.02 0.24 0. Travel 0.03 0.02 0.02 0.02 0.02 0.01 0.03 0.02 0.06 0.01 0.10 0.02 0.08 0.03 0.09 0.01 0.07 0.01 0.09 0.01 0.07 0.02 0.08 0.01 Adult 0.09 0.01 0.22 0.06 0.10 0.01 0.25 0.11 0.20 0.01 0.19 0.00 0.20 0.01 0.21 0.01 0.20 0.02 0.18 0.01 0.13 0.05 0.18 0.00 Abalone 0.06 0.01 0.19 0.04 0.06 0.00 0.22 0.03 0.08 0.02 0.08 0.01 0.11 0.03 0.07 0.00 0.05 0.01 0.14 0.02 0.16 0.01 0.14 0.02 Rainfall 0.03 0.01 0.12 0.00 0.08 0.00 0.12 0.01 N/A* 0.27 0.17 0.45 0.09* 0.24 0.17 0.39 0.11 0.46 0.07* 0.31 0.21 0.24 0.24 House 0.07 0.06 0.19 0.06 0.03 0.01 0.23 0.03 0.16 0.01 0.16 0.01 0.19 0.01 0.16 0.00 0.18 0.03 0.18 0.01 0.20 0.01 0.16 0.01 Table 10: Distance to Closest Record (DCR) metric (defined in Appendix D) for Base non-Tabby GPT2 models, as well as Tabby models with MoE layers applied to the transformer MLPs, language modeling head, or both (notated as MMLP, MH, and MMLP+MH respectively). Plain Non-Tabby Plain Tabby MMLP Plain Tabby MH Plain Tabby MMLP+MH GReaT Non-Tabby GReaT Tabby MMLP GReaT Tabby MH GReaT Tabby MMLP+MH GTT Non-Tabby GTT Tabby MMLP GTT Tabby MH GTT Tabby MMLP+MH Diabetes 0.01 0.00 0.35 0.03 0.02 0.00 0.34 0.02 0.33 0.00 0.34 0.01 0.36 0.00 0.33 0.02 0.31 0.01 0.31 0.02 0.37 0.01 0.31 0.00 Travel 0.01 0.00 0.08 0.00 0.01 0.00 0.07 0.00 0.02 0.01 0.02 0.00 0.01 0.00 0.02 0.00 0.02 0.00 0.02 0.00 0.02 0.00 0.02 0.00 Adult 0.33 0.15 0.55 0.09 0.25 0.03 0.39 0.15 0.12 0.03 0.12 0.01 0.17 0.08 0.11 0.01 0.14 0.01 0.14 0.03 0.16 0.07 0.11 0.02 Abalone 0.01 0.00 0.21 0.02 0.01 0.01 0.20 0.03 0.10 0.00 0.10 0.00 0.10 0.01 0.10 0.00 0.10 0.00 0.10 0.00 0.10 0.00 0.11 0.00 Rainfall 0.00 0.00 0.03 0.00 0.01 0.00 0.03 0.01 N/A* 0.00 0.01 0.01* 0.01 0.00 0.02 0.00 0.01* 0.00 0.01 0.01 0. House 0.03 0.01 1.7e12 2.7e12 0.04 0.00 2.3e12 4.1e12 0.06 0.00 0.06 0.00 0.06 0.00 0.06 0.00 0.06 0.00 0.06 0.00 0.05 0.00 0.06 0.00 18 Table 11: All evaluation metrics, for non-Tabby models and Tabby models with MoE applied to the transformer attention blocks (abbreviated as Tabby MA). Base LLM is DGPT2. MLE () Discrimination () Diabetes House Diabetes House DCR () Diabetes House Original (Upper Bound) Plain Non-Tabby Plain Tabby MA GTT Non-Tabby GTT Tabby MA 0.73 0.75 0.62 0.72 0.62 0.61 0.55 0.08 0.55 0.56 0.04 0.23 0.27 0.31 0.07 0.28 0.18 0.17 0.01 0.41 0.31 0. 0.03 0.08 0.06 0.06 Table 12: Results using Plain training for all three datasets of the experiment in Section 3.2, which compares non-Tabby and Tabby MH models across base LLMs of varying sizes. Travel Diabetes House MLE () 0.87 Original (Upper Bound) 0.86 0.01 Base Pythia 14m 0.82 0.02 Tabby MH Pythia 14m 0.88 0.00 Base Distilled-GPT2 0.89 0.02 Tabby MH Distilled-GPT2 0.89 0.01 Base GPT2 0.87 0.01 Tabby MH GPT2 Base Pythia 160M 0.87 0.01 Tabby MH Pythia 160M 0.86 0.00 Base Pythia 410M 0.86 0.02 Tabby MH Pythia 410M 0.88 0.03 0.82 0.01 0.84 0.02 0.84 0.01 0.86 0. Base Llama 3.2 1B Tabby MH Llama 3.2 1B Base Llama 3.1 8B Tabby MH Llama 3.1 8B Params 14M 53M 82M 310M 120M 360M 160M 390M 410M 710M 1.2B 2.8B 8.0B 11B MLE () 0.73 0.76 0.02 0.77 0.00 0.73 0.02 0.73 0.01 0.76 0.01 0.73 0.03 0.75 0.04 0.73 0.02 0.74 0.03 0.72 0.05 0.73 0.01 0.68 0.09 0.75 0.01 0.72 0.01 Params 14M 66M 82M 390M 120M 430M 160M 470M 410M 820M 1.2B 3.3B 8.0B 12B MLE () 0.61 0.52 0.07 0.54 0.01 0.53 0.10 0.61 0.01 0.60 0.00 0.53 0.11 0.52 0.11 0.54 0.02 0.28 0.40 0.54 0.02 0.29 0.01 0.18 0.26 0.35 0.01 0.30 0.01 Params 14M 66M 82M 390M 120M 430M 160M 470M 410M 820M 1.2B 3.3B 8.0B 12B Table 13: Results using GReaT and Tabula training for all three datasets of the experiment in Section 3.2, which compares non-Tabby and Tabby MH models across base LLMs of varying sizes. Travel Diabetes House MLE () 0.87 Original (Upper Bound) 0.81 0.00 Base Pythia 14m 0.81 0.00 Tabby MH Pythia 14m 0.86 0.00 Base Distilled-GPT2 0.84 0.00 Tabby MH Distilled-GPT2 0.85 0.02 Base GPT2 0.87 0.03 Tabby MH GPT2 Base Pythia 160M 0.81 0.01 Tabby MH Pythia 160M 0.82 0.02 Base Pythia 410M 0.85 0.01 Tabby MH Pythia 410M 0.83 0.01 0.82 0.01 0.78 0.03 0.78 0.04 0.83 0.03 Base Llama 3.2 1B Tabby MH Llama 3.2 1B Base Llama 3.1 8B Tabby MH Llama 3.1 8B Params 14M 53M 82M 310M 120M 360M 160M 390M 410M 710M 1.2B 2.8B 8.0B 11B MLE () 0.73 0.60 0.04 0.67 0.01 0.62 0.00 0.70 0.06 0.64 0.02 0.74 0.03 0.70 0.01 0.73 0.03 0.73 0.03 0.74 0.04 0.70 0.08 0.71 0.03 0.67 0.01 0.73 0. Params 14M 66M 82M 390M 120M 430M 160M 470M 410M 820M 1.2B 3.3B 8.0B 12B MLE () 0.61 0.46 0.06 0.51 0.03 0.57 0.00 0.56 0.01 0.55 0.00 0.58 0.01 0.00 0.00 0.54 0.02 0.53 0.02 0.58 0.01 0.53 0.01 0.43 0.08 0.53 0.01 0.45 0.00 Params 14M 66M 82M 390M 120M 430M 160M 470M 410M 820M 1.2B 3.3B 8.0B 12B Figure 8: An overview of the Tabby MH modifications that can occur inside the LLM transformer blocks. Left to right: an original, non-Tabby LLM, Tabby LLM with MoE MLP block, Tabby LLM with MoE attention block, and Tabby LLM with both MoE MLP and attention blocks. Tabby is very flexible, so as to accommodate wide variety of tabular datasets. E.4 Analysis from Tracking the Adaptation to Individual Columns We observe that Occupancy is the largest contributor to the models loss until step 32000. While Median Incomes loss is initially the second-lowest, it improves little throughout the training process and exhibits the highest loss of all columns at the end of training. Additionally, we view that convergence occurs around step 40000. These insights are useful in cases where the model struggles to learn some columns more than others. Such information may indicate need for better preprocessing for difficult column, or gathering more datapoints to demonstrate the columns distribution. Additionally, the ability to track each columns loss individually and to determine that the losses are roughly balanced across columns, rather than very low in some columns and very high in others, may improve trust in the modelwe can understand that there is low, aleatoric error in each column as opposed to sizeable epistemic error in few columns. E.5 Hyperparameters for All Experiments We list the learning rates chosen for Section 3.1 in Table 14, Section 3.2 in Table 15 and Section 17 in Table 17. We select the learning rate that yields lowest training loss on one run from the set {1e 3, 1e 4, 1e 6, 1e 8}. For non-LLM methods in our experiments, we use the hyperparameters recommended by their respective papers. 20 Table 14: Learning rates for LLM results presented in Section 3.1. Plain Non-Tabby Plain Tabby MMLP Plain Tabby MH Plain Tabby MMLP+MH GReaT Non-Tabby GReaT Tabby MMLP GReaT Tabby MH GReaT Tabby MMLP+MH GTT Non-Tabby GTT Tabby MMLP GTT Tabby MH GTT Tabby MMLP+MH Diabetes 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 6 1e 4 1e 4 1e 4 1e 6 1e 4 Travel 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 Adult 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e Abalone Rainfall House 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 Table 15: Learning rates for Plain-trained LLMs of varying sizes in Section 3.2. Plain Training Base Pythia 14M 1e 4 Tabby MH Pythia 14M 1e 6 1e 4 Base Distilled-GPT2 Tabby MH Distilled-GPT2 1e 4 1e 4 Base GPT2 Tabby MH GPT2 1e 4 Base Pythia 160M 1e 4 Tabby MH Pythia 160M 1e 4 Base Pythia 410M 1e 6 Tabby MH Pythia 410M 1e 6 Base Llama 3.2 1B 1e 6 Tabby MH Llama 3.2 1B 1e 6 Base Llama 3.1 8B 1e 6 Tabby MH Llama 3.1 8B 1e 6 Travel Diabetes House 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 6 1e 4 1e 4 1e 4 1e 6 1e 4 1e 4 1e 6 1e 6 1e 4 1e 6 1e 4 1e 6 1e 6 1e 6 1e 6 Table 16: Learning rates for GReaT (plus TapTap)-trained LLMs of varying sizes in Section 3.2. GReaT + TapTap Training Base Pythia 14M 1e 4 Tabby MH Pythia 14M 1e 6 1e 4 Base Distilled-GPT2 Tabby MH Distilled-GPT2 1e 4 1e 4 Base GPT2 Tabby MH GPT2 1e 4 Base Pythia 160M 1e 4 Tabby MH Pythia 160M 1e 6 Base Pythia 410M 1e 6 Tabby MH Pythia 410M 1e 4 Base Llama 3.2 1B 1e 4 Tabby MH Llama 3.2 1B 1e 4 Base Llama 3.1 8B 1e 4 Tabby MH Llama 3.1 8B 1e 4 Travel Diabetes House 1e 4 1e 6 1e 4 1e 6 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 4 1e 6 1e 6 1e 6 1e 6 1e 6 1e 6 1e 6 1e 6 1e 4 1e 4 1e 4 1e 6 1e 4 1e 6 1e 4 Table 17: Learning rates for JSON Glaucoma [15] experiment presented in Section 3.4. Base DGPT2 Tabby MH DGPT2 Glaucoma 1e 4 1e"
        }
    ],
    "affiliations": [
        "GE HealthCare",
        "University of Wisconsin-Madison"
    ]
}