{
    "paper_title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction",
    "authors": [
        "Zhixiong Zhang",
        "Shuangrui Ding",
        "Xiaoyi Dong",
        "Songxin He",
        "Jianfan Lin",
        "Junsong Tang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 5 8 5 1 . 7 0 5 2 : r SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction Zhixiong Zhang1,2 , Shuangrui Ding3, Xiaoyi Dong2,3 , Songxin He4, Jianfan Lin4, Junsong Tang4, Yuhang Zang2, Yuhang Cao2, Dahua Lin2,3, Jiaqi Wang2 1Shanghai Jiao Tong University 3The Chinese University of Hong Kong 2Shanghai AI Laboratory 4Harbin Institute of Technology"
        },
        {
            "title": "Abstract",
            "content": "Video Object Segmentation (VOS) is core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the humanlike conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding highlevel conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. Empirical evaluations demonstrate that SeC substantially outperforms state-of-the-art approaches, including SAM 2 and its advanced variants, on both SeCVOS and standard VOS benchmarks. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing new state-of-the-art in concept-aware video object segmentation. The code and dataset are released at https://github.com/OpenIXCLab/SeC."
        },
        {
            "title": "Introduction",
            "content": "Video Object Segmentation (VOS) is pivotal task in computer vision, focusing on the precise delineation and temporal tracking of target objects within video sequences. By capturing both spatial and temporal dynamics, VOS enables comprehensive scene understanding, which is essential for range of applications including autonomous driving [35], robotic perception [16], video editing [38], augmented reality [15], and intelligent surveillance systems [1]. Recent VOS models have achieved high accuracy on standard benchmarks [30, 44]. key component in many of these approaches Equal Contribution Corresponding Author Preprint. Under review. Figure 1: Overview of our Segment Concept (SeC) framework. Left: Compared to SAM 2, our model maintains better target tracking under severe appearance changes and scene transitions by leveraging concept-level guidance. Right: Quantitative results show that SeC consistently outperforms strong baselines, especially in scenarios involving multiple scene changes. is memory-based matching, where the model identifies the target in each frame by measuring its similarity to previously observed instances. Despite their success, these methods remain far inferior to human capability in real-world scenarios, particularly when the appearance of the target changes drastically across frames due to occlusions, viewpoint shifts, or complex scenes. We argue that this limitation arises from fundamental gap between how machines and humans perceive objects over time. Human perception is not confined to surface-level similarity; instead, it involves the construction of holistic, conceptual understanding of the target object by integrating observations across frames. This high-level representationwhat we refer to as an object-level conceptallows humans to robustly recognize the same object even under significant appearance variations. Take the left part of Figure 1 as an example: although the target (Harry Potter) remains visually consistent with his red and gold uniform, SAM 2 frequently loses track of him when the scene changes or other characters with similar appearances are introduced. However, if the model were capable of concept-level reasoning, for example by recognizing that Harry is an active player rather than spectator, such errors could be significantly reduced. This observation motivates paradigm shift: from conventional appearance matching to conceptdriven segmentation. We take step in this direction by equipping segmentation models with the ability to form and leverage high-level object concepts over time. To achieve this, we introduce Segment Concept (SeC), concept-driven segmentation framework that progressively constructs concept-level representation of the target object by integrating information across frames. Rather than relying on superficial appearance matching, SeC leverages the conceptual reasoning capabilities of large vision-language models (LVLMs), drawing upon their rich visual understanding and world-prior knowledge to build and refine object-level concepts. This enables robust segmentation under challenging conditions such as occlusions, appearance changes, and scene variations. During online segmentation, SeC samples diverse subset of past frames to serve as input to the LVLM, progressively modeling the concept of the target object over time. These keyframes, arranged in temporal order along with the current query frame, are processed by the LVLM, which uses learnable embedding to distill the concept essence of the target. This semantic representation is then injected into the query frame via cross-attention, guiding segmentation through conceptual priors rather than low-level feature similarity. When reviewing human behaviors, we find that humans perceive videos adaptively: for most coherent frames, quick glances are sufficient; only when significant changes occur, such as occlusions or abrupt shifts, do we engage deeper reasoning, leveraging previously formed concepts to re-identify the target. Motivated by this behavior, SeC further employs scene-adaptive activation strategy: it invokes LVLM-based concept reasoning when complex variations arise, updating the concept representation accordingly. For simpler, stable scenes, it falls back to an enhanced matching mechanism for efficient segmentation. This design ensures robust yet computationally efficient segmentation pipeline. 2 To jointly advance algorithmic development and its evaluation, we curate the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS), specifically designed to evaluate models on complex scenarios demanding high-level reasoning. SeCVOS consists of 160 manually annotated multi-shot videos, selected from the Shot2Story dataset [18] and additional videos crawled from YouTube, featuring highly discontinuous frame sequences with frequent object re-appearances and dynamic visual changes, presenting significant challenges to existing VOS methods. Experimental evaluations demonstrate that state-of-the-art models such as Cutie [6] and SAM 2 [32] achieve limited success on SeCVOS, all scoring below 65 &F, highlighting the necessity for improved semantic reasoning capabilities in current VOS approaches. We intend to release SeCVOS as an open-source benchmark to facilitate further advancements in semantic-level video object segmentation. Extensive evaluations across multiple established VOS benchmarks validate the effectiveness of SeC. On the challenging SeCVOS dataset, our method significantly outperforms SAM 2.1 and its recent variants, achieving an average improvement of 11.8 points in &F over SAM 2.1. Besides, SeC consistently surpasses prior state-of-the-art across standard benchmarks, including SA-V and LVOS. Specifically, it improves over SAM 2.1 by 4.1 on SA-V and 2.4 on LVOS v2. This demonstrates the advantage of integrating fine-grained pixel association with object-level semantic reasoning derived from multimodal LLMs."
        },
        {
            "title": "2 Related work",
            "content": "Memory-based VOS. VOS models typically propagate labels by matching pixel-level features between query and memory frames. Classical memory-based models [13, 29, 7, 52, 14, 25, 28, 34, 8, 43, 48, 47] perform well on short-term tracking but often struggle with distractors due to their reliance on low-level visual cues. Recent methods incorporate object-level information to improve robustness [2, 39, 6, 31]. For instance, Cutie [6] introduces object-level memory queries that encode semantic and long-term context, enabling stronger target-background separation. ISVOS [39] injects features from pre-trained Mask2Former [5] detector to make embeddings instance-aware. While both models show the benefits of adding semantic cues, their semantic reasoning remains limited to instance-level features. In our work, we leverage LVLMs to inject rich concept-level semantic features into the memory module, further strengthening the models semantic understanding. LVLMs for fine-grained perception. Large vision-language models (LVLMs) [21, 37, 40, 4, 51] have recently emerged as powerful tools for bringing semantic understanding into dense prediction tasks [26, 22, 45, 3, 49, 36]. LISA [22] pioneered reasoning-based segmentation for images by using an LVLMs with special <SEG> token that is decoded into mask. VISA [45] extends this concept to videos by integrating text-guided keyframe selection with SAM-style decoder for per-frame segmentation. UFO [36] takes this methodology step further by unifying detection, segmentation, and captioning tasks through an open-ended language interface. Unlike traditional vision-only models, LVLMs introduce new level of interpretability and task flexibility through language interfaces, offering improved robustness for handling complex queries via multimodal reasoning. In contrast to these text-driven paradigms, our work focuses on implicitly leveraging the conceptual reasoning capacity of LVLMs without requiring textual inputs or outputs. We repurpose the LVLM as visual concept extractor to guide segmentation directly through latent object-level reasoning. VOS benchmarks. Several recent datasets [23, 27, 44, 11, 20, 32] have pushed VOS evaluation toward more challenging settings. MOSE [11] introduces complex real-world scenes with frequent occlusions, crowded backgrounds, and disappearing-reappearing targets, exposing failure cases where traditional models struggle. SA-V [32] scales up to massive dataset of 51k videos, including small, occluded, and reappearing objects to evaluate mask propagation. Meanwhile, LVOS [20] focuses on long-term segmentation: its videos average over 60 seconds and feature long-duration object interactions such as objects leaving and later re-entering the scene. However, they still primarily measure how well model can match pixel-level masks across time. Notably, none incorporate multi-view scenarios or concept-level variation, making it difficult to assess models higher-level semantic perception or reasoning capabilities. In contrast, our proposed benchmark SeCVOS is designed to fill this gap. It includes complex multi-shot contextual changes throughout the sequence. This setup requires models to go beyond low-level tracking. They must reason about the targets identity, roles, and intent as the contextual shifts, effectively evaluating semantic understanding in video object segmentation."
        },
        {
            "title": "3.1 Preliminary study on current VOS",
            "content": "To understand the limitations of current VOS approaches in complex scenarios, we conduct detailed evaluation on our SeCVOS benchmark. As shown in Figure 1, we categorize videos by the number of scene transitions and report the standard metric &F. Surprisingly, even the state-of-the-art SAM 2 model [32] exhibits substantial performance degradation in videos with only one scene changes. These results indicate the limitations of memory-based designs that rely heavily on lowlevel visual similarity, lacking the conceptual reasoning needed to maintain object identity across drastic appearance variations. In contrast, recent large vision-language models (LVLMs) [17, 4, 41] have demonstrated impressive visual understanding and reasoning capabilities. This raises an important question: can LVLMs help address the conceptual limitations of traditional VOS models? To explore this, we conduct simple experiment using GPT-4o [21], the cutting-edge LVLM capable of interpreting visual scenes. Given sequence of reference frames and query frame with significant appearance changes, GPT-4o is able to correctly localize the target object. As shown in Figure 1, the model not only makes accurate predictions, but also provides textual justifications that connect the object in the query frame to prior visual evidence. This suggests that LVLMs possess the ability to infer object identity beyond surface-level cues, by leveraging powerful visual perception and conceptual reasoning grounded in vast multimodal knowledge. Inspired by this, we propose SeC, novel framework that integrates LVLM-based object concepts into the video segmentation pipeline. Our model demonstrates strong robustness against drastic scene variations, major limitation of prior VOS methods."
        },
        {
            "title": "3.2 Segement Concept model",
            "content": "Our goal is to enhance VOS model with concept-level LVLM-based guidance, which enables the learning of object-level representations that are robust to significant appearance changes. At the same time, the model retains the ability to provide reliable pixel-level guidance when no visual scene change is detected. Concept guidance with an LVLM. To facilitate robust concept-level reasoning, we maintain sparse keyframe bank throughout the video, which provides diverse view of the target concept to large vision-language model (LVLM). This bank is initialized with the first annotated frame and dynamically updated during tracking. new frame is added when it both differs significantly from existing keyframes and yields confident segmentation result, ensuring diversity without sacrificing reliability. To balance efficiency and semantic coverage, we retain only the initial frame and FIFO buffer of the most recent representative keyframes, capped by fixed window size. This ensures that the LVLM receives compact yet semantically rich set of frames for robust concept distillation. Inspired by LISA [22], we append special <SEG> token to the end of the keyframe sequence, prompting the LVLM to summarize the object concept into this special token. The hidden state corresponding to the <SEG> token is then extracted as the object-level concept guidance vector. Scene-adaptive activation strategy. Since most consecutive frames exhibit high temporal coherence, applying concept-level guidance to every frame is computationally redundant. Instead, lightweight pixel-level matching suffices in these cases. To this end, we propose scene-adaptive activation strategy. Specifically, we detect whether the incoming frame exhibits significant scene change compared to the previous one. If no such change is detected, we rely solely on the pixel-level association memory and feed the memory-enhanced image features directly into the mask decoder to generate the final prediction. Otherwise, we activate concept-level reasoning via the LVLM. The resulting concept guidance vector is fused with the current frame features through lightweight cross-attention module. The concept-enhanced spatial features are then pointwise added to the memory-enhanced features, enabling the model to produce segmentation predictions guided by both semantic priors and low-level visual correspondence. This fusion effectively combines high-level semantic priors from LVLMs with fine-grained visual cues, enabling the model to remain robust across drastic appearance variations. 4 Table 1: Ablation on concept guidance. The offline mode constructs more holistic concept of the target object. Table 2: Efficiency comparison of SeC and SAM 2. Throughput is measured on one NVIDIA A800 GPU."
        },
        {
            "title": "Concept\nconstruction",
            "content": "J &F F"
        },
        {
            "title": "None\nOnline\nOffline",
            "content": "62.2 70.0 71.8 61.8 69.7 71.5 62.6 70.2 72.1 Benchmark Method &F Con. Guid. Ratio (%) Throughput (s1) SeCVOS SA-V SeC SAM 2 SeC SAM 2 70.0 58.2 82.7 78. 7.4 N/A 1.0 N/A 14.8 22.0 18.1 22.0 Implementation details. In practice, we build upon the memory attention mechanism of SAM 2 [32] as the foundation for our pixel-level association memory. On top of this, we augment the memory module with an enhanced long-term memory by extending the temporal positional encoding to support wider temporal window of up to 22 frames. Following SAM2Long [12], we apply an object-aware filtering strategy that picks only frames with non-zero occlusion scores, ensuring that memory is constructed from frames where visible object is present. This ensures that memory is both temporally broad and semantically relevant, reducing noise from uninformative frames."
        },
        {
            "title": "3.3 Discussion",
            "content": "In this section, we present two-part practical analysis to shed light on the intuition behind SeC. Does SeC progressively construct concept-level representation? During the online video segmentation process, frames are segmented sequentially, and the object concept is incrementally constructed as the video progresses. As result, the final concept obtained after processing the entire video can be considered an expressive representation. This naturally leads to an intuitive idea: if the concept is indeed refined progressively, re-segmenting the video using the finalized concept should yield improved results. To validate this hypothesis, we define this re-segmentation process as an offline segmentation task and evaluate its effectiveness on the SeCVOS benchmark. As shown in Table 1, the offline strategy yields the highest performance, indicating that concept representations constructed from more diverse and comprehensive set of frames lead to better segmentation quality. This aligns well with our core intuition: the model benefits from observing richer set of visual cues to form more complete and robust understanding of the target object. Does SeC require frequent concept guidance? To investigate the optimal frequency for activating LVLM-based concept reasoning in SeC, we conduct an ablation study on SeCVOS, varying the concept activation rate. This is implemented by adjusting the threshold used to determine whether scene change has occurred. As illustrated in Figure 2, enabling concept guidance on fewer than 10% of frames already results in significant improvement in segmentation performance, with marginal gains beyond that point. This observation suggests that frequent activation of concept-level reasoning is unnecessary. Sparse yet timely activations are sufficient to capture critical semantic transitions. Figure 2: &F Curve in terms of concept guidance ratio on SeCVOS. Sparse activation (e.g., under 10%) already achieves strong performance. Furthermore, Table 2 highlights that SeC maintains competitive inference speed despite the additional reasoning cost. On both SeCVOS and SA-V benchmarks, SeC achieves higher &F scores with minimal concept guidance usage (7.4% and 1.0%, respectively). This confirms that our scene-adaptive activation strategy effectively balances accuracy and efficiency, selectively invoking concept reasoning only when appearance variations demand it. 5 Table 3: Comparison between our SeCVOS and existing VOS benchmarks in terms of videos count, average duration, number of scenes, and disappearance rate (Disapp. Rate). Scene counts are consistently estimated using the scenedetect library. Estimated using 6 FPS for MOSE. #Videos Avg. Duration (s) Disapp. Rate Avg. #Scene DAVIS [30] YTVOS [44] MOSE [11] SA-V [32] LVOS [20] SeCVOS (ours) 90 507 311 155 140 160 2.87 4.51 8.68 17.24 78.36 29. 16.1% 13.0% 41.5% 25.5% 7.8% 30.2% 1.06 1.03 1.06 1.09 1.47 4."
        },
        {
            "title": "4 SeCVOS benchmark",
            "content": "Benchmarks play crucial role in driving model breakthroughs by providing standardized evaluation protocols. However, we observe that most existing VOS benchmarks [11, 32, 19, 27, 23] are becoming saturated, with state-of-the-art models already achieving over 90 in &F scores on widely-used datasets such as YouTube-VOS [44] and DAVIS [30]. As result, further improvements on these benchmarks offer diminishing insights into model robustness. More critically, current benchmarks fail to incorporate dedicated evaluation settings that assess models performance under semantically challenging conditions, such as long-range occlusions, scene discontinuities, and cross-shot object reasoning. To address this gap, we propose the Semantic Complex Scenarios Video Object Segmentation (SeCVOS) benchmark, specifically designed to assess models ability to perform high-level semantic reasoning across complex visual narratives. SeCVOS contains 160 carefully curated multi-shot videos characterized by: 1) Highly discontinuous frame sequences, 2) Frequent reappearance of objects across disparate scenes, and 3) Abrupt shot transitions and dynamic camera motion. These characteristics introduce substantial challenges for existing memory-based approaches, which predominantly rely on local visual similarity and often fail to maintain object identity across shots. Despite the challenges within these scenarios, they are frequently encountered in real-world VOS applications, such as video editing, surveillance, and story-centric content understanding. Therefore, developing benchmarks that target these conditions is both necessary and important. To construct the SeCVOS benchmark, we first filtered videos with three criteria to ensure sufficient spatiotemporal complexity: (1) minimum duration of 20 seconds, (2) semantically meaningful. The semantics are filtered following the strategy introduced in the Shot2Story [18] to remove less informative videos. Next, we employed GPT-4o to analyze the video content and identify target objects that appear frequently and unambiguously across scenes. Initial object masks were generated using SAM 2 [32], and subsequently refined through multiple rounds of manual correction to ensure high-quality and accurate annotations. The resulting SeCVOS benchmark consists of 160 multi-shot videos, each averaging 29.36 seconds in duration and containing 4.26 distinct scenes per video, significantly surpassing existing benchmarks in scene diversity. As shown in Table 3, SeCVOS features high disappearance rate of 30.2%, reflecting the frequent occlusions and reappearances of objects across shots. In contrast, prior benchmarks contain mostly single-scene with low semantic discontinuity. We will release SeCVOS as an open-source benchmark to support future research in concept-driven video object segmentation."
        },
        {
            "title": "5 Experiments",
            "content": "5."
        },
        {
            "title": "Implementation details",
            "content": "Model Architecture. Our model is built upon the SAM 2.1-large backbone [32]. Specifically, we reuse its image encoder and mask decoder components without fine-tuning. On top of this base, we incorporate pixel-level association memory and an LVLM-based concept guidance module to enhance temporal modeling and semantic reasoning. 6 Method Xmem [8] DEVA [7] Cutie-base [6] SAM2.1 [32] SAMURAI [46] SAM2.1Long [12] SeC (Ours) Table 4: Performance comparison on SeCVOS datasets. No Scene Change Single Scene Change Multi Scene Change &F 71.9 71.6 72.5 79.4 81.8 81.3 84.2 +4.8 72.0 71.6 72.2 79.1 81.6 81.0 83.8 71.8 71.5 72.8 79.7 81.9 81.6 84.5 &F 47.0 48.5 53.0 58.5 60.6 61.8 69.6 +11.1 47.9 48.4 52.9 58.2 60.6 61.6 69.5 46.2 48.6 53.2 58.8 60.7 62.0 69.7 &F 41.9 46.4 48.3 52.4 59.3 58.5 67.5 +15.1 42.4 46.0 47.8 52.1 58.9 58.1 67.0 41.4 46.8 48.9 52.6 59.7 58.9 68.0 Overall &F 48.4 49.7 52.7 58.2 62.2 62.3 70.0 +11.8 Scene change detection. To determine whether frame should trigger concept-level reasoning, we employ lightweight HSV-based scene change detector. Specifically, we compute 2D color histograms over the hue and saturation channels of the current and previous frames, normalize them, and measure their difference using the Bhattacharyya distance. scene change is detected if the distance exceeds predefined threshold, which we set to 0.35 by default. Empirically, we find this threshold to be robust against minor variations while remaining sensitive to significant appearance shifts. Training. We adopt two-stage training approach: (1) training the pixel-level association memory module for long-term temporal modeling, and (2) fine-tuning the LVLM-based semantic guidance module for concept modeling. In the first stage, we train the pixel-level association memory using 2k videos from the SA-V training set, selected based on the highest number of scene transitions as detected by SceneDetect. For each video, 24 shuffled frames are randomly sampled for training. During this stage, only the memory attention module is updated, while all other components remain frozen. The model is trained for 40 epochs with batch size of 64 and learning rate of 5 106. In the second stage, we fine-tune InternVL 2.5 [4] on approximately 190k object instances from the SA-V training set, each containing at least three visible masks. For each training sample, 1 to 7 reference frames are randomly selected. Instead of overlaying an alpha-blended object mask, we draw green contour around the target object. This contour effectively highlights the segmentation target without obstructing the visual features needed for LVLM-based perception. Among these, 0 to 2 are distractor frames containing incorrect annotations, while the rest provide valid visual prompts. Additionally, one non-overlapping query frame is included. All images are resized to 448 448. We apply LoRA-based fine-tuning to the InternVL 2.5, while keeping all SAM 2 parameters frozen. The model is trained for 3 epochs with batch size of 64 and learning rate of 4 105. All experiments are conducted on 8 NVIDIA A800 GPUs, and the loss function remains consistent with that of SAM 2. Benchmarks. To evaluate our method, we conduct experiments on six standard video object segmentation (VOS) benchmarks: SA-V [32], LVOS v2 [20], MOSE [11], DAVIS [30], YouTubeVOS [44], and our proposed SeCVOS dataset. We report three commonly used metrics: region similarity (J ), contour accuracy (F), and their average (J &F). All evaluations are performed under the semi-supervised setting, where the ground-truth mask of the first frame is provided."
        },
        {
            "title": "5.2 Main results on SeCVOS",
            "content": "We present the performance comparison on the SeCVOS benchmark in Table 4. Our approach consistently outperforms prior art across various settings, including no scene transition, single-scene, and multi-scene scenarios. Notably, as the number of scene transitions increases, the performance gap between our method and prior approaches becomes larger. Even Cutie [6], which claims to leverage object-level representations for improved tracking, fails to maintain performance on SeCVOS. This aligns with our hypothesis that previous VOS methods largely rely on superficial object appearance cues and lack the capacity to form robust, concept-level understanding. This verifies our integration of LVLM-based concept reasoning into the segmentation pipeline enables the model to effectively distill object-level concepts across diverse and discontinuous scenes. 7 Table 5: Performance comparison with prior work on standard VOS benchmarks. &F Method STCN [9] SwinB-AOT [47] SwinB-DeAOT [48] RDE [24] XMem [8] SimVOS-B [42] JointFormer [50] ISVOS [39] DEVA [7] Cutie-base [6] Cutie-base+ [6] SAM 2.1 [32] SAMURAI [46] SAM2.1Long [12] SeC (Ours) SA-V val 61.0 51.1 61.4 51.8 60.1 44.2 - - 55.4 60.7 61.3 78.6 79.8 81.1 82.7 +4.1 SA-V test 62.5 50.3 61.8 53.9 62.3 44.1 - - 56.2 62.7 62.8 79.6 80.0 81.2 81.7 +2.1 LVOS v2 val 60.6 - 63.9 62.2 64.5 - - - - - - 84.1 84.2 85.9 86.5 +2. MOSE val 52.5 59.4 59.9 46.8 59.6 - - - 66.0 69.9 71.7 74.5 72.6 75.2 75.3 +0.8 DAVIS 2017 val YTVOS 2019 val 85.4 85.4 86.2 84.2 86.0 88.0 90.1 88.2 87.0 87.9 88.1 90.6 89.9 91.4 91.3 +0.7 82.7 84.5 86.1 81.9 85.6 84.2 87.4 86.3 85.4 87.0 87.5 88.7 88.3 88.7 88."
        },
        {
            "title": "5.3 Comparison on standard VOS benchmarks",
            "content": "We further compare SeC against state-of-the-art methods on standard video object segmentation (VOS) benchmarks. The comparison encompasses both traditional matching-based segmentation algorithms and recent SAM 2 and its variants. As reported in Table 5, SeC achieves competitive or superior performance across all benchmarks. Notably, our method achieves &F scores of 82.7 and 81.7 on the SA-V validation and test sets, respectively, and reaches 86.5 on LVOS v2. These results establish SeC as the new state-of-the-art across wide range of benchmarks, validating both the effectiveness and the versatility of our framework."
        },
        {
            "title": "5.4 Ablation study",
            "content": "We conduct series of ablation studies on the SA-V validation set and our proposed SeCVOS benchmark, with results presented in Table 6 and Table 7. Effectiveness of proposed modules. Table 6 presents an ablation study evaluating the contributions of the pixel-level association and concept guidance modules. Enabling only the pixel-level association leads to significant improvement on the SA-V benchmark and modest gain on SeCVOS, highlighting its effectiveness in capturing low-level visual patterns. particularly beneficial in the single-shot scenarios of SA-V. When the concept guidance module is further introduced, performance on SeCVOS improves by 7.8 points, demonstrating that concept-level reasoning is critical for handling the complex, multi-shot nature of SeCVOS, where simple pixel-level matching is insufficient. The marginal improvement on SA-V is expected, as this benchmark does not involve substantial semantic discontinuities or scene transitions that require high-level reasoning. Effectiveness of large vision-language model size. Table 7 analyzes the effect of varying model parameter scales. As the parameter count increases from 1B to 4B, the model performance consistently improves across the three main metrics on the SeCVOS benchmark. However, further scaling to 8B leads to marginal gains, with results nearly identical to those of the 4B model. This indicates that beyond certain scale, the benefits of increasing model size begin to saturate, and no longer translate into proportional performance improvements."
        },
        {
            "title": "5.5 Visualization",
            "content": "To more intuitively demonstrate the segmentation performance of our method, we conducted visual comparison between our approach and the SAM 2 baseline on the SeCVOS benchmark. 8 Table 6: Ablation studies on proposed modules. Table 7: Ablation studies on LVLM size. Pixel-level Association Guidance &F"
        },
        {
            "title": "Concept",
            "content": "SA-V SeCVOS 78.6 82.4 82.7 &F 58.2 62.2 70."
        },
        {
            "title": "LVLM Size",
            "content": "J &F 1B 2B 4B 8B 68.4 69.5 70.0 70.3 68.2 69.3 69.7 70.1 68.7 69.8 70.2 70.7 Figure 3: Qualitative comparison between SAM 2 and SeC (ours) on the SeCVOS benchmark."
        },
        {
            "title": "6 Conclusion",
            "content": "We present Segment Concept (SeC), novel concept-driven framework for Video Object Segmentation that moves beyond traditional appearance-based matching by leveraging high-level object-centric reasoning. By integrating the conceptual perception capabilities of Large Vision-Language Models (LVLMs), SeC constructs and updates robust semantic representations over time, enabling consistent tracking under challenging conditions such as dynamic scene transitions. To evaluate these capabilities, we introduce SeCVOS, new benchmark specifically designed to test semantic-level understanding in complex, multi-shot video scenarios. Extensive experiments show that SeC significantly outperforms existing state-of-the-art models, including SAM 2 and its variants, across both SeCVOS and standard benchmarks, while maintaining competitive efficiency. We hope SeC and SeCVOS will inspire further exploration of concept-level modeling for long-term and semantically grounded video understanding. Limitations. Despite its promising results, our work still leaves room for improvement. First, the current transition detection mechanism is lightweight and simple, but may fail in certain edge cases. more robust approach would involve learning dynamic indicator to decide when to invoke LVLM-based reasoning. Second, although SeCVOS introduces multi-shot complexity, its overall video length remains shorter than that of existing datasets like LVOS [20]. While SeCVOS already presents significant challenges for current methods, extending it with longer-duration videos would further evaluate the temporal reasoning capabilities of future models."
        },
        {
            "title": "A Supplementary Material Overview",
            "content": "This supplementary material provides further details about the SeCVOS benchmark, including dataset composition, annotation quality, and supported tasks. We also present qualitative results of our method, comparative visualizations, and analyze typical failure cases. Additionally, we discuss ethical considerations and the intended scope of use for SeCVOS."
        },
        {
            "title": "B Details of SeCVOS",
            "content": "The SeCVOS benchmark consists of diverse set of video sequences captured in various environments, including indoor, outdoor, and animated settings. These sequences feature broad range of tracking targets, such as humans, vehicles, and animals. The videos present significant challenges due to their complex scene transitions, rapid object movements, and environmental interferences, such as frequent occlusions and the appearance and disappearance of objects. To ensure high accuracy, each video has been meticulously reviewed and annotated with high-quality target masks, which track the shape and positional changes of the objects over time. Figure 4 displays these annotated masks within the video frames, providing an intuitive visual reference. This benchmark aims to push the limits of video object segmentation and tracking technologies, particularly in dealing with dynamic and complex scenes. Figure 4: Example video sequences from the SeCVOS benchmark with overlaid target masks. Each row corresponds to frames from single video sequence, illustrating the annotated object masks. 10 Figure 5: Example video sequences and corresponding referring expressions from the SeCVOS benchmark."
        },
        {
            "title": "C Referring Video Object Segmentation on SeCVOS",
            "content": "In addition to the semi-supervised Video Object Segmentation task, our proposed SeCVOS dataset also supports the Referring Video Object Segmentation task. In this task, we generate detailed descriptions for each object in the SeCVOS dataset. These descriptions are initially generated by the Gemini 2.5 Pro [37] and subsequently refined through rigorous manual verification and editing to ensure accuracy. Figure 5 depicts several data samples, and notably, in the presence of visually similar distractor objects, we provide additional fine-grained descriptions to support precise model discrimination of the target objects. Total Method Table 8: Performance comparison on Ref-SeCVOS. Under this setting, we evaluated several stateof-the-art RefVOS methods, including both LVLM based approaches and traditional temporal propagation baselines. As shown in Table 8, the performance of all methods on the SeCVOS benchmark remains limited. VISA [45] and GLUS-A [26] performed comparatively better, possibly because they were trained on datasets with more complex textual instructions, which helps with cross-modal reasoning and object discrimination. Overall, these results highlight the challenges of the SeCVOS benchmark in terms of scene complexity, fine-grained language descriptions, and visual discrimination, indicating that there is still significant room for improvement in RefVOS. LVLM Based Method VideoLISA [3] Sa2VA [49] GLUS-A [26] VISA [45] Propagation Based Method Grounded SAM 2 [33] SAMWISE [10] 3.8 43.7 8 51.5 7 59.7 7 60.4 400 48.4 210 54.1 42.8 51.8 59.8 59.5 41.8 52.0 60.0 58.6 SeCVOS-Ref 48.9 54.0 49.3 53.9 Params &F J"
        },
        {
            "title": "D Additional Qualitative Results",
            "content": "We present additional qualitative results in Figure 6 to further demonstrate the performance of our method across variety of challenging scenarios. Compared to the SAM 2, our SeC model consistently delivers reliable segmentation results by constructing well-formed concept representation, particularly in handling complex situations such as viewpoint changes, background interference, and object occlusion. 11 However, since this concept is learned from limited set of viewpoints, the models performance may decline in extreme cases where the current viewpoint differs significantly from those encountered during concept construction. For example, as shown in Figure 7, the interior view of the sailboat in the fifth image poses challenge. The drastic shift in perspective leads to failure in matching the frame to the learned concept, resulting in incorrect segmentation. Figure 6: Additional qualitative comparison between SAM 2 and SeC (ours) on the SeCVOS benchmark. 12 Figure 7: Failure case example from the SeCVOS benchmark."
        },
        {
            "title": "E Broader Impacts",
            "content": "The SeCVOS benchmark is constructed using only publicly available video data, which is used exclusively for academic research purposes. All annotation work was performed by volunteers who were fully informed about the nature of the project. No private, sensitive, or restricted data were used. The goal of our research is to support the development of technologies that can positively impact society, such as autonomous systems, assistive technologies, and tools for enhanced human-computer interaction. However, we acknowledge the potential risks associated with the misuse of segmentation technologies, including privacy concerns and unauthorized surveillance. We encourage the responsible use of our benchmark and methods, and explicitly discourage any applications that may infringe upon personal privacy or be deployed for harmful purposes. All annotations and experimental results presented in this work were generated solely for research purposes and adhere to ethical guidelines regarding the use of public visual data within the academic community."
        },
        {
            "title": "References",
            "content": "[1] Sirine Ammar, Thierry Bouwmans, Nizar Zaghden, and Mahmoud Neji. Moving objects segmentation based on deepsphere in video surveillance. In Advances in Visual Computing: 14th International Symposium on Visual Computing, ISVC 2019, Lake Tahoe, NV, USA, October 79, 2019, Proceedings, Part II 14, pages 307319. Springer, 2019. [2] Ali Athar, Jonathon Luiten, Alexander Hermans, Deva Ramanan, and Bastian Leibe. Hodor: High-level object descriptors for object re-segmentation in video learned from static images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30223031, 2022. [3] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. Advances in Neural Information Processing Systems, 37:68336859, 2024. [4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [5] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In CVPR, 2022. [6] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, and Alexander Schwing. Putting the object back into video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31513161, 2024. [7] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13161326, 2023. [8] Ho Kei Cheng and Alexander Schwing. Xmem: Long-term video object segmentation with an atkinsonshiffrin memory model. In European Conference on Computer Vision, pages 640658. Springer, 2022. [9] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethinking space-time networks with improved memory coverage for efficient video object segmentation. Advances in Neural Information Processing Systems, 34:1178111794, 2021. [10] Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, and Giuseppe Averta. Samwise: Infusing wisdom in sam2 for text-driven video segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 33953405, 2025. [11] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. Mose: new dataset for video object segmentation in complex scenes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2022420234, 2023. [12] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi Wang. Sam2long: Enhancing sam 2 for long video segmentation with training-free memory tree. arXiv preprint arXiv:2410.16268, 2024. [13] Shuangrui Ding, Rui Qian, Haohang Xu, Dahua Lin, and Hongkai Xiong. Betrayed by attention: simple yet effective approach for self-supervised video object segmentation. In European Conference on Computer Vision, pages 215233. Springer, 2024. [14] Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham Aarabi, and Graham Taylor. Sstvos: Sparse spatiotemporal transformers for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59125921, 2021. [15] Wei Fang, Lianyu Zheng, Huanjun Deng, and Hongbo Zhang. Real-time motion tracking for mobile augmented/virtual reality using adaptive visual-inertial fusion. Sensors, 17(5):1037, 2017. [16] Brent Griffin, Victoria Florence, and Jason Corso. Video object segmentation-based visual servo control and object depth estimation on mobile robot. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16471657, 2020. [17] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [18] Mingfei Han, Linjie Yang, Xiaojun Chang, Lina Yao, and Heng Wang. Shot2story: new benchmark for comprehensive understanding of multi-shot videos. In The Thirteenth International Conference on Learning Representations, 2025. 14 [19] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1348013492, 2023. [20] Lingyi Hong, Zhongying Liu, Wenchao Chen, Chenzhi Tan, Yuang Feng, Xinyu Zhou, Pinxue Guo, Jinglun Li, Zhaoyu Chen, Shuyong Gao, et al. Lvos: benchmark for large-scale long-term video object segmentation. arXiv preprint arXiv:2404.19326, 2024. [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [22] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [23] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James Rehg. Video segmentation by tracking many figure-ground segments. In Proceedings of the IEEE international conference on computer vision, pages 21922199, 2013. [24] Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan, and Dong Liu. Recurrent dynamic embedding for video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13321341, 2022. [25] Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:34303441, 2020. [26] Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into single large language model for video segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [27] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. IEEE transactions on pattern analysis and machine intelligence, 36(6):11871200, 2013. [28] Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and Seon Joo Kim. Fast video object segmentation by reference-guided mask propagation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 73767385, 2018. [29] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using spacetime memory networks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92269235, 2019. [30] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. [31] Rui Qian, Shuangrui Ding, Xian Liu, and Dahua Lin. Semantics meets temporal correspondence: Selfsupervised object-centric learning in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1667516687, 2023. [32] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations, 2025. [33] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. [34] Hongje Seong, Junhyuk Hyun, and Euntai Kim. Kernelized memory network for video object segmentation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXII 16, pages 629645. Springer, 2020. [35] Mennatullah Siam, Alex Kendall, and Martin Jagersand. Video class agnostic segmentation benchmark for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28252834, 2021. 15 [36] Hao Tang, Chenwei Xie, Haiyang Wang, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, and Liwei Wang. Ufo: unified approach to fine-grained visual perception via open-ended language interface. arXiv preprint arXiv:2503.01342, 2025. [37] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [38] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video object insertion with precise motion control. arXiv preprint arXiv:2501.01427, 2025. [39] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao, Yujia Xie, Lu Yuan, and Yu-Gang Jiang. Look before you match: Instance understanding matters in video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22682278, 2023. [40] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [42] Qiangqiang Wu, Tianyu Yang, Wei Wu, and Antoni Chan. Scalable video object segmentation with simplified framework. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1387913889, 2023. [43] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping Zhang, and Wenxiu Sun. Efficient regional memory network for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12861295, 2021. [44] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. [45] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In European Conference on Computer Vision, pages 98115. Springer, 2024. [46] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. [47] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object segmentation. Advances in Neural Information Processing Systems, 34:24912502, 2021. [48] Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. Advances in Neural Information Processing Systems, 35:3632436336, 2022. [49] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [50] Jiaming Zhang, Yutao Cui, Gangshan Wu, and Limin Wang. Jointformer: unified framework with joint modeling for video object segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 117, 2025. [51] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Internlm-xcomposer-2.5: versatile large vision language model Bin Wang, Linke Ouyang, et al. supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. [52] Junbao Zhou, Ziqi Pang, and Yu-Xiong Wang. Rmem: Restricted memory banks improve video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1860218611, 2024."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}