{
    "paper_title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment",
    "authors": [
        "Yuecheng Li",
        "Hengwei Ju",
        "Zeyu Song",
        "Wei Yang",
        "Chi Lu",
        "Peng Jiang",
        "Kun Gai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec."
        },
        {
            "title": "Start",
            "content": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment Hengwei Ju Fudan University Shanghai, China juhengwei@kuaishou.com Yuecheng Li Kuaishou Technology Beijing, China liyuecheng@kuaishou.com Zeyu Song Kuaishou Technology Beijing, China songzeyu@kuaishou.com 6 2 0 2 1 ] . [ 1 2 8 6 0 0 . 2 0 6 2 : r Wei Yang University of Southern California Los Angeles, USA weiyangvia@gmail.com Peng Jiang Kuaishou Technology Beijing, China jiangpeng@kuaishou.com Chi Lu Kuaishou Technology Beijing, China luchi@kuaishou.com Kun Gai Unaffiliated Beijing, China gai.kun@qq.com Abstract Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs) including large language models (LLMs), large vision models (LVMs), and multimodal large language models (MLLMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding, cross-modal knowledge fusion, and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distributionlevel semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Under Review, 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on large-scale online advertising platform confirms the models effectiveness and scalability in industrial recommendation scenarios. Our code is available at https://github.com/ 6lyc/RecGOAT-LLM4Rec. CCS Concepts Information systems Recommender systems; Multimedia and multimodal retrieval. Keywords Multimodal Recommendation, Large Language Models, Semantic Alignment, Graph Neural Networks, Optimal Transport ACM Reference Format: Yuecheng Li, Hengwei Ju, Zeyu Song, Wei Yang, Chi Lu, Peng Jiang, and Kun Gai. 2018. RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment. In Proceedings of (Under Review). ACM, New York, NY, USA, 11 pages. https://doi.org/ XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nRecommendation systems (RS) have been widely adopted as es-\nsential filtering mechanisms in the era of information overload\n[14, 21, 36, 40, 53]. However, the sparsity of explicit user-item in-\nteraction data severely constrains recommendation performance,\nparticularly in large-scale recommendation scenarios. To address\nthis, multimodal recommendation improves system performance by\nleveraging rich item content (such as textual descriptions and prod-\nuct images) to complement interaction signals, thereby alleviating\ndata sparsity, enabling more accurate personalized recommenda-\ntions, and enhancing the user experience [6, 42, 45, 46, 56].",
            "content": "Earlier studies primarily focused on employing convolutional neural networks (e.g., VGG, ResNet) and word embedding models (e.g., GloVe, BERT) to learn visual and textual modal information Under Review, , Yuecheng Li, Hengwei Ju, Zeyu Song, Wei Yang, Chi Lu, Peng Jiang, and Kun Gai To address the aforementioned challenges, this paper proposes dual semantic alignment multimodal recommendation framework via Graph Optimal Adaptive Transport (RecGOAT). For intramodal learning, we construct collaborative signal representation enhancement module based on multimodal attentive graphs. By building multiple graphs across text, image, and interaction with attention mechanisms, our approach strengthens multi-hop collaborative signals among item-item, user-item, and user-user, thereby capturing high-order structural relationships on both the user and item sides. To fully leverage the world knowledge and reasoning capabilities of large models, we employ Qwen3-Embedding-8B and LLaVA-1.5-7B to encode textual and visual features of items, respectively. Meanwhile, by constructing personalized behavioral prompts, we utilize QwQ-32B to infer each users multi-dimensional item preferences, which serve as initialized textual features for users. For cross-modal alignment, we design dual-granularity semantic alignment framework between LLM-enhanced modalities and recommendation IDs. First, we perform instance-level alignment via cross-modal contrastive learning across text, vision, and ID modalities, obtaining discriminative multimodal representations. Second, we introduce an optimal adaptive transport technique to achieve distribution-level alignment and representation fusion between semantic modalities and recommendation IDs. By minimizing the 1-Wasserstein distance between different modal distributions, their feature embeddings are transported via an optimal transport matrix into unified aligned space, yielding consistent and comprehensive fused item representations. Additionally, we incorporate adaptive learnable parameters into each transport matrix, which bridges OT alignment with the downstream recommendation task and enables precise supervision of the transport process. Notably, we provide theoretical proof that the mutual constraint between any single-source modal distribution and the unified fused distribution can be bounded by the Wasserstein distance and the InfoNCE loss. This demonstrates that the unified representations optimized in RecGOAT achieves strong alignment consistency and fusion comprehensiveness. The main contributions of this paper are summarized as follows: We propose RecGOAT, an LLM-enhanced multimodal recommendation framework that resolves the semantic heterogeneity between LLM-derived modality representations and ID-based collaborative signals. Our RecGOAT unifies structure-aware graph augmentation with dual-granularity alignment objective, consisting of instance-level cross-modal contrastive learning (CMCL) and distribution-level optimal adaptive transport (OAT). We theoretically establish that the unified representation achieves lower target error than any individual modality, with the error gap rigorously bounded by the Wasserstein distance and the InfoNCE loss, thereby offering guarantees of fusion comprehensiveness and alignment consistency. Extensive experiments on three public datasets and largescale online advertising platform demonstrate the effectiveness and scalability of RecGOAT. Ablations and analyses further confirm the necessity of OT-based distribution alignment and validate the claimed alignment consistency and fusion comprehensiveness. Figure 1: Performance comparison between LM representations with or without alignment for recommendation systems on Baby Dataset. (A) Due to semantic heterogeneity, LM Representation w/o alignment leads to degradation in recommendation performance. (B) Through our dual-granularity alignment, the semantic conflict is resolved, yielding performance improvements of 59% and 70%, respectively. [8, 12, 29]. These features were then fused with ID features via weighting, concatenation, or element-wise multiplication [8, 25]. To better capture higher-order interactions, many researchers have introduced graph neural networks (GNN) into multimodal recommendation, constructing user-item and item-item graphs to learn complex structural relations [1, 10]. However, due to constraints in network architecture and depth, the feature extraction capabilities of these models remain limited, and they lack sufficient semantic understanding of users and items. For example, they struggle to effectively initialize user ID embeddings, typically relying on random initialization [13, 55] or aggregating modal features from historically interacted items [20]. In practice, this means they fail to accurately depict user characteristics and genuine preferences, ultimately depending on intricate behavior modeling and supervised learning in downstream tasks. Recently, several new architectures have been introduced into multimodal recommendation to address the challenge of deep semantic understanding, such as Transformer-based modal fusion frameworks [17, 48], Diffusion Model-based modal denoising and generative models [15, 18, 47], and Mamba-based efficient sequential modeling methods [35]. With the scaling law being successfully validated across various fields, the latest research has further attempted to integrate LLMs, LVMs, and MLLMs with recommendation systems [2, 38, 49], aiming to leverage their rich world knowledge to enhance semantic representations of each modality. However, current LM-enhanced multimodal recommendation models still exhibit noticeable deficiencies in aligning modal signals with interaction IDs [2, 37]. We observe that there exists significant semantic heterogeneity between the world knowledge encapsulated in generative large models and the ID signals relied upon for user-item interaction modeling, as illustrated in Figure 1. This heterogeneity hinders existing methods from fully unleashing the potential of large models in recommendation tasks. Therefore, achieving thorough and consistent alignment between the large models and recommendation systems has become key to pushing the performance ceiling of current recommendation systems."
        },
        {
            "title": "RecGOAT",
            "content": "Under Review, ,"
        },
        {
            "title": "2 Related Work\n2.1 Multimodal Recommendation\nMultimodal recommendation address the challenge of sparse user-item\ninteraction by extracting and integrating rich content features, such\nas textual, visual, and acoustic information. VBPR [12] first ex-\ntracted visual features via CNN and fused them with ID features\nthrough a weighted loss function. Given the strong capability of\nGNNs in capturing high-order structural relations, MMGCN [39]\nconstructs user-item bipartite graphs per modality and performs\nmulti-level message propagation. Further, LightGCN [13] simpli-\nfies graph convolution for collaborative filtering by retaining only\nthe essential neighbor aggregation component. Moreover, LAT-\nTICE [51] and FREEDOM [55] construct dynamic/frozen item-item\ngraphs to further explore the potential of graph learning.",
            "content": "In recent years, inspired by the success of novel architectures such as Transformer [33] and Mamba [11] in various fields, researchers have also introduced them into multimodal recommendation to enhance the representation of user preferences and item features. RecFormer [17] discards item ID dependency and addresses cold-start and cross-domain transfer problems with bidirectional Transformer. UGT [48] strengthens modality alignment and fusion through an end-to-end architecture combining multi-way Transformer and unified GNN. Furthermore, the Diffusion Model paradigm shifts recommendation from classification to generation: DreamRec [47] generates oracle items via guided diffusion, avoiding negative sampling to eliminate noise interference; DiffMM [15] extends multimodal adaptation by generating modality-aware interaction graphs and incorporating cross-modal contrastive learning. To improve model efficiency, FindRec [35] employs linear-complexity Mamba layers to model long-range sequential dependencies, integrating Stein kernel distribution alignment with cross-modal expert routing. Despite notable progress in feature representation, scenario adaptation, and efficiency optimization achieved by the above multimodal recommendation methods, their core limitation lies in their lack of the model scale required for deep semantic abstraction and reasoning."
        },
        {
            "title": "2.2 LM-enhanced Recommendation\nLarge models (LMs), empowered by their strong semantic under-\nstanding, cross-modal integration, and knowledge transfer capabili-\nties, have effectively compensated for key limitations of traditional\nrecommendation systems, including inefficient modal fusion and\ninsufficient fine-grained preference modeling. They have thus grad-\nually become a core driving force in advancing multimodal recom-\nmendation [24]. TALLRec [2] proposed an efficient two-stage tun-\ning framework, offering a foundational solution for adapting LLMs\nto recommendation scenarios. Rec-GPT4V [23] designed a Visual-\nSummary Thought strategy to convert item images into structured\ntextual summaries. To address the issue that LLMs tend to over-\nlook visual information in end-to-end fine-tuning, NoteLLM-2 [50]\nintroduced multimodal in-context learning, contrastive learning,\nand a late-fusion mechanism to balance attention across modalities.\nDiffering from a single-task focus, UniMP [38] constructed a unified\nmultimodal personalization framework, integrating heterogeneous\ninformation through a unified data format and realizing multimodal",
            "content": "alignment and fusion via cross-layer cross-attention. IRLLRec [37] focused on intent representation learning, employing dual-tower alignment and momentum distillation to align textual intents with interaction intents. While the aforementioned LM-enhanced recommendation methods have demonstrated strong performance, their alignment mechanisms remain largely confined to instance-level or pair-wise local alignment, without considering global distribution-level alignment. As result, these models struggle to capture global patterns across multimodal, cross-domain, or interactive data."
        },
        {
            "title": "Augmentation and Graph Learning",
            "content": "For each modality, we introduce large models (including LLMs and LVLMs) for enhancement and extract high-order collaborative information by constructing attentive graphs. Under Review, , Yuecheng Li, Hengwei Ju, Zeyu Song, Wei Yang, Chi Lu, Peng Jiang, and Kun Gai Figure 2: The overall framework of our RecGOAT. Item-Item Multimodal Graph Representation Learning. To en3.1.1 hance high-order collaborative relationships between items, we construct separate textual and visual modality graphs Gğ‘š = {I, Eğ‘š, Xğ‘š }, where ğ‘š {ğ‘¡, ğ‘£ }. Inspired by FREEDOM [55], we adopt the K-nearest neighbors (KNN) algorithm to build frozen item-item graphs based on the initial LM-enhanced modal features Xğ‘š = {ğ’™ğ’ ğ‘– = ğ’Š 1, 2, . . . , I}. The LM-enhanced modal features Xğ‘š are obtained by processing raw item content with pretrained large models. Specifically, for an item ğ‘–, its raw textual description ğ‘¡ğ‘– and visual image ğ‘£ğ‘– are encoded separately: ğ’Š = ğ‘“ğ‘¡ (ğ‘¡ğ‘– ğœƒğ‘¡ ), ğ’™ ğ’• ğ’Š = ğ‘“ğ‘£ (ğ‘£ğ‘– ğœƒğ‘£), ğ’™ ğ’— (1) where ğ‘“ğ‘¡ and ğ‘“ğ‘£ represent the pretrained LLM (e.g., Qwen-Embedding [52]) and LVLM (e.g., LLaVA [22]) parameterized by ğœƒğ‘¡ and ğœƒğ‘£, respectively. For two items ğ‘–, ğ‘— Iğ‘š, the edge weight in the graph is computed via cosine similarity as follows: ğ‘ ğ‘š . ğ‘– ğ‘— = ğ’™ğ’ ğ’Š ğ’™ğ’ ğ’Š ğ’™ğ’ ğ’‹ ğ’™ğ’ ğ’‹ To sparsify the modality graph, we retain only the top-ğ¾ edges with the highest similarity for each item node: Eğ‘š = (cid:8)ğ‘’ğ‘š ğ‘– ğ‘— ğ‘–, ğ‘— Iğ‘š (cid:9), ğ‘’ğ‘š ğ‘– ğ‘— = (cid:40)1, 0, ğ‘– ğ‘— top-K({ğ‘ ğ‘š if ğ‘ ğ‘š otherwise, ğ‘–ğ‘˜ ğ‘˜ ğ‘–}), (2) ğ‘– ğ‘— = 1 indicates the presence of an association edge between where ğ‘’ğ‘š the two items. After obtaining the graph Gğ‘š for each modality, we employ : Graph Attention Network [34] to learn node representations ğ’›ğ’ ğ’Š ğ’›ğ’ ğ’Š = ğ» (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) â„= ğ‘— Nğ‘š ğ‘– ğœ (cid:169) (cid:173) (cid:171) ğ›¼ğ‘š,â„ â„ ğ‘– ğ‘— ğ’™ğ’ ğ’‹ Rğ‘‘, (cid:170) (cid:174) (cid:172) (3) where ğ›¼ğ‘š,â„ is the normalized attention weight between node ğ‘– and ğ‘– ğ‘— its neighbor ğ‘— Nğ‘š ğ‘– computed by the â„-th attention head, Wâ„ is the transformation matrix corresponding to the â„-th head. Ultimately, the representation of each item under modality ğ‘š is enhanced by aggregating neighborhood information through multi-head attention, thereby more effectively capturing high-order collaborative signals within the modality. ID Embedding Learning from User-Item Interaction Graph. To 3.1.2 capture collaborative filtering signals from implicit feedback, we construct user-item interaction graph Gğ‘¢ğ‘– = (U I, Eğ‘¢ğ‘– ), where and represent the sets of users and items, respectively. An edge ğ‘’ğ‘¢ğ‘– Eğ‘¢ğ‘– exists if user ğ‘¢ has interacted with item ğ‘–. The ID embeddings for users and items are initialized as learnable parameters, denoted as E(0) ğ‘¢ ğ‘‘ and E(0) ğ‘‘ , where ğ‘‘ is the embedding dimension. Following the lightweight design of LightGCN [13], the propagation rule at the ğ‘™-th layer is defined as: ğ‘– E(ğ‘™+1) ğ‘¢ = ğ‘– Nğ‘¢ ğ‘Ÿğ‘¢ğ‘– Nğ‘¢ Nğ‘– E(ğ‘™ ) ğ‘– , E(ğ‘™+1) ğ‘– = ğ‘Ÿğ‘¢ğ‘– Nğ‘¢ Nğ‘– E(ğ‘™ ) ğ‘¢ , ğ‘¢ Nğ‘– (4) where Nğ‘¢ and Nğ‘– are the neighbor sets of user ğ‘¢ and item ğ‘– in Gğ‘¢ğ‘– , respectively. To incorporate explicit preference signals available in certain datasets (e.g., review ratings in Amazon datasets), we introduce the rating value ğ‘Ÿğ‘¢ğ‘– as attention coefficient between user ğ‘¢ and item ğ‘–. ğ’› ğ’Šğ’… ğ’– = 1 ğ¿ + 1 ğ¿ ğ‘™= E(ğ‘™ ) ğ‘¢ , ğ’› ğ’Šğ’… ğ’Š = 1 ğ¿ + 1 ğ¿ ğ‘™=0 E(ğ‘™ ) ğ‘– . (5)"
        },
        {
            "title": "RecGOAT",
            "content": "Under Review, , These refined ID embeddings encode high-order collaborative relations and are subsequently used for cross-modal alignment with LLM semantics."
        },
        {
            "title": "3.1.3 User-User Graph Learning with LLM Contextual Enhancement.\nTraditional ID-based recommendation models, compared to LLMs,\nlack persistent world knowledge and operate on coarse-grained\nIDs, which limits their generalization ability and understanding of\nvarious items. To fully leverage the world knowledge and reasoning\ncapabilities of LLMs, we construct personalized behavioral prompts\nfor each user based on their interaction history within graph Gğ‘¢ğ‘–\nand the corresponding textual descriptions of interacted items. The\nprompt template is designed as follows:",
            "content": "Prompt: User Preference Reasoning You are professional data analyst. Your task is to analyze users interaction history to infer their preferences. - User ID: {user id} - Interaction History: {item ids & text description} Please conduct structured reasoning by two steps: - Identify Common Attributes Across Items: ... ... - Summarize Preferences Across Multiple Dimensions: ... ... Output Format: <think> reasoning process here </think> <answer> answer here </answer> Next, the open-source QwQ-32B [43] is employed to infer user preferences based on this prompt. The generated answer within the <answer>...</answer> are encoded into embeddings that serve as the users textual modal features. Let Hğ‘¢ = {(ğ‘–, ğ‘¡ğ‘– ) ğ‘– Nğ‘¢ } denote the set of interacted items and their textual descriptions for user ğ‘¢, and let represent the structured prompt template. The LLM-based preference reasoning and embedding generation are modeled as: ğ‘ğ‘¢ = ğ‘“ğ‘¢ (Hğ‘¢ P, ğœƒğ‘¢ ), (6) where ğ‘ğ‘¢ is the structured textual answer generated by the LLM parameterized by ğœƒğ‘¢ , and ğ‘¥ğ‘¡ ğ‘¢ denotes the resulting textual modal feature obtained via the text encoder parameterized by ğœƒğ‘¡ . ğ’– = ğ‘“ğ‘¡ (ğ‘ğ‘¢ ğœƒğ‘¡ ), ğ’™ ğ’• ğ‘¢ = {ğ’™ ğ’• ğ‘¢ğ‘¢ = (U, Eğ‘¡ ğ’– }, and edges Eğ‘¡ Subsequently, user-user textual modal graph Gğ‘¡ ğ‘¢ğ‘¢, Xğ‘¡ ğ‘¢ ) is constructed, where nodes represent users, features are the LLMenhanced embeddings Xğ‘¡ ğ‘¢ğ‘¢ are established based on the cosine similarity between user features, sparsified by retaining only the top-K connections for each node. Graph learning is then performed on Gğ‘¡ ğ‘¢ğ‘¢ following the same multi-head graph attention network described in Section 3.1.1. This allows the propagation and refinement of high-level, semantically enriched preferences among similar users, and the final refined user representation ğ’›ğ’• ğ’– aggregates contextual signals from peers with semantically aligned preferences."
        },
        {
            "title": "3.2 Cross-modal: Dual-Granularity Alignment\nof LLM-enhanced Modalities and ID Signals\nTo fully align LLM-enhanced semantic representations with rec-\nommendation ID signals, we propose a dual-granularity align-\nment framework, consisting of instance-level alignment based on",
            "content": "cross-modal contrastive learning and distribution-level alignment based on optimal adaptive transport. These two components interact with and reinforce each other, jointly optimizing towards consistent and comprehensive item embeddings that organically integrate LLM semantics with interaction signals, thereby unleashing the full potential of LLM-enhanced multimodal recommendation."
        },
        {
            "title": "3.2.1\nInstance-level Alignment via Cross-Modal Contrastive Learning\n( CMCL). To achieve fine-grained semantic alignment at the in-\nstance level, we perform cross-modal contrastive learning that\nexplicitly narrows the representation gap between different modal-\nities of the same item while pushing apart those of different items.\nFor each item ğ‘–, we sample paired representations from its available\nmodalities: (ID, text), (ID, visual), and (text, visual). Each pair is\ntreated as a positive example, while representations from different\nitems within the same modality are considered negatives.",
            "content": "The contrastive objective is built upon the InfoNCE loss [5], which encourages the similarity between positive pairs to be higher than that between negative pairs. Formally, for given anchor representation ğ’›ğ’ğ’‚ from modality ğ‘šğ‘ and its positive counterpart ğ’ğ’‘ from modality ğ‘šğ‘ , the contrastive loss for this pair is defined ğ’› ğ’Š as: ğ’Š (ğ‘šğ‘,ğ‘šğ‘ ) ğ‘– = log exp (cid:0)sim(ğ’›ğ’ğ’‚ , ğ’› (cid:205)ğ‘— exp (cid:0)sim(ğ’›ğ’ğ’‚ ğ’Š ğ’ğ’‘ ğ’Š , ğ’› ğ’Š )/ğœ (cid:1) ğ’ğ’‘ ğ’‹ )/ğœ (cid:1) , (7) where sim(, ) denotes cosine similarity, ğœ is temperature hyperparameter, and is the set of all items in the current batch that provide negative samples. The overall instance-level cross-modal contrastive learning loss aggregates over all three modality pairs: LCMCL = 1 ğµ ğµ ğ‘–=1 (ğ‘šğ‘,ğ‘šğ‘ ) (ğ‘šğ‘,ğ‘šğ‘ ) ğ‘– , (8) with MP = {(ğ‘–ğ‘‘, ğ‘¡), (ğ‘–ğ‘‘, ğ‘£), (ğ‘¡, ğ‘£)}. The representations refined through this contrastive learning process are not only semantically discriminative within and across modalities, but also provide meaningful similarity structure that reflects genuine semantic relatedness [19]. In the next section, these aligned representations are used to compute the cost matrix in the Optimal Transport problem. This ensures that the transport cost between two items captures their deep semantic discrepancy, thereby guiding the OT to perform semantically-aware distribution alignment rather than relying solely on raw feature distances."
        },
        {
            "title": "3.2.2 Distribution-level Alignment via Optimal Adaptive Transport\n( OAT). Semantic heterogeneity at the distribution level undermines\nthe efficacy of large models in multimodal recommendation and\ncaps the performance ceiling of existing systems. To achieve prin-\ncipled alignment between LLM-enhanced modal representations\nand recommendation ID embeddings, we formulate the seman-\ntic alignment process as an Optimal Transport (OT) problem [28].\nSpecifically, we aim to transport the LLM-augmented semantic\nfeature distribution (source) to match the collaborative ID feature\ndistribution (target), which naturally quantifies and minimizes the\ndistributional divergence between heterogeneous semantic spaces.",
            "content": "Under Review, , Yuecheng Li, Hengwei Ju, Zeyu Song, Wei Yang, Chi Lu, Peng Jiang, and Kun Gai Formally, let ğ‘ƒğ‘š denote the empirical distribution of the LLMenhanced modality ğ‘š (where ğ‘š {ğ‘¡, ğ‘£ }), and ğ‘„ğ‘–ğ‘‘ denote the empirical distribution of the ID embeddings obtained from Section 3.1.2. The OT problem seeks coupling ğ‘¡ that minimizes the total cost of moving mass from ğ‘ƒğ‘š to ğ‘„ğ‘–ğ‘‘ . In its continuous form, this is expressed as: ğ‘‚ğ‘‡ (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) = inf ğ‘¡ Î  (ğ‘ƒğ‘š,ğ‘„ğ‘–ğ‘‘ ) ğ‘ (ğ’›ğ’, ğ’› ğ’Šğ’… ) ğ‘‘ ğ‘¡ (ğ’›ğ’, ğ’› ğ’Šğ’… ), (9) Zğ‘š Zğ‘–ğ‘‘ where Î (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) is the set of all joint distributions with marginals ğ‘ƒğ‘š and ğ‘„ğ‘–ğ‘‘ , and ğ‘ : Zğ‘š Zğ‘–ğ‘‘ R+ is cost function measuring the semantic dissimilarity between source feature ğ’›ğ’ and target feature ğ’› ğ’Šğ’… . To concretely quantify the gap between LLM semantics and IDbased collaborative signals, we define the feature-wise cost as the normalized ğ¿1 distance between feature distributions. For batch of ğµ samples, let ğ’ ğ’ Rğµğ‘‘ represent the LLM-enhanced features from modality ğ‘š and ğ’ ğ’Šğ’… Rğµğ‘‘ represent the ID embeddings. The cost matrix ğ‘ª ğ’ Rğ‘‘ ğ‘‘ is computed as: ğ‘š ğ‘– ğ‘— = ğ‘  1 ğµ ğµ ğ‘=1 (cid:13) (cid:13)ğ’ ğ’ (cid:13) ğ’ƒ,ğ’Š ğ’ ğ’Šğ’… ğ’ƒ,ğ’‹ (cid:13) (cid:13) (cid:13)1 , (10) Let ğ‘ƒğ‘š = 1 ğµ where ğ‘  is scaling factor used to adjust the scale of the cost matrix and ensure numerical stability. (cid:205)ğµ , where ğ›¿ () is the Dirac delta function. Then, the discrete OT problem then reduces to minimizing the 1-Wasserstein distance W1 between the two distributions: , and ğ‘„ğ‘–ğ‘‘ = 1 ğµ ğ‘–=1 ğ›¿ğ’›ğ’ ğ‘–=1 ğ›¿ (cid:205)ğµ ğ’› ğ’Šğ’… ğ’Š ğ’Š W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) = min ğ‘» Î  (ğ’‘,ğ’’) ğ‘», ğ‘ª ğ‘šğ¹ = min ğ‘» Î  (ğ’‘,ğ’’) ğ‘‘ ğ‘‘ ğ‘–=1 ğ‘—=1 ğ‘‡ğ‘– ğ‘—ğ¶ğ‘š ğ‘– ğ‘— , (11) subject to ğ‘» 1 = ğ’‘, ğ‘» 1 = ğ’’, where 1 is the all-one vector. Here, ğ‘» Rğ‘‘ ğ‘‘ is the transport plan matrix, ğ’‘ and ğ’’ are uniform weight vectors, and , ğ¹ denotes the Frobenius inner product. We solve this entropy-regularized OT problem efficiently using the Sinkhorn-Knopp algorithm [9, 32], which iteratively updates row and column scaling vectors to converge linearly to an approximate optimal transport plan Tğ‘š 0 for each modality ğ‘š. To enable the OT alignment to adapt to downstream recommendation tasks, we augment the base OT plan with learnable residual matrix (cid:101)Tğ‘š. The final adaptive transport optimal matrix for modality ğ‘š is: ğ‘š = ğ‘» ğ‘» ğ‘š 0 + (cid:101)ğ‘» ğ‘š. (12) This allows the model to fine-tune the purely geometry-driven coupling ğ‘»ğ‘š 0 with task-specific semantic corrections. Using Tğ‘š, each LLM-enhanced modal feature is transported toward the ID embedding space via: Ë†ğ’ ğ’ = ğ’ ğ’ ğ‘» ğ’. Based on the distribution-level alignment process described above, we have effectively mitigated the semantic heterogeneity between the LLM-enhanced modal spaces and the ID-based collaborative space. This yields three semantically aligned item representations: Ë†Zğ‘¡ , Ë†Zğ‘£, and Zğ‘–ğ‘‘ . While each of these representations captures consistent semantic information from its respective (13) space, comprehensive item embedding must integrate complementary cues from all available modalities. To this end, we fuse the three aligned representations via weighted averaging: ğ’ = ğ›¾ğ‘¡ Ë†ğ’ ğ‘¡ + ğ›¾ğ‘£ Ë†ğ’ ğ‘£ + (cid:0)1 ğ›¾ğ‘¡ ğ›¾ğ‘£ (cid:1) ğ’ ğ‘–ğ‘‘, (14) where ğ›¾ğ‘¡, ğ›¾ğ‘£ [0, 1] are hyperparameters, which determine the relative contribution of each aligned modality to the final unified representation. The unified representation thus embodies both semantic consistency, inherited from the distribution-aligned features, and informational comprehensiveness, achieved by combining multimodal and collaborative views. It serves as the final item embedding for downstream preference prediction, seamlessly bridging the rich, world-aware semantics from large models with the interaction-driven relational knowledge captured by GNN-based ID representations."
        },
        {
            "title": "3.3 Theoretical Guarantees for Alignment",
            "content": "Consistency and Fusion Comprehensiveness To rigorously analyze the performance of RecGOAT, we provide mathematical proofs for its alignment consistency and fusion comprehensiveness, which are promoted by the joint optimization of instance-level and distribution-level alignment losses. The analysis focuses on the item side while treating user embeddings as fixed, under well-defined and realistic assumptions [7]."
        },
        {
            "title": "3.3.1 Problem Setup and Assumptions. Let ğ‘¼ be the set of fixed\nuser embeddings. For any ğ’– âˆˆ ğ‘¼ , we assume âˆ¥ğ’– âˆ¥ â‰¤ ğ¾ for a constant\nğ¾ > 0. The true preference function is denoted by ğ‘“ âˆ— (ğ’–, ğ’—), which\nmaps a user ğ’– and an item ğ’— to a real-valued score. Our modelâ€™s\nrating function is the inner product ğ‘“ (ğ’–, ğ’›) = ğ’–âŠ¤ğ’›, where ğ’› is an\nitem representation. Let ğ‘ ğ¹ be the distribution of the final unified\nitem representations. We define the modality-specific error and the\nunified representation error as:\nğœ–ğ‘š (ğ‘“ ) = Eğ‘§âˆ¼ğ‘ƒğ‘š (cid:2)|ğ‘“ (ğ‘¢, ğ‘§)âˆ’ğ‘“ âˆ— (ğ‘¢, ğ‘£)|(cid:3), ğœ–ğ¹ (ğ‘“ ) = Eğ‘§âˆ¼ğ‘ ğ¹ (cid:2)|ğ‘“ (ğ‘¢, ğ‘§)âˆ’ğ‘“ âˆ— (ğ‘¢, ğ‘£)|(cid:3) .\nTo facilitate the derivation, we adopt the following reasonable",
            "content": "assumptions [3]. Assumption 3.1 (Bounded User Embeddings). All user embeddings are fixed and bounded, i.e., ğ’– ğ¾. Consequently, for fixed ğ’–, the scoring function ğ‘“ (ğ’–, ğ’›) = ğ’–ğ’› is ğ¾-Lipschitz continuous w.r.t ğ’›: ğ‘“ (ğ’–, ğ’›1) ğ‘“ (ğ’–, ğ’›2) ğ¾ ğ’›1 ğ’›2 . This follows directly from the Cauchy-Schwarz inequality: ğ‘¢ (ğ’›1 ğ’›2) ğ’– ğ’›1 ğ’›2 ğ¾ ğ’›1 ğ’›2 . Assumption 3.2 (Lipschitz Continuity of True Preference). The true preference function ğ‘“ (ğ’–, ğ’—) is ğ¿-Lipschitz continuous with respect to the item representation ğ’›: ğ‘“ (ğ’–, ğ’›1)ğ‘“ (ğ’–, ğ’›2) ğ¿ ğ’›1 ğ’›2 . This reflects the inherent smoothness of user preferences. Supporting Lemmas and Main Theorem. Building upon the 3.3.2 preceding definitions and assumptions, two key lemmas are introduced, which subsequently lead to the theorem and proof concerning alignment consistency and fusion comprehensiveness. Lemma 3.3 (Instance-level Distance Bound). Let ğ’›ğ’ and ğ’›ğ’Š be ğ’Š the ğ¿2-normalized representations for modality ğ‘š and the unified"
        },
        {
            "title": "RecGOAT",
            "content": "Under Review, , representation for item ğ‘– (ğ‘–.ğ‘’., ğ’›ğ’ expected pairwise distance is bounded by the contrastive loss: ğ’Š = 1, ğ’›ğ’Š = 1), respectively. The Eğ‘–(cid:2)ğ‘§ğ‘š ğ‘– ğ‘§ğ‘– (cid:3) 2ğœ LCMCL + 2ğœ log ğµ, (15) where LCMCL is the InfoNCE-based cross-modal contrastive loss, ğœ is the temperature parameter, and ğµ is the batch size. Lemma 3.4 (Modality-to-Unified Error Bound). For any modality ğ‘š and any fixed user ğ’–, the difference between the modality-specific error and the unified error is bounded by both distributional and instance-level alignment terms: ğœ–ğ‘š (ğ‘“ ) ğœ–ğ¹ (ğ‘“ ) (ğ¾ + ğ¿) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) + (ğ¾ + ğ¿) Eğ‘– ğ’›ğ’ ğ’Š ğ’›ğ’Š , (16) where W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) is the 1-Wasserstein distance between the modality distribution ğ‘ƒğ‘š and the ID-based target distribution ğ‘„ğ‘–ğ‘‘ . The right-hand side of the inequality is bounded by the distributionlevel Wasserstein distance and the instance-level Euclidean distance. This lemma formally justifies the rationality of our dual-granularity semantic alignment design. Next, we give the Theorem 3.5 and its proof. Theorem 3.5 (Alignment Consistency and Fusion Comprehensiveness of RecGOAT). For any fixed user embedding ğ’– and for all modalities ğ‘š = {ğ‘¡, ğ‘£, id}, the following guarantees hold: (1) Consistency Guarantee: ğœ–ğ‘š (ğ‘“ ) ğœ–ğ¹ (ğ‘“ ) (ğ¾ + ğ¿) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) max ğ‘š + 2ğœ (ğ¾ + ğ¿)2 (LCMCL + log ğµ). (2) Comprehensiveness Guarantee: ğœ–ğ¹ (ğ‘“ ) min ğ‘š (cid:110) ğœ–ğ‘š (ğ‘“ ) + (ğ¾ + ğ¿) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) + 2ğœ (ğ¾ + ğ¿)2 (LCMCL + log ğµ) (cid:111) . (17) (18) Based on Theorem 3.5, Eq. (17) indicates that by optimizing the Wasserstein distance W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ), while reducing the contrastive learning loss LCMCL, the recommendation consistency between the modal representations with the largest error and the unified representation can be better aligned. On the other hand, Eq.(18) shows that the error of the fused unified representation does not exceed the error of any single modality plus the dual-granularity alignment error. That is, through OT-based distribution alignment and cross-modal contrastive learning, the fused representation can effectively integrate multimodal information and enhance recommendation performance. This theorem provides theoretical assurance and principled foundation for our RecGOAT: through dual-granularity alignment (instance-level contrastive learning + distribution-level OT mapping), the model successfully bridges the semantic gap between LLM-enhanced modalities and ID-based interaction signals to achieve both consistent and comprehensive multimodal fusion."
        },
        {
            "title": "3.4 Preference Optimization for Recommender\nIn summary, we optimize the downstream recommendation task\nusing the Bayesian Personalized Ranking (BPR) loss [30]. The fused\nuser representation ğ‘¼ is obtained by weighting the user ID em-\nfrom Section 3.1.2 and the enhanced textual user rep-\nbedding ğ’› ğ’Šğ’…\nğ’–\nresentation ğ’›ğ’•\nfrom Section 3.1.3. Together with the unified item\nğ’–\nrepresentation ğ’ from Eq. (14), the model is optimized with the\nBPR loss as follows:",
            "content": "LBPR = (ğ‘¢,ğ‘–,ğ‘— ) ln ğœ (ğ‘“ (ğ’–, ğ’›ğ’Š) ğ‘“ (ğ’–, ğ’›ğ’‹)), (19) where denotes the set of observed (ğ‘¢ğ‘ ğ‘’ğ‘Ÿ, ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘–ğ‘¡ğ‘’ğ‘š, ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘–ğ‘¡ğ‘’ğ‘š) triplets, ğœ is the sigmoid function, and the scoring function is defined as ğ‘“ (ğ‘¢, ğ‘§) = ğ‘¢ğ‘§. Items with higher predicted scores are ranked as high-potential candidates for recommendation. Proof. Starting from Lemma 3.4, we have for any modality ğ‘š: ğœ–ğ‘š (ğ‘“ ) ğœ–ğ¹ (ğ‘“ ) (ğ¾ + ğ¿) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) + (ğ¾ + ğ¿) Eğ‘– ğ’›ğ’ ğ’Š ğ’›ğ’Š . Applying the bound from Lemma 3.3 to the instance-level term yields: ğœ–ğ‘š (ğ‘“ ) ğœ–ğ¹ (ğ‘“ )+(ğ¾+ğ¿)W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ )+(ğ¾+ğ¿)2ğœ (LCMCL + log ğµ). Since this inequality holds for all ğ‘š M, taking the maximum over modalities on the left side yields the Consistency Guarantee: ğœ–ğ‘š (ğ‘¢ ) ğœ–ğ¹ (ğ‘¢ ) + (ğ¾ +ğ¿ ) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) +2ğœ (ğ¾ + ğ¿ ) 2 ( LCMCL + log ğµ). max ğ‘šğ‘€ Similarly, from Lemma 3.4 we also have: ğœ–ğ¹ (ğ‘“ ) ğœ–ğ‘š (ğ‘“ ) (ğ¾ + ğ¿ ) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) + (ğ¾ + ğ¿ ) Eğ‘– ğ’›ğ’ ğ’Š ğ’›ğ’Š . Applying the same substitution from Lemma 3.3 gives: ğœ–ğ¹ (ğ‘¢ ) ğœ–ğ‘š (ğ‘¢ ) + (ğ¾ + ğ¿ ) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) + (ğ¾ + ğ¿ ) 2ğœ ( LCMCL + log ğµ). As this is valid for all ğ‘š ğ‘€, the tightest bound is achieved by taking the minimum over modalities on the right-hand side, resulting in the Comprehensiveness Guarantee: ğœ–ğ¹ (ğ‘¢ ) min ğ‘šğ‘€ (cid:110) ğœ–ğ‘š (ğ‘¢ ) + (ğ¾ + ğ¿ ) W1 (ğ‘ƒğ‘š, ğ‘„ğ‘–ğ‘‘ ) + 2ğœ (ğ¾ + ğ¿ ) 2 ( LCMCL + log ğµ) (cid:111) ."
        },
        {
            "title": "4 Experiments\nIn this section, we conduct extensive experiments on three public\nAmazon datasets and a large-scale online advertising platform to\naddress the following key research questions:",
            "content": "RQ1: Does our RecGOAT achieve SOTA performance compared to classical recommendation methods as well as leading multimodal and large model-based approaches? RQ2: What is the negative impact of semantic conflict between LLM-enhanced modalities and ID signals? What are the individual and combined contributions of Cross-Modal Contrastive Learning (CMCL) and Optimal Adaptive Transport (OAT) in resolving semantic heterogeneity and improving recommendation performance? RQ3: How does our RecGOAT demonstrate alignment consistency and fusion comprehensiveness? RQ4: How effective are the key modules of RecGOAT in improving performance for an industrial-level online advertising system? Under Review, , Yuecheng Li, Hengwei Ju, Zeyu Song, Wei Yang, Chi Lu, Peng Jiang, and Kun Gai Table 1: Recommendation performance on three amazon datasets. Here, R@10 and N@10 denote Recall@10 and NDCG@10, respectively. The best results are highlighted in bold, and the second-best are underlined. The asterisk* indicates that the improvement of our RecGOAT is statistically significant based on t-test with ğ‘-value < 0.001. Our model achieves statistically significant state-of-the-art performance on all metrics across each dataset. ID-based Methods Multimodal Methods Large Models-based Methods Ours Dataset Metric BPR LightGCN VBPR FREEDOM DiffMM UGT FindRec TALLRec A-LLMRec UniMP IRLLRec (UAI09) (SIGIR20) (AAAI16) (MM23) (MM24) (RecSys24) (KDD25) (RecSys23) (KDD24) (ICLR24) (SIGIR25) Baby Sports Electronics R@10 0.0357 N@10 0. R@10 0.0432 N@10 0.0241 R@10 0. N@10 0.0127 0.0479 0.0257 0.0569 0. 0.0363 0.0204 0.0423 0.0223 0.0558 0. 0.0293 0.0159 0.0624 0.0324 0.0710 0. 0.0396 0.0220 0.0617 0.0321 0.0687 0. 0.0386 0.0228 0.0602 0.0325 0.0705 0. 0.0430 0.0254 0.0647 0.0348 0.0707 0. 0.0395 0.0210 0.0382 0.0197 0.0418 0. 0.0374 0.0178 0.0379 0.0203 0.0402 0. 0.0347 0.0201 0.0472 0.0267 0.0528 0. 0.0363 0.0215 0.0624 0.0318 0.0712 0. 0.0419 0.0248 RecGOAT Improv. 0.0671* 0.0369* 0.0745* 0.0415* 0.0468* 0.0271* 3.71% 6.03% 4.63% 6.14% 8.84% 6.69% Table 2: Statistics of our experimental datasets."
        },
        {
            "title": "Datasets",
            "content": "# Users # Items # Interactions"
        },
        {
            "title": "Baby\nSports\nElectronics",
            "content": "19,445 35,598 192,403 7,050 18,357 63,001 160,792 296,337 1,689,188 99.88% 99.95% 99.99%"
        },
        {
            "title": "4.1.2 Baselines and Evaluation Metrics. We compare our RecGOAT\nwith the following three categories of representative multimodal\nrecommendation methods: (1) Traditional ID-based Methods:\nBPR [30] and LightGCN [13]. (2) Multimodal Methods: VBPR [12]\n(CNN-based), FREEDOM [55] (GNN-based), DiffMM [15] (Diffusion-\nbased), UGT [48] (Transformer-based), and FindRec [35] (Mamba-\nbased). (3) LM-enhanced Methods (with different semantic align-\nment/fusion paradigms): TALLRec [2] (fine-tuning), A-LLMRec [16]\n(in-context learning), UniMP [38] (cross-attention), and IRLLRec\n[37] (contrastive learning + KL divergence).",
            "content": "To evaluate the recommendation performance, we adopt two widely-used metrics: Recall (R@K) and Normalized Discounted Cumulative Gain (NDCG, N@K), where is set to 10. Each metric is computed over 10 runs, and the average result is reported. Implementation Details. Following common practice [41, 56], 4.1.3 we split each dataset into an 8:1:1 ratio for training, validation, and testing under the 5-core setting. For multimodal baselines, we uniformly employ the publicly available 4096-dimensional visual features and 384-dimensional textual features provided by the opensource framework MMRec [54], adhering to its standard parameter configuration. 1https://cseweb.ucsd.edu/jmcauley/datasets/amazon/links.html (1) Multimodal methods (e.g., FindRec, FREEDOM) outperform traditional ID-based methods (e.g., LightGCN), confirming the auxiliary role of multimodal information in alleviating the sparsity of interaction IDs. However, LLM-enhanced methods, despite their powerful semantic extraction and generation capabilities, generally underperform multimodal baselines while incurring significantly higher computational costs. This performance gap stems from their insufficient emphasis on aligning world knowledge with recommendation ID signals. (2) Our RecGOAT achieves substantial improvements over LLMenhanced baselines (e.g., 0.0468 vs. 0.0419 for Electronics), primarily due to its theoretically-grounded dual semantic alignment, especially the previously overlooked principle of distribution-level alignment between LLM-enhanced modalities and ID embeddings. Specifically, RecGOATs superior performance over TALLRec (fine tuning-based alignment), A-LLMRec (in-context learning-based alignment), and UniMP (cross-attention-based alignment) demonstrates that alignment from distribution perspective is crucial, and instance-level or pair-wise alignment alone is insufficient. (3) Our RecGOAT outperforms IRLLRec (contrastive learning + KL divergence-based alignment), indicating the advantage of the Wasserstein distance (OT) over KL divergence for distribution alignment. KL divergence only measures the ratio of probability densities and is insensitive to the geometric structure of the sample space. For example, aligning the feature red as purple incurs similar penalty as aligning it as indoor item under KL divergence, whereas the Wasserstein distance would assign much lower cost to the former, more semantically related distribution."
        },
        {
            "title": "RecGOAT",
            "content": "Under Review, , Table 3: Ablation study on different alignment and fusion strategies for three amazon datasets."
        },
        {
            "title": "Dataset Metric",
            "content": "ID-only Naive MM Fusion Our Alignment"
        },
        {
            "title": "Sum",
            "content": "w/ CMCL w/ OAT RecGOAT"
        },
        {
            "title": "Sports",
            "content": "Elec. R@10 N@10 R@10 N@10 R@10 N@10 0.0479 0.0257 0.0569 0. 0.0363 0.0204 0.0472 0.0244 0.0573 0.0305 0.0402 0.0222 0.0422 0.0217 0.0525 0. 0.0385 0.0210 0.0601 0.0330 0.0695 0.0381 0.0388 0.0217 0.0623 0.0346 0.0718 0. 0.0437 0.0245 0.0671* 0.0369* 0.0745* 0.0415* 0.0468* 0.0271* Overall, by integrating cross-modal contrastive learning and optimal adaptive transport within dual-alignment framework, RecGOAT achieves state-of-the-art recommendation performance."
        },
        {
            "title": "4.3 Ablation Study (RQ2)\nTo demonstrate the semantic conflict between LLM-enhanced modal-\nities and ID signals and to quantify the effectiveness of differ-\nent alignment strategies, we evaluate several variants: an ID-only\nmethod (LightGCN), naive Multimodal (MM) fusion (i.e., Concat\nand Sum) with ID from LightGCN and modality form GAT, and our\nRecGOAT with individual or combined alignment components (i.e.,\nCMCL and OAT). The results are summarized in Table 3, leading to\nthe following observations:",
            "content": "Simple fusion of LLM-enhanced modal embeddings via concatenation or summation yields inferior or inconsistent performance compared to the ID-only LightGCN (e.g., on the Baby dataset), confirming the severe semantic heterogeneity between large model semantics and recommendation IDs. Within our dual-granularity alignment framework, OAT consistently outperforms CMCL across all datasets, highlighting the critical role of distribution-level alignment. Furthermore, the organic integration of instance-level and distributionlevel alignment mutually reinforces both components, resulting in comprehensive semantic fusion and optimal multimodal recommendation performance."
        },
        {
            "title": "4.4 Alignment Consistency and Fusion",
            "content": "Comprehensiveness (RQ3) To validate the theoretical conclusions established in Section 3.3, we conducted experiments on the Baby dataset to examine alignment consistency and fusion comprehensiveness, as illustrated in Figure 3. First, Figure (3a) demonstrates that the impact of different weighting coefficients in Eq. (14) on the final recommendation performance is robust, indicating strong consistency among the aligned representations Ë†ğ’ ğ‘¡ , Ë†ğ’ ğ‘£, and ğ’ğ‘–ğ‘‘ . Second, Figure (3b) shows that the fused item representation ğ’ achieves superior recommendation performance compared to any single aligned modality Ë†ğ’ ğ‘¡ , Ë†ğ’ ğ‘£, and ğ’ğ‘–ğ‘‘ . This observation is consistent with the conclusion of Theorem 3.5 (2), which supports the comprehensiveness of the unified representation. (a) Consistency: Triangular heatmap of performance with different modality weights in Eq. (14) (b) Comprehensiveness: Performance comparison of different modalities and the fused representation after alignment Figure 3: Alignment Consistency and Fusion Comprehensiveness of RecGOAT on the Baby Dataset"
        },
        {
            "title": "5 Conclusions\nIn this paper, we propose RecGOAT, a dual-granularity seman-\ntic alignment framework for LLM-enhanced multimodal recom-\nmendation. It integrates instance-level alignment via cross-modal\ncontrastive learning and distribution-level alignment via optimal\nadaptive transport to resolve the semantic heterogeneity between\nlarge-model representations and recommendation ID signals. The-\noretically, we prove the consistency and comprehensiveness of the\naligned representations derived from RecGOAT. Extensive experi-\nments on three Amazon datasets validate our theoretical results and\ndemonstrate SOTA performance against relevant baselines. Further-\nmore, A/B testing on a large-scale advertising platform confirms the\nscalability of RecGOAT. In future work, we will explore interactions\namong multiple optimal transport alignments and extend semantic\nalignment solutions to omni-modal large recommendation model.",
            "content": "References [1] Vineeta Anand and Ashish Kumar Maurya. 2025. survey on recommender systems using graph neural network. ACM Transactions on Information Systems 43, 1 (2025), 149. [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM conference on recommender systems. 10071014. [3] Zongsheng Cao, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. 2022. OTKGE: Multi-modal Knowledge Graph Embeddings via Optimal Transport. In Advances in Neural Information Processing Systems. https://openreview.net/forum?id=gbXqMdxsZIP Under Review, , Yuecheng Li, Hengwei Ju, Zeyu Song, Wei Yang, Chi Lu, Peng Jiang, and Kun Gai [4] Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. 2020. Graph optimal transport for cross-domain alignment. In International Conference on Machine Learning. PMLR, 15421553. [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. simple framework for contrastive learning of visual representations. In International conference on machine learning. PmLR, 15971607. [6] Xu Chen, Hanxiong Chen, Hongteng Xu, Yongfeng Zhang, Yixin Cao, Zheng Qin, and Hongyuan Zha. 2019. Personalized fashion recommendation with visual explanations based on multimodal attention network: Towards visually explainable recommendation. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. 765774. [7] Nicolas Courty, RÃ©mi Flamary, Amaury Habrard, and Alain Rakotomamonjy. 2017. Joint distribution optimal transportation for domain adaptation. Advances in neural information processing systems 30 (2017). [8] Qiang Cui, Shu Wu, Qiang Liu, Wen Zhong, and Liang Wang. 2018. MV-RNN: multi-view recurrent neural network for sequential recommendation. IEEE Transactions on Knowledge and Data Engineering 32, 2 (2018), 317331. [9] Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems 26 (2013). [10] Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He, et al. 2023. survey of graph neural networks for recommender systems: Challenges, methods, and directions. ACM Transactions on Recommender Systems 1, 1 (2023), 151. [11] Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. In First conference on language modeling. [12] Ruining He and Julian McAuley. 2016. VBPR: visual bayesian personalized ranking from implicit feedback. In Proceedings of the AAAI conference on artificial intelligence, Vol. 30. [13] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639648. [14] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web. 173182. [15] Yangqin Jiang, Lianghao Xia, Wei Wei, Da Luo, Kangyi Lin, and Chao Huang. 2024. Diffmm: Multi-modal diffusion model for recommendation. In Proceedings of the 32nd ACM International Conference on Multimedia. 75917599. [16] Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park. 2024. Large language models meet collaborative filtering: An efficient all-round llm-based recommender system. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 13951406. [17] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023. Text is all you need: Learning language representations for sequential recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 12581267. [18] Jin Li, Shoujin Wang, Qi Zhang, Shui Yu, and Fang Chen. 2025. Generating with fairness: modality-diffused counterfactual framework for incomplete multimodal recommendations. In Proceedings of the ACM on Web Conference 2025. 27872798. [19] Yuecheng Li, Jialong Chen, Chuan Chen, Lei Yang, and Zibin Zheng. 2024. Contrastive deep nonnegative matrix factorization for community detection. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 67256729. [20] Guojiao Lin, Meng Zhen, Dongjie Wang, Qingqing Long, Yuanchun Zhou, and Meng Xiao. 2024. GUME: Graphs and User Modalities Enhancement for LongTail Multimodal Recommendation. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. 14001409. [21] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang, Yong Liu, Chuhan Wu, Xiangyang Li, Chenxu Zhu, et al. 2025. How can recommender systems benefit from large language models: survey. ACM Transactions on Information Systems 43, 2 (2025), 147. [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems 36 (2023), 3489234916. [23] Yuqing Liu, Yu Wang, Lichao Sun, and Philip Yu. 2024. Rec-gpt4v: Multimodal recommendation with large vision-language models. arXiv preprint arXiv:2402.08670 (2024). [24] Alejo Lopez-Avila and Jinhua Du. 2025. Survey on Large Language Models in Multimodal Recommender Systems. arXiv preprint arXiv:2505.09777 (2025). [25] Daniele Malitesta, Giandomenico Cornacchia, Claudio Pomo, Felice Antonio Merra, Tommaso Di Noia, and Eugenio Di Sciascio. 2025. Formalizing multimedia recommendation through multimodal deep learning. ACM Transactions on Recommender Systems 3, 3 (2025), 133. [26] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. 4352. [27] Gabriel PeyrÃ©. 2025. Optimal Transport for Machine Learners. arXiv preprint arXiv:2505.06589 (2025). [28] Gabriel PeyrÃ©, Marco Cuturi, et al. 2019. Computational optimal transport: With applications to data science. Foundations and Trends in Machine Learning 11, 5-6 (2019), 355607. [29] Claudio Pomo, Matteo Attimonelli, Danilo Danese, Fedelucio Narducci, and Tommaso Di Noia. 2025. Do Recommender Systems Really Leverage Multimodal Content? Comprehensive Analysis on Multimodal Representations for Recommendation. In Proceedings of the 34th ACM International Conference on Information and Knowledge Management. 23772387. [30] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. 452461. [31] Filippo Santambrogio. 2015. Optimal transport for applied mathematicians. (2015). [32] Richard Sinkhorn and Paul Knopp. 1967. Concerning nonnegative matrices and doubly stochastic matrices. Pacific J. Math. 21, 2 (1967), 343348. [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [34] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In International Conference on Learning Representations. [35] Maolin Wang, Yutian Xiao, Binhao Wang, Sheng Zhang, Shanshan Ye, Wanyu Wang, Hongzhi Yin, Ruocheng Guo, and Zenglin Xu. 2025. FindRec: Stein-Guided Entropic Flow for Multi-Modal Sequential Recommendation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2. 30083018. [36] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. 165174. [37] Yu Wang, Lei Sang, Yi Zhang, and Yiwen Zhang. 2025. Intent representation learning with large language model for recommendation. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. 18701879. [38] Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, and Xianfeng Tang. 2024. Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=khAE1sTMdX [39] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM international conference on multimedia. 14371445. [40] Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin, and Jimmy Huang. 2022. Hypergraph contrastive collaborative filtering. In Proceedings of the 45th International ACM SIGIR conference on research and development in information retrieval. 7079. [41] Jinfeng Xu, Zheyu Chen, Shuo Yang, Jinze Li, and Edith CH Ngai. 2025. The Best is Yet to Come: Graph Convolution in the Testing Phase for Multimodal Recommendation. In Proceedings of the 33rd ACM International Conference on Multimedia. 63256334. [42] Jinfeng Xu, Zheyu Chen, Shuo Yang, Jinze Li, Wei Wang, Xiping Hu, Steven Hoi, and Edith Ngai. 2025. Survey on Multimodal Recommender Systems: Recent Advances and Future Directions. arXiv preprint arXiv:2502.15711 (2025). [43] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [44] Wei Yang, Jie Yang, and Yuan Liu. 2023. Multimodal optimal transport knowledge distillation for cross-domain recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 29592968. [45] Wei Yang and Qingchen Yang. 2024. Multimodal-aware Multi-intention Learning for Recommendation. In Proceedings of the 32nd ACM International Conference on Multimedia. 56635672. [46] Wei Yang, Rui Zhong, Yiqun Chen, Shixuan Li, Heng Ping, Chi Lu, and Peng Jiang. 2025. FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning. In Proceedings of the 33rd ACM International Conference on Multimedia. 61936202. [47] Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, and Xiangnan He. 2023. Generate what you prefer: Reshaping sequential recommendation via guided diffusion. Advances in Neural Information Processing Systems 36 (2023), 2424724261. [48] Zixuan Yi and Iadh Ounis. 2024. unified graph transformer for overcoming isolations in multi-modal recommendation. In Proceedings of the 18th ACM Conference on Recommender Systems. 518527. [49] Zixuan Yi and Iadh Ounis. 2025. Multi-modal Large Language Model with Graph-of-Thought for Effective Recommendation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational"
        },
        {
            "title": "RecGOAT",
            "content": "Under Review, , Linguistics: Human Language Technologies (Volume 1: Long Papers). 15911606. [50] Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, and Enhong Chen. 2025. NoteLLM-2: Multimodal Large Representation Models for Recommendation. In 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2025). Association for Computing Machinery, 28152826. [51] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. 2021. Mining latent structures for multimedia recommendation. In Proceedings of the 29th ACM international conference on multimedia. 38723880. [52] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv preprint arXiv:2506.05176 (2025). [53] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2024. Recommender systems in the era of large language models (llms). IEEE Transactions on Knowledge and Data Engineering 36, 11 (2024), 68896907. [54] Xin Zhou. 2023. Mmrec: Simplifying multimodal recommendation. In Proceedings of the 5th ACM International Conference on Multimedia in Asia Workshops. 12. [55] Xin Zhou and Zhiqi Shen. 2023. tale of two graphs: Freezing and denoising graph structures for multimodal recommendation. In Proceedings of the 31st ACM international conference on multimedia. 935943. [56] Xin Zhou, Hongyu Zhou, Yong Liu, Zhiwei Zeng, Chunyan Miao, Pengwei Wang, Yuan You, and Feijun Jiang. 2023. Bootstrap latent representations for multimodal recommendation. In Proceedings of the ACM web conference 2023. 845854."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Kuaishou Technology",
        "Unaffiliated",
        "University of Southern California"
    ]
}