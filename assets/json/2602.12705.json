{
    "paper_title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
    "authors": [
        "Baorong Shi",
        "Bo Cui",
        "Boyuan Jiang",
        "Deli Yu",
        "Fang Qian",
        "Haihua Yang",
        "Huichao Wang",
        "Jiale Chen",
        "Jianfei Pan",
        "Jieqiong Cao",
        "Jinghao Lin",
        "Kai Wu",
        "Lin Yang",
        "Shengsheng Yao",
        "Tao Chen",
        "Xiaojun Xiao",
        "Xiaozhong Ji",
        "Xu Wang",
        "Yijun He",
        "Zhixiong Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research."
        },
        {
            "title": "Start",
            "content": "MedXIAOHE: Comprehensive Recipe for Building Medical MLLMs"
        },
        {
            "title": "ByteDance XiaoHe Medical AI",
            "content": "See Contributions section for full author list."
        },
        {
            "title": "Abstract",
            "content": "We present MedXIAOHE, medical vision-language foundation model designed to advance generalpurpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entityaware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research. Correspondence: yanghaihua@bytedance.com 6 2 0 2 3 1 ] . [ 1 5 0 7 2 1 . 2 0 6 2 : r 1 Figure 1 Performance comparison of MedXIAOHE against SOTA models on comprehensive medical benchmarks. The left panel shows the overall average score across 30+ benchmarks, demonstrating the strong performance of MedXIAOHE. The right panels detail the comparative results across six key capabilities. In the upper-left bar chart, dark bars represent scores on public benchmarks, and light bars represent scores on in-house benchmarks. We did not evaluate in-house benchmarks on Gemini 3.0 Pro because of changes in its privacy protocols."
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Overview and Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Continual Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Medical Entity Tree (MET) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Medical Entity Tree Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scalable Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 . . . . . . . . . . . . . . . . . . . . . 3.2.3 Quantitative Evaluation of Knowledge Coverage 3.3 Continual Pre-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Text Knowledge 3.3.2 Image Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.3 Caption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.4 OCR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.5 Grounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.6 Human-in-the-loop Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Training Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Mid-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Mid-Training Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Internal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Agentic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Training Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Post-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Supervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . SFT Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Instruction Following . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 RL Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 5.1.2 4 5 7 7 8 8 10 10 11 11 12 13 13 13 13 14 15 15 15 17 18 19 19 19 20 20 2 5.2.2 Multi-Layered Hybrid Reward System . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.3 RFT-Enhanced Curriculum Reinforcement Learning . . . . . . . . . . . . . . . . . . . 6 Unified Med-VLM Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 Public Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inhouse Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Qualitative examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Medical DeepResearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Think with Medical Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Think with Grounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Expert-Level Complex Diagnostic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Real-World Clinical Report Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Deformed Clinical Report Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 22 23 24 25 25 31 32 32 33 34 35 36 37 Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 Prompts for Entity Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40"
        },
        {
            "title": "Introduction",
            "content": "Foundation visionlanguage models (VLMs) have made rapid progress in recent years, showing strong ability to connect visual information with natural language. As these models scale, they have become capable multimodal assistants that can follow instructions and reason over complex inputs. Healthcare is natural and high-impact next step, given that clinical decision-making is inherently multimodal, involving patient symptoms, medical histories, physical examinations, radiology and pathology images, and structured reports. Encouraging progress has already emerged from large medical VLM efforts [7, 32, 40, 43, 48, 49, 56], suggesting that single medical foundation model may eventually support wide range of clinical-facing tasks. At the same time, building medical foundation model that is genuinely applicable in the real world remains diﬀicult. Clinical tasks demand not only accuracy, but also disciplined evidence use, stable behavior across diverse settings, and reliable long-form generation. Many high-value scenarios sit in the medical long tail (e.g., rare diseases, atypical presentations, complex comorbidities), where coverage gaps and spurious shortcuts can surface as confident but incorrect answers. Moreover, medical inputs are heterogeneous: imaging protocols vary across institutions, visual inspection images are captured under uncontrolled conditions, and reports can contain subtle details whose omission or distortion changes clinical meaning [5, 18]. These characteristics make healthcare stringent test of robustness and reliability for multimodal foundation models. Another practical obstacle is evaluation. Medical multimodal progress is often reported across patchwork of benchmarks with inconsistent prompting, scoring, and contamination control, which makes results hard to reproduce and hard to compare. In parallel, capabilities that strongly influence product readiness, such as medical report OCR robustness and lesion-level description faithfulness, are absent or under-measured from many public leaderboards. The broader biomedical AI community has increasingly emphasized rigorous documentation and reproducibility practices [11], reinforcing the need for evaluation that is standardized, interpretable, and actionable for iterative development. In this report, we propose MedXIAOHE, medical vision-language foundation model built to narrow the gap between benchmark performance and clinical-facing usability. MedXIAOHE unifies medical text understanding, visual inspection, medical imaging, medical OCR, and high-quality long-form report generation within single model. To scale medical knowledge while reducing long-tail gaps, we introduce entity-aware, native, multimodal continual pretraining that organizes heterogeneous corpora around an entity-centric taxonomy, thereby improving coverage and robustness across specialties and modalities. To improve long-form reliability, we build an evidence-grounded captioning and report pipeline that combines multi-stage drafting, medical-entity extraction and retrieval, critique-and-rewrite, and rubric-trained generative reward models to suppress subtle hallucinations and over-interpretation in clinical descriptions. Beyond static question answering, real clinical workflows are interactive: high-quality assistance often requires gathering missing information, consulting references, and verifying intermediate hypotheses. MedXIAOHE therefore integrates clinical thinking patterns with reinforcement learning and tool-augmented agentic training, enabling multistep diagnostic reasoning with verifiable decision traces [8, 12, 44]. We also emphasize application-friendly behavior, including robust instruction-following and alignment with health-oriented preferences in evaluations [2]. Another contribution of this work is unified evaluation framework for medical VLMs. We introduce Unified Med-VLM Benchmark that consolidates 30+ public benchmarks under standardized protocol with unified prompting, scoring, and decontamination, enabling reproducible measurement and more meaningful cross-model comparisons. We complement public datasets with targeted inhouse evaluations designed to stress clinically important but under-measured capabilities. Together, these components provide practical measurement backbone that connects the model and real-world medical practice. In summary, this report provides the following highlights: State-of-the-Art Medical VLM Performance. We report strong end-to-end results for medical VLMs across more than 30 public and in-house benchmarks. Entity-Aware Continual Pretraining. We propose an entity-centric continual pretraining strategy that expands medical knowledge coverage and improves robustness to long-tail cases across specialties and 4 Figure 2 The architecture of MedXIAOHE. The model utilizes Multimodal Native-Resolution Transformer to process diverse medical imaging modalities (e.g., X-ray, CT, Pathology) with varying resolutions and aspect ratios. Visual features encoded by Seed-ViT are projected via an MLP Adapter and interleaved with text tokens, integrating medical knowledge and patient records to support multi-turn dialogue and reasoning-based generation. modalities. Robust Reasoning Across Diverse Scenarios. Our post-training approach supports reliable reasoning for text-only, vision-only, and multimodal tasks, including clinical questions that require the integration of mixed evidence. Advanced Agentic Diagnostic Capability. We demonstrate the effectiveness of tool-augmented, multi-step diagnostic reasoning with verifiable decision traces on complex, real-world diagnostic tasks."
        },
        {
            "title": "2 Overview and Architecture",
            "content": "MedXIAOHE is built upon the Seed vision-language foundation model and follows the same core design: strong vision encoder coupled with large language model (LLM), trained as unified multimodal decoder. Concretely, the model consists of three main components: (1) vision backbone (Seed-ViT [19]) that encodes one or multiple images into sequence of visual tokens; (2) lightweight multimodal connector that maps visual features into the LLM embedding space; and (3) an autoregressive LLM that performs instruction following, reasoning, and generation conditioned on interleaved visual and textual inputs. As shown in Fig. 2, the architecture offers two practical advantages that are particularly important for medical applications. First, it unifies diverse medical tasks under single interface: the same model can read long textual context (e.g., histories, guidelines, reports), interpret clinical photos and medical images, extract information from medical documents via OCR-style understanding, and generate structured or long5 Table 1 Performance of MedXIAOHE on the unified Med-VLM benchmark compared to SOTA models. All benchmarks are evaluated with greedy decoding. We report Pass@1 in these benchmarks. For all other benchmarks, higher numbers are better. The highest score in each benchmark is marked in bold, and the second is underlined. We did not evaluate in-house benchmarks on Gemini 3.0 Pro because of changes in its privacy protocols. Capability Benchmark GPT-5.2 Thinking Gemini3.0 Pro Gemini2.5 Pro MedXIAOHE Visual Diagnosis & Image Recognition Medical Imaging Diagnosis Medical Text Medical Report Instruction Following Inhouse VQA Inhouse Caption Inhouse OCR MMMU_val-Med[64] MMMU_Pro-Med[63] GMAI-MMBench[60] VQA_RAD[30] SLAKE[33] PATH_VQA[21] PMC_VQA[65] OmniMedVQA[23] MedXpertQA-MM[69] MedXpertQA-Text[69] DiagnosisArena[68] HLE-Med[44] RareBench[9] MedBrowseComp[8] PubMedQA[26] MedQA_MCMLE[42] MedQA_USMLE[25] Medbullets_op4[6] Medbullets_op5[6] SuperGPQA-Med[16] CMExam[34] MedMCQA[42] HealthBench-hard[2] HealthBench-normal[2] MIMIC-CXR[27] CheXpert Plus[4] IU-Xray[14] MultiChallenge[15] MulDimIF[61] MedMTbench[57] 68.27 36.71 74.86 79.73 71.85 60.42 68.96 72.83 47.88 68.65 73.95 73.25 55.27 45.57 24.70 42.19 19.00 79.80 91.48 96.39 91.23 88.31 65.37 89.12 84.77 42.04 63.34 46.44 41.85 51. 60.00 78.60 51.30 83.33 73.43 66.81 74.50 76.22 54.37 70.15 81.96 77.20 69.47 26.45 36.91 41.00 26.00 80.80 94.48 95.52 91.88 86.04 72.45 93.13 86.01 17.70 48.40 48.99 42.15 73. 66.80 77.80 49.80 69.64 45.92 80.82 79.60 65.91 61.67 67.41 70.20 52.60 66.85 71.86 45.40 41.67 20.87 17.85 36.01 25.00 75.60 91.54 93.01 82.79 72.73 63.77 87.49 84.22 16.80 49.80 42.39 38.96 61. 56.60 73.20 45.00 76.77 46.89 81.92 87.53 73.88 55.21 66.96 82.62 59.15 63.90 83.40 71.00 63.14 42.08 25.77 46.79 29.00 86.00 96.21 97.88 95.78 94.16 71.94 96.12 86.54 46.10 65.20 50.86 49.43 65. 61.90 78.70 63.75 form outputs such as findings/impressions. Second, the decoder-centric design naturally supports multi-turn interaction: the model can incorporate prior dialogue turns, intermediate hypotheses, and retrieved evidence into single context window, and produce responses that are consistent with the evolving conversation state. Difference in Design. MedXIAOHE extends the Seed foundation [19] through continual training rather than architecture re-design. This choice is intentional: for clinical-facing systems, reliability and coverage often depend more on (1) domain-specific data quality, (2) knowledge coverage in the long tail, and (3) post-training alignment for reasoning and preference, rather than on introducing complex architectural novelty. Performance. MedXIAOHE demonstrates state-of-the-art performance across diverse benchmarks as shown in Tab. 1. MedXIAOHE is evaluated in thinking mode against leading contemporary models, including GPT5.2 Thinking, Gemini 3.0 Pro, and Gemini 2.5 Pro, using greedy decoding. Visual diagnosis & image recognition. MedXIAOHE achieves consistently strong results on both inhouse and public evaluations, outperforming SOTA models on Inhouse VQA/Caption/OCR and the MMMU medical series (MMMU_val-Med, MMMU_Pro-Med), indicating robust medical visual understanding and recognition. Medical imaging. The model exhibits clear strengths on heterogeneous radiology/pathology VQA tasks such as SLAKE, PATH_VQA, and OmniMedVQA, suggesting effective cross-modal reasoning on clinical imagery. Diagnosis. MedXIAOHE shows competitive diagnostic reasoning performance on RareBench and MedBrowseComp and maintains strong results on DiagnosisArena and HLE-Med, indicating robust medical decision-making capability in complex and rare disease diagnosis. Medical text. Our model demonstrates strong medical knowledge and reasoning across PubMedQA, MedQA (MCMLE/USMLE), MedBullets, CMExam, MedMCQA, and HealthBench, reflecting solid competence in clinical QA and challenging medical reasoning. Medical report generation. MedXIAOHE performs well on MIMIC-CXR and CheXpert Plus, indicating reliable radiology report generation and clinical finding coverage. Results on IU-Xray are comparatively lower than the best baseline, suggesting remaining headroom for narrative in certain report-generation regimes. Instruction following. The model is effective on multi-constraint and medically oriented instruction benchmarks (MulDimIF, MedMTbench), and remains competitive on MultiChallenge, supporting its practical usability in instruction-driven medical VLM applications."
        },
        {
            "title": "3 Continual Pre-training",
            "content": "Figure 3 Data-cleaning pipeline for the continual pretraining corpus. The pipeline comprises two main stages: text-cleaning workflow and multimodal data production workflow. To construct high-quality pretraining corpus, we apply combination of hash-based deduplication, rule-based filtering, and model-based quality control."
        },
        {
            "title": "3.1 Data Collection",
            "content": "To construct rich and factually grounded pretraining corpus, we curate data from multiple sources and apply source-specific processing to account for differences in scale and noise. In terms of volume, public web data exceeds licensed books, papers, and online clinical data; in terms of quality, the ordering is reversed. 7 For public web data, we collect 310B tokens and enhance using fine-grained topic classification and updated captions. Given the corpus scale and heterogeneous quality, we apply lightweight content-based deduplication, followed by model-based quality filtering, text cleaning and normalization, secondary deduplication, and imagetext relevance filtering. For licensed medical books and papers, we perform OCR on scanned materials to obtain 280B tokens. To maximize precision, we prioritize recognition accuracy and fine-grained categorization. We also collect 28B tokens clinical lesion images for better visual diagnosis and 22B tokens open-source datasets. The continual pretraining corpus consists of total of approximately 640 billion tokens. Next, as shown in Fig. 3, we adopt three-stage data cleaning pipeline: (1) global deduplication, where we apply hash-based methods to remove redundant samples at scale; (2) rule-based filtering and normalization, where we use simple heuristics to filter and reshape the data (e.g., splitting long documents into manageable chunks with sliding-window strategy); and (3) model-based quality filtering, where we train medical FastText [29] classifier to select high-quality samples for final training. We build this classifier with selfbootstrapped positive/negative loop: we first sample instances after stages (1) and (2), label them as high or low-quality, and train an initial classifier. We then sample additional instances from the remaining pool, run inference with the classifier, and perform second-pass verification with more capable LLM. From the verified data, we uniformly sample positive and negative instances to further refine the classifier. We repeat this loop until the classifier reaches our target accuracy."
        },
        {
            "title": "3.2 Medical Entity Tree (MET)",
            "content": "The construction of the Medical Entity Tree serves three pivotal objectives: (1) Balancing Entity Training, mitigating long-tail distribution issues; (2) Quantifying Knowledge Coverage, offering metric to evaluate the breadth of medical knowledge in pre-training data; and (3) Guiding Data Collection, identifying sparse domains for targeted acquisition. Figure 4 Architecture overview of the Medical Entity Tree. This hierarchical taxonomy organizes medical concepts into aligned categories to facilitate balanced entity training, precise knowledge coverage quantification, and entitydriven data acquisition. 3.2.1 Medical Entity Tree Construction We developed multi-stage pipeline evolving from coarse extraction to fine-grained refinement to mine entities from these materials, effectively navigating the trade-off between extraction quality and computational 8 eﬀiciency. Initially, we leveraged LLMs to identify medical entities from Stage 1: High-Efficiency Entity Extraction. this massive corpus. To mitigate inference latency, we implemented Batch Chunking strategy, identifying entities within consolidated multi-sentence input units. To ensure precision and alignment within this batched framework, we designed rigorous prompt that enforces strict JSON formatting and domain filtering. The specific instruction is shown below: Prompt for Stage 1: Batch Entity Extraction Below are several sentences. Analyze them for medical entities and output according to the following requirements: 1. Determine if it is medical entity; if not, do not output. 2. Separate entity nouns with commas; do not include duplicates. 3. If there are no medical-related entity nouns, output \"None\". 4. Output strictly in JSON format. The example format is as follows: {{'Sentence0': 'Entity1,Entity2,...', 'Sentence1': 'Entity1,Entity2,...', ...}} Sentences: {lines} This prompt design significantly facilitates downstream parsing. By enforcing strict JSON output with explicit handling of null cases (outputting None), it yielded processing speedup of approximately 30 while maintaining high extraction recall and effectively filtering non-medical noise. Stage 2: Joint Typing and Hierarchical Clustering. To evolve from flat noun list to structured system, the second stage advanced the prompt engineering strategy to simultaneously perform entity extraction and type abstraction. We designed the prompt to address specific challenges in medical texts, such as handling noisy characters (often from OCR or formatting) and ensuring appropriate entity granularity by splitting overly long phrases. The specific instruction is shown in Supp. C. Post-extraction, we enforced rigorous cleansing protocol: entities appearing fewer than 10 times and types containing fewer than 5 entities were filtered to reduce noise. Subsequently, we applied K-Means clustering [39] on entity embeddings, utilizing the Silhouette Coeﬀicient [46] to dynamically optimize the cluster count to define secondary types. By applying bottom-up, frequency-weighted aggregation of embeddings, we recursively constructed multi-level entity hierarchy. Stage 3: Controlled Expansion via Tree Attachment. Upon completing Stages 1 and 2, we obtained concise, high-quality core taxonomy. Stage 3 focuses on scaling this repository by integrating entities extracted from the broader corpus. Incremental Tree Attachment. Instead of rebuilding the tree from scratch, we adopted an Incremental Tree Attachment strategy. We treat the core taxonomy as fixed skeleton and utilize the semantic reasoning capabilities of LLMs to graft new candidate entities onto the most appropriate nodes. To ensure the integrity of the existing structure during this expansion, we designed strict directive prompt. The LLM acts as taxonomic expert, mandated to identify the correct insertion path without altering existing nodes. The prompt is included in Supp. C. Optimization via Deferred Insertion. As the taxonomy expands, fitting the entire tree structure into the LLM context window becomes infeasible. We addressed this through Deferred Insertion strategy: we freeze the high-quality core tree as the context anchor. For subsequent insertions, the LLM predicts the insertion path relative to this frozen core. These operations are buffered and executed in batch updates, preventing context overflow while maintaining global consistency. Conflict Resolution via ReAct Agent. While the deferred insertion ensures scalability, it introduces structural ambiguity: single entity may develop high aﬀinity with multiple parent nodes (e.g., Kluver-Bucy syndrome appearing under both Neurological Disorders and Digestive Symptoms). This creates logical conflicts where the taxonomy degrades into graph structure with overlapping definitions. Solely relying on the internal parametric knowledge of LLMs to resolve these conflicts often leads to hallucinations. To address this, we implemented ReAct Agent framework [59]. We designed specialized prompt with rigorous logic, detailed in Supp. C. The transition from passive LLM to an active agent offers three critical benefits: Hallucination Mitigation: By mandating Retrieval-Augmented Generation (RAG), we force the model to base its arbitration on real-time, objective external evidence rather than potentially outdated or hallucinated internal weights. Explainability and Auditability: Every pruning operation generates structured log containing the <Reasoning> and <SearchEvidence>. This creates transparent audit trail, allowing human experts to verify why specific classification decision was made. Dynamic Knowledge Adaptation: Since the agent connects to the live web, it can correctly classify newly discovered rare diseases or recently approved drugs that may lie beyond the pre-training data cutoff of standard LLMs. Finally, we established rigorous five-tier taxonomy comprising 1.4 million entities. 3.2.2 Scalable Mapping After constructing the medical taxonomy, the challenge shifts to eﬀiciently mapping the tens of millions of entries in the pre-training corpus to it. We implemented the Aho-Corasick (AC) Automaton algorithm [1]. By constructing Trie tree with Fail Pointers from the finalized entity dictionary, we achieve O(N ) time complexity for corpus scanning. This decouples processing speed from dictionary size, allowing us to rapidly calculate knowledge coverage and extract representative entities from massive datasets in approximately 20 hours (a 3 speedup over parallel LLM scanning). 3.2.3 Quantitative Evaluation of Knowledge Coverage The core challenge in constructing medical taxonomy lies in defining and measuring comprehensiveness. Our methodology is grounded in the hypothesis that open-source medical literature encapsulates the majority of currently established medical knowledge. Consequently, taxonomy distilled directly from these sources serves as the ground truth. To quantitatively validate this, we propose semantic coverage metric to compare our taxonomy against established benchmarks. We employ AMCS, semantic similarity metric, to measure the extent to which Target Set (A) is covered by Reference Set (Ref ). The metric is defined as: AMCS(A, Ref ) = 1 A i=1 max j[1,Ref ] CosineSimilarity(ai, rj) (1) where ai and rj denote the embedding vectors of entities in set and set Ref , respectively, generated by pretrained encoder (e.g., Sentence-BERT [45]). This metric is asymmetric, offering two distinct interpretations: Forward Coverage AMCS(A, Ref ): high value indicates that most concepts in the target set are semantically present in our Reference taxonomy. This establishes the validity of our tree. Backward Coverage AMCS(Ref, A): lower value implies that our Reference taxonomy contains numerous concepts absent from set A. This establishes the comprehensive superiority of our tree. Experimental Setup and Baselines. We designate our constructed Medical Entity Tree as the Reference Set (Ref ). We selected three distinct datasets as Target Sets (A) for evaluation: 1. Clinical Knowledge Data: clinical capability benchmarks that measure the coverage of various aspects of the model, representing clinical practice focus. 10 2. Common Crawl Medical Corpus: large-scale general dataset representing broad medical data distribution. 3. CMeKG (Chinese Medical Knowledge Graph) [3]: An authoritative open-source benchmark utilizing natural language processing on massive medical texts, referencing international standards like ICD [41]. Results and Analysis. The quantitative results are presented in Tab. 2. Table 2 Semantic Coverage Analysis: Forward vs. Backward AMCS Target Set (A) Forward Coverage Backward Coverage Interpretation AMCS(A, Ref ) AMCS(Ref, A) Clinical Knowledge Data Common Crawl Medical Corpus CMeKG [3] 0.96 0.95 0.97 0.68 0.89 0.79 High inclusion, Ref is broader High inclusion, Ref is broader Near-total inclusion The analysis yields two critical insights: 1. Near-Complete Forward Coverage: Our Medical Entity Tree achieves Forward Coverage AMCS scores exceeding 0.95 across all datasets (0.96, 0.95, 0.97). This confirms that our tree successfully encompasses the vast majority of medical concepts found in both clinical benchmarks and established knowledge graphs like CMeKG. 2. Significant Backward Gap: Conversely, the lower Backward Coverage AMCS scores (e.g., 0.68 for Clinical Knowledge Data, 0.79 for CMeKG) demonstrate that our Medical Entity Tree contains substantial number of long-tail entities and fine-grained concepts not present in the baselines. This empirically supports our hypothesis that extracting directly from authoritative literature yields more comprehensive ontology than existing collections."
        },
        {
            "title": "3.3 Continual Pre-training Data",
            "content": "Based on the balanced dataset built with the medical entity tree, we construct continual pre-training data with high medical knowledge coverage. The curation procedure for each category is described in this section. 3.3.1 Text Knowledge To effectively expand the models knowledge boundaries while mitigating reliance on raw web corporawhich are often compromised by noise and factual hallucinationswe introduce data synthesis strategy grounded in large-scale medical knowledge graph (KG). This approach aims to augment the model with high-quality, comprehensive domain knowledge, echoing recent efforts in leveraging structured knowledge to guide reliable synthetic data generation [10, 13, 58]. We initially constructed an authoritative medical KG by collecting raw documents from medical textbooks and academic papers. Following text segmentation and high-quality filtering, we extracted precise medical entities and relationships. These elements were integrated to form robust medical KG, serving as the cornerstone for our subsequent data synthesis. Leveraging this KG, we generated two categories of knowledge-centric QA data to address distinct granularity requirements: Atomic QA: Designed to inject discrete knowledge points. We generated independent QA pairs based on the descriptions of individual nodes (entities) and edges (relationships) within the KG. Aggregated QA: Designed to enhance multi-fact synthesis and comprehension. We sampled relevant subgraphs via random walks, rephrasing the interconnected entities and relationships into coherent passages. Subsequently, an LLM was employed to generate QA pairs requiring the synthesis of multiple pieces of information from the passages. We performed large-scale data synthesis on the two categories above. To address challenges such as ambiguity from dangling references and training ineﬀiciency caused by data volume, we implemented rigorous filtering for 11 non-self-contained expressions and selected high-quality samples based on Perplexity. This ensured eﬀicient training, significantly improving the models medical knowledge coverage and compositional generalization capabilities. 3.3.2 Image Knowledge Unlike general-domain vision-language models, which rely on simple object-caption pairs, medical imagery requires nuanced understanding of anatomical structures, pathological subtleties, and varying imaging modalities. significant bottleneck in current research is the scarcity of large-scale, high-quality image-text pairs that go beyond sparse classification labels. To address this problem, we propose systematic pipeline for instruction generation that synthesizes detailed captions from both massive public repositories and curated inhouse datasets. Our objective is to construct dense narrative for every image, facilitating fine-grained cross-modal alignment and empowering the model to follow complex clinical instructions with precision. Dual-Source Data Acquisition: Breadth and Depth. depth of our model, we employ dual-source data strategy. To ensure both the generalizability and the clinical Public Dataset Integration for Breadth. We leverage established open-source datasets (e.g., MIMIC-CXR [28], CheXpert [24], and ROCO [47]) to provide foundational layer of visual variety. These datasets offer vast quantity of samples covering common pathologies and standard anatomical views. However, raw annotations in public datasets are often noisy or limited to binary classification tags (e.g., Pneumonia: Positive). In-House Clinical Annotation for Depth. To complement the public data, we introduce proprietary dataset collected from partner medical institutions. This dataset is characterized by high-resolution imaging and, crucially, expert-level annotations provided by board-certified radiologists. Unlike public data, our inhouse annotations include granular details such as precise lesion localization, measurements of lesion progression, subtle textural changes in tissue, and differential diagnosis notes. This gold-standard data is pivotal for training the model on rare diseases, complex composite pathologies, and institution-specific imaging protocols that are underrepresented in open sources. The Caption Construction Pipeline: From Sparse Labels to Dense Narratives. The core of our data construction pipeline is the transformation of structured annotations into natural language instructions. model cannot learn diagnostic reasoning from simple label like Label: 1. It requires narrative context. We implement multi-stage Label-to-Caption-to-Instruction transformation pipeline. Stage 1: Structured Attribute Extraction. For every image, we aggregate all available metadata. For public data, we parse DICOM headers and tabular labels. For inhouse data, we extract structured reports including findings, impressions, and ROI (Region of Interest) coordinates. We formalize this into structured JSON object containing key-value pairs representing anatomical location, finding type, severity, laterality, and technical quality. Stage 2: LLM-Driven Semantic Expansion. Raw attributes are often disjointed. To create fluent, coherent medical text, we employ sophisticated prompt engineering strategy using general-domain LLM [19] as rewriting engine. We feed the structured attributes into the LLM with specific system prompt: Act as senior radiologist. Convert the following structured findings into detailed diagnostic report description. Ensure medical terminology is accurate, describe the visual appearance of the findings. The resulting outputs are then reviewed by human annotators. This process expands sparse input into rich caption. Stage 3: Multi-Granularity Instruction Formatting. To enable the model to handle various user intents, we do not simply pair the image with single caption. Instead, we generate diverse instruction-response pairs derived from the generated caption. These include: (1) Descriptive Instructions (e.g. Describe the findings in this chest X-ray.), (2) Diagnostic Q&A (e.g. Is there evidence of pleural effusion? If so, describe its location.), (3) Localization Tasks (e.g. Identify the region responsible for the patients shortness of breath.), and (4) Reasoning Prompts (e.g. Based on the visual evidence, what is the most likely diagnosis and what follow-up is recommended?). 12 3.3.3 Caption To construct high-quality dataset for medical image captioning, we employ multi-stage synthesis pipeline utilizing specialized fine-tuned model. The generation process begins by producing an initial draft caption derived from the source images existing alt text found in large-scale raw data. Subsequently, we extract key medical entities (e.g., pharmaceutical names and disease names) from the initial draft to query an internal knowledge base. The retrieved domain knowledge, combined with the initial draft, is then fed into LLM which generates specific revision suggestions. Based on these expert-level recommendations, the initial draft undergoes final round of modification to produce refined and knowledgeable caption. Following generation, the data undergoes rigorous filtration process to ensure clinical accuracy and descriptive rigor. primary component of this phase is the rubric-based Generative Reward Model (GenRM) designed to evaluate the quality of the caption. Given that full-scale manual inspection of the synthetic data is prohibitively expensive, we sampled subset of data for detailed human annotation, categorizing errors related to visual omissions, incorrect identifications, and lack of rigorous phrasing. These annotations were converted into structured rubrics, which served as the basis for training the GenRM via SFT followed by reinforcement learning. This model automatically scores captions based on weighted objective criteriasuch as anatomical localization, morphological accuracy, and the strictness of evidence-based assertionseffectively filtering out hallucinations or ungrounded diagnostic claims. 3.3.4 OCR Data Source and Collection. Medical images with text were selected from our medical database and processed through standardized preprocessing pipeline, including image deduplication, low-resolution filtering, aspectratio filtering, and corrupted image removal. To selectively recall medical-reporttype images with OCRrelevant content from the corpus, we adopted two-stage weakly supervised strategy: (1) visionlanguage model [19] was used to generate pseudo-labels for 10K sampled images, where medical report was defined as one target category; (2) these pseudo-labeled samples were then used to train ViT-Base classifier, which was applied to the full database to retrieve approximately 9 million medical report images. Data Distillation, Filtering, and Augmentation. For the recalled images, Seed1.5-VL [19] was further employed to distill OCR-style textual descriptions, yielding paired imagetext supervision. We then cleaned the distilled data using rule-based filtering strategy that removed approximately 0.01% of anomalous, noisy samples. subset of the remaining samples was subsequently subjected to human review to ensure quality. Finally, lightweight data augmentation strategy was applied, where 10% of the images were randomly rotated, to improve robustness to orientation variations commonly observed in real-world medical documents. 3.3.5 Grounding Precise object grounding serves as the critical bridge connecting textual semantics with visual regions, capability indispensable for verifiable medical diagnostics [36]. To enhance the fundamental visual grounding precision of MedXIAOHE, we developed an automated pipeline to generate high-quality training data from large-scale inhouse medical image-text dataset. This process begins by utilizing prior internal model to caption images, followed by an MLLM-based visual verifier to perform rigorous, image-grounded entity extraction. Governed by strict system prompt, the verifier cross-references textual mentions with visual evidence, extracting only verifiably present entities while filtering out abstract concepts. This establishes robust foundation for object-level recognition. 3.3.6 Human-in-the-loop Data Generation For other types of data, we develop human-in-the-loop pipeline that is systematically incorporated into the training process. Label-based annotation framework. We adopt standardized label-based annotation paradigm to enable scalable execution and reliable quality control across both data annotation and verification stages. Licensed physicians are recruited to participate in the image annotation workflow, which includes image filtering, label 13 assignment, and result verification. First, annotators assess the quality of each image and discard low-quality or non-medical images. Next, senior medical expert panel defines standardized annotation guidelines, covering anatomical regions or organs, lesion types, lesion attributes, disease categories, and treatment or medication recommendations. Finally, all annotation results are reviewed and corrected by the senior expert panel to ensure consistency, achieving entity-level precision and recall exceeding 95% after expert auditing. VQA synthesis from annotated labels. Using the curated annotated labels together with the image as inputs, we prompt LLMs to automatically synthesize VQA instructions. Owing to their stronger instruction-following capabilities, we employ LLMs rather than VLMs for the synthesis stage. In addition, we incorporate rewriting techniques to further increase the complexity of the generated VQA instructions. General VLM-based data filtering. We apply two-dimensional filtering process using general-purpose VLM, focusing on both relevance and plausibility of the synthesized VQA data. This step aims to remove extreme or abnormal cases, such as answers that are irrelevant, hallucinated, template-based, or not grounded in the image content. Each sample is evaluated and categorized along two axesfor example, image relevance (relevant / uncertain / irrelevant) and answer plausibility (plausible / uncertain / implausible). This process substantially reduces the proportion of superficially valid but practically useless VQA pairs. Expert VLM rollout-based sampling and data validation. medical expert, VLM, is trained on the filtered VQA data and then used for rollout-based sampling to further enforce medical correctness. For each question, the expert VLM performs pass@k rollouts, generating multiple independent answers that are compared against the ground-truth annotations to assess consistency. Samples for which rollouts are fully or partially correct are retained for training, while samples with all incorrect outputs undergo additional sampling or physician manual review before final decision is made. This stage specifically targets the correction of subtle medical inaccuracies that may pass general-purpose filtering."
        },
        {
            "title": "3.4 Training Recipe",
            "content": "Optimizing heterogeneous capabilities within single continual pre-training (CPT) process can induce gradient interference, particularly when medical specialization and general-purpose multimodal reasoning are learned concurrently [20, 50, 62]. Rather than separating training into multiple phases, we adopt singlestage recipe that relies on data ordering to expose the model to structured curricula over unified corpus. The central hypothesis is that the conventional random shuffling paradigm obscures informative structure in multimodal medical data, including domain, modality, and task diﬀiculty, thereby interleaving unrelated objectives and increasing gradient variance and conflicts during optimization. In contrast, an ordered presentation of semantically coherent samples encourages locally consistent updates and yields more stable trajectory for joint capability acquisition. Our data ordering paradigm reorganizes the entire CPT corpus into curriculum over groups of examples instead of isolated instances. We first train lightweight warm-up model on small random subset of the data (e.g., 10%) and compute fixed-length embedding for every training sample by extracting token representations from an intermediate decoder layer (for eﬀiciency) followed by token-wise average pooling. Using these embeddings, we perform dimensionality reduction with UMAP [38] and density-based clustering with HDBSCAN [37] to obtain semantically coherent clusters spanning medical images, reports, captions, OCR-derived text, and general-domain QA. To induce an easy-to-hard curriculum, we rank clusters using compactness score, defined as the mean pairwise Euclidean distance between embeddings of randomly sampled instances within each cluster. We treat more compact clusters (i.e., lower scores) as easier, as they exhibit higher intra-cluster similarity and are expected to incur less gradient conflict. Clusters are then ordered in ascending compactness score. To avoid sharp optimization shifts at cluster boundaries, we introduce smooth transitions between consecutive clusters via mixed regions that jointly sample from both clusters and randomly shuffle the samples. Each mixed region additionally includes small replay buffer drawn from earlier clusters, which reduces forgetting while maintaining forward progress. This single-stage, ordered training schedule produces unified pre-training process that preserves the benefits of joint optimization while systematically reducing gradient conflict through structured exposure, thereby improving stability and data eﬀiciency without requiring 14 explicit stage-wise parameter freezing or capability-specific phases."
        },
        {
            "title": "4 Mid-Training",
            "content": "While foundational pre-training provides broad general knowledge, it does not fully meet the complex requirements of professional clinical diagnosis. To address this limitation, the mid-training phase is designed as critical stage that systematically strengthens the models advanced reasoning abilities. Specifically, this phase aims to: (1) develop atomic combinational skills, allowing the model to integrate individual capabilities, such as visual grounding and tool use, into coherent workflows; (2) substantially improve medical reasoning by moving from simple information retrieval to rigorous, multi-step clinical inference; and (3) build high-quality supervision signal that offers strong initialization for reinforcement learning, supplying reliable policy priors for later alignment and the development of more agentic behavior."
        },
        {
            "title": "4.1 Mid-Training Data Construction",
            "content": "Figure 5 Mid-training data construction overview. The framework illustrates the comprehensive pipeline designed to synthesize high-fidelity medical reasoning data from diverse sources. a, The data synthesis engine aggregates unsupervised and supervised corpora, utilizing knowledge graphs and multi-agent consensus to construct structured reasoning datasets. b, multi-expert reject sampling mechanism with dual-quality gates is employed to distill diverse and causally valid reasoning trajectories. c, The process incorporates structured Chain-of-Thought construction pipeline with automatic quality checks, strictly aligning visual perception with logical deduction to eliminate hallucinations. major challenge in developing medical reasoning models is the limited availability of high-quality chainof-thought data. Although large medical corpora are available, they mainly provide final diagnoses and rarely include the explicit step-by-step reasoning needed to learn clinical logic. To address this gap, we developed multi-dimensional data synthesis engine (see Fig. 5a). The framework supports diverse task formats, including multi-turn question answering, detailed image captioning, and precise visual grounding. Rather than relying on simple data augmentation, it systematically reconstructs the clinical reasoning space. By synthesizing range of reasoning structures, from basic logical inference to complex tool-assisted decisionmaking, the engine provides high-fidelity training signals that better reflect the cognitive processes used by clinical experts. 4.1.1 Internal Reasoning KG-Guided QA Synthesis. The medical domain is characterized by complex clinical problems and diagnostic scenarios that demand robust reasoning capabilities to analyze symptoms, integrate disparate information, and derive reliable conclusions. To elevate the models performance in this regard, we designed several data synthesis methods yielding high-quality medical reasoning data. To enrich the diversity and complexity of medical reasoning QA data, we leverage the KG to synthesize multihop QA pairs. We first evaluate the models performance on KG entities using the pass@k metric to identify weak entities. For these targets, we retrieve supplementary information via our retrieval system, extracting new entities and relationships to update the KG. Starting from these weak entities, we sample complex paths (exceeding 5 hops) via random walks. The terminal node is designated as the answer, while some intermediate attributes are masked or obfuscated along the path to increase diﬀiculty, ultimately generating complex multi-hop QA pairs and reasoning trajectories. To tackle more complex problems and ensure the reliability and verifiability of the reasoning trajectories, we constructed KG-based verifiable reasoning synthesis pipeline. For an existing QA pair, we first extract entities from both the question and the answer and map them to nodes in the graph through multi-stage matching. Next, we search for potential multi-hop paths between the question entities and answer entities and re-rank the candidate paths to retain those that are relevant and correct. Finally, using these filtered reliable paths as conditions, we guide the model to generate verifiable reasoning trajectories. Notably, in the synthesis processes described above, we preserved the native reasoning patterns of our base model. This ensures the training focus remains on the reasoning content itself, avoiding performance degradation potentially caused by pattern distribution shifts. Complementing the structured knowledge synthesis with broader cogniMulti-Expert Reject Sampling. tive diversity, we engineered multi-expert reject sampling pipeline that incorporates multi-domain data (spanning visual examination, medical imaging, diagnosis, and text). This pipeline shifts synthesis from single-source generation to an ensemble-driven distillation process [53] (See Fig. 5b). By orchestrating expert models with varying cognitive configurations and employing best-of-N rejection sampling, we synthesize diverse spectrum of reasoning trajectoriesfrom concise perceptual inferences to multi-step diagnostic deductionsthereby creating high-fidelity search space essential for generalization. We strictly enforce dual-quality gate mechanism that systematically validates both the terminal outcome and the intermediate CoT against clinical causality. This includes Outcome-Verifya comprehensive verification mechanism that supports judgment, selection, fill-in-the-blank, Q&A, and other question types to ensure answer accuracy and Thinking-Verifya mechanism that concurrently performs logical verification of the reasoning chain to ensure its soundness. Through this pipeline, we are able to build fundamental medical reasoning capabilities from scratch. Structured CoT. Complex reasoning ability is crucial for solving diﬀicult medical problems, which often relies on high-quality Chain-of-Thought [54, 55, 66] data. Standard forward rejection sampling often fails in complex tasks (e.g., diagnostic deduction, report generation) due to open-ended divergence. We resolve this via specialized Reverse Structured CoT Synthesis framework. Depending on the input modality, this approach anchors reasoning on low-hallucination caption solely for image-based tasks to prevent perceptual errors, whereas pure text inputs proceed directly to the synthesis phase. To simulate the authentic cognitive process of real clinicians, we employ medical expert role-play strategy that strictly operates without hindsight, ensuring the reasoning path is constructed without premature reliance on the ground-truth answer. By enforcing four-stage structural constraint (i.e., Understanding, Visual Observation / Knowledge Recall, Reasoning and Conclusion), we align the internal rationale with formal diagnostic workflows. In the observation stage specifically, the mechanism performs visual observation for imaging data, while for textual queries, it focuses on knowledge recall to bridge current context with prior medical knowledge. Finally, an automatic quality check validates the generated rationale across six comprehensive dimensions: hindsight-free logic, logical soundness, instruction following, answer consistency, authentic thinking, and global consistency. This ensures the output is not merely linguistic expansion but verifiable reproduction of expert clinical derivation, effectively eliminating reasoning drift, as in Fig. 5c. Personalized Visual CoT. Aggressive training on long reasoning tasks can paradoxically degrade visual pattern recognition. To address this, we developed specialized Dual-Track CoT paradigm. Our initial experiments confirmed pervasive industry-wide bottleneck known as the perception-reasoning conflict: complex, text-heavy reasoning chains tend to detach from visual evidence, causing the model to operate purely in the textual space and leading to significant regressions in fine-grained perceptual tasks. Positing that brevity preserves fidelitywhere concise reasoning prevents the dilution of visual signals and sustains visual attention we identified that short CoT is inherently superior for perception-intensive tasks. Consequently, we developed Personalized CoT Closed-Loop Pipeline that fundamentally reimagines the synthesis process by strictly separating logical content from cognitive style. It begins by generating low-hallucination caption that isolates core visual details, followed by hierarchical reasoning mechanism that constructs first-principles logical core strictly bound to visual observations. Crucially, we employ multi-agent architecturecomprising image analysts, reasoning experts, and style transfer specialiststo adapt this core logic into task-specific formats. By enforcing dual-layer quality gate that validates both the structural logic and the post-adaptation style consistency, this pipeline produces high-fidelity training data that enhances reasoning depth without compromising the models fundamental sensitivity to visual stimuli, successfully breaking the stagnation observed in standard one-size-fits-all reasoning paradigms. 4.1.2 Agentic Reasoning Figure 6 Agentic Data Synthesis and Training Pipeline. The system integrates comprehensive toolset including General Search (Google, Scholar), Medical Search (Drug Labels, Clinical Records), and Image Editing (Zoom In, Rotate). The pipeline synthesizes challenging QA pairs through diﬀiculty filtering, ensuring questions require multistep tool use. Finally, an Agentic RL process trains the model to interact, observe, and summarize, producing structured traces that include specific tool definitions, thought processes, and execution results. Medical DeepResearch. DeepResearch [35, 52] tightly couples search with reasoning: the model decomposes problem step by step, verifies critical details against reliable external evidence, and then produces an answer, which helps mitigate knowledge forgetting and hallucinations. We specialize DeepResearch for the medical domain by equipping the model with medical-specific tools and data, enabling it to use these tools effectively in diagnostic contexts. We integrate standard DeepResearch tools, including General Search, Scholar Search, and Visit. General Search and Scholar Search retrieve evidence from the web and scholarly sources given query, while Visit is invoked when the model decides to open particular URL and extract the information needed, returning concise summary of the page. We further introduce medical-specific tools [17, 51]Search Drug and Search Clinicalfor drug information and clinical cases, respectively. General DeepResearch models are good at fact-seeking tasks but lack medical reasoning expertise. Unlike general factual QA that deals with single-hop medical entity facts, medical scenarios require multi-hop decision-making in consistent clinical contexts, including appropriate tool selection and chaining. To tackle this, we create multi-hop medical questions and their solution paths by building multi-hop entity chains through random walks on an internal medical knowledge graph. This approach faces two core challenges: (1) Coverage: We address it by using common diseases and drugs within each specialty as starting nodes and frequently restarting walks to avoid biased learning in small subgraphs. (2) Relational plausibility: We solve it with an LLM-based sampling method to pick logically and clinically valid next-hop nodes from candidate neighbors. Finally, we use an LLM to generate relevant multi-hop questions, and apply three-tier filtering to ensure their diﬀiculty and quality. Think with Medical Image. To elevate the models visual reasoning ability in medical contexts, we introduce specialized grounded reasoning paradigm centered on anatomical structures. Moving beyond generic object detection, this paradigm establishes anatomical landmarks as the fundamental anchors for vision-language alignment. It implements structured Analyze-Reason-Conclude workflow that mirrors the clinical standard: The analysis phase strictly adheres to the radiological protocol, systematically traversing anatomical landmarks while interleaving textual findings with precise bounding box coordinates. This mechanism explicitly binds reasoning steps to visual evidence, bridging the modality gap. Based on these grounded observations, the reasoning phase synthesizes evidence to deduce pathologies, ensuring that every diagnostic conclusion can be traced to specific visual region. By integrating this anatomy-centered paradigm, we transform grounding from simple detection task into structured, evidence-based reasoning process that significantly enhances the transparency and trustworthiness of the models clinical judgments (as shown in Fig. 11). While anatomical reasoning provides the logical framework, visual limitations can hinder fine-grained detection. To address this, we integrated Think with Image [22, 67] design, enabling the model to perform secondary operationssuch as rotating or zooming into facilitate deeper understanding of visual information. This capability holds tremendous potential in medical imaging; for instance, zooming in on affected regions allows the model to perform granular inspection. Through exploration of tools in the multimodal domain, we identified Zoom in and Rotate as the most valuable primitives: the former is primarily designed for identifying subtle lesions in affected regions, while the latter assists with maintaining spatial orientation during the interpretation of complex medical images. The training dataset is built mainly from radiology imaging data (e.g., X-rays, CT scans, MRI) and public de-identified radiology datasets, which include pathological/normal cases, metadata and expert annotations. Given the varying diagnostic complexity and tool utility of raw radiology data, two-stage filtering pipeline is adopted to create an effective RL training set: Fine-grained perception filtering: multi-agent uncertainty consensus framework is used to select cases requiring localized, region-specific perception (instead of global image context), filtering out easily solvable examples and retaining clinically challenging ones. Task-with-instrument effectiveness filtering: An iterative process assesses example diﬀiculty with base model (without tools) and checks if tool-augmented inference improves correctness, retaining only tooleffective cases. This step is interleaved with training to adapt to the models evolving competence. This filtering pipeline produces high-quality RL dataset focused on diagnostically challenging, tool-enhanced cases, guiding the model to learn appropriate tool use for effective clinical reasoning."
        },
        {
            "title": "4.2 Training Recipe",
            "content": "To effectively synthesize the heterogeneous cognitive architectures into resilient clinical system, we implemented unified, multi-objective curriculum learning framework. This consolidated strategy is explicitly targeted at simultaneously resolving the deficit in fundamental reasoning capabilities and the destructive interference between perceptual fidelity and abstract reasoning. Our approach orchestrates systematic evolution that fuses foundational logic, adaptive execution, and perceptual grounding within single comprehensive training phase through the following strategic pillars: Curriculum Learning. We implement progressive training strategy to build medical reasoning capabilities, advancing from foundational logic initialization to complex long-horizon deduction, then to perceptuallogic alignment, and finally to adaptive hybrid fusion. This hierarchical approach mimics human cognition 18 to systematically resolve core limitations: missing reasoning baselines, logical discontinuities in complex tasks, and the trade-off between perception and reasoning. ViT Joint Training Optimization. Medical VQA relies heavily on distinguishing minute visual discrepancies that frozen encoders might overlook. We unfreeze the visual backbone to enable joint optimization with the language model. This ensures that the extracted visual features are not static, but instead adapt to detect density changes and subtle anomalies specific to medical images. Progressive Distribution Warm-up. To mitigate the risk of catastrophic forgetting and capability imbalance caused by drastic distributional shifts between pre-training and mid-training, we implemented progressive adjustment strategy. Rather than an abrupt regimen change, we gradually anneal the proportion of target reasoning data throughout the training steps. By incrementally increasing the density of complex medical reasoning samples while maintaining replay buffer of general-domain data, we effectively smoothed the distribution fluctuations, allowing the model to securely anchor new capabilities without eroding its original competencies. Balancing Perception and Reasoning. To reconcile the verbose logic of complex derivations with finegrained perception, we implemented dual-track alignment strategy featuring anatomy-centered grounding. Furthermore, to transcend the limitations of closed-system parameterization, we incorporated toolaugmented trajectories derived from medical knowledge graphs, transforming the model into reasoner that maintains perceptual fidelity while verifying critical details with external evidence."
        },
        {
            "title": "5 Post-training",
            "content": "Figure 7 Post-training Pipeline. During post-training, we adopt the same synthesis pipeline as in mid-training, augment it with expert annotations, and perform SFT followed by RL optimization. In parallel, we mine hard negatives via rejection sampling and feed them back into the training loop for iterative refinement. The core objective of post-training is to establish foundational clinical capabilities, focusing on accuracy and regulatory compliance through SFT. We also aim to improve reasoning in complex medical scenarios through RL, ultimately achieving reliability suitable for clinical deployment. The overall post-training pipeline is illustrated in Fig. 7."
        },
        {
            "title": "5.1 Supervised Fine-Tuning",
            "content": "Supervised fine-tuning (SFT) uses the highest-quality data in the training pipeline and typically provides broad task coverage and strong instruction following capabilities, distinguishing it from mid-training. 5.1.1 SFT Data Construction Human annotations. Our human-preference data are triggered primarily by multi-expert consistency mechanism. For the same imagetext prompt, we query multiple mutually independent VLMs to produce candidate responses, and then aggregate and score these candidates using an expert-consistency metric. If model disagreement is substantial, or if key medical entities and conclusions cannot be reliably determined from consistency signals, the sample is escalated to human annotation workflow. During annotation, 19 medically trained annotators consult the multi-model outputs and their consensus evidence to fact-check candidate answers and provide preference ranking, yielding high-quality preference pairs. Synthetic data. Our synthetic preference data follows the closed-loop synthesis pipeline used in the midtraining stage  (Fig. 5)  , with additional enhancements for diversity and robustness. Specifically, we perform prompt rewriting and generate expression variants without altering medical facts or ground-truth answers. Meanwhile, data-balancing module conducts coverage-based sampling over the site/disease/lesion diﬀiculty grid, and applies targeted oversampling to necessary long-tail and high-risk regions. As result, the synthetic preference signal achieves both balanced coverage of overall capabilities and stronger supervision on error-prone critical points. 5.1.2 Instruction Following Instruction following reflects whether model can correctly interpret and execute user requests under constraints such as task scope, formatting, multi-turn context, and safety rules. In clinical settings, this requirement is stricter because instructions are often long-context, multi-turn, and may include implicit constraints or conflicts. Therefore, our SFT stage explicitly targets robust medical instruction following, enabling users to switch task paradigms via configurable prompts. prerequisite for reliable instruction following is accurate instruction understanding. The model must correctly parse explicit rules and infer implicit requirements, especially under layered constraints. We improve this capability from both the data and method perspectives. Instruction Data Analysis and Synthesis. We collect diverse medical tasks from open-source datasets and online resources, and then use LLMs to expand them in depth and breadth. These tasks are standardized into an atomic instruction set. We further sample conflict-free combinations of atomic instructions to construct both single-turn and multi-turn instruction following data, ensuring coverage of departments, task types, and constraint patterns. Real-world medical requests often contain hidden reasoning steps rather than explicit instructions. To model this, we build graph-structured representation that links knowledge nodes, relations, and operations. We then convert the graph into natural-language instructions that preserve implicit reasoning requirements, and use the graph as reference to generate structured rationales for training. Instruction Understanding and Reasoning. Existing models are prone to non-compliance under strict medical constraints, which may lead to safety risks (e.g., violating prohibitive rules). We first conduct large-scale SFT with the synthesized instruction data to strengthen basic compliance. However, forward-only synthesis can introduce subtle instructionresponse misalignment and makes quality control diﬀicult. We therefore adopt reverse construction strategy: start from high-quality clinician-like responses, infer the most compatible instructions and form high-quality instructionresponse pairs, improving instruction parsing and constraint adherence. To further strengthen reasoning under constraints, we introduce structured thinking supervision in SFT. We generate two types of rationales: (1) prompt-guided structured rationales with explicit constraint checking, and (2) rationales expanded from the explicit reasoning graph to encourage fine-grained logical decomposition."
        },
        {
            "title": "5.2 Reinforcement Learning",
            "content": "Following data engineering, mid-training, and SFT, the model possesses extensive domain knowledge but lacks the proficiency to fully mobilize it for complex clinical reasoning. To bridge the gap between latent competence and practical performance, we implement comprehensive Reinforcement Learning (RL) stage. 5.2.1 RL Data Construction We follow the data production logic established in the SFT stage and additionally introduce data-filtering module to select high-value samples from the candidate pool, e.g., complex cases with SFT accuracy in the 60%80% range, so as to prioritize coverage of the models diﬀicult regions. For samples that require LLM-as-a-Judge evaluation, we further extract and construct high-quality, discriminative atomic evaluation points to ensure that the reward signal is clear and reliable. Building on this, we augment the corpus with RL-specific incremental data, including tool-assisted reasoning instances and conflict scenarios in multi-turn dialogues, to strengthen robustness and instruction consistency in complex settings. 20 5.2.2 Multi-Layered Hybrid Reward System The Multi-Layered Hybrid Reward System implements hierarchical architecture that systematically validates, routes, and scores model outputs through specialized evaluation modules  (Fig. 8)  . This framework begins with distributing samples to task-appropriate graders, ultimately synthesizing diverse reward signals into unified optimization objective that balances clinical accuracy, logical coherence, and safety alignment. Figure 8 Architecture of the Multi-Layered Hybrid Reward System. Data Router directs inputs to two parallel reward modules: Rule-based Reward, and Rubric Reward. The Aggregation Layer combines these signals into final scalar reward for RL optimization. Data Router. The Data Router dynamically directs input samples to specific evaluation pipelines based on metadata. Instead of monolithic approach, it interprets prompt intent to select the appropriate Grader routing alignment tasks to the Rule-based and the Rubrics reward model. This ensures reward signals are contextually tailored to specific training objectives. Rule-based Reward. This deterministic evaluator handles tasks with well-defined ground truth that can be verified through exact string matching or regular expression patterns. Employing set-theoretic principles, it compares model predictions against reference answers, awarding proportional credit for partial overlaps and maximum scores for exact matches. All calculations are normalized to unified scale to ensure compatibility within the broader optimization framework. Rubrics Reward. The Rubric Reward mechanism provides unified verification framework that evaluates candidate responses against multidimensional assessment criteria, ensuring comprehensive alignment with both contextual requirements and substantive quality standards. To establish reliable scoring anchors, we generate high-quality reference responses through best-of-N sampling from expert models fine-tuned on domain-specific datasets curated and labeled by human specialists. Hybrid Rubric Construction. We establish evaluation rubrics through dual-pathway approach combining dynamic context adaptation with consensus-based quality standards. The dynamic pathway generates instance-specific rubrics: an Expert Analyst model analyzes the system prompt, dialogue history, and user query to construct tailored evaluation criteria capturing contextual requirements such as logical coherence, empathetic expression, and intent alignment. The static pathway establishes domain-specific gold standard rubrics through multi-model consensus filtering followed by human expert refinement, encompassing critical assessment dimensionsfor instance, Medical Knowledge and Patient Experience in medical dialogue systems. For tasks with explicit reference answers, rubrics incorporate semantic alignment criteria where LLMs/VLMs assess meaning equivalence beyond surface-level lexical matching. Candidate responses are scored against both dynamic contextual rubrics and static domain rubrics, enabling the model to satisfy immediate conversational demands while maintaining substantive quality benchmarks. Process-Supervised Reasoning Verification. To enhance multi-step reasoning capabilities, we introduce dense supervision of the models internal deliberation process encoded in <think> blocks. dedicated evaluation module assesses this intermediate chain of thought across three dimensions: framework completeness (whether all relevant constraints are identified), logical robustness (validity of inference steps), and exploratory depth (thoroughness of solution space examination). This process supervision compels the model to explicitly analyze task constraints and formulate execution strategies before generating responses, substantially improving performance on complex reasoning tasks. To mitigate the instability inherent in pointwise scoring for nuanced medical criteria, we adopted ReferenceAugmented Rubric Evaluation. By anchoring assessments to gold-standard references, this framework reformulates evaluation as pairwise semantic matching task. This methodology significantly reduces variance, ensuring that clinical reasoning is validated against concrete ground truth rather than abstract definitions. Building on this robust baseline, we apply Soft Shaping to optimize auxiliary quality metrics using continuous multipliers that scale the base evaluation scores based on non-core criteria like response length, formatting, and Pharmacist Evaluation. These multiplicative adjustments compound penalties for deviations while preserving value for high-quality outputs, creating smooth optimization gradients that drive balanced performance without compromising semantic accuracy. Reward Signal Fusion. We synthesize unified optimization objective by non-linearly fusing criterion-based medical scoring, fundamental evaluations, and reasoning process rewards with strict safety constraints. This formula is governed by binary gating signal that nullifies the entire reward if critical safety rules are violated. The core reward aggregates these componentsalong with partial order correction against expert baselinesusing dynamically computed weights to balance task characteristics. Specifically, the dominant rubrics-based score is further modulated by soft shaping multipliers, ensuring the final signal rigorously promotes clinical accuracy, logical coherence, and safety alignment. 5.2.3 RFT-Enhanced Curriculum Reinforcement Learning Training medical agents presents fundamental challenge: reconciling heterogeneous multi-modal data spanning basic instruction following to long-horizon clinical reasoning over high-resolution imagingunder layered, expert-driven, safety-constrained rewards (5.2.1, 5.2.2). This heterogeneity creates conflicting optimization pressures that general method cannot resolve. Our preliminary experiments reveal that simultaneous training (Fusion Paradigm) induces gradient conflicts and capability oscillations, while sequential training (Staged Paradigm) causes entropy collapse, where overconfidence on simple tasks eliminates plasticity for complex reasoning. We therefore propose the RFT-enhanced Iterative Curriculum strategy to address these limitations. Iterative Curriculum Strategy. To address the limitations of both the fusion and staged paradigms, we implement an Iterative Curriculum strategy grounded in Pareto multi-task learning theory that alternates between supervised capability distillation and curriculum-based policy optimization. This approach frames the training process as multi-objective Pareto optimization system operating under dynamic constraints. Rather than attempting to resolve data heterogeneity through static mixing or rigid sequencing, we leverage it as controllable variable within dynamic curriculum where rejection sampling fine-tuning (RFT) and reinforcement learning create synergistic improvement cycles. Iteration. Each iteration comprises four-phase progression that implements two core principles: mitigating gradient conflicts through focused micro-cycles that prevent simultaneous destabilization of parameter updates, and regulating entropy via periodic re-injection of foundational data to preserve model plasticity across the capability spectrum. The RFT Phase initiates each cycle by converting the models latent competence into systematic supervision: we sample multiple responses from the current policy for each training instance, identify cases where at least one output meets our multi-layered reward criteria, and distill these superior responses back into the training corpus, systematically converting sporadic successes into reliable behavioral cloning signals. Following this capability distillation, the model proceeds through three comple22 mentary RL curriculum phases that form coherent learning cycle: the Foundation Phase establishes reward stability through short-context instructions and simple multimodal samples, anchoring gradients while maintaining core instruction following capabilities; the Specialization Phase then targets high-diﬀiculty clinical reasoning by substantially upweighting complex sampleslong-horizon cases and high-resolution imaging to push performance boundaries on capabilities that failed to emerge under staged training due to premature entropy collapse; finally, the Alignment Phase reintroduces general domain and safety-constrained samples as both regularization and plasticity preservation, preventing catastrophic forgetting while maintaining safe distributional boundaries and addressing the overconfidence observed in preliminary staged experiments. Benefit. This RFT-driven capability distillation creates synergistic support across the three-phase RL curriculum. During the Foundation Phase, RFT-enhanced data enables stable gradient anchoring by providing cleaner behavioral cloning signals, allowing the model to eﬀiciently internalize correct reasoning patterns rather than learning from noisy generation distributions. In the Specialization Phase, improved supervision facilitates effective reward bootstrapping on complex clinical cases: the model can reliably generate intermediate-quality responses that receive positive reinforcement, enabling productive exploration of longhorizon reasoning and high-resolution imaging analysis that would otherwise yield sparse rewards. During the Alignment Phase, the reduced distribution shift between RFT-supervised and RL-optimized policies prevents catastrophic forgetting by ensuring regularization operates on coherent capability manifold, while also preserving policy diversity to prepare the model for subsequent RFT iterations as frontier capabilities expand. Through iterative cycling, the model achieves stepwise improvement on the hybrid reward signal, progressively integrating capabilities without the gradient interference of simultaneous training or the inflexibility of sequential paradigms. However, two fundamental challenges persist due to medical reasoning data characteristics: the cold start problem in early iterations, where sparse rewards from complex clinical conditions hinder bootstrapping of valid reasoning chains, and exploration stagnation in later iterations, where accumulated confidence suppresses discovery of alternative solution paths. We address these through two stage-specific algorithmic interventions: Dynamic Hint-based Curriculum addresses the cold start problem by providing structured scaffolding during early iterations. We inject hierarchical hintsranging from visual perception cues to intermediate logical stepsinto the input space, serving as gradient primers that transform sparse reward signals into dense learning opportunities. These hints enable the model to bootstrap valid reasoning chains even when facing novel complex tasks. Across subsequent iterations, we apply linear decay schedule to hint density, forcing gradual transition from external assistance to internalized parameters, ensuring zero-shot capability by intermediate stages. Entropy-aware Adaptive Regulation counters exploration stagnation by dynamically modulating the exploration versus exploitation trade-off in later iterations. We integrate real-time monitoring system that tracks policy entropy as proxy for reasoning diversity. When low entropy periods are detectedindicating overconfidence in familiar patternsthe system dynamically introduces an entropy bonus into the loss function. This mechanism prevents premature convergence to local optima, reinvigorating exploration of alternative reasoning paths and ensuring continuous optimization even as performance approaches the ceiling."
        },
        {
            "title": "6 Unified Med-VLM Benchmark",
            "content": "Medical vision-language models are increasingly positioned as foundation components for clinical-facing systems, spanning tasks from visual inspection and radiology understanding to medical dialogue, documentation, and report writing. However, evaluation practice in the medical VLM literature remains fragmented: different works adopt different benchmark subsets, prompting styles, scoring scripts, and data hygiene assumptions, which makes cross-paper comparisons brittle and slows down community convergence on what reliably better means in medicine. This is visible across recent representative systems [31, 48, 49, 56], where reported numbers are often not directly comparable due to protocol-level mismatches rather than model differences. To address this, we introduce Unified Med-VLM Benchmark. The suite consolidates 30+ public benchmarks and pairs them with standardized evaluation harness. 23 Design goals. We build the suite around three practical goals. First, capability coverage: medical models can fail in many different ways, so the benchmark suite should reflect broad capability surface rather than single leaderboard. Second, protocol reproducibility: evaluation results for the same model checkpoint should be stable across runs and robust to minor formatting variations. Third, deployment relevance: beyond answer correctness, we emphasize behaviors that determine usability in real-word products, such as multi-turn consistency, safety-aware responses, and long-form report generation. Across benchmarks, we normalize evaluation with consistent harness: Task normalization: each dataset is mapped into small set of task families (MCQ, short QA, long QA/dialogue, report/caption generation, OCR, agentic search/decision). Prompt templates: we use capability-aware templates that standardize instruction phrasing, answer format constraints, and the placement of auxiliary context. Answer parsing: for closed-form tasks (MCQ/short QA), we enforce deterministic parsing (option extraction, numeric normalization, whitespace/punctuation normalization). For open-form tasks (dialogue/report/caption), we score with task-appropriate metrics and/or rubric-style structured judging to distinguish clinical correctness, completeness, and unsupported claims. Reporting: we report per-benchmark scores and also provide category-level summaries (macro-averaged across benchmarks within category) to avoid single large dataset dominating the overall picture. This protocol turns diverse set of medical evaluations into coherent measurement system, while still respecting each benchmarks original intent."
        },
        {
            "title": "6.1 Public Benchmarks",
            "content": "We organize public benchmarks into six capability categories. Each category has its own table, and each table is referenced in the corresponding capability description to make the mapping explicit and auditable. Visual Diagnosis & Image Recognition. This category targets clinical visual understanding broadly, including recognizing visually salient disease cues and answering multimodal questions that require grounding in the image rather than text-only recall. We include medical subsets of comprehensive multimodal benchmarks to stress generalization across diverse medical visual concepts, as in Tab. 3. Medical Imaging. Medical imaging evaluation emphasizes radiology and pathology perception, multi-scale cues (organ-level, lesion-level, pattern-level), and multimodal fusion between imaging evidence and clinical knowledge. We include benchmarks spanning multiple modalities and task types to measure both breadth (coverage) and depth (reasoning under domain constraints) (Tab. 4). Diagnosis. Diagnosis benchmarks focus on end-to-end clinical reasoning: synthesizing evidence, performing differential diagnosis, handling uncertainty, and generalizing to rare conditions. Compared to exam-style QA, many diagnosis tasks are closer to real clinical narratives and therefore stress robustness to distractors and the ability to justify conclusions from evidence. Tab. 5 summarizes the diagnosis benchmarks we include. Medical Text. Medical text benchmarks measure professional knowledge coverage and text-only clinical reasoning, including licensing-exam formats, biomedical QA, and multi-turn medical dialogue. This category captures common real-world failure modes such as missing knowledge with overconfident reasoning on dosage, contraindications, and guideline questions. We include both Chinese and English evaluations to measure multilingual robustness (Tab. 6). Medical Report. Medical report generation stresses long-form coherence and, more importantly, faithfulness: the ability to describe findings without introducing subtle hallucinations or unsupported claims. Evaluation in this category is particularly sensitive to protocol choices (e.g., what constitutes an omission versus an incorrect addition), which motivates our unified prompting, structured output requirements where applicable, and complementary inhouse faithfulness tests. Tab. 7 lists the report benchmarks included. Instruction-following evaluation measures controllability in long medical dialogues: Instruction Following. constraint satisfaction, memory, consistency across turns, and handling of implicit requirements (e.g., multi24 step instructions embedded in patient-facing interactions). These behaviors often determine whether strong medical knowledge translates into stable user experience under real system prompts. Tab. 8 summarizes the benchmarks used."
        },
        {
            "title": "6.2 Inhouse Benchmarks",
            "content": "Public benchmarks remain indispensable but do not fully cover several deployment-critical axes, particularly for real-world patient images and medical documents. We therefore introduce targeted inhouse evaluations that complement the public suite while following the same unified protocol. These benchmarks are designed to be high-signal gap detectors: they emphasize capabilities that frequently cause failure in practical systems yet are under-measured in the public landscape. Inhouse VQA. We include an inhouse VQA benchmark focused on real-world clinical images where lesion cues and disease regions must be recognized reliably. The benchmark contains more than 100k questions and fully aligns with real-world clinical applications. This benchmark complements the broader public visualdiagnosis probes by testing deployment-like imagery and question styles. Inhouse OCR is constructed from real, human-captured images of medical reports, deliberInhouse OCR. ately retaining perspective distortion, blur, shadows, glare, partial occlusion, and cluttered backgrounds to reflect patient and clinician workflows in routine practice. The benchmark evaluates not only character-level recognition accuracy but also the real-world ability of medical AI assistants to recognize and interpret reports. Inhouse Caption. We introduce an inhouse caption benchmark built from real-world clinical images focusing on lesions and other visually grounded findings, drawn from specialties where image evidence is essential for diagnosis (e.g., dermatology and dentistry). For each image, we curate structured set of clinically meaningful key points, including lesion-focused description, likely diagnosis, and management recommendations. To ensure reliability, the correctness of all key points is cross-validated by two licensed physicians through independent review and adjudication. Building on these verified key points, we propose an automatic scoring scheme for evaluating model-generated captions: correctly covered key points receive positive credit, while incorrect statements are penalized. This reward-and-penalty design encourages clinically faithful, high-utility captions and enables fine-grained comparison of models under consistent, medically grounded rubric."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we presented MedXIAOHE, medical vision-language foundation model designed for strong multimodal understanding and generation in real-world healthcare scenarios. Our results show that MedXIAOHE is competitive across broad range of medical benchmarks and exhibits robust integrated capabilities across medical text, medical images, and clinical understanding. Beyond benchmark performance, MedXIAOHE demonstrates generalization to practical medical workflows that require grounding outputs in evidence and handling multi-step reasoning with external tools. These behaviors are important for moving from isolated tasks toward systems that can support clinicians in realistic settings. Looking forward, our analysis suggests several promising directions. First, scaling training data and compute together with stronger evaluationremains an immediate path to improving performance and robustness. Second, we identify limitations shared by current medical VLMs, including mitigating hallucinations in longform generation, improving reliability under distribution shift, and strengthening multi-step medical reasoning. Addressing these challenges is core part of our ongoing work, including deeper evidence-grounding, more reliable tool-use, and broader coverage of medical concepts and modalities. Overall, we hope MedXIAOHE and the accompanying evaluation framework will help advance the development of trustworthy medical multimodal foundation models and encourage more standardized, clinically meaningful assessment going forward."
        },
        {
            "title": "References",
            "content": "[1] Alfred Aho and Margaret Corasick. Eﬀicient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333340, 1975. [2] Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, and Karan Singhal. HealthBench: Evaluating large language models towards improved human health, 2025. URL https://arxiv.org/abs/2505.08775. [3] Odma Byambasuren, Yunfei Yang, Zhifang Sui, Damai Dai, Baobao Chang, Sujian Li, and Hongying Zan. Preliminary study on the construction of chinese medical knowledge graph. Journal of Chinese Information Processing, 33(10):19, 2019. [4] Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang, Zhihong Chen, Maya Varma, Steven QH Truong, Chu The Chuong, and Curtis Langlotz. Chexpert plus: Augmenting large chest xray dataset with text radiology reports, patient demographics and additional image formats. arXiv preprint arXiv:2405.19538, 2024. [5] Mikaël Chelli, Jules Descamps, Vincent Lavoué, Christophe Trojani, Michel Azar, Marcel Deckert, Jean-Luc Raynier, Gilles Clowez, Pascal Boileau, Caroline Ruetsch-Chelli, et al. Hallucination rates and reference accuracy of chatgpt and bard for systematic reviews: comparative analysis. Journal of medical Internet research, 26(1): e53164, 2024. [6] Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language models on answering In Proceedings of the 2025 Conference of the Nations of the and explaining challenging medical questions. Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35633599, 2025. [7] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, and Benyou Wang. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale, 2024. URL https://arxiv.org/abs/2406. 19280. [8] Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, and Danielle Bitterman. Medbrowsecomp: Benchmarking medical deep research and computer use. arXiv preprint arXiv:2505.14963, 2025. [9] Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, and Ting Chen. Rarebench: Can llms serve as rare diseases specialists? arXiv preprint arXiv:2402.06341, 2024. URL https://arxiv.org/abs/2402. 06341. [10] Zihong Chen, Wanli Jiang, Jinzhe Li, Zhonghang Yuan, Huanjun Kong, Wanli Ouyang, and Nanqing Dong. Graphgen: Enhancing supervised fine-tuning for llms with knowledge-driven synthetic data generation, 2025. URL https://arxiv.org/abs/2505.20416. [11] Gary S. Collins, Karel G. M. Moons, et al. The AIMe registry for artificial intelligence in biomedical research. Nature Methods, 18(11):13331336, 2021. doi: 10.1038/s41592-021-01241-0. [12] CureBench Organizers. Curebench: benchmark and competition for agentic clinical reasoning (neurips 2025), 2025. Accessed 2025-12. [13] Bhishma Dedhia, Yuval Kansal, and Niraj K. Jha. Bottom-up domain-specific superintelligence: reliable knowledge graph is what we need, 2025. URL https://arxiv.org/abs/2507.13966. [14] Dina Demner-Fushman, Marc Kohli, Marc Rosenman, Sonya Shooshan, Laritza Rodriguez, Sameer Antani, George Thoma, and Clement McDonald. Preparing collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304310, 2015. [15] Kaustubh Deshpande, Ved Sirdeshmukh, Johannes Baptist Mols, Lifeng Jin, Ed-Yeremai Hernandez-Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1863218702, 2025. 26 [16] Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. URL https://arxiv.org/abs/2502.14739. NeurIPS 2025 Datasets and Benchmarks Track poster. [17] Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, and Bo Wang. Medrax: Medical reasoning agent for chest x-ray, 2025. URL https://arxiv.org/abs/2502.02673. [18] Sam Farquhar et al. Detecting hallucinations in large language models using uncertainty estimation. Nature, 2024. doi: 10.1038/s41586-024-07421-0. [19] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [20] Yihang Guo, Tianyuan Yu, Liang Bai, Yanming Guo, Yirun Ruan, William Li, and Weishi Zheng. Revisit the imbalance optimization in multi-task learning: An experimental analysis. arXiv preprint arXiv:2509.23915, 2025. [21] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. [22] Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, and Xing Yu. Deepeyesv2: Toward agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025. [23] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2217022183, 2024. URL https://openaccess.thecvf.com/content/CVPR2024/html/Hu_OmniMedVQA_A_New_Large-Scale_ Comprehensive_Evaluation_Benchmark_for_Medical_LVLM_CVPR_2024_paper.html. lvlm. [24] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590597, 2019. [25] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11 (14):6421, 2021. [26] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 25672577, 2019. [27] Alistair E. W. Johnson et al. MIMIC-CXR-JPG, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. [28] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chihying Deng, Roger Mark, and Steven Horng. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. [29] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for eﬀicient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427431. Association for Computational Linguistics, April 2017. 27 [30] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. [31] Ao Li, Bin Yan, Bingfeng Cai, Chenxi Li, Cunzhong Zhao, Fugen Yao, Gaoqiang Liu, Guanjun Jiang, Jian Xu, Liang Dong, et al. Quarkmed medical foundation model technical report. arXiv preprint arXiv:2508.11894, 2025. [32] Chunyuan Li et al. Llava-med: Training large language-and-vision assistant for biomedicine. arXiv preprint arXiv:2306.00890, 2023. [33] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. [34] Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, et al. Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset. Advances in Neural Information Processing Systems, 36:5243052452, 2023. [35] Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, and Yuxiao Dong. Deepdive: Advancing deep search agents with knowledge graphs and multi-turn rl. arXiv preprint arXiv:2509.10446, 2025. [36] Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, and Ting Chen. VividMed: Vision language model with versatile visual grounding for medicine. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 18001821, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.89. URL https://aclanthology.org/2025.naacl-long.89/. [37] Leland McInnes and John Healy. Accelerated hierarchical density based clustering. In 2017 IEEE international conference on data mining workshops (ICDMW), pages 3342. IEEE, 2017. [38] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. [39] James McQueen. Some methods of classification and analysis of multivariate observations. In Proc. of 5th Berkeley Symposium on Math. Stat. and Prob., pages 281297, 1967. [40] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. [41] World Health Organization et al. International classification of diseases-icd. World Health Organization - 2009, 2009. [42] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning, pages 248260. PMLR, 2022. [43] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 337347. Springer, 2025. [44] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [45] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. [46] Peter Rousseeuw. Silhouettes: graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:5365, 1987. [47] Johannes Rückert, Louise Bloch, Raphael Brüngel, Ahmad Idrissi-Yaghir, Henning Schäfer, Cynthia Schmidt, Sven Koitka, Obioma Pelka, Asma Ben Abacha, Alba G. Seco de Herrera, et al. Rocov2: Radiology objects in context version 2, an updated multimodal image dataset. Scientific Data, 11(1):688, 2024. 28 [48] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, Mike Schaekermann, Aishwarya Kamath, Yong Cheng, David G. T. Barrett, Cathy Cheung, Basil Mustafa, Anil Palepu, Daniel McDuff, Le Hou, Tomer Golany, Luyang Liu, Jean baptiste Alayrac, Neil Houlsby, Nenad Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh Azizi, Kimberly Kanada, SiWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He, Ben Caine, Albert Webson, Natasha Latysheva, Melvin Johnson, Philip Mansfield, Jian Lu, Ehud Rivlin, Jesper Anderson, Bradley Green, Renee Wong, Jonathan Krause, Jonathon Shlens, Ewa Dominowska, S. M. Ali Eslami, Katherine Chou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean, Demis Hassabis, Yossi Matias, Dale Webster, Joelle Barral, Greg Corrado, Christopher Semturs, S. Sara Mahdavi, Juraj Gottweis, Alan Karthikesalingam, and Vivek Natarajan. Capabilities of Gemini models in medicine, 2024. URL https://arxiv.org/abs/2404.18416. [49] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, arXiv preprint Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, et al. Medgemma technical report. arXiv:2507.05201, 2025. [50] Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, and Jie Yu. How to alleviate catastrophic forgetting in llms finetuning? hierarchical layer-wise and element-wise regularization. arXiv preprint arXiv:2501.13669, 2025. [51] Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, et al. Medagentsbench: Benchmarking thinking models and agent frameworks for complex medical reasoning. arXiv preprint arXiv:2503.07459, 2025. [52] Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, arXiv preprint Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv:2510.24701, 2025. [53] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/ abs/2201.11903. [55] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language In Proceedings of the IEEE/CVF International Conference on Computer Vision models reason step-by-step. (ICCV), pages 20872098, October 2025. [56] Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning. arXiv preprint arXiv:2506.07044, 2025. [57] Lin yang, Yuancheng Yang, Xu Wang, Changkun Liu, and Yanghaihua. MedMT-bench: Can LLMs memorize and understand long multi-turn conversations in medical scenarios?, 2025. URL https://openreview.net/forum? id=aKyBCsPOHB. [58] Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candès, and Tatsunori Hashimoto. Synthetic continued pretraining, 2024. URL https://arxiv.org/abs/2409.07431. [59] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: In The eleventh international conference on learning Synergizing reasoning and acting in language models. representations, 2022. [60] Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, et al. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. Advances in Neural Information Processing Systems, 37:9432794427, 2024. [61] Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, and Xuanjing Huang. multidimensional constraint framework for evaluating and improving instruction following in large language models, 2025. URL https://arxiv.org/abs/2505.07591. 29 [62] Yiwen Ye, Yutong Xie, Jianpeng Zhang, Ziyang Chen, Qi Wu, and Yong Xia. Continual self-supervised learning: Towards universal multi-modal medical data representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1111411124, 2024. [63] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513415186, 2025. [64] Xiang Yue et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [65] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. [66] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alexander J. Smola. Multimodal chain-of-thought reasoning in language models. Trans. Mach. Learn. Res., 2024, 2023. URL https://api. semanticscholar.org/CorpusID:256504063. [67] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [68] Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Shaoting Zhang, Pengfei Liu, and Xiaofan Zhang. Diagnosisarena: Benchmarking diagnostic reasoning for large language models. arXiv preprint arXiv:2505.14107, 2025. URL https://arxiv.org/abs/2505.14107. [69] Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362, 2025. URL https://arxiv.org/abs/2501.18362."
        },
        {
            "title": "8 Contributions",
            "content": "The authors are listed in alphabetical order by their first names. Contributors Baorong Shi, Bo Cui, Boyuan Jiang, Deli Yu, Fang Qian, Haihua Yang, Huichao Wang, Jiale Chen, Jianfei Pan, Jieqiong Cao, Jinghao Lin, Kai Wu, Lin Yang, Shengsheng Yao, Tao Chen, Xiaojun Xiao, Xiaozhong Ji, Xu Wang, Yijun He, Zhixiong Yang"
        },
        {
            "title": "A Qualitative examples",
            "content": "A.1 Medical DeepResearch Milestones for Medical Deepresearch. Our team has achieved #1 place in both the Internal Reasoning and Agentic Reasoning tracks of Cure-Bench@NeurIPS 2025 (https://curebench.ai/), which is competition and benchmark for evaluating AI models in drug decision-making and treatment planning hosted by Harvard Medical School in NeurIPS 2025. Figure 9 An example of tool-assisted medical reasoning: the model retrieves and verifies evidence before producing the answer. 32 A.2 Think with Medical Image Prompt: Which diseases are visible in the image? Model Responses: <think> The user now needs to follow the analysis steps and first use the ZOOM tool to enlarge the regions in the chest CT that may contain disease. There appear to be two suspected lesions in the image, so both suspicious areas need to be magnified for closer inspection. First, determine that imgidx is 0 (since this is the first image in the current session). The bbox_str should be chosen to include the mediastinum or other potentially abnormal regions. The first suspicious area is located in the mediastinal region. Set imgidx to 0, label to lesion1_calcification, and temporarily set bbox_str to 384 414 585 583. The cropping range should be slightly expanded to avoid missing the boundary. Set scale to 2 to facilitate evaluation of whether this represents calcification. The second suspicious area is located in the peripheral lung region. ... allowing clearer assessment of its margin characteristics and distribution features. </think> <tool_call> arguments: {label: lesion1_calcification, imgidx:0, bbox_str: <bbox>384 414 585 583</bbox>, scale:2}, name: ZOOM, ... </tool_call> <think> After enlarging the two regions separately using the ZOOM tool, the first mediastinal lesion appears as high-density focus with relatively clear boundaries. Its shape and density are more consistent with the characteristics of calcified lesion. The second lesion is located in the peripheral lung. It presents lobulated shape with irregular margins (possibly with spiculations), and its distribution is not along the bronchial pathways, which are imaging features suggestive of peripheral lung cancer. ... </think> After magnified observation of the two suspicious areas, the first is identified as mediastinal calcification, while the second appears as lobulated mass in the peripheral lung with irregular edges and no bronchial distribution, highly suspicious for peripheral lung cancer. In summary, the diseases shown in the image are: mediastinal calcification + peripheral lung cancer. Answer: Lung Cancer Figure 10 An example of magnification-assisted reasoning. The process flows vertically: the model analyzes the original image, calls zoom tool, observes the zoomed-in crop (inserted in the middle), and concludes the diagnosis. A.3 Think with Grounding Prompts: what abnormalities are visible in this image? Model Responses: <think> Okay, am beginning my systematic review of this chest X-ray. Starting with the airways. The trachea [478,10,629,437] appears midline with normal caliber throughout its visualized course. No deviation or narrowing is evident. The right lung [93,62,541,807] shows clear Moving on to the lungs. lung fields in all zones - the right upper lung zone [219,86,541,341], right mid lung zone [170,341,526,514], and right lower lung zone [93,514,502,807] all demonstrate appropriate aeration without focal opacities. Now looking at the left lung [604,62,1063,807] - similarly, the left upper lung zone [604,86,926,341], left mid lung zone [609,341,970,514], and left lower lung zone [624,514,1063,807] appear clear with normal vascular markings. Both right hilar structures [351,307,531,548] and left hilar structures [604,307,780,548] are within normal limits without enlargement or abnormal contour. No evidence of consolidation, nodules, or masses is seen in either lung. Now assessing the cardiovascular structures. The mediastinum [419,144,916,745] appears widened, particularly in the upper mediastinum [478,144,702,437] region. This widening could be related to vascular structures. Looking specifically at the cardiac silhouette [419,442,916,745], theres an increased transverse diameter, which suggests cardiomegaly. The aortic arch [575,245,677,341] appears prominent, which may contribute to the mediastinal widening. The descending aorta [580,317,702,711] also shows some tortuosity. These findings together suggest possible vascular congestion. Next, evaluating the diaphragms. The right hemidiaphragm [93,658,580,807] and left hemidiaphragm [604,658,1063,807] are both visible and appear to be in normal position. The right costophrenic angle [44,735,141,831] and left costophrenic angle [1014,735,1087,831] are sharp and clear without evidence of pleural effusion. The right cardiophrenic angle [380,629,453,740] and left cardiophrenic angle [873,615,975,745] also appear unremarkable. Finally, the spine [93,432,994,577] shows normal alignment. The right clavicle [190,38,521,206] and left clavicle [634,38,965,206] are intact without fractures. There are no visible foreign bodies or devices other than what appears to be deep venous catheter coursing from the subclavian region into the superior vena cava. Conclusion: The primary abnormalities identified on this chest X-ray include cardiomegaly as evidenced by the enlarged cardiac silhouette [419,442,916,745], and mild vascular congestion affecting the mediastinum [419,144,916,745]. The lungs are clear without evidence of consolidation, effusion, or pneumothorax. The trachea is midline, and the diaphragms and costophrenic angles are normal. </think> [{\"finding\": \"Cardiomegaly\", \"bbox\": [419,442,916,745]}, {\"finding\": \"Mild vascular congestion\", \"bbox\": [419,144,916,745]}] Figure 11 Process of Medical Grounding Reasoning: The model takes the raw X-ray (top), analyzes it via logical reasoning (middle text), and outputs grounded bounding boxes for abnormalities (displayed in middle and bottom visualizations). 34 A.4 Expert-Level Complex Diagnostic Question 64-year-old female presents to her physician with complaints of persistent fatigue. She reports decreased appetite, describing constant sensation of fullness and lack of hunger. She also mentions frequent illnesses, noting this is her fourth clinic visit in six months. Her vital signs are stable, but she appears pale and distressed. Physical examination reveals splenomegaly as the only significant finding. Blood tests demonstrate pancytopenia, and her peripheral smear is shown below. Which of the following characteristics is associated with the cells observed on her peripheral smear? Answer Choices: (A) Tartrate-resistant acid phosphatase positive. (B) Myeloid cells in all stages of development. (C) Myeloperoxidase positive Auer rods. (D) > 20% lymphoblast cells in the peripheral smear. Model Response <think> Okay, lets try to figure out this question. First, the question is about 64-year-old female with fatigue, fullness, frequent illnesses, and splenomegaly. The labs show pancytopenia, and we need to look at the peripheral smear to identify characteristic of the cells shown. First, lets look at the smear. The image shows several large, abnormal lymphoid cells. They have fair amount of pale, slightly basophilic cytoplasm. The key feature is the cell border its not smooth. It has these fine, irregular, hair-like projections all around it. The nuclei are round to oval, and the chromatin looks bit loose or stippled, not densely clumped. These look like classic hairy cells. Now, lets connect this to the clinical picture. An older adult with pancytopenia (explaining fatigue/pallor, infections, and likely bleeding risk), splenomegaly (causing the sensation of fullness), and these hairy cells on the smear is the textbook presentation of Hairy Cell Leukemia (HCL). So, the question is really asking: which of these options is characteristic of Hairy Cell Leukemia? Lets think about each option. Option A: Tartrate-resistant acid phosphatase positive (TRAP+). remember that HCL is famous for this. The hairy cells are positive for acid phosphatase, and this activity is not inhibited by tartrate. This was classic diagnostic test before flow cytometry became standard. This seems very likely to be correct. Option B: Myeloid cells in all stages of development. This describes the peripheral smear of Chronic Myeloid Leukemia (CML). CML can cause splenomegaly, but the cells are granulocytes (neutrophils, basophils, etc.), not these lymphoid cells with hairy projections. So, this is incorrect. Option C: Myeloperoxidase positive Auer rods. Auer rods are needle-like inclusions found in myeloblasts. This is hallmark of Acute Myeloid Leukemia (AML). The cells in the smear are not blasts, and they dont have Auer rods. So, this is incorrect. Option D: > 20% lymphoblast cells in the peripheral smear. This is the definition of Acute Lymphoblastic Leukemia (ALL). Lymphoblasts are typically smaller, with very scant cytoplasm, high nuclear-to-cytoplasmic ratio, and condensed chromatin. The cells in the smear are larger and have that distinct hairy cytoplasm. So, this is incorrect. So, the patients clinical presentation and the peripheral smear are classic for Hairy Cell Leukemia. The most specific and well-known characteristic of hairy cells among the choices is that they are TRAP positive. Yes, that makes sense. So the answer is A. </think> Answer: A. Figure 12 An example of complex diagnostic. The model links pancytopenia with splenomegaly to hairy lymphoid cells on the peripheral smear and concludes Hairy Cell Leukemia (TRAP+). 35 A.5 Real-World Clinical Report Generation Prompt: You are helpful assistant. Please generate report for the given images, including both findings and impressions. Return the report in the following format: Findings:{}Impression:{}. Model Response Step 1: Task Understanding need to generate an IU - XRAY style report that includes Findings (objective observations from the chest - rays) and Impression (clinical interpretation). The report must follow the specified format. Step 2: Image Observation Analyze key structures in each view: - Posteroanterior (PA) view: - Trachea: Appears as longitudinal lucent band and is at the midline. - Cardiomediastinal silhouette: Has regular shape. - Lungs: Mainly show black lucent areas; there are no focal high - density infiltrates, no blunting of costophrenic angles (so no pleural effusion), and no abnormal lucent space between the lung and chest wall (so no pneumothorax). - Bony structures (clavicles, ribs, scapulae): The bone cortex is continuous, with no acute fractures or dislocations. - First lateral view: - Thoracic spine: Vertebral alignment is clear and the bone cortex is continuous. - Lungs: The gray density is uniform, with no focal high - density opacities. - Relationship between heart, thoracic spine, and sternum: No abnormal shift. - Costophrenic angle: No increased density or shape change (so no effusion). - Chest wall soft tissues and bony structures (ribs, thoracic spine): No signs of acute injury. - Second lateral view: - Thoracic spine and ribs: The bony structure is intact, with no fractures or osteolytic lesions. - Lungs: The lucency is uniform, with no focal infiltration or abnormal lucent areas caused by pneumothorax. - Mediastinal and cardiac contours: No widening or deformation. - Diaphragm: The position and shape are normal, with no blurring of the diaphragm margin or elevation related to pleural effusion. Step 3: Correlation & Reasoning - Trachea at midline: Indicates no tracheal deviation, which Link observations to medical knowledge: could be caused by conditions like tension pneumothorax or large pleural effusion. - Clear lungs without focal infiltrates: Rules out acute pneumonia, pulmonary edema, and other focal parenchymal diseases. - No pleural effusion (sharp costophrenic angles, normal diaphragm): Excludes fluid accumulation in the - No pneumothorax (no abnormal pleural air): Rules out this acute and potentially life - pleural space. threatening condition. - Normal bony structures: Indicates no acute fractures or bony lesions, such as those from traumatic injury or osteomyelitis. - Normal mediastinal, cardiac, and diaphragmatic findings: Shows no signs of cardiomegaly, mediastinal widening (e.g., from hematoma or adenopathy), or diaphragmatic dysfunction. Step 4: Structuring the Report For Findings, systematically report the status of each component (trachea, mediastinum, lungs, pleural spaces, bony structures) across all views, keeping the description concise. For Impression, synthesize the Findings to state the absence of acute cardiopulmonary or bony abnormalities. Final Answer: Findings: The trachea is midline. The cardiomediastinal silhouette is normal. The lungs are clear, without evidence of focal infiltrate or effusion. There is no pneumothorax. There is no evidence of acute or chronic bone injury. Impression: No obvious cardiopulmonary abnormalities were observed. Figure 13 Example of radiology report generation with structured CoT, showing three input chest X-ray views, the prompt, stepwise reasoning, and the final Findings/Impression output. 36 A.6 Deformed Clinical Report Understanding System: You are an OCR QA model. Your task is to generate response compliant with the requirements based on the input image and the users question. The response requirements are as follows: The answer must be based on the image and cannot exceed the information provided. If the image contains no relevant information, the answer must directly be NOT_FOUND. Use the recognized information to generate the answer. Do not alter the recognized content unless required. Output the answer directly; do not include any explanation or additional text. User Input: User: Is the measured value of mean corpuscular hemoglobin (MCH) within the reference range? Model Responses: <think> The user now needs to judge whether the measured value of mean corpuscular hemoglobin (MCH) is within the reference range. Based on the analysis, should first use the ZOOM tool to magnify the area where this item appears. From the image, mean corpuscular hemoglobin is item 16, located roughly in the middle-lower part. First confirm imgidx is 0 (the first image). The label can be set to Mean Corpuscular Hemoglobin. The bbox_str should cover the row; assume coordinates are <bbox>10 700 990 750</bbox>. Set scale to 2 for clearer viewing. So the first step is to call the ZOOM tool with these parameters. </think> <tool_call> arguments: {label:Mean Corpuscular Hemoglobin,imgidx:0,bbox_str:<bbox>10 700 990 750</bbox>,scale:2}, name: ZOOM </tool_call> <think> The zoomed result shows the mean corpuscular hemoglobin (MCH) measured value is 26.2, and the reference interval is 2734. downward arrow indicates it is below the reference range. Therefore, the measured value is not within the reference range. No further tool calls are needed. </think> Answer: The MCH measured value is 26.2 pg and the reference range is 2734 pg, so it is below the reference range and not within it. Figure 14 An example of precise information extraction from deformed clinical report. The model identifies the specific row in the measurement table, uses the ZOOM tool to correct for resolution and deformation issues, and accurately extracts the numerical value requested by the user. The above content was originally in Chinese."
        },
        {
            "title": "B Evaluation Details",
            "content": "Table 3 Public benchmarks for Visual Diagnosis & Image Recognition. Benchmark What it is designed to evaluate MMMU_val-Med[64] MMMU_Pro-Med[63] Broad medical multimodal understanding and reasoning across disciplines; tests whether models connect visual evidence with domain knowledge. Harder medical subset emphasizing advanced perception and complex reasoning under stronger distractors and tighter requirements. Table 4 Public benchmarks for Medical Imaging. Benchmark What it is designed to evaluate GMAI-MMBench[60] VQA_RAD[30] SLAKE[33] PATH_VQA[21] PMC_VQA[65] OmniMedVQA[23] Broad medical visual evaluation across many modalities and clinical tasks; emphasizes coverage and robustness. Radiology VQA requiring clinically grounded visual understanding and logical reasoning over imaging evidence. Knowledge-enhanced medical VQA; probes multimodal fusion and structured medical knowledge usage. Pathology microscopy understanding with textbook-level knowledge; stresses fine-grained morphology and interpretation. Generative VQA from biomedical literature figures and captions; tests figure understanding and biomedical grounding. Large-scale unified VQA spanning multiple modalities and anatomy; stresses long-tail coverage and generalization. Table 5 Public benchmarks for Diagnosis Benchmark What it is designed to evaluate MedXpertQA-MM[69] MedXpertQA-Text[69] DiagnosisArena[68] HLE-Med[44] RareBench[9] MedBrowseComp[8] Expert-level multimodal medical questions requiring integrated clinical reasoning beyond single-modality cues. Expert-level text questions emphasizing deep medical knowledge reasoning and robust clinical judgement. Case-based diagnostic reasoning from high-quality sources; evaluates end-to-end evidence synthesis and conclusions. Expert-challenging closed-form problems; probes near-ceiling reasoning and failure modes under diﬀiculty spikes. Rare disease reasoning, phenotype extraction, and differential diagnosis; stresses long-tail medical coverage. Medical deep-search / browsing agents; evaluates evidence gathering and fact synthesis for decision making. 38 Table 6 Public benchmarks for Medical Text. Benchmark What it is designed to evaluate PubMedQA[26] MedQA_MCMLE[42] MedQA_USMLE[25] Medbullets_op4[6] Medbullets_op5[6] SuperGPQA-Med[16] CMExam[34] MedMCQA[42] HealthBench-hard[2] HealthBench-normal[2] Reasoning over biomedical abstracts (background, results, conclusions); tests evidence-based reading comprehension. Chinese physician qualification-style questions; probes professional knowledge and clinical reasoning. US medical licensing-style questions; evaluates English medical knowledge and clinical reasoning. Clinical MCQ recall and application with 4 options; emphasizes accurate discrimination under limited choices. Harder variant with more distractors; stresses robustness against confounders. Graduate-level interdisciplinary medical questions; evaluates depth and compositional reasoning. Large-scale Chinese medical exam benchmark; tests coverage and reasoning with fine-grained metadata. Broad-coverage medical MCQs; evaluates general medical knowledge across subjects and diﬀiculty levels. Diﬀicult multi-turn medical dialogues; stresses correctness, safety, and communication quality under hard scenarios. Realistic medical conversations; evaluates helpfulness, alignment, and safety in typical patient-facing settings. Table 7 Public benchmarks for Medical Report. Benchmark What it is designed to evaluate MIMIC-CXR[27] CheXpert Plus[4] IU-Xray[14] Chest X-ray understanding and report generation; tests image-text alignment and clinically grounded descriptions. Fine-grained chest X-ray understanding; probes pathology-level discrimination beyond coarse labels. Radiology report/caption generation; evaluates descriptive faithfulness and content coverage. Table 8 Public benchmarks for Instruction Following. Benchmark What it is designed to evaluate MultiChallenge[15] MedMTbench[57] MulDimIF[61] Complex multi-turn medical dialogues; tests constraint tracking, consistency, and dialogue memory. Long-context medical instruction following with implicit demands; stresses real-world conversational controllability. Constraint-based instruction following across multiple patterns, categories (e.g., content/language/format/length), and diﬀiculty levels using code-verifiable checks."
        },
        {
            "title": "C Prompts for Entity Extraction",
            "content": "Prompt for Stage 2: Joint Extraction and Typing Below are several sentences. Analyze these sentences for **medical** entity nouns and their types, and output according to the following requirements: 1. Entity nouns must be informative proper nouns. Secondarily, determine if they are **medical** entities; if not, do not output. 2. Pay attention to overly long medical entity nouns and determine if they can be segmented/split. 3. The sentences below may contain special symbols and meaningless spaces; please ignore them directly. 4. Replace <EntityType> with the specific entity category. 5. Replace <EntityName> with the specific entity noun. 6. Output strictly in JSON format. The example format is as follows: {{ Sentence0: [<EntityType>:<EntityName>, ...], Sentence1: [...], ... }} Sentences: {lines} Prompt for Stage 3: Entity Tree Attachment You are medical entity taxonomy expert. You must strictly follow the guidelines below to integrate the list of medical entity nouns provide into the existing medical entity tree. **Maintain Original Structure**: You are NOT allowed to change the names of nodes on the medical entity tree. **Precise Insertion**: Insert the medical entity nouns into the appropriate sub-categories of their parent nodes. Ensure only valid medical entities are inserted and adhere to hierarchical relationships. **Rationality of New Insertions**: If you insert medical entity into the tree, you must provide the reason for its insertion to ensure interpretability and traceability. Output Format: <Reason>xxx</Reason><InsertionPath>Node1.Node2.InsertedNode</InsertionPath> **Handling Unclassifiable Cases**: Report medical entity nouns that cannot be classified and explain the reasons for the uncertainty. If unsure about the classification, provide detailed reasoning. Output Format: <Reason>xxx</Reason><Reasoning>yyy</Reasoning> Medical Entity Tree: {tree} Medical Entity Nouns: {entity} 40 Prompt for Stage 3 Agent: Conflict Resolution You are rigorous medical knowledge base construction Agent. Your task is to resolve entity classification conflicts. The current entity {entity} is attached to multiple parent nodes: 1. Parent Path A: {path_a} 2. Parent Path B: {path_b} ... You must search for the exact medical definition via Google/Wiki and adjudicate based on the following principles: 1. Principle of Etiological Dominance: Classification based on pathological mechanism or anatomical location takes precedence over clinical symptoms. 2. Principle of Specificity: If one parent is subset of another and describes the entity more accurately, prefer the more specific one. Thinking Steps: Step 1: Construct query terms and call tools to search for the definition. Step 2: Analyze results and compare the validity of each parent path. Step 3: Decide which path to keep and delete the others. Output Format: <SearchEvidence>Excerpt from Wiki/Search...</SearchEvidence><Reasoning>Since evidence shows..., and Path focuses on XX while Path focuses on XX, according to the Principle of Etiological Dominance, Path is more appropriate.</Reasoning><FinalAction>Keep: {path_a}, Delete: {path_b}</FinalAction>"
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}