{
    "paper_title": "OViP: Online Vision-Language Preference Learning",
    "authors": [
        "Shujun Liu",
        "Siyuan Wang",
        "Zejun Li",
        "Jianxiang Wang",
        "Cheng Zeng",
        "Zhongyu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 3 6 9 5 1 . 5 0 5 2 : r OViP: Online Vision-Language Preference Learning Shujun Liu Fudan University Siyuan Wang University of Southern California Zejun Li Fudan University Jianxiang Wang ByteDance Cheng Zeng ByteDance Zhongyu Wei Fudan University"
        },
        {
            "title": "Abstract",
            "content": "Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the models own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities."
        },
        {
            "title": "Introduction",
            "content": "Large vision-language models (LVLMs) [1, 2, 3, 4, 5] have demonstrated remarkable performance across wide range of multi-modal tasks [6, 7, 8, 9] by integrating pre-trained visual encoders with large language models (LLMs) to process and generate language grounded in visual inputs. However, LVLMs continue to struggle with persistent hallucination issues [10, 11], often exhibiting incorrect references to visual content [12, 13, 11]. These errors manifest as misattributing object properties, describing nonexistent entities, or fabricating spatial relationships that do not align with the image. Such inconsistencies undermine the models faithfulness to the input and hinder further reasoning capabilities, significantly limiting the reliability of LVLMs in real-world applications. Recent success of Direct Preference Optimization (DPO) [14] in LLMs alignment has inspired the exploration of multi-modal DPO to mitigate hallucination in LVLMs [15, 16, 17, 18]. However, early efforts directly extend the original DPO designs from LLMs to LVLMs by constructing preference pairs solely on textual responses given the same image input, primarily focusing on responseside preference optimization and showing limited effectiveness. Recent advancements incorporate additional preference pairs conditioned on varying image inputs while keeping the same response, optimizing both visual and textual preference optimization [19, 20, 21]. This paradigm provides complementary training signal that encourages the model to attend more closely to visual content. However, prior work mainly relies on existing paired datasets [20] or expert-defined pattens to construct negative image inputs, using techniques such as random cropping [19], noise disruption [22], object removal [23], or human/LLMs generated element-replaced response for image editing [17]. shujuanliu24@m.fudan.edu.cn Preprint. Under review. Figure 1: Offline training (a) relies on static, predefined datasets and fails to adapt to the models evolving failure patterns, limiting its ability to address hallucinations effectively. Moreover, neglecting the role of visual input will result in overfitting to language priors. In contrast, OViP (b) combines online preference learning with image-aware training in unified framework, enabling real-time data construction grounded in model behavior. These strategies are typically not explicitly tied to model failures, resulting in distribution misalignment between the generated negatives and the models actual hallucination behavior, thereby offering limited improvement and failing to support adaptive and continual online learning [24]. To address these limitations, we propose failure-guided negative generation strategy that directly targets self-generated hallucinated responses, enabling the real-time creation of more in-distribution counterexamples through text-to-image generation. Specifically, we sample and filter positive and negative response pairs from the models textual outputs. Then we employ LLMs to generate an image prompt based on the negative response, particular emphasizing its differences from the positive response, and subsequently synthesize the corresponding negative image using diffusion model [25]. Building upon this negative generation strategy, we further introduce an online vision-language preference learning framework (OViP) on both response and image sides, to dynamically construct and learn from preference data during training. Similar to reinforcement learning paradigms, OViP samples LVLMs outputs throughout the training process and creates both response-centric and image-centric preference pairs in real time. As illustrated in Figure 1, these dual signals supervise the model to generate outputs more faithfully grounded in visual content. By continuously sampling and integrating new preference pairs based on emerging failure patterns, OViP enables adaptive learning that aligns with the evolving output distributions of the LVLMs. This ongoing adaptation mitigates the limitations of static datasets and reduces the reliance on extensive manual curation, resulting in more robust and generalizable vision-language alignment. We evaluate our framework across diverse multi-modal benchmarks, including hallucination-focused and general benchmarks. Through extensive experiments, we observe notable trade-off between hallucination suppression and general language-vision capabilities, and some existing hallucinationelimination methods prove to overfit to hallucination-related metrics, achieving high scores without genuinely improving visual grounding. For example, some methods reduce hallucination by making descriptions or responses less informative. To address this, we refine existing evaluation protocols and introduce more robust evaluation strategy that jointly assesses hallucination and visual-language competence. Experimental results demonstrate that our method consistently achieves improved performance on both the original and our refined metrics when applied to various LVLMs, demonstrating reduced hallucination and preserved general capability."
        },
        {
            "title": "2 Methodology",
            "content": "In this section, we first provide an overview of the Online Vision-Language Preference Learning (OViP) framework (Section 2.1). We then elaborate the process of constructing the online preference pairs during training (Section 2.2) followed by how to learn from these preference data (Section 2.3). 2.1 Overview As illustrated in Figure 2, our OViP framework is designed to dynamically construct real-time preference pairs during training, by collecting in-distribution success and failure responses along 2 Figure 2: Overview of OViP. Given an image and query, we employ the current model πt to generate multiple responses, which are then evaluated by an external LLM with reference to the ground truth. We filter and select response pairs and use diffusion model to generate corresponding negative images. The collected data are used to update πt. The filtering strategy is detailed in Section 2.2. with their corresponding original and synthesized images. These preference pairs are then integrated into the next training iteration for direct preference optimization on both image and response sides, providing continuous feedback loop that refines the models visual grounding and improves its ability to distinguish high-quality outputs from suboptimal ones. Specifically, given an input image +, an instruction Q, and reference response A, OViP first samples multiple candidate responses using the target model π. These responses are then filtered and selected to form positive and negative pairs (A+, A). Based on the semantic discrepancies between the response pairs, contrastive images are further synthesized to describing the negative responses. Finally, both image-level and response-level contrastive losses are applied to update the target model π. detailed workflow of the OViP algorithm is provided in Appendix C. 2.2 In-Distribution Preference Data Construction We adopt training-time inference to dynamically construct richer preference pairs reflecting indistribution failures concurrently with the training process, expanding limited and static offline datasets. Specifically, our method involves three integral stages: (1) real-time generation of candidate outputs given visual inputs and instructions, (2) quality-aware sampling of informative preference pairs, and (3) inverse construction of input data conditioned directly on these sampled outputs. To ensure training stability, we implement dynamic sampling techniques and an experience buffer. Real-time Generation of Output Data At each training step s, given visual input + and its corresponding textual instruction Q, our model πs generates = 16 candidate responses Ai (i = 1, 2, . . . , k) through stochastic sampling. Each generated response is then individually evaluated by an LLM-based reward function (denoted as Gr), which assigns numerical reward score to each response, reflecting its alignment with the ground-truth answer A. Ai πs (cid:0)I +, Q(cid:1) ; ri = Gr (cid:0)Ai, A(cid:1) (1) Contrasting Response Pair Sampling To effectively learn from preference data, it is crucial to construct pairs with sufficient contrast in quality [26], yet usually struggled when the generated responses exhibit similar quality. To address this, we dynamically construct preference pairs by selecting response pairs within each batch that display significant score disparities. Specifically, for each set of candidate responses {Ai}k i=1, we compute the standard deviation σr of the reward scores and select pairs (A+, A) that satisfy: i=1 with corresponding rewards {ri}k r+ > max (δ, 2σr) 3 where δ is fixed lower-bound margin. This criterion ensures that only response pairs exhibiting substantial contrast in reward scores are selected, effectively emphasizing informative differences between success and failure responses. Additionally, we enforce quality constraints by requiring that the accepted positive responses meet predefined quality criterion (i.e., r+ > τpos), while rejected negative responses fall below specified threshold (i.e., < τneg). In cases where all candidate responses collectively perform poorly, we leverage offline ground-truth answers as positive responses to guide the model learning effectively, practice reminiscent of the mixed-policy approach in [27]. Dpair = (cid:8)(Q, +, A+, A) (cid:12) i=1, (cid:12) A+, {Ai}k r+ > max(δ, 2σr), r+ > τpos, < τneg (cid:9) (2) Inverse Negative Image Synthesis After obtaining response pairs (Q, +, A+, A) Dpair, we synthesize negative images corresponding to negative responses while taking input images as positive. Specifically, we utilize an external LLM (denoted as Gdiff) to identify set of semantic differences between the positive and negative responses, including entities, attributes, and spatial relations, and then generate textual description = Gdiff(Q, A+, A) that encapsulates the semantic content of the negative response A. Subsequently, diffusion-based image generation model (denoted as Diff) synthesizes hard negative image as follows: = Diff(T ) (3) This inverse generation process, in which the image is conditioned on the textual output, ensures that the synthesized image captures hallucinated or incorrect content, providing more targeted supervision for hallucination mitigation. Moreover, as the generation is explicitly driven by response-level discrepancies, the resulting negative images exhibit higher semantic relevance and visual specificity. Dynamic Inference and Experience Buffer To stabilize batch-wise training while retaining the flexibility of online sampling, we maintain an experience buffer that stores dynamically constructed contrastive training samples. At each training step, the current model πs performs inference and response sampling, producing contrastive samples that are continuously added to B. This sampling process persists until the accumulated samples reach the predefined batch size . Once , batch of samples is retrieved from for loss computation and gradient updates. The remaining samples in the buffer are preserved for subsequent iterations, ensuring the training process to proceed smoothly even under variable sampling yields. 2.3 Imageand Response-Side Preference Optimization To effectively align both textual and visual modalities during training, we formulate unified optimization framework that simultaneously considers response-level and image-level preference signals. The overall optimization objective consists of two complementary components. The first is the text DPO loss[14], which guides the model to learn response-level preferences conditioned on the input image and instruction: LText (cid:0)A+, A; +, Q(cid:1) = log σ (cid:18) (cid:20) β log πθ(A+I +, Q) πref (A+I +, Q) log πθ(AI +, Q) πref (AI +, Q) (cid:21)(cid:19) (4) In addition to response-level alignment, we incorporate contrastive objective focused on the visual input. By keeping the query and response fixed, the model is required to learn preferences solely from differences in the visual input. On top of this, to further ensure that the models output maintains reasonable and smooth probability distribution, we introduce the image-free term πθ(AQ) and implement the image-side loss as in [20]: LImage(I +, ; Q, A+) = log σ (cid:18) (cid:20) β1 log (cid:20) +β2 log πθ(A+I +, Q) πref (A+I +, Q) πθ(A+Q) πref (A+Q) log πθ(A+Q) πref (A+Q) πθ(A+I , Q) πref (A+I , Q) log (cid:21) (cid:21) (cid:19) (5) The overall loss function is then defined as: LOViP (cid:0)Q, +, , A+, A(cid:1) = LText (cid:0)A+, A; I, Q(cid:1) + LImage (cid:0)I +, ; Q, A+(cid:1) (6)"
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Experimental Setup Implementation Details We conduct our experiments on LLaVA-1.5-7B-hf and LLaVA-1.5-13Bhf[5], with CLIP ViT-L-336px as the visual encoder and Vicuna-7b/13b as the backbone respectively. The training dataset, sourced from [28], consists of 8,730 samples and 4,013 distinct imagequery combinations, including image description, question answering, and some yes/no questions. Both the 7B and 13B models are trained for single epoch using cosine learning rate schedule with global batch size of 16. We use LoRA [29] with rank of 256 and alpha of 512. We set β = β1 = β2 = 0.1 in Eq. 4 and Eq. 5. Learning rates are 1e-6 for 7B model and 5e-7 for 13B model. Baselines We compare OViP with SFT, DPO [14], mDPO [19] and GRPO [30]. As the original versions of SFT, DPO and mDPO are offline methods, we additionally implement iterative DPO and GRPO to facilitate more comprehensive comparison. Furthermore, we evaluate several prior works with publicly available model weights, including HA-DPO [31], HALVA [18], RLAIF-V [16] and OPA-DPO [28]. Among them, our OViP and OPA-DPO use the same original training data, which is subset of the dataset used by RLAIF-V. 3.2 Evaluation Metrics The evaluation benchmarks are categorized into two types: hallucination-related evaluation and out-of-domain general capability evaluation. Hallucination-Related Evaluation. These benchmarks assess LVLMs hallucination as well as visual competence. We provide illustrative examples in Appendix A.2 to highlight the necessity for improving the metrics for MMHal, AMBER_gen and ObjectHal. MMHal-Bench (MMHal) [32] is model-evaluated question-answering benchmark covering 8 categories and 12 topics. While the original evaluation strategy uses GPT-4 to judge model responses, the use of text-only model will introduce considerable judging-time hallucinations and errors (as pointed by [33]). We following [33] and use gpt-4o-2024-05-13 for evaluation. AMBER generative (AMBgen) [34] is judging-model-free benchmark for the image description task, comprising 1,004 samples. The metric Chair measures the object-level hallucination rate as the average precision of objects mentioned in the models descriptions, while Cover indicates the recall of objects. We observe noticeable trade-off between these two metrics across various methods, where improvements in one often come at the expense of the other. To provide more balanced and overall assessment, we introduce new F1 score calculated as the harmonic mean of Chair and Cover. Object HalBench (ObjectHal) [35] evaluates object-level completeness and hallucination rates using off-the-shelf LLMs. The generation prompts are augmented from [15]. Chairr denotes the response-level hallucination rate. Like AMBER_gen, we also introduce an object-level F1 metric to comprehensively measures the balance between hallucination and object coverage. Objects extraction is performed using gpt-4o-2024-05-13. Llava-Bench-in-the-Wild (LV) [4] evaluates models abilities in visual understanding, reasoning, and instruction following, using 60 open-ended questions grounded in 24 diverse images from real-world and abstract scenarios. The evaluation is conducted using gpt-4o-2024-05-13. AMBER discriminative (AMBERdis) [34] includes 14,216 Yes/No questions regarding objects in image. We use the F1-score as its metrics. To aggregate performance across five hallucination-related benchmarks, we introduce the Hallucination Reduction Index (HRI) as unified measure of overall improvement. Specifically, for each benchmark, we compute the performance change relative to the baseline and apply proportional rescaling to map the values approximately within the range [0, 2]. The final HRI score is obtained by summing the rescaled improvements from all benchmarks. As dynamic metric that depends on actual performance, detailed calculations for each specific HRI are provided in Appendix A.1. Out-of-domain General Capability Evaluation. In addition to the trade-off between hallucination reduction and completeness, we also observe pronounced trade-off between hallucination reduction 5 Table 1: Main Results for OViP and other methods across different benchmarks. The five shaded metrics highlight the primary balanced and overall results for each benchmark. HRI (Hallucination Reduction Index) is the average improvement across five benchmarks. AccDif is the total accuracy changes across TextVQA[37], RealworldQA[36], MMStar[39] and CVBench[38]. GPT4-V()s results are cited from [40][34][41] for reference. indicates the use of original evaluation strategy. denotes methods with publicly released model weights trained on their own datasets, which we direct evaluate without re-training. signifies methods trained on datasets that are the same as or larger than ours, allowing for direct comparison. 2-ep specifies results obtained after two epochs of training. We separate offline methods from non-offline methods for clearer comparison. Chair AMBgen Cover GPT4-V Baseline HA-DPO[31] HALVA[18] RLAIF-V[16] OPA-DPO[28] SFT DPO mDPO DPOiterative GRPO2ep OViP OViP2ep Baseline HALVA[18] OPA-DPO[28] SFT DPO mDPO GRPO2ep OViP OViP2ep 7 - 5 . 1 - L 3 1 - 5 . 1 - L 4.6 7.1 5.6 5.7 3.1 2.4 3.5 3.7 3.4 3.9 4.8 4.0 4.0 6.5 6.0 2.8 4.5 3.6 3.9 3.8 4.4 3. 67.1 50.0 49.4 52.9 49.8 45.2 50.6 48.9 48.6 48.7 51.2 51.1 51.6 51.0 52.2 47.8 50.0 50.6 50.1 52.4 53.1 53.7 F1 78.8 65.01 64.86 67.78 65.79 61.79 66.39 64.86 64.67 64.64 66.59 66.70 67. 65.99 67.12 64.08 65.64 66.37 65.86 67.84 68.28 68.98 MMHal Score 3.49 1.90 1.95 2.12 2.54 2.78 2.52 2.35 2.55 2.32 2.45 2.52 2.65 2.24 2.45 2.88 2.38 2.53 2.51 2.38 2.58 2.57 ObjectHal Chairr F1 LV Score AMBdis F1 13.6 51.38 37.15 43.40 9.35 6.37 20.60 26.60 25.45 27.11 34.98 33.22 29. 46.18 35.07 5.88 31.21 25.00 21.79 23.76 36.30 28.62 - 72.40 73.81 76.01 69.78 63.26 70.30 71.95 73.92 72.33 73.83 73.50 74.18 76.73 77.75 64.46 75.81 75.00 75.35 75.55 76.52 76.75 95.3 57.20 57.30 58.60 58.90 64.80 52.20 56.70 55.80 56.40 58.70 63.10 60. 62.60 61.70 64.70 64.00 65.30 64.50 66.70 64.60 67.90 87.4 85.5 85.4 86.5 86.4 86.7 86.1 86.8 86.1 86.5 86.8 87.3 87.4 89.1 90.0 89.3 89.9 89.6 89.5 90.4 89.7 90.2 HRI - - 1.52 9.08 1.37 -5.60 -1.47 1.65 2.99 1.31 6.75 9.58 10.00 - 4.22 -7.05 1.79 2.42 1.78 4.96 5.25 8.02 General AccDif - - -11.59 -7.36 -6.74 -11.82 -8.07 -3.86 -3.05 -2.98 -3.83 +0.88 -1.01 - -5.45 -15.25 -1.24 +0.12 -1.12 -1.48 +0.85 +2. and general visual capability, i.e., hallucination-targeted models often exhibit performance declines on non-hallucination benchmarks. To assess this impact, we further evaluate LVLMs capabilities on general benchmarks, including RealworldQA [36], TextVQA [37], CVBench [38], MMStar [39]. We aggregate the results across these benchmarks and compute the Accuracy Difference, serving as unified metric to quantify overall performance variation after training. 3.3 Main Results Table 1 presents results of OViP and other methods across multiple benchmarks on various LVLM backbones. OViP consistently achieves significant improvements across most primary metrics while effectively preserving models general visual capabilities, in contrast to most other methods that exhibit varying degrees of degradation in general benchmarks. Moreover, OViPs performance gains become increasingly pronounced with larger LVLM backbones, highlighting its scalability. We further observe that our OViP can continuously learn from itself to enhance performance, with capabilities improving progressively over additional training epochs, demonstrating the self-improving nature of its online learning approach. Notably, even with just one training epoch, OViP surpasses HALVA and the two-epoch GRPO, both of which utilize twice the training data, underscoring the exceptional training efficiency of our approach. critical phenomenon often overlooked in previous work [17, 28, 21, 40, 19, 15, 16] is that offline hallucination methods tend to make model outputs less informative, as evidenced by declined Cover metric in AMBgen. However, information omission is also form of hallucination, especially in image captioning tasks, where omitting entities in the image is effectively equivalent to the model assuming that those entities do not exist. While many prior methods reduce conventional hallucinations, they inadvertently exacerbate omission-based hallucinations. In contrast, OViP achieves more balanced trade-off between informativeness and hallucination reduction. 3.4 Ablation Study The Impact of Loss functions. We evaluated various combinations of loss functions for online preference learning in hallucination mitigation to derive the final formulation in Equation 6. Our ablation study examines the effectiveness of different training objectives, including text-side (LText), imageside and auxiliary losses. Specifically for image-side losses, we examine our image loss LImage alongside two variants Lbase Image and LImageSym. For auxiliary loss, we compare the anchor loss proposed by [19] and the bidirectional anchor loss, which enforce the probability of positive response to increase and the negative one to decrease. Detailed formulations are provided in Appendix B.1. Table 2: Results of different loss functions on hallucination-related benchmarks, with key metrics shown. OViP loss is LText + LImage. means the removal of the corresponding loss from OViP. Loss Functions OViP LText LImage LText + base Image LImageSym Training from scratch AMBgen MMHal AMBdis 65.38 66.61 63.58 65.10 65.54 2.35 2.12 2.06 2.17 2. 86.6 85.4 85.9 86.2 85.6 HRI 4.32 4.23 -2.29 4.08 -0.32 Iterative training ObjectHal MMHal AMBdis HRI 74.14 74.01 72.63 73.45 73.86 2.70 2.60 2.40 2.58 2.41 85.7 86.1 86.6 86.3 86.3 7.94 7.71 4.56 7.50 6.57 We first investigate the effectiveness of text-side and image-side losses, as well as different variants of image-side losses. Specifically, we conduct experiments under two training regimes: (1) training from scratch, and (2) iterative training initialized with DPO-pretrained model using the existing dataset, to ablate these losses on top of different initialized models with varying capabilities. As shown in Table 2, The full OViP loss consistently outperforms all variants under both training regimes, with the LImage and LText + base Image performing comparably. However, when training from scratch, LImage exhibits inconsistent behavior: it significantly improves object-level metrics such as AMBgen, but underperforms on sequence-level metrics such as MMHal and Yes/No questions. In contrast, the text-only loss LText consistently performs the worst, highlighting the necessity of incorporating image-side supervision for mitigating hallucination effectively. Figure 3: Effect of auxiliary loss on text-side and image-side losses. HRI measures hallucinationreduction, while General Score reflects general performance change. Marker color indicates the primary loss function used, and marker shape denotes the usage of auxiliary loss. Figure 4: Results of offline and online training strategy with DPO and OViP. Cover measures the informativeness of model response from AMBER generative benchmark. We further analyze the impact of auxiliary loss in Figure 3. Contrary to the findings in [19], we find that incorporating anchor loss consistently reduces general capability and increases hallucinations across all other loss combinations. Moreover, while applying bi-directional anchor loss slightly improves general capabilities, it does not necessarily enhance hallucination mitigation. Among them, the OViP loss without any auxiliary loss achieves the best performance on both hallucination reduction and general capabilities. We therefore adopt OViP loss as our default training loss function. Online v.s. Offline. Figure 4 demonstrates that online training significantly outperforms its offline counterpart. For the offline setting, we construct training data by sampling and filtering model 7 Table 3: Experiments about the training process. (a): Token-wise average logprobability. Base -0.90 -1.23 DPOOff DPOOn -0.97 -1. -1.08 -1.17 IND OOD (b): Sampling with temp. = 1.2, num_return_sequences = 16 Steps Num. 10 100 500 14.4 15.2 15. 15.5 (c): Negative image construction R.Crop Offline Online HRI General 9.76 0.34 9.23 -0.88 10.88 1.44 outputs prior to training. Whether using DPO or our proposed OViP loss, online training consistently outperforms offline training by margin of no less than 4 points on the HRI metric within just one epoch. This improvement is not limited to the aggregated metric: across every individual benchmark and each corresponding metric, online training yields more stable and superior performance. Detailed results are provided in Appendix B.3. Another notable observation is that online training also enhances the informativeness of model outputs. Even when trained solely with DPO, the Cover score remains above 50. In contrast, previous studies [16, 28, 21] using similar datasets typically exhibit drop in this aspect."
        },
        {
            "title": "4 Further Study",
            "content": "Figure 5: Results using different training strategy. Offline represents training with existing dataset. Off Policy means training with sampled data and Iterative means the dataset of the epoch 2 is from model sampling. Figure 6: Comparison of response quality distributions after training with different methods. The x-axis represents the semantic quality of model responses, with the leftmost region indicating severe hallucinations. The y-axis denotes the probability density over sampled responses. rightward shift in the distribution corresponds to higher overall response quality and reduced hallucination. Online Learners Gets More Figure 5 illustrates how hallucination-related metrics evolve during iterative training under different optimization strategies. key finding is that all non-online methods exhibit varying degrees of overfitting in multi-round training, as evidenced by decline in the HRI. In contrast, only online strategies consistently yield stable improvements. Notably, training with external datasets leads to severe overfitting in the second round. Some prior works (such as [28, 21]) adopt at least two rounds of training on small, static datasets. While they report significant gains on certain metricssuch as AMBgen Chair and ObjectHal Chairra more holistic view reveals substantial overfitting. In Table 1, HA-DPO, RLAIF-V, and OPA-DPO fall into this category. Both Online DPO and Off-Policy DPO exhibit an initial drop in performance, meanwhile, GRPO and OViP show relatively slow improvement during the early stages of training. This phenomenon is attributed to the models initially skewed output distribution, which has low perplexity over few responses it generates (Table 3(a)(b)). At this stage, the model tends to forget more than it learns, leading to temporary decline in performance. As training progresses and the distribution gradually smooths out, the model begins to receive more diverse and informative supervision, allowing it to shift toward higher-score regions. In contrast, Off-Policy DPO continues to rely on fixed data sampled from the early-stage distribution. As the model evolves, this static data becomes increasingly misaligned with the models current distribution, leading to ineffective learning due to the mismatch between the training samples and the models evolving behavior. Learning Dynamics To investigate output distribution shifts, we focus on hard queries in the test set and compare the distributions of sampled responses across models. Our key findings are: (1) Online learning enables both hallucination suppression and promotion of high-quality responses, effectively optimizing both ends of the distribution; (2) Image contrastive learning leads to overall quality improvement. Moreover, as shown in Table 3(c), the quality of negative image samples also matters. Hard negatives constructed via online strategies yield the best training results."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 LVLM Hallucination wide range of prior work has explored synthetic data construction for mitigating hallucination in VLMs, which can be broadly categorized into image-related synthesis and text-only synthesis. On the image side, several approaches leverage entity extraction and masking to perform targeted image editing, generating visually similar but semantically distinct counterfactuals [17, 23]. In contrast, HallusionBench [42] adopts manual approach, carefully crafting counterfactual images to probe specific failure modes. Other works take generative perspective: SynthVLM [43] and SynthEmbedding [44] utilize off-the-shelf models to synthesize new images or directly generate image embeddings for hallucination-aware training. Meanwhile, larger body of research focuses on text-side data augmentation. VoCoT [45] introduces new prompting patterns and response types to generate hallucination-prone QA data at scale. Other works such as [22, 18, 33] introduce noise through perturbation, masking, or controlled corruption to simulate erroneous responses. More recent approaches [40, 15] aim to detect and correct hallucinated content at varying levels of granularity, from token-level edits to full-sequence rewrites. These efforts significantly improve the diversity and coverage of supervision signals available for training hallucination-robust VLMs. 5.2 Allocating Computation on Training Sample Construction Recent research has increasingly adopted the paradigm of allocating additional computation during training to get better training samples. Several studies utilize reinforcement learning with human or AI-generated feedback to guide VLM outputs. RLHF-V [15] leverages fine-grained human annotations to correct hallucinated content, while RLAIF-V [16] replaces human labels with feedback from ensembles of open-source models, significantly reducing annotation overhead. Similarly, OPADPO [28] employs an on-policy editing step prior to DPO, aligning training samples closely with model predictions to enhance data efficiency. Active learning methods further embody this approach, selectively querying uncertain or diverse samples to maximize training informativeness [46], while CLIP-based methods dynamically filtered self-generated samples for high-quality training pairs [47]. Other methods integrate auxiliary reward models or evaluators during training, providing continuous and adaptive feedback loops [32, 48]. Finally, recent approaches incorporate reasoning or editing mechanisms directly into training, using iterative self-feedback or generative data augmentation techniques to dynamically refine model outputs [31, 49]. These strategies improve model alignment and factuality by enriching the quality and relevance of supervision signals during training."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose the Online Vision-language Preference Learning (OViP) framework to efficiently address the pervasive hallucination problem in LVLMs. By integrating online preference learning with image-aware training, OViP enables real-time construction of high-quality contrastive data during trainingwithout relying on teacher LVLM. Furthermore, to better assess the trade-offs between hallucination reduction and overall performance, we refine and extend existing evaluation protocols, proposing more comprehensive metric suite. Experimental results demonstrate that OViP significantly outperforms prior offline/online training approaches, achieving substantial hallucination reduction while preserving general vision-language capabilities. Our investigation into training-time dynamics also sheds light on the underlying mechanisms behind OViPs effectiveness."
        },
        {
            "title": "References",
            "content": "[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., Flamingo: visual language model for few-shot learning, Advances in neural information processing systems, vol. 35, pp. 23 71623 736, 2022. [2] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and M. Elhoseiny, Minigpt-v2: large language model as unified interface for vision-language multi-task learning, arXiv preprint arXiv:2310.09478, 2023. [3] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin, Sharegpt4v: Improving large multi-modal models with better captions, in European Conference on Computer Vision. Springer, 2024, pp. 370387. [4] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, pp. 34 89234 916, 2023. [5] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 29626 306. [6] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. [Online]. Available: https://arxiv.org/abs/2305.06500 [7] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in International conference on machine learning. PMLR, 2023, pp. 19 73019 742. [8] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [9] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [10] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, Evaluating object hallucination in large vision-language models, arXiv preprint arXiv:2305.10355, 2023. [11] Z. Bai, P. Wang, T. Xiao, T. He, Z. Han, Z. Zhang, and M. Z. Shou, Hallucination of multimodal large language models: survey, arXiv preprint arXiv:2404.18930, 2024. [12] H. Liu, W. Xue, Y. Chen, D. Chen, X. Zhao, K. Wang, L. Hou, R. Li, and W. Peng, survey on hallucination in large vision-language models, arXiv preprint arXiv:2402.00253, 2024. [13] Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao, Analyzing and mitigating object hallucination in large vision-language models, arXiv preprint arXiv:2310.00754, 2023. [14] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, in Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36. Curran Associates, Inc., 2023, pp. 53 72853 741. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2023/file/ a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf [15] T. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu, H.-T. Zheng, M. Sun et al., Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13 80713 816. [16] T. Yu, H. Zhang, Y. Yao, Y. Dang, D. Chen, X. Lu, G. Cui, T. He, Z. Liu, T.-S. Chua et al., Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness, arXiv preprint arXiv:2405.17220, 2024. [17] Y. Xie, G. Li, X. Xu, and M.-Y. Kan, V-dpo: Mitigating hallucination in large vision language models via vision-guided direct preference optimization, arXiv preprint arXiv:2411.02712, 2024. 10 [18] P. Sarkar, S. Ebrahimi, A. Etemad, A. Beirami, S. Ö. Arık, and T. Pfister, Data-augmented phrase-level alignment for mitigating object hallucination, arXiv preprint arXiv:2405.18654, 2024. [19] F. Wang, W. Zhou, J. Y. Huang, N. Xu, S. Zhang, H. Poon, and M. Chen, mdpo: Conditional preference optimization for multimodal large language models, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 80788088. [20] S. Wu, F.-Y. Sun, K. Wen, and N. Haber, Symmetrical visual contrastive optimization: Aligning vision-language models with minimal contrastive images, arXiv preprint arXiv:2502.13928, 2025. [21] J. Fu, S. Huangfu, H. Fei, X. Shen, B. Hooi, X. Qiu, and S.-K. Ng, Chip: Cross-modal hierarchical direct preference optimization for multimodal llms, 2025. [Online]. Available: https://arxiv.org/abs/2501.16629 [22] Y. Zhou, C. Cui, R. Rafailov, C. Finn, and H. Yao, Aligning modalities in vision large language models via preference fine-tuning, arXiv preprint arXiv:2402.11411, 2024. [23] J. Lu, J. Li, Y. Gao, J. Wu, J. Wu, X. Wang, and X. He, Adavip: Aligning multi-modal llms via adaptive vision-enhanced preference optimization, arXiv preprint arXiv:2504.15619, 2025. [24] S. Guo, B. Zhang, T. Liu, T. Liu, M. Khalman, F. Llinares, A. Rame, T. Mesnard, Y. Zhao, B. Piot et al., Direct language model alignment from online ai feedback, arXiv preprint arXiv:2402.04792, 2024. [25] C. Zhang, C. Zhang, M. Zhang, and I. S. Kweon, Text-to-image diffusion models in generative ai: survey, arXiv preprint arXiv:2303.07909, 2023. [26] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu et al., Dapo: An open-source llm reinforcement learning system at scale, arXiv preprint arXiv:2503.14476, 2025. [27] J. Yan, Y. Li, Z. Hu, Z. Wang, G. Cui, X. Qu, Y. Cheng, and Y. Zhang, Learning to reason under off-policy guidance, arXiv preprint arXiv:2504.14945, 2025. [28] Z. Yang, X. Luo, D. Han, Y. Xu, and D. Li, Mitigating hallucinations in large vision-language models via dpo: On-policy data hold the key, arXiv preprint arXiv:2501.09695, 2025. [29] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. ICLR, vol. 1, no. 2, p. 3, 2022. [30] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [31] Z. Zhao, B. Wang, L. Ouyang, X. Dong, J. Wang, and C. He, Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization, arXiv preprint arXiv:2311.16839, 2023. [32] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L. Gui, Y.-X. Wang, Y. Yang et al., Aligning large multimodal models with factually augmented rlhf, in Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 13 08813 110. [33] E. Amirloo, J.-P. Fauconnier, C. Roesmann, C. Kerl, R. Boney, Y. Qian, Z. Wang, A. Dehghan, Y. Yang, Z. Gan et al., Understanding alignment in multimodal llms: comprehensive study, arXiv preprint arXiv:2407.02477, 2024. [34] J. Wang, Y. Wang, G. Xu, J. Zhang, Y. Gu, H. Jia, J. Wang, H. Xu, M. Yan, J. Zhang et al., Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation, arXiv preprint arXiv:2311.07397, 2023. [35] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, Object hallucination in image captioning, in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 40354045. [36] xAI, Grok-1.5 vision preview, April 2024, accessed: 2024-12-12. [Online]. Available: https://x.ai/blog/grok-1.5v [37] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, Towards vqa models that can read, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 83178326. 11 [38] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, A. Wang, R. Fergus, Y. LeCun, and S. Xie, Cambrian-1: llms, in Advances in Neural fully open, vision-centric exploration of multimodal Information Processing Systems, A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, Eds., vol. 37. Curran Associates, Inc., 2024, pp. 87 31087 356. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2024/file/ 9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf [39] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin et al., Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [40] W. Xiao, Z. Huang, L. Gan, W. He, H. Li, Z. Yu, F. Shu, H. Jiang, and L. Zhu, Detecting and mitigating hallucination in large vision language models via fine-grained ai feedback, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 24, 2025, pp. 25 54325 551. [41] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, Y. Zang, P. Zhang, J. Wang et al., Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 11 19811 201. [42] T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob et al., Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 14 37514 385. [43] Z. Liu, H. Liang, X. Huang, W. Xiong, Q. Yu, L. Sun, C. Chen, C. He, B. Cui, and W. Zhang, Synthvlm: High-efficiency and high-quality synthetic data for vision language models, arXiv preprint arXiv:2407.20756, 2024. [44] S. Sharifzadeh, C. Kaplanis, S. Pathak, D. Kumaran, A. Ilic, J. Mitrovic, C. Blundell, and A. Banino, Synth 2: Boosting visual-language models with synthetic captions and image embeddings, arXiv preprint arXiv:2403.07750, 2024. [45] Z. Li, R. Luo, J. Zhang, M. Qiu, X. Huang, and Z. Wei, Vocot: Unleashing visually grounded multi-step reasoning in large multi-modal models, arXiv preprint arXiv:2405.16919, 2024. [46] B. Safaei and V. M. Patel, Active learning for vision-language models, in 2025 IEEE/CVF IEEE, 2025, pp. 49024912. Winter Conference on Applications of Computer Vision (WACV). [47] Y. Ouali, A. Bulat, B. Martinez, and G. Tzimiropoulos, Clip-dpo: Vision-language models as source of preference for fixing hallucinations in lvlms, in European Conference on Computer Vision. Springer, 2024, pp. 395413. [48] S. Yan, M. Bai, W. Chen, X. Zhou, Q. Huang, and L. E. Li, Vigor: Improving visual grounding of large vision language models with fine-grained reward modeling, in European Conference on Computer Vision. Springer, 2024, pp. 3753. [49] M. Kim, M. Kim, J. Bae, S. Choi, S. Kim, and B. Chang, Exploiting semantic reconstruction to mitigate hallucinations in vision-language models, in European Conference on Computer Vision. Springer, 2024, pp. 236252."
        },
        {
            "title": "A Evaluation",
            "content": "A.1 Hallucination Reduction Index A.1.1 Metric Design HRI represents an aggregate improvement metric across five different benchmarks. Simply summing the raw scores from each benchmark would not be reasonable or rigorous approach, as the metrics are not directly comparable. Therefore, we calculate the improvement ratio for each benchmark based on its potential improvement range, effectively converting the raw metric gains into an additive proportion of improvement. Furthermore, we employ conservative aggregation method to avoid overestimating the effectiveness of our approach. 12 (cid:19) (cid:19) (cid:19) Let ai, {1, 2, 3, 4, 5} denotes F1AMBgen, ScoreMMHal, F1ObjectHal, LVscore, F1AMBdis respectively, namely the results on each benchmark, superscript base represents performances of the baseline model and ref represents the set reference performances. Then HRI is calculated as: HRI = 2 5 (cid:88) i=1 ai abase abase aref (7) A.1.2 Main Results For 7B model, we set the reference performances as OViP2ep, so it comes: HRI = 2 (cid:18) a1 65.01 67.12 65. + a2 1.90 2.65 1.90 + a3 72.40 74.18 72.40 + a4 57.20 60.90 57. + a5 85.5 87.4 85.5 For 13B model, we also use OViP2ep as the reference performances except for the ObjectHal benchmark which almost all methods fail to improve. We set the reference performance of ObjectHal to 79.0. HRI = 2 (cid:18) a1 65.99 68.98 65.99 + a2 2.24 2.57 2.24 + a3 76.73 79.00 76.73 + a4 62.60 67.90 62.60 + a5 89.1 90.2 89.1 A.1.3 Ablation Study: Loss Functions There is no method surpassing other methods significantly, so we consider the best performance on the benchmark as its reference peerformance. HRI = 2 (cid:18) a1 65.01 68.57 65.01 + a2 1.90 2.70 1.90 + a3 72.40 74.14 72.40 + a4 57.20 64.10 57.20 + a5 85.5 87.20 85.5 A.1.4 Ablation Study: Online and Offline Same as Main Results. HRI = 2 (cid:18) a1 65.01 67.12 65.01 A.1.5 Further Study Same as Main Results. HRI = 2 (cid:18) a1 65.01 67.12 65.01 + a2 1.90 2.65 1.90 + a3 72.40 74.18 72.40 + a4 57.20 60.90 57.20 + a5 85.5 87.4 85.5 + a2 1.90 2.65 1.90 + a3 72.40 74.18 72.40 + a4 57.20 60.90 57.20 + a5 85.5 87.4 85.5 (cid:19) (cid:19) A.2 Bad Cases A.2.1 MMHal Shown in Figure 7, the original evaluation protocol utilizes the text-only gpt-4-turbo-2024-04-09 to evaluate the model response, which has no access to the input image and can only infer from the given image contents and ground truth, so it will lead to many incorrect judgments. We replace it with gpt-4o-2024-05-13, which yields more accurate assessments. A.2.2 AMBER-generative & ObjectHal AMBER uses an automatic method for detecting the hallucination entity, which primarily relies on the pre-defined hallucination words. ObjectHal introduces LLM to extract the mentioned entities, its metrics are basically the same with AMBER. Figure 8 illustrates several cases of misjudgment in AMBER. Since the score is determined solely by the presence of specific predefined words rather than the actual semantic correctness, the hallucination rate (Chair score) is often overestimated. Moreover, this issue becomes more pronounced as the diversity and informativeness of model responses increases. Many methods achieve great improvements in the Chair score (entity-wise hallucination rate), but often at the cost of significant decrease in the cover rate (completeness and informativeness). Figure 9 provides an example of this information deficit phenomenon, which should also be considered in the evaluation of model performance. 13 Figure 7: Text-only LLM can not correctly judge the response. Figure 8: Rule-based extraction will lead to misjudgments to some extent. Figure 9: OPA-DPO fails to mention the man, deficiency that is captured by the Cover metric but often overlooked in previous evaluations. vehicles is incorrectly identified as hallucination word."
        },
        {
            "title": "B Experiments",
            "content": "B.1 Loss Functions"
        },
        {
            "title": "Base image loss Lbase",
            "content": "Image is similar to DPO loss which replace the response pair with the image pair:"
        },
        {
            "title": "Lbase\nImage",
            "content": "(cid:0)I +, ; Q, A+(cid:1) = log σ (cid:18) (cid:20) β log πθ(A+I +, Q) πref (A+I +, Q) log πθ(A+I , Q) πref (A+I , Q) (cid:21)(cid:19) Symmetrical image loss LImageSym considers the negative image and the negative response correct pair, then calculate Image loss using negative response and image as the positive one: LImageSym (cid:18) log β1 = log σ (cid:0)I +, , A+, A; Q(cid:1) = LImage(I +, ; Q, A+) + LImage(I , +; Q, A) (cid:20) πθ(A+Q) πref (A+Q) πθ(A+I , Q) πref (A+I , Q) πθ(AQ) πref (AQ) πθ(AI +, Q) πref (AI +, Q) πθ(A+I +, Q) πref (A+I +, Q) πθ(A+Q) πref (A+Q) πθ(AI , Q) πref (AI , Q) πθ(AQ) πref (AQ) log σ +β2 +β2 log log log log β1 log log log (cid:18) (cid:20) (cid:20) (cid:20) (cid:21) (cid:21) (cid:19) (cid:21) (cid:21) (cid:19) Anchor loss LAnchor directly enforces the probability of positive response to be higher for intuitively better optimization results. LAnchor(A+, A; Q, +) = log σ (cid:18) β log πθ(A+I +, Q) πref (A+I +, Q) (cid:19) Bi-directional anchor loss LBiAnchor not only exerts supervision on the positive response, but it also makes the negative response probability to be lower. LBiAnchor(A+, A; Q, +) = log σ (cid:18) β log πθ(A+I +, Q) πref (A+I +, Q) (cid:19) (cid:18) + log σ β log (cid:19) πθ(AQ) πref (AQ) B.2 Settings By default, we use the following settings: Software infrastructure. In our implementation, we deploy the non-training LLM and diffusion models as services using FastAPI. During training, the system interacts with these services via API calls to obtain feedback, image prompts, and the paths to generated images. Models. The LLM we use for judging response and providing image-generation prompt is Qwen-2.5-7b-instruct (https://huggingface.co/Qwen/Qwen2.5-7B-Instruct). The diffusion model for image generation is FLUX.1-dev (https://huggingface.co/black-forest-labs/FLUX.1-dev). Sampling and Filter. The score is between 0 and 10, which 10 means perfect response and 0 means totally incorrect response. We sample 16 responses for one query and set the lower-bound margin δ to 3. Moreover, the quality criterion coefficients τpos = τneg = 5, which means the score of positive response should be at least 6 and negative response be at most 4. The temperature of the LLM scorer is 0.1. Image Generation. For image prompt generation, we set the models temperature as 0.1, top_p as 0.9, and max_new_tokens as 128. We generate 384 384 image given the prompt with num_inference_steps=40 and guidance_scale=7.5. Table 4: OViP pseudocode Algorithm 1 Algorithm of OViP Input: training dataset = {(I +, Q, A)}; target model π; reward model Gr; prompt generator Gdiff ; diffusion model diff Initialize: experience buffer Output: optimized model π for each (I +, Q, A) do i=1 π(I +, Q) Sample candidate responses {Ai}k Compute reward scores: ri = Gr(Ai, A) Compute standard deviation σr of {ri} Initialize temporary pair list while (A+, A) satisfying: r+ > max(δ, 2σr), r+ > τpos, < τneg do Add (A+, A) to and remove from candidate pool end while if = and mini ri < τneg then Let be the lowest-scoring response Add (A, A) to endif for each (A+, A) do Generate prompt: = Gdiff (A+, A) Synthesize image: = diff(T ) Add (I +, , Q, A+, A) to buffer end for if then Sample samples from for training Compute total loss: LOViP Update π π ηπLOViP endif end for Training. We list training setups in Section 3.1. We perform ablation and further study using LLaVA-1.5-7B. The following describes the relevant experimental settings. B.2.1 Ablation on Loss Functions We fine-tune the model for one epoch using data generated by the model itself immediately before training, following the OViP data construction pipeline. For iterative training, we first fine-tune the base model on the original dataset using DPO to obtain stronger initialization. We then sample and filter 4,730 instances as the second-stage contrastive dataset, which remains fixed across all variants. To improve supervision quality, model responses are annotated using DeepSeek-V3 for more accurate reward estimation. B.2.2 Ablation on Online Learning Although online methods can continuously improve when trained with another epoch, we conduct the experiment with one epoch for both online and offline methods. 16 B.2.3 Further Study We save several checkpoints during training and evaluate them to gain complete understanding of the training process. We select 227 instances from the original OPA-DPO datasetdistinct from the training setfor analyzing changes in token-wise log-probability and output quality distributions. For token-wise log-probability analysis, we treat the original negative samples from the dataset as out-of-distribution (OOD) responses and compute their log-probabilities. For in-distribution (IND) responses, we perform 16 response samplings per query using our model with temperature of 0.2, and compute the average log-probability across these samples. To examine output distribution shifts, we sample 16 responses per query with higher temperature of 1.2, assign scores to each response, and analyze the distribution of these scores. For comparison about different image generation strategy, we implement two other representative methods. 1. Random Cropping (R.Crop). We randomly crop 020% area from the image to form the negative image. This method is from mDPO. 2. Offline construction. We use an LLM to generate the image description with some inaccordance with the positive image, then we use the diffusion model to generate the image according to the description. The prompt is at Table 8."
        },
        {
            "title": "C Algorithm",
            "content": "The pseudocode is at Table 4."
        },
        {
            "title": "D Efficiency and Time Consuming",
            "content": "OViP training takes approximately 17 hours on 7 A800 (40G) GPUs. Among them, 4 GPUs are allocated for VLM training, 1 GPU for LLM deployment, and 2 GPUs for diffusion model deployment. We divide each training step into six stages: sampling (response generation), scoring (response evaluation), description (image prompt construction), negative image (counterfactual image generation), forward (model inference), and post-processing. Figure 10 illustrates the proportion of time spent on each stage, where post-processing refers to the period after forward propagation and before the next training step begins, including gradient accumulation, backpropagation, optimizer updates, and other related operations. Excluding post-processing, the most time-consuming component is the sampling stage, similar to reinforcement learning. This is because it requires autoregressive generation of 16 responses, one token at time. The second most expensive stage is negative image generation. To reduce latency, we parallelize this process by assigning two diffusion models to handle image generation requests from four sampling subprocesses. Additionally, since the experience buffer is implemented independently in our system, repeated sampling by one subprocess may block others due to synchronization constraints. This can indirectly slow down the forward and post-processing stages as some processes await completion."
        },
        {
            "title": "E Limitations",
            "content": "This work introduces an online training framework that integrates dual contrastive learning across vision and language. While our loss function follows the DPO formulation, we do not explore how existing reinforcement learning algorithmssuch as PPO or GRPOcould be effectively combined with image-level contrastive objectives. In terms of evaluation, although we identify and discuss several limitations of prior protocols and propose improved metrics and procedures, the current benchmarks still fall short of fully capturing model capability. We manually identified subset of erroneous cases through inspection, but did not conduct comprehensive correction. Lastly, our data filtering strategy during sampling has not been carefully tuned, and more refined design could potentially lead to better training dynamics and model performance. 17 Figure 10: Time consumption for each stage during training."
        },
        {
            "title": "F Broader Impacts",
            "content": "This work focuses on improving the factual reliability of vision-language models by reducing hallucination. While it does not directly engage with societal applications, it contributes to the broader goal of building more trustworthy and robust AI systems. Although the method itself does not pose obvious risks, we note that even improved generation quality does not eliminate the possibility of misuse, such as producing misleading content. Responsible deployment and proper safeguards remain necessary when integrating such models into real-world applications."
        },
        {
            "title": "Prompt for Quality Judgment",
            "content": "# Task Your role is as discerning assistant tasked with evaluating model responses for multimodal tasks (though you have no access with the image). Upon being presented with question that requires the interpretation of both text and images, you will receive two distinct responses. The first is crafted by our sophisticated multimodal model, while the second represents an approximate ideal answerit may be incomplete. Your objective is to meticulously and precisely assess the model-generated response (the former) based on the provided reference answer (the latter). - Heres how you should approach the assessment process: 1. The quality of the response depends on its accuracy and the degree of adherence to the correct answer. Therefore, if the response is much more detailed than the reference answer, it should not be considered very good response (although it may still be considered good one). 2. Directly provide the score of the response, with full score of 10. Your response should follow this format: \"Score: [x]n\", where \"[x]\" represents the score you give, and \"n\" is line break. 3. Please do not provide additional reasoning, just give the score directly. # Question {question} # Response {response} # Correct answer {answer} Table 5: The prompt for judging model response given the answer. Prompt for image generation-Part 1 # Task Given an unknown image-related question, correct answer, and an inaccurate response, carefully analyze the differences between the response and the answer. Then, provide brief description of the image so that it aligns with the correct answer and differs from the incorrect responses. In other words, infer the content of the image. # Example [Example 1] **<Question>** What is on the sandwich? **<Answer>** The sandwich has tomatoes and lettuce on it. **<Response>** The sandwich has slice of egg and tomato on it. **<Output Description>** sandwich with only tomatoes and lettuce on it. **Explanation**: The answer mentions lettuce and tomato, while the incorrect response mentions tomato and egg. So there is no egg on the sandwich. 19 Prompt for image generation-Part [Example 2] **<Question>** Can you point out the details that make this image unique? **<Answer>** In the image, there is plate with slice of pizza topped with tomatoes, herbs, and cheese. The distinctive detail about the image is that the pizza is missing two slices, leaving just one slice remaining on the plate. This suggests that someone has already started enjoying the pizza. The slice appears to be well-cooked and freshly served, creating an appetizing and mouthwatering scene for the viewer. **<Response>** In the image, there is slice of pizza on plate with tomatoes and cheese. The pizza appears to be homemade and has been cut into two pieces. The tomatoes are sliced in half, revealing their juicy interior. The cheese on top of the pizza is melted, creating delicious-looking dish. Additionally, there is fork nearby, suggesting that someone might be planning to enjoy this pizza soon. **<Output Description>** plate with one-third remaining piece of pizza, topped with herbs, cheese, and tomatoes; someone has finished eating and left. **Explanation**: The answer mentions that only one-third of the pizza remains and that someone has just finished eating and left, which is inconsistent with the response. Therefore, the image should include these two features. [Example 3] **<Question>** Bird or cow? **<Answer>** Bird **<Response>** The bird in the image is small, brown and white bird with distinctive head shape and coloration. It is not cow. The bird is perched on branch, which is situated in front of white building. **<Output Description>** big, blue bird perched on branch in front of black building. **Explanation**: Both the answer and the response mention the bird, but the response is more detailed. So the description should be contrastive to the features of the bird in the response. # Requirements - The description should be brief but precise. - If both the answer and the response are long, focus on describing the one or two most significant differences. - Do not provide any analysis or explanation; only describe the image. - common approach is to describe what is present in the image and what is missing. **<Question>** {question} **<Answer>** {answer} **<Response>** {response} **<Output Description>** Table 6: The prompt for image generation instruction. 20 Prompt for image distortion-Part 1 # Task Given an unknown image-related question, correct answer, and an inaccurate response, carefully analyze the differences between the response and the answer. Then, provide brief description of the image so that it aligns with the correct answer and differs from the incorrect responses. In other words, infer the content of the image. # Example [Example 1] **<Question>** What is on the sandwich? **<Answer>** The sandwich has tomatoes and lettuce on it. **<Response>** The sandwich has slice of egg and tomato on it. **<Output Description>** sandwich with only tomatoes and lettuce on it. **Explanation**: The answer mentions lettuce and tomato, while the incorrect response mentions tomato and egg. So there is no egg on the sandwich. [Example 2] **<Question>** Can you point out the details that make this image unique? **<Answer>** In the image, there is plate with slice of pizza topped with tomatoes, herbs, and cheese. The distinctive detail about the image is that the pizza is missing two slices, leaving just one slice remaining on the plate. This suggests that someone has already started enjoying the pizza. The slice appears to be well-cooked and freshly served, creating an appetizing and mouthwatering scene for the viewer. **<Response>** In the image, there is slice of pizza on plate with tomatoes and cheese. The pizza appears to be homemade and has been cut into two pieces. The tomatoes are sliced in half, revealing their juicy interior. The cheese on top of the pizza is melted, creating delicious-looking dish. Additionally, there is fork nearby, suggesting that someone might be planning to enjoy this pizza soon. **<Output Description>** plate with one-third remaining piece of pizza, topped with herbs, cheese, and tomatoes; someone has finished eating and left. **Explanation**: The answer mentions that only one-third of the pizza remains and that someone has just finished eating and left, which is inconsistent with the response. Therefore, the image should include these two features. Table 7: The prompt for distorted image generation instruction. 21 Prompt for image distortion-Part [Example 3] **<Question>** Bird or cow? **<Answer>** Bird **<Response>** The bird in the image is small, brown and white bird with distinctive head shape and coloration. It is not cow. The bird is perched on branch, which is situated in front of white building. **<Output Description>** big, blue bird perched on branch in front of black building. **Explanation**: Both the answer and the response mention the bird, but the response is more detailed. So the description should be contrastive to the features of the bird in the response. # Requirements - The description should be brief but precise. - If both the answer and the response are long, focus on describing the one or two most significant differences. - Do not provide any analysis or explanation; only describe the image. - common approach is to describe what is present in the image and what is missing. **<Question>** question **<Answer>** answer **<Response>** response **<Output Description>** Table 8: The prompt for distorted image generation instruction."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Fudan University",
        "University of Southern California"
    ]
}